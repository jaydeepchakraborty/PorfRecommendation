Computers & Industrial Engineering 48 (2005) 593–608
www.elsevier.com/locate/dsw

Forming effective worker teams with multi-functional
skill requirements
Erin L. Fitzpatricka,1, Ronald G. Askinb,*
a

b

University of Arizona, 239 Loma Alta Avenue, Los Gatos, CA 95030, USA
Department of Systems and Industrial Engineering, University of Arizona, Tucson, AZ 85721-0020, USA
Received 1 April 2001; revised 1 March 2004; accepted 1 April 2004
Available online 13 January 2005

Abstract
Throughout much of the past century, manufacturing efficiencies were gained by constructing systems from
independently designed and optimized tasks. Recent theories and practice have extolled the virtues of team-based
practices that rely on human flexibility and empowerment to improve integrated system performance. The
formation of teams requires consideration of innate tendencies and interpersonal skills as well as technical skills.
In this project we develop and test mathematical models for formation of effective human teams. Team
membership is selected to ensure sufficient breadth and depth of technical skills. In addition, measures of worker
conative tendencies are used along with empirical results on desirable team mix to form maximally effective
teams. A mathematical programming formulation for the team selection problem is presented. A heuristic solution
is proposed and evaluated.
q 2005 Elsevier Ltd. All rights reserved.
Keywords: Cellular manufacturing; Mathematical programming; Team performance; Heuristics; Employee empowerment

1. Introduction
Facing intensified competition in a growing global market, manufacturing companies are reengineering
their integrated production systems to achieve lean manufacturing (Askin & Huang, 2001). In recent
years there appears to be a trend showing increasing popularity of cellular manufacturing, CM, and other
team-related approaches in the workplace (Bailey, 1997; Hut & Molleman, 1998) to achieve this goal.
* Corresponding author. Tel.: C1 520 621 6548; fax: C1 520 621 6555.
E-mail addresses: efitzpat@brocade.com (E.L. Fitzpatrick), ron@sie.arizona.edu (R.G. Askin).
1
Tel.: C1 408 399 6428; fax: C1 408 256 3039.
0360-8352/$ - see front matter q 2005 Elsevier Ltd. All rights reserved.
doi:10.1016/j.cie.2004.12.014

594

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Substantial research has been performed to improve the grouping of machines and parts into cells as a
result of this trend towards CM (see Askin & Vakharia, 1990; Burbidge, 1975; King & Nakornchai, 1982;
Kusiak, 1987; Shafer, 1998; Singh, 1993; Suresh, 1991, 1992 for general reviews). Much less research has
been conducted regarding the selection of team members and subsequent training requirements.
Manufacturing cells that have been formed solely on machine-part interaction have frequently shown
limited benefits (see Carr & Groves, 1998 for a list of examples). This failure has led researchers to search
for other factors that impact the performance of the work cell, culminating in an increasing interest in the
effects of personal skills and traits in the performance of teams.
The elements of effective team formation are not limited to personal skills and traits (Askin &Sodhi,
1994). Burbidge (1975) listed a set of dedicated workers as a key principle of cell autonomy (or
independence) that in turn is an essential aspect of successful cells in practice. In a survey of industry,
Askin and Estrada (1999) found that training of workers was one of the top concerns when implementing
cells. The conversion from traditional jobshop production to CM brings a new culture context to the
worker team. In creating cells, workers with process oriented skills must be divided into part oriented
teams and assigned to cells with heterogeneous processes. Worker training becomes an integral part of
cellular team formation and success. In creating empowered teams, additional technical, teamwork, and
administrative skills must be developed among the workforce. Cell productivity depends not only on the
technical and administrative skills the workers possess but also the effective interaction among team
members. This interaction and the related personality aspects are difficult to include in the
aforementioned models due to the problems associated with quantifying their measures. Systems
exist that attempt to do so and we will discuss the potential of several of these to be measured
quantitatively as well as their demonstrated impact on team productivity.

2. Problem statement
Based on this need for effective interaction among team members, the purpose of this paper is outlined
as follows. Given an existing labor pool, it is desired to extract multiple teams. This would be required if
we were to shift from a non-cellular manufacturing environment to a cellular manufacturing
environment. In this case we would need to determine which skilled individuals to place together in
which cell. Multiple teams may comprise the entire existing labor pool or just some subset of that pool.
For example, if an entire segment of an organization were shifting to cellular manufacturing, the entire
labor pool might need to be redistributed. However, if only a portion of the organization was being
formed into a small number of teams, the entire labor pool would be considered but only a portion of it
allocated. Depending on the nature of the work, cells could have the same makeup or vary from cell to
cell. We assume the labor pool itself is segregated into skill categories. Each member of the labor pool is
assigned to one and only one skill category. The categories are defined according to the jobs or roles that
need to be fulfilled on the team(s). For example, a team may require a milling machine operator, a
turning machine operator, an inspector and an assembler. Each of these would become a skill category
and any individuals belonging to the labor pool would have to be classified according to one of these
skills. In some instances, workers will receive additional cross-training to become multi-functional, but
we assume that initially they have a key skill or experience. Furthermore, even cells with extensive
cross-training will benefit from having lead individuals for each task to assist in problem solving and
training others.

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

595

There are several assumptions made for this problem scenario. It is first assumed those skill categories
and team skill requirements have been clearly defined to the satisfaction of management. This should be
developed with due care as lack of appropriate skills will prevent a team from completing its job. The set
of individuals in each skill group is assumed known as well. The case where an individual possesses
more that one skill and so may choose which skill to fulfill is not considered here. It is also assumed that
there exist sufficient individuals in the various skill categories to meet the requirements for the teams.
That is, all team skill requirements will be met. It is expected that any deficiencies in requirement
availability have been removed through training and/or hiring as appropriate. Several papers (Askin &
Huang, 1997, 2001; Ebeling & Lee, 1994; Min & Shin, 1993; Süer, 1996) exist that have formulations
for solving the training and related aspects of this problem. It is further assumed that all members of a
skill category possess adequate skill to successfully perform that task in any cell that requires that skill.
While all skill category members may not have equal skill in practice, the assumption may be viewed as
recognition that available individuals must be used, and each team has approximately the same
importance. Necessary enhancements may be achieved through suitable training. We assume that we
have personality profiles of all potential team members being considered with which to measure the
interpersonal mix against desired levels. The desired mix and tools for measuring effective team
interaction will be described in the next section. This interpersonal mix will be an important factor in
deciding the construction of the team.
Fig. 1 illustrates the multiple team, partial labor pool, varying construction problem scenario. All
individuals are classified into appropriate skill categories. The problem is then to assign individuals to

Fig. 1. Illustration of problem statement.

596

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

the disparate teams in such a way that each team has a ‘good’ interpersonal construction and all technical
skill requirements are covered. ‘Good’ interpersonal construction will be defined later.

3. Literature review
It is stated in this paper that several factors including interpersonal relationships are important in
forming teams. For evidence of this see Warner, Needy, and Bidanda (1997). Here, relevant factors in
assigning workers to cellular teams from both technological and human interaction perspectives are
discussed. In addition, the problem statement made several assumptions. Mainly it was assumed that
skill requirements were identified and necessary training had been determined and carried out. Several
papers show these factors in the absence of interpersonal relations. In Süer (1996) mixed integer and
integer programming models to achieve optimal product and worker assignment to labor-intensive
manufacturing cells are developed. Min and Shin (1993) developed a multi-objective model to form
cells with consideration of both machines and multi-skilled workers. Training was not considered in
these models. The following papers give insight into how this has been addressed. Ebeling and Lee
(1994) analyzed the cost and benefit of employee cross-training process on a mixed-model assembly line
and formulated a mixed integer programming model to guide cross-training assignments for a specific
number of assembly jobs and workers. Askin and Huang (2001) formulated an integer programming
model for an aggregate worker assignment and training problem for use in converting a functionally
organized manufacturing environment into a CM arrangement. Suresh and Slomp (2001) also provide a
cross-training model and link team formation with the cell formation problem.
In addition to the personal skills and training requirements addressed in the previous section, a
comprehensive evaluation of personality factors is necessary for an effective picture of team
performance. Wechsler (1997), as president of the Division of Clinical and Abnormal Psychology,
‘holds that intellectual ability (including skills) cannot be measured independently of drive,
temperament, or emotion’, hereafter referred to as personality traits. There are an increasing variety
of suggested methodologies for the definition and use of these traits in the formation of successful teams.
They all address the same basic questions: can we improve the performance of teams through the
inclusion of such traits and if so, how? Although psychology literature does not provide a universal
consensus on personality elements, we find that most commonly, as Wechsler (1997) puts forth, there are
three main classifications of these elements: cognitive, conative, and temperament. Cognitive refers to
intellectual ability. This is represented in the individual’s ability to learn the skills required, thus we do
not consider this here. Conative refers to a person’s will, drive or instinct; that approach which they will
feel drawn to take in any given situation. Temperament delves into the emotional state and behavior of
the individual.
Several sources demonstrate the validity of various types of personality measurement in models of
team performance (Bailey, 1999; Barrick & Mount, 1991; Henry & Stevens, 1999; Lingard & Berry,
2000). Juran, Schultz, Hammer, McClain, and Schruben (1997) proposes that any variables representing
psychological or behavioral characteristics must meet certain criteria before they are used to model a
system:
(1) Variables must be stable enough over time so as to justify efforts to plan production systems around
them.

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

597

(2) Variables must be easily and reliably measured.
(3) Variables must be demonstrably linked to the productivity and/or quality of the production system.
These criteria are used in support of the personality dimension measurement system commonly referred
to as ‘The Big Five’. It focuses on the temperament element of personality. The variables used in this
metric are Neuroticism, Extraversion, Openness, Agreeableness, and Conscientiousness. The study also
included additional demographic factors. The results of the experiment demonstrated that most of the
metrics could be used to explain a large proportion of the variability in productivity. Another frequently
employed metric is the social-style implement, Myers-Briggs Type Indicator (MBTI). Hammer and
Huszczo (1996) review its use in improving and predicting team performance. Major results in this
regard appear limited to a negative correlation between ‘similarity’ according to the MBTI and
performance. Hermann (1996) confirms that teams of individuals with diverse thinking styles obtain
better results than homogeneous teams. Individuals are classified according to their Human Brain
Dominance Index (HBDI) reflecting relative affinity for creativity, facts, form, and feelings. Belbin
(1981) evaluates the formation of teams in a management setting using eight roles of team members:
company worker, chairman, shaper, plant, resource investigator, monitor-evaluator, team worker, and
completer-finisher. Studies and anecdotal evidence support this method of team formation (Belbin,
Aston, & Mottram, 1976; Belbin, Watson, & West, 1997; Henry & Stevens, 1999). However, Henry and
Stevens (1999) comment on the need for subjective expertise for the analysis to be effective and
Furnham, Steele, and Pendleton (1993) question the validity of its psychometric value.
The Kolbe Conative Index (KCI) gives a psychometric system that can be used to form successful
teams. This system is presented in Kolbe (1989, 1993). The index measures conation, which refers to the
part of the mind that controls conscious effort and strives to carry out volitional acts. In other words, it
measures the way an individual instinctively approaches problem solving, arranges ideas or objects, and
uses time and energy. Behaviors are seen as varying across fact finding, procedure, intuition, and model
building. The KCI has some resemblance to the HBDI of Hermann (1996) in the sense that thinking and
behavioral styles are divided into four modes with some clear similarities in the definition of modes for
the two systems. In both systems, each individual can be portrayed by a profile across these styles,
although the KCI index has a more definitive numerical definition. Several studies have shown the index
to be effective in predicting team performance. Lingard and Berry (2000) report that a team synergy
measure computed from the KCI showed a significant correlation with team performance in software
engineering. Fitzpatrick, Askin, and Goldberg (2001) likewise found significant correlation when
evaluating design teams composed of engineering students. An additional experimental study with MBA
students at the University of Chicago concluded that the Kolbe Concept was accurate in predicting the
performance of teams (Tepke & Davis,1990). Due to the heavy use of the KCI measure in this research,
we explain it more fully in the next section.

4. Kolbe Concept measures
The Kolbe Concept offers one approach that satisfies Juran’s criteria. Profiles of individuals may be
quickly constructed through an on-line test. Testing and retesting of individuals showed that a
significantly high percentage of individuals stay within 10% of their original scores (Kolbe, 1989).

598

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Additionally, the unbiased nature of the Kolbe Concept carries importance when considering people for
placement in employment or for a position within an organization. When statistical analysis is carried
out on the very large pool of individuals who have completed a Kolbe index measurement tool, there is
no correlation with a specific score to gender, race, or socio-economic background (Kolbe, 1989).
4.1. Individual Kolbe measures—the Kolbe conative index
The Kolbe Conative Index (KCI) measures an individual’s instinctive behavior or drive. Instinct
refers to an innate, action-oriented, sub-conscious drive to approach and perform tasks in a specific
manner. It is not a learned behavior but an inherent part of who we are. The KCI defined instincts are
neither ‘good’ nor ‘bad’ instincts, merely a means to classify and understand the reasons we do things the
way we do or react to things in certain ways.
The Kolbe Concept classifies instinctive behaviors into four categories: probing, patterning,
innovating and demonstrating. Each category has its own corresponding ‘Action Mode’. The
probing instinct creates a need to investigate in depth. Its corresponding action mode is termed Fact
Finder. Fact Finder behaviors are generally linked in some way to the gathering of information;
someone who initiates in this instinctive trait will be concerned with details, strategies, research,
etc. The patterning instinct causes us to seek a sense of order. The corresponding action mode here
is called Follow Through. This mode is characterized by how a person relates to structure. The
third category of instinct is innovating, which is the force behind experimentation. Its action mode,
Quick Start, is distinguished by perspective on risk. Finally, the demonstrating instinct converts
ideas into tangible form. The related action mode of Implementor determines how we relate to
objects and physical space. The Kolbe Concept considers that each person has a finite amount of
instinctive energy that is allocated over the four modes. How much or how little instinct an
individual has in each mode defines an instinctual makeup.
Using a bank of test questions to identify action preferences, individuals can be classified into
Prevent, Accommodate, and Initiate zones for each mode. Prevention is characterized by the instinct to
resist a form of action mode. Fundamentally, a person who is in the prevention-operating zone for an
action mode will tend to avoid behaving in that mode and attempt to reign in someone who is behaving in
that mode. For example, if an individual is in the prevention zone for follow through, they will tend to
prevent over-regulating or getting boxed in by someone who has a significant amount of follow through
instinct. Response is characterized by the ability to accommodate behavior in an action mode. While
someone who is in the response zone may not initiate a type of behavior they can certainly work in this
way for a limited amount of time; either a small to moderate portion of each day or all day for a short
time. An initiator will often insist performing in the corresponding action mode given any choice. They
will tend to initiate that type of behavior and are most comfortable working in that way.
4.2. Kolbe team measures
Now that we have discussed measures of an individual’s instinctive talent, we shift our focus to
understanding how this will help us form effective and productive teams. The primary goal in all of the
measures we will present is balance. A team’s success or failure hinges strongly on the balance of
conative energies present within it. A raw team score is constructed by calculating the percent of the
team’s members who fall in each operating zone for each action mode. This is shown in Fig. 2 below.

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

599

Fig. 2. Kolbe team synergy.

Synergy is the first measure we will discuss. It refers to the distribution of team energy across
the operating zones, m, without respect to the action modes, t. Kolbe Corp. has determined through
experimental evidence that the optimum distribution of synergy is 25% prevent, 50% respond, and 25%
initiate. Allowances are given of G5, G10, and G5%, respectively. Synergistic teams are generally
more successful because there is enough ‘push and pull’ from members’ differing natural strengths and
ways of acting (prevention and initiation zones) to create a balanced opposition of efforts. Second, an
ideal team has enough members that can bridge the difference, evaluate and accommodate actions of
both sides (response zone), and help to create a mutually acceptable solution in the creative problemsolving process. The synergy of the team shown is shown in Fig. 2.
It can be seen in Fig. 2 above that the team measured only deviates from the optimum range in the initiate
operating zone, and by only one percent. The Kolbe Concept uses a measure termed profitability to indicate
the deviation from ideal synergy limits. For simplicity, and in keeping with the idea that any deviation from
C
target is undesirable, we will define synergy deviation to be the positive ðdK
m Þ or negative ðdm Þ deviation from
the ideal mix. Synergy is then the score out of one hundred that represents how close the team is to ideal
synergy. High synergy indicates the team uses an appropriate amount of energy initiating solutions to
problems, accommodating actions required, and preventing problems from occurring.
A second measure we consider is inertia. Instead of aggregating across action modes, inertia reviews
them individually. Inertia occurs when there is an excess of conative energy in a combination of action
mode and operating zone. An excess ðdK
mt Þ is considered anything above 30% in action mode in the
prevent or initiate zone and above 60% in the respond zone. To illustrate, consider a team that is heavily
weighted in fact finder initiators. The team is likely to be unable to make decisions or take action, as
there will be a strong sentiment that more data and justification is needed. A team with an excess of
follow thru resistors, will refuse to do adequate planning and be thwarted by constant surprises and
uncoordinated efforts.

600

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

The level of inertia present on a team is quantified by a measure out of one hundred computed in
a fashion similar to synergy but summed across all modes. (The Kolbe Concept uses the term
goal attainment as a measure of the team’s deviation from the desirable inertia limits). The team in Fig. 2
has a poor inertia score indicating there is a stagnation of energy on the team. Passive behavior,
sluggishness, and procrastination characterize the team. Finally, viability is also a score out of one
hundred and is a weighted average of synergy and inertia that gives a comprehensive prediction of team
performance.

5. MIP formulation
5.1. Labor pool extraction model—multiple teams
The situation we will model requires one to form multiple teams from a pool of candidates with the
necessary skills. The teams may consist of all or some of the available employees. This situation often
occurs when converting to a new cellular manufacturing layout. The model assumes that the partmachine cells have been predetermined, the set of employee skills for the cells has been evaluated, these
skills are available in the pool of employees and each employee has been categorized as to the skill role
they fill. The problem then becomes the selection of the appropriate employees for each role in the
various cells in such a way that the team performance indices of synergy and inertia are maximized. The
model is:
Decision variables:
YikZ1 if worker i is assigned to team k and 0 otherwise.
Data coefficients:
PjZset of workers in skill group j;
aitmZ1 if worker i exhibits operating zone m for action mode t and 0 otherwise;
SjkZnumber of workers of skill group j required for team k;
wm, wmtZrelative weights for zone and mode-zone combinations;
ZmZdesired level of zone m across all modes t (Z1ZZ3Z1, Z2Z2);
ZmtZmaximum desired level of mode t at zone m.
Labor pool extraction model—multiple teams (LPEMT):
Minimize D Z

XX
k

subject to:
X
Yik Z Sjk ;
i2Pj

m

c j; k

K
wm ðdC
mk C dmk Þ C

XXX
k

m

wmt dK
mtk

(1)

t

(2)

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

X

Yik % 1;

ci

601

(3)

k

XX
t

X

K
aitm Yik C dC
mk K dmk Z Zm $

i
K
aitm Yik C dC
mtk K dmtk Z Zmt $

i

Yik 2f0; 1g;

X

X

Sjk ;

c m; k

(4)

c m; t; k

(5)

j

Sjk ;

j

All vars R 0

The objective function (1) computes the deviation from optimal Kolbe indices across all teams.
Only the deviations in excess of the maximum values are included for the zone/mode combination as
we are unconcerned with how low the values reach. Note that the sum over zones within a mode is
fixed to 100. Thus, the values are correlated and significant shortages in one combination necessarily
reflect excess levels in one or more others. The factors are weighted to facilitate model flexibility.
Eq. (2) ensures that each team is formed with the appropriate number of members from each skill
group. Eq. (3) ensures that a worker is assigned to at most one team. Eqs. (4) and (5) define the
deviations for each team from the optimal synergy and inertia indices used in the objective function.
The optimal values of the indices are set as recommended by the Kolbe system. Finally, we enforce
the binary and non-negativity restrictions on the variables. If required, the model can be run with
only one team. The model could be easily modified to minimize maximum deviation for any team or
to set upper bounds on deviations.

6. Heuristic approach
Initial computational experience (using CPLEX) indicated that even moderately small instances of
LPEMT are difficult to solve optimally by standard integer programming techniques. In this section we
propose a heuristic for solving the group formation problem. The notation used is given below.
Notation:
Pj, set of employees in skill group j
Fl, set of unfulfilled teams
Tk, members of team k
Qs, set of unfulfilled skill groups
aitmZ1 if employee i operates in zone m for action mode t,
Z0 otherwise
njZnumber of individuals available from skill group j

j2[1, J] skill groups
i2[1, I] employees
k2[1, K] teams
t2[1, 4] action modes; FF, FT, QS, I, respectively
m2[1,3] operating zones; P, R, I, respectively
nkZnumber of members required for team k

When multiple teams are required, we propose a Balanced Placement Heuristic. This algorithm is
designed to combine operating zones of high density in available employees with operating zones of low
density in partially formed teams. This idea is combined with careful selection and matching procedures
to form the Balanced Placement Heuristic described below (Fig. 3).
Our approach will be to sequentially build teams, always trying to obtain a good balance of modal
instincts in teams, but constraining assignments to the least flexible skill group-mode combination. The
justification is that we want to retain flexibility to balance mode properties with the latest assignments.

602

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Fig. 3. Balanced placement Heuristic.

We hope to avoid the bad properties of many greedy heuristics that begin strong but are forced to make
arbitrarily bad decisions at the end of the assignment stage.
The heuristic begins by computing the average concentration of each skill group in each zone.
After scaling for the desirable concentration in the accommodate zone, the skill group-zone combination
with the least balance (highest concentration) is selected and we assign workers from this skill group
to teams.

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

603

6.1. Balanced Placement Heuristic
(1) Initialize. Obtain all 4-tuple employee Kolbe indices and convert to aitm values. Let Qs be the set of
skills that are required to form the teams. Let Fl be the set of required teams.
(2) Calculate zone distribution of workers in each skill group:
Wmj Z

4
XX

aitm =nj

c m Z 1; 3; c j 2Qs ;

i2Pj tZ1

Wmj Z

4
1 XX
a =n
2 i2P tZ1 itm j

c m Z 2; c j 2Qs :

j

(3) Select the skill group-operating zone combination
with highest concentration. Set (m*, j*)Z
P
argmaxm,j (Wmj). Break tie by selecting max ð 3mZ1 Wmj Þ, if still tie choose smallest index.
(4) Assign. Place one i2Pj* into each team Tk where k2Fl with a requirement for same.
(5) Update. Remove all placed personnel from Pj* and remove skill group j* from Qs.
(6) Update skill balance. Recompute zone distribution as in Step (2) using unassigned workers.
(7) Select most concentrated skill group operating zone combination as in Step (3).
(8) Identify team zone scores. Calculate
Uk Z

4
XX

aitm =nk

c k 2Fl :

i2Tk tZ1

(9) Select minimum team score. Set (k*)Zargmink (Uk), where k2Fl. Ties go to the smallest index.
(10) Evaluate eligible assignments. Calculate
X
aitm c t; m:
Vtm Z
i2Tk

Calculate

Di Z

ditm Z jVtm K aitm j

c i 2Pj ; t; m Z 1; 3;

ditm Z 2jVtm K aitm j

c i 2Pj ; t; m Z 2:

4 X
3
X

ditm

c i 2Pj ;

Di0 Z ðmaxðditm ÞÞ

c i 2Pj :

tZ1 mZ1

(11) Assign worker that most improves score. Set (i*)Zargmaxi (Di), place i* in Tk*. If tie, select
employee i* corresponding to (i*)Zargmini (D 0 i). If tie persists, choose smallest index.
(12) Update needs and worker availability. Remove employee i* from Pj* and remove team k* from Fl.
(13)
If all teams’ requirements have been met, go to (14). If Fl;0, go to (9), else remove skill group j*
from Qs. If Qx;0, go to (6), else reset Qs to include all skill groups still required by teams and go to
(6).
(14) Report teams Tk ck, END.

604

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Table 1
Balancing heuristic example problem
Worker

Skill group

Team ID

KI (matrix form)a

1
2
3
4
5
6

1
2
1
2
1
2

999
999
999
999
999
999

1
0
0
0
0
1

a

00
01
10
01
01
00

00
10
10
10
01
10

11
00
00
00
00
00

00
01
01
01
10
10

00
10
10
10
01
00

1
0
0
0
0
1

Each set of three consecutive values corresponds to resist, accommodate, and insist for a mode.

The balancing heuristic sequentially selects skill groups with poor distribution of worker profiles and
matches that group with the team most in need of that operating zone. The group individual most
different from the current partial team is assigned. When a concentrated skill group is identified, an
inner loop attempts to add one of its individuals to each team requiring that skill until all teams have
been examined. The next most concentrated skill group is then identified and the process repeated.
When all skill groups have been allocated once, the entire process reinitializes considering the remaining
skill needs and available workers.
Example. We briefly illustrate the procedure with an example. Consider the formation of two teams,
each requiring one worker from each of two skill groups. The six available individuals are shown in
Table 1. First note that an individual’s Kolbe Index defines the aitm values. For instance a KCI score of
8363 indicating a Fact Finder insist, Follow Thru resist, Quick Start accommodate and Implementor
resist would become the following for worker i:
2
3
0 1 0 1
6
7
83630 4 0 0 1 0 5:
1

0

0

0

Rows indicate zones and columns the modes.
Step 1. Table 1 gives the worker number, skill, team, and KCI. The team id is initially allotted as 999
to signify that the teams are empty.
Step 2. The weights wmj are determined by summing across operating zone m and skill group j.
To normalize, we divide by the number of people in a group and divide operating zone two
by two so that all three zones can be compared directly. Thus, w11Z4/3Z1.33, likewise
w21Z0.50, w31Z1.67, w12Z2.00, w22Z0.17, w32Z1.67.
Step 3. The maximum weight from step two is w12. Hence we let the operating zone m*Z1 and skill
group j*Z2.
Step 4. We start by placing a member from skill group two into each of the teams. The team for
individual 2 is 1 and the team for individual 4 is 2.
Step 5. After removing workers 2 and 4, the skill group operating zone weights for remaining
available workers are updated to: w11Z1.33, w21Z0.50, w31Z1.67, w12Z0.00, w22Z0.00,
w32Z0.00. (We only consider skill group one because skill group two has already been
assigned).

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

605

Step 6–7. The maximum weight is 1.67 and we select operating zone m*Z3 and skill group j*Z1.
Step 8. To select the team in which a member of skill group one will be placed, we look for low
concentration in m*Z3 in the current partially formed teams. The weight for both partial
teams is (1C0C1C0)/2Z1 As the weights are identical we arbitrarily choose k*Z1.
Step 9–12. Recognizing that the team needs a member different than those already assigned, we
compute a distance measure Duv between candidate u and the partial team v. The distances
for potential members of skill group one are found by comparing the corresponding rows in
Table 1 column by column. The differences for worker 1 are 1,0,1,1,0,1,1,0,1,1,0,1 for a
weight D11Z8. Similarly (noting zone 2 differences are doubled), D31Z3 and D51Z8.
There is a tie for the maximum distance. We break the tie by selecting the member with the
smallest maximum element difference. This corresponds to worker 1. Thus, worker 1 is
placed in Team 1, and Team 1 is temporarily removed from consideration.
Step 13. We have not met all the requirements. We have not considered all teams on this pass so we
return to Step 9.
Step 9. Team 2 is selected for consideration. Comparing partial Team 2 to remaining skill group 1
workers, we select worker 5, the most discordant. All requirements are met and the process
ends with workers 1 and 2 in Team 1 and workers 4 and 5 in Team 2.

7. Empirical evaluation of Balanced Placement Heuristic
The Balanced Placement Heuristic was coded and tested on a series of problems. Problem
characteristics are described in Table 2. Nine different problem sizes were tested. Ten problem instances
were randomly generated for each trial set. Each test problem was required to have four skill groups.
Each team required an equal number of individuals from each skill group. Thus, if there were 12
members per team, each skill group would contribute three members.
Bounds on the team performance measures were calculated for each data set. Values are deviations
from the ideal balance for synergy and inertia. Team viability was taken as a weighted average of
synergy and inertia, with a 55% weight on synergy. Synergy, inertia and viability deviation scores
were normalized to have a maximum of one. The lower bounds for synergy were determined by
Table 2
Description of test problems
Trial set

Number of teams

Members per team

Member pool

1
2
3
4
5
6
7
8
9

4
4
4
8
8
8
12
12
12

4
8
12
4
8
12
4
8
12

16
32
48
32
64
96
48
96
144

606

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Table 3
Trial results
Trial set

1
2
3
4
5
6
7
8
9

Average maximum team deviation

Average team deviation

Synergy

Inertia

Viability

Synergy

Inertia

Viability

0.272
0.266
0.246
0.267
0.278
0.260
0.282
0.345
0.273

0.133
0.110
0.105
0.150
0.144
0.134
0.162
0.196
0.174

0.209
0.196
0.182
0.214
0.218
0.203
0.228
0.278
0.228

0.141
0.120
0.114
0.114
0.121
0.109
0.112
0.125
0.106

0.0797
0.0664
0.0611
0.0795
0.0798
0.0792
0.0788
0.0865
0.0854

0.113
0.096
0.090
0.098
0.102
0.096
0.097
0.108
0.097

summing the operating zones for the entire pool of members and dividing by the number of teams. The
result was then normalized to a value out of one. This represents the best possible distribution of possible
team members to teams for synergy. The lower bounds for inertia were constructed similarly but instead
of summing over just operating zones, they were calculated over each combination of operating zone and
action mode. Values were divided by the number of teams and the result was also normalized to a value
out of one. This represents the best possible distribution of possible team members to teams for the
Inertia measure. The weighted average of these two measures is the lower bound on viability.
The results were then compared to the lower bounds and a difference obtained. The average
differences from lower bounds of these trials are shown in Table 3. The maximum deviation columns
refer to the average over a trial set of the largest deviation of any team for a particular problem solution.
The lower bounds were also calculated for the pool of individuals whose team assignment was solved
to optimality. These lower bounds and the corresponding performance measures for the optimal solution
are given in Table 4.
It is apparent from the tables that the heuristic provided reasonable solutions. Average deviations
from the lower bound were 11.8% for synergy and 7.7% for inertia. However, these bounds may not be
feasible. From Table 4 we see that solutions exceeded the optimum by between 2 and 6%.

8. Conclusions
Team performance depends on individual behaviors and interpersonal interactions as well as
technical competence. The Kolbe Concept describes one measure, the conative or instinctual tendencies
Table 4
Performance of optimally solved problem
Measure

Bounds

Maximum deviation
among teams in the trial

Average deviation over trial

Synergy
Inertia
Viability

0.0000
0.1450
0.0652

0.125
0.093
0.110

0.0583
0.0237
0.0428

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

607

of individuals that can capture this information and serve as a performance metric in quantitative team
design models. A mathematical formulation of the team construction problem using a set of labor skill
pools was presented. Due to computational complexity a construction heuristic was proposed that
guarantees satisfying technical requirements while attempting to form high performance work teams.
The heuristic itself had both positive and negative performance characteristics. The average deviation
from the lower bound was always less than 12% in our experiments. In itself this is a positive result. It
has the potential to be improved with the addition of stronger lower bounds. As we would expect, the
maximum deviations from the selected measures are higher but we still, on average, stay below 22%.
There are some situations, however, where the heuristic has the possibility of performing poorly. For
instance, if the pool of potential members is significantly greater than the required number of members,
the calculated weights might not be a good indicator of potential problem areas. Concentrations in each
operating zone/action mode combination may provide a better route. Another potential shortcoming is
the method of team member selection. Members are selected only considering one measure, inertia. This
could lead to arbitrarily bad solutions with respect to synergy. Consideration of a combined measure,
such as viability, for team member selection may prove a more robust choice.

References
Askin, R. G., & Estrada, S. (1999). A survey of cellular manufacturing practices. In S. Irani (Ed.), Handbook of cellular
manufacturing systems. New York: Wiley.
Askin, R. G., & Huang, Y. (1997). Employee training and assignment for facility reconfiguration. Proceedings of the sixth
IERC, Miami, FL (pp. 426–431).
Askin, R. G., & Huang, Y. (2001). Forming effective worker teams for cellular manufacturing. International Journal of
Production Research, 39(11), 2431–2451.
Askin, R. G., & Sodhi, M. (1994). Organization of teams in concurrent engineering. In R. Dorf, & A. Kusiak (Eds.), Handbook
of manufacturing and automation. New York: Wiley.
Askin, R. G., & Vakharia, A. (1990). Group technology planning and operation. In D. Cleland, & B. Bopaya (Eds.), The
automated factory handbook (pp. 317–366). Blue Ridge Summit, PA: TAB Books, 317–366.
Bailey, D. E. (1997). Manufacturing improvement team programs in the semiconductor industry. IEEE Transactions on
Semiconductor Manufacturing, 10, 1–10.
Bailey, D. E. (1999). Modeling work group effectiveness in high-technology manufacturing environments. IIE Transactions,
32, 361–368.
Barrick, M. R., & Mount, M. K. (1991). The big five personality dimensions and job performance: A meta-analysis. Personnel
Psychology, 44(1), 1–26.
Belbin, R. M. (1981). Management teams: Why they succeed or fail. London: Heinemann.
Belbin, R. M., Aston, B. R., & Mottram, R. D. (1976). Building effective management teams. Journal of General Management,
3(3), 23–29.
Belbin, R. M., Watson, B., & West, C. (1997). True colours. People Management, 3(5), 34–38.
Burbidge, J. L. (1975). The introduction of group technology. New York: Wiley.
Carr, P. D., & Groves, G. (1998). Teams and cellular manufacturing, the role of the team leader. In N. C. Suresh, & J. M. Kay
(Eds.), Group technology and cellular manufacturing. Boston, MA: Kluwer.
Ebeling, A. C., & Lee, C.-Y. (1994). Cross training effectiveness and profitability. International Journal of Production
Research, 32, 2843–2859.
Fitzpatrick, E., Askin, R., & Goldberg, J. (2001). Using student conative behaviors and technical skills to form effective project
teams. Proceedings of the 31st ASEE/IEEE frontiers in education conference, Reno, NV (pp. S2G8–S2G13).
Furnham, A., Steele, H., & Pendleton, D. (1993). A psychometric assessment of the Belbin team-role self-perception inventory.
Journal of Occupational and Organizational Psychology, 55, 245–257.

608

E.L. Fitzpatrick, R.G. Askin / Computers & Industrial Engineering 48 (2005) 593–608

Hammer, A. L., & Huszczo, G. E. (1996). Teams. In A. L. Hammer (Ed.), MBTI applications. Palo Alto, CA: Consulting
Psychologists Press.
Henry, S. M., & Stevens, K. T. (1999). Using Belbin’s leadership role to improve team effectiveness: An empirical
investigation. The Journal of Systems and Software, 44, 241–250.
Hermann, N. (1996). The whole brain business book. New York: McGraw-Hill.
Hut, J., & Molleman, E. (1998). Empowerment and team development. Team Performance Management, 4(2), 53–66.
Juran, D. C., Schultz, K. L., Hammer, T. H., McClain, J. O., & Schruben, L. W. (1997). Personality and the performance of
production systems (http://www.johnson.cornell.edu/faculty/mcclain/Papers/Papers.htm#Personality).
King, J. R., & Nakornchai, V. (1982). Machine component group formation in group technology—review and extension.
International Journal of Production Research, 20, 117–133.
Kolbe, K. (1989). The conative connection. New York: Addison-Wesley.
Kolbe, K. (1993). Pure instinct. New York: Random House.
Kusiak, A. (1987). The generalized group technology concept. International Journal of Production Research, 25(4), 561–569.
Lingard, R., & Berry, E. (2000). Improving team performance in software engineering (http://www.ecs.scun.edu/wrlingard/
Publications/TeamLearning/TeachLearnPaper.html).
Min, H., & Shin, D. (1993). Simultaneous formation of machine and human cells in group technology: A multiple objective
approach. International Journal of Production Research, 31, 2307–2318.
Shafer, S. (1998). Part-machine-labor grouping: The problem and solution methods. In N. Suresh, & J. Kay (Eds.), Group
technology and cellular manufacturing (pp. 131–152). Boston, MA: Kluwer, 131–152.
Singh, N. (1993). Design of cellular manufacturing systems: An invited review. European Journal of Operational Research,
69(3), 284–291.
Süer, G. A. (1996). Optimal operator assignment and cell loading in labor-intensive manufacturing cells. Computers and
Industrial Engineering, 26, 155–158.
Suresh, N. C. (1991). Partitioning work centers for group technology: Insights from an analytical model. Decision Sciences,
22(4), 772–791.
Suresh, N. C. (1992). Partitioning work centers for group technology: Analytical extension and shop-level simulation
investigation. Decision Sciences, 23(2), 267–290.
Suresh, N. C., & Slomp, J. (2001). A multi-objective procedure for labour assignments and grouping in capacitated cell
formation problems. International Journal of Production Research, 39(18), 4103–4131.
Tepke, D., & Davis, H. (1990). Working paper. University of Chicago Study.
Warner, R. C., Needy, K. L., & Bidanda, B. (1997). Worker assignment in implementing manufacturing cells. In: Proceedings
of the IERC (pp. 240–245).
Wechsler, D. (1997). Cognitive, conative, and non-intellective intelligence. In J. M. Notterman (Ed.), The evolution of
psychology: Fifty years of the American psychologist. Washington, DC: American Psychological Association.

Proceedings of the 2008 Winter Simulation Conference
S. J. Mason, R. R. Hill, L. Mönch, O. Rose, T. Jefferson, J. W. Fowler eds.

FRAMEWORK FOR EXECUTION LEVEL CAPACITY ALLOCATION DECISIONS FOR ASSEMBLY – TEST
FACILITIES USING INTEGRATED OPTIMIZATION - SIMULATION MODELS
Shrikant Jarugumilli
Mengying Fu

Naiping Keng
Chad DeJong

Ronald Askin
John Fowler

Dept. of Industrial Engineering
502 Goldwater Center
Arizona State University
Tempe, AZ 85287, USA

Intel Corporation
5000 West Chandler Boulevard

Dept. of Industrial Engineering
502 Goldwater Center
Arizona State University
Tempe, AZ 85287, USA

Chandler, AZ 85226, USA

Sort, Assembly and Test. On the wafer fabrication side, the
integrated circuits are fabricated on silicon wafers by
growing films of material with different electric characteristics and patterning using photolithography and etching
processes. At Sort, the integrated circuits (dies) on each
wafer are tested for their performance characteristics e.g.,
speed and power consumption. At the Assembly-Test facilities the individual dies are packaged and tested before
they are shipped to the customer.
The focus of this work is to present an initial optimization model for the capacity allocation decisions for the Assembly-Test plants. A-T plants consist of a series of operations which share overlapping resources, i.e. multiple
operations might be performed on a single resource. There
are often several machines which can be utilized to perform a given operation.
We also propose a conceptual framework for shortterm capacity allocation decisions which consists of an optimization model, a simulation model and an adjustment
model. The optimization model is used to generate a capacity plan for the factory in two-hour time periods for a
couple of shifts and a shift-wise plan for subsequent shifts
for a two week planning horizon. Even modeling only the
bottleneck stages in the facilities, the optimization problem
is very large in terms of number of variables (both binary
and real) and constraints. Also, the modeling complexities
include other factory specific rules and conditions such as:
limited equipment, multiple levels of product mapping (or
BOM Structure), product flow complexity (many to many
relationships), high product and volume mix, shifting bottlenecks, setups (conversions), qualification requirements,
among others. The solution of the optimization model generates a short-term capacity plan which is passed on to the
simulation model using a adjustment model.
The simulation tool is used to model the entire factory
including the non-bottleneck operations to get high fidelity
estimate of the performance measures for the plan generated by the optimization model. The adjustment model

ABSTRACT
We present a framework for capacity allocation decisions
for Assembly-Test (A-T) facilities that is comprised of an
optimization model and a simulation model. The optimization and simulation models are used iteratively until a feasible and profitable capacity plan is generated. The models
communicate using an automated feedback loop and at
each iteration the model parameters are adjusted. We describe the role of the optimization model, the simulation
model and the feedback loop. Once the capacity plan is
generated, it is passed down to the shop-floor for implementation. Hence, decision makers can develop accurate
and more profitable execution level capacity plans using
the integrated model which utilizes both optimization and
simulation models. In this paper, we focus on the optimization model for capacity planning for the entire A-T facility
at the individual equipment (resource) level for a two-week
planning period and briefly discuss the simulation and the
adjustment model.
1

INTRODUCTION AND PROBLEM
DESCRIPTION

Semiconductor manufacturing involves a series of complicated processes which span several weeks. The industry is
characterized by short product life cycles and high equipment costs. Most of the companies in this industry use the
same type of equipment and have similar processes. Hence,
companies have to constantly innovate on the silicon technology and utilize their existing resource capacity efficiently in order to survive the fierce competition. Allocating capacity to the already present equipment in an optimal
or near optimal manner is challenging due to the complicated process flow, high product mix, and complex bill of
material structure which is specific to the industry.
The semiconductor manufacturing process can be
broadly classified into the following stages: Fabrication,

978-1-4244-2708-6/08/$25.00 ©2008 IEEE

2292

Jarugumilli, Fu, Keng, DeJong, Askin, and Fowler
passes the information between the models and also resets
the parameters or constraints as required.
While implementing execution level (i.e. short-term)
capacity plans it is important that the software tool performance (in terms of the run time) is low so that there is not
a big difference in the shop-floor status during the run of
the tool itself.
While allocating capacity, the planners need to ensure
the availability of the raw material. This problem gets a bit
complicated because of the high product mix and the complicated bill of material structure. Hence, the planning
needs to be carried out on multiple time horizons. By making sure that these time horizons are small we can ensure
real-time control and implementation of shop floor operations.
Capacity requirements planning is based on long-term
demand forecasts of the various final products. It is interesting that these long term planning assumptions typically
change and are not valid at the execution level on the shop
floor. This causes a gap in what is being produced versus
what needs to be actually produced. In order to narrow this
gap, planners need to have real-time control over the shop
floor operations to make real-time decisions, which is a big
challenge considering the huge product mix which ships
out of its factories, the process and the product complexities. Often it is seen that these constraints may lead to underload or overload situations for particular time periods
resulting in the loss of capacity or creation of a new bottleneck. It is the goal of this research to solve this problem of
assigning the right capacity to all equipment on the shopfloor in order to satisfy the demand and have shorter cycle
times for the final products.
Based on this problem definition, the short-term capacity allocation problem is considered. Our approach includes modeling of major machines at the various stages in
two hour time intervals for a two week planning period under operating rules which are specific to Intel’s A-T factories.

sider product-dependent setups for a large scale problem
instances. They extend this work in Dillenberger et al.
(1994), where they solve the capacity requirements at the
execution level. In their model they take into account considerable shop floor information including: machine availability, minor and major setups, storable and non-storable
resources and they account for costs associated with overlapping setups. Though this model comes very close to our
problem, this model uses an excessively large number of
binary variables and only models a single stage of operations.
Quadt and Khun (2005) describe a conceptual framework for lot-sizing and scheduling of flexible flow shops.
The problem described in their research is a large time
bucket lot sizing model that attempts to effectively utilize
the bottleneck (i.e. a single stage model). They also capture
the concept of setup state carryover but do not model the
set up time carry over. Other complexities such as product
substitution and limited enabler constraints are missing. In
this paper, only the framework is presented and the actual
solution and results are not presented.
3 INTEGRATED MODEL FRAMEWORK
The capacity allocation tool we are designing will consist
of an optimization model, a simulation model and a feedback mechanism as shown in figure 1. In this section we
will describe the scope of each of these models.
3.1

Optimization Model

The optimization model mainly handles the deterministic
parameters representing the factory operations, e.g.
processing times, setup times, etc. The various bottleneck
operations are modeled in the optimization model and the
non-bottleneck operations are considered as additional time
delays between the modeled operations.
The input parameters for the optimization model include: demand for each product, the product flow and the
process flow, complete bill of material, setup times,
processing times, throughput times for non-bottleneck operations, transfer times, number of machines at bottleneck
stages, yields, potential bottlenecks, machine availability,
WIP limits, qualification matrix, product priority, material
availability, factory calendar among others. The model generates the capacity allocation plan for each machine for a
specific operation at each stage in two hour time periods
for the first two shifts and in shiftly time periods for the
next thirteen days. The model also accounts for sequence
dependent setups which last for a couple of minutes to a
few hours and reports the volume of the unmet demand.
The objective function of the optimization model is to minimize the cycle time, the work in process and the missed

2 LITERATURE REVIEW
Over the years, spreadsheet based models (Occhino 2000)
for industrial capacity planning have been very popular
since they are easy to use and provide reasonable results in
a short amount of time. Recently, other techniques including simulation (Chou and Everton 1996, etc.), and mathematical programming (Karabuk and Wu 2002, Uribe et al.
2003, etc.) have been used as advanced planning tools in
industry.
In their work, Dillenberger et al. (1993) present a production allocation model for machines and time periods in
a single-level, multi-capacitated production environment
with initial setups and setups which are partially dependent
on production sequence. In this particular work, they con-

2293

Jarugumilli, Fu, Keng, DeJong, Askin, and Fowler
demand and is subject to inventory balance, capacity and
setup constraints. The optimization model was developed
using CPLEX 10.2.

Binary Variables (Setup Tracking Variables):
n
Y ptm

Shop Floor
Status
-------Customer
Demand

n
Z ptm

n
U ptm

Optimization
-------Generate
Allocation

Adjustment Model
-------e.g. Addition of
Constraints

(after) stage ‘N’
:
Initial
inventory of product family ‘p’ at (after) stage
I
‘n’
n
:
Inventory
holding cost of product family ‘p’ at stage
hp
‘n’
:
Back
order cost of product family ‘p’ at stage ‘N’
op
C: Capacity of machine per time period (in machine hours
available per period)
:
Demand
volume of product family ‘p’ in period ‘t’ at
D pt

Figure 1: Integrated Framework Model

s

Decision Variables
Continuous Variables:
B pt : Back order volume of the product family ‘p’ at the
end of period ‘t’ at the last stage ‘N’
n
X ptm : Production volume of product family ‘p’ during pe-

product ‘q’ in period ‘t’ at stage ‘N’
Cumulative setup time for product family ‘p’ on

the last stage ‘N’
: set up time for set up product family ‘p’ on machine

n
pm

‘m’ in stage ‘n’
: unit processing time of product family ‘p’ on ma-

n
pqt

chine ‘m’ in stage ‘n’
: unit substitution cost for using product ‘p’ to satisfy

n
ζ pq

‘q’ demand in time ‘t’ at stage ‘N’ at time ‘t’. Note:
we assume all cost parameters for product ‘p’ are
strictly greater than ‘q’
1 : if product family ‘p’ can be used to satisfy de-

c

of period ‘t’ at (after) stage ‘n’
n
: Setup time for product family ‘p’ on machine ‘m’
W ptm

Lnptm :

n
pm

t

riod ‘t’ on machine ‘m’ at stage ‘n’
Inventory volume of product family ‘p’ at the end

in stage ‘n’ in period ‘t’
Quantity of product ‘p’ used to meet demand for

period ‘t’ and can be produced in ‘t+1’ without
incurring a setup
0: otherwise
1: if setup operation for product family ‘p’ is going

n
p0

Reports
--------Utilization,
Product
Quantities,
Date/Time

n
:
R ptm

chine ‘m’ stage ‘n’ is finished in period ‘t’
0: otherwise
1: if product family ‘p’ can be produced at end of

on at the end of the period ‘t’ and will continue
at the beginning of period ‘t+1’ on machine ‘m’
at stage ‘n’ at the end of period ‘t’
0: otherwise
Parameters:
M: number of parallel machines at each stage
N: number of stages
P: number of product families
T: number of periods
κ: a big number
B p 0 : Initial back order volume of product family ‘p’ at

Simulation
--------Simulate System for the
Planning
Horizon

I ptn :

1: if setup operation for product family ‘p’ on ma

g ql

mand for product family ‘q’ at stage N
0 : Otherwise
: the number of die type ‘q’ which gets into package

assembly for product ‘l’

machine ‘m’ in stage ‘n’ at the end of period ‘t’

2294

Jarugumilli, Fu, Keng, DeJong, Askin, and Fowler
Objective Function:

min( ∑ h pn I ptn + ∑ o p B pt +
p ,t , n

p ,t

∑c

n
pqt

n
R pqt
)

cost. This is subject to various inventory balance, capacity
and setup constraints which are described below.
The constraint sets (2) and (3) are the inventory balance constraints which ensure the “conservation of material” principle. Constraint set (3) is specifically for the last
stage while constraint set (2) is for all other operations excluding the last stage. Constraint set (4) ensures that the
conservation of material is ensured when the product can
be processed at multiple stages within a specific time period, i.e. the processing time for several operations is less
than the length of the time period itself, k* represents the
number of stages the product can flow thorough within a
given time period. The capacity constraint set (5) ensure
that a lot manufactured in a given period only if the machine is setup for the product; this could be possible as a
result of setup state carry over from the previous period or
a completion of the setup operation in a given period. The
value of ‘ κ ’ is defined as the maximum production that
can be done on a resource in a given time period. Constraint set (6) ensures that the amount of time available in a
time period is either utilized in making a product or is utilized for making setups or is accounted as idle time. The
value of ‘Ct’ for the first shift will be 2 hours, followed by
the subsequent shifts which will be 12 hours. Constraint set
(7) ensures that if a setup is completed in a given time period, then the total cumulative time to complete the setup
across all the previous periods is less than or equal to the
setup time for the specific product and machine combination. Constraint set (8) ensures that at the end of the time
period the machine is either carrying a setup state to the
next time period or is undergoing a setup for a particular
product. Constraint set (9) ensures that the setup time cannot be initialized for a product on a particular machine at a
particular stage in a given time period unless we complete
the setup or the setup operation is in progress during the
time period. Constraint sets (10) and (11) ensure that the
value of the setup carryover variable is reset depending on
if a setup operation is complete or is in progress. Constraint set (12) adds the setup time incurred in a given time
period to the cumulative setup time from the previous periods. Constraint set (13) resets the value of the cumulative
setup time if the setup is not active at the end of a given
time period. Constraint sets (14) and (15) limits on setup
time and number of setups. Constraint set (16) ensure that
between any two setup operations, a minimum number of
lots are produced. Constraint set (17) ensures that minimum inventory of products is maintained at all stages at all
times. Constraint set (18) sets the limit on the number of
products of type ‘p’ which can be used for satisfying the
demand of product ‘q’. Constraint set (19) ensures that the
setup operations have to be fully completed before the end
of a shift.

(1)

p , q ,t
p>q

Subject to:
n
n
n
n +1
n
I ptn −1 + ∑ X ptm
−∑ X ptm
−∑ ζ pq
R pqt
+ ∑ ζ qpn Rqpt
=I ptn
q

q

m

m

for all p, t, m, n<N

n
n
n
n
I ptn −1 − B pt −1 + ∑ X ptm
−D pt − ∑ ζ pq
R pqt
+ ∑ ζ qpn Rqpt
=I ptn − B pt
m

∑X

q

≤ ∑k =1 I
k*

n
ptm

m

(2)

q

for all p, t, m

(3)

n−k
pt −1m

(4)

n
n
X ptm
≤ κ ( Z ptn −1m + Y ptm
) for all p, t, m, n

(5)

∑t

(6)

n
pm

n
n
X ptm
+ ∑W ptm
≤ Ct for all t, m, n
p

p

n
n
s npm * Y ptm
≤ Lnpt −1m + W ptm

(7)

∑Z

(8)

n
ptm

n
+ ∑ U ptm
≤ 1 for all t, m, n
p

p

n
n
n
W ptm
≤ Ct (Y ptm
+ U ptm
) for all p, t, m, n

(9)

n
n
Z ptm
≤ 1 − ∑q =1 U qtm
for all p, t, m, n

(10)

p

q≠ p

n
n
n
for all p, t, m, n, q;
Z ptm
≤ Z ptn −1m + Y ptm
− Yqtm

q ≠ p (11)
n
ptm

L

≤L

n
pt −1m

+W

n
Lnptm ≤ U ptm
* s npm

∑W

n
ptm

n
ptm

for all p, t, m, n

for all p, t, m, n

(12)
(13)

≤ fixednumber for all p, t, m, n

(14)

≤ fixednumber for all p, t, m, n

(15)

p ,t , m , n

∑Y

n
ptm

p ,t , m , n

n
X ptm
+ X ptn +1m ≥ const for all p, t, m, n

(16)

I ptn ≥ lower limit

(17)

for all p, t, n

n
R pqt
≤ limit on substitution for all p, t, q

(18)

n
U ptm
= 0 for t =6,12 etc… end of shift

for all p, m, n

(19)

The objective function (1), minimizes the sum of the inventory holding costs across all the operations and products, the back order costs at the last stage, and the product
substitution costs, which are the costs incurred due to conversion to a lesser value product. The last two terms were
added to the objective function to distinguish between various alternate optimal solutions by incorporating a penalty
function which decides the machine-job pair at the lowest

2295

Jarugumilli, Fu, Keng, DeJong, Askin, and Fowler
3.2

Simulation Model

ACKNOWLEDGMENTS

The role of the simulation model is to lay out the schedule
as per the optimization model’s output in a deterministic
setting. The input to the simulation model includes all the
parameters used for the optimization along with the output
generated by the optimization model and the processing
times for the non-bottleneck operations. The simulation
model is mainly used to get estimates of various performance measures such as: cycle time, tool utilization,
throughput time for non-bottleneck operations based on the
current solution given by the optimization model. The simulation model is a more detailed model which consists of
the various non-bottleneck operations and other factory parameters which were not captured in the optimization model. The simulation model was built in AutoSched AP. (AutoSched AP User Manual v 8.0, 2004)
3.3

We would like to thank the Intel Research Council for
funding this project.
REFERENCES
Brooks Automation, Inc. 2004. AutoSched AP User’s
Guide v 8.0 Chelmsford, MA.
Chou, W., and J. Everton. 1996. Capacity planning for development wafer fab expansion. In Proceedings of the
1996 IEEE/SEMI Advanced Semiconductor Manufacturing Conference, Cambridge, MA, 17-22.
Dillenberger, C., L. F. Escudero, A. Wollensak, and W.
Zhang. 1993. On solving a large-scale resource allocation problem in production planning. In: Fandel, G.,
Gulledge, T., and Jones A. (Eds), Operations Research
in Production Planning and Control, Springer, Berlin,
Germany, 105-119.
Dillenberger, C., L. F. Escudero, A. Wollensak, and W.
Zhang. 1994. On practical resource allocation for production planning and scheduling with period overlapping setups. European Journal of Operational Research 75:275-286.
Karabuk, S., and S. D. Wu. 2002. Decentralizing semiconductor capacity planning via internal market coordination. Inst. Ind. Eng. Trans. 34:743–759.
Occhino, T. J. 2000. Capacity planning model: the important inputs, formulas, and benefits. In Proceedings
IEEE/SEMI Advanced Semiconductor Manufacturing
Conference, 455-458.
Quadt, D., and H. Kuhn. 2005. Conceptual framework for
lot-sizing and scheduling of flexible flow lines. International Journal of Production Research 43:2291–
2308.
Uribe, A. M., J. K. Cochran, and D. L. Shunk. 2003. Twostage simulation optimization for agile manufacturing
capacity planning. International Journal of Production
Research 41:1181–1197.

Adjustment Model

The adjustment model is used to transfer and adjust data
between the optimization model and the simulation model.
Initially, the adjustment model sends the schedule generated by the optimization model to the simulation model.
Once the simulation runs to completion, various statistics
are collected and checked for feasibility and practicality for
implementation. Based on the simulation results some parameters and constraints might need to be modified. This
information is sent back to the optimization model.
These iterations continue till the desired factory metrics are
achieved or the factory metrics converge in a couple of
successive iterations. Also, the user can predefine the
number of iterations between the models before the results
are accepted.
4 CONCLUSIONS AND RESUTLS
The main objective of the paper was to present the framework for capacity planning using an integrated optimization and simulation models. In this paper, we have presented an optimization model which considers the setup
times varying from a couple of minutes to a few hours and
models the capacity lost due to setups accurately. Also, we
have presented a conceptual framework for development of
a capacity planning tool comprising of an optimization
model, a simulation model and a feedback loop. We believe accurate and more profitable execution level capacity
plans can be generated using an integrated model which
utilizes both simulation and optimization models. We intend to present our initial results at the conference in December.

AUTHOR BIOGRAPHIES
SHRIKANT JARUGUMILLI is a graduate student currently working as a graduate research associate in the Industrial Engineering Department at Arizona State University. He did a summer internship with Intel Corporation in
Summer 2007. He has a MS degree in Engineering Management from University of Missouri, Rolla and a B.E.
(Industrial Engineering and Management) from Visvesvaraya Technological University, India. His research interests
include modeling and analysis of manufacturing systems.
He is also the Vice-President of the ASU Chapter of
INFORMS. His e-mail address is <sjarugumilli@gmail.com>.

2296

Jarugumilli, Fu, Keng, DeJong, Askin, and Fowler
tor Research Corporation. His research interests include
modeling, analysis, and control of semiconductor manufacturing systems. Dr. Fowler is a member of IIE, INFORMS,
and SCS. He is an Area Editor for SIMULATION: Transactions of the Society for Modeling and Simulation International and an Associate Editor of IEEE Transactions on
Semiconductor Manufacturing. He is an IIE Fellow and is
on the Winter Simulation Conference Board of Directors.

MENGYING FU is currently pursuing the Doctoral degree in Department of Industrial Engineering, Arizona
State University, Arizona State, US. Her academic interests
include production scheduling and integer optimization.
She received the B.S. in Industrial Engineering from
Tsinghua University, Beijing, China.
Email:
<Mengying.Fu@asu.edu>.
NAIPING KENG is a Principal Engineer at Intel Corporation, where he has designed and implemented various
planning and scheduling tools for fab, sort, and assembly
and test for the past 19 years. He is the chief architect of
several production planning and execution tools for assembly and test (A/T) factories. His current projects and interests include machine setup optimization, shiftly full factory
capacity allocation and alignment, automatic generation of
capacity and material feasible production goals, and business process design. Dr. Keng received a Ph.D. in Computer Science from Southern Methodist University in 1989.
CHAD DEJONG is a Systems Engineer with the Operations Decision Support Technologies group at Intel Corporation. He is primarily responsible for the design, layout,
and modeling of automated material handling systems. He
earned a B.S. in Industrial and Operations Engineering
from the University of Michigan. He also earned an M.S.
in Industrial and Systems Engineering from Georgia Institute of Technology, and completed the Management of
Technology certificate program. His previous professional
background includes hospital and health care systems, and
automotive component manufacturing. Current research
and professional interests in the semiconductor industry are
in whole factory capacity and operations modeling, supply
chain simulation, model communication, model design and
execution time reduction, and the statistical validation of
modeling
tools.
His
email
address
is
<chad.d.dejong@intel.com>.
RONALD ASKIN, Ph.D., is Department Chair and Professor of Industrial Engineering at Arizona State University. He received his PhD from Georgia Institute of Technology and has 30 years of experience in the development,
teaching and application of methods for production systems analysis. He is a Fellow of IIE and has published extensively. His list of awards includes a National Science
Foundation Presidential Young Investigator Award, the
Shingo Prize for Excellence in Manufacturing Research,
IIE Joint Publishers Book of the Year Award, and the IIE
Transactions Development and Applications Award.
JOHN FOWLER is a Professor of Industrial Engineering
at Arizona State University and was the Center Director for
the Factory Operations Research Center that was jointly
funded by International SEMATECH and the Semiconduc-

2297

Available online at www.sciencedirect.com

European Journal of Operational Research 193 (2009) 23–34
www.elsevier.com/locate/ejor

Discrete Optimization

Project selection, scheduling and resource allocation with
time dependent returns
Jiaqiong Chen a, Ronald G. Askin
a

b,*

FedEx World Headquarters, 3680 Hacks Cross Road, Building H, Second Floor, Memphis, TN 38125, United States
b
Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287-5906, United States
Received 8 June 2006; accepted 12 October 2007
Available online 1 November 2007

Abstract
In this paper we formulate and analyze the joint problem of project selection and task scheduling. We study the situation where a
manager has many alternative projects to pursue such as developing new product platforms or technologies, incremental product
upgrades, or continuing education of human resources. Project return is assumed to be a known function of project completion time.
Resources are limited and renewable. The objective is to maximize present worth of proﬁt. A general mathematical formulation that
can address several versions of the problem is presented. An implicit enumeration procedure is then developed and tested to provide good
solutions based on project ordering and a prioritization rule for resource allocation. The algorithm uses an imbedded module for solving
the resource-constrained project scheduling problem at each stage. The importance of integrating the impact of resource constraints into
the selection of projects is demonstrated.
 2007 Elsevier B.V. All rights reserved.
Keywords: Project management; Project selection; Integer programming; Heuristics; Implicit enumeration

1. Introduction and background
Typical industrial projects include process improvement
and re-engineering, new product development, software
implementation, technology innovation, and facilities
upgrade or expansion. Projects are very common. Bounds
(1998) states, ‘‘more than $250 billion is spent in the United
States each year on approximately 175,000 information
technology projects’’. Of those information technology
projects, only 26% are completed on time and within budget. Accordingly, project planning and control have
received signiﬁcant attention from both researchers and
practicing managers. However most of this attention has
been devoted to individual projects rather than coordinated decision across multiple projects.
*

Corresponding author. Tel.: +1 480 965 2567.
E-mail addresses: Jq.chen@fedex.com (J. Chen), Ron.Askin@asu.edu
(R.G. Askin).
0377-2217/$ - see front matter  2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2007.10.040

In this paper, we consider the situation where multiple
projects are available to be pursued. The decision maker
must select a subset of these projects and schedule their
execution subject to limited, renewable resources. The
objective is to maximize present worth of total proﬁt where
the proﬁt realized from each project is a decreasing function of its completion time. To visualize the problem, consider a large engineering design group. The company
produces and markets a large array of products. The engineering manager is charged with product innovation and
development to ensure a competitive mix of product oﬀerings. At any point in time, based on current product mix,
technology and market demand, the manager has a large
set of possible R& D projects to pursue. These may involve
updating existing products, developing new products,
developing or acquiring new technologies, human resource
development (training), and so forth. Considering limited
resources and potential return, the manager must select a
subset of projects to pursue. For the selected projects, the

24

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

manager also must provide a detailed schedule showing
time-phased activity levels for all tasks subject to the available resources.
We assume that project return is sensitive to its completion time. A McKinsey & Company study indicated that a
high tech product reaching the market six months late will
reduce its proﬁt by one third over the next ﬁve years (Musselwhite, 1990). Other research regarding sensitivity to
time-to-market also conﬁrms the advantage of early market entry. For example, Robinson and Fornell (1985) and
Urban et al. (1986) observed that early entrants will have
a long-term beneﬁt of higher market share than later competitors in consumer goods.
R& D project selection models in the literature generally
do not include project scheduling as part of the selection
criteria (Coﬃn and Taylor, 1996). Such models usually
are designed to select a portfolio of projects from a pool
of available projects to meet expected return objectives
and budget constraints (Fox et al., 1984). These models
sometimes also consider resource allocation (Taylor
et al., 1982) and risk (Heidenberger, 1996; Gabriel et al.,
2006) as part of the selection process. A few models include
the scheduling aspect (Coﬃn and Taylor, 1996; Kolisch
and Meyer, 2006), yet none has considered time-dependent
return nor fully explored the project selection decision
space with integrated consideration of task scheduling.
Scheduling a single project to complete as early as possible subject to limited resources is known as the Resource
Constrained Project Scheduling Problem (RCPSP). The
classical RCPSP assumes each task is completed in ﬁxed
time by applying a ﬁxed amount of resource in each time
period. Both optimal procedures and priority-rule based
heuristics are studied extensively. Recent classiﬁcation
and survey can be found in Brucker et al. (1999) and Kolisch and Hartmann (2006). Recent optimal approaches
include Demeulemeester and Herroelen (1992, 1997), Brucker et al. (1998) and Mingozzi et al. (1998). The depth-ﬁrst
search implicit enumeration algorithm by Demeulemeester
and Herroelen (1992, 1997) provides a current standard for
solving the classical RCPSP problem. More recent work on
RCPSP is due to Dorndorf et al. (2000b), Brucker and
Knust (2000), Demassey et al. (2005), Möhring et al.
(2003), Sprecher (2000), Stork and Uetz (2005), and Valls
et al. (2005).
Research in RCPSP has been extended to more general
assumptions, which allows tasks to have ready/due times,
to be executed in diﬀerent speed (multi-mode) (Sprecher
et al., 1997; De Reyck et al., 1998; Hartmann and Drexl,
1998; Demeulemeester et al., 2000; Erenguc et al., 2001;
Heilmann, 2003; Brucker and Knust, 2003; Sprecher and
Drexl, 1998), and to obey generalized precedence relations
(GRP) (De Reyck and Herroelen, 1998; Dorndorf et al.,
2000a; Demeulemeester and Herroelen, 1997; De Reyck
and Herroelen, 1999). Objectives other than minimizing
makespan, such as maximizing net-present-value (NPV)
(Icmeli and Erenguc, 1996; Neumann and Zimmermann,
2000; Ulusoy et al., 2001; Vanhoucke et al., 2001b; Kimms,

2001; Schwindt and Zimmermann, 2001) and minimizing
weighted earliness/tardiness costs (Vanhoucke et al.,
2001a), have also been studied. Recent work on the discrete
time-cost tradeoﬀ problem is due to Akkan et al. (2005).
Resource constraints are extended from renewable
resources to partially renewable resources (Böttcher et al.,
1999) and other conditions.
Priority rule based heuristics have been studied by several authors. An early study by Davis and Patterson
(1975) found that the Minimum Job Slack (MINSLK),
minimum Late Start Time (LST), Resource Scheduling
Method (RSM), and minimum Late Finish Time (LFT)
rules produce better schedules than others (Davis and Patterson, 1975). More recent study focuses on new priority
rules (Kolisch, 1996a) and more complicated methods such
as multi-pass priority rules, sampling methods, and metaheuristics (Alcaraz et al., 2003; Bouleimen and Lecocq,
2003; Kolisch and Drexl, 1997; Klein, 2000; Knotts et al.,
2000; Franck et al., 2001; Jozefowska et al., 2001; Schirmer, 2001; Tormos and Lova, 2001; Hartmann, 2002). A
comprehensive empirical investigation reported in Hartmann and Kolisch (2000) is expanded and updated in Kolisch and Hartmann (2006). Metaheuristics are found to
perform best with the methods of Alcaraz et al. (2004),
Debels et al. (2006), Kochetov and Stolyar (2003) and Valls
et al. (2003) being dominant. Overall results indicate that
using a parallel schedule generation scheme in addition to
a serial scheme, and incorporating a forward-backward
improvement scheme are advisable.
On theoretical aspects, Sprecher et al. (1995) gave a formal deﬁnition of semi-active, active, and non-delay schedules for the RCPSP. Kolisch (1996b) revisited the theory of
serial and parallel scheduling methods in RCPSP. On test
instances, the 110 multi-resource test problems assembled
by Patterson (1984) was the standard. Recently, the parameterized data set from a project generator named ProGen
(Kolisch et al., 1995; Drexl et al., 2000; Kolisch and Sprecher, 1997) has become more widely used. Another generator named RanGen is developed by Demeulemeester
et al. (2003). Recent books for research in RCPSP include
Demeulemeester and Herroelen (2002), Neumann et al.
(2003), Brucker and Knust (2006) and Jozefowska and
Weglarz (2006).
In the following section we describe a mathematical
model to guide project selection and task scheduling with
time sensitive returns and renewable resources. Section 3
provides an implicit enumeration algorithm followed by
an example. Section 4 provides details of an empirical evaluation of the algorithm including the use of several heuristic priority rules. Lastly, Section 5 ﬁnishes with conclusions
and remarks for future research.
2. Mathematical model
We label the problem as the Project Selection and Fixed
Intensity Task Scheduling (PSFITS) problem. A formal
mathematical model for the problem is presented below.

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

The decision variables involve two sets, one for project
selection decisions and the other for task scheduling. A list
of data coeﬃcients is also provided. Job 1 of each project is
the dummy start job and the last job is the dummy end job.
Decision variables:
• Yi = 1 if project i is selected, 0 otherwise.
• Cijt = 1, if job j of project i completes at time t; and 0
otherwise.
Technological coeﬃcients and parameters:
•
•
•
•
•
•
•
•
•
•
•

N: number of candidate projects.
K: number of resource types.
Ji = ﬁnal task for project i.
T = time upper bound for projects’ completion.
EFij = early ﬁnish time for task ij .
LFij = late ﬁnish time for task ij, given the time upper
bound T.
P(i, t) = expected proﬁt or discounted present worth of
expected proﬁt if project i completes in period t.
dij = duration of activity ij at normal activity level.
S(ij) = set of immediate successor tasks for ij.
rijk = resource k requirement for activity ij at normal
level.
Rkt = resource k available in t.

Formulation:
LFiJ i
N
X
X

Max

P ði; tÞ  C iJ i t ;

ð1Þ

25

tions. Speciﬁcally, (3) is a special treatment of the dummy
start job’s direct successors, in light of the fact that Cijt is
not deﬁned for the dummy start job for the sake of implementation convenience. Constraints (5) enforce the
resource limits for each resource type and in each time period. Constraints (6) and (7) declare the decision variables.
In terms of computational complexity, the general
PSFITS problem is NP-hard. A sub-problem, the ﬁxed
intensity task scheduling problem, is proven to be NP-hard
(Demeulemeester and Herroelen, 2002). This sub-problem
is a special case of our PSFITS problem, when the number
of candidate projects is only one.
In terms of number of decision variables and constraints, we have less than N + N Æ J Æ T variables, all of
which are binary variables. The number of constraints is
bounded above by N Æ J + N Æ J2 + K Æ T.
The model Eqs. (1)–(7) may be modiﬁed in several
dimensions. As stated, the model assumes ﬁxed task intensity. In some cases it may be possible to either vary the level
of eﬀort allocated to a task over time or to allow selection
of one of multiple modes for performing each task. Additionally, resources may be specialized such that each unit
is unique. For instance, it may be important to identify
the speciﬁc person within a resource class. Productivities
or availabilities may vary over time. Likewise, extension
to model worker compatibility can be incorporated to
ensure team synergy based on behavioral and personality
factors. Further discussion of these extensions is provided
in Chen (2005). Combinations of Yi variables can be constrained to represent alternative projects or required portfolio balances.

i¼1 t¼EFiJ i

s:t:
3. Implicit enumeration algorithm

LFij
X

C ijt ¼ Y i

8i ¼ 1; 2; . . . ; N ; j ¼ 2; . . . ; J i ;

ð2Þ

t¼EFij
LFih
X

06

ðt  d ih Þ  C iht

8i ¼ 1; 2; . . . ; N ; ih 2 Sði1Þ;

ð3Þ

t¼EFih
LFij
X

t  C ijt 6

t¼EFij

LFih
X

ðt  d ih Þ  C iht

8i ¼ 1; 2; . . . ; N ;

t¼EFih

j ¼ 2; . . . ; J i  1; ih 2 SðijÞ;
N
X

J i 1
X

i¼1

j¼2

rijk

minftþd
ij 1;LFij g
X

C ijq 6 Rkt

ð4Þ
8 k ¼ 1; 2; . . . ; K;

q¼maxft;EFij g

t ¼ 1; 2; . . . ; T ;
C ijt 2 f0; 1g 8i ¼ 1; 2; . . . ; N ; j ¼ 2; . . . ; J i ;

ð5Þ

t ¼ EFij ; . . . ; LFij ;
Y i 2 f0; 1g 8i ¼ 1; 2; . . . ; N :

ð6Þ
ð7Þ

The objective, (1), seeks to maximize total proﬁts from
completing selected projects. Constraints (2) ensure that
all jobs of a selected project are completed. It also enforces
that none of the tasks of unselected projects are executed.
Constraints (3) and (4) take care of the precedence rela-

This section describes an algorithm that implicitly enumerates all possible project priority sequences to obtain
the sequence with highest proﬁt. A depth ﬁrst search
method is used to sequence projects for consideration.
Fathoming rules are developed to assist in pruning
branches. As a project sequence in the tree is explored, a
decision is made whether to accept the next project in the
sequence. The return from including a project at a node
is determined by solving a RCPSP using remaining available resources.
It is important to recognize that while the proposed
method sequentially selects and schedules projects, in general the optimal solution need not possess the property that
a strict priority ordering of projects exists when scheduling
tasks. Thus, the implicit enumeration over projects is still
heuristic unless all tasks of all selected projects are simultaneously considered when constructing schedules. Chen
(2005) provides greater details and special cases in which
project prioritization is optimal, but here we provide only
a demonstration of the nonoptimality of project prioritization for two projects with a single resource when tasks have
only a single operational mode.

26

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

cute the task. The arrows stand for precedence relations.
The small circles represent the dummy start/end tasks.
Assume linearly decreasing proﬁt proﬁles of project
completion time as follows:
Project A proﬁt = 16 [1  (tA  8)rA].
Project B proﬁt = 15 [1  (tB  7) (0.10)].

Fig. 1. Project networks.

Fig. 1 shows two projects. Both projects have two tasks.
The number above each task is the (ﬁxed) duration it takes
to execute the task. The number below represents the
number of resource units required (per time unit) to exe-

Note that tA (tB) represents the completion time of project
A (B) and rA is the rate of proﬁt decrease per time period
delay for project A. The rate of proﬁt decrease for project
B is ﬁxed for 10%. Lastly, assume 6 resource units are
available at each time period.
To compute the maximal proﬁt, enumeration of all possible schedules produces the set of three schedules shown in
Fig. 2. This set of schedules possesses the following property: for any value of rA > 0, at least one of the schedules
in the set will be optimal. This follows since decreasing
proﬁt implies an optimal schedule can be found with no

Fig. 2. A set of potential optimal schedules.

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

free slack. Thus the 4! possible left-shifted task sequences
along with the possibilities of 0 or 1 selection decisions
deﬁne the set of possible optimal schedules. Hence, to
derive the optimal proﬁt, we only need to compute the
maximum of these three.
Schedule (i) results from giving full priority to project A.
Schedule (iii) is derived from giving full priority to project
B. Schedule (ii), however, gives neither project complete
priority.
Computing the total proﬁt realized by each schedule, we
have the following result:
Proﬁt of schedule (i) = 19 + 32rA.
Proﬁt of schedule (ii) = 23.5.
Proﬁt of schedule (iii) = 28  64rA.
The optimal proﬁt, which is the upper envelope of the three
linear functions, is then:
8
>
< 28  64  rA ; rA 6 0:0703125;
optimal profit ¼ 23:5;
0:0703125 < rA 6 0:140625;
>
:
19 þ 32  rA ; rA > 0:140625:
When rA < 0.0703125, schedule (iii) – giving full priority to
project B, is the champion schedule. When rA > 0.140625,
schedule (i) – giving full priority to project A, achieves
the maximum proﬁt. When 0.0703125 < rA < 0.140625,
however, it is not optimal to give full priority to any
project.
A non-prioritized optimal solution can occur even when
task preemption is allowed. However, Chen (2005) shows
that the optimal solution must be a prioritized schedule
when the additional constraint is added that no two projects can have active tasks in the same period.

27

1. n - Node number
2. pn - Parent node number
3. in - Project scheduled at this node
4. fn - Profit realized by the project scheduled at this node
5. cfn - Cumulative profit to this node
6. tj - Job finish time for job j of project i
7. [Rkt]n - Remaining resources of type k in period t
8. Fn - Set of Finished projects up to node n
9. En - Set of Eligible projects for node n
10. Vn - Set of Visited projects for node n
11. Un - Set of Unvisited projects for node n
Fig. 3. Deﬁnition of a node.

with the resource available at node n, [Rkt]n.
Assume the task completion times are tj (j = 1,
. . ., Ji), the proﬁt of project i is f, and the remaining
resources are [Rkt]x. Update Un = Un  {i} and
Vn = Vn + {i}. Branch into a new node m, where
m = current largest node number + 1. Let pm = n,
im = i, fm = f, cfm = cfn + fm, and tij = tj for
j = 1, . . . , Ji. Let [Rkt]m = [Rkt]x, Fm = Fn + {i},
Em = En  {i}, Vm = {empty}, and Um = Em.
Check if the cumulative proﬁt of the new node is
better than the best node (cfm > cfl). If yes, update
the best node to be the new node, i.e. l = m. Finally,
let the new node be the current node, i.e. n = m,
and repeat step 2.
Step 3. (Backtracking). Check if current node n is node 0.
If yes, stop and retrieve the best priority sequence
by traversing from the best node l to node 0. Otherwise, let current node to be its parent node, i.e.
n = pn and go to step 2.

3.1. Basic enumeration

3.2. Fathoming rules

The implicit enumeration explores a tree consisting of all
permutations of ordered sequences of projects. Nodes correspond to projects. Branching to a new node of the tree
corresponds to considering adding the next project in that
sequence by scheduling its tasks using the remaining available resources. The algorithm is a depth ﬁrst search method
and it is enhanced by a number of fathoming rules to prune
branches. Deﬁne a node as shown in Fig. 3. The basic enumeration scheme is described ﬁrst as a complete enumeration of all priority sequences. This illustrates use of the
variables deﬁned for a node.

Recall that project proﬁt is non-increasing with respect
to project completion time. If proﬁt fi of project i at node
n is nonpositive, we can fathom. Let the parent node of n
be m. For any other branch out of m, the remaining
resources would not be greater than what is there at m.
Thus, project i can be eliminated from the Em, i.e. any
future node branching from node m need not consider project i.
As another observation, any node branching from node
n would not yield an optimal solution, since the resources
consumed by i are wasted. Any continuation from this
node will be dominated by another path emanating from
the parent node m. Thus, we should backtrack to node m.
Combination of the above two observations yields fathoming rule 1:

Step 1. (Initialization).Let n = 0. Let f0 = cf0 = 0. Let
[Rkt]0 = units of resource k available in t. Let
F0 = V0 = {empty} and E0 = U0 = {1, . . ., N}.
Let the best node (deﬁned as l) be node 0. Go to
step 2.
Step 2. (Branching). Check if Un is empty. If yes, go to step
3. Otherwise, pick the ﬁrst project in Un, say project
i, and schedule it to complete as soon as possible

3.2.1. Fathoming rule 1 (FR1)
If proﬁt fi of project i at node n is such that fi 6 0, eliminate project i from Em, where m is the parent node of n,
and backtrack to node m.

28

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

Since scheduling single project under non-constant
resource availability can be computationally expensive,
we added a simple enhancement to FR1 as follows:
3.2.2. Fathoming rule 2 (FR2)
Record {i, [Rkt]n} if fi 6 0 at node n. Each time we
branch into a new node m, eliminate any project h from

Em if a record {h, [Rkt]n} is found where [Rkt]n P [Rkt]m
for all k, t.
Assume that current node n is being compared to a previous node q. If cfn 6 cfq, En is a subset of Eiq (initial Eq, i.e.
set Eq when node q is ﬁrst generated), and [Rkt]n 6 [Rkt]q,
for all k, t, we know that continuing from node n would
only yield at most as much proﬁt as that found through

Table 1
Implicit enumeration process
Node

Scheduled project

Remark

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48

None
1
2
4
5
3
2
4
5
4
2
3
5
2
2
1
3
1
4
5
1
4
1
5
1
5
1
3
1
2
4
1
5
5
1
4
1
2
3
5
3
2
3
5
1
2
3
5
1

49
50

2
3

Initialize
Becomes the new best node
Candidate project 3 fathomed by FR1. Becomes the best node
Candidate project 5 fathomed by FR1. Becomes the best node
Candidate project 4 fathomed by FR1
(None)
This node is fathomed by node 2 via FR3
Project 2 and 5 are fathomed by FR1
Project 2 and 4 are fathomed by FR1
Project 5 is fathomed by FR1
Project 3 and 5 are fathomed by FR2 (due to node 2 and 3)
Project 2 and 5 are fathomed by FR2 (due to node 7)
Project 3 is fathomed by FR2 due to node 2. Project 4 is fathomed by FR1
Project 4 is fathomed by FR1
(None)
Project 3 is fathomed by FR2 due to node 2. This node is then fathomed by FR3 by node 2
(None)
Project 5 is fathomed by FR2 due to node 9. Project 4 is fathomed by FR1
Project 5 is fathomed by FR2 due to node 9. Project 1 is fathomed by FR1
Project 4 is fathomed by FR2 due to node 17
E20 is empty
Project 3 is fathomed by FR2 due to node 2
Project 5 is fathomed by FR2 due to node 7
(None)
The best node! Cumulative proﬁt becomes +8
Project 3 and 4 are fathomed by FR2 due to node 2 and 17, respectively
(None)
(None)
This node is fathomed by node 5 thru either FR3 or FR4
This node is fathomed by node 16 thru either FR3 or FR4
Project 2 is fathomed by FR1
Project 2 is fathomed by FR1. Node is fathomed by node 21 via FR3
Node is fathomed by node 21 via FR3
Project 2 is fathomed by FR1. Node is fathomed by node 16 via FR3
Project 4 is fathomed by FR2 due to node 17. Project 2 is fathomed by FR1
(None)
(None)
Project 3 and 5 are fathomed by FR2 due to node 2 and 7, respectively
Project 2 is fathomed by FR2 due to node 31. Node becomes fathomed by node 21 via FR3
Project 2 if fathomed by FR1
(None)
Project 3 is fathomed by FR2 due to node 2. Node becomes fathomed by node 21 via FR3 or FR4
Project 2 is fathomed by FR2 due to node 30. Node becomes fathomed by node 30 via FR3 or FR4
(None)
Project 2 is fathomed by FR2 due to node 39. Node then becomes fathomed by node 39 (FR3 or FR4)
Project 3 is fathomed by FR2 due to node 2. Node is fathomed by node 39 (FR3 or FR4)
Project 2 is fathomed by FR2 due to node 8. Node is then fathomed by node 21 by FR3
Project 4 is fathomed by FR1
Project 3 and 4 are fathomed by FR2 due to node 2 and 12, respectively. Node is then fathomed by node 12 (FR3 or
FR4)
Project 3 and 4 are fathomed by FR2 de to node 2 and 17, respectively. Node is then fathomed by node 25 (FR3 or FR4)
Project 2 is fathomed by FR2 due to node 33. Node becomes fathomed by node 33 via FR4 or node 16 via FR3

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

exploring node q. Thus, we can fathom any branches from
node n and backtrack to its parent node pn.
3.2.3. Fathoming rule 3 (FR3)
Fathom any branches from current node n and backtrack to pn if there exists a previous node q such that: (1)
cfn 6 cfq; (2) En is a subset of Eiq ; (3) [Rkt]n 6 [Rkt]q, for
all k, t.
A small enhancement to FR3 is stated as follows:
3.2.4. Fathoming rule 4 (FR4)
Fathom any branches from current node n and backtrack to pn if there exists a previous node q such that: (1)
cfn 6 cfq; (2) Fn is same to Fq; (3) [Rkt]n 6 [Rkt]q, for all k, t.
The second condition in (FR4) is not equivalent to that
in (FR3). There are situations where Fn is the same as Fq
yet En is not a subset of Eiq (e.g. node 50 in Table 1 below).
This happens when node n and q are on two diﬀerent paths
and some element of En was fathomed along the path to
node q yet not fathomed along the path to node n.
3.3. The implicit enumeration algorithm
In addition to the fathoming rules, the set of candidate
projects is presorted in a ‘‘max proﬁt per resource unit’’
order. The index is computed as the maximum proﬁt
achievable over total resource consumption. The maximum

29

proﬁt achievable is computed by running the Demeulemeester and Herroelen (1992) makespan-minimizing algorithm assuming all resources are available to the single
project. In addition, we check fathoming rules FR1 and
FR2 before we branch into a new node instead of after
the branching. A complete ﬂow chart for the algorithm is
available in Chen (2005).
Step 1. (Initialization). Presort the projects in non-increasing proﬁt/resource order. Let n = 0. Let f0 = cf0 =
0. Let [Rkt]0 = units of resource k available in t.
Let F0 = V0 = {empty} and E0 = U0 = {1, . . . ,N}.
Record Ei0 ¼ E0 . Let the best node be node 0, i.e.
l = 0. Go to step 2.
Step 2. (Branching). Check if Un is empty. If yes, go to step
3. Otherwise, pick the lowest numbered project in
Un, say project i, and schedule it to complete as
soon as possible with the resource available at node
n, [Rkt]n. Assume the task completion times are tj
(j = 1, . . ., Ji), the proﬁt of project i is f, and the
remaining resources are [Rkt]x. Update Un = Un 
{i} and Vn = Vn + {i}.If f 6 0, invoke FR1 and
FR2. Let En = En  {i}, record {i, [Rkt]n} and
repeat step 2. Otherwise, branch into a new node
m, where m = current largest node number + 1.
Let pm = n, im = i, fm = f, cfm = cfn + fm, and tij = tj
for j = 1, . . . ,Ji. Let [Rkt]m = [Rkt]x, Fm = Fn + {i},

Fig. 4. Enumeration example instance.

30

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

Eim ¼ En  fig,
Em ¼ Eim  fany fathomable
projectvia FR2g, Vm = {empty}, and Um = Em.
Check if the cumulative proﬁt of the new node is
better than the best node (cfm > cfl). If yes, update
the best node to be the new node, i.e. l = m.Check
if new node m is fathomed via FR3 or FR4. If so,
go to step 3. Otherwise, let the new node be the current node, i.e. n = m, and repeat step 2.
Step 3. (Backtracking). Check if current node n is node 0.
If yes, stop and retrieve the best priority sequence
by traversing from the best node l to node 0. Otherwise, let current node to be its parent node, i.e.
n = pn and go to step 2.

An enumeration example is given in this section to illustrate the algorithm. Assume that the set of candidate projects in Fig. 4 is available. The ﬁgure gives the project
networks, resource requirement (rkt), proﬁt function and
max proﬁt/resource ratio (r1 to r5). The resource availability is assumed to be six units for every time period.
The projects are pre-sorted in non-increasing max
proﬁt/resource order. Initialize node 0. Based on the proﬁt
proﬁle, a planning horizon length of six time periods is sufﬁcient (T = 6) for proﬁtability. The enumeration process is
illustrated through Fig. 5 and Table 1. The highest proﬁt

(2, 2)
1
1

Legend:
(fn, cfn)
in
n

(2, 5)
1
15
(2, 5)
3
16

(0, 0)
-0

3.4. An example

(3, 3)
2
14

(2, 2)
3
27

(3, 3)
4
35

(3, 3)
5
47

(3, 6)
4
21

(3, 6)
5
25

(1, 4)
1
36
(3, 6)
2
41
(2, 5)
3
42
(1, 4)
5
43
(2, 5)
1
48
(3, 6)
2
49
(2, 5)
3
50

(3, 5)
2
2

(1, 6)
1
17
(1, 6)
4
18
(1, 6)
5
19
(1, 7)
1
22
(1, 7)
5
23
(1, 7)
1
26

(3, 7)
2
37
(2, 6)
3
38
(1, 5)
5
39
(1, 5)
1
44
(3, 7)
2
45
(2, 6)
3
46

(2,4)
3
5

(1, 7)
1
20

(1, 8)
1
24

(2,4)
4
9
(3, 5)
5
12

(2, 4)
1
28
(3, 5)
2
29
(3, 5)
4
30
(3, 5)
5
33
(2, 7)
3
40

Fig. 5. Implicit enumeration tree.

(2, 7)
4
3
(2, 7)
5
4
(1, 5)
2
6
(2, 6)
4
7
(2, 6)
5
8
(3, 7)
2
10
(2, 6)
3
11
(1, 6)
2
13

(1, 6)
1
31
(1, 6)
5
32
(1, 6)
1
34

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

(+8) is found at node 24 with priority sequence
2 ! 4 ! 5 ! 1. Complete details are available in Chen
(2005).
Regarding the ﬁgure, above each circular node is the
proﬁt and cumulative proﬁt (fn, cfn). Inside the circle is
the project number (in). Below each circle is the node number (n), which describes the order in which the nodes are
visited. The table lists the node number, the project scheduled, and any application of the fathoming rules.
Overall, the number of nodes is equal to 50 (excluding
node zero) and the number of single project evaluations is
 
P5
5
68. Comparing to the full exploration of i¼1
i! ¼ 325
i
nodes and 325 single project scheduling, a signiﬁcant saving is evident. During computational experiments, even
simple instances, such as the ﬁrst instance in Table 3
described below takes more than 6 minutes and exceeds
the memory allowed (600MB) and if we attempt to run a
complete enumeration. Combining the enumeration with
our fathoming rules, however, we are able to compute
the optimal priority sequence in a few seconds using much
less memory.
4. Computational results
In Chen (2005), a set of simple priority rules are shown
to be very eﬀective in ﬁnding good priority sequences. They
are the ‘‘max proﬁt’’, ‘‘max resource’’ and ‘‘max proﬁt/
resource’’ rules, respectively. The ‘‘max proﬁt’’ rule, for
example, is to prioritize and schedule projects in the order
of maximum proﬁts. A project achieves its maximum proﬁt
when it completes within its minimum makespan, assuming
abundant resources are available. The project with highest
maximum proﬁt is picked from the unscheduled projects at
each stage. The ‘‘max resource’’ rule follows the order of
total resource consumption, with least resource consuming
project at the end of waiting line. For the ‘‘max proﬁt/
resource’’ rule, ratios of maximum proﬁt over total
resource consumption for each project are calculated and
the projects follow descending order according to this
ratio. All three rules perform signiﬁcantly better than the
benchmark random priority rule. The average proﬁt
improvements are 38.7%, 27.9% and 21.2% for the data
set. The ﬁrst and last rule holds intuitively yet the ‘‘max
resource’’ rule also holds due to an assumption that project
proﬁtability is positively correlated to its resource consumption. The champion, ‘‘max proﬁt’’ priority rule, is
included for comparison in the computational study
reported in this section.
A complete problem instance consists of a pool of candidate projects, a set of proﬁt proﬁles for all projects, and
the resources available for each type during each time period. The basic project networks in our data set come from
the project scheduling literature (Patterson, 1984). With
regard to resource availability, we impose a percentage
(labeled resource tightness) to the peak resource consump-

31

Table 2
Experiment design

Cell
Cell
Cell
Cell

1
2
3
4

Resource tightness (%)

Proﬁt decreasing rate (%)

20
20
60
60

5
25
5
25

tions if all projects proceed as quickly as possible. Proﬁt
function for each project is generated by applying a linear
decreasing rate to a base proﬁt. A project is assumed to
achieve its base proﬁt if it completes in shortest duration
possible, i.e. its Critical Path Length, CPL. Such a base
proﬁt is computed as a uniform distributed factor
a  [0.5, 1.5] times its total resource consumption. The random factor a helps creating projects with varying proﬁt/
resource ratio. Using total resource consumption conforms
to our assumption that proﬁt should be positively correlated to resource consumed.
The Patterson set of 110 RCPSP instances has 70
instances of distinct project networks using three
resource-types. The other instances either have the same
project network with diﬀerent resource availability or the
number of resource type is not 3. We choose these 70 project networks and randomly assign them to seven project
pools with 10 projects each. Each pool of 10 projects, along
with a combination of a resource tightness level and a
proﬁt decreasing rate, becomes one distinct instance for
our experiment. We choose two levels for both resource
tightness (20% or 60%) and proﬁt decreasing rate (5% or
25%), which gives us a full factorial of four experiment cells
(Table 2). A ‘‘cell’’, or design point, is a combination of a
resource tightness level and a proﬁt decreasing rate. This 22
experimental design crossed with our seven project pools
yields 28 instances for our experiment.
4.1. Proﬁt comparison
For the above 28 instances, the proﬁt achieved by the
implicit enumeration algorithm (‘‘Enum.’’), by the ‘‘max
proﬁt’’ priority rule (‘‘Rule 1’’) and by the benchmark random priority rule (‘‘Rule 2’’) are listed in Table 3. The average proﬁt per data cell is listed in Table 4. The average
percentage improvement relative to the benchmark is also
listed in Table 4 and plotted in Fig. 6. The implicit enumeration algorithm achieves on average 46.8% proﬁt improvement compared to the random priority rule. The ‘‘max
proﬁt’’ rule performs very close to the enumeration algorithm, with an average improvement of 38.7%. Analysis
of Variance based on the proﬁt data in Table 3 shows that
the performance diﬀerences among the three algorithms are
statistically signiﬁcant with a P-value less than 0.001.
4.2. Computation time
The algorithms were implemented in C++ on a Sun
Blade 100 workstation with one 500 MHz CPU and 256

32

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

MB physical memory. The average computation times (in
seconds) are shown in Table 5.

Table 3
Proﬁt data for pat set
Pat set

Instance number

Enum.

Rule 1

Rule 2

Cell 1

1
2
3
4
5
6
7

2002
3635
1736
2467
3681
2295
2141

1961
2958
1736
1728
3681
2295
2141

1090
2511
977
1363
2555
1453
1579

Cell 2

1
2
3
4
5
6
7

2218
3635
1922
2467
3681
2295
2454

2145
3500
1652
2411
3681
2295
2454

1505
1443
1333
1363
2609
1507
1602

Cell 3

1
2
3
4
5
6
7

5271
7504
5074
6012
7378
5632
6050

4677
7504
4644
5841
6850
5632
5769

3497
5602
4001
4683
5489
4282
4379

1
2
3
4
5
6
7

5454
7924
5259
6464
7799
5787
6267

5020
7881
4662
5978
7085
5632
5769

3497
5776
4429
4683
5489
4643
5386

Cell 4

Table 5
Average CPU time
Average CPU

Enum.

Rule 1

Rule 2

CELL 1
CELL 2
CELL 3
CELL 4
Average

1.294
3.449
147.329
210.490
90.640

0.014
0.014
0.017
0.017
0.016

0.016
0.010
0.014
0.014
0.014

Average gap

LP
(%)

LP_YII
(%)

Enum.
(%)

Rule 1
(%)

Rule 2
(%)

1646.9
1623.1
4561.9
4843.3

Cell
Cell
Cell
Cell

8.7
0.8
2.4
0.0

0.0
0.0
0.0
0.0

21.0
24.8
18.0
16.7

27.0
27.3
22.0
22.3

50.4
53.4
39.1
37.0

0.0%
0.0%
0.0%
0.0%
0.0%

Cells average

3.0

0.0

20.1

24.7

45.0

Average proﬁt

Enum.

Rule 1

Rule 2

Cell
Cell
Cell
Cell

2565.3
2667.4
6131.6
6422.0

2357.1
2591.1
5845.3
6003.9

55.8%
64.3%
34.4%
32.6%
46.8%

43.1%
59.6%
28.1%
24.0%
38.7%

Cell 1
Cell 2
Cell 3
Cell 4
Average

Two versions of the Mixed Integer Program (MIP)
model were solved to obtain an upper bound. The ﬁrst version (‘‘LP’’) is a complete linear relaxation and the second
version (‘‘LP_YII’’) relaxes the task completion time variables (Cijt) yet preserves the project selection decision variables (Yi) as binary variables. The average gaps to the semirelaxed upper bound are shown in Table 6 and Fig. 7. The
implicit enumeration achieves an average gap of 20.1%
with the upper bound on optimal proﬁt. The greedy
‘‘max proﬁt’’ rule obtains an average gap of 24.7%, while
the benchmark is 45% comparing to the upper bound.

Table 6
Average gap of solutions to upper bounds

Table 4
Average proﬁt per cell

1
2
3
4

4.3. Solution quality

1
2
3
4

Average Gap to Upper Bound
20.0%
LP

Profit Percentage to Rule 2

10.0%

Enum.

LP_YII

Rule 2

Rule 1

Enum.

60.0%

Rule 1
50.0%

Rule 2

40.0%
30.0%
20.0%

Percentage Gap

Profit Percentage

70.0%

0.0%
Cell 1

Cell 2

Cell 3

-10.0%
-20.0%
-30.0%
-40.0%

10.0%

-50.0%

0.0%
Cell 1

Cell 2

Cell 3

Instances
Fig. 6. Proﬁt percentage according to benchmark.

Cell 4

-60.0%

Instances
Fig. 7. Average gap to upper bound.

Cell 4

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

5. Conclusions
This paper examines a proﬁt-maximizing project selection and scheduling problem. Algorithms for selecting a
proﬁtable set of projects to pursue are developed and evaluated. One important conclusion is that it is important to
consider the entire set of potential projects and their interaction through resource constraints when selecting projects. Poor selection can potentially lead to signiﬁcantly
reduced proﬁts due to the diﬃculty of obtaining accurate
prior estimates of project completion time.
A second conclusion is that the order in which selected
projects are prioritized is very important. Except for the
special case of only one active project per period, prioritizing selected projects for task scheduling is sub-optimal for
both non-preemptive and preemptive tasks. However,
greedy priority rules are shown to perform very well. More
improvement can be achieved by an implicit enumeration
to obtain an optimal priority sequence. The computational
experiments in this paper use a MINSLK single-pass single
project scheduling algorithm, yet both the greedy priority
rule and the implicit enumeration framework can be
deployed with other single project scheduling algorithms.
The best algorithm based on project prioritization (i.e.
the implicit enumeration scheme) can achieve on average
46.8% more proﬁt than if we order the projects randomly.
Future research may involve developing algorithms for
the variable intensity (or multi-mode) version of the problem. We are also interested in identifying more general conditions where project prioritization would be optimal. It may
be desirable to enhance the implicit enumeration algorithm
by developing additional upper bound fathoming rules.
References
Akkan, C., Drexl, A., Kimms, A., 2005. Network decomposition-based
benchmark results for the discrete time-cost tradeoﬀ problem. European Journal of Operational Research 165 (2), 339–358.
Alcaraz, J., Maroto, C., Ruiz, R., 2003. Solving the multi-mode resourceconstrained project scheduling problem with genetic algorithms.
Journal of the Operational Research Society 54 (6), 614–626.
Alcaraz, J., Maroto, C., Ruiz, R., 2004. Improving the performance of
genetic algorithms for the RCPS problema. In: Proceedings of the
Ninth International Workshop on Project Management and Scheduling, Nancy, pp. 40–43.
Böttcher, J., Drexl, A., Kolisch, R., Salewski, F., 1999. Project scheduling
under partially renewable resource constraints. Management Science
45 (4), 543–559.
Bouleimen, K., Lecocq, H., 2003. A new eﬃcient simulated annealing
algorithm for the resource-constrained project scheduling problem and
its multiple mode version. European Journal of Operational Research.
149 (2), 268–281.
Bounds, G., 1998. The last word on project management. IIE Solutions 30
(11), 41–43.
Brucker, P., Knust, S., Schoo, A., Thiele, O., 1998. A branch and bound
algorithm for the resource-constrained project scheduling problem.
European Journal of Operational Research 107 (2), 272–288.
Brucker, P., Drexl, A., Mohring, R., Neumann, K., Pesch, E., 1999.
Resource-constrained project scheduling: Notation, classiﬁcation,
models, and methods. European Journal of Operational Research
112 (1), 3–41.

33

Brucker, P., Knust, S., 2000. A linear programming and constraint
propagation-based lower bound for the RCPSP. European Journal of
Operational Research 127 (2), 355–362.
Brucker, P., Knust, S., 2003. Lower bounds for resource-constrained
project scheduling problems. European Journal of Operational
Research 149 (2), 302–313.
Brucker, P., Knust, S., 2006. Complex Scheduling. Springer, BerlinHeidelberg.
Chen, J., 2005. Project selection, scheduling and resource allocation for
engineering design groups. Ph.D. Dissertation. The University of
Arizona, Tucson, Arizona, USA.
Coﬃn, M.A., Taylor, W.B., 1996. R& D project selection and scheduling
with a ﬁltered beam search approach. IIE Transactions 28, 167–176.
Davis, E.W., Patterson, J.H., 1975. A comparison of heuristic and
optimum solutions in resource-constrained project scheduling. Management Science 21 (8), 944–955.
Debels, D., De Reyck, B., Leus, R., Vanhoucke, M., 2006. A hybrid sctter
search/electromagnetism meta-heuristic for project scheduling. European Journal of Operational Research 196 (2), 638–653.
De Reyck, B., Demeulemeester, E., Herroelen, W., 1998. Local search
methods for the discrete time/resource trade-oﬀ problem in project
networks. Naval Research Logistics 45 (6), 553–578.
De Reyck, B., Herroelen, W., 1998. A branch-and-bound procedure for
the resource-constrained project scheduling problem with generalized
precedence relations. European Journal of Operational Research 111
(1), 152–174.
De Reyck, B., Herroelen, W., 1999. The multi-mode resource-constrained
project scheduling problem with generalized precedence relations.
European Journal of Operational Research 119 (2), 538–556.
Demassey, S., Artigues, C., Michelon, P., 2005. Constraint-progagationbased cutting planes: An application to the resource-constrained
project scheduling problem. INFORMS Journal on Computing 17 (1),
52–65.
Demeulemeester, E., De Reyck, B., Herroelen, W., 2000. The discrete
time/resource trade-oﬀ problem in project networks: A branch-andbound approach. IIE Transactions 32 (11), 1059–1069.
Demeulemeester, E., Herroelen, W., 1992. A branch-and-bound procedure
for the multiple resource-constrained project scheduling problem.
Management Science 38 (12), 1803–1818.
Demeulemeester, E., Herroelen, W., 1997. A branch-and-bound procedure
for the generalized resource-constrained project scheduling problem.
Operations Research 45 (2), 201–212.
Demeulemeester, E., Herroelen, W., 2002. Project Scheduling: A Research
Handbook. Kluwer Academic Publishers.
Demeulemeester, E.L., Vanhoucke, M., Herroelen, W.S., 2003. RanGen:
A network generator for activity-on-the-node networks. Journal of
Scheduling 6, 17–38.
Dorndorf, U., Pesch, E., Phan-Huy, T., 2000a. A time-oriented branchand-bound algorithm for resource-constrained project scheduling with
generalised precedence constraints. Management Science 46 (10),
1365–1384.
Dorndorf, U., Pesch, E., Phan-Huy, T., 2000b. A branch-and-bound
algorithm for the resource-constrained project scheduling problem.
Mathematical Methods of Operations Research 52 (3), 413–439.
Drexl, A., Nissen, R., Patterson, J., Salewski, F., 2000. ProGen/pi x – an
instance generator for resource-constrained project scheduling problems with partially renewable resources and further extensions.
European Journal of Operational Research 125 (1), 59–72.
Erenguc, S., Ahn, T., Conway, D., 2001. The resource constrained project
scheduling problem with multiple crashable modes: An exact solution
method. Naval Research Logistics 48 (2), 107–127.
Fox, G.E., Baker, R.N., Bryant, J.L., 1984. Economic models for R and D
project selection in the presence of project interactions. Management
Science 30 (7), 890–902.
Franck, B., Neumann, K., Schwindt, C., 2001. Truncated branch-andbound, schedule-construction, and schedule-improvement procedures
for resource-constrained project scheduling. OR Spektrum 23 (3), 297–
324.

34

J. Chen, R.G. Askin / European Journal of Operational Research 193 (2009) 23–34

Gabriel, S.A., Ordóñez, J.F., Faria, J.A., 2006. Contingency planning in
project selection using multiobjective optimization and chance constraints. Journal of Infrastructure Systems 12 (2), 112–120.
Hartmann, S., 2002. A self-adapting genetic algorithm for project
scheduling under resource constraints. Naval Research Logistics 49
(5), 433–448.
Hartmann, S., Drexl, A., 1998. Project scheduling with multiple modes: A
comparison of exact algorithms. Networks 32 (4), 283–297.
Hartmann, S., Kolisch, R., 2000. Experimental evaluation of state-of-theart heuristics for the resource-constrained project scheduling problem.
European Journal of Operational Research 127 (2), 394–407.
Heidenberger, K., 1996. Dynamic project selection and funding under
risk: A decision tree based MILP approach. European Journal of
Operational Research 95, 284–298.
Heilmann, R., 2003. A branch-and-bound procedure for the multi-mode
resource-constrained project scheduling problem with minimum and
maximum time lags. European Journal of Operational Research 144
(2), 348–365.
Icmeli, O., Erenguc, S., 1996. A branch and bound procedure for the
resource constrained project scheduling problem with discounted cash
ﬂows. Management Science 42 (10), 1395–1408.
Jozefowska, J., Mika, M., Rozycki, R., Waligora, G., Weglarz, J., 2001.
Simulated annealing for multi-mode resource-constrained project
scheduling. Annals of Operations Research 102, 137–155.
Jozefowska, J., Weglarz, J. (Eds.), 2006. Perspectives in Modern Project
Scheduling. Springer, New York.
Kimms, A., 2001. Maximizing the net present value of a project under
resource constraints using a Lagrangian relaxation based heuristic with
tight upper bounds. Annals of Operations Research 102, 221–236.
Klein, R., 2000. Bidirectional planning: Improving priority rule-based
heuristics for scheduling resource-constrained projects. European
Journal of Operational Research 127 (3), 619–638.
Knotts, G., Dror, M., Hartman, B., 2000. Agent-based project scheduling.
IIE Transactions 32 (5), 387–401.
Kochetov, Y., Stolyar, A., 2003. Evolutionary local search with variable
neighborhood for the resource constrained project scheduling problem. In: Proceedings of the 3rd International Workshop of Computer
Science and Information Technologies, Russia.
Kolisch, R., 1996a. Eﬃcient priority rules for the resource-constrained
project scheduling problem. Journal of Operations Management 14
(3), 179–192.
Kolisch, R., 1996b. Serial and parallel resource-constrained project
scheduling methods revisited: Theory and computation. European
Journal of Operational Research 90 (2), 320–333.
Kolisch, R., Drexl, A., 1997. Local search for nonpreemptive multi-mode
resource-constrained project scheduling. IIE Tranactions 29 (11), 987–
999.
Kolisch, R., Hartmann, S., 2006. Experimental investigation of heuristics
for resource-constrained project scheduling: An update. European
Journal of Operational Research 174 (1), 23–37.
Kolisch, R., Meyer, K., 2006. Selection and scheduling of pharmaceutical
research projects. In: Jozefowsk, J., Weglarz, J. (Eds.), Perspectives in
Modern Project Scheduling. Springer Science, New York, pp. 321–344.
Kolisch, R., Sprecher, A., 1997. PSPLIB – a project scheduling problem
library. European Journal of Operational Research 96 (1), 205–216.
Kolisch, R., Sprecher, A., Drexl, A., 1995. Characterization and generation of a general class of resource-constrained project scheduling
problems. Management Science 41 (10), 1693–1703.
Mingozzi, A., Maniezzo, V., Ricciardelli, S., Bianco, L., 1998. An exact
algorithm for the resource-constrained project scheduling problem
based on a new mathematical formulation. Management Science 44
(5), 714–729.

Möhring, R.H., Schulz, A.S., Stork, F., Uetz, M., 2003. Solving project
scheduling problems by minimum cut computations. Management
Science 49 (3), 330–350.
Musselwhite, C., 1990. Time-based innovation: The new competitive
advantage. Training and Development Journal 55, 53–56.
Neumann, K., Schwindt, C., Zimmermann, J., 2003. In: Project Scheduling with Time Windows and Scarce Resources, vol. 2. Auﬂ. SpringerVerlag, Berlin.
Neumann, K., Zimmermann, J., 2000. Procedures for resource leveling
and net present value problems in project scheduling with general
temporal and resource constraints. European Journal of Operational
Research 127 (2), 425–443.
Patterson, J.H., 1984. A comparison of exact approaches for solving the
multiple constrained resource project scheduling problem. Management Science 30 (7), 854–867.
Robinson, W.T., Fornell, C., 1985. Sources of market pioneer advantages in
consumer goods industries. Journal of Marketing Research 22, 305–317.
Schirmer, A., 2001. Resource-constrained project scheduling: An evaluation of adaptive control schemes for parameterized sampling heuristics. International Journal of Production Research 39 (7), 1343–
1365.
Schwindt, C., Zimmermann, J., 2001. A steepest ascent approach to
maximizing the net present value of projects. Mathematical Methods
of Operations Research 53 (3), 435–450.
Sprecher, A., Hartmann, S., Drexl, A., 1997. An exact algorithm for
project scheduling with multiple modes. OR Spektrum 19 (3), 195–203.
Sprecher, A., Kolisch, R., Drexl, A., 1995. Semi-active, active, and nondelay schedules for the resource-constrained project scheduling problem. European Journal of Operational Research 80 (1), 94–102.
Sprecher, A., Drexl, A., 1998. Solving multi-mode resource-constrained
project scheduling problems by a simple, general and powerful
sequencing algorithm. European Journal of Operational Research
107 (2), 431–450.
Sprecher, A., 2000. Scheduling resource-constrained projects competitively at modest memory requirements. Management Science 46 (5),
710–723.
Stork, F., Uetz, M., 2005. On the generation of circuits and minimal
forbidden sets. Mathematical Programming A. 102, 185–203.
Taylor, B.W., Moore, J.L., Clayton, E.R., 1982. R& D project selection
and manpower allocation with integer nonlinear goal programming.
Management Science 28 (10), 1149–1158.
Tormos, P., Lova, A., 2001. A competitive heuristic solution technique for
resource-constrained project scheduling. Annals of Operations
Research 102, 65–81.
Ulusoy, G., Sivrikaya-Serifoglu, F., Sahin, S., 2001. Four payment models for
the multi-mode resource constrained project scheduling problem with
discounted cash ﬂows. Annals of Operations Research 102, 237–261.
Urban, G.L., Carter, T., Gaskin, S., Mucha, Z., 1986. Market share
rewards to pioneering brands: An empirical analysis and strategic
implications. Management Science 32 (6), 645–659.
Valls, V., Ballestin, F., Quintanilla, M.S., 2003. A hybrid genetic
algorithm for the RCPSP. Technical Report, Department of Statistics
and Operations Research, University of Valencia.
Valls, V., Ballestı́n, F., Quintanilla, M.S., 2005. Justiﬁcation and RCPSP:
A technique that pays. European Journal of Operational Research 165
(2), 375–386.
Vanhoucke, M., Demeulemeester, E., Herroelen, W., 2001a. An exact procedure
for the resource-constrained weighted earliness-tardiness project scheduling
problem. Annals of Operations Research 102, 179–196.
Vanhoucke, M., Demeulemeester, E., Herroelen, W., 2001b. On maximizing the net present value of a project under renewable resource
constraints. Management Science 47 (8), 1113–1121.

520

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

A Modeling Framework for Engineered
Complex Adaptive Systems
Moeed Haghnevis and Ronald G. Askin

Abstract—The objective of this paper is to develop an integrated method to study emergent behavior and consequences
of evolution and adaptation in a certain engineered complex
adaptive system. A conceptual framework is provided to describe
the structure of a class of engineered complex systems and predict
their future adaptive patterns. The proposed modeling approach
allows examining complexity in the structure and the behavior
of components as a result of their connections and in relation to
their environment. Electrical power demand is used to illustrate
the applicability of the modeling approach. We describe and use
the major differences of natural complex adaptive systems (CASs)
with artificial/engineered CASs to build our framework. The
framework allows focus on the critical factors of an engineered
system, but also enables one to synthetically employ engineering
and mathematical models to analyze and measure complexity
in such systems without complex modeling. This paper adopts
concepts of complex systems science to management science and
system-of-systems engineering.
Index Terms—Complex adaptive systems (CASs), decentralization, emergence, engineered complexity, evolution, system of
systems.

I. Introduction

T

RADITIONALLY, we analyze a system by reductionism.
In other words, we study behaviors of large systems
by decomposing the system into components, analyzing the
components, and then inferring system behavior by aggregation of component behaviors. However, this bottom-up method
of describing systems often fails to analyze complex levels
and fully describe behavior. Holism reveals that the sum of
components is less than the whole system [1]. This idea
becomes important in studies of complex systems.
Complex systems have been widely studied; however, there
is not yet a comprehensive and widely accepted mathematical
model for engineered systems. Defense Research and Development Canada-Valcartier, Valcartier, QC, Canada, distributed
four comprehensive reports dedicated to the study of complex
systems. The first document provides 471 references and 713
related Internet addresses in list of projects, organizations,
journals, and conferences [2]. The second one provides different formulations and measures of complexity [3]. Their glossary defined 335 related keywords [4]. An overview of theoretical concepts of complexity theory is presented in the fourth

Manuscript received October 29, 2010; revised June 28, 2011; accepted
January 12, 2012. Date of publication April 18, 2012; date of current version
August 21, 2012.
The authors are with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ 85287 USA
(e-mail: moeed.haghnevis@asu.edu; ron.askin@asu.edu).
Digital Object Identifier 10.1109/JSYST.2012.2190696

document [5]. Magee and Weck [6] classified complex systems
and presented several examples for each group. While these
surveys show the extent of prior research, they also indicate
the lack of a comprehensive engineering model and motivate
us to consider engineered complex adaptive systems (ECASs).
Current research (mentioned in the surveys) usually considers natural systems (biological, physical, and chemical
systems) where the emergence and evolutionary behaviors can
be studied by thermodynamic laws, biological rules, and their
intrinsic dynamics that are innate parts of these systems. However, in engineered systems, decision makers or system designers develop or define rules and procedures to engineer the outcomes and control the possibilities as needed. In ECASs, objectives are artificially defined and interoperabilities between
components can be manipulated to achieve desired goals;
however, objectives and interoperabilities of natural systems
are naturally embedded. These facts motivate us to propose a
new framework for modeling this class of complex adaptive
systems (CASs). Our framework does not design CASs, it
enables us to control or at least predict ECASs mutating issues.
This paper considers the hallmarks of ECASs as emergence, evolution, and adaptation. We define emergence as the
capability of components of a system to do something or
present a new behavior in interaction and dependent to other
components that they are unable to do or present individually.
Also, we define evolution as a process of change and agility for
the whole system. Adaptation is the ability of systems to learn
and adjust to a new environment to promote their survival.
Similar definitions can be found in [4]. We will explain how
to study these hallmarks in our framework for ECASs in detail.
Study of CASs is challenging because of abstract theoretical
concepts, no applicable complete framework, and difficulty in
understanding emergence [1]. The main barrier to analyzing
ECASs by traditional methods stems from the theory of
complex systems that focuses on emergence at the lower level
and evolution at the upper system level whereas engineering
focuses on purposes and outcomes. Some research has considered complex system science in engineering environments.
Scope and Scale [7] studied properties of the structure of
complex systems and interdependence of components. The
complexity profile [8] helps measure the amount of information needed to describe each level of detail. These methods are
not mature enough to analyze and predict ECASs completely.
While electricity consumption profiles will be utilized for
illustration and validation, we will discuss how this framework
could likewise be applied in other ECASs, such as traffic and

c 2012 IEEE
1932-8184/$31.00 

HAGHNEVIS AND ASKIN: MODELING FRAMEWORK FOR ENGINEERED COMPLEX ADAPTIVE SYSTEMS

crowed behaviors, wholesale marketing, health care systems,
urban design, robotics and AI, supply chain management,
modern defense sectors, and other meta-systems. The Electric Power Research Institute, Palo Alto, CA, estimates 26%
growth of electricity consumption by 2030 in the U.S. (1.7%
annually from 1996 to 2006) [9]. Electric power grids are
ECASs with high economic impact driven by the maximum
consumption rate and uniformity of aggregate regional demand. Applying our integrated model allows reduction of
disuniformity in electricity consumption. Economic incentives
motivate local consumers to adjust behavior to limit maximum
system usage.
One of the most engineered and mathematically modeled complex systems is complex networks. Previous studies
quantify dynamics of small-world networks [10] and model
evolutionary structure of population and components in social
networks [11]. For example, structural properties of the power
grids of Southern California [12] and New York [13] have
been analyzed. We will apply some of the concepts of complex
network science at the last step of our framework.
In this paper, we focus on human decision making. Humans
can adjust their structural artifacts and actions to respond to the
challenges and opportunities of their environment. This ability
usually increases complexity. Three developed approaches to
mimic human decision behaviors are classified by [14]. Most
of the research on human networks assumes some kind of
hierarchy in the system. These studies are useful in organizational systems that have different levels of authority, such as
military and education systems that have leaders and followers.
However, complexities in heterarchical systems (components
share the same authority) are not studied.
The remainder of this paper is organized as follows. Section II presents our framework. Hallmarks and theoretical concepts of complexity are considered in building this framework.
Other sections are mapped to the profiles of the framework.
Sections III and IV detail the mathematical mechanisms
of features and relationships of components (step 1 of the
framework). These lead to analyzing the interoperabilities that
induce emergence in Section V (step 2). Evolution of traits
as the process of system adaptation and their response to the
changes is covered in Section VI (steps 3, 4). Various examples
demonstrate the validity of our method in each section.
II. Framework for Engineered Complex
Adaptive Systems
Couture and Charpentier [1] and Mostashari and Sussman [15] presented a framework to study complex systems.
Prokopenko et al. [16] depicted complex system science
concepts. Also, Sheard and Mostashari [17] visualized characteristics of complex systems. Frameworks for ECASs are still
incomplete and fragmented. In this paper, we propose a more
detailed framework for ECASs (Fig. 1). The framework can
help us focus on critical factors that change the states of an
ECAS, and enables us to synthetically employ engineering and
mathematical models to analyze and measure complexity in an
adaptive system without complex modeling. Four profiles of
ECASs and their characteristics are presented in component

Fig. 1.

521

Framework for engineered complex adaptive systems.

and system levels to show behavior of the three hallmarks.
In our proposed approach, a preparatory step identifies
adaptive complexity in an engineered system. This step is
necessary to make sure we do not spend unnecessary resources
to analyze a normal system as a complex system. To identify
a complex engineered system, we check [18] the following.
1) System structure:
a) displays no or incomplete central organizing for
the system organization (prescriptive hierarchically controlled systems are assumed to not be
complex systems);
b) behavioral interactions among components at
lower levels are revealed by observing behavior
of the system at higher level.
2) Analysis of system behavior:
a) analyzing components fails to explain higher level
behavior;
b) reductionist approach does not satisfactorily describe the whole system.
Total electricity consumption grows every year, affecting the
topology of power grids. Some researchers believe this huge
growth supports the idea of transformation from a centralized
network to a less centralized one (from producer-controlled
to consumer-interactive). This decentralization results in complexity in this system by decreasing central organization.
Moreover, the interaction of physics with the design of the
transmission links increases its complexity as do the diversity
of people, their interdependences, and their willingness to
cooperate. Time dependence of the network [19], scale-free
or single-scale feature of these networks (their node degree
distribution follows a power-law or Gaussian distribution in
long run) [20], and human decisions based on other consumers
all justify considering the electric power grid as an ECAS.
These factors have placed the U.S. power grid beyond the
capability of mathematical modeling to date [13].
To take advantage of the fundamental theories of complex systems, we study and analyze complex systems based

522

on the framework in Fig. 1. Systems are composed of
components. Components possess individual features and interoperable behaviors. Systems then have traits and learning
behaviors. Together, these form the system profile comprised
of the following aspects (we define state of each profile in
parentheses).
1) Features (components readjust themselves continuously): Here, dissection of features leads to decomposability (e.g., number of each component type and
patterns of individual behaviors) and willingness (e.g.,
fitness rate of each component and behavioral/decision
rules). The environment of the system may also affect
component actions. A measurable property of this profile
is self-information (entropy) of components. Entropy
is increased with the diversity of components and is
decreased with their compatibility. Sections III and IV
mathematically model and analyze the dissection of
features and show how self-organization appears.
2) Interoperabilities (components update their interdependences): In this profile, emergence as the hallmark
of interoperability shows what components can do in
interaction and dependent to other components that they
would not do individually. Components have exchangeability and synchronization. Autonomy increases and
dependence decreases the interrelationship of components. This profile helps us to infer the behavior of the
components. Section V models this profile.
3) Traits (system tries to improve its efficiency and effectiveness): In this profile, systems may evolve. The
whole system applies its resilience and agile abilities
to perform more effectively and efficiently. Categories
of trait structures or behaviors will be considered here.
The threshold for changing the nature or perceived
characteristic of the system is the measurable property
of this profile. It is discussed in Section VI.
4) Learning (system has flexibility to perform in unforeseen
situations): After evolving, the system must adapt to the
new situation. Systems need to be adaptive to survive;
otherwise, they may collapse in dynamic conditions.
Flexibility and robustness allow systems to adapt and
show the performance of the system. In some studies, adaptation is one kind of evolution while other
researchers delineate a difference between evolution and
adaptation (modeled in Section VI).
We define complexity of a system with the measurable
properties of the profiles; entropy (E), interoperabilities (I),
and evolution thresholds (τ). E measures diversity versus
compatibility of component features (Sections III, IV). Is define sensitivity (autonomy versus dependence) to other related
components and their effects (Section V). τs are milestones for
changes and adjustments in the system performance that can
differentiate trait categories (Section VI). In addition, a system
may have a goal. In our case, this is to minimize disuniformity
of electricity demand, D, to be formally defined later in this
paper.
The framework starts with dissection of features. First, we
study dynamics of components similar to noncomplex systems

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

(Sections III-A, III-B). Then, we define a new measure to
depict the relationships (Section III-C). These relationships
are the initial source of emergence and are defined based on
ECAS goals (in natural CASs unlike ECASs this measure is
embedded to the system and should be found by analyzing the
system behavior).
Then, we focus on the emergence phenomena of ECASs as
the core concept of complex adaptive behaviors and the source
of dynamic evolution. We present a comprehensive section on
dissection of features and propose four detailed theorems to
show controllability and predictability of the framework at the
emergence level of a system. Then, we generalize the theorems
in the comprehensive theory of mechanisms of components for
ECASs (Section IV).
To distinguish an ECAS from a pure multiagent system
(MAS), we define interoperability as the behavioral changes
that are caused by interactions (Section V). In MASs components have relationships; however, in CASs the interactions
and behaviors evolve. Interoperability shows how components
cooperate/compete based on other components and interactions to evolve and adapt to new environments (see new
measures in Section V). While either would suffice, we use the
term interoperability instead of interaction to indicate information sharing and beneficial behavior coordination. Finally, the
framework shows the adaptability and learning behavior of a
system at Section VI.
III. Dissection of Features
Various studies apply the concept of information theory to
study system complexities. The key point is that the required
length to describe a system is related to its complexity [21].
Yu and Efstathiou defined a complexity measure based on
entropy and a quantitative method to evaluate the performance
of manufacturing networks [22]. These studies applied the
concept of entropy in their research; however, they did not
discuss other hallmarks of CASs. Here, we start with the same
idea then we extend it to the other hallmarks.
A. Exponential Fitness
Consider a system of components with n different patterns
of behavior. For example, there may be n daily electricity
usage profiles for the different classes of consumers. If population of pattern i (Xi , i = 1, ..., n) changes exponentially with
fitness rate bi
Xi
(1)
= bi Xi .
t
To increase the readability of the formulation in the following
sections all ts are suppressed from the expressions except when
necessary to compare different times. The probabilities of the
patterns can be measured by the percentage of each pattern
Xi (t + 1) = bi · Xi (t) + Xi (t) or

Xi
Pi =  .
(2)
Xi
We obtain the growth equation for percentage of each group



dPi bi Xi Xi − Xi bi Xi

=
=
b
P
bi Pi . (3)
−
P
i
i
i
dt
( X i )2

HAGHNEVIS AND ASKIN: MODELING FRAMEWORK FOR ENGINEERED COMPLEX ADAPTIVE SYSTEMS

In the long run, we may assume small periods of ts
as continuous intervals. In continuous time, the exponential
function (4) replaces (1)
Xi = αi eβi t or

dXi
= αi βi eβi t
dt


dPi
= βi Pi − Pi
β i Pi
dt

(4)

(5)

βi t
where Pi = αi eα eβi t . To find self-information of components,
i

i

we can measure the entropy of population by
E=−



Pi log2 Pi .

(6)

So growth of entropy is
 dPi 1
dE
=−
[
(
+ log2 Pi )].
dt
dt ln 2
From (3) and (7)

dE 
=
bi Pi (
Pi log2 Pi − log2 Pi ).
dt

0

If population Xi has limit Li , its growth follows a logistic
function, (1) will change to
Xi
dXi
= bi Xi (1 − ).
dt
Li

(9)


Xi
dPi
Xi
= bi Pi (1 − ) − Pi [
bi Pi (1 − )].
dt
Li
Li

(10)

Thus, (3) becomes

Xi
,
Li

then


dPi
bi μi Pi ).
= Pi (bi μi −
dt
From (11) and (7), (8) can be rewritten as follows:


D=

(8)

(11)


dE 
=
μi bi Pi (
Pi log2 Pi − log2 Pi ).
(12)
dt
Growth of entropy shows how the population changes in
time by the exponential or logistic function (entropy is selfinformation). However, it is not sufficient for interpreting
the combination of components as any combination of three
components with 0.3, 0.3, and 0.4 probability leads to the same
entropy. In addition, engineered systems have a defined goal
that is not shown in the entropy (we call it disuniformity).
C. Disuniformity
Let Cit (w) be the average consumption of electricity at time
w for pattern i in period t. The disuniformity of pattern i in
time t is as follows:
 w0
Di (t) =
(Cit (w) − Cit )2 dw
(13)
0
wt

Ct (w)dw

= 0 wi
and w is a cyclic time in period t.
where
For example, if we want to show patterns of consumption
in each quarterly season for the next 20 years, w0 covers
the 24 h of consumption each day while t (t = 1, ..., 80)
shows each season. The pattern of consumption in the first
season Ci1 (w) may be different from the second one Ci2 (w).
We will illustrate how disuniformity can be extended to other
ECASs. At first glance, the disuniformity of an individual
component, (13), looks similar to variance. We do not use this
term because the consumption is not considered as a random
variable. Furthermore, it is customary to refer to the variance
as the range/noise of consumption at a specific time w.
The control objective is to minimize the disuniformity
(consumers cooperate to have uniform aggregate consumption
at each time). Thus, we seek to minimize D

(7)

B. Logistic Fitness

Define growth potential μi = 1 −

 w0
Cit

523

⎛
w0

⎜(
⎝


i∈S

 w0 
Ci (w)Xi ) −


0

i∈S Xi

i∈S

Ci (w)Xi dw

w0

⎞2
⎟
⎠ dw (14)

where population S is a connected graph of components to
show their interactions. These interactions are the source of
interoperabilities in Section V. Note that we remove ts in our
formula to increase readability. However, D, C, and X are
functions of t.
Generally, we define disuniformity as a normalized measure
of difference between the current state of components and the
goal state. Disuniformity could be reduced by incentives that
change one or more profiles or rearrange class probabilities
(source of self-organization).
Here, we use disuniformity to show how the system behaves
as an ECAS (we will show how it causes dependences between
behaviors later). Concepts from information theory are adapted
to describe complexity, self-organization and emergence in
the context of our ECASs [16]. Controlling disuniformity
is a source of self-organization in ECASs (see Section IV).
Shalizi [23] and Shalizi et al. [24] defined a quantifying
self-organization for discrete random fields (e.g., cellular automata). We reinterpret these concepts to apply them in ECASs
that may have continuous states and, unlike natural physical
systems, may not have a natural embedded energy dynamic or
self-directing law. Self-organization and adaptive agents are
analyzed by [25]. We will extend these concepts to all hallmarks of ECASs. Bashkirov [26] described self-organization
in a complex system by using Renyi and Gibbs-Shannon
entropy. These studies are applicable in natural and physical
systems. For example, a biological application, gene-gene
and gene-environment interactions, is identified by interaction
information and generalization of mutual information in [27].

IV. Entropy Versus Disuniformity, Source of
Self-Organization
In this section, we connect the concept of entropy and
disuniformity for component patterns. We prove lemmas for a
system with two components that interact in a basic dominance
scenario. Then, we generalize our lemmas to more complicated

524

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

structures of patterns and behaviors for the n-component case.
These theorems allow control, predict behaviors of features
and their relationships, and enable us to study emergence by
modeling interoperability in the next section.
Definition I:
1) Dominance: behavior i dominates behavior j (i  j) if
D i ≤ Dj .
2) Strict positive dominance: behavior i strictly positively
dominates behavior j (i  j) if Di < Dj , |Ci (w) −
Ci | ≤ |Cj (w) − Cj | for all w and sgn(Ci (w) − Ci ) =
sgn(Cj (w) − Cj ) for all w.
3) Positive dominance: behavior i positively dominates behavior j (i  j) if Di < Dj , |Ci (w)−Ci | > |Cj (w)−Cj |
for some w and sgn(Ci (w) − Ci ) = sgn(Cj (w) − Cj ) for
all w.
4) Strict negative dominance: behavior i strictly negatively
dominates behavior j (i  j) if Di < Dj , |Ci (w) −
Ci | ≤ |Cj (w) − Cj | for all w and sgn(Ci (w) − Ci ) =
sgn(Cj (w) − Cj ) for all w.
5) Negative dominance: behavior i negatively dominates
behavior j (i  j) if Di < Dj , |Ci (w) − Ci | > |Cj (w) −
Cj | for some w and sgn(Ci (w) − Ci ) = sgn(Cj (w) − Cj )
for all w.
Note that E is increasing in time (E ↑) means E(t + 1) >
E(t) and (E ↓) means E(t + 1) < E(t). We use the same
definition for (D ↑) and (D ↓). Here, Pi refers to Pi (t).
Lemma I: Given two different patterns of behavior (i and
j) in the population and i  j:
I.1) Pi < Pj (Xi < Xj ) and bi > bj iff E is increasing
time (E ↑) and D decreases in time (D ↓);
I.2) Pi > Pj (Xi > Xj ) and bi > bj iff E is decreasing
time (E ↓) and D decreases in time (D ↓);
I.3) Pi < Pj (Xi < Xj ) and bi < bj iff E is decreasing
time (E ↓) and D increases in time (D ↑);
I.4) Pi > Pj (Xi > Xj ) and bi < bj iff E is increasing
time (E ↑) and D increases in time (D ↑).

in
in
in
in

Proof (Sufficiency of Lemma I): We are given n = 2, Pi (t) +
Pj (t) = 1, and Pi (t) < Pj (t), so, Pi (t) < 1/2 and Pj (t) > 1/2.
Also, bi > bj results in
bi Xi (t) + Xi (t)
Xi (t)
<
Xi (t) + Xj (t)
bi Xi (t) + Xi (t) + bj Xj (t) + Xj (t)

(15)

thus Pi (t) < Pi (t + 1) and similarly, Pj (t) > Pj (t + 1). So the
probabilities are closer to a uniform distribution (Pi is closer
to Pj ) in t + 1.
Recall that the uniform distribution of Xi s (frequency of
patterns) gives the maximum entropy of the system (see [28]
for proof). Suppose Pi = n1 is the uniform probability mass
function for Xi , i = 1, ..., n, so the maximum entropy of the
system is log2 n.
From the recall max(E) = 1 when n = 2 and Pi (t) = Pj (t) =
1/2 at time t, hence, E is an increasing function of time t, i.e.,
E(t + 1) > E(t) while Pi (t) < Pj (t).
Furthermore, because i strictly dominants j, for all time
intervals w, and has similar sign with j, increasing the portion

of

Xi
Xj

decreases disuniformity in (14) because here
|Ci (w) − Ci | ≤ |Cj (w) − Cj |

(16)

Xi (t)
Xi (t + 1)
<
Xj (t)
Xj (t + 1)

(17)

and Di < Dj ; therefore, D(t + 1) < D(t), i.e., D increases.
It is easy to show that in Lemma I.2) E is a decreasing
function of t and apply the same argument for I.3) and I.4).
Necessity of Lemma I (Proof by Contradiction): Suppose
E increases and D decreases but one or both conditions of
Lemma I.1) do not hold. In this case, necessary conditions
for one of the I.2), I.3), or I.4) hold. For example, if bi > bj
but Xi > Xj instead of Xi < Xj , this is Lemma I.2) and E
decreases which contradicts our assumption of I.1). Note that
we do not consider bi = bj or Pi = Pj , because they are neutral
cases and do not have any effect. So all four combinations of
bs and ps are generated in this lemma.
Corollary I: When conditions of Lemma I hold and t → ∞:
I.1) in exponential growth Di is a lower bound for D and
E ∈ (0, 1) when D decreases [Lemma I.1), I.2)]; also,
Dj is an upper bound for D and E ∈ (0, 1) when D
increases [Lemma I.3), I.4)];
I.2) consider logistic growth where f , f  , g, and g are
functions of the logistic limits Li ; then, max{Di , f (Li )}
is a lower bound for D and E ∈ (0, g(Li )) when D
decreases [Lemma I.1), I.2)]. Also, min{Dj , f  (Lj )} is
an upper bound for D and E ∈ (0, g (Lj )) when D
increase [Lemma I.3), I.4)].
Proof: In Corollary I.1), D decreases when proportion XXji
increases (due to the dominance condition), so min(D) = Di
when all components are i ( XXji → ∞ and E = 0). And D
increases when proportion XXji decreases, so max(D) = Dj
when all components are j ( XXji → 0 and E = 0). However,
max(E) = log2 n and n = 2, so max(E) = 1 and E is
nonnegative.
When the fitness follows a logistic function [Corollary I.2)],
we have limits for the number of is and js, XXji < ∞ if Xj = 0
and XXji > 0 if Xi = 0. So min(D) is a function of the limit of
i when XXji increases and max(D) is a function of the limit of
j when XXji decreases. Clearly, min(D) = Di when Xj = 0 and
max(D) = Dj when Xi = 0. Using the same argument we can
find the range of E which is a function of limits.
Theorem I: Given n different patterns of behavior (i =
1, ..., n) in population S, bk ≥ 0, ∀k ∈ S and i  j, for
i ∈ S  and j ∈ S − S  :

I.1) E < − log2 Pi ( i∈S Pi log2 Pi > log2 Pi ) and bi > bj
for i ∈ S  and j ∈ S − S  iff E is increasing in time
(E ↑) and D decreases
in time (D ↓);

I.2) E > − log2 Pi ( i∈S Pi log2 Pi < log2 Pi ) and bi > bj
for i ∈ S  and j ∈ S − S iff E is decreasing in time
(E ↓) and D decreases
in time (D ↓);

I.3) E < − log2 Pi ( i∈S Pi log2 Pi > log2 Pi ) and bi < bj
for i ∈ S  and j ∈ S − S  iff E is decreasing in time
(E ↓) and D increases in time (D ↑);

HAGHNEVIS AND ASKIN: MODELING FRAMEWORK FOR ENGINEERED COMPLEX ADAPTIVE SYSTEMS


I.4) E > − log2 Pi ( i∈S Pi log2 Pi < log2 Pi ) and bi < bj
for i ∈ S  and j ∈ S − S  iff E is increasing in time
(E ↑) and D increases in time (D ↑).
Proof (Sufficiency of Theorem I): This theorem generalizes
Lemma I to n components. Similar to Lemma I the entropy
of the system increases when probability of components is
closer to uniform distribution. It happens when
for exponential
growth in (8) or for logistic growth in (12), i∈S Pi log2 Pi =
log2 Pi . To reach this point, E increases if there is a larger
fitness rate for components which have probability less than
uniform distribution. In general, larger fitness rates increase
the entropy if − log2 Pi > E [Theorem I.1)] for the cases
that we cannot reach the uniform distribution or if we want
to compare some components where all have smaller or larger
probabilities than uniform.
Like Lemma I, increasing the number of dominant components decreases the total disuniformity (14). The same
argument will prove Theorem I.2), I.3), and I.4). We can also
prove the necessity of Theorem I by contradiction.
Corollary II: When conditions of Theorem I hold and
t → ∞:
II.1) Corollary I.1) can be generalized to n components in
Theorem I with E ∈ (0, log2 n);
II.2) Corollary I.2) can be generalized to n components in
Theorem I with different f , f  , g, and g functions.
Note that bk > 0, ∀k ∈ S means all Xi s are growing over
time; however, some Pi s may decrease.
Lemma II: Given two different patterns of behavior (i and
j) in the population and i  j; Lemma I.1), I.2), I.3), and I.4)
and Corollary I.1) and I.2) are valid.
Proof: This is a generalization of Lemma I to the positive
dominance case. This case allows j to dominate i in some
time interval w; however, the proof is still valid because D is
total disuniformity.
Theorem II: Given n different patterns of behavior (i =
1, ..., n) in population S, bk ≥ 0, ∀k ∈ S and i  j, for
i ∈ S  and j ∈ S − S  ; Theorem I.1), I.2), I.3), and I.4) and
Corollary II.1) and II.2) are valid.
Proof: This theorem is a generalization of Lemma II to n
components. We can use the same argument which we used
to generalize Lemma I to Theorem I to generalize Lemma II
to Theorem II.
Example 1: (Features in Fig. 1) assume there are 100 components in a complex system which only follows three patterns
i, j, and k. At time t = 1, 15% of components follow pattern
i, 65% follow j, and 20% follow k. Let bi = 0.2, bj = 0.1,
and bk = 0.3. Fig. 2(a) shows the patterns of electricity
consumption in 24 h. The objective is simulating and analyzing
the complex system for the next 20 years (80 seasons).
At t = 1 the system follows Theorem II.1)
Pi (t = 1) = 0.15, Pj (t = 1) = 0.65, Pk (t = 1) = 0.2,
E(t = 1) = 1.28, D(t = 1) = 65.08.
At t = 9 we have max(i) [follows Theorem II.2)]
Pi (t = 9) = 0.18, Pj (t = 9) = 0.38, Pk (t = 9) = 0.44,
E(t = 9) = 1.49, D(t = 9) = 59.08.
At t = 19 disuniformity starts increasing again
Pi (t = 19) = 0.13, Pj (t = 19) = 0.12, Pk (t = 19) = 0.75,
E(t = 19) = 1.07, D(t = 19) = 57.45 (D(t = 18) = 57.36).

Fig. 2.

525

Example for Theorem II. (a) Patterns. (b) Fitness. (c) D versus E.

Fig. 2(b) shows the probability changes and Fig. 2(c)
presents the behavior of components and simulates entropy and
disuniformity of the system for 80 seasons. Fig. 2(c) shows
the three different possible areas for Theorem II.
Lemma III: Given two different patterns of behavior (i and
j) in the population and i  j:
III.1) Pi < Pj (Xi < Xj ) and bi > bj iff E is increasing in
time (E ↑) and D decreases
 in time (D ↓) until D = 0
(Xi (Ci (w) − Ci )dw = Xj (Cj (w) − Cj )dw) afterward
D increases in time (D ↑);
III.2) Pi > Pj (Xi > Xj ) and bi > bj iff E is decreasing in
time (E ↓) and D decreases
 in time (D ↓) until D = 0
(Xi (Ci (w) − Ci )dw = Xj (Cj (w) − Cj )dw) afterward
D increases in time (D ↑);
III.3) Pi < Pj (Xi < Xj ) and bi < bj iff E is decreasing in
time (E ↓) and D increases in time (D ↑);
III.4) Pi > Pj (Xi > Xj ) and bi < bj iff E is increasing
(E ↑) and D increases in time (D ↑).
Proof: To prove this lemma, we should consider different
sgn(Ci (w) − Ci ) between disuniformity of i and j, for all w.
So, the total disuniformity decreases until 0 and increases
after that [because of power of 2 in (14)]. D = 0 when
the weighted disuniformity for all components i is equal to
weighted disuniformity for all components j. When the total
disuniformity increases [Lemma III.3), III.4)] we do not need
to consider any minimum point, because the function is non
decreasing.

526

Corollary III: When conditions of Lemma III hold and
t → ∞:
III.1) in exponential growth ∃ > 0 where, D <  ( is a
lower bound for D) and E ∈ (0, 1) when D decreases
[Lemma III.1), III.2)]; also, Dj is an upper bound for
D and E ∈ (0, 1) when D increases [Lemma III.3),
III.4)];
III.2) in logistic growth max{0, f (Li )} is a lower bound for
D and E ∈ (0, g(Li )) when D decreases [Lemma III.1),
III.2)]; also, min{Dj , f  (Lj )} is an upper bound for D
and E ∈ (0, g (Lj )) when D increase [Lemma III.3),
III.4)].
Proof: Proof is similar to Corollary I; however,
for a specific

w = w0 where Xi (Ci (w0 )−Ci )dw0 ≈ Xj (Cj (w0 )−Cj )dw0 ,
we have D ≈ 0. This point may happen before all components
become similar to is, so min(D) = 0 where E = 0 and E = 0
where D = 0.
Theorem III: Given n different patterns of behavior (i =
1, ..., n) in population S, bk ≥ 0, ∀k ∈ S and i  j for i ∈ S 
and j ∈ S − S  :

III.1) E < − log2 Pi ( i∈S Pi log2 Pi > log2 Pi ) and bi >
bj for i ∈ S  and j ∈ S − S  iff E is increasing in
time
in time
 and D decreases 
 (D ↓) until D =
(E ↑)
0 ( Xi (Ci (w) − Ci )dw =
Xj (Cj (w) − Cj )dw)
afterward D increases
 in time (D ↑);
III.2) E > − log2 Pi ( i∈S Pi log2 Pi < log2 Pi ) and bi >
bj for i ∈ S  and j ∈ S − S iff E is decreasing in
time
in time
 and D decreases 
 (D ↓) until D =
(E ↓)
0 ( Xi (Ci (w) − Ci )dw =
Xj (Cj (w) − Cj )dw)
afterward D increases
in time (D ↑);

III.3) E < − log2 Pi ( i∈S Pi log2 Pi > log2 Pi ) and bi < bj
for i ∈ S  and j ∈ S − S  iff E is decreasing in time
(E ↓) and D increases
in time (D ↑);

III.4) E > − log2 Pi ( i∈S Pi log2 Pi < log2 Pi ) and bi < bj
for i ∈ S  and j ∈ S − S  iff E is increasing in time
(E ↑) and D increases in time (D ↑).
Corollary IV: When conditions of Theorem III hold and
t → ∞:
IV.1) Corollary III.1) can be generalized to n components in
Theorem III with E ∈ (0, log2 n);
IV.2) Corollary III.2) can be generalized to n components
in Theorem III with different f , f  , g, and g
functions.
Lemma IV: Given two different patterns of behavior (i and
j) in the population and i  j, Lemma III.1), III.2), III.3), and
III.4) and Corollary III.1) and III.2) apply.
Theorem IV: Given n different patterns of behavior (i =
1, ..., n) in population S, bk ≥ 0, ∀k ∈ S and i  j, for i ∈ S 
and j ∈ S − S  , Theorem III.1), III.2), III.3), and III.4) and
Corollary IV.1) and IV.2) apply.
Example 2: (Features on Fig. 1) modify Example 1 to three
components with negative dominance, Theorem IV [Fig. 3(a)].
Fig. 3(b) shows the behavior of the complex system. Fig. 3(c)
shows the different possible cases of Theorem IV for scenario
Fig. 3(b), respectively.
Summary: We can summarize the results of Theorems I, II,
III, IV in Table I and conclude Theorem V as a general theo-

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

Fig. 3.

Example for Theorem IV. (a) Pattern. (b) Fitness. (c) D versus E.

rem to control decomposability and willingness of components
of a complex system in all dominance cases.
Theorem V (Mechanisms of Components): If i  j, i.e.,
is dominate js, disuniformity of the system is decreasing in
time if the entropy increases in time when − log2 Pi > E or
if
time when
− log2 Pi < E while,

the entropy decreases in 
Xi (Ci (w) − Ci )dw <
Xj (Cj (w) − Cj )dw for both
conditions.
We can apply this theorem to control or at least predict the
complex behaviors in large ECASs. Here, we provide incentives to motivate the components to decrease the disuniformity
by adjusting their patterns (this adjustment changes the fitness
rates bi s dynamically). This heterarchical rearrangement with
external changes to the environment but without central organization is a source of self-organizing in components. As
an illustration, assume n patterns of consumption in a system.
When n is large (e.g., patterns of consumers in large metropolitan area), it is impossible to control and predict all behaviors
and their relationships. We can focus on a few groups (pattern
i where − log2 Pi > E) and increase the entropy by motivating
other consumers to adjust to this pattern (migrate to this
pattern or increase its fitness portion). This phenomena makes
nonlinear complex dynamic fitness rates, i.e., bi = K(R(D); E).
Here, K is a function of R(D) and population of other patterns
(i.e., E). R(D) shows the motivations based on D (e.g.,
rewards that consumers receive by cooperating to reduce the
disuniformity). These changes in bi s make Xi dependent on
each other. To predict the behaviors at each time, we can map
the system conditions (dominance, entropy, and fitness rates)

HAGHNEVIS AND ASKIN: MODELING FRAMEWORK FOR ENGINEERED COMPLEX ADAPTIVE SYSTEMS

527

TABLE I
Summary of Emergence

ij
ij
ij
ij

− log2 Pi > E
bi > bj
bi < bj
E↑∧D↓
E↓∧D↑
E↑∧D↓
E↓∧D↑
E↑∧D↓D↑ E↓∧D↑
E↑∧D↓D↑
E↓∧D↑





− log2 Pi < E
bi > bj
bi < bj
E↓∧D↓
E↑∧D↑
E↓∧D↓
E↑∧D↑
E↓∧D↓D↑
E↑∧D↑
E↓∧D↓D↑ E↑∧D↑





*Note:  means
Xi (Ci (w) − Ci )dw >
Xj (Cj (w) − Cj )dw changes


Xi (Ci (w) − Ci )dw <
to
Xj (Cj (w) − Cj )dw in time, or vice versa.

to an appropriate theorem. In the next section, we will show
how we can control the interoperability between patterns by
using a third pattern (catalyst), i.e., indirectly utilize Theorem
V to decrease the disuniformity.

V. Emergence as the Effect of Interoperability
In this step of the framework, we study the engineering
concept of emergence in ECASs. Bar Yam [29] conceptually
and mathematically showed the possibility of defining a notion
of emergence and described four concepts of emergence.
Conceptual classification for emergence is proposed by Halley
and Winkler [30]. Prokopenko et al. [16] interpreted concepts
of emergence and self-organization by information theory
and compared them in CASs. We borrow some concept of
information theory to analyze and predict emergence behaviors
of ECASs and show the applicability of Theorem V.
Emergence cannot be defined by properties and relationships of the lower component level [23]. Assume there is an
interaction between pattern i and j at their current level. Then
(6) becomes
E(i, j) = −

Mj
Mi 


Pmi mj log2 Pmi mj

(18)

mi =1 mj =1

where Pmi mj is the joint probability to find simultaneously
pattern i and pattern j in state mi and mj . The interaction
information (mutual information) of i and j
p

I (i; j) =

Mj
Mi 

mi =1 mj =1

Pmi mj log2

Pm i m j
Pm i Pm j

(19)

measures the interoperability between i and j which is the
amount of information that i and j share and reduce the
uncertainty of each other, where Pmi is the marginal probability
for State mi . We can obtain (see [28])

completely autonomic). We can use this property to control
the entropy in Lemmas I–IV.
The generalization of (20) to three-pattern cases is
E = E(i, j, k) = −[E(i) + E(j) + E(k)] − I(i; j; k) +
E(i, j) + E(i, k) + E(k, j)

(21)

where interoperability I can be negative and
I(i; j; k) = I(i; j) − I(i; j|k).

(22)

Positive I means k supports and increases the interoperability between i and j. However, negative I shows k inhibits and
decreases the interoperability.
Definition II:
1) Catalyst: Pattern k is a positive catalyst for other patterns
in the system if k supports their interoperability and is
a negative catalyst if inhibits their interoperability.
It is possible to generalize (21) and (22) to n patterns [27]

E(μ) =
(−1)(|μ|−|ν|−1) E(ν) − I(μ)
ν⊆μ,ν=μ
(23)
μ = {im |m = 1, ..., n}
I(i1 ; ...; in ) = I(i1 ; ...; in−1 ) − I(i1 ; ...; in−1 |in ).

(24)

Generally, for multiple catalyst (k number of catalyst)
I(i1 ; ...; in ) = I(i1 ; ...; in−k ) − I(i1 ; ...; in−k |i(n−k+1) ; ...; in ).
(25)
In Theorem V, instead of increasing or decreasing the
entropy we can change the interoperability. We add catalyst(s)
to control (inhibit or support) the interoperability.
Definition III:
1) Catalyst-associate interoperability (CAI)
CAI = I(μ|k) − I(μ).

(26)

2) Effect of catalyst (EOC)
E = E(i, j) = E(i) + E(j) − I(i; j)

(20)

where E(i) = I(i, i) is the self-information of i.
From (20) when I(i; j) increases (I ↑), E decreases (E ↓).
For the case of only two groups of patterns in the system,
the mutual information is a positive number with maximum
of one, 0 ≤ I ≤ 1 [from (19)]. E is minimal when i and j are
identical, I = 1 (one group follows the other one) and E is at
its maximum when i and j are independent, I = 0 (groups are

Et − Et
(27)
CAI
where E (t) and E(t) are entropy in time t after and
before applying the catalyst(s), respectively.
Example 3: (Interoperability in Fig. 1) assume Table II is
the joint probabilities for i and j in Example 1 if i can be 0.2,
0.4 or 0.6 and j can be 0.1, 0.15 or 0.2 of total consumers.
Population of other patterns and their effects are negligible.
EOC =

528

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

TABLE II
Prior Probabilities for k ≈ 0
P(mi , mj )
0.20
0.40
0.60

0.10
0.20
0.15
0.05

0.15
0.05
0.15
0.05

0.20
0.02
0.15
0.18

TABLE III
Posterior Probabilities for k > 0
P(mi , mj |k)
0.20
0.40
0.60

0.10
0.23
0.15
0.02

0.15
0.03
0.19
0.03

0.20
0.02
0.13
0.20

From (18), (19), and (20), E(i) = 1.56, E(j) = 1.54,
E(i, j) = 2.90, I(i; j) = 0.20. If adding catalyst k updates Table
II to Table III (users k affect the interrelationships between is
and js), E(i) = 1.56, E(j) = 1.53, E(i, j) = 2.73, I(i; j) = 0.36.
So we increase the entropy by increasing the interoperability
which decreases the disuniformity in Example 1
CAI = 0.36 − 0.2 = 0.16
EOC = 2.73−2.90
= −1.06.
0.16
We can use the concept of EOC to select an appropriate
catalyst. For example, assume n patterns of consumption in a
social population, where i1 and i2 have the majority of population and thus the largest effect on the disuniformity of the
consumption. We are planning to decrease the disuniformity
with a limited amount of resources (e.g., some rewards to give
to cooperative consumers). Instead of distributing the reward
between a large group (say i1 ) to cooperate with the other
group which is not so effective (because the portion of each
individual is too low), we can reward a small group of catalyst
(say i3 ) to improve the interoperability between i1 and i2 . This
idea is similar to finding and investing on hubs in a social
network (based on power law the numbers of components
with higher relationships decrease exponentially [12]). The
next step is to show how these emergence phenomena cause
evolution in the system.

VI. Evolution Because of Updates in the Traits
Here, we analyze the evolution process. Then, in the last
step of the framework we depict the adaptation and learning in
the system. Some measures are developed for the complexity
threshold parameter of physical complex systems in previous
studies [31]. Erdo" s and Rényi [32] studied probability threshold function and evolution in random graphs. We borrow the
concept of threshold [32].
Let Mλ (t), λ = 1, ..., λ0 , be the number of components in
patterns which have the trait λ at time t. Here, φ(λ, t) is a
binary variable that shows the system possesses trait λ at time t
⎧
⎨ 1,
if MλX(t)(t) ≥ τλ
i i
φ(λ, t) =
(28)
⎩ 0,
if MλX(t)(t) < τλ
i

where τλ is the threshold for trait λ.

i

Fig. 4. Complicated example. (a) Nonstationary adaptation. (b) Fast
adaptation.

Let 
(t) = (φ(λ, t); λ = 1, ..., λ0 ) be a vector of 0 and
1s, where its λth position is 1 if φ(λ, t) = 1. Let (t) be a
predefined finite set of 
s at time t. Based on the definition,
the system evolves when ∃t  > t, 
(t) ∈  & 
(t  ) ∈
/  (or

(t) ∈
/  & 
(t  ) ∈ ).
Definition IV:
1) Stagnation: systems are stagnant when they are not
evolvable, i.e., 
(t) ∈  (or 
(t) ∈
/ ) ∀t.
Example 4: (Traits in Fig. 1) assume τi = 0.2, τj = 0.4,
τk = 0.3, and  = {[0 1 1], [1 1 1]} in Example 1
⎫
26
t = 4 : MiX(t)(t) = 156
≤ 0.2 ⎪
⎪
⎪
i i
⎪
⎬
M
(t)
j
87
t = 4 :  X (t) = 156 ≥ 0.4
⇒ 
(4) = [0 1 0]
⎪
i i
⎪
⎪
M
(t)
⎪
44
t = 4 :  kX (t) = 156
≤ 0.3 ⎭
i i
⎫
31
t = 5 : MiX(t)(t) = 183
≤ 0.2 ⎪
⎪
i
⎪
i
⎪
⎬
M
(t)
j
95
t = 5 :  X (t) = 183 ≥ 0.4
⇒ 
(4) = [0 1 1]
⎪
i i
⎪
⎪
57
⎭
t = 5 : MkX(t)(t) = 183
≥ 0.3 ⎪
i i
⎫
55
t = 9 : MiX(t)(t) = 367
≤ 0.2 ⎪
⎪
⎪
i i
⎪
⎬
M
(t)
j
139

t=9:
=
≤
0.4
⇒ 
(4) = [0 0 1].
367
X (t)
⎪
i i
⎪
⎪
⎭
t = 9 : Mk (t) = 163
≥ 0.3 ⎪
367
i

Xi (t)

So the system evolves when t = 5 and t = 9. If we assume
the system evolves only when it possesses all traits (i.e.,  =
{[1 1 1]}), this system is stagnant.
In this example, the system is adjusted by two evolutions.
This adapting situation can be nonstationary. Fig. 4(a) simulates a case where components adjust their behaviors several
times to increase their objectives. Here, i, j, and k compete to
get more rewards by reducing the disuniformity. However, to
reduce the disuniformity they should cooperate by adjusting
their behaviors (changing fitness rates, bi s). Adding a learning
procedure (do not adjust to previous tried states) omits the nonstationary evolution and causes faster adaptation in Fig. 4(b).
To extend this framework, the concept of dissection of
features can extend to other ECASs easily. Entropy of components is a general concept for all systems and disuniformity can be interpreted in different ECASs. For example,
reducing demand fluctuations in wholesale marketing, resource
allocations in supply chain management, and synergism of
commands to reduce the distances to a target in AI or

HAGHNEVIS AND ASKIN: MODELING FRAMEWORK FOR ENGINEERED COMPLEX ADAPTIVE SYSTEMS

defense sectors are other types of disuniformity. Decision
makers may assign different objectives to ECASs based on
their requirement and they are not limited to disuniformity.
However, any ECAS of the class being addressed needs at
least one minimizing/maximizing measure to study dissection
of features other than component entropy. Other hallmarks
(evolution and adaptation) are driven from the emergence
concept (dissection of features and the interactions) and their
mathematical calculation is not limited to electricity usage.
VII. Conclusion and Future Work
In this paper, we presented a framework that helped us employ engineering and mathematical models to analyze certain
ECASs. We can apply this framework to study and predict the
hallmarks of complex heterarchical engineered systems. The
proposed method was used to engineer emergence of human
decisions in an ECAS, evolution of the behaviors, and its
adaptation to new environments. We illustrated how we can
extend the concept of our measures to other ECASs.
We employed information theory in our mathematical
model. All possible dominance cases in complex systems were
defined and four theorems were presented to calibrate the
current situation and predict future behaviors of each case.
Theorem V (mechanisms of components) can be employed to
study self-organization in ECASs.
Catalyst associate interoperability and stagnation of the
system are new concepts that can help us measure or scale
the emergence and evolution behaviors without complex modeling. Researchers may control the interoperability of components with CAI. Also, they can measure evolvability or
stagnation of a complex system by a threshold function.
Varying fitness rates by time, bi (t), may lead to a new
formulation in future research. We can consider statistical or
dynamical functions for fitness rates. Agent-based modeling
and simulation can support and extend the mathematical basis
of this research for investigating real cases.
Acknowledgment
The authors would like to thank Prof. D. Armbruster, School
of Mathematical and Statistical Sciences, Arizona State University, Tempe, for his constructive comments that improved
the quality of this paper.
References
[1] M. Couture and R. Charpentier, “Elements of a framework for studying
complex systems,” in Proc. 12th Int. Command Control Res. Technol.
Symp., Jun. 2007, pp. 1–17.
[2] M. Couture, “Complexity and chaos: State-of-the-art; list of works, experts, organizations, projects, journals, conferences and tools,” Defense
Res. Develop. Canada-Valcartier, Valcartier, QC, Canada, Tech. Rep. TN
2006-450, 2006.
[3] M. Couture, “Complexity and chaos: State-of-the-art; formulations and
measures of complexity,” Defense Res. Develop. Canada-Valcartier,
Valcartier, QC, Canada, Tech. Rep. TN 2006-451, 2006.
[4] M. Couture, “Complexity and chaos: State-of-the-art; glossary,” Defense
Res. Develop. Canada-Valcartier, Valcartier, QC, Canada, Tech. Rep. TN
2006-452, 2006.

529

[5] M. Couture, “Complexity and chaos: State-of-the-art; overview of theoretical concepts,” Defense Res. Develop. Canada-Valcartier, Valcartier,
QC, Canada, Tech. Rep. TN 2006-453, 2007.
[6] C. L. Magee and O. L. de Weck, “Complex system classification,” in
Proc. 14th Annu. Int. Symp. INCOSE, Jun. 2004, pp. 1–18.
[7] Y. Bar-Yam, “Multiscale complexity/entropy,” Adv. Complex Syst., vol. 7,
no. 1, pp. 47–63, 2004.
[8] Y. Bar-Yam. (2000). Complexity Rising: From Human Beings to Human
Civilization, a Complexity Profile [Online]. Available: http://necsi.org/
Civilization.html
[9] N. Parks, “Energy efficiency and the smart grid,” Environ. Sci. Technol.,
vol. 43, no. 9, pp. 2999–3000, May 2009.
[10] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-world’
networks,” Nature, vol. 393, pp. 440–442, Jun. 1998.
[11] C. Avin and D. Dayan-Rosenman, “Evolutionary reputation games
on social networks,” Complex Syst., vol. 17, no. 3, pp. 259–277,
2007.
[12] L. A. N. Amaral, A. Scala, M. Barthelemy, and H. E. Stanley, “Classes
of small-world networks,” Proc. Nat. Acad. Sci., vol. 97, no. 21, pp.
11 149–11 152, Oct. 2000.
[13] S. H. Strogatz, “Exploring complex networks,” Nature, vol. 410, pp.
268–276, Mar. 2001.
[14] S. Lee, Y. Son, and J. Jin, “Decision field theory extensions for behavior
modeling in dynamic environment using Bayesian belief network,”
Inform. Sci., vol. 178, no. 10, pp. 2297–2314, 2008.
[15] A. Mostashari and J. M. Sussman, “A framework for analysis, design
and management of complex large-scale interconnected open sociotechnological systems,” Int. J. Decision Support Syst. Technol., vol. 1, no. 2,
pp. 53–68, 2009.
[16] M. Prokopenko, F. Boschietti, and A. J. Ryan, “An information-theoretic
primer on complexity, self-organization, and emergence,” Complexity,
vol. 15, no. 1, pp. 11–28, 2009.
[17] S. Sheard and A. Mostashari, “Principles of complex systems for
systems engineering,” Syst. Eng., vol. 12, no. 4, pp. 295–311,
2009.
[18] J. Ottino, “Engineering complex systems,” Nature, vol. 427, p. 399, Jan.
2004.
[19] D. Braha and Y. Bar-Yam, “The statistical mechanics of complex product
development: Empirical and analytical results,” Manage. Sci., vol. 53,
no. 7, pp. 1127–1145, Jul. 2007.
[20] B. Shargel, H. Sayama, I. Epstein, and Y. Bar-Yam, “Optimization of
robustness and connectivity in complex networks,” Phys. Rev. Lett.,
vol. 90, no. 6, pp. 068701-1–068701-4, 2003.
[21] K. Kaneko and I. Tsuda, Complex Systems: Chaos and Beyond: A Constructive Approach With Applications in Life Sciences. Berlin, Germany:
Springer, 2001.
[22] S. B. Yu and J. Efstathiou, “An introduction to network complexity,” in
Proc. Manuf. Complexity Netw. Conf., Apr. 2002, pp. 1–10.
[23] C. R. Shalizi, “Causal architecture, complexity and self-organization
in time series and cellular automata,” Ph.D. dissertation, Center Study
Complex Syst., Univ. Michigan, Ann Arbor, May 2001.
[24] C. R. Shalizi, K. L. Shalizi, and R. Haslinger, “Quantifying selforganization with optimal predictors,” Phys. Rev. Lett., vol. 93, no. 14,
pp. 1 187 011–1 187 014, 2004.
[25] S. E. Page, “Self organization and coordination,” Comput. Econ., vol. 18,
no. 1, pp. 25–48, Aug. 2001.
[26] A. G. Bashkirov, “Renyi entropy as a statistical entropy for complex systems,” Theor. Math. Phys., vol. 1149, no. 2, pp. 1559–1573,
2006.
[27] P. Chanda, L. Sucheston, A. Zhang, D. Brazeau, J. L. Freudenheim,
C. Ambrosone, and M. Ramanathan, “Ambience: A novel approach and
efficient algorithm for identifying informative genetic and environmental
associations with complex phenotypes,” Genetics, vol. 180, no. 2, pp.
1191–1210, 2008.
[28] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
Hoboken, NJ: Wiley-Interscience, 2006.
[29] Y. Bar-Yam, “A mathematical theory of strong emergence using multiscale variety,” Complexity, vol. 9, no. 4, pp. 15–24, 2004.
[30] J. D. Halley and D. A. Winkler, “Classification of emergence and its
relation to self-organization emergence,” Complexity, vol. 13, no. 5, pp.
10–15, 2008.
[31] C. Langton, “Computation at the edge of chaos: Phase transitions and
emergent computation,” Physica D, vol. 42, nos. 1–3, pp. 12–37, 1990.
[32] P. Erdo" s and A. Rényi, “On the evolution of random graphs,” Publication
Math. Instit. Hungarian Acad. Sci., vol. 5, pp. 17–61, 1960.

530

Moeed Haghnevis received the B.Sc. and M.Sc.
degrees in industrial and system engineering from
the Amirkabir University of Technology, Tehran,
Iran, and the University of Tehran, Tehran, respectively. He is currently pursuing the Ph.D. degree in
industrial engineering with the School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe.
Before pursuing the Ph.D. degree, he served as
an Adjunct Instructor with two universities. He has
published a book and several papers. His current
research interests include engineered complex systems, agent-based modeling,
and simulation.

IEEE SYSTEMS JOURNAL, VOL. 6, NO. 3, SEPTEMBER 2012

Ronald G. Askin received the Ph.D. degree from
the Georgia Institute of Technology, Atlanta.
He is currently the Director of the School of
Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe. He has
30 years of experience in systems modeling and
analysis.
Dr. Askin is a Fellow of the IIE. He has received
the National Science Foundation Presidential Young
Investigator Award, the Shingo Prize for Excellence
in Manufacturing Research, the IIE Joint Publishers
Book of the Year Award, and the IIE Transactions Development and Applications Award.

Complex Adaptive Systems: Energy, Information and Intelligence: Papers from the 2011 AAAI Fall Symposium (FS-11-03)

Modeling Properties and Behavior of the US Power System as an
Engineered Complex Adaptive System
Extended Abstract
Moeed Haghnevis and Ronald G. Askin
School of Computing, Informatics and Decision Systems Engineering
Arizona State University, Tempe, AZ 85287-8809
Moeed.Haghnevis@asu.edu and Ron.Askin@asu.edu

Introduction

that may change totally in the next 5 minutes). This adaptive complexity in consumer behavioral level motivates us
to study the interrelationships between consumers, their interoperability, and willingness to cooperate or compete in
the system. Some previous studies considered other parts of
the US power market, e.g. physical level or business issues,
as an ECAS (Li and Tesfatsion 2009), (Conzelmann et al.
2004), (ANL ), (Barton et al. 2000), and (Wildberger and
Amin 1999).

Classically, system analysts consider the physical world as
an collection of components and their approximately linear interactions. This assumption allows studying a system
by reductionism (bottom-up understanding of decomposed
components and then aggregating the partitions) to analyze
the whole system behaviors. Today, holism evidences that
the sum of components fails to describe systems comprised
of myriad interoperabilities between agents. Emergent, evolutionary, and adaptive behaviors of the real-world depict a
fruitful source of inspiration for modeling behavior of complex adaptive systems (CAS). Traditional mathematical and
engineering modeling of CASs (such as equilibrium or game
theory models) are still incomplete and fragmented. They
are usually unable to study real characteristics of agents and
their decision behaviors. Complexity theory and concepts
are well studied in the literature (Couture 2007) and (Couture 2006b). Also, researchers tried to present mathematical methods and measures to study CASs (Couture 2006a),
(Bar-Yam 2004b), (Bar-Yam 2004a), and (Bar-Yam 2000).
This research aims to deﬁne a novel framework and
platform to employ engineering and mathematical models
to study adaptive dynamics in certain engineered complex
adaptive systems (ECAS). We analyze a class of decentralized heterarchial complex systems to infer emergent behavior of the components, evolution processes, and adaptations
of the whole systems. While the US electric power system
will be utilized for demonstration and validation, the framework has applicability to the general class of ECASs. Conditioned on parameterization of the framework, a theorem will
be presented to calibrate current situation and predict future
behaviors of an ECAS.
The huge growth of the US power system (3.7 billion
KWh consumption in 2009 i.e. 13 times greater than 1950
and expected growth to 4.88 billion KWh by 2035 (EIA ),
consumer-interactive controls, time dependency of the market, and complexity in its network topology are main reasons
to consider the US power system as an ECAS. Locational
Marginal Price (LMP) of electricity vary by time, location,
and consumer type (e.g. Fig 1 provides the LMP contour
map of the Midwest ISO (MISO ) in April 26,2011 at 17:25

Figure 1: LMP contour map of the Midwest ISO
Applying this integrated model has the following beneﬁts
for the US power system:
• Reduction of dis-uniformity in electricity consumption to
reduce investment in new generators, transmission and
distribution infrastructure.
• Communicates energy information to encourage people to
change their behavior in high stress times or vulnerabilities (e.g., speciﬁc weather, high demand days, and accident or fault in power lines or generators).
• Allows analyzing changes in the behaviors by increasing
the effectiveness of the dynamic pricing strategies.

Engineered CASs Framework
A multi-proﬁle descriptive framework is developed to calibrate the current structure of an ECAS and to predict its dynamic behaviors. Four-tuple proﬁles of ECASs, their characteristics, and the proper measures are presented in Table 1.

c 2011, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

76

Components have both individual features and interoperabilities. Systems have traits such as resilience that contribute to
adaptability and the potential to learn.

reduction or load balancing).
 w 
( i Ci (w)Xi ) −

(
D=
0

Table 1: Proﬁles of the framework for ECASs
Proﬁle
Features
Interoperabilities
System Traits
Learning

Characteristics
Decomposability,
Willingness
Synchronization,
exchangeability
Resilience,
Agility
Flexibility,
Robustness

0

Xi

i

Ci (w)Xi dw
2
w

) dw, (4)

We deﬁne behavior i dominates behavior j (i  j) if
Di ≤ Dj . Table 2 deﬁnes other possible types of dominance in the patterns of behaviors conditioning on values
and signature of variation in short-term pattern of behaviors,
σ(w) where, σ(w) = C(w) − C. For example i exhibits
Strict Positive Dominance over j if |σi (w)| ≤ |σj (w)| and
sgn(σi (w)) = sgn(σj (w)) for all w. To control decomposability and willingness of components of a CAS in all
dominance cases we can apply the theorem of mechanisms
of component, deﬁned as follows.

Indicators
Diversity vs.
Compatibility
Autonomy vs.
Dependency
Categories
Performance

At an aggregate level, we describe the system complexity
with dissection of features (Entropy, E), interoperabilities
(sensitivity and interrelationship to neighbors, R), system
traits, and learning (milestones for changes in the system
performance i.e. Evolution Thresholds, τ ). In addition a
system may have a goal. In our case, it is to minimize disuniformity of electricity consumption. Entities (consumers)
in the system are classiﬁed into behavioral types deﬁned by
patterns of daily consumption.

Table 2: Deﬁnition of dominance possibilities
Dominance
symbol
|σi (w)|
sgn(σi (w)), ∀w
Strict Positive
ij
≤ |σj (w)|∀w
= sgn(σj (w))
Positive
ij
> |σj (w)|∃w
= sgn(σj (w))
= sgn(σj (w))
Strict Negative
ij
≤ |σj (w)|∀w
Negative
ij
> |σj (w)|∃w
= sgn(σj (w))
where, σi (w) = Ci (w) − Ci

Notation:
i = 1, ..., n: set of patterns of behaviors,
Xi (t): population of pattern i at period t,
Pi (t): proportion of entities with pattern i at period t,
bi : ﬁtness rate of pattern i (static in different periods),
Cit (w): electricity consumption of pattern i at time w for
period t.
Short term cyclic time w may correspond to the hours of
a day while t refers to months or seasons. We remove t’s in
the following formulas except when necessary to compare
different periods to increase readability.

Theorem (mechanisms of components):
Consider
 
the
case
where
i

j
and
X
(w)dw
<
σ
i
i


Xj σj (w)dw. Dis-uniformity of the system is then decreasing in time if the Entropy increases in time when
− log2 Pi > E or if the Entropy decreases in time when
− log2 Pi < E.
This theorem is a result of analyzing detailed dominance
that is summarized in Table 3 (see (Haghnevis and Askin
2010) for proofs). It shows how E and D are decreasing
(↓) or increasing (↑) in time for all dominance cases (i :
j) conditioning on ﬁtness rates (bi : bj ) and the entropy
situations (log2 Pi : E). For example assume n different
patterns of behavior (i = 1, ..., n) in population S, bk ≥ 0,
∀k ∈ S and i 
 j, for i ∈ S  and j ∈ S − S  . Then
E < − log2 Pi ( i∈S Pi log2 Pi > log2 Pi ) and bi > bj for
i ∈ S  and j ∈ S − S  if f E is increasing in time (E ↑)
and D decreases in time (D ↓).

Features Dissection
Assume the population Xi grows exponentially by Eq. 1
i
where, ΔX
Δt = bi Xi .
Xi (t + 1) = bi .Xi (t) + Xi (t), i = 1, ..., n.
(1)

Then the growth of entropy, E = − Pi log2 Pi , follows
Eq. 2 where, Pi = XXi i .


ΔE
=
bi Pi (
Pi log2 Pi − log2 Pi ),
Δt

i

w 

Interoperabilites cause Emergence
We consider four classes of agents based on interoperabilities, the abilities of agents to connect and effect each other in
a decision network; Inﬂuencers(INF), Early Followers(EF),
Late Followers(LF), and Isolated(ISO). The four classes exhibit descending rank of interoperabilities on the basis of
their inﬂuences and ascending rank on the basis of their frequency in the decision network.
Let I c represent interoperability between agents in different classes. There can be other interoperabilities in a system such as interoperability between pattern types. I generally shows the interoperabilities between properties of
agents. We measure the interrelationship (R) between agents
by the interoperability (I c ) matrix and their classiﬁcation.
min(I c ) = 0 when two classes are independent (autonomic)

i = 1, ..., n.
(2)

The dis-uniformity variation
of pattern i by time is shown in
w t
Ci (w)dw
t
0
.
Eq. 3 where, Ci =
w
 w
(Cit (w) − Cit )2 dw, i = 1, ..., n,
Di (t) =
(3)
0
wt
One part of the decision objective is to minimize the total
dis-uniformity, Eq. 4 (consumers cooperate to have less ﬂuctuations in the total consumption in different times for peak

77

Table 3: Details of mechanisms of component
− log2 Pi > E
− log2 Pi < E
bi > bj
bi < bj
bi > bj
bi < bj
ij
E ↑∧D↓
E ↓∧D↑
E ↓∧D↓
E ↑∧D
ij
E ↑∧D↓
E ↓∧D↑
E ↓∧D↓
E ↑∧D
i  j E ↑ ∧ D ↓ if (1), D ↑ if (2) E ↓ ∧ D ↑ E ↓ ∧ D ↓ if (1), D ↑ if (2) E ↑ ∧ D
i  j E ↑ ∧ D ↓ if (1), D ↑ if (2) E ↓ ∧ D ↑ E ↓ ∧ D ↓ if (1), D ↑ if (2) E ↑ ∧ D


 

 

(1) Xi σi (w)dw >
Xj σj (w)dw, (2) Xi σi (w)dw <
Xj σj (w)dw.
and max(I c ) = 1 when they follow each other (identical).
We use Eq. 5 to calculate interrelationship of Agent υ with
its neighbors (interrelated) in Pattern i.

entropy of features. A preferential attachment of a social network of consumer deﬁnes the interoperabilities I c where, its
edges depict the interrelationships R. The nodes of the network grow by ﬁtness rate bi where, the node degree distribution follows power law (scale-free network (Barabasi and
Bonabeau 2003)). Statistical analysis of the system presents
the initial bi ’s that may change based on behavioral characteristics of consumers (they may be persuaded to migrate to
other patterns by motivators such as dynamic pricing and attractiveness) and cause the emergence. Here, required data is
gathered manually and simulated for future behaviors. Electricity regulators strive to balance the workload or reduce
the peak time by deﬁning evolution thresholds and sets in
respond to demand ﬂuctuations. They seek to control consumer behaviors by providing incentives and social education.



θυδ .I c .Xδνi
 νδυ δν
,
δν
i Xδνi

Xi ,
i = 1, ..., n, υ = 1, ...,
Rυi =

δν

(5)

i

Iδcυ δν

where,
is the interoperability of the agent in Class
δυ with agent in Class δν . Xδνi is the number of connected
agents (neighbors) to Agent υ in Class δν with Pattern i. Average self-preference, 0.5 ≤ θυδν ≤ 1, lets two agents, from
a same set of classiﬁcation, have different interoperability
with a speciﬁc agent.
To select an appropriate pattern for evolving, we use Eq. 6
here, switchυt is the patten that Agent υ chooses to evolve
in time t.
switchυt = argi {max(Rυi )},

Rυi > Υi ,

∀i,

Conclusions and Future Research
The proposed framework enables us to model characteristics
and behaviors of agents within a system and examine their
correlations and responses to environmental changes. Our
model allows studying hallmarks of CASs and helps us to
analyze complexity in a system without complex modeling.
This study can help us to deﬁne new measures for ECASs.
In the future, agent-based modeling and simulation of
the dynamic system can support the mathematical modeling of this paper. We can improve the decision mechanisms
in evolution by adding statistical or optimization learning
algorithms. Moreover, we can study behavior of the system based on different complex decision network topologies
(e.g. scale-free and single-scale networks). Some other characteristics of consumers such as irrationality (information,
pricing, and communal beneﬁt) or effect of externalities in
their decisions may give closer results to the real cases. Here,
we tried to minimize the dis-uniformity. However, controlling or predicting dis-uniformity is a more general objective
that can be studied in the future under the presence of additional factors in the model such as price incentives.

(6)

where, Υi is the required support (motivation or proﬁt) for
switching i. To avoid evolving to similar patters we may use
Eq. 7 instead of Eq. 6.
switchυt = argi {max(Rυi )}, Rυi > Υi ,
∀i = Pattern(υ).

↑
↑
↑
↑

(7)

Updates and adjustments cause Evolution
The probability that a system possesses attribute λ, such as
willingness to adjust consumption pattern for a speciﬁc cost
saving, is:

1,
if MλX(t)
≥ τλ , ∃t0 ∈ [0, t],
i (t)
i
φ(λ, t0 ) =
< τλ , ∀t0 ∈ [0, t],
0,
if MλX(t)
i (t)
i
(8)
where, τλ is the threshold for property λ and Mλ (t) is the
number of patterns which have the attribute λ.
Let Φ(t) = (φ(λ, t); λ = 1, ..., λ0 ) be a vector of 0 and
1’s where, its λth position is 1 if φ(λ, t) = 1. The system evolves when ∃t > t, Φ(t) ∈ Ψ(or Ψ ) & Φ(t ) ∈
Ψ (or Ψ). Where, Ψ(t) is a predeﬁned ﬁnite set of Φ’s at
time t and Ψ is all other possible combinations of 0 and 1
for Φ’s that are not in Ψ (i.e. complement of Ψ).
For example, in a power system agents are consumers of
electricity. Each follows one of n different daily consumption patterns with probability Pi . These probabilities deﬁne

References
ANL. Electricity market complex adaptive system (EMCAS): A new long-term power market simulation tool. Decision and Information Sciences Division, Argonne National
Laboratory, www.dis.anl.gov/pubs/60358.pdf.
Bar-Yam, Y. 2000. Complexity rising: From human beings to human civilization, a complexity proﬁle. available
at: http://necsi.org/Civilization.html.

78

list of works, experts, organizations, projects, journals, conferences and tools. Technical Report TN 2006-450, DRDC
Valcartier.
Couture, M. 2007. Complexity and chaos - state-of-theart; overview of theoretical concepts. Technical Report TN
2006-453, DRDC Valcartier.
EIA. U.S. Energy Information Administration. www.eia.gov
(Last visited: March 2011).
Haghnevis, M., and Askin, R. G. 2010. Modeling framework for engineered complex adaptive systems. IEEE Systems Journal, special issue; Complexity in Engineering:
from Complex Systems Science to Complex Systems Technology, under review.
Li, H., and Tesfatsion, L. 2009. The AMES wholesale power
market test bed: A computational laboratory for research,
teaching, and training. Calgary, Alberta, Canada: Power and
Energy Society.
MISO.
Midwest Independent System Operators.
www.midwestiso.org (Last visited: March 2011).
Wildberger, M., and Amin, M. 1999. Simulator for electric
power industry agents (SEPIA)-complex adaptive strategies.
Technical Report TR-112816, EPRI, California, USA.

Bar-Yam, Y. 2004a. Multiscale complexity/entropy. Advances in Complex Systems 7(1):4763.
Bar-Yam, Y. 2004b. Multiscale variety in complex systems.
Complexity 9(4):37–45.
Barabasi, A., and Bonabeau, E. 2003. Scale-free networks.
Scientiﬁc American 288(5):50–59.
Barton, D. C.; Eidson, E. D.; Schoenwald, D. A.; Stamber,
K. L.; and Reinert, R. K. 2000. Aspen-EE: An agent-based
model of infrastructure interdependency. Technical Report
SAND2000-2925, Infrastructure Surety Department, Sandia
National Laboratories, Albuquerque, NM, USA.
Conzelmann, G.; North, M.; Boyd, G.; Cirillo, R.; Koritarov,
V.; Macal, C.; Thimmapuram, P.; and Veselka1, T. 2004.
Simulating strategic market behavior using an agent-based
modeling approach-results of a power market analysis for
the midwestern united states. Zurich: 6th IAEE European
Energy Conference on Modeling in Energy Economics and
Policy.
Couture, M. 2006a. Complexity and chaos - state-of-the- art;
formulations and measures of complexity. Technical Report
TN 2006-451, DRDC Valcartier.
Couture, M. 2006b. Complexity and chaos - state-of-the- art;

79

Proceedings of the 2010 Winter Simulation Conference
B. Johansson, S. Jain, J. Montoya-Torres, J. Hugan, and E. Yücesan, eds.

MACHINE QUALIFICATION MANAGEMENT FOR A SEMICONDUCTOR BACK-END FACILITY
Mengying Fu
Moeed Haghnevis
Ronald Askin
John Fowler
Muhong Zhang
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287, USA

ABSTRACT
In order to process a product in a semiconductor back-end facility, a machine needs to be qualified first by having a
product-specific software program installed on it. Then a tool set must be available and attached on the machine while
it is processing the product. In general not all machines are qualified to process all products due to the high machine
qualification cost and tool set availability. However, the machine qualification decision is very important because it
affects capacity allocation in the facility and subsequently affects daily production scheduling. To balance the tradeoff
between the machine qualification costs and backorder costs, a product-machine qualification optimization model is
proposed in this paper.
1

INTRODUCTION

The semiconductor manufacturing process consists of two main parts: the front-end process and the back-end process.
The front-end process, also known as wafer fabrication, typically has a small number of products and very complex
reentrant product flow. In contrast, the back-end process, also known as assembly and test, typically has hundreds or
thousands of different products and relatively linear product flow. The research presented in this paper focuses on the
back-end process.
In a semiconductor back-end facility, each machine has to be configured for each of the products it will process in
the future. This configuration (machine qualification) process includes installing and testing a software program for each
product on the machine. Due to the wide product mix (i.e. thousands of products), if all machines were to be qualified
for all products, the machine qualification process could take a really long time, thus incurring a high nonnegligible
machine qualification cost. Meanwhile, not all machines are even able to be qualified for all products. Because of
short product life cycles and fast development of new products in the semiconductor industry, machines need to be
updated with new products. As a result, machines that perform the same operation could belong to different machine
types/generations, with each type/generation only being able to be qualified for a subset of products. In addition, the
product-machine qualification decision affects the capacity planning decision and subsequently the daily production
scheduling in the future. Poor product-machine qualification decisions could cause shortages by not qualifying enough
machines for a given product or machine utilization imbalance by qualifying too many products on a small subset of
machines. In this paper, a mixed integer linear programming model (MILP) is proposed to optimize product-machine
qualification while considering production scheduling in a medium term planning horizon (e.g. a couple of weeks).
As the last part of the semiconductor manufacturing system, on time delivery of customer orders is generally the most
important goal in the back-end process. Hence the objective of the MILP is set to minimize the weighted machine
qualification costs and backorder costs.
The remainder of the paper is organized as follows. Section 2 is a literature review about machine qualification
optimization. In Section 3, a mixed integer linear programming model (MILP) is presented to optimize product-machine
qualification at a stage in the semiconductor back-end process. This is followed by Section 4, in which extensions of
the MILP model are discussed. Finally, conclusions and future research directions are provided in Section 5.

978-1-4244-9865-9/10/$26.00 ©2010 IEEE

2486

Fu, Haghnevis, Askin, Fowler, Zhang
2

LITERATURE REVIEW

Product-machine or operation-machine qualification is a very common feature in the modern semiconductor manufacturing process. A few papers consider this feature in their scheduling models (Hurink et al. 1994, Jurisch 1995,
Brucker et al. 1997, Mati and Xie 2004, Wu et al. 2006, Wu et al. 2008), but none of them proposes any change or
optimization on the current machine qualification. There are also some other papers that utilize short-term machine
dedication to schedule the production (Campbell 1992, Bourland and Carl 1994). An operation-machine qualification
management system is proposed by Johnzén et al. (2008) for a semiconductor front-end facility, and four flexibility
measures are developed for each operation-machine qualification. Impact of different operation-machine qualifications,
with different scores according to each of the four flexibility measures, on scheduling is showed through simulation.
Aubry et al. (2008) present a mixed integer linear programming model (MILP) for the product-machine qualification
optimization of parallel multi-purpose machines. The objective is to minimize machine configuration costs while still
obtaining a load-balanced capacity allocation. The MILP formulation is proved to be strongly NP-hard but could
be relaxed to a transportation problem under some assumptions. Rossi (2010) provide a robustness measure for the
multi-purpose machine configuration model based on Aubry et al. (2008). Maximal disturbance of the demand that
would change the optimal configuration is used as the robustness measure. Ignizio (2009) proposes a binary optimization
model for operation-machine qualification at the photolithography machines in a wafer fabrication factory. The objective
is to obtain a load-balanced schedule at minimal machine qualification costs. The cycle time in the factory is shown
to be decreased using the binary optimization model compared to qualifications developed by heuristic or “educated
guess” means. In somewhat related work, Drexl and Mundschenk (2008) propose an integer programming model for
long-term employee staffing based on qualification profiles. The objective is to accomplish all tasks with minimal total
employment costs. Employee scheduling could be another application area of the methodologies developed for the
machine qualification management in the factory.
However, none of these papers above considers multi-stage manufacturing systems or production scheduling in
the machine qualification optimization model. In this research, a mixed integer linear programming model (MILP) is
proposed to optimize product-machine qualification in a multi-stage manufacturing system while considering detailed
scheduling in a medium term planning horizon (e.g. several weeks). The MILP minimizes the total machine qualification
costs and backorder costs.
3

MACHINE QUALIFICATION OPTIMIZATION MODEL (MQO)

A mixed integer linear programming model (called MQO) is proposed to optimize product-machine qualification in a
semiconductor back-end facility. The facility has multiple stages with parallel machines at each stage. Products are
processed in lots with a fixed numbers of units in each lot. Setup times are sequence-dependent and not included
in the lot processing time. However, setup times are not modeled explicitly in MQO. Instead, the setup times are
considered by decreasing the machine capacity by a certain percentage (defined as 1 − A in the MQO). Product-machine
qualification is considered in the model, and thus only qualified machines can process a given product at a given
step. Initial product-machine qualifications could be provided though an existing qualification matrix. If the demand
is not completely satisfied with the initial product-machine qualifications, additional machines will be qualified while
balancing backorder costs and qualification costs. The planning horizon is divided into small time buckets to model
the material handling/movement in the system. Meanwhile, the production quantity of each product on each machine
will be scheduled for each time bucket.
Notation:
P: number of products, with index p
N: number of stages, with index n
M[n]: number of unrelated machines at stage n
T : number of time periods, with index t
C: capacity of a machine in each time period (Cn,m if it is machine dependent)
A: available percentage of the machine capacity for production in each time period after setup reservation
B p,0 : initial back order quantity of product family p
I p,n,0 : initial inventory of product p at (after) stage n
b p : back order cost of product p
d p,t : demand quantity of product p at the end of time period t
t p,n,m : unit processing time of product p on machine m at stage n
q[p, n, m] : 1 if machine m at stage n is qualified for product p, 0 otherwise
c p,n,m : cost of qualifying machine m at stage n for product p
Decision Variables:
Xp,n,m,t : continuous variable, production quantity for product p in time period t on machine m at stage n
Ip,n,t : continuous variable, inventory quantity of product p at the end of time period t after stage n
B p,t : continuous variable, back order quantity of the product p at the end of time period t

2487

Fu, Haghnevis, Askin, Fowler, Zhang
Q p,n,m : binary variable, 1 if machine m at stage n is recommended to be qualified for product p, 0 otherwise
(MQO)
b p B p,t +

min
p,t

s.t.

c p,n,m Q p,n,m

(1)

p,n,mq[p,n,m]=0

I p,n,t−1 +

Xp,n+1,m,t = I p,n,t , for all p, n < N,t

(2)

Xp,N,m,t − d p,t = I p,N,t − B p,t , for all p,t

(3)

Xp,n,m,t −
m

m

I p,N,t−1 − B p,t−1 +
m

Xp,n+1,m,t ≤ I p,n,t−1 , for all p, n < N,t

(4)

t p,n,m Xp,n,m,t ≤ C × A, for all n, 1 ≤ m ≤ M[n],t

(5)

C
× Q p,n,m , for all p, n, mq[p,n,m]=0 ,t
t p,n,m

(6)

m
p

Xp,n,m,t ≤

Xp,n,m,t , I p,n,t , B p,t ∈ R+ , for all p, n, m,t
Q p,n,m,t ∈ B, for all p, n, m,t

(7)
(8)

The objective (1) is to minimize the total backorder and machine qualification costs. Constraints (2) are the inventory
balance constraints for every product at every stage, except the last stage, in each time period. They indicate that
the inventory quantity at the end of current time period must equal to the previous inventory plus production minus
consumption at the next stage. Constraints (3) are the inventory balance constraints for every product at the last stage in
each time period. They are similar to constraints (2) except that the consumption at the next stage is replaced by demand.
Backorders are allowed but incur cumulative backorder costs as shown in the objective expression (1). Constraints (4)
are the material availability constraints, which state that the production quantity at stage n in period t must be less
than the inventory quantity at stage n − 1 in period t − 1. Constraints (5) are the capacity constraints for every machine
in each time period, which state that the total production time must be less than the available machine capacity after
setup time reservation. Constraints (6) are the machine qualification constraints, which allow the production quantities
to be positive if the machine is qualified for the product. Constraints (7) and (8) are the positive and binary constraints
for the decision variables.
The model could be easily extended to include different process routes for different products and material handling
time between stages by slightly modifying the subscripts. For example, instead of X p,n+1,m,t , Xp,n+2,m,t should be used
in constraints (2) and (4) if product p skips stage n+1. The material handling time for product p between stage n and
stage n+1 needs to be added on to the subscript t of all Xp,n+1,m,t ’s in constraints (2) and (4).
4

MACHINE QUALIFICATION MANAGEMENT SYSTEM

A machine qualification management system with a graphical user interface was developed with Excel macro based
on the above machine qualification optimization model. The architecture of the system is shown in Figure 1. First,
data from the factory are transformed using the input generator into formatted text files. There are three categories
of data. The first category consists of static factory data, which includes physical production system information (e.g.
number of stages, number of parallel machines at each stage), process information (e.g. lot processing time of each
product on each machine), product information (e.g. number of products, priority for each product), and a matrix of
existing machine qualifications. The static data do not change frequently. The second category consists of the dynamic
demand information for the planning period. Those data could change from week to week and can be changed by the
user to do a what-if analysis. The third category consists of user preferences. First users will be asked to provide the
preference between each pair of machines, which is determined by the distance between the machines. For example,
machines close to each other are preferred to be qualified for the same product so that the material handling time is
shorter. Then users need to input the preference between each pair of machine and product, which may be determined
by the lot processing time for a product on a machine. Usually faster machines are more preferred.
The formatted text files are then read in by the qualification feasibility checker. The qualification feasibility
checker utilizes the MQO model but sets all additional machine qualification variables Q p,n,m ’s to 0. The output of the
qualification feasibility checker consists of machine utilizations and backorder quantities in each time period. If there
are backorders, users will have two options (manual or automatic) for qualifying additional machines to decrease the
backorder quantities. In the manual qualification optimization mode, the utilization of each machine in each time period
will be presented to the user, who could then improve the machine qualification based on expert knowledge. As the
automatic qualification optimization mode is chosen, the MQO model will be used to identify additional machines to
be qualified while balancing the machine qualification and backorder costs. Machine utilization and backorder reports

2488

Fu, Haghnevis, Askin, Fowler, Zhang
are generated from the MQO model.

Factory

Input
Generator

Data

Formatted txt
Files

Qualification
Feasibility Checker
Machine
Utilization,
Missed
Demand

Qualification
Management System with
Graphic User Interface

Reports

New
Qualification

Qualification
Optimization
(Manual/Automatic)

Figure 1: Architecture of the machine qualification management system.
All the above functions are integrated in a graphical user interface (GUI) with a report generator. The GUI has
three main screens: run, report, and analyze. In the run screen, users set parameters for the MQO model, such as
the available percentage of the machine capacity A and a percentage parameter used to change the demand quantity
for what-if analysis. Furthermore, users can input and modify machine-machine preferences and product-machine
preferences. The report generator in the GUI generates a weekly report, which is the report screen, showing the capacity
of each machine used on each product and the backorder quantity of each product for each week in the planning
horizon. Figure 2 shows a sample report for 2 weeks in the report screen. Take the week1 report for example, the
available machine capacity for production (Machine Availability A) and machine capacity spent on production (Effective
Utilization U) are listed for each machine, with the percentage of machine capacity spent on each product on the
right side of the table. On the bottom of the table are the demand (QTG, which a factory term short for quantity to
go), production quantity (Completed), and end-of-week shortage (Accumulative Miss) for each product. In the analyze
screen, users can choose whether to manually qualify machines for products with backorders or run the automatic qualification optimization algorithm to obtain qualification recommendations that balance the qualification and backorder costs.
4.1 AN EXAMPLE
An example is presented here, with 2 products and 3 stages. Product 1 has a higher priority of 5 and product 2 has a
lower priority of 1. There are 8, 10, and 9 machines at the first, second, and third stages, respectively. Parallel machines
have different processing times for the same product, as shown in Table 1. Px represents product x, and Sy z means the
zth machine at stage y. If the lot processing time is 0, e.g. for product 1 on machine 1 at stage 3, the machine can not
be qualified for the product. The initial product machine qualification matrix is shown in Table 2, with 1 meaning the
machine is qualified for a given product and 0 otherwise. The planning period is limited to 3 weeks, with the weekly
demands for product 1 as 0 lot, 38 lots, and 0 lot, and for product 2 as 38 lots, 87 lots, and 88 lots.
Table 1: Lot processing time (hr) of each product on each machine.

P1
P2
P1
P2
P1
P2

S1 1
2.05
1.73
S2 1
2.07
1.55
S3 1
0.00
0.00

S1 2
2.05
1.75
S2 2
2.07
1.55
S3 2
2.05
2.37

S1 3
2.03
1.49
S2 3
2.07
1.55
S3 3
2.05
2.37

S1 4
2.03
1.49
S2 4
2.07
1.55
S3 4
2.05
2.37

S1 5
2.05
1.49
S2 5
2.07
1.55
S3 5
2.05
2.37

2489

S1 6
2.05
1.49
S2 6
2.07
1.55
S3 6
2.05
2.37

S1 7
2.03
1.49
S2 7
2.07
1.55
S3 7
2.05
2.37

S1 8
2.05
1.49
S2 8
2.07
1.55
S3 8
2.07
2.37

S2 9
2.07
1.55
S3 9
2.05
2.37

S2 10
2.07
1.55

Fu, Haghnevis, Askin, Fowler, Zhang

Weekly Machine Utilization Summary

Machine Effective Machine
availability utilization utilization
A(%)
U(%)
U/A(%)
prod1
prod2
prod3
test1
80
80
100
3.43
76.57
test2
80
80
100
0
0
0
test3
80
80
100
0.95
0
test4
80
80
100
0
0
0
test5
80
80
100
0
12.08
QTG WW1
25
40
75
Completed WW1
2.24
40
75
Accumulative Miss
22.76
0
0
Week1

Machine Effective Machine
availability utilization utilization
A(%)
U(%)
U/A(%)
prod1
prod2
prod3
test1
80
80
100
16.19
63.81
test2
80
68.2
85.25
41.76
0
0
test3
80
69.52
86.9
44.51
0
test4
80
71.43
89.29
38.1
0
0
test5
80
80
100
26.94
42.58
QTG WW2
63
48
20
Completed WW2
85.76
48
20
Accumulative Miss
0
0
0

prod4
0
0
0

0
0
0
0
0
68
68
0

Week2

prod4
0
0
0

0
0
0
0
0
25
25
0

Figure 2: Report of the machine qualification management system.
The results of qualification feasibility checker and qualification optimization with two different A values are shown
in Table 3. It should be noted that a negative number for shortage means inventory at the end of the week. Comparing the
shortages and total costs between current qualification and optimized qualification, we can conclude that the qualification
optimization eliminated the shortage with minimal machine qualification cost for the example. In addition, the value
of A affects the feasibility checker and qualification optimization results. Therefore, choosing a proper A value is very
important for the machine qualification management system. In the next Section 4.2, a potential solution for obtaining
a proper A value is presented.
4.2 INTEGRATION WITH A SCHEDULING SYSTEM
The production scheduling considered in the MQO model is simplified compared to the real shop floor scheduling,
as it does not consider the sequence-dependent setups or the non-preemptive machine scheduling rules. The available
percentage of the machine capacity for production A is generally inaccurate. If A is set to be larger than the actual
value, it might cause shortages in the real production. On the other hand, if A is set to be smaller than the actual
value, it could cause unnecessary machine qualification cost. As a result, the machine qualification management system
Table 2: Initial product-machine qualification.

P1
P2
P1
P2
P1
P2

S1 1
1
1
S2 1
0
1
S3 1
0
0

S1 2
1
1
S2 2
0
1
S3 2
0
0

S1 3
0
0
S2 3
0
0
S3 3
0
0

S1 4
1
0
S2 4
1
0
S3 4
1
0

S1 5
1
0
S2 5
0
0
S3 5
0
0

2490

S1 6
1
0
S2 6
1
0
S3 6
1
0

S1 7
0
0
S2 7
0
0
S3 7
0
0

S1 8
1
0
S2 8
1
0
S3 8
1
1

S2 9
1
0
S3 9
0
1

S2 10
0
0

Fu, Haghnevis, Askin, Fowler, Zhang
Table 3: Computational result.

Week1 Shortage
Week2 Shortage
Week3 Shortage
Backorder Cost
Qualification Cost
Total Costs

A = 80%
Current
Optimized
Qualification
Qualification
P1
P2
P1
P2
-16.41 -18.81 -27.87 -31.55
0
11.38
0
-17.1
0
42.57
0
0
78.1
0
0
7.8
78.1
7.8

A = 90%
Current
Optimized
Qualification
Qualification
P1
P2
P1
P2
-8.79 -25.91 -28.03 -30.04
0
-2.82
0
-16.08
0
21.27
0
0
17.3
0
0
7.8
17.3
7.8

should be integrated with a scheduling system, in which the suggested machine qualification could be verified and the
parameter A could be adjusted.
5

CONCLUSION

In this paper, a mixed integer linear programming model (MILP) is proposed to optimize product-machine qualifications
for a semiconductor back-end facility. Production scheduling in a medium term planning horizon is considered and
setup times are modeled indirectly. Based on the MILP model, a Excel-based machine qualification management system
with a graphical user interface (GUI) is developed to be used in the factory.
In future research, the MILP model and the management system with GUI will be tested with real data and in
a real factory. An interesting topic will be adding stochastic factors in the optimization of the machine qualification,
such as the demand uncertainty and random machine breakdowns.
REFERENCES
Aubry, A., A. Rossi, M. Espinouse, and M. Jacomino. 2008. Minimizing setup costs for parallel multi-purpose machines
under load-balancing constraint. European Journal of Operational Research 187 (3): 1115–1125.
Bourland, K., and L. Carl. 1994. Parallel-machine scheduling with fractional operator requirements. IIE Transactions 26
(5): 56–65.
Brucker, P., B. Jurisch, and A. Krämer. 1997. Complexity of scheduling problems with multi-purpose machines. Annals
of Operations Research 70:57–73.
Campbell, G. 1992. Using short-term dedication for scheduling multiple products on parallel machines. Production and
Operations Management 1 (3): 295–307.
Drexl, A., and M. Mundschenk. 2008. Long-term staffing based on qualification profiles. Mathematical Methods of
Operations Research 68 (1): 21–47.
Hurink, J., B. Jurisch, and M. Thole. 1994. Tabu search for the job-shop scheduling problem with multi-purpose
machines. OR Spectrum 15 (4): 205–215.
Ignizio, J. 2009. Cycle time reduction via machine-to-operation qualification. International Journal of Production
Research 47 (24): 6899–6906.
Johnzén, C., P. Vialletelle, S. Dauzère-Pérès, C. Yugma, and A. Derreumaux. 2008. Impact of qualification management
on scheduling in semiconductor manufacturing. In Proceedings of the 40th Conference on Winter Simulation, ed.
S. Mason, R. Hill, L. Monch, T. Jefferson, and J. Fowler, 2059–2066: Piscataway, New Jersey: Institute of Electrical
and Electronics Engineers.
Jurisch, B. 1995. Lower bounds for the job-shop scheduling problem on multi-purpose machines* 1. Discrete Applied
Mathematics 58 (2): 145–156.
Mati, Y., and X. Xie. 2004. The complexity of two-job shop problems with multi-purpose unrelated machines. European
Journal of Operational Research 152 (1): 159–169.
Rossi, A. 2010. A robustness measure of the configuration of multi-purpose machines. International journal of production
research 48 (3-4): 1013–1033.
Wu, M., Y. Huang, Y. Chang, and K. Yang. 2006. Dispatching in semiconductor fabs with machine-dedication features.
The International Journal of Advanced Manufacturing Technology 28 (9): 978–984.
Wu, M., H. Jiang Jr, and W. Chang. 2008. Scheduling a hybrid MTO/MTS semiconductor fab with machine-dedication
features. International Journal of Production Economics 112 (1): 416–426.

2491

Fu, Haghnevis, Askin, Fowler, Zhang
AUTHOR BIOGRAPHIES
MENGYING FU is currently pursuing the doctoral degree in the Industrial Engineering program of the School of
Computing, Informatics, and Decision Systems Engineering at Arizona State University, Arizona State, US. Her academic
interests include production scheduling and integer optimization. She received the B.S. in Industrial Engineering from
Tsinghua University, Beijing, China. Her email address is <Mengying.Fu@asu.edu>.
MOEED HAGHNEVIS is a PhD student of Industrial Engineering in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University. He received his M.Sc. and B.S. in Industrial and System
Engineering from University of Tehran and Amirkabir University of Technology, Iran, respectively. His interests include
engineering complex systems, computer simulation, and project management. He has published a book in project
management and several journal and conference papers. Before starting his PhD, he served as an Adjunct Instructor at
Iran University of Science and Technology and Amirkabir University of Technology.
RONALD ASKIN is a Professor of Industrial Engineering and Director of the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University. He received his PhD from Georgia Institute of Technology
and has 30 years of experience in the development, teaching and application of methods for production systems analysis.
He is a Fellow of IIE and has published extensively. His list of awards includes a National Science Foundation
Presidential Young Investigator Award, the Shingo Prize for Excellence in Manufacturing Research, IIE Joint Publishers
Book of the Year Award, and the IIE Transactions Development and Applications Award.
JOHN W. FOWLER is the Avnet Professor of Supply Networks and Program Chair of Industrial Engineering in the
School of Computing, Informatics, and Decision Systems Engineering at Arizona State University (ASU). His research
interests include modeling, analysis, and control of manufacturing and service systems. His research has been supported
by the National Science Foundation, Semiconductor Research Corp., International SEMATECH, Asyst, IBM, Intel,
Motorola, Infineon Technologies, ST Microelectronics, and the Mayo Clinic. Dr. Fowler is an author on over 75 journal
publications, 100 conference papers, and 10 book chapters. He is the founding editor of the new journal IIE Transactions
on Healthcare Systems Engineering. He is also an Area Editor for SIMULATION: Transactions of the Society for
Modeling and Simulation International, an Associate Editor for IEEE Transactions on Semiconductor Manufacturing,
and on the Editorial Board for the Journal of Simulation. He is a Fellow of the Institute of Industrial Engineers, was
recently the INFORMS Vice President for Chapters/Fora, and is on the Winter Simulation Conference Board of Directors.
MUHONG ZHANG is an Assistant Professor of Industrial Engineering in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University. Dr. Zhang received her B.S. in Applied Mathematics from
Beijing University of Chemical Technology, China, in 1999, an M.A. in Operations Research from Chinese Academy
of Sciences, in 2002 and a Ph.D. in Industrial Engineering and Operations Research from University of California, at
Berkeley, in 2006. Her research interests include integer programming, robust optimization, computational optimization
and network problems. Dr. Zhang is a member of the Institute for Operations Research and the Management Sciences
(INFORMS).

2492

IEEE TRANSACTIONS ON RELIABILITY, VOL. 53, NO. 1, MARCH 2004

77

A Generalized SSI Reliability Model Considering
Stochastic Loading and Strength Aging Degradation
Wei Huang and Ronald G. Askin

defined variable used for integration

Abstract—Summary & Conclusions—A generalized
stress-strength interference (SSI) reliability model to consider stochastic loading and strength aging degradation is
presented in this paper. This model conforms to previous models
for special cases, but also demonstrates the weakness of those
models when multiple stochastic elements exist. It can be used
for any nonhomogeneous Poisson loading process, and any kind
of strength aging degradation model. To solve the SSI reliability
equation, a numerical recurrence formula is presented based on
the Gauss-Legendre quadrature formula to calculate multiple
integrations of a random variable vector. Numerical analysis of
three examples shows this SSI reliability model provides accurate
results for both homogeneous & nonhomogeneous Poisson loading
processes.

mean value
standard deviation
lower bound of vector in integral
upper bound of vector in integral
lower bound of variable
in integral
upper bound of variable
in integral
number of Gaussian integration points
Gaussian integration point
weight factor corresponding to the Gaussian integration point

Index Terms—Gauss-Legendre quadrature formula, numerical
multiple integration, SSI reliability model, stochastic loading, stochastic strength aging degradation.

A
SSI
PCB
I/O

standard normal distribution function of random
variable

CRONYMS1

stress-strength interference
printed circuit board
input/output

I. INTRODUCTION

P

NOTATION
time or mission duration
vector containing multiple random variables
, and
multivariate joint pdf of
, and
multivariate joint Cdf of
, and
strength aging degradation model, as a function of
and
Cdf of strength
nonhomogeneous Poisson loading process
intensity function of the loading process
loading amplitude, a random variable
Cdf of
failure probability given stochastic loading appears
at time
reliability in a mission duration
time increment
Manuscript received August 1, 2001; revised January 1, 2003. Responsible
Editor: J. Lambert.
Wei Huang is with Seagate Technology, Longmont, CO 80503 USA (e-mail:
whuang62@yahoo.com).
Ronald G. Askin is with the Department of Systems & Industrial Engineering, The University of Arizona, Tucson, AZ 85721 USA (e-mail:
ron@sie.arizona.edu).
Digital Object Identifier 10.1109/TR.2004.823847
1The

singular and plural of an acronym are always spelled the same.

RODUCT failure occurs when external loading is higher
than product strength (or capacity). For a random loading
or strength, the probability that the strength is always greater
than the loading in a mission duration provides product reliability for that time period. This results in the so-called SSI
theory.
Traditionally, the SSI theory deals with either static loading
or static strength. In reality, however, product strength could
degrade due to aging. For example, the strength of a PCB FR-4
material deteriorates after a time period of high temperature
thermal exposure. Hence, considering strength to be a stochastic aging process might be necessary in some electronic
reliability analyses, in practice. Furthermore, instead of acting
on a product consistently, external loading may appear as
a stochastic process. Examples include randomly occurring
voltage/current overloading, stochastic high temperature
thermal impact on electronic devices installed in uncontrolled
environment causing device electrical performance degradation
or I/O fatigue failure, random vibration of portable computers
in transportation, etc.
Fig. 1 graphically shows the interaction of stochastic loading
and stochastic strength aging degradation. In Fig. 1, at , the
mean value of strength is much greater than loading amplitude,
and thus the reliability is high. At , however, the mean value of
the strength degrades to a lower level, and the loading amplitude
becomes comparable to the mean value of the strength. Hence
at , the product reliability is reduced.
Some efforts have been made to improve the traditional SSI
theory with considering stochastic loading processes. Melchers

0018-9529/04$20.00 © 2004 IEEE

78

Fig. 1.

IEEE TRANSACTIONS ON RELIABILITY, VOL. 53, NO. 1, MARCH 2004

Schematic showing stochastic loading and strength aging degradation.

[8], and Wen, & Chen [11] developed the SSI model to analyze the reliability of mechanical structures under time varying
loads. Hooke [4], and Boehm & Lewis [1] applied the SSI theory
to investigate fatigue & risk analysis under stochastic loading
processes. Zibdeh & Heller [13] presented a reliability analysis
of rocket motors under stochastic loading processes. However
none of these publications involves strength aging degradation.
Lewis & Chen [6] presented a SSI reliability model involving
strength aging degradation, but their model is limited to two
simple cases:
1) with deterministic loading and stochastic strength aging
degradation, or
2) with stochastic loading and deterministic strength aging
degradation.
Xue & Yang [12] presented an improved SSI reliability model
involving both stochastic loading and strength aging degradation. But their model assumes a homogeneous Poisson loading
process with s-normally distributed load amplitudes.
Because no comprehensive studies involving both stochastic
loading and stochastic strength aging degradation have been
found in existing publications, we present a generalized SSI reliability model in this paper.

4) The random variable vector has a known multivariate
joint distribution as
. Due to independence of the random variables,
is a multiplication of the individual Cdf of each random variable
in . If there are
independent random variables in
, denoting
, then
.
5) Occurrence of the stochastic loading
follows
a nonhomogeneous Poisson process with the intensity
function of
.
6) When the loading appears at time , its amplitude follows a known distribution as
.
Assumptions 4, 5, & 6 extend the existing SSI reliability
model proposed by Xue & Yang [12] to a generalized one. In
fact, Assumption 5 covers many practical loading processes. For
example, impact force on a vehicle due to road bumping can be
assumed to follow an Erlang process with a time varying intensity function, which has the potential to cause damages on
electronic packaging items, such as I/O pins or interfaces.
III. THE GENERALIZED SSI RELIABILITY MODEL
A. For Deterministic Strength Aging Degradation
At first, let’s consider a deterministic strength aging degradais not random.
tion process, in which
Denote
as the failure probability function under sto, given the loading force appears at time
chastic loading

(1)
The product reliability function under the stochastic loading
in a mission duration is defined by

II. MODELING ASSUMPTIONS
The SSI reliability model to be established here is based on
the following assumptions:
1) Stochastic loading and strength aging degradation are statistically independent.
2) Both the stochastic loading and strength aging degradation are time-increment-independent, such that a one-dimensional probability distribution is sufficient to describe
each stochastic process.
3) The strength degradation follows a decreasing continuous-state stochastic process in time. In general, the
strength aging degradation model can be expressed
as a function of time
and some random variables
independent on each other. In practice, multiple random
variable coefficients may be involved in the strength
aging degradation model. A typical example is the PCB
FR-4 material strength degradation model due to tiny
crack propagation, which contains two random coefficients in the strength model, as shown by Lu and Meeker
[7]. For convenience, these random variables are written
as a random variable vector . Hence, the strength aging
degradation model can be expressed as
.

(2)
For a small enough time increment
is
at time

, the reliability function

(3)
The second part of the right hand side of (3) can be expressed
as

(4)

HUANG AND ASKIN: A GENERALIZED SSI RELIABILITY MODEL

79

Noting that
, the assumption that the loading
follows the nonhomogeneous Poisson process results in
(5)

For a static strength, the strength aging degradation model
. A homogeneous
is time independent; i.e.,
. Thus, (9) becomes
Poisson loading process implies
(10)

Substituting (5) into (4) yields

(6)
Substituting (6) into (3) yields

i.e.,

Let
. Noting that
above equation yields

and

When the random variable vector contains only one variable,
(10) gives the same result as that presented by Lewis & Chen
[6].
Furthermore, assume that the loading amplitude is deterministic with the value of . Then, from the stand point of event
probability, the reliability in a mission-duration is equal to the
probability that the strength is greater than
or the strength
but the inter-occurrence time of the loading is
is less than
greater than ; i.e.,

, the

(7)
Noting that

, solving (7) yields
(11)
Now apply (10) to derive the reliability function.
For the deterministic loading amplitude , the Cdf of
comes
(8)

Clearly, the derived SSI reliability model for deterministic
strength aging degradation belongs to the generalized exponential distribution family.

be-

,
.
Substituting

into (10) yields

B. For Stochastic Strength Aging Degradation
For a stochastic strength aging degradation, the vector
in the strength aging degradation model
represents a
random variable vector. Using the Bayes law for continuous
variables, the SSI reliability model under both stochastic
loading & stochastic strength aging degradation is established
from

It is the same as (11). Hence, the simple application verifies the generalized SSI reliability model valid for deterministic
loading and static strength.
V. NUMERICAL ALGORITHM FOR CALCULATING
Calculating the reliability function
multiple integrations.

(9)
From (9), it can be seen that the established SSI reliability
model
under both stochastic loading & stochastic strength
aging degradation does not belong to the generalized exponential distribution family, due to the existence of the pdf of the
random variable vector .

from (9) involves
(12)

(13)
At time

, (13) becomes

IV. APPLICATION OF THE GENERALIZED SSI RELIABILITY
MODEL TO A SPECIAL CASE
Application of the generalized SSI reliability model
for
a static strength and a homogeneous Poisson loading process is
shown in this section.

(14)

80

IEEE TRANSACTIONS ON RELIABILITY, VOL. 53, NO. 1, MARCH 2004

When the time increment
is small enough, the second term
of the right hand side of (14) can be approximately expressed,
using the trapezoidal rule, as

(15)

TABLE I
THE GAUSSIAN INTEGRATION POINTS AND WEIGHT FACTORS FOR

K=9

For a single integral of the right hand side of (20), applying
the Gauss-Legendre quadrature formula [3] yields

Substituting (15) into (14) yields
(16)
Furthermore, substituting (16) into (9) yields

(21)

(17)
Applying a Taylor series expansion leads to

Table I lists the Gaussian integration points and weight factors
for
.
Combining (19), (20) & (21), the numerical recurrence forat time
is
mula for calculating
given by

(18)
Substituting (18) into (17) yields

(22)
where
(23)
(19)
Calculating
now requires the multiple integrations
of the right hand side of (19). For any given pdf, the integration
area far beyond the mean value contributes little. Hence, it is
reasonable to calculate the integration by truncating the infinite
integral limits, and using finite upper & lower bounds. For each
random variable of , the finite bounds can be determined based
on its distribution. For example, for a s-normally distributed
random variable, the “6- ” rule is used here. By doing so, the
. The error due to trunintegral bounds are set as the
.
cating the infinite limits is less than
Assume that the finite integral bound of is
. Then,
the integral of the right hand side of (19) can be expressed approximately as

(24)
VI. NUMERICAL EXAMPLE AND DISCUSSION OF RESULTS
Three examples are presented in this section.
A. Homogeneous Poisson Loading Process and Linear
Strength Degradation Model With a Single Random Coefficient
Assume that the stochastic strength aging degradation model
is given by
(25)
where

(20)

follows a s-normal distribution with the mean value of
MPa, and the standard deviation of
MPa.
Equation (25) implies that the strength follows a s-normal
MPa, and
distribution with
MPa.
The loading process rate is
hr . When the loading
occurs, its amplitude follows a s-normal distribution with the

HUANG AND ASKIN: A GENERALIZED SSI RELIABILITY MODEL

81

Fig. 2. Reliability function with a single random coefficient.

Fig. 3. Reliability function and bounds with double random coefficients.

mean value of
MPa, and the standard deviation of
MPa.
is
From (9), the reliability function

The loading process rate is
hr , and the loading
amplitude follows the s-normal distribution with
MPa, and
MPa.
is
From (9), the reliability function

(26)
where

(31)
(27)

where
(32)

(28)
(33)
(29)
To apply the proposed numerical algorithm to calculate
from (26), the finite integral bound is determined using the
” rule, which yields
MPa, and
“
MPa.
Fig. 2 shows the calculated reliability function. For comparison, the result presented by Lewis and Chen [6] is also shown in
the figure. From Fig. 2, it is clear that over the entire time zone,
the reliability function calculated using the presented numerical
method is identical with that presented by Lewis and Chen [6].

(34)

(35)
The finite integral bounds for the random variables
are determined using the “
” rule, respectively, as

&

B. Homogeneous Poisson Loading Process and Linear
Strength Degradation Model With Double Random Coefficients
Xue and Yang [12] analyzed this example and presented a
lower and an upper bound of the reliability function.
Assume that the stochastic strength aging degradation model
is
(30)
where
and
are independent to each other, and both follow
the s-normal distribution with the mean value of
MPa, and
MPa hr , respectively; and the standard
deviation of
MPa, and
MPa hr ,
respectively.
Equation (30) implies that the strength follows a s-normal
MPa,
distribution with the mean value of
and the standard deviation of
MPa.

and
Fig. 3 shows the calculated reliability function. For comparison, the lower and upper bounds obtained by Xue and Yang
[12] are also shown in this figure. From Fig. 3, it can be seen
that the upper bound becomes invalid beyond a certain point in
time.
C. Non-Homogeneous Poisson Loading Process and Linear
Strength Degradation Model With Double Random Coefficients
Assume that the stochastic strength aging degradation model
is the same as that used in Section VI.B; i.e.,
(36)

82

IEEE TRANSACTIONS ON RELIABILITY, VOL. 53, NO. 1, MARCH 2004

[8] R. E. Melchers, “Load-space formulation for time-dependent structural
reliability,” J. Engng. Mechanics, vol. 108, pp. 853–870, May 1992.
[9] S. S. Rao, Reliability Based Design: McGraw-Hill, 1992.
[10] M. Shooman, Probabilistic Reliability: An Engineering Approach, 2nd
ed: Robert E. Krieger, 1991.
[11] Y. K. Wen and H. C. Chen, “Systems reliability under time varying loads:
I,” J. Engng. Mechanics, vol. 115, pp. 808–823, Apr. 1989.
[12] J. Xue and K. Yang, “Upper & lower bounds of stress-strength interference reliability with random strength-degradation,” IEEE Trans. Rel.,
vol. 46, no. 1, pp. 142–145, Mar. 1997.
[13] H. S. Zibdeh and R. A. Heller, “Rocket motor service life calculations
based on first passage methods,” J. Spacecraft & Rockets, vol. 26, pp.
279–284, June 1989.

Fig. 4.

Reliability function and Erlang rate.

The stochastic loading follows a nonhomogeneous Poisson
process with an Erlang rate of
(37)
Fig. 4 shows the Erlang rate as a function of time .
Assume that the loading amplitude follows a s-normal distriMPa, and
MPa. Combution with
paring this example with that in Section VI.B, (31) & (32) are
still valid, but (33) becomes
(38)
Fig. 4 shows the calculated reliability function. From Fig. 4,
hr, the Erlang process rate is
it can be seen that after
hr, the
almost constant at 1.0 hr . This means that after
reliability function in this case should be comparable to that calculated in Section VI.B. Fig. 4 proves this. Thus the generalized
SSI reliability model and the proposed numerical algorithm are
applicable to nonhomogeneous Poisson loading processes.
REFERENCES
[1] F. Boehm and E. E. Lewis, “A stress-strength interference approach to
reliability analysis of ceramics: Part II—Delayed fracture,” in Probabilistic Engng. Mechanics, vol. 7, Jan. 1992, pp. 9–14.
[2] A. D. S. Carter, Mechanical Reliability, 2nd ed: John Wiley & Sons,
1986.
[3] P. J. Davis and P. Rabinowitz, Numerical Integration, Blaisdell, 1967.
[4] F. R. Hooke, “Aircraft structural reliability and risk analysis,” in Probabilistic Fracture Mechanics & Reliability, J. W. Provan, Ed: Martinus
Nijhoff Publishers, 1987.
[5] E. E. Lewis, Introduction to Reliability Engineering: John Wiley & Sons,
1987.
[6] E. E. Lewis and H. C. Chen, “Load-capacity interference and the bathtub
curve,” IEEE Trans. Rel., vol. 43, no. 3, pp. 470–475, Sept. 1994.
[7] C. J. Lu and W. Q. Meeker, “Using degradation measures to estimate a
time-to-failure distribution,” Technometrics, vol. 35, pp. 161–174, May
1993.

Wei Huang received the M.S. degree in Reliability and Quality Engineering
and the Ph.D. degree in Systems and Industrial Engineering from the University of Arizona, USA in 1998 and 2002, respectively; and the B.S., M.S., and
Ph.D. degrees in Mechanical Engineering from Nanjing University of Science
and Technology, P.R. China in 1982, 1985, and 1988, respectively.
He has been a Reliability/FA Staff Engineer at Seagate Technology,
Longmont, CO, USA since 2003. Before joining Seagate Technology, he was
a Member of Technical Staff at Onix Microsystems, Richmond, CA, USA in
2000–2002 and a Senior Electronic Quality and Reliability Engineer at GE
Corporate Research and Development, Schenectady, NY, USA in 1999–2000.
He was an Associate Professor at the Department of Mechanical Engineering,
Nanjing University of Science and Technology, Nanjing, P.R. China in
1988–1996. He has authored or co-authored over 50 professional publications,
primarily on statistical reliability modeling and analysis, electronic packaging
design, impact dynamics, structural analysis, etc. His current research interests
center on MEMS design and modeling, electronic packaging design and
reliability, statistical reliability analysis, etc.

Ronald G. Askin received the B.S. degree Industrial Engineering from Lehigh
University, the M.S. degree in Operations Research, and the Ph.D. degree in Industrial & Systems Engineering from Georgia Institute of Technology, respectively.
He is currently a Professor and Department Head of Systems and Industrial
Engineering at The University of Arizona. He is a Fellow of IIE, and an active member of INFORMS and SME. He has previously served as President
of the INFORMS Manufacturing and Service Operations Management Society
(MSOM), Chairman of the ORSA Technical Section on Manufacturing Management, and the Statistics Division of ASQC. He has been the editor of IIE Transactions on Design and Manufacturing and also served on the editorial boards
of the Journal of Manufacturing Systems and IIE Solutions. He has authored or
co-authored over 80 professional publications, primarily on the application of
operations research and statistical methods to the design and analysis of production systems. He co-authored the texts Modeling and Analysis of Manufacturing
Systems (1993), which was awarded the 1994 IIE Joint Publishers Book of the
Year Award, and Design and Analysis of Lean Production Systems (2001). Other
awards he has received include the IIE Transactions on Design and Manufacturing Best Paper Award (twice as coauthor), the Shingo Award for Excellence
in Manufacturing Research, IIE Transactions Development and Applications
Award (coauthor), the ASEE/IIE Eugene L. Grant Award (coauthor), and a National Science Foundation Presidential Young Investigator Award. He has consulted with a variety of industrial companies in the areas of scheduling, facilities
planning, quality improvement, and performance evaluation of manufacturing
systems.

European Journal of Operational Research 168 (2006) 853–869
www.elsevier.com/locate/ejor

Dynamic task assignment for throughput maximization
with worksharing
Ronald G. Askin *, Jiaqiong Chen
Department of Systems and Industrial Engineering, The University of Arizona, Tucson, AZ 85721, USA
Available online 11 September 2004

Abstract
The traditional assembly line balancing problem involves assigning a set of partially precedence constrained tasks to
workstations to maximize eﬃciency. Each task is assigned to a unique workstation. The case is considered where task
sequences are known but the workforce is partially cross-trained and some tasks can alternate between workstations.
The ﬂexibility aﬀorded by cross-training allows the line balance to improve. Task times are allowed to be random and
small buﬀers are allowed between workstations. Decision rules are developed and tested for various levels of cross-training between adjacent workers. Cross-training is shown to have signiﬁcant impact on throughput and easy to administer
rules are proven to be eﬀective. The number of decision points for deciding to hold or pass a unit of product is also
shown to be important.
Ó 2004 Elsevier B.V. All rights reserved.
Keywords: Production; Dynamic line balancing; Worksharing; Cross-training; Throughput maximization

1. Introduction
Many production systems involve labor constrained serial production lines. Product enters
from the ﬁrst workstation, passes to the second,
the third and so on until the end of the line. Historically, assembly line balancing tries to assign
tasks to workstations to balance workload and

*

Corresponding author.
E-mail addresses: ron@sie.arizona.edu
jqchen@sie.arizona.edu (J. Chen).

(R.G.

Askin),

minimize the number of workstations required.
This approach assumes a completely static and
deterministic operation. Furthermore, discrete
task times typically make planned idle time
inevitable.
The trend towards lean manufacturing has led
to numerous operational changes and performance improvements. Cross-training is one organizational change that can be used to enhance
performance. In addition to reducing idle time
and reducing buﬀer sizes, cross-training workers
has other beneﬁts, such as job enrichment, ﬂexibility, and reduced risk of repetitive worker injuries.

0377-2217/$ - see front matter Ó 2004 Elsevier B.V. All rights reserved.
doi:10.1016/j.ejor.2004.07.033

854

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

Two types of implementation of worksharing
can be found in the literature: Dynamic assembly-Line Balancing (DLB) and Moving Worker
Modules (MWM). MWM applications usually
have more machines than workers. Each operation
has a designated machine. Workers carry work
pieces along the line within zones and share use
of machines. Typical MWM systems include the
Bucket-Brigade System (BBS) and Toyota Sewnproduct management System (TSS). Bartholdi
and Eisenstein [2] prove that by sorting workers
from slowest to fastest, a BBS can always converge
to an optimal task division and the line balances
itself on-the-ﬂy.
DLB has matched machines and workers.
Workers remain at their machines. Some tasks
(operations) on a job are assigned to a designated
worker, which are called ﬁxed tasks. Other tasks
can be performed by either of an adjacent pair of
workers. These are called shared tasks. A worker
chooses to either pass on a job with the shared task
undone or complete the shared task, according
to speciﬁc rules taking into account system
information.
Askin and Iyer [1] compare scheduling policies
for manufacturing cells. They examine traditional
manufacturing shop scheduling, dedicated cell
loading where the cell is dedicated to one part type
at a time, and champion strategies where each
batch is assigned to one cross-trained worker. In
their results, dedicated cell loading is generally
the best policy. However, they also show that complete cross-training champion policies can be valuable when machine utilization is not very high.
Iyer and Askin [7] present a general framework
for comparing operating policies in manufacturing
cells. They ﬁnd that the interactions between crosstraining levels, the number of workers, processing
time variability and utilization levels are signiﬁcant
and that cross-training can substitute for workin-process (WIP).
Assuming complete cross training, Van Oyen
et al. [15] explores two new types of classiﬁcation
for moving worker modules. The ﬁrst classiﬁcation
is collaborative tasks versus non-collaborative
tasks. In collaborative tasks, multiple workers
can work on one unit simultaneously. The second
classiﬁcation is demand-constrained versus capac-

ity-constrained environment, where cycle time,
i.e. responsiveness, is the benchmark in the former
case and throughput is the performance measure
in the later case.
Two major opinions about the role of buﬀers in
worksharing systems exist. Bartholdi and Eisenstein [2] hold the view that buﬀers are harmful to
their Bucket-Brigade production system, claiming
that buﬀers will lower the eﬃciency. They recommend ‘‘no leave’’ and ‘‘no buﬀers’’ for the BBS.
By ‘‘no leave’’, they mean that a worker, when
blocked by a downstream worker, shall not leave
the work piece and seek upstream. Rather, the person should hold the work piece and wait. To reach
these conclusions, they assume deterministic
processing time and distinguishable workers, i.e.,
workers can be simply indexed by their speed.
Other researchers in this area, such as Bischak
[3], McClain et al. [9], Van Oyen et al. [15], hold
the opposite opinion. They ﬁnd that inserting buﬀers can improve the eﬃciency of worksharing over
a ﬁxed-assignment system. Their results follow
from the assumptions of identical workers and
random processing time. Bartholdi and Eisenstein
[2] give rationale for their assumptions by investigating examples from the apparel industry. They
claim that the variances of processing time are
far smaller than the variance of speed among
workers. Their tests in actual environments show
that worker velocities can be indexed.
Bischak [3] examines U-shaped moving-worker
modules with nine machines and 2–8 workers.
She points out two performance measures to be
considered: throughput rate (expected average
output per unit time) versus throughput time (the
time that a work piece spends in the system). By
comparing such worksharing modules with and
without buﬀers, she ﬁnds that processing time variability determines the potential beneﬁt of placing
buﬀers. She ﬁnds that the coeﬃcient of variance
(CV: standard deviation over mean processing
time) is the determining factor for whether buﬀers
are beneﬁcial. Assuming identical workers, buﬀers
will increase the throughput rate but at the cost of
a much higher throughput time for high CV cases.
For low CV cases, buﬀers show limited gains in
throughput rate yet increase throughput time. This
is conﬁrmed by McClain et al. [9], where they

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

examine distinguishable-workers systems with
processing time variation. They ﬁnd that buﬀers
would cushion against the variation and ‘‘leave’’
policy outperforms the ‘‘no leave’’ policy. Other
ﬁndings in [3] are that buﬀers improve capacity
more in balanced lines than in unbalanced ones,
and buﬀers are less important in moving-worker
modules than ﬁxed-seated worker system.
Conway et al. [4] study the role of buﬀers in traditional non-worksharing serial production systems. They ﬁnd that placing buﬀers between
workstations can remedy the interference loss of
capacity in an asynchronous serial line. Generally,
small buﬀers are suﬃcient, since the improvement
diminishes as the buﬀer size increases. They also
conﬁrm the interesting ‘‘bowl phenomenon’’
[6,11], which says that placing more buﬀers at or
near the center is preferable.
Ostolaza et al. [12] show that by using a shared
task in combination with a half-full buﬀer control
policy, DLB can both reduce the Work-In-Process
(WIP) inventory and improve the eﬃciency at the
same time. In the subsequent paper, McClain
et al. [8], they show that using shared tasks can increase eﬃciency even when buﬀers are absent. In
the former paper, [12], jobs in the buﬀer are classiﬁed as either a long job with shared task uncompleted, or a short job with shared task completed.
Task times are exponentially distributed and jobs
in buﬀer are served not on ﬁrst-come-ﬁrst-served
(FCFS) basis, but by shortest-processing-time
(SPT) policy. In the later paper, [8], a new model
diﬀerent to the long-job/short-job model, the subtask model, is employed. Task times are Erlang-k
distributed in the subtask model and both SPT
and FCFS queuing disciplines are examined. By
comparing FCFS to SPT policy, the performance
of dynamic line balancing is shown more clearly.
Gel et al. [5] examine two-stage DLB lines with a
Constant Work-In-Process (CONWIP) policy.
They adapt the half-full buﬀer rule to diﬀerent
assumptions. Such adaptation is shown to be successful, for the heuristic is a near-optimal threshold
policy. In addition to the half-full buﬀer policy they
adapted, they propose a 50–50 workcontent heuristic, which utilizes the ratio of workcontent available
at upstream workstation to total work load in the
system. They ﬁnd that three factors are important

855

as far as logistical beneﬁts of worksharing is concerned: preemptability of shared task (continuous
preemptability), granularity of shared task (discrete
preemptability), and processing time variability. In
preemptive systems, more cross training promises
better performance, while in non-preemptive cases,
there exists an optimal level of cross training beyond which performance is actually harmed. The
existence of an optimal level of cross-training conﬁrms the result in Ostolaza et al. [12]. Our research
also shows that such an optimal level exists under
the non-preemptive assumption.
Our paper considers the two-stage DLB model
as in Gel et al. [5] under non-preemptive and granular shared task assumptions. We are interested in
systems with relatively low levels of WIP. Our ultimate objective is to deﬁne the tradeoﬀ between the
cost of work-in-process inventory (investment on
WIP) and the cost of cross-training (investment
on worker). Given appropriate cost models, this
problem can be simpliﬁed to examining the eﬃciency improvement gained from cross training
under diﬀerent environment parameters, especially, work-in-process inventory level. A simulation model and the various heuristic rules that
attempt to approximate the optimal threshold values are explained in Section 2. A threshold policy
is important because the optimal buﬀer control
policy is state-dependent and thus can be too complex to implement in real-life scenarios [5]. Simulation results and observations are discussed in
Section 3. The ﬁnal section summarizes results
and depicts future research.

2. Environmental assumptions and threshold
heuristics
A two-stage tandem production line with dynamic line balancing (DLB) mechanism is considered. Each workstation is preceded by an inﬁnite
capacity queue, yet the total number of jobs in
the system is restricted by a Constant-WorkIn-Process (CONWIP) policy [14]. The CONWIP
inventory levels are restricted to small numbers,
3, 4, and 5. In eﬀect, each buﬀer is small. By using
‘‘inﬁnite’’ capacity queue, it is assumed there are
enough buﬀer spaces to accommodate the small

856

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

inventory and thus there is no blocking in the system. The CONWIP policy is chosen to make it
more convenient to evaluate the trade-oﬀ between
WIP and cross-training.
Each job has three tasks to be performed, tasks
A, B and C. Task A can only be performed at
workstation one and task C can only be performed
at workstation two. These are called ﬁxed tasks.
Task B, the shared task, can be completed at either
workstation. Both jobs and tasks need to be completed in FCFS order. Following the convention in
Gel et al. [5], a job with task C uncompleted is
called a type-C job, a job with task B and C left
a type-B job, and a new job a type-A job.
We wish to study the eﬀect of shared task capability on throughput with random task times. To
eliminate the eﬀect of total task time variance in
comparing diﬀerent task divisions, a subtask
model is used. In the subtask model [8], jobs consist of an equal number of subtasks, each of which
takes an identically distributed subtask time to
complete. A task division is a partition of subtasks
into discrete tasks. For example, a 3–4–3 task division aggregates the ﬁrst three subtasks to task A,
the middle four subtasks to task B, and the rest
to task C. The task times are Erlang-k distributed
if subtasks are exponentially distributed.
It can be proved that a type-C prioritized queue
between workstation one and two can always perform better than a ﬁrst-come-ﬁrst-served (FCFS)
queue. Processing type-C jobs ﬁrst will maximize
throughput rate at any point in time. Such a policy
will complete jobs from queue two no slower than
FCFS and this helps prevent workstation one from
starvation under the CONWIP constraint. Ostolaza et al. [12] use this type of policy, shortestprocessing-time ﬁrst (SPT). McClain et al. [8]
shows that SPT achieves about 3% higher eﬃciency than FCFS in some cases. However, there
are occasions in industry where FCFS is required
and we focus on FCFS at this time.
Additional assumptions for our model include:
(1) Workers speeds are assumed to be equal.
(2) The line is unidirectional. Once a job is passed
on to the next workstation, it cannot go back
to the previous one, regardless of whether or
not the shared task has been done.

(3) Worker one decides whether to perform the
shared task or to pass along the item.
The most important characteristic of dynamic
line balancing is the decision making process.
After workstation one ﬁnishes task A on a job, it
needs to make a decision: to keep the job and continue with task B, or to pass it on. Appropriate
decisions will give more work to station two when
it is more available or potentially so, or pass less
work onto it when otherwise. Overall, total workload is balanced between two stages through making decisions on-the-ﬂy.
In case of a non-preemptive shared task, if station one begins task B, then task B needs to be
completed before the job can be released. When
the shared task is assumed to be granular, station
one will make a decision each time one or multiple
subtasks of task B is completed, in addition to the
time that task A is ﬁnished.
Ostolaza et al. [12] make decisions by keeping
track of the number of long jobs in the buﬀer.
McClain et al. [8] use more accurate information
to make decisions: the number of subtasks available to station two, i.e., number of subtasks waiting
in buﬀer plus number of subtasks not yet completed on the job being processed. In addition to
using the information from [8], Gel et al. [5] proposed using the ratio of workcontent (counted also
by number of subtasks) available at station one to
total workcontent in the system. The information
used in [8] is adopted in our study, named Model
B. In the mean time, it is noticed that the model
will be simpliﬁed if only the number of subtasks
in the buﬀer is kept track of. For example, for a
type-C job with 3–4–3 task division to go through
station two, the workcontent information needs to
be updated three times. Updating occurs seven
times for a type-B job. If only the number of subtasks in the buﬀer is kept track of, workcontent
information is only updated when workstation
one passes on a job or when station two grabs a
job from the buﬀer. Thus, models using this simpliﬁed information are also built in our study,
named Model A. In practice, Model B would
require real-time tracking of remaining processing times of active jobs at workstations. In next
section, it will be shown that this leads to a

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

trade-oﬀ between control complexity and system
eﬃciency.
Unfortunately, optimal decision policies (making the best decision at all time) are generally
state-dependent and too complicated to compute
and implement easily [5]. A threshold policy is
hence more practical. Ostolaza et al. [12] propose
the half-full buﬀer (HFB) policy, which keeps the
number of long jobs in buﬀer one half of the
capacity. McClain et al. [8] translate the HFB policy to a subtask model and ﬁnd it near-optimal as
a threshold policy. Gel et al. [5] adapt the HFB
policy from [8] to CONWIP. It will be shown in
next section that this is near-optimal as well. Gel
et al. [5] also present the 50–50 Content rule which
keeps the workload ratio of station one to the system at 50 percent.
Optimal threshold values for both Model A and
Model B are searched under various task divisions
and CONWIP levels. We ﬁnd a heuristic that gives
optimal threshold values in Model A, which is
named the ‘‘Smallest R No Starvation (SRNS)
rule’’. Then we show that the HFB rule in [5] is
near-optimal in Model B. Before showing the results, we present the SRNS rule and HFB rule.
2.1. Smallest R no starvation (SRNS)
We propose the following rule: station one will
start or keep working on the shared task if R or
more units of subtasks are available in the buﬀer
between workstations. The threshold value, R, is
given by Eq. (1), where N is the CONWIP level
and tC (tB) is the number of subtasks for task C
(B). Note that [tB, (N  2) Æ tC] means that R can
take any integer value between tB and (N  2) Æ tC.
Our rule is then
SRNS rule

if tB P ðN  2Þ  tC ;
ðN  2Þ  tC ;
R¼
½tB ; ðN  2Þ  tC ; if tB < ðN  2Þ  tC :
ð1Þ
The key to balance a production line is to prevent blocking and starvation. Under our assumption of inﬁnite buﬀers, blocking is impossible.
Starvation may happen to workstation one as
well as to workstation two due to the CONWIP

857

constraint. Starvation of either workstation
should be prevented as much as possible. Experience tells us that putting a lesser load on workstation two would help. This is so because
workstation one has the power of decision and
ﬂow is unidirectional. Gel et al. [5] state that this
is true in a preemptive system but that in a nonpreemptive line, a bad decision of keeping a long
shared task can starve station two. In our study,
however, it is found that the idea can work for a
threshold policy in Model A even with the nonpreemptive assumption. We will state two propositions that motivate the SRNS rule. A complete
enumeration of the possible cases is provided in
the Appendix A to justify the rule and illustrate
its performance.
Deﬁne the term ‘‘myopic starvation’’ as starvation prior to completion of the current unit. Let T
be the contents of the buﬀer between workstations
one and two at a decision point.
Proposition 1. Setting R P tB prevents myopic
starvation at station two if task times are
deterministic.
Assume R P tB. If T < R, the shared task of the
current unit will be skipped and the unit will be
passed on. Then station two could not be starved
before the completion of the current unit. If
T P R, the current unit will be retained and continue to be processed on the shared task. Since station two has more than tB subtasks available in the
buﬀer, it will not be starved before station one ﬁnishes the shared task of the current job. This proposition seeks to avoid the situation that all jobs are
in station one.
Proposition 2. Setting R 6 (N  2) Æ tC prevents
immediate starvation at station one.
If, at a decision point, there are N  1 jobs at
station two, then the unit at station one should
be retained. (N  2) Æ tC (or less) work in the buffer indicates that at most N  2 jobs are present
in the buﬀer. Thus, there are at most N  1 jobs
available to station two in total. Therefore, there
is at least one job at station one. Assume
R 6 (N  2) Æ tC. If T < R, it is safe to skip the
shared task, since there is at least one type A

858

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

job available to station one. If T P R, then
N  1 jobs could be at station two. Station one
may face immediate starvation if it passes the
current unit on. Therefore, it may be desirable
to retain the unit and continue processing the
shared task. This proposition seeks to avoid the
situation that all jobs are at station two.
These propositions form the supporting basis
for the SRNS rule. A more detailed analysis
using these propositions to show the eﬀect of
SRNS in all situations is provided in Appendix
A.

in the buﬀer. Following the idea of a half-full buffer, rule 1 is proposed, which takes half of the
maximum load as a cutoﬀ level.

2.2. Half-full buﬀer (HFB) rule

w estimates the proportion of task B that will be
completed at station one over the long run to balance workload.

The HFB rule states: station one will start or
keep working on the shared task if R or more units
of subtasks are available to station two. This
amount includes both jobs in the buﬀer and the
job being processed at station two. The threshold
value, R, is given by (2).
tB ðtB þ tC Þ þ tC N  2
R¼ þ
:

2
2
2

ð2Þ

ð1  wÞ  tB þ tC
:
2

ð4Þ

Rule 2 is a modiﬁcation of rule 1. It takes into account a balance factor that is adapted from Gel
et al. [5]. This factor, w, is the solution to the following equation:
tA þ w  tB ¼ ð1  wÞ  tB þ tC ;

ð5Þ

2.3.2. Model B rules
Rule 3A
N  ðtA þ tB þ tC Þ  tA
:
2

ð6Þ

The workload in the system is maximized when all
N jobs are present at station one. At the decision
point, the maximum workload as seen by station
one is N Æ (tA + tB + tC)tA because one task A
has been completed. We propose rule 3A which
will keep the expected workload on station two
half of this maximum.
Rule 3B

2.3. Other rules
Several other heuristics have been tested for
both Model A and Model B. They are found to
be less productive than the SRNS and HFB rules.
They are presented here for discussion.
2.3.1. Model A rules
Rule 1
tB þ tC
;
2

R ¼ ðN  2Þ 

RA ¼

Explanation for the HFB rule can be found in
[5,8]. Brieﬂy, the terms correspond to the breakeven point for idling of stations one and two in
an otherwise empty system as a result of the hold
or pass decision plus one half the potential buﬀer
content.

R ¼ ðN  2Þ 

Rule 2

ð3Þ

N  2 is the maximum number of jobs in the buﬀer
if neither station is starved and tB + tC is the expected task time for a type-B job. Thus,
(N  2) Æ (tB + tC) deﬁnes the maximum workload

RB ¼

tB þ N  tC
:
2

ð7Þ

The workload in the system is minimized when
N  1 type-C jobs are at station two. The minimal
workload in the system that station one sees at the
decision point is tB + N Æ tC. Thus, rule 3B is proposed to take half of this minimum as a cutoﬀ
level.
Rule 3C
RC ¼

RA þ RB
:
2

ð8Þ

Rule 3C simply take the average of the maximumbased and the minimum-based cutoﬀ levels.

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

859

with only a few exceptions. For these exceptions,
SRNS gives R values falling out of the optimal
R-value range in addition to values falling within
range. Even for these exceptions, the maximum
eﬃciency lost is no more than 2.5%.
We are not interested in higher CONWIP levels,
since the eﬃciencies that DLB models achieved are
already very close to 1 with just small inventories.
For example, with N = 5, task division 4–2–4 and
3–4–3 in Model A achieve eﬃciencies of 0.9873
and 0.9893, respectively. As to be presented in
the next observation, task 4–2–4 and 3–4–3 in
Model B achieve eﬃciencies that are even closer
to 1, 0.9896 and 0.9929, respectively. The benchmark eﬃciency, that of 5–0–5 with N = 5, is
0.9510.
Table 1 shows the match of SRNS rule to the
best policy and Table 2 shows the eﬃciencies.
The best eﬃciencies (‘‘Optimal R ’’ columns in
Table 2) are examined by employing a single factor
(task division) ANOVA analysis with ﬁve levels
(5–0–5, 4–2–4, 3–4–3, 2–6–2 and 1–8–1). It is conﬁrmed that the factor considered, task division, is
signiﬁcant to the systems throughput (or eﬃciency). The p-value for the 5-level factor is less
than 0.001.
Conﬁdence intervals (95%) for the best eﬃciencies are also drawn. For N = 3, it is revealed that
the eﬃciency improvement of only 4–2–4 and 3–
4–3 over 5–0–5 are statistically signiﬁcant. Task
division 1–8–1 actually performs worse than 5–0–
5. With more inventory, N = 4 and 5, all four task
division with non-zero shared task perform better
than 5–0–5. Task divisions with a shorter shared
task (4–2–4, 3–4–3) perform better than those with
larger shared task (2–6–2, 1–8–1) when inventory

3. Results and observations
Simulation models have been built and run
using Arena 5.0Ò. In evaluating the models, common random numbers (CRN) are used to reduce
variance. Each conﬁguration is run for ﬁve replications with length of 20,000 time units, 5000 of
which are set to be warm-up. The warm-up length
is obtained by checking plots of throughput rates.
Optimal threshold values are obtained using OptQuestÒ utility for ArenaÒ. Comparisons between
methods are tested for statistical signiﬁcance
[10,13]. Eﬃciencies are measured by the ratio of
simulated throughput rates over maximum
achievable.
Following the problem size of [5], the mean subtask time is set to be 1 and the mean task time of a
job to be 10. Subtasks are exponentially distributed and tasks are Erlang-k distributed.
As mentioned in Section 2, a threshold policy is
more practical than a state-dependent rule,
although it is not optimal in general [5]. In this section, a rule is deﬁned to be ‘‘best’’ if it is the threshold policy in its model with highest eﬃciency. The
R values with highest eﬃciency are called the optimal R values [8].
3.1. Observation one: Near-optimality of SRNS rule
in Model A
The optimal R value for a speciﬁc conﬁguration
is usually not a unique number, but a range of
numbers. For CONWIP levels of 3, 4 and 5, the
SRNS rule gives very good match to the optimal
threshold values across all balanced task divisions
that were tested (4–2–4, 3–4–3, 2–6–2, and 1–8–1)

Table 1
Match of threshold values
Conﬁguration

4–2–4
3–4–3
2–6–2
1–8–1
*
a

N=3

N=4

N=5

Optimal R

SRNS

Optimal R

SRNS

Optimal R

SRNS

1–4a
1–3
1, 2
1

2–4a
3
2
1

1–4
4–6
3, 4
2

2–8*
4–6
4
2

5, 6
4–6
5, 6
3

2–12*
4–9*
6
3

SRNS range that includes non-optimal values.
Bolded numbers stand for optimality.

860

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

Table 2
Eﬃciencies of SRNS rule versus optimal R
Eﬃciency

N=3
Optimal R

SRNS

Optimal R

SRNS

Optimal R

SRNS

5–0–5
4–2–4
3–4–3
2–6–2
1–8–1

0.901
0.930a
0.929
0.905
0.868

0.901
0.930a
0.929
0.905
0.868

0.934
0.973
0.972
0.967
0.948

0.934
0.949–0.973
0.972
0.967
0.948

0.951
0.987
0.989
0.986
0.977

0.951
0.967–0.987
0.988–0.989
0.986
0.977

a

N=4

N=5

Bolded numbers stand for optimality.

Confidence Interval (95%) for Model B
Performance with Optimal R

5=3 0-5
N 4-2
=3 -4
N 3-4
=3 -3
N 2-6
=3 -2
N 1-8
=4 -1
N 5-0
=4 -5
N 4-2
=4 -4
N 3-4
=4 -3
N 2-6
=4 -2
N 1-8
=5 -1
N 5-0
=5 -5
N 4-2
=5 -4
N 3-4
=5 -3
N 2-6
=5 -2
181
N

N

N

N

=3

5=3 0-5
4=3 2-4
N 3-4
=3 -3
N 2-6
=3 -2
N 1-8
=4 -1
N 5-0
=4 -5
N 4-2
=4 -4
N 3-4
=4 -3
N 2-6
=4 -2
N 1-8
=5 -1
N 5-0
=5 -5
N 4-2
=5 -4
N 3-4
=5 -3
N 2-6
=5 -2
181

0.95
0.93
0.91
0.89
0.87
0.85

0.99
0.97
0.95
0.93
0.91
0.89
0.87
0.85

=3

Efficiencies

0.99
0.97

N

Efficiencies

Confidence Interval (95%) for Model A
Performance with Optimal R

Models: N = 3 ~ 5, Task division of 5-0-5 to 1-8-1

Models: N = 3 ~ 5, Task division of 5-0-5 to 1-8-1

Fig. 1. Conﬁdence intervals for best eﬃciencies, Model A.

Fig. 2. Conﬁdence intervals for best eﬃciencies, Model B.

is small (N = 3, 4). This diﬀerence diminishes as
the inventory is allowed to increase (N = 5).
Fig. 1 shows the conﬁdence intervals, where each
box represents a conﬁdence interval and the tails
of which stand for minimum and maximum values
across replications.

but this diﬀerence diminishes as inventory increases (see Fig. 2).
Notice that in Table 3, the HFB fractional
threshold values (obtained from Eq. (2)) can actually be replaced by their integer ceilings. For example, for N = 3 4–2–4, the HFB threshold is 3.5.
This is equivalent to 4, since the threshold rule
states: ‘‘station one will start or keep working on
the shared task if R or more units of subtasks
are available to station two.’’ Model eﬃciencies
appear in Table 4.

3.2. Observation two: Near-optimality of HFB rule
in Model B
It is found that the HFB rule by Gel et al. [5] is
very close to the best policy. Similar observations
to those of SRNS in Model A are found. The
factor considered, task division, is signiﬁcant in
aﬀecting system performance. Task divisions with
non-zero shared task all perform better than the
benchmark 5–0–5 task division except one case,
1–8–1 with N = 3. Task division 4–2–4 and 3–4–
3 perform similarly well, with no statistically signiﬁcant diﬀerence. Small to medium sized shared
task cases perform better than a large shared task,

3.3. Observation three: Beneﬁt of DLB in reducing
inventory
The saving of inventory via DLB is signiﬁcant.
Fig. 3 shows this. For example, task division 3–4–3
in Model B achieves an eﬃciency of 0.9929 with a
small inventory of ﬁve jobs, while the non-DLB
model, 5–0–5, achieves a lower eﬃciency of

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

861

Table 3
Near-optimality of HFB rule: Threshold values
Conﬁguration

N=3
Optimal R

HFB

Optimal R

HFB

Optimal R

HFB

4–2–4
3–4–3
2–6–2
1–8–1

4, 5a
5
6
5

3.5a
4.5
5.5
6.5

7
8
9
9

6
7
8
9

9
10
11
12

8.5
9.5
10.5
11.5

a

N=4

N=5

Bolded numbers stand for optimality.

Table 4
Near-optimality of HFB rule: Eﬃciencies
Eﬃciency

N=3

5–0–5
4–2–4
3–4–3
2–6–2
1–8–1

N=4

N=5

Optimal R

HFB

Optimal R

HFB

Optimal R

HFB

0.901
0.939a
0.938
0.914
0.885

0.901
0.939a
0.938
0.914
0.881*

0.934
0.9751
0.9776
0.9659
0.948

0.934
0.9741**
0.9768***
0.9651****
0.948

0.951
0.990
0.993
0.989
0.985

0.951
0.990
0.993
0.989
0.985

*

Paired-t test, Optimal R vs. HFB, p- value 0.001.
Paired-t test, Optimal R vs. HFB, p-value 0.003.
***
Paired-t test, Optimal R vs. HFB, p-value 0.004.
****
Paired-t test, Optimal R vs. HFB, p-value 0.492, insigniﬁcant diﬀerence.
a
Bolded numbers stand for optimality.
**

Benefit of DLB in
Reducing Inventory
1

Efficiency

0.98
0.96

5-0-5

0.94

3-4-3 Model B

0.92

3-4-3 Model A

0.9

19

17

15

13

9

11

7

5

3

0.88

CONWIP (N )

Fig. 3. Reducing inventory via DLB.

0.9859 with an inventory of 14 jobs (diﬀerence
0.0070, Paired-t test with a p-value of 0.027).
3.4. Observation four: Tradeoﬀ between
information complexity and line eﬃciency
As mentioned in Section 2, Model A uses
less information (tC to tC + tB times less) than

Model B. In some situations, particularly as
the results are extended to a longer line, acquiring the additional information could become
costly. Thus, it is desirable to see how much
higher eﬃciency Model B can achieve than
Model A. Using less information, however,
Model A is found to perform almost as good
as Model B.
For example, for CONWIP of 3, the eﬃciency
lost due to using less accurate information is generally about 0.001. Given larger buﬀer capacity
(N = 4), the eﬃciency lost is as low as 0.006.
The eﬃciency lost is bounded by 0.0073 in the
case of N = 5. Table 5 gives the detailed numbers
with the highest eﬃciencies in each row being
bolded.
Paired-t tests are conducted to verify the diﬀerence between the eﬃciency of Model A versus
Model B. The respective p-values are given in
Table 6. It can be seen that most of the diﬀerences
in performance are signiﬁcant.

862

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

Table 5
Comparison of eﬃciencies of Model A and Model B
Eﬃciency

5–0–5
4–2–4
3–4–3
2–6–2
1–8–1

N=4

N=5

Model A

Model B

Model A

Model B

Model A

Model B

0.9009
0.9303
0.9289
0.9045
0.8681

0.9009
0.9386
0.9383
0.9140
0.8845

0.9336
0.9731
0.9717
0.9667a
0.9485a

0.9336
0.9751
0.9776
0.9659a
0.9476a

0.9513
0.9873
0.9893
0.9859
0.9773

0.9513
0.9896
0.9929
0.9889
0.9846

Such diﬀerences are statistically insigniﬁcant (assuming a signiﬁcance level of 0.05).

3.6. Observation six: Improved eﬃciency as
shared task becomes granular
Intuitively, allowing the shared task to be preemptive gives more agility to the system and thus
should improve eﬃciency. In real life, however,
not all tasks can be preempted. Between the two

Best Efficiencies for both Balanced
and Unbalanced Lines
1.0000
N=3 Model A
N=3 Model B
N=4 Model A
N=4 Model B
N=5 Model A
N=5 Model B

0.9500
0.9000
0.8500

4

42
4-

3

42-

6

16-

1

13-

8-

2

0.8000

1-

Unbalanced work assignment is generally
avoided in designing assembly lines. However,
the nature of the tasks may necessitate this in some
instances. Four unbalanced task divisions (3–1–6,
6–1–3, 2–4–4 and 4–4–2) are designed and compared to balanced assignments in our DLB model.
Conforming to expectations, balanced task divisions have higher yield than unbalanced ones.
Take the case of 6–1–3 for example. For N = 4,
the eﬃciency is 10.4% lower than the static assignment case (5–0–5). However, a medium-size shared
task can counteract the eﬀect of imbalance. Task
division 4–4–2 has higher eﬃciency than the static
5–0–5 assignment, and has almost the same eﬃciency as the best balanced task division (3–4–3).
Hence, in case that unbalanced task divisions are
inevitable when designing production line, worksharing may be a valuable approach.
Dynamic line balancing (DLB) is able to remedy eﬃciency lost due to imbalance in task division
of 4–4–2 and 2–4–4, but not in the case of 6–1–3

3

3.5. Observation ﬁve: DLB as a solution to
unbalanced lines

6-

a
Such diﬀerences are statistically insigniﬁcant (assuming a
signiﬁcance level of 0.05).

2-

0.005
0.001
0.010
0.001

4

0.003
< 0.001
0.485a
0.556a

4-

< 0.001
0.001
0.001
0.002

3-

4–2–4
3–4–3
2–6–2
1–8–1

5

N=5

2-

N=4

0-

N=3

4-

p-Value

and 3–1–6. This is because DLB rule forces WS1
to always pass on (keep) shared task in 6–1–3
(3–1–6) task division, as is expected. This makes
the line resemble a 6–4 (4–6) unbalanced line. In
the case of 4–4–2 (2–4–4), worksharing dynamically allocates 1/4 (3/4) of shared task, on average,
to WS1 and the rest to WS2. This eﬀectively balances the line. Undergoing such ‘‘on-the-ﬂy’’ balancing, the line is able to gain back lost eﬃciency.
Fig. 4 shows the eﬃciencies across all task divisions under CONWIP of 3, 4 and 5. Notice that
there exists an optimal level of cross-training for
balanced task divisions, which conform to the
result in [5] and [9].

5-

Table 6
p-Values for the comparison of Model A vs. Model B

Efficiency

a

N=3

Task Division

Fig. 4. Best eﬃciencies for both balanced and unbalanced
Lines.

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

extreme cases of preemptive and non-preemptive
shared task, there are natural discrete breakpoints
for a shared task sometimes. Gel et al. [5] starts the
examination of such cases, i.e. shared task with
granularity. They found that as the shared task becomes more granular (workstation one has more
decision points when performing the shared task),
the optimal throughput rate increases. The optimum in their sense is the state-dependent optimum. Our work conﬁrms their conclusion in the
scope of threshold policies and gives some insight
to how the best threshold values change as granularity varies.

863

Three task divisions (4–2–4, 3–4–3, and 2–6–2)
are chosen and tested under CONWIP levels of
3, 4 and 5. Workstation one makes decisions upon
completion of each subtask that belongs to the
shared task if it is given maximum granularity.
For less granular shared task, the decision points
may come after multiple subtasks are completed.
For example, 4–2–4 has maximum of two decision
points (denoted by D2) and minimum of one (D1),
which is the same as non-preemptive case. Task
division 3–4–3 has three granularity levels, D1,
D2 and D4, corresponding to decision epochs before subtask 1 of the shared task, before subtask

Table 7
Eﬃciencies under various levels of granularity
Model A

SRNS

Model B

HFB

3–4–3
N=3
D1
D2
D4

0.9289
0.9385
0.9393

0.9289
0.9385
0.9393

0.9383a
0.9557
0.9646

0.9383a
0.9525
0.9558

N=4
D1
D2
D4

0.9717
0.9809
0.9856

0.9717
0.9721
0.9707

0.9776
0.9876
0.9896*

0.9768
0.9876
0.9895*

N=5
D1
D2
D4

0.9893
0.9952
0.9965

0.9877
0.9854
0.9862

0.9929
0.9967
0.9972

0.9929
0.9967
0.9972

2–6–2
N=3
D1
D2
D3
D6

0.9045
0.9349
0.9405
0.9429

0.9045
0.9349
0.9405
0.9429

0.9140
0.9456
0.9588
0.9695

0.9140
0.9369
0.9393
0.9370

N=4
D1
D2
D3
D6

0.9667
0.9771
0.9807
0.9876

0.9667
0.9771
0.9789
0.9781

0.9659
0.9840*
0.9887
0.9927

0.9651
0.9827*
0.9834
0.9838

N=5
D1
D2
D3
D6

0.9859
0.9931
0.9948
0.9979

0.9859
0.9931
0.9948
0.9941

0.9889
0.9959
0.9977
0.9989

0.9889
0.9945
0.9946
0.9939

*
a

Such diﬀerences are statistically insigniﬁcant (a = 0.05).
Bolded numbers stand for highest eﬃciencies.

864

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

points and could starve WS2 longer [5]. (That is
why 3–4–3 D1 works better than 2–6–2 D1.) However, granularity can counteract this eﬀect and improve eﬃciency for large shared tasks. An
interesting aside is that the DLB line resembles a
Bucket-Brigade System (BSS), in the extreme case
of full cross-training (0–10–0 task division) with
preemptive assumption.
Optimal R values tend to decrease in both
Model A and Model B as granularity increases.
The eﬃciencies of heuristic rules, the SRNS and
the HFB rule, climb as granularity increases, but
not as much as the best policy in each model. This
is because the optimal R value declines as the
shared task becomes more granular. Hence, the
R value given by the heuristic rules is oﬀset more
from the optimal R value. We credit this Rdecreasing phenomenon to the fact that WS1 is
given more decision ﬂexibility and thus keeping
less work at WS2 is preferable. Table 8 presents
the optimal R values for both models given various
granularities. (4–2–4 cases are omitted for brevity.)

1 and 3 of the shared task, and before each one of
the four subtasks.
The best eﬃciencies are found to increase as the
shared task becomes more granular and the
improvements are signiﬁcant. Consider 2–6–2 task
division with CONWIP of 3 (N = 3) in Model B
for example. The optimal eﬃciency with six decision points is 0.9695, which is 0.0555 higher than
that of the non-preemptive case, 0.9140 (Paired-t
test with p-value less than 0.001). Table 7 and
Fig. 5 show the eﬃciencies (4–2–4 cases are omitted for brevity.) Model A and Model B columns
in Table 7 represent the best eﬃciencies (those of
models with optimal threshold values).
With preemption allowed, eﬃciency increases as
the shared task size becomes bigger [5]. Unsurprisingly, larger shared tasks achieve higher eﬃciency
than smaller shared tasks when given higher granularity. For example, for CONWIP of 3, 2–6–2 D6
outperforms 3–4–3 D4 by 0.9695 to 0.9646 in
Model B (Paired-t test with a p -value of 0.017)
and by 0.9429 to 0.9393 in Model A (Paired-t test
with a p-value of 0.043). Under the non-preemptive shared task assumption, small to medium
sized shared tasks are more preferable than a large
shared task. A bad decision by WS1 to keep a bigger shared task increases time between decision

3-4-3 N=3

0.9650

3.7. Observation seven: Evaluation of other rules
The remaining rules described in Section 2.3
were also tested. Results were, in all cases similar

3-4-3 N=4

0.9900

3-4-3 N=5
0.9990

0.9600

0.9970

0.9500
0.9450
0.9400
0.9350

Efficiency

0.9850

Efficiency

Efficiency

0.9550

0.9800
0.9750

0.9300

SRNS

0.9930

Model B

0.9910

HFB

0.9890
0.9870

0.9250

0.9700

D1

D2
Granularity

D4

0.9850

D1

D2

D4

D1

Granularity

2-6-2 N=3

2-6-2 N=4
0.9990

0.9600

0.9900

0.9970

0.9500

0.9850

0.9950
Efficiency

0.9950

0.9400

D2

D4

Granularity

0.9700

Efficiency

Efficiency

Model A

0.9950

0.9800

2-6-2 N=5

Model A

0.9930

SRNS

0.9910

Model B

0.9200

0.9700

0.9890

HFB

0.9100

0.9650

0.9870

0.9300

0.9750

0.9600

0.9000
D1

D2
D3
Granularity

D6

0.9850
D1

D2

D3

D6

D1

Granularity

Fig. 5. Eﬃciencies increase as granularity increases.

D2

D3

Granularity

D6

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

865

Table 8
Optimal R decreases as granularity increases
Model A*

SRNS

Model B*

HFB

3a
3
3

5
4
4

4.5a
4.5
4.5

3–4–3
N=3
D1
D2
D4

1–3
1–3
1–3

N=4
D1
D2
D4

4–6
1–3
1–3

4–6
4–6
4–6

8
7
6,7

N=5
D1
D2
D4

4–6
6
5,6

4–9
4–9
4–9

10
9,10
8,9,10

9.5
9.5
9.5

2–6–2
N=3
D1
D2
D3
D6

1,2
1,2
1,2
1,2

2
2
2
2

6
4,5
4
3,4

5.5
5.5
5.5
5.5

N=4
D1
D2
D3
D6

3,4
3,4
1,2,3,4
1,2

4
4
4
4

9
6,7,8
6,7
5,6

8
8
8
8

N=5
D1
D2
D3
D6

5,6
6
3,4,5,6
3,4

6
6
6
6

11
9,10
8,9
8,9

10.5
10.5
10.5
10.5

*
a

7
7
7

Optimal R values.
Bolded R for SRNS and HFB shows match to optimal R value.

or inferior to the HFB (Model B) and SRNS
(Model A) results. Results are shown in Tables 9
and 10.

4. Extension to longer lines
In this section, the SRNS rule and HFB rule are
generalized to a M -stage line for investigation in
regard of how well threshold heuristics perform
in longer lines. We hypothesize that such generalized rules should work reasonably well in a Mstage line. We intend to study such lines in detail
in subsequent work.

4.1. M-stage SRNS rule
An appropriate condition to meet proposition two (as described in Section 2.1) becomes
complicated in a multiple stage model. Proposition one, however, can be easily migrated to
multi-stage lines and constitutes the M-stage
SRNS rule: worker k (k = 1, 2, . . . , M  1) will
start or keep working on the shared task if
Rk or more units of subtasks are available in
the input buﬀer of worker k + 1. The threshold
value, Rk, is given by Eq. (9), where t2k is the
size of the task shared by worker k and
worker k + 1.

866

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

Table 9
Model A results
N=3

N=4

Optimal R

Rule 1

Rule 2

Optimal R

Rule 1

Rule 2

Conﬁguration
4–2–4
3–4–3
2–6–2
1–8–1

1, 2, 3, 4
1, 2, 3
1, 2
1

3a
3.5
4
4.5

2.5
2.5
2.5
2.5

1,2,3,4
4,5,6
3,4
2

6
7
8
9

5
5
5
5

Eﬃciency
5–0–5
4–2–4
3–4–3
2–6–2
1–8–1

0.901
0.930
0.929
0.905
0.868

0.901
0.930a
0.880
0.857
0.818

0.901
0.930
0.929
0.857
0.818

0.934
0.973
0.972
0.967
0.948

0.934
0.967
0.959
0.956
0.946

0.934
0.967
0.972
0.956
0.946

a

Bolded numbers stand for optimality.

Table 10
Model B results
N=3

N=4

Optimal R

Rule 3A

Rule 3B

Rule 3C

Optimal R

Rule 3A

Rule 3B

Rule 3C

Conﬁguration
4–2–4
3–4–3
2–6–2
1–8–1

4,5
5
6
5

13
13.5
14
14.5

7
6.5
6a
5.5

10
10
10
10

7
8
9
9

18
18.5
19
19.5

9
8
7
6

13.5
13.25
13
12.75

Eﬃciency
5–0–5
4–2–4
3–4–3
2–6–2
1–8–1

0.901
0.939
0.938
0.914
0.885

0.901
0.826
0.754
0.785
0.774

0.901
0.920
0.924
0.914a
0.883

0.901
0.869
0.859
0.854
0.848

0.934
0.975
0.978
0.966
0.948

0.934
0.845
0.799
0.765
0.717

0.934
0.970
0.978
0.959
0.923

0.934
0.909
0.905
0.925
0.930

a

Bolded numbers stand for optimality.

Rk ¼ t2k :

ð9Þ

This rule avoids downstream starvation and most
closely follows the goal of minimizing R to avoid
starvation of the decision maker.
4.2. M-stage HFB rule
We generalize the HFB rule to an M-stage
line as follows: worker k (k = 1, 2, . . ., M  1)
will start or keep working on the shared task
if Rk or more units of subtasks are available

to station k+1. The threshold value, Rk, is given
by
8
;
R1 ¼ t22 þ ½ð1  p1 Þ  ðt2 þ t3 Þ þ p1  t3   N M
>
M
>
>
>
t4
N M
>
R
¼
þ
½ð1

p
Þ

ðt
þ
t
Þ
þ
p

t


;
>
4
5
5
2
2
M
< 2 2
t2i
Ri ¼ 2 þ ½ð1  pi Þ  ðt2i þ t2iþ1 Þ þ pi  t2iþ1   N M
;
M
>
>
t2M2
>
RM1 ¼ 2 þ ½ð1  pM1 Þ  ðt2M2 þ t2M1 Þ
>
>
>
:
þ pM1  t2M1   N M
;
M
ð10Þ
where the pis can be found by solving the following set of balance equations:

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

t1 þ p1 t2 ¼ ð1  p1 Þt2 þ t3 þ p2 t4 ¼ . . .
¼ ð1  pi1 Þt2i2 þ t2i1 þ pi t2i ¼ . . .
¼ ð1  pM1 Þt2M2 þ t2M1 :

ð11Þ

In the special case of a 2-stage line, t1, t2, and t3,
the notation for tasks, correspond to tA, tB, and tC,
respectively.

5. Summary and conclusions
A 2-stage serial line with partial cross-training
has been examined. It was found that the ﬂexibility
aﬀorded by cross-training can lead to measurably
higher throughput even with low levels of WIP
and random processing times. Simple control rules
based on buﬀer content are eﬀective. Results indicate that shared tasks can help balance unbalanced
lines and eﬃciency increases with the frequency of
the ‘‘hold-pass’’ decisions.
It would be especially worthwhile to investigate how well threshold heuristics, such as those
tested in this paper, perform in longer production lines. It would also be of interest to generalize the model to incorporate other processing
time distributions than exponential (or Erlang).
The eﬀect of blocking in longer lines may become more important and may deserve investigation as well.

Acknowledgement
This material is based upon work supported by
the National Science Foundation under Grant No.
9900057.

Appendix A. Demonstration of validity of the
SRNS rule
The SRNS rule seeks to eliminate myopic starvation. To show this, we examine all possible task
time situations. For each situation we demonstrate
the validity of the SRNS rule. Where applicable,
results are related to the propositions discussed
previously.

867

(Case 1) tB > (N  2) Æ tC
(1.1) SettingR P tB:
(1.1.1) If T P R, WS1 will retain the current job
and be busy with the shared task. WS2 will
not face myopic starvation because T P tB
(Proposition 1).
(1.1.2) If (N  2) Æ tC 6 T < R, WS1 will pass
on the job. WS2 will be busy until this
job ﬁnishes processing. WS1, however,
may be starved immediately because
T P (N  2) Æ tC (Proposition 2).
(1.1.3) If T < (N  2) Æ tC, WS1 will pass on the
job and WS2 will not be starved before
the completion of this job. WS1 will
not be starved either because we are sure
that at least one other job is available to
WS1 right now (T < (N  2) Æ tC, Proposition 2).
In short, setting R P tB may starve WS1 immediately when (N  2) Æ tC 6 T < R.
(1.2) Setting (N  2) Æ tC < R < tB:
(1.2.1) If T P tB, WS1 will keep the current
job and remain busy. WS2 will not
face myopic starvation either, since
T P t B.
(1.2.2) If R 6 T < tB, WS1 will keep the current
job. WS2 may soon face myopic starvation because T < tB.
(1.2.3) If (N  2) Æ tC 6 T < R, WS1 will pass on
the job. WS2 will not be starved. WS1
may face immediately starvation because
T P (N  2) Æ tC.
(1.2.4) If T < (N  2) Æ tC, WS1 will pass on the
job. WS2 will not be starved. WS1 will
not
be
starved
either
because
T < (N  2) Æ tC.
In short, setting (N  2) Æ tC < R < tB may
starve WS1 immediately when (N  2) Æ tC 6
T < R and may lead to myopic starvation of
WS2 when R 6 T < tB.
(1.3) Setting R = (N  2) Æ tC:
(1.3.1) If T P tB, WS1 will keep the job. WS1 will
be busy. WS2 has enough work to avoid
myopic starvation.

868

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

(1.3.2) If R 6 T < tB, WS1 will keep the job. WS1
will be busy. WS2 faces myopic starvation.
(1.3.3) If T < R, WS1 will pass on the job. WS2
will not be starved. WS1 will not be
starved either because T < (N  2) Æ tC.
In short, setting R = (N  2) Æ tC may lead to
myopic starvation of WS2 when R 6 T < tB.
(1.4) Setting R < (N  2) Æ tC:
(1.4.1) If T P tB, WS1 will keep the job. WS1 will
be busy. WS2 has enough work to avoid
myopic starvation.
(1.4.2) If R 6 T < tB, WS1 will keep the job. WS1
will be busy. WS2 faces myopic starvation
because T < tB.
(1.4.3) If T < R, WS1 will pass on the job. WS2
will not be starved. WS1 will not be staved
either because T < (N  2) Æ tC.
In short, setting R < (N  2) Æ tC will lead to
myopic starvation of WS2 when R 6 T < tB.
Conclusion 1. Both setting R P tB and (N  2) Æ
tC < R < tB may lead to immediate starvation
of WS1, while setting R = (N  2) Æ tC and R <
(N  2) Æ tC may only lead to myopic starvation
of WS2. Preventing immediate starvation of
WS1 is more important than preventing possible
myopic starvation of WS2 because we have random processing times. We recommend setting
R = (N  2) Æ tC rather than R < (N  2) Æ tC since
the former has less possibility of causing myopic
starvation.
In all, we propose setting R = (N  2) Æ tC when
tB > (N2) Æ tC.
(Case 2) tB = (N  2) Æ tC
(2.1) Setting R > tB = (N  2) Æ tC:
(2.1.1) If T P R, WS1 will keep the job. WS1
will be busy with the shared task. WS2
have enough work to avoid myopic starvation because T > tB.
(2.1.2) If (N  2) Æ tC 6 T < R, WS1 will pass
on the job. WS2 will not be starved.
WS1 may face immediate starvation
because T P (N  2) Æ tC.

(2.1.3) If T < (N  2) Æ tC, WS1 will pass on the
job. WS2 will not be starved. WS1
will not be starved either because
T < (N  2) Æ tC.
In short, setting R > tB may starve WS1 immediately when (N  2) Æ tC 6 T < R.
(2.2) Setting R = tB = (N  2) Æ tC:
(2.2.1) If T P R, WS1 will keep the job. WS1
will be busy with the shared task. WS2
has enough work to avoid myopic
starvation.
(2.2.2) If T < R, WS1 will pass on the job. WS2
will not starved. WS1 will not be starved
either because T < (N  2) Æ tC.
In short, setting R = tB = (N  2) Æ tC will neither starve WS1 immediately, nor lead to myopic
starvation of WS2.
(2.3) Setting R < tB = (N  2) Æ tC:
(2.3.1) If T P tB, WS1 will keep the job. WS1
will be busy. WS2 has enough work to
do.
(2.3.2) If R 6 T < tB, WS1 will keep the job. WS1
will be busy. WS2 faces myopic starvation
because T < tB.
(2.3.3) If T < R, WS1 will pass on the job. WS2
will not be starved. WS1 will not be
starved either because T < (N  2) Æ tC.
In short, setting R < tB = (N  2) Æ tC leads to
myopic starvation when R 6 T < tB.
Conclusion 2. Out of the three options, setting
R = tB = (N  2) Æ tC is the only one that leads to
no starvation and thus is what we recommend.
In all, we propose setting R = tB = (N  2) Æ tC
when tB = (N  2) Æ tC.
(Case 3) tB < (N  2) Æ tC
(3.1) Setting R > (N  2) Æ tC:
(3.1.1) If T P R, WS1 will keep the job.
WS1 will be busy. WS2 has enough
work to cushion against the myopic
starvation.

R.G. Askin, J. Chen / European Journal of Operational Research 168 (2006) 853–869

(3.1.2) If (N  2) Æ tC 6 T < R, WS1 will pass
on the job. WS2 will not be starved.
WS1 may be starved immediately.
(3.1.3) If T < (N  2) Æ tC, WS1 will pass on
the job. WS2 will not be starved.
WS1 not be starved either because
T < (N  2) Æ tC.
In short, setting R > (N  2) Æ tC may starve
WS1 immediately when (N  2) Æ tC 6 T < R.
(3.2) Setting tB 6 R 6 (N  2) Æ tC:
(3.2.1) If T P R, WS1 will keep the job. WS2 will
not be starved either.
(3.2.2) If T < R, WS1 will pass on the job. WS2
will not be starved. WS1 will not be
starved either because T < (N  2) Æ tC.
In short, setting tB 6 R 6 (N  2) Æ tC will not
starve either workstation.
(3.3) Setting R < tB:
(3.3.1) If T P tB, WS1 will keep the job. WS1 will
be busy. WS2 has enough work to avoid
myopic starvation.
(3.3.2) If R 6 T < tB, WS1 will keep the job. WS1
will be busy. WS2 faces myopic starvation
because T < tB.
(3.3.3) If T < R, WS1 will pass on the job. WS2
will not be starved. WS1 will not be
starved either because T < (N  2) Æ tC.
In short, setting R < tB leads to myopic starvation of WS2 when R 6 T < tB.
Conclusion 3. In all, setting tB 6 R 6 (N  2) Æ tC is
the best choice when tB < (N  2) Æ tC.
Summarizing the analysis above, we conclude
with the following rule––SRNS:
(A) set R = (N  2) Æ tC when tB > (N  2) Æ tC;
(B) set R = (N  2) Æ tC when tB = (N  2) Æ tC;
(C) set tB 6 R 6 (N  2) Æ tC when tB < (N  2) Æ
tC.

869

The Model A analysis above intentionally discards the information about the job in process at
station two.
References
[1] R.G. Askin, A. Iyer, A comparison of scheduling philosophies for manufacturing cells, European Journal of
Operational Research 69 (1993) 438–449.
[2] J.J. Bartholdi, D.D. Eisenstein, A production line that balances itself, Operations Research 44 (1) (1996) 21–34.
[3] D.P. Bischak, Performance of a manufacturing module
with moving workers, IIE Transactions 28 (1) (1996) 723–
733.
[4] R. Conway, W. Maxwell, J.O. McClain, L.J. Thomas, The
role of work-in-process inventory in serial production lines,
Operation Research 36 (2) (1988) 229–241.
[5] E.S. Gel, W.J. Hopp, M.P. Van Oyen, Factors aﬀecting
opportunity of worksharing as a dynamic line balancing mechanism, IIE Transactions 34 (10) (2002) 847–
863.
[6] M. Hillier, Characterizing the optimal allocation of storage
space in production line systems with variable processing
times, IIE Transactions 32 (1) (2000) 1–8.
[7] A. Iyer, R.G. Askin, A general framework for comparing
operating policies in manufacturing cells, Annals of Operations Research 77 (1998) 23–50.
[8] J.O. McClain, J. Thomas, C. Sox, On-the-ﬂy line balancing with very little WIP, International Journal of Production Economics 27 (1992) 283–289.
[9] J.O. McClain, K.L. Schultz, L.J. Thomas, Management of
worksharing systems, Manufacturing and Service Operations Management 2 (1) (2000) 49–67.
[10] D.C. Montgomery, Design and Analysis of Experiments,
ﬁfth ed., John Willey and Sons, Inc., New York, 2001.
[11] E. Muth, The production rate of a series of work stations
with variable processing times, International Journal of
Production Research 11 (2) (1973) 155–169.
[12] J. Ostolaza, J.O. McClain, J. Thomas, The use of dynamic
state-dependent assembly-line balancing to improve
throughput, Journal of Manufacturing and Operation
Management 3 (1990) 105–133.
[13] R.G. Petersen, Design and Analysis of Experiments,
Marcel Dekker, Inc., New York, 1985.
[14] M.L. Spearman, D.L. Woodruﬀ, W.J. Hopp, CONWIP––
a pull alternative to Kanban, International Journal of
Production Research 28 (5) (1990) 879–894.
[15] M.P. Van Oyen, E.S. Gel, W.J. Hopp, Performance
opportunity for workforce agility in collaborative and
noncollaborative work systems, IIE Transactions 33 (2001)
761–777.

European Journal of Operational Research 159 (2004) 66–82
www.elsevier.com/locate/dsw

Discrete Optimization

Scheduling ﬂexible ﬂow lines with sequence-dependent
setup times
Mary E. Kurz
b

a,*

, Ronald G. Askin

b

a
Department of Industrial Engineering, Clemson University, Clemson, SC 29634-0920, USA
Department of Systems and Industrial Engineering, The University of Arizona, Tucson, AZ 85721-0020, USA

Received 16 October 2001; accepted 19 May 2003
Available online 30 August 2003

Abstract
This paper examines scheduling in ﬂexible ﬂow lines with sequence-dependent setup times to minimize makespan.
This type of manufacturing environment is found in industries such as printed circuit board and automobile manufacture. An integer program that incorporates these aspects of the problem is formulated and discussed. Because of the
diﬃculty in solving the IP directly, several heuristics are developed, based on greedy methods, ﬂow line methods, the
Insertion Heuristic for the Traveling Salesman Problem and the Random Keys Genetic Algorithm. Problem data is
generated in order to evaluate the heuristics. The characteristics are chosen to reﬂect those used by previous researchers.
A lower bound has been created in order to evaluate the heuristics, and is itself evaluated. An application of the
Random Keys Genetic Algorithm is found to be very eﬀective for the problems examined. Conclusions are then drawn
and areas for future research are identiﬁed.
Ó 2003 Elsevier B.V. All rights reserved.
Keywords: Scheduling; Heuristics; Genetic algorithms; Flexible ﬂow lines

1. Introduction
Traditional manufacturing systems have taken many general forms. In increasingly complex manufacturing environments, more complex manufacturing systems have been created in order to address such
factors as limited capacity and complicated process plans. For example, the semiconductor industry uses reentrant ﬂow lines, in which multiple machines may exist at each stage and jobs revisit previous stages many
times in a cyclic manner. The printed circuit board and automobile industries make use of ﬂow lines with
multiple machines at some stages and allow jobs to skip stages (Piramuthu et al., 1994; Agnetis et al., 1997).
Moreover, these industries encounter sequence-dependent setup times which result in even more diﬃcult
scheduling problems. The scheduling objective in such industries may vary. Due date related criteria may be

*

Corresponding author. Tel.: +1-864-6564652; fax: +1-864-6560795.
E-mail addresses: mkurz@clemson.edu (M.E. Kurz), ron@sie.arizona.edu (R.G. Askin).

0377-2217/$ - see front matter Ó 2003 Elsevier B.V. All rights reserved.
doi:10.1016/S0377-2217(03)00401-6

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

67

important. The makespan criterion has been used by many researchers and has been selected for this
research. Scheduling to minimize makespan in ﬂow lines with multiple parallel machines, jobs that may skip
stages, and sequence-dependent setup times is the focus of this paper. This kind of manufacturing environment introduces new diﬃculties that scheduling in simple ﬂow lines, for example, did not address.
To begin, we deﬁne the scope of the problem considered in this research. We use the term ‘‘hybrid’’ ﬂow
line to indicate ﬂow lines with the presence of multiple identical machines in parallel at some or all stages,
though jobs still require processing at exactly one machine per stage. A ﬂexible ﬂow line is a hybrid or
(regular) ﬂow line where at least one job need not be processed on any machines in at least one stage. That
is, every job must be processed on at most one machine per stage. A ﬂexible ﬂow line consists of several
stages in series. A job may not revisit a stage that it has already visited. Each stage has at least one machine,
and at least one stage must have more than one machine. At this point, this structure may be considered a
hybrid ﬂow line or a ﬂow line with multiple machines. However, the feature that makes our application a
ﬂexible ﬂow line is that jobs may skip stages. This could occur in an industry in which some jobs do not
require an operation. This situation exists in the printed circuit board manufacturing line modeled by
Wittrock (1985, 1988). Three of the thirteen part types required processing on only two of the three stages.
The potential variants of the basic ﬂexible ﬂow line described above that can be studied are nearly limitless. Now we shall describe the particular features of this research. All data in this problem are known
deterministically when scheduling is undertaken. Machines are available at all times, with no breakdowns or
scheduled or unscheduled maintenance. Jobs are always processed without error. Job processing cannot be
interrupted (no preemption is allowed) and jobs have no associated priority values. Inﬁnite buﬀers exist
between stages and before the ﬁrst stage and after the last stage; machines cannot be blocked because the
current job has nowhere to go. There is no travel time between stages; jobs are available for processing at a
stage immediately after completing processing at the previous stage. The ready time for each job is the larger
of 0 and the time it completes processing on the previous stage. Machines in parallel are identical in capability and processing rate. A key characteristic of this research topic is that non-anticipatory sequencedependent setup times exist between jobs at each stage. After completing processing of one job and before
beginning processing of the next job, some sort of setup must be performed. The length of time required to
do the setup depends on both the prior and the current job to be processed; that is, the setup times are
sequence-dependent. Piramuthu et al. (1994) incorporate sequence-dependent setup times in their model of
an actual printed circuit board line. Rios-Mercado and Bard (1998) note that sequence-dependent setup
times are found in the container manufacturing industry as well as the printed circuit board industry. The
formulations in Rios-Mercado and Bard (1998) indicate the assumption has been made that setup can only
be performed after the machine is no longer processing any job and the job for which setup is being performed is ready. The examples in Rios-Mercado and Bard (1999a,b) discussing the container manufacturing
industry state that the machines must be adjusted whenever the dimensions of the containers are changed,
which presumably cannot be done until the machine is idle. We follow this concept and require the machine
on which setup is to be performed to be idle and the job for which setup is required to be available as well.
This paper continues with a review of related research in Section 2. An integer programming model is
presented and described in Section 3. Lower bounds are developed in Section 4 for use in evaluating
schedules produced with the four heuristics described in Section 5. Using randomly generated test problems, described in Section 6, the heuristics are compared in Section 7. The quality of the lower bounds is
also discussed. Section 8 concludes the paper.

2. Literature review
This literature review will have one component regarding scheduling and a second regarding the random keys genetic algorithm. Scheduling in ﬂexible ﬂow lines and the less general hybrid ﬂow lines can be

68

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

organized by the approaches used to solve them. We will categorize approaches based on the use of branchand-bound, extensions of previous ﬂow line techniques, applications of metaheuristics and development of
new techniques.
Salvador (1973) ﬁrst considered multiple machines at serial stages (hybrid ﬂow line) with no buﬀers
between stages. Branch-and-bound techniques are applied to determine the optimal permutation schedule
in terms of makespan. Brah and Hunsucker (1991) used branch-and-bound in the hybrid ﬂow shop with an
arbitrary number of stages and intermediate buﬀers. Moreover, they provide a means by which nonpermutation schedules or schedules with inserted idle time can be created. Rajendran and Chaudhuri (1992)
also utilized branch-and-bound but restricted the resultant schedule to the set of permutation schedules.
Santos et al. (1995) combined permutation schedules with the FIFO queuing discipline at all stages after the
ﬁrst stage. The jobs enter the line according to one of the n! permutations and then begin processing at
every stage thereafter in the order they completed processing at the previous stage. The best permutation
solution is found by considering every possible permutation of jobs that can enter the line, tempered by the
introduction of a lower bound on the optimal makespan.
Extending heuristics developed for the single line ﬂowshop has been considered by numerous authors.
Sriskandarajah and Sethi (1989) examined worst case performance for various heuristics based on
JohnsonÕs Rule applied to two stage hybrid ﬂow shops. Lee and Vairaktarakis (1994) developed heuristics
for multistage hybrid ﬂow shops by extending results for a two stage hybrid ﬂow shop and aggregating
machines at each stage. JohnsonÕs Rule was also applied by Gupta (1997) to the case with one machine in
the ﬁrst stage and multiple machines in the second stage. Oguz et al. (1997) examined a three stage ﬂexible
ﬂow line with one machine at each stage where the diﬀerence in job routings were handled as diﬀerent job
types (jobs visit either stages 1 and 3 or 2 and 3). JohnsonÕs Rule was used for each of the types of jobs.
Ding and Kittichatphayak (1994) modeled the hybrid ﬂow line as an extension of a single line ﬂow shop,
adapting Campbell et al.Õs (1970) method for ﬂow shops or placing jobs at the end of the current sequence
considering the idle time of the machines. Riane et al. (1998) considered a three stage hybrid ﬂow shop with
two machines at the second stage and one at each of the other stages. They developed a dynamic programming-based heuristic based on the Campbell et al. heuristic for single ﬂow lines and a branch-andbound heuristic. Santos et al. (1996) also considered the use of heuristics developed for the single ﬂow shop
case as a method to generate an initial permutation schedule that would then be followed by the application
of FIFO. Brah and Loo (1999) evaluated how heuristics developed for the single ﬂow line case would
perform in the hybrid environment.
Nowicki and Smutnicki (1998) built on their prior experience with tabu search in single machine (Nowicki and Smutnicki, 1994) and ﬂow shop (Nowicki and Smutnicki, 1996) scheduling. Robust
local search improvement techniques for ﬂexible ﬂow line scheduling were considered by Leon and
Ramamoorthy (1997). Rather than considering neighborhoods of the schedules, they considered neighborhoods of the problem data. The perturbed data is then used by a problem speciﬁc heuristic to generate a
solution whose quality is assessed using the original problem data. Lee et al. (1997) have applied genetic
algorithms to the joint problem of determining lot sizes and sequence to minimize makespan in ﬂexible ﬂow
lines. Though this research included sequence-dependent setup times, buﬀers between stages were limited
and jobs must be processed in the same order on the machines. Combining genetic algorithms with simulated annealing was also considered.
The examination of ﬂexible ﬂow lines as deﬁned in this paper and the development of heuristics speciﬁcally for this problem began with Wittrock (1985, 1988). Kochhar and Morris (1987) model ﬂexible ﬂow
lines in a more complete manner in that they allow for setups between jobs, ﬁnite buﬀers which may cause
blocking, and machine down-time. They extend a Wittrock algorithm and evaluate several policies with a
deterministic simulation. Sawik (1992) has developed numerous results for the ﬂexible ﬂow line scheduling
problem. The basic model includes factors such as transportation time between stages and non-zero release
times. However, sequence-dependent setup times are not included. The Route Idle Time Minimization

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

69

(RITM) heuristic aims to minimize makespan by minimizing the idle time of the machines. In this case,
buﬀers are limited in size so that blocking can occur. Later, Sawik (1994, 1995) extended the RITM heuristic to the case of no buﬀers between stages.
While many papers have been written in the area of scheduling hybrid and ﬂexible ﬂow lines, many of
them are restricted to special cases of two stages or speciﬁc conﬁgurations of machines at stages. There does
not seem to be any published work addressing heuristics for ﬂexible ﬂow lines (multiple serial stages that
need not have the same number of machines per stage and jobs that need not visit all stages) with sequencedependent setup times. Rios-Mercado and Bard (1998, 1999a,b) do consider ﬂow shops with sequencedependent setup times in several papers, but these papers all require exactly one machine per stage and
permutation schedules (all jobs visit each stage in the same order).
Genetic algorithms were introduced by Holland (1975). He provided the basic framework for genetic
algorithms: chromosomes represent solutions which reproduce based on how well they solve the problem at
hand in a manner analogous to survival of the ﬁttest. A key feature of genetic algorithms is their randomness. Chromosomes are chosen to reproduce randomly and experience changes based on random
chance, as organisms do in the natural environment. Theoretically, the best chromosome will survive to the
ﬁnal generation of chromosomes. The chromosomal representation of the scheduling information can take
many forms and inﬂuence the types of genetic operators. A common problem for combinatorial applications of genetic algorithms is that some operations may create feasibility problems. Bean (1994) has introduced an alternative method to encode problem solutions using random numbers called a Random Keys
Genetic Algorithm (RKGA), which has been applied to resource allocation problems, quadratic assignment
problems, multiple machine tardiness scheduling problems, jobshop makespan minimization problems and
the generalized traveling salesman problem (Bean, 1994; Norman and Bean, 1999; Snyder and Daskin,
2001). RKGA will be discussed further in a later section.

3. Integer programming model
The problem addressed in this research can be expressed formally as an integer program. Let g be the
number of stages. Let n be the number of jobs to be scheduled and mt be the number of machines at stage t.
We assume that machines are initially setup for a nominal job 0 and must ﬁnish setup for a tear down job
n þ 1 at every stage. We have the following deﬁnitions.
n
g
gj
mt
pit
stij
Si
St
cti
xtij

number of true jobs to be scheduled
number of serial stages
last stage visited by job j
number of machines at stage t
processing time for job i at stage t (assumed to be integral)
setup time from job i to job j at stage t
set of stages visited by job i
set of jobs that visit stage t ¼ fi : pit > 0g
completion time for job i at stage t
1 if job i is scheduled immediately before job j at stage t and 0 otherwise

The processing times of jobs 0 and n þ 1 are set at 0 and the setup times are times to move from and to
the nominal set point state. We assume that all jobs currently in the system must be completed at each stage
before the jobs requiring scheduling may begin setup. The completion times of job 0 at each machine at
each stage are set to the earliest time setup may begin at that stage. We include the restriction that every
stage must be visited by at least as many jobs as there are machines in that stage. This is expressed by the

70

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

inequality jS t j P mt , t ¼ 1; 2; . . . ; g, so n P maxt¼1;2;...;g fmt g. If a stage is visited by fewer jobs than there are
machines, there is not a diﬃcult sequencing decision to be made, because each job could be assigned its own
machine. The formulation also assumes that a job that does not visit a stage has a processing time of 0.
That is, pit ¼ 0 if i 62 S t . Additionally, we assume pit P 1 if i 2 S t . The formulation becomes
P : min

ð1Þ

z;
n
X

s:t:

xtoj ¼ mt ;

t ¼ 1; . . . ; g;

ð2Þ

j¼1

X

xtij ¼ 1;

i ¼ 1; . . . ; n; t 2 Si ;

ð3Þ

j2fS t ;nþ1g

X

xtij ¼ 1;

j ¼ 1; . . . ; n; t 2 Si ;

ð4Þ

i2f0;S t g

ctj 
 cti þ M t ð1 
 xtij Þ P stij þ pjt ;
(
(

ctj 
 cjt
1 þ Mjt ð1 
 xtij Þ P stij þ pjt ;
xtij
xtji

6 pjt ;
6 pjt ;

i ¼ 0; . . . ; n; j ¼ 1; . . . ; n; t 2 Si ;
i ¼ 0; . . . ; n; j ¼ 1; . . . ; n; t 2 Si 
 f1g;

i; j 2 f0; 1; . . . ; n; n þ 1g; t ¼ 1; . . . ; g;
j ¼ 1; . . . ; n;

ctj 
 cjt
1 P M t pjt ;

j ¼ 1; . . . ; n; t ¼ 2; . . . ; g

xtij
xtij

j ¼ 1; . . . ; n; t ¼ 1; . . . ; g;

g
z P cj j ;

j ¼ 1; . . . ; n;

2 f0; 1g;
¼ 0;

ð7Þ
ð8Þ

;

ð9Þ
ð10Þ

i; j 2 f0; 1; . . . ; n; n þ 1g; t ¼ 1; . . . ; g;

ð11Þ

i ¼ j; t ¼ 1; . . . ; g;

ctj P 0;

ð6Þ

)

c1j 
 c10 P M t pj1 ;
ctj P ct0 ;

ð5Þ

j ¼ 0; . . . ; n; t ¼ 1; . . . ; g:

ð12Þ

This formulation is based on the TSP. Each stage exists independently except that stage tÕs completion
times are stage t þ 1Õs ready times. Eq. (1) deﬁnes the objective function which is to minimize the makespan
z. We assume setup times satisfy some suﬃcient condition to ensure that an optimal solution will always
exist that utilizes all mt machines at each stage, as in the single stage case. For instance,
st0j 6 min ðpit þ stij Þ
i6¼0

8j

and
P
pjt þ st0j 6

i6¼j

ðpit þ mink stki Þ
mt 
 1

8j

are both suﬃcient. Constraint set (2) ensures that mt machines are scheduled in each stage. Constraint sets
(3) and (4) ensure that each job is scheduled on one and only one machine in each stage. Constraint set (5)
forces job j to follow job i by at least iÕs processing time plus the setup time from i to j if i is immediately
before j. The value M t is an upper bound on the time stage t completes processing, similar to the upper
bound A in Rios-Mercado and Bard (1998).


n 
n 
X
X
1
1
1
t
t
1
t
t
pi þ max sji
pi þ max sji :
M ¼
and M ¼ M þ
i¼1

j2f0;...;ng

i¼1

j2f0;...;ng

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

71

Constraint set (6) forces job j at stage t to complete after it completes at stage t 
 1, plus its processing time
at stage t, plus the setup time from its predecessor to j. The value Mjt is set to Mjt ¼ maxi ðstij Þ þ pjt . Constraint sets (5) and (6) together ensure that a job cannot begin setup until it is available (done at the
previous stage) and the previous job at the current stage is complete. Constraint sets (5) and (6) also serve as
sub-tour elimination constraints. Constraint set (7) ensures that jobs that do not visit a stage are not assigned to that stage. Constraint sets (8) and (9) ensure that the completion time of a job at stage t, that does
not visit stage t, is set to the jobÕs completion time at stage t 
 1. The value M t is an upper bound on the
time stage t completes processing and is the same as that used in constraint set (5). Constraint set (10) links
the decision variables cgj and z. Constraint sets (11) and (12) provide limits on the decision variables.
Due to the fact that each machine at each stage is a TSP once jobs have been assigned to the machine,
this problem is NP-hard. In this research, we need not only sequence jobs on machines, we must consider
which jobs are to be assigned to the machines. Fourteen small problems were considered in order to
evaluate the feasibility of solving this MIP directly. Each problem has integer processing times selected
from a uniform distribution between 50 and 70 and integer setup times selected from a uniform distribution
between 12 and 24. Table 1 contains additional problem characteristics. Problems 1–9 all have 1 machine at
every stage. Problems 10–14 have stages with diﬀerent numbers of machines at each stage; the number of
machines at each stage is shown in order. Problems 4, 5, 11 and 12 have diﬀerent sets of jobs visiting each
stage; the number of jobs that visit each stage is shown. In the other problems, all jobs visit all stages. These
problems have been solved in CPLEX 7.5 on a Sun Microsystems Enterprise 6000 with UltraSPARC-II 336
MHz cpus and 4.0 GB of memory. Each problem was allowed a maximum of 7200 seconds of CPU time
(two hours) using the CPLEX setting ‘‘set timelimit 7200’’. Of these, only two were solved to optimality in
the two hour time limit. These problems both had 2 stages, 1 machine per stage and 6 jobs which visited
both stages. They were solved in 810.37 and 695.24 seconds respectively. The other twelve problems were
stopped due to the time limit before ﬁnding an optimal integer solution, and in three cases, stopped before
ﬁnding an integer feasible answer. Table 1Õs ﬁnal column contains summary information from the attempted solution with CPLEX.
Table 1Õs contents indicate that a quite sizable gap still exists for most of these problems after a fairly
lengthy amount of time. The only problems that were solved to optimality are very small. Heuristic approaches will therefore be used in this research.

Table 1
Problems given to CPLEX
Problem

Number of stages

Number of machines per
stage

Number of jobs per
stage

CPLEX results

1
2
3
4
5

2
2
2
2
2

1
1
1
1
1

6
30
30
29/29
99/95

Optimal solution found
Gap: 97.61%
Optimal solution found
Gap: 98.26%
Gap: inﬁnite

6
7
8
9
10

4
4
8
8
2

1
1
1
1
5/10

6
30
6
30
30

Gap:
Gap:
Gap:
Gap:
Gap:

65.05%
98.73%
88.32%
inﬁnite
86.78%

11
12
13
14

2
2
4
8

7/8
3/7
1/10/10/7
10/3/7/9/3/1/9/10

28/30
95/91
30
30

Gap:
Gap:
Gap:
Gap:

87.47%
inﬁnite
95.14%
95.68%

72

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

4. Lower bounds
This section contains a theorem with three lower bounds for the problem P . The notation ‘‘min½k
 ’’ will be
used to indicate the (k þ 1)st from the lowest value and min½0
  min. For example, given a list of values {2,
5, 7, 8, 9}, min½1
 ¼ 5.
Theorem. The following are lower bounds on any feasible solution to P :
)
(
X
ð1Þ
ðpit þ min stji Þ ;
LB ¼ max
i¼1;...;n

(
LB

ð2Þ

¼ max

t¼1;...;g

j¼0;...;n

t2Si

P

g
X
ðpit þ minj¼0;...;n stji Þ
þ
min
ðpis þ min ssji Þ
t
j¼0;...;n
j¼0;...;n
i2S
i2S t
m
s¼1
s¼tþ1
#)
"
t

1
t
1
t
1
X
X
1 mX
s
s
þ t
min
ðp
s
Þ


min
ðpis þ min ssji Þ :
þ
min
i
ji
j¼0;...;n
j¼0;...;n
i2S t
m k¼1 i2S t ½k
 s¼1
s¼1

mint

t
1
X

ðpis

þ min

ssji Þ

þ

ð13Þ

i2S t

ð14Þ

Proof
LB(1): This is a job-based bound. Every job i must be processed at each stage and must also be setup, which
requires at least the minimal amount of time required to setup job i. Solutions which are feasible to
constraint sets (5) and (6) satisfy this condition.
LB(2): This is a machine based bound. Every stage t needs time to process job 0 and then the preemptive
processing and minimal setup time for the rest of the jobs. In addition to minimum setup and processing at each stage, we can add in the minimum time to get to the stage plus the minimum time to
ﬁnish after the stage. Furthermore, we may be able to bound idle time for parallel machines at each
stage waiting for the ﬁrst available job. This yields the bound LBð3Þ originally proposed in Kurz and
Askin (2001) as an extension to Leon and Ramamoorthy (1997). The ﬁrst term represents the minimum time required for a job to reach stage t. The second term assumes that jobs are processed preemptively at stage t. The third term represents the minimum time for a job to ﬁnish processing and
setup on the stages after t. The ﬁnal term requires the observation that the second machine at stage t
does not begin processing until the second job arrives, and so on for all the machines at stage t. We
ﬁnd the minimum times for the second, third, etc. jobs to reach stage t and allocate this time to all
the machines at this stage. Solutions which are feasible to constraint sets (5) and (6) satisfy this condition. h

5. Heuristics
The ﬁrst heuristic is a na€ıve approach that simply assigns jobs to machines in a greedy fashion. The
second expands on a multiple machine insertion heuristic used in previous work done on the single stage
problem, in order to take advantage of the sequence-dependent nature of the setup times. The third is based
on JohnsonÕs Rule. The fourth is an application of the random keys genetic algorithm. Note that the second
caters to setup aspects of the problem while the third derives from standard ﬂow shops. No restrictions on
the form of the resultant schedules is made.
Let [i] indicate the ith job in an ordered sequence in the following. In many of the following heuristics, a
modiﬁed processing time is used. It is denoted by p~it for job i in stage t and is deﬁned as p~it ¼ pit þ minj stji .
This time represents the minimum time at a stage t that must elapse before job i could be completed.

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

73

5.1. SPT cyclic
This is a na€ıve greedy heuristic that assign jobs to machines with little or no regard for setup times or the
interactions between stages. Because of its simplistic nature, it provides a basis of comparison for the other
heuristics. In the SPT Cyclic Heuristic (SPTCH), the jobs are ordered at stage 1 in increasing order of the
modiﬁed processing times p~i1 . At subsequent stages, jobs are assigned in earliest ready time order. Jobs are
assigned to the machine in every stage that allows it to complete at the earliest time as measured in a greedy
fashion.
Create the modiﬁed processing times p~i1 .
Order the jobs in non-decreasing order (SPT) of p~i1 .
At each stage t ¼ 1; . . . ; g, assign job 0 to each machine in that stage.
For stage 1:
a. Let bestmc ¼ 1.
b. For ½i
 ¼ 1 to n, i 2 S 1 :
For mc ¼ 1 to m1 :
Place job [i] last on machine mc.
Find the completion time of job [i]. If this time is less on mc than on bestmc,
let bestmc ¼ mc.
Assign job [i] to the last position on machine bestmc.
5. For each stage t ¼ 2; . . . ; g:
a. Update the ready times in stage t to be the completion times in stage t 
 1.
b. Arrange jobs in increasing order of ready times.
c. Let bestmc ¼ 1.
d. For ½i
 ¼ 1 to n, i 2 S t :
For mc ¼ 1 to mt :
Place job [i] last on machine mc.
Find the completion time of job [i]. If this time is less on mc than on bestmc,
let bestmc ¼ mc.
Assign job [i] to the last position on machine bestmc.
1.
2.
3.
4.

5.2. Flowtime Multiple Insertion Heuristic
The Flowtime Multiple Insertion Heuristic (FTMIH) is a multiple insertion heuristic to minimize the
sum of ﬂowtimes (completion-ready times) at each stage. It is a multiple machine, multiple stage adaptation
of the Insertion Heuristic for the TSP. This multiple insertion heuristic with completion time criteria was
found to be eﬀective for the single stage case. Setup times are accounted for by integrating their values into
the processing times using p~it . The Insertion Heuristic can then be performed using these modiﬁed processing times at each stage. Once jobs have been assigned to machines, the true processing and setup times
can be used. The FTMIH has the following steps for each stage t:
1. Create the modiﬁed processing times p~it .
2. Order the jobs in non-increasing order (LPT) of p~it .
3. For ½i
 ¼ 1 to n, i 2 S t :
a. Insert job [i] into every position on each machine.
b. Calculate the true sum of ﬂowtimes using the actual setup times.
c. Place job i in the position on the machine with the lowest resultant sum of ﬂowtimes.
4. Update the ready times in stage t þ 1 to be the completion times in stage t.

74

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

5.3. The g=2, g=2 Johnson’s Rule
JohnsonÕs Rule (1954) ﬁnds the optimal makespan solution for F 2kCmax . Variants have been created, for
example by Campbell et al. (1970), for the ﬂow shop with more than two stages. This heuristic is an extension of JohnsonÕs Rule to take into account the setup times. The aggregated ﬁrst half of the stages and
the aggregated last half of the stages are considered to create the order for assignment in stage 1. The value
p~i1 is the sum of modiﬁed processing times for stages 1 to bg=2c and p~ig is the sum over stages bg=2c þ 1 to g.
1. Create the modiﬁed processing times p~i1 and p~ig .
2. Let U ¼ fjj~
pj1 < p~jg g and V ¼ fjj~
pj1 P p~jg g.
3. Arrange jobs in U in non-decreasing order of p~i1 and arrange jobs in V in non-increasing order of p~ig .
Append the ordered list V to the end of U .
4. At each stage t ¼ 1; . . . ; g, assign job 0 to each machine in that stage.
5. For ½i
 ¼ 1 to n, i 2 S 1 :
a. For mc ¼ 1 to m1 :
Place job [i] last on machine mc.
If this placement results in the lowest completion time for job ½i
, let m ¼ mc.
b. Place job [i] last on machine m.
6. For each stage t ¼ 2; . . . ; g:
a. Update the ready times in stage t to be the completion times in stage t 
 1.
b. Arrange jobs in increasing order of ready times.
c. For ½i
 ¼ 1 to n, i 2 S t :
(1) For mc ¼ 1 to mt :
Place job [i] last on machine mc.
If this placement results in the lowest completion time for job [i], let m ¼ mc.
(2) Place job [i] last on machine m.
5.4. Random keys genetic algorithm
RKGA diﬀers from traditional genetic algorithms most notably in the solution representation. Random
numbers serve as sort keys in order to decode the solution. The decoded solution is evaluated with a ﬁtness
function that is appropriate for the problem at hand. For example, Norman and Bean (1999) suggest using
the following solution representation for an identical multiple machine problem. Each job is assigned a real
number whose integer part is the machine number to which the job is assigned and whose fractional part is
used to sort the jobs assigned to each machine. Once the job assignments and order on each machine are
found through the decoding, a schedule can be built incorporating additional factors such as non-zero
ready times and sequence-dependent setup times. The desired performance measure can then be found
using the schedule. In this research, this representation is used for the jobs in the ﬁrst stage. The assignment
of jobs to machines in subsequent stages follows the method used in SPTCH and the JohnsonÕs Rule Based
Heuristics, where each job is assigned to the machine that allows it to complete at the earliest time as
measured in a greedy fashion.
The genetic operators and related parameters used in this research are based on those in Bean (1994).
Each generation has a population of 100 chromosomes. The initial population is generated randomly. An
elitist strategy is used for reproduction. Each chromosome is decoded and the resulting schedule is evaluated for the makespan. Chromosomes with lower makespans are more desirable, so 20% of the chromosomes with the lowest makespan values are automatically copied to the next generation. Parametrized
uniform crossover is used to select 79% of the chromosomes in the next generation. For each chromosome
in the next generation, the following is performed. Two chromosomes in the current generation are selected

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

Job
(a)

Parent 1
Parent 2

1
1.23

2
2.03

3
1.45

2.15

2.45

1.85

2.89
1.03

75

4

(b)

Crossover

0.45

0.23

0.68

0.75

(c)

Child

1.23

2.03

1.45

1.03

Fig. 1. Parametrized uniform crossover example.

at random. For each job, a random number is generated. If the value is less than 0.7 (following Bean, 1994),
the value from the ‘‘ﬁrst’’ chromosome is copied to the new chromosome, otherwise the value from the
‘‘second’’ chromosome is selected. The remaining 1% of the next generation is ﬁlled through ‘‘immigration’’, in which new chromosomes are randomly generated. The above procedures are repeated until we
are fairly sure that the population has settled into a good location in the search space. In this research, we
continue until 100 generations have been examined without ﬁnding an improved schedule. This value was
selected empirically.
A small example of the parametrized uniform crossover technique is provided in Fig. 1 to illustrate the
structure of the chromosome as well as the parametrized uniform crossover technique itself. Consider a 4
job, single stage, 2 machine problem. Two parent chromosomes have been selected in Fig. 1(a). The ﬁrst
chromosome tells us that there is a schedule where machine 1 has jobs 1 and 3, in that order and machine 2
has jobs 2 and 4, in that order. The second parent chromosome tells us that there is a schedule where
machine 1 has jobs 4 and 3, in that order and machine 2 has jobs 1 and 2, in that order. This information,
when combined with the other problem data such as processing time, ready times, etc, is used to determine
when each job completes processing on the machines for each schedule. The parametrized uniform
crossover technique requires four random numbers to be generated, as shown in Fig. 1(b). The ﬁrst three
random numbers tell us to copy the genes from Parent 1 to the resulting child and the last random number
tells us to copy the genes from Parent 2 to the resulting child. The resulting child is shown in Fig. 1(c). The
child chromosome is decoded to tell us that there is a schedule where machine 1 has jobs 4, 1 and 3, in that
order and machine 2 has job 2 only.

6. Generation of test data
An experiment was conducted to test the performance of the heuristics. Integer data was generated. Data
required for a problem consist of the number of jobs, range of processing times, number of stages and
whether all stages have the same number of machines or not. Each stage requires data deﬁning how many
machines exist at that stage, the sequence dependent setup times, the processing times and the ready times.
The ready times for stage 1 are set to 0 for all jobs. The ready times at stage t þ 1 are the completion times
at stage t, so this data need not be generated. Processing times are distributed uniformly over two ranges
with a mean of 60:[50–70] and [20–100]. Flexible ﬂow lines are considered by allowing some jobs to skip
some stages. Following Leon and Ramamoorthy (1997), the probability of skipping a stage is set at 0, 0.05,
or 0.40. The setup times are uniformly distributed from 12 to 24 which is 20% to 40% of the mean of the
processing time. The setup time matrices are asymmetric and satisfy the triangle inequality. The setup time
characteristics follow Rios-Mercado and Bard (1998).
The problem data can be characterized by six factors: the probability that a job skips a stage, range of
processing times, number of stages, whether the number of machines per stage is constant or variable, range

76

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

Table 2
Factor levels
Factor

Levels

Skipping probability

0.00
0.05
0.40

Processing times

Unif (50–70)
Unif (20–100)

Number of stages

2
4
8

Machine distribution

Constant
Variable
Depends on machine distribution
Constant

Number of machines

1
2
10

Number of jobs

6
30
100

Variable
Unif(1,4)
Unif(1,10)

in number of machines per stage and number of jobs. Each of these factors can have at least two levels.
These levels are shown in Table 2.
In general, all combinations of these levels will be tested. However, some further restrictions are introduced. The variable machine distribution factor requires that at least one stage have a diﬀerent number
of machines than the others. Also, the largest number of machines in a stage must be less than the number
of jobs. Thus, the combination with 10 machines at each stage and 6 jobs will be skipped and the combination of 1–10 machines per stage with 6 jobs will be changed to 1–6 machines per stage with 6 jobs.
There are 252 test scenarios and ten data sets are generated for each one.

7. Experimental results
This section discusses the eﬀectiveness of the proposed lower bounds and the proposed construction
heuristics. The heuristics were implemented in C, compiled with Microsoft Visual C++ and run on a PC
with a Pentium III 800 MHz processor with 512 MB of RAM. ‘‘Loss’’ is the (makespan ) lower bound)/
lower bound. The best lower bound was used for each problem. The running times were found using the
clock() function.
7.1. Comparing heuristics
Every heuristic considered here was run on the same 2520 data sets. RKGA was run 16 times and the
minimum and average loss over the 16 runs were found for each of the 2520 data sets. Summary statistics
over all the data sets are presented in Table 3. RKGA achieves the lowest values for the loss statistics and
ﬁnds the minimum loss schedules many more times than the other heuristics. The variation seen within the
16 RKGA runs will be discussed later. A single factor ANOVA for the algorithm (ALGO$) was performed,

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

77

Table 3
Loss statistics for heuristics
Heuristic
SPTCH
FTMIH
g=2, g=2 JohnsonÕs
RKGA
a

Loss

Number of times

Average

Standard deviation

Maximum

Minimuma

0.25
0.24
0.21
0.16

0.16
0.15
0.13
0.11

0.98
0.94
0.79
0.60

64
180
123
2386

Using the best of the 16 RKGA runs.

Table 4
ANOVA results
Source

Sum of squares

df

Mean square

F -ratio

P

ALGO
Error

13.682
201.497

3
10076

4.561
0.020

228.033

0.000

and the results are shown in Table 4. These results indicate that there is at least one heuristic that is diﬀerent
in mean response. This motivated the use of FisherÕs least signiﬁcant diﬀerence method (Montogomery and
Runger, 2003), with the results summarized in Table 5. These values indicate that RKGA is preferred with
99% conﬁdence.
SPTCH and the g=2, g=2 JohnsonÕs Based Rule never required more than 0.01 seconds of CPU time in
these experiments. FTMIH required up to 25 seconds and RKGA required more time before stopping for
every single one of the 2520 problem instances, up to 380 seconds for some larger problems. The results of
the FisherÕs least signiﬁcant diﬀerence method indicates that RKGA is better than the g=2, g=2 JohnsonÕs
Based Rule in general, but the larger running time of RKGA induces the question of when RKGA should
not be used, and instead perhaps the g=2, g=2 JohnsonÕs Based Rule should be used. The low running time
of the heuristics other than RKGA certainly indicate that perhaps they could be run in the initialization
step of RKGA to provide three potentially good solutions to the ﬁrst generation.
RKGA generally dominates the g=2, g=2 JohnsonÕs Based Rule in loss performance, with the g=2, g=2
JohnsonÕs Based Rule only outperforming the best of 16 runs of RKGA 15 of the 2520 problem instances.
These ﬁfteen instances represent six diﬀerent conﬁgurations, as shown in Table 6. Every one of these ﬁfteen
instances involved problems with exactly 10 machines per stage, eleven involved problems with two stages
and twelve involved problems with the smaller range of processing times. This is reasonable because this
kind of problem somewhat matches the scenario for which JohnsonÕs Rule was designed. In this case, we
have a two stage ﬂow line and each job must visit each stage. However, instead of only one machine for
each stage, we have ten. This means that a tie in JohnsonÕs order does not force a job that should be ﬁrst on

Table 5
FisherÕs least signiﬁcant diﬀerence method
Heuristics

Diﬀerence of means

Signiﬁcant diﬀerence at 99% level? (Yes if diﬀerence > 0.01)

SPTCH vs RKGA
SPTCH vs g=2, g=2
SPTCH vs FTMIH
FTMIH vs RKGA
FTMIH vs g=2, g=2
g=2, g=2 vs RKGA

0.09
0.04
0.01
0.08
0.03
0.05

Yes
Yes
No
Yes
Yes
Yes

78

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

Table 6
Problems where g=2, g=2 JohnsonÕs based rule outperforms RKGA
Skipping
probability

Processing time
range

Number of stages

Number of machines per stage

Number of jobs

Number of ﬁles

0.00
0.00
0.00
0.00
0.05
0.05

50–70
50–70
20–100
20–100
50–70
50–70

2
2
2
2
2
4

10
10
10
10
10
10

30
100
30
100
100
100

1
5
2
1
2
4

a machine to be second; the job can be ﬁrst on another machine at that stage. In this way, we allow several
jobs that are early in the JohnsonÕs order to move towards the beginning of the schedule on the ﬁrst stage.
In the second stage, by considering jobs in ready time order and allowing jobs to be placed on the machine
on which it ends soonest, we continue with the JohnsonÕs paradigm. We allow jobs that arrive to the second
stage and will move through it quickly (before later jobs from stage 1) to be scheduled earlier. These jobs
should not impact the overall makespan much because the later jobs in stage 1 will not arrive in stage 2 very
soon regardless. The low range of processing times and the fact that all jobs visit both stages indicates that
the stages are fairly balanced in terms of workload.
7.2. Discussion of RKGA runs
It has been noted before that 16 RKGA runs were made of the 2520 problem instances, with an average
loss (that is, relative deviation from the lower bound) of 0.16. Since RKGA is a stochastic method, meaning
that each run may not provide the same answer, this section intends to discuss the variation seen in the 16
RKGA runs. The average loss of 16 runs varies from 0, meaning all 16 runs attained the lower bound, to
0.5962, meaning the average deviation from the lower bound for that problem instance was 59.62%. The
values of the best RKGA run in the set of 16 varies from 0 to 0.5759. The standard deviation of the 16
RKGA runs has been calculated for each problem instance as well and varies from 0 to 0.0792. The width
of a 99% conﬁdence interval on the mean loss, with the variance unknown, has been calculated for each
problem instance as well, and varies from 0 to 0.1119. Fig. 2 plots the loss values of the best vs worst
RKGA runs. The points plotted exactly on the x ¼ y line correspond to the runs with 0 variance. The graph

Best and Worst RKGA Loss
0.7

Worst RKGA Loss

0.6
0.5
0.4
0.3
0.2
0.1
0
0

0.1

0.2
0.3
0.4
Best RKGA Loss

0.5

0.6

Fig. 2. Best vs worst RKGA performance over 16 runs.

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

79

illustrates that the RKGA is settling into a fairly consistent solution value, though that value sometimes
appears to be far from the lower bound.
7.3. Quality of the lower bounds
Two lower bounds were presented in a previous section. Recall that LBð1Þ is the job based bound, giving
the maximum time over all jobs to process and minimally setup each job in all stages. LBð2Þ is the machine
based bound, giving the maximum time over all stages to preemptively process and minimally setup all jobs
in that stage, plus the time to reach each stage. While LBð2Þ is the larger 2250 out of 2520 times, LBð1Þ is
larger 270 times. Since LBð1Þ is easy to compute, it should be included even though it is not as good a lower
bound in general as LBð2Þ .
It is noted that RKGA occasionally found solutions very close to the lower bound LBð2Þ . Of the 2520 test
ﬁles, the best RKGA solution (of the 16 runs) was equivalent to the lower bound 3 times and within 1% of
the lower bound 29 additional times. Of the 40320 individual RKGA runs, the lower bound was hit 44
times and RKGA was within 1% of the lower bound 344 additional times.
A comparison between the lower bound and optimal solution of each of these problems could be made,
if these problems were easily solved to proven optimality. However, we have discussed this diﬃculty earlier.
We compare the value of LBð2Þ for the 14 speciﬁc problems discussed earlier to the best value found by our
heuristics (always by RKGA in these cases) and to the values returned by CPLEX, when CPLEX was given
a two hour CPU time limit. These comparisons are summarized in Table 7.
Recall that problems 1 and 3 were solved to optimality within this time limit and no integer feasible
solutions to problems 5, 9 and 12 were found within this time limit. Because the RKGA runs and the
CPLEX runs were performed on diﬀerent machine types, it seems diﬃcult to compare the running times.
However, we note that the longest running time of any of the RKGA runs (not just the 14 discussed here)
on a 800 MHz was about 380 seconds. Whenever CPLEX was unable to solve the problem to optimality, it
provided a current MIP best bound, which was always much lower than the value of LBð2Þ . In the two cases
where CPLEX did solve the problem to optimality, the current MIP best bound was higher than LBð2Þ , but
not much higher. Of course, having LBð2Þ be the same as the optimal solution is ideal, but not always
attainable. This as well as the discussion above regarding the performance of RKGA lead us to conclude
that LBð2Þ is an eﬀective lower bound.
Table 7
Eﬀectiveness of LBð2Þ
Problem

LBð2Þ

Best heuristic solution
value

Current MIP best
bound

Best integer feasible solution

1
2
3
4
5
6
7
8
9
10
11
12
13
14

515
2222
533
1935
6874
669
2384
980
2689
519
384
2056
2355
2640

522
2272
543
2191
7236
708
2489
1040
2876
544
407
2407
2384
2715

521.95
89.00
542.95
604.93
499.39
363.51
1082.78
293.00
192.16
113.00
109.00
60.99
149.00
247.01

522
3721
543
3485
None
1040
8508
2509
None
855
870
None
3065
5720

Note: Problems (1) and (3) were solved to optimality.

80

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

8. Conclusions and future work
This paper has examined four heuristics to ﬁnd schedules minimizing makespan in ﬂexible ﬂow lines with
sequence-dependent setups. These methods included a simplistic greedy method, approaches based on the
TSP nature of the problem and the ﬂow line nature of the problem and an application of the Random Keys
Genetic Algorithm approaches. Lower bounds have been used in the evaluation of these heuristics. The
lower bounds were investigated for performance and found to be eﬃcient. The data characteristics investigated were designed to reﬂect characteristics used by other researchers, resulting in 252 types of data
ﬁles, with 10 of each type generated. The heuristics were compared on 2520 data ﬁles. Through examination
of the experimental results, it was determined that RKGA performed best on the problems examined here.
However, in the speciﬁc situation of two stages, jobs that visit each stage and 10 machines per stage, the
g=2, g=2 JohnsonÕs Based Heuristic was found to be eﬀective as well. Intuition has been provided to explain
this result.
There are potentially unlimited opportunities for research in scheduling to minimize makespan in ﬂexible
ﬂow lines with sequence-dependent setup times. In this paper, we have addressed only a few areas. The
research in this paper has lead to many more questions regarding ﬂexible ﬂow lines with sequencedependent setup times. For example, we wonder whether there a deﬁnition of a permutation schedule that is
general enough to handle multiple machines in serial stages where not all jobs visit each stage and sequencedependent setup times exist? Potential deﬁnitions may include a condition that cannot be violated, but does
not tell explicitly what all the permutations may look like. For example, a permutation schedule may be one
in which the relative ordering of start of setup times at each stage does not change from stage to stage. This
deﬁnition in no way tells us what jobs should be assigned to what machine in stages with multiple machines
and in fact allows for several alternative assignments at each stage. However, this deﬁnition also includes
the traditional deﬁnition of a permutation schedule in a ﬂow line. By creating a general permutation
schedule deﬁnition, we may be able to ﬁnd a class of schedules that contains the optimal makespan schedule
for some special case, such as two stages with one machine at the ﬁrst stage and two machines at the second,
with sequence-dependent setup times at both.
We note that the integer programming formulation for the ﬂexible ﬂow line with sequence-dependent
setup times has not been thoroughly investigated. It has been solved for some speciﬁc cases, but its performance on much larger problems is unknown. Work applying branch-and-bound, Lagrangean relaxation
and alternative formulations of this problem are on-going.
The performance of the g=2, g=2 JohnsonÕs Based Rule, coupled with the performance of a variant in
Kurz and Askin (2003), indicates that perhaps this could be a basis for the heuristic space based search
neighborhoods discussed in Storer et al. (1992). By considering several partitioning schemes on the g stages
to create the two processing times, along the lines of Campbell et al. (1970), a space of the heuristics can be
deﬁned. (We thank an anonymous referee for this insight.)

Acknowledgements
This research was supported by the Engineering Research Program of the Oﬃce of Basic Energy Sciences at the Department of Energy and by the National Science Foundation under Grant DMII 99-00052.

References
Agnetis, A., Paciﬁci, A., Rossi, F., Lucertini, M., Nicoletti, S., Nicolo, F., Oriolo, G., Pacciarelli, D., Pesaro, E., 1997. Scheduling of
ﬂexible ﬂow lines in an automobile assembly plant. European Journal of Operational Research 97, 348–362.

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

81

Bean, J.C., 1994. Genetic algorithms and random keys for sequencing and optimization. ORSA Journal on Computing 6, 154–
160.
Brah, S.A., Hunsucker, J.L., 1991. Branch and bound algorithm for the ﬂow shop with multiple processors. European Journal of
Operational Research 51, 88–99.
Brah, S.A., Loo, L.L., 1999. Heuristics for scheduling in a ﬂow shop with multiple processors. European Journal of Operational
Research 113, 113–122.
Campbell, H.G., Dudek, R.A., Smith, M.L., 1970. A heuristic algorithm for the n job, m machine sequencing problem. Management
Science 16 (10), B630–B637.
Ding, F.-Y., Kittichatphayak, D., 1994. Heuristics for scheduling ﬂexible ﬂow lines. Computers and Industrial Engineering 26 (1), 27–
34.
Gupta, J.N.D., 1997. A ﬂowshop scheduling problem with two operations per job. International Journal of Production Research 35
(8), 2309–2325.
Holland, J.H., 1975. Adaptation in Natural and Artiﬁcial Systems. The University of Michigan Press, Ann Arbor.
Johnson, S.M., 1954. Optimal two- and three-stage production schedules with setup times included. Naval Research Logistics
Quarterly 1, 61–67.
Kochhar, S., Morris, R.J.T., 1987. Heuristic methods for ﬂexible ﬂow line scheduling. Journal of Manufacturing Systems 6 (4), 299–
314.
Kurz, M.E., Askin, R.G., 2001. Note on ‘‘An adaptable problem-space-based search method for ﬂexible ﬂow line scheduling’’. IIE
Transactions 33, 691–693.
Kurz, M.E., Askin, R.G., 2003. Comparing scheduling rules for ﬂexible ﬂow lines. International Journal of Production Economics 85
(3), 371–388.
Lee, C.-Y., Vairaktarakis, G.L., 1994. Minimizing makespan in hybrid ﬂowshops. Operations Research Letters 16, 149–158.
Lee, I., Sikora, R., Shaw, M.J., 1997. A genetic algorithm-based approach to ﬂexible ﬂow-line scheduling with variable lot sizes. IEEE
Transactions on Systems, Man, and Cybernetics––Part B: Cybernetics 27 (1), 36–54.
Leon, V.J., Ramamoorthy, B., 1997. An adaptable problem-space-based search method for ﬂexible ﬂow line scheduling. IIE
Transactions 29, 115–125.
Montogomery, D.C., Runger, G.C., 2003. Applied Statistics and Probability for Engineers. John Wiley, New York.
Norman, B.A., Bean, J.C., 1999. A genetic algorithm methodology for complex scheduling problems. Naval Research Logistics 46,
199–211.
Nowicki, E., Smutnicki, C., 1996. A fast tabu search algorithm for the permutation ﬂow-shop problem. European Journal of
Operational Research 91, 160–175.
Nowicki, E., Smutnicki, C., 1994. An approximation algorithm for a single-machine scheduling problem with release times and
delivery times. Discrete Applied Mathematics 48, 69–79.
Nowicki, E., Smutnicki, C., 1998. The ﬂow shop with parallel machines: A tabu search approach. European Journal of Operational
Research 106, 226–253.
Oguz, C., Lin, B.M.T., Cheng, T.C.E., 1997. Two-stage ﬂowshop scheduling with a common second-stage machine. Computers and
Operations Research 24 (12), 1169–1174.
Piramuthu, S., Raman, N., Shaw, M.J., 1994. Learning-based scheduling in a ﬂexible manufacturing ﬂow line. IEEE Transactions on
Engineering Management 41 (2), 172–182.
Rajendran, C., Chaudhuri, D., 1992. Scheduling in n-job, m-stage ﬂowshop with parallel processors to minimize makespan.
International Journal of Production Economics 27, 137–143.
Riane, F., Artiba, A., Elmaghraby, S.E., 1998. A hybrid three-stage ﬂowshop problem: Eﬃcient heuristics to minimize makespan.
European Journal of Operational Research 109, 321–329.
Rios-Mercado, R.Z., Bard, J.F., 1998. Computational experience with a branch-and-cut algorithm for ﬂowshop scheduling with
setups. Computers and Operations Research. 25 (5), 351–366.
Rios-Mercado, R.Z., Bard, J.F., 1999a. An enhanced TSP-based heuristic for makespan minimization in a ﬂow shop with setup times.
Journal of Heuristics 5, 53–70.
Rios-Mercado, R.Z., Bard, J.F., 1999b. A branch-and-bound algorithm for permutation ﬂow shops with sequence-dependent setup
times. IIE Transactions 31, 721–731.
Salvador, M.S., 1973. A solution to a special case of ﬂow shop scheduling problems. In: Elmaghraby, S.E. (Ed.), Symposium on the
Theory of Scheduling and Its Applications. Springer-Verlag, Berlin, pp. 83–91.
Santos, D.L., Hunsucker, J.L., Deal, D.E., 1996. An evaluation of sequencing heuristics in ﬂow shops with multiple processors.
Computers and Industrial Engineering 30 (4), 681–692.
Santos, D.L., Hunsucker, J.L., Deal, D.E., 1995. FLOWMULT: Permutation sequences for ﬂow shops with multiple processors.
Journal of Information and Optimization Sciences 16 (2), 351–366.
Sawik, T.J., 1992. A scheduling algorithm for ﬂexible ﬂow lines with limited intermediate buﬀers. Proceedings of the 8th International
Conference on CAD/CAM, Robotics and Factories of the Future, vol. 2. pp. 1711–1722.

82

M.E. Kurz, R.G. Askin / European Journal of Operational Research 159 (2004) 66–82

Sawik, T.J., 1994. New algorithms for scheduling ﬂexible ﬂow lines. Proceedings of the 1994 Japan-USA Symposium on Flexible
Automation, vol. 3. pp. 1091–1094.
Sawik, T.J., 1995. Scheduling ﬂexible ﬂow lines with no in-process buﬀers. International Journal of Production Research 33 (5), 1357–
1367.
Snyder, L.V., Daskin, M.S., 2001. A random-key genetic algorithm for the generalized traveling salesman problem. Working Paper,
Department of Industrial Engineering and Management Sciences, Northwestern University.
Sriskandarajah, C., Sethi, S.P., 1989. Scheduling algorithms for ﬂexible ﬂowshops: Worst and average case performance. European
Journal of Operational Research 43, 143–160.
Storer, R.H., Wu, S.D., Vaccari, R., 1992. New search spaces for sequencing problems with application to job shop scheduling.
Management Science 38 (10), 1495–1509.
Wittrock, R., 1988. An adaptable scheduling algorithm for ﬂexible ﬂow lines. Operations Research 36 (3), 445–453.
Wittrock, R., 1985. Scheduling algorithms for ﬂexible ﬂow lines. IBM Journal of Research and Development 29 (24), 401–412.

