arXiv:1201.1587v3 [cs.LG] 21 Mar 2012

Feature Selection via Regularized Trees
Houtao Deng

George Runger

Intuit
Mountain View, California, USA
Email: hdeng3@asu.edu

School of Computing, Informatics & Decision Systems Engineering
Arizona State university, Tempe, Arizona, USA
Email: george.runger@asu.edu

Abstract—We propose a tree regularization framework, which
enables many tree models to perform feature selection efficiently.
The key idea of the regularization framework is to penalize
selecting a new feature for splitting when its gain (e.g. information
gain) is similar to the features used in previous splits. The
regularization framework is applied on random forest and
boosted trees here, and can be easily applied to other tree models.
Experimental studies show that the regularized trees can select
high-quality feature subsets with regard to both strong and
weak classifiers. Because tree models can naturally deal with
categorical and numerical variables, missing values, different
scales between variables, interactions and nonlinearities etc., the
tree regularization framework provides an effective and efficient
feature selection solution for many practical problems.
Index Terms—regularized boosted trees; RBoost; regularized
random forest; RRF; tree regularization.

I. I NTRODUCTION
In supervised learning, given a training data set consisting of
N instances, M predictor variables X1 , X2 , ...XM and the target variable Y ∈ {0, 1, ...C−1}, feature selection is commonly
used to select a compact feature subset F ⊂ {X1 , X2 , ...XM }
without significant loss of the predictive information about Y .
Feature selection methods play an important role in defying
the curse of dimensionality, improving efficiency both in time
and space, and facilitating interpretability [1].
We propose a tree regularization framework for feature selection in decision trees. The regularization framework avoids
selecting a new feature for splitting the data in a tree node
when that feature produces a similar gain (e.g. information
gain) to features already selected, and thus produces a compact
feature subset. The regularization framework only requires
a single model to be built, and can be easily added to a
wide range of tree-based models which use one feature for
splitting data at a node. We implemented the regularization
framework on random forest (RF) [2] and boosted trees [3].
Experiments demonstrate the effectiveness and efficiency of
the two regularized tree ensembles. As tree models naturally
handle categorical and numerical variables, missing values,
different scales between variables, interactions and nonlinearities etc., the tree regularization framework provides an
effective and efficient feature selection solution for many
practical problems.
Section II describes related work and background. Section
III presents the relationship between decision trees and the
Max-Dependency scheme [4]. Section IV proposes the tree
regularization framework, the regularized random forest (RRF)

and the regularized boosted random trees (RBoost). Section V
establishes the evaluation criteria for feature selection. Section
VI demonstrates the effectiveness and efficiency of RRF and
RBoost by extensive experiments. Section VII concludes this
work.
II. R ELATED W ORK

AND

BACKGROUND

A. Related work
Feature selection methods can be divided into filters, wrappers and embedded methods [5]. Filters select features based
on criteria independent of any supervised learner [6], [7].
Therefore, the performance of filters may not be optimum for a
chosen learner. Wrappers use a learner as a black box to evaluate the relative usefulness of a feature subset [8]. Wrappers
search the best feature subset for a given supervised learner,
however, wrappers tend to be computationally expensive [9].
Instead of treating a learner as a black box, embedded
methods select features using the information obtained from
training a learner. A well-known example is SVM-RFE (support vector machine based on recursive feature elimination)
[10]. At each iteration, SVM-RFE eliminates the feature with
the smallest weight obtained from a trained SVM. The RFE
framework can be extended to classifiers able to provide
variable importance scores, e.g. tree-based models [11]. Also,
decision trees such as C4.5 [12] are often used as embedded
methods as they intrinsically perform feature selection at each
node. Single tree models were used for feature selection [13],
however, the quality of the selected features may be limited
because the accuracy of a single tree model may be limited.
In contrast, tree ensembles, consisting of multiple trees are
believed to be significantly more accurate than a single tree
[2]. However, the features extracted from a tree ensemble
are usually more redundant than a single tree. Recently, [14]
proposed ACE (artificial contrasts with ensembles) to select
a feature subset from tree ensembles. ACE selects a set of
relevant features using a random forest [2], then eliminates
redundant features using the surrogate concept [15]. Also
multiple iterations are used to uncover features of secondary
effects.
The wrappers and embedded methods introduced above
require building multiple models, e.g. the RFE framework
[10] requires building potentially O(M ) models. Even at the
expense of some acceptable loss in prediction performance,
it is very desirable to develop feature selection methods that
only require training a single model which may considerably

reduce the training time [5]. The tree regularization framework
proposed here enables many types of decision tree models to
perform feature subset selection by building the models only
one time. Since tree models are popularly used for data mining,
the tree regularization framework provides an effective and
efficient solution for many practical problems.
B. Information-theoretic measures and issues
Information-theoretic measures have been widely used for
feature selection [16], [17], [7], [6], [4]. Entropy is an
important concept in the information-theoretic criteria. The
entropy of a categorical variable A P
can be expressed in terms
of prior probabilities: H(A) = − a∈A p(a) log2 p(a). The
entropy of A after observing
another
categorical variable B
P
P
is: H(A|B) = − b∈B p(b) a∈A p(a|b) log2 p(a|b). The
increase in the amount of information about A after observing
B is called the mutual information or, alternatively, information gain [6]:
I(A; B) = H(A) − H(A|B)

(1)

I(A; B) is symmetric, i.e. I(A; B) = I(B; A), and models the
degree of association between A and B. Therefore, one can
use I(Xi ; Y ) to evaluate the relevancy of Xi for predicting
the class Y , and use I(Xi ; Xj ) to evaluate the redundancy
in a pair of predictor variables [4]. In addition, a measure called symmetric uncertainty: SU (A; B) = 2(H(A) −
H(A|B))/(H(A) + H(B)) is used in feature selection methods such as CFS (correlation-based feature selection) [6] and
FCBF (fast correlation-based filter) [7].
Measures like I(A; B) and SU (A; B) capture only twoway relationships between variables and can not capture the
relationship between two variables given other variables, e.g.
I(X1 ; Y |X2 ) [16], [17]. [17] illustrated this limitation using
an exclusive OR example: Y = XOR(X1 , X2 ), in which
neither X1 nor X2 individually is predictive, but X1 and X2
together can correctly determine Y . To this end, [16], [17]
proposed measures which can capture three-way interactions.
Still, a feature selection method capable of handling n-way
interactions when n > 3 is desirable [16]. However, it is
computationally expensive to do so [17].
C. Tree-based models and issues
Univariate decision trees such as C4.5 [12] or CART [15]
recursively split data into subsets. For many tree models, the
feature used for splitting in a node is selected to optimize an
information-theoretic measure such as information gain.
A tree model is able to capture multi-way interactions
between the splitting variables and potentially is a solution
for the issue of the information-theoretic measures mentioned
in Section II-B. However, tree models have their own problems
for selecting a non-redundant feature set. A decision tree
selects a feature at each node by optimizing, commonly, an
information-theoretic criterion and does not consider if the
feature is redundant to the features selected in previous splits,
which results in feature redundancy. The feature redundancy

problem in tree models is illustrated in Figure 1. For the twoclass data shown in the figure, after splitting on X2 (“split
1”), either X1 or X2 can separate the two classes (“split 2”).
Therefore {X2 } is the minimal feature set that can separate the
two-class data. However, a decision tree may use X2 for “split
1” and X1 for “split 2” and thus introduce feature redundancy.
The redundancy problem becomes even more severe in tree
ensembles which consist of multiple trees. To eliminate the
feature redundancy in a tree model, some regularization is
used here to penalize selecting a new feature similar to the
ones selected in previous splits.
III. R ELATIONSHIP BETWEEN DECISION TREES
M AX -D EPENDENCY SCHEME

AND THE

The conditional mutual information, that is, the mutual
information between two features A and B given a set of
other features C1 , ...Cp , is defined as
I(A; B|C1 , ...Cp ) =
X
X
wC1 =c1 ,...Cp=cp I(A; B|C1 = c1 , ...Cp = cp )
...
c1 ∈C1

cp ∈Cp

(2)

where wC1 =c1 ,...Cp=cp is the ratio of the number of instances
satisfying {C1 = c1 , ...Cp = cp } to the total number of
instances.
A first-order incremental feature selection scheme, referred
to as the Max-Dependency (MD)[4] scheme, is defined as
M

i = arg max I(Xm ; Y |F (j −1)); F (j) = {F (j −1), Xi } (3)
m=1

where j is the step number, F (j) is the feature set selected in
the first j steps (F (0) = ∅), i is the index of the feature
selected at each step, I(Xm ; Y |F (j − 1)) is the mutual
information between Xm and Y given the feature set F (j −1).
Here we consider the relationship between the MD scheme
and decision trees. Because Equation (2) is limited to categorical variables, the analysis in this section is limited to categorical variables. We also assume the decision trees discussed
in this section select the splitting variable by maximizing the
information gain and split a non-leaf node into K child nodes,
where K is the number of values of the splitting variable.
However the tree regularization framework introduced later is
not limited to such assumptions.
In a decision tree, a node can be located by its level (depth)
Lj and its position in that level. An example of a decision tree
is shown in Figure 2(a). The tree has four levels, and one to
six nodes (positions) at each level. Note that in the figure, a
tree node that is not split is not a leaf node. Instead, we let all
the instances in the node pass to its “imaginary” child node, to
keep a form similar to the MD tree structure introduced later.
Also, let Sν denote the set of feature-value pairs that define
the path from the root node to node ν. For example, for node
P6 at level 4 in Figure 2(a), Sν = {X1 = x1 , X3 = x3 , X5 =
x5 }. For a decision tree node ν, a variable Xk is selected to

10

10

X2

4

6

8

split 1

2

2

4

X2

6

8

split 1

split 2

2

4

6

8

class 1
class 2

0

0
0

split 2

class 1
class 2
10

0

2

4

X1

6

8

10

X1

(a) A decision tree may use both X1 and X2 to (b) X2 alone can perfectly separate the two classes.
split.
Fig. 1. An illustration of feature redundancy in decision trees. A decision tree may use both features to split, but X2 alone can perfectly separate the two
classes.
X1

Level 1

P1

Level 2

Level 3

P1

X3

X2

X4

P2

X1

Level 1

P3

P2

X5

Level 2

P4

P1

Level 3
P1

Level 4

P2

X2

X4

X4

P2

P3

X4

Level 4
P1

P2

P3

P4

P5

P6

P1

P2

P3

P4

P5

(a) At each level, a decision tree can have different (b) At each level, the MD scheme uses only one variable
variables for splitting the nodes.
for splitting all the nodes.
Fig. 2. Illustrations of a decision tree and the MD scheme in terms of a tree structure. A node having more than one child node is marked with the splitting
variable. For a decision tree node that can not be split, we let all the instances in the node pass to its “imaginary” child node, to keep a form similar to the
MD tree.

maximize the information gain conditioned on Sν . That is,
M

k = arg max I(Xm ; Y |Sν )
m=1

(4)

By viewing each step of the MD scheme as a level in
a decision tree, the MD scheme can be expressed as a tree
structure, referred to an MD tree. An example of an MD tree
is shown in Figure 2(b). Note in an MD tree, only one feature
is selected at each level. Furthermore, for the MD tree, Xk is
selected at Lj so that
M X
k = arg max
wν ∗ I(Xm ; Y |Sν )
(5)
m=1

ν∈Lj

where wν is the ratio of the number of instances at node ν to
the total number of training instances.
Note Equation (4) maximizes the conditional mutual information at each node, while Equation (5) maximizes a
weighted sum of the conditional mutual information from all
the nodes in the same level. Calculating Equation (5) is more
computationally expensive than Equation (4). However, at each
level Lj , an MD tree selects only one feature that adds the
maximum non-redundant information to the selected features,
while decision trees can select multiple features and there is
no constraint on the redundancy of these features.

IV. R EGULARIZED

TREES

We are now in a position to introduce the tree regularization
framework which can be applied to many tree models which
recursively split data based on a single feature at each node.
Let gain(Xj ) be the evaluation measure calculated for feature
Xj . Without loss of generality, assume the splitting feature
at a tree node is selected by maximizing gain(Xj ) (e.g.
information gain). Let F be the feature set used in previous
splits in a tree model. When the tree model is built, then F
becomes the final feature subset.
The idea of the tree regularization framework is to avoid
selecting a new feature Xj , i.e., avoid features not belonging to F , unless gain(Xj ) is substantially larger than
maxi (gain(Xi )) for Xi ∈ F . To achieve this goal, we
consider a penalty to gain(Xj ) for Xj ∈
/ F . A new measure
is calculated as
(
λ · gain(Xj ) Xj ∈
/F
gainR (Xj ) =
(6)
gain(Xi )
Xj ∈ F
where λ ∈ [0, 1]. Here λ is called the coefficient. A smaller
λ produces a larger penalty to a feature not belonging to F .
Using gainR (·) for selecting the splitting feature at each tree

Algorithm 1 Feature selection via the regularized random tree model: F = tree(data, F, λ), where F is the feature subset
selected by previous splits and is initialized to an empty set. Details not directly relevant to the regularization framework are
omitted. Brief comments are provided after “//”.
1: gain∗ = 0
2: count = 0 // the number of new features tested
3: for m = 1 : M do
4:
gainR (Xm )=0
5:
if Xm ∈ F then gainR (X√
m ) = gain(Xm ) end if //calculate the gainR for all variables in F
6:
if Xm ∈
/ F and count < ⌈ M ⌉ then
7:
gainR (Xm ) = λ · gain(Xm ) //penalize using new features
8:
count = count+1
9:
end if
10:
if gainR (Xm ) > gain∗ then gain∗ = gainR (Xm ), X ∗ = Xm end if
11: end for
12: if gain∗ = 0 then make this node as a leaf and return F end if
13: if X ∗ ∈
/ F then F = {F, X ∗ } end if
14: split data into γ child nodes by X ∗ : data1 , ...dataγ
15: for g = 1 : γ do
16:
F = tree(datag , F, λ)
17: end for
18: return F

node is called a tree regularization framework. A tree model
using the tree regularization framework is called a regularized
tree model. A regularized tree model sequentially adds new
features to F if those features provide substantially new
predictive information about Y . The F from a built regularized
tree model is expected to contain a set of informative, but
non-redundant features. Here F provides the selected features
directly, which has the advantage over a feature ranking
method (e.g. SVM-RFE) in which a follow-up selection rule
needs to be applied.
A similar penalized form to gainR (·) was used for suppressing spurious interaction effects in the rules extracted from tree
models [18]. The objective of [18] was different from the goal
of a compact feature subset here. Also, the regularization in
[18] only reduced the redundancy in each path from the root
node to a leaf node, but the features extracted from tree models
using such a regularization [18] can still be redundant.
Here we apply the regularization framework on the random
tree model available at Weka [19]. The random tree randomly
selects and tests K √
variables out of M variables at each node
(here we use K = ⌈ M ⌉ which is commonly used for random
forest [2]), and recursively splits data using the information
gain criterion.
The random tree using the regularization framework is
called the regularized random tree algorithm which is shown
in Algorithm 1. The algorithm focuses on illustrating the tree
regularization framework and omits some details not directly
relevant to the regularization framework. The regularized random tree differs from the original random tree in the following
ways: 1) gainR (Xj ) is used for selecting the splitting feature;
2) gainR of all variables
√ belonging to F are calculated, and
the gainR of up to ⌈ M ⌉ randomly selected variables not
belonging to F are calculated. Consequently, to enter F a

variable needs to improve upon the gain of all the currently
selected variables, even after its gain is penalized with λ.
Algorithm 2 Feature selection via the regularized tree ensemble: F = ensemble(data, F, λ, nT ree), where F is feature
subset selected by previous splits and is initialized to an empty
set, nT ree is the number of regularized trees in the tree
ensemble.
1: for iTree = 1:nT ree do
2:
select datai from data with some criterion, e.g. randomly select
3:
F = tree(datai , F, λ)
4: end for
The tree regularization framework can be easily applied
to a tree ensemble consisting of multiple single trees. The
regularized tree ensemble algorithm is shown in Algorithm
2. F now represents the feature set used in previous splits
not only from the current tree, but also from the previous
built trees. Details not relevant to the regularization framework
are omitted in Algorithm 2. The computational complexity
of a regularized tree ensemble with nT ree regularized trees
is nT ree times the complexity of the single regularized tree
algorithm. The simplicity of Algorithm 2 suggests the easiness
of extending a single regularized tree to a regularized tree
ensemble. Indeed, the regularization framework can be applied
to many forms of tree ensembles such as bagged trees [20]
and boosted trees [3]. In the experiments, we applied the
regularization framework to bagged random trees, referred to
as random forest (RF) [2], and boosted random trees. The
regularized versions are called the regularized random forest
(RRF) and regularized boosted random trees (RBoost).

0.6
0.5

accuracy

0.4

A feature selection evaluation criterion is needed to measure
the performance of a feature selection method. Theoretically,
the optimal feature subset should be a minimal feature set
without loss of predictive information and can be formulated
as a Markov blanket of Y (M B(Y )) [21], [22]. The Markov
blanket can be defined as [22]:
Definition 1: Markov blanket of Y: A set M B(Y ) is a
minimal set of features with the following property. For each
feature subset f with no intersection with M B(Y ), Y ⊥
f |M B(Y ). That is, Y and f are conditionally independent
given M B(Y ). In [23], this terminology is called the Markov
Boundary.
In practice, the ground-truth M B(Y ) is usually unknown
and the evaluation criterion of feature selection is commonly
associated with the expected loss of a classifier model, referred
to as the empirical criterion here (similar to the definition of
“feature selection problem” [22]):
Definition 2: Empirical criterion: Given a set of training
instances of instantiations of feature set X drawn from distribution D, a classifier induction algorithm C, and a loss
function L, find the smallest subset of variables F ⊆ X such
that F minimizes the expected loss L(C, D) in distribution D.
The expected loss L(C, D) is commonly measured by
classification generalization error. According to Definition 2,
to evaluate two feature subsets, the subset with a smaller
generalization error is preferred. With similar errors, then the
smaller feature subset is preferred.
Both evaluation criteria prefer a feature subset with less loss
of predictive information. However, the theoretical criterion
(Definition 1) does not depend on a particular classifier, while
the empirical criterion (Definition 2) measures the information
loss using a particular classifier. Because a relatively strong
classifier generally captures the predictive information from
features better than a weak classifier, the accuracy of a strong
classifier may be more consistent with the amount of predictive
information contained in a feature subset.
To illustrate this point, we randomly split the Vehicle data
set from the UCI database [24] into a training set and a testing
set with the same number of instances. Starting from an empty
feature set, each time a new feature was randomly selected and
added to the set. Then C4.5 [12], NB, and a relatively strong
classifier random forest (RF) [2] were trained using the feature
subsets, respectively. The accuracy of each classifier on the
testing set versus the number of features is shown in Figure 3.
For C4.5 and NB, the accuracy stops increasing after adding a
certain number of features. However, RF continues to improve
as more features are added, which indicates the added features
contain additional predictive information. Therefore, compared
to RF, the accuracy performance of C4.5 and NB may be
less consistent with the amount of predictive information
contained in the features. This point is also validated by
experiments shown later in this paper. Furthermore, in many
cases higher classification accuracy and thus a relatively strong
classifier may be preferred. Therefore, a feature selection

0.7

CRITERIA FOR FEATURE SELECTION

C4.5
NB
RF

0.3

V. E VALUATION

5

10

15

features

Fig. 3. Accuracy of C4.5, naive Bayes (NB) and random forest (RF) for
different numbers of features for the Vehicle data set from the UCI database.
Starting from an empty feature set, each time a new feature is randomly
selected and added to the set. The accuracy of RF continues to improve as
more features are used, while the accuracy of C4.5 and NB stops improving
after adding a certain number of features.

method capable of producing a high-quality feature subset with
regard to a strong classifier is desirable.
VI. E XPERIMENTS
Data sets from the UCI benchmark database [24], the
NIPS 2003 feature selection benchmark database, and the
IJCNN 2007 Agnostic Learning vs. Prior Knowledge Challenge database were used for evaluation. These data sets
are summarized in Table I. We implemented the regularized
random forest (RRF) and the regularized boosted random trees
(RBoost) under the Weka framework [19]. Here λ = 0.5 is
used and initial experiments show that, for most data sets, the
classification accuracy results do not change dramatically with
λ.
The regularized trees were empirically compared to CFS
[6], FCBF [7], and SVM-RFE [10]. These methods were
selected for comparison because they are well-recognized and
widely-used. These methods were run in Weka with the default
settings.
We applied the following classifiers: RF (200 trees) [2] and
C4.5 [12] on all the features and the features selected by RRF,
RBoost, CFS and FCBF for each data set, respectively. We
ran 10 replicates of two-fold cross-validation for evaluation.
Table II shows the number of original features, and the average
number of features selected by the different feature selection
methods for each data set. Table III show the accuracy of
RF and C4.5 applied to all features and the feature subsets,
respectively. The average accuracy of different algorithms, and
a paired t-test between using the feature subsets and using all
features over the 10 replicates are shown in the table. The
feature subsets having significantly better/worse accuracy than
all features at a 0.05 level are denoted as +/-, respectively.
The numbers of significant wins/losses/ties using the feature
subsets over using all features are also shown.

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal

instances
1000
5000
368
195
205
3163
2800
351
898

features
20
21
22
22
25
25
29
34
38

classes
2
3
2
2
6
2
2
2
5

Data
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene

instances
4147
208
606
476
452
2000
3153
3845
100

features
48
60
100
166
279
500
970
1617
10000

classes
2
2
2
2
13
2
2
2
2

TABLE I
S UMMARY OF THE DATA SETS USED IN EXPERIMENTS .

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal

All
20
21
22
22
25
25
29
34
38

RRF
17.9
21.0
18.4
10.6
8.2
12.4
12.3
15.2
11.5

RBoost
18.7
21.0
19.3
12.3
8.4
14.5
16.3
18.5
11.7

CFS
4.9
15.3
3.9
7.8
6.8
5.3
5.4
11.7
5.8

FCBF
3.6
7.1
3.9
3.5
4.5
5.5
5.6
9.1
6.9

Data
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene

All
48
60
100
166
279
500
970
1617
10000

RRF
39.1
18.9
30.7
34.5
26.8
72.5
83.0
146.1
22.5

RBoost
41.2
21.4
33.5
34.8
28.9
76.9
95.4
192.6
28.2

CFS
8.4
10.8
1.0
29.2
17.7
10.7
51.6
38.6
49.4

FCBF
7.0
6.6
1.0
11.0
8.2
4.7
16.1
13.6
35.1

TABLE II
T HE TOTAL NUMBER OF FEATURES (“A LL ”), AND THE AVERAGE NUMBER OF FEATURES SELECTED BY DIFFERENT FEATURE SELECTION METHODS .

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene
win/lose/tie

All
0.752
0.849
0.858
0.892
0.756
0.989
0.979
0.931
0.944
0.840
0.803
0.546
0.865
0.682
0.671
0.924
0.967
0.760
-

RRF
0.750
0.849
0.857
0.891
0.756
0.990
0.981
0.926
0.940
0.839
0.783
0.511
0.849
0.704
0.706
0.915
0.967
0.683
4/6/8

+
+
−
−
−
−
+
+
−
−

Classifier: RF
RBoost
CFS
0.750
0.704
−
0.849
0.846
−
0.853
−
0.824
−
0.891
0.878
−
0.759
0.746
0.990
+
0.985
−
0.980
+
0.966
−
0.928
0.925
−
0.941
0.904
−
0.839
0.823
−
0.774
−
0.739
−
0.514
−
0.489
−
0.853
−
0.840
−
0.699
+
0.721
+
0.675
0.784
+
0.914
−
0.891
−
0.967
0.966
0.676
−
0.713
−
3/6/9
2/14/2

FCBF
0.684
−
0.788
−
0.825
−
0.846
−
0.715
−
0.990
0.966
−
0.919
−
0.919
−
0.831
−
0.734
−
0.498
−
0.821
−
0.685
0.602
−
0.832
−
0.965
−
0.702
−
0/16/2

All
0.716
0.757
0.843
0.842
0.662
0.992
0.982
0.887
0.897
0.830
0.701
0.503
0.766
0.642
0.593
0.847
0.961
0.603
-

RRF
0.719
0.757
0.843
0.843
0.634
0.992
0.982
0.881
0.896
0.829
0.693
0.503
0.746
−
0.648
0.661
+
0.851
0.961
0.633
1/1/16

Classifier: C4.5
RBoost
CFS
0.716
0.723
0.757
0.765
+
0.842
0.835
0.841
0.841
0.638
0.637
0.992
0.988
−
0.982
0.973
−
0.881
0.889
0.893
0.869
−
0.830
0.842
+
0.691
0.689
0.503
0.503
0.768
0.771
0.649
0.662
+
0.643
+
0.696
+
0.848
0.854
0.964
+
0.965
+
0.606
0.566
2/0/16
5/3/10

FCBF
0.713
0.749
−
0.836
0.839
0.640
0.991
0.973
−
0.880
0.890
0.840
+
0.697
0.503
0.752
0.657
0.611
+
0.817
−
0.965
+
0.586
3/3/12

TABLE III
T HE AVERAGE ACCURACY OF RANDOM FOREST (RF) AND C4.5 APPLIED TO ALL FEATURES , AND THE FEATURE SUBSETS SELECTED BY DIFFERENT
METHODS RESPECTIVELY. T HE FEATURE SUBSETS HAVING SIGNIFICANTLY BETTER / WORSE ACCURACY THAN ALL FEATURES AT A 0.05 LEVEL ARE
DENOTED AS +/-.

Some trends are evident. In general, CFS and FCBF tend
to select fewer features than the regularized tree ensembles.
However, RF using the features selected by CFS or FCBF
has many more losses than wins on accuracy, compared to
using all the features. Note both CFS and FCBF consider
only two-way interactions between the features, and, therefore,
they may miss some features which are useful only when other
features are present. In contrast, RF using the features selected
by the regularized tree ensembles is competitive to using all
the features. This indicates that though the regularized tree
ensembles select more features than CFS and FCBF, these additional features indeed add additional predictive information.
For some data sets where the number of instances is small

(e.g. arcene), RF using the features from RRF or RBoost do
not have an advantage over RF using the features from CFS.
This may be because a small number of instances leads to
small trees, which are less capable of capturing multi-way
feature interactions.
The relatively weak classifier C4.5 performs differently
from RF. The accuracy of C4.5 using the features from
every feature selection method is competitive to using all the
features, even though the performance of RF suggests that
CFS and FCBF may miss some useful predictive information.
This indicates that that C4.5 may be less capable than RF on
extracting predictive information from features.
In addition, the regularized tree ensembles: RRF and RBoost

musk

arrhythmia

50

SVM−RFE
RRF

15

30

35

error
20
25

error
40 45

30

SVM−RFE
RRF

0

50

100
features

150

0

50

100 150 200 250
features

(a) The musk data. The SVM-RFE took 109 seconds to run, (b) The arrhythmia data. SVM-RFE took 442 seconds to run,
while RRF took only 4 seconds on average.
while RRF took only 6 seconds on average.
Fig. 4. The results of SVM-RFE and RRF. Plotted points show the errors versus the number of backward elimination iterations used in SVM-RFE. The
circles correspond to the average error versus the average number of features over 10 runs of RRF. The straight lines on the circles are the standard errors
(vertical lines) or number of features (horizontal lines).

have similar performances regarding the number of features
selected or the classification accuracy over these data sets.
Next we compare the regularized tree ensembles to SVMRFE. For simplicity, here we only compare RRF to SVM-RFE.
The algorithms are evaluated using the musk and arrhythmia
data sets. Each data set is split into the training set and testing
set with equal number of instances. The training set is used for
feature selection and training a RF classifier, and the testing
set is used for testing the accuracy of the RF. Figure 4 plots
the RF accuracy versus the number of backward elimination
iterations used in SVM-RFE. Note that RRF can automatically
decide the number of features. Therefore, the accuracy of RF
using the features from RRF is a single point on the figure.
We also considered the randomness of RRF. We run RRF 10
times for each data set and Figure 4 shows the average RF error
versus the average number of selected features. The standard
errors are also shown.
For both data sets, RF’s accuracy using the features from
RRF is competitive to using the optimum point of SVM-RFE.
It should be noted that SVM-RFE still needs to select a cutoff
value for the number of features by strategies such as crossvalidation, which not necessarily selects the optimum point,
and also further increase the computational time. Furthermore,
RRF (took less than 10 seconds in average to run for each data
set) is considerably more efficient than SVM-RFE (took more
than 100 seconds to run for each data set).
VII. C ONCLUSION
We propose a tree regularization framework, which adds a
feature selection capability to many tree models. We applied
the regularization framework on random forest and boosted
trees to generate regularized versions (RRF and RBoost,

respectively). Experimental studies show that RRF and RBoost
produce high-quality feature subsets for both strong and weak
classifiers. As tree models are computationally fast and can
naturally deal with categorical and numerical variables, missing values, different scales (units) between variables, interactions and nonlinearities etc., the tree regularization framework
provides an effective and efficient feature selection solution
for many practical problems.
ACKNOWLEDGEMENTS
This research was partially supported by ONR grant
N00014-09-1-0656.
R EFERENCES
[1] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “An introduction to
variable and feature selection,” Journal of Machine Learning Research,
vol. 3, pp. 1157–1182, 2003.
[2] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.
5–32, 2001.
[3] Y. Freund and R. Schapire, “Experiments with a new boosting algorithm,” in Proceedings of the Thirteenth International Conference on
Machine Learning, 1996, pp. 148–156.
[4] H. Peng, F. Long, and C. Ding, “Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 27, no. 8, pp. 1226–1238, 2005.
[5] I. Guyon, A. Saffari, G. Dror, and G. Cawley, “Model selection: beyond
the bayesian/frequentist divide,” Journal of Machine Learning Research,
vol. 11, pp. 61–87, 2010.
[6] M. A. Hall, “Correlation-based feature selection for discrete and numeric class machine learning,” in Proceedings of the 17th International
Conference on Machine Learning, 2000, pp. 359–366.
[7] L. Yu and H. Liu, “Efficient feature selection via analysis of relevance
and redundancy,” Journal of Machine Learning Research, vol. 5, pp.
1205–1224, 2004.
[8] R. Kohavi and G. John, “Wrappers for feature subset selection,” Artificial
intelligence, vol. 97, no. 1-2, pp. 273–324, 1997.

[9] H. Liu and L. Yu, “Toward integrating feature selection algorithms for
classification and clustering,” IEEE Transactions on Knowledge and
Data Engineering, vol. 17, no. 4, pp. 491–502, 2005.
[10] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “Gene selection for
cancer classification using support vector machines,” Machine Learning,
vol. 46, no. 1-3, pp. 389–422, 2002.
[11] R. Dı́az-Uriarte and S. De Andres, “Gene selection and classification
of microarray data using random forest,” BMC bioinformatics, vol. 7,
no. 1, p. 3, 2006.
[12] J. Quinlan, C4.5: programs for machine learning. Morgan Kaufmann,
1993.
[13] L. Frey, D. Fisher, I. Tsamardinos, C. Aliferis, and A. Statnikov, “Identifying markov blankets with decision tree induction,” in Proceedings
of the third IEEE International Conference on Data Mining, Nov 2003,
pp. 59–66.
[14] E. Tuv, A. Borisov, G. Runger, and K. Torkkola, “Feature selection with
ensembles, artificial variables, and redundancy elimination,” Journal of
Machine Learning Research, vol. 10, pp. 1341–1366, 2009.
[15] L. Breiman, J. Friedman, R. Olshen, C. Stone, D. Steinberg, and P. Colla,
“CART: Classification and Regression Trees,” Wadsworth: Belmont, CA,
1983.
[16] A. Jakulin and I. Bratko, “Analyzing attribute dependencies,” in Proceedings of the 7th European Conference on Principles and Practice of
Knowledge Discovery in Databases, 2003, pp. 229–240.
[17] F. Fleuret, “Fast binary feature selection with conditional mutual information,” Journal of Machine Learning Research, vol. 5, pp. 1531–1555,
2004.
[18] J. H. Friedman and Popescu, “Predictive learning via rule ensembles,”
Annal of Appied Statistics, vol. 2, no. 3, pp. 916–954, 2008.
[19] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten, “The weka data mining software: an update,” SIGKDD
Explorations, vol. 11, no. 1, pp. 10–18, 2009.
[20] L. Breiman, “Bagging Predictors,” Machine Learning, vol. 24, pp. 123–
140, 1996.
[21] D. Koller and M. Sahami, “Toward optimal feature selection,” in
Proceedings of the 13th International Conference on Machine Learning,
1996, pp. 284–292.
[22] C. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. Koutsoukos,
“Local causal and markov blanket induction for causal discovery and
feature selection for classification part i: Algorithms and empirical
evaluation,” Journal of Machine Learning Research, vol. 11, pp. 171–
234, 2010.
[23] J. Pearl, Probabilistic Reasoning in Intelligent Systems.
Morgan
Kaufmann, 1988.
[24] C. Blake and C. Merz, “UCI repository of machine learning databases,”
1998.

Data Min Knowl Disc (2015) 29:400–422
DOI 10.1007/s10618-014-0349-y

Learning a symbolic representation for multivariate
time series classification
Mustafa Gokce Baydogan · George Runger

Received: 9 March 2013 / Accepted: 3 January 2014 / Published online: 16 February 2014
© The Author(s) 2014

Abstract Multivariate time series (MTS) classification has gained importance with
the increase in the number of temporal datasets in different domains (such as medicine,
finance, multimedia, etc.). Similarity-based approaches, such as nearest-neighbor classifiers, are often used for univariate time series, but MTS are characterized not only by
individual attributes, but also by their relationships. Here we provide a classifier based
on a new symbolic representation for MTS (denoted as SMTS) with several important
elements. SMTS considers all attributes of MTS simultaneously, rather than separately, to extract information contained in the relationships. Symbols are learned from
a supervised algorithm that does not require pre-defined intervals, nor features. An
elementary representation is used that consists of the time index, and the values (and
first differences for numerical attributes) of the individual time series as columns. That
is, there is essentially no feature extraction (aside from first differences) and the local
series values are fused to time position through the time index. The initial representation of raw data is quite simple conceptually and operationally. Still, a tree-based
ensemble can detect interactions in the space of the time index and time values and
this is exploited to generate a high-dimensional codebook from the terminal nodes
of the trees. Because the time index is included as an attribute, each MTS is learned
to be segmented by time, or by the value of one of its attributes. The codebook is
processed with a second ensemble where now implicit feature selection is exploited

Responsible editor: M. J. Zaki.
M. G. Baydogan (B)
Department of Industrial Engineering, Boğaziçi University,
Bebek, Istanbul 34342, Turkey
e-mail: mustafa.baydogan@boun.edu.tr
G. Runger
School of Computing, Informatics & Decision Systems Engineering,
Arizona State University, Tempe, AZ, USA

123

Learning a symbolic representation for multivariate time series classification

401

to handle the high-dimensional input. The constituent properties produce a distinctly
different algorithm. Moreover, MTS with nominal and missing values are handled efficiently with tree learners. Experiments demonstrate the effectiveness of the proposed
approach in terms of accuracy and computation times in a large collection multivariate
(and univariate) datasets.
Keywords

Supervised learning · Codebook · Decision trees

1 Introduction
Multivariate time series (MTS) classification is a supervised learning problem in which
each example consists of one or more time series (attributes). MTS data is common in
fields such as medicine, finance and multimedia. For example, in motion capture studies humans perform a series of tasks and the positions of joints are tracked by markers
(McGovern et al. 2011). Learning scientists are interested in electroencephalography (EEG), (recordings of electrical activity at different locations along the scalp)
to understand the perceived difficulty of a puzzle-solving task in a learning environment. Moreover, in the domain of relational marketing, the behavior of a customer is
observed through time, and the interactions and responses are represented as MTS.
Also, Orsenigo and Vercellis (2010) described a telecommunication application where
customer loyalty was analyzed from the transactions of each customer in time periods
and described by duration, economic value and number of calls of different type (i.e.,
cell to cell, cell to landline etc.).
The problem has been studied in fields such as statistics, signal processing and
control theory (Kadous and Sammut 2005). The most common approach is to obtain
a rectangular representation of MTS by transforming the set of input series to a fixed
number of columns using different rectangularization approaches (Orsenigo and Vercellis 2010). For example, singular value decomposition (SVD) was used by Li et al.
(2006), Li et al. (2007), Weng and Shen (2008). Any supervised learner can be trained
on the transformed data for classification. These approaches assume that the attributes
are numerical. However, some attributes might be nominal or contain missing values.
Another strategy is to modify the similarity-based approaches used for univariate
time series (UTS). For example, Akl and Valaee (2010), Liu et al. (2009) focused on
gesture recognition based on dynamic time warping (DTW) distance. DTW (Sakoe
1978) provides a similarity measure independent of certain non-linear variations in
the time dimension, and is considered a strong solution for time series problems
(Ratanamahatana and Keogh 2005). Another similarity approach uses a kernel function determined by pairwise similarities between the attributes of a MTS. Thus, Chaovalitwongse and Pardalos (2008) used kernels based on DTW for brain activity classification and Orsenigo and Vercellis (2010) proposed a temporal discrete SVM for
MTS classification. Similarity is based on a term that depends on the warping distances
(Orsenigo and Vercellis 2010). However, an important limitation of these approaches
is that MTS are not only characterized by individual attributes, but also by the relationships between the attributes (Bankó and Abonyi 2012) and such information is not
captured by the similarity between the individual attributes (Weng and Shen 2008).

123

402

M. G. Baydogan, G. Runger

Another important challenge for MTS classification is the high dimensionality
introduced by multiple attributes and longer series. For longer time series, higher-level
representations are proposed Lin et al. (2007), such as Fourier transforms, wavelets,
piecewise polynomial models, etc. (Lin et al. 2003). Also, symbolic aggregate approximation (SAX) (Lin et al. 2007, 2012; Shieh and Keogh 2008), is a simple symbolic
representation for univariate series that segments the series into fixed-length intervals
(and a symbol represents the mean of the values). This representation is similar to
Piecewise Aggregate Approximation (PAA) (Chakrabarti et al. 2002). In the multivariate scenario, two alternative representations (illustrated in Fig. 1) are considered
(Kuksa 2012). MTS with M attributes and T observations can be discretized to obtain
a one-dimensional (1D) representation using vector quantization approaches similar
to those used for univariate series (Kuksa 2012). Alternatively, each attribute of MTS
can be discretized and combined to obtain a two-dimensional (2D) representation of
MTS.
Here we provide a classifier based on new symbolic representation for MTS
(denoted as SMTS) with several important elements. SMTS considers all attributes of
MTS simultaneously, rather than separately, to extract information contained in relationships. The symbols are learned from a supervised algorithm that does not require
pre-defined intervals, nor features. An elementary representation is used that consists
of the time index, and the values (and first differences) of the individual time series as
columns. That is, there is essentially no feature extraction (aside from first differences)
and the local series values are fused to time position through the time index.
Each MTS is concatenated vertically and each instance is labeled with the class of
the corresponding time series. A tree learner [e.g., CART (Breiman et al. 1984) or C4.5
(Quinlan 1993)] is applied to the simple representation and each instance is assigned
to a terminal node of the tree. That is, a symbol is associated with each time index
(instance), instead of intervals of the time series. With R terminal nodes, the dictionary
contains R symbols. Because the time index is included as an attribute, each MTS may
be segmented by time, or by the value of one of its attributes. Consequently, the time
order of the data can be incorporated in the model. At the next level of the learning

Fig. 1 Alternative representations for MTS (Kuksa 2012). MTS with M attributes and T observations are
mapped to a 1D representation by the function g1D (left) or a 2D representation in which each attribute of
MTS is mapped to 1D representation by g2D (right). Although the length of the symbolic representation
is shown to be the same as T , it can be smaller based on the mapping strategy. Also, the 2D representation
may have fewer columns than M depending on the mapping

123

Learning a symbolic representation for multivariate time series classification

403

algorithm, the set of instances from each MTS is summarized by the frequency over
the R symbols. Because of the greedy nature of a single tree, the procedure to grow a
tree is repeated to generate a tree ensemble (Breiman 2001). Consequently, each MTS
is represented by a collection of distributions and the concatenated vector becomes the
codebook for the second layer in the algorithm. A codebook is learned automatically
from a simple representation of the raw data.
Interactions between the attributes of MTS are considered through the multivariate,
tree-based symbol generation used in SMTS. Although our symbolic representation
is of the same length as the time series, it does not generate a representation for each
attribute of MTS. Therefore, SMTS scales well with a the number of attributes which
makes it computationally efficient when compared to other approaches. Furthermore,
an ensemble learner that scales well with a large number of attributes and long time
series is used. SMTS can handle MTS examples of different lengths. Moreover, as
discussed in a telecommunication application (Orsenigo and Vercellis 2010), data
can be nominal (e.g., call type) for which similarity computations and other representations are not well-defined. MTS with nominal and missing values are handled
efficiently with tree learners. Our experiments demonstrate the effectiveness of the
proposed approach in terms of accuracy and computation times in both UTS and MTS
datasets.
The remainder of this paper is organized as follows. Section 2 summarizes related
work. Section 3 provides a brief background. Section 4 presents the problem and
describes the framework. Section 5 demonstrates the effectiveness and efficiency of our
proposed approach by testing on a full set of benchmark datasets from CMU (2012),
Bache and Lichman (2013), Keogh et al. (2011) and Olszewski (2012). Section 6
provides conclusions.

2 Related work
The necessity of alternative representations for MTS classification was discussed
by Kadous and Sammut (2005). A concept called a metafeature is introduced and
used to represent a MTS. However, metafeatures must be defined by users in this
approach. One of the metafeatures partitions the feature space using Voronoi tiling
and selects these regions. The partitioning is not supervised in this approach and, as
mentioned by Kadous and Sammut (2005), designing a good metafeature is not an easy
task.
A 2D representation for MTS was considered by McGovern et al. (2011). Each
attribute of MTS is discretized using SAX and salient attributes are identified from
statistical performance measures. Patterns are identified on each individual attribute,
without taking other attributes into account. Consequently, this approach is greedy
because the relationships between the attributes may carry the important description
of a complex system (Bankó and Abonyi 2012). Although the patterns are combined
later, one may potentially miss a certain pattern that appears unimportant when time
series are considered separately in the first step. Also, McGovern et al. (2011) required
an approach for a multiclass extension since the rules were generated for binary classification.

123

404

M. G. Baydogan, G. Runger

Two MTS representations based on SAX to classify physiological data were presented by Ordonez et al. (2011). Multivariate words are generated by combining the
symbols of each attribute at a particular time and a bag-of-patterns (BoP) approach
is used to classify MTS. This considers the relationships between the time series
by combining individual representations. However, the length of the words obtained
by concatenating the symbols may increase substantially as the number of attributes
increase. This potentially affects the effectiveness because longer words carry less
information (i.e. curse of dimensionality). Also, this representation is not sensitive to
warpings of the patterns because words are synchronized over time. Furthermore, the
symbols are computed from a discretization of each time series separately, rather than
from the multivariate distribution. Moreover, the fixed-length intervals have the potential to omit patterns that appear with different lengths (dilations) and be split across
the time points (Fu 2011). A stacked Bags-of-Patterns was also proposed by Ordonez
et al. (2011). This concatenates the representation of multiple univariate series into
a single one. However, this representation does not take the relationships between
attributes into account.
Time series similarity based on a BoW representation was also considered by Lin
et al. (2012) for univariate time series classification. Time series were discretized
by SAX (Lin et al. 2007) and time series were represented as words using the symbols generated by SAX. Similarity of the time series were then computed using the
representations from document classification approaches. Unlike Lin et al. (2012),
our approach generates symbols in a supervised manner and handles the multivariate
case.
Also, Kudo et al. (1999) proposed an approach for multidimensional curve classification. Each attribute is discretized separately (as in the existing symbolic MTS
methods). Also, each attribute of MTS is partitioned into rectangular regions of equal
dimensions. Then classification rules are discovered based on the common regions
through which only curves of one class pass. Their approach has similarities to McGovern et al. (2011) in terms of the discretization and rule generation. However, because
the discretization does not consider the attributes simultaneously and the rules are
discovered based on each individual attribute, there is a potential to miss important
relationships between the attributes.
For image classification problems, Moosmann et al. (2008) trained trees on features
extracted from image patches in a supervised manner and the terminal nodes were
used as individual clusters. An image is represented as the frequency distribution of
the cluster IDs from an image (visual codebook) and any supervised learner can be
trained on the visual codebook. Unlike Moosmann et al. (2008), we do not sample
patches and we do not generate features. Our simple representation is used to learn
symbols directly. Furthermore, Moosmann et al. (2008) did not consider the location
information (time index) as a feature during the tree learning as we do. Instead, they
proposed to guide sampling to important locations of the images. They also use very
few trees in the ensemble, whereas we consider a codebook of much higher dimension.
For univariate time series classification, a bag-of-features approach, TSBF, was
proposed by Baydogan et al. (2013). TSBF summarizes time series with class probability estimates from tree ensembles. However, TSBF requires simple features (means,
variances) to be extracted from subsequences of time series. Instead, SMTS does not

123

Learning a symbolic representation for multivariate time series classification

405

generate features. Each observation is taken as an instance and the symbols are learned.
An extension of TSBF to the classification of MTS requires an appropriate integration
of features and individual series as additional steps.
3 Background
Decision tree learners are comprehensible models with satisfactory accuracy that are
successfully used in many applications. Univariate trees such as CART (Breiman et
al. 1984) and C4.5 (Quinlan 1993) split data based on only one attribute at each node,
and, thus, are limited to splits that are orthogonal to the attribute’s axis (Brodley and
Utgoff 1995).
Tree ensembles are proposed to mitigate the greedy nature of univariate trees. A
random forest (RF) classifier (Breiman 2001) is used here to partition the feature
space. A RF is an ensemble of J decision trees, {g j , j = 1, 2, . . . , J }. Each tree is
constructed from a different bootstrap sample of the original data. The instances left
out of a bootstrap sample and not used in the construction of a single tree are called
out-of-bag (OOB) instances. At each node of each tree, a RF considers
√ the best split
based on only a random sample of features. Often, the sample size is ν, where ν is
the number of features. The random selection reduces the variance of the classifier,
and√also reduces the computational complexity of a single tree from O(νη log η) to
O( νη log η) (assuming the depth of tree is O(log η) where η is the number of training
(in-bag) instances). Therefore, for a large number of features and instances, a RF can
be as computationally efficient as a single decision tree.
The prediction for instance x from tree g j is ŷ j (x) = argmaxc p cj (x), where p cj (x)
is the proportion of class c in the corresponding leaf of the j-th tree, for c = 0, 1, . . . ,
C − 1. Let G(x) denote the set of all trees in the RF where instance x is OOB. The
OOB class probability estimate of x is



1
I ŷ j (x) = c
p c (x) =
|G(x)|
g j ∈G(x)

where I (·) is an indicator function that equals one if its argument is true and zero
otherwise. The predicted class is ŷ(x) = argmaxc p c (x). The estimates computed
from OOB predictions are easily obtained and have been shown to be good estimates
of generalization error (Breiman 2001).
RF provides a number of desirable properties for the time series problem. Highdimensional feature spaces, nominal features, multiple classes, and missing values are
handled. Nonlinear models and interactions between features are allowed. It is scale
(unit) invariant and robust to outliers, and computations are reasonable even for large
datasets.
4 Time series discretization using tree-based classifiers
A MTS, X n , is an M-attribute time series each of which has T observations where
xmn is the mth attribute of series n and xmn (t) denotes the observation at time t. For

123

406

M. G. Baydogan, G. Runger

illustration purposes, time series are assumed to be of the same length, T , although our
approach can handle time series of different length. MTS example X n is represented
by a T × M matrix as


n
, . . . , xnM
X n = xn1 , x2n , . . . , xm

(1)

where


n
xm
= xmn (1), xmn (2), . . . , xmn (T )
is the time series in column m. There are N training MTS, each of which is associated
with a class label y n ∈ {0, 1, 2, . . . , C − 1} for n = 1, 2, . . . , N . Given a set of
unlabeled MTS, the task is to map each MTS to one of the predefined classes.
4.1 Representation of multivariate time series
Instead of extracting features from each time series, each row of X n is considered
to be an instance in our approach. This is achieved by creating a matrix of instances
D N T ×M where
⎡

D N T ×M

x11 x21 . . . x1M

⎤

⎢ 2 2
⎥
⎢ x x . . . x2 ⎥
M ⎥
⎢ 1 2
⎢
⎥
⎢
⎥
.
⎢
⎥
⎢
⎥
.
=⎢
⎥
⎢ xn xn . . . xn ⎥
M ⎥
⎢ 1 2
⎢
⎥
⎢
⎥
.
⎢
⎥
⎣
⎦
.
N
N
N
x1 x2 . . . x M

(2)

Equation 2 is basically the concatenation of training examples X n in equation 1. We
assign the label of each instance to be the same as the time series.
We map D N T ×M to the feature space  N T ×(2M+1) that adds the following columns:
time index, first differences for each numerical attribute. The row of  for series n at
time index t is


n
n
n
(t), x M
(t) − x M
(t − 1)
t, x1n (t), x1n (t) − x1n (t − 1), . . . , x M



(3)

The differences provide trend information. A tree learner can capture this information
if it relates to the class. This difference is not available for the first observation of
MTS, which is assumed to be missing. If an attribute is nominal, first differences are
not included. For both numerical and nominal values, missing values are handled by
the tree learners. Also, the number of instances may differ across different MTS, but
are assumed to be the same for attributes within an MTS. See Table 1 for an example
representation.

123

Learning a symbolic representation for multivariate time series classification
Table 1 Sample data table for
one univariate time series from
each of three classes

To represent MTS, columns are
generated for time index, each
attribute, first differences for
each numerical attribute, and
class

Series

Time index

Attribute

407
Difference

Class

1

1

0.5

–

1

1

2

0.5

0

1

1

...

1

19

0.7

...

...
0.2

1
1

2

1

−0.4

–

3

2

...

...

...

3

2

19

3

1

−0.3

3

0

–

2

...

0.1

3

...

...

3

19

−0.2

0.1

2
2

A RF tree learner is trained on  assuming that each instance has the same class
label as its time series. This is denoted as R Fins (for RF applied to the instances)
with Jins trees in the forest. Each instance of  is mapped to a terminal node of each
tree g j , j = 1, 2, . . . , Jins . Although the trees of R Fins without any modification
are unpruned, we restrict the number of terminal nodes of each tree to R and that
determines the alphabet size in our approach. The trees are trained in a breadth-first
order. One level of the tree is built at a time and training stops when there are R
terminal nodes. A second parameter is the number of trees Jins . Each tree of R Fins
provides a symbolic representation for the time series.
Figure 2 illustrates the representation from one tree for three univariate time series.
The plot of raw data versus time index is illustrated in Fig. 2a (with the time index
plotted on the horizontal axis and the value from a time series plotted on the vertical
axis) and we refer to this as signal space. The terminal nodes for the example tree
are shown in Fig. 2b to illustrate the partition used for the symbolic representation.
These terminal nodes correspond to the partition of signal space shown in Fig. 2a.
We omit the first differences in the example so that the partition and symbols can be
illustrated on a 2D plot. The symbols are locally sensitive because the time index is a
candidate feature to split a node (as illustrated in Fig. 2b). The bar chart of the symbol
distribution is shown in Fig. 2c.
A simple way to view a time series is as a set of points in the two-dimensional
signal space in Fig. 2a, denoted as S. If there are differences between the classes,
there are regions in S where one class of points dominate. When the RF classifier is
applied to , boundaries in S are learned to partition points from different classes.
Because time is used as a predictor variable, and because RFs can effectively handle interactions, complex regions in S where one class dominates can be detected.
In this sense, the time ordering of the data is used. Rather than one tree, the RF
ensemble is used to handle structures that are not parallel to the coordinate axes.
Each tree of R Fins is trained on a random subsample of features and instances.
Therefore, the final representation includes different views of the same time series.
The representation maps a time series to the high-dimensional space of terminal
nodes (that correspond to regions in S where one class dominates). Because trends

123

408

M. G. Baydogan, G. Runger

(a)

(b)

(c)
Fig. 2 a Time series from three classes are plotted in signal space. b A decision tree learned from the time
series in signal space partitions the space and the terminal nodes provide symbols. c The symbol distribution
for each series is shown in a bar chart

are often important properties in time series, the added first differences improve the
models.

4.2 Classification
After the symbolic representation is generated from the trees in R Fins, a bag-ofwords (BoW) approach is used to classify the time series. Each symbol is simply
considered to be a word and the relative frequency vector of the symbols from each
tree are concatenated and used to classify the time series. This frequency vector from
each tree is normalized by the number of instances in the time series to obtain the
relative frequency vector. More details follow.

123

Learning a symbolic representation for multivariate time series classification

409

Table 2 A visual example of the representation based on symbol frequencies
Tree 1
A1

Tree 2
B1

C1

D1

E1

A2

. . . Tree Jins
B2

C2

D2

E2

. . . .

.

.

.

.

1

0.26 0.37 0.26 0.00 0.11 0.11 0.53 0.21 0.00 0.16 . . . 0.63 0.00 0.11 0.26 0.00

2

0.26 0.11 0.21 0.32 0.11 0.26 0.26 0.00 0.21 0.26 . . . 0.11 0.53 0.21 0.00 0.16

3

0.00 0.37 0.21 0.05 0.37 0.16 0.16 0.21 0.47 0.00 . . . 0.42 0.42 0.00 0.16 0.00

.

.

.

.

.

.

.

.

.

.

.

. . . .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . .

.

.

.

.

N 0.42 0.11 0.16 0.21 0.11 0.16 0.37 0.21 0.00 0.26 . . . 0.21 0.00 0.21 0.37 0.21

Each column denotes a symbol from a tree of R Fins (with R = 5 terminal nodes each), and each row
denotes a MTS. Each table entry is the proportion of time series indices (for the MTS corresponding to the
row) in the terminal node

Let H j (X n ) be the R × 1 relative frequency vector of the terminal nodes from tree
g j for MTS X n . Let the function q j (·) assign a row of  to a terminal node of tree
g j . Each entry of H j (X n ) is the proportion of time indices from MTS n assigned to a
terminal node (say r ) by tree g j . That is, the r th element of H j (X n ) is

T
t=1



I q j (nt ) = r
T

for r = 1, 2, . . . , R, where I (·) is the indicator function and nt denotes the row t of
MTS n in .
We concatenate the H j (X n ) (relative frequency vectors) from each of the Jins trees
g j of R Fins to obtain the final representation of each time series, denoted H (X n ),
of length R × Jins . Table 2 illustrates the representation for symbol frequencies with
the number of terminal nodes R = 5. A classifier is then trained on the H (X n ). The
cardinality of H (X n ) might be large based on the setting of R and Jins . Therefore, a
scalable classifier that can handle interactions and correlations such as a RF is preferred
for this task. This RF is referred as R Fts for which we train Jts trees. To classify a
test MTS, X 0 , the frequency representation H (X 0 ) is obtained and R Fts assigns the
class.

5 Experiments and results
Although our focus is on MTS, SMTS is tested on several UTS and MTS datasets.
These datasets are from domains such as speech recognition, activity recognition,
medicine, etc. The UTS are from Keogh et al. (2011) and provide diverse characteristics
such as the lengths of the series, the number of classes, etc. (as shown in the first
columns in Tables 4 and 5). A total of 22 MTS from CMU (2012), Bache and Lichman
(2013), Keogh et al. (2011) and Olszewski (2012) are used to illustrate the performance

123

410

M. G. Baydogan, G. Runger
0.038
R = 20
0.036

R = 40

0.034

R = 60

R = 40

0.045

R = 60
R = 80
R = 100

0.04

0.035

Test Error Rate

OOB Error Rate

R = 20

R = 80
0.032

R = 100

0.03
0.028
0.026
0.024

0.03
20

40

60

J

ins

80

100

0.022

20

40

60

80

100

Jins

Fig. 3 Average OOB and test error rates with selected R and Jins for StarLightCurves dataset (five replications). As representation size (R × Jins ) increases, marginal improvement in OOB error rates decreases

of our approach on MTS data. The dataset characteristics are given in Table 6. We
randomly selected train and test examples if no default train/test split is provided for
the dataset.

5.1 Parameter settings
Our algorithm does not require the setting of many parameters and it is robust to the
settings. Three parameters of our approach are R, Jins and Jts . RF is insensitive to both
the number of trees and, also the number of candidate attributes scored to potentially
split a node (Breiman 2001). The number of features evaluated at each node of a tree
is set to the default that equals the approximate square root of the number of features.
For R Fins, there are 2 × M + 1 features (assuming that all attributes are numerical).
For R Fts, the number of features is R × Jins . Consequently, the only parameter of
importance is the number of terminal nodes (alphabet size) R. Still, we allow for
different settings for Jins and Jts .
The parameters R and Jins are set in two steps. We first set R based on a mini
experiment, because it determines the level of detail captured. In this experiment,
we evaluate all R levels by running R Fins with 25 trees. Then R Fts with 50 trees
is trained on each representation. The R value providing the best OOB error rate
based on the training data is chosen. After setting R, we generate a representation
for each level of Jins in the second mini experiment. R Fts with 50 trees is trained
on each representation to select the Jins value to minimize the OOB error rate on the
training data. Finally, Jts is set based on the progress of OOB error rates at discrete
number of tree levels (step size of 50 trees). If the OOB error rate does not improve
more than a tolerance level (set to 0.05 times the OOB error rate from the previous
step), we stop adding new trees to RF. We illustrate the behavior of SMTS on the
StarLightCurves dataset with selected values for the representation size (R and Jins
levels) in Fig. 3. Error rates improve as the representation size is increased and stabilize
after a certain level. Also, this confirms our strategy to select R first because the error
rates are primarily affected by the R setting. If there is no concern about the training

123

Learning a symbolic representation for multivariate time series classification

411

time, Jins can be set large. Based on our preliminary experiments, three values are
evaluated for R ∈ {20, 50, 100} and Jins ∈ {20, 50, 100}. After the parameters are
set from only the training data, the model is used for the classification of the test time
series.

5.2 Classification accuracy
5.2.1 Univariate datasets
SMTS is compared to nearest neighbors (NN) classifiers with DTW. Two versions of
DTW are considered: NNDTWBest (Ratanamahatana and Keogh 2004) searches for
the best warping window, based on the training data, then uses the learned window
on the test data, while NNDTWNoWin has no warping window. Note that DTW is a
strong solution for time series problems in a variety of domains (Ratanamahatana and
Keogh 2005). The results for NNDTW classifiers were obtained from Keogh et al.
(2011). We also standardize each time series to zero mean and unit standard deviation.
This adjusts for potentially different baselines or scales that are not considered to be
relevant (or persistent) for a learner.
An unsupervised BoW approach with a codebook derived from K-means clustering is also considered for comparison. In the unsupervised approach, the Euclidean distance between each row of  is computed. K-means clustering with k
selected from the set k ∈ {20, 50, 100, 250, 500} is used to label the observations. Then, we use the frequency of the cluster assignments to generate the codebook. Similar to our approach, we train a RF on the codebook. The results are
reported for the k level providing the minimum OOB error rate on the training
data.
We also compare to a RF classifier applied directly to the times series values. That
is, the number of features is T for each time series. This approach requires time series
of the same length. For all RFs trained with alternative approaches, OOB error rates
at a discrete number of tree levels are considered to set the number of trees. As before,
the number of features evaluated at each node of a tree is set to the default which
equals the approximate square root of the number of features.
The results for a nearest-neighbor classifier with Euclidean distance on a BoP
representation (NNBoP) are from Lin et al. (2012). The results are reported for 20 of
the UCR datasets with the best parameter setting combination selected by Lin et al.
(2012). Consequently, we compare to NNBoP over these 20 datasets and compare to
the other methods over 45 datasets.
We provide the significance levels for Wilcoxon matched-pairs signed-ranks tests
for SMTS and the number of wins/ties/losses against the algorithm in Table 3. Tables 4
and 5 provide detailed results from 10 replications of our algorithm on the test
data.
The performance of SMTS is better than the BoW approach with K-means clustering on univariate datasets. This shows an advantage of our supervised signal space
partition compared to an unsupervised one. Also, SMTS provides better results than
RF on the values. This is expected, due to the problems of supervised learners trained

123

412

M. G. Baydogan, G. Runger

Table 3 Performance of SMTS (average over 10 replications) against competitor approaches, BoW
approach with K-means clustering, TSBF (Baydogan et al. 2013), nearest-neighbor classifiers with dynamic
time warping distance (NNDTWBest and NNDTWNoWin), RF applied directly to time series values and
nearest-neighbor classifier with Euclidean distance on BoP representation (Lin et al. 2012)
BoW

TSBF

K-means
20 datasets
45 datasets

NNDTW
Best

RF

NN

NoWin

BoP

win/tie/lose

13/0/7

6/1/13

12/0/8

14/1/5

16/0/4

12/1/7

Wilcoxon

0.124

0.934

0.131

0.011

0.001

0.102

win/tie/lose

34/2/9

18/1/26

28/0/17

32/1/12

38/0/7

a

Wilcoxon

0.000

0.728

0.038

0.001

0.000

a Reported the error rates over 20 datasets

Table 4 Error rates over 20 datasets, SMTS (average over 10 replications), BoW approach with K-means
clustering, TSBF (Baydogan et al. 2013), nearest-neighbor classifiers with dynamic time warping distance
(NNDTWBest and NNDTWNoWin), RF applied directly to time series values and nearest-neighbor classifier with Euclidean distance on BoP representation (Lin et al. 2012)
# Of classes Dataset size
Train

Length BoW

Test

SMTS K-means

TSBF NNDTW
Best

NoWin

RF

NN
BoP

50Words

50

450

455 270

0.289 0.308

0.209 0.242 0.310

0.348 0.466

Adiac

37

390

391 176

0.248 0.304

0.245 0.391 0.396

0.361 0.432

Beef

5

30

30 470

0.260 0.333

0.287 0.467 0.500

0.300 0.433

CBF

3

30

900 128

0.020 0.022

0.009 0.004 0.003

0.112 0.013

Coffee

2

28

28 286

0.029 0.107

0.004 0.179 0.179

0.007 0.036

2

100

14

560

ECG
Face (all)

96

0.159 0.200

0.145 0.120 0.230

0.184 0.150

1,690 131

100

0.191 0.150

0.234 0.192 0.192

0.190 0.219

Face (four)

4

24

88 350

0.165 0.284

0.051 0.114 0.170

0.211 0.023

Fish

7

175

175 463

0.147 0.051

0.080 0.160 0.167

0.221 0.074

Gun-Point

2

50

150 150

0.011 0.007

0.011 0.087 0.093

0.073 0.027

Lighting-2

2

60

61 637

0.269 0.213

0.257 0.131 0.131

0.244 0.164

Lighting-7

7

70

73 319

0.255 0.315

0.262 0.288 0.274

0.263 0.466

OliveOil

4

30

30 570

0.177 0.100

0.090 0.167 0.133

0.107 0.133

OSU L.

6

200

242 427

0.377 0.331

0.329 0.384 0.409

0.518 0.256

Swedish L. 15

500

625 128

0.080 0.106

0.075 0.157 0.210

0.126 0.198

Synt. Cont.

300

300

0.025 0.020

0.008 0.017 0.007

0.046 0.037

6

60

Trace

4

100

100 275

0.000 0.030

0.020 0.010 0.000

0.165 0.000

Two Pat.

4

1,000

4,000 128

0.003 0.011

0.001 0.002 0.000

0.158 0.129

Wafer

2

1,000

6,174 152

0.000 0.008

0.004 0.005 0.020

0.012 0.003

Yoga

2

300

3,000 426

0.094 0.173

0.149 0.155 0.164

0.191 0.170

Bold values indicate the best performance (i.e. minimum error rate)

on the features from fixed locations and dimensions as discussed by many authors
(Baydogan et al. 2013; Geurts 2001). Obviously, our symbol generation process combined with our BoW representation helps to avoid this problem. SMTS performs

123

Learning a symbolic representation for multivariate time series classification

413

Table 5 Error rates over 25 datasets, SMTS (average over 10 replications), BoW approach with
K-means clustering, TSBF (Baydogan et al. 2013), nearest-neighbor classifiers with dynamic time warping
distance (NNDTWBest and NNDTWNoWin), RF applied directly to time series values
# Of classes Dataset size
Train
ChlorineConc.

3

CinC_ECG_torso

4

Test

467 3,840

Length BoW
SMTS K-means

TSBF NNDTW
Best

RF

NoWin

166 0.328 0.328

0.336 0.350 0.352

40 1,380 1,639 0.100 0.236

0.262 0.070 0.349

0.291
0.250

Cricket_X

12

390

390

300 0.302 0.359

0.278 0.236 0.223

0.427

Cricket_Y

12

390

390

300 0.298 0.297

0.259 0.197 0.208

0.396

Cricket_Z

12

390

390

300 0.259 0.321

0.263 0.180 0.208

0.406
0.093

DiatomSize

4

16

306

345 0.094 0.144

0.126 0.065 0.033

ECG 5 days

2

23

861

136 0.190 0.214

0.183 0.203 0.232

0.210

FacesUCR

14

200 2,050

131 0.157 0.204

0.090 0.088 0.095

0.215
0.551

Haptics

5

155

308 1,092 0.485 0.513

0.488 0.588 0.623

InlineSkate

7

100

550 1,882 0.562 0.562

0.603 0.613 0.616

0.665

ItalyPowerDemand

2

0.096 0.045 0.050

0.033

MALLAT
MedicalImages

8
10

67 1,029

24 0.038 0.090

55 2,345 1,024 0.052 0.034
381

0.037 0.086 0.066

0.082

760

99 0.273 0.345

0.269 0.253 0.263

0.277

0.135 0.134 0.165

0.119

MoteStrain

2

20 1,252

84 0.051 0.126

SonyRobot

2

20

601

70 0.208 0.394

0.175 0.305 0.275

0.321

SonyRobotII

2

27

953

65 0.151 0.197

0.196 0.141 0.169

0.197

StarLightCurves

3

0.022 0.095 0.093

0.052

Symbols

6

25

995

398 0.035 0.104

0.034 0.062 0.050

0.148

TwoLeadECG

2

23 1,139

82 0.027 0.036

0.046 0.132 0.096

0.268

uWaveGesture_X

8

896 3,582

315 0.180 0.217

0.164 0.227 0.273

0.245

uWaveGesture_Y

8

896 3,582

315 0.257 0.293

0.249 0.301 0.366

0.314

uWaveGesture_Z

8

896 3,582

315 0.242 0.275

0.217 0.322 0.342

0.290

WordsSynonyms

25

267

638

270 0.399 0.409

0.302 0.252 0.351

0.439

Thorax1

42

1,800 1,965

750 0.099 0.126

0.138 0.185 0.209

0.123

Thorax2

42

1,800 1,965

750 0.071 0.102

0.130 0.129 0.135

0.090

1,000 8,236 1,024 0.025 0.026

Bold values indicate the best performance (i.e. minimum error rate)

better for most of the datasets when compared to NN approaches. We are performing
slightly better than NNBoP approach over the 20 datasets considered. TSBF outperforms SMTS in most of the UTS datasets, but there is not a significant difference. The
good performance of TSBF lies in its own representation, because each time series is
represented with multiple subsequences from which features are extracted. This allows
for mechanisms to handle the possible problems related to noise, translation and dilation of patterns. We mentioned previously that additional steps would be needed to
extend TSBF to multivariate series which are the focus here, and this is discussed in
a following section.
Although not shown here, SMTS experiments with and without difference features
were also conducted. SMTS with difference features provided better results for 39
datasets of the 45 datasets and this illustrates the benefits of the difference information.

123

414

M. G. Baydogan, G. Runger

Table 6 Characteristics of MTS: number of classes, number of attributes, length of time series, number of
training instances and number of testing instances
# of
Classes

# of
Length
Attributes

Dataset size

AUSLAN

95

22

1140

1425

Pendigits

10

2

8

300

10692

9

12

7–29

270

370

4

6

15

38

50

LP2

5

6

15

17

30

LP3

4

6

15

17

30

LP4

3

6

15

42

75

LP5

5

6

15

64

100

2

2

39–152

100

100

Japanese Vowels

45–136

Train

CV

Source

Tenfold

Bache and
Lichman
(2013)

Fivefold

Bache and
Lichman
(2013)

Tenfold

Olszewski
(2012)

Tenfold

CMU (2012)
Bache and
Lichman
(2013)

Test

Robot Failure
LP1

ECG
Wafer

2

6

104–198

298

896

CMU_MOCAP
_S16
ArabicDigits

2

62

127–580

29

29

10

13

4–93

6600

2200

×

CharacterTrajectories 20

3

109–205

300

2558

×

LIBRAS

2

45

180

180

×

15

Test performance is also reported for all datasets. Column “CV” indicates if comparisons are also based on
cross-validation. The source of the datasets are in the last column

5.2.2 Multivariate datasets
The MTS datasets used to evaluate SMTS are commonly used by other MTS classification studies. However, some researchers downsample certain datasets to fewer
classes or instances due to the high number of classes [e.g., Weng and Shen (2008)
used instances from 25 classes of AUSLAN]. Moreover, some algorithms preprocess
the data for different purposes such as smoothing or obtaining an appropriate representation [e.g., Bankó and Abonyi (2012), Weng and Shen (2008) truncated some
datasets to obtain time series of same length]. We compare SMTS to the approaches
which do not preprocess the data.
Most of the MTS studies follow different strategies for experimentation which complicates the comparisons. For instance, Lin et al. (2012) and Orsenigo and Vercellis
(2010) evaluated the performance using cross-validation (CV). To have a fair comparison with the competitor algorithms, we also follow their experimental strategy. The
datasets for which CV is performed are given in Table 6. For the CV, we combine the
training and test data to obtain a single dataset. We replicate the CV five times and
report average error rates.

123

Learning a symbolic representation for multivariate time series classification

415

The motif discovery approach by McGovern et al. (2011) works on binary classification problems and the critical success index (CSI) by Schaefer (1990) is used to
assess the performance of the approach. This measure evaluates success as the ratio
of the number of true positives to the number of instances that are predicted to be positive. In order to make this approach comparable to SMTS, we modified the proposed
approach and use the accuracy as the performance measure. As a binary classifier, we
are able to run the approach on only three datasets in Table 6. There are five parameters
of the approach: alphabet size ∈ {3, 4, . . . , 8}, word size ∈ {2, 3}, the number of time
steps averaged into piecewise aggregation ∈ {1, 2, . . . , 5}, minimum probability of
detection (POD) as 0.9 and maximum false alarm ratio (FAR) as 0.8. This yields 60
different parameter combinations for the experiment. The error rates are reported for
the parameter setting that provides the best accuracy on training data over tenfold CV.
This approach required an average of 18 hours per dataset with the same hardware
used for SMTS. Computational times are discussed in Sect. 5.3.
We also compare SMTS with 1-NN classifiers with DTW and a multivariate extension of TSBF (MTSBF) (Baydogan et al. 2013) on the test data. For DTW calculations,
each time series is standardized to have a mean of zero and a standard deviation of
one before distance computations. The DTW distance between two MTS is taken to
be the sum of the DTW distances between the associated univariate time series. TSBF
generates a representation based on the features extracted from univariate time series
in its original implementation. For MTSBF, we generate a representation for each
attribute of MTS and then concatenate these representations columnwise to obtain the
final representation for MTS. Therefore, this requires running TSBF on each attribute.
The only important parameter of TSBF is the subsequence length factor (z in TSBF’s
notation) which determines the minimum length of the subsequences to be used for
feature extraction. The other parameters are kept as recommended by Baydogan et al.
(2013). Evaluated z values are z ∈ {0.25, 0.5, 0.75}. For each univariate series, this
parameter is selected based on the procedure described by Baydogan et al. (2013).
The codes for MTSBF are available on Baydogan (2013). MTSBF generates a representation for each attribute of MTS which requires running TSBF M times. This may
not be adequate for MTS with a large number of attributes.
Table 7 summarizes the results from our CV experiments and reported error rates
from other references. SMTS performs consistently better when compared to the classification approaches considered by Orsenigo and Vercellis (2010). The best error rate
for AUSLAN reported by Kadous and Sammut (2005) is from an ensemble of 11 different classifiers trained on the extracted metafeatures. SMTS performs equally well for
this particular dataset. Also, Lin et al. (2012) reported error rates for two versions of
NN classifiers with DTW: NNDTW-Best and NNDTW-NoWin. SMTS outperforms
the similarity-based approaches for these two datasets.
For binary classification problems, SMTS outperforms the predictive motif discovery method from McGovern et al. (2011) with an approach that is conceptually and
operationally quite simple. Our experiments show that the motif discovery approach is
sensitive to the setting of the parameters and has problems with overfitting. The parameters are set based on the suggestions and experiments in McGovern et al. (2011). It
has the potential to perform better with different settings and how parameters should
set to avoid overfitting is further discussed by McGovern et al. (2011).

123

123

Pendigits

0.010

0.027

0.007

Wafer

AUSLAN

CMU_MOCAP_S16

0.034

0.329

0.145

0.319

0.362

0.148

0.037

0.054

0.379

0.128

0.342

0.362

0.182

0.066

0.077

0.348

0.137

0.383

0.404

0.182

0.055

0.091

0.189
0.066

0.172

BestWin

NoWin

1NNWD

TDVM

SVMDTW

NNDTW (k=3) Lin et al. (2012)

Orsenigo and Vercellis (2010)

Best CV error rates reported by other MTS classification papers

0.261

0.063

LP4

0.134

0.189

LP3

LP5

0.384

LP2

ECG

0.062

LP1

Robot failure

0.035

0.010

Japanese Vowels

SMTS

Table 7 CV error rates for SMTS (average over five replications)

0.021

Voting

Tclass Kadous and Sammut (2005)

0.480

0.130

0.241

Motif

McGovern et al. (2011)

416
M. G. Baydogan, G. Runger

Learning a symbolic representation for multivariate time series classification

417

Table 8 Test error rates for SMTS (average over 10 replications), nearest-neighbor classifier with dynamic
time warping distance, and a multivariate version of a bag of features method
SMTS

NNDTW
NoWin

MTSBF

Other

CMU_MOCAP_S16

0.003

0.069

0.003

AUSLAN

0.053

0.238

0.000

ArabicDigits

0.036

0.092

–

0.069 by Hammami and Bedda (2010)

Japanese Vowels

0.031

0.351

–

0.032 by Bicego et al. (2009),
Geurts (2001), 0.059 by
Kudo et al. (1999)

LP1

0.144

0.280

–

LP2

0.240

0.467

–

LP3

0.240

0.500

–

LP4

0.105

0.187

–

LP5

0.349

0.480

–

Wafer

0.035

0.023

0.015

CharacterTrajectories

0.008

0.040

0.033

uWaveGestureLibrary

0.059

0.071

0.101

LIBRAS

0.091

0.200

0.183

ECG

0.182

0.150

0.165

Pendigits

0.083

0.088

–

Robot Failure

Best error rates reported by other MTS classification papers

The error rates on the test data are given in Table 8. The datasets are sorted (descending) based on the number of attributes to illustrate the effectiveness of SMTS. SMTS
provides better results for the datasets with larger number of attributes, while the performance is comparable for the remaining datasets when compared to similarity-based
approaches.
SMTS performs better than MTSBF for some datasets where MTSBF significantly
outperforms for the others. TSBF generates a representation for each attribute without
taking the others into consideration. However, as discussed in Sect. 2, this approach
is greedy because the relationships between the attributes may describe the class.
Depending on the problem characteristics, individual handling of the attributes may
provide inferior results. For example, certain classes from uWaveGestureLibrary are
defined by the patterns over multiple time series as shown by Baydogan (2012) and
MTSBF provides worse results for this particular dataset. SMTS performs equally well
on the ArabicDigits and Japanese Vowels dataset when compared the other studies in
the literature.
5.3 Computational time analysis
Here we empirically evaluate the runtime of SMTS with different settings of problem
characteristics and parameters. SMTS is implemented in both C and R Software and
our experiments use an Ubuntu 12.04 system with 16 GB RAM, quad core CPU

123

418

M. G. Baydogan, G. Runger

(i7-3620M 2.7 GHz). Although the CPU can handle eight threads in parallel, only
two threads are used. The parallel implementation of SMTS is also available from
Baydogan (2013).
The overall computational complexity of SMTS is mainly determined
by R Fins
√
and R Fts. The time complexity of building a single tree in RF is O( νηβ), where ν is
the number of features, η is the number of instances, and β is the depth of the tree. For
R Fins, ν = 2M + 1 is the number of features, η = (N × T ) is the number of training
instances and β = R−1 is the depth of the tree in the worst case
√assuming that the depth
takes the largest possible value. Thus, complexity is O[Jins 2M + 1(N T )(R − 1)]
in the worst case. For R Fts, η = N , ν =
√ R × Jins and β = log N (assuming the
depth of tree is O(log N )) which is O(Jts R × Jins N log N ).
The StarLightCurves dataset from Keogh et al. (2011) is used to demonstrate the
effect of the parameters Jins , N , T and R on the computation times. For the multivariate case, SMTS’s performance as a function of the number of attributes M, is
illustrated on the AUSLAN dataset from Bache and Lichman (2013). For each dataset,
we randomly selected δ ∈ {0.2, 0.4, . . . , 1} proportion of the number of instances
(δ N ), number of observations (δT ) and number of attributes (δ M ). The levels considered for R and Jins are R ∈ {20, 40, 60, 80, 100}, Jins ∈ {20, 40, 60, 80, 100}. Here
10 replications are conducted for each setting combination.
We first illustrate the computation time required to generate symbols. Figure 4a,
b schematize the average symbol generation time across a variety of settings. The
computation times with changing N and T are analyzed for fixed values of R = 100
and Jins = 100. If both N and T increase, the symbol generation time is expected
to increase quadratically (because the number of instances for R Fins is N × T ).
However, the symbol generation works faster in practice. This is due to the nature of
the time series. The time to sort all observations in each dimension prior to building
the tree does not increase drastically. Since the values are in some fixed range, a faster
computation time is achieved by avoiding the comparisons during the sorting process.
We also analyzed the symbol generation times with selected values of R and Jins with
fixed values of δ N = 1 and δT = 1. From the computation times in Fig. 4b, the practical
complexity of symbol generation is approximately linear on both R and Jins settings.
The complexity of training is also evaluated with changing R and Jins where δ = 1
for the remaining parameters. Here R and Jins determine the representation size.
Figure 4c illustrates the average training time with these parameters. The time for
training increases linearly with the increase in R and Jins which is consistent with the
complexities of R Fins and R Fts. Furthermore, the linear behavior of the training
time with the increase in the representation size (i.e. both R and Jins ) is an advantage
of the proposed approach. This behavior is due to the selection of the square root of
the number of features to evaluate at each split node by R Fts.
SMTS symbol generation times for selected numbers of attributes are illustrated
in Fig. 4d when other parameters√are fixed. The runtime increase with the number
of attributes is consistent with O( 2M + 1) from R Fins. The discrete levels of M
evaluated are M ∈ {4, 8, 13, 17, 22} which explains the near-linear behavior of the
symbol generation times.
The median training time is 16.4 s per dataset with a minimum of 0.8 s and a maximum of 1381.9 s (for SonyRobot and Thorax2, respectively) for univariate datasets.

123

Learning a symbolic representation for multivariate time series classification

80

(b) 80

δT = 0.2

70

Jins = 20

δT = 0.4

60

δT = 0.6

50

δT = 0.8

40

δT = 1

Symbol Generation Time (sec)

Symbol Generation Time (sec)

(a)

30
20
10
0

0.2

0.4

0.6

0.8

70

ins

= 40

60

Jins = 60

50

Jins = 80
Jins = 100

40
30
20
10

1

J

20

40

δ

60

80

100

0.8

1

R

N

(d)20
J

= 20

J

= 40

J

= 60

J

= 80

J

= 100

ins

160

ins

140

ins
ins

120

ins

Symbol Generation Time (sec)

(c) 180
Training Time (sec)

419

100
80
60
40
20

40

60

R

80

100

18
16
14
12
10
8
6

0.2

0.4

0.6

δM

Fig. 4 Average symbol generation times with changing N and T (R = 100, Jins = 100) and R and
Jins (δ N = 1, δT = 1) (top) and with changing M (R = 100, Jins = 100, δ N = 1, δT = 1) (bottom
left). Overall training time with changing R and Jins (δ N = 1, δT = 1) (bottom right)

For multivariate datasets, the median training time is 9.8 s per dataset with a minimum
of 0.6 s and a maximum of 2025.7 seconds (for LP2 and ArabicDigits, respectively).
We did not evaluate the time for testing since our approach is very fast in classification
of the series (takes less than a millisecond). It only requires the traversal of the trees
from R Fins and R Fts after feature representation. SMTS is very fast and convenient
for real-time classification of time series data.

5.4 Missing values
For time series with missing values, missing values are usually interpolated. However,
the estimation method itself adds an additional parameter to the time series problem.
Our proposed approach naturally handles the data with missing values, without any
additional steps, because tree-based learning implicitly handles the attributes with
missing values (Breiman et al. 1984).
CBF, GunPoint, MoteStrain, Symbols and ECG 5 days datasets are selected to
illustrate the performance of SMTS when there are missing values. We randomly

123

420

M. G. Baydogan, G. Runger
0.4
CBF
GunPoint
MoteStrain
Symbols
ECGFiveDays

0.35

Error rate

0.3
0.25
0.2
0.15
0.1
0.05
0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Proportion of the missing observations

Fig. 5 The test error rates with different proportions of missing values in the training observations for five
univariate datasets from Keogh et al. (2011). SMTS does not require a specific mechanism to handle the
missing values and it is insensitive to large proportions of missing values

removed δ ∈ {0.05, 0.1, . . . , 0.5} proportion of the values for each time series. The
error rates over 10 replications are provided in Fig. 5. For these datasets, SMTS
performs reasonably well even with large proportions of missing values.

6 Conclusions
The representation of the complex data in MTS is an important challenge for many
methods. The SMTS here does not require pre-defined time intervals or features.
Instead, the symbolic representation is learned, and all attributes of MTS are considered simultaneously during a supervised process. Thus, relationships between the individual attributes are taken into account. Furthermore, the initial representation of raw
data (and first differences) is quite simple conceptually and operationally. Still, a RF
can detect interactions in the space S of time index and time value and this is exploited
to generate a codebook. The codebook is processed with a second RF where now the
implicit feature selection is exploited to handle the high-dimensional input. The constituent properties yield an approach quite different from current methods. Moreover,
MTS with nominal and missing values are handled efficiently with tree learners.
Ensemble learners that scale well with large number of attributes and long time
series make SMTS computationally efficient. Our experiments demonstrate the effectiveness of the proposed approach in terms of accuracy and computation times for
MTS, and that the approach is approximately equal to the best alternatives for univariate time series. Although not explored here, the proposed representation can be used
for similarity analysis, and tasks such as clustering.
Acknowledgments

123

This research was partially supported by ONR Grant N00014-09-1-0656.

Learning a symbolic representation for multivariate time series classification

421

References
Akl A, Valaee S (2010) Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, compressive sensing. In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on, pp 2270–2273, March
Bache K, Lichman M (2013) UCI machine learning repository. University of California, School of Information and Computer Science, Irvine, CA. http://archive.ics.uci.edu/ml
Bankó Z, Abonyi J (2012) Correlation based dynamic time warping of multivariate time series. Expert
Systems with Applications 18(5):231–241
Baydogan MG (2012) Modeling time series data for supervised learning. PhD thesis, Arizona State University, Dec.
Baydogan MG (2013) Multivariate time series classification. homepage: www.mustafabaydogan.com/
multivariate-time-series-discretization-for-classification.html
Baydogan MG, Runger G, Tuv E (2013) A bag-of-features framework to classify time series. Pattern
Analysis and Machine Intelligence, IEEE Transactions on 35(11):2796–2802
Bicego M, Pekalska E, Tax DMJ, Duin RPW (2009) Component-based discriminative classification for
hidden Markov models. Pattern Recognition 42(11):2637–2648
Breiman L (2001) Random forests. Machine Learning 45(1):5–32
Breiman L, Friedman J, Olshen R, Stone C (1984) Classification and Regression Trees. Wadsworth, Belmont,
MA
Brodley C, Utgoff P (1995) Multivariate decision trees. Machine Learning 19(1):45–77
Chakrabarti K, Keogh E, Mehrotra S, Pazzani M (2002) Locally adaptive dimensionality reduction for
indexing large time series databases. ACM Trans. Database Syst. 27(2):188–228
Chaovalitwongse W, Pardalos P (2008) On the time series support vector machine using dynamic time
warping kernel for brain activity classification. Cybernetics and Systems Analysis 44:125–138
Fu T-C (2011) A review on time series data mining. Engineering Applications of Artificial Intelligence
24:164–181
Geurts P (2001) Pattern extraction for time series classification. Principles of Data Mining and Knowledge
Discovery, volume 2168 of Lecture Notes in Computer ScienceSpringer, Berlin / Heidelberg, pp 115–127
Hammami N, Bedda M (2010) Improved tree model for arabic speech recognition. In Computer Science
and Information Technology (ICCSIT), 2010 3rd IEEE International Conference on, volume 5, pages
521–526, July
Kadous MW, Sammut C (2005) Classification of multivariate time series and structured data using constructive induction. Machine Learning 58:179–216
Keogh E, Zhu Q, Hu B, Y. H, Xi X, Wei L, Ratanamahatana CA (2011) The UCR time series classification/clustering. homepage: www.cs.ucr.edu/~eamonn/time_series_data/
Kudo M, Toyama J, Shimbo M (1999) Multidimensional curve classification using passing-through regions.
Pattern Recognition Letters 20(1113):1103–1111
Kuksa PP (2012) 2d similarity kernels for biological sequence classification. In ACM SIGKDD Workshop
on Data Mining in Bioinformatics
Li C, Khan L, Prabhakaran B (2006) Real-time classification of variable length multi-attribute motions.
Knowledge and Information Systems 10:163–183
Li C, Khan L, Prabhakaran B (2007) Feature selection for classification of variable length multiattribute
motions. In Multimedia Data Mining and Knowledge Discovery, pages 116–137. Springer London
Lin J, Keogh E, Lonardi S, Chiu B (2003) A symbolic representation of time series, with implications for
streaming algorithms. In In Proceedings of the 8th ACM SIGMOD Workshop on Research Issues in
Data Mining and Knowledge Discovery, pp 2–11. ACM Press
Lin J, Keogh E, Wei L, Lonardi S (2007) Experiencing SAX: a novel symbolic representation of time series.
Data Mining and Knowledge Discovery 15:107–144
Lin J, Khade R, Li Y (2012) Rotation-invariant similarity in time series using bag-of-patterns representation.
Journal of Intelligent Information Systems, pp 1–29
Lin J, Williamson S, Borne K, DeBarr D (2012) Pattern recognition in time series. In Advances in Machine
Learning and Data Mining for Astronomy, Chapman & Hall, To appear.
Liu J, Wang Z, Zhong L, Wickramasuriya J, Vasudevan V (2009) uWave: Accelerometer-based personalized
gesture recognition and its applications. Pervasive Computing and Communications, IEEE International
Conference on 0:1–9

123

422

M. G. Baydogan, G. Runger

McGovern A, Rosendahl D, Brown R, Droegemeier K (2011) Identifying predictive multi-dimensional
time series motifs: an application to severe weather prediction. Data Mining and Knowledge Discovery
22:232–258
Moosmann F, Nowak E, Jurie F (2008) Randomized clustering forests for image classification. IEEE
Transactions on Pattern Analysis and Machine Intelligence 30:1632–1646
Olszewski RT (2012) http://www.cs.cmu.edu/~bobski/. accessed: June 10
Ordonez P, Armstrong T, Oates T, Fackler J (2011) Using modified multivariate bag-of-words models to
classify physiological data. In Proceedings of the 2011 IEEE 11th International Conference on Data
Mining Workshops, ICDMW ’11, pages 534–539, Washington, DC, USA, IEEE Computer Society.
Orsenigo C, Vercellis C (2010) Combining discrete svm and fixed cardinality warping distances for multivariate time series classification. Pattern Recognition 43(11):3787–3794
Quinlan JR (1993) C4.5: Programs for Machine Learning. Morgan Kaufmann,
Ratanamahatana C, Keogh E (2004) Making time-series classification more accurate using learned constraints. In Proceedings of SIAM International Conference on Data Mining (SDM04), pp 11–22
Ratanamahatana C, Keogh E (2005) Three myths about dynamic time warping data mining. In Proceedings
of SIAM International Conference on Data Mining (SDM05), volume 21, pp 506–510
Sakoe H (1978) Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing 26:43–49
Schaefer JT (1990) The Critical Success Index as an Indicator of Warning Skill. Weather and Forecasting
5(4):570–575
Shieh J, Keogh E (2008) isax: indexing and mining terabyte sized time series. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages
623–631, New York, NY, USA, ACM.
CMU Graphics Lab Motion Capture Database. Homepage: mocap.cs.cmu.edu, 2012
Weng X, Shen J (2008) Classification of multivariate time series using locality preserving projections.
Knowledge-Based Systems 21(7):581–587

123

Proceedings

of the 1993

G. W. Evans,

Winter

Simulation

M. Mollaghasemi,

E.C.

THE

A NEW

Conference

Russell,

W.E.

Biles

THRESHOLD

APPROACH

(eds.)

BOOTSTRAP:

TO SIMULATION

OUTPUT

ANALYSIS
T. R. Willemain

Y. B. Kim
Department
New

Mexico

J. Haddock

of Mathematics
Institute

G. C. Runger

of Mining

Department of Deeision Sciences
and Engineering Systems
Rensselaer Polytechnic Institute
Troy, NY 12180

and Technology
Socorro,

NM 87801

2

ABSTRACT

BOOTSTRAP

METHODS

FOR

TIME

SERIES
The threshold bootstrap (TB) is a promising
method of inference for a single autocorrclatcd

ncw
data

Rcsampling

tcchniqucs, such as the jackkn~e
and
[Efron 1982, IAger et al. 1992],
have achicvcd wide acceptance as nonparametric
The conventional
bootstrap
variance estimators.

boots[rap

series, such as the output
of a discrctc event
simulation.
The method works by rcsampling runs of
data created when the series crosses a threshold Icvcl,
such as the series mean.
We performed a Monlc
Carlo evaluation of the TB using three LYPCSof data:
white noise, first-order autorcgrcssivc, and delays in
The results show that tbc TB
an M/M/l
queue.

analy~cs pseudo-data constructed by resampling the
original data. However, the conventional
bootstrap
assumes indcpcndcnce

1

of the sample mean and valid

A

For instance, a simulation
processing

a single replication

[Liu

steel

workstation

chosen

[1992]

detailed

resampling

three

techniques

to

influence

1992, Kiinsch

problcm

to be m-dependen[,

[Muro

are crca~ed by concatenating

by rcsampling

and Singh

theoretical

about eight hours for

on a SPARC

Kim

Icngth, and pseudo-data
blocks

replications.

of the entire Kawasaki

plant in Japan tics

by

to extending

data, which is

output.

autocorrclatcd
data sets. In the first approach, an
ARIMA
model is fit to the data, then pseudo-series
arc crcatcd by rcsampling residuals and adding them
to Lhc lilted model [Thorns and Schucany 1990]. In
the
Lhc second approach, the moving block bootstrap,
data series is divided into adjacent blocks of fixed

The inherent autocorrclation
in simulation
ou[put
Onc
creates a challcngc
for statistical infcrcncc.
method of handling
this problcm
is indcpcndcnt
replications.
However, in some situations of inwrcsl
to generate multiple

survey

approaches

confidence

INTRODUCTION

it is not practical

of the original

not the case in simulation

produces accurate and tight estimates of the standard
deviation
intervals.

me(hods

without
1989].

replacement
A significant

is that the data series are assumed
i.e., only the last m data values

the current datum.

This assumption

is not

true for heavily loaded queucing systems, which have
functions.
slowly
decaying
autocorrclation
Furthermore, Lhc choice of block size is not a simple
bootstrap
matter. In the third approach, the stationary
[Politis
and Romano
1992], the data series Me
rcsamp]cd by concatenating
blocks whose starting

et al. 1991].
Fur[hcrmore,
it is inefficient
to usc
indcpcndcnt replications and discard the observations
in the transient period for all replications.
The afternativcs to indcpcndcnt rcpl ications arc
single replication methods. Batch means is the most
However, its
common single replication
method.
major drawback is that it requires substantial effort to

point is chosen at random and whose length is
geometrically
distributed with some chosen mean p.

determine the proper batch size [Sargent ct al. 1992].
Another approach is ARIMA
(time series) modeling

The choice of mean block length crcatcs the same
problcm as the choice of fixed block length in the
moving block bootstrap.
Kim, Haddock and Willemain
[1993] described

of simulation output data. ARIMA
modeling is not
WC1l suited for widespread
usc because ARIMA
models are inhercntfy complex and the parametric
assumptions for ARIMA
modeling may not bc valid
for a particular
simulation.
Wc propose a ncw
nonparametric
method for infcrcncc from a single
simulation run.

boomtrup
for binary data.
The binary
the binary
bootstrap rcsamplcs altcmatcly from the runs of zeros
and ones that comprise any binary series. This is a
simpler apprw~ch to dividing the data into chunks for

49R

498

The Threshold

resampling.

Rather

than

Bootstrap:

taking

fixed

A New Approach

sti

blocks

or

distributed blocks, the data defiie runs
that are used for resampling.
Empirical
mmparisons
of the binary bootstrap
geometrically

with the batch means method were favorable
binary

bootstrap.

autoeorrelation)

For

Bernoulli

and first-order

the binary

bootstrap

(no

processes, the

Figure

is that it does not require

3

THE

i:
i;
ill
i+
i:
1:

the

We next compared
of original

data series

2 shows

BOOTSTRAP

threshold

bootstrap

works

as follows

boo&rapped

concatenating
the

runs chosen alternately

populations

Resample

of

the

replacement.

high

runs

and

at

low

1.0

ACF IPHI=.8 , N=500)
10 WXSTRAP lilIPLICAIIONS
C41 ** I.M
C42 ** w
ma J* IAt
04 ,* IAt
C45 ** IA!
m6 ?* Ml
C07 V* I.M
C28 *O m
Cb$ ,* IA21
C20**I.M

-. 5
-1.0’

runs

LA6

when their total length exceeds N.
Compute

Step 5:
Step 6

sample mean.
Repeat Steps 3 and 4 a totat of B times.
Analyze the statistics from Steps 4 and 5 as
replications.
if they were from independent

4

PLAUSIBILITY

We began

by conducting

concatenation

by crossing

a threshold

structure
which

fust

of a data

AR1SWLE
1.0

a plausibility

preserved

plots

2s1 ?* LMi
C$2 ?*
c+% ● *
134 ,*
c% **
c% **
(37 Ve
No 9*
C39 J*
C202 **

.5

randomly

study

to

defined

the autoeorrelation

In this

autoregressive

the time

ACF (PHI=.8 . N=500)
10 INDEPENDENT REPLICATIONS

0.0

of blocks

series.

used frost-order

compared

such as the

STUDY

see whether

data

data

0.0

from

Step 4:

statistic,

AR1

with

Truncate the concatemted

the desired

functions
of

.s

runs.

random

replications

plausibility

study,

(AR 1) dat.zI we

of the original

data

I

IAc

Figure

of ten

Ml SWLE

I

Step 1: Select a threshold value, such as the sample
mean.
Step 2: Divide the series into runs that are either
above or below the threshold (high and low
runs).
Step 3: Create
a
bootstrap
replication
by

function

function

series.

Autocorretation

2:

independent
The

the autoeorrelation
and pseudodata

the autocorrelation

Figure

Step O: Obtain a time seriI& with N values.

m

*

binary

(TB).

bootstrap

THRESHOLD

or AR1 series and Imotstrap

ARl WTA AND 5 SOOTSTRAP REPLIMTIIDNS
PHI=.8 , THREWLO=MEM

an

bootstrap with binary time series, we investigated a
generalization
that does not require binary data or
non-binary data clipped to binary form. We call the
the threshold

1: Tlmeplots

replications

One advantage of

extensive search for proper batch size.
Encouraged
by the success of

generalization

499

series and the pseudo-series.

and D/M/10 queues, the confidence intervals of both
methods were comparable in coverage, mean halfof half-width.

Analysis

series and several pseudo-series which were generated
Figure 1 shows Ithat there
by the threshold bootstmp.
were no gross differences between the original data

binary bootstrap produced excellent estimates of the
standard deviation of the sample mean. For a heavily
loaded M/M/l
queue, the binary bootstrap produced
valid cxmfidence intervals for the probability of long
delay, at run lengths smaller than required by the
batch means method. With longer runs fmm M/M/l

width and stability

Output

for the

trials

Markov

to Simulation

LAG
w’
I.M
w?
LM
lilt
LAt
w
w

of
and

Kim

500

independent
replications

ARl
series and the
of one of the independent

sets of autocorrelation
and variability.
5

MONTE

CARLO

functions

et al.

manufacturing

bootstrappcd
series. Both

were similar

in shape

replication

EVALUATION

(IR).

the TB to work
lengths.

with
The

series had 20,000 points.
The qucucing
data series had both 20,000 and
100,000 points; by the standards of single-series
methods in discrete event simulation,
even 100,000
white

noise

and AR1

data

points represent a modest series length. In general for

Encouraged
by the plausibility
study,
wc
proceeded to Monte Carlo comparison of the TB with
independent

and challenged

very slowly decaying autocomelations.
The study used series of various

single series methods of inference, the greater the
degree of autocorrelation,
the longer the run lengths
nccdcd to achieve stable estimates.
For each type of series, the Monte Carlo study

We focused on whether

the TB accurately estimated he standard deviation of
the sample mean and achieved nominal 90% coverage
for confidence intervals for the sample mean. The
study used three kinds of data white noise, first-order
autoregressive time series (i.e., Xt = +X,-l+%) with

used 100 independent replicates. The IMSL package
gcncrwcd the random deviates. Wc performed B=500
replications of the TB on each of the 100 independent
rcplicatcs, thereby generating
100 estimates of the

parameter @O.8, and delays before service in an
M/M/l
queue with utilization
p=O.8. Wc studied
white noise to confirm that the TB results malch

standard

deviation

confidence

intervals.

of

the

sample

mean

and

100

We compared the TB estimates

against known theoretical

values and against IR. Our

conventional rcsuks in the case of no autocorrclation.
The AR 1 data resembled series cncountcrcd in finance
The M/M/l
data
and statistical process control.

cxpcricncc is that the number of bootstrap replications
B must bc 500 or more; some trials with B=1OO did

represented

not produce valid confidence

the type of data cncountcrcd

in

Table 1 summarizes

‘I’able

1:

Summary

of Simulation

intervals.

the Monte

Carlo results.

Results

“~ype of Data
Statistic

White

# Data Points
Average

20,000

of Sample

M/M/l

p = .8

20,000

M/M/l

p = .8

100,000

.000
-.oo3t.oo5
-.W1 ~.()()6

3.200
3.217i.041

3.200
3.187+.016
3.189+.016

3.217+.039

Mean

Interval

.007

.035

.314*

.141*

.007
.~7*.t)f)o+

.031
.035 f.000+

.248
.248 f.013

.099
. 109*.OO2

.900
.890*.052

.900
.940*.039

.900
.900*.050

Coverage

Nominat
TB ~ 1.65 SE
* Asymptotic

20,000

.000
-.000t.ool
-.000+.001

Theory
IR
TB f 1.65 SE
Confidence

AR1 (j = .8

Mean

Theory
IR * 1.65 SE
TB ~ 1.65 SE
SD of Sample

Noise

approximation

.900
.870&055
[Whitt

1989]

Note Independent Replications (IR) results based on 100 indcpcndcnt
based on B=500 bootstrap replications.

replications.

Threshold

Bootstrap

(TB) results

The Threshold

The seetion labeled “Average

Bootstrap:

A New Approach

to Simulation

Kim,

labeled “SD of Sample Mean” shows that the TB
produces estimates of uncertainty
comparable
in
accuracy to HZ. The section labeled “Confidence
Interval
Coverage”
shows that TB
confidence
intervals are valid, even when using sample sizes rhat
are small by conventional simulation standards (e.g.,
M/M/l
with N=20,000 points). Overall, the results in
Table 1 document that the TB achieves with single
samples the kind of results otherwise available with
IR.

Y.
B.,
Willemain.
Inference

SUMMARY

AND

with

The

threshold

Dependence.

is a promising

new

autocorrckitcd

data

for single

series, such as arise in large
discrete
event
simulations.
The method works by rcsampling runs
of data created when the series crosses a threshold
level, such as the series mean.
Our plausibility
study showed that TB pscudoseries have the same appearance
and,
more
importantly,
the same autocorrelation
structure as the
data from
Carlo

which

evaluation

they are gcncratcd.
showed

Binary

Data”,

and Computation,

22,

In

Exploring

R. LePage

the

and L.

Lim”ts

Billard

of

(eds.).

Wiley,

CONCLUSIONS

bootstrap

of inference

Autocorrelated

Simulation

Kunsch,
H. R. 1989. “The Jackknife
and the
Bootstrap for General Stationary Observations”,
Ann. Statist., 17, 1217-1241.
Liu,
R., and K. Singh.
1992. Moving
Blocks
Jackknife
and
Bootstrap
Capture
Weak

L6gcr,
method

Institute, Troy,

J.
Haddock,
and
T.
R.
1993.
“The
Binary
Bootstrap

Comm Stat:
205-216.

Bootstrap.
6

501

Anulysis

Sciences, Rensselaer Polytechnic
NY.

of Sample Mean” shows

that the TB achieves unbiased estimates equivalent
imprecision
to those achieved by IR.
The section

Output

Our

Ncw York.
and J. P. Romano.
C., D. N. Politis,
1992. “Bootstrap Technology and Applications”,

Tcchnometrics,

Muro,

34, 378-398.

Z.,
N. Yanagihara,
1991. “Super Simulation
Proceedings

B.L.

Nelson,

1991 Winter

W.D.

and
Shell

H. Fujimoto.
for Plants”, In

Simulation

Kelton

(CdS.). 289-293.
Politis,
D. N. and J. P. Romano.

and

Conference.

G.M.

Clark

1992. The Stationmy

the Limits
of ,Bootstrap.
Bootstrap. In Exploring
R. LcPagc and L. Billard (eds.). Wiley, New

Monte

York.

that the TB can produce

unbiased estimates of the series mean, accuralc and
precise estimates of the standard deviation of lhc
mean, and valid contidcncc intervals for the mean.
The TB has some advantages over altcmativc
single series method.s of infcrcncc.
First, it is
conceptually simple. Second, it dots not require the

Confidence Interval Estimator”, Opn$. Res. 40,
898-913.
Thorns, L. A, and W. R. Schucany. 1990. “Bootstrap
Prediction
Intervals
for Autoregression”,
J.

calculation of an appropriate batch size. Third, the
TB algorithm is inherently suited to implementation

Whitt,

on parallel

computers,

in which

each processor can

produce and analyze a pseudo-series.
Further research on the TB should

focus on,

first, development of a sound theoretical explanation
for our empirical
results; second, invcs~i@ion
of
which types of time series and statistics arc suitable
for bootstrapping;
third, implementation
on parallel
computers
to bring closer the day of real-time
simulation output analysis.

Sargent, R.G., K. Kang, and D. Goldsman. 1992. “An
Investigation
of Finite-Sample
Belhavior of

Amer.

W.

Statist.

Management

AUTIiOR

Assoc.,

1989. “Planning
Science,

85,486-492.

Queueing
35,

Simulation”,

1341-1366.

BIOGRAPHIES

YUN BAE KIM
is an Assistant Professor in the
Dcpamncnt of Mathematics at New Mexico Institute
of Mining and Technology.
Hc received the B.S.
dcgrcc in Industrial Engineering at Sung Kyun Kwan
University,
Seoul,
Korea,
the M.S.
clegrcc in
Induswial
and Systcms
Engineering
from
the

REFERENCES

University

Efron,

Scicncc from Rcnssclacr Polytechnic Institute in 1992.
His interests focus on simulation modeling and output
analysis.

B.

Boots(rap

Kim,

1982.
and

The
O[her

Jackknife,

the

Resampling

Plans,

CBMS:NSF,
Philadelphia.
Y. B. 1992. Output
Analysis
of Single
Replication Methods in Simulation Expcrirncnts,
Ph.D. Dissertation,
Department
of Decision

of Florida,

and the Ph.D. in Engineering

l’HOMAS
R. WILLEMAIN
is an Associate
Professor in lhc Department of Decision Sciences and
Engineering
Systems
at Renssehtcr
Polytechnic

Kim

502

Institute.
He received the B.S.E degree in Electrical
Engineering
from Princeton University
of 1969 and
the S.M. and Ph.D. in Electrical Engineering
from
Massachusetts Institute of Technology
in 1970 and
1972, respectivc1y. Hisresearch interests includc time
series analysis, forecasting,
formulation.
JORGE
HADDOCK
Industrial Engineering
Department

and the processor

model

is an Associate Professor of
and Operations Research in the

of Decision

Sciences

Systems at Rensselaer Polytechnic

and Engineering
Institute.

He holds

a BSCE from the University
of Puerto Rico, a
MSMgtE
from Rensselaer, and a PhD in Industrial
Professor
Engineering
from Purdue University.
Haddock’s
primary
teaching
interests
include
Operations Research and Production
Planning and
Inventory Control courses at the undergraduate and
His primary
research interests
graduate levels.
involve modeling
of manufacturingjproduction
and
inventory control systems, as WC1l as the design and
implementation
tools.

of simulation

modeling

and analysis

GEORGE
C. RUNGER
is an Assistant Professor in
the Department of Decision Scicnccs and Engineering
Systems at Rensselaer
Polytechnic
Insthute.
Hc
received the B.S. in Industrial
Engineering
from
Cornell University in 1974 and the Ph.D. in Statistics
from the University
of Minnesota
in 1981. His
research interests
include
the mcasurcmcnt
and
analysis of on-line
for
high-volume,

manufacturing
data (particularly
sensor-based
data acquisition
systems), statistical quality control for autocorrclatcd
real-time
control,
novel
data,
adaptive
and
experimental
designs for small sample sizes using
partial or incomplete prior knowledge.

et al.

Computational Statistics & Data Analysis 43 (2003) 341 – 355
www.elsevier.com/locate/csda

Resampling methods for variable selection in
robust regression
James W. Wisnowskia;∗ , James R. Simpsonb ,
Douglas C. Montgomeryc , George C. Rungerc
a Department

of Mathematical Sciences, United States Air Force Academy, USAF/DFMS, 2354
Fairchild Dr., Suite 6D2A CO 80840-6252, USA
b Department of Industrial and Manufacturing Engineering, Florida State University, Florida A& M
University, Tallahassee, FL 32310-6046, USA
c Department of Industrial Engineering, Arizona State University, Tempe, AZ 85287-5906, USA
Received 1 September 2001; received in revised form 1 July 2002; accepted 1 July 2002

Abstract
With the inundation of large data sets requiring analysis and empirical model building, outliers have become commonplace. Fortunately, several standard statistical software packages have
allowed practitioners to use robust regression estimators to easily 1t data sets that are contaminated with outliers. However, little guidance is available for selecting the best subset of the
predictor variables when using these robust estimators. We initially consider cross-validation and
bootstrap resampling methods that have performed well for least-squares variable selection. It
turns out that these variable selection methods cannot be directly applied to contaminated data
sets using a robust estimation scheme. The prediction errors, in5ated by the outliers, are not
reliable measures of how well the robust model 1ts the data.
As a result, new resampling variable selection methods are proposed by introducing alternative
estimates of prediction error in the contaminated model. We demonstrate that, although robust
estimation and resampling variable selection are computationally complex procedures, we can
combine both techniques for superior results using modest computational resources. Monte Carlo
simulation is used to evaluate the proposed variable selection procedures against alternatives
through a designed experiment approach. The experiment factors include percentage of outliers,
outlier geometry, bootstrap sample size, number of bootstrap samples, and cross-validation assessment size. The results are summarized and recommendations for use are provided.
c 2002 Elsevier B.V. All rights reserved.

Keywords: Outliers; Robust regression; Variable selection; Bootstrap; Cross-validation

∗

Corresponding author.
E-mail addresses: jim.wisnowski@usafa.af.mil (J.W. Wisnowski), simpson@wombat.eng.fsu.edu (J.R.
Simpson), doug.montgomery@asu.edu (D.C. Montgomery), runger@asu.edu (G.C. Runger).
c 2002 Elsevier B.V. All rights reserved.
0167-9473/03/$ - see front matter 
PII: S 0 1 6 7 - 9 4 7 3 ( 0 2 ) 0 0 2 3 5 - 9

342

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

1. Introduction
A critical aspect of the regression model building strategy is identifying the subset
of signi1cant regressor variables from the candidate variable list. We consider the usual
multiple regression model, y=XR+U where X is the n×p matrix of regressor variables,
R the p vector of parameters and U the random error assumed to be independent and
identically distributed with mean 0 and variance 2 I. The R vector is partitioned into an
active variable set, R1 of p−q parameters and inactive set R2 of q parameters. The goal
of a variable selection procedure is to have the signi1cant regressor variables included
in set R1 with high probability, while simultaneously achieving a high probability that
the insigni1cant variables are contained in set R2 . This article addresses the variable
selection problem for robust regression estimators used to accommodate a potentially
large number of outliers in the data set.
The regression model building strategy for least-squares and robust estimators is
an iterative process that involves selection of an active subset of the p regressors
followed by model diagnostics to assess the 1t. The objective is to 1nd the best subset
of the p parameters to include in the model that leads to good prediction capability yet
minimizes the variance of prediction. The 1rst objective would suggest including all
p variables while the second suggests using as small of a subset as possible because
the variance of prediction always increases as regression variables are added. Models
with fewer variables are also preferred for simplicity in interpretation and for ease of
future data collection.
There are numerous variable selection methods for the least-squares model. The most
common are the class of computer-intensive variable selection methods (e.g. forward,
backward, stepwise, and best subsets regression). Selection criteria for these methods
are often based on R2 , adjusted R2 , F test statistics (F-to-enter and F-to-leave) or
Mallows’s (1973) Cp criterion. Unfortunately, Miller (1990) demonstrates that all of
these measures are biased and not recommended for variable selection. Breiman (1995)
states that the preferred measure of performance for variable selection in regression is
some measure of prediction error using the resampling methods of bootstrapping and
cross-validation.
The apparent 
prediction error or resubstitution error for a regression model is de1ned
n
by 	ˆ App = n−1 i=1 (yi − ŷ i )2 . Note that this quantity diJers from the usual MSE
estimate because n replaces n − p. Another metric, the 1nal prediction error (FPE)
criteria (Zhang, 1992), accounts for the number of regressors in the model and is
computed by n	ˆ App + p, where  is the penalty constant for including extra variables.
When  = 2, FPE can be shown to be equivalent to Mallows’s Cp . It has been well
documented that the FPE, Cp and 	ˆ App measures are highly biased (Miller, 1990;
Zhang, 1992; Shao, 1993; Breiman, 1995; Davison and Hinkley, 1997, Section 6.4.2)
and not recommended for variable selection. Direct minimization of these measures
leads to models that have too many signi1cant variables, indicating that the dimension
of R1 is too large.
Shao (1993, 1996) and Breiman (1995) propose computationally complex resampling
methods to address the shortcomings of the usual methods for least-squares variable
selection. These authors recommend modifying the common resampling methods of the

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

343

Aggregate Prediction Error (Impurity)

200

150

100

50

0
1

2

3

4

5

6

7

Number of Parameters

Fig. 1. Representative screeplot of aggregate prediction error.

bootstrap and cross-validation described in Section 2 to estimate prediction error. The
model with the minimum value for prediction error is then the correct model.
We propose relaxing the requirement for the absolute minimum prediction error
and selecting a model with the fewest number of variables and a low (not necessarily minimum) prediction error. A reasonable strategy involves observing a screeplot
(line-connected scatterplot of the number of parameters versus prediction error) of
candidate models of increasing dimension. The model with the fewest parameters,
where the screeplot levels oJ, is selected. For example, the screeplot in Fig. 1 suggests
that, although the 7-parameter model has the minimum value of prediction error, little
improvement is gained after 5 parameters are included in the model. This criterion
is eJective for both cross-validation and bootstrap estimates of prediction error. Our
results indicate that this criterion is universally better than direct minimization in the
least-squares model when there are no outliers present (Wisnowski et al., 2002). The
limitations of this approach are that it works best in relatively low dimension (p ¡ 10)
under the assumption that a subset of eJects is active (sparcity of eJects).
The complexity of variable selection signi1cantly increases for regression models
contaminated with outliers. Tests on least-squares regression parameters lose power
dramatically in the presence of outliers and leverage points. One approach to overcome the loss of power is to use a robust regression estimator. Least-squares estimation is not appropriate in contaminated samples but compound estimators demonstrate
solid overall performance. These multi-stage estimators typically consist of an initial
high breakdown estimate, followed by an evaluation of the regressor space to identify high-leverage points, and a 1nal (usually iterative) estimation to downweight the
outliers.

344

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

The objectives of this paper are to develop a variable selection method for robust regression and to evaluate alternative approaches across a variety of outlier scenarios. To introduce the proposed and alternative methods, we 1rst describe several
cross-validation and bootstrap resampling methods to calculate prediction error in Section 2. Section 3 provides some detail regarding the proposed criterion for variable
selection in regression models. Section 4 covers the testing and analysis for the same
selection methods against data sets with outliers. Several experiments are performed
to identify the better performing methods across a variety of outlier scenarios. Results
are summarized in Section 5.
2. Resampling measures of prediction error
Cross-validation and bootstrapping are the two classes of resampling methods currently recommended for prediction error measures for variable selection. Cross-validation
procedures partition the data into two disjoint sets. The model is 1t with one set (the
training set), which is subsequently used to predict the responses for the observations in
the second set (assessment set). Bootstrap procedures form many samples of the original data by sampling with replacement. Details of these methods and their application
to the variable selection problem in regression are outlined below.
2.1. Cross-validation techniques
An intuitively appealing method to calculate a predicted response value is to use
the parameter estimates from the 1t obtained with the entire data set with the
exception of the observation to be predicted. This predicted response value is denoted
by ŷ (i) . The leave-one-out cross-validation estimate of average prediction error is then
n
computed using this predicted response value as 	ˆ CV; 1 = n−1 i=1 (yi − ŷ (i) )2 . Shao
(1993) proves with asymptotic results and simulations that the model with the minimum value for the leave-one-out cross-validation estimate of prediction error is often
overspeci1ed. That is, too many insigni1cant variables are contained in set R1 . He recommends using a method that leaves out a subset of observations, called K-fold crossvalidation.
In K-fold cross-validation, the training set omits approximately n=K observations
from the training set. To predict the response values for the kth assessment set, Sk; a ,
all observations apart from those in Sk; a are in the training set, Sk; t . Sk; t is used to
estimate the 
model parameters. The K-fold cross-validation average prediction error is
n
	ˆ CV; K = n−1 i=1 (yi − ŷ (k; i) )2 , where ŷ (k; i) is the ith predicted response from Sk; a .
Davison and Hinkley (1997, Section 6.4.1) recommend K = min(n1=2 ; 10) in practice.
This procedure decreases the variance of prediction error over that of the leave-one-out
cross-validation estimate but at the expense of increased bias. Surprisingly, Shao (1993)
demonstrates that the smaller the training set, the better the K-fold model selection
estimate. He shows that both the leave-one-out and K-fold cross-validation procedures
have a negligible probability of selecting an underspeci1ed model for least-squares
estimators. The challenge is avoiding an over1t model.

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

345

2.2. Bootstrap estimators
Bootstrap estimators in regression have received considerable attention in the literature since their introduction by Efron (1979). Wu (1986) provides the theoretical results
for bootstrap methods applied to regression. The fundamental element of a bootstrap
procedure is the bootstrap sample. For bootstrapping pairs in regression (Efron, 1982),
the sample is formed by randomly sampling with replacement n times both a response
value and its associated vector of regressor variable values from the original sample.
All future references in this article to the bootstrap will mean bootstrapping pairs. The
bootstrap sample may contain an observation from the original sample once, multiple
times or not at all. A regression model is then 1t to the bootstrap sample to obtain
the bootstrap parameter estimates R̂∗ . A large number of bootstrap samples (B ¿ 100,
Davison and Hinkley, 1997, Section 6.4.1) are constructed from the original sample
for model inference.
For the variable selection problem, 
the estimate of the average prediction error for
n
the bth bootstrap sample is 	ˆ b = n−1 i=1 (yi − xi R̂∗b )2 , where yi and xi are from the
original sample. Efron (1983) provides
the unbiased estimator
of prediction error
n
n
n for the
bth sample as 	ˆ b; unbiased = n−1 i=1 (yi − xi R̂)2 + n−1 i=1 (yi − xi R̂∗b )2 − n−1 i=1 (yi∗ −
xi∗ R̂∗b )2 , where xi∗ is the vector of regressor values for the ith observation in the bth
bootstrap sample. The 
overall unbiased bootstrap estimate of average prediction error
B
is simply 	ˆ BS = B−1 b=1 	ˆ b; unbiased where B is the number of bootstrap samples.
Shao (1996) shows that selecting the model with the minimum 	ˆ BS is inconsistent.
Inconsistency implies that the probability that the true model has the minimum bootstrap
average prediction error does not equal 1.0 as n approaches in1nity. Shao corrects this
inconsistency for bootstrapping pairs by using substantially fewer than n observations
to construct each of the bootstrap samples.
3. A proposed criterion for variable selection
Recent results indicate that most of the traditional measures used in variable selection
such as R2 , adjusted R2 , Cp , and PRESS, are heavily biased and not suitable for
variable selection. In addition, the forward, backwards, and stepwise computer selection
methods are essentially based on these traditional measures and also provide biased
results. We believe suitable results, and in many cases superior results, are obtainable
by relaxing the requirement that the optimum value criterion be used for selection.
Rather, one could select the model that has the near optimum prediction error with the
fewest number of variables. A practitioner would more likely adopt this strategy given
the prediction errors from the candidate models. Obviously, there is some subjectivity
involved with this criterion; so a few guidelines are in order.
To illustrate this methodology, suppose the average prediction errors (	ˆ BS ) from 100
bootstrap samples each are generated for models with an increasing number of active
variables (Table 1). Using the well-traveled Gunst and Mason (1980) data, the model
is y = XR + U, where X is the design matrix formed by augmenting the four regressor
variables with a column of ones, R is the known vector of parameters and U is the vector

346

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

Table 1
Average prediction error from 100 bootstrap samples as a function of the number of variables in the model.
The column headings are the true model

of NID (0; 2 I) error terms. This data set is used extensively in the Shao (1993, 1996)
studies and later in this paper. The column headings of Table 1 display the known
generating vector R used to calculate the response values. Our procedure looks at the
change in prediction error between a model of dimension j and one with dimension
j + 1. Final model dimension is determined to be the number of variables associated
with a signi1cantly diminished change in prediction error. For example, in column 2
of Table 1, our strategy would correctly choose the 2-parameter model (intercept and
3 ) rather than the model with the minimum prediction error; the 5-parameter model.
Similarly, the proposed method would select the correct model (the shaded cells) for
the succeeding three columns.
In practice, the proposed criterion requires a tolerance or guideline for the allowable
change in prediction error. For the simulation studies, we specify the minimum change
in prediction error required to select the next higher dimension model. A reasonable approach is to mirror the impurity logic used to split and terminate nodes in classi1cation
and regression trees (Breiman et al., 1984). If the change in prediction error does not
exceed a certain percentage of the prediction error for the null model (intercept only),
then the lower dimension model is selected. For example, if our minimum change in
prediction error criterion were 1% of the null model prediction error of 18.51, then the
reduction in prediction error must be at least 0.1851 between models of size j and j +1
for column two in Table 1. We are not advocating using a speci1c percentage for the
parameter as much as we suggest carefully inspecting the change in prediction errors
between candidate models. The guidelines are useful for comparative studies in simulations. Note that the simulation results are robust to the selection of this parameter.

4. Variable selection in the presence of outliers
In this section, we 1rst review the related work in robust regression variable selection. We then conduct an empirical evaluation of resampling methods using a compound
robust regression estimator. Because the combined approach involving compound estimation and resampling methods is computationally complex, very few studies have
been performed. The results of this study should enable researchers to pursue promising
avenues of further study.

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

347

4.1. Variable selection with robust regression estimators
Although numerous robust estimators have been proposed in the last 25 years, relatively few investigations have explored variable selection procedures for the robust
regression model. Most robust regression variable selection methods are based on robust
versions of the general linear test that use the asymptotic covariance matrix (Hampel
et al., 1986). Markatou and He (1994) and Hertier and Ronchetti (1994) extend the
Wald (similar to t-tests) and drop-in-dispersion tests (similar to F-tests) to Generalized M and compound estimators. Field (1997) and Field and Welsh (1998) propose
saddlepoint approximations of tail area probabilities for robust regression hypothesis
testing as improvements to the asymptotic approach. The results are mixed and they
recommend further testing in 1nite samples. Ronchetti and Staudte (1994) propose
a robust version of Mallows’s Cp . The method multiplies the squared residuals by
the 1nal weights from a robust 1t to compute the residual sum of squares. Two
additional quantities are also added to the residual sum of squares that are a function of the number of parameters and the selected robust estimator. The robust Cp
appears to work satisfactorily for their three examples, but no simulation results are
reported.
Wilcox (1998) presents an interesting approach to the variable selection problem in
robust regression by using a bootstrap resampling scheme. He uses a percentile bootstrap approach to 1nd critical values for the joint con1dence region on the Mahalanobis
distance for the model parameters. Wilcox (1998) states that there is room for improvement with this method because the probability of a Type I error can be substantially
less than nominal levels in many circumstances. He cautions that this approach does
not work well with least-squares and correction factors through simulation are required
to achieve the right coverage probabilities. Our experiments with compound estimators
indicate that the Wilcox algorithm is a dependable diagnostic to test if at least one
of the variables is active; however, the test statistics do not help diJerentiate between
competing models.
The Wald test is currently preferred because of its asymptotic chi-square distribution
and the relative ease to calculate the asymptotic covariance matrix. Wilcox (1997,
Section 9.1) experiments (results not reported) with the Wald test using the M -estimator
and the Coakley and Hettmansperger (1993) compound estimator. For both estimators,
he 1nds poor control over the Type I error. All authors of the above methods conclude
that it is important to do further testing and evaluation to understand the strengths and
weaknesses of the methods in 1nite samples.
Davison and Hinkley (1997, Section 6.5) provide a brief discussion of resampling
methods in robust regression. Their guidance on resampling methods for variable selection in robust regression focuses on two main points: (1) remove gross outliers from
the analysis because too many outliers could appear in the resampled data leading
to ineOciency and breakdown and (2) most of the prediction error methods for least
squares should apply to robust regression.
Thus, little guidance exists for variable selection using cross-validation or bootstrap
estimates of prediction error in robust regression. The next section modi1es the Gunst
and Mason data to contain residual outliers.

348

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

Table 2
Results for 1000 simulations for model selection from Shao (1996) using least-squares parameter estimates.
Observations 8, 15, 28, and 39 are made residual outliers. The true model [2; 0; 0; 4; 8] is shaded. The top
number in each cell is the fraction of time the model was selected using the minimization of prediction error
metric and the bottom uses the change in purity metric with constant 0.025

4.2. Modi?ed Gunst and Mason data
To produce a data set for evaluating robust regression variable selection methods,
we modify the Gunst and Mason data set of 40 observations used extensively for the
Shao (1993, 1996) studies by planting residual outliers on four of the high-leverage
points identi1ed by the Rocke and WoodruJ (1996) robust distances (observations 8,
15, 28, and 39). A value of 10 was added to the response value for each of these
observations. The Gunst and Mason data set now contains 10% residual outliers at a
distance of 10. The simulations are conducted exactly as described in Section 3 with
the generating vector and R = [2; 0; 0; 4; 8].
The probabilities in Table 2 indicate that no resampling technique using any criterion
successfully selects the correct 3-parameter model under least-squares estimation. The
two inactive variables are now signi1cant in model selection because the least-squares
estimator has used them to 1t the outliers. The planted outliers are masked; they do not
have unusual residual values. The least-squares estimator has failed and we see the important linkage between outlier identi1cation and variable selection in model building.
As an alternative to least squares, a compound estimator is a logical choice for
this data set contaminated with high-leverage points and residual outliers. Resampling
estimates of prediction error for compound estimator variable selection could potentially
pose some problems. For example, the estimator could breakdown because, by chance,
a bootstrap sample may contain too many of the planted outliers. Breakdown means
that the parameter estimates are no longer valid for the bulk of the data. Also, if
the compound estimator successfully downweights the outliers, the resulting prediction
error may not necessarily be low relative to the other models. This is explained by
the existence of two sources of error contributing to overall prediction error: (1) the
poor 1t due to model misspeci1cation, and (2) the overall prediction error is in5ated

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

349

Table 3
Results for 100 simulations for model selection from Shao (1996) using the Simpson and Montgomery
compound estimator. Observations 8, 15, 28, and 39 are made residual outliers. The true model [2; 0; 0; 4; 8]
is shaded. The top number in each cell is the fraction of time the model was selected using the minimization
of prediction error metric and the bottom number uses the change in prediction error criterion with constant
0.025

because the estimator works and has assigned large residual values to the outliers. Due
to the latter reason, we weight the residuals by the 1nal weights from the compound
estimator for our experiments. This method provides superior results (especially for
the bootstrap) to not weighting the residuals and is more in tune with the compound
estimator properties. Additionally, Ronchetti and Staudte’s (1994) robust Cp criterion
uses weighted residual values.
The values in Table 3 are the proportion of 100 replicates that the model was
selected if the Simpson and Montgomery (1998) compound estimator replaces least
squares. The change in prediction error criterion (constant = 0:025) reliably identi1es
the correct model for all resampling methods with a slight edge given to the bootstrap
full sample (n = 40). The minimum prediction error criterion is not useful except for
the bootstrap half sample (n = 20).
4.3. A designed experiment for resampling methods with compound estimators
Based on the modi1ed Gunst and Mason data and our own pilot studies with larger
dimension models, it appears as if resampling methods are appropriate for variable
selection when outliers are present. To gain a better understanding of resampling methods’ performance for the variable selection problem with multiple outliers, we run a
designed experiment using Monte Carlo simulation. The experiment varies characteristics not only of the data set, but also of the resampling method and prediction error
criteria in order to quantify the expected performance of the various techniques. For the
proposed prediction error criterion, pilot studies show that 0.001 multiplied by the null
model prediction error is a reasonable value to determine if the next higher dimension
model is required. This constant is used for all scenarios. Also, we use the Simpson
and Montgomery (1998) compound estimator for all simulations.

350

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

4.3.1. Planning the simulation experiment
For this study, all data sets consist of n = 40 observations and p = 5 parameters
as in Shao (1993, 1996). The response vector is generated as y = XR + U where X
is the design matrix of i.i.d. random variates from the standard normal distribution
(not the Gunst and Mason data set), R is the vector of known parameters [2; 4; 8; 0; 0],
and U is the vector of random error variates from a N (0; e2 I) distribution. For the
last four observations, a value of  is added to each regressor variable value to create
high-leverage points. Residual outliers are created for the last four or eight observations
(depending on the factor setting) by adding  to the expected response value.
Factors for the experiment. From the previous results, pilot studies, and knowledge
of compound estimators, the following factors are included:
• Percentage of outliers contaminating the sample. This could be an important factor
because resampling methods could form samples with too many outliers that cause
the estimator to break down. The outlier density levels are 10% and 20%.
• Outlying distance, . This measures how many standard deviations from the mean to
make the outliers, both in leverage and residual. Larger values could lead to greater
prediction error. The levels are 5 standard deviations and 10 standard deviations.
• Signal-to-noise ratio, measured through . Previous analysis of noisy data demonstrates that this is a critical factor in determining the success of a procedure. The
probability of correctly selecting the model is directly proportional to the model
signal-to-noise ratio. The levels for  are 1 and 5, which corresponds to approximate R2 values of 0.98 and 0.80 respectively for an OLS 1t on the uncontaminated
portion of the data.
• Bootstrap sample size. Shao (1996) demonstrates that this is the single most important factor to correctly identify the active model parameters. Previous studies indicate
the half sample size is preferred. However, there may not be an appreciable diJerence for contaminated data sets. The levels are the full sample (n = 40) and the half
sample (n = 20).
• Number of bootstraps per replication. Up to this point, we have followed Davison
and Hinkley’s (1997, Section 6.4.1) recommendation to use 100 bootstrap samples
as an absolute minimum. Breiman and Spector (1992) and Breiman (1996) do not
exceed 50 bootstrap samples and conclude for some applications that as few as 5 may
suOce. Clearly, fewer bootstraps than 100 would be preferred when resampling with
a compound estimator. The levels in the experiment are B = 25 and 100 bootstrap
samples.
• Size of assessment set in cross-validation. This factor replaces the previous two
bootstrap-speci1c factors in the cross-validation runs. The purpose of this factor is
to determine if there is a signi1cant diJerence between K-Fold and leave-one-out
cross-validation procedures. The levels are 6 (K-Fold) and 1 (leave-one-out).
Experimental designs and response. All 1ve factors for the bootstrap method design
have only two levels; therefore, an attractive screening design for this experiment is a
5−1
2V
design. This design can estimate the main eJects and the two-factor interactions
free from aliasing. The cross-validation design is a full factorial 24 . The response

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

351

Table 4
design and results for bootstrap methods using compound estimators. The columns represent models
25−1
V
with p = 2; 3; 4, and 5 parameters. The last four columns are results using weighted residuals to compute
prediction error. The shaded columns represent the correct model. The top values in each cell are the
proportion of times that the model is selected out of 250 replications using the minimum prediction error
criterion. The bottom values are the proportion selected using the change in prediction error criterion

value is the proportion of the 250 replicates in which the various parameter models
are selected. Note that very little additional information is gained if the number of
replicates is increased.
4.3.2. Simulation results
Bootstrap methods. The most striking result for bootstrap methods (Table 4) is
the general failure of the minimum prediction error criterion in all cases. It incorrectly

352

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

selects the largest model (p=5) in most cases. This 1nding is counter to the outlier-free
results where at least the method worked with bootstraps using half the original sample
size. The proposed criterion (bottom value in each cell) provides far superior results in
all scenarios. The ANOVA (Adjusted R2 = 0:85) signi1cant eJects are the outlier percentage, the outlying distance, the signal-to-noise ratio and their associated interactions.
This suggests that neither the bootstrap sample size nor the total number of bootstrap
samples has much eJect. The most challenging con1guration for this approach is the
10% outlier,  = 10, e = 5 con1guration. It is believed that, because this con1guration
results in large prediction error values for the null (intercept only) model, slight modi1cations to the change in prediction error constant would signi1cantly improve performance
for this extreme outlier condition. Our experiments with 50 replicates show signi1cant
improvement if one-half the value for the constant (0.0005) is used. Note that the
selection of the constant is relatively robust except in this extreme (50) outlier case.
Cross-validation methods. From Table 5, it is clear that the change in prediction error
criterion once again signi1cantly outperforms the minimum prediction error criterion.
The runs with =10 and =5 are exceptions. The poor performance here by the change
in prediction error is easily explained by these scenarios having very large outliers that
make the initial estimate of prediction error from the null model extremely high. Using
0.001 of null model prediction error as a criterion to go to the next higher dimension
model is too high. If the parameter is changed to 0.0001, the correct model is selected
approximately 75% of the time in our 50 replicates for these scenarios (runs 7, 8, 15
and 16).
The minimum prediction error criterion no longer selects exclusively the largest
parameter model. This criterion appears to select the correct model, independent of
factor settings or weighting residuals, between 40% and 50% of the time. There are
no signi1cant factors in ANOVA for the minimum prediction error criterion.
For the change in prediction error criterion, all factors are signi1cant from ANOVA
(Adjusted R2 = 0:87) except for the percentage of outliers. Additionally, the two-factor
interaction between the number of observations left out of the training set (1 or 6) and
the signal-to-noise ratio is also signi1cant.
Clearly, the best method across all scenarios is the change in prediction error criterion
applied to prediction error from the bootstrap procedure. Decent results are also possible
with cross-validation methods using the change in prediction error criterion. There does
not seem to be much diJerence between K-Fold and leave-one-out cross-validation
procedures for either criterion.
5. Summary
This paper proposes and tests several methodologies for model selection with contaminated data sets. We extend the best performing selection methods from the literature
for least-squares to a robust regression compound estimator. These methods select the
model with the minimum prediction error from a resampling procedure (leave-one-out
cross-validation, K-fold cross-validation, adjusted K-fold cross-validation, the bootstrap,
and the bootstrap half sample).

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

353

Table 5
24 design and results for cross-validation methods using compound estimators. The columns represent models
with p = 2; 3; 4, and 5 parameters. The last four columns are results using weighted residuals to compute
prediction error. The shaded columns represent the correct model. The top values in each cell are the
proportion of times that the model is selected out of 250 replications using the minimum prediction error
criterion. The bottom values are the proportion selected using the change in prediction error criterion

Rather than choosing a model with a strict minimum prediction error from these
methods, better results can be achieved with our proposed criterion that chooses models
with fewer variables and a near-minimum prediction error. To implement and test
this procedure, an operational version is introduced that increments the dimension of
the model until the change in prediction error is less than a speci1ed percentage of
total prediction error in the intercept-only model. Extensive Monte Carlo simulation
suggests that this criterion often outperforms the minimum prediction error criterion in
contaminated (and uncontaminated) samples.

354

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

The contaminated data experiments performed clearly reveal insights fruitful for
continued research. For instance, variable selection methods and criteria applied to
least-squares estimates of the Gunst and Mason contaminated data set consistently fail
to identify the true model. If a robust regression compound estimator replaces least
squares in these situations, the proposed criteria selects the correct model 93% of the
time, regardless of the variable selection method employed.
A comprehensive designed experiment investigating the eJect of outlier density,
outlier magnitude, signal-to-noise ratio, and sample size for the resampling methods
demonstrates that the proposed change in prediction error criterion is preferable to the
minimum prediction error criterion. The correct model is almost always selected with
the proposed criterion for the bootstrap methods while the minimum prediction error
criterion always over 1ts.
Future work in this area could involve a sensitivity analysis of the constant value
selected for the proposed criterion parameter. This constant represents the necessary
change in the prediction error as the regressor dimension increases in order to accept the
higher dimension model. The constant is a percentage of the prediction error associated
with an intercept-only model. Alternative approaches would attempt to mathematically
identify the “knee” in the prediction error curve plotted against the number of variables
in the model.
References
Breiman, L., 1995. Better subset regression using the nonnegative garrote. Technometrics 37, 373–384.
Breiman, L., 1996. Bagging predictors. Mach. Learning 24, 123–140.
Breiman, L., Spector, P., 1992. Submodel selection and evaluation in regression. The x-random case. Internat.
Statist. Rev. 60, 291–319.
Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J., 1984. Classi1cation and Regression Trees. Wadsworth
& Brooks/Cole, Paci1c Grove, CA.
Coakley, C.W., Hettmansperger, T.P., 1993. A bounded-in5uence, high breakdown, eOcient regression
estimator. J. Amer. Statist. Assoc. 88, 872–880.
Davison, A.C., Hinkley, D.V., 1997. Bootstrap Methods and their Application. Cambridge University Press,
Cambridge, UK.
Efron, B., 1979. Bootstrap methods: another look at the jackknife. Ann. Statist. 7, 1–26.
Efron, B., 1982. The jackknife, the bootstrap and other resampling plans. In: CBMS-NSF Regional
Conference Series in Applied Mathematics, Vol. 38. SIAM, Philadelphia, PA.
Efron, B., 1983. Estimating the error rate of a prediction rule: improvement on cross-validation. J. Amer.
Statist. Assoc. 76, 312–319.
Field, C.A., 1997. Robust regression and small sample con1dence intervals. J. Statist. Plann. Inference 57,
39–48.
Field, C.A., Welsh, A.H., 1998. Robust regression con1dence intervals for regression parameters. Austral.
N. Z. J. Statist. 40, 55–64.
Gunst, G.F., Mason, R.L., 1980. Regression Analysis and its Applications. Marcel Dekker, New York.
Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J., Stahel, W.A., 1986. Robust Statistics: The Approach Based
on In5uence Functions. Wiley, New York.
Hertier, S., Ronchetti, E., 1994. Robust bounded-in5uence tests in general parametric models. J. Amer. Statist.
Assoc. 89, 897–904.
Mallows, C.L., 1973. Some comments on Cp . Technometrics 15, 661–675.
Markatou, M., He, X., 1994. Bounded in5uence and high breakdown point testing procedures in linear
models. J. Amer. Statist. Assoc. 89, 543–549.

J.W. Wisnowski et al. / Computational Statistics & Data Analysis 43 (2003) 341 – 355

355

Miller, A.J., 1990. Subset Selection in Regression. Chapman & Hall, London.
Rocke, D.M., WoodruJ, D.L., 1996. Indenti1cation of outliers in multivariate data. J. Amer. Statist. Assoc.
91, 1047–1061.
Ronchetti, E., Staudte, R.G., 1994. A robust version of Mallows’s Cp . J. Amer. Statist. Assoc. 89, 550–559.
Shao, J., 1993. Linear model selection by cross-validation. J. Amer. Statist. Assoc. 88, 486–494.
Shao, J., 1996. Bootstrap model selection. J. Amer. Statist. Assoc. 91, 655–665.
Simpson, J.R., Montgomery, D.C., 1998. The development and evaluation of alternative generalized-M
estimation techniques. Comm. Statist. Simulation Comput. 27 1031–1049.
Wilcox, R.R., 1997. Introduction to Robust Estimation and Hypothesis Testing. Academic Press, San Diego,
CA.
Wilcox, R.R., 1998. The goals and strategies of robust methods. British J. Math. Statist. Psychol. 51, 1–39.
Wisnowski, J.W., Simpson, J.R., Montgomery, D.C., Runger, G.C., 2002. An alternative prediction error
criterion for regression model selection. Comm. Statist. Simulation Comput., to appear.
Wu, C.F.J., 1986. Jackknife, bootstrap, and other resampling methods in regression analysis. Ann. Statist.
14, 1261–1295.
Zhang, P., 1992. On the distributional properties of model selection criteria. J. Amer. Statist. Assoc. 87,
732–737.

Data Min Knowl Disc (2016) 30:476–509
DOI 10.1007/s10618-015-0425-y

Time series representation and similarity based on local
autopatterns
Mustafa Gokce Baydogan1 · George Runger2

Received: 17 November 2014 / Accepted: 19 June 2015 / Published online: 7 July 2015
© The Author(s) 2015

Abstract Time series data mining has received much greater interest along with the
increase in temporal data sets from different domains such as medicine, finance, multimedia, etc. Representations are important to reduce dimensionality and generate useful
similarity measures. High-level representations such as Fourier transforms, wavelets,
piecewise polynomial models, etc., were considered previously. Recently, autoregressive kernels were introduced to reflect the similarity of the time series. We introduce
a novel approach to model the dependency structure in time series that generalizes
the concept of autoregression to local autopatterns. Our approach generates a patternbased representation along with a similarity measure called learned pattern similarity
(LPS). A tree-based ensemble-learning strategy that is fast and insensitive to parameter settings is the basis for the approach. Then, a robust similarity measure based on
the learned patterns is presented. This unsupervised approach to represent and measure the similarity between time series generally applies to a number of data mining
tasks (e.g., clustering, anomaly detection, classification). Furthermore, an embedded
learning of the representation avoids pre-defined features and an extraction step which
is common in some feature-based approaches. The method generalizes in a straightforward manner to multivariate time series. The effectiveness of LPS is evaluated on
time series classification problems from various domains. We compare LPS to eleven
well-known similarity measures. Our experimental results show that LPS provides fast
and competitive results on benchmark datasets from several domains. Furthermore,

Responsible editor: Eamonn Keogh.

B

Mustafa Gokce Baydogan
mustafa.baydogan@boun.edu.tr

1

Department of Industrial Engineering, Boğaziçi University, Istanbul, Turkey

2

School of Computing, Informatics and Decision Systems Engineering, Arizona State University,
Tempe, AZ, USA

123

Time series representation and similarity based on local autopatterns

477

LPS provides a research direction and template approach that breaks from the linear
dependency models to potentially foster other promising nonlinear approaches.
Keywords Time series · Similarity · Pattern discovery · Autoregression ·
Regression tree

1 Introduction
Machine learning on large time series databases has received considerable interest
over the past decade as time series data has increased in applications. An important
challenge for the analysis is the high dimensionality of time series data. Much of
the research has focused on a high-level representation by transforming the original
data to another domain to reduce the dimension (Ratanamahatana et al. 2010). Moreover, the trends, shapes and patterns within the data often provide more information
than the individual values (Ratanamahatana et al. 2010). Consequently, higher-level
representations are also preferred to capture such properties (Lin et al. 2007). These
representations include Fourier transforms, wavelets, piecewise polynomial models,
etc. (Lin et al. 2003). Also, discretization approaches to represent the time series have
become popular in the last decade. For example, symbolic aggregate approximation
(SAX) (Lin et al. 2007, 2012; Shieh and Keogh 2008) is a simple symbolic representation for univariate series that segments the series into fixed-length intervals (and
uses a symbol to represent the mean of the values). This representation is similar to
piecewise aggregate approximation (PAA) (Chakrabarti et al. 2002). Overviews of the
time series representation approaches were provided by Fu (2011), Lin et al. (2007),
Ratanamahatana et al. (2010), Wang et al. (2013).
A generative model is another model where the series is represented through the
learned model parameters (Chen et al. 2013; Warren Liao 2005). These approaches are
referred to as “model-based kernels” (Chen et al. 2013). Approaches in this category
assume a parametric model of a certain form. Kernels such as probability product
kernels (Jebara et al. 2004), subsequence kernels (Kuksa and Pavlovic 2010), Fisher
kernels (Jaakkola et al. 1999), etc., implicitly generate a similarity measure after
transforming the series based on a model. Autoregressive (AR) kernels (Cuturi 2011)
in this category assume that there is a linear recurrence relation between the time series
values. AR models focus on the dynamic aspects of time series by specifying that a
value at a specific time depends linearly on previous values.
An efficient and effective similarity search over time series databases is another
important topic for time series learning as such data become ubiquitous. A distance
measure that can properly capture the underlying information and reflect the similarity
of the data is of fundamental importance for a variety of data mining tasks such as
clustering, anomaly detection, classification, etc. (Han and Kamber 2001). See Wang
et al. (2013) for a comprehensive evaluation and comparison of the most popular time
series similarity approaches.
As a parameter-free approach, similarity based on Euclidean distance is very popular and it was shown to work well for many applications (Wang et al. 2010). Euclidean
distance falls in the category of lock-step measures because it compares the ith value

123

478

M. G. Baydogan, G. Runger

of one time series to the ith value of another (Wang et al. 2013). This makes Euclidean
distance sensitive to the noise, scaling, translation and dilation of the patterns within
the time series. On the other hand, it can perform well for certain applications as the
training data size increases (Wang et al. 2013).
Alternatively, elastic measures compute the similarity invariant to certain nonlinear
variations in the time dimension. This is achieved through the comparison of one-tomany points as in dynamic time warping (DTW) (Ratanamahatana and Keogh 2005) or
one-to-many/one-to-none points as in longest common subsequence (LCSS) (Latecki
et al. 2005). DTW distance is considered to be strong for many time series data mining
problems (Ratanamahatana and Keogh 2005). Alternative approaches based on the idea
of DTW are also proposed in the literature. A weighting scheme is used by WDTW
Jeong et al. (2011) to weight against large warpings. Derivative DTW (DDTW) (Keogh
and Pazzani 2001) uses of the differences between consecutive time values. Also, Edit
distance based approaches are shown to be competitive in this domain. Edit distance
with a real penalty (ERP) (Chen et al. 2005), time warp edit (TWE) distance (Marteau
2009) and the move-split-merge (MSM) (Stefan et al. 2013) are some of the successful
approaches in this category.
Definition of the similarity is also critical for “similarity-based kernels” (Lowe
1995). These kernels make use of the similarity information for time series data mining.
For example, a kernel based on DTW is proposed for applications to speech recognition tasks by Cuturi (2011). Similarity-based kernels do not compare the dynamics
directly, but measure alignments between time series (Gaidon et al. 2011). Most of the
time series kernels (both similarity and model-based kernels) attempt to solve certain
invariance problems in the time dimension. Consequently, there are relationships with
the computer vision literature where patches are extracted from the images to account
for certain invariances such as location, scale, etc. Motivated by similar ideas, studies
based on time series segments (Baydogan et al. 2013; Grabocka and Schmidt-Thieme
2014; Lin et al. 2012) were recently proposed in the time series mining literature to
handle the invariances. Time series are characterized by feature vectors derived from
their segments using a bag-of-words (BoW) type of representation (Baydogan et al.
2013).
The Learned Pattern Similarity (LPS) method described here is also motivated by
BoW approaches. LPS first learns a representation from the segments of the time series
in a manner similar to autoregression then introduces a similarity measure based on
this representation. In order to illustrate the basics of LPS, we use a synthetic time
series classification problem. Consider a two-class problem in which series from class
one are defined by three peaks, whereas two peaks define class two, regardless of
locations. Figure 1a illustrates 10 time series from each class with a heatmap with
time on the x-axis and the time series on the y-axis. Figure 1b plots the values at time t
versus t + 1 to provide intuition regarding the AR models. A model trained on values
at time t to predict the value at t + 1 is called an AR model of lag 1, AR(1). As can be
seen from the scatter plot, the linearity assumption in the AR models is restrictive. The
peaks in this example are the main reason for the non-linear autoregressive behavior
(i.e., points around x = 2). Hence, we use a nonlinear approach to model the dependencies based on a tree-based learning strategy. This allows our model to capture more
complex dependencies with a robust model (few and insensitive to parameters). Anal-

123

Time series representation and similarity based on local autopatterns

(a)
2
2

479

(b)
3

Class 1

Class 2

4

1

6

0.5

8

0

10

−0.5

12

−1

14

−1.5

16

−2

18

−2.5

20
100

200

300

400

Observation at time t + 1

1.5
2
1
0
−1
−2
−3
−3

−2

−1

0

1

2

3

Observation at time t
Fig. 1 Time series database with 20 instances. The time series are 400 time units long. Values are presented
as a heatmap with time shown on the x-axis and time series values on the y-axis (a). Scatter plot of values
at time t and t + 1 over all the time series (b). Note that there are several overlapping observations at point
(2,2)

ogous to autoregression, we view these dependencies as autopatterns. A regression
tree is trained on the data visualized in Fig. 1. The structure of the tree is provided in
Fig. 2a.
The simple example in Fig. 1 (2 vs. 3 peaks) shows how the trees can encode the
dependency structure in the time series. However, AR modeling has the potential to
miss location information which can be important for some time series analysis problems. Consider a case where single predictor segment cannot adequately separate the
classes (e.g., where the locations of the peaks define the classes). Suppose a time series
database has time series of length 100 from two classes. Class 1 is defined by a peak
between time points 1 and 50, whereas Class 2 has the same peak between time points
51 and 100. Obviously, an AR(1) model cannot capture the difference of these series
as it generates the same representation for both classes. In such cases, modeling the
change of the autocorrelation structure over time is important. Consequently, instead
of learning a single tree structure, LPS trains an ensemble of regression trees to account
for multiple predictor segments of multiple lengths. The concept of encoding the local
autopatterns present in the time series through generalized models of autocorrelation
remains the same as the simple example in Fig. 1. However, the segments are allowed
to change in location and length at every split node of the trees in the ensemble. This
is analogous to modeling autocorrelation at multiple lags and at multiple locations
as in autoregressive kernels (Cuturi 2011), but with more expressive models than the
linear autoregressive counterparts. Furthermore, model-based approaches generally
fit models to each individual time series and compare their parameters. Modeling
each series separately is an iterative and potentially time-consuming process. On the
other hand, our LPS approach fits a single auto-pattern model to all series simultaneously.
LPS enjoys the benefits of recursive partitioning of the feature space to capture
nonlinear relationships and ensembles to identify behaviors that differ in regions of

123

480

M. G. Baydogan, G. Runger
Terminal Node Distribution

[1]

no

[3]

1.986791
n=1000

1

[2]

−0.2830683
n=6960

3

11

2

0.002089858
n=7960

Time Series

yes

T1 < 1.74

0

100

200

300

400

Frequency

(a)

(b)

Fig. 2 Regression tree trained on observations at time t to predict observations at time point t + 1 (a).
Corresponding terminal node distribution for time series 1 and 11 from classes 1 and 2 respectively (b).
Level differences in the frequency of observations at each terminal node reveal the difference of the time
series

the feature space. One also needs to differentiate the models of individual time series.
In LPS, each series is represented by the distribution of the values assigned to regions
defined by the recursive partitioning (terminal nodes) learned by the trees. Implicitly,
trees learn regions where dependencies are similar. Then, for each time series, the
frequency of the values residing at each terminal node of the learned ensemble is used
in the representation. This is illustrated as a boxplot in Fig. 2b for two time series from
different classes and one tree.
LPS extends to multivariate time series (MTS) in a straightforward manner without
any additional computational cost. Most of the studies on MTS similarity make use
of univariate approaches and weight the distance between each attribute to generate
a final similarity measure. This is common in many gesture recognition (GR) tasks
(Liu et al. 2009). For example, Akl and Valaee (2010), Liu et al. (2009) focused
on GR based on DTW distance. With the high dimensionality introduced by many
attributes and longer series, it becomes difficult to compute the similarity between
multivariate series. Also, the relationship between the attributes is not considered
when the similarity is computed over individual series and this is problematic for
certain applications with interaction effects between the attributes (as discussed by
Baydogan and Runger 2014). Our LPS similarity measure considers the interactions
between the individual attributes of a MTS.
Our approach inherits the properties of the tree-based ensembles. That is, it can
handle numerical, categorical and ordinal data, nonlinear and interactive effects. It is
scale invariant and robust to missing values. Most of the existing time series representation approaches have problems handling missing values or the data types other than
numeric. LPS can handle dilations and translations of the patterns (i.e., scale and shift
invariance) with the representation learning. These comments apply to both univariate
and multivariate time series. Furthermore, LPS allows for a simple parallel implementation which makes it computationally efficient. Our approach provides fast and
competitive results on benchmark datasets from the UCR time series database (Keogh
et al. 2011) and other published work (Frank and Asuncion 2010; Hills et al. 2014;
Lines and Bagnall 2014; Olszewski 2012; Rakthanmanon and Keogh 2013; Sübakan
et al. 2014; CMU 2012).

123

Time series representation and similarity based on local autopatterns

481

LPS provides generalized approach to model dependencies that are nonlinear (along
with dilations and translations), that generalizes the concept of autoregression. We
think of these dependencies as local autopatterns in the time series. Thus, LPS provides
a research direction for time series modeling that breaks from the linear dependency
models to potentially foster other promising nonlinear approaches. LPS provides an
example template for the steps to generate nonlinear autopatterns, local in time, represent time series, and produce similarity measures that can be used in a number of
analysis tasks. This template can be a guide for alternatives that extend upon LPS.
The remainder of this paper is organized as follows. Section 2 provides background
and a summary of related work. Section 3 describes the framework for learning the
patterns and computing the similarity. Section 4 demonstrates the effectiveness and
efficiency of our proposed approach by testing on a full set of benchmark datasets.
Section 5 provides conclusions.

2 Background and related work
A univariate time series, x n = (x n (1), x n (2), . . . , x n (t), . . . , x n (T )) is an ordered set
of T values. We assume time series are measured at equally-spaced time points. A
time series database, X , stores N univariate time series.
2.1 Autoregressive model
The autoregressive model of lag p, AR( p), is a collection of linear models to predict a
value at time t, x n (t), based on the previous values x n (t −1), x n (t − 2), . . . , x n (t − p).
The form of AR( p) models is
x (t) =
n

p


φ j x n (t − j) + t

(1)

j=1

where the mean is assumed to be zero and the regression coefficients, φ j , are parameters to be estimated. Given the lag p, there are several approaches to estimate the
coefficients. Least-squares estimation is commonly employed to find the regression
coefficients. This approach assumes that the error terms, t , have independent Gaussian
(normal) distributions with zero-mean and constant variance.
AR( p) models the lagged dependence between the observations. However, AR
models of this type assumes linear relations which might be problematic for applications. Moreover, the optimum model lag is not known a priori and has to be determined
via lag selection criteria. Also, the coefficients may change over time, but Eq. 1 assumes
that relations are the same for the entire time period.
2.2 Regression trees
Our approach makes use of regression trees, but much differently than the traditional
approach. A regression tree partitions the feature space to decrease the impurity of a

123

482

M. G. Baydogan, G. Runger

target y at the terminal nodes (Breiman et al. 1984).The impurity at a node is usually
measured with the sum of squared error, SSE = i (yi − ȳ)2 , where the sum and
the mean ȳ is computed over the instances assigned to the node. A split is chosen
to minimize the weighted average of the SSE over the child nodes. Finding the best
partition is generally computationally infeasible (Hastie et al. 2009). Hence, regression
trees use a greedy strategy to partition the input space. The prediction for an instance
assigned to terminal node m is the mean of the target attribute ȳ over the instances
in the training set assigned to m. Models of this type are sometimes called piecewise
constant regression models as they partition the predictor space in a set of regions and
fit a constant value within each region.

2.3 Time series representation
Several representations have been proposed for efficient data mining in time series
databases. We refer to Ratanamahatana et al. (2010) for detailed categorization and
description of these approaches. Discrete representations are common in time series
research Ratanamahatana et al. (2010). For example, SAX (Lin et al. 2007) discretizes
the values based on the mean of values in fixed-length intervals. This representation
is similar to PAA mentioned previously (Chakrabarti et al. 2002).
A traditional role for tree-based learners with time series is to approximate with a
piecewise constant model in a recursive manner (Geurts 2001). A popular regression
tree-based representation uses (t, x n (t)) as the data where the time index t is the
only predictor and x n (t) is the target (Geurts 2001). See Fig. 3 for one of the time
series from CBF dataset (Keogh et al. 2011). Initially, the mean of all values are
zero. A split minimizing the weighted sum of squared error (SSE) for the parent node
partitions the values into two nodes for which the mean values are −0.83 and 0.42. The
tree recursively partitions the time series values to minimize overall SSE in a greedy
manner. Because time is used as the predictor, the values residing at each terminal
node are contiguous and define an interval as shown in Fig. 3a. The discretized vector
has 128 elements (length of the times series) in this example.
The number of values residing at each terminal node can be used to represent the
time series (Geurts 2001). There are six terminal nodes defining the discretization
illustrated in Fig. 3a. Simply a vector of length six, (Hastie et al. 2009; Geurts 2001;
Geurts et al. 2006; Breiman et al. 1984; Keogh et al. 2006; Jebara et al. 2004) can be
used to represent the whole time series.
Tree-based representations for time series were considered by Baydogan and
Runger (2014), Baydogan et al. (2013) specifically for classification tasks. The previous work differs from the methods here in a number of ways. First, the previous work
utilized the class attribute for the representation. Also, Baydogan et al. (2013) used a
quite different approach in which simple features (such as means and standard deviations) were extracted from segments before a codebook was generated. The work by
Baydogan and Runger (2014) considered node counts for a representation. However,
the procedure was again focused on the class attribute for splitting rules and used a
substantially different data structure (in addition, without overlapping segments). Here
the approach is entirely unsupervised, and we generate splits in a different manner.

123

Time series representation and similarity based on local autopatterns

483

Fig. 3 A representation of time series from CBF dataset (Keogh et al. 2011) (a) and the regression tree
trained to obtain the representation (b). a Feature space. b Regression tree

We provide a new representation and also develop a similarity measure that can be
used for data mining tasks other than classification.
2.4 Time series similarity
Popular time series similarity measures have been summarized and evaluated by Lines
and Bagnall (2014). Eleven measures are empirically compared on 75 time series
classification datasets from different domains. The conclusion is that there is no one
measure that can significantly outperform the others (Lines and Bagnall 2014). Also,
it is shown that there is no statistically significant difference in the performance of the
elastic measures. The top three ranked algorithms on these datasets are claimed to be
WDTW, MSM and DTW with the best warping window (referred to as DTWBest).
As these approaches perform approximately at the same level, DTWBest is used for
comparisons, which is a common practice in the literature (Batista et al. 2014). See
Lines and Bagnall (2014) and Wang et al. (2013) for further discussion of the time
series similarity measures.
Moreover, similarity computation for MTS is a challenging task as the problem of
finding similarity between multiple series is not well-defined. To solve this problem,
similarity-based approaches are commonly employed over individual attributes of
MTS and the similarity over individual series of MTS is weighted to obtain a final
similarity measure. However, MTS are not only characterized by individual attributes
but also by their relationships.

3 Time series representation based on local autopatterns
LPS learns dependency patterns (autopatterns) from the time series by modeling the
relationships between the time series segments. We introduce a segmentation that is
related conceptually to multiple lag values for autocorrelation. After representing each
time series as a matrix of segments, a tree-based learning strategy to discover the depen-

123

484

M. G. Baydogan, G. Runger

dency structure is discussed. A BoW-type representation that encodes the dependency
patterns is generated for each time series. Then, a novel similarity measure based on
the proposed representation named as “learned pattern similarity (LPS)” is introduced.
3.1 Recursive partitioning on time series segments to learn autopatterns
Our approach extracts all possible segments of length L(L < T ) starting from each
time index t = 1, 2, . . . , T − L + 1. Here, a segment refers to the values from
the series that are contiguous in time. A segment starting at time t is denoted as
stn = {x n (t), x n (t + 1), . . . , x n (t + L − 1)}. The segment matrix S n in Eq. 2 for each
time series x n is generated with columns equal to all possible segments (T − L + 1
segments of length L are possible) for each series
⎡

n
SL×(T
−L+1)

⎤
x n (1) x n (2) . . . x n (T − L + 1)
⎢ x n (2) x n (3) . . . x n (T − L + 2) ⎥
⎢
⎥
⎥
=⎢
⎢ ...
⎥
⎣ ...
⎦
x n (T )
x n (L) x n (L + 1) . . .

(2)

After generating the segment matrix for each time series in the database, we concatenate the matrices row wise to learn the dependence relations over all the time series.
We denote this segment matrix as S N L×(T −L+1) . Our approach uses regression trees
to identify the structural dependencies between the time series observations. Before
training the regression tree based on the segment matrix, a random column, r th column
of S, is selected as the target segment. Then, we train a regression tree which selects
a random pth column of the segment matrix as the predictor at each split. Note that
the index p used here is different than the lag parameter p used by AR( p) models.
Similar to the split selection criterion in regression trees, the value that minimizes SSE
is used as the split decision. This is illustrated in a simple example in Sect. 1 where the
split is determined to be T 1 < 1.740247 (Fig. 2a). The regression tree trained in this
manner learns a nonlinear autoregressive model. The index of the column determines
the starting time of a segment. Therefore, the lag level is determined by the selection
of p and r . In order to allow for the discovery of autopatterns based on the multiple
(potentially) different local relationships, p is selected randomly at each node. Random strategies related to this were also shown to perform well in another regression
context by Geurts et al. (2006). A random selection of p at each split also enables LPS
to model a dependency that changes over time.
The setting of L basically sets an upper bound on the lag level in the approach.
Obviously, the lag cannot be greater than T − L. To model all possible lag levels,
we introduce a new learning strategy that trains J trees, {g j , j = 1, 2, . . . , J }, in an
ensemble framework. In addition to selecting a random predictor segment at each node
to account for multiple lags, each tree uses a random segment length in the approach.
This allows for a large number of possible lag levels to be modeled. Also, the depth
of the trees is restricted to be D to control the complexity. Algorithm 1 shows the
steps to build a single tree. The method that generates the split value in Step 6 can be

123

Time series representation and similarity based on local autopatterns

485

modified for computational speed. We consider two splitting strategies: “regression”
and “random” splits. This is discussed further in Sect. 3.3.
Algorithm 1 Regression tree algorithm: tr ee(S, depth, r ), where S is a N L × (T −
L + 1) matrix with i jth entry si j and r is a random target column of S in the regression
split setting.
1: if cannot further split or depth = D then
2: Designate this node as a terminal node
3: Return
4: end if
5: Select a random column index p
6: Obtain p ∗ , the split point for column p
7: Sle f t ← instances (rows) i with si p ≤ p ∗
8: Sright ← instances (rows) i with si p > p ∗
9: depth = depth + 1
10: tr ee(Sle f t , depth, r )
11: tr ee(Sright , depth, r )

When all time series are used for training, the algorithm is analogous to searching
for common patterns over all time series. Each tree generates a representation and the
final time series representation is obtained via concatenation. For simplicity, assume
that all trees contain the same number of terminal nodes R. The general case is easily
handled. Let H j (x n ) denote the R-dimensional frequency vector of instances in the
terminal nodes from tree g j for time series x n . We concatenate the frequency vectors
over the trees to obtain the final representation of each time series, denoted as H (x n ),
of length R × J (and modified obviously for non-constant R). Our representation
summarizes the patterns in the time series based on the terminal node distribution of
the instances over the trees.
The descriptions and examples are provided for time series of the same length, but
lengths can differ. Our segment extraction scheme should be modified in such cases.
Keeping the same number of segments, longer segments should be extracted for longer
series. Then the representation obtained should be normalized for each series based
on the segment lengths.
Moreover, interpolation is commonly required to estimate the missing values for
time series with missing values. However, the estimation method itself adds an additional parameter to the time series problem. Our proposed approach naturally handles
the data with missing values, without any additional steps, because tree-based learning
implicitly handles the attributes with missing values (Breiman et al. 1984). Robustness of the proposed approach to missing values is empirically evaluated in Sect. 4.7.
Although the descriptions are provided for numerical time series, LPS can also be
applied to categorical time series such as DNA sequences.
3.2 Extension to multivariate time series
An MTS is an M-attribute time series. In the multivariate scenario, the segment matrix
S n should be generated for each attribute of the multivariate series and concatenated
column wise to obtain a segment matrix of size L × (M × (T − L + 1)) for each

123

486

M. G. Baydogan, G. Runger

multivariate series. A positive property of LPS is that the rest of the algorithm remains
the same. With the help of the random selection of the columns at each node of the
tree, interactions between multiple attributes are modeled. This enables our approach
to model generalized cross-correlation at different lag levels. Depending on the number
of attributes, the number of trees and the depth level might be set larger to capture the
relevant information. Also, the complexity of LPS is not affected due to the random
segment selection at each iteration.

3.3 Splitting strategies
The split decision is one step of LPS and our approach considers two splitting strategies.
In the first alternative, referred to as “random splits”, the split value is determined
randomly from a uniform distribution based on the minimum and maximum of the
values in Step 6 of Algorithm 1.
The second alternative introduces a split similar to the ones used in regression
trees. In this alternative, tree construction in Algorithm 1 is modified slightly to learn
a regression tree. A regression tree requires a target and a random column is chosen to
be the target for each tree. Then, Step 6 sets the split value to minimize the weighted
average of the SSE on the target column over the child nodes. This alternative, referred
to as “regression splits”, provides certain benefits. With the regression tree approach,
a search for autopatterns is done in a more intelligent way as opposed to “random
splits”.
With an explicit objective function (i.e., minimize the weighted average of the SSE),
the split value on a random predictor column is selected to partition the values of the
target around the child nodes mean levels. In a sense, two columns of the segment
matrix (predictor and target) are discretized simultaneously. If the dependencies of the
patterns within series are important for similarity, “regression splits” has the potential
to work better. In other words, “regression splits” model a dependency between time
periods and it has the potential to work well if this behavior is important. This is
especially important for MTS as the relationship between multiple attributes is likely
to provide information about the learning task. Rows (segments) from the segment
matrix S are assigned to tree nodes as in the “random splits” case.
Generating a split value with “random splits” is very fast computationally. On
the other hand, “regression splits” evaluates of all possible split values at each node.
Added complexity arises from sorting before the evaluation of possible split locations.
This requires more computation, but the tree-based methods are well known to be
computationally fast (Breiman et al. 1984). Both strategies are evaluated empirically
in Sect. 4 and further discussion on the split choice is provided in Sect. 4.5.

3.4 Difference series
Regression trees find dependencies between segments based on the mean levels of the
values. In order to introduce dependencies in terms of trends in the representation, we
also generate segments (both predictor and target in the case of “regression trees”)

123

Time series representation and similarity based on local autopatterns

487

from the differences of consecutive values. Here, T − L difference segments for each
time series are generated as
⎡
⎢
⎢
⎢
⎢
⎣

x n (2) − x n (1)
x n (3) − x n (2)
...
...
x n (L + 1) − x n (L)

x n (3) − x n (2)
x n (4) − x n (3)

...
...

x n (L + 2) − x n (L + 1)

...

⎤
x n (T − L + 1) − x n (T − L)
n
− L + 2) − x (T − L + 1)⎥
⎥
⎥
⎥
⎦
x n (T ) − x n (T − 1)

x n (T

(3)
In our modified approach with differences, the difference columns are concatenated
column wise with the original segment matrix S and segments are randomly selected
from this enlarged matrix. Hence, a segment matrix of size N L × (2T − 2L + 1)
is used for representation learning in Algorithm 1. As studied in the experiments,
potentially, a better representation can be learned with this strategy. The addition of
the difference segments does not effect the complexity because our approach selects
a random segment at each iteration. With the addition of the difference series, LPS
enjoys the similar advantages as derivative dynamic time warping (DDTW) (Keogh
and Pazzani 2001). DDTW measures the similarity based on the trends by estimating
the local derivatives of the data. Similar information is captured by LPS with the
introduction of the difference series.
3.5 Similarity measure
Given the representation described previously, a similarity measure is developed. Suppose h nk is the kth entry of H (x n ), then the similarity between the time series x n and

x n is set to


sim(x n , x n ) =

R×J
1 

min(h nk , h nk )
R×J

(4)

k=1

As the similarity measure counts the number of matched values in the representation,
LPS can be categorized as a pattern-based similarity measure. Because of the random
selection of segments, we aggregate the similarity over all the trees as given in Eq. 4.
This enables our approach to capture patterns from different lags and locations. By
matching based on the minimum number of values in the pattern, the measure has some
relationship to similarity approaches based on subsequences, such as longest common
subsequence (LCSS) (Latecki et al. 2005). Our matching strategy also allows us to
handle the problem of dilation with this matching strategy.
Instead of generating a measure of similarity using Eq. 4, we propose a dissimilarity
measure to benefit from bounding schemes such as early abandoning (Keogh et al.
2005) which can accelerate the similarity search over the time series. The dissimilarity

between the time series x n and x n is set to


dissim(x n , x n ) =

R×J


1  

 n



h k − h nk 

R×J

(5)

k=1

123

488

M. G. Baydogan, G. Runger

The dissimilarity measure in Eq. 5 penalizes the number of mismatched values between
the time series. Moreover, it provides the opposite information as the similarity measure in Eq. 4. This can be seen as follows. Suppose that the absolute difference in the
sum in Eq. 5 is written as
R×J

1 


max(h nk , h nk ) − min(h nk , h nk )
R×J



dissim(x n , x n ) =

(6)

k=1

If the sum is distributed over the terms in Eq. 6, we obtain the sum of the maximums
minus the sum of the minimums. The sum of the entries in a representation is constant,
B, for each series where B is equal to sum of the segment lengths considered for each
tree. Hence,
R×J


R×J

 R×J



n n
n n
n n
max(h k , h k )+min(h k , h k ) =
max(h k , h k )+
min(h nk , h nk )

k=1

k=1

2B =

R×J



max(h nk , h nk ) +

k=1
R×J



max(h nk , h nk )

(7)

k=1
R×J




min(h nk , h nk )

(8)

k=1

= 2B −

k=1

R×J




min(h nk , h nk )

(9)

k=1

Plugging Eq. 9 back in Eq. 6, produces the dissimilarity in Eq. 10, which has the same
summation term as the similarity measure in Eq. 4, but with a negative sign. Because
the rest of the terms are constant in Eq. 10, the similarity in Eq. 4 is basically the
opposite of dissimilarity in Eq. 10.
2
dissim(x , x ) =
R×J
n

n


B−

R×J




min(h nk , h nk )

(10)

k=1

Although the length of the final representation can be larger than the time series length
depending on D and J , it is still computationally efficient as further illustrated in Sect.
4.4. Moreover, bounding strategies still work for MTS with the proposed representation
and similarity measure because LPS transforms an MTS into a univariate vector.
3.6 Parameters
There are four parameters in our approach: the splitting strategy, the number of trees
J , the depth D and the subsequence length L. However, LPS is robust to the settings of
these parameters if they are set in certain range. For example, L is selected randomly
for each tree. J and D can be set large if there is no concern regarding the computation
time. Similarly, “regression splits” is preferred if training time is not a problem. LPS is
quite insensitive to parameter settings and we illustrate its robustness on several data
sets to support this claim empirically.

123

Time series representation and similarity based on local autopatterns

489

If there is information regarding the application, one may want to set the parameters accordingly. The most important parameter of the approach is L. First, L sets
an upper bound as T − L on the lag as discussed earlier. Therefore, if only short
term dependencies are important in “regression trees”, L may be set large. This way,
dependencies are modeled over shorter time windows. However, interesting patterns
of the time series may be missed if long-term dependencies are important. To account
for long-term dependencies, a smaller L is preferred.
The setting of L is preferred to be handled with a simple approach that leverages
the large number of trees we typically use in LPS. Instead of setting L to a certain
level, L is set randomly for each tree. This provides robust performance, as shown in
our experiments, and removes the need to specify a value for L. Another option is to
set the parameters based on the cross-validation accuracy on training data. Section 4
further discusses how the parameters are handled in the experiments.

3.7 Algorithmic complexity
The time for learning the representation is mainly determined by the training of the
trees. The time complexity of building a single tree is O(νηβ), where ν = 1 is the
number of the features evaluated at each split, η = N × L is the number of instances
in the segment matrix and β = D is the depth of the tree. Because we set L as the
proportion of a full time series length, we define γ as L = γ T . As we build J trees
in a random fashion, the overall complexity of training is O(J N T D). Moreover, the
columns of S are generated at the splitting stage to avoid unnecessary storage of the
overlapping segments. Hence, our proposed approach is efficient in terms of memory
usage.
The testing complexity is determined by the complexity of the representation and
the classification. A time series representation requires the traversal of the trees which
is O(T J D). The time complexity of the classification is similar to the complexity of
NNEuclidean which is linear in the representation length. The time series are represented with a R × J length vector where R is the number of terminal nodes. Here,
R is determined by the depth parameter D and it is non-constant. Assuming that R is
constant and equal to the maximum possible value, R = 2 D , the worst case testing
complexity of LPS is O(N J 2 D ).
Theoretically, the worst-case complexity of LPS in testing is exponential to the
setting of D. However the proposed approach is very fast in practice which is further
discussed in Sect. 4.4. If a small decrease in the computation time is of practical
concern, bounding schemes can be used to accelerate this approach. The simplest
and well-known approach for NNEuclidean is early abandoning (Keogh et al. 2005),
as mentioned earlier. For example, during the computation of the LPS similarity for
nearest-neighbor classification, we can stop the calculation if the current sum of the
absolute differences between each pair of corresponding data points exceeds the best
similarity so far (Keogh et al. 2006). The computation time can be reduced significantly
with this bounding scheme (Rakthanmanon et al. 2012).
More importantly, almost all the steps of LPS are embarrassingly parallel. The trees
in the ensemble can be trained in parallel to learn the representation. Likewise, the

123

490

M. G. Baydogan, G. Runger

similarity computation can be done in parallel over multiple trees. This makes LPS
very suitable for large-scale similarity search in a parallel environment.

4 Experiments and results
LPS is implemented as an R R Core (2014) package named as LPStimeSeries which is
publicly available. We also provide a MATLAB implementation of LPS at (Baydogan
2013). All the scripts and information to run both implementations are provided at
(Baydogan 2013) to promote a full reproducibility of our results. Here we report the
results from the R implementation of LPS.
The effectiveness of the proposed representation and similarity measure is evaluated
based on the accuracy of a one nearest-neighbor (1NN) classifier. This scheme was
proposed as an objective evaluation method Keogh and Kasetty (2003). We refer the
reader to Wang et al. (2013) for further discussion of the evaluation methods for the
similarity measures. We emphasize that our similarity measure has greater applicability
to other tasks, but the 1NN analysis provides a convenient structure for comparisons.
Our approach is tested on 75 univariate time series classification datasets, where
46 are available in the UCR time series database (Keogh et al. 2011) and the rest
are available in other published work (Hills et al. 2014; Lines and Bagnall 2014;
Rakthanmanon and Keogh 2013). As discussed by Lines and Bagnall (2014), these
datasets have diverse characteristics such as the lengths of the series, the number of
classes, etc. We standardize each time series to zero mean and unit standard deviation.
This adjusts for potentially different baselines or scales that are not considered to
be relevant (or persistent) for a learner. We also performed experiments on several
multivariate time series classification problems. The details of the experimentation
and results are discussed in Sect. 4.2.
The datasets are grouped into four categories by Lines and Bagnall (2014) to facilitate better interpretation. The largest category with 29 data sets is the group of sensor
readings. The second largest category has 28 data sets from image outline classification. As many of the problems in this category are not rotationally aligned, classifiers
working in the time domain have the potential to fail for this category (Lines and
Bagnall 2014). The challenge with rotational invariance is common for many of the
data sets in this category. The third group has 12 data sets taken from motion capture
devices attached to human subjects. The simulated data sets form the last category.
See Lines and Bagnall (2014) for the details of the categorization.
Our approach is compared to nearest neighbors (NN) classifiers discussed by Lines
and Bagnall (2014). Eleven similarity measures are considered in our comprehensive
evaluation: DTW and DDTW Dynamic time warping and derivative dynamic time
warping that use the full warping window (Ratanamahatana and Keogh 2005; Keogh
and Pazzani 2001), DTWBest and DDTWBest DTW and DDTW with the window
size setting determined through cross-validation, DTWWeight and DDTWWeight
Weighted version of DTW and DDTW (Jeong et al. 2011), LCSS Longest common
subsequence (Latecki et al. 2005), MSM Move-Split-Merge (Stefan et al. 2013), TWE
Time warp edit distance (Marteau 2009), ERP85 Edit distance with real penalty (Chen
et al. 2005), and ED Euclidean distance.

123

Time series representation and similarity based on local autopatterns

491

The results for all NN classifiers were obtained from Lines and Bagnall (2014). As
mentioned by Lines and Bagnall (2014), some of these measures require certain hyperparameters to be set. The parameter optimization is done on the training set through
cross-validation by allowing at most 100 model evaluations for each approach.
The most important parameter in LPS is the segment length setting as discussed
in Sect. 3.6. We introduce two strategies for the segment length setting. The first
strategy sets the segment length L as the proportion of full time series length, γ ∈
{0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95}, and, furthermore, we consider the depth D ∈
{2, 4, 6} based on a leave-one-out cross-validation on the training data. For the crossvalidation, we train 25 trees (J = 25) for each fold. This version of LPS is named
as LPSBest as it is analogous to DTWBest. Hence, we allow for |γ | × |D| = 7 ×
3 = 21 model evaluations in our study. Although the depth can be fixed as LPS is
insensitive to depth setting if set large enough, three levels of depth are evaluated.
Because training the ensemble is performed once with the largest depth setting and
similarity is evaluated for each depth setting, evaluation of smaller depth levels does not
introduce additional computational costs for training. After the parameters providing
the best cross-validation accuracy are obtained, we train J = 200 trees in the ensemble
with the selected parameters to obtain the final representation.
In the second strategy, the segment length L is chosen randomly for each tree as
the proportion of full time series length between 0.05 × T and 0.95 × T . Also, the
number of trees J and the depth D are fixed to 200 and 6, respectively, for all datasets.
This version of LPS is referred to as LPS. The values of the parameters are set the
same for all datasets to illustrate the robustness of LPS. In other words, no parameter
tuning is conducted for this strategy. As we discussed in Sect. 3.6, J and D can be set
large if there is no concern regarding the computational time. Empirical evaluation of
this strategy is provided in Sect. 4.3.
The random selection of the segment lengths in LPS saves significant computational
time in training because we avoid the cross-validation step. Also, both splitting options
(regression and random) are evaluated for this strategy. We run 10 replications and
report the median performance because of the random nature of our proposed approach.

4.1 Classification accuracy
Tables 1 and 2 summarize the median error rates from 10 replications of our algorithm
on the test data. We also provide the classification of the data sets in terms of the
problem types in these tables. Comparison of LPS to multiple classifiers over all
datasets is done using a procedure suggested by Demšar (2006). The testing procedure
employs a Friedman test (Friedman 1940) followed by the Nemenyi test (1963) if a
significant difference is identified by Friedman’s test. It is basically a non-parametric
form of Analysis of Variance based on the ranks of the methods on each dataset (Lines
and Bagnall 2014).
Figure 4 shows the average ranks for all classifiers on 75 datasets. LPSBest has
best average rank and LPS is second best. Based on the Friedman test, we find that
there is a significant difference between the 13 classifiers at the 0.05 level. Proceeding
with the Nemenyi test, we compute the critical difference (CD). This test concludes

123

123

0.000

0.183

0.234

0.237

0.208

0.305

0.049

0.237

0.234

0.327

CricketY♣

CricketZ♣

DiatomSize•

DistPhalanxAge•

DistPhalanxOut•

DistPhalanxTW•

0.186

0.335

0.049

0.278

0.213

0.309

0.136

0.282

0.036

0.216

CricketX♣

Coffee

0.358

0.000

0.150

0.300

0.046

Computers

0.064

0.071

CinCECGtorso

0.002

0.050

BirdChicken•

Car

0.352

0.150

BeetleFly•

CBF

0.367

ChlorineConc

0.167

0.004

ARSim

Beef

0.200

0.200

ArrowHead•

0.235

0.211

Adiac•

0.324

0.239

0.245

0.036

0.215

0.218

0.236

0.124

0.000

0.378

0.378

0.003

0.267

0.350

0.350

0.367

0.404

0.429

0.396

DTW

Full

Best

Full

LPS

0.324

0.254

0.201

0.075

0.177

0.205

0.246

0.124

0.000

0.071

0.375

0.006

0.233

0.350

0.350

0.333

0.417

0.217

0.389

Best

0.324

0.246

0.223

0.036

0.187

0.187

0.236

0.124

0.000

0.068

0.373

0.003

0.217

0.400

0.450

0.300

0.407

0.211

0.394

Weight

0.345

0.246

0.216

0.291

0.459

0.546

0.462

0.212

0.071

0.375

0.353

0.408

0.267

0.350

0.150

0.333

0.101

0.320

0.414

Full

DDTW

0.317

0.228

0.237

0.092

0.454

0.482

0.438

0.200

0.036

0.071

0.351

0.428

0.217

0.300

0.100

0.300

0.103

0.211

0.330

Best

0.345

0.210

0.237

0.118

0.408

0.454

0.415

0.212

0.036

0.088

0.349

0.409

0.217

0.300

0.200

0.300

0.101

0.206

0.327

Weight

Table 1 The full results of LPS based measures and 11 similarity measures on 75 datasets (part one of two)

0.381

0.268

0.194

0.101

0.215

0.182

0.249

0.244

0.000

0.068

0.441

0.010

0.167

0.250

0.350

0.233

0.270

0.217

0.749

LCSS

0.338

0.246

0.209

0.046

0.249

0.177

0.241

0.220

0.107

0.081

0.382

0.030

0.100

0.350

0.600

0.533

0.311

0.257

0.373

MSM

0.309

0.279

0.209

0.049

0.221

0.238

0.241

0.160

0.000

0.237

0.379

0.009

0.083

0.550

0.550

0.400

0.491

0.229

0.366

TWE

0.331

0.236

0.252

0.075

0.179

0.162

0.244

0.116

0.000

0.118

0.362

0.002

0.233

0.400

0.300

0.333

0.433

0.183

0.391

ERP

0.317

0.239

0.252

0.075

0.387

0.346

0.421

0.272

0.000

0.102

0.369

0.148

0.267

0.400

0.350

0.333

0.489

0.183

0.389

ED

492
M. G. Baydogan, G. Runger

0.398

0.494

0.053

Herring•

InlineSkate♣

ItalyPower

0.315

0.213

0.347

0.073

0.514

0.430

0.575

0.003

0.252

0.098

0.054

0.190

0.069

0.271

0.274

0.131

0.264

0.060

0.629

0.406

0.601

0.093

0.341

0.276

0.166

0.310

0.106

0.170

0.192

0.329

0.243

0.288

0.131

0.264

0.039

0.615

0.344

0.594

0.087

0.330

0.206

0.166

0.235

0.091

0.102

0.192

0.295

0.200

0.309

Best

0.233

0.098

0.264

0.057

0.598

0.313

0.607

0.020

0.320

0.213

0.154

0.229

0.087

0.125

0.206

0.303

0.245

0.281

Weight

0.425

0.328

0.269

0.110

0.738

0.344

0.698

0.007

0.295

0.203

0.103

0.308

0.166

0.375

0.127

0.333

0.307

0.353

Full

DDTW

0.301

0.230

0.285

0.027

0.725

0.406

0.591

0.000

0.284

0.183

0.080

0.237

0.152

0.261

0.118

0.300

0.282

0.331

Best

0.315

0.180

0.269

0.054

0.785

0.438

0.594

0.007

0.281

0.181

0.040

0.231

0.149

0.284

0.103

0.303

0.304

0.353

Weight

0.425

0.230

0.456

0.049

0.587

0.328

0.623

0.027

0.275

0.212

0.131

0.202

0.046

0.034

0.199

0.562

0.233

0.317

LCSS

0.247

0.180

0.243

0.064

0.576

0.344

0.578

0.027

0.277

0.228

0.063

0.187

0.037

0.057

0.191

0.287

0.230

0.338

MSM

0.260

0.164

0.299

0.050

0.576

0.250

0.549

0.047

0.302

0.252

0.069

0.207

0.083

0.148

0.214

0.358

0.221

0.324

TWE

Bold shows the best error rate
Legend for the data set category: • image outline classification, ♣ motion classification,  sensor reading classification,  simulated data classification

0.411

0.562

Haptics♣

Lightning7

0.000

GunPoint♣

0.157

0.223

FordB

0.197

0.090

FordA

Lightning2

0.094

fish•

LargeKitchen

0.213

fiftywords•

0.057

0.040

0.242

FaceAll•

0.098

0.273

ElectricDevices

FaceFour•

0.232

0.155

FacesUCR•

0.335

0.331

Earthquakes

ECGFiveDays

0.295

Full

Full

Best

0.188

DTW

LPS

Table 1 continued

0.260

0.131

0.387

0.040

0.589

0.266

0.627

0.053

0.314

0.205

0.126

0.288

0.077

0.136

0.207

0.305

0.197

0.295

ERP

0.425

0.246

0.517

0.039

0.676

0.266

0.627

0.087

0.404

0.314

0.217

0.369

0.229

0.216

0.286

0.456

0.200

0.302

ED

Time series representation and similarity based on local autopatterns
493

123

123

0.093

0.297

MALLAT

MedicalImages•

0.006

0.218

ShapeletSim 

ShapesAll•

0.172

ProxPhalanxOut•

0.440

0.112

ProxPhalanxAge•

ScreenType

0.000

Plane

0.278

0.226

Phalanges•

0.329

0.134

OSULeaf•

ProxPhalanxTW•

0.133

OliveOil

Refr.Devices

0.147

NonInvThorax2

0.076

0.114

0.183

MoteStrain

NonInvThorax1

0.419

0.497

MidPhalanxTW•

0.536

0.188

0.083

0.431

0.320

0.254

0.172

0.132

0.000

0.220

0.248

0.133

0.161

0.202

0.232

0.523

0.208

MidPhalanxAge•

MidPhalanxOut•

0.278

0.333

0.427

0.509

0.234

0.213

0.137

0.000

0.260

0.409

0.167

0.173

0.274

0.175

0.649

0.247

0.539

0.270

0.069

0.103

Best

0.279

DTW

Full

Full

LPS

0.247

0.328

0.445

0.515

0.278

0.196

0.127

0.000

0.228

0.401

0.133

0.132

0.196

0.134

0.682

0.199

0.539

0.261

0.090

Best

0.252

0.261

0.448

0.488

0.239

0.189

0.137

0.000

0.237

0.376

0.167

0.142

0.205

0.141

0.688

0.223

0.565

0.274

0.058

Weight

0.185

0.494

0.477

0.549

0.220

0.182

0.141

0.000

0.244

0.120

0.133

0.421

0.599

0.291

0.643

0.223

0.506

0.349

0.074

Full

DDTW

0.182

0.456

0.485

0.560

0.224

0.162

0.156

0.000

0.196

0.128

0.167

0.304

0.404

0.228

0.636

0.206

0.461

0.341

0.052

Best

0.173

0.511

0.477

0.549

0.229

0.165

0.141

0.000

0.198

0.112

0.133

0.274

0.399

0.204

0.662

0.216

0.481

0.336

0.048

Weight

Table 2 The full results of LPS based measures and 11 similarity measures on 75 datasets (part two of two)

0.203

0.106

0.435

0.376

0.229

0.175

0.112

0.000

0.219

0.211

0.600

0.170

0.215

0.131

0.649

0.227

0.435

0.341

0.091

LCSS

0.190

0.139

0.536

0.416

0.273

0.192

0.122

0.010

0.248

0.227

0.167

0.117

0.193

0.128

0.649

0.254

0.506

0.261

0.067

MSM

0.337

0.178

0.485

0.480

0.224

0.223

0.122

0.000

0.281

0.223

0.133

0.128

0.188

0.191

0.636

0.289

0.539

0.299

0.067

TWE

0.247

0.367

0.459

0.365

0.259

0.220

0.132

0.000

0.242

0.397

0.133

0.121

0.185

0.130

0.656

0.220

0.506

0.324

0.090

ERP

0.272

0.444

0.533

0.515

0.278

0.206

0.127

0.038

0.246

0.483

0.133

0.132

0.196

0.125

0.695

0.254

0.526

0.311

0.090

ED

494
M. G. Baydogan, G. Runger

0.072

0.038

0.123

0.033

0.072

SonyRobot2

StarLightCurves

SwedishLeaf•

0.224

0.225

0.263

0.253

0.025

UwaveY♣

UwaveZ♣

UwaveAll♣

0.130

0.251

0.175

0.008

0.059

0.020

0.108

0.092

0.164

0.367

0.020

0.107

0.357

0.376

0.278

0.000

0.134

0.000

0.077

0.105

0.007

0.055

0.208

0.096

0.171

0.276

0.299

0.157

0.260

0.004

0.035

0.327

0.302

0.226

0.002

0.149

0.010

0.108

0.101

0.017

0.069

0.154

0.097

0.143

0.301

0.256

Best

0.147

0.276

0.003

0.034

0.335

0.303

0.226

0.000

0.134

0.000

0.077

0.105

0.007

0.055

0.126

0.096

0.140

0.266

0.293

Weight

DDTW

0.180

0.417

0.022

0.150

0.472

0.463

0.357

0.003

0.084

0.000

0.246

0.136

0.433

0.114

0.115

0.098

0.149

0.258

0.296

Full

0.171

0.315

0.003

0.066

0.375

0.377

0.270

0.003

0.086

0.010

0.154

0.154

0.433

0.087

0.096

0.091

0.150

0.309

0.296

Best

0.160

0.303

0.003

0.063

0.377

0.368

0.269

0.003

0.084

0.000

0.162

0.136

0.433

0.085

0.107

0.086

0.149

0.268

0.299

Weight

0.140

0.260

0.010

0.038

0.317

0.332

0.229

0.001

0.203

0.030

0.046

0.167

0.047

0.050

0.112

0.126

0.183

0.319

0.456

LCSS

0.135

0.229

0.003

0.036

0.301

0.302

0.232

0.001

0.060

0.070

0.115

0.132

0.027

0.033

0.104

0.114

0.126

0.260

0.232

MSM

0.132

0.254

0.004

0.061

0.312

0.314

0.229

0.002

0.040

0.010

0.138

0.132

0.013

0.030

0.109

0.119

0.139

0.319

0.293

TWE

Bold shows the best error rate
Legend for the data set category: • image outline classification, ♣ motion classification,  sensor reading classification,  simulated data classification

0.136

0.189

UwaveX♣

yoga•

0.004

0.014

TwoPatterns

0.001

0.061

TwoLeadECG

0.270

0.020

Trace

wafer

0.236

0.034

0.100

ToeSegmentation2♣

WordSynonyms•

0.240

0.077

ToeSegmentation1♣

0.027

0.030

0.027

Symbols•

SyntheticControl

0.038

0.136

0.240

0.225

SmallKitchen

DTW

Full

Best

Full

LPS

SonyRobot1

Table 2 continued

0.153

0.321

0.004

0.044

0.336

0.319

0.228

0.000

0.102

0.050

0.108

0.110

0.027

0.080

0.138

0.150

0.179

0.301

0.272

ERP

0.170

0.382

0.005

0.052

0.351

0.336

0.265

0.093

0.260

0.240

0.377

0.325

0.120

0.108

0.211

0.147

0.143

0.301

0.645

ED

Time series representation and similarity based on local autopatterns
495

123

496

M. G. Baydogan, G. Runger

Fig. 4 The average ranks for all classifiers on 75 datasets. The critical differences at 0.05 and 0.10 levels
are 2.107 and 1.957, respectively. The performance of LPSBest is significantly better than DTWBest at
level 0.10

that two classifiers have a significant difference in their performances if their average
ranks differ by at least the critical difference (Demšar 2006). The critical differences
at significance level 0.05 and 0.10 are 2.107 and 1.957, respectively.The performance
LPSBest is not significantly different at significance level 0.05 when compared to
LPS, MSM, WDTW and DTWBest, but the performance is significantly better than
DTWBest at level 0.10, and LPSBest has the best average rank. The full set of results
are available on (Baydogan 2013).
A more detailed view of the results follows the graphical approach from Ding
et al. (2008). Scatter plots in Fig. 5 show pairwise comparisons of error rates from
LPSBest against LPS, MSM, WDTW and DTWBest. Each axis represents a method
and each dot represents the error rate for a particular dataset. We draw the line x = y
to represent the region where both methods perform about the same. A point above
the line indicates that the approach on the x axis has better accuracy than the one on
the y axis for the corresponding dataset.
As seen in Fig. 5, our experiments show that LPSBest performs slightly better for
most of the datasets than MSM, WDTW and DTWBest. Similar performance of LPSBest and LPS is further discussed in Sect. 4.5. LPSBest performs significantly worse
for only four data sets (CricketX, CricketY, CricketZ, Coffee). However, Coffee only
has 28 test series and LPSBest misclassifies two more instances than the competitor for
this data set. Also, Cricket and Uwave datasets treat the attributes from a multivariate
time series separately. The performance based on the individual attributes may not be
conclusive as the class has the potential to be defined by the relationships between the
attributes.
We also compare the approaches based on the problem category to determine any
sensitivities to the type of datasets. Table 3 illustrates the average ranks of five similarity
measures based on the problem types. LPSBest is only outperformed in one case:
WDTW with the simulated data. This is our least interesting problem type over the
four problem categories. MSM provides similar performance to LPSBest for the image
data. Otherwise, LPSBest provides the best scores and LPS is second best.
We also compare approaches with respect to the length of the time series. A data
set is labeled as “long” if its length is greater than the median length over the 75 time

123

Time series representation and similarity based on local autopatterns

0.6

0.6

0.5

0.5

WDTW

(b) 0.7

LPS

(a) 0.7

0.4
0.3

0.4
0.3

0.2

0.2

0.1

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0

0.7

497

0

0.1

0.2

LPSBest

0.6

0.6

0.5

0.5

DTWBest

(d) 0.7

MSM

(c) 0.7

0.4
0.3

0.1

0.1
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.5

0.6

0.7

0.3
0.2

0

0.4

0.4

0.2

0

0.3

LPSBest

0.5

0.6

0.7

0

0

0.1

LPSBest

0.2

0.3

0.4

LPSBest

Fig. 5 Error rates for LPSBest (median of 10 replications) versus LPS, WDTW, MSM and DTWBest
Table 3 Average ranks of five classifiers for the 75 datasets by problem category
LPSBest

LPS

WDTW

MSM

DTWBest

Image

2.607

2.429

3.607

2.697

3.661

Motion

2.167

2.542

3.167

3.584

3.542

Sensor

2.207

2.811

3.207

3.500

3.276

Simulated

2.833

3.000

2.334

3.167

3.667

Overall

2.400

2.640

3.280

3.187

3.494

series, 300 time units, otherwise it is “short”. This strategy divides the problems into
two groups (with almost equal number of data sets) and the same test is conducted
for each length category. Figure 6 shows the average ranks for all classifiers on each
category. LPSBest performs considerably better for the longer time series (i.e., rank
difference from the closest competitor, MSM, is greater for the “long” category).
As mentioned by Batista et al. (2014), in order to understand the predictive capability of an algorithm, it is useful to show if better performance of a method can be
predicted ahead of time. The a priori usefulness of a classifier can be assessed with a
Texas sharpshooter plot introduced by Batista et al. (2014). The main idea is that the

123

498

M. G. Baydogan, G. Runger

Fig. 6 The average ranks for all classifiers based on the length of the time series. LPSBest performs
considerably better for the longer time series. a Long time series (T > 300). CDs for 0.05 and 0.1 levels
are 2.999 and 2.786, respectively. b Short time series (T ≤ 300). CDs for 0.05 and 0.1 levels are 2.960 and
2.749, respectively

ratio of cross-validation accuracy of two classifiers should be consistent with the ratio
of the accuracy over the test data. This way, if the method provides reliable ratios, then
one can decide which classifier to use. In other words, if both ratios are greater than
one, we predict a gain in one classifier and also observe a gain (true positive, TP). If
both ratios are less than one, we expect a degradation in the performance and observe a
worse performance (true negative, TN). In all the other cases, the outcome is not desirable. A Texas sharpshooter plot of LPSBest versus DTWBest is provided in Fig. 7.
For 58 of the data sets, we correctly predicted LPSBest to be better or worse (i.e., TP
+ TN). There are a number of data sets whose ratio is around point (1, 1). These data
sets represent marginal increases/decreases in accuracy (Batista et al. 2014). There is
only one data set with a substantive false negative, MidPhalanxTW, where LPSBest
is claimed to decrease accuracy, but the actual accuracy increased.
The test error rates of LPSBest and LPS with regression versus LPS with random
splits are illustrated in Fig. 8. Reported values are the median of the error rates over
10 replications. There are few datasets for which “regression splits” are performing

123

Time series representation and similarity based on local autopatterns

499

1.8

Actual Accuracy Gain

1.6
1.4
1.2
False Negative

True Positive

True Negative

False Positive

1
0.8
0.6
0.4
0.2
0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

Expected Accuracy Gain

Fig. 7 Texas sharpshooter plot of LPSBest vs DTWBest. Scatter plot of expected accuracy gain versus
the actual gain in accuracy over 75 data sets shows if one can rely on the cross-validation accuracy of
LPSBest when compared to DTWBest. Expected accuracy gain is the ratio of the cross-validation accuracy
of LPSBest and DTWBest (on training data). Similarly actual accuracy gain is computed by dividing
LPSBest’s accuracy by DTWBest’s on the test data. Points around point (1, 1) are not interesting as they
represent marginal increases/decreases in accuracy (Batista et al. 2014)

0.6

0.6

0.5

0.5

LPS random

(b) 0.7

LPS random

(a) 0.7

0.4
0.3
0.2
0.1
0

0.4
0.3
0.2
0.1

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0

0

LPSBest

0.1

0.2

0.3

0.4

0.5

0.6

0.7

LPS regression

Fig. 8 The test error rates of LPSBest and LPS with regression versus LPS with random splits. “Regression
splits” slightly improves the accuracy, but the improvements are small. Overall, LPSBest and LPS with
“regression splits” provides better error rates than “random splits” on 51 and 45 data sets, respectively

substantially better than “random splits”. “Regression splits” slightly improves the
accuracy, but the improvements are small. Overall, LPSBest and LPS with “regression splits” provides better error rates than “random splits” on 51 and 45 data sets,
respectively. The good performance of “random splits” is discussed further in Sect. 4.5.
4.2 LPS for multivariate time series similarity
Similarity computation over multivariate time series is a challenging task. Most of
the similarity-based approaches compute the similarity over individual attributes of

123

500

M. G. Baydogan, G. Runger

MTS and use the weighted sum of the similarities to obtain a final similarity measure for MTS. However, MTS are not only characterized by individual attributes
but also their relationships. Hence use of similarities between individual attributes
might be problematic for MTS. Moreover, some of the attributes of MTS may be
categorical or they may differ in scale even if they are numeric. For example, consider a network traffic classification problem where observations are the series of
network flows (Sübakan et al. 2014) and the aim is to identify the application that
generates the flow. Each flow is defined as a series of network packets transferred
between IP-Port pairs. Each packet is characterized by four attributes: packet size,
transfer direction, payload and the duration between this packet and the previous
packet. The transfer direction of a packet (i.e. upstream or downstream) is basically
a categorical attribute. Duration between the packets has a different scale. Similarity computation in such cases may not be effective with traditional approaches when
individual attributes are considered. On the other hand, LPS is capable of generating
a similarity measure based on the relationships between the attributes under these
circumstances.
We illustrate the benefits of our proposed approach on 15 MTS datasets introduced
by Frank and Asuncion (2010), Keogh et al. (2011), Olszewski (2012), Sübakan et al.
(2014), CMU (2012). The characteristics of the datasets are provided in Table 4. We
compare LPS to DTW with a full window. We only consider DTW with a full window
since the selection of the best window based on individual attributes is not well-defined
in multivariate settings. Furthermore, DTW requires each attribute to have the same
scale. We employ the same strategy in Sect. 4 and standardize each attribute to zero
mean and unit standard deviation. This transformation is not performed for LPS as trees
are scale invariant and this standardization has the potential to distort the interactions
between the attributes (Cortina 1993). Another transformation is required for categorical predictors to obtain numerical attributes for DTW. The direction information in
“Network Flow” dataset is transformed to a binary representation.
Table 4 summarizes the error rates of LPS and DTW with a full warping window.
We use the same settings as in the univariate case for LPS (D = 6, J = 200) and
report the median error rate over 10 replications. LPS performs better than DTW with
a full window for most of the datasets. For some of the datasets such as Japanese
Vowels or Network Flow, DTW suffers from the standardization of each attribute. For
example, Network Flow data has the transfer direction and the duration between the
consecutive packets as time series data. Standardization of the binary variable used to
represent the transfer direction is not well-defined. Similarly, scaling the time between
consecutive packets is also problematic. On the other hand, LPS models the local
correlation within and between the attributes of multivariate time series. The Network
Flow dataset contains irregularly-spaced time series (time between the packets is not
the same), but the time between consecutive packets is considered as an attribute.
As a tree-based approach, LPS can model the dependency structure for this type of
dataset. On the other hand, the warping path computation in DTW is problematic for
Network Flow as the time between the observations is not constant. Note that there is
no parameter tuning to make LPS comparable to DTW with a full window. The same
set of parameters are used for all datasets. This setting is even the same one used by
LPS for univariate time series.

123

12

Japanese Vowels 

2

2

2

Libras ♣

DigitsShape •

Shapes •
52–98

30–98

45

39–152

315

60–182

50–997

104–198

7–29

4–93

45–136

127-580

128–1918

274–841

144

length

3

4

15

2

8

20

2

2

9

10

95

2

2

2

7

# of classes

• image outline classification, ♣ motion classification,  sensor reading classification

3

2

UWaveMTS ♣

ECG 

3

13

ArabicDigits 

Handwri. Char. ♣

22

AUSLAN ♣

4

62

CMU_MOCAP_S16 ♣

Network Flow 

62

WalkvsRun ♣

6

62

KickvsPunch ♣

Wafer 

963

PEMS 

# of attributes

Table 4 Multivariate time series classification datasets and their characteristics

18

24

180

100

896

300

803

298

270

6600

1140

29

28

16

12

16

180

100

3582

2558

534

4896

370

2200

1425

29

16

10

173

0.000

0.000

0.097

0.180

0.020

0.035

0.032

0.038

0.049

0.029

0.246

0.000

0.000

0.100

0.156

0.000

0.063

0.200

0.150

0.071

0.033

0.288

0.040

0.351

0.092

0.238

0.069

0.000

0.100

0.168

Full

267

DTW
LPS

Train

Test

Dataset size

Sübakan et al. (2014)

Sübakan et al. (2014)

Frank and Asuncion (2010)

Olszewski (2012)

Keogh et al. (2011)

Frank and Asuncion (2010)

Sübakan et al. (2014)

Olszewski (2012)

Frank and Asuncion (2010)

Frank and Asuncion (2010)

Frank and Asuncion (2010)

CMU (2012)

CMU (2012)

CMU (2012)

Frank and Asuncion (2010)

Source

Time series representation and similarity based on local autopatterns
501

123

502

M. G. Baydogan, G. Runger

Moreover, Table 2 also has the results for uWaveGestureLibrary where each axis
is combined to obtain a univariate time series. This is shown in the row for uWaveAll.
Consider the performance of full LPS with regression splits where the error rate is
0.034. Although a combined univariate analysis with LPS accounts for some relationships between individual axes (depending on the segment length, shorter segments are
required to model long-term dependencies), explicitly modeling relationships between
individual attributes (i.e. cross-correlation) by the segment selection in the multivariate version (with full LPS and regression splits) improves the results substantially to
0.020. This result is also better than the LPSBest result (i.e. 0.025) for the combined
univariate series (uWaveAll). A multivariate implementation of LPS is also provided
as a MATLAB script at Baydogan (2013).

4.3 Sensitivity analysis
Given a splitting strategy, LPS requires the setting of three parameters, segment length
(L) and tree building parameters (J and D). Segment length is the only parameter of
importance in our approach because LPS is robust to the settings of J and D if set
sufficiently large. On the other hand, segment length is selected randomly by each
tree by LPS and it has been shown to work well. To illustrate the robustness of our
approach, we choose five datasets (Fish, SweedishLeaf, SmallKitchen, MedicalImages,WordsSynonyms) and show the classification accuracy for selected tree building
parameters (J and D). These datasets provide a reasonable number of training and test
time series. Here 10 replications are conducted for each setting combination. Depth
and number of tree levels considered in this experimentation are D ∈ {2, 4, 6, 8, 10}
and J ∈ {10, 50, 100, 250, 500}.
Average test error rates over the datasets and the replications are shown in Fig. 9. The
depth is prefixed to D = 6 in the experiments that use different J settings for which the
results are illustrated in Fig. 9a, b. We also report the average 10-fold cross-validation
(CV) error rate for each setting. Larger J provides better error rates. Moreover, the
results are more stable with a larger number of trees (i.e., error variance is smaller over
10 replications). If there is no concern regarding the computation time, J should be
set large. Our experiments in Sect. 4 used J = 200 which provided reasonable results
over all datasets. The change in the average CV error rates is similar to the progression
of test error rates.
Figure 9c, d show the sensitivity of the error rates to the depth parameter, D, when
J = 200. As discussed in Sect. 3.6, D can be dropped by growing a full tree. However,
the control over the representation size is lost with this approach. If there is a need to
control the complexity of the representation, this may be inappropriate. The figures
show limited sensitivity to D for modest depth values. As for J , the CV and test error
rate progressions are similar.
We also performed another experiment on all datasets to illustrate the robustness of
LPS to depth setting. We ran 10 replicates of LPS over all datasets with D ∈ {6, 8, 10}
and J = 100. Figure 10 shows the boxplot of the error rates obtained for each setting.
Each boxplot shows the distribution of 75×10 = 750 error rates. There is no significant
difference between the performances for different depth settings.

123

Time series representation and similarity based on local autopatterns

0.45
0.4

0.3
0.25
0.2

0.35
0.3
0.25
0.2
0.15

0.15

0.1

0.1

0.05

0.05

10

50

100

250

MedicalImages
SmallKitchen
SwedishLeaf
WordSynonyms
fish

0.4

Test error rate

CV error rate

0.45

MedicalImages
SmallKitchen
SwedishLeaf
WordSynonyms
fish

0.35

503

0

500

10

50

J

500

0.45
MedicalImages
SmallKitchen
SwedishLeaf
WordSynonyms
fish

0.4

0.3
0.25
0.2

0.35
0.3
0.25
0.2
0.15

0.15

0.1

0.1

0.05
2

4

6

8

10

MedicalImages
SmallKitchen
SwedishLeaf
WordSynonyms
fish

0.4

Test error rate

0.35

CV error rate

250

J

0.45

0.05

100

0

2

4

D

6

8

10

D

Fig. 9 Test and 10-fold CV error rates for selected values of J (D = 6) and D (J = 200) over five datasets
(10 replications)
Fig. 10 The boxplot of the error
rates obtained for each depth
setting, D ∈ {6, 8, 10}. Each
boxplot shows the distribution of
75 × 10 = 750 error rates. The
distribution of error rates does
not change significantly for
different depth settings

0.6
0.5
0.4
0.3
0.2
0.1
0
6

8

10

D

4.4 Computational complexity
We implemented LPS as an R Core (2014) package and our experiments use an Ubuntu
14.04 system with 16 GB RAM, dual core CPU (i7-3540M, 3.0 GHz). Although the
CPU can handle four threads in parallel, only a single thread is used.

123

504

M. G. Baydogan, G. Runger

StarLightCurves dataset from Keogh et al. (2011) is used to demonstrate the effect
of the parameters J, N , T and D on the testing times empirically. Testing time is
basically the elapsed time for querying the training time series database to find the
best match (i.e., the most similar time series) and smaller query times are required for
many applications. For each dataset, we randomly selected γ ∈ {0.2, 0.4, 0.6, 0.8, 1}
proportion of the number of time series (γ N ) and the length of the time series (γT ). The
levels considered for J and D are J ∈ {50, 100, 150, 200, 250} and D ∈ {4, 5, 6, 7, 8}.
Here 10 replications are conducted for each setting combination.
The time for testing (J = 200 and D = 6) is illustrated in Fig. 11a. A linear increase
in the query time with γ N and γT is consistent with the complexity of LPS discussed
in Sect. 3.7. Consequently, LPS is very convenient to use with large databases with
long time series.
In practice, the testing time does not increase exponentially with the increase in
D as illustrated in Fig. 11b. Empirically, the change in computational time is similar
to D log D behavior. Also, the proposed approach is very fast in practice. If a very
small decrease in the computational time is of practical concern, bounding schemes
can be used to accelerate this approach. The simplest and well-known approach for
NNEuclidean is early abandoning (EA) (Keogh et al. 2005) as mentioned earlier.
Similar to EA for NNEuclidean, we can stop the calculation if the current sum of the
absolute differences between each pair of representations exceeds the best similarity
so far (Keogh et al. 2006). The computational time can be reduced substantially with
this bounding scheme (Rakthanmanon et al. 2012).
4.5 Discussion on LPS based approaches and the splitting strategies
Section 3.1 discusses how LPS models the relationship within the time series (both
univariate and multivariate). Empirical evidence for the ability of LPS to model such
relationships is provided by the good results of LPS-based measures on the ARSim
dataset. The error rate of LPSBest is 0.004 where the closest competitor provides
0.101 (DDTW and WDDTW). ARSim is a simulated data set designed to introduce

Test Time (sec)

0.1

γT
γT
γT
γT
γT

=
=
=
=
=

(b)

0.2
0.4
0.6
0.8
1

0.3

0.08
0.06
0.04
0.02
0

0.4
0.35

Test Time (sec)

(a) 0.12

J
J
J
J
J

=
=
=
=
=

50
100
150
200
250

0.25
0.2
0.15
0.1
0.05

0.2

0.4

0.6

γN

0.8

1

0

4

5

6

7

8

D

Fig. 11 Test times with changing values of the of the parameters J, N , T and D. a (J = 200 and D = 6) and
b (γ N = 1 and γ T = 1)

123

Time series representation and similarity based on local autopatterns

505

a challenge for classifiers working in the time domain (Bagnall et al. 2012). It is a
binary classification problem where each class follows two different AR(7) models.
Because of the autoregressive nature of LPS (i.e., one segment is used to predict the
other), the error rates are substantially lower for this data set.
The good performance of “random splits” shown in Fig. 8 is not surprising because
of the substantial overlap between the columns of a segment matrix. The columns of a
segment matrix are obtained by sliding a window of length L by one time unit (stride
= 1). Therefore, patterns are highly likely to be captured with the help of recursive
partitioning, even if each segment is selected randomly.
However, “random splits” is likely to perform poorly for multivariate time series
because the dependencies between the multiple series are not considered explicitly. A
multivariate time series is transformed to univariate time series by concatenating the
individual axis in UWaveAll dataset. Although it is treated as a univariate time series
in Table 1, “regression splits” can model the dependencies between axes with an error
rate of 0.034 (Sect. 4.2) where the error rate of “random split” is 0.051. This dataset
is a good example because gestures are not defined by the movements over individual
axes. The interaction of the moves over different axes is important to define the classes.
Thus, the multivariate version of LPS has an error rate of 0.022. As parameter selection
introduces a better selection of the segment length which is important for this particular
problem, the error rate for LPSBest is also very small (i.e., 0.025).

4.6 Use of learned representation as an input to learning algorithms
LPS obtains similarity from a representation based on tree-based ensembles. The
learned representation can also be used as an input to learning algorithms. Each tree
in the ensemble generates a representation and the combined representation obtained
from the trees generates a sparse vector. Each tree potentially captures different information about the series which is important for many learning tasks.
In order to illustrate other potential uses of the learned representation, we apply
principal component analysis (PCA) to two alternative representations of the CBF
dataset. The first representation considers the raw values as the feature vector (a vector
of length 128). Then we generate the LPS representation with the same parameter
settings as in our experiments. Figure 12 plots the first two PCA scores for each time
series from both representations. To assess the performance of both representations
visually, the class information is color-coded. PCA on the LPS representation shows
better separation of the three classes as illustrated in Fig. 12. Although this is an
example for a classification task, the LPS representation can be used for other tasks
such as forecasting and it has a natural extension to clustering, anomaly detection,
etc., with the proposed similarity measure.
If the learned representation is used as the input to a learning algorithm, feature
selection might help. LPS generates a sparse representation where each tree provides
different information. With the help of feature selection, relevant information can be
captured. Split choice should be less important in such cases and use of “random splits”
has the advantage of its computational efficiency. However, details of this study are
beyond the scope of the current work here.

123

506

M. G. Baydogan, G. Runger

(a) 10

(b) 50

Class 1
Class 2
Class 3

8

30

6

20

PC 2

PC 2

4
2
0

10
0
−10

−2

−20

−4

−30

−6
−10

Class 1
Class 2
Class 3

40

−5

0

5

10

15

−40
−60

−40

PC 1

−20

0

20

40

PC 1

Fig. 12 First two principal components from PCA applied to the LPS representation and the raw time
series of the CBF dataset. PCA on LPS representation shows better separation of the three classes. a PCA
on raw time series. b PCA on LPS representation

4.7 Missing values
Estimation of the missing values is commonly employed for time series datasets.
However, the estimation method introduces a new parameter to the time series problem.
On the other hand, LPS handles the data with missing values without a need for any
additional step as our learning strategy inherits the properties of decision tree learning.
uWaveGestureLibrary Liu et al. (2009) is used here to illustrate the performance
of the multivariate version of LPS when there are missing values. To simplify the
experiments, difference series are discarded. For each axis and instance, we randomly
removed γ ∈ {0.01, 0.05, 0.1, 0.25, 0.5} proportion of the values in the training data
and the test data. The error rates over 10 replications are provided in Fig. 13. For the
0.065
0.06

Error rate

0.055
0.05
0.045
0.04
0.035
0.03
0.01

0.05

0.1

0.25

0.5

γ
Fig. 13 Boxplot of the test error rates with different proportions of missing values for uWaveGestureLibrary
(Liu et al. 2009) (10 replications). LPS is robust to large proportions of missing values without requiring
specific mechanisms

123

Time series representation and similarity based on local autopatterns

507

gesture recognition task, LPS performs reasonably well even with large proportions
of missing values.

5 Conclusions
This study proposes a novel time series representation based on the idea of nonlinear dependencies described as autopatterns that can occur locally in time. Time
series segments are extracted and then partitioned according to simple rules to detect
these patterns. Random partitions and those generated from regression trees applied
to random predictor and target segments are used to generate a representation. Consequently, features are learned within the method based on the dependencies in the time
series. This avoids the feature extraction step which is common in the feature-based
approaches. The method discovers the patterns of the time series with a tree-based
ensemble learning strategy. The approach conceptually generalizes autoregressive
models to detect dependencies that are potentially local and nonlinear in the time
series. Consequently, LPS is based on a traditional approach, but provides a promising new research direction.
A robust similarity measure called learned pattern similarity (LPS) based on the
matching patterns of the time series is also presented. Our experimental results show
that LPS does not require the setting of many parameters and provides fast and competitive results on benchmark datasets from several domains. Also, an R Core (2014)
package (named as LPStimeSeries) is implemented as part of this study.
Acknowledgments This research was partially supported by the Scientific and Technological Research
Council of Turkey (TUBITAK) Grant Number 114C103.

References
Akl A, Valaee S (2010) Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, compressive sensing. In: 2010 IEEE International conference on acoustics speech and signal
processing (ICASSP), pp 2270–2273
Bagnall A, Davis LM, Hills J, Lines J (2012) Transformation based ensembles for time series classification.
In: SDM, vol. 12. SIAM, pp 307–318
Batista G, Keogh E, Tataw O, de Souza V (2014) Cid: an efficient complexity-invariant distance for time
series. Data Min Knowl Discov 28(3):634–669. doi:10.1007/s10618-013-0312-3
Baydogan MG (2013) Learned pattern similarity (LPS). homepage: www.mustafabaydogan.com/
learned-pattern-similarity-lps.html
Baydogan MG, Runger G (2014) Learning a symbolic representation for multivariate time series classification. Data Min Knowl Discov pp 1–23. doi:10.1007/s10618-014-0349-y
Baydogan MG, Runger G, Tuv E (2013) A bag-of-features framework to classify time series. IEEE Trans
Pattern Anal Mach Intell 35(11):2796–2802
Breiman L, Friedman J, Olshen R, Stone C (1984) Classification and regression trees. Wadsworth, Belmont
Chakrabarti K, Keogh E, Mehrotra S, Pazzani M (2002) Locally adaptive dimensionality reduction for
indexing large time series databases. ACM Trans Database Syst 27(2):188–228
Chen H, Tang F, Tino P, Yao X (2013) Model-based kernel for efficient time series analysis. In: Proceedings
of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,
New York, pp 392–400

123

508

M. G. Baydogan, G. Runger

Chen L, Özsu MT, Oria V (2005) Robust and fast similarity search for moving object trajectories. In:
Proceedings of the 2005 ACM SIGMOD International conference on management of data, SIGMOD
’05. ACM, New York, pp 491–502. doi:10.1145/1066157.1066213
Cortina JM (1993) Interaction, nonlinearity, and multicollinearity: implications for multiple regression. J
Manag 19(4):915–922
Cuturi M (2011) Fast global alignment kernels. In: Getoor L, Scheffer T (ed) Proceedings of the 28th
international conference on machine learning (ICML-11). ACM, New York, pp 929–936
CMU (2012) Graphics Lab Motion Capture Database: Homepage: mocap.cs.cmu.edu
Demšar J (2006) Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 7:1–30
Ding H, Trajcevski G, Scheuermann P, Wang X, Keogh E (2008) Querying and mining of time series data:
experimental comparison of representations and distance measures. Proc VLDB Endow 1:1542–1552
Frank A, Asuncion A (2010) UCI machine learning repository. http://archive.ics.uci.edu/ml
Friedman M (1940) A comparison of alternative tests of significance for the problem of m rankings. Ann
Math Stat 11(1):86–92. http://www.jstor.org/stable/2235971
Fu T (2011) A review on time series data mining. Eng Appl Artif Intell 24:164–181
Gaidon A, Harchaoui Z, Schmid C (2011) A time series kernel for action recognition. In: BMVC 2011British machine vision conference. BMVA Press, Dundee, pp 63–1
Geurts P (2001) Pattern extraction for time series classification. Principles of data mining and knowledge
discovery. Lecture Notes in Computer Science, vol 2168. Springer, Berlin, pp 115–127
Geurts P, Ernst D, Wehenkel L (2006) Extremely randomized trees. Mach Learn 63(1):3–42
Grabocka J, Schmidt-Thieme L (2014) Invariant time-series factorization. Data Min Knowl Discov 28(5—
-6):1455–1479
Han J, Kamber M, (2001) Data mining: concepts and techniques. The Morgan Kaufmann Series In Data
Management Systems. Elsevier Books, Oxford. http://books.google.com/books?id=6hkR_ixby08C
Hastie T, Tibshirani R, Friedman J (2009) Elements of statistical learning. Springer, Berlin
Hills J, Lines J, Baranauskas E, Mapp J, Bagnall A (2014) Classification of time series by shapelet transformation. Data Min Knowl Discov 28(4):851–881. doi:10.1007/s10618-013-0322-1
Jaakkola T, Diekhans M, Haussler D (1999) Using the fisher kernel method to detect remote protein homologies. In: ISMB vol. 99, pp 149–158
Jebara T, Kondor R, Howard A (2004) Probability product kernels. J Mach Learn Res 5:819–844. http://dl.
acm.org/citation.cfm?id=1005332.1016786
Jeong YS, Jeong MK, Omitaomu OA, (2011) Weighted dynamic time warping for time series classification.
Pattern Recognit 44(9): 2231–2240. doi:10.1016/j.patcog.2010.09.022. http://www.sciencedirect.
com/science/article/pii/S003132031000484X. Computer Analysis of Images and Patterns
Keogh E, Kasetty S (2003) On the need for time series data mining benchmarks: a survey and empirical
demonstration. Data Min Knowl Discov 7(4):349–371
Keogh E, Lin J, Fu A (2005) HOT SAX: efficiently finding the most unusual time series subsequence. In:
Proceedings of the fifth IEEE international conference on data mining, ICDM ’05. IEEE Computer
Society, Washington, DC, pp 226–233
Keogh E, Wei L, Xi X, Lee SH, Vlachos M (2006) LB_Keogh supports exact indexing of shapes under
rotation invariance with arbitrary representations and distance measures. In: Proceedings of the 32nd
international conference on very large data bases, VLDB ’06. VLDB Endowment, pp 882–893
Keogh E, Zhu Q, Hu BYH, Xi X, Wei L, Ratanamahatana CA (2011) The UCR time series classification/clustering. homepage:www.cs.ucr.edu/~eamonn/time_series_data/
Keogh EJ, Pazzani MJ (2001) Derivative dynamic time warping. In: SDM, vol. 1. SIAM, pp 5–7
Kuksa P, Pavlovic V (2010) Spatial representation for efficient sequence classification. In: 2010 20th International conference on pattern recognition (ICPR), pp 3320–3323
Latecki L, Megalooikonomou V, Wang Q, Lakaemper R, Ratanamahatana C, Keogh E (2005) Partial elastic
matching of time series. In: Fifth IEEE international conference on data mining, pp 701–704
Liao TW (2005) Clustering of time series data-a survey. Pattern Recogn 38(11):1857–1874. doi:10.1016/
j.patcog.2005.01.025
Lin J, Keogh E, Lonardi S, Chiu B (2003) A symbolic representation of time series, with implications for
streaming algorithms. In: Proceedings of the 8th ACM SIGMOD workshop on research issues in data
mining and knowledge discovery. ACM Press, New York, pp 2–11
Lin J, Keogh E, Wei L, Lonardi S (2007) Experiencing SAX: a novel symbolic representation of time series.
Data Min Knowl Discov 15:107–144

123

Time series representation and similarity based on local autopatterns

509

Lin J, Khade R, Li Y (2012) Rotation-invariant similarity in time series using bag-of-patterns representation.
J Intell Inf Syst 39(2):287–315
Lines J, Bagnall A (2014) Time series classification with ensembles of elastic distance measures. Data Min
Knowl Discov 29(3):565–592. doi:10.1007/s10618-014-0361-2
Liu J, Wang Z, Zhong L, Wickramasuriya J, Vasudevan V (2009) uWave: Accelerometer-based personalized
gesture recognition and its applications. IEEE International conference on pervasive computing and
communications, pp 1–9
Lowe DG (1995) Similarity metric learning for a variable-kernel classifier. Neural Comput 7(1):72–85
Marteau PF (2009) Time warp edit distance with stiffness adjustment for time series matching. IEEE Trans
Pattern Anal Mach Intell 31(2):306–318. doi:10.1109/TPAMI.2008.76
Nemenyi P (1963) Distribution-free multiple comparisons. Princeton University, Princeton
Olszewski RT (2012)http://www.cs.cmu.edu/~bobski/. Accessed June 10
R Core Team (2014) R: A language and environment for statistical computing. R Foundation for Statistical
Computing, Vienna.http://www.R-project.org/
Rakthanmanon T, Campana B, Mueen A, Batista G, Westover B, Zhu Q, Zakaria J, Keogh E (2012) Searching
and mining trillions of time series subsequences under dynamic time warping. In: Proceedings of the
18th ACM SIGKDD international conference on knowledge discovery and data mining, KDD ’12.
ACM, New York, pp 262–270
Rakthanmanon T, Keogh E, Fast Shapelets: A scalable algorithm for discovering time series shapelets, chap.
73, pp. 668–676. doi:10.1137/1.9781611972832.74
Ratanamahatana C, Keogh E (2005) Three myths about dynamic time warping data mining. In: Proceedings
of SIAM international conference on data mining (SDM05), vol 21, pp 506–510
Ratanamahatana CA, Lin J, Gunopulos D, Keogh E, Vlachos M, Das G (2010) Mining time series data.
In: Maimon O, Rokach L (eds) Data mining and knowledge discovery handbook. Springer, Berlin, pp
1049–1077
Shieh J, Keogh E (2008) iSAX: indexing and mining terabyte sized time series. In: Proceedings of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08. ACM,
New York, pp 623–631
Stefan A, Athitsos V, Das G (2013) The move-split-merge metric for time series. IEEE Trans Knowl Data
Eng 25(6):1425–1438. doi:10.1109/TKDE.2012.88
Sübakan YC, Kurt B, Cemgil AT, Sankur B (2014) Probabilistic sequence clustering with spectral learning. Dig Signal Process 29(0):1–19. doi:10.1016/j.dsp.2014.02.014. http://www.sciencedirect.com/
science/article/pii/S1051200414000517
Wang Q, Megalooikonomou V, Faloutsos C (2010) Time series analysis with multiple resolutions. Inf Syst
35(1):56–74
Wang X, Mueen A, Ding H, Trajcevski G, Scheuermann P, Keogh E (2013) Experimental comparison of
representation methods and distance measures for time series data. Data Min Knowl Discov 26(2):275–
309

123

2796

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Short Papers

Mustafa Gokce Baydogan,
George Runger, and Eugene Tuv
Abstract—Time series classification is an important task with many challenging
applications. A nearest neighbor (NN) classifier with dynamic time warping (DTW)
distance is a strong solution in this context. On the other hand, feature-based
approaches have been proposed as both classifiers and to provide insight into the
series, but these approaches have problems handling translations and dilations in
local patterns. Considering these shortcomings, we present a framework to
classify time series based on a bag-of-features representation (TSBF). Multiple
subsequences selected from random locations and of random lengths are
partitioned into shorter intervals to capture the local information. Consequently,
features computed from these subsequences measure properties at different
locations and dilations when viewed from the original series. This provides a
feature-based approach that can handle warping (although differently from DTW).
Moreover, a supervised learner (that handles mixed data types, different units,
etc.) integrates location information into a compact codebook through class
probability estimates. Additionally, relevant global features can easily supplement
the codebook. TSBF is compared to NN classifiers and other alternatives (bag-ofwords strategies, sparse spatial sample kernels, shapelets). Our experimental
results show that TSBF provides better results than competitive methods on
benchmark datasets from the UCR time series database.
Index Terms—Supervised learning, feature extraction, codebook

Ç
INTRODUCTION

CLASSIFICATION of time series is an important task with many
challenging applications such as signature verification, speech
recognition, or financial analysis. Algorithms can be divided into
instance-based and feature-based methods. Instance-based classifiers predict a test instance based on its similarity to the training
instances. For time series, one nearest neighbor (NN) classifiers
with euclidean (NNEuclidean), or a dynamic time warping
distance (NNDTW) have been widely and successfully used [1],
[2], [3], [4], [5]. Although euclidean distance is time and space
efficient, it is often weak in terms of classification accuracy [3].
DTW [6] allows a measure of the similarity invariant to certain
nonlinear variations in the time dimension and is considered as a
strong solution for time series problems [7]. DTW attempts to
compensate for possible time translations/dilations between
patterns. However, previous authors argued that it is more
appropriate to measure similarity from higher level structures in
long time series, rather than point-to-point, local comparisons [8].
DTW also provides limited insight into the classifier. Instances can

. M.G. Baydogan is with Security and Defense System Initiative, 781 E.
Terrace Rd., Tempe, AZ 85287. E-mail: mbaydoga@asu.edu.
. G. Runger is with the School of Computing, Informatics and Decision
Systems Engineering, Arizona State University, 699 S. Mill Avenue,
Brickyard Engineering (BYENG) 553, Tempe, AZ 85281.
E-mail: runger@asu.edu.
. E. Tuv is with Intel, Logic Technology Development, 5000 W Chandler
Blvd, CH5295, Chandler, AZ 85226.
Manuscript received 6 Apr. 2012; revised 6 Oct. 2012; accepted 19 Mar. 2013;
published online 11 Apr. 2013.
Recommended for acceptance by F. de la Torre.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number
TPAMI-2012-04-0254.
Digital Object Identifier no. 10.1109/TPAMI.2013.72.
0162-8828/13/$31.00 ß 2013 IEEE

NO. 11,

NOVEMBER 2013

___________________________________________________________________________________________________

A Bag-of-Features Framework
to Classify Time Series

1

VOL. 35,

Published by the IEEE Computer Society

be partitioned based on DTW distance [9], but distances are still
not easily interpreted. As an alternative, [10], [11], [12] proposed to
identify subsequences (called shapelets) that are representative of a
class using euclidean distance. Furthermore, Kuksa and Pavlovic
[13] similarly generated subsequences from the time series and
defined spatial similarity kernels based on the subsequences
(similarity-based approach), with classification from a support
vector machine (SVM) [14].
Feature-based approaches are generally faster (depending on
feature extraction and classification algorithms). In this direction,
Geurts [15] used the discontinuity points from a piecewise-linear
approximation of the time series to detect patterns and classify the
series. A genetic algorithm-based feature extraction was proposed
by Eads et al. [16] and SVM was trained on the extracted features.
Also, Nanopoulos et al. [17] proposed a multilayer perceptron
neural network fed by statistical features (e.g., means and standard
deviations) calculated from the series. Moreover, Rodrı́guez et al.
[18] used intervals of time series to extract features on which an
SVM was trained. Standard classification algorithms can be built
on global features easily, but they may omit important local
characteristics. Local features can supplement global information
with useful patterns, but the set of local features may vary in
cardinality and lack a meaningful ordering that can cause
problems for algorithms that require feature vectors with a fixed
dimension. Furthermore, methods based on features of intervals
(such as [19], [20]) assume that patterns exist in the same time
interval over the instances, but a pattern that defines a certain class
may exist anywhere in time [15], as well as be dilated in time.
Our work is based on the bag-of-features (BoF) approach in
which complex objects are characterized by feature vectors of
subobjects. BoF representations are popular, mostly in computer
vision as content-based image retrieval [21], [22], [23], natural
scene classification [24], and object detection and recognition [25],
[26], [27], [28], [29] because of their simplicity and good
performance [30]. A BoF is also referred to as a bag-of-words
(BoW) [31] in document classification, a bag-of-instances in
multiple instance learning [32], [33], and a bag-of-frames in audio
and speech recognition [34], [35]. Local image descriptors are
sampled (e.g., random, interest point detector [36]) and characterized by their feature vectors (e.g., distribution of the pixel values).
A visual dictionary is learned using the vectors of visual
descriptors (e.g., clustering to assign discrete labels to descriptors).
The resulting distribution of descriptors is quantized through
the codebook (e.g., a histogram of the cluster assignments for the
sampled descriptors of each instance) as the summary of the
image. Similarly, time series segments may contain rich local
information about the time series. A BoF representation allows one
to integrate local information from segments of the time series in
an efficient way. Moreover, assumptions on the cardinality of the
local feature set and patterns in the same time interval can be
relaxed by this framework.
Studies on BoF representations for time series data are limited,
with few studies in audio and speech recognition literature [34],
[35], [37], [38], [39]. Time series similarity based on a BoW
representation was considered by Lin and Li [8]. Also, time series
were discretized by symbolic aggregate approximation (SAX) [40]
and time series were represented as words using the symbols
generated by SAX. Similarities of the time series were then
computed using the representations from document classification
approaches. Our study also works on the interval features, but we
consider fixed- and variable-length intervals, and we also include
shape-based features such as the slope and variance. Moreover, we
learn the bag-of-features representation by training a classifier on

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

the interval features, and this provides a substantially different
representation than those from document classification literature.
In [38], the speech signals were represented as images through
preprocessing (simulation, strobe detection, temporal integration)
and patches were segmented from the images. Using vector
quantization, segments were represented by sparse codes and they
were aggregated through histograms to generate features at the
bag level. Also, Fu et al. [35] used a clustering approach to
summarize the local information to a bag level.
Histogram-based approaches for image classification problems
do not take the spatial location of the local patches into account in
codebook generation. Analogously, BoF models in time series
ignore the temporal ordering inherent in the signal and therefore
may not identify a specific content or pattern [41]. Consequently,
here we use a feature-based approach, but extract multiple
subsequences from each time series. We consider uniform
subsequences which are systematically segmented from the time
series and subsequences selected from random locations and
random lengths. Features computed from the random subsequences (e.g., mean, standard deviation) measure properties at
different locations and dilations when viewed from the original
time series. We form a matrix of these features, but the value in
row i and row j of the same column may be calculated from
subsequences that differ in location and/or length. We further
partition subsequences into intervals to detect patterns represented
by a series of values over shorter time segments. Moreover,
location information is added and easily integrated via a
supervised process to construct a codebook and a learner that
handles mixed data types, different units, and so on.
Subsequences are labeled and these features are input to a treebased (recursive partitioning) ensemble that partitions subsequences relevant to the class. In this manner, we provide a
feature-based approach that can handle warping (but differently
from DTW). The ensembles are trained to construct a compact
codebook from simple class probability estimate (CPE) distributions. Additionally, relevant global features can easily supplement
the codebook in our framework. Our supervised approach
provides a fast, efficient representation, even with very basic
features such as slopes, means, and variances from the subsequences. We demonstrate TSBF is efficient and accurate by
comparing to alternative time series classifiers on a full set of
benchmark datasets.
Random and uniform subsequence generation are compared.
Also, we compare to classifiers based on an unsupervised
codebook (from K-means clustering) and to classifiers that do
not use codebooks (applied to the raw values or to extracted
features). We also compare to sparse spatial sample kernels (SSSK)
[13] and shapelets [12].
The remainder of this paper is organized as follows: We
summarize the problem and describe the TSBF framework in
Section 2. Section 3 demonstrates the TSBF by testing on a full set
of benchmark datasets from UCR time series database [42]. Section
4 provides sensitivity results. Conclusions are drawn in Section 5.

2

TIME SERIES CLASSIFICATION WITH A BAG
OF FEATURES

A univariate time series, xn ¼ ðxn1 ; xn2 ; . . . ; xnT Þ is an ordered set of
T values. We assume time series are measured at equally spaced
time points. We consider univariate time series for simplicity,
although our method can be extended to multivariate time series
(MTS) in a straightforward manner. Each time series is associated
with a class label yn , for n ¼ 1; 2; . . . ; N and yn 2 f0; 1; 2; . . . ;
C  1g. Given a set of unlabeled time series, the task of time series
classification is to map each time series to one of the predefined
classes.
Note that we first standardize each time series to zero mean and
unit standard deviation. This adjusts for potentially different

VOL. 35, NO. 11,

NOVEMBER 2013

2797

baselines or scales that are not considered to be relevant (or
persistent) for a learner.

2.1

Subsequences and Feature Extraction

Time series classification approaches that are based on global
properties of a time series can potentially be improved with local
patterns that may define the class. Therefore, we represent each
time series with feature vectors derived from subsequences. Here,
a subsequence refers to values from the time series that are
contiguous in time. To capture patterns along the time series, each
subsequence s is represented by the features of smaller segments
called intervals.
We consider subsequences of fixed and random lengths. The
random length subsequences can potentially detect patterns that
appear with different lengths and be split across the time points
[43]. Thus, we generate subsequences of random length ls and
segment them using the same number of intervals to preserve the
same number of intervals d for each subsequence. This allows
splits in tree-based models to be based on the features from
different length intervals ws ¼ ls =d. Therefore, the relationships of
patterns with different lengths can be better captured.
We set a lower bound on the subsequence length lðminÞ as a
proportion z ð0 < z  1Þ of the length of the time series. Thus,
ls  lmin ¼ z  T . We also set a minimum interval length wmin so
that extracted features are meaningful (that is, we avoid a slope
computed from an interval with one point). Therefore, given z and
wmin the number of intervals used to represent a subsequence is
determined as d ¼ bwzT
c. There are r ¼ bwTmin c possible intervals in a
min
time series if the time series is represented using the minimum
interval length. For a subsequence with d intervals, r  d intervals
are not covered. Therefore, we generate r  d subsequences so that
for every interval the expected number of subsequences that cover
it is at least one.
Interval features fk ðt1 ; t2 Þ, ð0 < t1  t2  T Þ for k ¼ 1; 2; . . . ; K,
are extracted and combined to represent a subsequence. For each
interval, the slope of the fitted regression line, mean of the values,
and variance of the values are extracted. These features provide
information about the shape, level, and distribution of the values.
In addition, the mean and variance of all the values in the
subsequence, together with the start and end time points, are also
included in the feature vector. Start and end points introduce
potentially useful location information.
Subsequences of random lengths are motivated from scalespace theory, which is a framework for multiscale signal
representation [44]. Alternatively, subsequences are generated in
a fixed, uniform manner. In the uniform generation, the subsequence length is fixed as z  T and each subsequence is
represented by bwzT
c intervals. Starting from the first interval of
min
the time series, a stride of one interval is used to generate all
subsequences. Then the same number of subsequences as in the
random generation is obtained.

2.2

Codebook and Learning

After the local feature extraction, a new dataset is generated
where the features from each subsequence provide an instance,
and each time series forms the bag. The class label defined for
each instance is the class of the corresponding time series. A
classifier that generates a CPE for each instance is used to score
the strength of an assignment. Let pnc ðsÞ denote the CPE for
class c from subsequence s of series xn . The CPEs are discretized
into b bins and the distribution of pnc ðsÞ over the bag is summarized
with a histogram for each c (denoted by a vector hnc ). The vectors
are concatenated over c to form the codebook, hn , for time
series xn . Because the sum of CPEs for a subsequence is equal to
one, the features for one class can be dropped in the codebook. We
use equally spaced bins in our approach so that ðC  1Þ  b

2798

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 11,

NOVEMBER 2013

TABLE 1
Evaluated Parameter Settings for SVM Training on Codebook

Fig. 1. Time series classification with a bag-of-features (TSBF) algorithm.
Subsequences are sampled and partitioned into intervals for feature extraction.
Each subsequence is labeled with the class of the time series, and a learner
generates class probability estimates. Histograms of the class probability
estimates are generated (and concatenated) to summarize the subsequence
information. Global features are added. A final classifier is then trained on the new
representation to assign each time series.

elements are in the codebook. We aim to capture the details of the
similarity between subsequences with the histograms of CPEs. The
relative frequencies of the predicted classes over each series are
also concatenated in the codebook. Moreover, global features such
as autocorrelation are easily introduced (but not used here).
A codebook is an effective way to simplify the information in
the subsequences in terms of speed and discrimination [30], [45].
A random forest (RF) classifier [46] is used here to both generate
CPEs for codebooks and to classify time series (although another
learner that provides CPEs can be used in the framework). An RF
is an ensemble of decision trees (each constructed from a different
bootstrap sample). The instances not used in the construction of a
single tree are called out-of-bag (OOB) instances and they can
provide an adequate estimate of generalization error [46]. We
denote the RF applied to the subsequence dataset as RFsub.
Given the codebook and the global features, an RF (denoted as
RFts) is applied. RF is competitive with the widely used learners
on high-dimensional problems [47]. Fast evaluation is also
important and RF is fast for training and testing [46]. Moreover,
RF’s invariance to attribute scales (units) allows the global
features to be easily integrated with the CPE histograms.
Furthermore, it is inherently multiclass. Fig. 1 illustrates some
of the steps of our approach.

3

EXPERIMENTS AND RESULTS

TSBF is tested on a full set of time series data from [42]. The testbed
provides diverse characteristics such as the lengths of the series,
the number of classes, and so on (as shown in the first columns in
Table 2), which enables a comprehensive evaluation. For reprodu-

cibility of the results, we built a Web page [48] that contains the
code of TSBF and our detailed results.
TSBF does not require the setting of many parameters and it is
robust to the settings. RF is insensitive to both the number of trees
and the number of candidate attributes scored to potentially split a
node [46]. The number of features evaluated at each node of the
tree is set to the default here (approximate square root of the
number of features). For all RFs, the number of trees are set based
on the progress of OOB error rates at discrete number of tree levels
with a step size of 50 trees.
Subsequences are determined from two parameters. For each
dataset in the experiments, we select the z parameter from the set
f0:1; 0:25; 0:5; 0:75g to minimize the OOB error rate of RFts on the
training data. A similar procedure is used to generate uniform
subsequences. The same z levels as in the random case are
considered and parameters are again selected to minimize OOB
error on the training data. This idea is similar to the searching for
best warping window in NNDTWBest. Also, we set the minimum
interval length wmin as five time units to have meaningful features
(such as slopes). Again, this setting can be adjusted based on the
dataset characteristics (via OOB error) in favor of our algorithm,
but we did not modify it here. The number of bins b is set to 10 in
our experiments. This parameter is expected to have a small effect
on performance if it is set large enough because of the embedded
feature selection in RFs.
Our supervised BoF approach is compared to an unsupervised
BoW approach with a codebook derived from K-means clustering.
In the unsupervised approach, the euclidean distance between
subsequences is computed. The subsequences are generated for
each level of z as in TSBF with uniform subsequence extraction.
Then, K-means clustering with k selected from the set
f25; 50; 100; 250; 500; 1;000g is used to label subsequences for each
z setting. We use the histogram of the cluster assignments to
generate the codebook.
Two classifiers, RF and SVM, are trained on the codebook for
classification. For SVM, the z and k settings and the parameters of
the SVM (kernel type, cost parameter) are determined based on 10fold cross-validation (CV) on the training data. Evaluated parameter settings are provided in Table 1. For multiclass classification,
SVMs are trained using the one-against-one approach, in which
CðC1Þ
binary classifiers are trained. The class is assigned by a
2
majority voting approach. For RF, the z and k parameters are
determined from the OOB error rate on the training data. Our
detailed results (the selected settings for RF and SVM) are available
in [48].
We also compare to an RF classifier applied directly to the
times series values RF (Raw), and to an RF classifier applied to
features extracted from fixed-uniform intervals RF (Feature). For
the latter RF, an interval length of five units (the same as in our
wmin setting) is used, and the three features, mean, variance,
slope, are extracted from each interval. Codebooks are not
constructed for these classifiers.

3.1

Classification Accuracy

TSBF, with random and uniform subsequences, is compared to the
alternatives mentioned previously and to NNs classifiers with DTW
and SSSKs. Two versions of DTW are considered: NNDTWBest [3]

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35, NO. 11,

NOVEMBER 2013

2799

TABLE 2
Error Rates of TSBF (with Random and Uniform Features) over 10 Replicates, NN Classifiers with Dynamic Time Warping Distance (NNDTWBest
and NNDTWNoWin), an NN Classifier with Sparse Spatial Sample Kernels (NNSSSK), BoW Approach with RF and SVM, RF (Raw)
Applied Directly to Time Series Values, and RF (Feature) Applied Directly to Interval Features

searches for the best warping window, based on the training data,
then uses the learned window on the test data, while NNDTWNoWin has no warping window. Note that DTW is a strong solution for
time series problems in a variety of domains [7].
Table 2 summarizes the average error rates from 10 replications
of our algorithm on the test data. In some cases, different z settings
for TSBF generated the same OOB error rates on the training data.
In these cases, the worst error rate on the testing data is selected.
Features generated for the test data are computed from the same
locations generated for training. The results for NN classifiers were
obtained from [42]. Details for SSSK are provided in a later section.
Table 2 provides the significance levels for Wilcoxon matchedpairs signed-ranks tests for TSBF (with random subsequences) and
the number of wins/losses against the algorithm on the column.
We also use the same approach proposed by Ding et al. [49] to
compare results. Scatter plots are used to conduct pairwise
comparisons of error rates. Each axis represents a method and
each dot represents the error rate for a particular dataset. The line
x ¼ y is drawn to represent the region where both methods

perform about the same. A dot above the line indicates that
approach on the x-axis has better accuracy than the one on the yaxis for the corresponding dataset. Fig. 2 illustrates the performance of TSBF (average over 10 replicates) with random
subsequence generation versus NN alternatives. The performance
of TSBF is better for most of the datasets.

3.2

Computational Complexity

TSBF is implemented in both C and R Software and our
experiments use an Ubuntu 12.04 system with 8-GB RAM, dualcore CPU (i7-3620M 2.7 GHz). We use R only for building the RFs
and implemented the algorithms for subsequence and codebook
generation in C because R is computationally inefficient in
execution of the loops. Moreover, although the CPU can handle
four threads in parallel, only a single thread is used. A parallel
implementation of TSBF is also available from [48].
The overall computational complexity of our algorithm is
mainly due to RF sub. The time complexity of building a single
pﬃﬃﬃ
tree in RF sub is Oð   log Þ, where  ¼ K  d þ L is the number

2800

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35,

NO. 11,

NOVEMBER 2013

Fig. 2. Error rates for TSBF (random, average over 10 replications) versus
NN classifiers.

Fig. 4. Average error rates of TSBF (random, average over 10 replications) versus
BoW approach with RF and SVM.

of features extracted from each subsequence and  is the number
of training instances for RF sub (equals N  ðr  dÞ). The smaller
z is, the more subsequences are generated, but with fewer
features for each.
The training time of TSBF depends on the number of z levels
evaluated. We considered four levels of z in this study. We analyzed
the average training times over 10 replicates for the datasets in
Table 2. The median training time is 16.3 seconds per dataset with a
minimum of 1 second and a maximum of 1,949 seconds (for
ItalyPower and Thorax1, respectively). The test time to classify one
series (feature generation and classification through RFs) takes less
than a millisecond after the models are built. TSBF is very fast and
convenient for real-time classification of time series data.

Fig. 4 compares TSBF (random, average over 10 replications) to
BoW strategies based on K-means clustering of subsequences
generated in a uniform manner. TSBF performs better than BoW
approaches. A BoW representation has the potential to be affected
by the curse of dimensionality (specifically the distance calculation)
as the number of features extracted from subsequences
increases. This is illustrated on a sample problem by Baydogan
[50]. On the other hand, our supervised approach uses only the
relevant features from the supervised learning and generates the
CPEs (codebook) accordingly. Also, the performance difference
between SVM and RF on the BoW representation is not significant.

4
4.1

SENSITIVITY AND ADDITIONAL EXPERIMENTS
Benefits of Two-Stage Classification Approach

The performance of our two-stage classification approach is
compared to RF (raw) and RF (feature). Introducing interval
features improves the error rates when compared to training on
time series values. However, TSBF provides significantly better
results than both approaches, as illustrated in Fig. 3. This shows
the benefit of our two-stage approach. A detailed discussion was
also provided by Baydogan [50].

Fig. 3. Average error rates of TSBF (random, average over 10 replications) versus
RF (raw) applied directly to time series values and RF (feature) applied directly to
features extracted from uniform subsequences. Results show improved performance from the use of a codebook.

4.2

Effect of the Number of Subsequences

We choose six datasets to illustrate the properties of TSBF (random,
averaged over 10 replications) versus the number of subsequences
in the local feature extraction. These datasets provide reasonable
number of training and test time series. Test error rates for TSBF
versus the number of subsequences are shown in Fig. 5 for z ¼ 0:5.
For each dataset, we randomly generated  2 f0:5; 1; 1:5; 2; 2:5g
proportion of the default number of subsequences setting (i.e.,
r  d). Marginal improvements are observed as the number of
subsequences increases. Similar results are obtained for other
parameter settings (not shown). For all  settings, 500 trees are used
for RF sub but increasing the number of trees for a greater number
of subsequences may provide better results.

Fig. 5. Error rate of TSBF versus the number of subsequences as a multiplier  of
the default. Sensitivity to  is slight.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 35, NO. 11,

NOVEMBER 2013

2801

TABLE 3
Error Rates of Logical-Shapelets and TSBF on Eight Datasets

TSBF has better or comparable performance on the datasets except for Sony
AIBO Robot.

Fig. 6. Plot of average error rates of TSBF with random and uniform subsequence
generation.

4.3

Random and Uniform Subsequences

The average test error rates over 10 replications of TSBF with
random and uniform subsequence generation are illustrated in
Fig. 6. Random generation provides slightly better results, but the
difference is not significant. Some replicates of the random
sequences yield noticeably better performance as provided in our
detailed results [48]. Presumably the warping may be handled
better with a certain set of randomly generated subsequences.
On the other hand, uniform extraction provides more stable results
across the replications. Moreover, the number of trees trained for
RFsub and RFts is approximately the same for both approaches.

4.4

Sparse Spatial Sample Kernels and TSBF

Sparse spatial sample kernels embed time series into a highdimensional feature space and use a multiscale representation of
strings to improve sequence matching [13]. The series are first
discretized to generate a symbolic representation. Then, the
similarity between time series is computed over subsequences
that do not need to be contiguous. With k ¼ 1, similarity is
computed over the symbols. That is, contiguous symbols are not
concatenated to generate words (as in bag-of-words). We consider
double and triple kernels as proposed by Kuksa and Pavlovic [13].
There are a number of hyperparameters that have to be chosen
carefully, such as the kernel parameter d, alphabet size, discretization scheme (uniform binning, VQ, k-means, etc.), and related
parameters (e.g., number of bins b).
We discretize the time series using SAX [40]. We consider five
levels for the alphabet size and interval lengths of f4; 8; 12; 16; 20g.
The kernel parameter d is selected from the f5; 10; . . . ;
min ð50; representation length=2Þg. The representation length is
basically the ratio of time series length to interval length. The
evaluated settings are different for ItalyPower because the length is
only 24 for this particular dataset. To set the parameters, we
perform leave-one-out cross-validation on the training data. The
parameter combination providing the best CV error rate is used for
testing. The results are provided in Table 2. Fig. 2 illustrates that
TSBF provides substantially better results in these experiments.

4.5

Shapelets and TSBF

Time series subsequences which are likely to represent a class are
referred to as shapelets [10], and multiple shapelets were
extended with rules [12]. Shapelet methods are distance-based
methods, while ours is feature-based, but both use local patterns
related to the classes. Instead of rules, we use the summarized
version of this information in the form of CPEs and efficient
representation through the BoF. We compare TSBF to logical
shapelets [12] in Table 3. To be fair, the parameters of the logical
shapelet algorithm are set to search for all possible shapelets.

However, because of the computational requirements, we could
not achieve this for certain datasets.
Furthermore, we do not tune the parameters of TSBF for the
new datasets. We use the same settings as previously. The
algorithms are not compared in terms of computation time because
it depends to a large extent on parameter settings. An additional
two datasets (Cricket and Passgraphs) from [12] are also added for
comparison. TSBF has better or comparable performance on all
datasets except for Sony AIBO Robot.

4.6

Classification of Multivariate Time Series and Time
Series of Different Lengths

A multivariate time series is an M-variable time series. TSBF can be
modified in several ways for MTS classification. In the first
alternative, each univariate time series of MTS can be handled
separately and TSBF can be conducted M times to generate a
codebook for each of them. Then, the codebooks can be
concatenated to represent the MTS (late fusion). Another option
is to segment the subsequences from the univariate time series,
combine them in a single dataset, and train RFsub on this set (early
fusion). The codebook can be generated for each univariate time
series using the CPEs and then the codebooks can be concatenated
as in the first approach.
When the time series are of different lengths, the number of
subsequences needs to be modified based on the length of the
time series. More (less) subsequences should be segmented for
long (short) time series and the CPE histograms are generated
accordingly.

5

CONCLUSIONS

A framework is presented to learn a bag-of-features representation
for time series classification. Subsequences extracted from random
locations and of random lengths provides a method to handle
the time warping of patterns in a feature-based approach.
Furthermore, the partition into intervals allows one to detect
patterns represented by a series of measurements over shorter time
segments. The supervised codebook enables the integration of
additional information (such as subsequence locations) through a
fast, efficient learner that handles mixed data types, different units,
and so on. TSBF provides a comprehensive representation that
handles both global and local features. The flexible bag-of-features
representation allows for the use of any supervised learner for
classification. Our experimental results show that TSBF gives
better results than competitive methods on the benchmark datasets
from the UCR time series database [42]. Although our focus in this
study is on the classification of the time series, the bag-of-features
approach can be adjusted to other applications such as similarity
analysis, clustering, and so forth.

ACKNOWLEDGMENTS
This research was partially supported by US Office of Naval
Research grant no. N00014-09-1-0656.

2802

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

REFERENCES
[1]
[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]
[20]

[21]
[22]

[23]

[24]
[25]

[26]

[27]

[28]

[29]

Y.-S. Jeong, M.K. Jeong, and O.A. Omitaomu, “Weighted Dynamic Time
Warping for Time Series Classification,” Pattern Recognition, vol. 44, no. 9,
pp. 2231-2240, 2011.
E. Keogh and S. Kasetty, “On the Need for Time Series Data Mining
Benchmarks: A Survey and Empirical Demonstration,” Data Mining and
Knowledge Discovery, vol. 7, no. 4, pp. 349-371, 2003.
C. Ratanamahatana and E. Keogh, “Making Time-Series Classification
More Accurate Using Learned Constraints,” Proc. SIAM Int’l Conf. Data
Mining, pp. 11-22, 2004,
K. Ueno, X. Xi, E. Keogh, and D. Lee, “Anytime Classification Using the
Nearest Neighbor Algorithm with Applications to Stream Mining,” Proc.
IEEE Int’l Conf. Data Mining, pp. 623-632, 2007.
Z. Xing, J. Pei, and P.S. Yu, “Early Prediction on Time Series: A Nearest
Neighbor Approach,” Proc. Int’l Joint Conf. Artificial Intelligence, pp. 12971302, 2009.
H. Sakoe, “Dynamic Programming Algorithm Optimization for Spoken
Word Recognition,” IEEE Trans. Acoustics, Speech, and Signal Processing,
vol. 26, no. 1, pp. 43-49, Feb. 1978.
C. Ratanamahatana and E. Keogh, “Three Myths about Dynamic Time
Warping Data Mining,” Proc. SIAM Int’l Conf. Data Mining, vol. 21, pp. 506510, 2005.
J. Lin and Y. Li, “Finding Structural Similarity in Time Series Data Using
Bag-of-Patterns Representation,” Proc. Int’l Conf. Scientific and Statistical
Database Management, pp. 461-477, 2009.
Y. Yamada, H. Yokoi, and K. Takabayashi, “Decision-Tree Induction from
Time-Series Data Based on Standard-Example Split Test,” Proc. Int’l Conf.
Machine Learning, pp. 840-847, 2003.
L. Ye and E. Keogh, “Time Series Shapelets: A New Primitive for Data
Mining,” Proc. 15th ACM SIGKDD Int’l Conf. Knowledge Discovery and Data
Mining, pp. 947-956, 2009.
L. Ye and E. Keogh, “Time Series Shapelets: A Novel Technique That
Allows Accurate, Interpretable and Fast Classification,” Data Mining and
Knowledge Discovery, vol. 22, pp. 149-182, 2011.
A. Mueen, E.J. Keogh, and N. Young, “Logical-Shapelets: An Expressive
Primitive for Time Series Classification,” Proc. 17th ACM SIGKDD Int’l
Conf. Knowledge Discovery and Data Mining, pp. 1154-1162, 2011.
P. Kuksa and V. Pavlovic, “Spatial Representation for Efficient Sequence
Classification,” Proc. 20th Int’l Conf. Pattern Recognition, pp. 3320-3323, Aug.
2010.
M. Hearst, S. Dumais, E. Osman, J. Platt, and B. Scholkopf, “Support Vector
Machines,” IEEE Intelligent Systems and Their Applications, vol. 13, no. 4,
pp. 18-28, 1998.
P. Geurts, “Pattern Extraction for Time Series Classification,” Proc. Fifth
European Conf. Principles of Data Mining and Knowledge Discovery, vol. 2168,
pp. 115-127, 2001.
D. Eads, K. Glocer, S. Perkins, and J. Theiler, “Grammar-Guided Feature
Extraction for Time Series Classification,” Proc. Conf. Neural Information
Processing Systems, 2005.
A. Nanopoulos, R. Alcock, and Y. Manolopoulos, “Feature-Based Classification of Time-Series Data,” Int’l J. Computer Research, vol. 10, pp. 49-61,
2001.
J. Rodrı́guez, C. Alonso, and J. Maestro, “Support Vector Machines of
Interval-Based Features for Time Series Classification,” Knowledge-Based
Systems, vol. 18, nos. 4/5, pp. 171-178, 2005.
J. Rodrı́guez, C. Alonso, and H. Boström, “Boosting Interval Based
Literals,” Intelligent Data Analysis, vol. 5, no. 3, pp. 245-262, 2001.
J.J. Rodrı́guez and C.J. Alonso, “Interval and Dynamic Time WarpingBased Decision Trees,” Proc. ACM Symp. Applied Computing, pp. 548-552,
2004.
R. Rahmani and S.A. Goldman, “MISSL: Multiple-Instance Semi-Supervised Learning,” Proc. Int’l Conf. Machine Learning, pp. 705-712, 2006.
C. Zhang, X. Chen, M. Chen, S.-C. Chen, and M.-L. Shyu, “A Multiple
Instance Learning Approach for Content Based Image Retrieval Using OneClass Support Vector Machine,” Proc. IEEE Int’l Conf. Multimedia and Expo,
pp. 1142-1145, 2005.
Q. Zhang, S.A. Goldman, W. Yu, and J.E. Fritts, “Content-Based Image
Retrieval Using Multiple-Instance Learning,” Proc. Int’l Conf. Machine
Learning, pp. 682-689, 2002.
O. Maron and A.L. Ratan, “Multiple-Instance Learning for Natural Scene
Classification,” Proc. Int’l Conf. Machine Learning), pp. 341-349, 1998.
B. Babenko, M.-H. Yang, and S. Belongie, “Robust Object Tracking with
Online Multiple Instance Learning,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 33, no. 8, pp. 1619-1632, Aug. 2011.
P. Dollár, B. Babenko, S. Belongie, P. Perona, and Z. Tu, “Multiple
Component Learning for Object Detection,” Proc. European Conf. Computer
Vision, pp. 211-224, 2008.
R. Fergus, P. Perona, and A. Zisserman, “Object Class Recognition by
Unsupervised Scale-Invariant Learning,” Proc. IEEE Conf. Computer Vision
and Pattern Recognition, vol. 2, pp. 264-271, 2003.
A. Mohan, C. Papageorgiou, and T. Poggio, “Example-Based Object
Detection in Images by Components,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 23, no. 4, pp. 349-361, Apr. 2001.
V.C. Raykar, B. Krishnapuram, J. Bi, M. Dundar, and R.B. Rao, “Bayesian
Multiple Instance Learning: Automatic Feature Selection and Inductive
Transfer,” Proc. Int’l Conf. Machine Learning, pp. 808-815, 2008.

[30]

[31]

[32]

[33]

[34]

[35]

[36]
[37]

[38]

[39]

[40]

[41]

[42]

[43]
[44]
[45]

[46]
[47]

[48]

[49]

[50]

VOL. 35,

NO. 11,

NOVEMBER 2013

E. Nowak, F. Jurie, and B. Triggs, “Sampling Strategies for Bag-of-Features
Image Classification,” Proc. European Conf. Computer Vision, pp. 490-503,
2006.
D. Lewis, “Naive (Bayes) at Forty: The Independence Assumption in
Information Retrieval,” Proc. European Conf. Machine Learning, pp. 4-15,
1998.
Y. Chen, J. Bi, and J.Z. Wang, “Miles: Multiple-Instance Learning via
Embedded Instance Selection,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 28, pp. 1931-1947, Dec. 2006.
T.G. Dietterich, R.H. Lathrop, and T. Lozano-Perez, “Solving the Multiple
Instance Problem with Axis-Parallel Rectangles,” Artificial Intelligence,
vol. 89, nos. 1/2, pp. 31-71, 1997.
F. Briggs, R. Raich, and X.Z. Fern, “Audio Classification of Bird Species: A
Statistical Manifold Approach,” Proc. IEEE CS Int’l Conf. Data Mining,
pp. 51-60, 2009.
Z. Fu, G. Lu, K.M. Ting, and D. Zhang, “Music Classification via the Bag-ofFeatures Approach,” Pattern Recognition Letters, vol. 32, no. 14, pp. 17681777, 2011.
C. Harris and M. Stephens, “A Combined Corner and Edge Detector,” Proc.
Fourth Alvey Vision Conf., pp. 147-151, 1988.
J.-J. Aucouturier, B. Defreville, and F. Pachet, “The Bag-of-Frames
Approach to Audio Pattern Recognition: A Sufficient Model for Urban
Soundscapes but Not for Polyphonic Music,” The J. Acoustical Soc. of Am.,
vol. 122, no. 2, pp. 881-891, 2007.
R.F. Lyon, M. Rehn, S. Bengio, T.C. Walters, and G. Chechik, “Sound
Retrieval and Ranking Using Sparse Auditory Representations,” Neural
Computation, vol. 22, pp. 2390-2416, 2010.
D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet, “Semantic
Annotation and Retrieval of Music and Sound Effects,” IEEE Trans. Audio,
Speech, and Language Processing, vol. 16, no. 2, pp. 467-476, Feb. 2008.
J. Lin, E. Keogh, L. Wei, and S. Lonardi, “Experiencing SAX: A Novel
Symbolic Representation of Time Series,” Data Mining and Knowledge
Discovery, vol. 15, pp. 107-144, 2007.
M. Casey, C. Rhodes, and M. Slaney, “Analysis of Minimum Distances in
High-Dimensional Musical Spaces,” IEEE Trans. Audio, Speech, and Language
Processing, vol. 16, no. 5, pp. 1015-1028, July 2008.
E. Keogh, Q. Zhu, B. Hu, H. Y., X. Xi, L. Wei, and C.A. Ratanamahatana,
“The UCR Time Series Classification/Clustering Homepage,” http://
www.cs.ucr.edu/eamonn/time_series_data/, 2011.
T.-c. Fu, “A Review on Time Series Data Mining,” Eng. Applications of
Artificial Intelligence, vol. 24, pp. 164-181, 2011.
T. Lindeberg, “Scale-Space Theory: A Basic Tool for Analyzing Structures at
Different Scales,” J. Applied Statistics, vol. 21, nos. 1/2, pp. 225-270, 1994.
F. Moosmann, E. Nowak, and F. Jurie, “Randomized Clustering Forests for
Image Classification,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 30, no. 9, pp. 1632-1646, Sept. 2008.
L. Breiman, “Random Forests,” Machine Learning, vol. 45, no. 1, pp. 5-32,
2001.
R. Caruana, N. Karampatziakis, and A. Yessenalina, “An Empirical
Evaluation of Supervised Learning in High Dimensions,” Proc. Int’l Conf.
Machine Learning, pp. 96-103, 2008.
M.G. Baydogan, “A Bag-of-Features Framework to Classify Time Series
Homepage,” www.mustafabaydogan.com/a-bag-of-features-frameworkto-classify-time-series-tsbf.html, 2012.
H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, and E. Keogh, “Querying
and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures,” Proc. VLDB Endowment, vol. 1, pp. 1542-1552,
Aug. 2008.
M.G. Baydogan, “Modeling Time Series Data for Supervised Learning,”
PhD thesis, Arizona State Univ., Dec. 2012.

. For more information on this or any other computing topic, please visit our
Digital Library at www.computer.org/publications/dlib.

Decision Support Systems 59 (2014) 163–170

Contents lists available at ScienceDirect

Decision Support Systems
journal homepage: www.elsevier.com/locate/dss

CBC: An associative classiﬁer with a small number of rules☆
Houtao Deng a,⁎, George Runger b, Eugene Tuv c, Wade Bannister d
a

Intuit, Mountain View, CA, USA
Arizona State University, Tempe, AZ, USA
c
Intel, Chandler, AZ, USA
d
Ingenix Consulting, Irvine, CA, USA
b

a r t i c l e

i n f o

Article history:
Received 4 January 2013
Received in revised form 25 September 2013
Accepted 17 November 2013
Available online 4 December 2013
Keywords:
Association rule
Decision tree
Feature selection
Rule-based classiﬁer
Rule pruning

a b s t r a c t
Associative classiﬁers have been proposed to achieve an accurate model with each individual rule being interpretable. However, existing associative classiﬁers often consist of a large number of rules and, thus, can be difﬁcult to
interpret. We show that associative classiﬁers consisting of an ordered rule set can be represented as a tree
model. From this view, it is clear that these classiﬁers are restricted in that at least one child node of a non-leaf
node is never split. We propose a new tree model, i.e., condition-based tree (CBT), to relax the restriction. Furthermore, we also propose an algorithm to transform a CBT to an ordered rule set with concise rule conditions. This
ordered rule set is referred to as a condition-based classiﬁer (CBC). Thus, the interpretability of an associative classiﬁer is maintained, but more expressive models are possible. The rule transformation algorithm can be also applied
to regular binary decision trees to extract an ordered set of rules with simple rule conditions. Feature selection is
applied to a binary representation of conditions to simplify/improve the models further. Experimental studies
show that CBC has competitive accuracy performance, and has a signiﬁcantly smaller number of rules (median of
10 rules per data set) than well-known associative classiﬁers such as CBA (median of 47) and GARC (median of
21). CBC with feature selection has even a smaller number of rules.
© 2013 Elsevier B.V. All rights reserved.

1. Introduction
The comprehensibility of a classiﬁer is vital in decision support
systems [18,12]. An interpretable classiﬁer can provide insights about
why objects are classiﬁed into a speciﬁc class. For example, Ref. [27]
employed classiﬁcation rules to understand the patterns causing
patients' hospitalization. By using the explicit knowledge from a comprehensible classiﬁer, decision makers can take actions to improve the
existing system [21]. Furthermore, domain experts and decision makers
are also able to validate if a comprehensible classiﬁer is applicable in
practice. This is particularly useful when the data are messy, the amount
of data is not large enough, or the decision makers are not comfortable
in completely relying on data-driven models.
Decision tree classiﬁers such as C4.5 [20] and CART [4] are easy to
understand, and have been popularly used. However, decision trees
usually consider only one variable at each step/node, and tend to be
greedy and have difﬁculty in capturing strong variable interactions,
such as an exclusive OR (XOR) relationship where an individual attribute
is not predictive, but the combination can be effective.

☆ This research was partially supported by ONR grant N00014-09-1-0656.
⁎ Corresponding author.
E-mail addresses: hdeng3@asu.edu (H. Deng), george.runger@asu.edu (G. Runger),
eugene.tuv@intel.com (E. Tuv), wade.bannister@ingenixconsulting.com (W. Bannister).
0167-9236/$ – see front matter © 2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.dss.2013.11.004

An associative classiﬁer [17] is another type of comprehensible and
popularly used classiﬁer. The associative classiﬁcation rules are generated
by association rule algorithms, such as the Apriori algorithm [1], but
with the right hand side of the rule being ﬁxed as the class attribute.
An associative classiﬁcation rule is just a IF-THEN rule. For example,
{(outlook = sunny ∧ wind = false) class = play tennis} implies that
the target class is play tennis if the attributes outlook is sunny and wind
is false. Because associative classiﬁcation rule-generating algorithms
usually consider multiple variables simultaneously, a classiﬁer formed
by these rules can be less greedy than decision trees. Indeed, previous
work has successfully shown the competitive accuracy performance of
associative classiﬁers, such as CMAR [16], CPAR [28], HARMONY [24],
LAC [23] and GARC [5]. Associative classiﬁcation has been popularly
applied in practice, such as detecting products missing from the shelf
[19], maximizing customer satisfaction [14] and predicting patients'
hospitalization [27].
However, in the associative classiﬁcation rule generating process,
a large number of rules can be generated, and, thus, there can be many redundant or irrelevant rules. Pessimistic error estimation and x2 testing
were used for pruning rules, but these methods can still leave a large
number of rules unpruned [17,16]. Berrado and Runger [2] used metarules to organize and summarize rules. Zaïane and Antonie [29] proposed
a visual and interactive user application for pruning rules, which can be
useful when domain knowledge is present. Wang et al. [25] pruned
rules using a decision tree structure, and Chen et al. [5] pruned rules

164

H. Deng et al. / Decision Support Systems 59 (2014) 163–170

using information gain and certain conﬂict/redundancy resolution strategies. These methods reduced the number of rules in classiﬁcation to some
degree. An associative classiﬁer with a small number of rules is often
favorable over another with a larger number of rules with similar accuracy performance [5].
We show that associative classiﬁers consisting of an ordered rule set
can be represented as a tree model. This tree tests a rule condition at a
node, instead of testing a single variable (ordinary trees). However, in
the tree at least one child node of a non-leaf node is never split further.
This may limit the expressiveness of the tree. Motivated by the limitation,
we propose a condition-based tree (CBT) that relaxes the restriction
on splits. Furthermore, we show that CBT can be equivalently transformed to an ordered rule set with simpliﬁed conditions, referred to
as a condition-based classiﬁer (CBC). Thus, the interpretability of an
associative classiﬁer is maintained, but more expressive models are
possible. We also provide procedures of constructing CBT and CBC.
Experimental studies show that CBC has signiﬁcantly fewer rules than
existing associative classiﬁers CBA and GARC, with similar accuracy performance. Also, feature selection is applied to simplify/improve the
models further.
The remainder of this paper is organized as follows. Section 2
presents ordinary decision trees and associative classiﬁers. Section 3 introduces CBT and CBC. Section 4 discusses the experimental results and
Section 5 draws the conclusions.
2. Decision trees and associative classiﬁers
2.1. Deﬁnitions
Let X = (X1,…XT) be the predictor variables, Y be the class variable,
and Yk (k = 0,…,K − 1) be the class labels. Let D = {(xi,yi)|i = 1…N}
denote a training data set, where xi and yi are realizations of tX and Y. A
classiﬁcation rule rj describing that D can be expressed as {cj ⇒ Y = Yk},
and cj referred to as the condition of rj, is a conjunction of attribute–
value pairs, e.g., (X1 = small ∧ X2 = green). The support of the rule
{cj ⇒ Y = Yk} is the proportion of rows in D that satisfy both cj and
Y = Yk. The support of the condition cj is the proportion of rows in D
that satisfy cj. The conﬁdence of the rule is the ratio between the rule support and the rule condition support.

Fig. 1. A regular binary decision tree.

be arranged in any order. Fig. 1 illustrates a decision tree and the rules
for the decision tree are as follows
If
If
If
If

X 1 ¼ small∧X 2 ¼ red
X 1 ¼ small∧X 2 ≠red
X 1 ≠small∧X 3 ¼ square
X 1 ≠small∧X 3 ≠square

⇒Y
⇒Y
⇒Y
⇒Y

¼ class
¼ class
¼ class
¼ class

0;
1;
0;
1:

2.3. Associative classiﬁer
In associative classiﬁcation, classiﬁcation rules satisfying a certain
support and conﬁdence value are generated by an algorithm such as
Apriori [1]. Associative classiﬁers such as CBA [17] are formed by ordering
the rules according to some criteria such as the rule conﬁdence and support. For example,
c1
c2
c3
Else

⇒Y
⇒Y
⇒Y
⇒Y

¼ class
¼ class
¼ class
¼ class

0;
1;
0;
1:

where cj is a rule condition (a conjunction of attribute–value pairs). If an
instance satisﬁes the conditions of multiple rules, the ﬁrst rule is used.
Therefore, the order of the rules affects the classiﬁcation results. The ordered rules can be presented in a form of tree shown in Fig. 2. It can be
seen that the left child nodes are not split further.

2.2. Classiﬁcation rules from decision trees
3. Condition-based tree and classiﬁer
We consider decision trees that split a non-leaf node into M (M ≥ 2)
child nodes according to attribute X⁎ by maximizing the information gain
of Y, where M = 2 if X⁎ is continuous, and M = |X∗| if X⁎ is categorical.
Let p(yk|Pv) denote the proportion of instances belonging to class yk at
node v. The entropy of Y at a node v can be expressed in terms of prior
probabilities as

Decision trees do not necessarily produce rules with the maximum
conﬁdence as they only seek locally optimal solutions, while associative
classiﬁers can include all the rules above a minimum conﬁdence.

X
pðyk jP v Þlog2 pðyk jP v Þ
H ðYjP v Þ ¼ −
k

where Pv is the conjunction of the attribute–value pairs along the path
from the root node to v, e.g., Pv = {X1 = large ∧ X2 = green ∧ X5 N 10}.
If v is a non-leaf node, the split variable X⁎ is selected by maximizing the
information gain
X





Gain YjP v ; X ¼ H ðYjP v Þ− wm H YjP v ; X ¼ am
m

where wm is the ratio of the number of instances at child node vm to
the total number of instances at v, and the ams denote the values of the
categorical attribute X⁎ (with an appropriate revision for a numerical
attribute). If v is a leaf node, it is assigned with the class Yk that is
most frequent in the node, and a classiﬁcation rule can be formed as
Pv ⇒ Yk. The rules from a decision tree are mutually exclusive and can

Fig. 2. A tree presentation of a set of ordered associative classiﬁcation rules.

H. Deng et al. / Decision Support Systems 59 (2014) 163–170

Decision trees select one variable at a time and, therefore, are challenged by some patterns. For example, a challenge occurs when all attributes are needed for a rule (e.g., the XOR logic). However, associative
classiﬁcation considers multiple attributes simultaneously and, therefore, can ﬁnd the combination of several attributes useful. However, as
seen in Fig. 2, for associative classiﬁers consisting of an ordered
rule set (e.g. CBA), the child nodes satisfying the splitting criterion are
always leaf nodes. This may limit the expressiveness of the tree.
Motivated by the restriction, we propose a new tree model, referred
to as a condition-based tree (CBT), illustrated in Fig. 3. In a non-leaf node
at a CBT, data are split by testing if a condition is satisﬁed, where a condition is a conjunction of attribute–value pairs. A CBT relaxes the restriction
of CBA that at least one of the child nodes is never split, and, thus, can be
more expressive. Also, a CBT relaxes the restriction of ordinary decision
trees that the condition tested at each node is limited to one attribute.
Furthermore, in the following sections we show that the CBT can be
transformed to a set of ordered rules, referred to as the condition-based
classiﬁer (CBC). The rules in this set consist of conjunctive terms similar to
an associative classiﬁer, but the new rules can contain more conjuncts
than the original rules, and the class is determined from the tree. Thus,
more expressive models are possible.
Here we introduce one way to construct a CBT and the corresponding
CBC. Consider training data set D and a set of associative classiﬁcation
rules generated from D: {r1,r2,…,rJ}.
The procedure of constructing CBT and CBC is shown in Fig. 4. Each
component in the procedure is introduced in the following subsections.
3.1. Data transform
Let {c1,c2,…,cJ} denote the condition set of a rule set {r1,r2,…,rJ}. Let Iij
indicate whether a condition cj ∈ {c1,c2,…,cJ} is satisﬁed for xi of a training
instance (xi,yi). That is,

Iij ¼

1 c j is satisfied for xi
:
0
otherwise

ð1Þ

A new data set I is formed by integrating the indicators and the class
labels: {[Ii1,…,IiJ,yi],i = 1,…N}.
Let [I1,I2,…,IJ] denote the predictor variables.
The class variable in I is still Y. Similar data transform techniques
were used for a set of rules [2] (where the focus was to organize/
prune rules graphically), and a set of patterns [6]. This data step transforms a rule analysis problem to a supervised learning problem, in
which Ij (j = 1,…,J) are predictor variables representing conditions cj,
and Y is the class variable.
As an illustrative example, an exclusive OR (XOR) data set with 160
rows is simulated, and summarized in Table 1. There are only four
distinct rows among the 160 rows, we duplicated the rows because an
algorithm such as the tree model C4.5 [20] has parameters such as the
minimum size for a node. A set of associative classiﬁcation rules were
extracted from the data set (Table 2).

165

Fig. 4. The procedure of constructing a condition-based tree and the corresponding
condition-based classiﬁer. After transforming the original data and the classiﬁcation
rules to an indicator data set, feature selection methods can be used for dimensional reduction, and decision tree C4.5 is used to construct a CBT. Then a CBT can be transformed
to a CBC, with equivalent classiﬁcation outcomes. Note the dimensional reduction step is
not required, but it can reduce the number of rules in a CBT and the corresponding CBC.

The new data set I, shown in Table 3, was formed based on the data
in Table 1 and the conditions in Table 2.
3.2. Dimensional reduction via feature selection
In the new indicator data set I, an indicator variable Ij corresponds to
the rule condition cj. Therefore, the conditions can be reduced by selecting
a feature subset from I. We can apply feature selection (FS) algorithms on
I to obtain a reduced subset of conditions and the resultant indicator data
set is denoted as I⁎. Since feature selection methods are generally developed for high-dimensional data, the condition pruning can be as efﬁcient
as the feature selection methods.
The FS algorithm ACE [22] was shown to be generally effective for
high-dimensional data and has the advantage that it uses tree-based
ensembles. However, a more widely-used method is applied here to
focus on the advantages of the CBC with a simpler feature selection
method. Consequently, CFS [10] is applied here.
For the XOR example, CFS applied to I selects {c6,c7,c13,c14}. The
selected conditions correspond to the four XOR logical rules. We also
note that the dimensional reduction step is not required, but this can
reduce the number of rules in the ﬁnal classiﬁer, as shown in our experiments discussed later.
3.3. Condition-based tree
A CBT is built by applying an ordinary decision tree (C4.5 for our
work) to I or I⁎. The decision rule at each node follows: if Ij = 1 (if condition cj is satisﬁed), then go to one branch; else, go to another branch.
The class is assigned at the leaf nodes.
The condition-based tree for the XOR example is shown in Table 4.
The classiﬁer produces three leaf nodes and correctly captures the true
patterns.
3.4. Transform CBT to CBC
A classiﬁcation rule can be extracted from a CBT via the conjunction
of all the condition–value pairs along the path from the root node to a
leaf node v. That is, c1 = value1 ∧ c2 = value2 ∧ … ⇒ Y = Yv, where
Table 1
Example data set with 40 rows generated for each XOR logic state.

Fig. 3. A condition-based tree where cj is a conjunction of attribute–value pairs,
e.g., X1 = small ∧ X2 = red.

Data row ID

X1

X2

Y

1–40
41–80
81–120
121–160

0
0
1
1

0
1
0
1

0
1
1
0

166

H. Deng et al. / Decision Support Systems 59 (2014) 163–170
Table 2
Associative classiﬁcation rules generated from the XOR data.

Table 4
The condition-based tree (CBT) for the XOR example.

Rule ID

Conditions

Class

c7 = 1 ⇒ Y = 0

r1
r2
r3
r4
r5
r6
r7
r8
r9
r10
r11
r12
r13
r14

c1: {}
c2: X1 = 0
c3: X2 = 0
c4: X2 = 1
c5: X1 = 1
c6: X1 = 0 ∧ X2 = 0
c7: X1 = 1 ∧ X2 = 1
c8: {}
c9: X1 = 0
c10: X2 = 0
c11: X2 = 1
c12: X1 = 1
c13: X1 = 0 ∧ X2 = 1
c14: X1 = 1 ∧ X2 = 0

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

c7 = 0
c6 = 1 ⇒ Y = 0
c6 = 0 ⇒ Y = 1

=
=
=
=
=
=
=
=
=
=
=
=
=
=

0
0
0
0
0
0
0
1
1
1
1
1
1
1

cg is a condition in the path, valueg ∈ {True,False}, and Y = Yv is the class
assigned at node v.
Each condition–value pair in a rule can be decomposed into a conjunction of attribute–value pairs, but the conditions with value True can be
easier to understand than conditions with value False. For example,
suppose there are three variables X 1 ∈ {small,medium,large},
X2 ∈ {red,green,blue} and X3 ∈ {square,circle}, and two conditions
{c1 : X1 = small ∧ X2 = green} and {c2 : X3 = square}. Then c1 = True
is equivalent to X1 = small ∧ X2 = green. In contrast, c1 = False is
equivalent to X1 ≠ small ∨ X2 ≠ green, and, therefore, equivalent
to X1 = medium ∨ X1 = large ∨ X2 = green ∨ X2 = blue, which becomes even more complex when X1 or X2 has more categories. Similarly,
all conditions with value True can be integrated into a concise conjunction of attribute–value pairs, e.g., c1 = True ∧ c2 = True is equivalent
to X1 = small ∧ X2 = green ∧ X3 = square. However, conditions having
value False are more complex and more difﬁcult to execute after
transforming to attribute–value pairs. Consequently, we prefer to avoid
using the condition–value pairs with value False in a rule. In the following
we transform CBT to a set of ordered rules with only condition–value
pairs with the value True.
At a node, an instance is sent to a child node based on a splitting rule:
“instances are sent to the left child node if the condition is true at the
node”, and “to the right child node if the condition is false”. Alternatively,
we can present the splitting rules as: “instances are sent to the left child
node if the condition is true at the node”, and “Else, sent to the right
child node”. Clearly the later approach avoids using the condition–value
pair with the value equal to False, but these two splitting rules are ordered. Starting from the root node, we can recursively use this principle
to order the rules and avoid using the condition–value pairs with value
False.
We assume that a CBT node stores the following information:
ﬂag_leaf indicates whether the current node is a leaf node, Cnode is the
condition at the current node, classnode is the class assigned to a nonleaf node, and node.childleft and node.childright, respectively, are the left
and right child nodes of the current node. Without loss of generality,
we assume that the data in the left child node of a non-leaf node always
satisﬁes the condition. Algorithm 3.4 describes a way to transform CBT
to an ordered rule set, referred to as CBC. The CBC for the XOR example
is shown in Table 5. The CBC correctly and concisely learns the model.

Table 3
New data set formed from the raw data in Table 1 and the conditions in
Table 2.
Data row ID

I1

I2…I14

Y

1–40
41–80
81–120
121–160

1
1
1
1

1…0
1…0
0…1
0…0

0
1
1
0

4. Experiments
We used the statistical programming language R [13] and R packages
“RWeka” [11] and “arules” [9] to implement the algorithms considered in
our experiments. We evaluated our methods on data sets from the UCI
Machine Learning Repository [3]. These data sets have been commonly
used for evaluating associative classiﬁers [17,5] and are summarized in
Table 6.
To generate associative classiﬁcation rules, we ﬁrst discretized the
continuous attributes using the entropy method [8] and then extracted
the associative
Algorithm 1.

classiﬁcation rules with minimum support = 0.05 and minimum
conﬁdence = 0.6. The sensitivity to the minimum support and conﬁdence was also investigated.
In the experiments, we considered CBC-FS (CBC with feature selection), CBC (CBC without feature selection), a decision tree algorithm
C4.5 and two associative classiﬁers CBA and GARC. Here 10-fold cross
validation was used to evaluate the algorithms and the results of CBA
and GARC were from the original work [17,5]. To compare CBC-FS to
competitors, the Wilcoxon signed-rank tests were conducted [26].
4.1. Classiﬁcation performance
The number of rules of different classiﬁers is shown in Table 7. Clearly
CBC has fewer rules than C4.5, GARC and CBA, and therefore CBC effectively reduced the number of rules used in classiﬁcation. CBC-FS has
even fewer rules than CBC. Consequently, feature selection successfully
further reduced the complexity of the classiﬁer here. The Wilcoxon
signed rank tests indicate that CBC-FS has signiﬁcantly fewer rules than
the other methods (at signiﬁcance level of 0.05).
The error rates (%) of different classiﬁers are also shown in Table 7.
According to the Wilcoxon signed ranks tests, CBC-FS is not signiﬁcantly
different from CBC, C4.5 and CBA, but is signiﬁcantly better than GARC at
signiﬁcance level 0.05. Consequently, feature selection can substantially
reduce the number of rules of CBC without signiﬁcant loss of accuracy
Table 5
The condition-based classiﬁer (CBC) extracted from the CBT shown in
Table 4.
X1 = 1 ∧ X2 = 1

⇒Y=0

X1 = 0 ∧ X2 = 0
Else

⇒Y=0
⇒Y=1

H. Deng et al. / Decision Support Systems 59 (2014) 163–170

167

Table 6
A summary of the data sets used in the experiments.
Data summary
Data set name

# features

# instances

# classes

Australian
Auto
Breast
Crx
German
Glass
Heart
Hepatitis
Horse
Iris
Labor
Led7
Lymph
Pima
Tic-tac-toe
Vehicle
Waveform
Wine
Zoo

14
25
10
15
20
9
13
19
22
4
16
7
18
8
9
18
21
13
16

690
205
699
690
1000
214
270
155
368
150
57
3200
148
768
958
846
5000
178
101

2
7
2
2
2
7
2
2
2
3
2
10
4
2
2
4
3
3
7

performance. This means feature selection is capable of pruning irrelevant or redundant rules.
We mentioned one key advantage of associative classiﬁers over
ordinary decision trees is that associative classiﬁcation rules are generated by considering multiple variables simultaneously, while decision
trees consider one variable at a time. The tic-tac-toe data set is a perfect
example illustrating this point. The tic-tac-toe data set encodes the
complete set of possible board conﬁgurations at the end of tic-tac-toe
games. Let X1,…,X9 denote the 9 positions (3 by 3) on a board, and
“xtitx” is assumed to have played ﬁrst. Fig. 5 illustrates the tic-tac-toe
game board. The target variable Y is “win” for 8 possible ways in
which there are three “x”s in a row, and is “lose” otherwise. Therefore,
to capture the patterns of winning a game, a classiﬁer must consider
multiple variable simultaneously. C4.5 has more than 100 leaf nodes
and still cannot learn all the correct patterns. All the associative classiﬁers are able to capture the right patterns and have zero error rate.
CBC-FS, CBC and CBA have 8 rules, and GARC has 26 rules. The CBC for
the tic-tac-toe data set is shown in Table 8.

Fig. 5. The tic-tac-toe game board. X1,…,X9 represent the 9 positions. To win a game, a
player must place three chess pieces in a row (e.g., X1, X5 and X9) before the competitor.

Also consider the labor data set. The data set includes all collective
agreements reached in the business and personal services sector for
local unions with at least 500 members (teachers, nurses, university
staff, police, etc.) in Canada in 1987 and the ﬁrst quarter of 1988 [3].
There are 16 attributes including the information about wage increases,
employer's contributions to the pension plan, health plan and dental
plan, the number of working hours, education allowance, employer's
help during employee long-term disability, etc. The class is either good
or bad, which standards for an acceptable or unacceptable contract, respectively. The CBC for this data set is shown in Table 9. With a compact
rule set, CBC is able to achieve lower error rate than other classiﬁers
with more rules.
4.2. Titanic passenger survival data
Here we use CBC to learn what people were likely to survive in the
wreck of the Titanic, based on a data set available at Kaggle [15]. The
data set consists of 891 passengers and the class is whether a passenger
survived (1: survive, 0: non-survive). Particularly, 342 passengers
survived and 549 did not. One of the reasons that the shipwreck led to
such loss of life was that there were not enough lifeboats [15]. We considered 7 predictor variables: sex, age, the number of siblings/spouses
aboard (sibsp), the number of parents/children aboard (parch), passenger

Table 7
The number of rules used in CBC-FS (CBC with feature selection), CBC (without feature selection), C4.5, GARC and CBA, and the error rates (%) of these classiﬁers. The mean and median of
the number of rules and error rates from each classiﬁer, and the p-values of the Wilcoxon signed rank tests between CBC-FS and other classiﬁers are calculated.
Number of rules
Dataset
Australian
Auto
Breast
Crx
German
Glass
Heart
Hepatitis
Horse
Iris
Labor
Led7
Lymph
Pima
Tic-tac-toe
Waveform
Wine
Zoo
Mean
Median
p-Value

CBC-FS
6
22
8
12
16
14
7
5
4
2
4
27
6
6
8
234
3
7
21.7
6.9
–

Error rates
CBC
13
23
12
27
103
18
11
6
9
3
4
28
8
7
8
426
3
6
39.8
10.2
0.001

C4.5
10
108
16
23
110
30
13
5
5
4
4
40
10
12
115
692
12
8
67.7
12.7
0.000

GARC
17
650
21
21
78
17
12
23
26
7
15
33
17
6
26
25
16
90
61.1
21.0
0.002

CBA
148
54
49
142
172
27
52
23
97
5
12
71
36
45
8
386
10
7
74.7
47.0
0.000

CBC-FS
14.5
28.0
4.6
16.2
27.3
27.6
18.1
18.1
15.0
6.7
12.3
26.4
21.0
24.5
0.0
21.9
8.0
5.0
16.39
17.16
–

CBC
14.1
27.0
5.6
16.7
29.9
25.7
15.6
20.0
16.3
6.7
12.7
26.1
20.9
24.7
0.0
23.2
9.1
4.9
16.61
16.49
0.366

C4.5
12.9
22.0
5.3
15.7
28.5
25.6
18.1
21.9
15.8
6.0
19.7
25.9
23.7
25.1
13.8
25.0
9.5
5.9
17.80
18.91
0.118

GARC
12.6
28.7
5.2
17.5
24.8
31.9
19.4
13.3
25.0
6.0
17.7
43.5
22.4
26.2
0.0
28.9
16.5
17.7
19.84
18.54
0.029

CBA
13.4
27.2
4.2
14.1
26.5
27.4
18.5
15.1
18.7
7.1
17.0
27.8
19.6
27.6
0.0
20.6
8.4
5.4
16.59
17.75
1.000

168

H. Deng et al. / Decision Support Systems 59 (2014) 163–170
Table 8
The CBC for the tic-tac-toe data set has only 8 rules in addition to the default
rule, which correctly captures the pattern for winning a tic-tac-toe game.
For this data set, CBA also has 8 rules and GARC has 26 rules, both with
zero error rate. C4.5 has more than 100 leaf nodes and does not correctly
capture all the patterns.
X3 = x ∧ X5 = x ∧ X7 = x
X1 =
X2 =
X1 =
X3 =
X1 =
X7 =
X4 =
Else

x
x
x
x
x
x
x

∧
∧
∧
∧
∧
∧
∧

X5
X5
X4
X6
X2
X8
X5

=
=
=
=
=
=
=

x
x
x
x
x
x
x

∧
∧
∧
∧
∧
∧
∧

X9
X8
X7
X9
X3
X9
X6

=
=
=
=
=
=
=

x
x
x
x
x
x
x

Table 10
The decision tree for the Titanic data set.
sex = female ∧ pclass = 1
sex
sex
sex
sex
sex

⇒ Y = win
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒

Y
Y
Y
Y
Y
Y
Y
Y

=
=
=
=
=
=
=
=

win
win
win
win
win
win
win
lose

Table 9
The CBC for the labor data set. For this data set, GARC has 15 rules and CBA has 12 rules
with greater error rates.
Contribute to pension plan = none

⇒ Y = bad

Contribute to health plan = none
Wage increased in the ﬁrst year ≤2.65
Help during employee longterm disabil = no
Else

⇒
⇒
⇒
⇒

=
=
=
=

female
female
female
female
male

∧
∧
∧
∧

pclass
pclass
pclass
pclass

=
=
=
=

2
3 ∧ embarked = C
3 ∧ embarked = Q
3 ∧ embarked = S

⇒ Y = 1 (94/3)
⇒
⇒
⇒
⇒
⇒

Y
Y
Y
Y
Y

=
=
=
=
=

1 (76/6)
1 (23/8)
1 (33/9)
0 (88/33)
0 (577/109)

4.3. Sensitivity to support and conﬁdence

class (1 = 1st; 2 = 2nd; 3 = 3rd), passenger fare and the port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton). Furthermore, age and fare are discretized into three levels: small, medium and
large. We know that, although there was some element of luck involved
in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class [15].
Therefore, it is interesting to ﬁnd out if the knowledge from CBC is consistent with human knowledge.
For comparison, a decision tree C4.5 was applied to the data, and
the rules from the tree are shown in Table 10. The two numbers (n1/n2)
after each rule are, respectively, the number of passengers satisfying
the condition (the left hand side of the rule), and the number of passengers with a different outcome from the rule consequent. For example, for the ﬁrst rule, there were totally 94 passengers satisfying
“sex = female ∧ pclass = 1”, and 3 of them did not satisfy Y = 1. A
rule with a smaller n2 (given the same n1) indicates a smaller number
of exceptions, and greater accuracy.
The decision tree reveals that female passengers were more likely to
survive than male passengers, but misses other information. There is
only one rule describing the male passengers and that rule has 109 exceptions and is not accurate. In fact, the training error rate from the tree
is only 0.047 for class 0, but is as large as 0.42 for class 1. Therefore,
many passengers survived are classiﬁed to 0 (not-survived).
The CBC with minimum support = 0.05 and conﬁdence = 0.5 for
this data set is shown in Table 11. Although CBC has more rules than
the decision tree, these rules are not irrelevant or redundant. The training
error rate from CBC is 0.07 for class0, and is 0.27 for class 1, signiﬁcantly
lower than decision tree (0.42), which indicates that these rules capture
useful information about class 1. Some CBC rules have three or more
attribute–value pairs in their conditions and are reasonably accurate,
while the decision tree contains rules with at most three attribute–
value pairs. For both CBC and decision trees, rules with long conditions
are expected to have high conﬁdence, otherwise the conditions can be
pruned to maintain the trade-off between accuracy and complexity.
Therefore, this demonstrates that CBC is more capable of discovering
highly-conﬁdent rules.
Also note this is an imbalanced classiﬁcation problem. That is, there
are more passengers who did not survive. It can be seen that C4.5 has
unsatisfactory performance on identifying survivors, while CBC can
capture more information for predicting survivors.

Y
Y
Y
Y

=
=
=
=
=

bad
bad
bad
good

Here we investigate the sensitivity of CBC to the changes of minimum
support and minimum conﬁdence for mining associative classiﬁcation
rules. First we ﬁx the minimum support = 0.05 and analyze different
minimum conﬁdence values: {0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9}. Then
we ﬁx the minimum conﬁdence = 0.6 and analyze different minimum
support values: {0.05, 0.06, 0.07, 0.08, 0.09, 0.1}. The number of rules or
error rates versus the minimum support or minimum conﬁdence, for
four data sets (other data sets are omitted to avoid too many lines on
the ﬁgures), is shown in Figs. 6, 7, 8 and 9.
As expected, for most data sets, the number of rules tends to decrease as the minimum support or minimum conﬁdence increases
(shown in Figs. 6 and 7). When the thresholds increase, the number of
rules that can be used decreases, and, thus, the classiﬁers become
more concise. Because some useful rules can be eliminated by setting
a larger threshold, the error rates tend to increase for data sets lymph
and pima (shown in Figs. 8 and 9). Also, for these two data sets, the
decrease in the number of rules in CBC is relatively large when changing
the minimum conﬁdence from 0.8 to 0.9, and, correspondingly, the
increase in error rate is also relatively large for the same change. This indicates that the rules with conﬁdence greater than 0.8 provide the main
information for building the CBC, and removing these rules has a negative impact on the accuracy. In contrast, the error rate changes for the
other two data sets austra and breast are not obvious even when increasing minimum conﬁdence to 0.9. This indicates that only the rules
with conﬁdence greater than 0.9 are needed to capture the characteristics of the data sets, which may also indicate that the classiﬁcation

Table 11
The CBC for the Titanic data set. The rules are ordered by priority, and should be executed
from the top to the bottom.
pclass = 1 ∧ sex = female

⇒ Y = 1 (94.0/3.0)

pclass = 2 ∧ sex = female
sex = female ∧ sibsp = 0 ∧ age = large
sex = female ∧ sibsp = 0 ∧ parch = 0 ∧ fare = medium
sex = female ∧ sibsp = 0 ∧ fare = small ∧ embarked = S
∧ age = small
sex = female ∧ sibsp = 0 ∧ fare = small ∧ embarked = S
sex = female ∧ sibsp = 0
sex = female ∧ embarked = C ∧ sibsp N = 2
sex = female ∧ embarked = C ∧ age = small ∧ parch = 0
sex = female ∧ embarked = C
sex = female ∧ parch = 0 ∧ fare = medium ∧ age = small
sex = female ∧ parch = 0 ∧ fare = medium
pclass = 1 ∧ sibsp = 1 ∧ age = small ∧ parch = 0
pclass = 1 ∧ sibsp = 1 ∧ age = small
pclass = 1 ∧ sibsp = 1 ∧ parch = 0 ∧ embarked = C
pclass = 1 ∧ sibsp = 1
sibsp = 0 ∧ fare = large ∧ parch N = 2 ∧ embarked = S
sibsp = 0 ∧ fare = large ∧ age = medium ∧ parch = 0
sibsp = 0 ∧ fare = large
parch = 1 ∧ age = small ∧ sibsp = 0
parch = 1 ∧ sibsp = 1 ∧ fare = medium ∧ age = medium
parch = 1 ∧ sibsp = 1 ∧ fare = medium ∧ sex = male ∧
age = large
parch = 1 ∧ sibsp = 1 ∧ fare = medium
parch = 1
Else

⇒
⇒
⇒
⇒

Y
Y
Y
Y

=
=
=
=

1 (76.0/6.0)
0 (7.0/1.0)
0 (10.0/4.0)
1 (7.0/2.0)

⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒
⇒

Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y

=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=

0 (11.0/4.0)
1 (46.0/12.0)
1 (3.0)
1 (3.0/1.0)
0 (4.0/1.0)
0 (4.0)
1 (14.0/4.0)
0 (2.0)
1 (2.0)
1 (7.0/2.0)
0 (20.0/8.0)
1 (2.0)
1 (16.0/6.0)
0 (72.0/24.0)
1 (2.0)
0 (7.0/1.0)
0 (2.0)

⇒ Y = 1 (16.0/2.0)
⇒ Y = 0 (29.0/2.0)
⇒ Y = 0 (435.0/47.0)

lymph
pima

austra
breast

lymph
pima

10

20

error rate(%)

15
10

0

5

num of rules

169

30

austra
breast

40

20

H. Deng et al. / Decision Support Systems 59 (2014) 163–170

0.05

0.06

0.07

0.08

0.09

0.10

0.05

0.06

0.07

support

0.08

0.09

0.10

support
Fig. 8. The error rates of CBC as the minimum support changes with ﬁxed minimum
conﬁdence = 0.6.

problems are less complex. Indeed, the error rates of the two data sets
are smaller than data sets lymph and pima. Furthermore, from experiments not shown here, we found that the change in the average error
rate over all the data sets is reasonably stable with regard to minimum
support or minimum conﬁdence, while the average number of rules
tends to decrease as the minimum support or minimum conﬁdence
increases.

Experimental studies show that CBC has signiﬁcantly fewer rules than
existing associative classiﬁers: CBA and GARC, with similar accuracy.
We also found that feature selection can substantially reduce the
number of rules in CBC. CBC is built using a decision tree algorithm.
Ref. [7] discussed the node sparsity issue in decision tree algorithms.
That is, many features can share the same information gain if there are
a small number of instances and a large number of features in a node.
Thus, a feature that is not globally strong can be used to split the node.
Feature selection scores the features using all the data and may be expected to select strongly relevant features, which prevents a decision tree
from splitting on “noisy” features and becoming unnecessarily large.
Existing associative classiﬁers can be accurate, however, they often
have a large number of rules. Having a compact set of rules in a rulebased classiﬁer is a key for decision makers to understand, validate and
trust the data-driven model. Therefore, this work promotes associative
classiﬁcation as an accurate and comprehensible data-driven model.
Although CBC or CBC-FS, in general, has fewer rules than the competitors with similar accuracy, in some cases such as the Titanic passenger
survival prediction problem, CBC has more rules than an ordinary decision tree C4.5. This may be because CBC is able to form rules with high
conﬁdence, and, thus, rules are not pruned. As expected, in the Titanic
passenger survival prediction problem, C4.5 misses some important
rules and has unsatisfactory performance on identifying survivals.

5. Conclusions

20

Ordinary decision trees consider one variable at a time, and, thus, they
are challenged by problems where the interactions of multiple variables
are predictive. We illustrate this by two examples, the exclusive OR
logic and the tic-tac-toe data set. Associative classiﬁcation leverages association rules which consider multiple variables simultaneously and,
therefore, can capture variable interactions well. However, when an associative classiﬁer consisting of an ordered rule set (e.g., CBA) is represented
as a tree, at least one child node of a non-leaf node in the tree is never split
further. The expressiveness of the tree may be limited.
We propose the condition-based tree (CBT) that relaxes the restrictions of both ordinary trees and associative classiﬁers. To facilitate interpretability, we propose a condition-based classiﬁer (CBC), which is a
equivalent representation of a CBT, but with simpliﬁed conditions.

lymph
pima

lymph
pima

10

20

error rate(%)

15
10

0

5

num of rules

austra
breast

30

austra
breast

40

Fig. 6. The number of rules in CBC as the minimum support changes with ﬁxed minimum
conﬁdence = 0.6.

0.60

0.65

0.70

0.75

0.80

0.85

0.90

confidence
Fig. 7. The number of rules for CBC as the minimum conﬁdence changes with ﬁxed
minimum support = 0.05.

0.60

0.65

0.70

0.75

0.80

0.85

0.90

confidence
Fig. 9. The error rates of CBC as the minimum conﬁdence changes with ﬁxed minimum
support = 0.05.

170

H. Deng et al. / Decision Support Systems 59 (2014) 163–170

Because CBC uses the combination of multiple attributes to split a
node, individual rules in CBC can be longer than rules in ordinary trees
that only use one attribute at a node. However, the complexity of individual rules depends on the nature of the data set. The length of rules
is expected to increase as the complexity of the data set increases. For
example, the maximum number of attributes in a rule for the XOR example is two, while the maximum number of attributes in a rule for
the tic-tac-toe example is three.
CBC inherits some favorable characteristics from associative rule
algorithms and decision trees. For example, the punning function is
able to prevent over-ﬁtting and maintain a balance between training
accuracy and model complexity. On the other hand CBC also has a few
disadvantages similar to associative rule algorithms. First, the attributes
have to be categorical or need to be discretized. Discretization may
cause information lost, and, therefore, might have a negative impact
on accuracy. Second, associative rules are often generated in a bruteforce mode, and, therefore, the efﬁciency becomes a concern for highdimensional data. To solve this issue, one can perform feature selection
to reduce the number of attributes. Also, one can limit the length of rule
conditions to avoid expensive computations.
It should be noted that the classiﬁcation rules used to construct CBT
do not have to be associative classiﬁcation rules. Future research includes building CBT based on rules/conditions from efﬁcient algorithms
such as tree ensembles, which naturally handle mixed numerical and
categorical variables.
It may also be of interest to apply the CBT–CBC rule transformation
algorithm to binary decision trees. The rules after transformation should
be executed in a certain order, but can have much simpler conditions
than the rules generated by the ordinary way, that is, extracting a rule
by aggregating the variable–value pairs from the root node to each leaf
node.
Acknowledgments
We thank the anonymous reviewers for the insightful comments.
References
[1] R. Agrawal, R. Srikant, Fast algorithms for mining association rules, Proceedings of
the 20th International Conference on Very Large Data Bases, Morgan Kaufmann,
1994, pp. 487–499.
[2] A. Berrado, G. Runger, Using metarules to organize and group discovered association
rules, Data Min. Knowl. Disc. 14 (2007) 409–431.
[3] C. Blake, C. Merz, UCI Repository of Machine Learning Databases, 1998.
[4] L. Breiman, J. Friedman, R. Olshen, C. Stone, Classiﬁcation and Regression Trees,
Wadsworth, Belmont, MA, 1984.
[5] G. Chen, H. Liu, L. Yu, Q. Wei, X. Zhang, A new approach to classiﬁcation based on
association rule mining, Decis. Support. Syst. 42 (2006) 674–689.
[6] H. Cheng, X. Yan, J. Han, C. Hsu, Discriminative frequent pattern analysis for effective
classiﬁcation, Proccedings of the 23rd International Conference on Data Engineering,
IEEE, 2007, pp. 716–725.

[7] H. Deng, G. Runger, Gene selection with guided regularized random forest, Pattern
Recogn. 46 (2013) 3483–3489.
[8] U.M. Fayyad, K.B. Irani, Multi-interval discretization of continuous-valued attributes
for classiﬁcation learning, Proceedings of the 13th International Joint Conference on
Artiﬁcial Intelligence, Morgan Kaufmann, 1993, pp. 1022–1027.
[9] M. Hahsler, B. Gruen, K. Hornik, A computational environment for mining association
rules and frequent item sets, J. Stat. Softw. 14 (2005) 1–25.
[10] M.A. Hall, Correlation-based feature selection for discrete and numeric class machine
learning, Proceedings of the 17th International Conference on Machine Learning,
Morgan Kaufmann, 2000, pp. 359–366.
[11] K. Hornik, C. Buchta, A. Zeileis, Open-source machine learning: R meets Weka,
Comput. Stat. 24 (2009) 225–232.
[12] J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, B. Baesens, An empirical evaluation
of the comprehensibility of decision table, tree and rule based predictive models,
Decis. Support. Syst. 51 (2011) 141–154.
[13] R. Ihaka, R. Gentleman, R: a language for data analysis and graphics, J. Comput.
Graph. Stat. 5 (1996) 299–314.
[14] Y. Jiang, J. Shang, Y. Liu, Maximizing customer satisfaction through an online recommendation system: a novel associative classiﬁcation model, Decis. Support. Syst. 48
(2010) 470–479.
[15] Kaggle, Titanic: machine learning from disaster, URL http://www.kaggle.com/c/
titanic-gettingStarted/data2013.
[16] W. Li, J. Han, J. Pei, Cmar: accurate and efﬁcient classiﬁcation based on multiple
class-association rules, Proceedings of the 2001 IEEE International Conference on
Data Mining, IEEE, 2001, pp. 369–376.
[17] B. Liu, W. Hsu, Y. Ma, Integrating classiﬁcation and association rule mining, Proceeding
of the 1998 International Conference on Knowledge Discovery and Data Mining, ACM,
1998, pp. 80–86.
[18] D. Martens, J. Vanthienen, W. Verbeke, B. Baesens, Performance of classiﬁcation
models from a user perspective, Decis. Support. Syst. 51 (2011) 782–793.
[19] D. Papakiriakopoulos, K. Pramatari, G. Doukidis, A decision support system for detecting products missing from the shelf based on heuristic rules, Decis. Support.
Syst. 46 (2009) 685–694.
[20] J. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann, 1993.
[21] P. Su, W. Mao, D. Zeng, H. Zhao, Mining actionable behavioral rules, Decis. Support.
Syst. 54 (2012) 142–152.
[22] E. Tuv, A. Borisov, G. Runger, K. Torkkola, Feature selection with ensembles, artiﬁcial
variables, and redundancy elimination, J. Mach. Learn. Res. 10 (2009) 1341–1366.
[23] A. Veloso, W. Meira, M. Zaki, Lazy Associative Classiﬁcation, IEEE, 2006, pp. 645–654.
[24] J. Wang, G. Karypis, HARMONY: efﬁciently mining the best rules for classiﬁcation,
Proceedings of the 2005 SIAM International Conference on Data Mining, SIAM,
2005, pp. 205–215.
[25] K. Wang, S. Zhou, Y. He, Growing decision trees on support-less association rules,
in: Proceedings of the sixth International Conference on Knowledge Discovery and
Data Mining, ACM, 2000, pp. 265–269.
[26] F. Wilcoxon, Individual comparisons by ranking methods, Biom. Bull. 1 (1945)
80–83.
[27] J. Yeh, T. Wu, C. Tsao, Using data mining techniques to predict hospitalization of
hemodialysis patients, Decis. Support. Syst. 50 (2011) 439–448.
[28] X. Yin, J. Han, Cpar: classiﬁcation based on predictive association rules, Proceedings
the 2003 SIAM International Conference on Data Mining, SIAM, 2003, pp. 331–335.
[29] O. Zaïane, M. Antonie, On pruning and tuning rules for associative classiﬁers,
Knowledge-Based Intelligent Information and Engineering Systems, Springer,
2005, pp. 966–973.
Houtao Deng, PhD, Data Scientist at Intuit, California, USA. Email: hdeng3@asu.edu.
George Runger, PhD, Professor at Arizona State University, Arizona, USA. Email: george.
runger@asu.edu.
Eugene Tuv, PhD, Principal Scientist at Intel, Arizona, USA. Email: eugene.tuv@intel.com.
Wade Bannister, PhD, Ingenix Consulting, California, USA. Email: wade.bannister@
ingenixconsulting.com.

394

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

An Automated Feature Selection Method
for Visual Inspection Systems
Hugo C. Garcia, J. René Villalobos, and George C. Runger

Abstract—Automated visual inspection (AVI) systems these
days are considered essential in the assembly of surface-mounted
devices (SMDs) in the electronics industry. This industry has faced
the problem of rapid introduction and retirement of SMD-based
products with the consequent obsolescence of the inspection
systems already in the assembly lines. The constant introduction
of new products has caused AVI systems to become rapidly obsolete. The general goal of this research centers on developing
self-training AVI systems for the inspection of SMD components.
The premise is that these systems would be less prone to obsolescence. In this paper, the authors describe the methodology
being used for automatically selecting the features to inspect new
components. In particular, this paper explores the use of multivariate stepwise discriminant analysis techniques, such as Wilks’
Lambda, in order to automate the feature selection process. All of
these techniques are applied to a case study of the inspection of
SMD components.
Note to Practitioners—In this paper, we present a methodology
that would allow the automation of the tedious task of exploring
and selecting features to develop algorithms for automated visual
inspection systems. In particular, the proposed method selects a
subset of features among all of the known features. The chosen
subset seeks to minimize inspection errors while keeping the algorithmic development time to a minimum. This is particularly useful
for adapting pre-existing systems to inspect new components, especially when the characteristics of the new components are similar
to those of components already inspected by the inspection system.
We applied this methodology to a case of study of the inspection of
surface-mounted devices.
Index Terms—Automated visual inspection (AVI), feature selection, quadratic vector classifier.

I. INTRODUCTION

T

HE continuous miniaturization of surface-mounted device
(SMD) components has made automated visual inspection (AVI) systems an indispensable tool in the assembly of
electronic products. However, the full potential of AVI systems
has not yet been exploited. For instance, to date, the main use
of these systems involves performing a screening inspection
(that is, the identification and removal of defective components)
rather than as a tool for the continuous improvement of the underlying manufacturing system. One reason for the absence of
AVI systems with continuous improvement capabilities is the
Manuscript received November 15, 2005. This work was supported by the
National Science Foundation under Grant DMI-0300361. This paper was recommended for publication by Associate Editor Y. Ding and Editor P. Ferreira
upon evaluation of the reviewers’ comments.
The authors are with the Department of Industrial Engineering at Arizona
State University, Tempe, AZ 85281 USA (e-mail: hugo.garcia@asu.edu;
rene.villalobos@asu.edu; George.Runger@asu.edu).
Digital Object Identifier 10.1109/TASE.2006.877399

lack of flexibility in the existing AVI systems to adapt its inspection algorithms to new products. This problem of reconfiguration, combined with the rapid introduction and retirement of
electronic products, has deterred equipment manufacturers and
the electronic-assembly industry from investing in the development of AVI systems for continuous process improvement.
Currently, when new components are introduced into an existing assembly line, the AVI system requires reprogramming
to incorporate the new characteristics of the new components
into the inspection algorithms. Often, this process of reconfiguration requires the involvement of the original equipment manufacturer (OEM) of the AVI system to develop the new inspection
algorithms and adjust the existing AVI system to the new components. The cost of this adjustment period is frequently one of
the main deterrents for the appropriate use of AVI systems on the
factory floor. Therefore, it is necessary to develop the tools and
methodologies that would allow the rapid adaptation of existing
AVI systems to new electronics products so that the AVI systems are not rendered obsolete by incremental changes in SMD
component technology.
One of the crucial and most time-consuming processes in
the training phase of AVI systems is the selection of inspection features. A feature, in our context, is usually a prominent
or distinctive characteristic that can be extracted from a digital image of an SMD component. Ideally, the features used
share common characteristics in that they are computationally
inexpensive, and simple enough to accommodate new components without major modifications to the actual system. However, a downside of the features simplicity is that they alone do
not provide an error-free classification between the populations.
In the training phase, the developer of the AVI system needs
to identify, among known features, a subset of them that provides an adequate level of discrimination between defective and
nondefective components. If no subset of features provides the
needed discrimination, it is necessary to develop new features
to attain the desired level of discrimination. The feature selection process requires a great deal of time and a knowledgeable
human developer. This paper addresses the problem of accelerating, feature selection for SMD inspection by introducing a
general methodology whose aim is to accelerate and eventually
automate the generation of SMD inspection routines. In particular, this paper focuses on solving the problem of automatically
selecting a subset of features among the larger set of features
known to provide at least reasonable discrimination for similar
components.
This paper focuses on using multivariate statistical techniques
to automate the feature selection process. When a large number
of features to select from is available, for instance in a database,

1545-5955/$20.00 © 2006 IEEE

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

395

Fig. 2. Side view of the AVI system.

Fig. 1. SMD defects proportions.

it is impossible to develop an exhaustive search of all the possible subset combinations to find the subset that gives the best
discrimination. For a discussion in the underlying complexity
of an exhaustive search, the reader is referred to [27]. In particular, in order to identify a subset of features to be used for
the inspection of new components, we explore the use of multivariate stepwise discriminant methods (MSDM) such as Wilks’
Lamba, unexplained variance, Mahalanobis Distance, smallest
distance, and Rao’s V. The subset selected by the MSDM in the
training phase of the classifier rule is termed “the best subset.”
In this phase, data are acquired to determine the features and
their corresponding parameters that provide an adequate level
of discrimination. When this phase is concluded, a classification rule is identified to be used for the actual inspection of the
components.
This paper is organized as follows. Sections II-A and B describe the experimental setting and the features used in this research. Section II introduces the background of the problem and
the platform to be used in the experiments. Section III describes
the feature selection problem. Section IV gives the framework
to deal with the classification problem as a multiple linear regression problem. Section V introduces the feature selection
problem as a multistep discrimination problem. Section VI illustrates the application of the methodology to SMD inspection
and presents some numerical results. Finally, Section VII offers
the conclusions of the work presented.
II. SMD INSPECTION PROBLEM
According to Feld [8], the two most common defects in electronics assembly are missing and misaligned components. He
reports (Fig. 1) that these two defects represent 40% of all the
defects found in SMD assembly.
Therefore, the detection and the correction of the underlying
causes of these defects would represent a significant improvement for the current state of the art of the electronics assembly.
In theory, human inspectors could be utilized to detect
missing components. However, in practice, the continuous
miniaturization and the increasing speed of assembly of these
components make, for all practical purposes, human inspection
obsolete and the use of AVI systems a necessity. However,
because of their lack of flexibility, these systems have been
used mostly as a way to detect defects rather than to use the

information generated by these systems to improve the underlying manufacturing systems. We believe that a key to provide
the flexibility needed in the AVI systems for SMD inspection
is the automation of the development and reconfiguration of
inspection routines. Because of the time it requires, a very
important element in achieving this goal is the automation of
the selection of the features to use for the inspection of SMD
components. The introduction of a methodology to automate
this part of the process is the main purpose of this paper.
In our discussion, we use an experimental platform available
at the Electronics Assembly Laboratory at Arizona State University. The platform used consists of a mechanical positioning
system, an image-acquisition system, an illumination system, a
Matrox® board, and a personal computer (PC). The image-acquisition system consists of four Pulnix™ cameras each with a
25-mm lens. The resolution of the system is 0.0703 mm/pixel.
The lighting system consists of 8 light-emitting diode (LED)
panels. Fig. 2 presents a picture of the experimental platform.
For a more detailed description of the experimental platform
and the image-acquisition methodology, the reader is referred
to [35]. In order to acquire the images of the components to be
inspected, the board is divided into “component windows” that
correspond to those areas of the board where a component is
supposed to be present. Once these component windows are defined, the system only examines the images of these regions.
A. Description of the Classification Features
One of the underlying practical requirements for the development of an automated feature selection process is the computational simplicity of the features. Another assumption is that
there are many features to select from. In order to build a system
that is reliable and easy to implement, the potential inspection
features should be computationally inexpensive, provide a good,
yet not perfect, level of discrimination between defective and
nondefective components, and present an appropriate level of
cross-correlation with the other considered. For illustration purposes, we introduce six representative features to classify a component. These features are termed energy (E), correlation (C),
diffusion (D), fenergy (F), blob (B), and texture (T). While these
features were carefully selected to illustrate the feature selection
method to be introduced in this paper, it must be mentioned that
the validity of the methodology described in this paper is independent of the particular features selected for this discussion. A
brief description of the features is given next. A more detailed
description of these features can be found in [35] and [36].

396

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

Energy is a measure of the “brightness” of an image and it
is given by the number of pixels whose value is above a certain threshold. If no pixels exist above this threshold, a value of
zero is assigned as the energy of the image. Ideally, a threshold
value is defined such that the perfect discrimination between
the population of present components and absent components is
achieved. Diffusion is the finite-difference diffusion equation. It
is a measure of the definition of local features. It can be thought
of as an iterative, local correlation operation, in which a mask
of a particular feature of interest is used to highlight all occurrences of the feature in a signal. This feature is computed on
the one-dimensional (1-D) projection obtained in the first step
of the computation of the correlation feature. The fenergy feature, which derives its name from its close relationship to the
energy feature, is a weighted sum of all the white pixels from
a projected image of an SMD component. The blob feature is
simply the average area of homogeneous regions defined by a
certain value of pixel brightness. In the case of the SMD inspection application being described, the value for this feature
is typically much higher for a nondefective image. The texture
feature is based on a comparison of the image under inspection
to an ideal image, which is prepared offline, similar to the case
of correlation. In order to show the computationally simplicity
of features used in this paper, the correlation feature is explained
next in more detail.
Correlation is a measure of the likeness of a signature obtained from the image when compared against a syntactic signal
representing the corresponding signature of a defect-free image.
The correlation feature is defined as the maximum value returned by the convolution operation between a signal of the
image under inspection and an ideal signal. In order to perform this operation in real time, a simplification of the process
is necessary. The simplification consists of taking the projection of the two-dimensional (2-D) image on its major axis. This
way, the convolution can be performed using two 1-D signals.
In order to add more stability to the resulting signal, its discrete first derivative is obtained. This process is illustrated in
Fig. 3. Fig. 3(a) presents the original image when a component
is present, Fig. 3(b) presents the projection of the 2-D image on
its major axis. Fig. 3(c) presents the derivative of this signal. In
order to apply the convolution operation, it is necessary to have a
syntactic signal representing a signal of an image with a component present. This signal is currently prepared offline for each
different type of component under inspection and it is totally
defined by three parameters: width of positive signal, width of
negative signal, and distance between the signal’s peaks. An example of this signal is given in Fig. 3(d) (and repeated for illustration purposes in Fig. 3(i). Once the projected signal and the
syntactic signal are available, the convolution operation takes
place. Fig. 3(e) presents the results of the convolution operation for an image with a component present. Figs. 3(f)–(j) show
the correlation process applied to an image with a component
missing. The peak or maximum value produced by this correlation will be the value of the correlation feature for the particular component under inspection. Ideally, if the component is
present, the maximum value returned by this operation is high,
and if the component is not present, the value is low. This can
be observed in Fig. 3(e) and (j).

B. Discrimination of Selected Features
Ideally, for each feature, a threshold value could be defined
to separate the values generated by defective and nondefective
images perfectly, thus rendering perfect discrimination between
the populations. In practice, this perfect discrimination is rarely
achieved, having to rely on several features to obtain a low-error
classification. For instance, the six features previously described
were applied to the images of a type of component (“805 V”)
present in a preselected printed-circuit board (PCB).
Fig. 4 presents a histogram of each feature for the nondefective and defective populations. As the reader can see, none of the
features used independently can result in free misclassification
errors. A usual approach is to use more than one feature, very
often in a sequential fashion, to classify the component under inspection as “nondefective (or present)” or “defective (absent).”
A problem with using a sequential approach is that it can result in inflating the type II error (classifying a nondefective component as defective). In the approach that we propose, a vectorbased approach is used. This vector approach is described in
Section III-A. However, the question that still remains is what
features to use to perform the classification of components. In
order to answer this question, we need to look beyond the levels
of discrimination provided by each feature independently.
First, we need to establish the level of discrimination desired.
Second, we need to determine the level of correlation among
the features being considered. The consideration of the level of
correlation among the features being considered is extremely
important in the development of an automated feature selection
process. For instance, from the information provided in the histograms of Fig. 4, if correlation is not considered and only two
features were to be selected, the obvious approach would be to
select the “C” and “D” features using the misclassification error
rate (MER) information from Table I. However, we will see later
that this is a particularly poor choice. A better option is to select features “C” and “B.” The way that the latter combination
was deemed better than the former will be explained later in this
paper. However, something that is worth mentioning is that this
is a surprising result since feature “B” would seem to be a poor
choice because of the high number of potential misclassification
errors rendered by this feature.
A possible explanation is given by the low level of cross correlation exhibited by these two features as presented in Table II.
This table shows the values of the cross correlation between
the six features analyzed. However, the question still remains
of what constitutes a good set of features and how to construct
these set of features. The methodology to be presented in Section V of this paper is an approach that we propose to answer
these questions.
III. FEATURE SELECTION PROBLEM
DEFINITION AND BACKGROUND
A classification function, or classifier, is a rule that decides
which population each point under analysis should be assigned.
One of the main issues in classification is determining the
number of features used in the classification function. In practical problems, usually a large number of features are initially
considered, only some are developed, and few are used in the
final implementation of the classification system. Hence, a

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

Fig. 3.

397

Steps of correlation feature.

problem that often arises is the way to accelerate the selection
process of the features.
The problem of feature selection has been addressed extensively for applications other than AVI systems, such as classification and diagnostic problems in engineering, social, and

medical sciences (see, for instance, [2], [5], and [24], respectively). The literature concerned with feature selection for discriminant analysis is vast; numerous references of applications
of this analysis appear in the literature; some examples include
[10], [12], [21], and [33].

398

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

Fig. 4. Defective and nondefective population histograms.
TABLE I
LEVEL OF DISCRIMINATION

TABLE II
CROSS-CORRELATION BETWEEN FEATURES

Reference [15] defines the problem of feature selection in
inspection systems as follows: “Given a set of features, sethat leads to the smallest classification
lect a subset of size
error.” Feature selection methods are often employed to determine which features are critical in order to reduce the classification error or the amount of data needed to train the classification
system.
A new area of application and development of the feature selection methods is called sensor selection. In this case, instead
of searching for features, the problem is to select the set of sensors that will result in the small classification error. Some in-

formative papers that address this topic are as follows: [4], [7],
[9], [11], and [32]. Therefore, the sensor or feature selection
problem can also be stated as an optimization problem.
A possible definition of the optimization problem is as follows: given feature sets of size , select the subset of size that
optimizes the measure subject to a set of constraints . The
measure can assume a multitude of forms. However, some
measures are commonly used to assess the performance of the
classification system. For instance, a measure that is commonly
used is the expected number of classification errors or the proportion of attempted classifications that result in an error, this
measure is called the MER. Common examples of constraints
imposed on the feature selection process include the maximum
number of features as part of the final classification set, a certain maximum level of probability misclassification, the total inspection time available, the discrimination power, and so forth.
In our case, in addition to the traditional constraints such as the
ones previously listed, the authors are also concerned with the
overall time needed to identify the set of features that will render
an acceptable classification. Therefore, it is not only important
to obtain a certain level of discrimination but also to reduce the
time and data as much as possible to identify an acceptable set of
features to perform the inspection phase. Since the data needed
for the discriminant analysis are directly related to the number
of features used, it is important to reduce this number as much
as possible [13]. As a consequence, if two different subsets of
features render the same level of discrimination, the one using
the least number of features is to be preferred in order to perform the inspection of the components.
The reduction in the number of features can be accomplished
by taking advantage of the cross-correlation or redundancies
among the features [19]. For instance, [20] states that increasing
the number of features does not necessarily result in an increase

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

in the discriminatory power and that it may indeed decrease if
too many unnecessary features are used. These authors also contend that the greater the number of features are, the larger
the sample needed to achieve the same level of discrimination.
However, the feature selection to be used for discrimination purposes is only part of the discrimination problem: in order to
detect the effectiveness of a discriminatory features subset, it
is necessary to know the way that this subset behaves in the
training and inspection phases.
Similar comments were made by [22]. Additionally, if the
sample size remains the same as more irrelevant features are
added, the power of the discriminant test decreases and the
variability of the estimates increases. As a result, decreasing
the number of features is useful in many cases [28]. Reference
[34] presents an argument for the reduction of features from
the perspective of time; they state that if the number of features
is ten or more, the statistical computation becomes an extremely time-consuming process, even with the use of modern
computers. Therefore, one of our goals centers on reducing
the number of features used in the classification process. The
purpose in feature selection involves minimizing the complexity of the methodology by including features that are as
computationally simple as possible while providing acceptable
levels of discrimination.
A. Feature Selection in the Context of the
SMD Inspection Problem
The AVI of SMDs depends on an efficient classification
system to determine if a component is defective or nondefective. Under this approach, each feature acts as a function whose
domain is the digital image and whose range is the real line.
It is hoped that each function will tend to map the images of
defective and nondefective components to distinct regions of
the real line for each population to make an easy and defect-free
classification process possible. When more than two features
are used in the inspection, the AVI classification problem
becomes one of discrimination between two multivariate populations. One of the problems in multivariate discrimination is
how to use the data rendered by different features. A frequent
approach uses classification functions or classifiers that take as
input the information provided by each feature and deliver a
single value to classify the image under analysis.
We base the classification methodology on a vector discrimination approach for the identification of defective components
whose basic characteristics have already been developed [35].
Although multiple classification functions exist (see, for instance, [7] and [14]), one function that is particularly well
suited for SMDs’ classification is known as the quadratic
classification function (QCF). The QCF has advantages over
the traditional sequential decision classification because the
probability of type I error does not increase when the number
of features used in the analysis increases. Another characteristic of this classifier is the simplicity of the features used in
it, which make the application of the classification method
possible for real-time automated inspection systems. The QCF
is the classifier that the authors use throughout this paper and
is briefly described next.

399

The QCF is a multivariate method used to discriminate different populations of observation and allocates new observations to the population previously formed. The QCF assigns an
observation under consideration to the most likely of popudenoted as
“defect free” and
lations (in our case,
“defective”). Since the real population parameters are not
known in advance, it is necessary to obtain their estimates from
samples obtained in a training phase. For more details about the
development of the QCF equation and the way that it is used in
the inspection of SMD components, see [18] and [35]. The QCF
for two populations using training data becomes:
(new observation) to
if
Allocate

(1)
to , where
Otherwise, allocate
value of multivariate classifier;

vector of feature values of the new inspected component;
sample covariance matrix for population ;
sample covariance matrix for population ;
sample mean vector of the features for population
;
sample mean vector of the features for population
;
cost of misclassification (observation from
is incorrectly classified as );
cost of misclassification (observation from
is incorrectly classified as );
prior probability ratio.
It is important to highlight two issues: first, the multivariate
classifier just described relies on assumptions of normality and
which is a
second, the classifier renders a single value of
measure of the closeness of the point under consideration to
a targeted population which determines the final classification
of the component under consideration. Regarding the normality
assumptions, commonly the data rendered by features used in
SMD inspection depart from these assumptions. However, the
QCF can accommodate well small departures from normality.
If the departures from normality are significant, the data can
be transformed to obtain better results. Reference [1] discusses
ways to deal with departures from normality.
One requirement for the use of the QCF is the availability
of a data set of features. We define the problem as finding the
methodology that automatically renders the minimal set of features that will meet the constraints of a minimum level of discrimination. Fig. 5 depicts the general organization of an envisioned environment for the automated generation of inspection
routines for SMD. In this paper, the authors focus on the feature selection module of this environment. The explanation to
be presented relies on the techniques used to select independent
variables (or regressors) used in regression analysis. For an introduction to these techniques, the reader is referred to [6] and
[26].

400

Fig. 5.

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

feature selection problem as follows: to create a multiple linear
regression equation for the particular response in terms of the
. For instance, assume
basic regressor or features
that
are all functions of one or more of the
’s and the ’s represent the complete set of features from
which the final equation is to be chosen, using the best subset
of features. The subset selected can include any functions of
the original set of features, such as linear, squares, cross-products, and so forth. Therefore, the feature selection process
will consist of identifying the best function. However, this
approach is problematic because the feature selection problem
has opposing criteria to determine the best subset, namely the
following two:
1) For discrimination purposes, the model should include as
many ’s as are necessary to keep the bias of the error
small, and to determine reliable MER values.
2) The cost and time involved in obtaining information on
a large number of ’s and subsequently monitoring the
features increment the variance of the predictor variable;
therefore, it is desired that one include as few ’s as possible in the model.
Notice that the feature selection problem in the multivariate
regression analysis and in the discriminant analysis is similar.
In fact, the mathematical background is almost the same for the
two problems. In multivariate regression, the principal objective centers on minimizing the square error between the model
and the experimental data while in multivariate discriminant
analysis, the principal goal involves minimizing the MER. For
instance, [13] mentions that the MER in discriminant analysis
(the coefficient of mulis inversely analogous to the statistic
tiple determination) of regression analysis. The MER reveals
how well the discriminant function classifies the data under
indicates how much variance of data
analysis while the
under analysis can be explained by the regression equation. In
the rest of the paper, we use discriminant analysis exclusively.

SMD inspection problem.

IV. CLASSIFICATION AS A MULTIPLE LINEAR
REGRESSION PROBLEM
In general, in a linear regression model, the response variable
is related to the predictor regressor (features) through the
following mathematical model:
(2)
Equation (2) is called a multiple linear regression model with
regressors. The parameters , for
, are called
the regression coefficients or partial regression coefficients. The
random error is denoted by . The model describes a hyperplane
in the -dimensional space defined by the regressor variables .
This formulation can also be a representation of the feature selection problem provided the response variable is the one used
to perform the discrimination between the defective and nondefective populations or the one that provides a mapping to the
appropriate region. Notice that in this case, would be a binary
variable, taking values of 0 or 1, accordingly. This representation will be used in the discrimination problem.
Different authors have researched the feature selection
problem under different contexts; for instance, [6] defined the

V. FEATURE SELECTION AS A MULTISTEP
DISCRIMINANT PROBLEM
In this paper, the authors focus on the use of multivariate stepwise discriminant methods (MSDM) to solve the feature selection problem that arises in the development of AVI algorithms.
The use of a stepwise procedure is a widely accepted practice
for feature selection when the comparison of all possible subsets is unfeasible. A typical procedure used by MSDM for the
selection of features is in following steps.
Step 1) Start with a discriminant function with no features
in it and a list of potential features candidates to be
used in the discriminant function.
Step 2) From the list of features, select the feature in the
list that maximizes a given criterion, add that feature to the model.
Step 3) The marginal contribution, as measured by the
given criterion, is calculated for each remaining
potential feature in the list. This marginal contribution is calculated over the feature(s) already in
the model.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

Step 4)

The feature that in combination with the included
feature(s) maximizes the criterion is selected.
Step 5) If the selected feature meets a given condition it is
introduced into the model.
Step 6) The partial statistic for each feature in the model
is re-examined. If a feature does not meet a given
condition, it is removed from the model.
Step 7) Steps 2)–5) are repeated until no more features are
added to the model.
A pseudocode for the previous procedure is given by:
Let
; Initialize the Best current
Subset of features used by the classifier as empty
; Set of all potenLet
tial features
; Define AF as the set of
Let
features to be considered
Examine all the potential
For
features
; Select feature
from AF that maximizes a chosen figure
of merit FM given BS
threshold for inclusion
If
; update AF
if it is not
Continue; discard
promising

401

The previous description of a general MSDM relied on a partial statistic, or figure of merit, to guide the inclusion and exclusion of features. Numerous approaches are available to arrive at
this partial statistic. The authors tested the following five different methods in this research: Wilks’ Lambda, unexplained
variance, Mahalanobis Distance, smallest f ratio, and Rao’s V.
The principal difference between these methods is that each has
different criteria to select and remove the features that will be
included in the final subset. Since similar results were obtained
with the five methods, we only describe the Wilks’ Lambda
method in this paper because this method is the most commonly
used in practice. For details in the other four methods considered, the reader is referred to [25] and [28].
as the th observation taken from the th popuWe define
lation during the training phase of the development of the clas,
,
, repsifier. Where each
.
resents independently sample points observed from
Where is the total number of populations, is the number of
observations of the training data set, and represents the observation number. In matrix notation, the data obtained during the
training phase is given next

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

We use a short-hand notation for the sample totals and means as
follows:

else
; add

to the current best

subset
; update AF
For
Marginal contribution
;
marginal contribution of
in the new
subset
threshold
If (marginal Contribution
for removal)
; remove feature y from current best subset

For additional technical details about the previous procedure,
the reader is referred to [3]. The conditions discussed in Steps
4) and 5) above are parameters that may be specified by the
user, and the most typical ones are referred to as -to-enter and
-to-stay ( means the type I error). This parameter, together
with the criterion defined in Step 3), gives the decision thresholds to include or eliminate the features from a given subset.
Some recommended values of have been given in [23] in the
–
whereas [20] differs from this recommendarange of
tion, giving a value of 0.15.

Wilks’ Lambda method is based on Wilks’ –criterion, which is
a ratio expressing the total variability not explained by population differences [16]. This criterion is expressed in the following
equation:
(3)
where
is the determinant of the estimated generalized vari“deance within the two populations ( “defect free” and
fective”), represents the variation between populations, and
is the determinant of the estimated generalized total
variance of the training data. The definitions of these matrices
are given in (4) and (5), respectively
(4)

(5)

402

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

The
“hypothesis” matrix has, on its diagonal, the between sum of squares (BSS) for each of feature. The offdiagonal elements of are analogous sums of products for all pairs
of variables. The
“error” matrix
has on its diagonal the
within sum of squares (WSS) for each variable with the analogous sum of products offdiagonal. The has a Wilks’ Lambda
,
distribution with degrees of freedom (DOF) ,
and
. Reference [29] shows the values of this
distribution.
, in which larger
The value lambda is between
values indicate a poor separation between populations, and
smaller values indicate good separation between groups. The
reader is referred to [16] and [18] for more computational
details of this ratio.
Using Wilks’ statistic as a criterion for finding a good
subset is analogous in multiple regression analysis to finding
the set of variables that maximizes the -ratio. In fact, for
stepwise discriminant analysis, the partial -statistic is used to
estimate the corresponding -statistic. The partial -statistic,
denoted as
, is a ratio that measures the change in
that results from the addition of variable to the subset
.
is defined by the following
equation:
(6)
This equation has a Wilks’ Lambda distribution , ,
and is independent of
. Since
, this partial -statistic
has an exact -transformation given by the following equation:
(7)
Equation (7) becomes a partial statistics with the DOF of
and
. Recall that is the number of features in the model.
Therefore, this method selects for inclusion in the model the
variable that minimizes the overall Wilks’ value.
A. Measuring the Performance of the Selected Subset
Once a subset of features is identified, it is necessary to identify the metrics to be used to determine the effectiveness of the
selection. Several useful statistics can be used to measure the
performance of the selected subset by the MSDM; because of
simplicity, however, the authors focus on three nonparametric
statistics. These measures are based on the MER and are called
the following: the apparent error rate (APER), the expected actual error rate (EAER), and the inspection error rate (IER). The
first two methods are based on the training phase of the classifier
and the last one is based on the inspection or validation phase
of the AVI system. The three statistics represent the proportion
of errors that result from the use of selected features in the classifier. Each of these statistics is described next.
The APER is applied to data gathered during the training
phase and it is simply the proportion of components that are
misclassified for the given subset of features
(8)

the total number of defect-free observations,
where
the total number of defective observations, and
is the
total number of defect-free observations that are misclassified
is the total number of defecas being defective, and
tive observations that are misclassified as being defect free. The
APER statistic is a biased estimator of the true MER of the systems, because it is evaluated on the training data. This procedure is often misleading because it provides optimistic values
for MER, because the same observations that are used to compute the discrimination function in the training phase are used
to evaluate its performance in the validation phase [30].
The EAER is an indication of the way that a classification
function will perform in future samples of observations. The
statistic cannot be directly calculated because it depends on the
and . Howunknown density functions of the populations
ever, an estimated value can be projected using the method of
Lachenbruch’s holdout, which is also commonly referred to as
the cross-validation method [18]. The EAER is performed by
omitting one observation from the populations and developing
the QCF function based on the remaining observations. The absent observation is then classified using the QCF. The EAER is
a nearly unbiased estimate of the AER and EAER [33].
The inspection error rate (IER) is the statistic of the overall
errors percentages obtained from the inspection data set of observations. The IER is an unbiased and consistent estimator of
the MER because it uses two independent data samples, one
(training data) to build the QCF and the other (inspection data)
to evaluate it [15]. While the APER is an error percentage calculated on the training sample, the IER is calculated on the inspection sample. The IER is calculated in the same way as the
APER, but the difference lies in the use of the inspection sample
data instead of the training data.

VI. APPLICATION OF THE FEATURE SELECTION METHODOLOGY
The results to be presented are based on SMD images obtained with the experimental platform introduced in Section II.
These images are classified based on multiple features considered simultaneously in the QCF previously described. This
vector-based decision approach is used to perform the classification of an image as defective (SMD component absent)
or nondefective (SMD component present). The six features
described in Section II are used as candidates for inclusion in
the QCF to evaluate the MSDM methods previously described.
The first step consisted of getting data from a particular type
of component present in the PCB inspected (i.e., the 805-V component). This component was selected because it was the most
common in the PCB inspected. The parameters of the six features were fine-tuned for the inspection of this component. The
frequency distributions of the six features for the defective and
nondefective populations were obtained next. The resulting distributions were tested for normality. Although some of the distributions did not pass the normality tests (Kolmogorov–Smirnov
and Chi–Square), the decision was made to not transform the
data. The reason behind this decision was that our aim was to
test the overall performance of the feature selection methods and
not the performance of the QCF.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

In order to conduct additional stepwise discriminant experiments, it was assumed that the full data set was composed
of only five potential features instead of six. For example,
the full set of features (energy, correlation, diffusion, fenergy,
, which represents
blob, and texture) is labeled
the original set of features to be examined in the experiment.
Under the new assumption, the following new feature sets of
size five may be denoted as new six independent experiments
,
,
,
,
,
. These six subsets and the original subset made
and
the seven groups of features used in the experiments compare
the efficiency of the five stepwise discriminant methods based
in the level of discrimination between the populations and the
number of features selected on the final subset.
Two statistics were created to measure the efficiency of the
stepwise methods. The statistics evaluate the performance of
the subset selected by the MSDM. The objective of these statistics centers on ranking each subset of features and identifying
the position of the subset selected by the MSDM. The statistics
are based on the three MER statistics previously described. The
first statistic is called MER criteria (MERc). MERc uses IER to
rank the subsets. When two or more subsets have the same IER
value, then the other two error rates—EAER and APER—are
used as primary and secondary tiebreakers, respectively. Then
each subset is ranked from 1 to , where is equal to the total
number of subsets, based on their respective MERc.
The second statistic is the error-type ratio (ETR). Generally,
in the manufacturing field, it is considerably more serious to
incur in a type II error (whose probability is denoted by ) in
the inspection phase than a type I error (denoted as ). In AVI
systems, type II errors are also considered more serious and usually have higher associated costs. Therefore, the proportion of
the expected number of type II errors to the overall expected errors should be examined in order to evaluate the performance
of a particular subset of features. The and values are estimated using the number of type I and type II errors obtained in
is computed
the training phase of the classifier. The ratio
and is referred to as the ETR. A large ETR value indicates that
the likelihood of type II errors is much greater than type I errors
and, therefore, a value less than one is desirable.
A. Numerical Results
The PCB used in the experiment is comprised of 281 elements of the 805-V component. The AVI system inspected 40
printed-circuit boards PCB for the two populations: present and
absent components. The data set obtained for the training phase
was based on the average of 30 runs and the remaining ten
runs were assigned to the inspection phase. Therefore, 281 data
points were available for the training phase and 2810 data points
for the inspection phase for each population.
The five multivariate stepwise discriminant methods mentioned in Section IV were tested with the data obtained by the
AVI system, this with the objective to compare the performance
of each method. The objective of this comparison involved determining which method gave the best subset: the subset with the
minimum MER using the minimum number of features. In hypothesis testing, this value is equivalent to (i.e., not including
a feature into the model when, in fact, the feature is relevant

403

TABLE III
MULTIVARIATE STEPWISE DISCRIMINANT SUMMARY

to discriminate between two populations). A common value of
used in hypothesis testing is 0.05. However, in our case, we
use higher levels of in each MSDM step to include or remove
one feature from the subset because we want to have more certainty that a feature is really relevant when it is included in the
subset or irrelevant when it is removed from the subset. Table III
presents the progression of the variable selection process of the
MSDM along with its associated Wilks’ Lambda and values
for the particular case of
. The five methods gave the
same subset of features; correlation, diffusion, fenergy and texfor any value of in the range of
–
. In
ture,
the example presented in Table III, no variables were removed
from the set of features once they entered the set.
In addition, the MER was calculated for the selected subset
and was compared to the full set of six features
. Table IV represents the ranking system created
to evaluate the performance of each subset. This table illustrates
the results of using the six features, providing a total of 63
different combinations of subsets. The measure used to rank
the different subsets was the MERc previously above. The ETR
was used as a tiebreaker when two subsets had the same MERc
values.
The previous table shows that the selected subset from the
performs better than the full set of
stepwise methods
features
because it is ranked in position 6 instead
of position 28 of the complete set. The significance of obtaining
is the decrease in the MER and the reducthe subset
with
tion in time and cost of acquiring two less features
respect to the complete set.
The summary of the results of the stepwise experiments
conducted on the other six groups of five features previously
,
,
,
,
described
, and
are presented in Table V. The five
methods of stepwise resulted in the same suggested subset for
each experiment.
Table VI shows the ranked positions for the complete set and
subset determined by the MSDM procedure for the seven experiments. The second column presents the number of features
for each complete set. The third column gives the number of
different subsets derived from the number of features given in
column 2. Rank 1 displays the position of each full set for all
possible combinations. Rank 2 reveals the location of the best
subset selected by all five stepwise methods. The last column
illustrates the difference between the ranks; this number represents the number of ranking error positions that the best subset
incremented or decreased regarding the full set.
Table VII illustrates that six of the seven subsets selected by
the stepwise methods resulted in less classification errors than

404

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

TABLE IV
SUBSET RANK

TABLE V
SUMMARY OF MSDM

TABLE VI
RANK POSITIONS USING MSDM

using the original set of features. It can be concluded that in
general, the proposed techniques use less features and render
less classification errors than when using the full set of features

TABLE VII
REDUCTION OF THE IER

available. This conclusion means that using fewer features resulted in lower IER values in the experiments. The values of the
IER are presented in Table VII.
Although Table VI shows that the MSDM renders a subset
of features providing better discrimination than the full set of
features, it also shows that the MSDM did not result in the best
subset. There are several reasons to explain this situation. The
first reason is the MSDM are based on the assumption that the
features values are from a multinormal population with equal
covariance matrices. In our case, the features do not meet this
assumption. The other reason is that the partial statistics used for
the inclusion or exclusion of features is based in maximizing the
statistical distance between the populations, not in minimizing
the MER. However, if there is a departure from the normality
assumptions, the relationship between statistical distance and
misclassification errors is not necessarily linear.

GARCIA et al.: AUTOMATED FEATURE SELECTION METHOD FOR VISUAL INSPECTION SYSTEMS

In this case, we decided to rank the performance of the subsets based in the MER instead of the statistical distance because of its practical importance in the inspection process. Finally, there is the issue of correlation; the SMDM may not reach
the “best” subset in any sense, except when the correlation between the features is zero, a situation that, in practice, hardly
exists. Thus, it is important to highlight that any feature selection process needs to consider the correlation between features
to obtain better results as shown in the previous examples. The
feature selection procedure introduced, while not perfect, represents an efficient way to handle the cross correlation exhibited
from the features being considered. For instance, Table II shows
that the values of the correlation between features are between
0.27 and 0.93.
VII. CONCLUSION
In this paper, the authors showed that using multivariate stepwise discriminant analysis methods can result in the selection of
reduced subsets of decision features for the inspection of SMD
components. These subsets of features perform usually better
than the complete set of decision features. The authors have also
shown that a careful selection of ranking metrics is essential for
the selection of features. In particular, our findings reveal that
the use of the Wilks’ Lambda method is a good alternative for
the selection of features. One of the main findings is that the
multivariate stepwise discriminant methods present a good option to automate the feature selection process in the development
and reconfiguration of automated visual inspection systems. The
authors believe that the feature selection method presented can
be extended to the sensor selection problem when the aim is to
recognize or discriminate between populations.
Although this research is still in its preliminary stages, the
results reveal the importance of having an efficient feature selection methodology for automating the generation of inspection algorithms. Obtaining an efficient feature selection methodology is the starting point for additional development in the
automated generation of inspection algorithms. While the results obtained in this paper support the proposed methodology
as a way to automate feature selection, these results also show
that there is much room for improvement. For instance, with a
methodology that takes more explicitly into account, the individual contributions of a single or of a combination of features
under different levels of correlation should be explored in the
future. Additional work should also include the refinement of
the methodology to explicitly include the time needed for feature selection as well as the refinement of the figure of merit for
the inclusion/exclusion of features in the resulting feature set.
Another fertile area for research is the development of classifiers that depart from the normality assumptions usually made
by the classifiers.
REFERENCES
[1] M. Arellano, “Analysis and development of a vector classification technique for SMD components,” M.S. thesis, Dept. Elect. Comput. Eng.,
Univ. Texas at El Paso, El Paso, 1999.
[2] M. J. Baxter, “Stepwise discriminant analysis in archaeometry: A Critique,” J. Archaeol. Sci., vol. 21, pp. 659–699, 1994.
[3] K. N. Benk, “Forward and backward stepping in variable selection,” J.
Stat. Comput. Simul., pp. 177–185, 1980.

405

[4] R. Debouk, S. Lafortune, and D. Teneketzis, “On an optimization
problem in sensor selection,” Discrete Event Dynamic Syst.: Theory
Appl., vol. 12, pp. 417–445, 2002.
[5] Y. Ding, P. Kim, D. Ceglarek, and J. Ji, “Optimal sensor distribution
for variable diagnosis in multistation assembly processes,” IEEE Trans.
Robot. Autom., vol. 19, no. 4, pp. 543–554, Aug. 2003.
[6] N. Draper and H. Smith, Applied Regression Analysis, 3rd ed. New
York: Wiley, 1998.
[7] R. Duda, P. Hart, and D. Stork, Pattern Classification, 2nd
ed. Hoboken, NJ: Wiley, 2001.
[8] M. Feld, “Simplification of automated optical inspection systems for
assembled circuit boards,” SMT Expr., vol. 3, no. 4, 2001.
[9] L. M. Fraleigh, M. Guay, and J. F. Forbes, “Sensor selection for modelbased real-time optimization: Relating design of experiments and design
cost,” J. Process Control, vol. 13, pp. 667–678, 2003.
[10] J. J. Glenn, “Integer programming methods for normalization and
variable selection in mathematical programming discriminant analysis
models,” J. Oper. Res. Soc., vol. 50, no. 10, pp. 1043–1053, Oct. 1999.
[11] I. Gunyon, S. Gunn, M. Nikravesh, and L. Zadeh, Variable/Feature Selection and Ensemble Learning: A Dual View. New York: SpringerVerlag, 2005, ch. Feature Extraction, Foundations and Applications.
[12] A. Gupta, T. Logan, and J. Chen, “A variable selection technique in
discriminant analysis with application in marketing data,” J. Statist.
Comput. Simulation, vol. 63, no. 2, pp. 187–199, 1999.
[13] J. Hair, R. Anderson, R. Tatham, and W. Black, Multivariate Data Analysis With Readings, 4th ed. Englewood Cliffs, NJ: Prentice-Hall, 1995.
[14] T. Hastie, R. Tibshirani, and J. Firedman, The Elements of Statistical Learning, Data Mining, Inference and Prediction. New York:
Springer, 2001.
[15] A. Jain, R. Duin, and J. Mao, “Statistical pattern recognition: A review,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 1, pp. 4–37, Jan.
2000.
[16] R. I. Jenrich, “Stepwise discriminant analysis,” in Statistical Methods
for Digital Computers, 1997, ch. 8, pp. 576–95.
[17] S. Jiang, R. Kumar, and H. Garcia, “Optimal sensor selection for discrete-event systems with partial observation,” IEEE Trans. Autom. Control, vol. 48, no. 3, pp. 369–381, Mar. 2003.
[18] R. A. Johnson and D. W. Wichern, Applied Multivariate Statistical Analysis, 3rd ed. Upper Saddle River, NJ: Prentice-Hall, 1996.
[19] S. Kachigan, Multivariate Statistical Analysis. New York: Radius
Press, 1982.
[20] R. Khattree and D. Naik, Multivariate Data Reduction and Discrimination With SAS Software. New York: Wiley, 2000.
[21] E. Kinney and D. Murphy, “Comparison of the ID3 algorithm versus discriminant analysis for performing feature selection,” Comput. Biomed.
Res., vol. 20, no. 5, pp. 467–476, Oct. 1987.
[22] W. J. Krzanowski, Principles of Multivariate Analysis, a User’s Perspective. Oxford, U.K.: Oxford Science, 1998.
[23] C. Le, Applied Categorical Data Analysis. New York: Wiley, 1998, pp.
127–147.
[24] K. Lee, N. Sha, E. Dougherty, M. Vannucci, and B. Mallick, “Gene selection: A Bayesian variable selection approach,” Bioinformatics, vol.
19, no. 1, pp. 90–97, 2003.
[25] G. McCabe, “Computations for variable selection in discriminant analysis,” Technometrics, vol. 17, no. 1, Feb. 1975.
[26] D. Montgomery, E. Peck, and V. Geoffrey, Introduction to Linear Regression Analysis, 3rd ed. New York: Wiley, 2001.
[27] T. Pavlenko, “On feature selection, curse-of-dimensionality and error
probability in discriminant analysis,” J. Statist. Planning Inference, vol.
115, no. 2, pp. 565–584, Aug. 2003.
[28] A. Rencher, “The contribution of individual variables to hotelling T ,
wilks and R ,” Biometrics, vol. 49, pp. 479–489, Jun. 1993.
[29]
, Methods of Multivariate Analysis, 2nd ed. New York: Wiley,
2002.
[30] A. Rshirsagar, Multivariate Analysis. New York: Marcel Dekker,
1972.
[31] C. Rutter, V. Flack, and P. Lanchenbruch, “Bias in error rate estimates
in discriminant analysis when stepwise variable selection is employed,”
Commun. Statistics–Simulation Comput., vol. 20, no. 1, pp. 1–22, 1991.
[32] S. Sampatraj, K. Abhishek, and Y. Ding, “A survey of inspection
strategy and sensor distribution studies in discrete-part manufacturing
processes,” Ph.D. dissertation, Dept. Ind. Eng., Texas A&M Univ.,
College Station, 2005.

1

406

IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING, VOL. 3, NO. 4, OCTOBER 2006

[33] O. Snorrason and F. Garber, “Evaluation of nonparametric discriminant
analysis techniques for radar signal feature and extraction,” Opt. Eng.,
vol. 31, no. 12, pp. 2608–2617, Dec. 1992.
[34] M. Tatsuoka and P. Lohnes, Multivariate Analysis, 2nd ed. New York:
Macmillan, 1998.
[35] J. R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector classification of SMD images,” J. Manuf. Syst., vol. 22, no. 4, pp. 265–282,
2004.
[36] L. Williams, “Development of a feature selection methodology for automated visual inspection systems,” Dept. Thesis Coursework-Ind. Eng.,
Arizona State Univ., Tempe, 2004.

J. René Villalobos received the B.S. degree in industrial–mechanical engineering from the Chihuahua
Technological Institute in Mexico, Chihuahua,
Mexico, in 1982, the M.S. degree in industrial engineering from the University of Texas at El Paso in
1987, and the Ph.D. degree in industrial engineering
from Texas A&M University, College Station, in
1991.
Currently, he is an Associate Professor in the Department of Industrial Engineering with the Ira A.
Fulton School of Engineering at Arizona State University, Tempe. His research interests are in the areas of logistics, automated
quality systems, manufacturing systems, and applied operations research.

Hugo C. Garcia received the B.S. degree in industrial and systems engineering from the Instituto
Tecnológico y de Estudios Superiores de Monterrey,
Campus Ciudad Juárez, Juarez, Mexico, in 2000; the
M.B.A. degree from the Universidad Autónoma de
Ciudad Juárez, Juarez, in 2002; and the M.S. degree
in statistics from The University of Texas at El Paso
in 2003. He is currently pursing the Ph.D. degree in
industrial engineering at Arizona State University,
Tempe, and the M.S. degree in industrial engineering
at Instituto Tecnológico de Ciudad Juárez, Juarez.
His main research interest is in automated visual inspection systems.

George C. Runger received the B.S. degree in industrial engineering from Cornell University, Ithaca, NY,
and the Ph.D. degree in statistics from the University
of Minnesota, Minneapolis.
Currently, he is a Professor in the Department of
Industrial Engineering at Arizona State University,
Tempe. His research is on real-time monitoring and
control, data mining, and other data-analysis methods
with a focus on large, complex, and multivariate data
streams. His work is funded by grants from the National Science Foundation and corporations. He was
a Senior Engineer with IBM, Tucson, AZ.

644

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

Tangent Hyperplane Kernel Principal
Component Analysis for Denoising
Joon-Ku Im, Daniel W. Apley, and George C. Runger

Abstract— Kernel principal component analysis (KPCA) is
a method widely used for denoising multivariate data. Using
geometric arguments, we investigate why a projection operation
inherent to all existing KPCA denoising algorithms can sometimes cause very poor denoising. Based on this, we propose
a modification to the projection operation that remedies this
problem and can be incorporated into any of the existing
KPCA algorithms. Using toy examples and real datasets, we
show that the proposed algorithm can substantially improve
denoising performance and is more robust to misspecification
of an important tuning parameter.
Index Terms— Denoising, kernel, kernel principal component
analysis (KPCA), preimage problem.

I. I NTRODUCTION

K

ERNEL principal component analysis (KPCA) is a
nonlinear generalization of linear principal component
analysis (PCA) and is widely used for denoising multivariate
data to identify underlying patterns and for other purposes [1].
Let x be a multivariate observation vector having n components, and suppose the data consist of N such observations {x1 ,
x2 , . . ., x N }. In KPCA, one defines a set of M features {φ1 (x),
φ2 (x), . . ., φ M (x)}, each of which is some appropriately
chosen nonlinear function of x, and forms a feature vector
φ(x) = [φ 1 (x), φ 2 (x), . . ., φ M (x)] . One then conducts PCA
on the set of feature vectors {φ(x1 ), φ(x2 ), . . ., φ(x N )}. If
we consider Rn the “input space” in which x lies and R M the
“feature space” in which φ(x) lies, we can view the feature
map as φ: Rn → R M . In practice, the feature map is usually
defined implicitly by some kernel function for computational
reasons [2], [3].
Several different KPCA denoising methods have been proposed [2]–[9], which we review and contrast more thoroughly
in Section II. Their commonality is that they all share a general
framework that involves two primary operations, which we
refer to as projection and preimage approximation. By projection, we mean that, to denoise an observation x via KPCA,
one projects its feature vector φ(x) onto the principal subspace
in the feature space, as illustrated in Fig. 1. The principal
Manuscript received December 2, 2010; accepted December 28, 2011. Date
of publication February 21, 2012; date of current version March 6, 2012.
This work was supported in part by the National Science Foundation under
Grant CMMI-0826081 and Grant CMMI-0825331.
J.-K. Im and D. W. Apley are with the Department of Industrial Engineering
and Management Sciences, Northwestern University, Evanston, IL 60208 USA
(e-mail: ijk@u.northwestern.edu; apley@northwestern.edu).
G. C. Runger is with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ 85287 USA
(e-mail: george.runger@asu.edu).
Digital Object Identifier 10.1109/TNNLS.2012.2185950

subspace is defined as the span of the dominant eigenvectors
of the M × M covariance matrix of {φ(x1 ), φ(x2 ), . . ., φ(x N )}.
The projected feature vector, which we denote by Pφ(x),
is a point in the feature space. In order to denoise x, we
must find a point x̂ in the input space that best corresponds
to Pφ(x). The exact preimage of Pφ(x) (under the map φ)
generally does not exist. That is, there generally exists no x̂
such that φ(x̂) = Pφ(x) exactly. This is the so-called preimage
problem in KPCA denoising [3]. By preimage approximation,
we mean that one must find a value x̂ that minimizes some
appropriately defined measure of dissimilarity between x̂ [or
φ(x̂)] and Pφ(x).
All of the aforementioned KPCA denoising methods employ
the same orthogonal projection operation. Their distinctions,
which we discuss in more detail in Section II, lie in how
they perform the preimage approximation. For example, the
original method of Schölkopf et al. [3] finds the point x̂ that
minimizes a dissimilarity measure defined as the Euclidean
distance between φ(x̂) and Pφ(x). That is, they take, x̂ =
arg minz ||φ(z) − Pφ(x)||2 as the denoised x, where the norm
is the standard Euclidean 2-norm. Fig. 2 illustrates the results
of this method for a toy example with N = 100 observations
of n = 2 variables, using a polynomial kernel of degree 2.
Fig. 2(a) shows the original data, and Fig. 2(b) shows the
denoised data using the method of Schölkopf et al. [3]. The
salient feature of Fig. 2(b) is that the points around the middle
of the curve (e.g., x A ) are not denoised nearly as well as those
near the ends of the curve (e.g., x B ).
In this paper, we investigate this phenomenon and show that
it is a consequence of the projection operation, as opposed to
the preimage approximation. Consequently, this undesirable
characteristic (much poorer denoising in some regions of
the input space than in other regions) is inherent to all the
aforementioned KPCA algorithms, and the solution to this
problem that we propose can be used to enhance any of the
these algorithms.
An intuitive explanation for the poor denoising in some
regions is as follows. Consider the image of Rn under the
map φ, which is the subset of the feature space for which
each point has an exact preimage. This subset having exact
preimages forms a manifold in R M, which we refer to as the
full manifold. As operations in R M, we can view projection
onto the principal subspace as taking the point φ(x) away from
the full manifold to obtain Pφ(x), and preimage approximation
as taking the projection Pφ(x) back to some point on the full
manifold that is as similar to Pφ(x) as possible, to obtain
φ(x̂), or equivalently x̂. As we will show, how well a point

2162–237X/$31.00 © 2012 IEEE

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

645

and the covariance matrix of the feature vectors as
S = N −1

N



˜  .
˜
φ̃(x j )φ̃ (x j ) = N −1 

j =1

Fig. 1. Denoising an observation x involves two operations: projection of its
feature vector φ(x) onto the principal subspace and preimage approximation
to find an x̂ that minimizes some measure of dissimilarity between x̂(or φ(x̂))
and Pφ (x).

x is denoised is strongly influenced by how far from the full
manifold its projection Pφ(x) falls. For points such as x A in
Fig. 2, the projection step takes them quite far away from the
full manifold. Hence, Pφ(x A ) must be moved a large distance
to get back to the full manifold in the preimage approximation
step, which results in poor denoising. On the other hand, for
points such as x B , the projection step keeps them close to the
full manifold, which results in more effective denoising.
We propose a modification to the projection step which
is designed to keep the “projected” feature vector closer to
the full manifold, thereby improving denoising performance,
especially at points such as x A in Fig. 2, which are more
difficult to denoise. We refer to the approach as the tangent
hyperplane KPCA algorithm for reasons that will become
apparent later. The modified projection can be used in conjunction with the preimage approximation algorithms of any
of the aforementioned KPCA approaches.
The remainder of this paper is organized as follows.
Section II briefly reviews the general KPCA approach and various preimage approximation algorithms. Section III provides
a geometric explanation of why certain points such as x A in
Fig. 2 are denoised poorly using the regular projection step in
KPCA. In Section IV, we introduce our tangent hyperplane
KPCA approach as a solution to this problem. Section V
presents denoising results for the toy example of Fig. 2 and
for real image datasets, together with a discussion of the characteristics of the approach. Section VI concludes this paper.

II. R EVIEW OF P RIOR KPCA W ORK AND THE
P REIMAGE P ROBLEM
Regular (linear) PCA on the n-dimensional data {x1 , x2 , …,
x N } finds the set of orthonormal basis vectors along which the
components of x have largest variance. These basis vectors are
the eigenvectors of the n ×n (sample) covariance matrix of the
data [10]–[12]. KPCA is PCA on the M-dimensional feature
vectors, {φ(x1 ), φ(x2 ), …, φ(x N )}, where M >> n usually.
Define thecentered feature vector as φ̃(x) = φ(x) − φ̄ where
φ̄ = N −1 Nj=1 φ(x j ), the feature matrix as
˜ = [ φ̃(x1 ) φ̃(x2 ) · · · φ̃(x N )] 


KPCA therefore involves the eigenvalues and eigenvectors of
the M × M matrix S .
Given that direct calculation of the eigenvectors of S is
usually computationally infeasible (M is often large), the novel
enabling aspect of KPCA is the kernel trick, by which we
can solve this eigenproblem without actually computing the
high-dimensional features or their covariance matrix S . This
is possible because in KPCA the feature map φ is defined
implicitly via its inner products φ(x),φ(y) = φ  (x)φ(y) =
K (x,y) for some appropriately chosen positive definite kernel
function K (•,•) (see [2], [3]; for comprehensive treatments of
kernel functions, see [13]), and the inner products are all that
are required in the computations. Common choices for the
kernel are the polynomial kernel K (x,y) = (1 + x y)d of
degree d, and the Gaussian radial basis function (RBF) kernel
K (x,y) = exp(–||x − y||2/ρ) where the parameters d and ρ
are chosen by the users. A polynomial kernel of degree 2 was
used in the Fig. 2 example.
More specifically, define the centered kernel function
K̃ (x, y) = φ̃(x), φ̃(y) = φ(x) − φ̄, φ(y) − φ̄
= φ(x), φ(y) − φ(x), φ̄ − φ(y), φ̄ + φ̄, φ̄
N
N


= K (x, y) − N −1
K (x, xi ) − N −1
K (y, xi )
i=1

+ N −2

N
N 


i=1

K (xi , x j )

i=1 j =1

= K (x, y) − N −1 1 Kx − N −1 1 Ky + N −2 1 K1
where 1 denotes a column vector of ones, and Kx =
[K (x,x1), K (x,x2), . . . , K (x,x N )] . Also define the centered
˜
˜  , whose (i , j )th element is K̃ i j =
kernel matrix K̃ = 
K̃ (xi , x j ). Note that K̃ is an N × N matrix that can be
calculated via evaluations of the kernel function. This is computationally significant, because the eigenvector/eigenvalue
pairs {(vk , λk ): k = 1, 2, . . ., L 0 } of S with nonzero
eigenvalue (we refer to such an eigenvector as a nonzero
eigenvector, and assume there exist L 0 of them) and the
nonzero eigenvector/eigenvalue pairs {(α k = [α k,1 , α k,2 , …,
α k,N ] , ηk ): k = 1, 2, . . . , L 0 } of K̃ are closely related (see
N
˜ αk =
˜
[1] for details): vk = 
i=1  (xi ) αk,i , and ηk =
Nλk , where the convention for scaling the eigenvectors is to
−1/2
take ||vk || = 1, which implies ||α k || = (Nλk )−1/2 = ηk .
Using this, one can solve the eigenproblem for the M × M
matrix S by solving the much less computationally expensive
eigenproblem for the N × N matrix K̃ (N << M typically).
As discussed in the introduction, the existing KPCA denoising methods share a common projection operation. That is,
given an observation x, they find (implicitly) the orthogonal
projection
˜
P (x)
=

L

k=1

φ̃(x), vk vk

646

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

Fig. 2. (a) Original data (white markers) distributed approximately along a quadratic curve (solid curve). (b) Denoised data (white markers) obtained using
the method of [3]. Points such as x A near the middle of the curve are denoised less effectively than points such as x B near the ends of the curve.

of φ̃(x) onto the principal subspace spanned by the L principal
eigenvectors {v1 , v2 , . . ., v L } of S . This is followed by the
preimage approximation for finding the x̂ that is most similar
to P φ̃(x), which is where the KPCA denoising methods
mentioned in the introduction differ. Specifically, they differ
in how they define the dissimilarity measure and/or how they
implement the optimization algorithm to minimize it, including
the variables over which they optimize.
In [2] and [3], the authors suggested minimization of the
Euclidean distance
||φ̃(z) − P φ̃(x)||2

(1)

over z. In other words, the dissimilarity measure between x̂ (or
φ̃(x̂)) and P φ̃(x) is the Euclidean distance between φ̃(x̂) and
P φ̃(x) in the feature space. The computational kernel trick
that is central to all KPCA methods is based on the fact that
for any x ∈ Rn , z ∈ Rn , and nonzero eigenvector vk , we can
evaluate the following inner products via the kernel:


N

φ̃(x), vk  = φ̃(x),
φ̃(xi ) αk,i
i=1

=

N




φ̃(x), φ̃(xi ) αk,i = K̃x α k

i=1

and


φ̃(z), P φ̃(x) = φ̃(z),

L



φ̃(x), vk vk

k=1

=
=
=

L

k=1
L


φ̃(z), vk φ̃(x), vk 




(K̃z α k )(K̃x α k )

k=1

K̃z [ α 1

· · · α L ][ α 1 · · · α L ] K̃x

where K̃x = [ K̃ (x, x1 ), K̃ (x, x2 ), · · · , K̃ (x, x N )] . Thus, the
quantity to be minimized (with respect to z, for a given x
to be denoised) in the preimage approximation of [3] can be
written as
||φ̃(z) − P φ̃(x)||2
= φ̃(z), φ̃(z) − 2φ̃(z), P φ̃(x) + P φ̃(x), P φ̃(x)
= K̃ (z, z) − 2βz βx + βx βx
where





β x = v1 v2 · · · v L φ̃ (x) = α 1 α 2 · · · α L K̃x

is the vector of principal components scores of φ̃(x).
In [2] and [3], the authors suggested using a fixed-point
iterative algorithm, related to a gradient descent algorithm,
to minimize (1) with respect to z over the entire input space
Rn . Takahashi and Kurita [4] modified the fixed-point iterative
algorithm by updating certain weighting coefficients at each
iteration, based on updating the feature vector to be projected
onto the principal subspace. A fixed point of their algorithm is
more consistent with minimizing ||φ̃(z) − P φ̃(z)||2 . Teixeira
et al. [5] proposed an alternative initial guess for the fixedpoint iterative algorithm.
While the preceding methods share a common dissimilarity
measure (1), others use different dissimilarity measures. Kwok
and Tsang [6] proposed a method akin to multidimensional
scaling. To denoise x, they found x̂ so that the distances
||xi − x̂|| (i = 1, 2, . . . , n) are as close as possible to
the distances ||φ̃(xi ) − P φ̃(x)||2 (i = 1, 2, . . . , n). Their
dissimilarity measure is the distance between the vector of
||xi − x̂||2 ’s and the vector of ||φ̃(xi ) − P φ̃(x)||2 ’s for
values of i such that φ̃(xi ) is a neighbor of P φ̃(x), and
to reduce computational expense they restricted x̂ to being
a linear combination of the nearest neighbors. Bakır et al.
[7] suggested a kernel regression method to estimate the
map (P φ̃(x)) = [1 (P φ̃(x)), 2 (P φ̃(x)), . . . , n (P φ̃(x))] ,

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

647

(a)

(b)

Fig. 3. (a) Data with noise in the input space. The solid curve is the true pattern that the data would follow with no noise. (b) Centered feature vectors and
true pattern mapped to the feature space.

(a)

(b)

Fig. 4. (a) Result of [3] in the input space. Approximate preimages (black markers). (b) Result of [3] in the feature space. Projections (gray marker) of
the features of the noisy data (white marker) using the simple 3-D feature and the method of [3]. The approximate preimages in (a) are supposed to best
correspond to the projections in (b). We can observe that x B is denoised much more than x A .

where one views the components of P φ̃(x) as the predictor
variables and the components of x̂ as the response variables.
They assume the kernel regression model  j (P φ̃(x)) =
N
j
i=1 βi κ(P φ̃(x), P φ̃(xi )), where κ is a different kernel
operator, and minimize the sum of squared regression residuN
j
||xi − (P φ̃(xi ))||2 with respect to the βi  s. Their
als i=1
approach applies mainly to the situation in which noiseless
observations are available for training. Zheng et al. [8] noted
that a denoised observation of x obtained by most of the
preceding methods
 N is a weighted sum of {x1 , x2 , . . ., x N }.
That is, x̂ =
i=1 wi xi for some weights w1 , . . ., w N .

N
wi xi ) −
Hence, they used a dissimilarity measure ||φ̃( i=1
2
P φ̃(x)|| , and chose the weights to minimize this under certain
constraints.
Nguyen and De la Torre [9] approached the problem in a
different way. They proposed to solve x̂ = arg minz ||φ̃ (z) −
P φ̃ (z) ||2 + C · d(z, x) where d(z,x) is any distance measure
between z and x, and C is a weight that is chosen to balance
between the two terms. Although this method slightly deviates
from the general framework we have portrayed, it is similar
in the sense that it still involves an orthogonal projection onto
the principal subspace.

648

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

(a)

Fig. 5. Feature vectors for the data (open dots), true pattern (solid curve),
and full manifold (surface) from the toy example, in the feature space. All
feature vectors must reside exactly in the full manifold.

III. G EOMETRIC I NTERPRETATION OF
KPCA D ENOISING P ERFORMANCE
In this section, we provide a geometric interpretation of the
inconsistent denoising performance seen in Fig. 2 (i.e., points
such as x A are denoised less effectively than points such as x B )
and relate it to the orthogonal projection procedure common
to all of the KPCA methods. We argue that the orthogonal
projection has undesirable properties and that modifying this
procedure (as described in the next section) can substantially
improve denoising performance.
To illustrate, consider again the toy example introduced in
Section I, for which the data are distributed along a quadratic
curve in R2 with Gaussian noise added. In order to better
visualize the feature space (to facilitate the illustration), instead
of using a feature map induced by a polynomial kernel, we
use the simple feature map φ(x) = (x 1 , x 2 , x 12 ) which is
sufficiently rich to capture the quadratic pattern. Fig. 3(a)
shows the noisy data and the true quadratic pattern in the
input space, and Fig. 3(b) shows the same data and true pattern
mapped to the 3-D (centered) feature space. The data points
are the feature vectors, and the solid curve is the image of the
true pattern under the feature map. Ideally, we would like the
denoised observation to lie close to the solid curve.
Consider the orthogonal projection P φ̃(x) of a feature
vector onto the principal subspace. For the toy example with
L = 2, the projections of the feature vectors are plotted in
Fig. 4(b) and their approximate preimages using the method of
[3] are plotted in Fig. 4(a). As with linear PCA, we would hope
that the projections in the feature space are close to the true
pattern. However, from Fig. 4(b), the projections of points near
the middle of the curve are much farther from the true pattern
than are the projections of points near both ends of the curve.
This is consistent with the variable denoising performance that
we see in Fig. 4(a) and discussed in the context of Fig. 2.

Fig. 6. (a) Method of [3] in the feature space. Depiction in the feature space
of the principal projection P φ̃(x) and preimage projection φ̃(x̂) for points x A
and x B , together with the full manifold and principal subspace. (b) Expanded
callout for x A . (c) Expanded callout for x B .

The reason behind the poor denoising of x A and the good
denoising of x B becomes more apparent when we visualize the
full manifold and the projection and preimage approximation
steps in the feature space. Recall that the full manifold φ̃(Rn )
is the image of Rn under the feature map, which is an ndimensional manifold in M-dimensional space. Fig. 5 plots
the full manifold for the toy example. Notice that the feature
vector for any data point must lie in this full manifold, regardless of the level of noise. More generally, the feature vector
for any point in Rn must lie on the full manifold, and the full
manifold is the set of all points in the feature space that have
exact preimages. Consequently, after finding the projection
P φ̃(x) onto the principal subspace, we can view φ̃(x̂) from the
preimage approximation x̂ = arg minz ||φ̃(z) − P φ̃(x)||2 of [3]
as an orthogonal projection of P φ̃(x) onto the full manifold
in the feature space. To distinguish between these two feature
space projections, we refer to P φ̃(x) as the principal projection
and φ̃(x̂) as the preimage projection.
Fig. 6 shows the principal projections and preimage projections for points x A and x B , together with the full manifold
and the principal subspace. The reason why x A is denoised less
effectively than x B has much more to do with the principal
projection than the preimage projection. Because of the local

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

geometry around φ̃(x A ) of the full manifold and the principal
subspace [see Fig. 6(b)], the principal projection P φ̃(x A ) for
the point x A falls quite far away from the full manifold. That is
to say, it is quite far from having a preimage. Hence, in order
to get back to the full manifold when finding the preimage
projection, we must choose a point φ̃(x̂ A ) that is quite far
from the principal subspace. In turn, this means that φ̃(x̂ A ) is
quite far from the image of the true pattern, which means that
the denoised point x̂ A is quite far from the true pattern (i.e.,
poor denoising).
The situation for the point x B is very different. Because
of the local geometry around φ̃(x B ) [see Fig. 6(c)], the
principal projection P φ̃(x B ) remains relatively close to the
full manifold, so that the preimage projection φ̃(x̂ B ) remains
close to the principal subspace. The result is that x̂ B is close
to the true pattern (i.e., good denoising).

IV. TANGENT H YPERPLANE M ETHOD FOR
KPCA D ENOISING
The arguments in the previous section indicate that the
denoising performance is strongly influenced by the projection
step. The geometric interpretation in Fig. 6 motivates our
proposed modification to the projection step, which can be
incorporated into the preimage approximation criteria for any
of the KPCA denoising algorithms discussed previously. For
each point x to be denoised, instead of projecting orthogonally
onto the principal subspace, we incorporate information on
the local geometry of the full manifold in the neighborhood
of φ̃(x), so that the principal projection remains closer to the
full manifold. This is accomplished as follows.
First note that any point
 L in the principal subspace can be
bk vk for some b = [b1 , b2 , …,
represented as P φ̃(x) + k=1
b L ] ∈ R L . For any x, we define the tangent hyperplane for
φ̃(x) as the n-dimensional hyperplane (in the M-dimensional
feature space) that is tangent to the full manifold at φ̃(x).
More precisely, the
tangent hyperplane for φ̃(x) is the set of
all points φ̃(x) + nj =1 a j u j for a = [a1 , a2 , …, an ] ∈ Rn ,
where
uj =

∂ φ̃(x) 
=
∂x j

∂ φ̃ 1 (x) ∂ φ̃ 2 (x)
∂x j
∂x j

···

∂ φ̃ M (x)
∂x j

	

.

In the following, we refer to the quantity that will replace
P φ̃(x) in the projection step as the principal projection,
although technically it is not a projection. Rather, the principal
projection in our algorithm is defined as the point on the
principal subspace that is closest to the tangent hyperplane
for φ̃(x). This is equivalent to finding the minimum distance
between the two subspaces (the tangent hyperplane and the
principal subspace). As in [14], which computes the minimum
distance between two hyperplanes in a different context, the
solution is obtained by finding the coefficient vectors a and b
to minimize the distance between
P φ̃(x) +

L

k=1

bk vk and φ̃(x) +

n

j =1

ajuj.

649

Then we take the principal projection to be
PT φ̃(x) = P φ̃(x) +

L


bk∗ vk

(2)

k=1

where a∗ = [a1∗ , a2∗ , …, an∗ ] and b∗ = [b1∗ , b2∗ , …, b∗L ]
denote the coefficient vectors that minimize the distance. The
subscript T on the projection operator indicates it is for the
tangent hyperplane method.
The solution is obtained by defining U = [u1 , u2 , …, un ] and
V = [v1 , v2 , …, v L ], the sets of basis vectors for the tangent
hyperplane and for the principal subspace, respectively. The
optimal coefficient vector minimizes


 

2





 φ̃(x) + Ua − P φ̃(x) + Vb 




 

 


 a 
2


U,
−V
=

φ̃(x)
−
P
φ̃(x)
+


b 


2







= 
 φ̃(x) − P φ̃(x) + Ac

with respect to c = [a , b ] , and A = [U, −V], the solution
to which is the standard linear least squares solution.
Because the solution c∗ may not be unique (if the columns
of A are linearly dependent) or may be poorly conditioned (if
the columns of A are nearly linearly dependent), we add a
regularization penalty and choose c∗ so that
c∗ = arg min ||(φ̃(x) − P φ̃(x)) + Ac||2 + δ||c||2

(3)

c

for some small user-selected δ as in [14]. In Section V, we
compare the results with different levels of δ and discuss
its appropriate choice. The regularization penalty has other
intuitive appeal. It penalizes
 solutions for which the tangent
hyperplane point φ̃(x)+ nj =1 a j u j is further away from φ̃(x)
L
bk vk is furand/or the principal subspace point P φ̃(x)+ k=1
ther away from P φ̃(x). Hence, in the extreme case of δ → ∞,
our principal projection coincides with the standard principal
projection P φ̃(x). Moreover, the regularization penalty keeps
the solution well defined in the case where A is singular. For
example, in the special case of linear PCA [which corresponds
to the feature map φ(x) = x], the tangent hyperplane is all of
Rn , so that the distance is zero between any point on the
principal subspace and the tangent hyperplane. Including the
regularization penalty ensures that the solution is unique and
coincides with the standard linear PCA solution in this case.
It is straightforward to show that the solution to (3) is
c∗ = (A A + δI)−1 A (P φ̃(x) − φ̃(x))

(4)

from which we obtain our principal projection as defined in
(2). One can then use the preimage approximation criterion
from any of the existing KPCA denoising methods to find
the approximate preimage of PT φ̃(x), which serves as the
denoised point x̂.
It is important to note that the computational kernel trick can
be used in all steps of the algorithm. Hence, no calculations
need to be performed in the M-dimensional feature space. The
penalized least squares solution for the principal projection
PT φ̃(x) requires only inner products in the feature space,

650

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

and 6), x A is denoised substantially better, and x B is denoised
slightly better. In fact, x A is now denoised as well as x B .
The reason for the dramatic improvement for the denoising
of points such as x A is illustrated geometrically in Fig. 7(a),
which also shows the tangent hyperplanes for the two points
φ̃(x A ) and φ̃(x B ). The approach forces the principal projection
to be close to the tangent hyperplane [compare PT φ̃(x A ) in
Fig. 7(b) with P φ̃(x A ) in Fig. 6(b)], which also keeps it close
to the full manifold. If the principal projection is close to the
full manifold, then it is close to having an exact preimage,
so that its preimage projection φ̃(x̂) is more likely to remain
close to the principal subspace. The result is that the denoised
point is more likely to be close to the true pattern.
V. R ESULTS AND D ISCUSSION

Fig. 7. Proposed method in the feature space. Geometric depiction of the
tangent hyperplane method for denoising points x A and x B . Forcing the
principal projection PT φ̃(x) to be close to the tangent hyperplane also keeps
it close to the full manifold and close to having an exact preimage. Notice
that because the preimage projections φ̃(x̂ A ) and φ̃(x̂ B ) are so close to their
respective principal projections PT φ̃(x̂ A ) and PT φ̃(x̂ B ), it is difficult to
visually distinguish them in the figure. (b), (c) Expanded callouts of the two
regions in (a).

and the Appendix shows how all these can be calculated via
the kernel trick. Because the principal projection is a linear
combination of the eigenvectors, the kernel trick can be applied
as usual in any of the preimage approximation approaches.
Regarding the computational complexity, because the tangent hyperplane method simply modifies the projection step of
existing KPCA methods, its computational expense is the same
as that of the existing methods but with the additional expense
of the modified projection step. The most computationally
expensive part of the modified projection step is the inversion
of (n + L) × (n + L) matrix A A + δI. This is generally much
less expensive than the remainder of the KPCA algorithm
(especially if one uses a matrix inversion algorithm that takes
advantage of the orthogonality of the columns of A that are
associated with V). Hence, the modified projection step of the
tangent hyperplane method does not substantially increase the
computational expense of the entire KPCA algorithm.
In Fig. 7, we illustrate the tangent hyperplane method for
the same toy example with 3-D feature space. The preimage
approximation is still that of [3]. Relative to the denoising
results using the conventional orthogonal projection (Figs. 4

In this section, we apply the proposed tangent hyperplane
method (using the preimage approximation criterion of [3])
to three different datasets: the toy data that we have used
to illustrate the approach and two different image datasets
in which each multivariate observation represents a different
image. For the toy dataset, we use a feature map consisting
of all polynomial terms of degree ≤ 2, which corresponds
to the kernel K (x, y) = (1 + x y)2 . For the image datasets,
we used the Gaussian RBF kernel K (x,y) = exp(–||x −
y||2 /ρ) with ρ = nc, where c equals 0.5 times the average
of component variances, which we chose because it is more
appropriate than the polynomial kernels on both methods. We
compare the tangent hyperplane approach with that of [3].
As we have discussed, the tangent hyperplane approach is
a modification of the principal projection step that can be
used with any of the preimage approximation criteria. For
the preimage approximation, we used the quasi-Newton BFGS
algorithm to minimize ||φ̃(z)− P φ̃(x)||2 or ||φ̃(z)− PT φ̃(x)||2 .
A. Results for the Toy Example
The data for the toy example were generated by standardizing a simulated training set of size N = 100 bivariate observations of the random vector x = [x 1 , x 2 ] = [t, t 2 ] + [w1 , w2 ] ,
where t follows a uniform distribution over the interval [−1,
1], and w1 and w2 are independent Gaussian random variables
with zero mean and standard deviation 0.1. We can view the
quadratic curve {[t, t 2 ] : t ∈ [−1, 1]} as the true pattern, and
each observation x as a random point along the true pattern
plus Gaussian noise. Fig. 8(a) shows the denoising result using
the method of [3], which differs slightly from Fig. 4(a) because
of slightly different choice of feature map: φ(x) = (x 1 , x 2 , x 12 )
in Fig.
√ 4(a)
√ versus the √full quadratic feature map φ(x) =
(1, 2x 1 , 2x 2 , x 12 , x 22 , 2x 1 x 2 ) in Fig. 8(a). Fig. 8(b) shows
the denoising result using the proposed tangent hyperplane
method. For both methods, L = 4 principal eigenvectors were
retained. L = 4 provided the best denoising for both methods
(results for other L are omitted for brevity).
Comparing Fig. 8(a) and (b), it is clear that for this example
the modified principal projection of the tangent hyperplane
method provides much better denoising than the conventional
orthogonal principal projection. We also conducted simulations for a modified toy example with higher dimensional data.

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

651

(a)
(a)

2

(b)

δ 0 =10−4
RMS Distance

Method of [3]
Our Proposed Method

δ 0 =1

1.5

1

0.5

0
0

50

100

150
L

200

250

300

(b)
3.5

Method of [3]
Our Proposed Method

Fig. 8. For the toy example, a comparison of the denoising results using
(a) the method of [3] and (b) the tangent hyperplane method, which has
substantially better performance.

RMS Distance

3
2.5
2
1.5
1

(a)

0
0

(b)

(c)

δ0 =10

0.5
50

100

B. Results for the Image Examples
One of the two image datasets was obtained from the
website http://cs.nyu.edu/∼roweis/data.html. Each image is of
the same person’s face but with different facial expressions and
angles. The elements of each original multivariate observation
x are the grayscale levels (over the range [0, 1]) for n = 28 ×
20 = 560 pixels. The data to be denoised were contaminated
versions of the images for which we added independent

200

250

300

Gaussian noise at two different noise levels. We standardized
the data before applying the denoising methods. The training
data consisted of N = 500 noisy images. Examples of the
original clean images and the noisy images for the two
different noise levels are shown in Fig. 9.
As the denoising performance depends on choice of L
(the number of retained principal components) and δ [the
regularization parameter in (3)], we investigated the effects
of these parameters. Fig. 10 plots, as a function of L, the root
mean square (RMS) distance
N −1

The true pattern was a 1-D quadratic curve in a 2-D subspace
of R20 , rotated so that it had components along all each natural
basis vector. The denoising results for the higher dimensional
example were quite similar to those shown in Fig. 8, especially
the relative performance of the two methods. For brevity, we
omit the results for this example and instead focus on the
higher dimensional example in the following section.

δ0 =1

Fig. 10. RMS distance for the method of [3] (dashed) and the tangent
hyperplane method (solid), as a function of L for (a) the images with low
noise and (b) the images with high noise levels. For the low noise, the method
of [3] shows much better performance than the tangent hyperplane method.
For high noise, the tangent hyperplane method with δ 0 = 1 has the best
optimal performance for this example, and the tangent hyperplane method
with both settings is much more robust to misspecification of L.



Fig. 9. Examples of (a) clean images, (b) images with low noise, and (c)
images with high noise. The noisy ones are the clean ones with Gaussian
noise added with different standard deviations (b) 0.04 and (c) 0.12.

150
L

−4

N


 12
||xi,true − x̂i ||2

i=1

between the denoised images (x̂i ) and the original clean
images (xi,true ) over the set of N training images. After
generating the noisy images, the original clean images were
used only for evaluation purposes but not within the algorithms
in any way.
Fig. 10(a) shows the denoising results for the images with
low noise. The dotted horizontal line is the RMS distance
between the noisy images and the original clean images,
representing the performance with no denoising. The dashed
curve is the RMS distance for the method of [3], and the two
solid curves are the RMS distance for the proposed tangent
hyperplane method with two different values for δ, which
we take to be δ 0 times the maximum eigenvalue of A A for
δ 0 =1 and 10−4 . Fig. 10(b) is analogous to Fig. 10(a) but
for the images with high noise. Notice that the method of [3]
corresponds to the tangent hyperplane method with δ 0 = ∞.
A number of notable characteristics can be observed from
Fig. 10. First, when the noise level is low, the method of [3]
performs better, in terms of RMS distance, than the tangent

652

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

(a)

(a)

(b)

(b)

(c)

(c)

Fig. 11. Sample of the denoised image result for the low noise case using
(a) method of [3] with L = 200, and (b) and (c) our method with L =
300, δ 0 = 1 and L = 150, δ 0 = 10−4 , respectively. Although RMS distance
corresponding to (c) is high, the denoised images are quite clean and retain
key characteristics/patterns in the original data.

Fig. 12. Sample of the denoised image result for the high noise case using
(a) method of [3] with L = 30, and (b) and (c) our method with L = 75,
δ 0 = 1 and L = 100, δ 0 = 10−4 , respectively. Our method under both settings
shows better denoising performance overall, which is consistent with what we
observe from Fig. 10(b).

hyperplane method with finite δ 0 . However, the RMS distance
measure does not necessarily capture all relevant aspects of
performance. For example, consider Fig. 11, which visually
compares the denoised images for the two approaches for a
sample of 16 images. For each method, L was chosen to be
optimal, i.e., L = 200 was used for the method of [3], L = 300
for the tangent hyperplane method with δ 0 = 1, and L = 150
for the tangent hyperplane method with δ 0 = 10−4 . Although
the tangent hyperplane method results in worse RMS distance,
it clearly provides more denoising and produces images that
appear cleaner than the images produced by [3]. The upside
of this is that the facial expressions are generally cleaner in
Fig. 11(b) and (c), than in Fig. 11(a). But the downside is
that, in addition to removing more of the noise, the tangent
hyperplane method has removed some of the real patterns
in the images. For example, although some of the facial
expression patterns are retained in Fig. 11(b) and (c), others are
not retained, and the pattern resulting from the subject looking
to his right has been “denoised” out of the data [as apparent
from the second image in the top rows of Fig. 11(a)–(c)].
This is perhaps because the subject’s rotation to the far right
is something of an outlier expression.
The results are quite different when the noise level is
higher. Fig. 10(b) shows that, in this case, the tangent hyperplane method with δ 0 = 1 has the best optimal performance
according the RMS distance measure, although the optimal
performances of the other two methods are not far behind.
This can also be seen in Fig. 12, which again compares results

for optimal choices of L per each method for the same 16
samples shown in Fig. 11. However, quite significantly, our
method is much more robust to the choice of L. For any
L between roughly 50 and 200 for δ 0 = 10−4 , our method
performs nearly as well as for its optimal L. In contrast, the
“sweet spot” for the method of [3] is much smaller, with
performance rapidly degrading for L outside the range 15–75,
roughly. This robustness to choice of L is important, because
in practice it is difficult to determine the optimal L. When
using the polynomial kernel, the same trend was observed but
the overall performances of all methods were not as good.
Notice that plots such as Fig. 10 cannot be used in practice,
because the plotted RMS distances were calculated using the
true noiseless images, which are not available in practice.
The second image dataset, which are also of faces but of
a different nature than in the previous example, was obtained
from the website http://isomap.stanford.edu. As described in
[15], the dataset consists of a sequence of 4096-dimensional
vectors, representing the grayscale levels (over the range [0,
1]) for n = 64 × 64 = 4096 pixels. Each image is an animated
face with different poses and lighting directions. The data to be
denoised were again contaminated versions of the images for
which we added independent Gaussian noise at two different
noise levels. As the data dimension (4096) is quite high, for
dimensionality reduction purpose we preprocessed the data
using PCA before applying the denoising methods. The training data consisted of N = 698 noisy images. Examples of the
original clean images and the noisy ones are shown in Fig. 13.

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

653

(a)

(a)

(b)

(b)

(c)

(c)

Fig. 13. Examples of (a) clean, (b) and (c) noisy image data. The noisy
ones are the clean ones with Gaussian noise added with standard deviations
(b) 0.04 and (c) 0.12.

Fig. 15. Sample of the denoised image result for the low noise case using
(a) method of [3] with L = 250, and (b) and (c) our method with L = 300,
δ 0 = 1 and L = 75, δ 0 = 10−4 , respectively.
(a)

(a)
12

δ 0 =10−4

RMS Distance

10

δ 0 =1

8
6
4
2
0
0

Method of [3]
Our Proposed Method

50

100

150
L

200

250

(b)

300

(b)
12

δ 0 =1

RMS Distance

10
8
6

δ 0 =10

4
2
0
0

(c)

−4

Method of [3]
Our Proposed Method

50

100

150
L

200

250

300

Fig. 14. RMS distance for the method of [3] (dashed) and the tangent
hyperplane method (solid), as a function of L for both (a) low noise and
(b) high noise cases. As in the previous example, the method of [3] is better
in the low noise case while our method better in high noise case and robust
to the choice of L.

As in the previous example, we applied both the method of
[3] and the tangent hyperplane method with δ 0 = 1 and δ 0 =
10−4 . Figs. 14–16, which are analogous to Figs. 10–12, show
the results. The key observations are quite consistent with the
previous example. In terms of RMS distance, our method is
outperformed by the method of [3] when the noise level is low.
The tangent hyperplane method appears to result in greater
“denoising,” but of legitimate patterns as well as noise. At high
noise level, on the other hand, the tangent hyperplane method
outperforms the method of [3] in terms of RMS distances as
well as robustness to L.
We also performed simulations for medium noise cases and
for δ 0 = 10−2 . Because the results were predictable, falling

Fig. 16. Sample of the denoised image result for the high noise case using
(a) method of [3] with L = 75, and (b) and (c) our method with L = 100,
δ 0 = 1 and L = 200, δ 0 = 10−4 , respectively.

somewhere between the low and high noise cases and the
δ 0 = 1 and δ 0 = 10−4 cases shown previously, we omit them
for brevity.
C. Choice of the Parameter δ and Kernels
From the previous experimental results, it can be observed
that δ substantially influences the denoising results. Although
we envision no clear strategy for optimally choosing δ, we
do recommend that it be chosen to at least accomplish its
intended purpose: namely, to prevent erratic results caused by
poor conditioning of A when its columns are (nearly) linearly
dependent. As a rule of thumb, we suggest choosing δ =
10−4 times the maximum eigenvalue of A A. One could also

654

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

independent. Hence, no point in φ̃(x) + span{u1, u2 , …, un }
lies exactly in span{v1, v2 , …, v L } and vice versa (unless
they intersect at a unique point). It therefore follows that
for any two points u and ũ in φ̃(x)+ span{u1, u2 , …, un },
we have ||PT H °P°(u−ũ)|| < ||u−ũ||. Hence, PT H °P is a
contraction map in this subspace, and the statement follows
because contraction maps have unique fixed points.
VI. C ONCLUSION
Fig. 17. Interpretation of the tangent hyperplane method as a series of paired
projections.

conceivably use
of some characteristic of the denoised
plots
N
2
images (e.g.,
||x
i − x̂i || ) versus δ and look for the
i=1
elbow, as is commonly done in ridge regression.
Our modification of the principal projection in the tangent
hyperplane method was motivated by the geometry of the
full manifold and the principal subspace in the feature space.
Although we illustrated this motivation with a polynomial kernel, based on the facial images examples and other simulation
results that are not shown, we have observed that the relative
improvement in denoising is as significant when the feature
map is that for a Gaussian RBF kernel.
D. Interpretation as a Series of Projections
In this section, we provide an alternative interpretation of
the tangent hyperplane method. For the ridge penalty term
δ = 0, taking PT φ̃(x) as the principal projection in the
tangent hyperplane method can be interpreted as a series of
paired projections (the first onto the principal subspace and the
second onto the tangent hyperplane) until convergence, followed by an additional projection onto the principal subspace.
We illustrate this in Fig. 17 and provide an informal proof
in the following paragraph. In Fig. 17, PT H and P denote
the projection operators onto the tangent hyperplane and the
principal subspace, respectively. Hence, the tangent hyperplane
approach is like applying the method of Schölkopf et al. [3]
iteratively with the preimage projection onto the full manifold
replaced by the projection onto the tangent hyperplane. The
final step of projecting back onto the full manifold is the same
for both methods. We discuss this iterative scheme only for
interpretation purposes. It should not be used to implement
the approach, because we have a closed-form expression for
the solution PT φ̃(x).
To justify this interpretation, it is easy to show that, with
no ridge penalty, any solution PT φ̃(x) to the least squares
problem (3) (regardless of whether it is unique) is a fixed point
of the map PT H °P (by the orthogonality characterization of
minimum norm problems). Likewise, it is easy to show that
any fixed point of the map PT H °P is a solution to the least
squares problem (also by the orthogonality characterization
of minimum norm problems). Furthermore, if the solution to
the least squares problem is unique, then repeated application
of PT H °P converges to a unique fixed point (which is the
same as the unique least squares solution). This last statement
follows because if the least squares solution is unique, then
the vectors {u1 , u2 , . . . , un } and {v1 , v2 , …, v L } are linearly

We have argued that all existing KPCA denoising methods
share a general framework that involves two operations—
an orthogonal projection (onto the principal subspace in the
feature space) and a preimage approximation (moving the
projection P φ̃(x) to the full manifold in the feature space).
Moreover, they all share a common orthogonal projection
procedure, and their distinctions lie entirely in the preimage
approximation procedure.
Using geometrical arguments, we have shown that the
orthogonal projection step can result in variable denoising
performance, by which points in some regions are denoised
effectively, and in other regions poorly. We proposed, as a
remedy for this shortcoming, a modification to the orthogonal
projection step that we termed the tangent hyperplane method.
Essentially, the method attempts to project onto the principal
subspace in a manner that keeps the principal projection as
close to the full manifold as possible. In other words, it helps
to ensure that the principal projection is close to having a
preimage, which helps to ensure that the approximate preimage
is close to the underlying pattern. Because the tangent hyperplane method is a modification to only the projection step, it
can be used in conjunction with the preimage approximation
step of any of the existing KPCA denoising algorithms.
We compared the performance of the tangent hyperplane
method and the method of [3] for three datasets: a toy example
in which the data represent a quadratic pattern buried in noise;
and two different image data sets under different noise levels
and ridge parameter choices. In the toy example, the tangent
hyperplane method resulted in remarkably better denoising
(see Fig. 8). For the image data examples with high noise
levels, the tangent hyperplane method performed better than
the method of [3] using the optimal L for each method, and
it was also substantially more robust to misspecification of
L. Specifically, the performance of the tangent hyperplane
method was nearly optimal for a wide range of L. For the
image data examples with low noise levels, the method of
[3] performed better than the tangent hyperplane method with
finite δ (they are the same for infinite δ) in terms of RMS
distance. However, the tangent hyperplane method appears
to result in more extreme denoising (of noise as well as of
legitimate patterns that one wishes to retain).
A PPENDIX
U SING THE K ERNEL T RICK TO C ALCULATE THE
P RINCIPAL P ROJECTION PT φ̃(x)
Here we show how the kernel trick is used to calculate (4)
and implicitly calculate the principal projection (2) for the
tangent hyperplane method described in Section IV.

IM et al.: TANGENT HYPERPLANE KERNEL PCA FOR DENOISING

655

First note that we can express the matrix A via the feature
space inner products
⎡  ⎤
u1
⎢
⎥
⎢ .. ⎥
⎢ . ⎥
⎢
⎥
⎢  ⎥
⎢ u ⎥
	
⎢ n ⎥
u1 · · · un −v1 · · · −v L
⎥
A A = ⎢
⎢ −v ⎥
1⎥
⎢
⎢
⎥
⎢ . ⎥
⎢ .. ⎥
⎢
⎥
⎣  ⎦
−v L
⎡ 
⎤
u1 u1 · · · u1 un −u1 v1 · · · −u1 v L
⎢
⎥
⎢ ..
..
..
.. ⎥
..
..
⎢ .
.
.
.
.
. ⎥
⎢
⎥
⎢ 
⎥
⎢ u u1 · · · u un −u v1 · · · −u v L ⎥
n
n
n
⎢ n
⎥
⎥
=⎢
⎢ −v u1 · · · −v un v v1 · · · v v L ⎥ .
1
1
1
1
⎢
⎥
⎢
⎥
⎢ .
⎥
.
.
.
.
.
..
..
..
..
.. ⎥
⎢ ..
⎢
⎥
⎣ 
⎦
−v L u1 · · · −vL un vL v1 · · · vL v L

After inverting A A+δI, it remains to calculate the (n + L)length vector A (P φ̃(x) − φ̃(x)) via the kernel. We note that
its first n components are ui (P φ̃(x) − φ̃(x)) for i = 1, 2,
. . ., n, and its last L components are −vj (P φ̃(x) − φ̃(x)) for
j = 1, 2, . . ., L. The latter are zero, because P φ̃(x) − φ̃(x)
is orthogonal to each eigenvector v j for j = 1, 2, . . ., L. The
former are

 L 





ui P φ̃ (x) − φ̃ (x) = ui
K̃x α k vk − φ̃ (x)
k=1


L  


=
K̃x α k ui vk − ∂ K̃∂(y,x)
yi 

Notice that the vi ’s are standardized by definition. For numerical stability of the least squares solution, we will later
standardize
the ui ’s by dividing i th column and row of A A


by ui ui for i = 1, 2, . . . , n (and rescale the elements of
A accordingly). However, for ease of presentation, here we
assume they are not yet standardized.
We consider separately each of the four submatrices of A A
indicated above. The lower-right submatrix (of size L × L) is
the identity matrix, because the vi ’s are defined as an orthonormal set of eigenvectors of S . The upper-right submatrix (of
size n × L) requires that we evaluate terms of the form ui  v j .
Recall that for a specific x to be denoised, ui is defined as

	 

∂ φ̃ (y) 
= ∂ φ̃∂1y(y) ∂ φ̃∂2y(y) · · · ∂ φ̃∂My (y) 
.
ui =

i
i
i
∂yi 
y=x

[1] B. Schölkopf, A. Smola, and K.-R. Müller, “Nonlinear component
analysis as a kernel eigenvalue problem,” Neural Comput., vol. 10, no.
5, pp. 1299–1319, Jul. 1998.
[2] S. Mika, B. Schölkopf, A. Smola, K.-R. Müller, M. Scholz, and G.
Rätsch, “Kernel PCA and de-noising in feature spaces,” in Advances in
Neural Information Processing Systems 11. Cambridge, MA: MIT Press,
1999, pp. 536–542.
[3] B. Schölkopf, S. Mika, C. J. C. Burges, P. Knirsch, K.-R. Müller, G.
Rätsch, and A. J. Smola, “Input space versus feature space in kernelbased methods,” IEEE Trans. Neural Netw., vol. 10, no. 5, pp. 1000–
1017, Sep. 1999.
[4] T. Takahashi and T. Kurita, “Robust de-noising by kernel PCA,” in Proc.
Int. Conf. Artif. Neural Netw., vol. 2415. 2002, pp. 739–744.
[5] A. R. Teixeira, A. M. Tomé, K. Stadlthanner, and E. W. Lang, “KPCA
denoising and the pre-image problem revisited,” Dig. Signal Process.,
vol. 18, no. 4, pp. 568–580, Jul. 2008.
[6] J. T.-Y. Kwok and I. W.-H. Tsang, “The pre-image problem in kernel
methods,” IEEE Trans. Neural Netw., vol. 15, no. 6, pp. 1517–1525,
Nov. 2004.
[7] G. H. Bakir, J. Weston, and B. Schölkopf, “Learning to find pre-images,”
in Advances in Neural Information Processing Systems 16, S. Thrun, L.
K. Saul, and B. Schölkopf, Eds. Cambridge, MA: MIT Press, 2003, pp.
449–456.
[8] W.-S. Zheng, J. Lai, and P. C. Yuen, “Penalized preimage learning in
kernel principal component analysis,” IEEE Trans. Neural Netw., vol.
21, no. 4, pp. 551–570, Apr. 2010.
[9] M. H. Nguyen and F. De la Torre, “Robust kernel principal component analysis,” in Proc. 22nd Annu. Conf. Neural Inf. Process. Syst.,
Vancouver, BC, Canada, 2009, pp. 1185–1192.
[10] I. T. Jolliffe, Principal Component Analysis. New York: Springer-Verlag,
2002.
[11] R. A. Johnson and D. W. Wichern, Applied Multivariate Statistical
Analysis. Englewood Cliffs, NJ: Prentice Hall, 2007.
[12] A. J. Izenman, Modern Multivariate Statistical Techniques: Regression,
Classification, and Manifold Learning. New York: Springer-Verlag,
2008.
[13] B. Schölkopf and A. J. Smola, Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. Cambridge, MA:
MIT Press, 2001.
[14] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri, “Transformation invariance in pattern recognition tangent distance and tangent
propagation,” in Neural Networks: Tricks of the Trade, G. B. Orr and
K.-R. Muller, Eds. New York: Springer-Verlag, 1998.
[15] J. B. Tenenbaum, V. D. Silva, and J. C. Langford, “A global geometric
framework for nonlinear dimensionality reduction,” Science, vol. 290,
no. 5500, pp. 2319–2323, 2000.

y=x

˜  α j , we have
Combining this with v j = 


ui v j = ∂ φ̃(y)
∂ yi y=x [ φ̃ (x1 ) φ̃ (x2 ) · · · φ̃ (x N ) ]α j

	


∂ K̃ (y,x N )
1 ) ∂ K̃ (y,x2 )
α
= ∂ K̃∂(y,x
···
j
y
∂y
∂y
i

i

i

y=x

which can be evaluated by differentiating the kernel function.
The lower-left submatrix is the transpose of the upper-right
submatrix, by the symmetry of A A.
For the upper-left matrix (of size n × n), we have

∂ φ̃ (y) ∂ φ̃ (z) 

ui u j =

∂yi
∂z j 
 y=z=x


2
∂ φ̃ (y) φ̃ (z) 
∂ 2 K (y, z) 
=
=


∂yi ∂z j
∂yi ∂z j y=z=x
y=z=x

which can also be evaluated by differentiating the kernel
function.

y=x

k=1

in which the terms ui  vk are evaluated as above, and the farright term is evaluated by differentiating the kernel. Therefore,
c∗ , and hence the principal projection PT φ̃(x), can be evaluated using the kernel trick.
ACKNOWLEDGMENT
The authors would to thank the three anonymous referees
and the editor for their numerous helpful comments.
R EFERENCES

656

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 4, APRIL 2012

Joon-Ku Im received the B.S. degree in mathematics from Seoul National University, Seoul, South
Korea, the M.S. degree in mathematics and the Master of Statistics degree from Ohio State University,
Columbus. He is currently pursuing the Ph.D. degree
in industrial engineering and management sciences
with Northwestern University, Evanston, IL.
His current research interests include statistical
modeling and data mining for discovering knowledge in large datasets with emphasis on manufacturing variation reduction for processes with advanced
measurement technologies, healthcare applications, and financial risk
modeling.

Daniel W. Apley received the B.S., M.S., and Ph.D.
degrees in mechanical engineering and the M.S.
degree in electrical engineering from the University
of Michigan, Ann Arbor.
He is an Associate Professor of industrial engineering and management sciences with Northwestern
University, Evanston, IL. His research has been
supported by numerous industries and government
agencies. His current research interests include the
interface of engineering modeling, statistical analysis, and data mining, with particular emphasis on
manufacturing variation reduction applications.
Dr. Apley received the National Science Foundation CAREER Award in
2001, the IIE Transactions Best Paper Award in 2003, and the Wilcoxon Prize
for Best Practical Application Paper appearing in Technometrics in 2008. He
currently serves as an Editor-in-Chief for the Journal of Quality Technology
and has served as the Chair of the Quality, Statistics, and Reliability Section
of INFORMS, the Director of the Manufacturing and Design Engineering
Program at Northwestern, and an Associate Editor for Technometrics.

George C. Runger received the Degree in industrial
engineering and statistics and the Ph.D. degree.
He was a Senior Engineer at IBM, Murray Hill,
NJ. He is a Professor with the School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe. His current
research interests include real-time monitoring and
other data-analysis methods with a focus on large,
complex data sets with applications in areas such as
manufacturing and health care.

WCCI 2012 IEEE World Congress on Computational Intelligence
June, 10-15, 2012 - Brisbane, Australia

IJCNN

Feature Selection via Regularized Trees
Houtao Deng

George Runger

Intuit
Mountain View, California, USA
Email: hdeng3@asu.edu

School of Computing, Informatics & Decision Systems Engineering
Arizona State university, Tempe, Arizona, USA
Email: george.runger@asu.edu

Abstract—We propose a tree regularization framework, which
enables many tree models to perform feature selection efﬁciently.
The key idea of the regularization framework is to penalize
selecting a new feature for splitting when its gain (e.g. information
gain) is similar to the features used in previous splits. The
regularization framework is applied on random forest and
boosted trees here, and can be easily applied to other tree models.
Experimental studies show that the regularized trees can select
high-quality feature subsets with regard to both strong and
weak classiﬁers. Because tree models can naturally deal with
categorical and numerical variables, missing values, different
scales between variables, interactions and nonlinearities etc., the
tree regularization framework provides an effective and efﬁcient
feature selection solution for many practical problems.
Index Terms—regularized boosted trees; RBoost; regularized
random forest; RRF; tree regularization.

I. I NTRODUCTION
In supervised learning, given a training data set consisting of
N instances, M predictor variables X1 , X2 , ...XM and the target variable Y ∈ {0, 1, ...C−1}, feature selection is commonly
used to select a compact feature subset F ⊂ {X1 , X2 , ...XM }
without signiﬁcant loss of the predictive information about Y .
Feature selection methods play an important role in defying
the curse of dimensionality, improving efﬁciency both in time
and space, and facilitating interpretability [1].
We propose a tree regularization framework for feature selection in decision trees. The regularization framework avoids
selecting a new feature for splitting the data in a tree node
when that feature produces a similar gain (e.g. information
gain) to features already selected, and thus produces a compact
feature subset. The regularization framework only requires
a single model to be built, and can be easily added to a
wide range of tree-based models which use one feature for
splitting data at a node. We implemented the regularization
framework on random forest (RF) [2] and boosted trees [3].
Experiments demonstrate the effectiveness and efﬁciency of
the two regularized tree ensembles. As tree models naturally
handle categorical and numerical variables, missing values,
different scales between variables, interactions and nonlinearities etc., the tree regularization framework provides an
effective and efﬁcient feature selection solution for many
practical problems.
Section II describes related work and background. Section
III presents the relationship between decision trees and the
Max-Dependency scheme [4]. Section IV proposes the tree
regularization framework, the regularized random forest (RRF)

978-1-4673-1490-9/12/$31.00 ©2012 IEEE

and the regularized boosted random trees (RBoost). Section V
establishes the evaluation criteria for feature selection. Section
VI demonstrates the effectiveness and efﬁciency of RRF and
RBoost by extensive experiments. Section VII concludes this
work.
II. R ELATED W ORK AND BACKGROUND
A. Related work
Feature selection methods can be divided into ﬁlters, wrappers and embedded methods [5]. Filters select features based
on criteria independent of any supervised learner [6], [7].
Therefore, the performance of ﬁlters may not be optimum for a
chosen learner. Wrappers use a learner as a black box to evaluate the relative usefulness of a feature subset [8]. Wrappers
search the best feature subset for a given supervised learner,
however, wrappers tend to be computationally expensive [9].
Instead of treating a learner as a black box, embedded
methods select features using the information obtained from
training a learner. A well-known example is SVM-RFE (support vector machine based on recursive feature elimination)
[10]. At each iteration, SVM-RFE eliminates the feature with
the smallest weight obtained from a trained SVM. The RFE
framework can be extended to classiﬁers able to provide
variable importance scores, e.g. tree-based models [11]. Also,
decision trees such as C4.5 [12] are often used as embedded
methods as they intrinsically perform feature selection at each
node. Single tree models were used for feature selection [13],
however, the quality of the selected features may be limited
because the accuracy of a single tree model may be limited.
In contrast, tree ensembles, consisting of multiple trees are
believed to be signiﬁcantly more accurate than a single tree
[2]. However, the features extracted from a tree ensemble
are usually more redundant than a single tree. Recently, [14]
proposed ACE (artiﬁcial contrasts with ensembles) to select
a feature subset from tree ensembles. ACE selects a set of
relevant features using a random forest [2], then eliminates
redundant features using the surrogate concept [15]. Also
multiple iterations are used to uncover features of secondary
effects.
The wrappers and embedded methods introduced above
require building multiple models, e.g. the RFE framework
[10] requires building potentially O(M ) models. Even at the
expense of some acceptable loss in prediction performance,
it is very desirable to develop feature selection methods that
only require training a single model which may considerably

reduce the training time [5]. The tree regularization framework
proposed here enables many types of decision tree models to
perform feature subset selection by building the models only
one time. Since tree models are popularly used for data mining,
the tree regularization framework provides an effective and
efﬁcient solution for many practical problems.
B. Information-theoretic measures and issues
Information-theoretic measures have been widely used for
feature selection [16], [17], [7], [6], [4]. Entropy is an
important concept in the information-theoretic criteria. The
entropy of a categorical variable A 
can be expressed in terms
of prior probabilities: H(A) = − a∈A p(a) log2 p(a). The
entropy of A after observing
another
categorical variable B


is: H(A|B) = − b∈B p(b) a∈A p(a|b) log2 p(a|b). The
increase in the amount of information about A after observing
B is called the mutual information or, alternatively, information gain [6]:
I(A; B) = H(A) − H(A|B)

(1)

I(A; B) is symmetric, i.e. I(A; B) = I(B; A), and models the
degree of association between A and B. Therefore, one can
use I(Xi ; Y ) to evaluate the relevancy of Xi for predicting
the class Y , and use I(Xi ; Xj ) to evaluate the redundancy
in a pair of predictor variables [4]. In addition, a measure called symmetric uncertainty: SU (A; B) = 2(H(A) −
H(A|B))/(H(A) + H(B)) is used in feature selection methods such as CFS (correlation-based feature selection) [6] and
FCBF (fast correlation-based ﬁlter) [7].
Measures like I(A; B) and SU (A; B) capture only twoway relationships between variables and can not capture the
relationship between two variables given other variables, e.g.
I(X1 ; Y |X2 ) [16], [17]. [17] illustrated this limitation using
an exclusive OR example: Y = XOR(X1 , X2 ), in which
neither X1 nor X2 individually is predictive, but X1 and X2
together can correctly determine Y . To this end, [16], [17]
proposed measures which can capture three-way interactions.
Still, a feature selection method capable of handling n-way
interactions when n > 3 is desirable [16]. However, it is
computationally expensive to do so [17].
C. Tree-based models and issues
Univariate decision trees such as C4.5 [12] or CART [15]
recursively split data into subsets. For many tree models, the
feature used for splitting in a node is selected to optimize an
information-theoretic measure such as information gain.
A tree model is able to capture multi-way interactions
between the splitting variables and potentially is a solution
for the issue of the information-theoretic measures mentioned
in Section II-B. However, tree models have their own problems
for selecting a non-redundant feature set. A decision tree
selects a feature at each node by optimizing, commonly, an
information-theoretic criterion and does not consider if the
feature is redundant to the features selected in previous splits,
which results in feature redundancy. The feature redundancy

problem in tree models is illustrated in Figure 1. For the twoclass data shown in the ﬁgure, after splitting on X2 (“split
1”), either X1 or X2 can separate the two classes (“split 2”).
Therefore {X2 } is the minimal feature set that can separate the
two-class data. However, a decision tree may use X2 for “split
1” and X1 for “split 2” and thus introduce feature redundancy.
The redundancy problem becomes even more severe in tree
ensembles which consist of multiple trees. To eliminate the
feature redundancy in a tree model, some regularization is
used here to penalize selecting a new feature similar to the
ones selected in previous splits.
III. R ELATIONSHIP BETWEEN DECISION TREES AND THE
M AX -D EPENDENCY SCHEME
The conditional mutual information, that is, the mutual
information between two features A and B given a set of
other features C1 , ...Cp , is deﬁned as
I(A; B|C1 , ...Cp ) =


...
wC1 =c1 ,...Cp =cp I(A; B|C1 = c1 , ...Cp = cp )
c1 ∈C1

cp ∈Cp

(2)
where wC1 =c1 ,...Cp =cp is the ratio of the number of instances
satisfying {C1 = c1 , ...Cp = cp } to the total number of
instances.
A ﬁrst-order incremental feature selection scheme, referred
to as the Max-Dependency (MD)[4] scheme, is deﬁned as
M

i = arg max I(Xm ; Y |F (j −1)); F (j) = {F (j −1), Xi } (3)
m=1

where j is the step number, F (j) is the feature set selected in
the ﬁrst j steps (F (0) = ∅), i is the index of the feature
selected at each step, I(Xm ; Y |F (j − 1)) is the mutual
information between Xm and Y given the feature set F (j −1).
Here we consider the relationship between the MD scheme
and decision trees. Because Equation (2) is limited to categorical variables, the analysis in this section is limited to categorical variables. We also assume the decision trees discussed
in this section select the splitting variable by maximizing the
information gain and split a non-leaf node into K child nodes,
where K is the number of values of the splitting variable.
However the tree regularization framework introduced later is
not limited to such assumptions.
In a decision tree, a node can be located by its level (depth)
Lj and its position in that level. An example of a decision tree
is shown in Figure 2(a). The tree has four levels, and one to
six nodes (positions) at each level. Note that in the ﬁgure, a
tree node that is not split is not a leaf node. Instead, we let all
the instances in the node pass to its “imaginary” child node, to
keep a form similar to the MD tree structure introduced later.
Also, let Sν denote the set of feature-value pairs that deﬁne
the path from the root node to node ν. For example, for node
P6 at level 4 in Figure 2(a), Sν = {X1 = x1 , X3 = x3 , X5 =
x5 }. For a decision tree node ν, a variable Xk is selected to

10

10

6

8

split 1

2

4

X2

6
2

4

X2

8

split 1

split 2
2

4

6

8

class 1
class 2

0

0
0

split 2

class 1
class 2
10

0

2

4

X1

6

8

10

X1

(a) A decision tree may use both X1 and X2 to (b) X2 alone can perfectly separate the two classes.
split.
Fig. 1. An illustration of feature redundancy in decision trees. A decision tree may use both features to split, but X2 alone can perfectly separate the two
classes.
X1

Level 1
P1

Level 2

Level 3

P1

X3

X2

X4

P2

X1

Level 1

P3

P2

X5

Level 2

P4

P1

Level 3

Level 4

P1

P2

X2

X4

X4

P2

P3

X4

Level 4
P1

P2

P3

P4

P5

P6

P1

P2

P3

P4

P5

(a) At each level, a decision tree can have different (b) At each level, the MD scheme uses only one variable
variables for splitting the nodes.
for splitting all the nodes.
Fig. 2. Illustrations of a decision tree and the MD scheme in terms of a tree structure. A node having more than one child node is marked with the splitting
variable. For a decision tree node that can not be split, we let all the instances in the node pass to its “imaginary” child node, to keep a form similar to the
MD tree.

maximize the information gain conditioned on Sν . That is,
M

k = arg max I(Xm ; Y |Sν )
m=1

(4)

By viewing each step of the MD scheme as a level in
a decision tree, the MD scheme can be expressed as a tree
structure, referred to an MD tree. An example of an MD tree
is shown in Figure 2(b). Note in an MD tree, only one feature
is selected at each level. Furthermore, for the MD tree, Xk is
selected at Lj so that
M 
wν ∗ I(Xm ; Y |Sν )
(5)
k = arg max
m=1

ν∈Lj

where wν is the ratio of the number of instances at node ν to
the total number of training instances.
Note Equation (4) maximizes the conditional mutual information at each node, while Equation (5) maximizes a
weighted sum of the conditional mutual information from all
the nodes in the same level. Calculating Equation (5) is more
computationally expensive than Equation (4). However, at each
level Lj , an MD tree selects only one feature that adds the
maximum non-redundant information to the selected features,
while decision trees can select multiple features and there is
no constraint on the redundancy of these features.

IV. R EGULARIZED TREES
We are now in a position to introduce the tree regularization
framework which can be applied to many tree models which
recursively split data based on a single feature at each node.
Let gain(Xj ) be the evaluation measure calculated for feature
Xj . Without loss of generality, assume the splitting feature
at a tree node is selected by maximizing gain(Xj ) (e.g.
information gain). Let F be the feature set used in previous
splits in a tree model. When the tree model is built, then F
becomes the ﬁnal feature subset.
The idea of the tree regularization framework is to avoid
selecting a new feature Xj , i.e., avoid features not belonging to F , unless gain(Xj ) is substantially larger than
maxi (gain(Xi )) for Xi ∈ F . To achieve this goal, we
consider a penalty to gain(Xj ) for Xj ∈
/ F . A new measure
is calculated as

/F
λ · gain(Xj ) Xj ∈
gainR (Xj ) =
(6)
gain(Xi )
Xj ∈ F
where λ ∈ [0, 1]. Here λ is called the coefﬁcient. A smaller
λ produces a larger penalty to a feature not belonging to F .
Using gainR (·) for selecting the splitting feature at each tree

Algorithm 1 Feature selection via the regularized random tree model: F = tree(data, F, λ), where F is the feature subset
selected by previous splits and is initialized to an empty set. Details not directly relevant to the regularization framework are
omitted. Brief comments are provided after “//”.
1: gain∗ = 0
2: count = 0 // the number of new features tested
3: for m = 1 : M do
4:
gainR (Xm )=0
5:
if Xm ∈ F then gainR (X√
m ) = gain(Xm ) end if //calculate the gainR for all variables in F
6:
if Xm ∈
/ F and count <  M  then
7:
gainR (Xm ) = λ · gain(Xm ) //penalize using new features
8:
count = count+1
9:
end if
10:
if gainR (Xm ) > gain∗ then gain∗ = gainR (Xm ), X ∗ = Xm end if
11: end for
12: if gain∗ = 0 then make this node as a leaf and return F end if
13: if X ∗ ∈
/ F then F = {F, X ∗ } end if
14: split data into γ child nodes by X ∗ : data1 , ...dataγ
15: for g = 1 : γ do
16:
F = tree(datag , F, λ)
17: end for
18: return F

node is called a tree regularization framework. A tree model
using the tree regularization framework is called a regularized
tree model. A regularized tree model sequentially adds new
features to F if those features provide substantially new
predictive information about Y . The F from a built regularized
tree model is expected to contain a set of informative, but
non-redundant features. Here F provides the selected features
directly, which has the advantage over a feature ranking
method (e.g. SVM-RFE) in which a follow-up selection rule
needs to be applied.
A similar penalized form to gainR (·) was used for suppressing spurious interaction effects in the rules extracted from tree
models [18]. The objective of [18] was different from the goal
of a compact feature subset here. Also, the regularization in
[18] only reduced the redundancy in each path from the root
node to a leaf node, but the features extracted from tree models
using such a regularization [18] can still be redundant.
Here we apply the regularization framework on the random
tree model available at Weka [19]. The random tree randomly
selects and tests K √
variables out of M variables at each node
(here we use K =  M  which is commonly used for random
forest [2]), and recursively splits data using the information
gain criterion.
The random tree using the regularization framework is
called the regularized random tree algorithm which is shown
in Algorithm 1. The algorithm focuses on illustrating the tree
regularization framework and omits some details not directly
relevant to the regularization framework. The regularized random tree differs from the original random tree in the following
ways: 1) gainR (Xj ) is used for selecting the splitting feature;
2) gainR of all variables
√ belonging to F are calculated, and
the gainR of up to  M  randomly selected variables not
belonging to F are calculated. Consequently, to enter F a

variable needs to improve upon the gain of all the currently
selected variables, even after its gain is penalized with λ.
Algorithm 2 Feature selection via the regularized tree ensemble: F = ensemble(data, F, λ, nT ree), where F is feature
subset selected by previous splits and is initialized to an empty
set, nT ree is the number of regularized trees in the tree
ensemble.
1: for iTree = 1:nT ree do
2:
select datai from data with some criterion, e.g. randomly select
3:
F = tree(datai , F, λ)
4: end for
The tree regularization framework can be easily applied
to a tree ensemble consisting of multiple single trees. The
regularized tree ensemble algorithm is shown in Algorithm
2. F now represents the feature set used in previous splits
not only from the current tree, but also from the previous
built trees. Details not relevant to the regularization framework
are omitted in Algorithm 2. The computational complexity
of a regularized tree ensemble with nT ree regularized trees
is nT ree times the complexity of the single regularized tree
algorithm. The simplicity of Algorithm 2 suggests the easiness
of extending a single regularized tree to a regularized tree
ensemble. Indeed, the regularization framework can be applied
to many forms of tree ensembles such as bagged trees [20]
and boosted trees [3]. In the experiments, we applied the
regularization framework to bagged random trees, referred to
as random forest (RF) [2], and boosted random trees. The
regularized versions are called the regularized random forest
(RRF) and regularized boosted random trees (RBoost).

0.6
0.5

accuracy

0.4

C4.5
NB
RF

0.3

A feature selection evaluation criterion is needed to measure
the performance of a feature selection method. Theoretically,
the optimal feature subset should be a minimal feature set
without loss of predictive information and can be formulated
as a Markov blanket of Y (M B(Y )) [21], [22]. The Markov
blanket can be deﬁned as [22]:
Deﬁnition 1: Markov blanket of Y: A set M B(Y ) is a
minimal set of features with the following property. For each
feature subset f with no intersection with M B(Y ), Y ⊥
f |M B(Y ). That is, Y and f are conditionally independent
given M B(Y ). In [23], this terminology is called the Markov
Boundary.
In practice, the ground-truth M B(Y ) is usually unknown
and the evaluation criterion of feature selection is commonly
associated with the expected loss of a classiﬁer model, referred
to as the empirical criterion here (similar to the deﬁnition of
“feature selection problem” [22]):
Deﬁnition 2: Empirical criterion: Given a set of training
instances of instantiations of feature set X drawn from distribution D, a classiﬁer induction algorithm C, and a loss
function L, ﬁnd the smallest subset of variables F ⊆ X such
that F minimizes the expected loss L(C, D) in distribution D.
The expected loss L(C, D) is commonly measured by
classiﬁcation generalization error. According to Deﬁnition 2,
to evaluate two feature subsets, the subset with a smaller
generalization error is preferred. With similar errors, then the
smaller feature subset is preferred.
Both evaluation criteria prefer a feature subset with less loss
of predictive information. However, the theoretical criterion
(Deﬁnition 1) does not depend on a particular classiﬁer, while
the empirical criterion (Deﬁnition 2) measures the information
loss using a particular classiﬁer. Because a relatively strong
classiﬁer generally captures the predictive information from
features better than a weak classiﬁer, the accuracy of a strong
classiﬁer may be more consistent with the amount of predictive
information contained in a feature subset.
To illustrate this point, we randomly split the Vehicle data
set from the UCI database [24] into a training set and a testing
set with the same number of instances. Starting from an empty
feature set, each time a new feature was randomly selected and
added to the set. Then C4.5 [12], NB, and a relatively strong
classiﬁer random forest (RF) [2] were trained using the feature
subsets, respectively. The accuracy of each classiﬁer on the
testing set versus the number of features is shown in Figure 3.
For C4.5 and NB, the accuracy stops increasing after adding a
certain number of features. However, RF continues to improve
as more features are added, which indicates the added features
contain additional predictive information. Therefore, compared
to RF, the accuracy performance of C4.5 and NB may be
less consistent with the amount of predictive information
contained in the features. This point is also validated by
experiments shown later in this paper. Furthermore, in many
cases higher classiﬁcation accuracy and thus a relatively strong
classiﬁer may be preferred. Therefore, a feature selection

0.7

V. E VALUATION CRITERIA FOR FEATURE SELECTION

5

10

15

features

Fig. 3. Accuracy of C4.5, naive Bayes (NB) and random forest (RF) for
different numbers of features for the Vehicle data set from the UCI database.
Starting from an empty feature set, each time a new feature is randomly
selected and added to the set. The accuracy of RF continues to improve as
more features are used, while the accuracy of C4.5 and NB stops improving
after adding a certain number of features.

method capable of producing a high-quality feature subset with
regard to a strong classiﬁer is desirable.
VI. E XPERIMENTS
Data sets from the UCI benchmark database [24], the
NIPS 2003 feature selection benchmark database, and the
IJCNN 2007 Agnostic Learning vs. Prior Knowledge Challenge database were used for evaluation. These data sets
are summarized in Table I. We implemented the regularized
random forest (RRF) and the regularized boosted random trees
(RBoost) under the Weka framework [19]. Here λ = 0.5 is
used and initial experiments show that, for most data sets, the
classiﬁcation accuracy results do not change dramatically with
λ.
The regularized trees were empirically compared to CFS
[6], FCBF [7], and SVM-RFE [10]. These methods were
selected for comparison because they are well-recognized and
widely-used. These methods were run in Weka with the default
settings.
We applied the following classiﬁers: RF (200 trees) [2] and
C4.5 [12] on all the features and the features selected by RRF,
RBoost, CFS and FCBF for each data set, respectively. We
ran 10 replicates of two-fold cross-validation for evaluation.
Table II shows the number of original features, and the average
number of features selected by the different feature selection
methods for each data set. Table III show the accuracy of
RF and C4.5 applied to all features and the feature subsets,
respectively. The average accuracy of different algorithms, and
a paired t-test between using the feature subsets and using all
features over the 10 replicates are shown in the table. The
feature subsets having signiﬁcantly better/worse accuracy than
all features at a 0.05 level are denoted as +/-, respectively.
The numbers of signiﬁcant wins/losses/ties using the feature
subsets over using all features are also shown.

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal

instances
1000
5000
368
195
205
3163
2800
351
898

features
20
21
22
22
25
25
29
34
38

classes
2
3
2
2
6
2
2
2
5

Data
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene

instances
4147
208
606
476
452
2000
3153
3845
100

features
48
60
100
166
279
500
970
1617
10000

classes
2
2
2
2
13
2
2
2
2

TABLE I
S UMMARY OF THE DATA SETS USED IN EXPERIMENTS .

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal

All
20
21
22
22
25
25
29
34
38

RRF
17.9
21.0
18.4
10.6
8.2
12.4
12.3
15.2
11.5

RBoost
18.7
21.0
19.3
12.3
8.4
14.5
16.3
18.5
11.7

CFS
4.9
15.3
3.9
7.8
6.8
5.3
5.4
11.7
5.8

FCBF
3.6
7.1
3.9
3.5
4.5
5.5
5.6
9.1
6.9

Data
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene

All
48
60
100
166
279
500
970
1617
10000

RRF
39.1
18.9
30.7
34.5
26.8
72.5
83.0
146.1
22.5

RBoost
41.2
21.4
33.5
34.8
28.9
76.9
95.4
192.6
28.2

CFS
8.4
10.8
1.0
29.2
17.7
10.7
51.6
38.6
49.4

FCBF
7.0
6.6
1.0
11.0
8.2
4.7
16.1
13.6
35.1

TABLE II
T HE TOTAL NUMBER OF FEATURES (“A LL ”), AND THE AVERAGE NUMBER OF FEATURES SELECTED BY DIFFERENT FEATURE SELECTION METHODS .

Data
german
waveform
horse
parkinsons
auto
hypo
sick
iono
anneal
ada
sonar
HillValley
musk
arrhythmia
madelon
gina
hiva
arcene
win/lose/tie

All
0.752
0.849
0.858
0.892
0.756
0.989
0.979
0.931
0.944
0.840
0.803
0.546
0.865
0.682
0.671
0.924
0.967
0.760
-

RRF
0.750
0.849
0.857
0.891
0.756
0.990
0.981
0.926
0.940
0.839
0.783
0.511
0.849
0.704
0.706
0.915
0.967
0.683
4/6/8

+
+
−
−
−
−
+
+
−
−

Classiﬁer: RF
RBoost
CFS
0.750
0.704
−
0.849
0.846
−
0.853
−
0.824
−
0.891
0.878
−
0.759
0.746
0.990
+
0.985
−
0.980
+
0.966
−
0.928
0.925
−
0.941
0.904
−
0.839
0.823
−
0.774
−
0.739
−
0.514
−
0.489
−
0.853
−
0.840
−
0.699
+
0.721
+
0.675
0.784
+
0.914
−
0.891
−
0.967
0.966
0.676
−
0.713
−
3/6/9
2/14/2

FCBF
0.684
−
0.788
−
0.825
−
0.846
−
0.715
−
0.990
0.966
−
0.919
−
0.919
−
0.831
−
0.734
−
0.498
−
0.821
−
0.685
0.602
−
0.832
−
0.965
−
0.702
−
0/16/2

All
0.716
0.757
0.843
0.842
0.662
0.992
0.982
0.887
0.897
0.830
0.701
0.503
0.766
0.642
0.593
0.847
0.961
0.603
-

RRF
0.719
0.757
0.843
0.843
0.634
0.992
0.982
0.881
0.896
0.829
0.693
0.503
0.746
−
0.648
0.661
+
0.851
0.961
0.633
1/1/16

Classiﬁer: C4.5
RBoost
CFS
0.716
0.723
0.757
0.765
+
0.842
0.835
0.841
0.841
0.638
0.637
0.992
0.988
−
0.982
0.973
−
0.881
0.889
0.893
0.869
−
0.830
0.842
+
0.691
0.689
0.503
0.503
0.768
0.771
0.649
0.662
+
0.643
+
0.696
+
0.848
0.854
0.964
+
0.965
+
0.606
0.566
2/0/16
5/3/10

FCBF
0.713
0.749
−
0.836
0.839
0.640
0.991
0.973
−
0.880
0.890
0.840
+
0.697
0.503
0.752
0.657
0.611
+
0.817
−
0.965
+
0.586
3/3/12

TABLE III
T HE AVERAGE ACCURACY OF RANDOM FOREST (RF) AND C4.5 APPLIED TO ALL FEATURES , AND THE FEATURE SUBSETS SELECTED BY DIFFERENT
METHODS RESPECTIVELY. T HE FEATURE SUBSETS HAVING SIGNIFICANTLY BETTER / WORSE ACCURACY THAN ALL FEATURES AT A 0.05 LEVEL ARE
DENOTED AS +/-.

Some trends are evident. In general, CFS and FCBF tend
to select fewer features than the regularized tree ensembles.
However, RF using the features selected by CFS or FCBF
has many more losses than wins on accuracy, compared to
using all the features. Note both CFS and FCBF consider
only two-way interactions between the features, and, therefore,
they may miss some features which are useful only when other
features are present. In contrast, RF using the features selected
by the regularized tree ensembles is competitive to using all
the features. This indicates that though the regularized tree
ensembles select more features than CFS and FCBF, these additional features indeed add additional predictive information.
For some data sets where the number of instances is small

(e.g. arcene), RF using the features from RRF or RBoost do
not have an advantage over RF using the features from CFS.
This may be because a small number of instances leads to
small trees, which are less capable of capturing multi-way
feature interactions.
The relatively weak classiﬁer C4.5 performs differently
from RF. The accuracy of C4.5 using the features from
every feature selection method is competitive to using all the
features, even though the performance of RF suggests that
CFS and FCBF may miss some useful predictive information.
This indicates that that C4.5 may be less capable than RF on
extracting predictive information from features.
In addition, the regularized tree ensembles: RRF and RBoost

musk

arrhythmia

50

SVM−RFE
RRF

15

30

35

error
20
25

error
40 45

30

SVM−RFE
RRF

0

50

100
features

150

0

50

100 150 200 250
features

(a) The musk data. The SVM-RFE took 109 seconds to run, (b) The arrhythmia data. SVM-RFE took 442 seconds to run,
while RRF took only 4 seconds on average.
while RRF took only 6 seconds on average.
Fig. 4. The results of SVM-RFE and RRF. Plotted points show the errors versus the number of backward elimination iterations used in SVM-RFE. The
circles correspond to the average error versus the average number of features over 10 runs of RRF. The straight lines on the circles are the standard errors
(vertical lines) or number of features (horizontal lines).

have similar performances regarding the number of features
selected or the classiﬁcation accuracy over these data sets.
Next we compare the regularized tree ensembles to SVMRFE. For simplicity, here we only compare RRF to SVM-RFE.
The algorithms are evaluated using the musk and arrhythmia
data sets. Each data set is split into the training set and testing
set with equal number of instances. The training set is used for
feature selection and training a RF classiﬁer, and the testing
set is used for testing the accuracy of the RF. Figure 4 plots
the RF accuracy versus the number of backward elimination
iterations used in SVM-RFE. Note that RRF can automatically
decide the number of features. Therefore, the accuracy of RF
using the features from RRF is a single point on the ﬁgure.
We also considered the randomness of RRF. We run RRF 10
times for each data set and Figure 4 shows the average RF error
versus the average number of selected features. The standard
errors are also shown.
For both data sets, RF’s accuracy using the features from
RRF is competitive to using the optimum point of SVM-RFE.
It should be noted that SVM-RFE still needs to select a cutoff
value for the number of features by strategies such as crossvalidation, which not necessarily selects the optimum point,
and also further increase the computational time. Furthermore,
RRF (took less than 10 seconds in average to run for each data
set) is considerably more efﬁcient than SVM-RFE (took more
than 100 seconds to run for each data set).
VII. C ONCLUSION
We propose a tree regularization framework, which adds a
feature selection capability to many tree models. We applied
the regularization framework on random forest and boosted
trees to generate regularized versions (RRF and RBoost,

respectively). Experimental studies show that RRF and RBoost
produce high-quality feature subsets for both strong and weak
classiﬁers. As tree models are computationally fast and can
naturally deal with categorical and numerical variables, missing values, different scales (units) between variables, interactions and nonlinearities etc., the tree regularization framework
provides an effective and efﬁcient feature selection solution
for many practical problems.
ACKNOWLEDGEMENTS
This research was partially supported by ONR grant
N00014-09-1-0656.
R EFERENCES
[1] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “An introduction to
variable and feature selection,” Journal of Machine Learning Research,
vol. 3, pp. 1157–1182, 2003.
[2] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.
5–32, 2001.
[3] Y. Freund and R. Schapire, “Experiments with a new boosting algorithm,” in Proceedings of the Thirteenth International Conference on
Machine Learning, 1996, pp. 148–156.
[4] H. Peng, F. Long, and C. Ding, “Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 27, no. 8, pp. 1226–1238, 2005.
[5] I. Guyon, A. Saffari, G. Dror, and G. Cawley, “Model selection: beyond
the bayesian/frequentist divide,” Journal of Machine Learning Research,
vol. 11, pp. 61–87, 2010.
[6] M. A. Hall, “Correlation-based feature selection for discrete and numeric class machine learning,” in Proceedings of the 17th International
Conference on Machine Learning, 2000, pp. 359–366.
[7] L. Yu and H. Liu, “Efﬁcient feature selection via analysis of relevance
and redundancy,” Journal of Machine Learning Research, vol. 5, pp.
1205–1224, 2004.
[8] R. Kohavi and G. John, “Wrappers for feature subset selection,” Artiﬁcial
intelligence, vol. 97, no. 1-2, pp. 273–324, 1997.

[9] H. Liu and L. Yu, “Toward integrating feature selection algorithms for
classiﬁcation and clustering,” IEEE Transactions on Knowledge and
Data Engineering, vol. 17, no. 4, pp. 491–502, 2005.
[10] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “Gene selection for
cancer classiﬁcation using support vector machines,” Machine Learning,
vol. 46, no. 1-3, pp. 389–422, 2002.
[11] R. Dı́az-Uriarte and S. De Andres, “Gene selection and classiﬁcation
of microarray data using random forest,” BMC bioinformatics, vol. 7,
no. 1, p. 3, 2006.
[12] J. Quinlan, C4.5: programs for machine learning. Morgan Kaufmann,
1993.
[13] L. Frey, D. Fisher, I. Tsamardinos, C. Aliferis, and A. Statnikov, “Identifying markov blankets with decision tree induction,” in Proceedings
of the third IEEE International Conference on Data Mining, Nov 2003,
pp. 59–66.
[14] E. Tuv, A. Borisov, G. Runger, and K. Torkkola, “Feature selection with
ensembles, artiﬁcial variables, and redundancy elimination,” Journal of
Machine Learning Research, vol. 10, pp. 1341–1366, 2009.
[15] L. Breiman, J. Friedman, R. Olshen, C. Stone, D. Steinberg, and P. Colla,
“CART: Classiﬁcation and Regression Trees,” Wadsworth: Belmont, CA,
1983.
[16] A. Jakulin and I. Bratko, “Analyzing attribute dependencies,” in Proceedings of the 7th European Conference on Principles and Practice of
Knowledge Discovery in Databases, 2003, pp. 229–240.
[17] F. Fleuret, “Fast binary feature selection with conditional mutual information,” Journal of Machine Learning Research, vol. 5, pp. 1531–1555,
2004.
[18] J. H. Friedman and Popescu, “Predictive learning via rule ensembles,”
Annal of Appied Statistics, vol. 2, no. 3, pp. 916–954, 2008.
[19] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten, “The weka data mining software: an update,” SIGKDD
Explorations, vol. 11, no. 1, pp. 10–18, 2009.
[20] L. Breiman, “Bagging Predictors,” Machine Learning, vol. 24, pp. 123–
140, 1996.
[21] D. Koller and M. Sahami, “Toward optimal feature selection,” in
Proceedings of the 13th International Conference on Machine Learning,
1996, pp. 284–292.
[22] C. Aliferis, A. Statnikov, I. Tsamardinos, S. Mani, and X. Koutsoukos,
“Local causal and markov blanket induction for causal discovery and
feature selection for classiﬁcation part i: Algorithms and empirical
evaluation,” Journal of Machine Learning Research, vol. 11, pp. 171–
234, 2010.
[23] J. Pearl, Probabilistic Reasoning in Intelligent Systems.
Morgan
Kaufmann, 1988.
[24] C. Blake and C. Merz, “UCI repository of machine learning databases,”
1998.

E n h a n c i n g

I n f o r m a t i o n

Scoring Levels of
Categorical Variables
with Heterogeneous
Data
Eugene Tuv, Intel
George C. Runger, Arizona State University

H

eterogeneous (mixed-type) data present significant challenges in both supervised
and unsupervised learning. The situation is even more complicated when nom-

inal variables have several levels (values) that make using indicator variables (for every
categorical level) infeasible. Some well-established regression-classification methods,

An efficient procedure
for enriching an
original data set
assigns numerical
scores to the levels of
categorical variables.
A novel scoring
objective attempts to
preserve the mutual
information between
all the variables using
a nontraditional
clustering approach

such as classification and regression trees (CART),1
multivariate adaptive regression splines (MARS),2
and multivariate adaptive regression trees (MART),3,4
deal naturally with mixed-type data sets. Outside of
these tree-rooted approaches, however, mixed data is
more problematic. Previous work considered
instance-based learning that uses generalized distances (many based on the discretization of numeric
variables), but this involves a response variable. D.
Randall Wilson and Tony Martinez summarize such
methods,5 which we later compare to our method.
With unsupervised learning, several fairly
involved, computationally intensive, nonlinear multivariate techniques6,7 iteratively alternate data transformations with optimal scoring. These seek to optimize an objective on the basis of a covariance matrix.
Also, in mixed-type data clustering, it’s common to
use weak-matching distance.8 We discuss both methods in more detail later.
Our goal is to find a computationally efficient and
flexible method for mapping categorical variables to
numeric scores in mixed-type data. We attempt to go
beyond optimizing second-order statistics (such as
covariance) and enable distance-based methods by
exploring mutual relationships or bumps of dependencies between variables. This is a new objective
for a scoring method that’s based on patterns learned
from all the available variables.

for mixed data.
Learning category scores
Given a categorical variable, the basic idea is to
14

1094-7167/04/$20.00 © 2004 IEEE
Published by the IEEE Computer Society

map the categories (assuming the variable has more
than two levels) to a numerical low-dimensional
space (one- or two-dimensional) that allows us to
discriminate between different nontrivial patterns
(unknown in advance) in the data. First, we define
and summarize patterns as regions of nonrandom
mutual dependence between variables. That is, we
cluster data into regions with different mechanisms
of associations. Then we use known multivariate data
reduction techniques for two groups of indicator variables to find optimal low-dimensional scoring. One
group represents the target categorical variable we
will quantify. The other corresponds to a learned
cluster assignment that’s a fixed “global” characteristic of the data set.
Clusters of dependencies
Motivation for our approach came from an elegant technique for transforming a density estimation
problem to one of function approximation and even
more elegant applications of function approximation
in the context of independent component analysis
and generalized association rules.9
Suppose f(x) is an unknown density for estimation and f0(x) is a specified reference density function (for example, uniform) over a multidimensional domain. Combine the original data set x1,
x2, ..., xN and a random sample of size N drawn
from f0(x). If we assign y = 1 to each sample point
drawn from f(x) and y = 0 for those drawn from
f0(x), then
IEEE INTELLIGENT SYSTEMS

1 11
3
1
1
1111 1 1 1
1
1 1
1
1111 1
311
1
1
33 1
13 3 11 1 1 1
31
3
1
3
1 1 3
1
3 3
131 3
133
31
3
1 31
3
33333
3 3333
33 3 3 3
3
33
3 3 3 33

Petal width

2.0

1.5

1.0

0.5

2

4.0

2
2
2
2 22

3.5

3.0

2.5

2
2
2 222 2
222 2
2 22
2222
22
22
22 2
2 22

1

2
2

Sepal width

2.5

3

4
Petal length

5

6

1 1

1
22
222
2
1 11
2 2 222 2
22
1
1
1
111 1
2 22
11
2
1
1 1
2 22
11
3
3 33 331
22
222
1111
33
33313 3
2
1
33313
1
33
1
3 3 3
3
31
3 33
3
333
3
3 3
1
3
3
3
3
3
2
3 3

2.0

2

2

7

2

1

1

3

5

6
Sepal length

7

8

Figure 1. Two projections of the results of K-means clustering using Iris data (K = 3 clusters).

( ) f ( x ) +( f) ( x ) .
0

p( x ) = E y x =

f x

So, we can solve the regression problem on the
combined sample (x1, y1), (x2, y2), ..., (x2N, y2N)
to get an estimate of unknown density f(x):
pˆ ( x )
.
fˆ( x ) = f 0 ( x )
1 − pˆ ( x )
For the clustering problem used in the first
phase of our scoring procedure, we’re looking for groups of observations with nonrandom associations between variables that
form interesting structures. So, the natural
reference distribution is formed as a joint
density of independent variables. Assume a
k-dimensional domain for x with density for
xi denoted as fi(xi) for i = 1, 2, …, k. The reference density is
f0(x) =

k

∏ fi (xi ) .
i =1

Although it seems like extensive distribution knowledge is needed for each variable,
this isn’t the case. We can use the original data
to easily generate a sample from f0(x) by randomly permuting values for each variable
(independently). Then we form a training sample (xi,yi) from the combined data in the same
way we did it in the density estimation problem
described previously. Given our data set’s
MARCH/APRIL 2004

nature (massive, mixed-type, missing values),
CART seems to be a perfect candidate for the
just-defined supervised-learning problem.
Terminal nodes of CART’s solution with
maximum probability of class y = 1 provide
a partition on the x-domain leading to cluster
definitions. Furthermore, the number of clusters is derived naturally from the CART algorithm rather than prespecified. We could also
up-weight the misclassification cost for class
y = 1. We refer to this method as supervisedcontrasting-independence (SCI) clustering,
which we can enhance in a couple of ways.
Solution stability. We can enhance the solution’s stability as follows. We generate multiple reference data sets {DRi} with different
permutations of the original data and build
the optimally pruned tree Ti for each data set.
For every tree Ti, we compute the average
misclassification error on {Tj, j ≠ i}. We
select the tree with the lowest error to represent the clustering results.
Misclassified observations. We must assign
to a cluster those observations from the original data set that end up at terminal nodes
with maximum probability of the reference
class. The proposed clustering allocation follows. We first assign cluster IDs to the original data that aren’t misclassified. For these
observations, we build a model using boosted
CART (MART) to predict the assigned cluster IDs. We use this new model to “recover”
www.computer.org/intelligent

or predict cluster IDs for the originally misclassified observations.
Our method can also incorporate an a priori specified number of clusters. Suppose that
the number of clusters is specified to be K. If
the number of natural clusters (leaves with
dominant class y = 1) is greater than K, we
select the top K clusters in terms of maximum expected relative size and treat the
remaining observations as misclassified with
cluster ID imputed by MART (as in the misclassified case we described).
We use Ronald Fisher’s Iris data set to
illustrate the points just made. Because this
data set is perfect for distance-based clustering, we compare the results of K-means with
our (nondistance-based) method. Figure 1
shows the results of K-means clustering
(with two projections).
Figures 2a and 2b show the natural CARTbased SCI clustering before and after we
impute misclassified observations.
Figure 3 shows CART-based SCI clustering when three groups are prespecified. The
groups are labeled as 2, 3, and 5.
Also, we can easily estimate expected crossentropy for the learned p̂( x ) from the data
Lˆ = N −1

N

∑[ pˆ ( x ) log pˆ ( x ) + (1 − pˆ ( x )) log(1 − pˆ ( x ))],
i =1

which represents a natural distance between
the joint distribution and its independent version. It’s large in the regions where the original data is easily “distinguished” from its
15

i n f o r m a t i o n

2.5

5 5*
5
5
*
5555 5 5 *
*
55
*
5555 5
555
*
55 5
*
5
55
5 5 55 5 5 *
3
4
34 *
*
3 3
444 4* *
3
*
4 *4*4
3 3334
34434
1* * * *
1
11
1 1 1 **

1.5

1.0

0.5

2

2.5

Petal width

1.5

1.0

0.5

2

2
2
2 22
22
*
222
2
2 2 222 2
3 55
5
22
4
5
2 22
2
5
45
5
554 5
2 22
4 5
4
5
5
222
22
5*
544
5 5 5*
55
3 3* 3
33
3 * 454 4
2
*
35
3* 5 * 5 4
4
*
3
55
3 1
5* *
*
* 1*
4
3 1
5
5
315
1
1
*
3
1
4
* 4

3.0

3

4
Petal length

5

6

7

5

5

6

6
Sepal length

*

*

7

8

2
2
2

4.0
2

3.5

3.0

2
2
2 22
5
22
2
222
2 2 222 2
3 55
5
22
4
5
5
554 5
2
2 22
45
5
5
5
2 22
4
4 5
545
555
22
222
3 32 3
3
45
54
5
2
33
33454 4
5
35
35454
1
4
55
3
3 5
1 3
3
1 11
5
3 1
5
315
4
1
1
1
3
1
4
3
1 4

2.0

4
Petal length

*

2

2.5

3

*

1

2.0

2
2
2 222 2
122 2
2
2 22
2222
22
22
22 2
2 22

1

(b)

3.5

2.5

5 55
5
5
5555 5 5 5
5
55
5
5555 5
5
555
5
55 5
5
5
55
5 5 55 5 5 5
3
4
4
34 3
3 4
344 434
3
3
4 34
34
34434
3 3334
11 2 1 1
1
11
1 1 1 11

2.0

2
2
2

2
2
2 222 2
2*22 2
2 22
22
22 2
2222
22
2 22

1

(a)

2
4.0

Sepal width

Petal width

2.0

2
2

Sepal width

E n h a n c i n g

7

5 5

2

5

5

1

5

6
Sepal length

7

8

Figure 2. CART-based SCI clustering using Iris data (a) before and (b) after misclassified observations are imputed. The number of
clusters was unrestricted. In (a), * = unclustered observations. In (b) the unclustered observations were imputed.

independent version. However, if the original data blends well with its independent version, pˆ ( x ) ~ 1 2 and L̂ is near its minimum.
Thus, L̂ provides a relatively simple approach
for calculating the mutual information on data
sets of any complexity, including mixed types.
Optimal scoring
After we complete clustering, we start the
scoring phase. Suppose we have a categorical variable C with m levels {c i }i =1,m . Then
a set of m corresponding indicator variables
{x i }i =1,m contain the same information as C.
So, replacing levels of C with numeric scores
16

{α i }i =1,m is equivalent to replacing indicator
variables {x i }i =1,m with a linear combination

α1 ⋅ X1 + ... + αm ⋅ Xm.
From the clustering phase, we have a
global categorical variable (learned cluster
assignment) for the discovered clusters that
represents regions of significant, mutual
dependence between the variables. It seems
sensible, therefore, that a scoring of categorical variables would discriminate between
learned clusters in some optimal way. Consequently, we find ourselves in the familiar
territory of several standard multivariate statistical settings:
www.computer.org/intelligent

• Canonical correlation analysis (CCA)
• Correspondence analysis (CA)
• Fisher’s linear discriminant analysis
(LDA) or Fisher’s optimal scoring (FOS)
Although CA and FOS have different roots,
in this case they’re algebraically equivalent.
Both represent a special case of CCA that
attempts to quantify the associations between
two sets of variables.
CA (also known as correspondence mapping, perceptual mapping, social space
analysis, dual scaling, principal component
analysis of qualitative data, correspondence
IEEE INTELLIGENT SYSTEMS

2.5

5 55
5
5
5555 5 5 5
5
55
5
5555 5
5
5
555
55 5
5
5
55
5 5 55 5 5 5
3
5
5
35 5
3 335 555
5
3
3 355
3 3333
33333
33 3 3 5
3
33
3 3 3 33

1.5

1.0

0.5

2
2
2
2 22

2

3.5

3.0

2.5

2
2
2 222 2
222 2
2 22
222
222 2
22
22
2 22

1

2

4.0

Sepal width

Petal width

2.0

2
2

4
Petal length

5

6

5 5

2

22
5
222
2
3 55
2 2 222 2
22
5
5
5
2 22
2
35
5
555 5
3 5
2 22
5
5
222
3 33 5
355
3
55
22
5355
33
35353 3
2
5
35553
5
5
35
5
3 5
3 5
3
55
3 33
5
335
3 3
5
5
3
3
3
3
2
3
3 3
5

2.0

3

2

5

5

3

7

5

6
Sepal length

7

8

Figure 3. CART-based SCI clustering with three prespecified groups labeled as 2, 3, and 5.

factor analysis, and so on)10–12 is the principal multivariate method for graphical representations of associations in frequency tables.
CA’s goal is to approximate χ2 distances
between rows (or columns) of crosstables by
Euclidian distances in some low-dimensional
space (usually two-dimensional). In other
words, CA attempts to display deviations
from independence between row and column
variables.
FOS is a method of linear discrimination.
Suppose that each record in the data set
becomes a row in the matrix X. It seeks a linear combination Xa of variables that maximizes the ratio of its between-group variance
to within-group variance

where the within-group and between-group
variance matrices are

B=

Pij =

(

f ij f − ( f i . f ) f . j f

( f i. f )( f. j

f

)

) = f ij − fri c j ,
f ri c j

where ri = fi. /f and cj = f⋅.j /f are the proportions in each row and column. The SVD (singular value decomposition) of P is
P = UΛV T .
The optimal row scores are then given (after
normalization) by

a ′ Ba
,
a ′ Wa

W=

discovered cluster variable C will quantify.
Let R and C also denote matrices of the
group indicators of the rows and columns, so
RTC = T. The correlation matrix is

n−g
g −1

.

Here, G is an indicator matrix that identifies
the group to which each observation belongs,
and M is the group means’ matrix.
Suppose we have an r × c frequency table
T(fij) corresponding to the cross-tabulation
of levels of a categorical variable R, which
MARCH/APRIL 2004

UΛ ,

where Dr is the diagonal matrix of r. The first
k columns of Y give the best k-dimensional
scoring for the levels of R. We used k = 1, 2
in the examples.

( X − GM) ′ ( X − GM)
(GM − 1x ) ′ (GM − 1x )

−1 2

Y = Dr

Experiments
We tested the scoring in supervised-learning settings. We scored categorical predictors
via several methods, then compared the error
rates of the supervised learners built from these
predictors. Specifically, we used K-nearest
neighbor (KNN) classifiers with Euclidean distance on data where categorical variables were
www.computer.org/intelligent

replaced with our corresponding learned-score
variables. We compared our quantification’s
results with a naive scoring where arbitrary
(but fixed) numeric ranks were assigned to the
levels of categorical variables. We also compared them to a standard approach (for mixedtype data in statistical learning), a generalized,
weak distance.8 In the machine-learning literature, it’s often called the Heterogeneous
Euclidean-Overlap Metric (HEOM).5 Here
k
k
δ ( )d ( )
∑
k =1 i , j i , j ,
d (i , j ) =
p
∑ k =1δ i(,kj)
p

(k )
where δ ij is equal to 1 when both measurements xik and xjk for the kth variable are not
missing, and 0 otherwise. If variable k is
(k )
nominal or binary, then d ij is defined as

1, if x ik ≠ x jk
k
d (i , j) =
.
0, otherwise
( )
If variable k is interval-scaled, then d ij is
defined as
k

x ik − x jk
k
,
d (i , j) =
Rk
where Rk is the range of variable k, defined as
R k = max x lk − min x lk
l

l

17

E n h a n c i n g

i n f o r m a t i o n

0.60

Error rate

0.55

0.50

0.45

1 OS

1 RAND

3 OS

3 RAND

5 OS

5 RAND

Classifiers

Figure 4. The K1,3,5-NN misclassification rate for OS (proposed optimal scoring) and
RAND (naive scoring) using the UCI Flag database. The dotted line corresponds
to the reported accuracy of 48.84 percent for a nearest-neighbor Heterogeneous
Euclidean-Overlap Metric -based classifier.

0.28
0.26

Error rate

0.24
0.22
0.20
0.18
0.16
0.14
1 OS

1 RAND

3 OS

3 RAND

5 OS

5 RAND

Classifiers
Figure 5. The K1,3,5-NN misclassification rate for OS (proposed optimal scoring) and
RAND (naive scoring) using the UCI Heart (Cleveland) database. The dotted line
corresponds to a reported accuracy of 74.96 percent for an NN-HOEM-based classifier.

for l over all nonmissing objects for variable
k. We first replace ordinal variables with
ranks, then apply the formula for numeric
variables. So, the distance’s categorical portion corresponds to a simplistic matching and
18

fails to use the additional information the categorical-variable values provide that can aid
in generalization.
We used two mixed-type (with continuous, ordinal, and nominal predictors) data
www.computer.org/intelligent

sets of different complexity from the University of California, Irvine machine-learning database. The more challenging data set,
Flag, has 194 observations and contains
details of various nations and their flags. We
used a country’s religion as the response (8
classes). Of the 28 predictors, there are three
continuous variables, six multilevel nominal (up to 10 levels) variables, and 19 ordinal or binary variables. We used the best
two-dimensional scoring for each nominal
variable. To assess statistical confidence, we
replicated all experimental settings 50 times.
For the results with nearest-neighbor classifications, we averaged cross-validation
(CV) error over 10 folds. Sources of variation include cross-validation, clustering, and
rank assignment in naive quantification.
Figure 4 shows CV error rates for the proposed method, labeled OS, and naive scoring, labeled RAND. The number in front of
the method represents the neighborhood size
(1, 3, or 5) used in the KNN classification.
The dotted line represents the HOEM average CV error rate that Wilson and Martinez
report.5
So, we achieved significantly higher accuracy with the data set “enriched” (by the proposed scoring) for a KNN classifier with the
basic Euclidean distance compared to the
same KNN method with a best-known generalized distance that doesn’t use the response
variable. Clearly, both methods significantly
outperform the naive score assignment.
Wilson and Martinez5 investigated four
more heterogeneous distances, all based on
a conditional distribution of the response
(target) variable (conditional on the levels of
predictors). We found that directly scoring a
predictor variable’s categories against target
categories using simple CA generated classification results as good or better than those
Wilson and Martinez reported.5 Because our
goal is to learn scores in a general situation
(where no target variable is assumed), we
don’t proceed further in this direction.
The second truly mixed-type data set,
Heart (Cleveland), is much simpler in terms
of categorical variables: five out of six have
just three levels, and one has four. The
response and two predictors are binary, and
the remaining five numeric predictors are
continuous. Figure 5 doesn’t show a significant advantage of the OS method for KNN
classification. Naive but exhaustive quantification of nominal variables shows slightly
better results than the “smart projection” OS
uses or HOEM’s simple matching.
IEEE INTELLIGENT SYSTEMS

I

n this work, we obtained scoring from
learned nonrandom patterns of significant
mutual information between variables. This
is a new objective for a scoring method that’s
motivated by applications in which pattern
detection is important. Also, we used a nontraditional, supervised clustering method
(interesting by itself) to find these associations. We then used simple, and relatively
inexpensive, correspondence analysis to
learn the numeric scores for levels of categorical variables in heterogeneous data. The
clustering uses CART’s learning engine and
therefore has a natural mechanism to deal
with mixed-type data and, more generally,
with data of practically any complexity.
Although we chose an independent version
of the original data set as a contrast in an
attempt to capture rare patterns, you can also
use a uniform version of the original data.
Presumably, it would give closer results to
the traditional distance-based clustering.
Scoring categorical variables can enrich
data to improve the quality of distance-based
supervised learning compared to traditional
methods. At the same time, if visually exploring the complex heterogeneous data is the
primary goal, the proposed simple quantification could be extremely useful when used
with metric-based methods such as self-organizing maps.

T h e

A u t h o r s
Eugene Tuv is a technical staff member at Intel. His research interests include
supervised and unsupervised nonparametric learning with massive heterogeneous data. He received his PhD in applied statistics from Arizona State
University. Contact him at Analysis & Control Technology, Intel Corp.,
CH5-255, 5000 W. Chandler Blvd., Chandler, AZ 85226; eugene.tuv@
intel.com.

George C. Runger is a professor in the Department of Industrial Engineering

at Arizona State University. His research interests include real-time monitoring and control, data mining, and other data-analysis methods with a focus on
multivariate data streams. He received his PhD in statistics from the University of Minnesota. He is member of the INFORMS, the American Statistical Association, the American Society for Quality, and the American Association for
Artificial Intelligence. Contact him at Industrial Engineering, Arizona State
Univ., Tempe,AZ 85287-5906; runger@asu.edu; www.eas.asu.edu/~masmlab.

tem of Descriptive Multivariate Analysis,”
Statistical Science, vol. 13, 1998, pp.
307–336.
7. F.W. Young,Y. Takane, and J. de Leeuw, “The
Principal Components of Mixed Measurement Level Multivariate Data: An Alternating Least Squares Method with Optimal Scaling Features,” Psychometrika, vol. 43, 1978,
pp. 279–281.
8. L. Kaufman and P. Rousseeuw, Finding
Groups in Data: An Introduction to Cluster
Analysis, John Wiley and Sons, 1990.

Acknowledgments
National Science Foundation grant DMI-0085041
partly supported George Runger’s research.

References

9. T. Hastie, R. Tibshirani, and J. Friedman, The
Elements of Statistical Learning: Data Mining, Inference and Prediction, SpringerVerlag, 2001.
10. J.P. Benzecri, Correspondence Analysis
Handbook, Dunod, 1992.

1. L. Breiman et al., Classification and Regression Trees, Wadsworth, 1984.

11. M.J. Greenacre, Theory and Applications of
Correspondence Analysis, Academic Press,
1984.

2. J.H. Friedman, “Multivariate Adaptive
Regression Splines (with discussion),” Annals
of Statistics, vol. 19, no. 1, 1991, pp. 1–141.

12. S. Nishisato, Elements of Dual Scaling: An
Introduction to Practical Data Analysis,
Lawrence Erlbaum Associates, Academic
Press, 1993.

3. J.H. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, tech.
report, Dept. of Statistics, Stanford Univ., 1999.
4. J.H. Friedman, Stochastic Gradient Boosting,
tech. report, Dept. of Statistics, Stanford
Univ., 1999.
5. D.R. Wilson and T. Martinez, “Improved Heterogeneous Distance Functions,” J. Artificial
Intelligence Research, vol. 6, 1997, pp. 1–34.
6. G. Michilidis and J. de Leeuw, “The Gifi SysMARCH/APRIL 2004

For more information on this or any other computing topic, please visit our Digital Library at
www.computer.org/publications/dlib.
www.computer.org/intelligent

Classiﬁed Advertising
SUBMISSION DETAILS: Rates are
$110.00 per column inch ($300 minimum). Eight lines per column inch
and average five typeset words per
line. Send copy at least one month
prior to publication date to: Marian
Anderson, IEEE Intelligent Systems,
10662 Los Vaqueros Circle, PO Box
3014, Los Alamitos, CA 90720-1314;
(714) 821-8380; fax (714) 821-4010.
Email: manderson@computer.org.
Rockwell Collins - Manager of the
Information Sciences Section, Information Systems Department, ATC.
Responsibilities include management,
strategic planning, and project execution
for the Information Sciences section (intelligent algorithms, data mining, prognostics, decision aides, and data fusion) of the
Rockwell Collins Advanced Technology
Center. The section pursues contract
research, and the manager is the leader for
business pursuits with agencies and laboratories such as the DARPA, ONR, ARL,
AFRL, NASA, FAA and other aerospace
companies. This position is planned at
50% management and 50% technical
research and project execution responsibilities. An advanced degree in Electrical
Engineering, Computer Engineering,
Computer Science, or Mathematics or
equivalent is strongly preferred. PhD
desired but not required. The ability to
obtain a U.S. government security clearance is a requirement. Please email
resumes to rmgatto@rockwellcollins.com.
www.rockwellcollins.com.

19

Ann Oper Res (2010) 174: 67–81
DOI 10.1007/s10479-009-0610-8

Time-based detection of changes to multivariate patterns
Jing Hu · George Runger

Published online: 1 September 2009
© Springer Science+Business Media, LLC 2009

Abstract Detection of changes to multivariate patterns is an important topic in a number of
different domains. Modern data sets often include categorical and numerical data and potentially complex in-control regions. Given a flexible, robust decision rule for this environment
that signals based on an individual observation vector, an important issue is how to extend
the rule to incorporate time-based information. A decision rule can be learned to detect shifts
through artificial data that transforms the problem to one of supervised learning. Then class
probability ratios are derived from a relationship to likelihood ratios to form the basis for
time-weighted updates of the monitoring scheme.
Keywords Time-based detection of changes · Multivariate patterns · Supervised learning
1 Introduction
Detection of changes to multivariate patterns is an important topic that has been widely
studied in a number of different domains, such as multivariate statistical process control and
data stream mining. In the field of statistical process control, Hotelling’s T 2 (Hotelling 1947)
is widely used. However, the test statistic only incorporates the most recent observation.
Performance can be improved from decision rules that incorporate time information (the
history of the data). Consequently, time-weighted decision rules were derived from T 2 , such
as a multivariate exponentially weighted moving average (MEWMA) (Lowry et al. 1992;
Prabhu and Runger 1997) and several multivariate cumulative sum (MCUSUM) (Testik and
Runger 2004) procedures.
The T 2 and related procedures were developed from multivariate normally distributed
data. Although these procedures are robust to moderate departures from these assumptions,

National Science Foundation Grant No. 0355575.
J. Hu ()
Novartis, 180 Park Avenue, Bldg 104, Florham Park, NJ 07936, USA
e-mail: jing-1.hu@novartis.com
G. Runger
Arizona State University, Arizona, USA

68

Ann Oper Res (2010) 174: 67–81

the use of Mahalanobis distance implies that numerical measurements are required, and
that the control regions are elliptical in shape. In modern data sets with large numbers
of both numerical and categorical variables, a more flexible, robust method to monitor is
needed. Hwang et al. (2007) proposed a simple, computationally feasible approach to detect
changes to multivariate patterns. The control problem was transformed to supervised learning through the use of artificial data that formed a contrast. Classification error rates were
used to generate a control boundary for a specified type I error and each point was calculated
to be inside or outside the boundary; a point outside generated a signal of a potential process
shift. However, the decision only relied on the most recent observation and the benefits of
time information would naturally seem to apply to modern data sets and flexible monitors.
Consequently, time-weighted extensions for modern data sets (analogous to T 2 extensions)
is the scope of this work and a general method that can be applied to a number of monitors
is presented. When only the most recent observation defines the decision, many equivalent
statistics (monotonic transformations) can be monitored. Consequently, there is not a single
choice for a time-weighted accumulation. In this work, an analogy of Hwang et al.’s (2007)
work to a generalized likelihood ratio test is developed and estimates are used to define a
control statistic that can be used to accumulate time information.
In the database and data mining literature, work has been done on processing data
streams. However, only some of this work addresses the problems of change in a data stream.
A framework to diagnose changes based on velocity density estimation with only heuristics
to find trends was proposed (Aggarwal 2003). A non-parametric statistic to detect changes
based on the samples in a reference window and the current sliding window was proposed
(Kifer et al. 2004). However this work did not address high dimensionality nor a practical
approach to decision limits.

2 Monitors from artificial contrasts and supervised learning
Hwang et al. (2007) used in-control observations collected during normal operations and
artificial data that were simulated to represent an alternative without a pattern. Artificial
data were usually simulated from a multi-dimensional uniform distribution which encompassed the in-control observations. In particular for continuous variables, let [L(j ), U (j )]
and s(j ) denote the range and standard deviation of variable j , respectively. Uniform data
were usually generated over a range extended by one standard deviation. That is, over
[L(j ) − s(j ), U (j ) + s(j )], independently for each variable. For categorical variables, discrete uniform data can be generated by randomly sampling from each value of the variable.
Other artificial data can be used. For example, data for each variable can be randomly permuted to create independent artificial data, but with marginal distributions the same as the
in-control data. Still other choices for the artificial data are possible. For example, Hu et al.
(2007) discussed alternative artificial data to highlight particular fault conditions. In-control
observations and artificial data were used as the training data to build a classifier that formed
a control procedure.
Figure 1 illustrates a boundary learned from such a procedure. This used a regularized
least squares classifier (RLSC) (Poggio and Smale 2003). The ellipse is the boundary from
Hotelling’s T 2 chart. The three dotted contours with the numbers in the boxes show options
for values to use as limits for the RLSC to establish control regions. The exact values to
use as limits depend on the classifier. The important point is that a control region obtained
from the classifier can be selected to closely match the T 2 region. For example, the learned
boundary with the limit being 0.4 is very close to Hotelling’s T 2 elliptical boundary in Fig. 1.

Ann Oper Res (2010) 174: 67–81

69

Fig. 1 Example of control
boundary learned from artificial
contrasts versus Hotelling’s T 2
elliptical boundary. The number
in the box is the cut-off value for
each boundary. A point within
this boundary has a value greater
than the cut-off from the RLSC
classifier while a point outside
this boundary has a value smaller
than the cut-off

In particular, suppose that f0 (x) is an unknown probability density for the in-control data,
and f1 (x) is a specified density for the artificial data. Combine the original, in-control data
set {x1 , x2 , . . . , xN0 } and a random sample of size N1 drawn from f1 (x). Also, create a class
label y with y = 0 and y = 1 for each sample from f0 (x) and f1 (x), respectively. A solution
to this two-class classification problem defines a control region. Points with predicted ŷ = 0
are assigned to the “on-target” class while ŷ = 1 assigns points into the “off-target” class.
A constant (limit) associated with the classifier is used to adjust the relative magnitudes of
the two types of errors (class 0 assigned to class 1, and vice versa). Furthermore, Hu et
al. (2005) provided a method to identify variables that contribute to a signal from such a
monitor.

3 Time-weighting of information
Section 2 provided a decision rule to monitor a process based only on the most recent observation. The manner in which information should be combined over time is still important.
For example, based on a supervised learner with a control boundary, one might monitor the
cumulative distances of points to the control boundary over time. One might also accumulate
the probability of an off-target condition.
Likelihood principles can be used as a guide to derive a general approach to incorporate
time information. The general framework is the following. Suppose that the p-dimensional
vector of observations from the process at time t is denoted as Xt . The vectors are assumed
to be independent. Consider the formulation of a control monitor from a change point hypothesis testing framework:
H0 : X1 , X2 , . . . , Xt ∼ f0 (x),
H1 : X1 , X2 , . . . , Xτ ∼ f0 (x),

Xτ +1 , . . . , Xt ∼ f1 (x)

for an unknown change time τ . The likelihood ratio is
τ
t
t
i=1 f0 (xi )
i=τ +1 f1 (xi )
+1 f1 (xi )
L=
= i=τ
.
t
t
f
(x
)
i=1 0 i
i=τ +1 f0 (xi )

70

Ann Oper Res (2010) 174: 67–81

Because τ is unknown, a generalized likelihood ratio statistic can be used that maximizes
the ratio of the likelihoods. The log of the generalized likelihood ratio is
Lt = max
τ

t

i=τ +1

ln

f1 (xi )
.
f0 (xi )

Consequently, the control decision computes Lt (maximizing over possible sample times
for τ ) and signals if the value exceeds a limit. For the case of univariate normal distributions, this computation is equivalent to a set of simpler recursive equations so that the control decision can be based on a Markov chain (Hawkins and Olwell 1998). This results in
the well-known cumulative sum (CUSUM) control chart (Hawkins and Olwell 1998). However, in the multivariate case, even for a normal distribution, the result does not analogously
simplify (Testik and Runger 2004). From the relationship between exponentially weighted
moving average (EWMA) control charts and CUSUM control charts and likelihood ratios,
it is expected that a decision rule simpler than Lt can be effective. The change-point formulation provides the guidance to additively combine time information with the log likelihood
ratio at time t
f1 (xt )
.
(1)
lt = ln
f0 (xt )
Furthermore, any time-weighted control chart such as an EWMA or CUSUM can then be
applied to the lt ’s.
3.1 Estimate of in-control probability
The following calculation from Hastie et al. (2001) can be exploited to obtain an estimate of
the statistic in (1). According to Bayes’ Theorem,
p(y = 1|x) =

p(x|y = 1)p(y = 1)
f1 (x)N1
=
.
p(x|1)p(y = 1) + p(x|0)p(y = 0) f1 (x)N1 + f0 (x)N0

This algebraically leads to
f1 (x) p(y = 1|x) N0
=
.
f0 (x) p(y = 0|x) N1

(2)

If a supervised learner provides probability estimates (or an estimate of probability ratios),
the density ratio can be estimated from direct substitution into (2). For equal sample sizes,
the log likelihood ratio is lt = ln p1 (xt ) − ln p0 (xt ). Some classification methods such as
Regularized Least Square Classifier (RLSC) (Poggio and Smale 2003), logistic regression,
and decision-tree methods provide estimates of class probabilities. Also research has reviewed and modified existing classification methods for better estimation (Niculescu-Mizil
and Caruana 2005; Provost and Domingos 2003).
Any classification method that generates an estimate of probability ratios can be used
with the statistic in (1). In our experiment, a Random Forest (RF) is used as the supervised learning method (Breiman 2001). A random forest is an ensemble of tree predictors,
each constructed from resampling the original observations. The forest prediction is the unweighted majority of class votes over all the trees. The test set error rates are monotonically
decreasing and converge to a limit. Thus, there is no over-fitting as the number of trees increases. The key to accuracy for RF is low correlation and bias. To keep bias low, trees are
grown to maximum depth. To keep correlation low, the following randomization is used:

Ann Oper Res (2010) 174: 67–81

71

(1) each tree is grown on a bootstrap sample of the training set; (2) at each node, m variables
are selected at random out of the total number of variables p (m being much smaller than p).
Based on heuristic arguments and empirical evidence Breiman (2001) recommended the de√
fault value for m as p. But RF is not sensitive to the value of m over a wide range.
A RF has an internal mechanism to monitor generalization error. For every tree grown,
about one third of the cases are out of the bootstrap (OOB) sample. The OOB samples can
serve as a test set for the tree grown on the non-OOB data and provides unbiased estimates
of the forest test set error. Another powerful feature of a tree-based method such as RF is
that they naturally incorporate categorical as well as numerical variables. Results are also
invariant to variable scales and robust to outliers among the predictors. The analysis scales
well for large data sets.
3.2 Time-based detection procedure
In Phase 1, in-control data are collected from the process. When no prior knowledge of the
shift of the process is known, artificial data can be drawn from independent uniform distributions for each variable, as described previously. It could also be collected from previous
known out-of-control situations.
In Phase 2, for each new sample xt from the process, its in- and out-of-control probability
pˆ0 t and pˆ1 t , respectively, are estimated by the classifier. A time-weighted control chart is
updated with the ratio in (1).
Any time-weighted control chart can be applied to lt . For example, an EWMA is defined
as zt = λlt + (1 − λ)zt−1 where 0 < λ ≤ 1 (Roberts 1959). The mean μ and the variance σ 2
of the likelihood ratio lt can be estimated through the in-control observations. The control
limits for the EWMA control chart (from
√the steady-state, approximate formula for the variance) are usually set to UCL = μ + Lσ λ/(2 − λ) (Montgomery 2001), which the control
limits in the illustrative examples in Sect. 5 are based on. The control limits can also be
empirically selected based on in-control training data to meet a target in-control average run
length (ARL0), which is illustrated in the simulations in Sect. 4.

4 Average run length examples for time weighting
A few experiments to demonstrate the average run length advantages from time-weighting
in the artificial contrast cases are shown. Although time-weighting is known to be effective
for the traditional statistics under multivariate normality, the benefits for a method as different as artificial contrasts are illustrated here. We used equal sample sizes with N0 and N1
observations for the in-control and artificial data, respectively. For the EWMA control charts
we set smoothing parameter to the standard value λ = 0.2. Artificial data were generated by
the method with independent, uniformly distributed observations described previously.
An important practical issue is to estimate control limits for the lt statistic and this requires an estimate of the distribution of lt under the in-control case. However, it is expected
that there is sufficient training data that a form of resampling can be applied. Originally OOB
data in the RF was used to estimate the in-control distribution of lt . However, this estimate
did not compare well against estimates from independent test data. Therefore, the OOB data
were replaced with 10-fold cross-validation (CV). In each fold, 9/10 of the training data
were used to learn the classifier and 1/10 of the data were used to calculate lt . At the completion of all folds there were N0 estimates of lt available. For an individual control chart,
the control limit was chosen as the 99.5th percentile of this distribution to approximate an

72

Ann Oper Res (2010) 174: 67–81

ARL0 = 200. For an EWMA control chart this CV procedure was repeated 100 times and
the collection of lt ’s were arranged in sequence. An EWMA control chart was applied to
this sequence and the control limit was empirically selected so that the number of signals in
this sequence corresponded to approximately an ARL0 = 200. These methods to estimate
control limits compared well with estimates from independent test data and they can be computed from only the training data. In addition, we obtained a more precise estimate of the
control limits from a much larger sample size and we display these estimates (with standard
deviations) in the tables that follow. The estimation method for the control limits compares
well to the more precise estimate. The more precise control limit was used to evaluate the
ARL in the following tables. The mean and variance of lt were also computed.
In Tables 1 and 2 each experiment is repeated 10 times and the results show the mean
and variance over the 10 replicates. In each replicate the reference data and the artificial
data were independently generated. The artificial contrast method applies to large training
data so that one does not expect large differences over the replicates and this is shown in the
standard deviations in the tables. The calculations in this research were performed in the R
statistical package.
In one experiment bivariate normal data with independent and then correlated elements
are considered. In both cases the in-control mean vector is (0, 0) and the variables have unit
variances. For the correlated case the correlation coefficient is ρ = 0.7. The sample sizes
are equal and N0 = N1 = 5000. Table 1 shows the ARLs for an EWMA for the independent
case (ρ = 0) and the correlated case (ρ = 0.7). For comparison, ARLs for individuals control
charts based on the statistic in (1) is also shown. The summary statistics for the mean and
variance of lt and the control limit estimated from the method described previously, as well
as the more precise estimate of the control limit, are also shown in the table.
As a comparison, a MEWMA control chart designed for normally distributed data with
ARL0 ≈ 200 and λ = 0.2 is considered. For a MEWMA control chart under normality,
the performance is a function only of the magnitude of the shift calculated according to
Table 1 ARLs for an EWMA and individuals charts applied to the lt statistic for bivariate normal data
Mean

Var

Est.

Control ARL0 ARL1 at shifted mean

limit

limit

EWMA ρ = 0
Average

−2.58 3.92

St dev

0.128

−0.82 −0.88

0.200 0.093

0.096

−3.07 3.91

St dev

0.170

δ = 1.0

2.0

1.41

2.83

1.41

24.0

4.4

11.2

2.5

9.7

2.4

3.90

2.51

0.46

1.58

0.16

0.72

0.19
5.16

δ = 1.40 2.80

1.08

2.17

2.58

11.6

2.6

19.9

3.9

3.1

1.5

0.176

2.92

1.31

0.15

2.03

0.23

0.22

0.04

δ = 1.0

2.0

1.41

2.83

1.41

2.83

4.50

209.0

46.0

7.6

23.2

3.6

23.0

3.3

0.357

10.90

11.1

1.30

4.45

0.69

4.76

0.62

δ = 1.40 2.80

1.08

2.17

2.58

5.16

3.95

202.3

26.1

3.6

35.9

6.0

5.3

1.3

0.294

8.19

5.72

0.68

8.72

0.98

0.92

0.06

Ind ρ = 0
Average

−2.58 3.92

St dev

0.128

4.84

0.200 0.415

Ind ρ = 0.7
Average

−3.07 3.91

St dev

0.170

3.86

0.187 0.272

2.83

201.3

−1.35 −1.30

0.188 0.147

(2,0) (1,1) (2,2) (1,−1) (2,−2)

201.3

EWMA ρ = 0.7
Average

(1,0)

Ann Oper Res (2010) 174: 67–81

73

Table 2 ARLs for selected schemes applied to the lt statistic for 10-dimensional, independent, normal data
Mean

Variance

Est. limit

Control

ARL0

limit

5 vars shift 1σ

10 vars shift 1σ

δ = 2.24

δ = 3.16

EWMA
Average

−4.29

4.99

−2.42

−2.44

202.8

10.1

4.68

St dev

0.071

0.099

0.076

0.089

3.65

1.21

0.27

Ind
Average

−4.29

4.99

1.57

1.41

200.5

39.4

11.8

St dev

0.071

0.099

0.189

0.187

5.79

18.68

5.62

Mahalanobis distance:
δ=



(μ1 − μ0 ) Σ −1 (μ1 − μ0 )

(3)

where μ0 and μ1 are the mean vectors of the observations before and after the shift, respectively, and Σ is the covariance matrix of the observations. The MEWMA chart with smoothing parameters λ = 0.2 provides the following results: ARL1 = 10.2 and ARL1 = 3.8 at a
Mahalanobis distance δ = 1 and δ = 2, respectively (Prabhu and Runger 1997).
The ARL0 for the time-weighted artificial contrast method in the tables is greater, and
the ARL1 is comparable for shifts with δ ≥ 2. Not surprisingly, the MEWMA procedure
designed for the known normal distribution excels for the smaller shift of δ = 1. The objective of this research is not to compare to normal theory, but to evaluate the time-weighted
information. The comparison of the EWMA and individuals charts in Table 1 illustrates the
advantages of the time-weighting applied to the artificial contrast method. As expected, the
time-weighting becomes more effective as the magnitude of a shift decreases.
Another experiment uses 10-dimensional, independent normal variables (Table 2). Here
ARL1 is evaluated for two out-of-control situations: 5 out of the 10 variables shift 1σ in
the mean and all the 10 variables shift 1σ in the mean. The sample sizes are equal and
N0 = N1 = 2000. The results are also compared with those of the individual charts. In high
dimensions, the magnitudes of shifts in the table are still quite small so weighting information over time is useful.
Again for the MEWMA chart the performance depends only on δ for either the independent or correlated case. The MEWMA chart with smoothing parameters λ = 0.2 provides
the following results: ARL0 = 200 and ARL1 = 5.6 and ARL1 = 3.4 at a Mahalanobis distance δ = 2 and δ = 3, respectively (Prabhu and Runger 1997). The MEWMA does well
for the traditional normal model, but the artificial contrast method is competitive for the
larger shifts. Also, the ARL0 is greater for the artificial contrast method. Furthermore, we
did not attempt extensive tuning of the parameters for the artificial contrast method and it is
expected that performance can be improved with this tuning.
We do explicitly consider ample size effects here because this is linked to the form of
the supervised learner, complexity control and parameter fitting for the learner. The work
cited by Hwang et al. (2007) showed examples of the effects of sample size and we do not
reproduce those results here. The objective of this work is to demonstrate how to incorporate time information through the generalized approach to an estimated likelihood developed
here. We want to provide a methodology to extend to time information that is independent
of the supervised learner. We assume a sample size large enough that the learner can distinguish in- and out-of-control data and leave further study to research on specific learners and
parameter settings.

74

Ann Oper Res (2010) 174: 67–81

5 Illustrative examples
In this section, three synthetic examples are provided to show the efficacy of the proposed
method. The first example examines non-normal data in low dimensions. Because the performance of distance-based statistics (that form the basis for T 2 and the MEWMA) degrade
with the increase of the dimensions, the second example illustrates a scenario with nonnormal data where one out of 50 variables shifts. The advantage of the proposed method lies
in the variable selection capability built into the supervised learner. In the third example, a
variance change is simulated.
For these examples, our goal is to not to provide detailed average run length performance
because outside of normal distributions this depends on more complex characteristics of the
data to be monitored than the traditional non-centrality parameter. Performance depends on
the shape of the in-control process data as well as the shape and magnitude of the shifted
data and scenarios can be constructed to target a particular performance metric. Instead, the
figures clearly illustrate the ability of lt to detect changes to process data through a flexible method that incorporates many scenarios automatically. Furthermore, Hu et al. (2007)
showed that knowledge of process faults can be added to a one-point method to improve
performance when such knowledge is available. The time-weighted enhancement described
here can also be combined with fault knowledge.
5.1 Non-normal data in two dimensions
A synthetic example demonstrates the effectiveness of the proposed method when the normality assumption does not hold for two variables. The two variables, x1 and x2 , have a functional relationship of x2 = x12 for an in-control situation. The shift occurs when the mean of
x2 is decreased. In the training phase, artificial data are simulated from the uniform distribution to form the contrast. The data are shown in Fig. 2. In the real-time monitoring, 1000
data are simulated from the in-control process followed by 1000 from the out-of-control
process. Three methods are applied to this sequence: Hotelling’s T 2 statistic, MEWMA applied to the original observations and EWMA applied to lt , as shown in Figs. 3, 4, and 5,
respectively. For MEWMA control chart, δ = 0.69. We choose a small smoothing parameter
λ = 0.05. According to the Average Run length table (Prabhu and Runger 1997), H = 7.35.
For an EWMA of lt , λ = 0.05, L = 2.6 and the Upper Control Limit (UCL) is −1.31.
Fig. 2 (Color online) Illustration
of the in-control and
out-of-control processes: the
(black) dots stand for the
in-control data; the (red)
triangles for the artificial data
and the (blue) plus signs for the
shifted data

Ann Oper Res (2010) 174: 67–81

75

Fig. 3 Hotelling’s T 2 statistic
applied to the original
observations for the non-normal
data. The shift occurs at the time
1001. However, this control chart
does not capture it

Fig. 4 MEWMA applied to the
original observations for the
non-normal data. The shift occurs
at the time 1001 and there is
some ability to detect the shift in
this chart because the MEWMA
is robust to moderate departure
from normality. However, the
control limit calculated from
normal theory does not bound the
data prior to a shift

Fig. 5 EWMA applied to the lt
of the original observations
non-normal. The shift occurs at
the time 1001 and is very obvious
in this chart

The T 2 chart is not sensitive to the change. The MEWMA can detect the change because
it is robust to departures from normality. The flexible EWMA based on lt also can detect
reasonably well. The charts all exhibit false alarms prior to the shift at time 1000.

76

Ann Oper Res (2010) 174: 67–81

Fig. 6 (Color online) Illustration
of the in-control and
out-of-control processes for
50-dimensional, non-normal
data: the (black) dots stand for
the in-control data; the (red)
triangles for the artificial data
and the (blue) plus signs for the
shifted data

Fig. 7 Hotelling’s T 2 statistic
applied to the original
observations for 50-dimensional,
non-normal data. The shift occurs
at the time 1001. However, this
control chart does not capture it

5.2 Non-normal data in high dimensions
To examine the performance of different methods in high dimension, 48 noise variables
are added to the previous example. These noise variables are independently sampled from
standard normal distributions for both the in- and out-of-control processes.
In the training phase, artificial data are simulated from the uniform distribution to form
the contrast. The shift only occurs in the variables, x1 and x2 , and is shown in Fig. 6. The
other 48 variables do not change under the shift. In the real-time monitoring, 1000 data are
simulated from the in-control process followed by 1000 from the out-of-control process.
Three methods are applied to this sequence: Hotelling’s T 2 statistic, MEWMA applied to
the original observations and EWMA applied to lt , as shown in Figs. 7, 8 and 9, respectively.
For the MEWMA control chart, δ = 0.74 when calculated from the simulation data, which
is slightly greater than δ in the previous example due to the 48 noise variables. We chose a
small smoothing parameter λ = 0.05. For the EWMA of lt , λ = 0.05 and the Upper Control
Limit (UCL) is −0.46 with L = 2.6.
The T 2 chart is not sensitive to the change. Regarding the MEWMA chart, this example
illustrates that the control limit can be difficult to obtain. Available references do not provide
control limits for MEWMA for 50 dimensions. Consequently, a control limit is not shown
in Fig. 8. A MEWMA is a quadratic form applied to a multivariate EWMA. This is more

Ann Oper Res (2010) 174: 67–81

77

Fig. 8 MEWMA applied to the
original observations for
50-dimensional, non-normal
data. The shift occurs at the time
1001 but the shift is not clearly
distinguished. There is a weak
indication of the shift because a
MEWMA is robust to a moderate
departure from normality

Fig. 9 EWMA applied to the lt
of the original observations for
50-dimensional, non-normal
data. The shift occurs at the time
1001 and is obvious in this chart

complex than the control chart based on lt that is a simple univariate EWMA applied to
the univariate lt statistic. The EWMA based on lt can also detect the change of only two
variables reasonably well. We used a standard formula for a univariate EWMA control chart
to set the control limit. This makes for a simpler analysis and models.
5.3 Variance change
This synthetic example demonstrates the effectiveness of the proposed method when the
variances of the control variables change. The two variables, x1 and x2 , follow a bivariate
normal distribution with the mean (0, 0) and identity covariance matrix
 the in-control
 for
situation. The shift occurs when the covariance matrix change to Σ = 20 20 . In the training
phase, artificial data are simulated from a uniform distribution to form the contrast. The
in- and out-of-control data are shown in Fig. 10. In the real-time monitoring, 1000 data are
simulated from the in-control process followed by 1000 from the out-of-control process.
Two methods are applied to this sequence: MEWMA with a smoothing parameter λ = 0.05
applied to the original observations and EWMA applied to lt , as shown in Figs. 11 and 12,
respectively. For an EWMA of lt , λ = 0.05 and the Upper Control Limit (UCL) is −1.77
with L = 2.6.

78

Ann Oper Res (2010) 174: 67–81

Fig. 10 (Color online)
Illustration of in-control and
out-of-control process for the
example of variance change. The
(black) dots stand for the
in-control data; the (red)
triangles for the artificial data
and the (blue) plus signs for the
shifted data

Fig. 11 MEWMA applied to the
original observations for the
example of variance change. The
shift occurs at the time 1001.
MEWMA does not perform well
for a variance shift

Changes to the covariance matrix could be monitored with traditional multivariate covariance methods specifically designed for such cases (Montgomery 2001). However, these
methods are complex, assume multivariate normality, and add another statistic to monitor.
Instead, our approach simultaneously monitors these, mean shifts, and other fault patterns
automatically, without change to the methodology.
The MEWMA does not perform well for this type of shift (and T 2 is worse and not
shown). A simple EWMA based on lt is effective.
5.4 Credit data example
We demonstrate the effectiveness of the detection algorithm using German Credit Data from
the UCI KDD Archive (Hettich and Bay 1999) with the emphasis that the categorical variables are easily integrated into the proposed method. This data set contains records of 20
attributes, 7 numerical and 13 categorical, with an associated class label of “good” or “bad”
credit risk. The “good” data were considered to be in control and the objective was to detect
the “bad” data. Although the data are not time ordered, orderings were randomly generated

Ann Oper Res (2010) 174: 67–81

79

Fig. 12 EWMA applied to the lt
of the original observations for
the example of variance change.
The shift occurs at the time 1001
and it is very obvious in this chart

Fig. 13 EWMA applied to the lt
of the original observations for
the credit card data. The shift
occurs at the time 301

to evaluate the method. This is an interesting example that allows us to integrate numerical
and categorical data in high dimensions.
In the training phase, the artificial data were simulated from a continuous uniform distribution for numerical variables and a discrete uniform distribution for categorical variables.
Given a categorical variable, values from the variable were randomly sampled to generate
the artificial data. All artificial variables were constructed independently to remove structure
in the data. In the real-time monitoring, 2000 replicates were made with a random order of
300 “good” cases followed by 300 “bad” cases. From the 2000 replicates the estimated mean
of lt of the in-control process is −2.35 and the estimated standard deviation is 2.43, which
are very close to the results based on the in-control training data.
The parameters of the procedure are set as follows. The smoothing parameter λ = 0.05
and UCL = −0.85 with L = 2.6. Figure 13 shows the plot of EWMA of lt from one replicate. The chart shown is typical of the type of performance obtained from a monitor of lt
over the replicates. Because this is actual data the type of shift and magnitude of the shift
is not known. Only the classified “good” and “bad” categories are available. Still, the time
weighted method is illustrated to be effective for a general problem of this type with data of
mixed types.
The out-of-control Average Run Length (ARL1) is also evaluated based on the 2000
replicates. Because the shifted process only has 300 observations, the run length distribution

80

Ann Oper Res (2010) 174: 67–81

Fig. 14 The histogram of run
length evaluated from the 2000
replicates for the credit card data

is truncated at 300. Figure 14 shows the histogram of the run length, which has a median
of 97.

6 Conclusions
We developed a methodology to incorporate time information into a control problem transformed to supervised learning. Whenever a learner provides class probability memberships
the analysis here can be applied to generate a more sensitive control algorithm. Consequently, the transform along with the statistic to monitor makes for a conceptually simple
procedure with a broad range of applicability. Data of any type can be blended in the analysis. Representative examples were provided for a random forest learner. The results illustrate
the benefits of time-weighted information in the monitor.

References
Aggarwal, C. (2003). A framework for diagnosing changes in evolving data streams. In Proceedings of the
ACM SIGMOD conference, San Diego, California.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32.
Hastie, T., Tibshirani, R., & Firedman, J. (2001). Elements of statistical learning. New York: Springer.
Hawkins, D. M., & Olwell, D. H. (1998). Cumulative sum charts and charting for quality improvement. New
York: Springer.
Hettich, S., & Bay, S. (1999). The UCI KDD archive http://kdd.ics.uci.edu.
Hotelling, H. (1947). Multivariate quality control-illustrated by the air testing of sample bombsights. In
C. Eisenhart, M. Hastay, & W. Wallis (Eds.), Techniques of statistical analysis (pp. 111–184). New
York: McGraw-Hill.
Hu, J., Runger, G., & Tuv, E. (2005). Contributors to a signal from an artificial contrast. In Proceedings of the
2nd international conference on informatics in control, automation and robotics (ICINCO), Barcelona,
Spain.
Hu, J., Runger, G., & Tuv, E. (2007). Tuned artificial contrasts to detect signals. International Journal of
Production Research, 45(23), 5527–5534.
Hwang, W., Runger, G., & Tuv, E. (2007). Multivariate statistical process control with artificial contrasts. IIE
Transactions, 39(6), 659–669.
Kifer, D. Ben-David, S., & Gehrke, J., (2004). Detecting change in data streams. In Proceedings of the 30th
VLDB, Toronto, Canada.

Ann Oper Res (2010) 174: 67–81

81

Lowry, C., Woodall, W., Champ, C., & Rigdon, S. (1992). A multivariate exponentially weighted moving
average chart. Technometrics, 34, 46–53.
Montgomery, D. (2001). Introduction to statistical quality control. New York: Wiley.
Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on machine learning (ICML’05).
Poggio, T., & Smale, S. (2003). The mathematics of learning: dealing with data. American Mathematical
Society Notice, 50, 537–544.
Prabhu, S., & Runger, G. (1997). Designing a multivariate EWMA control chart. Journal of Quality Technology, 29(1), 8–15.
Provost, F., & Domingos, P. (2003). Tree induction for probability-based ranking. Machine Learning, 52,
199–215.
Roberts, S. W. (1959). Control chart test based on geometric moving averages. Technometrics, 42(1), 97–102.
Testik, M., & Runger, G. (2004). Multivariate extensions to cumulative sum control charts. Quality and
Reliability Engineering International, 20(4), 387–396.

1338

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

A Novel Feature Selection Methodology for
Automated Inspection Systems
Hugo C. Garcia, Jesus Rene Villalobos, Rong Pan,
and George C. Runger
Abstract—This paper proposes a new feature selection methodology. The
methodology is based on the stepwise variable selection procedure, but, instead of
using the traditional discriminant metrics such as Wilks’ Lambda, it uses an
estimation of the misclassification error as the figure of merit to evaluate the
introduction of new features. The expected misclassification error rate (MER) is
obtained by using the densities of a constructed function of random variables,
which is the stochastic representation of the conditional distribution of the
quadratic discriminant function estimate. The application of the proposed
methodology results in significant savings of computational time in the estimation
of classification error over the traditional simulation and cross-validation methods.
One of the main advantages of the proposed method is that it provides a direct
estimation of the expected misclassification error at the time of feature selection,
which provides an immediate assessment of the benefits of introducing an
additional feature into an inspection/classification algorithm.
Index Terms—Feature selection, misclassification error rate, quadratic
discriminant function.

Ç
1

INTRODUCTION

ONE of the key steps in the development of any inspection or
classification system is the feature selection process. This process
often considers all possible subsets of features. The result is
combinatorial in nature that makes it a long and tedious process,
especially when the selection is performed by humans and the
features being considered present complex cross-correlation
patterns. A process that is particularly tedious is testing the results
rendered by the different subsets of features used during the
algorithm development process. During this process, each time a
new subset is evaluated, the resulting inspection algorithm needs
to be implemented and tested to assess its performance in terms of
misclassification error rate (MER). This paper addresses this
problem by introducing a novel methodology that speeds up the
identification of the appropriate subset of features. We estimate the
misclassification error rate of the resulting inspection algorithm
even before it is implemented. The methodology uses the MER as
the guiding evaluation criterion of a stepwise feature selection that
has as its main goals the minimization of MER, while minimizing
the number of features. This is important because the larger the
feature set used, the more expensive the development of the
inspection system.
The proposed methodology is based on the Quadratic
Discriminant Function. This discriminant function has several
advantages with respect to other classifiers. For example, for
small sets of training samples, it is preferred over complicated
classifiers when class separation is not complex [1]. This classifier
also tends to decrease the design error while improving the
. H.C. Garcia is with L3, Electro-Optical Systems, 1215 S. 52nd Street,
Tempe, AZ 85281. E-mail: Hugo.Garcia@L-3com.com.
. J.R. Villalobos, R. Pan, and G.C. Runger are with the Department of
Industrial Engineering, Arizona State University, PO Box 875906, Tempe,
AZ 85287-8692. E-mail: {rene.villalobos, runger, rong.pan}@asu.edu.
Manuscript received 26 Mar. 2008; revised 29 Sept. 2008; accepted 30 Oct.
2008; published online 10 Nov. 2008.
Recommended for acceptance by S. Li.
For information on obtaining reprints of this article, please send e-mail to:
tpami@computer.org, and reference IEEECS Log Number
TPAMI-2008-03-0166.
Digital Object Identifier no. 10.1109/TPAMI.2008.276.
0162-8828/09/$25.00 ß 2009 IEEE

Published by the IEEE Computer Society

VOL. 31,

NO. 7, JULY 2009

understandability of the classification properties [2]. The methodology is based on previous research in the areas of applied and
theoretical statistics, in particular, the works by McFarland and
Richard [3] and Hua et al. [4].
One of the main motivations of the proposed feature selection
methodology is to develop tools needed for the emergence of selfreconfigurable automated inspection systems [5]. One of the main
conditions for the emergence of these systems is the automation of
decisions that are traditionally performed by human developers
such as feature generation, selection, and testing of the resulting
algorithms. The methodology presented in this paper contributes
to this goal by speeding up the feature selection process and by
providing an accurate method to estimate the MER associated with
a particular feature subset, even before the inspection algorithm is
fully developed.
This paper is organized as follows: Section 2 gives a summary of
the literature review. Section 3 presents the new methodology for
the selection of features including the new method to estimate the
MER. In Section 4, the experimental results are presented. Section 5
presents the conclusions of this research and the recommendations
for future research followed by the list of references.

2

PROBLEM BACKGROUND

One of the problems faced by the developer of an automated
inspection system is the issue of what features to use to inspect a
particular component or product. The optimal feature selection
process is a very important step of any development process
because it has an impact on the overall performance of the resulting
inspection algorithm. For instance, inspection features need to be
carefully chosen to avoid using redundant features that may cause
system instability and require additional costs and training effort.
In fact, reducing the number of features can be accomplished by
taking advantage of cross correlation or redundancies among the
features [6]. The problem of optimal feature selection is typically
defined as follows: “Given a set of p features, select a subset of
size m that leads to the smallest classification error” [7].
The topic of optimal feature selection has been addressed
extensively for applications other than automated inspection
systems, for instance, see [8], [9], [10], [11], and [12]. Furthermore,
a new area of application closely related to feature selection is
called sensor selection. Some papers that address this topic are [13],
[14], and [15]. The literature associated with feature selection for
classification purpose is vast. Some examples include [16], [17],
[18], and [19]. However, the existing feature selection methods
present serious shortcomings for their use in reconfigurable
environments. For instance, one shortcoming is the amount of
time required to determine the performance of the resulting
inspection algorithms. This procedure is usually very long, because
once the feature selection process is performed, it is necessary to
asses the performance of the resulting subset, usually using some
of the cross-validation techniques, to estimate the MER. For
example, the “leave-one-out” cross-validation method consists of
using n  1 observations or data points to compute the classification rule and then classifying the omitted observation. This
procedure is repeated for each observation to estimate the MER.
A second problem is that the evaluation criteria used during the
feature selection process do not always correspond to the metrics or
statistics used to assess the performance of the resulting inspection
algorithm. For instance, when metrics such as Wilks Lambda are
used to guide the feature selection process, the objective is to
optimize the value of the metric. However, the optimization of such
indirect metrics does not guarantee the selection of a subset of
features that renders the smallest MER in the assembly line [17].
Thus, it is important to use a metric that would give a direct measure
of the expected MER during the selection of the inspection features.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

This paper addresses this gap by proposing a metric, and the
method to compute it, to make the use of stepwise feature selection
methods significantly more efficient and accurate in the development and adaptation of inspection and classification algorithms.
The proposed method is based on an indirect estimation of the
MER that is used to guide the feature selection process. That is, the
MER is estimated through a distribution function instead of relying
on direct observations. The advantage of this approach is the
indirect computation of the MER from the multivariate distributions of the components’ features. The proposed method is
applicable when a Quadratic Discriminant Function (QDF) is used
as the algorithm to discriminate between the classes of components, or in a broader context, between two populations. The
proposed method of calculating MER consists of using conditional
distribution functions of the QDF (to be presented in (3) and (4)),
but, instead of calculating MER using simulation as proposed in [3]
or by doing cross-validation, as is commonly done, an approximation of the distribution, based on the normal distribution, is used.
Once this MER is obtained, it is used to guide the search for an
optimal feature subset.
Because of the inherent complexity of feature selection
problems, heuristics methods such as stepwise selection of
variables have been developed. A typical procedure for this
heuristic method is as follows:
Start with an empty subset and a list of potential feature
candidates to be included into this subset.
2. Add a feature to the subset from the list of available
features that maximizes (minimizes) a given evaluation
criterion.
3. Calculate the marginal contribution to the evaluation
criterion for each remaining potential feature in the list.
This marginal contribution is calculated over the feature(s)
already in the subset.
4. Select the feature that in combination with the included
feature(s) maximizes (minimizes) the evaluation criterion.
5. Include the selected feature into the subset if it meets the
constraints.
6. Reexamine the marginal contribution for each feature in
the subset. If a feature does not meet the constraints, it is
removed from the subset.
7. Repeat steps 3-6 until no more features can be added into
or removed from the subset.
For additional details about the previous procedure, the
reader is referred to [20], [21], and [22]. The previous
description of the stepwise methodology relied on an evaluation
criterion to guide the inclusion and exclusion of features. One
issue is which evaluation criterion to use in the heuristic
method. There are numerous criteria to conduct the feature
selection process. These include: Wilks’ Lambda, Unexplained
Variance, Mahalanobis Distance, Smallest F Ratio, and Rao’s V.
The reader is referred to [23] and [24] for more computational
details of these evaluation criteria.
For example, Wilks’ Lambda method is one of the most
commonly used in practice [25]. This method is based on Wilks’
-criterion, which is a ratio expressing the total variability not
explained by the population differences [24]. This criterion is
1.

¼

jWj
;
jB þ Wj

ð1Þ

where jWj is the determinant of the estimated generalized
variance within the two populations, B represents the variation
between populations, and jB þ Wj is the determinant of the
estimated generalized total variance of the training data. The
Lambda value is between 0    1, and larger values indicate a

VOL. 31, NO. 7,

JULY 2009

1339

poor separation between populations, while smaller values denote
good separation between populations.
Although these methods are the most commonly used to
conduct the selection of the features, they exhibit an important
disadvantage. That is, they do not provide a good indication of the
expected misclassification results of the underlying feature/
variable subset. That is, the principal assumption is that optimizing
the evaluation criterion results in the minimization of MER.
However, this assumption is not always true. For instance,
heuristic methods directly using the MER as the evaluation
criterion (i.e., the classifier is the evaluation function) render
superior performance than the other criteria, but they also tend to
be the most computationally expensive because every new subset
that is explored requires the acquisition of actual misclassification
data [17]. Thus, using the MER as an evaluation criterion for a large
number of features is not practical.
Ideally, we would like to have a method that is accurate, as
given by applying the MER evaluation criteria, and computationally efficient. This is the objective of the feature selection procedure
proposed in this paper. The proposed method seeks to estimate the
MER based on the stochastic representation of the conditional
distribution of the quadratic classifier. These misclassification rates
are estimated through the use of a “plug-in” QDF originally
proposed in [3], later expanded in [4], and modified to fit our
needs in this paper and [26]. A brief explanation of these works is
presented in the next section.

2.1

Stochastic Representation of the “Plug-In”
Quadratic Discriminant Function

Equation (2) presents what is called the “Plug-in” QDF for the
observation x. It is the representation of the QDF when the mean
vectors 1 and 2 , and the covariance matrices 1 and 2 of the
two competing normal populations (1 and 2 ) involved in the
classification are unknown and replaced by their unbiased
1 , x
1 , S1 , and S2 [27] and [28]:
estimators, x

 0 1
 1
1 
1
02 S1
 1 S1  x
 x0 S1
1  S2 x þ x
2 x
C
B 2
^¼B
C:
Q
A
@ 1 jS1 j 1 

0 1
0 1
 2 S2 x
1  x
2
 S x
 x
 log
2
2 1 1
jS2 j
0

ð2Þ

^ is a random variable, in theory, the appropriate
Since Q
distribution could be derived to determine misclassification
probabilities for the observation x. However, since x is also a
random variable that follows a particular normal distribution, the
^ is too complex to be obtained directly.
exact distribution of Q
McFarland and Richard [3] addressed this problem by deriving
instead approximations to the conditional distributions of the
^ i.e., PfQ
^  kjx 2 1 g and PfQ
^ < kjx 2 2 g to get the
variable Q,
expected probability of misclassifying an observation to one of the
two competing populations of multivariate normal distributions.
Through the study of its characteristic function, these authors
^ can be expressed as a
showed that the conditional distribution of Q
series of independent univariate random variables, which they
^ They used this result to
called the stochastic representation of Q.
^ through the use of Monte Carlo
estimate the distribution of Q
simulations based on the individual components of the stochastic
representation. In order to present this stochastic representation, it
is necessary to introduce some terminology.
Let H be an orthogonal matrix that diagonalizes the matrix
12
1
1
1
2 1 2 2 such that 2 2 1 2 2 ¼ HH0 , where  ¼ diagð1 ;
2 ; . . . ; p Þ is a diagonal matrix. The entities 1 ; 2 ; . . . ; p on the
column
main diagonal are the eigenvalues of 1
2 1 . The auxiliary
1
1  2 Þ.
vector  ¼ ð1 ; 2 ; . . . ; p Þ0 is defined such that  ¼ H0 2 2 ð

1340

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Let k be the threshold value classifying x to belong to 1 or 2 .
Thus, the misclassification probabilities are defined as P ð2j1Þ ¼
^  kjx 2 1 g and P ð1j2Þ ¼ 1  PfQ
^  kjx 2 2 g, where
P  fQ
P ð2j1Þ is the probability of misclassifying an observation x into
2 when x belongs to 1 , and otherwise, is denoted by P ð1j2Þ. Let
Q1 and Q2 be the stochastic representations of P ð2j1Þ and P ð1j2Þ
defined by
1
0
p


2

1X 
2 1=2
2
Z2j þ j  1 Z1j C
2j !3j Z1j þ 1  !3j
B
C
B 2 j¼1
B
!C
Q1 ¼ B
C;




p1
C
B 1

 X
np T2
n2  j
A
@þ

log 1p
þ
F
 log1

log
1
j
2
2
n
n2 T1

j
1
j¼1
ð3Þ
0
B
B
B
Q2 ¼ B
B
@

1
p



2 
1X
2
2 1=2
~3j
~3j Z1j þ 1  !
Z2j þ ~j
~1 Z1j  ~2j !
C
2 j¼1
C
!C
C;
 p 


p1
C

 X
1
n T2
n2  j
A
þ
log 1p
F
 log1

log
þ
1
j
2
2
n1  j
n2 T1
j¼1
ð4Þ
2ni p

where, for i ¼ 1; 2 and j ¼ 1; . . . ; p, Ti ¼
is a chi-square
distribution with ni  p degrees of freedom, Zij ¼ Nð0; 1Þ is a
standard normal random variable, and Fj is an F distribution with
ðn2  j; n1  jÞ degrees of freedom
1 ¼

ðn1  1Þðn1 þ 1Þ
;
ðn1 T1 Þ

2j ¼

ðn2  1Þðj þ n1
2 Þ
;
T2

ðn2  1Þðn2 þ 1Þ
;
ðn2 T2 Þ
1
ðn1  1Þð1
j þ n1 Þ
~2j ¼
:
T1
~1 ¼

3

VOL. 31,

NO. 7, JULY 2009

DESCRIPTION OF THE FEATURE
SELECTION METHODOLOGY

The proposed methodology, in this paper called the Approximation Method (AM), is based on the indirect estimation of the MER
as the evaluation criterion in the stepwise feature selection method.
That is, MER is approximated by multivariate normal distribution
functions of Q1 and Q2 instead of relying on Monte Carlo
simulation experiments. The outline of the proposed method is
as follows:
Based on sample information, calculate the means
and variances of the stochastic representations, Q1
and Q2 .
2. Once the estimates of the parameters of the stochastic
representations are available, the calculation of MER is
based on a normal approximation of the stochastic
distributions.
These steps are explained in the following two sections.
1.

3.1

Mean and Variance Estimations

Q1 is assumed to follow normal distributions whose mean and
variance are given by (5) and (6):
11
0
0
n1 n2 j þ ðn1 þ n2 j þ 1Þ
p B
CC
B 1 ðn  1Þ X
n2 ðn1 þ 1Þ
CC
B
2
B
CC
B
B
AC
B 2 ðn2  p  2Þ j¼1 @
ðn

p

2Þðn

1Þðn
þ
1Þ
2
1
1
2
C
B
þ

j
C
B
ðn

p

2Þðn

1Þn
1
2
1
C
B

1C;
0 
 n  p
 n  p
Q1 ¼ B
C
B
n1  1 p
1
2
C
B

CC
B log n2  1 þ
B
2
2
CC
B
B
C
B þ1B


 C C
p
p1  
C
B
X
2B
n2  j
n1  j
AA
@ X
@


log j þ
2
2
j¼1
j¼1

The remaining random variables and constants are defined as


n1 n2 j
ðn1 þ 1Þðj n2 þ 1Þ

 1
1 2
;
j ¼ j j þ
n2

!3j ¼

12



;

n1 n2
ðn2 þ 1Þðj þ n1 Þ

 1
j 2
~j ¼ j 1 þ
:
n1

~3j ¼
!

12
;

Once Q1 and Q2 are available, then the calculation of the
misclassification probabilities can be performed through the use of
Monte Carlo simulation based on each one of the independent
stochastic components. However, in order to estimate these
probabilities, a large number of time-consuming simulations runs
need to be performed.
The results obtained in [3] were extended in [4] to find an
analytic method to produce a misclassification error curve as a
function of the number of features in the classifier so that the
optimal number of features could be determined. Two shortcomings of the work in [4] are that it assumed that all of the
features had normal distributions with identical parameters and
that samples of the same sizes (n1 ¼ n2 ) were drawn from each
population to estimate these parameters. These two shortcomings
were addressed by [26]. This work considered the case of
populations that had different parameters and unequal sample
sizes to accommodate the inspection and classification situations
in which there is a difference in number of samples available
from the targeted populations. The results of this latter work are
the basis for the feature selection methodology presented in the
next section. In particular, the proposed method extends the work
in [4] to directly estimate the MER associated with a particular
feature subset.

ð5Þ
0

ðn2  1Þ2

6
X

1

k þ c C
B
C n1 > p þ 2;
B ðn2  p  2Þ2 ðn2  p  4Þ k¼1
2Q1 ¼ B




 C
C; n > p þ 4;
B
p
X
n2  j
A 2
@ 1
0 n1  j
þ 0
þ
4 j¼1
2
2

ð6Þ

0
where
ðxÞ is the Digamma function and
ðxÞ is the
derivative of ðxÞ. The parameters k and c are defined as
follows:
!

2
p
p X
p
X
X
1
n1
1 ¼
ðn2  p  1Þ
2j þ
i j ;
2 n1 þ 1
j¼1
i¼1 j¼1

1
p
X
2
ðn

p

1Þ
ðn
þ
n

þ
1Þ
1
2 j
C

2 B 2
C
B
j¼1
1
1
C;
B
2 ¼
C
B
p
p
X
X
2 n2 ðn1 þ 1Þ @
A
þ
ðn1 þ n2 i þ 1Þðn1 þ n2 j þ 1Þ
0

i¼1 j¼1

1
p
X
j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
n1
1
C;
B
3 ¼
p
p
C
n2 ðn1 þ 1Þ2 B
A
@ XX
þ
j ðn1 þ n2 i þ 1Þ
0

i¼1 j¼1

!
p
p X
p
X
X
n1
2
2
4 ¼
ðn2  p  1Þ
j j þ
i j ;
n1 þ 1
j¼1
i¼1 j¼1

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

1
p
X
2j ðn1 þ n2 j þ 1Þ C
B ðn2  p  1Þ
C
B
j¼1
1
C;
B
5 ¼
p
p
C
n2 ðn1 þ 1Þ B
A
@ XX 2
þ
i ðn1 þ n2 j þ 1Þ

JULY 2009

1341

0

TABLE 1
Wilks Lambda Steps with Correlation 0

i¼1 j¼1

0

p
X

!2

ðn1  1Þðn2  p  2Þ
þ
ðn2  1Þðn1  p  2Þ

1

C
B
2j
C
B
C
B
j¼1
C
B 0
1
p
C
B
X
C
B

p

4Þ

ðn
6 ¼ B B
2
j
C C;
C
B B
C
i¼1
B B
CC
B B
!
CC

C
B B
pðn1  1Þ2 ðn2  p  2Þðn2  p  4Þ
n1 þ 1 2 C
@ @
AA
þ
2ðn2  1Þðn1  p  2Þðn1  p  4Þ
n1
1
0 0

1
p 
ðn2  1Þ X
1
2

þ

þ
C
B B
j
j
C
C
B B 2ðn2  p  2Þ j¼1
n2
C
C
B B
C
C
B B
C


C
B B 0
1


C
n

p
2
p
C
B B
C
log 2n1 þ
C
B B
C
2
C
C
B B B
C
C
C
B B B
C
C
C
B B B
C
 p C
C
B B B
2
C
C
C
B B B 

log
2n
C
2
C
C
B B B ðn2  p  2Þ
C
C
C
B B B
C
C
C
B B B
C
A
C
B B @
n  p
C
C
B B
1
C
C
B B
þ
C
C
B B
2
C
C
B B
C


C
B B
C
C
B B
pðn1  1Þ
n1 þ 1
C
C
B Bþ
C
C
B B 2ðn1  p  2Þ
n
C
1
C
B B
C
c¼B B 0
C:
1C
 n  p
C
B B
C
 p
2
1
C
B B
C

þ
log
2n
C
B B B
2
C
2
ðn1  p  2Þ C
C
B B B
CC
C
B B B
C
C
C
B @ @
A
A


C
B
 p
n

p
2
C
B
 log 2n1 þ
C
B
2
C
B
B 0
11C
0
C
B
n1 n2 j þ ðn1 þ n2 j þ 1Þ
2
B B
þ j C C C
BX
C
B B
p
n
ðn
þ
1Þ
2
1
CCC
B B ðn2  1Þ B
CCC
B
B B
CCC
B B 2ðn2  p  2Þ B
@ j¼1 ðn2  p  2Þðn1  1Þðn1 þ 1Þ A C C
B B
CC
B B

CC
B B
ðn1  p  2Þðn2  1Þn1
CC
B B
CC
B B 

p
CC
B @




AA
@
n1  1
n1  p
n2  p
log

þ
n2  1
2
2
The derivation and computational details of the previous
equations are presented in [26]. Although the parameters of Q2
can be estimated in a similar way to Q1 , a more efficient method
originally proposed by Hua et al. [4], consists of representing Q2 as
a function of Q1 through the following transformation:
Q2 ð
;  Þ  Q1 ð
1=2  ; 1 Þ:

ð7Þ

Therefore, to obtain the mean and variance for Q2 using this
transformation,
it is necessary to replace the values of j and j by
1
j 2 j and 1
j , respectively, in (5) and (6).
Once the mean and variance equations are developed for the
general case of unequal sample size, the next step in our
proposed methodology is to approximate Q1 and Q2 with normal
distributions.

3.2

Approximation of the MER

The approximation of MER is based on the assumption that
the distributions of the random variables, Q1 and Q2 ,
follow normal distributions as NðQ1 ; 2Q1 Þ and NðQ2 ; 2Q2 Þ.
Thus, the proposed equation to approximate MER is
!
!
k  Q1
k  Q2
~ 
~
MER ¼ 
ð8Þ
p1 þ 
p2 ;
Q1
Q 2

R 1 u2 =2
~
e
du is the upper tail area of the standard
where ðxÞ
¼ p1ﬃﬃﬃﬃ
2 x
normal distribution and k is defined as follows: k ¼
logðp2 Cð1j2Þ=p1 Cð2j1ÞÞ, where p1 and p2 are known as a priori
probabilities for 1 and 2 . Further, Cð2j1Þ denotes the cost of
misclassifying x into 2 and the other misclassification cost is
denoted by Cð1j2Þ.
The results provided by the approximation method were
validated by using simulation data sets. The reader is referred to
[26] for a detailed discussion of the validation results. The next
section provides the results of the feature selection examples
using MER as the evaluation criterion in the stepwise feature
selection method.

4

EXPERIMENTAL RESULTS

The performance of the proposed evaluation criterion is compared against some of the most common existing evaluation
criteria [23] and [24]. These evaluation criteria are: Wilk’s
Lambda, Unexplained Variance, Mahalanobis Distance, Smallest
F Ratio, and Rao’s V. The five methods selected the same feature
subset in all our experiments; thus, these methods are grouped in
single class and referred to as Conventional Methods (CM). The
first part of this section gives one example to validate using
simulation data, while the second part presents one example
using real inspection data.

4.1

Simulated Data

This experiment was performed using 1,000 simulated pseudorandom data points for two multivariate normal distributions
with 15 features (A to O) for two populations: 1 and 2 . The
mean vector for 1 was set to 1 ¼ 1;000 and mean vector for 2
to  2 ¼ 0. The diagonals of the covariance matrices for 1 and
2 were set to 1j ¼ 2j ¼ 400 þ 100j, for j ¼ 1 to 15. Five data
sets were generated using different levels of correlation between
the features. The levels of correlation used were 0.0, 0.25, 0.5,
0.75, and 0.99.
One example sequence of the feature selection process (with the
level of correlation equal to 0) is presented in Table 1. This table
also presents, in the third column, the values of the Wilks Lambda
associated with each subset. The last column is the value of MER
for each subset computed using the Approximation Method
(MER_AM).
From Table 1, it can be seen that one of the disadvantages of the
conventional methods is that the statistic used as the main
evaluation criteria does not convey meaningful absolute information about the resulting MER. For example, when the Wilks
Lambda statistic is used as the evaluation criterion, the values of

1342

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. 7, JULY 2009

TABLE 2
Summary of the Feature Selection Methods

this statistic do not provide information in terms of the expected
MER. Therefore, the users of the Conventional Methods do not
obtain information to interpret the values of the evaluation
criterion in terms of MER. This disadvantage is eliminated when
using the MER estimated by the proposed approximation method.
The summary of the feature selection experiments using the five
different levels of correlation and three methods to estimate MER is
presented in Table 2. The methods are the Conventional Methods,
the Approximation Method, and the Resubstitution Method. The
first column presents the level of correlation. In order to obtain the
different number of subsets, the MER for the 32,767 subsets was
calculated by using the resubstitution method. The number of
different subsets based on the MER is presented in the second
column for each correlation of the section Data. The MER of the
full subset (using the 15 features) and the position (rank) of this
subset with respect to the optimal subset based on the MER
obtained for each level of correlation are presented under the
section labeled “Full.” The summary of the feature selection
methods is presented in three sections in this table. These
sections are called: Conventional, Approximation, and Resubstitution. The selected subset of features is presented in the first
column (Subset) of each section. The second column (MER) of
each section presents the corresponding MER using the
resubstitution method. The next column (Size) shows the
number of features in the selected subset. The final column
(Rank) of each section gives the position of this subset with
respect to the optimal subset of features based on the MER.
Additionally, the last section of the table under the heading
“Optimal” shows the optimal subset with its corresponding MER
and a number of features. For the last two rows of the optimal
subset column, there are multiple subsets with the target MER
equal to zero; the size for these examples represents the number of
features in the largest subset.
The previous results support the hypothesis that the approximation method renders similar performance to those of the
conventional methods. The values obtained in the three factors
analyzed on the previous experiment are very similar. However, it
is necessary to explaIn that, in the case of the approximation
method, no threshold value was used for the inclusion or
elimination of features. This means that any reduction of the
MER was enough to include a feature in the final subset. For
instance, see Table 3, which shows the progression of the feature
selection process for the example with correlation equal to 0. For
instance, this table presents the feature that was added in each step
of the feature selection process. The first column gives the step
number of the stepwise process. The second column shows the
subset for each step of the process. Columns 3 and 4 present MER

estimated from the Approximation Method (denoted as MER_AM)
and the Resubstitution Method (denoted as MER_R), respectively.
The fifth column shows the value differential between consecutive
estimates of MER.
As an example, consider step 4 of Table 3. Feature D was
added/included to the best subset of step 3 {A, B, C}. This feature
was added/included because it provided the maximum reduction
to MER_AM. The marginal reduction of the MER_AM from feature
D was 0:0804  0:0646 ¼ 0:0158. As was explained before, the
threshold used to include a feature was 0. Therefore, any reduction
in the MER_AM of feature being considered is enough to be
included in the best subset. This threshold was used to understand
the experimental behavior of the MER_AM versus the sample size
of the best subset. However, this approach may not be useful from
a practical point of view. For example, the reduction in MER_AM
from step 11 to step 12 in Table 3 is 0.000461. This reduction was
enough to include feature L although the reduction of MER_AM
was minimal. In practice, this reduction may not justify including
this feature in the resulting subset.
Given that the MER_AM statistic used in the feature selection
process is very similar to MER obtained using the resubstitution
method (MER_R), it can be used to determine a stopping point in
the feature selection progression according to the needs of the
users. The similarity between the MER_AM and the MER_R can be
analyzed in Table 3. For instance, the information provided in this
table can be used to determine the optimal number of features

TABLE 3
Feature Selection Summary

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. 7,

JULY 2009

1343

TABLE 4
Feature Selection Summary

Fig. 1. Histograms of the EAL features.

from the cost/benefit trade-off including features in the final
subset. For example, a user can determine that an error of 0.05 is
acceptable for the classification of the populations. Therefore, in
the first experiment, the feature selection process should be
stopped at step 6 with the features {A, B, C, D, O, E} instead of
step 12 with the features {A, B, C, D, O, E, N, F, G, M, H, L}
because, in step 6, the level of discrimination determined by the
user was achieved. Another example is provided by assuming that
the user cannot afford more than seven features due to the
inspection cost/time constraints; therefore, the process should stop
with the subset {A, B, C, D, O, E, N} with an MER AM ¼ 0:0438050
and the MER R ¼ 0:0435. It is important to note that this process of
setting a prespecified stopping MER could not be achieved by
using conventional methods alone.
Regarding the time the AM takes to estimate an MER, it is
significantly lower than the alternative of running simulations as
proposed by [3]. For instance, an experiment that consisted of
running 100,000 simulation points in a personal computer with an
Intel Core 2CPU T5600 at 1.83 GHz to estimate the MER took 190 sec,
while the same estimation based on the AM took 2 seconds.

4.2

Automated Visual Inspection Data

This example is applied to the inspection of Surface-Mounted
Devices (SMD) to determine if these electronic components are
present or absent on a Printed Circuit Board (PCB). The
experimental data used to validate the feature construction
methodology were collected in an experimental AVI system
available at the Electronics Assembly Laboratory (EAL) at Arizona
State University. For a description of the experimental setting, the
reader is referred to [29].
The experimental results are based on a single class of SMD.
There are 281 components on the PCB used in this experiment. The
AVI system inspected one PCB with all the SMD components
present and another PCB with each one of the components absent.
Therefore, there were 281 elements for the training phase for each
population. Six inspection features were utilized in this analysis.
These features are called Energy (E), Correlation (C), Diffusion (D),
Fenergy (F), Texture (T), and Blob (B). The details of these features
were discussed in [29]. In order to conduct more feature selection
experiments, an assumption was made that the full data set was
composed of only five features instead of six. This assumption
gives six extra combinations of feature sets that can be analyzed as
separate experiments. For example, the full set of features is
labeled {ECDFBT}, under the new assumption, and the six new

feature sets are: {ECDFB}, {ECDFT}, {ECDBT}, {ECFBT}, {EDFBT},
and {CDFBT}.
The features used were selected to highlight the application of
the selection methodology and not for their discrimination power.
Fig. 1 depicts, with histograms, the discrimination provided by the
features to highlight the degree of separation between the Present
(1 ) and Absent (2 ) populations of components.
In order to evaluate the performance of the new evaluation
criterion, its performance is compared again to the Conventional
Methods (CM).

4.3

Feature Selection Summary

The summary of the seven experiments with two methods to
estimate MER is presented in Table 4. The first column
presents the number of features. The total number of different
subsets using the original number of features in each experiment is presented in the second column (Sets) of the section
Data. The MER of the full subset and the position of this
subset with respect to the optimal subset for each combination
of features are presented under the section labeled “Full.”
The results of the feature selection process using the two
methods to estimate MER are presented in the sections of the table
under the headings “Conventional” and “Approximation,” respectively. The selected subset of features is presented in the first
column of each section. The second column of each section
presents the number of features in the selected subset. The next
column shows the change of the position of this subset with respect
to the optimal subset of features based on MER using the
resubstitution method.
The previous results support the hypothesis that the approximation method renders similar or better performance as compared to those provided by conventional methods.

5

CONCLUSIONS

The main contribution of this research was the design of a new
feature selection methodology based on the “plug-in” Quadratic
Discriminant Function. One of the key contributions of the paper is
to conduct the feature selection process through an estimate of the
misclassification error rate obtained from the densities of the
stochastic representations of the conditional quadratic discriminant function.
The application of the proposed methodology will result in
significant savings of computational time in the estimation of MER
over the traditional simulation and cross-validation methods. This
will significantly shorten the time needed to reconfigure a
preexisting inspection system, a key feature for the emergence of
self-reconfigurable inspection systems; systems that will be better
capable to deal with the rapid introduction and retirement of
products.
The proposed methodology for feature selection was developed
assuming that the density functions of the stochastic representations of the conditional probabilities of the “plug-in” QDF follow
normal distributions. While this assumption was numerically
supported in this paper, it is necessary to further validate it.

1344

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

Another interesting topic for future research is to consider the
strategy to include or exclude more than one feature at a time.
Also, addressing marginal feature inclusion from an economical
point of view is an interesting area of future research. For example,
it could be useful to include economical factors such as the cost
and time required for including/excluding preexisting features.
This is something that is also left for future research.

ACKNOWLEDGMENTS
The authors would like to acknowledge the support provided by
the US National Science Foundation through grant DMI-0300361
for the realization of this research. They would also like to
acknowledge the helpful suggestions of the anonymous reviewers
of this paper.

REFERENCES
[1]
[2]
[3]

[4]

[5]

[6]
[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]
[21]
[22]

[23]

R. Duda, P. Hart, and D. Stork, Pattern Classification, second ed. John Wiley
& Sons, 2003.
L. Devroye, L. Gyorfi, and G. Lugosi, A Probabilistic Theory of Pattern
Recognition. Springer, 1996.
R. McFarland and D. Richard, “Exact Misclassification Probabilities for
Plug-In Normal Quadratic Discriminant Function—The Heterogeneous
Case,” J. Multivariate Analysis, vol. 82, pp. 299-330, 2002.
J. Hua, Z. Xiong, and E. Dougherty, “Determination of the Optimal
Number of Features for Quadratic Discriminant Analysis via the Normal
Approximation to the Discriminant Distribution,” Pattern Recognition,
vol. 38, pp. 403-421, 2005.
H.C. Garcia and J.R Villalobos, “Development of a Methodological
Framework for the Self Reconfiguration of Automated Visual Inspection
Systems,” Proc. Fifth Int’l Conf. Industrial Informatics, July 2007.
S. Kachigan, Multivariate Statistical Analysis. Radius Press, 1982.
A. Jain, R. Duin, and J. Mao, “Statistical Pattern Recognition: A Review,”
IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 4-37,
Jan. 2000.
J.A. Zongker, “Feature Selection: Evaluation, Application, and Small
Sample Performance,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 19, no. 2, pp. 153-158, Feb. 1997.
P. Mitra, C. Murthy, and S. Pal, “Unsupervised Feature Selection Using
Feature Similarity,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 24, no. 3, pp. 285-297, Mar. 2002.
Y. Saeys, I. Inza, and P. Larranaga, “A Review of Feature Selection
Techniques in Bioinformatics,” Bioinformatics, vol. 23, no. 19, pp. 2507-2517,
2007.
P. Liu, N. Wu, and J. Zhu, “A Unified Strategy of Feature Selection,” Proc.
Int’l Conf. Advanced Data Mining and Applications, pp. 457-464, 2006.
H. Wei and S. Billings, “Feature Subset Selection and Ranking for Data
Dimensionality Reduction,” IEEE Trans. Pattern Analysis and Machine
Intelligence, vol. 29, no. 1, pp. 162-166, Jan. 2007.
S. Sampatraj, K. Abhishek, and Y. Ding, “A Survey of Inspection Strategy
and Sensor Distribution Studies in Discrete-Part Manufacturing Processes,”
IIE Trans., vol. 38, no. 4, pp. 309-328, 2005.
I. Gunyon, S. Gunn, M. Nikravesh, and L. Zadeh, “Variable-Feature
Selection and Ensemble Learning: A Dual View,” Feature Extraction,
Foundations and Applications, I. Guyon, L. Zadeh, and M. Nikravesh, eds.,
Springer-Verlag, 2005.
S. Jiang, R. Kumar, and H. Garcia, “Optimal Sensor Selection for DiscreteEvent Systems with Partial Observation,” IEEE Trans. Automatic Control,
vol. 48, no. 3, pp. 369-381, Mar. 2003.
H.C. Garcia, J.R. Villalobos, and G. Runger, “Automated Feature Selection
for Visual Inspection Systems,” IEEE Trans. Automation Science and Eng.,
vol. 3, no. 4, pp. 394-406, Oct. 2006.
H. Liu and L. Yu, “Toward Integrating Feature Selection Algorithms for
Classification and Clustering,” IEEE Trans. Knowledge and Data Eng., vol. 17,
no. 4, pp. 491-502, Apr. 2005.
N. Louw and S. Steel, “Variable Selection in Kernel Fisher Discriminant
Analysis by Means of Recursive Feature Elimination,” Computational
Statistics and Data Analysis, vol. 51, no. 3, pp. 2043-2055, 2006.
M. Ashihara and S. Abe, “Feature Selection Based on Kernel Discriminant
Analysis,” Proc. Int’l Conf. Artificial Neural Networks, Part 2, pp. 282-291,
2006.
C. Le, Applied Categorical Data Analysis. Wiley, 1998.
R. Khattree and D. Naik, Multivariate Data Reduction and Discrimination with
SAS Software. Wiley-Interscience, 2000.
C. Park, J. Koo, and P. Kim, “Stepwise Feature Selection Using Generalized
Logistic loss,” Computational Statistics and Data Analysis, vol. 52, no. 7,
pp. 3709-3718, 2008.
A. Rencher, “The Contribution of Individual Variables to Hotelling T2 ,
Wilks and R2 ,” Biometrics, vol. 49, pp. 479-489, 1993.

[24]
[25]
[26]

[27]
[28]

[29]

VOL. 31,

NO. 7, JULY 2009

R. Jenrich, “Stepwise Discriminant Analysis,” Statistical Methods for Digital
Computers, K. Enslein, ed., John Wiley & Sons, 1997.
A. Rencher, Methods of Multivariate Analysis, second ed. John Wiley & Sons,
2002.
H.C. Garcia, “A Framework for the Self Reconfiguration of Automated
Visual Inspection Systems,” PhD dissertation, Dept. of Industrial Eng.,
Arizona State Univ., 2008.
T.W. Anderson, An Introduction to Multivariate Statistical Analysis, third ed.
Wiley, 1984.
A. Wald, “On a Statistical Problem Arising in the Classification of an
Individual into One of Two Groups,” The Annals of Math. Statistics, vol. 15,
no. 2, pp. 145-162, 1944.
J.R. Villalobos, M. Arellano, A. Medina, and F. Aguirre, “Vector
Classification of SMD Images,” J. Manufacturing Systems, vol. 22, no. 4,
pp. 265-282, 2004.

. For more information on this or any other computing topic, please visit our
Digital Library at www.computer.org/publications/dlib.

Journal of Heuristics, 7: 77–97 (2000)
c 2001 Kluwer Academic Publishers
°

Using Experimental Design to Find Effective
Parameter Settings for Heuristics
STEVEN P. COY
Continental Airlines, HQSRT, 1600 Smith Street, Houston, TX 77002, USA
BRUCE L. GOLDEN
Robert H. Smith School of Business, University of Maryland, College Park, MD 20742, USA
GEORGE C. RUNGER
College of Engineering and Applied Sciences, Arizona State University, Tempe, AZ 85287, USA
EDWARD A. WASIL∗
Kogod School of Business, American University, Washington, DC 20016, USA
email: ewasil@american.edu

Abstract
In this paper, we propose a procedure, based on statistical design of experiments and gradient descent, that finds
effective settings for parameters found in heuristics. We develop our procedure using four experiments. We use
our procedure and a small subset of problems to find parameter settings for two new vehicle routing heuristics.
We then set the parameters of each heuristic and solve 19 capacity-constrained and 15 capacity-constrained and
route-length-constrained vehicle routing problems ranging in size from 50 to 483 customers. We conclude that our
procedure is an effective method that deserves serious consideration by both researchers and operations research
practitioners.
Key Words: statistical design of experiments, heuristics, vehicle routing

1.

Introduction

Over the last 10 years or so, researchers have devoted an enormous effort to tailoring
general-purpose metaheuristics such as simulated annealing, genetic algorithms, neural
networks, and tabu search to solve difficult combinatorial optimization problems including
the traveling salesman problem (TSP) and the vehicle routing problem (VRP). The volume
edited by Reeves (1993) and the paper by Osman and Kelly (1996) provide comprehensive,
accessible overviews of metaheuristics for combinatorial optimization problems.
The efforts of researchers in developing effective metaheuristics have already met with
some success. For example, best-known solutions to the well-studied 14 benchmark VRPs
of Christofides, Mingozzi, and Toth (1979) have been generated by tabu search procedures
including those of Taillard (1993), Gendreau, Hertz, and Laporte (1994), and Xu and
Kelly (1996). However, most of the efforts to develop effective metaheuristics have been
computationally burdensome and very time consuming.
∗ Author

to whom all correspondence should be addressed.

78

COY ET AL.

All of the metaheuristics that have been used to solve the VRP contain several parameters
(roughly anywhere from five parameters to more than 25 parameters) whose values need
to be set before the metaheuristic is run (Golden et al. (1998) provide a comprehensive
survey of metaheuristics for the VRP). For example, the network flow-based tabu search
heuristic of Xu and Kelly (1996) has 28 penalty, time-related, and control parameters and
four parameters (such as the number of elite solutions to store) that determine how their
heuristic solves a problem.
Our review of the literature indicates that it is not an easy task to determine appropriate
values for parameters found in VRP metaheuristics. The procedures used to set a parameter’s
value have ranged from simple trial-and-error to sophisticated sensitivity analysis. For
example, Van Breedam (1995) states: “The values that have to be assigned to all these
technical parameters are for the greater part determined by trial-and-error-like experiments.”
Gendreau, Hertz, and Laporte (1991) state: “This algorithm contains several parameters
and a number of variants can easily be envisaged. Several tests and a considerable amount of
fine tuning were carried out in order to arrive at the current version.” Once appropriate values
for parameters have been identified, it is standard practice in the literature to report results
generated by running a heuristic with the parameters fixed at these values (for example, the
single pass version of TABUROUTE reported by Gendreau, Hertz, and Laporte (1994)).
However, it is also common practice to report the best solution found during the course of
performing sensitivity analysis (for example, the multiple passes of TABUROUTE reported
by Gendreau, Hertz, and Laporte (1994)).
While, for the most part, researchers have set values of parameters in an ad hoc way,
there are a few researchers who have developed systematic ways of identifying effective
values of parameters found in VRP heuristics. Xu and Kelly (1996) try to identify the
relative contributions of five different components of their tabu search heuristic (network
flow moves, swap moves, tabu short-term memory, restart/recovery strategy, a simple tabu
search procedure (TSTSP) to find the best sequence of customers on a route). They disable
each component one at a time, execute their algorithm on seven VRPs, and compare the
solutions of the five different strategies. Xu and Kelly conclude: “. . . the TS [tabu search]
memory and restart/recovery strategy effectively help to locate extremely good solutions
and TSTSP provides an effective enhancement over 3-opt . . . ” Furthermore, the authors
run their heuristic with different frequencies for the swap moves—every two, three, five,
and six iterations—and conclude: “. . . the performance of our algorithm is sensitive to the
frequency . . . [and it] performs best using a medium frequency.”
Van Breedam (1996) tries to determine the significant effects of parameters for a genetic
algorithm (GA) procedure and a simulated annealing (SA) procedure for the VRP. He
attempts to discover the structure of the relation between total travel time and seven GA
parameters (including population size, number of generations, type of local improvement
operator, and quality of initial solution) and eight SA parameters (including cooling rate
and type of move) by applying the Automatic Interaction Detection technique (AID) of
Morgan and Sonquist (1963). AID is a tree-based classification method that uses analysis
of variance to summarize the relationship between predictor and response variables.
Van Breedam applies GA and SA to 15 test problems with 100 customers each (four
problems have time windows, two have pickups, and three have heterogeneous demand).

USING EXPERIMENTAL DESIGN

79

He concludes that certain parameters have “consistent significant effect for all problems.”
For example, in the case of GA, not using a local improvement operator gives worse solutions
and using good initial solutions produces better final solutions.
The development of systematic procedures that determine appropriate values for parameters is not limited to VRP heuristics. Robertson, Golden, and Wasil (1998) use a fractional factorial experiment to set parameter values in neural network models for a finance
application. Park and Kim (1998) use a nonlinear response surface optimization method
based on a simplex design to find parameter settings in several applications of simulated
annealing. Parsons and Johnson (1997) use statistical design of experiments to set the values of four parameters in a genetic algorithm. Xu, Chiu, and Glover (1996), in the context
of the Steiner Tree-Star problem, develop a procedure for fine-tuning five key factors in a
tabu search heuristic. Using two statistical tests in a very small number of experiments,
they are able to find a set of values for the five factors that generates improved results in
nearly three-quarters of their test problems. (For background information on designing
computational experiments, see the article by Barr et al. (1995). The book by Montgomery
(1991) is an excellent introduction to the design and analysis of experiments.)
In this paper, we show how statistical design of experiments can be used to find effective
settings for parameters found in heuristics. In Section 2, we give an overview of our procedure. In Section 3, we illustrate our parameter setting procedure with a case study of the
VRP. In Section 4, we give our conclusions.
2.

Procedure for setting parameter values

Our procedure takes a small number of the problems from the entire problem set, finds
high-quality parameter settings for each problem, and then combines the parameter settings
to determine good parameter settings for the entire set of problems. Our procedure has four
steps, which are outlined in figure 1.
In Step 1, we select a subset of problems to analyze (analysis set) from the entire set of
problems (see figure 2). We select the problems so that most of the structural differences
(for example, demand distribution and customer distribution) found in the problem set are
represented in the analysis set. Since the time it takes to perform our procedure is directly
related to the size of the analysis set, we select as few problems as possible.
In Step 2, we determine a starting level for each parameter and the range over which the
parameter can vary. These decisions require some a priori knowledge of the behavior of
the heuristic on the specific problem class. If we do not have the necessary computational
experience, we conduct a pilot study. This study can be performed in a few trials on a small

Figure 1.

Outline of procedure used to set parameter values.

80

COY ET AL.

Figure 2. The pilot study problem set (P) is a proper subset of the analysis problem set (A), that is P ⊂ A. S is
the entire problem set.

number of problems (taken from the analysis set). The pilot study has three objectives. The
first objective is to obtain a rough approximation of the best setting for each parameter—
this acts as a starting point. The second objective is to identify the experimental ranges
for each parameter. In many cases, the range of a parameter is dictated by the heuristic.
For example, the heuristic may not converge if a particular parameter is less than zero. In
this case, we would set the minimum of this parameter to zero. The third objective is to
identify the amount to change each parameter during the experimental design phase. We
will discuss the third objective in more detail later in this section.
Our procedure uses a two-level factorial design. In a two-level factorial design, each
parameter is tested at a low level and high level. The low and high settings are usually
denoted as −1 and +1, respectively. This indicates that the tests are performed at design
center −1 and design center +1. A two-level full factorial design is often abbreviated
2k , where k is the number of parameters. For example, a full factorial design consisting
of two parameters has 22 combinations of the two levels of each parameter. We point out
that a partial two-level design such as a Taguchi design might provide quality results more
efficiently.
When there are more than a few parameters, it is usually necessary to use a fractional
factorial design. A fractional factorial design is often abbreviated as a 2k− p where 1/2 p is
the size of the fraction. For example, a half-fraction of a three parameter factorial design,
23−1 , requires four runs (see Table 1). We plot an example of a 23−1 design in figure 3. The
choice of a fractional factorial design (for example, a half fraction or a quarter fraction)
consists of a trade-off between the desire to conduct as few experimental runs as possible
with the need to conduct enough runs to identify the significant effects.
The triples in figure 3 are called the coded variables associated with the factorial design.
Statisticians typically convert the natural variables (value of the parameter after adding or
subtracting 1) to a matrix of +1s and −1s (matrix of coded variables). In figure 3, we
see that the design center is located at (0, 0, 0). This is often called a zero point. An
experimental design is typically augmented with one or more zero points to estimate the
process behavior at the design center (for further details, see Montgomery (1991)).

81

USING EXPERIMENTAL DESIGN

Table 1.

A 23−1 fractional factorial design.

Run

A

B

C

1

−

−

+

2

+

−

−

3

−

+

−

4

+

+

+

Figure 3. Geometric interpretation of a 23−1 factorial design where 11 = 12 = 13 = 1. The triples are the
coded variables associated with the entries in Table 1.

We use the factorial experimental design to determine the parameter settings for each
experimental run. After we complete all of the experimental runs, we apply linear regression
to the results obtained from each run to find a linear approximation of the response surface
(the response is the quantity that we are trying to optimize with the heuristic). Next, we
calculate the path of steepest descent on the response surface (if the objective is to minimize),
using the starting point identified in the pilot study, and we make small steps along this path
by changing the parameter values. At each step, we conduct one or more trials (a trial is
one execution of a heuristic from a single initial solution). We continue until we reach the
limit of the experimental region or the best solution found has not changed for a specified
number of steps. At this point, we save the settings of each parameter (parameter vector)
associated with the minimum result.
In Step 4, we determine the final parameter settings for the heuristic by taking the average
of the parameter vectors that we obtained in Step 3 for each problem in the analysis set.
We point out that the response surface of a particular problem might not be convex. Thus,
moving down the path of steepest descent will probably not provide an optimal solution. In
fact, we do not perform exact optimization on any single problem (for example, by fitting
a quadratic model after the descent step; see Montgomery (1991) for details), so that it is

82

COY ET AL.

unlikely we will determine the exact local minimum. However, since we want to determine
parameter settings that work well over a number of similar problems with different response
surfaces, searching for the global minimum or exactly identifying the local minimum of
each problem in the analysis set would only add complexity to the procedure without a
substantial contribution to our overall objective.
3.

Case study

To illustrate our procedure, we use two local search heuristics to solve a total of 34 VRPs.
We report on four experiments. In each experiment, we apply a variant of Lagrangean
relaxation and an edge exchange procedure to the VRPs. In the first experiment, we solve
19 capacity-constrained VRPs with a Lagrangean-relaxed version of two-opt (LT) and in
the second experiment, we solve the same 19 VRPs with Lagrangean-relaxed sequential
smoothing (LS) (see Coy (1998) and Coy et al. (1997)) for a description of sequential
smoothing). In the third and fourth experiments, we solve 15 capacity-constrained and
route-length-constrained VRPs with Lagrangean relaxed two-opt and Lagrangean-relaxed
sequential smoothing, respectively. The Lagrangean relaxation procedure used by both
heuristics requires several parameters. In each experiment, we use statistical design of
experiments to set the parameter values.
3.1.

Background

In the capacity-constrained vehicle routing problem, a homogeneous fleet of vehicles with
limited capacity delivers items from a single depot to a set of customers with known demands. The objective of the VRP is to determine routes for the vehicles so that the sum
of the route lengths is minimized subject to the following constraints. Each customer can
be visited only once, each customer’s demand must be satisfied, the total demand on each
route may not exceed vehicle capacity, and each route must begin and end at the depot.
Additional constraints may include a route-length restriction and delivery time windows. In
this paper, we concentrate on the capacity-constrained VRP and on the capacity-constrained
VRP with route-length restrictions.
The articles by Bodin et al. (1983) and Laporte (1992), the edited volume by Golden and
Assad (1988) and the first five chapters in Ball et al. (1995) provide a survey of optimal and
heuristic methods for the capacity-constrained VRP and its variants. Gendreau, Laporte,
and Potvin (1997) and Golden et al. (1998) provide extensive reviews of metaheuristics for
the VRP.
3.2.

Problem sets

We use two problem sets in our experiments—the 14 problems of Christofides, Mingozzi,
and Toth (1979) (denoted CMT) and the 20 large-scale problems of Golden et al. (1998).
The characteristics of each problem set are described in Table 2. The problems range in size
from N = 50 to N = 483 customers. Fifteen problems have route-length restrictions.

83

USING EXPERIMENTAL DESIGN

Table 2. Problem sets used in our experiments. Problems 1 to 14 are from
CMT and problems 15 to 34 are from Golden et al. (1998).

N

Vehicle
capacity

50

160

∞

0

2

75

140

∞

0

3

100

200

∞

0

4

150

200

∞

0

5

199

200

∞

0

6

50

160

200

10

7

75

140

160

10

8

100

200

230

10

9

150

200

200

10

10

199

200

200

10

11

120

200

∞

0

12

100

200

∞

0

13

120

200

720

50

14

100

200

1040

90

15

240

550

650

0

16

320

700

900

0

17

400

900

1200

0

18

480

1000

1600

0

19

200

900

1800

0

20

280

900

1500

0

21

360

900

1300

0

22

440

900

1200

0

23

255

1000

∞

0

24

323

1000

∞

0

25

399

1000

∞

0

26

483

1000

∞

0

27

252

1000

∞

0

28

320

1000

∞

0

29

396

1000

∞

0

30

480

1000

∞

0

31

240

200

∞

0

32

300

200

∞

0

33

360

200

∞

0

34

420

200

∞

0

Problem
1

Max route
length

Service Time

84
3.3.

COY ET AL.

Parameters used by each heuristic

Both heuristics in our experiments use a variant of Lagrangean relaxation. In a Lagrangeanrelaxed VRP, the capacity constraints are relaxed and a penalty term is added to the objective
function. The penalty term is formulated so that if an infeasible condition exists, the objective
function is penalized. For example, if a route exceeds the capacity restriction, the penalty
term will add length to the objective function value. Infeasible moves are possible under
these conditions as long as the total route length reduction exceeds the penalty incurred. The
penalty term has an adjustable constant, λC . If λC is too small, the edge exchange procedure
will converge without finding a feasible solution. If λC is too large, the edge exchange
procedure may converge to a poor local minimum. Thus, a key feature of an implementation
of Lagrangean relaxation is the search for the right λC . Each of the parameters that we set
with our procedure is used to control this search. The function of each parameter is described
in Table 3. For further details, see Stewart and Golden (1984) and Coy (1998).
3.4.

Experiments 1 and 2

In this section, we illustrate the procedure outlined in figure 1. In Experiment 1, we give
a detailed description of each step of the procedure. In Experiment 2, we summarize the
results of the four steps (a full description of Experiment 2 is provided by Coy (1998)).
Since the two heuristics use the same Lagrangean relaxation procedure, we perform Step 1
and Step 2 only once and use the decisions made in these steps for both experiments.
3.4.1 Step 1. In Step 1, we need to select a subset of the 19 capacity-constrained problems
to form our analysis set. We want to select problems that are representative of the characteristics (problem size, distribution of customer location, and distribution of demand) found
Table 3.

Parameters set in Experiments 1 and 2 with design of experiments.

Parameter name

Description

Initial capacity lambda (λC0 )

First and smallest λC used in search

Large factor (LF)

Until the first feasible result has been found, increase λC and λD
by LF (e.g., λC1 = λC0 × LF)

Small factor (SF)

After a feasible result has been found, increase λC and λD
by 1 + SF × (1 − LF)

Excess load upper bound (LUB)

Used to determine whether solution is “close” to feasible; if total
excess load/number of routes ≤ LUB and the solution is either
feasible or close to feasible with respect to the route-length
constraint, attempt to force feasibility

Temporary lambda factor (λT )

When attempting to force feasibility, temporarily increase λC by
λT if the solution is infeasible with respect to vehicle capacity
and temporarily increase λD by λT if the solution is infeasible
with respect to the maximum route-length constraints

Number of feasible solutions (NFS)

Number of feasible solutions found on each trial

USING EXPERIMENTAL DESIGN

85

Figure 4. Three-dimensional plots of the four problems in the analysis set for Experiment 1 and Experiment 2.
Demand at each customer is shown on the vertical axis.

in the entire set of problems. In addition, since the time it takes to run our experiments is
related to the number of problems in the analysis set—the larger the number of problems,
the more time required to run our experiments—we select a small number of problems. We
require an analysis set that can be analyzed in a reasonable amount of time and produce
good average parameter values. We select four problems: 1, 5, 26, and 28. This analysis
set contains the smallest problem, the largest problem, and two mid-size problems. Two
problems have a random distribution of customers and random distribution of demand and
two problems have a symmetric arrangement of customers and clustered demand.
In figure 4, we illustrate the four problems in the analysis set. The x and y axes are
located on the bottom of each plot. The quantity of demand at each location is located on
the vertical axis. From these plots, we can determine how the customers are distributed and
how customer demand is distributed. For example, the plot of Problem 28 indicates that
the customers are placed at the corners of a grid and the locations with the highest demands
are located close to the depot.
3.4.2 Step 2. In Step 2, we make several decisions in order to initialize our procedure.
We choose an initial set of parameter values (design center), the size of the incremental
change of each parameter (1), and the limits of the experimental region. In preliminary
tests, we gained enough computational experience to make these choices for each of the
four experiments. In Table 4, we show the values that we use to initialize our procedure.
Based on the behavior of the two heuristics and the fact that our procedure relies on gradient
descent, we do not need to place upper limits on the parameters.

86

COY ET AL.
Table 4. Minimum value, design center, and incremental change for
each parameter used in Experiment 1 and Experiment 2.
Min value

Design center

1

λC0

0.01

0.6000

0.3000

LF

1.01

1.2500

0.1500

SF

0.01

0.5000

0.2500

Parameter

LUB

0.00

0.1500

0.1000

λT

1.01

1.7500

0.5000

NFS

1.00

6.0000

3.0000

If we did not have sufficient computational experience to initialize our procedure, we
could perform a pilot study using problems from the analysis set. The pilot study could be
used to determine rough approximations for the experimental region, the design center, and
the 1 for each parameter. We sketch a pilot study in the next paragraph.
To begin the pilot study, we use a trial-and-error approach with the problems in the
pilot study problem set to find a set of parameter values in which the heuristic performs
reasonably well. We call this set of parameter values the design center. Next, we identify
extreme values for each parameter. Briefly, we choose one parameter, denoted by P, and
hold the remaining parameters values constant at the design center. We increase the value
of P. After each increase, we conduct a trial on each of the problems in the pilot study
problem set and we evaluate how well the heuristic performs. When the heuristic stops
performing well (for example, the heuristic is taking far too long to process or the solutions
are very poor), we stop increasing P and call the current value of P the maximum value
for P. We then decrease the value of P in order to identify its minimum value. We repeat
this procedure with the other parameters. In the process of identifying the extreme values
for each parameter, we vary the incremental change of the parameter value. Using this
experience, we estimate how large a change in the parameter value is necessary to make a
significant difference in the performance of the heuristic and we call this value the 1 for
the parameter.
3.4.3 Step 3. In Step 3, we perform the nine steps shown in figure 5. We begin by
choosing a fractional factorial design. For Experiment 1 and Experiment 2, we choose a
fractional factorial experimental design with 26−1 = 32 runs. This allows us to conduct
each experiment with half of the runs of a full factorial design. We transform the fractional
factorial design to a matrix of coded variables and we augment our experimental design with
one zero point. The augmented matrix of coded variables for Experiment 1 and Experiment 2
is shown in Table 5.
We calculate the parameter values for each run by taking the value of a parameter at the
design center and either adding or subtracting 1. For example, to find the value for λC0 on
the first run, we take the value of λC0 at the design center and the size of the corresponding
1 (0.6 and 0.3, respectively from Table 4). We then add or subtract 1 depending on the
sign of the coded variable in Table 5 to obtain 0.6 − 0.3 = 0.3. The entire parameter vector

USING EXPERIMENTAL DESIGN

Figure 5.

87

Design of experiments procedure used for setting the parameters of a VRP heuristic.

for the first run is (0.3, 0.1, 0.25, 0.25, 2.25, 3). After computing the parameter vectors
associated with each row in the fractional factorial design, we conduct a set of five trials
for each parameter vector. We begin each trial from one of five solutions to the traveling
salesman problem (an infeasible solution to the VRP having only one route). We generate
each solution with the randomized greedy heuristic described by Johnson and McGeoch
(1997).
For each experiment, we construct a data set that has an augmented matrix of coded
variables (independent variables) and the average result from each run (dependent variable).
We then fit a linear model to the data set using linear regression to obtain an estimate of
the response surface. We point out that, since we use the same number of trials for all
parameter settings, the fitted linear regression model from the raw trial data is the same as
the model we fit from the averaged data. Consequently, the calculated gradient is the same.
The significance tests and R 2 values are more significant for the averaged data. Another
researcher with the same problem and parameters settings would obtain different solutions
from the randomized greedy heuristic used for initial solutions. We average over the runs to
provide an analysis that is somewhat less sensitive to this randomization—one that another
researcher could, at least on the average, duplicate.
For Experiments 1 and 2, we find that the linear regression models are significant at the
0.01 level and have adjusted R 2 values that range from 0.42 to 0.96. Each parameter that
we set with our procedure is statistically significant in at least one of the linear models. The
linear models that we use for each problem are described in Table 6.
It is critical to choose the 1 of each parameter so that the behavior of the process will
vary enough to allow a significant linear fit (using the F test). If a linear model does not fit
(F test fails), we recommend that the 1 of each parameter should be increased by as much
as 50% to 100% and that a new replication of the experiment should be performed. We use
the linear estimate of the response surface to determine how to set the parameter values.
Since we are minimizing, we find the path of steepest descent on the response surface.
The gradient of the linear model is the vector b = (b1 , b2 ,. . . , bk ) where b j is an estimated
regression coefficient. The path of steepest descent is the negative gradient of the linear
model (−b). To move along the path of steepest descent, a change of one unit in the coded
variable, x j , corresponds to a change of bi /b j units (coded) in variable xi . In order to make

88

COY ET AL.

Table 5. Augmented matrix of coded variables used in Experiments 1 and 2.
Parameters
λC0

LF

SF

LUB

λT

1

−1

−1

−1

+1

+1

−1

2

+1

−1

−1

+1

+1

+1

3

−1

+1

−1

+1

+1

+1

4

+1

+1

−1

+1

+1

−1
+1

Run

NFS

5

−1

−1

+1

+1

+1

6

+1

−1

+1

+1

+1

−1

7

−1

+1

+1

+1

+1

−1

8

+1

+1

+1

+1

+1

+1
+1

9

−1

−1

−1

−1

+1

10

+1

−1

−1

−1

+1

−1

11

−1

+1

−1

−1

+1

−1

12

+1

+1

−1

−1

+1

+1

13

−1

−1

+1

−1

+1

−1

14

+1

−1

+1

−1

+1

+1

15

−1

+1

+1

−1

+1

+1

16

+1

+1

+1

−1

+1

−1

17

−1

−1

−1

+1

−1

+1

18

+1

−1

−1

+1

−1

−1

19

−1

+1

−1

+1

−1

−1

20

+1

+1

−1

+1

−1

+1

21

−1

−1

+1

+1

−1

−1

22

+1

−1

+1

+1

−1

+1

23

−1

+1

+1

+1

−1

+1

24

+1

+1

+1

+1

−1

−1

25

−1

−1

−1

−1

−1

−1

26

+1

−1

−1

−1

−1

+1

27

−1

+1

−1

−1

−1

+1

28

+1

+1

−1

−1

−1

−1

29

−1

−1

+1

−1

−1

+1

30

+1

−1

+1

−1

−1

−1

31

−1

+1

+1

−1

−1

−1

32

+1

+1

+1

−1

−1

+1

33

0

0

0

0

0

0

89

USING EXPERIMENTAL DESIGN

Table 6. Coefficients of linear models from Experiments 1 and 2. All models are significant at the 0.01 level.
A zero indicates that the parameter is not significant at the 0.05 level.
Problem

Adj.R2

Intercept

λC0

LF

SF

LUB

λT

NFS

Experiment 1 (LT)
1

0.581

550.276

3.14865

0.00000

0.00000

0.00000

0.00000

−3.35173

5

0.677

1397.097

7.96334

2.59741

3.09920

0.00000

0.00000

−3.68940

28

0.447

1191.016

3.22946

3.42525

0.00000

0.00000

0.00000

0.00000

26

0.440

1247.358

2.82791

2.89954

0.00000

0.00000

0.00000

−3.42412

Experiment 2 (LS)
1

0.966

551.621

12.93426

0.00000

0.00000

−1.46257

0.00000

0.00000

5

0.758

1360.410

3.79195

1.41637

0.00000

0.00000

0.00000

−2.93175

28

0.596

1164.822

3.81332

0.00000

0.00000

0.00000

0.00000

−1.92033

26

0.420

1202.462

1.86986

1.71696

1.87747

0.00000 −1.62750 −1.53584

a one unit (coded) change in the variable with the maximum coefficient, we divide each
coefficient by the absolute value of the maximum coefficient in the model (bm ). To calculate
the step size (uncoded), we multiply each ratio b j /bm by 1 j .
To illustrate this technique, we demonstrate how to calculate a step along the path of
steepest descent using Problem 1 in Experiment 1 (see Table 6). We begin by finding
the regression coefficient with the largest absolute value. The largest absolute coefficient
is 3.35173 (the coefficient of NFS). We then divide each parameter’s coefficient by this
quantity and multiply each of these results by the parameter’s 1. The result is a full step
for each parameter. The step size for λC0 , is (3.14865/3.35173) × 0.3 = .2818, the step
size for NFS is (3.35173/3.35173) × 3 = 3, and the step sizes for the remaining parameters
are 0 (these parameters are not statistically significant).
In the last three steps of our procedure (Steps 3.7 to 3.9), we make steps along the path of
steepest descent. Starting at the design center, we subtract the step size of each parameter
from the previous level of each parameter. If the next step along the path will cause one of
the parameters to go outside the experimental region, we hold that parameter constant and
continue making steps with the other parameters. After calculating a step, we perform a
set of five trials to determine the performance of the heuristic at this point (using the same
five initial solutions used earlier). We continue making steps along the path until we fail to
improve the response for two full steps or when all of the parameters reach the limit of the
experimental region. Finally, we select the parameter settings associated with the minimum
response.
Table 7 shows our results for Problem 1 in Experiment 1. (In the remainder of the section,
we use the term length to refer to the sum of the route lengths in the VRP.) Using the step
sizes calculated above, we make 1/4 steps down the path of steepest descent (we make small
steps to avoid stepping over potentially good local minima) by subtracting 1/4 × the step
size from the value of each parameter in the previous parameter vector. For example, Step
0 in Table 7 represents the design center. We calculate the parameter vector for Step 1 in
the following manner. The value for λC0 is 0.6 − 1/4 × 0.2818 = 0.6 − .07045 = 0.52955,

90

COY ET AL.
Table 7. Setting parameters for Problem 1 in Experiment 1. The parameter vector for
each set of trials is 1/4 step down the path of steepest descent from the parameter vector
used in the previous set of trials.
Parameters
Step

λC0

LF

SF

LUB

λT

NFS

Average Average
length times (s)

0

0.6000

1.2500

0.5000

0.1500

1.7500

6.0000

553.54

0.36

1

0.5295

1.2500

0.5000

0.1500

1.7500

6.7500

544.45

0.41

2

0.4591

1.2500

0.5000

0.1500

1.7500

7.5000

546.06

0.42

3

0.3886

1.2500

0.5000

0.1500

1.7500

8.2500

546.42

0.50

4

0.3182

1.2500

0.5000

0.1500

1.7500

9.0000

542.16

0.54

5

0.2477

1.2500

0.5000

0.1500

1.7500

9.7500

542.61

0.61

6

0.1773

1.2500

0.5000

0.1500

1.7500

10.5000

543.35

0.68

7

0.1068

1.2500

0.5000

0.1500

1.7500

11.2500

539.39

0.74

8

0.0364

1.2500

0.5000

0.1500

1.7500

12.0000

540.47

0.85

9

0.0364

1.2500

0.5000

0.1500

1.7500

12.7500

540.47

0.90

10

0.0364

1.2500

0.5000

0.1500

1.7500

13.5000

540.47

0.94

11

0.0364

1.2500

0.5000

0.1500

1.7500

14.2500

540.47

0.93

12

0.0364

1.2500

0.5000

0.1500

1.7500

15.0000

540.47

0.96

13

0.0364

1.2500

0.5000

0.1500

1.7500

15.7500

540.47

1.03

14

0.0364

1.2500

0.5000

0.1500

1.7500

16.5000

540.47

1.08

15

0.0364

1.2500

0.5000

0.1500

1.7500

17.2500

540.47

1.07

the value for parameter NFS is 6 − 1/4 × (−3) = 6.75, and the values of LF, SF, LUB, and
λT do not change.
We chose to make several small steps along the gradient of descent rather than two large
steps. We did this so as not to step over a good local minimum along the path. Clearly,
our method is a compromise between performance and complexity. A traditional statistical
approach would make full steps down the path, stop after two steps, and fit a quadratic
model to the response surface to find the local minimum. Exact optimization based on
a quadratic model for one particular problem adds complexity to the procedure without
substantially improving our ability to determine parameter settings that work reasonably
well over a number of similar problems. Furthermore, smaller initial values for delta would
no doubt improve the solution to a specific problem, but again contribute little to our overall
objective.
After making a step, we conduct five trials using the same five initial solutions and the
current set of parameter settings. We then average the results of the five trials. In this
problem, we found the best average length on Step 7. In Step 9, λC0 did not decrease,
because the next step would have gone outside the experimental region. In this case, we
hold λC0 constant and we continue to make steps with NFS until we meet the stopping
criterion at Step 15.
In figure 6, we summarize the response surface procedure for each of the four test
problems. The panels on the left show the average length of each trial at each step. (We

USING EXPERIMENTAL DESIGN

Figure 6.

91

Response surface optimization for Experiment 1.

point out that some smoothing of the curve would assist in the location of a minimizing
step. However, our objective is to make a recommendation for a class of problems and the
additional accuracy is not necessary for any specific problem.) The panels on the right show
the average processing time for each trial at each step (that is, the average CPU time it took
to complete one trial). The top two panels (Problem 1) show the results given in Table 7.
We find the minimum result at Step 7 and the average length plateaus at a slightly higher
value until the search stops at Step 15. This indicates that decreasing λC0 in Step 8 does not
improve the average result and that the subsequent increase in NFS adds processing time,
but does not change the average length.
Problem 5 has four significant parameters, λC0 , LF, SF, and NFS (see Table 6). In
this problem, reducing the values of LF and SF (each has a positive coefficient) causes

92

COY ET AL.

processing time to increase rapidly for most of the steps. By Step 20, λC0 , LF, and SF
reach their minimum values. Increasing NFS (the only parameter that is still within the
experimental region) adds processing time at a slower rate, but does not reduce the average
length.
In Problem 28, NFS is not statistically significant. In this case, we find a minimum at
Step 6. The procedure stops at Step 9, since any further changes would cause the values of
λC0 and LF to leave the experimental region.
In Problem 26, we find the minimum average length at Step 12 and stop testing at Step
20 since no additional improvement has been made for eight 1/4 steps. This problem has
three statistically significant parameters (λC0 , LF, NFS). By Step 9, λC0 and LF reach the
limit of the experimental region. For the remaining steps, we increase NFS. In contrast
to Problems 1 and 5, increasing NFS, after the other parameters have reached their limits,
does decrease the average length.
3.4.4 Step 4. In the final step of our procedure, we average the parameter values produced
by our response surface procedure to obtain the final parameter values for the experiment.
In Table 8, we show the parameter values for each problem and the average parameter values
for both experiments.
3.4.5 Computational results. We solve all 19 capacity-constrained problems with LT and
LS using the average parameter values. The computational results are given in Table 9.
On average, LT generates solutions that are 4.81% above the best-known solutions on all
19 problems. On average, LS generates solutions that are 3.43% above the best-known
solutions on all 19 problems.
Table 8.

Parameter vectors for Experiments 1 and 2.
λC

LF

SF

LUB

λT

NFS

1

0.1068

1.2500

0.5000

0.1500

1.7500

11.2500

5

0.0750

1.0176

0.0135

0.1500

1.7500

12.9495

28

0.1757

1.0250

0.5000

0.1500

1.7500

6.0000

26

0.0425

1.0277

0.5000

0.1500

1.7500

15.0000

Average

0.1000

1.0801

0.3784

0.1500

1.7500

11.2999

1

0.0750

1.2500

0.5000

0.1698

1.7500

6.0000

5

0.0750

1.0819

0.2199

0.1500

1.7500

12.9583

28

0.3750

1.2500

0.5000

0.1500

1.7500

7.1331

26

0.3759

1.1472

0.3125

0.1500

2.0751

7.8406

Average

0.2252

1.1823

0.3831

0.1549

1.8313

8.4830

Problem
Experiment 1 (LT)

Experiment 2 (LS)

93

USING EXPERIMENTAL DESIGN
Table 9. Computational results from Experiments 1 and 2. Minimum result from 25 trials on each
problem.
Experiment 1
Problem

N

Best known

LT

50

524.61

524.61

0.61

524.61

0.95

2

75

835.26

850.88

1.95

849.84

2.27

3

100

826.14

837.22

2.07

836.36

2.36

4

150

1028.42

1054.23

5.68

1054.17

7.01

5

199

1291.45

1350.03

12.06

1340.53

16.48

11

100

1042.11

1046.91

3.15

1047.73

5.05

12

120

819.56

819.56

2.38

819.56

3.03

23

255

587.09

630.31

17.03

614.27

28.99

24

323

746.56

799.23

28.26

782.48

54.38

25

399

932.68

996.43

48.91

973.54

94.23

26

483

1137.18

1214.66

78.29

1177.14

147.38

27

252

881.04

913.41

27.80

902.76

31.83

28

320

1103.69

1168.24

46.28

1146.17

53.26

29

396

1364.23

1447.97

76.10

1416.65

86.26

30

480

1656.66

1744.97

121.73

1714.70

132.36

31

240

666.84

733.87

17.02

724.68

15.55

32

300

973.60

1058.85

30.34

1040.33

31.70

33

360

1338.78

1446.30

48.60

1409.63

51.47

34

420

1831.62

1918.26

73.01

1896.72

87.96

1

3.5.

Time (min)

Experiment 2
LS

Time (min)

Capacity-constrained and distance-constrained problems

Using the procedure described above to set the parameter values, we solve 15 capacity
and route-length-constrained problems with Lagrangean-relaxed two-opt and Lagrangeanrelaxed sequential smoothing. To accommodate the route-length constraint, the two heuristics require two additional parameters. We use a 28−2 fractional factorial experimental
design for the procedure.
We solve all 15 capacity-constrained and route-length-constrained problems with LT and
LS using the average parameter values. The computational results are given in Table 10.
On average, LT generates solutions that are 1.63% above the best-known solutions on
all 15 problems. On average, LS generates solutions that are 0.80% above the best-known
solutions on all 15 problems. In addition, LT generates two new best-known solutions and
LS generates five new best-known solutions. The routes for all of the new best-known
solutions as well as a detailed presentation of Experiments 3 and 4 are provided by Coy
(1998).

94

COY ET AL.
Table 10. Final results from Experiments 3 and 4. Each result is the minimum from 25 trials on each
problem. Bold print indicates a new best-known solution.
Experiment 3
Problem
6

3.6.

N

Best known

LT

Experiment 4

Time (min)

LS

Time (min)

50

555.43

561.77

0.95

560.89

1.70

7

75

909.68

937.33

3.48

929.19

8.69

8

100

865.94

879.07

3.27

866.87

5.20

9

150

1162.55

1189.08

14.18

1186.70

38.81

10

199

1395.85

1480.08

25.41

1443.95

69.95

13

100

1541.14

1561.45

9.02

1552.90

26.73

14

120

866.37

869.20

2.96

866.53

6.53

15

240

5646.46

5862.58

31.54

5761.64

49.92

16

320

8566.04

8606.01

62.18

8512.72

65.00

17

400

11649.06

11200.38

74.41

11242.54

84.43

18

480

14639.32

14031.06

53.80

13782.41

43.49

19

200

6702.73

6508.26

3.90

6466.68

12.38

20

280

9016.93

8540.19

10.36

8540.74

11.84

21

360

11047.69

10415.84

37.14

10334.90

36.24

22

440

12250.06

12042.50

111.29

11957.15

99.04

Summary of computational results

In this section, we compare the quality of the solutions generated by LT and LS to the
solutions generated by the tabu search heuristic (TS) of Xu and Kelly (1996) and the
record-to-record travel heuristic (RTR) of Golden et al. (1998).
In Tables 11 and 12, we summarize the performance of the four heuristics. Over all 34
problems, we make the following observations. LS is first in solution quality and third in
running time. It averages 2.27% above the best-known solutions and its running time is
approximately 1,412 minutes (23.5 hours). TS is a close second in solution quality and
fourth in running time. It averages 2.44% above the best-known solutions and its running
time is approximately 54,232 minutes (903.9 hours). RTR is third in solution quality and
first in running time. It averages 3.04% above the best-known solutions and its running
time is approximately 935 minutes (15.6 hours). LT is fourth in solution quality and second
in running time. It averages 3.41% above the best-known solutions and its running time is
approximately 1,085 minutes (18.1 hours).
Finally, we conduct the following experiment in order to examine how results produced
by our tuned parameters compare to results produced by random parameters. We generate
10 random parameter vectors where the values of the six parameters (λC0 , LF, SF, LUB,
λT , NFS) are selected from a uniform distribution on the interval (Minimum parameter
value, 2 × Design center − Minimum parameter value). We then run LT and LS using the
10 random parameter vectors on each of the 19 capacity-constrained problems. On each

95

USING EXPERIMENTAL DESIGN
Table 11. Percent above the best-known solutions
for the LT, LS, TS, and RTR heuristics. Bold print
indicates the best result for a particular category.
Problem set

LT

LS

TS

RTR

Capacity-constrained
CMT

1.53

1.40

0.09

2.10

Large-scale

6.72

4.61

0.25

1.79

Overall

4.81

3.43

0.19

1.91

Capacity-constrained and route-length-constrained
CMT

2.24

1.36

3.62

3.34

Large-scale

1.11

0.30

6.92

5.47

Overall

1.63

0.80

5.50

4.48

CMT

1.88

1.38

1.72

2.72

Large-scale

4.47

2.89

2.91

3.26

Overall

3.41

2.27

2.44

3.04

All problems

Table 12. Running times (in minutes) for the LT, LS, TS, and
RTR heuristics. Bold print indicates the best result for a particular
category.
Problem set

LT

LS

TS

RTR

Capacity-constrained
CMT

27.90

37.15

927.80

76.84

Large-scale

613.37

815.37

42145.56

390.10

Overall

641.27

852.52

43073.36

466.94

Capacity-constrained and route-length-constrained
CMT

59.27

157.60

1122.13

115.46

Large-scale

384.63

402.35

10037.12

352.94

Overall

443.89

559.96

11159.25

468.40

All problems
CMT
Large-scale
Overall

87.17

194.75

2049.93

192.30

998.00

1217.72

52182.68

743.04

1085.16

1412.48

54232.61

935.34

problem, we start each heuristic from the same 25 initial solutions used in our previous
experiments.
In Table 13, we give the average percent above the best-known solutions for LT and LS
using tuned parameter vectors and the 10 random parameter vectors on the 19 capacityconstrained problems. We observe that the average of 6.47% produced by LT with tuned
parameters is smaller than all 10 averages produced by LT with random parameters, and

96

COY ET AL.
Table 13. Average percent above the best-known solutions for LT and LS heuristics using tuned
and randomly generated parameter vectors on the 19 capacity-constrained problems.
Random vector
Tuned
Vector

1

2

3

4

5

6

7

8

9

10

LT

6.47

7.79

8.04

6.73

9.13

7.76

8.78

8.17

8.16

7.23

7.53

LS

4.61

6.18

6.21

4.71

6.79

6.15

6.51

5.94

6.05

4.72

4.82

the average of 4.61% produced by LS with tuned parameters is also smaller than all 10
averages produced by LS with random parameters.
4.

Conclusions

In this paper, we propose a procedure based on statistical design of experiments that systematically selects high-quality parameter values. Our parameter setting procedure has four
steps. In the first step, we select a subset of problems to analyze from the entire set of
problems. In the second step, we use computational experience to select the starting level
of each parameter, the range over which each parameter will be varied, and the amount
to change each parameter. In the third step, we select good parameter settings for each
problem in the analysis set using statistical design of experiments and response surface
optimization. In the fourth step, we average the parameter values obtained in the third
step to obtain high-quality parameter values. Using our procedure, we set the values of
parameters and run our heuristics on 34 test problems that range in size from 50 to 483
customers. Our computational results show that LT and LS are reasonably effective in terms
of solution quality. The accuracy of LS is comparable to tabu search and record-to-record
travel. Furthermore, LS is much faster than tabu search.
Perhaps, most importantly, our procedure is a single pass procedure. The fact that
complex heuristics such as LT and LS work so well under this restriction attests to the
robustness of the approach.
In our computational experiments, we found that our procedure worked well on both
the capacity-constrained and capacity-constrained and route-length-constrained problems.
This may not always be the case with other combinatorial optimization problems or heuristics. Poor performance may indicate that the class of problems being studied is too broad
for one set of parameter values. Thus, it may be necessary to divide the class into two
subclasses. If the heuristic does not perform well using the average settings and different problems require very different parameters settings, the problem class is probably too
broadly specified. To divide the class into subclasses, we would determine how the problems in the analysis set differ and which of these differences is significant (for example,
depot location or distribution of demand). Then we would divide the problems into two or
more classes based on these characteristics. Finally, we would apply our procedure again
on each of the new subclasses.
References
Ball, M., T. Magnanti, C. Monma, and G. Nemhauser (eds.). (1995). Network Models. Amsterdam: North-Holland.

USING EXPERIMENTAL DESIGN

97

Barr, R., B. Golden, J. Kelly, M. Resende, and W. Stewart. (1995). “Designing and Reporting on Computational
Experiments with Heuristic Methods.” Journal of Heuristics 1, 9–32.
Bodin, L., B. Golden, A. Assad, and M. Ball. (1983). “Routing and Scheduling of Vehicles and Crews.” Computers
& Operations Research 10(2), 63–211.
Christofides, N., A. Mingozzi, and P. Toth. (1979). “The Vehicle Routing Problem.” In N. Christofides, A. Mignozzi,
P. Toth, and C. Sandi (eds.), Combinatorial Optimization. Chichester, UK: John Wiley & Sons, pp. 315–338.
Coy, S. (1998). “Fine-Tuned Learning: A New Approach to Improving the Performance of Local Search Heuristics.” Ph.D. Dissertation, University of Maryland, College Park, Maryland.
Coy, S., B. Golden, G. Runger, and E. Wasil. (1997). “Solving the TSP with Sequential Smoothing.” In Proceedings
of the 2nd International Conference on Computational Intelligence and Neuroscience. Research Triangle Park,
North Carolina, pp. 280–283.
Gendreau, M., A. Hertz, and G. Laporte. (1991). “A Tabu Search Heuristic for the Vehicle Routing Problem.”
CRT-777, Centre de Recherche sur les Transports, Université de Montréal, Montréal, Canada.
Gendreau, M., A. Hertz, and G. Laporte. (1994). “A Tabu Search Heuristic for the VRP.” Management Science
40, 1276–1290.
Gendreau, M., G. Laporte, and J.-Y. Potvin. (1997). “Vehicle Routing: Modern Heuristics.” In E. Aarts and J.K.
Lenstra (eds.), Local Search in Combinatorial Optimization. London, England: John Wiley & Sons, Ltd., pp.
311–336.
Golden, B. and A. Assad (ed.). (1988). Vehicle Routing: Methods and Studies, Studies in Management Science
and Systems, Vol. 16. Amsterdam, The Netherlands: North Holland.
Golden, B., E. Wasil, J. Kelly, and I-M. Chao. (1998). “The Impact of Metaheuristics on Solving the Vehicle
Routing Problem: Algorithms, Problems Sets, and Computational Results.” In T. Crainic and G. Laporte (eds.),
Fleet Management and Logistics. Boston, MA: Kluwer Academic Publishers, pp. 33–56.
Johnson, D. and L. McGeoch. (1997). “The Traveling Salesman Problem: A Case Study in Optimization.” In
E. Aarts and J.K. Lenstra (eds.), Local Search in Combinatorial Optimization. London, England: John Wiley
& Sons, Ltd., pp. 215–310.
Laporte, G. (1992). “The Vehicle Routing Problem: An Overview of Exact and Approximate Algorithms.”
European Journal of Operational Research 59, 345–358.
Montgomery, D. (1991). Design and Analysis of Experiments, John Wiley & Sons, New York.
Morgan, J. and J. Sonquist. (1963). “Problems in the Analysis of Survey Data and a Proposal.” Journal of the
American Statistical Association 58, 415–434.
Osman, I. and J. Kelly. (1996). “Metaheuristics: An Overview.” In I. Osman and J. Kelly (eds.), Metaheuristics:
Theory and Applications. Boston, MA: Kluwer Academic Publishers, pp. 1–21.
Park, M.-W. and Y.-D. Kim. (1998). “A Systematic Procedure for Setting Parameters in Simulated Annealing
Algorithms.” Computers & Operations Research 24(3), 207–217.
Parsons, R. and M. Johnson. (1997). “A Case Study in Experimental Design Applied to Genetic Algorithms with
Applications to DNA Sequence Assembly.” American Journal of Mathematical and Management Sciences
17(3/4), 369–396.
Reeves, C. (ed.). (1993). Modern Heuristic Techniques for Combinatorial Problems. New York: Halstead Press.
Robertson, S., B. Golden, and E. Wasil. (1998). “Neural Network Models for Initial Public Offerings.” Neurocomputing 18, 165–182.
Stewart, W. and B. Golden. (1984). “A Lagrangean Relaxation Heuristic for Vehicle Routing.” European Journal
of Operational Research 15, 84–88.
Taillard, E. (1993). “Parallel Iterative Search Methods for Vehicle Routing Problems.” Networks 23, 661–673.
Van Breedam, A. (1996). “An Analysis of the Effect of Local Improvement Operators in Genetic Algorithms
and Simulated Annealing for the Vehicle Routing Problem.” RUCA Working Paper 96/14, Faculty of Applied
Economics, University of Antwerp, Antwerp, Belgium.
Van Breedam, A. (1995). “Improvement Heuristics for the Vehicle Routing Problem Based on Simulated Annealing.” European Journal of Operational Research 86, 480–490.
Xu, J., S. Chiu, and F. Glover. (1996). “Fine-tuning a Tabu Search Algorithm with Statistical Tests.” Working
Paper, Graduate School of Business, University of Colorado, Boulder, Colorado.
Xu, J. and J. Kelly. (1996). “A Network Flow-based Tabu Search Heuristic for the Vehicle Routing Problem.”
Transportation Science 30, 379–393.

Knowledge-Based Systems 72 (2014) 37–47

Contents lists available at ScienceDirect

Knowledge-Based Systems
journal homepage: www.elsevier.com/locate/knosys

Feature selection for noisy variation patterns using kernel principal
component analysis
Anshuman Sahu a,⇑, Daniel W. Apley b, George C. Runger a
a
b

School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287, USA
Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL 60208, USA

a r t i c l e

i n f o

Article history:
Received 12 February 2014
Received in revised form 5 August 2014
Accepted 29 August 2014
Available online 16 September 2014
Keywords:
Nonlinear PCA
Kernel feature space
Preimages
Variation patterns
Feature ensembles

a b s t r a c t
Kernel Principal Component Analysis (KPCA) is a technique widely used to understand and visualize nonlinear variation patterns by inverse mapping the projected data from a high-dimensional feature space
back to the original input space. Variation patterns often occur in a small number of relevant features
out of the overall set of features that are recorded in the data. It is, therefore, crucial to discern this set
of relevant features that deﬁne the pattern. Here we propose a feature selection procedure that augments
KPCA to obtain importance estimates of the features given the noisy training data. Our feature selection
strategy involves projecting the data points onto sparse random vectors for calculating the kernel matrix.
We then match pairs of such projections, and determine the preimages of the data with and without a
feature, thereby trying to identify the importance of that feature. Thus, preimages’ differences within
pairs are used to identify the relevant features. An advantage of our method is it can be used with any
suitable KPCA algorithm. Moreover, the computations can be parallelized easily leading to signiﬁcant
speedup. We demonstrate our method on several simulated and real data sets, and compare the results
to alternative approaches in the literature.
Ó 2014 Elsevier B.V. All rights reserved.

1. Introduction
Advances in signal acquisition and computational processing
coupled with cheap storage have resulted in massive multivariate
data being collected in today’s processes like semiconductor manufacturing, automobile-body assemblies, inspection systems, etc.
The data can be in form of spatial proﬁles, time series or images
where the measurements are recorded over several features. These
features are affected by different sources of variation which result
in variation patterns in the data. The goal, therefore, is to identify
these sources of variation based on the process data collected.
Moreover, the variation pattern may be present in only a small
subset of the process variables that are collected. Finding this relevant subset of features is, therefore, critical to understand the
process, and is the focus of our work presented in this paper.
Principal Component Analysis (PCA) is a common technique to
identify variation pattern in data by projecting along the directions
of maximum variability in the data. However, PCA can only identify
linear relationships among features in the data. Kernel Principal
⇑ Corresponding author.
E-mail addresses: anshuman.sahu@asu.edu (A. Sahu), apley@northwestern.edu
(D.W. Apley), george.runger@asu.edu (G.C. Runger).
http://dx.doi.org/10.1016/j.knosys.2014.08.027
0950-7051/Ó 2014 Elsevier B.V. All rights reserved.

Component Analysis (KPCA) extends PCA to the case where data
contain non-linear patterns as shown by Schölkopf et al. [1]. KPCA
identiﬁes non-linear patterns in data by mapping the data from
input space to a high-dimensional (possibly inﬁnite) feature space,
and performing PCA in the feature space. This is achieved by
employing the kernel trick [2]. Thus, only calculations in terms of
dot products in the input space are required, without an explicit
mapping to the feature space. KPCA is widely used for nonlinear
process monitoring [3–5], fault detection and diagnosis [6–9],
and anomaly detection [10,11].
To visualize the variation pattern in input space, an inverse
transform is used to map the denoised data from feature space
back to the input space. The exact preimage of a denoised point
in feature space might not exist, so that a number of algorithms
for estimating approximate preimages have been proposed [12–
15]. Also, [16,17] considered meta-methods to improve the preimage results by averaging from ensembles.
Our task now is to identify the relevant subset of the original set
of features over which the pattern exists (a feature selection task).
The difﬁculty is to handle the non-linear relationships between
features in input space. Because the feature space in KPCA already
provides an avenue to consider higher-order interactions between
features, it is more appealing to apply a feature selection procedure

38

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47

in feature space itself. However, it is not always possible to obtain
the feature representation in feature space (for example, in the
case of a Gaussian kernel) because the data are not explicitly
mapped. Therefore, the challenge here is to perform feature selection in the feature space.
Some work has considered feature selection in feature space for
supervised learning. A weighted feature approach was provided by
Allen [18] where weights are assigned to features while computing
the kernel. This feature weighting is incorporated into the loss
function corresponding to classiﬁcation or regression problem
and a lasso penalty is put on the weights. The features corresponding to non-zero weights obtained after minimizing the objective
(loss function with penalty) are considered the important ones.
Similarly, recent work [19,20] also employed feature weighting
for the cases of Support Vector Machine (SVM) classiﬁcation and
regression, respectively. For both the cases, an anisotropic Gaussian kernel was used to supply weights to features. Speciﬁcally,
Maldonado et al. [19] provided an iterative algorithm for solving
the feature selection problem by embedding the feature weighting
in the dual formulation of SVM problem. The algorithm begins with
an initial set of weights. At each iteration, it solves the SVM problem for the given set of feature weights, updates the weights using
the gradient of the objective function, and removes the features
that are below a certain given threshold. This procedure is repeated
till convergence. Finally, the features obtained with non-zero
weights are considered important.
Since KPCA is unsupervised, we next consider feature selection
in feature space for unsupervised learning. One common aspect of
all these algorithms, similar to their counterparts in supervised
setting, is they involve some kind of feature weighting mechanism,
and the relevant features are obtained by regularizing (shrinking)
the weights of irrelevant features using some criteria. A method
for feature selection in Local Learning-Based Clustering [21] was
proposed by Zeng and ming Cheung [22]. The feature selection is
achieved by regularizing the weights assigned to features. A
method to measure variable importance in KPCA was suggested
by Muniz et al. [23]. They computed the kernel between two data
points as weighted sum of individual kernels where each individual kernel is computed on a single feature of each of the two data
points, and the weights assigned to each kernel serve as a measure
of importance of the feature involved in computing the kernel.
They formulated a loss function where a lasso penalty was
imposed on the weights to determine the non-zero weights (and
the corresponding relevant features). In addition to feature selection in feature space for unsupervised learning, there exist several
other feature selection procedures for unsupervised learning that
operate in the input space. Laplacian Score (LS) was proposed by
He et al. [24] for each feature to estimate its ability to preserve
local structure. The authors construct a nearest neighbor graph,
and identify the important features as those which maintain this
graph structure. Multi-Cluster Feature Selection (MCFS) proposed
by Cai et al. [25] used spectral analysis to select the features that
preserve the multi-cluster structure of the data. The authors compute the nearest neighbors graph, deﬁne weights on edges in the
graph, construct the graph Laplacian, and solve the generalized
eigen-problem [26] to obtain the top K eigenvectors. For each
eigenvector, the contribution of each feature is found by solving
a L1-regularized regression. Each feature now has K contribution
values, and the maximum of it is assigned as the MCFS score of
the feature. The features with higher MCFS scores are important.
Unsupervised Discriminative Feature Selection (UDFS) proposed
by Yang et al. [27] aims to select the most discriminative features
which preserve the local structure of the data (via manifold) while
simultaneously accounting for feature correlation. The authors
assume the existence of a linear classiﬁer that classiﬁes each
data point to a class. They propose learning the classiﬁer that

maximizes their local discriminative score. To this end, they propose a regularized optimization problem by inducing ‘2;1 norm
on the coefﬁcients of the classiﬁer. Note that each coefﬁcient of
the linear classiﬁer corresponds to a feature in the dataset. They
also propose an iterative algorithm to solve this optimization problem. The top features are determined based on sorting the ‘2 norm
of the coefﬁcient vectors over all iterations in descending order.
The approaches provided in the literature focus on the case
when noise-free training data are available. However, this is not
the case in areas like manufacturing variation analysis. In practice,
the data are corrupted with noise and has a lot of irrelevant features. Thus, we work with a noisy data set from which we need
to ﬁnd the relevant subset of the features over which the patterns
in the data exist. To this end, we propose our novel approach.
As pointed out previously, an innovative way to do feature
selection in high-dimensional feature space is to assign weights
to features in input space. By using such an approach, we can compute the kernel using all the features instead of iteratively computing it using a subset of features at a time. The goal next is to
identify the weights (by some regularization criterion) so that
the non-zero weights correspond to the relevant features. We propose an alternative approach for this feature weighting mechanism. Instead of trying to determine the feature weights through
a regularization approach, we multiply the features by sparse random vectors whose entries are independent and identically distributed drawn from a distribution (such as Gaussian). After projecting
data points onto random subsets of features, we measure feature
importance from differences in preimages, where preimages are
computed with and without a feature. Therefore, more important
features are expected to result in greater differences. The process
is repeated iteratively with different sparse random vectors and
the differences are averaged to estimate the ﬁnal feature importance. Our approach above provides robustness to irrelevant features in the data by being able to project only on a small random
subset of features at a time, and calculating the ﬁnal mapped data
matrix in input space from an ensemble of feature subsets. Another
advantage of our approach is it works with any KPCA preimage
algorithm.
We organize the remaining part of our paper as follows. Section
2 provides a brief description of different methods used to visualize the variation patterns in KPCA. For our feature selection
method, we can consider any one of them as the base algorithm.
Section 3 presents a mathematical description of our methodology.
Section 4 shows the results of implementing our algorithm on several simulated and real datasets. Finally Section 5 provides
conclusions.
2. Background on preimages in KPCA
KPCA is equivalent to PCA in feature space [1]. Let X denote the
data set with N instances and F features where the instances are
denoted by x1 ; x2 ; . . . ; xN . Similar to PCA, we want to ﬁnd the eigenvalues and eigenvectors of the covariance matrix C in feature
space. If the corresponding set of points mapped in the feature
space uðxi Þ; i ¼ 1; 2; . . . ; N are assumed to be centered, C can be
calculated by

C¼

N
1X
uðxi Þuðxi Þ0
N i¼1

The eigenvalues k and eigenvectors

Cv ¼ kv

ð1Þ

v of matrix C are given by
ð2Þ

It can be shown that an eigenvector corresponding to non-zero
eigenvalue of C can be written as a linear combination of
uðx1 Þ; . . . ; uðxN Þ. Using this simpliﬁcation reduces the original

39

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47

problem of ﬁnding eigenvalues and eigenvectors of C to ﬁnding the
corresponding eigenvalues and eigenvectors of the kernel matrix K
with entries

Kij :¼ ðuðxi Þ  uðxj ÞÞ

ð3Þ

The product uðxi Þ  uðxj Þ is evaluated using the kernel trick Aizerman et al. [2] without explicitly computing the mapping uðÞ.
Training data are used to obtain a reliable estimate of the principal component subspace in feature space onto which the test
data can be projected. The procedure for visualizing variation pattern in test data can, thus, be summarized in four steps. The ﬁrst
step is to map the training data from input space to feature space
via the kernel trick [2]. The second step is to calculate the principal
component directions of the training data in feature space as
shown by Schölkopf et al. [1]. The third step is to map the test data
x to feature space and then project onto the space spanned by a
small subset of the principal component directions found above.
This projected test data (denoted by PuðxÞ ) is also called the denoised data in feature space. In order to observe the pattern in input
space, the denoised data are mapped back from feature space to
input space in the fourth step. This last step is also referred to as
obtaining the preimage ^
x in KPCA literature. The above steps can
be seen in Fig. 1.
The preimage can be used to visualize the variation pattern of
the data in input space. As mentioned, in general, such an inverse
mapping from feature space to input space may not exist, and the
preimage cannot always be determined exactly [12]. Hence, several algorithms have been proposed to estimate the preimage.
Mika et al. [12] proposed a gradient descent approach to numerically estimate the preimage matrix which, when mapped to the
feature space, is closest (in terms of Euclidean distance) to the
denoised matrix in feature space. Since the objective function
(Euclidean distance) to minimize is non-convex, this approach is
sensitive to initial starting solution. Kwok and Tsang [14] used
the relationship between distance in input space and the feature
space, and estimated the preimage of a test point as a linear combination of the training data points whose projections in feature
space are closest to the denoised data point in feature space. Kwok
and Tsang [14] chose only a few nearest training data points in
order to reduce the computational burden. Bakir et al. [13] applied
kernel regression to the preimage problem where the inverse mapping from feature space to input space is posed as a regression
problem. Both approaches by Kwok and Tsang [14] and Bakir
et al. [13] favor noise-free training data.

More recently, Sahu et al. [17] and Shinde et al. [16] considered
meta-methods to estimate the ﬁnal preimage. Since the training
data is noisy, we expect the principle component subspace estimation to be unreliable. Thus, instead of estimating a single preimage
from the training data, we resample the training data multiple
times and estimate the ﬁnal preimage by averaging the preimages
obtained from each sample.
3. Feature selection using sparse random vectors with matched
pairs
We now present a unique approach to learn the preimage as
well as understand the contribution of a feature towards the variation pattern in the data. We utilize the idea of random projections
where we project onto a small subset of features in each iteration.
Essentially we try to capture the effect of that subset of features in
feature space onto to which we randomly project. By repeating this
procedure over a number of iterations, we create a diversiﬁed
ensemble of feature subsets which account for the possible interactions between features that give rise to the variation pattern in the
data. We explain this concept by an example. Let the dataset have
features f 1 ; f 2 ; f 3 ; f 4 ; f 5 ; f 6 . Suppose we choose the number of
iterations to be three. In the ﬁrst iteration, we randomly project
onto {f 1 , f 2 }. In the second iteration, we randomly project onto
{f 2 , f 3 ; f 4 }. In the third iteration, we randomly project onto
{f 1 ; f 4 }. The ensemble of feature subsets is given by {{ f 1 ; f 2 },
{f 2 ; f 3 ; f 4 }, {f 1 , f 4 }}. We note that here we are creating an ensemble
of feature subsets to estimate the preimage as well as identify the
features that are relevant for the preimage, whereas previously we
created an ensemble of data points by resampling the training data
set [17,16] to estimate the primage. Matched pairs of projections
are created for each feature to estimate the effect of the feature
on the variation pattern. We then calculate the difference in the
preimage as a result of excluding the feature. Thus, important features are expected to result in high differences. We refer to our
procedure as MPFS, and describe it mathematically as follows.
Let w be a sparse random vector of dimension F where bcFc
entries are non-zero. Here c is a parameter that controls sparseness. The entries in the sparse random vector are independently
sampled from a distribution (such as Gaussian). Let B be a ﬁxed
number of iterations. Let K be the kernel matrix obtained from
instances in the input space. Let X denote the data matrix with N
rows, N being the total number of data points. Let xi and xj denote
two instances in input space. Assume that we are using a Gaussian
kernel. The ijth entry in K is calculated as


2 !
xi  xj F

kðxi ; xj Þ ¼ exp

ð4Þ

r

For the purpose of MPFS, we modify K to Kw where we obtain the
corresponding ijth entry in Kw as
2

kw ðxi ; xj Þ ¼ exp

Fig. 1. KPCA and the preimage problem. Training data are transformed to feature
space and used to learn a principal component plane. A test point x is transformed
and projected to the plane as P uðxÞ. The inverse transform of P uðxÞ may not exist,
and an approximate preimage ^
x is computed.

ðwT xi  wT xj Þ

r

!
ð5Þ

We also normalized w to unit length in Eq. (5). Preliminary experiments, however, did not show meaningful differences in results
obtained from normalized and nonnormalized w.
For each f ¼ 1; 2; . . . ; F in each iteration b (b ¼ 1; 2; . . . ; B), we
generate a sparse random vector wb . To create matched pairs, we
transform wb to wb by the following mechanism. Denote f th entry
of wb by wb ½f  and the corresponding entry in wb as wb ½f . Then, for
each b, we set

wb ½ f  ¼



0

if wb ½ f  – 0

1 otherwise

ð6Þ

40

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47

Thus, for every feature f at each iteration b, we generate matched
pairs wb and wb which differ only at the f th entry. We use wb to
obtain kernel matrix Kwb by substituting w with wb in Eq. (5) and
then use Kwb and X in a preimage estimation algorithm to obtain
b b at iteration b for each f. Similarly, we use w to obtain K and
X
wb
b
b f at iteration b for each f.
then use Kwb along with X to obtain X
b
The importance of feature f, denoted by impf , is calculated as

impf ¼

B
bb  X
b f jj
X
jj X

b F

ð7Þ

B

b¼1

where the Frobenius norm of the matrix is used. The Frobenius
norm of a matrix is deﬁned as the square root of the sum of squares
bb  X
b f ) is
of elements of the matrix. Thus, if Frobenius norm of ( X

algorithm for KPCA in the literature. Finally, we also note that in
addition to feature selection via MPFS, we also obtain the preimage
b as a result of our algorithm. This suggests that MPFS can also be
X
thought of as a ‘‘meta’’ approach for estimating preimage in KPCA.

4. Experimental results
We evaluate our method on several simulated and real data sets
to show its efﬁcacy. Since we know the relevant features in simulated data sets beforehand, we can verify if our algorithm is able to
identify them in different scenarios. We ﬁrst present the results on
simulated data sets, and then show the results on real data sets.

b

small, there is not much difference in the preimage estimated with
and without feature f. Thus, impf becomes small and feature f is not
bb  X
b f ) is high, imp
so important. Similarly, if Frobenius norm of ( X
b

f

is high and feature f becomes important.
We summarize MPFS in Algorithm 1. gðÞ denotes a preimage
estimation function. Note that the function gðÞ takes Kwb (or Kwb )
b b (or X
b f ) respectively at each iteraand X as input, and outputs X
b

tion b for each feature f.
Algorithm 1. MPFS: Feature Selection Algorithm

M¼0
Initialize b ¼ 1; f ¼ 1; c
Initialize feature importance vector imp with F zeros indexed
by impf ; f ¼ 1; 2; . . . ; F
for b ¼ 1 ! B do
for f ¼ 1 ! F do
Generate sparse random vector wb
Use wb to calculate Kwb
bb
gðKwb ; XÞ
X
if w½f  ¼¼ 0 then
Set w½f  ¼ 1 to generate wb
else
Set w½f  ¼ 0 to generate wb
end if
Use wb to obtain Kwb
bf
gðK ; XÞ
X
wb

b

bb  X
b f jj
impf þ jj X
impf
b F
c
c
bb
M
MþX
f
f þ1
end for
b
bþ1
end for
b
b = M
X
BF
for f ¼ 1 ! F do
impf
end for

impf
B

{importance of f th feature is given by impf }

For each iteration b (b ¼ 1; 2; . . . ; B), when we use wb , we are
essentially capturing the interaction between that subset of features via kernel matrix Kwb . Because the entries in wb in each iteration are sparse and independently sampled from a distribution,
we work with a diversiﬁed ensemble of feature subsets. An advantage is this tends to be more robust towards noisy and irrelevant
features in the data. This is important in our case because we do
not have noise-free training data for our algorithm. Moreover, this
also enables us to work with any existing preimage estimation

4.1. Results on simulated data
We generate several simulated data sets where each data set
has a pattern (linear or non-linear) embedded into it. The pattern
is only over a subset of relevant features out of the total set of features, and we want to ﬁnd those relevant features. Our feature
selection methodology can work with any KPCA algorithm. For
the purpose of this paper, we use the algorithm proposed in Sahu
et al. [17] as the base algorithm. Preliminary experiments did not
show sensitivity to B. We set B ¼ 50 for all the experiments. For
the experiments we set the Gaussian
pﬃﬃﬃ kernel parameter r ¼ 1,
and the sparseness parameter c ¼ 1= F , where F is the total number of features in the data. However, we conduct some sensitivity
experiments to evaluate the role of these parameters. We also vary
the noise level in the data through the standard deviation of added
Gaussian noise rG . We experimented with polynomial kernels, and
found the conclusions of the experiments to be the same as those
obtained using Gaussian kernel.
The ﬁrst data set is the Line2 data set which refers to the fact
that the pattern is linear only over two features out of the total
set of features. More speciﬁcally, the data set consisting of 50
instances and 70 features is generated as follows: x1 ¼ 0:1t for
t ¼ 1; 2; . . . ; 50; x2 ¼ 0:5ð1  x1 Þ, and x3 ; x4 ; . . . ; x70 are independent
Gaussian noise with mean 0 and variance r2G , Independent Gaussian noise with mean 0 and variance r2G are also added to x1 and x2 .
Fig. 2 shows the feature importance as a function of the feature
index, along with standard errors for feature importance values
obtained by applying MPFS on 10 independent replicates of the
data set.
The second data set Plane5 refers to the fact that the pattern is a
plane over ﬁve features. The data set consists of 50 instances and
70 features generated as follows: x1 ¼ 0:1t; t ¼ 1; 2; . . . ; 50,
x2 ; x3 ; x4 are independently, Gaussian distributed with mean 0
and
variance
1,
x5 ¼ 1  0:2x1 þ 3x2 þ 2x3 þ 0:5x4 ,
and
x6 ; x7 ; . . . ; x70 are independent, Gaussian noise with mean 0 and
variance r2G . Independent Gaussian noise with mean 0 and variance
r2G are added to x1 ; x2 ; x3 , x4 and x5 . The results are shown in Fig. 3
(standard errors for feature importance values are obtained from
generating different x2 ; x3 ; x4 10 times).
The third data set Curve3 refers to the fact that the pattern is a
curve over three features. The data set consists of 50 data points
and 70 features generated as follows: x1 ¼ 0:1t; t ¼ 1; 2; . . . ; 50,
x2 is Gaussian distributed with mean 0 and variance 1,
x3 ¼ x22 =x1 , and x4 ; x5 ; . . . ; x70 are independent, Gaussian noise with
mean 0 and variance r2G . Independent Gaussian noise with l ¼ 0
and variance r2G are added to x1 ; x2 ; x3 . Fig. 4 shows the results
(standard errors for feature importance values are obtained from
generating different x2 10 times).
The fourth data set Sphere3 refers to the fact that the pattern is
spherical over three features. The data set consists of 50 data
points and 70 features generated as follows. The pattern is of
the form x21 þ x22 þ x23 ¼ 25 where x1 ¼ 5 sinðtÞ cosðtÞ, x2 ¼ 5sinðtÞ

41

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47
16

23
22

14

Feature Importance

Feature Importance

21
12

10

8

20
19
18
17

6

4

16

0

10

20

30

40
Features

50

60

70

15

80

0

10

20

30

40
Features

50

60

70

80

Fig. 2. Feature importance plots for MPFS applied to the Line2 data set with for selected values of noise rG . The procedure is replicated 10 times to obtain standard errors for
feature importance. Under moderate noise (rG ¼ 0:9), x1 and x2 can be identiﬁed as relevant features visually.
1.8

0.7

1.6

0.6
0.5

1.2

Feature Importance

Feature Importance

1.4

1
0.8
0.6

0.4
0.3
0.2

0.4
0.1

0.2
0

0

5

0

10 15 20 25 30 35 40 45 50 55 60 65 70
Features

0

5

10 15 20 25 30 35 40 45 50 55 60 65 70
Features

Fig. 3. Feature importance plots for MPFS applied to the Plane5 data set for selected values of noise rG . The procedure is replicated 10 times to obtain standard errors for
feature importance. Under moderate noise (rG ¼ 0:9), x1 ; x2 ; x3 ; x4 ; x5 can be identiﬁed as relevant features visually.

sinðtÞ; x3 ¼ 5 cosðtÞ, for t ¼ 1; 2;.. .; 50, and x4 ;x5 ;. .. ;x70 are independent, Gaussian noise with mean 0 and variance r2G . Independent,
Gaussian noise noise with mean 0 and variance r2G are added to
x1 ;x2 ; x3 . Fig. 5 shows the results (standard errors for feature importance values are obtained from 10 replicates).
To evaluate the sensitivity of our results to the parameters
involved (r and c), we conducted the above experiments on Line2
and Sphere3 datasets setting (rG 2 f0:9; 3g). Speciﬁcally, we took
r 2 0:1; 10 and c 2 fp2ﬃﬃﬃﬃ
; p3ﬃﬃﬃﬃ
g. Figs. 6–9 show the results. To see
70
70
the effect of intermediate noise levels on Line2 and Sphere3 data. Fig. 10
sets, we chose (rG 2 f1:5; 2g) keeping r ¼ 1 and c ¼ p1ﬃﬃﬃﬃ
70
shows the results (standard errors are obtained from 10 replicates).
We see that for all datasets corrupted with a medium level of noise,
our algorithm is able to detect the important features. However,
when we increase the noise level to high (rG ¼ 3), the algorithm

cannot detect all the relevant features. Thus, our algorithm works
well for cases with moderate noise levels.
To compare our approach, we tested the algorithm in Muniz
et al. [23] on the Line2 and Sphere3 data sets with rG ¼ 0:9.
Fig. 11 shows the results. In both cases, it is not able to identify
all the relevant features. We also found that the algorithm in
Muniz et al. [23] is able to identify the relevant features in the data
sets when noise is not added. However, even in the presence of
moderate amount of noise, its performance deteriorates as shown
in Fig. 11.
4.2. Results on real data sets
We also evaluated MPFS on several real data sets available in
UCI Machine Learning Repository Bache and Lichman [28] (the

1.6

0.9

1.4

0.8
0.7
Feature Importance

Feature Importance

1.2
1
0.8
0.6
0.4

0.5
0.4
0.3
0.2

0.2
0

0.6

0.1
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70
Features

0

0

5

10 15 20 25 30 35 40 45 50 55 60 65 70
Features

Fig. 4. Feature importance plots for our algorithm applied to the Curve3 data set for selected values of noise rG . The procedure is replicated 10 times to obtain standard errors
for feature importance. Under moderate noise (rG ¼ 0:9), x1 ; x2 ; x3 can be identiﬁed as relevant features visually.

42

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47
18

22

16

21
20
Feature Importance

Feature Importance

14
12
10
8

18
17
16

6
4

19

15

0

10

20

30

40
Features

50

60

70

14

80

0

10

20

30

40
Features

50

60

70

80

16

16

14

14

12

12

Feature Importance

Feature Importance

Fig. 5. Feature importance plots for our algorithm applied to the Sphere3 dataset for selected values of noise rG . The procedure is replicated 10 times to obtain standard errors
for feature importance. Under moderate noise (rG ¼ 0:9), x1 ; x2 ; x3 can be identiﬁed as relevant features visually.

10

8

6

8

6

0

10

20

30

40
Features

50

60

70

4

80

19

23

18

22

17

21
Feature Importance

Feature Importance

4

10

16
15
14

16

20

30

40
Features

50

60

70

80

30

40
Features

50

60

70

80

0

10

20

30

40
Features

50

60

70

80

18

12

10

20

19

17

0

10

20

13

11

0

15

Fig. 6. Feature importance plots to illustrate the sensitivity of our algorithm to the kernel parameter r applied to the Line2 data set with selected values of noise
moderate noise (rG ¼ 0:9), x1 ; x2 can be identiﬁed as relevant features visually for different values of r.

address of the website is http://archive.ics.uci.edu/ml/). For our
experiments, we chose Wine, Ionosphere, Sonar, and Hill Valley data
sets. The details of these datasets as well as results obtained from
our experiments are described subsequently. We compare MPFS to
baseline ﬁlter based feature selection approaches (T-test, Information Gain Ratio (IGR)) as well as sophisticated unsupervised feature
selection procedures described before like LS, MCFS, and UDFS. For
each data set in our experiment, we ﬁrst standardize all the
features so that each feature has zero mean and unit variance.
Gaussian noise with mean 0 and variance r2G ¼ 0:81 is then added
to the data set. Different feature selection procedures are applied
with the appropriate parameter settings described by the authors
of the corresponding papers, and the selected features are fed to
a classiﬁer (logistic regression) where the results are computed
with 10-fold cross-validation. The entire procedure is repeated
10 times, and average values of salient metrics for classiﬁcation
like Accuracy, True Positive Rate(TPR), False Positive Rate (FPR),
Precision, F-Measure and Area under Curve(AUC) are reported (Recall

rG . Under

is equivalent to TPR, and hence not reported). We also show the
average values of these metrics when all the features are used
for classiﬁcation.
4.2.1. Results for wine data set
Wine data set has 178 instances, 13 features, and 3 classes. For
each instance belonging to a type of wine, each of the features corresponds to the quantities of a particular constituent for the class
of wine based on chemical analysis. The data is obtained from
three different cultivars in Italy. Since there are 3 classes, we used
a multinomial logistic regression. Here we used Chi-squared based
feature selection instead of T-test as a baseline ﬁlter because of 3
classes.
The results of our experiments are shown in Table 1. We see
that MPFS obtained the same results as UDFS which is the best
result among different feature selection methods in terms of all
the classiﬁcation metrics. It is noteworthy that even if both these
algorithms are unsupervised in nature, the classiﬁcation metrics

43

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47
16

20

12

Feature Importance

Feature Importance

14

10

8

15

10

6

4

0

10

20

30

40
Features

50

60

70

5

80

20

25

19

24

Feature Importance

Feature Importance

10

20

30

40
Features

50

60

70

80

0

10

20

30

40
Features

50

60

70

80

23

18
17
16
15
14

22
21
20
19
18

13
12

0

17
0

10

20

30

40
Features

50

60

70

16

80

Fig. 7. Feature importance plots to illustrate the sensitivity of our algorithm to the kernel parameter r applied to the Sphere3 data set with selected noise rG . Under moderate
noise (rG ¼ 0:9), x1 ; x2 ; x3 can be identiﬁed as relevant features visually for different values of r.

14

14

13
12

12

Feature Importance

Feature Importance

11
10
9
8
7
6

10

8

6

4

5
4

0

10

20

30

40
Features

50

60

70

2

80

22

0

10

20

30

40
Features

50

60

70

80

0

10

20

30

40
Features

50

60

70

80

22

21
20

20

Feature Importance

Feature Importance

19
18
17
16
15
14

18

16

14

12

13
12

0

10

20

30

40
Features

50

60

70

80

10

Fig. 8. Feature importance plots to illustrate the sensitivity of our algorithm to the sparseness parameter c applied to the Line2 data set with selected noise
moderate noise (rG ¼ 0:9), x1 ; x2 can be identiﬁed as relevant features visually for different values of c.

rG . Under

44

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47
18

16

16

14

Feature Importance

Feature Importance

14
12
10
8

10

8

6

6
4

12

0

10

20

30

40
Features

50

60

70

4

80

24

0

10

20

30

40
Features

50

60

70

80

0

10

20

30

40
Features

50

60

70

80

22

23
20

22

Feature Importance

Feature Importance

21
20
19
18
17
16

18

16

14

12

15
14

0

10

20

30

40
Features

50

60

70

10

80

18

20

17

19

16

18

15

17
Feature Importance

Feature Importance

Fig. 9. Feature importance plots to illustrate the sensitivity of our algorithm to the sparseness parameter c applied to the Sphere3 data set with selected values of noise
Under moderate noise (rG ¼ 0:9), x1 ; x2 ; x3 can be identiﬁed as relevant features visually for different values of c.

14
13
12

15
14
13

10

12

8

11
0

10

20

30

40
Features

50

60

70

10

80

18

19

17

18

16

Feature Importance

14
13
12
11

10

20

30

40
Features

50

60

70

80

0

10

20

30

40
Features

50

60

70

80

16
15
14
13
12

10

11

9
8

0

17

15
Feature Importance

16

11

9

rG .

0

10

20

30

40
Features

50

60

70

80

10

Fig. 10. Feature importance plots for our algorithm for selected values of rG added to the data sets. As the level of added noise (rG ) increases, it becomes difﬁcult to identify
all the relevant features.

45

0.55

0.6

0.5

0.55

0.45

0.5

0.4

0.45
Feature Importance

Feature Importance

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47

0.35
0.3
0.25
0.2

0.4
0.35
0.3
0.25

0.15

0.2

0.1

0.15

0.05

0

10

20

30
40
Features

50

60

70

0.1

0

10

20

30
40
Features

50

60

70

Fig. 11. Feature importance plots for Line2 and Sphere3 data sets based on the algorithm by Muniz et al. [23]. Based on the feature importance values, it is not able to identify
all the relevant features when the data sets are corrupted with moderate noise (rG ¼ 0:9).

Table 1
Classiﬁcation results for different feature selection methods for Wine dataset. ‘‘All’’
refers to the fact that all features were used by the classiﬁer (no feature selection),
and entries for this column are italicized. ‘‘CHI’’ refers to Chi-squared based feature
selection method. The best entries for each metric among the feature selection
methods only are marked in bold.
Metric

All

CHI

IGR

LS

MCFS

UDFS

MPFS

Accuracy
TPR
FPR
Precision
F  Measure
AUC

0.971
0.972
0.014
0.972
0.972
0.999

0.949
0.949
0.027
0.95
0.949
0.986

0.949
0.949
0.027
0.95
0.949
0.986

0.955
0.955
0.023
0.956
0.955
0.974

0.966
0.966
0.017
0.966
0.966
0.987

0.971
0.972
0.013
0.973
0.972
0.996

0.971
0.972
0.013
0.973
0.972
0.996

are very close to those obtained using all the features. We also see
that CHI and IGR methods gave similar results, and MPFS performed better than these baseline ﬁlter methods which utilize
the information about the class label in the dataset.
4.2.2. Results for ionosphere data set
Ionosphere data set has 351 instances, 34 features, and 2 classes.
The instances belonging to ‘‘Good’’ class convey the fact there is
evidence of some type of structure in the ionosphere whereas
the instances belonging to ‘‘Bad’’ class do not show the evidence
of any structure. The data is obtained from Goose Bay, Labrador.
Since there are 2 classes, we used two-class logistic regression
for classiﬁcation.
The results of our experiments are shown in Table 2. We can see
that overall MPFS performed better in all metrics (except
F  measure and AUC where it is slightly worse) among different
feature selection methods. We also note that using the total set
of features provided the best results.

Table 3
Classiﬁcation results for different feature selection methods for Sonar dataset. ‘‘All’’
refers to the fact that all features were used by the classiﬁer (no feature selection),
and entries for this column are italicized. The best entries for each metric among the
feature selection methods only are marked in bold.
Metric

All

T-test

IGR

LS

MCFS

UDFS

MPFS

Accuracy
TPR
FPR
Precision
F  Measure
AUC

0.730
0.731
0.273
0.731
0.731
0.763

0.721
0.721
0.284
0.721
0.721
0.779

0.735
0.736
0. 269
0.735
0.735
0.8

0.735
0.736
0.266
0.736
0.736
0.811

0.754
0.755
0.249
0.755
0.755
0.818

0.711
0.712
0.295
0.711
0.711
0.749

0.759
0.76
0.249
0.76
0.758
0.822

instances belong to ‘‘Rock’’ class. The features for ‘‘Mine’’ class
are obtained by recording sonar signals bouncing off a metal cylinder at different angles whereas the features for ‘‘Rock’’ class are
obtained under similar conditions from rocks. We used two-class
logistic regression for classiﬁcation.
The results of our experiments are shown in Table 3. We can see
that MPFS performed the best in all metrics among different feature selection methods. Moreover, the performance of MPFS is
slightly better than MCFS. Thus, both MPFS and MCFS deliver the
best results. It is noteworthy that although MPFS and MCFS are
unsupervised, the features provided by them perform better classiﬁcation than the case when all features are selected.

4.2.3. Results for sonar data set
Sonar data set has 208 instances, 60 features, and 2 classes out
of which 111 instances belong to the ‘‘Mine’’ class and rest 97

4.2.4. Results for hill valley data set
Hill Valley data set has 606 instances, 100 features, and 2 classes. Essentially, each instance is a sequence of 100 points classiﬁed
as ‘‘Hill’’ if a ‘‘bump’’ is observed when plotting the points on a
two-dimensional plot or classiﬁed as ‘‘Valley’’ if a dip is observed.
For our purpose, we took the dataset with noise where the bump/
dip is not trivial to observe. Again we used two-class logistic
regression for classiﬁcation.

Table 2
Classiﬁcation results for different feature selection methods for Ionosphere dataset.
‘‘All’’ refers to the fact that all features were used by the classiﬁer (no feature
selection), and entries for this column are italicized. The best entries for each metric
among the feature selection methods only are marked in bold.

Table 4
Classiﬁcation results for different feature selection methods for Hill Valley dataset.
‘‘All’’ refers to the fact that all features were used by the classiﬁer (no feature
selection), and entries for this column are italicized. The best entries for each metric
among the feature selection methods only are marked in bold.

Metric

All

T-test

IGR

LS

MCFS

UDFS

MPFS

Metric

All

T-test

IGR

LS

MCFS

UDFS

MPFS

Accuracy
TPR
FPR
Precision
F  Measure
AUC

0.888
0.889
0.153
0.889
0.887
0.87

0.712
0.712
0.5
0.761
0.653
0.535

0.811
0.812
0.259
0.81
0.807
0.843

0.806
0.806
0.301
0.812
0.795
0.74

0.814
0.815
0.264
0.814
0.809
0.799

0.803
0.803
0.302
0.808
0.793
0.719

0.817
0.818
0.284
0.824
0.808
0.818

Accuracy
TPR
FPR
Precision
F  Measure
AUC

0.526
0.526
0.474
0.526
0.526
0.524

0.486
0.487
0.522
0.468
0.422
0.447

0.511
0.512
0.489
0.511
0.511
0.494

0.615
0.616
0.392
0.657
0.585
0.645

0.625
0.625
0.382
0.687
0.589
0.665

0.608
0.609
0.399
0.664
0.57
0.652

0.622
0.622
0.385
0.666
0.593
0.67

46

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47

Feature selection (top 20%)

Feature selection (top 20%)

Feature selection (top 10%)

Feature selection (top 10%)

Feature selection (top 5%)

Feature selection (top 5%)

No feature selection

No feature selection

15

20
RSS

25

10

20
RSS

30

40

Fig. 12. RSS obtained with and without feature selection for two popular KPCA preimage estimation algorithms. Feature selection shows statistically signiﬁcant improvement
in RSS.

The results of our experiments are shown in Table 4. We can see
that the performance of MPFS was very competitive with MCFS. It
was slightly worse than MCFS in terms of Accuracy; TPR; FPR;
Precision but was better in F  measure; AUC. It is noteworthy that
MCFS and MPFS select features that result in much better classiﬁcation metrics than those obtained using all the features.
Overall we can see that MPFS resulted in the best (very close to
best in case of Hill Valley dataset) classiﬁcation performance.
Although MPFS is unsupervised, it outperformed the baseline ﬁlter
based feature selection procedures which make use of the information about class label.
4.3. Results on face image data
A real-world application of KPCA is denoising images. For our
purpose, we chose a real face data set available at http://isomap.stanford.edu/datasets.html. There are 698 data points and
the dimensionality of each data point is 4096. We took all 698
images, added independent Gaussian noise (l ¼ 0 and rG ¼ 0:5)
to the images to create the noisy data set, and subsequently denoised the test set by obtaining preimages. We chose the algorithms
proposed by Kwok and Tsang [14] and Zheng et al. [15] to obtain
preimages. To evaluate the efﬁcacy of our feature selection procedure, we computed residual error (RSS) for each image with and
without feature selection. RSS is computed as follows: Let
xi ; i ¼ 1; . . . ; N; xk 2 Rp represent a set of N observations in the
input space. We estimate the preimage residual root sum of
squared error (RSS) for a method by calculating the Euclidean dis^ and its true image t as
tance between the obtained preimage x
shown in Eq. (8).

rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
XN
^i  ti Þ2
RSS ¼
ðx
i¼1

ð8Þ

A smaller value of RSS shows better performance. We note that Eq.
(8) can be used to calculate the RSS for each image by setting N ¼ 1
for that particular image. Also we decided to choose three levels –
top 5%, top 10% and top 20% – of the features when performing feature selection. We then computed the RSS ﬁrst without performing
feature selection and then selecting features at the three levels
described previously. Fig. 12 shows the boxplot results of RSS
obtained for both algorithms under the scenario described above.
We computed the difference between the RSS value obtained
without feature selection, and then selected features at each of
the three levels for each of the 698 instances for both the algorithms. We then performed a one-sided Wilcoxon signed-rank test
(the alternate hypothesis is that the median difference is greater
than zero). The p-values obtained for all of the tests were smaller

than 0.001. Thus, MPFS on the preimage estimation algorithms
provides statistically signiﬁcant improvement over the results
obtained from without using MPFS.

5. Conclusion and further study
A new feature selection algorithm (MPFS) for KPCA for the case
of noisy training data is presented. The data points are projected
onto multiple sparse random subsets of features, and then a feature importance measure is calculated by denoising the data
matrix using matched pairs of projections (with and without a feature). An advantage of working with an ensemble of feature subsets is they tend to be more robust towards noisy and irrelevant
features in the data. Also, our feature selection methodology can
used with any suitable KPCA algorithm available in the literature.
Finally, we also note that in addition to feature selection via MPFS,
we can also obtain the preimage. This suggests that MPFS can also
be thought of as a ‘‘meta’’ approach for estimating preimage in
KPCA. We demonstrate the effectiveness of our algorithm on several simulated and real data sets.
Since the focus of this paper is on feature selection, we did not
investigate fully the ‘‘meta’’ approach ability of MPFS to estimate
the preimage for KPCA. Given the fact that preimage estimation
is important for many real applications like image denoising, manufacturing variation analysis, etc., we plan to work on it in future.
Acknowledgements
This material is based upon work supported by the National Science Foundation under Grant No. 0825331. We also thank the
reviewers for their constructive feedback.
References
[1] B. Schölkopf, A.J. Smola, K.R. Müller, Nonlinear component analysis as a kernel
eigenvalue problem, Neural Comput. 10 (5) (1998) 1299–1319.
[2] M. Aizerman, E. Braverman, L. Rozonoer, Theoretical foundations of the
potential function method in pattern recognition learning, Automat. Rem.
Contr. 25 (1964) 821–837.
[3] S.W. Choi, I.-B. Lee, Nonlinear dynamic process monitoring based on dynamic
kernel PCA, Chem. Eng. Sci. 59 (24) (2004) 5897–5908.
[4] X. Liu, U. Kruger, T. Littler, L. Xie, S. Wang, Moving window kernel PCA for
adaptive monitoring of nonlinear processes, Chemometr. Intell. Lab. Syst. 96
(2) (2009) 132–143.
[5] Z. Ge, C. Yang, Z. Song, Improved kernel PCA-based monitoring approach for
nonlinear processes, Chem. Eng. Sci. 64 (9) (2009) 2245–2255.
[6] J.-M. Lee, C. Yoo, I.-B. Lee, Fault detection of batch processes using multiway
kernel principal component analysis, Comput. Chem. Eng. 28 (9) (2004) 1837–
1847.

A. Sahu et al. / Knowledge-Based Systems 72 (2014) 37–47
[7] S.W. Choi, C. Lee, J.-M. Lee, J.H. Park, I.-B. Lee, Fault detection and identiﬁcation
of nonlinear processes based on kernel PCA, Chemometr. Intell. Lab. Syst. 75
(1) (2005) 55–67.
[8] P. Cui, J. Li, G. Wang, Improved kernel principal component analysis for fault
detection, Expert Syst. Appl. 34 (2) (2008) 1210–1219.
[9] R. Sun, F. Tsung, L. Qu, Evolving kernel principal component analysis for fault
diagnosis, Comput. Ind. Eng. 53 (2) (2007) 361–371.
[10] H. Hoffmann, Kernel PCA for novelty detection, Pattern Recogn. 40 (3) (2007)
863–874.
[11] Y. Gu, Y. Liu, Y. Zhang, A selective kernel PCA algorithm for anomaly detection
in hyperspectral imagery, in: IEEE International Conference on Acoustics,
Speech and Signal Processing, vol. 2, II–II, 2006.
[12] S. Mika, B. Schölkopf, A.J. Smola, K.R. Müller, M. Scholz, G. Rätsch, Kernel PCA
and de-noising in feature spaces, in: NIPS, 1998, pp. 536–542.
[13] G.H. Bakir, J. Weston, B. Schölkopf, Learning to ﬁnd pre-images, in: NIPS, 2003.
[14] J. Kwok, I. Tsang, The pre-image problem in kernel methods, IEEE Trans. Neural
Netw. 15 (2004) 1517–1525.
[15] W.S. Zheng, J. Lai, P.C. Yuen, Penalized preimage learning in kernel principal
component analysis, IEEE Trans. Neural Netw. 21 (4) (2010) 551–570.
[16] A. Shinde, A. Sahu, D. Apley, G. Runger, Preimages for variation patterns from
kernel PCA and bagging, IIE Trans. 46 (5) (2014) 429–456.
[17] A. Sahu, G. Runger, D. Apley, Image denoising with a multi-phase kernel
principal component approach and an ensemble version, in: IEEE Applied
Imagery Pattern Recognition Workshop (AIPR), 2011, pp. 1–7.
[18] G.I. Allen, Automatic feature selection via weighted kernels and regularization,
J. Comput. Graph Stat. 22 (2) (2013) 284–299.

47

[19] S. Maldonado, R. Weber, J. Basak, Simultaneous feature selection and
classiﬁcation using kernel-penalized support vector machines, Inf. Sci. 181
(1) (2011) 115–128.
[20] S. Maldonado, R. Weber, Feature selection for support vector regression via
Kernel penalization, in: IJCNN, 2010, pp. 1–7.
[21] M. Wu, B. Schölkopf, A local learning approach for clustering, in: NIPS, 2006,
pp. 1529–1536.
[22] H. Zeng, Y. ming Cheung, Feature selection and kernel learning for local
learning-based clustering, IEEE Trans. Pattern Anal. Mach. Intell. 33 (8) (2011)
1532–1547.
[23] V. Muniz, J.V. Horebeek, R. Ramos, Measuring the importance of variables in
kernel PCA, in: COMPSTAT, 2008, pp. 517–524.
[24] X. He, D. Cai, P. Niyogi, Laplacian score for feature selection, in: Y. Weiss, B.
Schölkopf, J. Platt (Eds.), Advances in Neural Information Processing Systems,
vol. 18, MIT Press, 2006, pp. 507–514. <http://papers.nips.cc/paper/2909laplacian-score-for-feature-selection.pdf>.
[25] D. Cai, C. Zhang, X. He, Unsupervised feature selection for multi-cluster data,
in: Proceedings of the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM, 2010, pp. 333–342.
[26] M. Belkin, P. Niyogi, Laplacian eigenmaps and spectral techniques for
embedding and clustering, in: NIPS, vol. 14, 2001, pp. 585–591.
[27] Y. Yang, H.T. Shen, Z. Ma, Z. Huang, X. Zhou, l2;1 -Norm regularized
discriminative feature selection for unsupervised learning, in: IJCAI, 2011,
pp. 1589–1594.
[28] K. Bache, M. Lichman, UCI Machine Learning Repository, 2013. <http://
archive.ics.uci.edu/ml>.

Extracting Geographic Knowledge from Sensor
Intervention Data Using Spatial Association Rules
Anirudh Kondaveeti

George Runger

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
(814)321-4454
anirudh.kondaveeti@asu.edu

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
(480)965-3193
runger@asu.edu

Huan Liu

Jeremy Rowe

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
(480)727-7349
huan.liu@asu.edu

School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
(480)965-8622
jeremy.rowe@asu.edu

Abstract—Large networks of sensors are used to detect intrusions and provide security at the borders of the United States.
Sensor signals are used to detect possible intrusions such as illegal
immigration trafﬁc in drugs, weapons, and smuggled goods at
speciﬁc targeted geographic locations. GIS systems can be used to
capture, store and analyze this location based intervention data.
Using a GIS system, a spatial database can be generated from
the sensor intervention data which can take into account relevant
geographic information in the vicinity of the sensed interventions.
Important geographic features that are close to the intervention
locations such as: plateaus, hills, valleys or roadways can be
extracted and added to the analysis using ArcGIS.
GIS techniques alone cannot reveal meaningful hidden information within geographic data. We have developed an integrated
approach involving data mining and GIS techniques to extract
patterns and trends in geographic data that can aid and inform analysis. Our approach uses both spatial and association
data mining techniques. Spatial data mining is the process
of discovering previously unknown, interesting and potentially
useful patterns from spatial datasets. Applying association rule
mining to the spatial data can reveal additional important spatial
relationships and help determine the relevance and importance
of the sensor data. Spatial association rule mining was used to
discover patterns in the intervention data, such as linking a sensed
intrusion with a potentially hidden location such as a canyon, to
infer a high probability of illegal trafﬁc or immigration.

I. I NTRODUCTION
Sensor networks that are used for surveillance purposes in
border security generate large amounts of data. The sensors are
used to identify a range of potential intrusions occurring at the
border. This sensor data includes some meaningful information
embedded within large datasets that include background noise
and potentially false alarms. Patterns can be identiﬁed within
the sensor signals data that can be used to identify and reduce
this noise and assist agents in identifying potentially actionable
interventions occurring at targeted geographic locations. The
identiﬁcation of important patterns from sensor signals has
been the focus of our previous study [1]. We developed

Fig. 1.

Interventions added as a layer on AZ map along the border.

an off-line learning system enables us to accurately identify
actionable interventions from the sensor signals. Once the
location and the type of interventions are known, spatial data
mining techniques can be applied to discover potentially useful
knowledge from the data. Currently, there is lot of work and
interest in Geographic data, as it has become a rich source of
structure and pattern making it ideal for data mining research
to address problems such as sensor data analysis [2][3]. The
current work focuses on integrating data mining and GIS
techniques to extract patterns and rules from the intervention
data.
This paper discusses the method we have employed to
extract patterns from the intervention data. Geographic knowledge of the vicinity of sensor intervention data is used to
create rules to extract useful patterns and information from the
data. Potentially important geographic features that are close
to the intervention locations such as: plateaus, hills valleys
or roadways can be extracted and added to the analysis using

___________________________________
978-1-4244-8351-8/11/$26.00 ©2011 IEEE



TABLE I
I NTERVENTION DATA OBTAINED FROM OFF - LINE LEARNING OF SENSOR

ArcGIS. These geographic features are extracted in the vicinity
of interventions and used to inform analysis of patterns in the
data.

PATTERNS

Intervention ID
1
2
3
4
5
6

II. M ETHOD
The objective of this work is to integrate the intervention
data and the GIS data to simplify identiﬁcation and extraction
of useful patterns. Spatial association rule mining is used
to extract spatial relationships between various potentially
relevant features in the geographic data. Interventions are
simulated at various geographic locations of potential illegal
activity such as illegal immigration, drug or weapon trafﬁc
(categorized A, B, C etc.). The locations are described by
geographic coordinates system, which use units of decimal
degrees for coordinates. Decimal degrees are angles, and these
units of measure are generally used with the GIS data. Xcoordinates are longitude values and Y-coordinates are latitude
values. For data in North America, the longitude values must
be negative numbers between 0 and -180 and latitude values
must be positive numbers from 0 and +90.
The intervention data has been added as a layer or map using
a GIS system. The speciﬁc software used for this purpose
is ArcGIS. Additional layers containing potentially relevant
geographic features of USA that are close to the intervention
locations such as: plateaus, hills valleys or roadways are added
as separate layers. Structured Query Language (SQL) expressions can be used to select potentially relevant features in
ArcGIS. Spatial join tools were used to extract the geographic
features proximal to the intervention locations. A table join
was created to append ﬁelds from one layer’s attribute table
to another layer’s attribute table based on the relative location
of features in the two layers.
All the relevant features were extracted in the vicinity of the
intervention test area to create a spatial database. Some of the
features were rivers within 5 miles, highways within 5 miles,
elevation value at the current intervention location, elevation
values within a 5-mile radius etc. The elevation values in the
vicinity of the intervention were used to classify the current
location as a hill or a valley. The maximum and minimum
values of the elevation within 5 miles of the intervention
location were recorded. The difference between the elevation
value at the current location and the maximum and minimum
values were used to classify the given intervention location as
a plateau, hill or valley.
Once the spatial database was generated techniques related
to association rule mining [4][5] were applied to extract
patterns in the data. A spatial association rule [2][6][7][8] is
used to describe the implication that one or a set of features
may have on another set of features in a spatial database. The
data is preprocessed to format similarly to a customer market
database to extract spatial association rules, which is discussed
in the later sections.
III. E XPERIMENTS
Sensors commonly used in surveillance applications generate vast amounts of data. Border security agents use this

X-Coordinate
-110.92
-112.02
-111.99
-111.94
-113.18
-110.30

Y-Coordinate
32.23
33.45
33.46
33.44
34.57
31.96

Type of Intervention
A
B
D
C
C
B

Fig. 2. Intervention data superimposed with geographic features on AZ map
as separate layers.

data to identify potential interventions such as illegal immigration, drug and weapon trafﬁc. The large amount of
data and high amount of noise and potentially false alarm
information within this data makes human analysis extremely
difﬁcult and problematic. In our current work, we simulated
border security sensor data, and embedded instances of speciﬁc
information about the type of intervention and the geographic
location of actionable events within the overall dataset. A
combination of data mining and GIS techniques were applied
to gain geographic knowledge and identify important patterns
from the data to attempt to separate meaningful actions from
background noise to identify the targets within the overall
datasets. In this section, we describe the data generation using
GIS techniques and analysis of the geographic data to extract
useful rules and patterns used in this process.
A. Data Generation
Since actual intervention data from the sensors was not
available, data was simulated for these experiments. Approximately 200 interventions were simulated along a 15-mile strip
of the AZ-Mexico border. The dataset consisted of the ID of
the intervention, its geographic coordinates and the type of



TABLE II
DATABASE CONSISTING OF GEOGRAPHICAL FEATURES IN THE VICINITY OF INTERVENTIONS .
Intervention ID
1
2
3
4

Rivers in 5 miles
Gila River
Colorado River

Fig. 3.

Fig. 4.

Elev at point
462
1058
280
406

Elev in 2 miles radius
730,857,710,802,692
875, 1041, 912, 882, 1011
294, 296, 304,297,287
536,541,545, 523, 519

Highways in 5 miles
I-40
I-40
I-10

An example market basket database[9].

A binary 0/1 representation of market basket data in Fig 3[9].

intervention as shown in Table I. These interventions were
added as a layer on the AZ map using ArcGIS as shown in
Fig 1.
GIS data of AZ containing various geographic features like
plateaus, hills, valleys, roadways, and elevation raster etc. are
added as separate layers as shown in Fig 2. Once all the
relevant features are added to the existing map, an SQL-like
query is used to extract the features in the vicinity of the
interventions. A database similar to that shown in Table II is
generated.

extracted. The distance for the neighborhood was chosen to
be 5 miles for highways and rivers. Elevation values at the
intervention locations were extracted from a raster shape ﬁle
with the pixel value ranging from 26-3745 with 26 being the
least elevation point and 3745 being the highest. The elevation
values within 2 miles of the intervention location were also
extracted. These values were used to classify the given location
as a plateau, hill or a valley by comparing the maximum and
minimum values with the value at the current intervention
location.

B. Extraction of features using GIS

C. Spatial Association Rule Mining

Geographic Information Systems (GIS) are used in various
ﬁelds and applications to explore large data sets to aid analysis
and obtain insight. Spatial or geographic queries are performed
on spatially indexed database to extract information regarding
spatial relationships between the data items. The intervention
data as shown in Fig 1 is superimposed with other geographic
layers as shown in Fig 2 to extract useful spatial relationships
such as containment, intersection, adjacency etc.
The spatial join tool in ArcGIS was used to extract geographic features in the vicinity of the simulated interventions
using an SQL-like query. This tool is used to ﬁnd the spatial
relationships between the features in various layers of a map.
For example, this tool enables identiﬁcation of the nearest
branch ofﬁces of a business to a particular location. The
distance of search can be varied from a few feet to several
miles.
Features such as plateaus, hills, valleys, roadways or elevation values in the vicinity of the 200 interventions were

A spatial association rule is a rule of the form ”X → Y ”,
where X and Y are sets of predicates, with some or both
being spatial ones. For example the following rules are spatial
association rules
Is a(x, of f ice) ∧ close to(x, downtown)
→
type of business(f inance)
where the consequent is non-spatial while the antecedent is
spatial
→
close to(x, downtown)
Is a(house with pool)
where consequent is spatial.
Strong spatial association rules reveal important information
in geographic data. To focus our study on strong rules,
measures such as support and conﬁdence were introduced
in Agrawal et al[4][5]. Extracting association rules from a
market basket database required a database transformation. As
an example, consider the database of a customer transactions
shown in Fig 3[9]. In order to extract association rule mining
to this item database, a transformation was required in the



TABLE III
DATABASE AFTER PREPROCESSING .
Intervention ID
1
2
3
4

Elev at point
462
1058
280
406

Increase in Elev
120
12
13
120

Decrease in Elev
22
213
18
117

form shown in Fig 4[9]. A similar transformation was required
for our simulated sensor spatial database to implement spatial
association rule mining.
Geographic data preprocessing is often time and effort
consuming step in the knowledge discovery process. Preprocessing of the intervention data generated (Table 2) is
required before association rule mining can occur. To apply
conventional data mining algorithms, spatial data have to be
described in terms of spatial predicates rather than as items.
Spatial predicates can include terms of topology, distance (like
close to) and directions (to north of ). The result of the
preprocessing is a table as shown in Table III, which contains
spatial relationships for spatial objects and data categorization
(if necessary) for non-spatial objects.
We applied supervised spatial association rule mining to the
preprocessed database (similar to Table 3) to discover spatial
patterns of the types of interventions occurring at various
geographic locations. The software CBA (v2.1) that integrates
classiﬁcation and association rule mining was used for this
task. We were able to successfully build accurate classiﬁers
from the intervention data, where each record speciﬁes the
type of intervention and a ﬁxed number of attributes used to
describe them. Interesting rules such as if the intervention is
occurring in a valley i.e.
Increase in Elev ≥ 400 → T ype of intervention = A
were discovered from the dataset.
IV. C ONCLUSION
In this paper we have incorporated GIS and data mining
techniques to classify the interventions based on the geographic locations. Spatial data was generated by considering
the spatial closeness of the interventions and the geographic
features from GIS data of the southern United States border
between Arizona and Mexico. The spatial data is preprocessed
to permit application of apply conventional data mining techniques. Supervised spatial association rules were discovered
from the data that could provide useful information about
the intervention locations, for example enabling the border
agents to identify potentially meaningful intrusions within the
voluminous sensor data, to analyze the potential reason for
the intrusion (i.e. drug or weapon intervention, illegal immigration, etc.) at that location, and then to take precautionary
action at those speciﬁc locations.
ACKNOWLEDGMENT
I would like to thank the United States Department of
Homeland Security through the National Center for Border
Security and Immigration for supporting the research under



Gila River
contains
?
?
?

Colorado River
?
?
contains
?

I-10
?
?
?
crosses

I-40
crosses
crosses
?
?

grant number 2008-ST-061-BS0002 . However, any opinions,
ﬁndings, and conclusions or recommendations in this document are those of the authors and do not necessarily reﬂect
views of the United States Department of Homeland Security.
R EFERENCES
[1] A.Kondaveeti, G. Runger, and H. Liu and J. Rowe, Identifying Important
Patterns and Reducing False Alarms in Sensor Networks,
Computing,Informatics and Decisions Systems Engineering, Arizona State
University, Technical Report 2009.
[2] Krzysztof Koperski and Jiawei Han, Discovery of spatial association
rules in geographic information databases, Proceedings of the Fourth
International Symposium on LargeSpatial Databases (SSD ’95), pages
47-66, 1995.
[3] Krzysztof Koperski and Jiawei Han and Junas Adhikary, Mining Knowledge in Geographical Data, Communications of the ACM, volume 26,
1998.
[4] Rakesh Agrawal and Tomasz Imielinski and Arun Swami, Mining Association Rules between sets of items in large databases. pages 207-216,
1993.
[5] Ramakrishnan Srikant and Rakesh Agrawal, Mining Generalized Association Rules, pages 407-419, 1995.
[6] Donato Malerba and Floriana Esposito and Francesca A. Lisi and Annalisa Appice, Mining Spatial Association Rules in Census Data, 2002.
[7] Jiawei Han and Krzysztof Koperski and Nebojsa Stefanovic, GeoMiner:
A System Prototype for Spatial Data Mining, pages 553-556, 1997.
[8] Salvatore Rinzivillo and Franco Turini, Extracting spatial association
rules from spatial transactions, GIS, pages 79-86, 2005.
[9] Pang-Ning Tan, Michael Steinbach and Vipin Kumar, Introduction to Data
Mining, Addison-Wesley,2005.

Image Denoising with a Multi-Phase Kernel
Principal Component Approach and an Ensemble
Version
Anshuman Sahu
and George Runger
Department of Industrial Engineering
SCIDSE, Arizona State University
Tempe, AZ 85287 USA
Email: anshuman.sahu@asu.edu, runger@asu.edu

Abstract—Image denoising is an important technique of practical significance which serves as a preliminary step for other
analyses like image feature extraction, image classification etc.
Two novel methods for denoising images are proposed which
deal with the case when there is no noise-free training data. The
basic method consists of several phases: the first phase involves
preprocessing the given noisy data matrix to obtain a good
approximation matrix; the second phase involves implementing
kernel principal component analysis (KPCA) on the approximation matrix obtained from the first phase. KPCA is one of the
useful non-linear techniques applied to image denoising. However,
an important problem faced in KPCA is estimating the denoised
pre-image. Consequently, we generate a pre-image by solving
a regularized regression problem. The second method is an
ensemble version of the basic method that provides robustness to
noisy instances. Some of the attractive properties of the proposed
methods include numerical stability and ease of implementation.
Also our methods are based on linear algebra and avoid any nonlinear optimization. Our methods are demonstrated on high-noise
cases (for both Gaussian noise and “salt and pepper” noise) for
the USPS digits dataset, and they perform better than existing
alternatives both in terms of low mean square error and better
visual quality of the reconstructed pre-images.

I. I NTRODUCTION
Image denoising is of significant importance in practice. It
consists of removing noise from corrupted images while retaining overall pattern structure. This serves as the preliminary
step for further analyses like image feature extraction, image
classification etc. A popular algorithm used in denoising is
kernel principal component analysis (KPCA) which finds the
nonlinear patterns in the data. An assumption in doing KPCA
is the availability of noise-free training datasets. However, it is
commonly found in applications like manufacturing variation
analysis that the training data is noisy, and the algorithms
devised for such applications have to take this fact into
consideration. Therefore, the objective of our research is to
present a new method for denoising images using KPCA for
the case when we don’t have noise-free training data.
Our contribution is two-fold: we augment the kernel ridge
regression procedure by a preprocessing step which is crucial
in practice for high noise scenarios; second we present an

Daniel Apley
Department of Industrial Engineering
and Management Sciences
Evanston, IL, 60208 USA
Email: apley@northwestern.edu

ensemble procedure that is robust to noisy instances and is
much faster and easier to implement in practice. The following is the outline of our paper: Section II describes KPCA
briefly and summarizes the problem; Section III describes our
methodology in detail; Experiment results on USPS data set
for different high noisy scenarios for both Gaussian and “salt
and pepper” noise are presented in section IV; Finally, we
conclude in section V.
II. KPCA D ISCUSSION AND P ROBLEM D ESCRIPTION
KPCA is a non-linear extension to principal component
analysis (PCA) wherein we transform the data in the input
space to the feature space and perform PCA in the feature
space [1]. The idea is to capture non-linear variations in
the data once we transform to a high-dimensional space.
Furthermore, the kernel trick [2] makes it easy to extend the
linear PCA calculations in the input space to the feature space.
Basically, we extract the eigenvectors in the feature space
based on the training data, project a test point in the feature
space unto the space spanned by leading eigenvectors, and then
compute the preimage of this projected point to estimate the
denoised test point in input space. The last step of obtaining
preimage is a hard one. Since the exact preimage may not exist
[3], we try to find an approximate solution. This is achieved by
minimizing the squared distance between 𝜑(X̂) and 𝑃 𝜑(X)
[3] where 𝑃 𝜑(X) is the projected test matrix in feature space,
X̂ is the desired preimage, and 𝜑 is defined as a nonlinear
mapping from input space to feature space. The preimage
problem has been studied extensively, and a lot of different
approaches have been proposed in literature. Mika et al [3]
proposed a gradient descent approach which suffers from numerical instabilities and getting trapped in local minima while
solving a non-linear optimization approach. Kwok and Tsang
[4] proposed a method similar to multidimensional scaling and
estimated the preimage as a linear combination of the nearest
neighbors. Bakir et al [5] apply kernel regression approach to
obtain preimage. Both the above approaches require noise-free
training observations. Zheng et al [6] developed a penalized

strategy for preimage learning where some penalization terms
are used to guide the preimage learning process. Nguyen et al
[7] present a method to handle noise, outlier and missing data
in the context of KPCA.
Most of the methods described above except for the one
proposed by Nguyen et al [7] require learning of eigenvectors
from a prototype noise-free training dataset which is not
available in our case. However, Nguyen et al [7] attempt to
solve a non-linear optimization problem which is sensitive
to initial guess and might have numerical issues. We clearly
want a simple approach that uses linear algebra. We are faced
with the problem of denoising a noisy data matrix which
itself serves as the training set. We present two approaches in
the next section-a basic two-phase method and its ensemble
version- to solve this problem.
III. A PPROACHES FOR D ENOISING
A. Two-Phase Method
The two-phase method essentially consists of a preprocessing phase and a kernel regression phase each of which is
described in a separate subsection below for convenience. We,
henceforth, denote this two-phase method by BKPCA.
1) First Phase: For the first phase, given a 𝑛 × 𝑑 noisy
data matrix X where 𝑛 is the number of data points and
𝑑 is the data dimensionality, we preprocess it to obtain an
approximation matrix X̃. The idea behind preprocessing is
that the approximation matrix retains the overall structure of
the original data while getting rid of some noise so that we get
a better starting point for the second phase. This preprocessing
is crucial because the second phase involves learning from this
approximation matrix the reverse mapping from feature space
to input space.
For our purpose, we use singular value decomposition
(SVD) of X to obtain X̃. Consider the decomposition of X
as X = UDV′ . Then X̃ = UD̃V′ where D̃ is the same
matrix as D except for the fact that it consists only of the
top 𝑠 singular values of D (truncated to top 𝑠 values) and
all other singular values are set to zero. Here 𝑠 is a user
defined parameter. In practice, since the data matrix is noisy,
lower singular values might be non-zero but still correspond to
noise. Thus, by selecting only the top 𝑠 (and not all non-zero)
singular values, we expect to get rid of some noise.
We can also employ several other techniques developed for
matrix approximation. Concept Decomposition was proposed
in [8] where the decomposition is obtained by taking the leastsquares approximation onto the linear subspace spanned by all
the concept vectors. [9] proposes Generalized Low Rank Approximations of Matrices (GLRAM) as an iterative algorithm
to reduce the approximation error in an iterative manner that
converges rapidly. The author also states that using GLRAM
with SVD, where GLRAM precedes SVD, can reduce the
approximate error greatly compared to GLRAM only while
keeping the computation cost low. Another approximation
method CUR matrix decomposition is developed in [10] where
the low-rank matrix decomposition is expressed in terms of a
small number of actual rows and/or columns of the original

data matrix making it more interpretable. The above references
also empirically establish that the approximation errors of
their methods are close to truncated SVD while achieving a
significant speed-up and/or memory storage compared to SVD.
Therefore, we can use the above methods for preprocessing
phase in case of high data dimensionality to obtain X̃.
2) Second Phase: The approximation matrix X̃ obtained
from the first phase will be the input matrix to the second
phase. We now apply KPCA to X̃ to estimate the denoised
preimage of X in input space denoted by X̂. Let K is the
kernel matrix given by (1)
K = 𝜑(X̃)𝜑(X̃)′

(1)

For our case we use a Gaussian kernel with parameter 𝜎. Also
𝜑(X̃) is the feature space transformation of X̃ and 𝑃 𝜑(X̃) is
the matrix of projection of the test points in feature space onto
space spanned by the chosen number of leading eigenvectors
𝜈. 𝜆 is a positive constant. We will also be using the Frobenius
norm of a matrix, denoted by ∥.∥𝐹 , in our case which is
calculated by taking the square-root of the sum of the squares
of each entry of the matrix.
We need to map 𝑃 𝜑(X̃) back to input space to obtain
X̂. We, therefore, seek a non-linear transformation Q from
the feature space
 back to the input space. Q is obtained


by minimizing X̃′ − Q𝜑(X̃)′  . A simple choice is to set
𝐹

Q = T𝜑(X̃) where T is a linear transformation matrix. We
now show how to derive X̂ for this simple choice of Q. Note
that our derivation can be thought of as an alternative to the
dual variable approach to kernel ridge regression as shown in
[11]. Substituting for Q, we now seek T satisfying




(2)
min X̃′ − TK
T

𝐹

where K is the kernel matrix. If K−1 exists, we find T =
X̃′ K−1 . Since K is positive semi-definite, we add the term 𝜆I
to K for 𝜆 > 0 to ensure numerical stability. This also ensures
a certain amount of regularization for non-zero 𝜆. Once we
obtain T and thus, obtain the non-linear transformation Q,
we obtain X̂ by mapping 𝑃 𝜑(X̃) from feature space to input
space using Q. This leads to
X̂ = 𝑃 𝜑(X̃)Q′ = 𝑃 𝜑(X̃)𝜑(X̃)′ (K + 𝜆I)−1 X̃

(3)

In order to calculate 𝑃 𝜑(X̃)𝜑(X̃)′ , we refer to [4] for the
following derivation: Let [x̃1 , x̃2 , . . . , x̃𝑛 ] be 𝑛 data points.
Let us consider the eigen decomposition of HKH = UΛU′
where H = I − 𝑛1 11′ , 1 = [1, 1, . . . , 1]′ is a 𝑛 × 1 vector,
U = [u1 , u2 , . . . , u𝑛 ] is the matrix containing eigenvectors
of K, and Λ is the diagonal matrix consisting of eigenvalues
(𝜆1 , 𝜆2 , . . . , 𝜆𝑛 ) of K. Also let kx̃ is a 𝑛 × 1 vector where
the 𝑖th entry is defined as the kernel dot product between x̃
and x̃𝑖 for 𝑖 = 1, 2, . . . , 𝑛, the kernel dot product here using
a Gaussian kernel is given by
)
(
2
− ∥x̃ − x̃𝑖 ∥𝐹
(4)
𝑘(x̃, x̃𝑖 ) = exp
𝜎

𝜈
∑
1
( )u𝑗 u′𝑗 where 𝜈 is the chosen number
𝜆𝑗
𝑗=1
of top eigenvectors to project onto, then for any data points x̃
and x̃𝑙

If we define L =

1
1
K1) + 1′ kx̃𝑙
(5)
𝑛
𝑛
The contribution here is to improve the kernel ridge regression
procedure through the preprocessing phase for the case when
training data is noisy. We present the ensemble version in the
next subsection.
𝑃 𝜑(x̃)𝜑(x̃𝑙 )′ = k′x̃𝑙 H′ LH(kx̃ −

B. Ensemble Version
The ensemble version of the basic two-phase method,
henceforth called EBKPCA, consists of repeating the twophase method on random subsets of data points chosen from
X for a given number of iterations and estimating the final
matrix of preimage X̂ as average of X̂ obtained across all
iterations. The random subset chosen in each iteration can be
thought of as a random sample from the training data that
has the size a fraction 𝜌 of the size of the training data. The
ensemble version provides some robustness to noisy training
data. Moreover, it can be parallelized and implemented easily.
The ensemble version also reduces the computational burden
by computing the kernel matrix on subsets of data rather than
the entire data.
Let us assume that the number of iterations is 𝐵, and in each
iteration we choose a random subset of data points (equal in
number for each iteration) from X with replacement.We apply
the two-phase method in each iteration and obtain an estimated
preimage for a test point. We estimate the final preimage of
the given test point as the average of the obtained 𝐵 points.
Let X̂𝑏 denote the preimage obtained from the 𝑏th sample. The
final estimated preimage X̂ is given by
X̂ =

𝐵
∑
X̂𝑏
𝑏=1

(6)

𝐵

IV. E XPERIMENTAL R ESULTS AND D ISCUSSION
We consider the USPS digit dataset available at http://wwwstat.stanford.edu/ tibs/ElemStatLearn/ for our experiments.We
are considering high noise scenarios: Gaussian noise with
mean 𝜇 = 0 and 𝜎𝐺 = 2 is considered. Also “salt and pepper”
noise with the probability parameter 𝑝 = 0.4 is considered,
where 𝑝 denotes the probability that a pixel value is set to
zero. The evaluation of our method will be based on the mean
squared error (MSE) as well as visual inspection. Let the true
data matrix be X0 . Then MSE is given by




(7)
𝑀 𝑆𝐸 = X̂ − X0 
𝐹

The true data matrix is used only for evaluation purpose. We
are using a Gaussian kernel for our purpose. We compare our
method with the one proposed in [4] henceforth called KMDS.
We carry out our experiments over a range of other parameters to study the behavior of the methods extensively.
For our experiments, we chose 300 data points for each digit

(0-9) and added the noise levels described in the beginning
of the section.The parameter values were chosen as follows:
𝜎 ∈ {50, 100, 500, 5000}; 𝜈 ∈ {50, 100}. For simplicity, we
kept 𝑠 = 𝜈. For the EBKPCA method, the number of iterations
𝐵 was chosen to be 50, and number of random data points
chosen at each iteration as kept equal to 150 (𝜌 = 0.5).
The number of nearest neighbors was kept equal to ten as
described by Kwok and Tsang [4]. We set 𝜆 to a small value,
R and compared
𝜆 = 0.01 . We wrote the code in MATLAB⃝,
our methods with KMDS using the algorithm provided on the
authors website (http://www.cse.ust.hk/ ivor/).
First we conducted a few initial experiments where we
ran the two-phase procedure for denoising the digits dataset
with and without the preprocessing phase for all the chosen
parameter values. We randomly chose three digits 4, 8, and
9 and added Gaussian noise with mean 𝜇 = 0 and 𝜎𝐺 = 2
for this initial experiment. The acronym PPC in Fig. 1 stands
for the case with preprocessing whereas the acronym NOPPC
stands for the case when we don’t use the preprocessing phase.
We see that preprocessing reduces MSE significantly. Thus, we
can see the preprocessing phase validates our intuition that
an approximation matrix results in a better reverse mapping
learning in the second phase than the original noisy data
matrix.
Now we show the results of our other experiments where
we use the two-phase method with the preprocessing phase
compared to KMDS. Figs 2 through 3 show how the MSE
varies for different parameter settings for all the algorithms
for the case with Gaussian noise 𝜇 = 0 and 𝜎𝐺 = 2 where
digits 0 through 4 are shown in Fig. 2, and digits 5 through
9 are shown in Fig. 3. Figs 4 through 5 show how the MSE
varies for different parameter settings for all the algorithms
for the case with “salt and pepper” noise with 𝑝 = 0.4 where
digits 0 through 4 are shown in Fig. 4, and digits 5 through 9
are shown in Fig. 5. We clearly see from the above Figs that
the BKPCA and EBKPCA methods outperform the KMDS
algorithm significantly for all the chosen paramter values.
Finally, we apply all the methods to the digits dataset. First,
we show the clean images in Fig. 6 to which we applied the
noise. For the instances corrupted with Gaussian noise 𝜇 = 0
and 𝜎𝐺 = 2, we show the noisy test images in Fig. 7 and show
the visual preimages in Figs 8-10 for the KMDS, BKPCA and
EBKPCA methods respectively.
For the cases corrupted with “salt and pepper” noise with
𝑝 = 0.4, we show the noisy test images in Fig. 11 and show
the visual preimages in Figs 12-14 for the KMDS, BKPCA
and EBKPCA methods respectively.
We can clearly see from all the above Figs that BKPCA
and EBKPCA methods produce visually superior denoised
preimages than KMDS method under different high noise
scenarios.
V. C ONCLUSION
We present a novel two-phase approach and its ensemble
version to image denoising using KPCA for the case when
we don’t have noise-free training data. The first phase enables

KMDSnu50digit0
KMDSnu100digit0
BKPCAnu50digit0
BKPCAnu100digit0
EBKPCAnu50digit0
EBKPCAnu100digit0

280

300
PPCnu50digit4
PPCnu100digit4
NOPPCnu50digit4
NOPPCnu100digit4

280

260

260
240

MSE

MSE

240

220

220
200

200
180

180

160

160

3

4

5

6
log(sigma)

7

8

9

3

4

6
log(sigma)

7

8

9

7

8

9

7

8

9

7

8

9

7

8

9

(a) Digit 0

(a) Digit 4
280

300
PPCnu50digit8
PPCnu100digit8
NOPPCnu50digit8
NOPPCnu100digit8

280

5

KMDSnu50digit1
KMDSnu100digit1
BKPCAnu50digit1
BKPCAnu100digit1
EBKPCAnu50digit1
EBKPCAnu100digit1

260
240

260

MSE

220

MSE

240

200
180

220
160

200

140
120

180

100

160

3

4

5

6
log(sigma)

7

8

3

4

5

9

6
log(sigma)

(b) Digit 1

(b) Digit 8
280

300
PPCnu50digit9
PPCnu100digit9
NOPPCnu50digit9
NOPPCnu100digit9

280

260
KMDSnu50digit2
KMDSnu100digit2
BKPCAnu50digit2
BKPCAnu100digit2
EBKPCAnu50digit2
EBKPCAnu100digit2

240

MSE

260

MSE

240

220

220

200

200

180

180
160

160

3

4

5

6
log(sigma)

7

8

3

4

5

6
log(sigma)

9

(c) Digit 2
(c) Digit 9
280

Fig. 1.
and 9

MSE values with and without preprocessing phase for digits 4, 8,
260

MSE

240

KMDSnu50digit3
KMDSnu100digit3
BKPCAnu50digit3
BKPCAnu100digit3
EBKPCAnu50digit3
EBKPCAnu100digit3

220

200

180

160

3

4

5

6
log(sigma)

(d) Digit 3
KMDSnu50digit4
KMDSnu100digit4
BKPCAnu50digit4
BKPCAnu100digit4
EBKPCAnu50digit4
EBKPCAnu100digit4

280

260

240

MSE

us to preprocess the given noisy data matrix obtain a good
initial starting point for the second phase. The second phase
implements KPCA and obtains the reconstructed preimage by
solving a regularized regression problem. The ensemble version also provides robustness to noisy instances in the training
set. Both these approaches are simple, numerically stable, and
easy to implement, especially the ensemble version which can
be easily parallelized. They are based on linear algebra and
do not involve non-linear optimization. Our method performs
better in terms of low mean square error as well as good visual
quality of the reconstructed preimages. Some future research
directions will involve considering other ensemble versions
like model stacking,model bumping and considering alternate
sampling schemes for the ensemble version.

220

200

180

160

3

4

5

6
log(sigma)

(e) Digit 4
Fig. 2. MSE for KMDS versus BKPCA and EBKPCA for Gaussian noise
𝜇 = 0 and 𝜎𝐺 = 2

280

266
264

260
262
KMDSnu50digit5
KMDSnu100digit5
BKPCAnu50digit5
BKPCAnu100digit5
EBKPCAnu50digit5
EBKPCAnu100digit5

220

260

MSE

MSE

240

200

KMDSnu50digit0
KMDSnu100digit0
BKPCAnu50digit0
BKPCAnu100digit0
EBKPCAnu50digit0
EBKPCAnu100digit0

258
256
254
252

180
250
160

3

4

5

6
log(sigma)

7

8

248

9

3

4

(a) Digit 5

260

6
log(sigma)

7

8

9

7

8

9

7

8

9

7

8

9

7

8

9

(a) Digit 0

KMDSnu50digit6
KMDSnu100digit6
BKPCAnu50digit6
BKPCAnu100digit6
EBKPCAnu50digit6
EBKPCAnu100digit6

280

5

315
310
305

240

300
KMDSnu50digit1
KMDSnu100digit1
BKPCAnu50digit1
BKPCAnu100digit1
EBKPCAnu50digit1
EBKPCAnu100digit1

295
MSE

MSE

220

200

290
285
280

180

275
160
270
140

3

4

5

6
log(sigma)

7

8

265

9

3

4

(b) Digit 6

260

240

6
log(sigma)

(b) Digit 1

KMDSnu50digit7
KMDSnu100digit7
BKPCAnu50digit7
BKPCAnu100digit7
EBKPCAnu50digit7
EBKPCAnu100digit7

280

5

285
280
KMDSnu50digit2
KMDSnu100digit2
BKPCAnu50digit2
BKPCAnu100digit2
EBKPCAnu50digit2
EBKPCAnu100digit2

275
270
MSE

MSE

220
265

200
260
180

255

160

140

250

3

4

5

6
log(sigma)

7

8

245

9

3

4

(c) Digit 7
285

260

280
275
KMDSnu50digit8
KMDSnu100digit8
BKPCAnu50digit8
BKPCAnu100digit8
EBKPCAnu50digit8
EBKPCAnu100digit8

200

265
260

180

255

160

140

KMDSnu50digit3
KMDSnu100digit3
BKPCAnu50digit3
BKPCAnu100digit3
EBKPCAnu50digit3
EBKPCAnu100digit3

270
MSE

MSE

220

6
log(sigma)

(c) Digit 2

280

240

5

250

3

4

5

6
log(sigma)

7

8

245

9

3

4

(d) Digit 8

260

6
log(sigma)

(d) Digit 3

KMDSnu50digit9
KMDSnu100digit9
BKPCAnu50digit9
BKPCAnu100digit9
EBKPCAnu50digit9
EBKPCAnu100digit9

280

5

300
295
290

240

KMDSnu50digit4
KMDSnu100digit4
BKPCAnu50digit4
BKPCAnu100digit4
EBKPCAnu50digit4
EBKPCAnu100digit4

285
280
MSE

MSE

220

200

275
270
265

180

260
160
255
140

3

4

5

6
log(sigma)

7

8

9

250

3

4

5

6
log(sigma)

(e) Digit 9

(e) Digit 4

Fig. 3. MSE for KMDS versus BKPCA and EBKPCA for Gaussian noise
𝜇 = 0 and 𝜎𝐺 = 2

Fig. 4. MSE for KMDS versus BKPCA, EBKPCA for “salt and pepper”
noise with 𝑝 = 0.4

Fig. 6.

285
2

2

4

8
10

12

12

14

14

14

16
4

6

8

10

12

14

16

4

6

8

10

12

14

16

8

10

12

14

16

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

10

12

14

16

8

12

14

16
8

10

12

14

6

6

8
10

12

4

4

6

8
10

2

2

4

6

8

16
2

2

4

6

14

16
2

2

4

12

14

16
4

10

12

14

2

2

8
10

12

16
6

6

8
10

14

4

4

6

8

12

2

2

4

6

10

16
2

2

4

6

8
10

12

2

2

4

6

8
10

16

Clean images for digits 0 through 9 serially

2

4

6

280

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

275
KMDSnu50digit5
KMDSnu100digit5
BKPCAnu50digit5
BKPCAnu100digit5
EBKPCAnu50digit5
EBKPCAnu100digit5

MSE

270
265

Fig. 7. Test images with Gaussian noise 𝜇 = 0 and 𝜎𝐺 = 2 of digits 0
through 9 serially
2

2

4

2

4

6

260

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

2

4

10

14

16
2

2

8

12

14

16
2

10

12

14

16

6

8
10

12

14

4

6

8
10

12

2

4

6

8
10

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

255
250
245

3

4

5

6
log(sigma)

7

8

9

Fig. 8. Denoised Preimages for KMDS method serially 0 through 9 for
Gaussian noise 𝜇 = 0 and 𝜎𝐺 = 2
2

2

4

290

6

8

10

12

14

16

6

8

10

12

14

16

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

Fig. 9. Denoised Preimages for BKPCA method serially 0 through 9 for
Gaussian noise 𝜇 = 0 and 𝜎𝐺 = 2

285

2

2

4

KMDSnu50digit6
KMDSnu100digit6
BKPCAnu50digit6
BKPCAnu100digit6
EBKPCAnu50digit6
EBKPCAnu100digit6

275
270

2

4

6

280

MSE

4

10

14

16
2

2

8

12

14

16
4

10

12

14

2

6

8
10

12

16

4

6

8
10

14

2

4

6

8

12

(a) Digit 5

2

4

6

10

2

4

6

2

4

6

2

4

6

2

4

6

2

4

6

2

4

6

2

4

6

4

6

6

8

8

8

8

8

8

8

8

8

8

10

10

10

10

10

10

10

10

10

10

12

12

12

12

12

12

12

12

12

12

14

14

14

14

14

14

14

14

14

14

16

16

16

16

16

16

16

16

16

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

Fig. 10. Denoised Preimages for EBKPCA method serially 0 through 9 for
Gaussian noise 𝜇 = 0 and 𝜎𝐺 = 2

265

2

2

4

2

4

6

260

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

2

4

10

14

16
2

2

8

12

14

16
2

10

12

14

16

6

8
10

12

14

4

6

8
10

12

2

4

6

8
10

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

255
250

3

4

5

6
log(sigma)

7

8

9

(b) Digit 6

Fig. 11. Test images with “salt and pepper” noise 𝑝 = 0.4 of digits 0 through
9 serially
2

2

4

2

4

6

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

2

4

10

14

16
2

2

8

12

14

16
2

10

12

14

16

6

8
10

12

14

4

6

8
10

12

2

4

6

8
10

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

300
295

Fig. 12. Denoised Preimages for KMDS method serially 0 through 9 for
“salt and pepper” noise 𝑝 = 0.4

290
KMDSnu50digit7
KMDSnu100digit7
BKPCAnu50digit7
BKPCAnu100digit7
EBKPCAnu50digit7
EBKPCAnu100digit7

MSE

285
280
275

2

2

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

2

4

10

14

16
2

2

8

12

14

16
4

10

12

14

2

6

8
10

12

16

4

6

8
10

14

2

4

6

8

12

14

16
2

4

6

8

10

12

14

16

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

Fig. 13. Denoised Preimages for BKPCA method serially 0 through 9 for
“salt and pepper” noise 𝑝 = 0.4

270
265

2

2

4

4

5

6
log(sigma)

7

8

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

6

8

10

12

14

16

8

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8

10

10

12

12

14

16
2

2

4

8

14

16
2

2

6

12

14

16
6

10

12

14

4

4

8
10

12

2

2

6

8
10

16
4

4

6

8

14

2

2

4

6

12

16
2

2

4

10

14

16
2

2

8

12

14

16
2

10

12

14

16

6

8
10

12

14

4

6

8
10

12

2

4

6

8
10

3

2

4

6

260
255

2

4

6

10

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

9

Fig. 14. Denoised Preimages for EBKPCA method serially 0 through 9 for
“salt and pepper” noise 𝑝 = 0.4

(c) Digit 7

2

2

4

2

4

6

4

6

8

10

12

14

16

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

4

6

8

10

12

14

16

10

12

14

16

4

6

8

10

12

14

16

4

6

8

10

12

14

16

4

6

6

8

8
10

12

12

14

16
2

2

4

10

14

16
2

2

8

12

14

16
8

10

12

14

6

6

8
10

12

4

4

6

8
10

2

2

4

6

8

16
2

2

4

6

14

16
2

2

4

12

14

16
2

10

12

14

16
2

10

12

14

16

8

10

12

14

6

8

10

12

4

6

8

10

2

4

6

8

285

14

16
2

4

6

8

10

12

14

16

16
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

280
275

KMDSnu50digit8
KMDSnu100digit8
BKPCAnu50digit8
BKPCAnu100digit8
EBKPCAnu50digit8
EBKPCAnu100digit8

MSE

270
265

ACKNOWLEDGEMENTS

260
255
250
245

3

4

5

6
log(sigma)

7

8

9

(d) Digit 8

R EFERENCES
[1] B. Schölkopf, A. J. Smola, and K.-R. Müller, “Nonlinear component
analysis as a kernel eigenvalue problem,” Neural Computation, vol. 10,
no. 5, pp. 1299–1319, 1998.
[2] M. Aizerman, E. Braverman, and L. Rozonoer, “Theoretical foundations of the potential function method in pattern recognition learning,”
Automation and Remote Control, vol. 25, pp. 821–837, 1964.
[3] S. Mika, B. Schölkopf, A. J. Smola, K.-R. Müller, M. Scholz, and
G. Rätsch, “Kernel pca and de-noising in feature spaces,” in Neural
Information Processing Systems NIPS, 1998, pp. 536–542.
[4] J. Kwok and I. Tsang, “The pre-image problem in kernel methods,”
IEEE Transactions on Neural Networks, vol. 15, pp. 1517–1525, 2004.

295
290
285

KMDSnu50digit9
KMDSnu100digit9
BKPCAnu50digit9
BKPCAnu100digit9
EBKPCAnu50digit9
EBKPCAnu100digit9

MSE

280
275
270
265
260
255
250

3

4

5

6
log(sigma)

This material is based upon work supported by the National
Science Foundation under Grant No. 0825331. Any opinions,
findings, and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.

7

8

9

(e) Digit 9
Fig. 5. MSE for KMDS versus BKPCA, EBKPCA for “salt and pepper”
noise with 𝑝 = 0.4

[5] G. H. Bakir, J. Weston, and B. Schölkopf, “Learning to find pre-images,”
in Advances in Neural Information Processing Systems, vol. 16, 2004,
pp. 449–456.
[6] W. S. Zheng, J. Lai, and P. C. Yuen, “Penalized preimage learning
in kernel principal component analysis,” IEEE Transactions on Neural
Networks, vol. 21, no. 4, pp. 551–570, 2010.
[7] M. H. Nguyen and F. D. la Torre, “Robust kernel principal component
analysis,” in Neural Information Processing Systems NIPS, 2008, pp.
1185–1192.
[8] I. S. Dhillon and D. S. Modha, “Concept decompositions for large sparse
text data using clustering,” Machine Learning, vol. 42, no. 1/2, pp. 143–
175, 2001.
[9] J. Ye, “Generalized low rank approximations of matrices,” Machine
Learning, vol. 61, no. 1-3, pp. 167–191, 2005.
[10] P. Drineas, M. W. Mahoney, and S. Muthukrishnan, “Relative-error cur
matrix decompositions,” SIAM J. Matrix Analysis Applications, vol. 30,
no. 2, pp. 844–881, 2008.
[11] C. Saunders, A. Gammerman, and V. Vovk, “Ridge regression learning
algorithm in dual variables,” in International Conference on Machine
Learning ICML, 1998, pp. 515–521.

NEUROCOMPUTING

Neurocomputing

18 (1998) 165-182

Neural network models for initial public offerings
Steven J. Robertsona, Bruce L. Goldenb3*, George C. Runger”,
Edward A. Wasild
aPrice Waterhouse LLC, 1616 North Fort Myer Drive, Arlington, VA 22209, USA
bMangement Science and Statistics, College of Business and Management, University of Maryland,
College Park MD 20742, USA
’ College of Engineering and Applied Sciences, Arizona State University, Tempe, AZ 85287, USA
dKogod College of Business Administration, American University. Washington, DC 20016, USA
Received 19 October 1996; accepted 24 August 1997

Abstract
In this article, we construct models that predict the first-day return of an initial public
offering. Our data set consists of the first-day returns for 1075 firms that went public between
1989 and 1994 and information that we gathered on 16 predictor variables. We segment the
data set into technology and nontechnology offerings and construct three types of models for
each segment - a regression model and two neural network models. Factorial experiments are
used to construct the neural network models. We find that the neural network models perform
well on both types of offerings. 0 1998 Elsevier Science B.V. All rights reserved.

Artificial neural networks; Initial public offerings; Backpropagation;
with ordinary least squares

Keywords:

Comparison

1. Introduction
An initial public offering (IPO) is the first public offering of equity or stock issued by
a corporation. In an IPO, the issuing corporation offers shares of stock to the public
in exchange for cash. Typically, a corporation will go public because it wants to raise
money to fund future growth, it wants to pay off debt, or it wants to create a market
for its shares [3]. Investors find IPOs attractive because they offer large average
returns during the first day of public trading.

*Corresponding

author. Tel.: 301 405 2232; e-mail: bgolden@mbs.umd.edu.

092.5-2312/98/$19.00 Copyright 0
PII SO925-2312(97)00077-S

1998 Elsevier Science B.V. All rights reserved.

166

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182

Investor demand for IPO shares often outpaces the supply of shares being offered
by corporations going public. On the average, this supply-demand imbalance provides IPO investors with large positive investment returns. Ritter [14] found that
investors who purchase shares of an IPO at the offer price and sell the shares at the
end of the first day of trading earn an average return of 14.8%. Because IPOs typically
yield high positive average returns, financial researchers are interested in trying to
predict the first-day (or initial) return (see [;)I).
Financial researchers frequently use ordinary least-squares (OLS) regression to
construct financial models (including models to predict the first day return of an IPO).
However, many researchers believe that financial markets can be modeled better with
nonlinear tools. To begin with, OLS assumes an underlying linear model. Although
residual analysis and other regression diagnostics are useful in identifying a linear
model failure, it is important to mediate such a failure with a nonlinear analysis.
Parametric, nonlinear regression models are numerically simple and widely available
Cl]. However, a parametric form for the model is still needed and the analysis provides
estimates of a relatively small number of unknown parameters. Nonparametric
regression has also received some attention in the statistical literature (e.g., see [7]).
However, nonparametric regression methods are most refined in the case of a single
independent variable. Multivariate generalizations (such as MARS) are available for
the more typical multiple regression analysis, but with a substantial increase in
complexity. Neural networks are more widely used and are, currently, easier to use
than a multivariate, nonparametric regression alternative. Recently, Jain and Nag
[lo] and Coy et al. [6] used neural networks to predict the first-day return with some
success, Numerous applications of neural networks to financial modeling have appeared in this journal (e.g., see [4,9, 111).
In this article, we construct three models to predict the first-day return of an IPO.
One model is constructed using OLS regression. Two models are constructed using
neural networks. The first neural network model is constructed with a computer
program developed by faculty and students in the College of Business and Management at the University of Maryland. The second neural network model is constructed
using a commercial off-the-shelf software package. Offerings for 1075 firms that went
public between 1989 and 1994 are modeled using 16 predictor variables. We compare
the performance of the neural network models to the regression model using mean
absolute error (MAE) of the predicted initial return as our measure of performance.
In Section 2, we present the data used to construct our models. The prediction
models are discussed in Section 3. Finally, in Section 4, we summarize our findings.

2. Data sets and variable generation
2. I. Overview
Our database was compiled from the Global Corporate New Issues [ 161 database
offered by Securities Data Company. The Global Corporate New Issues database
contains company, offering, and underwriting information. We compiled data for

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182

167

1075 IPOs from January 1989 to December 1994 and did not include unit offerings.
A unit offering is an IPO in which shares of stock and warrants are issued together to
form a unit security. The closing or final bid price for each IPO on the first day of
trading was taken from Standard and Poor’s Daily Stock Price Record: Over-theCounter (Standard and Poor’s Corporation [17]).
2.2. Dependent variable
The dependent variable (denoted by RETURN) is the initial return of an offering.
The initial return of an offering is defined as the percent change in the stock price from
its final offer price to the closing price after the first day of trading, i.e.,
RETURN = (PC - PF)/PF,
where PC is the first recorded closing or bid price listed in Standard and Poor’s Daily
Stock Price Record: Over-the Counter, and PF is the final offer price.
2.3. Ofering classiJication

The 1075 IPOs encompass a broad cross-section of industries. To improve the
prediction capability of our models, we designed a classification scheme. Using
standard industrial classification (SIC) codes, the observations were segmented into
two groups - technology offerings and nontechnology offerings. For example, IPOs
for corporations with SIC codes 3674 (semiconductors and related devices) or 7372
(prepackaged software) are considered to be technology offerings. For a complete
listing of the SIC codes used to generate the technology and nontechnology groups,
see Robertson [lS]. These two groups of offerings represent distinct market segments
according to one expert [12].
To ensure that these two groups are really different subsets of the entire sample,
descriptive statistics for RETURN were calculated for each classification. Table 1
displays these statistics. When compared to the nontechnology offerings, the technology offerings have greater initial returns as measured by the mean and median of
RETURN. Also, the returns for technology offerings are considered somewhat riskier
as measured by the standard deviation. The difference between the mean returns of the

Table 1
RETURN descriptive statistics for 1075 IPOs
RETURN statistics

Technology offerings

Nontechnology offerings

Mean
Median
Standard deviation
Minimum value
Maximum value
Number of observations

0.125
0.066
0.163
- 0.114
0.917
441

0.091
0.038
0.154
- 0.615
0.950
634

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182

168

two classifications is significant at the 1% level. Based on these statistics, the technology and nontechnology offerings can be viewed as different subsets of the 1075
offerings.
2.4. Independent variables
Sixteen independent variables are examined in building our models. Data for all of
these variables are available prior to the offer date. Table 2 lists these variables and
provides a short description of each one.
Hanley [8] found that the relationship between the final offer price and the range of
anticipated offer prices disclosed in the preliminary prospectus is a good predictor of
the initial return of an offering. The anticipated offer price range is set by the IPO
underwriter by analyzing comparable publicly traded companies to determine
price/earnings or price/book ratios that can be applied to the issuer. Let PH and
P, denote the highest price and lowest price in the offer range, respectively. The
variable %CHGOFF captures the relationship between the final offer price and the
range of anticipated offer prices in our models and is defined by
%CHGOFF

= (& - PE)/PE,

where PE is the expected offer price (that is, PE = (PH + P,)/2) and PF is the final offer
price.
To capture additional information about the relationship between the final offer
price and the initial offer price range, two indicator variables are used. ABOVE is
a binary variable that equals 1 if PF > PH and 0 otherwise. Offerings with PF > PH
are expected to produce a greater initial return than the return for IPOs with

Table 2
Independent
Variable

variables

name

Short description

%CHGOFF
ABOVE
BELOW
%CHGSHR
INCREASE
DECREASE
ABV_INC
BEL_DEC

Percent
Equals
Equals
Percent
Equals
Equals
Equals
Equals

change in final offer price
1 if Pr > Pn; 0 otherwise
1 if PF < PL; 0 otherwise
change in final number of shares offered
1 if N, z Np; 0 otherwise
1 if NA < Np; 0 otherwise
ABOVE x INCREASE
BELOW x DECREASE

AMT
CHGMKT
3MTHMKT
MKTSHR
QTY30
QTY90
RET30
RET90

Dollar size of the offering
Percent change in the stock
Percent change in the stock
Previous year’s underwriting
Number of offerings in the
Number of offerings in the
Average IPO initial return
Average IPO initial return

market from file date to offer date
market for the three months prior to the offer date
market share for book manager
last 30 d
last 90 d
in last 30 d
in last 90 d

S.J. Robertson et al. /Neurocompting

18 (1998) 165-182

169

PL < PF < Pu. Similarly, BELOW is a binary variable that equals 1 if PF < PL and
0 otherwise. Offerings with PF < PL are expected to produce a smaller initial return
than the return for IPOs for which PL -c PF < PH. ABOVE and BELOW together
represent the three possible states of the relationship between PF and the initial offer
price range.
Hanley [S] also analyzed the change in the number of shares offered between the
filing of the preliminary prospectus and the offer date. An increase in the number of
shares offered for issues is consistent with positive initial returns. To capture this
information, we use the variable %CHGSHR defined by
%CHGSHR

= (NA - Np)/Np,

where NA is the actual number of shares offered and NP is the preliminary estimate of
the number of shares to be offered. NP is disclosed in the preliminary prospectus.
To capture additional information about the relationship between the number of
shares estimated in the preliminary prospectus and the final number of shares offered,
two indicator variables are used. INCREASE is a binary variable that equals 1 if
NA > NP and 0 otherwise. Offerings with NA > NP are expected to produce a greater
initial return than the return for IPOs with NA = Np. DECREASE is a binary
variable that equals 1 if NA < NP and 0 otherwise. Offerings with NA < NP are
expected to produce a smaller initial return than the return for IPOs with NA = Nr.
INCREASE and DECREASE together represent the three possible states of the
relationship between NA and the preliminary estimate of the number of shares to be
offered.
For an offering with PF > PH and NA > Np, it is likely that positive information
about the offering was revealed during the pre-issue period. For an offering with
PF < P,. and NA < Np, it is likely that negative information was revealed about the
offering during the pre-issue period. To capture information about these two cases,
two indicator variables are used. In the former case, the binary variable ABVINC is
calculated as
ABV-INC = ABOVE x INCREASE.
In the latter case, the binary variable BEGDEC
BEL-DEC

is calculated as

= BELOW x DECREASE.

Ritter [14] reported that the standard deviation of offering returns is negatively
correlated with the size of an issue. Therefore, the size of the offering can be thought of
as a measure of the riskiness of the IPO. To capture this information, we use the
variable AMT. AMT is the amount of money raised by the issuing corporation and is
defined by
AMT = PF x NA.
The percent change in the stock market from the file date to the offer date is
expected to be positively related to the changes in the offer price (see [8]) and,
therefore, it is expected to be positively related to initial returns. To capture this
information, we use the variable CHGMKT. The variable CHGMKT is calculated as

170

S.J. Robertson et al. /Neurocomputing

18 (1998) 165-182

the percent change in the Dow Jones High Technology Index (NASDAQ Index) from
the file date to the last business day prior to the offer date for technology (nontechnology) offerings. To capture a longer run market trend, we use the variable 3MTHMKT,
which is defined as the percent change in the stock market for the three months
leading up to the offer date. The variable 3MTHMKT is calculated as the percent
change in the Dow Jones High Technology Index (NASDAQ Index) for the three
months prior to the last business day before the offer date for technology (nontechnology) offerings.
The reputation of the underwriters of an offering is hypothesized to be related to
IPO initial returns (see [S]). Smaller and less experienced underwriters may be less
likely to possess the research capability and expertise to evaluate the issuing corporation. This may lead to pricing errors. Larger and more experienced underwriters are
able to sell to a larger pool of institutional investors and therefore may be able to
gather more information about the offering during the pre-issue period. Mayer [ 121
says that investors evaluate an offering by looking at the book manager. To capture
all of this underwriter information, we use the variable MKTSHR which is the market
share of the book manager for an offering. MKTSHR for technology (nontechnology)
offerings is calculated as the technology (nontechnology) IPO market share of the
book manager for the calendar year prior to the offer date.
The final four variables capture information about the performance of other IPOs
prior to an offering. When the IPO market is strong and there is a great amount of
demand among investors for IPO shares, firms contemplating an offering will rush to
the market. When demand is soft for IPO shares, firms contemplating an IPO will not
rush to the market. Instead, they will wait until the market becomes stronger. To
capture information about the number of offerings prior to a specific offering, we use
the variables QTY30 and QTY90. For technology (nontechnology) offerings, QTY30
and QTY90 represent the number of technology (nontechnology) IPOs that have
taken place in the last 30 and 90 d, respectively. To capture information about the
returns on IPOs prior to an offering, we use the variables RET30 and RET90. For
technology (nontechnology) offerings, RET30 and RET90 represent the average initial
returns for technology (nontechnology) IPOs that have taken place in the last 30 and
90 d, respectively.
3. Modeling the initial return of an IPO
3.1. Overview
Our goal is to build high-quality models that can be used to successfully predict the
initial return of an IPO. Three different approaches are used: ordinary least-squares
regression, a neural network model trained and tested using a program developed at
the University of Maryland (Maryland Network Code, denoted by MNC), and
a neural network model trained and tested using a commercial software package
(Brainmaker). To evaluate the effectiveness of each model, we calculate the MAE of
the predicted RETURN and study the MAE from linear, parametric regression
models and nonlinear, nonparametric neural network models.

171

S.J. Robertson et al. 1Neurocomputing 18 (1998) 165-182
Table 3
Summary of data sets
Number of Offerings
Data set

Fraction

Technology

Nontechnology

Training
Testing
Validating

419
219
319

196
98
147

282
141
211

Total

919

441

634

Table 4
OLS linear regression model output: Technology offerings
R2
Adjusted RZ
F statistic
Observations

0.295
0.291
81.535
196
Coefficient

Intercept
%CHGOFF

t statistic

p-value

VIF

12.330
9.030

0.000
0.000

1.000

0.124
0.506

Before constructing our models, we subdivided the technology and nontechnology
offerings into three subsets. We randomly assigned the observations to a training set,
a testing set, and a validating set. A summary of the data sets is shown in Table 3.
The use of training, testing, and validating data sets is common in the artificial
neural network literature and that is why we adopt this type of data split. Our
regression models are built on the training data set (as are the neural network models)
and then evaluated on the validating data set (as are the neural network models). The
regression models do not take into account the observations in the testing data set.
Although the testing data set is used to identify the neural network parameter values
that perform best, it is not used to build the final neural network model that will be
validated (only the training set is used). Thus, the regression models and the neural
network models are built and evaluated on the same data sets.
3.2. OLS regression model: Technology oflerings
We use SPSS for Windows to construct the OLS regression model for RETURN.
The OLS model is built on the training set of technology IPO offerings using all 16
independent variables and a stepwise algorithm (for details, see [13]). The SPSS
output is summarized in Table 4.
The OLS model is given by
RETURN = 0.124 + 0.506%CHGOFF.

172

S.J. Robertson

et al. /Neurocomputing

18 (1998) 165-182

The F statistic, which measures overall model fit, is statistically significant at the
1% level. The adjusted R2 value for the OLS model is 0.291. This compares favorably
to the 0.178 adjusted R2 value that Hanley [S] obtained for her OLS model on
a different data set. The variable %CHGOFF is significant at the 5% level, Since this
model contains only one variable, multicollinearity is not a problem.
We point out that a plot of the prediction errors (residuals) against predicted values
for the OLS model indicates that the variance of the prediction errors is an increasing
function of the prediction. This is a violation of the OLS assumption that prediction
errors have a constant variance. The coefficient estimators are still unbiased, but
better estimators might be obtained from a transformation that compensates for the
nonconstant variance (see [ 133). Attempting nonlinear transformations might alleviate this problem. Alternatively, building nonlinear models (such as neural network
models) could be more useful with this data set.
Using the estimated coefficients shown in Table 4, we calculate the MAE of the
model’s predictions for the observations in the validating set. The MAE for this model
is 0.0983. This MAE serves as the benchmark against which we will compare the
MAEs of two other technology offering models that we construct in later sections of
this article.
3.3. OLS regression model: Nontechnology oflerings
We use the stepwise algorithm in SPSS for Windows to construct the OLS
regression model on the training set for nontechnology IPO observations. The SPSS
output is summarized in Table 5.
The OLS model is given by
RETURN = 0.103 + 0.380%CHGOFF

+ 0.048 INCREASE

- 0.022 DECREASE.
The F statistic, which measures overall model fit, is statistically significant at the
1% level. The adjusted R2 value for this OLS model is 0.165. This is much lower than
Table 5
OLS linear regression model output: Nontechnology offerings
R2

Adjusted R2
F statistic
Observations

Intercept
%CHGOFF
INCREASE
DECREASE

0.174
0.165
19.502
282
Coefficient

t statistic

0.103
0.380
0.048
- 0.022

8.888
6.543
2.333
- 0.869

p-value

VIF

0.000
0.000
0.020
0.386

1.043
1.076
1.103

S.J. Robertson et al. INeurocomputing 18 (1998) 165-182

173

the 0.291 adjusted RZ value for the technology offering OLS model. The variables
%CHGOFF and INCREASE are significant at the 5% level. Although DECREASE
is not statistically significant, it is included in the model because it is an indicator
variable that is coupled with INCREASE.
The variance inflation factor (VIF) is an important multicollinearity diagnostic.
A VIF value larger than 10 implies serious problems with multicollinearity for a data
sample [13]. The VIF values in the nontechnology OLS model are all less than 10, so
that multicollinearity does not appear to be a problem with this sample.
A scatter plot of the prediction errors (residuals) and predicted values for the
nontechnology OLS indicates that the prediction errors do not have a constant
variance. Nonlinear transformations might alleviate this problem or, alternatively,
building nonlinear models (such as neural network models) might be more useful with
this data set.
Using the estimated coefficients shown in Table 5, we calculate the MAE of the
model’s predictions for the observations in the nontechnology validating set. The
MAE for this model is 0.0962. This is essentially the same as the MAE of 0.0983
calculated for the technology OLS model.
3.4. MiVC model: Technology ofeerings
The Maryland Network Code (MNC) is a personal computer program developed
over the last five years by faculty and students in the College of Business and
Management at the University of Maryland, College Park. MNC is a research
program that contains state-of-the-art features (see [18] for details) and provides
a user with flexibility in building and training neural network models. A user can set
the values of a wide variety of parameters or rely on default settings provided in the
program. For example, different sigmoid slopes can be set for different neurons in an
MNC-created network. (While of little theoretical value, this often results in better
local minima than when a single sigmoid slope is used.) MNC uses the standard
backpropagation algorithm for training and can also perform network pruning
(see [18] for information on network pruning). Over the years, MNC has evolved
from a ‘batch driven’ program to the current version that has an easy-to-use
interface (with a few hours training, a person knowledgeable in neural networks can
use MNC).
Because of the interactions among parameters, finding the best settings for a neural
network model is very difficult. On the other hand, standard statistical methods can
be used to analyze the main effects of each factor (parameter) and the interaction
effects. In particular, we use a fractional factorial experiment to systematically find the
best parameter settings for MNC. We experiment with five parameters: number of
hidden nodes, output node sigmoid slope, learning rate, momentum, and network
pruning. The values of the first four parameters can be increased or decreased. The
pruning parameter can be set to on or off. Neural network training is performed on
the technology training data set and testing is performed on the technology testing set.
In our factorial experiment, we change the values of one or more of the five parameters
and measure the impact of these changes on the MAE of the technology testing set.

174

S.J. Robertson et al. / Neurocomputing 18 (1998) 165-182

Table 6
MNC settings and training procedures
Architecture
Hidden layer:
sigmoid slopes

Output node:
sigmoid slope
Learning rate
Momentum
Error threshold
Stopping criteria
Procedure

16 input nodes, 8 hidden nodes, 1 output node
0.15 node 1, 0.20 node 2, 0.25 node 3, 0.30 node 4,
0.35 node 5, 0.40 node 6, 0.45 node 7, 0.50 node 8 begin;
incremented by 0.2 after 100 iterations
0.5
0.2
0.4
0
200 iterations
Increase sigmoid slope after 100 iterations

Table 7
Parameter changes for MNC experiment
Parameter
Number of hidden nodes
Output node sigmoid slope
Learning rate
Momentum
Pruning

+

_

10
0.6
0.3
0.4
On

6
0.4
0.1
0.2
Off

After the best parameter settings have been identified, the final MAE is computed
using the technology IPO validating set.
Table 6 contains the initial parameter settings and procedure for training the neural
network. These parameter settings were found by Coy et al. [6] to provide the best
MAE in their MNC IPO models. Table 7 displays how the settings for each parameter change after the settings are increased or decreased. A plus sign ( + ) denotes an
increase in a parameter setting (pruning is turned on), while a minus sign ( - ) denotes
a decrease in a parameter setting (pruning is turned off ). Table 8 contains the results
of the experiments. The best MAE value for the testing data set is 0.0904.
Next, we construct an OLS regression model to determine which parameter values
have the greatest impact on MAE. The dependent variable for this regression model is
the MAE for the testing data set. The independent variables are the five parameter
settings. Since each parameter setting has two levels (increase or decrease, on or off),
the parameters can be represented by indicator variables that equal 1 for an increase
in a parameter setting and - 1 for a decrease in a parameter setting. We use SPSS for
Windows to build the regression model. The SPSS output is summarized in Table 9.
The SPSS results reveal that the number of hidden nodes and pruning are statistically significant at the 5% level. Furthermore, changes in these parameters are found
to be inversely related to the MAE for the testing set. The remaining three variables
are not statistically significant.

175

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182
Table 8
MNC factorial experiment results: Technology offerings
Hidden nodes
_
+
+
+
_
+
+
+
_
+
+

Learning rate

Sigmoid slope
_
-

_
_

+
+
_
-

+
+
+
+
_
_
-

+
+
_
_
+
+
_
-

+
+
+
+

+
+

Momentum
_
_
_
_
_

Pruning
+
_
+
+
+
_
_

+
+
+
+
+
+
+
+

+
+
+
+

MAE
0.0928
0.0921
0.0994
0.0910
0.1018
0.0927
0.0930
0.0910
0.0991
0.0904
0.0942
0.0944
0.0958
0.0912
0.0938
0.0907

Table 9
MNC OLS regression model: Technology offerings
R2
Adjusted R2
Observations

0.677
0.516
16
Coefficient

Intercept
Hidden nodes
Sigmoid slope
Learning rate
Momentum
Pruning

-

0.094
0.002
0.001
0.000
0.000
0.001

t statistic

p-value

157.357
- 3.810
- 0.879
- 0.360
- 0.439
- 2.324

O.OOil
0.034
0.400
0.729
0.670
0.043

Using the results of the OLS regression model, further experimentation is performed. We expect MAE values to decrease when pruning is enabled. Therefore, we
concentrate on increasing the number of hidden nodes. Since the value of the hidden
node parameter is inversely related to the value of MAE for the testing set, increasing
the number of hidden nodes should yield an improvement in the MAE value. Using
the parameter settings that provide the best MAE in the original experiment, we
increase the number of hidden nodes and track the value of MAE. The results of this
experiment are shown in Table 10. The MAE for the testing data set stops improving
when the number of hidden nodes reaches 12. With 12 hidden nodes, the MAE is
0.0898. We point out that another reasonable approach would be to use the initial

176

S.J. Robertson et al. /Neurocomputing

18 (1998) 165-182

Table 10
MNC hidden node experiment results: Technology offerings

Table 11
Final parameter settings for MNC model: Technology
offerings

Number of hidden nodes

MAE

Architecture

10
11
12
13

0.0904
0.0899
0.0898
0.0898

Output node sigmoid slope
Learning rate
Momentum
Pruning

16 input nodes
12 hidden nodes
1 output node
0.4
0.1
0.4
On

settings of the insignificant factors in subsequent experiments with the number of
hidden nodes.
The final parameter settings that produce the best testing MAE are shown in
Table 11. Using these parameter settings, we calculate the MAE of the model’s
predictions for the observations in the technology IPO validating data set. The MAE
for this model is 0.0903. This is the best MAE that was produced for the technology
IPO data set. It compares favorably to the 0.0983 MAE value for the OLS regression
model.
3.5. MNC model: Nontechnology ofSerings
The same five parameters that are used to build the MNC technology IPO neural
network model are used to build the MNC nontechnology IPO neural network
model. Network training is performed on the nontechnology training data set and
testing is performed on the nontechnology testing data set. The final MAE for the
nontechnology MNC model is calculated using the nontechnology validating data set.
Table 6 contains the initial parameter settings and procedure for training the neural
network with MNC. A fractional factorial experiment is used to find the best
parameter settings. Table 7 shows how the parameters are changed during the
experiment. Table 12 contains the results of our experiments. The best MAE value for
the testing data set is 0.0912.
Again, we construct an OLS regression model to determine which parameter values
have the greatest impact on MAE. The SPSS output for the MNC nontechnology
IPO model is summarized in Table 13.
The SPSS results reveal that the number of hidden nodes and pruning are statistically significant at the 5% level. Furthermore, changes in these parameters are found
to be inversely related to the MAE for the testing set. The remaining three variables
are not statistically significant.
We expect MAE values to decrease when pruning is enabled, so that we now
concentrate on increasing the number of hidden nodes. Since the value of the hidden
node parameter is inversely related to the value of MAE for the testing set, increasing
the number of hidden nodes should yield an improvement in the MAE value. Using
the parameter settings that provide the best MAE in the original experiment, further

111

S.J. Robertson et al. JNeurocomputing 18 (1998) 165-182
Table 12
MNC factorial experiment results: Nontechnology offerings
Hidden nodes
_
+
_
+
_
+
_
+
_
+
+
+
+

Learning rate

Sigmoid slope

_
_

_
_
+
+
_
_

+
+
+
+
_
_

+
+
_
+
+
_

+
+
+
+

+
+

Momentum
_
_
_
_
_
-

Pruning

MAE
0.0943
0.0968
0.1002
0.0924
0.0977
0.0921
0.0936
0.0940
0.0997
0.0958
0.0979
0.0919
0.0962
0.0929
0.0994
0.0912

+
_
+
_
+
+
_

+
+
+
+
+
+
+
+

+
+
_
+
+

Table 13
MNC OLS regression model: Nontechnology offerings
RZ
Adjusted R2
Observations

0.727
0.591
16
Coefficient

Intercept
Hidden nodes
Sigmoid slope
Learning rate
Momentum
Pruning

0.095
- 0.002
- 0.000
- 0.001
0.000
- 0.001

t statistic
199.532
- 4.171
- 0.641
- 1.556
0.600
- 2.497

p-value
0.000
0.002
0.536
0.151
0.621
0.032

experimentation with increasing the number of hidden nodes is performed. The results
of this experiment are shown in Table 14. The MAE for the testing data set stops
improving when the number of hidden nodes reaches 11. With 11 hidden nodes, the
MAE is 0.0910.
The final parameter settings that produce the best testing set MAE are shown in
Table 15. Using these parameter settings, we calculate the MAE of the model’s
predictions for the observations in the nontechnology IPO validating data set. The
MAE for this model is 0.0907. This is the best MAE that was produced for the
nontechnology IPO data set. It compares favorably to the 0.0962 MAE value for the
OLS regression model.

178

S.J. Robertson et al. /Neurocomputing

18 (1998) 165-182

Table 14
MNC hidden node experiment results: Nontechnology offerings

Table 15
Final parameter settings for MNC model: Nontechnology offerings

Number of hidden nodes

MAE

Architecture

10
11
12
13

0.0912
0.0910
0.0910
0.0910

Output node sigmoid slope
Learning rate
Momentum
Pruning

16 input nodes
11 hidden nodes
1 output node
0.6
0.3
0.4
On

3.6. Brainmaker models
Brainmaker (California Scientific Software [5]) is a commercial, off-the-shelf neural
network software package. Brainmaker uses the standard backpropagation algorithm
to train a network. The package lets a user specify the network architecture and set the
values of parameters such as the learning rate when designing a neural network
model.
We build two Brainmaker neural networks - one for the technology offerings and
one for the nontechnology offerings - using the modeling framework developed for
the MNC program. A fractional factorial experiment is used to find the best settings
for five parameters in Brainmaker: number of hidden nodes, sigmoid slope, learning
rate, momentum, and error threshold. Network training is performed on the training
data set and testing is performed on the testing data set. After the best parameter
settings have been identified, the final MAE for the model is calculated using the
validating data set. Next, we construct an OLS regression model to determine which
parameter values have the greatest impact on MAE and then perform some additional
experimentation on those parameters.
The final parameter settings in Brainmaker that produce the best MAE for the
technology testing data set and the best MAE for the nontechnology data set are
summarized in Tables 16 and 17, respectively. Using these parameter settings, we
calculate the MAE of each model’s predictions for the observations in the technology
and nontechnology IPO validating data sets. The MAE for the technology model is
0.0922 and for the nontechnology model it is 0.0936.
3.7. Summary of results
We summarize the results of our modeling efforts for technology offerings in
Table 18 and for nontechnology offerings in Table 19.
In both tables, we see that the MAE values are monotonically decreasing. As our
models become more sophisticated by accounting for nonlinearities in the data, they
perform better. On the right-hand side of each table, we show the percent improvement for each model over the MAE value of the OLS linear regression model. To
illustrate, in Table 18, for the technology offerings, we see that the MNC neural

179

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182

Table 16
Final parameter settings for Brainmaker model:
Technology offerings

Table 17
Final parameter settings for Brainmaker model:
Nontechnology offerings

Architecture

Architecture

Sigmoid slope
Learning rate
Momentum
Error threshold

16 input nodes
11 hidden nodes
1 output node
0.5
0.3
0.7
0.25

16 input nodes
13 hidden nodes
1 output node
0.9
0.3
0.7
0.075

Sigmoid slope
Learning rate
Momentum
Error threshold

Table 18
Summary of model results: Technology offerings

Model

Validating data set MAE

Percent improvement over OLS
linear regression MAE

OLS linear regression
Brainmaker
MNC

0.0983
0.0922
0.0903

0.00
6.21
8.14

Table 19
Summary of model results: Nontechnology offerings

Model

Validating data set MAE

Percent improvement over OLS
linear regression MAE

OLS linear regression
Brainmaker
MNC

0.0962
0.0936
0.0907

0.00
2.70
5.72

network model produces results with an MAE value that is 8.14% better than the
MAE value produced by the OLS linear regression model (i.e., 8.14% = 100(0.09830.0903)/0.0983). In Table 19, for nontechnology offerings, MNC produces results that
are 5.72% better than the results of the OLS linear regression model. In fact, for both
types of offerings, the MNC neural network models perform better than the regression
models and the Brainmaker models.

4. Conclusions
For both technology and nontechnology segments, our neural network models
were better than our linear regression models at predicting the first-day return of an

180

S.J. Robertson et al./Neurocomputing 18 (1998) 165-182

initial public offering. The predictions produced by the MNC neural network models were better than the predictions produced by linear regression and
Brainmaker.
We point out that predictor variables that characterize market information (i.e.,
strength of the market) and offering information (i.e., the size of an IPO) were used to
predict the first-day return and that all information was available prior to the offer
date. Therefore, an investor can use our models to predict the initial return for an
offering just prior to the offer date.
Using factorial experiments to understand the effects of different parameter settings
on the testing data MAE provided a systematic approach to building high-quality
neural network models. However, since we did not vary the parameter settings
exhaustively when we performed our experiments, it is likely that we found only
locally optimal values for the parameters. The experiments could be designed to
ensure that a more comprehensive search is performed in order to find optimal or
near-optimal parameter settings. We intend to pursue this direction in future
work.

Acknowledgment

We thank Kathleen Weiss Hanley for her help in data gathering and her suggestions regarding the finance literature.

References
[l] D.M Bates, D.G. Watts, Nonlinear Regression Analysis and its Applications, Wiley, New York,
1988.
[Z] L. Benveniste, P. Spindt, How investment bankers determine the offer price and allocation of new
issues, J. Financial Econom. 24 (1989) 343-361.
[3] S. Blowers, The Ernst & Young Guide to Taking Your Company Public, Wiley, New York, 1995.
[4] D. Brownstone, Using percentage accuracy to measure neural network predictions in stock market
movements, Neurocomputing 10 (1996) 237-250.
[S] California Scientific Software, Brainmaker User’s Guide and Reference Manual, Nevada City, CA,
1993.
[6] S.P. Coy, R. Balasubramanian, B.L. Golden, 0. Kwon, H. Beirjandi, Using neural networks to predict
the degree of underpricing of an initial public offering, in: R.S. Freedman (Ed.), 3rd Int. Conf.
on Artificial Intelligence Applications on Wall Street, Software Engineering Press, 1995,
pp. 223-231.
[7] P.J. Green, B.W. Silverman, Nonparametric Regression and GLM, Chapman & Hall, New York,
1994.
[S] K. Hanley, The underpricing of initial public offerings and the partial adjustment phenomenon, J.
Financial Econom. 34 (1993) 231-250.
[9] T.H. Hann, E. Steurer, Much ado about nothing? Exchange rate forecasting: neural networks vs.
linear models using monthly and weekly data, Neurocomputing 10 (1996) 323-339.
[lo] B. Jain, B. Nag, Artificial neural network models for pricing initial public offerings, Decision Sci. 26
(1995) 283-302.
[l l] M. Leshno, Y. Spector, Neural network prediction analysis: the bankruptcy case, Neurocomputing 10
(1996) 125-147.

S.J. Robertson et al. /Neurocomputing 18 (1998) 165-182

181

WI W.E. Mayer, November 1995, personal communication.
Cl31 D. Montgomery, E. Peck, Introduction to Linear Regression Analysis, Wiley, New York, 1992.
Cl41 J. Ritter, The cost of going public, J. Financial Econom. 19 (1987) 269-281.
Cl51 S. Robertson, Modeling initial public offerings with artificial neural networks, Master’s Thesis,
College of Business and Management, University of Maryland, College Park, MD, 1996.
Cl61 Securities Data Company, User’s Manual, New York, 1995.
1171 Standard and Poor’s Corporation, Standard and Poor’s Daily Stock Price Record: Over-the-Counter,
New York, published quarterly, 1989-1994.
Cl81 X. Sun, Neural network models for the wire bonding process, Ph.D. Dissertation, College of Business
and Management, University of Maryland, College Park, MD, 1994.

Steven J. Robertson is a Consultant with the Price Waterhouse Financial and
Economic Modeling Group. He has a BBA from the University of Wisconsin and
an MBA and MS from the University of Maryland.

I

Bruce L. Golden is a Professor and Distinguished Faculty Research Fellow in the
College of Business and Management at the University of Maryland. His research
interests include neural networks, heuristic search, and applied operations research. He has received numerous awards, including the 1994 Thomas L. Saaty
Prize. He is currently Editor-in-Chief of the INFORMS Journal on Computing.

George C. Runger is a faculty member in Industrial and Management Systems
Engineering at Arizona State University. His research interests are in process
control and experimental design strategies and he has received an Ellis Ott award
and a Brumbaugh award for his research. He is coauthor of the text Applied
Statistics and Probabilityfor Engineers. He was a faculty member in the College of
Business and Management at the University of Maryland and at Rensselaer
Polytechnic Institute and a senior engineer at IBM. He holds degrees in Industrial
Engineering and Statistics.

182

S.J. Robertson et al. /Neurocomputing I8 (1998) 165-182

Edward A. Wasif is a Professor of Management Science in the Kogod College of
Business Administration at American University in Washington, D.C., where he
1 has taught graduate courses in operations research and ap$ed statistics for the
last 12 years. His research interests include the application of neural networks to
business decision problems, network optimization, and the process of implemen1 ting management science models. Currently, Dr. Wasil serves as the Feature
Article Editor of the INFORMS Journal on Computing.

Journal of Machine Learning Research 10 (2009) 1341-1366

Submitted 5/08; Revised 3/09; Published 7/09

Feature Selection with Ensembles, Artificial Variables, and
Redundancy Elimination
Eugene Tuv

EUGENE . TUV @ INTEL . COM

Intel, Logic Technology Development
5000 W Chandler Blvd, CH5295
Chandler, AZ, 85226, USA

Alexander Borisov

ALEXANDER . BORISOV @ INTEL . COM

Intel, Logic Technology Development
Lopatina st. 3-199
N.Novgorod
Russia, 603163

George Runger

RUNGER @ ASU . EDU

Industrial Engineering Department
Arizona State University
Tempe, AZ, 85287, USA

Kari Torkkola

KARITO @ AMAZON . COM

Amazon.com
701 5th Avenue, Suite 1800
Seattle, WA, 98104, USA

Editor: Isabelle Guyon and Amir Reza Saffari

Abstract
Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and
categorical predictors, and may contain interactive effects that require complex models. This is a
challenge for filters, wrappers, and embedded feature selection methods. We describe details of
an algorithm using tree-based ensembles to generate a compact subset of non-redundant features.
Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking
and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness
of the approach.
Keywords: trees, resampling, importance, masking, residuals

1. Introduction
Large data sets are becoming the norm and traditional methods designed for data sets with a modest
number of features will struggle in the new environment. This problem area was described by
Guyon and Elisseeff (2003) along with other publications in the same issue, and it has increased
in importance since then. Additional comments and examples have been provided by Liu and Yu
(2005) in a recent survey article.
c

2009
Eugene Tuv, Alexander Borisov, George Runger and Kari Torkkola.

T UV, B ORISOV, RUNGER AND T ORKKOLA

1.1 Feature Selection
There are three major categories of feature selection methods. Filter methods score variables, typically individually, and eliminate some before a model is constructed. The filter needs to be generated
carefully to relate well to the requirements of the modeling task. In particular, the filter may not
consider the value of one variable in the presence of others. For example, the widely-used value
difference metric (VDM) (Stanfill and Waltz, 1986) and its modified version (MVDM) (Cost and
Salzberg, 1993) consider the conditional probability distribution of the response at a predictor value.
Such a measure is not sensitive to the effects of some predictors in a model with others present even
though interactions among predictors might be critical for an effective subset. A sequential, subset search is sometimes implemented to improve the performance when interactions are important,
although a greedy search also has disadvantages in the presence of interactions. Several common
filter methods such as ReliefF (Robnik-Sikonja and Kononenko, 2003), CFS (Hall, 2000), and FOCUS (Almuallin and Dietterich, 1994) were modified with sequential search and compared by Yu
and Liu (2004).
Wrapper methods form a second group of feature selection methods. The prediction accuracy
(or the change in accuracy) of a model directly measures the value of a feature set. Although
effective, the exponential number of possible subsets places computational limits for the wide data
sets that are the focus of this work.
Embedded methods form a third group for feature selection. These methods use all the variables to generate a model and then analyze the model to infer the importance of the variables.
Consequently, they directly link variable importance to the learner used to model the relationship.
1.2 Subset Feature Selection
Fundamentally, the goal of feature selection is to model a target response (or output) variable y,
with a subset of the (important) predictor variables (inputs). This is a general goal and several
more specific objectives can be identified. Each can lead to different strategies and algorithms. In
filtering the interest is to remove irrelevant variables. Another objective is variable ranking where
the interest is in obtaining relative relevance for all input variables with respect to the target. Finally,
we might be interested in a compact, yet effective model, where the goal is to identify the smallest
subset of independent features with the most predictive power, although a few alternative groups
might be reasonable. An important concept here is the masking relationships among the predictor
variables. Masking occurs when one variable can effectively represent others in a model. Along
with the related issue of masking, this paper focuses on the subset selection.
1.3 Contributions of this Paper
Existing tree ensembles such as random forest (Breiman, 2001) or gradient boosting trees (Friedman, 1999) were developed primarily for predictive modeling. In addition, they can provide an
importance ranking of the features, but this information has been considered an ad hoc benefit.
Random forest (RF) is a random subspace method, and is capable of efficiently ranking features for
large data sets. We exploit this property of RF, augment the original data with artificial contrast
variables constructed independently from the target, and use their ranking for removal of irrelevant
variables from the original set. The tree construction method is also modified to produce a more
reliable variable ranking in the presence of high cardinality variables. A variable masking measure
1342

F EATURE S ELECTION WITH E NSEMBLES

is then introduced that incorporates surrogate variable scores from ensembles of trees. This forms
the basis for redundancy elimination. Residual effects are calculated to enable the method to detect
variables of secondary importance. These elements are integrated into an efficient algorithm for
subset selection called ACE (artificial contrasts with ensembles).
The structure of this paper is as follows. In Section 2 we describe previous work and outline
directions taken in this paper. Section 3 describes variable importance measures defined through
tree ensembles and explains how they could be used to remove irrelevant features using random,
artificial features. Next, we introduce a masking measure and use it for redundancy elimination.
Section 4 describes the details of the ACE algorithm to generate the selected subset, and compares
ACE with its closest competitors in detail. Section 5 provides results from experiments. Section 6
provides conclusions.

2. Background
This section defines the problem of finding the best susbset of features, discusses previous approaches, and outlines our solution.
2.1 Markov Boundaries
Let F be a full set of features. A feature selection solution can be described in terms of a Markov
blanket (Koller and Sahami, 1996). Given a target feature Y , let M ⊂ F and Y ∈
/ M. M is said to be
a Markov blanket for Y if Y ⊥(F − M)|M. That is, Y is conditionally independent of other features
given M. A minimal Markov blanket is referred to as Markov boundary (MB) and such a subset
might be considered a feature selection solution. However, an important issue is that a MB need not
be unique. Redundant features can replace others in a feature subset. Usually feature redundancy is
defined in terms of feature correlation (Hall, 2000). For example, two features are redundant to each
other if their values are completely correlated. In reality, it is not so straightforward to determine
feature redundancy if a feature is partially correlated to a set of features.
Our goal is to focus on the important case with redundant features and obtain at least one MB. In
most real-life problems exactly determining the MB or measuring feature relevance is very difficult
because of a limited sample size, high time complexity, and noise in the data. Furthermore, evaluation of the distribution of the input variables and the response always relies on some model (linear,
support vector machine, frequency tables, trees, etc.). In practice, most algorithms just try to remove
irrelevant features and then apply some heuristics that remove “possibly” redundant variables.
2.2 Existing Approaches in Feature Selection
The nature of real life data sets provides strong restrictions for model fitting and feature selection
methods. First, data sets may be very large both in terms of the number of predictors and in the
number of samples (tens of thousands × tens of millions). Second, the predictors and the response
can be of mixed type (both numeric and categoric), and can contain missing values. Lastly and
also very importantly, dependency of the response on predictors can be highly non-linear, noisy and
multivariate.
This leaves most existing methods out of scope for such problems. For example, wrapper methods (forward selection or backward elimination) are simply computationally unfeasible when dealing with thousands of predictors. Filter methods are also useless for the minimal subset selection
1343

T UV, B ORISOV, RUNGER AND T ORKKOLA

problem, as they do not deal with the notion of redundancy and most of them are inherently univariate. However, there are filters that use a“local” feature importance measure (like RELIEF) that
can be considered multivariate (Kira and Rendell, 1992), but still they do not deal with redundancy
giving just a ranked list of features instead of a selected minimal subset.
Subset evaluation filter methods such as CFS (Hall, 2000) are neither suitable because they do
not deal explicitly with redundancy, and have to iterate over many feature subsets incurring a high
time complexity. For example, the time complexity of the CFS is at least quadratic in the number
of features and linear in number of samples. Also CFS is highly sensitive to outliers as it uses
correlations between features.
Many embedded methods that use a built-in feature relevance measurement, such as SVM-RFE
(Guyon et al., 2002) and linear regression with backward feature elimination are heavily dependent
on the model (linear or SVM), that can fail to fit the data well. These methods have at least quadratic
complexity in the number of samples for fitting an SVM and at least cubic complexity in the number
of features (O(nm2 + m3 ), where m is the number of features, and n is number of samples) for fitting
a regression model. Data sets with tens of thousands of features or samples become very time
consuming and impractical to handle. For example, SVM-RFE involves retraining the SVM after
features with smallest relevance are removed, thus incurring at least cubic complexity in number of
samples (O(max(m, n)n2 )).
An issue that discourages using regression methods and methods that rely on some kind of
distance measure between observations (linear regression, SVM, Kernel-based methods, RELIEF)
is the difficulty of dealing with outliers in the input (predictor) space. Also, selection of important
model parameters (kernel width and type, feature relevance thresholds, etc) is non-obvious, and the
results of feature selection depend heavily on them.
Most methods return just a ranked list of features instead of an optimal subset. These methods
include RELIEF, Koller’s Markov blanket based backward elimination (referred to here as MBBE)
(Koller and Sahami, 1996), and SVM-RFE. Some methods such as FCBS use a relevance threshold
that is not clear how to adjust (Yu and Liu, 2004). In reality, the user also obtains a number of
feature subsets corresponding to different values of parameters without a hint of how to choose the
best subset.
Many methods work with frequency tables. They can thus deal well with categorical inputs
only. For numerical inputs, they require discretization. Such methods are not always able to deal
with interacting variables and have great difficulties with multivariate dependencies on numerical
inputs. Examples of such methods are FCBS and MBBE. These two algorithms need discretization
because they use an entropy measure computed on frequency tables. If the number of categories is
large, or if we use frequency tables with more than two inputs, the tables can be sparse and may
not represent the data distribution well. Another issue for MBBE is computational complexity.
Considering all feature pairs incurs a quadratic complexity on the number of features.
Hence we see that most methods at hand are either not applicable at all to the best subset selection problem, or have some major problems. The most useful methods in such a setting (that appeared to be applicable to the examples of large “real-life” data in the challenge data sets discussed
in Sec. 5.3) are methods based on backward feature elimination using an approximate Markov blanket concept (Koller and Sahami, 1996; Yu and Liu, 2004). Our method approximates the optimal
Markov blanket redundancy elimination procedure, but without most of the drawbacks of previous
methods.
1344

F EATURE S ELECTION WITH E NSEMBLES

2.3 Towards Efficient and Approximately Optimal Feature Selection
We propose a method that uses an idea similar to those proposed by Koller and Sahami (1996) and
Yu and Liu (2004) that tries to overcome their limitations. It does not have quadratic time complexity
in the number of features, can deal with thousands of predictors, uses a model (ensembles of trees)
that can be applied to mixed variable types, thus eliminating need for discretization of numeric
inputs, does not require imputation of missing values, captures local information (like RELIEF), is
invariant to a monotone transformation of inputs, thus not very sensitive to noise and outliers, and
deals well with multivariate dependencies.
It is well known that trees and especially ensembles of trees can provide robust and accurate
models in “real-life” data settings. They handle mixed and noisy data, and are scale insensitive.
Ensembles of trees have high predictive power and are resistant to over-fitting (Breiman, 2001).
Our approach relies heavily on ensembles of trees.
First, we find irrelevant features that are conditionally independent of the response given the
rest of the features. It is accomplished by comparing the relevance of the original variables with
the relevance of random, artificial features (appended to the original data) constructed from the
same distribution, but independently from the response. These features are referred to as artificial
contrasts. We measure feature relevance as variable importance in random forests with a modified
robust splitting criteria. We assume that if an original variable had a relevance score not statistically
higher than that of an artificial probe (independent from the target by construction) then it is also
independent from the target, irrelevant, and should be removed. Note that we try to remove irrelevant
features by directly assessing conditional independence without searching for a MB, the existence
of which is a much stronger requirement. Although the idea of artificial contrasts was already used
by other researchers in simple filter methods with success (Stoppiglia et al., 2003), its application to
tree ensembles is novel and promising. Actually, our approach can be considered as non-parametric
because all parameters in our algorithm can be assigned reasonable default values that work well
for wide range of problems.
Then the redundant feature elimination step is performed. Redundancy between features is
measured using surrogate scores. The variable with the largest impurity reduction score at a node is
the primary splitter. If surrogate variables (ones that partition the node in same way as the primary
variable) are present, these surrogate variables are considered as “masked”. Masking scores between
all pairs of important variables are computed and evaluated using a statistical test, and variables
masked by more important variables (“approximately redundant”) are removed iteratively.
Finally, after a set of non-redundant relevant features has been found, our method removes the
influence of the found subset with an ensemble and proceeds. Because redundancy elimination is
approximate in nature this iterative approach is another advantage of our method. It allows one to
recover variables with small importance and to reduce the chance to lose important variables during
redundancy elimination.

3. Tree Ensembles for Feature Selection
For our embedded method, we focus on ensembles of decision trees for the following reasons. Trees
can be applied in ubiquitous scenarios so that they provide a good entry point for feature selection for
interdisciplinary, wide data sets. They apply to either a numerical or a categorical response. They
are nonlinear, simple and fast learners that handle also both numerical and categorical predictors
well. They are scale invariant and robust to missing values. A simple decision tree also provides an
1345

T UV, B ORISOV, RUNGER AND T ORKKOLA

embedded measure of variable importance that can be obtained from the number and the quality of
splits that are generated from a predictor variable. However, a single tree is produced by a greedy
algorithm that generates an unstable model. A small change to the data can result in a very different
model. Consequently, ensemble methods have been used to counteract the instability of a single
tree.
Supervised ensemble methods construct a set of simple models, called base learners, and use
their weighted outcome (or vote) to predict new data. That is, ensemble methods combine outputs
from multiple base learners to form a committee with improved performance. Numerous empirical studies confirm that ensemble methods often outperform any single base learner (Freund and
Schapire, 1996; Bauer and Kohavi, 1999; Dietterich, 2000a). The improvement can be dramatic
when a base algorithm is unstable. More recently, a series of theoretical developments (Bousquet
and Elisseeff, 2001; Poggio et al., 2002; Mukherjee et al., 2006; Poggio et al., 2004) also confirmed
the fundamental role of stability for the generalization of a learning algorithm. More comprehensive
overviews of ensemble methods were presented by Dietterich (2000b) and Valentini and Masulli
(2002). There are two primary approaches to ensemble construction: parallel and serial.
A parallel ensemble combines independently constructed and diverse base learners. That is,
different base learners should make different errors on new data. An ensemble of such base learners
can outperform any single one of its components since diverse errors cancel out (Hansen and Salamon, 1990; Amit and Geman, 1997). Parallel ensembles are variance-reduction techniques, and in
most cases, they are applied to unstable, high-variance algorithms (such as trees). Also, Valentini
and Dietterich (2003) showed that ensembles of low-bias support vector machines (SVMs) often
outperformed a single, best-tuned, canonical SVM (Boser et al., 1992).
Random forest (RF) is an exemplar for parallel ensembles (Breiman, 2001). It is an improved
bagging method (Breiman, 1996) that extends the “random subspace” method (Ho, 1998). It grows
a forest of random decision trees on bagged samples showing excellent results comparable with the
best known classifiers. A RF can be summarized as follows: (1) Grow each tree on a bootstrap sample of the training set to maximum depth, (2) Given M predictors, select at random m < M variables
at each node, and (3) Use the best split selected from the possible splits on these m variables. Note
that for every tree grown in RF, about one-third of the cases are out-of-bag (out of the bootstrap
sample). The out-of-bag (OOB) samples can serve as a test set for the tree grown on the non-OOB
data. We discuss later how OOB samples can be used for feature selection.
In serial ensembles, every new learner relies on previously built learners so that the weighted
combination forms an accurate model. A serial ensemble algorithm is often more complex. It is
targeted to reduce both bias and variance. A serial ensemble results in an additive model built by
a forward-stagewise algorithm. The adaboost algorithm was introduced by Freund and Schapire
(1996). At every step of ensemble construction the boosting scheme adds a new base learner that is
forced (by iteratively reweighting the training data) to concentrate on the training observations that
are misclassified by the previous sequence. Boosting showed dramatic improvement in accuracy
even with very weak base learners (like decision stumps, single split trees). Breiman (1998) and
Friedman et al. (2000) showed that the adaboost algorithm is a form of gradient optimization in
functional space, and is equivalent to a forward-stagewise, additive algorithm with the exponential
loss function Ψ(y, F(x)) = exp(−yF(x)) referred to as a gradient boosted tree (GBT).
1346

F EATURE S ELECTION WITH E NSEMBLES

3.1 Relative Variable Importance Metrics
A single decision tree partitions the input space into a set of disjoint regions, and assigns a response
value to each corresponding region. It uses a greedy, top-down recursive partitioning strategy. At
every step an exhaustive search is used to test all variables and split points to achieve the maximum
reduction in impurity. Therefore, the tree constructing process itself can be considered as a type
of variable selection (a kind of forward selection, embedded algorithm), and the impurity reduction
due to a split on a specific variable indicates the relative importance of that variable to the tree model
(Breiman et al., 1984). For ensembles, the metric is averaged over the collection of base learners.
Note, that this relative importance automatically incorporates variable interaction effects thus being
very different from the relevance measured by a univariate filter method.
For a single decision tree the measure of variable importance is
V I(Xi , T ) =

∑ ∆I(Xi ,t),

(1)

t∈T

where ∆I(Xi ,t) is the decrease in impurity due to an actual (or potential) split on variable Xi at a
node t of the optimally pruned tree T (Breiman et al., 1984). Node impurity I(t) for regression is
defined as ∑i∈t (yi − ȳ)2 /N(t) where the sum and mean are taken over all observations i in node t,
and N(t) is the number of observations in node t. For classification I(t) = Gini(t) where Gini(t) is
the Gini index of node t defined as
Gini(t) = ∑ pti ptj ,
i6= j

pti

is the proportion of observations in t whose response label equals i (y = i) and i, j run
and
through all response class numbers. The Gini index is in the same family of functions as crossentropy = − ∑i pti log(pti ), and measures node impurity. It is zero when t has observations only from
one class, and is maximum when classes are perfectly mixed. The decrease ∆I(Xi ,t) computes the
impurity at the node t and the weighted average of impurities at each child node of t. The weights
are proportional to the number of observations that are assigned to each child from the split at node
t so that ∆I(Xi ,t) = I(t) − pL I(tL ) − pR I(tR ).
For an ensemble of M trees this importance measure is easily generalized. It is simply averaged
over the trees
1 M
E(Xi ) =
(2)
∑ V I(Xi , Tm ).
M m=1
The averaging makes this measure more reliable.
This split weight measure ∆I(Xi ,t) in Equation (1) can be improved if OOB samples are used.
The split value for a variable is calculated using the training data as usual. However, the variable
selected as the primary splitter uses only the OOB samples. Also, the variable importance measure
is calculated from only the OOB samples. This provides a more accurate and unbiased estimate of
variable importance in each tree and improves the filtering of noise variables.
Breiman (2001) also proposed a sensitivity based measure of variable relevance evaluated by a
RF. For a classification problem it is summarized as follows: (1) Classify the OOB cases and count
the number of votes cast for the correct class in every tree grown in the forest, (2) randomly permute
the values of variable m in the OOB cases and classify these cases down the tree, (3) Subtract the
number of votes for the correct class in the variable-m-permuted OOB data from the original OOB
data, and (4) Average this number over all trees in the forest to obtain the raw importance score for
variable m. Similar ideas were presented by Parmanto et al. (1996) and a similar resampling strategy
1347

T UV, B ORISOV, RUNGER AND T ORKKOLA

was successfully used in a more traditional model by Wisnowski et al. (2003). The sensitivity
measure is computationally expensive. Furthermore, it does not account for masking, nor does it
consider an iterative process with residuals (that we describe in Sec. 4.2). Experiments by Tuv
(2006) demonstrated that weaker but independent predictors can rank higher than stronger, but
related predictors. Also, related predictors can all be identified as important. Neither of these
results are desirable for a best subset model and a more effective algorithm is described in Sec. 4.
With the importance measure (2) we can thus merely rank the variables. The following two
subsections discuss how to amend the ranking so that irrelevant variables can be reliably detected,
and how the redundancies among the remaining relevant variables can then be handled.
3.2 Removing Irrelevant Features by Artificial Contrasts
Although an ensemble can be used to calculate a relative feature ranking from the variable importance score in (2) the metric does not separate relevant features from irrelevant. Only a list of
importance values is produced without a clear indication which variables to include, and which to
discard. Also, trees tend to split on variables with more distinct values. This effect is more pronounced for categorical predictors with many levels. It often makes a less relevant (or completely
irrelevant) input variable more “attractive” for a split only because it has high cardinality.
The variable importance score in (2) is based on the relevance of an input variable to the target.
Consequently, any stable feature ranking method should favor a relevant input Xi over an artificially
generated variable with the same distribution as Xi but generated to be irrelevant to the target. That
is, a higher variable importance score is expected from a true relevant variable than from an artificially generated contrast variable. With sufficient replicates in an analysis one can select important
variables from those that have statistically significantly higher variable importance scores than the
contrast variables (Tuv et al., 2006). Here, these contrast variables are integrated into a subset
algorithm. We discuss this in detail in Section 4.
Also, artificial contrasts can be applied to masking discussed in the next subsection. Given a
selected subset of relevant variables, one computes the masking scores of all variables by elements
of this subset, and the masking of contrast variables by this subset. Masking scores statistically
higher than the contrast variables are considered to be real masking. Variables that are masked are
dropped from the relevant subset list over a sequence of iterations of the algorithm.
3.3 Masking Measures
An important issue for variable importance in tree-based models is how to evaluate or rank variables
that were masked by others with slightly higher splitting scores, but could provide as accurate a
model if used instead. One early approach in the CART methodology used surrogate splits (Breiman
et al., 1984). The predictive association of a surrogate variable X s for the best splitter X ∗ at a tree
node T is defined through the probability that X s predicts the action of X ∗ correctly and this is
estimated as
p(X s , X ∗ ) = pL (X s , X ∗ ) + pR (X s , X ∗ ),
where pL (X s , X ∗ ) and pR (X s , X ∗ ) define the estimated probabilities that both X s and X ∗ send a case
in T left (right). The predictive measure of association λ(X ∗ |X s ) between split X s and primary split
X ∗ is defined as
min(πL , πR ) − [1 − p(X s , X ∗ )]
λ(X ∗ |X s ) =
,
min(πL , πR )
1348

F EATURE S ELECTION WITH E NSEMBLES

where πL , πR are the proportions of cases sent to the left(or right) by X ∗ . It measures the relative
reduction in error (1 − p(X s , X ∗ )) due to using X s to predict X ∗ as compared with the “naive” rule
that matches the action with max(πL , πR ) (with error min(πL , πR )). If λ(X ∗ |X s ) < 0 then X s is
disregarded as a surrogate for X ∗ . Sometimes a small, nonnegative threshold is used instead. The
variable importance sum in Equation (1) is taken over all internal tree nodes where Xi is a primary
splitter or a surrogate variable (λ(X ∗ |Xi ) > 0 for a primary splitter X ∗ ). Often a variable that does not
appear as a primary splitter in a tree is still ranked high on the variable importance list constructed
using surrogate variables.
We extend the surrogate concept to define a masking score as follows. Variable i is said to mask
variable j in a tree, if there is a split in variable i in a tree with a surrogate on variable j. We define
the masking measure for a pair of variables i, j in tree T as
Mi j (T ) =

∑

{t∈T |split

w(Xi ,t)λ(Xi |X j ),
on Xi }

where w(Xi ,t) = ∆I(Xi ,t) is the decrease in impurity from the primary split on variable Xi , and
summation is done over the nodes where primary split was made on variable Xi . Here we take into
account both the similarity between variables Xi , X j at the node, and the contribution of the actual
split of variable Xi to the model. For an ensemble the masking measure is simply averaged over the
trees. Note that in general the measure is not symmetric in the variables. One variable may mask
several others, but for a single selected masked variable the reverse may not be true.

4. Algorithm: Ensemble-Based Feature Selection with Artificial Variables and
Redundancy Elimination
We now integrate the previously described concepts and metrics into a subset selection algorithm.
The fundamental steps outlined in Section 2.3 consist of using the advantages of a parallel ensemble
to detect important variables among potentially a very large feature set, using the advantages of a
serial ensemble to de-mask the important variables, and calculating residuals and repeating in order
to recover variables of secondary importance.
Within the algorithm, artificial contrast variables are re-generated a number of times. Then
the significance from a paired t-test over the replicates is used to identify important variables and
masked variables. Essentially the t-test is used to define thresholds for selection and masking. These
thresholds could also be set as tunable parameters. An advantage of the statistical test is that the
significance of selected variables relative to noise can be quantified.
4.1 Algorithm Details
1. Identify Important Variables: Artificially generated noise variables are used to determine
a threshold to test for statistically significant variable importance scores. The test is used
to remove irrelevant variables. Details are presented in the displayed algorithms and further
described as follows.
In each replicate r, r = 1, 2, . . . , R artificial variables are constructed as follows. For every real
variable X j j = 1, 2, . . . , M a corresponding artificial variable Z j is generated from a random
permutation. Then in each replicate a small RF is trained and variable importance scores
are computed for real and artificial variables. The scores from each replicate r are compiled
1349

T UV, B ORISOV, RUNGER AND T ORKKOLA

into the rth row of a matrix V R × 2M. Furthermore, the 1 − α percentile of the importance
scores in replicate r is calculated from only the artificial variables. This is denoted as vr and
the vector of percentiles over the R replicates is v R × 1. For each real variable X j a paired
t-test compares importance scores for X j (obtained from the jth column of V) to the vector of
scores v. A test that results in statistical significance identifies an important variable.
Significance is evaluated through a suitably small p-value. The use of a p-value requires a
feature to consistently score higher than the artificial variables over multiple replicates. Furthermore, this statistical testing framework also allows any method to control false selections
to be applied. We routinely use the Bonferroni adjustment, but a false discovery rate approach
is also reasonable. Each replicate uses a RF with L = 20-50 trees to score the importance of
the original and artificial noise variables. Also, the split weight calculation for variable importance in (2) only uses OOB samples as described previously.
2. Calculate Masking Scores: A masking matrix is computed from independent replicates in
order to evaluate the statistical significance of masking results. Suppose there are m important variables from step 1. For similar reasons as in the previous step, replicates and noise
variables are used to detect masking among the relevant variables. These are currently the
same replicates that are used for variable importance. A set of R independent GBT models
are generated each with L = 10-50 trees. Note that all variables are tested in each node in each
tree in a serial ensemble. Therefore, richer, more effective masking information is obtained
from a serial ensemble than from a random subspace method like RF. In these calculations,
the surrogate scores and the split weights are calculated from the OOB samples as in the
previous step. Let Mi,r j denote the masking score for variables Xi and X j from the ensemble
r denote the (1 − α)-percentile of the masking
in replicate r, for r = 1, 2, . . . , R. Also, let Mi,α
score in replicate r from the distribution of scores between variable Xi and the noise variables.
r denotes the (1 − α)-percentile of M r for j = m + 1, . . . , 2m. Similar to the check
That is, Mi,α
i, j
for variable importance, a paired t-test compares the masking score between variables (Xi , X j )
r computed from the noise variables. There is a significant masking
with masking score Mi,α
between variables (Xi , X j ) if the paired t-test is significant. Variable X j is masked by variable
Xi if the test is significant.
3. Eliminate Masked Variables: Masked variables are removed from the list of important variables as follows. Given a list of important variables upon entry to this step, the variables are
sorted by the importance score calculated in step 2. The most important variable is added to
an exit list, and dropped from the entry list. Assume this is variable Xi . All variables that are
masked by Xi are dropped from the entry list. This is repeated until the entry list is empty.
The exit list represents the unmasked important variables.
4. Generate Residuals for Incremental Adjustment: An iteration is used to enhance the ability of the algorithm to detect variables that are important, but possibly weaker than a primary
set. Given a current subset of important variables, only this subset is used to predict the
target. Residuals are calculated and form a new target. For a numerical target the residuals
are simply the actual minus the predicted values. For a classification problem residuals are
calculated from a multiclass logistic regression procedure (Friedman et al., 2000). We predict the log-odds of class probabilities for each class (typically GBT is used), and then take
1350

F EATURE S ELECTION WITH E NSEMBLES

pseudo residuals as summarized in the following multi-class logistic regression algorithm.
The algorithms are described using the notation in Table 1.
The iterations are similar to those used in forward selection. See, for example, Stoppiglia
et al. (2003). The Gram-Schmidt procedure first selects the variable with highest correlation
with the target. To remove the information from this variable the remaining predictors and the
target are othogonalized with respect to the selected variable. This provides residuals from the
fit of the target to the first selected variable. In the feature selection method here we do not require orthogonal predictors, but we adjust the target for the variables already selected through
residuals. We also can select more than a single variable in each iteration. The method also
uses a conservative selection criterion (Bonferroni adjustment) and the residuals allow a variable to enter on another iteration. There are similar procedures used elsewhere in regression
model building. Least angle regression (Efron et al., 2004) and projection pursuit methods
(Friedman et al., 1981) are well known examples that use residuals in forward-stagewise
modeling.
The algorithm returns to step 1 and continues until no variables with statistically significant
importance scores remain. The current subset of important variables is used for the prediction
model. Whenever step 1 is calculated, all variables are used to build the ensembles—not only the
currently important ones. This approach allows the algorithm to recover partially masked variables
that still contribute predictive power to the model. This can occur after the effect of a masking
variable is completely removed, and the partial masking is eliminated. The algorithms for numerical
(regression) and categorical (classification) targets are presented as algorithms 1 and 2. A separate
algorithm 3 describes the variable masking calculations.

Algorithm 1: Ensemble-Based Feature Selection, Regression
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

Set Φ ← {}; set F ← {X1 , . . . , XM }; set W = 0 (|W | = M).
for r = 1, . . . , R do
{Z1 , . . . , ZM } ← permute{X1 , . . . , XM }
set FP ← F ∪ {Z1 , . . . , ZM }
rth row of V = Vr. = gI (FP ,Y );
endfor
R × 1 vector (element wise) v = Percentile1−α (V[·, M + 1, . . . , 2M])
Set Φ̂ to those {X j } for which element wise V. j > v
with specified paired t-test significance (0.05)
Set Φ̂ = RemoveMasked(Φ̂,W + gI (FP ,Y ))
If Φ̂ is empty, then quit.
Φ ← Φ ∪ Φ̂;
Y = Y − gY (Φ̂,Y )
W (Φ̂) = W (Φ̂) + gI (Φ̂,Y )
Go to 2.

1351

T UV, B ORISOV, RUNGER AND T ORKKOLA

Algorithm 2: Ensemble-Based Feature Selection, Classification
1.
2.
3.
4.

5.
6.
7.
8.
9.
10.

11.
12.

set Φ ← {}; Gk (F) = 0,Wk = 0
for k = 1, . . . , K do
set V = 0.
for r = 1, . . . , R do
{Z1 , . . . , ZM } ← permute{X1 , . . . , XM }
set F ← X ∪ {Z1 , . . . , ZM }
Compute class proportion pk (x) = exp(Gk (x))/ ∑Kl=1 exp(Gl (x))
Compute pseudo-residuals Yik = I(Yi = k) − pk (xi )
Vr. = Vr. + gI (F,Y k );
endfor
Element wise v = Percentile1−α (V[·, M + 1, . . . , 2M])
Set Φ̂k to those {Xk } for which V.k > v
with specified paired t-test significance (0.05)
Set Φ̂k = RemoveMasked(Φ̂k ,Wk + gI (F,Y k ))
Φ ← Φ ∪ Φ̂k ;
for k = 1, ..., K do
Gk (F) = Gk (F) + gY (Φ̂k ,Y k )
Wk (Φ̂k ) = Wk (Φ̂k ) + gI (Φ̂k ,Y k )
endfor
endfor
If Φ̂k for all k = 1, . . . , K is empty, then quit.
Go to 2.

4.2 Comparison to Previous Work
Two earlier methods are closely related to ACE, FCBS (Yu and Liu, 2004) and MBBE (Koller
and Sahami, 1996). We compare our method in detail to these two methods. Because we use a
multivariate model (tree) instead of frequency tables, our method fits in the category of embedded
methods. This is unlike FCBS and MBBE that can be considered as correlation filters, although
Koller works with frequency tables of 2-5 variables.
FCBS first sorts features by correlation with the response using a symmetric uncertainty, optionally removing the bottom of the list by a user-specified threshold, then
1. The feature most correlated to the response is selected.
2. All features that have correlation with the selected feature higher than it’s correlation with
response are considered redundant and removed. The feature is added to the minimal subset
(and this is an approximate heuristic for Markov blanket filtering).
3. Return to 1).
FCBS is similar in structure to our method, with the following important differences.
1352

F EATURE S ELECTION WITH E NSEMBLES

Algorithm 3: RemoveMasked(F,W)
1
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

Let m = |F|.
for r = 1, . . . , R do
{Z1 , . . . , Zm } ← permute{X1 , . . . , Xm }
set FP ← F ∪ {Z1 , . . . , Zm }
Build GBT model Gr = GBT (FP ).
Calculate masking matrix M r = M(Gr ) (2m × 2m matrix).
endfor
r
Set Mi,α
= Percentile1−αm (M r [i, m + 1, . . . , 2m]), r = 1, . . . , R
m
∗
r , r = 1, . . . , R
Set Mi j = 1 for those i, j = 1 . . . m for which Mirj > Mi,α
m
with specified paired t-test significance (0.05), otherwise set Mi∗j = 0
Set L = F, L∗ = {}.
Move Xi ∈ L with i = argmaxi Wi to L∗ .
Remove all X j ∈ L from L, for which Mi∗j = 1.
Return to step 10 if L 6= {}.

1. We use tree importance instead of univariate correlation with the response. This makes ACE
much more robust and accurate.
2. We use a surrogate masking measure instead of correlation. This takes the response into
account, not only the correlations between inputs. No arbitrary thresholds for correlation are
used.
3. We compute residuals to find smaller effects reducing the chance to drop a non-redundant
feature.
Koller’s MBBE works as follows:
1. For each feature Xi , find the set Mi of K features (K = 1 − 4) that are most correlated to it.
(That is, which provide little information on the response when added to the selected feature in
frequency table models.) Additional information is measured as KL-distance (Kullback and
Liebler, 1951) D(P(y|Xi , X j ), P(y|Xi )). The set Mi is called the approximate Markov blanket
for feature Xi . The authors state that K = 1 − 2 gives the best results.
2. For each feature compute the relevance score δi = D(P(y|Mi , Xi ), P(y|Mi )). This represents
the additional information it brings when added to its approximate Markov blanket, and remove features that have the smallest relevance scores (i.e., most redundant).
3. Repeat (1,2) until all features are ranked in the order they are deleted. This method returns a
ranked list of features rather than one subset.
Our ACE algorithm works more like FCBS as it uses only one feature as an approximate MB for
each feature (as does the MBBE algorithm with K = 1). Furthermore, it filters features by relevance
before computing redundancy between the features, and reports a final minimum feature subset.
1353

T UV, B ORISOV, RUNGER AND T ORKKOLA

K
X
Y
M
R
α
αm
Z
W
Wk
F
Φ
V
Vr.
V. j
gI (F,Y )

gY (F,Y )
Gk (F)
GBT (F)
M(G)
Mk
M∗

Number of classes (if classification problem)
set of original variables
target variable
Number of variables
Number of replicates for t-test
quantile used for variable importance estimation
quantile used for variable masking estimation
permuted versions of X
cumulative variable importance vector.
cumulative variable importance vector for k-th class in classification.
current working set of variables
set of important variables
variable importance matrix (R × 2M)
rth row of variable importance matrix V, r = 1 . . . R
jth column of matrix V
function that trains an ensemble of L trees based on
variables F and target Y , and returns a row vector
of importance for each variable in F
function that trains an ensemble based on variables F
and target Y , and returns a prediction of Y
current predictions for log-odds of k-th class
GBT model built on variable set F
Masking measure matrix calculated from model G
Masking matrix for k-th GBT ensemble Gt .
Masking flags matrix
Table 1: Notation in Algorithms 1-3

However, the major difference is that our redundancy measure approximates KL-distance taking the
response into account and uses local information. Thus, it can deal with multivariate dependencies.
MBBE for K > 1 will incur three (or more) dimensional frequency tables that are hard to deal with
if number of categories is large.
The learner g(., .) in the ACE algorithms is an ensemble of trees. Any classifier/regressor function can be used, from which the variable importance from all variable interactions can be derived.
To our knowledge, only ensembles of trees can provide this conveniently.
The computational complexity of the algorithm is of the same order as the maximal complexity
of a RF on the whole feature set and a GBT model on the selected important feature subset. A
GBT model is usually more complex, because all surrogate splits at every tree node are computed.
However, a smaller tree depth setting for the GBT model reduces the calculations in this part of the
algorithm. The complexity is proportional to
(Fsel + Fimpvar) ∗ N ∗ logN ∗ Ntrees ∗ Nensembles ∗ Niter + Niter ∗ Fimpvar2 ,
1354

F EATURE S ELECTION WITH E NSEMBLES

where the variables are defined as follows: Niter is the number of iterations of the ACE algorithm
(for example, for the challenge discussed in Sec. 5.3 this was always less than 10 and usually
3-4); Nensembles is the number of replicates for t-tests (equal to 20 in the challenge); Ntrees is
the number of trees in the RF or ensemble (equal to 20-100 in the challenge); N is the number
of samples; Fsel is the number of selected variables per tree split in RF (equal to the square root
of the total number features or less); Fimpvar is the number of selected important variables (for
example, for the challenge data set NOVA discussed in Section 5.3 this was approximately 400-800
depending on parameters). The algorithm is very fast with approximately a minute for one feature
selection iteration on the challenge NOVA data set with 16K variables with 20 replicates with 70
trees on a Windows XP-based four-processor Xeon (2 x HT) 3.4GHz workstation.

5. Experiments
In order to evaluate the goodness of feature selection algorithms, two options have been used in
the literature. The first is not to evaluate the actual feature selection performance at all, but the
performance of a subsequent learner in some task. This facilitates the use of any data set in the
“evaluation” but does not give much useful information at all in characterizing the actual feature
selection. The second option is to directly evaluate the feature selection performance without using
a subsequent proxy task. The latter dictates the need to know the ground truth behind the data,
which typically means that the data must be artificially generated, either completely, or by adding
some redundant and/or irrelevant features to some known data.
As the topic of the paper at hand is a method for the subset feature selection, the first evaluation
method is affected by the choice of the classifier. The effects of feature selection are mixed in
with how well the learner is able to handle redundant or irrelevant features. The results would thus
depend on the choice of learners and on the choice of data sets. Therefore we will mainly describe
experiments with two types of simulated data with known ground truth.
Experiments on data with linear relationships are presented first. Then a nonlinear data generator
is used to study the sensitivity to multiple variable interactions with nonlinear relations. Further
results are from the 2007 International Joint Conference on Neural Networks (IJCNN), “Agnostic
learning vs. prior knowledge challenge & data representation discovery workshop”. The algorithm
described here had the second best performance in the agnostic track. Here we demonstrate the
effect of the subset to predictor performance as compared to the full set of features. Also, an actual
manufacturing data set as well as a comparison to a previous analysis of the well known Hepatitis
data are also presented in terms of predictive power of the resulting feature set.
5.1 Generated Data with Linear Relationships
The data in this experiment has an additive structure with one numeric response variable and 203
input variables. Inputs x1 , . . . , x100 are highly correlated with one another, and they are all reasonably
predictive of the response (regression R2 ∼ 0.5). But a, b, and c are independent variables that are
much weaker predictors (regression R2 ∼ 0.1). Further u1 , . . . , u100 are i.i.d. N(0, 1) variables that
are unrelated to the target. The target variable was generated as an additive model with additional
noise using y = x1 + a + b + c + ε, where ε ∼ N(0, 1). This structure is chosen because it is well
known that linear (oblique) relationships are not optimal for a tree representation. However, they
are ideal for correlation-based methods. Thus we have here the worst possible case for ACE and the
best possible case for CFS. The methods were evaluated on 50 data sets of size 400 samples.
1355

T UV, B ORISOV, RUNGER AND T ORKKOLA

Linear data
100
90
80
70
60
50
40
30
20
10
0

ace

cfs

cfs-gen

rfe4

relief4

Figure 1: Artificial data with linear relationships. Subset discovery methods (ACE, CFS, CFS-gen)
and methods finding a subset of predefined size four (RFE4, Relief4) are compared. The
results for each method consist of three bars. The first is the percentage of relevant variables detected (out of four), the second is the percentage of redundant variables detected
(out of 100), and the third is the percentage of irrelevant variables detected (out of 100).
The results are averages over 50 data sets.

Figure 1 depicts the performance of ACE against methods that also discover the subsets (CFS
with best-first search, CFS with genetic search), as well as against some subset ranking methods
(RFE, Relief).
RFE and Relief are ranking methods. In this experiment they were given the advantage of knowing the number of relevant features beforehand, that is, their task was to “find the best possible four
variable subset” (RFE4, Relief4), whereas ACE and CFS had to also find the number themselves.
A further advantage was given to RFE by matching the underlying support vector regressor to the
problem with a linear kernel (using the standard RBF kernel produced inferior results). This experiment demonstrates one aspect of the advantages of ACE. In a task ideal for correlation-based
methods but hard for trees, we show equal performance.
5.2 Generated Nonlinear Data
Next, experiments were conducted using a well-known data generator (Friedman, 1999), which
produces data sets with multiple non-linear interactions between input variables. The true model
can be designed with relevant, redundant, and noise inputs. We selected 10 relevant inputs plus
random, uniform (0, 1) noise. Also, 20 redundant inputs were used. Each was a random linear
combination of three inputs plus random, uniform noise. Finally, 40 noise inputs were added, so
that 70 features were available to the full model. The target function was generated as a weighted
sum of 10 multidimensional Gaussians, each Gaussian at a time involving about four input variables
randomly drawn from the relevant 10 variables. Thus all of the relevant 10 input variables are
involved in the target, to a varying degree. The Gaussian functions also have a random mean vector
and a random covariance matrix as described by Friedman (1999). Weights for the Gaussians were
randomly drawn from U[−1, 1].
The data generator produces continuous-valued variables. Thus the data sets can be used as such
for regression problems. Data sets of two different sizes were generated, 1000 and 4000 samples. In
1356

F EATURE S ELECTION WITH E NSEMBLES

order to generate classification problems, the target variable was discretized to two levels. Mixedtype data was generated by randomly discretizing half of the variables, each to a random number
of levels drawn from U[2, 32]. There are thus eight different experiments altogether. For each
experiment, 50 data sets were generated with different seeds. Figure 2 presents the results for each
case as average percentages of features selected in each group (relevant, redundant, or noise) over
the 50 generated data sets.
numeric, classification, N=1000

numeric, classification, N=4000

80

80

60

60

40

40

20

20

0

ace

cfs

cfs-gen

fcbs

0

rfe10 relief10

ace

numeric, regression, N=1000
80

80

60

60

40

40

20

20

0

ace

cfs

cfs-gen

fcbs

0

rfe10 relief10

ace

mixed, classification, N=1000
80

60

60

40

40

20

20
ace

cfs

cfs-gen

fcbs

0

rfe10 relief10

ace

mixed, regression, N=1000
80

60

60

40

40

20

20
ace

cfs

cfs-gen

fcbs

fcbs

rfe10 relief10

cfs

cfs-gen

fcbs

rfe10 relief10

cfs

cfs-gen

fcbs

rfe10 relief10

mixed, regression, N=4000

80

0

cfs-gen

mixed, classification, N=4000

80

0

cfs

numeric, regression, N=4000

0

rfe10 relief10

ace

cfs

cfs-gen

fcbs

rfe10 relief10

Figure 2: Artificial data with nonlinear relationships. Subset discovery methods (ACE, CFS, CFSgen, FCBS) and methods finding a subset of predefined size 10 (RFE10, Relief10) are
compared. FCBS works only in classification problems. The results for each method
consist of three bars. The first is the percentage of relevant variables detected (out of 10),
the second is the percentage of redundant variables detected (out of 20), and the third is
the percentage of irrelevant variables detected (out of 40). The results are averages over
50 data sets.
RFE and Relief were again given the advantage of knowing the number of relevant features
beforehand, that is, their task was to “find the best possible ten-variable subset”, whereas ACE,
CFS, and FCBS had to also find the number by themselves. A further advantage was given to RFE
by matching the underlying support vector classifier to the problem with an RBF kernel. Using a
linear kernel produced inferior results.
1357

T UV, B ORISOV, RUNGER AND T ORKKOLA

The notable failure of FCBS on this data can be explained as follows. Most numerical important
variables are dropped at the discretization step of FCBS, because MDL discretization works as a
filter method, and it cannot deal with the multivariate dependency from Friedmans’s generator. It
works well with discrete variables only when the number of categories is small and the response is
categorical with a small number of categories.
This experiment demonstrates another aspect of the universality of ACE. The only case where
another method (RFE10) showed a superior result was a classification problem with a smaller sample size and mixed type inputs. Again RFE10 was given the advantage of knowing the number of
relevant features and an appropriate kernel beforehand.
5.3 IJCNN 2007 Agnostic Learning vs. Prior Knowledge Challenge
In this experiment we show the effect of the selected subset within various classification tasks. The
ACE feature selection algorithm was applied to the data sets in the Agnostic Learning Challenge.
The number of training/validation/testing instances and the number of features are shown in the
following list:
• ADA, Marketing, 4147/415/41471, 48 features
• GINA, Handwriting recognition, 3153/315/31532, 970 features
• HIVA, Drug discovery, 3845/384/38449, 1617 features
• NOVA, Text, 1754/175/17537, 16969 features
• SYLVA, Ecology, 13086/1309/130857, 216 features
For feature selection with ACE, the number of trees, importance and masking quantiles were parameters that were optimized. Next GBT with embedded feature selection (to prevent overfitting)
(Borisov et al., 2006) was built on the subset. The following parameters of GBT were optimized:
number of trees, tree depth, shrinkage, number of selected features per tree node, and the importance adjustment rate for embedded feature selection, stratified sampling for 0/1 class proportions,
and priors. The optimization strategy (manual) was to set reasonable parameter values, and then
try to adjust each parameter sequentially, so that the test error decreased. The model was trained
on 60% of the training data during parameter optimization. Several passes over all the GBT parameters were used, and one for the feature selection parameters. Priors were selected using cross
validation. Feature selection and GBT were used on K partitions of the data and then optimal priors
were selected on the remaining part.
Table 2 shows the results before and after subset selection for the five challenge data sets. The
CV-error was either preserved or reduced through a good subset. The overall results were the second best in the agnostic learning challenge. Redundancy elimination was applied on ADA, HIVA,
SYLVA, and feature selection without redundancy elimination was used on NOVA and GINA.
5.4 TIED Data Set
A data set with multiple Markov boundaries was generated by Statnikov and Aliferis (2009). The
data was obtained from a discrete Bayesian network with 1000 variables and a target variable
with four classes. A training set was constructed with 750 instances simulated from the network.
1358

F EATURE S ELECTION WITH E NSEMBLES

Original

Features

Ada
Gina
Hiva
Nova
Sylva

47
970
1617
12993
212

CV-error from
all features
0.1909
0.0527
0.2847
0.0591
0.0133

Best
subset size
16
75
221
400
69

CV-error from
selected subset
0.1855
0.0506
0.2559
0.0518
0.0129

Table 2: IJCNN 2007 Agnostic Learning vs. Prior Knowledge Challenge results.
Variable
3
2
10
1
11
12
13
18
19
15
20
29
8
14
4
9

p-value
0
0
0
1.E-10
3.E-07
2.E-07
5.E-07
3.E-09
2.E-07
2.E-07
2.E-06
2.E-06
3.E-06
1.E-08
8.E-06
6.E-06

Importance Score
100.0%
98.4%
96.4%
96.4%
83.3%
83.3%
79.1%
67.5%
67.5%
41.4%
39.5%
29.8%
26.2%
11.6%
9.5%
8.3%

Table 3: Feature selection scores for the TIED data set. Variables in any Markov boundary are
recovered as significant with three false alarms.

The network contained 72 Markov boundaries. Each boundary contained five variables (one from
each of the following subsets):(1){X9 }, (2) {X4 , X8 }, (3){X11 , X12 , X13 }, (4) {X18 , X19 , X20 }, and (5)
{X1 , X2 , X3 , X10 }.
The ACE feature selection method described here was used to remove irrelevant features. After
three iterations of the residual calculations described previously the algorithm stopped with the important variables (and p-values from the artificial contrasts) shown in Table 3. The list of statistically
significant variables reproduces all the variables in any of the Markov boundaries listed above, with
false alarms from variables X14 , X15 , and X29 .
Although ACE recovered the variables in the Markov boundaries, there are limitations with
the masking methods for a multi-class target. The GBT ensembles model each class (versus the
rest) with a binary logistic function and averages variable masking scores over the binary models.
Consequently, some attenuation of the importance scores are expected. Redundancy elimination did
not effectively eliminate masking in the TIED data. However, we used the TIED network and TIED
1359

T UV, B ORISOV, RUNGER AND T ORKKOLA

data for binary problems with each class versus the rest. For example, for class 1 versus the rest
the TIED network generates the same collection of 72 Markov boundaries. The results from ACE
without redundancy elimination for the binary target are shown in Table 4. The list of statistically
significant variables reproduces all the variables in any of the Markov boundaries, with no false
alarms.
Variable
4
8
19
18
20
9
13
12
11
10
2
3
1
6

Importance Score
100.0%
100.0%
88.1%
88.1%
88.1%
64.8%
39.5%
39.5%
39.5%
21.9%
21.9%
21.9%
21.9%
0.0%

Table 4: Variable importance for TIED data modified for a binary target (class 1 versus the rest).
All variables in the true Markov boundaries are identified with no false alarms.

As the importance scores are arranged in decreasing order in Table 4, groups of variables with
similar scores become noticeable and these groups correspond to the subsets (equivalence classes)
in the cross-product that defines the Markov boundaries. That is, the most important variables in
Table 4 are those in the subset {X4 , X8 } in the Markov boundaries and the last group matches the
subset {X1 , X2 , X3 , X10 }. The equivalent groups are clear from their importance scores in this case.
The analysis with redundancy elimination generated the list of significantly significant variables
in Table 5. One equivalent variable from the subset {X1 , X2 , X3 , X10 } was missed in the recovery of
a Markov boundary. The contribution from this subset was, however, small. The predictive performance of a tree ensemble on the recovered variables is nearly identical to a model on a true Markov
boundary. In addition, the three variables {X18 , X20 , X4 } are identified in Table 5 as important, but
they are redundant in the true network. Although these comprise false alarms, the magnitudes of
the importance scores indicate that the last three variables are much less important than the others.
Similar results (not shown here) were obtained for the binary target class 2 (versus the rest). Results
without any errors were obtained for classes 0 and 3 (each versus the rest). Specifically, for class 0
the Markov boundaries from the TIED network consist of one element from {X1 , X2 , X3 , X10 }. In this
case the ACE analysis without redundancy elimination recovered these four variables without false
alarms. The analysis with redundancy elimination correctly recovered a single variable from this
set. Similarly for class 3, without redundancy elimination all variables in the Markov boundaries
{X12 , X13 , X14 } were recovered, and only one variable from this set was recovered with redundancy
elimination.
1360

F EATURE S ELECTION WITH E NSEMBLES

Variable
8
9
19
1
18
20
4

Importance Score
100.0%
61.3%
43.8%
10.2%
2.6%
0.9%
0.3%

Table 5: Variable importance for TIED data modified for a binary target (class 1 versus the rest)
with redundancy elimination.

5.5 Manufacturing Data
In multiple real world applications collecting unnecessary variables is a cost issue and finding a
suitable subset is critical in terms of cost-efficiency. As an example we present manufacturing data
from a process that contained approximately 10K rows and consisted of 35 predictors that were all
numerical, continuous measurements. The target was a binary response and approximately 20%
of the data belonged in the rare class. Because the data is actual manufacturing data, the specific
variable names are not provided. The data was analyzed extensively with traditional regression
methods (the response was coded as 0 and 1) and models obtained were complex and not accurate.
A list of the results from our algorithm is shown in Table 6. It is not unusual for manufacturing
data to consist of related predictors. Without redundancy elimination, 20 variables were identified
as related to the target. However, after masking scores were used to remove redundant predictors
the final subset model consisted of only five predictors.
The predictive accuracy for the binary target was nearly identical using a GBT model with these
5 predictors to the full set of 35 predictors. Table 6 also compares other subset selection algorithms
to ACE in terms of their predictive accuracy and the size of the selected feature set.
A previous analysis of this data by Berrado and Runger (2007) used association rules applied
after the predictors were discretized with simple equal-frequency discretization. Only rules with
consequent equal to the rare target class were considered. A total of 25 rules were detected that met
the minimum support threshold. These rules contained 14 variables and 13 out of 14 are listed in
the Table 6. Although the objectives of the association analysis were different, the relatively high
proportion of important variables is consistent with the results in Table 6.
5.6 Hepatitis Data
The hepatitis data available from the UC-Irvine repository has been widely analyzed. There are 155
patients and 19 predictors and the response is a binary survival result. Breiman (2001) considered
this data and cited a previous analysis from the Stanford Medical School and another analysis by
Diaconis and Efron (1983). The analysis from the medical school concluded that the important
variables were 6, 12, 14, 19. But Breiman (2001) concluded after a set of analyses that number 12
or 17 provided predictive power nearly equivalent to the full set of variables, and that these masked
each other. A notable difficulty is the small sample size in this example.
1361

T UV, B ORISOV, RUNGER AND T ORKKOLA

Variables
V11
V4
V5
V12
V14
V10
V2
V13
V8
V1
V9
V3
V19
V7
V20
V26
V27
Errors

ACE without
redundancy elim.
100.0%
96.1%
49.8%
48.6%
46.6%
43.5%
43.3%
38.7%
30.3%
27.9%
23.7%
23.6%
21.8%
21.5%
20.4%

ACE with
redundancy elim.
72.4%
100.0%
49.4%

36.4%
21.6%

CFS

CFS-gen

1
1
1
1
1
1
1

1
1
1
1
1
1
1

FCBS

1

1
1
1

0.145

0.144

1
1
0.145

0.190

Table 6: Manufacturing data with a binary target with redundancy elimination excludes many variables. Only a smaller subset of the relevant predictors remain. We compare the extracted
variables to other subset selection algorithms (selected variables are marked as ’1’ in the
table). The error rate for the full set of variables was 0.146.

We confirmed the strong masking between variables 12 and 17 (and vice versa) from our masking matrix. We also obtained a subset model that consists of variables 6, 17, 14, 19, and 11, similar
to medical school. Variable 11 was also identified in unpublished lecture notes by Breiman. The
subset selected by our algorithm has the lowest cross-validation error using logistic regression.

6. Conclusions
We have presented an efficient method for feature subset selection that builds upon the known
strengths of the tree ensembles and is designed explicitly to discover a non-redundant, effective
subset of features in large, dirty, and complex data sets.
Our method attempts to eliminate irrelevant variables using statistical comparisons with artificial
contrasts to obtain a threshold for importance estimated from the parallel ensembles of trees capable
of scoring very large number of variables.
It uses serial ensembles to discover significant masking effects for redundancy elimination. Furthermore we have showed that the redundancy elimination based on feature masking approximates
the Markov blanket redundancy filtering. It also uses an iterative strategy to allow for weaker predictors to be identified after stronger contributors.
1362

F EATURE S ELECTION WITH E NSEMBLES

Variables
malaise-6
albumin-17
bilirubin-14
histology-19
spiders-11
age-1
sex-2
ascites-12
varices-13
Errors

ACE
1
1
1
1
1

0.142

CFS
1

CFS-gen
1

1
1
1
1
1
1
1
0.155

1
1
1
1
1
1
1
0.155

FCBS

1
1
1
0.194

Table 7: Hepatitis data. Features selected from ACE compared to other subset selection algorithms
(selected variables are marked as ’1’ in the table). The baseline error rate for the full set
of variables was 0.148.

The superior performance of the algorithm is illustrated with a number of experiments on both
artificial and real data as well as by its success in the agnostic learning challenge.

Acknowledgments
This material is partly based upon work supported by the National Science Foundation under Grant
No. 0743160.

References
H. Almuallin and T. G. Dietterich. Learning boolean concepts in the presence of many irrelevant
features. Artificial Intelligence, 69(1-2):279–305, 1994.
Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation, 9(7):1545–88, 1997.
E. Bauer and R. Kohavi. An empirical comparison of voting classification algorithms: Bagging,
boosting and variants. Machine Learning, 36(1/2):525–536, 1999.
A. Berrado and G.C. Runger. Using metarules to organize and group discovered association rules.
Data Mining and Knowledge Discovery, 14(3):409–431, 2007.
A. Borisov, V. Eruhimov, and E. Tuv. Tree-based ensembles with dynamic soft feature selection.
In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature Extraction Foundations and
Applications: Studies in Fuzziness and Soft Computing. Springer, 2006.
B. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, 5th Annual ACM Workshop on COLT, Pittsburgh, PA, pages 144–152. ACM Press,
1992.
1363

T UV, B ORISOV, RUNGER AND T ORKKOLA

O. Bousquet and A. Elisseeff. Algorithmic stability and generalization performance. In Advances
in Neural Information Processing Systems, volume 13, pages 196–202. MIT Press, 2001.
L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
L. Breiman. Arcing classifiers. The Annals of Statistics, 26(3):801–849, 1998.
L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth,
Belmont, MA, 1984.
S. Cost and S. Salzberg. A weighted nearest neighbor algorithm for learning with symbolic features.
Machine Learning, 10(1):57–78, 1993.
P. Diaconis and B. Efron. Computer intensive methods in statistics. Scientific American, (248):
116–131, 1983.
T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization. Machine Learning, 40(2):139–157, 2000a.
T. G. Dietterich. Ensemble methods in machine learning. In First International Workshop on
Multiple Classifier Systems 2000, Cagliari, Italy, volume 1857 of Lecture Notes in Computer
Science, pages 1–15. Springer, 2000b.
B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics,
32:407–499, 2004.
Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In The 13th International
Conference on Machine Learning, pages 148–156. Morgan Kaufman, 1996.
J. Friedman. Greedy function approximation: a gradient boosting machine. Technical report, Dept.
of Statistics, Stanford University, 1999.
J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: A statistical view of boosting.
Annals of Statistics, 28:832–844, 2000.
J. H Friedman, M. Jacobson, and W. Stuetzle. Projection pursuit regression. Journal of the American
Statistical Association, 76:817–823, 1981.
I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine
Learning Research, 3:1157–1182, Mar 2003.
I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classification using
support vector machines. Machine Learning, 46(1-3):389–422, 2002.
M. A. Hall. Correlation-based feature selection for discrete and numeric class machine learning. In
Proceedings of the 17th International Conference on Machine Learning, pages 359–366, 2000.
L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 12(10):993–1001, 1990.
1364

F EATURE S ELECTION WITH E NSEMBLES

T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 20(8):832–844, 1998.
K. Kira and L. A. Rendell. A practical approach to feature selection. In ML92: Proceedings of the
ninth international workshop on Machine learning, pages 249–256, San Francisco, CA, USA,
1992. Morgan Kaufmann Publishers Inc. ISBN 1-5586-247-X.
D. Koller and M. Sahami. Toward optimal feature selection. In Proceedings of ICML-96,
13th International Conference on Machine Learning, pages 284–292, Bari, Italy, 1996. URL
citeseer.nj.nec.com/koller96toward.html.
S. Kullback and R.A. Liebler. On information and sufficiency. Annals of Mathematical Statistics,
22:76–86, 1951.
H. Liu and L. Yu. Toward integrating feature selection algorithms for classification and clustering.
IEEE Trans. Knowledge and Data Eng., 17(4):491–502, 2005.
S. Mukherjee, P. Niyogi, T. Poggio, and R. Rifkin. Learning theory: Stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Advances
in Computational Mathematics, 25:161–193, 2006.
B. Parmanto, P. Munro, and H. Doyle. Improving committee diagnosis with resampling techniques.
In D. S. Touretzky, M. C. Mozer, and M. Hesselmo, editors, Advances in Neural Information
Processing Systems 8, pages 882–888. Cambridge, MA: MIT Press, 1996.
T. Poggio, R. Rifkin, S. Mukherjee, and A. Rakhlin. Bagging regularizes. In CBCL Paper 214/AI
Memo 2002-003. MIT, Cambridge, MA, 2002.
T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi. General conditions for predictivity in learning
theory. Nature, 428:419–422, 2004.
M. Robnik-Sikonja and I. Kononenko. Theoretical and empirical analysis of relief and relieff.
Machine Learning, 53:23–69, 2003.
C. Stanfill and D. Waltz. Toward memory-based reasoning. Communications of the ACM, 29:
1213–1228, December 1986.
A. Statnikov and C.F. Aliferis. Tied: An artificially simulated dataset with multiple Markov boundaries. Journal of Machine Learning Research Workshop Conference & Proceedings, 2009. to
appear.
H. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. Ranking a random feature for variable and
feature selection. Journal of Machine Learning Research, 3:1399–1414, March 2003.
E. Tuv. Ensemble learning and feature selection. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh,
editors, Feature Extraction, Foundations and Applications. Springer, 2006.
E. Tuv, A. Borisov, and K. Torkkola. Feature selection using ensemble based ranking against artificial contrasts. In Proceedings of the International Joint Conference on Neural Networks (IJCNN),
2006.
1365

T UV, B ORISOV, RUNGER AND T ORKKOLA

G. Valentini and T. Dietterich. Low bias bagged support vector machines. In ICML 2003, pages
752–759, 2003.
G. Valentini and F. Masulli. Ensembles of learning machines. In M. Marinaro and R. Tagliaferri,
editors, Neural Nets WIRN Vietri-02, Lecture Notes in Computer Science. Springer-Verlag, 2002.
J.W. Wisnowski, J.R. Simpson, D.C. Montgomery, and G.C. Runger. Resampling methods for
variable selection in robust regression. Computational Statistics and Data Analysis, 43(3):341–
355, 2003.
L. Yu and H. Liu. Efficient feature selection via analysis of relevance and redundancy. J. of Machine
Learning Research, 5:1205–1224, 2004.

1366

Supervised Multivariate Discretization in Mixed Data
with Random Forests
Abdelaziz Berrado

Georger C. Runger

Industrial Engineering
EMI
Rabat, Morocco
a.berrado@emi.ac.ma

Industrial Engineering
ASU
Tempe, Arizona, USA
runger@asu.edu

Abstract—Discretizing continuous attributes is necessary before
association rules mining or using several inductive learning
algorithms with a heterogeneous data space. This data
preprocessing step should be carried out with a minimum
information loss; that is the mutual information between
attributes on the one hand and between attributes and the class
labels on the other should not be destroyed. This paper
introduces a novel supervised, global and dynamic discretization
algorithm, called RFDisc (Random Forests Discretizer). It
derives its ability in conserving the data properties from the
Random Forests learning algorithm. RFDisc is simple, relatively
fast and learns automatically the number of bins into which each
continuous attribute is to be discretized. Empirical results
indicate that the accuracies of classification algorithms such as
CART when used with several data sets are comparable before
and after discretization using RFDisc. Furthermore, C5.0
achieves the highest classification accuracy with data discretized
with RFDisc when compared with other well known
discretization algorithms.

I.

INTRODUCTION

Discretization transforms continuous valued attributes into
a finite number of bins or intervals. Each continuous value is
then mapped to the interval in which it falls. The need to
discretize numeric attributes is not new to the machine
learning community especially that several classification
algorithms apply exclusively to a nominal feature space.
Moreover, using a nominal feature space versus a numerical
one enhances the speed of induction algorithms [15, 14] and
improves their interpretability.
Furthermore, the discretization of a heterogeneous or
mixed mode dataset will enable association rules mining [1].
Significant efforts have been made to develop
discretization methods. Doughterty et al. [12] classified them
according to three major axes: global versus local, supervised
versus unsupervised, and static versus dynamic.
The global methods, such as the equal width or equal
frequency discretization [10], partition the entire instance
space by splitting each individual continuous attribute into
intervals independently of the other attributes. In contrast,

978-1-4244-3806-8/09/$25.00 © 2009 IEEE

local methods partition the multidimensional instance space
into localized regions and then apply partitions to the resulting
localized regions; in other words, this approach consists of
searching for threshold values for each individual attribute
during the induction of rules.
The supervised discretization methods utilize the class
labels in the discretization process while the unsupervised
methods do not. This could lose classification information in
situations where values that are strongly associated with
different classes belong to the same interval after
discretization.
Finally, discretization methods could be either static or
dynamic. The static methods are greedy because they perform
one discretization pass over the data representing each
attribute, potentially missing any interdependency between
attributes. In contrast, the dynamic methods search
simultaneously for partitions for all features. Using dynamic
discretization methods becomes necessary as the number of
attributes increases because interdependency between
attributes becomes more likely.
We introduce a new supervised, global and dynamic
discretization algorithm that can be universally applied to
continuous and mixed data spaces (which are not rarities in
research areas such as manufacturing). We use the trees grown
by a Random Forests [5] algorithm to decide how to discretize
the continuous attributes. The rest of this paper is organized as
follows. In the next section, we review, discuss and classify
several existing discretization algorithms. In Section 3, we
give a brief introduction about CART and the Random Forests
algorithm and then reveal details about the random forest
discretization algorithm. The empirical evaluation of the
Random Forest Discretizer follows in Section 4, while Section
5 is a conclusion.
II.

RELATED WORK

Equal-width
discretization
and
equal-frequency
discretization [10] are two early unsupervised discretization
algorithms. Both methods involve sorting the values of each
attribute in ascending order and dividing their respective

211

ranges into k bins, where k is a parameter defined by the user.
The equal-width method computes the bin width by dividing
the data range into the pre-specified number of equal width
bins, k, and then constructs bin boundaries or thresholds
starting from the minimum value of the attribute being
discretized. The equal frequency divides the range into k bins
each containing the same number of sorted values. These
methods are perhaps the simplest and least appropriate
methods to discretize data because they don't take into
consideration the interdependencies between the attributes and
ignore the relationship between attributes and class labels.
Discretizing a continuous attribute using any of these
unsupervised univariate methods, could lead to grouping
instances with different class labels into the same bins. Most
of the discretization methods that followed are static
(univariate) supervised methods.
The class information entropy is commonly used by
several univariate discretization methods [13, 7, 9, 30]. Wong
and Chiu [30] developed a discretization method based on the
concept of maximum marginal entropy, where the goal was to
reduce the amount of information loss after the discretization
process.
Chiu et al. [9] proposed a hierarchical method based on
maximizing the Shanon entropy over the discretized space.
Catlett's D-2 discretizer [7] followed, where an entropy
measure is used recursively to find a potential discretization
for each continuous attribute. This method requires a stopping
criterion to be predefined. Fayyad et al. [13] introduced their
Recursive Minimal Entropy Partitioning algorithm which uses
a minimum description length criterion to select useful cutpoints. This class of methods which use entropy could fail in
finding the proper boundaries of the continuous attributes.
Consider the Exclusive-Or (XOR) problem, making any
partition of one of the continuous attributes will result in two
subsets having roughly the same class distribution as each
other and as the whole data set, this means that we cannot find
boundaries that reduce the class information entropy and this
will disable this class of algorithms from proceeding.
Holte's 1R Discretizer [17] also referred to as One-Rule
Discretizer is a global, supervised discretization method. It
sorts the values of each continuous attribute and attempts to
divide the domain greedily into bins that each contains only
instances of one particular class. This method requires
specifying some minimum bin size because otherwise the
algorithm could possibly form one bin for each individual
instance.
Patterson-Niblett [24] is a supervised method which is
built into a decision tree algorithm [25].
Two supervised univariate discretization algorithms, the
class attribute dependence discretization (CADD) [8] and the
class-attribute interdependence maximization (CAIM) [19],
consider the class-attribute interdependency information as the
criterion for the optimal discretization. Unlike the CAIM
algorithm, the CADD algorithm requires the user to specify
the number of intervals and the maximum entropy
discretization method to initialize the intervals.

Several univariate supervised discretization algorithms are
based on statistics. The ChiMerge system, is a statistical
supervised heuristic method introduced first by Kerber [18]. It
consists of an initialization step where each real value is
placed into its own interval and a bottom up merging process
follows, where every two adjacent intervals that are not judged
statistically dependent are merged until a termination
condition is met. Merging two intervals means that the class
attribute is independent of the intervals. A χ2 test is used to
determine whether adjacent intervals should be merged. The
termination condition, χ2 threshold, is determined at a
significance level α, which is set manually in Kerber's initial
work; it also requires specifying the number of degrees of
freedom, which is one less the number of classes.
Nevertheless, too big or too small a significance level α will
over or under-discretize an attribute; this implies that it is too
difficult to find a proper α for the ChiMerge.
The Chi2 discretization algorithm authored by Liu and
Setiono [22] is a modification to the ChiMerge method. It
automated the discretization process by introducing an
inconsistency rate as a stopping value and it automatically
selects the significance value α from the data.
Francis & Lixian Shen [27] modified the merging criterion
and the stopping criterion of the Chi2 algorithm and termed
their algorithm; the modified Chi2 algorithm. Their
modifications made the discretization process completely
automated in the sense that the algorithm computes the
inconsistency checking criteria.
Khiops [4] is also a discretization algorithm based on the
chi-square statistic, it starts from the elementary single value
intervals and evaluates all merges between adjacent intervals
and selects the best one according to the Chi-square criterion
applied to the whole set of intervals. Unlike ChiMerge where
only two neighboring intervals are evaluated, the Khiops
method optimizes a global criterion that evaluates the partition
of the domain into intervals.
StatDisc is another bottom-up algorithm [26] which
produces a hierarchy of intervals. It is similar to ChiMerge in
the fact that it uses statistical testing; however, it is more
general in the fact that it tries to merge k adjacent intervals
(where k is specified by the user) instead of considering them
pairwise. After sorting the values of each attribute, the initial
intervals are assigned to leaf nodes of the interval hierarchy.
Then StatDisc iteratively computes the
value, a statistical
measure of association, for every group of k adjacent intervals
and merges the k adjacent intervals with the lowest value.
When intervals are merged, a new internal node (parent of the
merged intervals) is added to the interval hierarchy. The
merging process continues until all groups of k adjacent
intervals have a value greater than a given threshold. When
the merging process is over, the final hierarchy of intervals is
explored in order to select a suitable final discretization.
Recent discretization algorithms adopted a supervised,
dynamic (multivariate) approach. They were, however,
restricted to a continuous feature space. Kweldo and
Kretowski [20] proposed a new system called EDRL-MD, for
Evolutionary Decision Rule Learner with Multivariate
Discretization. They used an evolutionary algorithm as a

212

root node. At a given node, all possible binary splits
among all input variables are considered and the best split
that leads to the purest children nodes is chosen.

search heuristic. Gama et al. [16] developed a method that
looks at all possible discretizations of the continuous feature
space as a set of hierarchies.
Muhlenbach and Rakotomalala [23] adopted a
neighborhood graph approach to isolate the groups of points
bearing the same label and then discretized the predictive
attributes by projecting the groups' extreme values on all axes
of the representations space. Zembe and Rams [32] suggested
a multivariate feature discretization method based on Genetic
Algorithms. Wang and Liu [29], considered a globally greedy
heuristic for the concurrent discretization of multiple
attributes.
Bay [2] suggested a multivariate discretization method. It
uses a bottom up merging algorithm to discretize continuous
variables but does not include the class labels in the
discretization process.
Yang and Webb [31] conducted a thorough comparison of
several of the methods cited earlier when employed for naiveBayes classifiers. Liu et al. [21] provided a description of
existing discretization methods and suggested some guidelines
on how to choose a discretization method under various
settings.
Using information about the interdependency between the
attributes on the one hand and their relationships with the class
variable on the other are both necessary to achieve a good
global discretization of a multidimensional heterogeneous data
space; good in the sense that the discretized data captures as
much as possible of the information available in the original
data space.
We suggest a new approach based on Random Forests [5]
to achieve a supervised, global and dynamic discretization of
mixed-type data. Unlike most existing discretization methods,
our approach is directly applicable to mixed data.
Furthermore, the number of bins that should be created for
each continuous attribute does not need to be specified a
priori. Also, the continuous attributes are not necessarily split
into the same number of bins. Finally, unlike most existing
discretization algorithms no initialization and no stopping
criterion are necessary for the suggested approach.
III.

RANDOM FOREST DISCRETIZER

The Random Forest Discretizer uses the trees grown by a
Random Forests algorithm to learn how to discretize
continuous attributes. We give a brief tutorial about CART
and Random Forests as that will enhance the understanding of
the Random Forests Discretizer algorithm.
A. Tree Structured Classifiers and Random Forests
Tree structured classifiers, such as CART [6], are
constructed by repeated binary splits of subsets of the feature
space into descendant subsets, each split results in two nodes
representing disjoint regions of the feature space. Simple
models are fit in the regions represented by the final nodes of
the tree usually referred to by leaves. The CART algorithm
consists of the following three steps:
•

•

Tree Pruning; after growing the largest tree, a sequence of
nested trees is created by progressively chopping off the
weakest nodes.

•

Optimal Tree Selection; the optimal tree is then selected
using cross-validation or an independent test set.

CART is a relatively fast algorithm where the variable
selection is automatic and interactions are naturally
considered. Furthermore, it copes well with missing values
and is robust to outliers. Although, simple and easy to
understand models result from CART, they tend to evolve
around the strongest effects which affects the quality of the
predictive accuracy on some classes of problems.
Random Forest, an approach developed by Breiman [5]
uses an ensemble of trees to significantly improve the
predictive accuracy of a single predictive tree. Random
Forests for classification is a classifier consisting of a
collection of single trees grown each from a bootstrap sample
of the same training data set. The overall prediction accuracy
is determined by voting. Each tree in the forest is grown as
follows:
For each tree, a bootstrap sample of the same size as the
training data is sampled from the original data. This sample
will be the training set for growing the tree.
If there are m input variables, only r variables are selected
at random and tried at each node. r is usually noticeably
smaller than m, and is usually taken to be the square root of m.
A small r value guaranties a small correlation between the
trees in the Random Forest. That is, the trees a very different
from each other.
There is no pruning; each tree is grown to the largest
extent possible. This guaranties a correct classification for the
training data by each tree.
To classify a new data instance, it is put down each of the
trees in the forest and the class that most trees agree on is
assigned to the new data instance.
The Random Forest classifier is usually more accurate than
Tree Structured Classifiers because the latter can lead to a
local solution since the way in which because a single tree is
built in a greedy fashion especially that the splits obtained
from the children nodes are highly dependent on how their
parent node was split and this makes the tree evolve around
strong effect. This constitutes a major weakness that prevents
CART from building an accurate classifier in some classes of
problems. To contrast the strength of a Random Forest
classifier vis-a-vis a Tree Structured Classifier, we use both
algorithms to solve the traditional XOR problem. We found an
out-of-bag error for the Random Forest classifier with 50 trees
to be 0.15% which is considerably smaller than the 10-fold
cross-validation error of CART 15.15%.

Tree Growing; during this step, the largest tree is grown
by making recursive splits at each node, starting from the

213

B. Random Forest Discretization Algorithm(RFDisc)
Suppose, we have a training data set consisting of n instances,
where each instance is associated with one of k classes. Each
instance is described by p attributes; m continuous and the
rest categorical. Let A = {a1, a2,…,am} refer to the set of
continuous attributes and X = {x1, x2,…, xn} represent the set of
data instances. The observed value of the jth attribute for the
ith instance will be referred with xi,j and the class assignment
of the ith instance will be denoted with gi.
A discretization of the jth continuous attribute will
divide its range into pj + 1 intervals, where pj is a positive
integer. Let Sj = {sj,1, sj,2,…, sj,pj} represent the sorted values of
the points used to discretize the jth continuous attribute. The
set of intervals resulting from the discretization of the jth
attribute will be referred to by the set Dj = {[sj,0, sj,1), [sj,1,
sj,2),…,[sj,p, sj,pj+1]}. where sj,0 = mini{xi,j} and sj,pj+1= maxi{xi,j}
for 1≤ i ≤ n.
We propose a supervised, dynamic and global
discretization of the m continuous attributes. It is called
RFDisc (Random Forests Discretizer). The choice of the pj
split points for each continuous attribute aj is guided by a
Random Forest classifier learned from the training data set.
The RFDisc algorithm proceeds as described below:
1. Learn a Random Forest classifier, F, from the training
data. Let T = {t1,…, tc} represent the set of the c trees that
constitute the forest. Each tree will be defined by a set of
nodes. Every parent node indicates an orthogonal split
performed on one of the p attributes. Let Splits(ti, j)
represent the set of split points performed on the jth
continuous attribute in the tree ti.
2. Compile all the splits on each of the m continuous
attributes performed by each tree in the forest. Let
Splits(F; j) denote all the splits made by all the trees in
the forest on the jth attribute. This procedure is described
by the following loop:
Given F:
For j = 1 to m
For i = 1 to c
Find Splits(ti; j)
Next i
Splits(F; j) = Ui{Splits(ti, j)}, where i = {1,
2,…,c}
Next j
3. For each attribute j, plot a histogram of Splits(F; j) and
use it to determine split points with high density in the
forest, that is values of the jth attribute that were used as
split points by a majority or many of the trees from F. In
other words, we are casting votes from the trees about
the points where the splits should take place. Those
popular split points are used to discretize the attribute j
which we referred to earlier by Sj = {sj,1, sj,2,…, sj,pj}.
At this point we pick split points corresponding to
the modes of the constructed histogram. A threshold could be
defined instead and used to select high density split points.
We use the following example in Fig. 1 to illustrate how we
used the histograms to determine the split points for the

attribute “Petal Width" from the Iris plants data set [3]. Given
the histogram in Figure 1 we pick two split points at 0.75 and
1.65 which will discretize this variable into three bins.
.

Figure 1. Histogram of the splits points generated by the Random Forests
Classifier on the attribute Petal Width from the Iris data set

C. Properties of the Random Forests Discretizer
The Random Forest Discretizer is a fast supervised, global
and dynamic algorithm to discretize continuous attributes in
mixed mode data sets. Supervised, because a supervised
learning algorithm was employed to determine how to
discretize each continuous attribute. Each split point learned
during training guaranties splitting the parent nodes into purer
children nodes, that is a better separation of the classes in the
children nodes. It is Global, because it uses the majority votes
from the trees to decide whether or not a split point in the
random forest should be used to discretize a continuous
attribute. Furthermore, the bootstrap samples and randomness
associated with the choice of the attribute to split at each node
makes the trees that populate the random forest low in
correlation and not necessary evolving only around the strong
effects. The dynamic quality of the Random Forest Discretizer
stems from the way in which the individual trees are grown;
they automatically take into consideration the interdependency
between the attributes. Note that the random forest is learned
from all the training data involving all the continuous and
categorical attributes, so different levels of interaction
between attributes are evaluated during the training. The
random forest algorithm learns automatically which split
points should be used to discretize each continuous attribute.
Also, the number of intervals that will be used to discretize the
continuous attributes does not need to be specified or known a
priori. The algorithm does not require the specification of any
other parameters and no assumptions are made regarding the
structure of the data. Finally, we point out that training a
Random Forest is not a time consuming process and depends
solely on the size of the data set, which makes the RFDisc a
rather quick algorithm in terms of execution time

214

TABLE I.

MAJOR PROPERTIES OF THE DATASETS CONSIDERED IN THE EXPERIMENTATION

Datasets

Properties

Number of
classes
Number of
examples
Number of
attributes
Number of
continuous
attributes

IV.

Iris

Sat

Thy

Wav

ion

Smo

Hea

Pid

Proc

3

6

3

3

2

3

2

2

2

150

6435

7200

3600

351

2855

270

768

9424

4

36

21

21

34

13

13

8

35

4

36

6

21

32

2

6

8

35

EMPIRICAL EVALUATION OF THE RANDOM FOREST
DISCRETIZER

In this Section we compare the Random Forest Discretizer
with seven other discretization algorithms on eight wellknown continuous and mixed-mode data based on the
accuracies achieved by two learning algorithms on the
discretized data. The same discretization algorithms and
datasets were used for empirical evaluation in [19]. We also
illustrate the performance of the suggested algorithm when
applied to an actual dataset provided by a major manufacturer.
A. The experimental Setup
The nine data sets used to test the random forest
algorithm are:
• Iris Plants data set (iris),

discretizer

•

Johns Hopkins University Ionosphere dataset (ion),

•

Statlog Project Heart Disease data set (hea),

•

Pima Indians Diabetes data set (pid),

•

Statlog Project Satellite Image data set (sat),

•

Thyroid Disease data set (thy),

•

Waveform data set (wav),

•

Attitudes Towards Workplace Smoking Restrictions
data set (smo), and

•

Process data, provided by a major manufacturer (pro).

TABLE II.

The first seven data sets were obtained from the UC Irvine
ML repository [3]; the eighth data set was obtained from the
StatLib data set archive [28] and the last data set was
obtained from a major manufacturer. A description of the
data sets is shown in Table I.
Tests were performed for the Random Forest discretizer
and seven other discretization algorithms. The seven
discretization algorithms were a mixture of two unsupervised
algorithms including equal-width and equal frequency [10]
and five supervised algorithms including Patterson-Niblett
[24], IEM [13], Maximum Entropy [30], CADD [8] and
CAIM [19].
The discretization algorithms were compared through the
comparison of their accuracies by the C5.0 algorithm [11] for
the data sets. A 10-fold cross validation was used to evaluate
C5.0. A similar analysis was conducted in [19] using the same
discretization algorithms and datasets. Some results are
reproduced from [19] into Table IV for comparison.
We have also compared the classification error of CART
[6] with the original data and then with the discretized data.
10-fold cross validation was used to evaluate the accuracy of
CART.
B. Empirical Results
The number of intervals resulting from the discretization
of the nine data sets using the random forest discretizer are
summarized in Table II. For each data set, we included the
minimum, maximum and average number of intervals across
all the continuous variables.

MINIMUM, MAXIMUM AND AVERAGE NUMBER OF INTERVALS RESULTING FROM DISCRETIZATION USING THE RANDOM FOREST DISCRETIZER

Datasets
Number of
intervals
Minimum
Maximum
Average

Iris

Sat

Thy

Wav

ion

Smo

Heq

Pid

Proc

3
5
4

2
5
3.5

2
4
2.7

2
3
2.1

2
4
2.7

2
3
2.5

3
5
4.2

3
4
3.5

2
6
3.3

215

TABLE III.

CLASSIFICATION ERROR ( %) RESULTING FROM CART FOR NINE DATA SETS BEFORE DISCRETIZATION AND AFTER DISCRETIZATION USING THE
RANDOM FOREST DISCRETIZER

Datasets

CART

Iris

Sat

Thy

Wav

ion

Smo

Heq

Pid

Proc

Original

4.0

21.4

0.6

26.3

10.5

12.4

26.7

24.9

18.0

Discretized

4.0

21.1

0.2

25.7

11.7

12.4

18.1

28

17.9

The results summarized in Table III show that using the
Random Forest Discretizer enhances the accuracy of CART
with seven out of the nine datasets investigated. We can
conclude that the Random Forest Discretizer conserves most
of the data elements or properties present in the raw (undiscretized) data.
The accuracies achieved by the C5.0 learning algorithm
for the eight data sets discretized using eight different
discretization methods are summarized in Table IV.
Clearly, the C5.0 learning algorithm achieves the best
performance in five out the eight data sets when discretized
using the Random Forest Discretizer, for the remaining three
data sets its performance competes with the other
discretization algorithms.
V.

SUMMARY AND CONCLUSIONS

We introduced the Random Forest Discretizer; a novel
supervised global and dynamic discretization method. The
new discretization approach is fast and directly applicable to
heterogeneous data and the resulting number of bins for each

TABLE IV.

continuous variable is small and learned during the
discretization process and not specified a priori. The Empirical
results of this paper indicate that the Random Forest
Discretizer surpasses other discretization methods when used
for data preprocessing for C5.0. Moreover, comparing the
accuracy of CART with several datasets before and after
discretization indicated that the discretized data space is
highly comparable to the raw data space in terms of classattribute and attribute-attribute interdependence.
The discretization resulting from the RFDisc should not
depend on the results of the Random Forests classifier as long
it has reached convergence. That is, enough trees should be
used in the Random Forest to reach stable classification
accuracy.
Finally, we point out that the split points chosen by the
RFDisc algorithm will depend on the threshold adopted by the
user to define high density split point among the candidate
split points indicated by the Random Forests classifier. Future
work will investigate alternatives for defining a threshold for
choosing split points.

COMPARISON OF THE ACCURACIES ( %) BY THE C5.0 ALGORITHM FOR EIGHT DATA SETS USING EIGHT DISCRETIZATION SCHEMES

Datasets
Iris
Mean

Ion

Std

Mean

Hea

Std

Mean

Wav
Std

Mean

Thy

Std

Mean

Std

Pid
Mean

Sat

Std

Mean

Smo

Std

Mean

Std

RFDisc

96.0

3.2

93.7

3.8

81.1

3.2

77.8

1.2

97.3

0.6

73.2

4.4

86.8

0.9

69.5

0.0

CAIM

95.3

4.5

89.0

5.2

76.3

8.9

72.7

4.2

98.9

0.4

74.6

4.0

86.2

1.7

70.3

2.9

IEM

95.3

4.5

92.6

2.9

73.4

8.9

76.6

2.1

99.4

0.2

75.8

4.3

84.6

1.1

69.7

1.6

PatersonNiblett

94.0

4.9

85.0

8.1

79.9

7.1

74.8

5.6

97.8

0.4

71.7

4.4

83.0

1.0

70.1

3.2

Equal
Width

94.7

5.3

85.5

6.4

74.7

5.2

57.7

8.2

95.0

1.1

70.8

2.8

86.0

1.6

69.2

5.4

Equal
Frequency

94.0

5.8

81.0

12.4

69.3

5.7

57.5

7.9

97.6

1.2

70.3

5.4

85.1

1.5

70.1

1.7

Maximu
m Entropy

93.3

6.3

86.5

8.8

73.3

7.6

55.5

6.2

97.7

0.6

66.4

5.9

85.2

1.5

70.2

3.9

93.3

5.4

77.5

11.9

73.6

10.6

56.9

2.1

93.5

0.8

71.8

2.2

86.1

0.9

70.2

4.7

CADD

216

REFERENCES
[1]

[2]

[3]

[4]
[5]
[6]
[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]

[17]
[18]

[19]
[20]

[21]

[22]

R. Agrawal, T. Imielinski, and A. Swami, “ Mining Association Rules
between sets of items in large relational databases,” SIGMOD, pp.207216, 1993
S.D. Bay, “ Multivariate discretization of Continuous Variables for Set
Mining,” In proceeding of the sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 315-319,
2000
C.L. Blake, and C. J. Merz, UCI Repository of Machine Learning
Databases,http://www.ics.uci.edu/ mlearn/MLRepository.html, UC
Irvine, Dept. Information and Computer Science, 1998
M. Boulle, “Khiops: A Statistical Discretization Method of Continuous
Attributes,” Machine Learning, 55, pp. 5369, 2004
L. Breiman, “Random Forests,” Machine Learning, Vol.45, 2001
L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, Classification
and Regression Trees, Belmont, CA: Wadsworth, 1984
J. Catlett, “On changing continuous attributes into ordered discrete
attributes,” In Proceedings of the European Working Session on
Learning, Berlin, pp.164- 178, 1991
J.Y. Ching, A.K.C. Wong, and K.C.C. Chan, “Class-Dependent
Discretization for Inductive Learning from Continuous and Mixed
Mode Data,” IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 17, no. 7, pp. 641-651, July 1995
D. K. Y. Chiu, B. Cheung, and A.K.C. Wong, “Information synthesis
based on hierarchical entropy discretization,” Journal of Experimental
and Theoretical Artificial Intelligence 2, pp. 117-129, 1990
D. Chiu, A. Wong, and B. Cheung, “Information Discovery through
Hierarchical Maximum Entropy Discretization and Synthesis,”
Knowledge Discovery in Databases, G. Piatesky-Shapiro and W.J.
Frowley, ed., MIT Press, 1991
Data Mining Tools: http://www.rulequest.com/see5-info.html, 2003
J. Dougherty, R. Kohavi, and M. Sahami, “Supervised and
Unsupervised Discretization of Continuous Features,” Proceeding of
the Twelfth International Conference on Machine Learning, pp. 194202, 1995
U.M. Fayyad, and K.B. Irani, “Multi-Interval Discretization of
Continuous-Valued Attributes for Classification Learning,” Proceeding
of the Thirteenth International Joint Conference on Artificial
Intelligence, pp. 1022-1027, 1993
E. Frank, and I.H. Witten, “Making Better Use of Global
Discretization,” Proceeding of the 16th International Conference on
Machine Learning, 1999
A.A. Freitas, and S.H. lavington, “Speeding up Knowledge Discovery
in Large Relational Databases by Means of a New Discretization
Algorithm,” 14th British National Conference on Databases, 1996
J. Gama, L. Torgo, and C. Soares, “Dynamic Discretization of
Continuous Attributes,” Proceeding of the Sixth Ibero-American
Conference on AI, pp. 160-169, 1998
R.C. Holte, “Very Simple Classification Rules Perform Well on Most
Commonly Used Datasets,” Machine Learning 11, pp. 63-90, 1993
R. Kerber, “Chimerge: Discretization of Numeric Attributes,”
Proceeding of the Tenth National Conference on Artificial Intelligence,
pp. 123-128, 1992
L. A. Kurgan, and K.J. Cios, “CAIM Discretization Algorithm,” IEEE
Transactions of Knowledge and Data Engineering, 2003
W. Kwedlo, and M. Kretowski, “An Evolutionary Algorithm Using
Multivariate Discretization for Decision Rule Induction,” Proceeding of
the European Conference on Principles of Data Mining and Knowledge
Discovery, pp. 392-397, 1999
H. Liu, F. Hussain, C.L. Tan, and M. Dash, “Discretization: An
Enabling Technique,” Journal of Data Mining and Knowledge
Discovery, Vol. 6(4): pp. 393-423, 2002
H. Liu, and R. Setiono, “ Chi2: Feature Selection and Discretization of
Numeric Attributes,” Proceeding of the IEEE 7th International
Conference on Tools with Artificial Intelligence, pp. 388-391, 1995

[23] F. Muhlenbach, and R. Rakotomalala, “Multivariate Supervised
Discretization, a Neighborhood Graph Approach,” IEEE Transactions
on Knowledge and Data Engineering, pp. 314-321, 2002
[24] A. Paterson, and T.B. Niblett, ACLS Manual, Edinburgh: Intelligent
Terminals,Ltd, 1987
[25] J.R. Quinlan, C4.5 Programs for Machine Learning, MorganKaufmann, 1993
[26] M. Richeldi, and M. Rossotto, “ Class-driven statistical discretization
of continuous attributes,” In European Conference on Machine
Learning, pp. 335-338, 1995
[27] F.E.H. Tay, and L. Shen, “A Modified Chi2 Algorithm for
Discretization,” IEEE Transactions on Knowledge and Data
Engineering, pp. 666-670, 2002
[28] P. Vlachos, StatLib Project Repository, http://lib.stat.cmu.edu/
datasets/csb/, 2000
[29] K. Wang, and B. Liu, “Concurrent Discretization of Multiple
Attributes,” Pacific Rim International Conference on Artificial
Intelligence, 1998
[30] A.K.C. Wong, and D.K.Y. Chiu, “Synthesizing Statistical Knowledge
from Incomplete Mixed-Mode Data,” IEEE Trans. Pattern Analysis and
Machine Intelligence, vol. 9, pp. 796-805, 1987
[31] Y. Yang, and G.I. Webb “A comparative study of Discretization
methods for Naive-Bayes Classifiers,” Proceeding of the PKAW, pp.
159-173, 2002
[32] S. Zembe, M. Rams, “Multivariate Feature Coupling and
Discretization,” Proceeding of FEA, 2003

217

Gene Selection With
Guided Regularized Random Forest

arXiv:1209.6425v3 [cs.LG] 20 Jun 2013

Houtao Deng
Intuit, Mountain View, CA, USA

George Runger
Arizona State University, Tempe, AZ, USA

Abstract
The regularized random forest (RRF) was recently proposed for feature selection by building only one ensemble. In RRF the features are evaluated
on a part of the training data at each tree node. We derive an upper bound
for the number of distinct Gini information gain values in a node, and show
that many features can share the same information gain at a node with a
small number of instances and a large number of features. Therefore, in a
node with a small number of instances, RRF is likely to select a feature not
strongly relevant.
Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In GRRF, the importance scores from an ordinary random forest
(RF) are used to guide the feature selection process in RRF. Experiments on
10 gene data sets show that the accuracy performance of GRRF is, in general,
more robust than RRF when their parameters change. GRRF is computationally efficient, can select compact feature subsets, and has competitive
accuracy performance, compared to RRF, varSelRF and LASSO logistic regression (with evaluations from an RF classifier). Also, RF applied to the
features selected by RRF with the minimal regularization outperforms RF
applied to all the features for most of the data sets considered here. Therefore, if accuracy is considered more important than the size of the feature
This research was partially supported by ONR grant N00014-09-1-0656.
Email addresses: hdeng3@asu.com (Houtao Deng), george.runger@asu.edu
(George Runger)
∗

Preprint submitted to Elsevier

June 21, 2013

subset, RRF with the minimal regularization may be considered. We use the
accuracy performance of RF, a strong classifier, to evaluate feature selection
methods, and illustrate that weak classifiers are less capable of capturing the
information contained in a feature subset. Both RRF and GRRF were implemented in the “RRF” R package available at CRAN, the official R package
archive.
Keywords: classification; feature selection; random forest; variable
selection.
1. Introduction
Given a training data set consisting of N instances, P predictor variables/features Xi (i = 1, ..., P ) and the class Y ∈ {1, 2, ..., C}, the objective
of feature selection is to select a compact variable/feature subset without
loss of predictive information about Y . Note feature selection selects a subset of the original feature set, and, therefore, may be more interpretable than
feature extraction (e.g., principal component analysis [15] and partial least
squares regression [8]) which creates new features based on transformations
or combinations of the original feature set [13]. Feature selection has been
widely used in many applications such as gene selection [23, 30, 18] as it can
moderate the curse of dimensionality, improve interpretability [10] and avoid
the effort to analyze irrelevant or redundant features.
Information-theoretic measures such as symmetrical uncertainty and mutual information can measure the degree of association between a pair of
variables and have been successfully used for feature selection, e.g., CFS
(correlation-based feature selection) [12] and FCBF (fast correlation-based
filter) [29]. However, these measures are limited to two variables and do
not capture high-order interactions between variables well. For example, the
measures can not capture the exclusive OR relationship Y = XOR(X1 , X2 ),
in which neither X1 nor X2 is predictive individually, but X1 and X2 together
can correctly determine Y [14].
LASSO logistic regression [25] and recursive feature elimination with a
linear SVM (SVM-RFE) [11] are well-known feature selection methods based
on classifiers. These methods assume a linear relationship between the log
odds of the class and the predictor variables (LASSO logistic regression) or
between the class and the predictor variables (linear SVM). Furthermore,
before using these methods, one often needs to preprocess the data such as
2

transforming categorical variables to binary variables or normalizing variables
of different scales.
A random forest (RF)[3] classifier has been commonly used for measuring
feature importance [17]. An RF naturally handles numerical and categorical
variables, different scales, interactions and nonlinearities, etc. Although the
RF feature importance scores can be used to select K features with the
highest importance scores individually, there could be redundancy among
the K features. Consequently, the selected features subset can differ from
the best combination of K-features, that is, the best feature subset. Similarly,
Boruta [22], a method based on RF, aims to select a set of relevant features,
which is different from the objective of a relevant and also non-redundant
feature subset.
A feature selection method based on RF, varSelRF[27], has become popular. varSelRF consists of multiple iterations and eliminates the feature(s)
with the least importance score(s) at each iteration. Since eliminating one
feature at each iteration is computationally expensive, the authors considered
eliminating a fraction, e.g., 1/5, of the features at each iteration. However,
when there is a large number of features, many features are eliminated at
each iteration, and, thus, useful features, but with small importance scores,
can be eliminated.
The ACE algorithm [26] is another ensembles-based feature selection
method. It was shown to be effective, but it is more computationally demanding than the simpler approaches considered here. It requires multiple
forests to be constructed, along with multiple gradient boosted trees [6].
Recently, the regularized random forest (RRF) was proposed for feature
selection with one ensemble [4], instead of multiple ensembles [27, 26]. However, in RRF the features are evaluated on a part of the training data at each
tree node and the feature selection process may be greedy.
Here we analyze a feature evaluation issue that occurs in all the usual
splitting algorithms at tree nodes with a small number of training instances.
To solve this issue, we propose the guided RRF (GRRF) method, in which the
importance scores from an ordinary RF are used to guide the feature selection
process in RRF. Since the importance scores from an RF are aggregated from
all the trees based on all the training data, GRRF is expected to perform
better than RRF.
Section 2 presents previous work. Section 3 discusses the node sparsity
issue when evaluating features at tree nodes with a small number of training
instances. Section 4 describes the GRRF method. Section 5 presents and
3

discusses the experimental results. Section 6 concludes this work.
2. Background
2.1. Variable importance scores from Random Forest
A random forest (RF) [3] is a supervised learner that consists of multiple
decision trees, each of which grown on a bootstrap sample from the original
training data. The Gini index at node v, Gini(v), is defined as
Gini(v) =

C
X
c=1

p̂vc (1 − p̂vc )

where p̂vc is the proportion of class-c observations at node v. The Gini information gain of Xi for splitting node v, Gain(Xi , v), is the difference between
the impurity at the node v and the weighted average of impurities at each
child node of v. That is,
Gain(Xi , v) =
Gini(Xi , v) − wL Gini(Xi , v L ) − wR Gini(Xi , v R )
where v L and v R are the left and right child nodes of v, respectively, and wL
and wR are the proportions of instances assigned to the left and right child
nodes. At each node, a random set of mtry features out of P is evaluated,
and the feature with the maximum Gain(Xi , v) is used for splitting the node
v.
The importance score for variable Xi can be calculated as
Impi =

1 X
Gain(Xi , v)
ntree v∈S

(1)

Xi

where SXi is the set of nodes split by Xi in the RF with ntree trees. The
RF importance scores are commonly used to evaluate the contributions of
the features regarding predicting the classes.
2.2. Regularized Random Forest
The regularized random forest (RRF) [4] applies the tree regularization
framework to RF and can select a compact feature subset. While RRF is

4

Algorithm 1: Feature selection at node v.
input : F and λ
output: F
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

gain∗ ← 0, count ← 0, f ∗ ← −1, f1 ← {1, 2, 3, ..., P }, f2 ← ∅
while f1 6= ∅ do
m ← select(f1 ) //randomly select feature index m from f1
f2 ← {f2 , m} //add m to f2
f1 ← f1 − m //remove m from f1
gainR (Xm , v) ← 0
if Xm ∈ F then
gainR (Xm , v) ← gain(Xm , v) //calculate gainR for all variables in F
end
√
if Xm ∈
/ F and count < ⌈ P ⌉ then
gainR (Xm ) ← λ · gain(Xm ) //regularize the gain if the variable is not in F
count ← count + 1
end
if gainR (Xm , v) > gain∗ then
gain∗ ← gainR (Xm , v), f ∗ ← m
end
end
if f ∗ 6= −1 and f ∗ ∈
/ F then
F ← {F, f ∗ }
end
return F

built in a way similar to RF, the main difference is that the regularized
information gain, GainR (Xi , v), is used in RRF
(
λ · Gain(Xi , v) i ∈
/F
(2)
GainR (Xi , v) =
Gain(Xi , v)
i∈F
where F is the set of indices of features used for splitting in previous nodes
and is an empty set at the root node in the first tree. Here λ ∈ (0, 1] is
called the penalty coefficient. When i ∈
/ F , the coefficient penalizes the ith
feature for splitting node v. A smaller λ leads to a larger penalty. RRF uses
GainR (Xi , v) at each node, and adds the index of a new feature to F if the
feature adds enough predictive information to the selected features.
RRF with λ = 1, referred to as RRF(1), has the minimum regularization.
Still, a new feature has to be more informative at a node than the features
already selected to enter the feature subset. The feature subset selected by
RRF(1) is called the least regularized subset, indicating the minimal regularization from RRF.
Figure 1 illustrates the feature selection process. The nodes in the forest
are visited sequentially (from the left to the right and from the top to the
5

bottom). The indices of three distinct features used for splitting are added
to F in Tree 1, and the index of X5 is added to F in Tree 2. Algorithm 1
shows the feature selection process at each node.
F={}+1

F={1,3,4}

X1

X3
F={1,4}+3

F={1}+4
X4

F={1,3,4}+5

X3

X5

Tree 1

...

Tree 2

Figure 1: The feature selection procedure of RRF. The non-leaf nodes are marked with
the splitting variables. Three distinct features used for splitting are added to F in Tree 1,
and one feature X5 is added to F in Tree 2.

Similar to GainR (·), a penalized information gain was used to suppress
spurious interaction effects in the rules extracted from tree models [7]. The
objective of Friedman and Popescu [7] was different from the goal of a compact feature subset here. Also, the regularization used in Friedman and
Popescu [7] only reduces the redundancy in each path from the root node
to a leaf node, but the features extracted from such tree models can still be
redundant.
3. The Node Sparsity Issue
This section discusses an issue of evaluating features in a tree node with
a small number of instances, referred to as the node sparsity issue here. The
number of instances decreases as the instances are split recursively in a tree,
and, therefore, this issue can commonly occur in tree-based models.
In a tree node, the Gini information gain or other information-theoretic
measures are commonly used to evaluate and compare different features.
However, the measures calculated from a node with a small number of instances may not be able to distinguish the features with different predictive
information. In the following we establish an upper bound for the number
of distinct values of Gini information gain for binary classification.
Let D(f ) denote the number of distinct values of a function f (defined
over a specified range), N denote the number of instances at a non-leaf node
v and N ≥ 2 (otherwise it cannot be split). For simplicity, in the following
6

we also assume N is even. The following procedure is similar when N is odd.
Let N1 and N2 be the number of instances of class 1 and class 2, respectively,
at the node. Let L denote the number of instances at the left child node,
and let L1 and L2 denote the number of instances for class 1 and class 2
at the left child node, respectively, with similar notations R, R1 and R2 for
the right child node. Note L ≥ 1 and R ≥ 1. The notation L1 L2 denotes
the product function where L1 and L2 assume values in the feasible domain
0 ≤ L1 , L2 ≤ L and L1 + L2 = L. Then we have the following lemmas and
theorem.
Lemma 1. An upper bound of the number of distinct values of L1 L2 is ⌈(L+
1)/2⌉. That is, D(L1 L2 ) ≤ ⌈(L + 1)/2⌉, where ⌈·⌉ denotes the ceiling.
Proof. L1 has at most L + 1 values (L1 ∈{0,1,...,L}). Now let l1 l2 and m1 m2
be two realizations of L1 L2 , and let l1 l2 = m1 m2 , we have
(L − l1 )l1
= (L − m1 )m1
2
2
⇔
Ll1 − l1 − Lm1 + m1
=
0
⇔ L(l1 − m1 ) + (m1 + l1 )(m1 − l1 ) =
0
⇔
(l1 − m1 )(L − l1 − m1 )
=
0
⇔
l1 = m1 or l1 = L − m1
Therefore, we obtain an upper bound for the number of distinct values for
L1 L2 as D(L1 L2 ) <= ⌈(L + 1)/2⌉.
Lemma 2. D(L1 L2 /L) ≤ N(N + 2)/4 − 1.
Proof. For each L, L1 L2 /L has at most D(L1 L2 ) distinct values. Because
1 ≤ L ≤ N − 1, an upper bound for D(L1 L2 /L) is derived as follows
D(L1 L2 /L) ≤ ⌈2/2⌉ + ⌈3/2⌉ + ... + ⌈N/2⌉ = N(N + 2)/4 − 1
Lemma 3. D(L1 L2 /L + R1 R2 /R) ≤ N(N + 2)/4 − 1
Proof. Because L1 L2 /L and R1 R2 /R are symmetric, the upper bound is the
same as for one term. That is, D(L1 L2 /L + R1 R2 /R) ≤ N(N + 2)/4 − 1
Theorem 1. For binary classification, an upper bound of the number of
distinct information gain values is N(N + 2)/4 − 1.
7

Proof. The information gain for spliting node v is
Gain(v) = Gini(v) − wL Gini(v L ) − wR Gini(v R )
Because Gini(v) is a constant at node v, we only need to consider D(wL Gini(v L )+
wR Gini(v R )). For two classes
wL Gini(v L )
= (L/N)(L1 /L · L2 /L + L2 /L · L1 /L)
= 2(L/N)(L1 L2 )/L2
Then

wL Gini(v L ) + wR Gini(v R )
= 2(L/N)(L1 L2 )/L2 + 2(R/N)(R1 R2 )/R2
= (2/N)(L1 L2 /L) + (2/N)(R1 R2 /R)

Because N is a constant, we have D(wL Gini(v L )+wR Gini(v R )) = D(L1 L2 /L+
R1 R2 /R). According to Lemma 3, D(L1 L2/L + R1 R2 /R) ≤ N(N + 2)/4 −
1.
Note similar conclusions may be applied to other information-theoretic
measures such as the regularized information gain in Equation 2.
Consequently, when N is small, the number of distinct Gini information
gain values is small. For a large number of variables, many could have the
same Gini information gain. For example, at a node with 10 instances there
are at most 29 distinct Gini information gain values for binary classification
problems. For 1000 genes with two classes, there are at least 1000-29=971
genes having the same information gain as other genes. The number of
instances can be even smaller than 10 in a node of an RF or RRF as each
tree is grown completely.
In RRF, a feature with the maximum regularized information gain is
added at a node based on only the instances at that node. When multiple
features have the maximum regularized information gain, one of these features is randomly selected. As discussed above, at a node with only a small
number of instances less relevant, or redundant features may be selected. An
additional metric is useful to help distinguish the features. In the following
section, we introduce the guided regularized random forest that leverages the
importance scores calculated from an RF based on all the training data.

8

4. Guided Regularized Random Forest
The guided RRF (GRRF) uses the importance scores from a preliminary
RF to guide the feature selection process of RRF. Because the importance
scores from an RF are aggregated from all the trees of the RF based on all the
training data, GRRF may be able to handle the node sparsity issue discussed
in the previous section.
A normalized importance score is defined as
Imp′i =

Impi
maxPj=1 Impj

(3)

where Impi is the importance score from an RF (defined in Equation 1).
Here 0 ≤ Imp′i ≤ 1. Instead of assigning the same penalty coefficient to all
features in RRF, GRRF assigns a penalty coefficient to each feature. That
is,
(
λi Gain(Xi , v) Xi ∈
/F
(4)
GainR (Xi , v) =
Gain(Xi , v)
Xi ∈ F
where λi ∈ (0, 1] is the coefficient for Xi (i ∈ {1, ..., P }) and is calculated
based on the importance score of Xi from an ordinary RF. That is,
λi = (1 − γ)λ0 + γImp′i

(5)

where λ0 ∈ (0, 1] controls the degree of regularization and is called the base
coefficient, and γ ∈ [0, 1] controls the weight of the normalized importance
score and is called the importance coefficient. Note that RRF is a special case
of GRRF with γ = 0. Given λ0 and γ, a feature with a larger importance
score has a larger λi , and, therefore, is penalized less.
In our experiments we found the feature subset size can be effectively
controlled by changing either λ0 or γ, but changing the latter often leads
to better performance in terms of classification accuracy. To reduce the
number of parameters of GRRF, we fix λ0 to be 1 and consider γ as the only
parameter for GRRF. With λ0 = 1, we have
λi = (1 − γ) + γImp′i = 1 − γ(1 − Imp′i )

(6)

For a feature Xi that does not have the maximum importance score (Imp′i 6=
1), a larger γ leads to a smaller λi and, thus, a larger penalty on Gain(Xi , v)
when Xi has not been used in the nodes prior to node v. Consequently, γ is
essentially the degree of regularization. Furthermore, GRRF with γ = 0 is
equivalent to RRF(1), with the minimal regularization.
9

5. Experiments
We implemented the RRF and GRRF algorithms in the “RRF” R package
based on the “randomForest” package [17]. The “RRF” package is available
from CRAN (http://cran.r-project.org/), the official R package archive.
Work by [27] showed that the accuracy performance of RF is largely
independent
√ of the number of trees (between 1000 and 40000 trees), and
mtry = P is√often a reasonable choice. Therefore, we used ntree = 1000
and mtry = P for the classifier RF and the feature selection methods
RRF, GRRF and varSelRF. Guided by initial experiments, randomly sampling 63% of the data instances (the same as the default setting in the “randomForest” package) without replacement was used in RRF and GRRF. We
also evaluated two well-known feature selection methods: varSelRF available
in the “varSelRF” R package and LASSO logistic regression available in the
“glmnet” R package. Unless otherwise specified, the default values in the
packages were used. Also note λ is the parameter of RRF, and as discussed,
γ is considered as the only parameter of GRRF here.
Table 1: The number of groups identified, and the number of irrelevant or redundant
features selected for different algorithms.
Method
Average
Std. err

LASSO
4
0.000

# Groups
varSelRF
RRF
4.95
5
0.050
0.000

GRRF
4.95
0.050

# Irrelevant or Redundant Features
LASSO
varSelRF
RRF
GRRF
4.3
4.95
4.95
0.75
0.147
0.050
0.050
0.123

5.1. Simulated Data Sets
We start with a simulated data set generated by the following procedure. First generate 10 independent variables: X1 ,..., X10 , each is uniformly
distributed in the interval [0,1]. Variable Y is calculated by the formula
Y = 10 sin(πX1 X2 ) + 20(X3 − 0.5)2 + 10X4 + 5X5 + e
where e follows a standard normal distribution. Note the above data generation procedure was described in [5]. We simulated 1000 instances and
then calculated the median of Y as y. We then labeled class 2 to the instances with Y > y, and class 1 otherwise, so that it becomes a classification problem. Furthermore, we added five other variables with X11 =X1 ,
X12 =X2 , X13 =X3 , X14 =X4 , X15 =X5 . Consequently, the feature selection
solution is {(X1 |X11 ) & (X2 |X12 ) & (X3 |X13 ) & (X4 |X14 ) & (X5 |X15 )},
10

Table 2: Summary of the data sets.
Data set
adenocarcinoma
brain
breast.2.class
breast.3.class
colon
leukemia
lymphoma
nci60
prostate
srbct

Reference
[20]
[19]
[28]
[28]
[2]
[9]
[1]
[21]
[24]
[16]

# Examples
76
42
77
95
62
38
62
61
102
63

# Features
9868
5597
4869
4869
2000
3051
4026
5244
6033
2308

# classes
2
5
2
3
2
2
3
8
2
4

where & stands for “and” and (X1 |X11 ) indicates that one and only one
from this group should be selected. For example, for the data set considered
here, both (X1 , X2 , X3 , X4 , X5 ) and (X11 , X2 , X13 , X4 , X5 ) are correct solutions. However, (X1 , X3 , X4 , X5 ) misses the group (X2 |X12 ). Furthermore,
(X1 , X2 , X3 , X4 , X5 , X11 ) has a redundant variable because X11 is redundant
to X1 .
We simulated 20 replicates of the above data set, and then applied GRRF,
RRF and two well-known methods varSelRF and LASSO logistic regression
to the data sets. Here γ of GRRF was selected from {0.4,0.5,0.6}, λ of RRF
was selected from {0.6,0.7,0.8}, and the regularization parameter of LASSO
logistic regression was selected from {0.01,0.02,0.03,0.05,0.1,0.2}, all by 10fold CV.
The results are shown in Table 1. LASSO logistic regression identifies the
least number of groups on average (4). The methods varSelRF, RRF and
GRRF identify almost all the groups, but varSelRF and RRF select 4.95,
on average, irrelevant or redundant variables, and GRRF only selects 0.75
irrelevant or redundant variables. This experiment shows GRRF’s potential
in selecting a relevant and non-redundant feature subset.
5.2. Gene Data Sets
The 10 gene expression data sets analyzed by Uriarte and de Andres [27]
are considered in this section. The data sets are summarized in Table 2. For
each data set, a feature selection algorithm was applied to 2/3 of the instances
(selected randomly) to select a feature subset. Then a classifier was applied
to the feature subset. The error rate is obtained by applying the classifier to
the other 1/3 of the instances. This procedure was conducted 100 times with
different random seeds, and the average size of feature subsets and the average
error rate over 100 runs were calculated. In the experiments, we considered
11

Table 3: The number of features selected in the least regularized subset selected by RRF
(i.e., RRF(1)), the total number of original features (“All”), and the error rates of RF
applied to the least regularized subset and all the features. The win-lose-tie results of
“All” compared to RRF(1) are shown. Here “◦” or “•” represents a significant difference
at the 0.05 level, according to the paired t-test. RRF(1) uses many fewer features than
the original features, and wins on 7 data sets. There are significant differences for four
data sets.
adenocarcinoma
brain
breast.2.class
breast.3.class
colon
leukemia
lymphoma
nci
prostate
srbct
win-lose-tie

Number of features
RRF(1)
All
86
9868
97
5597
210
4869
253
4869
92
2000
24
3051
31
4026
197
5244
88
6033
51
2308
-

Average error rates
RRF(1)-RF
All-RF
0.158
0.159
0.159
0.170
0.352
0.371
0.397
0.415
0.162
0.158
0.053
0.064
0.018
0.006
0.332
0.321
0.082
0.109
0.027
0.032
3-7-0

◦
◦
•
◦

Table 4: The total number of original features (“All”) and the average number of features
(from 100 replicates for each data set) selected by different methods. All feature selection
methods are able to select a small number of features. Here GRRF(0.1) and RRF(0.9)
select a similar number of features, but it is shown later that GRRF(0.1) is more accurate
than RRF(0.9) (with an RF classifier) for most data sets.
adenocarcinoma
brain
breast.2.class
breast.3.class
colon
leukemia
lymphoma
nci
prostate
srbct

All
9868
5597
4869
4869
2000
3051
4026
5244
6033
2308

GRRF(0.1)
20
22
59
77
29
6
5
63
18
13

GRRF(0.2)
15
12
25
31
13
4
4
26
13
9

RRF(0.9)
23
27
60
78
27
6
4
61
19
14

varSelRF
4
28
7
12
4
2
81
60
6
34

LASSO
2
24
7
18
7
8
25
53
12
28

RRF, GRRF, varSelRF and LASSO logistic regression as the feature selection
algorithms, and random forest (RF) and C4.5 as the classifiers.
5.2.1. Feature selection and classification
First compare the accuracy of RF applied to all the features, denoted as
“All”, and the least regularized subset (i.e., features selected by RRF(1)).
The number of features and the error rates are shown in Table 3. The winlose-tie results of “All” compared to RRF(1) are also shown in the tables. To
investigate the statistical significance of these results, we applied the paired
12

Table 5: The error rates of RF applied to the feature subsets selected by GRRF(0.1),
GRRF(0.2), RRF(0.9), varSelRF and LASSO logistic regression, respectively (to three
decimal places). The win-lose-tie results of each competitor compared to GRRF(0.1) with
RF are shown. Here “◦” or “•” represents a significant difference between a method and
GRRF(0.1) with RF at the 0.05 level, according to the paired t-test. Here GRRF(0.1)
leads to competitive accuracy performance, compared to GRRF(0.2), RRF(0.9), LASSO
and varSelRF.
adenocarcinoma
brain
breast.2.class
breast.3.class
colon
leukemia
lymphoma
nci
prostate
srbct
win-lose-tie

GRRF(0.1)
-RF
0.169
0.214
0.345
0.387
0.175
0.080
0.067
0.389
0.085
0.064
-

GRRF(0.2)
-RF
0.168
0.259
0.359
0.403
0.186
0.093
0.076
0.452
0.085
0.072
1-9-0

◦
◦
◦
◦
◦

RRF(0.9)
-RF
0.160
0.234
0.367
0.410
0.190
0.091
0.098
0.405
0.101
0.074
1-9-0

◦
◦
◦
◦
◦

varSelRF
-RF
0.212
0.231
0.386
0.418
0.232
0.107
0.022
0.418
0.085
0.035
3-7-0

◦
◦
◦
◦
◦
•
◦
•

LASSO
-RF
0.189
0.259
0.366
0.400
0.180
0.076
0.009
0.396
0.088
0.007
3-7-0

◦
◦
◦

•
•

Table 6: The error rates of C4.5 applied to the feature subsets selected by GRRF(0.1),
GRRF(0.2), RRF(0.9), varSelRF and LASSO logistic regression, respectively. The winlose-tie results of each competitor compared to GRRF(0.1) with C4.5 are calculated. Here
“◦” or “•” represents a significant difference between a method and GRRF(0.1) with
C4.5 at the 0.05 level, according to the paired t-test. The GRRF methods and the other
methods have perform similarly in terms of the accuracy with C4.5. As expected, C4.5
has noticeably higher error rates than RF, shown in Table 5.
adenocarcinoma
brain
breast.2.class
breast.3.class
colon
leukemia
lymphoma
nci
prostate
srbct
win-lose-tie

GRRF(0.1)
-C4.5
0.260
0.438
0.402
0.497
0.274
0.133
0.126
0.626
0.157
0.235
-

GRRF(0.2)
-C4.5
0.251
0.428
0.411
0.494
0.262
0.157
0.125
0.636
0.138
0.208
7-3-0

◦
•
•

13

RRF(0.9)
-C4.5
0.241
0.418
0.412
0.491
0.298
0.138
0.126
0.658
0.154
0.207
5-5-0

◦
◦
•

varSelRF
-C4.5
0.248
0.394
0.413
0.481
0.273
0.146
0.152
0.641
0.115
0.209
6-4-0

•

◦
•
•

LASSO
-C4.5
0.218
0.476
0.391
0.482
0.242
0.166
0.138
0.646
0.144
0.194
6-4-0

•
◦
•
◦

•

t-test to the error rates of the two methods from 100 replicates for each data
set. The data sets with a significant difference at the 0.05 level are marked
with “◦” or “•” in the table.
The least regularized subset not only has many fewer features than “All”,
but also leads to better accuracy performance on 7 data sets out of 10, and
the error rates are significantly different on 4 data sets. Therefore, RRF(1)
not only improves interpretability by reducing the number of features, but
also can improve the accuracy performance of classification, even for RF, considered as a strong classifier capable of handling irrelevant and relevant variables [4]. It should also be noted that although the least regularized subset
is much smaller than the original feature set, the size may still be considered
large in some cases, e.g., more than 200 features for the breast.2.class data
set. GRRF and RRF with larger regularization, investigated in the following
experiments, are able to further reduce the number of features.
Next compare GRRF to RRF and two well-known methods: varSelRF
and LASSO logistic regression. The regularization parameter of LASSO logistic regression was selected from {0.01, 0.02, 0.03, 0.05, 0.1, 0.2} by 10-fold
CV. Here γ ∈ {0.1, 0.2} was used for GRRF (i.e., GRRF(0.1) or GRRF(0.2)),
and λ = 0.9 was used for RRF (i.e., RRF(0.9)). We used a fixed parameter
setting for GRRF or RRF, as the parameter sensitivity analysis in the following section shows a consistent trend that GRRF or RRF tends to select more
features and also tends to be more accurate, for a smaller γ or a larger λ. We
chose these parameters so that a reasonably small number of features can be
selected. One can also use cross-validation error to determine an appropriate
parameter value customized for each data set and potentially improve these
results.
The total number of original features and the average number of features
selected by each feature selection method are shown in Table 4. All the
feature selection methods are able to select a small number of features.
The average error rates of RF applied to all the features (“All”) and the
subsets selected by different feature selection methods are shown in Table
5. GRRF(0.1) with RF outperforms GRRF(0.2) with RF on 9 data sets out
of 10, 5 of which have significant differences at the 0.05 level. Even though
GRRF(0.1) selects more features than GRRF(0.2), the sizes of the feature
subsets are reasonably small (all less than 80 features).
GRRF(0.1) and RRF(0.9) select a similar number of features. However,
GRRF(0.1) with RF outperforms RRF(0.9) with RF on 9 data sets, 5 of
which have significant differences at the 0.05 level. Consequently, GRRF
14

selects stronger features than RRF, which is consistent with the simulated
experiments. According to the discussion in Section 3, a feature with the
maximum Gini information gain in a node with a small number instances
may not be truly strong. Yet RRF adds this feature to the subset.
GRRF(0.1) with RF outperforms varSelRF with RF on 7 data sets, 6
of which have significant differences. Therefore, GRRF(0.1) may be more
favorable than varSelRF for the data sets considered here. Also, as shown in
the following section, GRRF has a clear advantage over varSelRF in terms of
computational time. Furthermore, GRRF(0.1) with RF outperforms LASSO
logistic regression with RF on 7 data sets, 3 of which have significant differences. The accuracy performance may be improved by applying a logistic
regression model to the features selected by LASSO logistic regression. However, tree models like GRRF have a few desirable properties compared to
LASSO logistic regression: they can naturally handle mixed categorical and
numerical features, and multiple classes, etc.
The average error rates of C4.5 applied to all the features (“All”) and the
subsets selected by different feature selection methods are shown in Table 6.
It can be seen that the error rates of C4.5 are clearly higher than RF shown
in Table 5. Indeed, RF has been considered as a stronger classifier than
C4.5. Interestingly, the differences between the methods in terms of C4.5
are smaller than the RF results. As mentioned by [4], a relatively weaker
classifier is less capable of capturing information from data than a stronger
classifier. Consequently, a feature subset that includes strong features, but
misses the features of small contributions may not affect the accuracy of
C4.5 much, but can affect the accuracy of RF. A weak classifier should only
be used for evaluating a feature subset if that classifier is actually used for
classification after feature selection. However, if a strong classifier is used
for classification after feature selection, or the objective is to evaluate the
information contained in the feature subset, a strong classifier should be
considered [4].
5.2.2. Parameter Sensitivity and Computational Time
We investigated the performance of RRF and GRRF with different parameter settings: λ ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} for RRF
and γ ∈ {0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0} for GRRF. The parameter
values are arranged by the degree of regularization in a decreasing order for
both methods.
The sizes of the feature subsets, averaged over 100 replicates, for each
15

500 1000

500 1000

leukemia
lymphoma
nci
prostate
srbct

100
50

# features

5
1
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.9

0.8

base coefficient of RRF

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0.1

0

importance coefficient of GRRF

0.6

error rate

0.0

0.2

0.4

0.6
0.4
0.0

0.2

leukemia
lymphoma
nci
prostate
srbct

adenocarcinoma
brain
breast.2.class
breast.3.class
colon

0.8

leukemia
lymphoma
nci
prostate
srbct

0.8

adenocarcinoma
brain
breast.2.class
breast.3.class
colon

1.0

(b)

1.0

(a)

error rate

leukemia
lymphoma
nci
prostate
srbct

adenocarcinoma
brain
breast.2.class
breast.3.class
colon

10

50
1

5

10

# features

100

adenocarcinoma
brain
breast.2.class
breast.3.class
colon

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.9

base coefficient of RRF

0.8

0.7

0.6

0.5

0.4

0.3

0.2

importance coefficient of GRRF

(c)

(d)

Figure 2: The performance of RRF for selected values of λ and GRRF for selected values
of γ. Figure 2(a) shows the number of features selected by RRF. A smaller λ leads to
fewer features. Figure 2(b) shows the number of features selected by GRRF. A larger γ
leads to fewer features. Figure 2(c) shows the error rates of RF applied to the feature
subsets selected by RRF for different λ. The error rates tend to decrease as λ increases.
Figure 2(d) shows the error rates of RF applied to the feature subsets selected by GRRF
for different γ. The error rates tend to decrease as γ decreases, but are reasonably robust.

16

140
80
60

t
bc
sr

te

i

ta

nc

os
pr

a

ly

m

ph

om

ia

n
lo

uk
em
le

ss
.3

st
br

ea

co

la

ss

.c

la

n
.2
br

ea

st

ar
oc
ad

en

.c

br
ai

ci

no

m

a

0

20

40

Time(sec.)

100

120

RF
GRRF
varSelRF

Figure 3: Computational time of RF, GRRF and varSelRF. GRRF is more computationally efficient than varSelRF. GRRF takes about twice as much as RF as it builds two
ensembles.

parameter setting of RRF and GRRF, are shown in Figures 2(a) and 2(b),
respectively. The number of features tends to increase as λ increases for
RRF, or as γ decreases for GRRF. The consistent trend illustrates that one
can control the size of the feature subset by adjusting the parameters.
The error rates, averaged over 100 replicates, for each parameter setting of
RRF and GRRF are shown in Figures 2(c) and 2(d), respectively. In general,
the error rates tend to decrease as λ increases for RRF, or as γ decreases for
GRRF. However, for most data sets, the error rates of GRRF seem to be
reasonably robust to the changes of γ. As mentioned, RRF(1) and GRRF(0)
are equivalent, and, therefore, they have similar number of features and error
rates for every data set (differing only by random selections).
The computational time of the RF-based methods: RF, GRRF and varSelRF
is shown in Figure 3. As expected, RF was fast for training on these data
sets. GRRF builds two ensemble models, and, thus, the computational time
of GRRF is only about twice as much as RF. However, varSelRF needs to
17

build multiple ensemble models, and the computational advantage of GRRF
over varSelRF is clear.
6. Conclusions
We derive an upper bound for the number of distinct Gini information
gain values in a tree node for binary classification problems. The upper
bound indicates that the Gini information gain may not be able to distinguish
features at nodes with a small number of instances, which poses a challenge
for RRF that selects a feature only using the instances at a node. Motivated
by this node sparsity issue, we propose an enhanced method called the guided,
regularized random forest (GRRF), in which a preliminary random forest is
used to generate the initial variable importance scores to guide the regularized
feature selection process of RRF. The importance scores from the preliminary
RF help GRRF select better features when many features share the same
maximal Gini information gain at a node. For the experiments here, GRRF
is more favorable than RRF, computationally efficient, selects a small set of
features, and has competitive accuracy performance.
Feature selection eliminates irrelevant or redundant features, but also may
eliminate features of small importance. This may not affect the performance
of a weak classifier which is less capable of capturing small information, but
may affect the performance of a strong classifier such as RF [4]. Still, we
found that the least regularized subset selected by RRF with the minimal
regularization produces better accuracy performance than the complete feature set.
Finally we note that although RRF and GRRF can be used as classifiers,
they are designed for feature selection. The trees in RRF and GRRF are not
built independently as the features selected in previous trees have an impact
on the trees built later. Therefore, as a classifier, RRF or GRRF may have
a higher variance than RF because the trees are correlated. Consequently,
in this work we applied RF on the feature subset selected by GRRF or RRF
for classification.
References
[1] A.A. Alizadeh, M.B. Eisen, R.E. Davis, C. Ma, I.S. Lossos, A. Rosenwald, J.C. Boldrick, H. Sabet, T. Tran, X. Yu, J.I. Powell, L. Yang, G.E.

18

Marti, T. Moore, J. Hudson, L. Lu, D.B. Lewis, R. Tibshirani, G. Sherlock, W.C. Chan, T.C. Greiner, D.D. Weisenburger, J.O. Armitage,
R. Warnke, R. Levy, W. Wilson, M.R. Grever, J.C. Byrd, D. Botstein, P.O. Brown, L.M. Staudt, Distinct types of diffuse large B-cell
lymphoma identified by gene expression profiling., Nature 403 (2000)
503–511.
[2] U. Alon, N. Barkai, D.A. Notterman, K. Gish, S. Ybarra, D. Mack,
A.J. Levine, Broad patterns of gene expression revealed by clustering
analysis of tumor and normal colon tissues probed by oligonucleotide
arrays, Proceedings of the National Academy of Sciences of the United
States of America 96 (1999) 6745–6750.
[3] L. Breiman, Random forests, Machine Learning 45 (2001) 5–32.
[4] H. Deng, G.C. Runger, Feature selection via regularized trees, in:
The 2012 International Joint Conference on Neural Networks (IJCNN),
IEEE.
[5] J.H. Friedman, Multivariate adaptive regression splines, The annals of
statistics (1991) 1–67.
[6] J.H. Friedman, Greedy function approximation: A gradient boosting
machine, Annals of Statistics 29 (2001) 1189–1232.
[7] J.H. Friedman, B.E. Popescu, Predictive learning via rule ensembles,
Annals of Applied Statistics 2 (2008) 916–954.
[8] P. Geladi, B. Kowalski, Partial least-squares regression: a tutorial, Analytica chimica acta 185 (1986) 1–17.
[9] T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P.
Mesirov, H. Coller, M.L. Loh, J.R. Downing, M.A. Caligiuri, C.D.
Bloomfield, E.S. Lander, Molecular classification of cancer: class discovery and class prediction by gene expression monitoring., Science 286
(1999) 531–537.
[10] I. Guyon, A. Elisseeff, An introduction to variable and feature selection,
Journal of Machine Learning Research 3 (2003) 1157–1182.

19

[11] I. Guyon, J. Weston, S. Barnhill, V. Vapnik, Gene selection for cancer classification using support vector machines, Machine Learning 46
(2002) 389–422.
[12] M.A. Hall, Correlation-based feature selection for discrete and numeric
class machine learning, in: Proceedings of the 17th International Conference on Machine Learning, pp. 359–366.
[13] A. Jain, D. Zongker, Feature selection: Evaluation, application, and
small sample performance, Pattern Analysis and Machine Intelligence,
IEEE Transactions on 19 (1997) 153–158.
[14] A. Jakulin, I. Bratko, Analyzing attribute dependencies, Knowledge Discovery in Databases: PKDD 2003 (2003) 229–240.
[15] I. Jolliffe, MyiLibrary, Principal component analysis, volume 2, Wiley
Online Library, 2002.
[16] J. Khan, J.S. Wei, M. Ringner, L.H. Saal, M. Ladanyi, F. Westermann,
F. Berthold, M. Schwab, C.R. Antonescu, C. Peterson, P.S. Meltzer,
Classification and diagnostic prediction of cancers using gene expression
profiling and artificial neural networks, Nature Medicine 7 (2001) 673–
679.
[17] A. Liaw, M. Wiener, Classification and regression by randomforest, R
News 2 (2002) 18–22.
[18] H. Liu, L. Liu, H. Zhang, Ensemble gene selection for cancer classification, Pattern Recognition 43 (2010) 2763–2772.
[19] S.L. Pomeroy, P. Tamayo, M. Gaasenbeek, L.M. Sturla, M. Angelo,
M.E. Mclaughlin, J.Y.H. Kim, L.C. Goumnerova, P.M. Black, C. Lau,
J.C. Allen, D. Zagzag, J.M. Olson, T. Curran, C. Wetmore, J.A. Biegel,
T. Poggio, S. Mukherjee, R. Rifkin, A. Califano, G. Stolovitzky, D.N.
Louis, J.P. Mesirov, E.S. Lander, T.R. Golub, Prediction of central nervous system embryonal tumour outcome based on gene expression., Nature 415 (2002) 436–442.
[20] S. Ramaswamy, K.N. Ross, E.S. Lander, T.R. Golub, A molecular signature of metastasis in primary solid tumors., Nature genetics 33 (2003)
49–54.
20

[21] D.T. Ross, U. Scherf, M.B. Eisen, C.M. Perou, C. Rees, P. Spellman,
V. Iyer, S.S. Jeffrey, M.V. de Rijn, M. Waltham, A. Pergamenschikov,
J.C. Lee, D. Lashkari, D. Shalon, T.G. Myers, J.N. Weinstein, D. Botstein, P.O. Brown, Systematic variation in gene expression patterns in
human cancer cell lines, Nature Genetics 24 (2000) 227–35.
[22] W. Rudnicki, M. Kursa, Feature selection with the boruta package, Journal of Statistical Software 36 (2010).
[23] R. Ruiz, J. Riquelme, J. Aguilar-Ruiz, Incremental wrapper-based gene
selection from microarray data for cancer classification, Pattern Recognition 39 (2006) 2383–2392.
[24] D. Singh, P.G. Febbo, K. Ross, D.G. Jackson, J. Manola, C. Ladd,
P. Tamayo, A.A. Renshaw, A.V. D’Amico, J.P. Richie, E.S. Lander,
M. Loda, P.W. Kantoff, T.R. Golub, W.R. Sellers, Gene expression
correlates of clinical prostate cancer behavior., Cancer Cell 1 (2002)
203–209.
[25] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal
of the Royal Statistical Society. Series B (Methodological) (1996) 267–
288.
[26] E. Tuv, A. Borisov, G. Runger, K. Torkkola, Feature selection with
ensembles, artificial variables, and redundancy elimination, Journal of
Machine Learning Research 10 (2009) 1341–1366.
[27] R.D. Uriarte, S.A. de Andres, Gene selection and classification of microarray data using random forest, BMC Bioinformatics 7 (2006) 3+.
[28] L.J. van ’t Veer, H. Dai, M.J. van de Vijver, Y.D. He, A.A.M. Hart,
M. Mao, H.L. Peterse, K. van der Kooy, M.J. Marton, A.T. Witteveen,
G.J. Schreiber, R.M. Kerkhoven, C. Roberts, P.S. Linsley, R. Bernards,
S.H. Friend, Gene expression profiling predicts clinical outcome of breast
cancer, Nature 415 (2002) 530–536.
[29] L. Yu, H. Liu, Efficient feature selection via analysis of relevance and
redundancy, Journal of Machine Learning Research 5 (2004) 1205–1224.
[30] Z. Zhu, Y. Ong, M. Dash, Markov blanket-embedded genetic algorithm
for gene selection, Pattern Recognition 40 (2007) 3236–3248.
21

A Time Series Forest for
Classification and Feature Extraction✩

arXiv:1302.2277v2 [cs.LG] 18 Feb 2013

Houtao Deng∗
Intuit, Mountain View, CA, USA

George Runger
Arizona State University, Tempe, AZ, USA

Eugene Tuv, Martyanov Vladimir
Intel, Chandler, AZ, USA

Abstract
A tree-ensemble method, referred to as time series forest (TSF), is proposed
for time series classification. TSF employs a combination of entropy gain
and a distance measure, referred to as the Entrance (entropy and distance)
gain, for evaluating the splits. Experimental studies show that the Entrance
gain improves the accuracy of TSF. TSF randomly samples features at each
tree node and has computational complexity linear in the length of time
series, and can be built using parallel computing techniques. The temporal
importance curve is proposed to capture the temporal characteristics useful
for classification. Experimental studies show that TSF using simple features
such as mean, standard deviation and slope is computationally efficient and
outperforms strong competitors such as one-nearest-neighbor classifiers with
dynamic time warping.
Keywords: decision tree; ensemble; Entrance gain; interpretability; large
margin; time series classification;

Corresponding author: hdeng3@asu.edu
Email addresses: hdeng3@asu.edu (Houtao Deng), george.runger@asu.edu
(George Runger), eugene.tuv@intel.com (Eugene Tuv),
vladimir.martyanov@intel.com (Martyanov Vladimir)
∗

Preprint submitted to Elsevier

February 19, 2013

1. Introduction
Time series classification has been playing an important role in many
disciplines such as finance [25] and medicine [2]. Although one can treat the
value of each time point as a feature and use a regular classifier such as onenearest-neighbor (NN) with Euclidean distance for time series classification,
the classifier may be sensitive to the distortion of the time axis and can lead
to unsatisfactory accuracy performance. One-nearest-neighbor with dynamic
time warping (NNDTW) is robust to the distortion of the time axis and
has proven exceptionally difficult to beat [20]. However, NNDTW provides
limited insights into the temporal characteristics useful for distinguishing
time series from different classes.
The temporal features calculated over time series intervals [15], referred
to as interval features, can capture the temporal characteristics, and can also
handle the distortion in the time axis. For example, in the two-class time
series shown in Figure 1, the time series from one of the classes have sudden
changes between time 201 and time 400 but not in the same time points. An
interval feature such as the standard deviation between time 201 and time
400 is able to distinguish the two-class time series.
Previous work [15] has built decision trees on interval features. However,
a large number of interval features can be extracted from time series, and
there can be a large number of candidate splits to evaluate at each tree
node. Class-based measures (e.g., entropy gain), which evaluate the ability of
separating the classes, are commonly used to select the best split in a node.
However, there can be many splits having the same ability of separating
the classes. Therefore, measures able to further distinguish these splits are
desirable. Also, given a large number of features/splits, an efficient and
accurate classifier that can provide insights into the temporal characteristics
is valuable.
To this end, we propose a novel tree-ensemble classifier: time series forest
(TSF). TSF employs a new measure called the Entrance (entropy and distance) gain to identify high-quality splits. We show that TSF using Entrance
gain outperforms TSF using entropy gain and also two NNDTW algorithms.
By using a random feature sampling strategy, TSF has computational complexity linear in the time series length. Furthermore, we propose the temporal importance curve to capture the temporal characteristics informative for
time series classification.
The remainder of this paper is organized as follows. Section 2 presents the
2

Class 1
Class 2

12
10
8
6
4
2
0
0

200

400

600

800

1000

Figure 1: The time series from class 2 have sudden changes between time 201 and time
400. An interval feature such as the standard deviation between time 201 and time 400
can distinguish the time series from the two classes.

definition of the problem and related work. Section 3 introduces the interval
features. Section 4 describes the TSF method. Section 5 demonstrates the
effectiveness and efficiency of TSF by experiments. Conclusions are drawn
in Section 6.
2. Definition and Related Work
Given N training time series instances (examples): {e1 , ..., ei , ..., eN } and
the corresponding class labels {y1 , ..., yi , ..., yN }, where yi ∈ {1, 2, ..., C}, the
objective of time series classification is to predict the class labels for testing instances. Here we assume the values of time series are measured at
equally-spaced intervals, and also assume the training and testing time series
instances are of the same length M.
Time series classification methods can be divided into instance-based and
feature-based methods. Instance-based classifiers predict a testing instance
based on its similarity to the training instances. Among instance-based classifiers, nearest-neighbor classifiers with Euclidean distance (NNEuclidean)
or dynamic time warping (NNDTW) have been widely and successfully used
3

[12, 21, 8, 24]. Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and
is considered as a strong solution for time series problems [13]. Instancebased classifiers can be accurate, but they provide limited insights into the
temporal characteristics useful for classification.
Feature-based classifiers build models on temporal features, and potentially can be more interpretable than instance-based classifiers. Featurebased classifiers commonly consist of two steps: defining the temporal features and training a classifier based on the temporal features defined. Nanopoulos et al. [11] extracted statistical features such as the mean and deviation of
an entire time series, and then used a multi-layer perceptron neural network
for classification. This method only captured the global properties of time series. Local properties, potentially informative for classification, were ignored.
Geurts [7] extracted local temporal properties after discretizing the time series. Rodrı́guez et al. [15] boosted binary stumps on temporal features from
intervals of the time series and Rodrı́guez and Alonso [14], Rodrı́guez et al.
[16] applied classifiers such as a decision tree and a SVM on the temporal
features extracted from the boosted binary stumps. However, only binary
stumps were boosted, and the effect of using more complex base learners,
such as decision trees, should be studied [15] (but larger tree models impact
the computational complexity). Furthermore, in decision trees [15, 14, 16]
class-based measures are often used to evaluate the candidate splits in a
node. However, the number of candidate splits is generally large, and, thus,
there can be multiple splits having the same ability of separating the classes.
Consequently, additional measures able to further distinguish these features
are desirable. Ye and Keogh [23] briefly discussed strategies of introducing
additional measures to break ties, but it was in a different context.
Recently, Ye and Keogh [23] proposed time series shapelets to perform
interpretable time series classification. Shapelets are time series subsequences
which are in some sense maximally representative of a class [23]. Ye and
Keogh [23], Xing et al. [22], Lines et al. [10] have successfully shown that
time series shapelets can produce highly interpretable results. In term of
accuracy, Lines et al. [10] showed that the shapelet approach is comparable
to NNDTW for nine data sets investigated.
Eruhimov et al. [5] considered a massive number of features. The feature
sets were derived from statistical moments, wavelets, Chebyshev coefficients,
PCA coefficients, and the original values of time series. The method can
be accurate, but is hard to interpret and computationally expensive. The
4

objective of our work is to produce an effective and efficient classifier that
uses/yields a set of simple features that can contribute to the domain knowledge. For example, in manufacturing applications, specific properties of the
time series signals that discriminate conforming from un-conforming products are invaluable to diagnose, correct, and improve processes.
3. Interval Features
Interval features are calculated from a time series interval, e.g., “the interval between time 10 and time 30”. Many types of features over a time
interval can be considered, but one may prefer simple and interpretable features such as the mean and standard deviation, e.g., “the average of the time
series segment between time 10 and time 30”.
Let K be the number of feature types and fk (·) (k = 1, 2, ..., K) be the k th
type. Here we consider three types: f1 = mean, f2 = standard deviation,
f3 = slope. Let fk (t1 , t2 ) for 1 ≤ t1 ≤ t2 ≤ M denote the k th interval feature
calculated over the interval between t1 and t2 . Let vi be the value at time i
for a time series example. Then the three interval features for the example
are calculated as follows:
Pt2
i=t1 vi
f1 (t1 , t2 ) =
(1)
t2 − t1 + 1
r P
t2
2

i=t1 (vi −f1 (t1 ,t2 ))
t2 > t1
t2 −t1
(2)
f2 (t1 , t2 ) =

0
t2 = t1
(
β̂ t2 > t1
f3 (t1 , t2 ) =
(3)
0 t2 = t1
where β̂ is the slope of the least squares regression line of the training set
{(t1 , vt1 ), (t1 + 1, vt1 +1 ). . . . , (t2 , vt2 )}.
Interval features have been shown to be effective for time series classification [15, 14, 16]. However, the interval feature space is large (O(M 2 )).
Rodrı́guez et al. [15] considered using only intervals of lengths equal to powers of two, and, therefore, reduced the feature space to O(M log M). Here
we consider the random sampling strategy used in a random forest [1] that
reduces the feature space to O(M) at each tree node.

5

4. Time Series Forest Classifier
4.1. Splitting criterion
A time series tree is the base component of a time series forest, and the
splitting criterion is used to determine the best way to split a node in a tree.
A candidate split S in a time series tree node tests the following condition
(for simplicity and without loss of generality, we assume the root node here):
fk (t1 , t2 ) ≤ τ

(4)

for a threshold τ . The instances satisfying the condition are sent to the left
child node. Otherwise, the instances are sent to the right child node.
Let {fkn (t1 , t2 ), n ∈ 1, 2, ..., N} denote the set of values of fk (t1 , t2 ) for all
training instances at the node. To obtain a good threshold τ in equation 4,
one can sort the feature values of all the training instances and then select the
best threshold from the midpoints between pairs of consecutive values, but
this can be too costly [14]. We consider the strategy employed in Rodrı́guez
and Alonso [14]. The candidate thresholds for a particular type feature fk
n
n
N
are formed such that the range of [minN
n=1 (fk (t1 , t2 )), maxn=1 (fk (t1 , t2 )] is
divided into equal-width intervals. The number of candidate thresholds is
denoted as κ and is fixed, e.g., 20. The best threshold is then selected from
the candidate thresholds. In this manner, sorting is avoided, and only κ tests
are needed.
Furthermore, a splitting criterion is needed to define the best split S ∗ :
f∗ (t∗1 , t∗2 ) ≤ τ ∗ . We employ a combination of entropy gain and a distance
measure as the splitting criterion. Entropy gain are commonly used as the
splitting criterion in tree models. Denote the proportions of instances corresponding to classes {1, 2, ..., C} at a tree node as {γ1 , γ2, ..., γC }, respectively.
The entropy at the node is defined as
Entropy = −ΣC
c=1 γc log γc

(5)

The entropy gain △Entropy for a split is then the difference between the
weighted sum of entropy at the child nodes and the entropy at the parent
node, where the weight at a child node is the proportion of instances assigned
to that child node.
△Entropy evaluates the usefulness of separating the classes. However,
in time series classification, the number of candidate splits can be large,
and there are often cases where multiple candidate splits have the same
6

△Entropy. Therefore we consider an additional measure called Margin,
which calculates the distance between a candidate threshold and its nearest
feature value. The Margin of split fk (t1 , t2 ) ≤ τ is calculated as
Margin =

min

n=1,2,...,N

|fkn (t1 , t2 ) − τ |

(6)

where fkn (t1 , t2 ) is the value of fk (t1 , t2 ) for the nth instance at the node. A
new splitting criterion E, referred to as the Entrance (entropy and distance)
gain, is defined as a combination of △Entropy and Margin.
E = △Entropy + α · Margin

(7)

where α is small enough so that the only role for α in the model is to break
ties that can occur from the entropy gain alone. Alternatively, one can store
the values of △Entropy and Margin for a split, and use Margin to break
ties when another split has the same △Entropy.
Clearly, the split with the maximum E should be selected to split the
node. Furthermore, Margin and E are sensitive to the scale of the features,
and we employ the following strategy if different types of features have different scales. For each feature type fk , select the split with the maximum
Entrance gain. To compare the best splits from different feature types, the
split with the maximum △Entropy is selected. If the best splits from different feature types have the same maximum △Entropy, one of the best splits
is randomly selected.
f k (t 1 , t 2)
S1

S2

S3

Figure 2: Here the x-axis represents the value of an interval feature. The figure shows
six instances associated with three classes (blue, red, and green), and three splits (S1 , S2 ,
and S3 ) producing the same entropy gain. The Entrance gain E is able to select S3 as the
best split.

Figure 2 illustrates the intuition behind the criterion E. The figure shows,
in one dimension, six instances from three classes in different symbols/colors.
Three candidate splits S1 , S2 and S3 are also shown in the figure. Clearly,
all splits have the same △Entropy, but one may prefer S3 because S3 has a
larger margin than S1 and S2 . The Entrance gain is able to choose S3 as the
best split.
7

Algorithm 1 sample() function: randomly samples a set of intervals <
T1 , T2 >, where T1 is the set of starting time points of intervals, and T2 is
the set of ending points. The function RandSampNoRep(set, samplesize)
randomly selects samplesize elements from set without replacement.
T1 = ∅, T2 = ∅
√
W = RandSampN oRep({1, ..., M }, M )
for w in set W do
√
T1 = RandSampN oRep({1, ..., M − w + 1}, M − w + 1)
for t1 in set
S T1 do
T2 = T2 (t1 + w − 1)
end for
end for
return < T1 , T2 >

Algorithm 2 tree(data): Time series tree. For simplicity of the algorithm,
we assume different types of features are on the same scale so that E can be
compared.
< T1 , T2 >=sample()
calculate T hresholdk , the set of candidate thresholds for each feature type k
E ∗ = 0, △Entropy ∗ = 0, t∗1 = 0, t2 ∗ = 0, τ ∗ = 0, f∗ = ∅
for < t1 , t2 > in set < T1 , T2 > do
for k in 1:K do
for τ in T hresholdk do
calculate △Entropy and E for fk (t1 , t2 ) ≤ τ
if E > E ∗ then
E ∗ = E, △Entropy ∗ = △Entropy, t∗1 = t1 , t∗2 = t2 , τ ∗ = τ , f∗ = fk
end if
end for
end for
end for
if △Entropy ∗ = 0 then
label this node as a leaf and return
end if
datalef t ← time series with f∗ (t∗1 , t∗2 ) ≤ τ ∗
dataright ← time series with f∗ (t∗1 , t∗2 ) > τ ∗
tree(datalef t )
tree(dataright )

8

4.2. Time Series Tree and Time Series Forest
The construction of a time series tree follows a top-down, recursive strategy similar to standard decision tree algorithms, but uses the Entrance gain
as the splitting criterion. Furthermore, the random sampling strategy employed in random forest (RF) [1] is considered here. At each node, RF only
√
tests p features randomly sampled from the complete feature set consisting
of p√features. In each time series
√ tree node, we consider randomly sampling
O( M ) interval sizes and O( M) starting positions. Therefore, the feature
space is reduced to only O(M). The sampling algorithm is illustrated in
Algorithm 1.
The time series tree algorithm is shown in Algorithm 2. For simplicity,
we assume different types of features are on the same scale so that E can
be compared. If different types of features have different scales, the previous
mentioned strategy can be used, that is, for each feature type fk , select the
split with the maximum Entrance gain. To compare the best splits from
different feature types, the split with the maximum △Entropy is selected.
Furthermore, a node is labeled as a leaf if there is no improvement on the
entropy gain (e.g. all features have the same value or all instances belong to
the same class).
A time series forest (TSF) is a collection of time series trees. A TSF
predicts a testing instance to be the majority class according to the votes
from all time series trees.
4.3. Computational Complexity
Let nij denote the number of instances in the j th node at the ith depth in
a time series tree. At each node, calculating the splitting criterion of a single
interval feature has complexity O(nij κ), where κ is the number of candidate
thresholds. As O(M) interval features are randomly selected for evaluation,
the complexity for evaluating the features at a node is O(nij Mκ). As κ is
considered as a constant, the complexity at a node is O(nij M).
P
The total number of instances at each depth is at most N (i.e., j nij ≤
P
N). Therefore, at the ith depth in the tree, the complexity is O( j nij M ) ≤
O(NM). Assuming the maximum depth of a tree model is O(log N) [19], the
complexity of a time series tree becomes O(MN log N). Therefore, the complexity of a TSF with nT ree time series trees is at most O(nT reeMN log N),
linear in the length of time series.

9

4.4. Temporal Importance Curve
TSF consists of multiple trees and is difficult to understand. Here we
propose the temporal importance curve to provide insights into time series
classification. At each node of TSF, the entropy gain can be calculated for
the interval feature used for splitting. For a time index in the time series, one
can add the entropy gain of all the splits associated with the time index for
a particular type of feature. That is, for a feature type fk , the importance
score for time index t can be calculated as
X
Impk (t) =
△Entropy(fk (t1 , t2 ), ν)
(8)
t1 ≤t≤t2 ,ν∈SN

where SN is the set of split nodes in TSF, and △Entropy(fk (t1 , t2 ), ν) is the
entropy gain for feature fk (t1 , t2 ) at node ν. Note △Entropy(fk (t1 , t2 ), ν) = 0
if fk (t1 , t2 ) is not used for splitting node ν. Furthermore, one temporal
importance curve is generated for each feature type. Consequently, for the
mean, standard deviation and slope features, we calculate the mean, standard
deviation, and slope temporal importance curves, respectively.
To investigate the temporal importance curve, we simulated two data sets,
each with 1000 time points and two classes. For the first data set the time
series have the same distribution so that no feature is useful for separating
the classes. The time series values from both classes are normally distributed
with zero mean and unit variance. The time series and the importance curves
from TSF using Entrance gain are shown in Figure 3(a). It can be seen that
all curves have larger values in the middle.
Note that the number of intervals that include time index t in a time
series is
Num(t) = t(M − t + 1)
(9)
Consequently, different time indices are associated with different numbers of
intervals. The number of intervals for each time index for time series with
1000 time points is plotted in Figure 3(b). The indices in the middle have
more intervals than the indices on the edges of the time series. Because
Impk (t) is calculated by adding the entropy gain of all the splits associated
with time index t for feature fk , it can be biased towards the time points
having more interval features (particularly if no feature is important for classification).
For the second data set the time series from the two classes have different
means in interval [201, 250], and different standard deviations in interval
10

5

3

15
class 1
class 2

10

2.5

5
0
−5
0

2
200

400

600

800

1000

1.5

Mean
Std. Dev.
Slope

400

1

200
0
0

x 10

0.5

200

400

600

800

0
0

1000

200

400

600

800

1000

(a) The time series data and the importance (b) The number of intervals associated
curves from TSF.
with each time index. The time indices in
the middle are contained in more intervals.
Figure 3: When no feature is important for classification, the curves may be expected to
have larger values for the middle indices as there are more intervals associated with the
middle indices.

10

10

class 1
class 2

0

−10
0

0

200

400

600

800

−10
0

1000

200

200

400

600

200
Mean
Std. Dev.
Slope

100

0
0

class 1
class 2

200

400

600

800

0
0

1000

Mean
Std. Dev.
Slope

100

1000

800

200

400

600

800

1000

(a) The time series and the temporal im- (b) The time series and the temporal importance curves obtained from TSF using portance curves obtained from TSF using
Entrance gain.
entropy gain.
Figure 4: The time series from the two classes differ in the mean in interval [201, 250],
and differ in the standard deviation in interval [501, 550]. The importance curves from
TSF using Entrance gain are able to capture the informative intervals well. The curves
from TSF using entropy gain have peaks in interval [201, 250], but have long tails.

11

[501, 550]. The temporal importance curves from TSF using Entrance gain
are shown in Figure 4(a). The curves for the mean and slope have peaks in
interval [201, 250], and the curve for the standard deviation has a peak in
interval [501, 550]. Therefore, these curves capture the important temporal
characteristics.
We also built TSF using entropy gain, and the corresponding temporal
importance curves are shown in Figure 4(b). Although the curves also have
peaks in interval [201, 250], the curves have long tails. Indeed, the entropy
gain is not able to distinguish many interval features. For example, the mean
feature for interval [201,250], and the mean feature for interval [201,400] have
the same entropy gain as both can distinguish the two classes of time series.
However, the mean feature for interval [201,250] has a larger E than the
mean feature for interval [201,400]. Consequently, TSF using Entrance gain
is able to capture the temporal characteristics more accurately.
5. Experiments
5.1. Experimental Setup
The main functions of the TSF algorithm were implemented in Matlab,
while computationally expensive subfunctions such as interval feature calculations were written in C. The parameters were set as follows: the number
of trees = 500, f (·) = {mean, standard deviation, slope}, and the number
of candidate thresholds κ = 20. TSF was applied to a set of time series
benchmark data sets [9] summarized in Table 1. The training/testing split
setting is the same as in Keogh et al. [9]. The experiments were run on a
computer with four cores and the TSF algorithm was built in parallel.
The purpose of the experiments is to answer the following questions: (1)
Does the Entrance gain criterion improve the accuracy performance and how
is the accuracy performance of TSF compared to other time series classifiers? (2) Is TSF computationally efficient? (3) Can the temporal importance curves provide some insights about the temporal characteristics useful
for classification?
5.2. Results
We investigated the performance of TSF using the Entrance gain criterion
(denoted as TSF) and using the original entropy gain criterion (denoted
as TSF-entropy), respectively. We also considered alternative classifiers for
comparison: random forest [1] applied to the interval features with sizes
12

50words
Adiac
Beef
CBF
ChlorineConcentration
CinC ECG torso
Coffee
Cricket X
Cricket Y
Cricket Z
DiatomSizeReduction
ECG200
ECGFiveDays
FaceAll
FaceFour
FacesUCR
Fish
GunPoint
Haptics
InlineSkate
ItalyPowerDemand
Lighting2
Lighting7
MALLAT
MedicalImages
MoteStrain
NonInvasiveFatalECG Thorax1
NonInvasiveFatalECG Thorax2
OliveOil
OSULeaf
SonyAIBORobotSurface
SonyAIBORobotSurfaceII
StarLightCurves
SwedishLeaf
Symbols
Syntheticcontrol
Trace
TwoLeadECG
TwoPatterns
uWaveGestureLibrary X
uWaveGestureLibrary Y
uWaveGestureLibrary Z
Wafer
WordsSynonyms
Yoga

Length
270
176
470
128
166
1639
286
300
300
300
345
96
136
131
350
131
463
150
1092
1882
24
637
319
1024
99
84
750
750
570
427
70
65
1024
128
398
60
275
82
128
315
315
315
152
270
426

Training
instances
450
390
30
30
467
40
28
390
390
390
16
100
23
560
24
200
175
50
155
100
67
60
70
55
381
20
1800
1800
30
200
20
27
1000
500
25
300
100
23
1000
896
896
896
1000
267
300

Testing
instances
455
391
30
900
3840
1380
28
390
390
390
306
100
861
1690
88
2050
175
150
308
550
1029
61
73
2345
760
1252
1965
1965
30
242
601
953
8236
625
995
300
100
1139
4000
3582
3582
3582
6164
638
3000

Classes
50
37
5
3
3
4
2
12
12
12
4
2
2
14
4
14
7
2
5
7
2
2
7
8
10
2
42
42
4
6
2
2
3
15
6
6
4
2
4
8
8
8
2
25
2

Table 1: Summary of the time series data sets: the number of training and testing instances, the number of classes and the lengths of the time series.

power of two (interRF), the 1-nearest-neighbor (NN) classifier with Euclidean
distance (NNEuclidean), the 1-NN Best warping window DTW (DTWBest)
13

50words
Adiac
Beef
CBF
ChlorineConcentration
CinC ECG torso
Coffee
Cricket X
Cricket Y
Cricket Z
DiatomSizeReduction
ECG200
ECGFiveDays
FaceAll
FaceFour
FacesUCR
Fish
GunPoint
Haptics
InlineSkate
ItalyPowerDemand
Lighting2
Lighting7
MALLAT
MedicalImages
MoteStrain
NonInvasiveFatalECG Thorax1
NonInvasiveFatalECG Thorax2
OliveOil
OSULeaf
SonyAIBORobotSurface
SonyAIBORobotSurfaceII
StarLightCurves
SwedishLeaf
Symbols
Syntheticcontrol
Trace
TwoLeadECG
TwoPatterns
uWaveGestureLibrary X
uWaveGestureLibrary Y
uWaveGestureLibrary Z
Wafer
WordsSynonyms
Yoga
win/lose/tie
Average rank
Rank difference
Wilcoxon

TSF
Entrance
0.2659
0.2302
0.2333
0.0256
0.2537
0.0391
0.0357
0.2897
0.2000
0.2436
0.0490
0.0800
0.0557
0.2325
0.0227
0.1010
0.1543
0.0467
0.5520
0.6818
0.0301
0.1803
0.2603
0.0448
0.2237
0.1190
0.0987
0.0865
0.0667
0.4339
0.2330
0.1868
0.0357
0.1056
0.1116
0.0267
0.0200
0.1177
0.0543
0.2102
0.2876
0.2624
0.0054
0.3793
0.1513
2.48
-

TSF
entropy
0.2769
0.2609
0.3000
0.0389
0.2596
0.0688
0.0714
0.2872
0.2000
0.2385
0.1013
0.0700
0.0697
0.2314
0.0341
0.1088
0.1543
0.0467
0.5649
0.6746
0.0330
0.1803
0.2603
0.0716
0.2316
0.1182
0.1033
0.0936
0.1000
0.4256
0.2346
0.1773
0.0364
0.1088
0.1206
0.0233
0.0000
0.1115
0.0530
0.2127
0.2881
0.2669
0.0047
0.3809
0.1567
16/28/1
2.86
0.38
0.007

interRF
0.2989
0.2506
0.3000
0.0411
0.2273
0.1065
0.0000
0.3128
0.2436
0.2436
0.0980
0.1700
0.1231
0.2497
0.0568
0.1283
0.1486
0.0400
0.5487
0.6873
0.0321
0.2459
0.2740
0.0644
0.2658
0.0942
0.1104
0.0875
0.1333
0.4587
0.2562
0.2067
0.0327
0.0768
0.1216
0.0167
0.0400
0.1773
0.0153
0.2094
0.3023
0.2764
0.0071
0.4138
0.1380
13/32/0
3.43
0.96
0.000

NN
Euclidean
0.3690
0.3890
0.4670
0.1480
0.3500
0.1030
0.2500
0.4260
0.3560
0.3800
0.0650
0.1200
0.2030
0.2860
0.2160
0.2310
0.2170
0.0870
0.6300
0.6580
0.0450
0.2460
0.4250
0.0860
0.3160
0.1210
0.1710
0.1200
0.1330
0.4830
0.1410
0.3050
0.1510
0.2130
0.1000
0.1200
0.2400
0.2530
0.0900
0.2610
0.3380
0.3500
0.0050
0.3820
0.1700
4/41/0
5.04
2.57
0.000

DTW
Best
0.2420
0.3910
0.4670
0.0040
0.3500
0.0700
0.1790
0.2360
0.1970
0.1800
0.0650
0.1200
0.2030
0.1920
0.1140
0.0880
0.1600
0.0870
0.5880
0.6130
0.0450
0.1310
0.2880
0.0860
0.2530
0.1340
0.1850
0.1290
0.1670
0.3840
0.1410
0.3050
0.0950
0.1570
0.0620
0.0170
0.0100
0.1320
0.0015
0.2270
0.3010
0.3220
0.0050
0.2520
0.1550
17/28/0
3.31
0.83
0.065

DTW
NoWin
0.3100
0.3960
0.5000
0.0030
0.3520
0.3490
0.1790
0.2230
0.2080
0.2080
0.0330
0.2300
0.2320
0.1920
0.1700
0.0951
0.1670
0.0930
0.6230
0.6160
0.0500
0.1310
0.2740
0.0660
0.2630
0.1650
0.2090
0.1350
0.1330
0.4090
0.1690
0.2750
0.0930
0.2100
0.0500
0.0070
0.0000
0.0960
0.0000
0.2730
0.3660
0.3420
0.0200
0.3510
0.1640
16/29/0
3.88
1.40
0.006

Table 2: The error rates of TSF using the splitting criterion: Entrance gain (TSF) or
entropy gain (TSF-entropy), random forest with 500 trees applied to the interval features
with sizes power of two (interRF), 1-NN with Euclidean distance (NNEuclidean), 1-NN
with the best warping window DTW (DTWBest) [12], and 1-NN DTW with no warping
window (DTWNoWin). The win-lose-tie results of each competitor compared to TSF,
the average rank of each classifier, the rank difference and the Wilcoxon signed ranks test
between TSF and each competitor are also calculated. When multiple methods have the
same error rate for a data set, the average rank is used. For example, both DTWBest and
DTWNoWin have the minimum error rate 0.192 for the FaceAll data set, and, thus, the
rank for both is 1.5.

[12] and the 1-NN DTW with no warping window (DTWNoWin) methods

14

acquired directly from Keogh et al. [9]. DTWBest has a fixed window limiting
the window width and searches for the best window size, while DTWNoWin
does not use such a window.
The classification error rates are shown in Table 2. To compare multiple
classifiers to TSF over multiple data sets, we used the procedure for comparing multiple classifiers with a control over multiple data sets suggested by
Demšar [3], i.e., the Friedman test [6] followed by the Bonferroni-Dunn test
[4] if the Friedman test shows a significant difference between the classifiers.
In our case, the Friedman test shows that there is a significant difference
between the six classifiers at the 0.001 level. Therefore, we proceeded with
the Bonferroni-Dunn test.
For the Bonferroni-Dunn test, the performance of two classifiers is different at the α level if the their average ranks differ by at least the critical
difference (CD):
s
zα = qα

Nclassif ier (Nclassif ier + 1)
6Ndata

(10)

where Nclassif ier is the number of classifiers in the comparison (six classifiers
in our experiments), Ndata is the number of data sets (45 data sets in our
experiments), and qα is the critical value for the two-tailed Bonferroni-Dunn
test for multiple classifier comparison with a control. Note q0.05 = 2.576
and q0.1 = 2.326 (Table 5(b) in Demšar [3]), then according to Equation 10,
z0.05 = 1.016 and z0.1 = 0.917. The average rank of each classifier, and the
difference between the average ranks of TSF and each competitor are shown
in Table 2. According to the rank difference, there is a significant difference
between TSF and competitors NNEuclidean, DTWNoWin and interRF at
the 0.1 level.
In addition to the multi-classifier comparison procedure, we also considered Wilcoxon signed ranks test [18] suggested for comparing a pair of
classifiers, as the resolution for the multi-classifier comparison procedure can
be too low to distinguish two classifiers with significantly different performance, but with close average ranks. For example, for six classifiers and 45
data sets, assume classifier A always ranks the first and classifier B always
ranks the second. Although classifier A is always better than classifier B, the
average ranks of classifier A and classifier B differ by only one, and therefore
there is no significant difference between the two classifiers at the 0.05 level
according to the two-tailed Bonferroni-Dunn test.
The p-values of the Wilcoxon signed ranks tests between TSF and each
15

competitor are shown in Table 2. It can be seen there is a significant difference between TSF and all other competitors: TSF-entropy, interRF, NNEuclidean, DTWNoWin and DTWBest at the 0.1 level.
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

100

200

300

400

500

Figure 5: Plot of the error rate of each data set versus the number of trees in TSF, and the
average error rate over all data sets versus the number of trees (represented by the thicker
red line). We want to show the trend so different data sets are not distinguished. The
error rates tend to decrease as the number of trees increases, but the change is relatively
small for most data sets after 100 trees.

Next consider the robustness of TSF accuracy to the number of trees.
Figure 5 shows the error rate of each data set versus the number of trees,
and the average error rate over all data sets versus the number of trees
(represented by the thicker red line). The error rates tend to decrease as the
number of trees increases, but the change is relatively small for most data
sets after 100 trees.
The GunPoint and Wafter time series and their corresponding temporal
importance curves (mean, standard deviation and slope) are shown in Figure 6. For the GunPoint time series, the mean temporal importance curve
captures the characteristic that the two classes have different means in interval [60,100]. The standard deviation and slope temporal importance curves,
respectively, capture the characteristics that the two classes have different
standard deviations and slopes in the left and right sides of the time se16

3

12
class 1
class 2

2

class 1
class 2

10
8

1

6

0

4
2

−1

0
−2
−3

−2
20

40

60

80

100

120

−4

140

(a) GunPoint time series

20

40

60

80

100

120

140

(b) Wafer time series

200

300
Mean
Std. Dev.
Slope

250

150

Mean
Std. Dev.
Slope

200
100

150
100

50
50
0

20

40

60

80

100

120

0

140

20

40

60

80

100

120

140

(c) The temporal importance curves for (d) The temporal importance curves for the
the GunPoint data.
Wafer data.
Figure 6: The time series and the temporal importance curves (mean, standard deviation
and slope) for the GunPoint data set and the Wafer data set, respectively.

17

ries. For the Wafer time series, the standard deviation temporal importance
curve captures the sudden changes of the time series of class 1 near the 100th
point. Consequently, the temporal importance curve is able to provide insights into the temporal characteristics useful for distinguishing time series
from different classes.
5.3. Computational Complexity
First consider the computational complexity of TSF with regard to the
length of time series. We selected the data sets with more than 1000 time
points. For each data set, λM of the time points were randomly sampled,
where M is the length of the time series, and λ is a multiplier. The computational times for different values of λ are shown in Figure 7(a). Next consider
the computational complexity of TSF with regard to the number of training
instances. Data sets with more than 1000 training instances were selected.
For each data set, λN of the time points were randomly sampled, where N
is the number of training instances. The computational times for different
values of λ are shown in Figure 7(b). It can be seen that the computational
time tends to be linear both in the time series length and in the number of
training instances.
Therefore, TSF is a computationally efficient classifier for time series.
Furthermore, in the current TSF implementation, the interval features are
dynamically calculated at each node, as pre-computing the interval features
would need O(M 2 ) features to be stored. It should noted, however, dynamic
calculation can lead to repeated calculations of the interval features. Therefore, the implementation can be further improved by storing the interval
features already calculated to avoid repeated calculations.
6. Conclusions
Both high accuracy and interpretability are desirable for classifiers. Previous classifiers such as NNDTW can be accurate, but provide limited insights
into the temporal characteristics. Interval features can be used to capture
temporal characteristics, however, the huge feature space can result in many
splits having the same entropy gain. Furthermore, the computational complexity becomes a concern when the feature space becomes large.
Time series forest (TSF) proposed here addresses the challenges by using
the following two strategies. Firstly, TSF uses a new splitting criterion named
Entrance gain that combines the entropy gain and a distance measure to
18

1000

4000
CinC_ECG_torso
Haptics
InlineSkate
MALLAT
StarLightCurves

3500
Running Time (seconds)

Running Time (seconds)

1500

500

3000
2500

Thorax1
Thorax2
StarLightCurves
TwoPatterns
wafer

2000
1500
1000
500

0
0

0.2
0.4
0.6
0.8
Multiplier of the time series length

0
0

1

0.2
0.4
0.6
0.8
Multiplier of the number of examples

1

(a) The computational time of TSF with (b) The computational time of TSF with
regard to the time series length
regard to the number of training instances
Figure 7: The computational time of TSF with regard to the time series length and the
number of training instances, respectively. Data sets with more than 1000 time points and
1000 training instances were selected, respectively. The computational time tends to be
linear both in the time series length and in the number of training instances.

identify high-quality splits. Experimental studies on 45 benchmark data
sets show that the Entrance gain improves the accuracy of TSF. Secondly,
TSF randomly samples O(M) features from O(M 2 ) features, and thus makes
the computational complexity linear in the time series length. In addition,
each tree in TSF is grown independently, and, therefore, modern parallel
computing techniques can be leveraged to speed up TSF.
TSF is an ensemble of trees and is not easy to understand. However, we
propose the temporal importance curve, calculated from TSF, to capture the
informative interval features. The temporal importance curve enables one to
identify the important temporal characteristics.
TSF uses simple summary statistical features, but outperforms widely
used alternatives. More complex features, such as wavelets, can be also used
in the framework of TSF, which potentially can further improve the accuracy
performance, but at the cost of interpretability.
In summary, TSF is an accurate, efficient time series classifier, and is able
to provide insights on the temporal characteristics useful for distinguishing
time series from different classes. We also note that TSF assumes that the
time series are of the same length. Given a set of time series with different
lengths, techniques such as dynamic time warping can be used to align the
time series into the same length. Still, directly handling time series with
varying lengths would make TSF more convenient to use, and future work
19

includes such an extension.
Acknowledgements
This research was partially supported by ONR grant N00014-09-1-0656.
We also wish to thank the editor and anonymous reviewers for their valuable
comments.
References
[1] L. Breiman, Random forests, Machine Learning 45 (2001) 5–32.
[2] I. Costa, A. Schönhuth, C. Hafemeister, A. Schliep, Constrained mixture
estimation for analysis and robust classification of clinical time series,
Bioinformatics 25 (2009) i6–i14.
[3] J. Demšar, Statistical comparisons of classifiers over multiple data sets,
The Journal of Machine Learning Research 7 (2006) 1–30.
[4] O. Dunn, Multiple comparisons among means, Journal of the American
Statistical Association (1961) 52–64.
[5] V. Eruhimov, V. Martyanov, E. Tuv, Constructing high dimensional
feature space for time series classification, in: Proceedings of the 11th
European conference on Principles and Practice of Knowledge Discovery
in Databases (PKDD), Springer, 2007, pp. 414–421.
[6] M. Friedman, A comparison of alternative tests of significance for the
problem of m rankings, The Annals of Mathematical Statistics 11 (1940)
86–92.
[7] P. Geurts, Pattern extraction for time series classification, in: Proceedings of the 5th European conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Springer, 2001, pp. 115–127.
[8] Y. Jeong, M. Jeong, O. Omitaomu, Weighted dynamic time warping for
time series classification, Pattern Recognition 44 (2011) 2231–2240.
[9] E. Keogh,
X. Xi,
L. Wei,
C. Ratanamahatana,
The
ucr
time
series
classification/clustering.
homepage:
www.cs.ucr.edu/e eamonn/time series data/, 2006.
20

[10] J. Lines, L.M. Davis, J. Hills, A. Bagnall, A shapelet transform for time
series classification, in: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),
ACM, 2012, pp. 289–297.
[11] A. Nanopoulos, R. Alcock, Y. Manolopoulos, Feature-based classification of time-series data, International Journal of Computer Research 10
(2001) 49–61.
[12] C. Ratanamahatana, E. Keogh, Making time-series classification more
accurate using learned constraints, in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM, 2004, pp. 11–22.
[13] C. Ratanamahatana, E. Keogh, Three myths about dynamic time warping data mining, in: Proceedings of SIAM International Conference on
Data Mining (SDM), SIAM, 2005, pp. 506–510.
[14] J. Rodrı́guez, C. Alonso, Interval and dynamic time warping-based decision trees, in: Proceedings of the 2004 ACM symposium on Applied
computing, ACM, 2004, pp. 548–552.
[15] J. Rodrı́guez, C. Alonso, H. Boström, Boosting interval based literals,
Intelligent Data Analysis 5 (2001) 245–262.
[16] J. Rodrı́guez, C. Alonso, J. Maestro, Support vector machines of
interval-based features for time series classification, Knowledge-Based
Systems 18 (2005) 171–178.
[17] H. Sakoe, Dynamic programming algorithm optimization for spoken
word recognition, IEEE Transactions on Acoustics, Speech, and Signal
Processing 26 (1978) 43–49.
[18] F. Wilcoxon, Individual comparisons by ranking methods, Biometrics
Bulletin 1 (1945) 80–83.
[19] I. Witten, E. Frank, Data Mining: Practical Machine Learning Tools
and Techniques, Morgan Kaufmann, 2005.
[20] X. Xi, E. Keogh, C. Shelton, L. Wei, C.A. Ratanamahatana, Fast time
series classification using numerosity reduction, in: Proceedings of the
23rd international conference on Machine learning (ICML), ACM, 2006,
pp. 1033–1040.
21

[21] Z. Xing, J. Pei, P. Yu, Early prediction on time series: a nearest neighbor
approach, in: Proceedings of the 21st International Joint Conference on
Artifical Intelligence (IJCAI), Morgan Kaufmann, 2009, pp. 1297–1302.
[22] Z. Xing, J. Pei, P.S. Yu, K. Wang, Extracting interpretable features for
early classification on time series, in: Proceedings of SIAM International
Conference on Data Mining (SDM), SIAM, 2011, pp. 247–258.
[23] L. Ye, E. Keogh, Time series shapelets: a new primitive for data mining,
in: Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD), ACM, 2009, pp. 947–956.
[24] D. Yu, X. Yu, Q. Hu, J. Liu, A. Wu, Dynamic time warping constraint
learning for large margin nearest neighbor classification, Information
Sciences 181 (2011) 2787–2796.
[25] Z. Zeng, H. Yan, Supervised classification of share price trends, Information Sciences 178 (2008) 3943–3956.

22

454

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

See the Forest Before the Trees:
Fine-Tuned Learning and Its Application
to the Traveling Salesman Problem
Steven P. Coy, Bruce L. Golden, George C. Runger, and Edward A. Wasil
Abstract— In this paper, we introduce the concept of finetuned learning which relies on the notion of data approximation
followed by sequential data refinement. We seek to determine
whether fine-tuned learning is a viable approach to use when
trying to solve combinatorial optimization problems. In particular, we conduct an extensive computational experiment to study
the performance of fine-tuned-learning-based heuristics for the
traveling salesman problem (TSP). We provide important insight
that reveals how fine-tuned learning works and why it works well,
and conclude that it is a meritorious concept that deserves serious
consideration by researchers solving difficult problems.

I. INTRODUCTION

W

HEN we attempt to solve a complex problem with
abundant information, we typically evaluate the general
characteristics of the information in order to understand the big
picture first, before evaluating individual pieces of data. Consider how we perceive an object that is some distance away.
At first, only the general shape is discernible. Distance and a
variety of visual distractions prevent us from making positive
identification. Yet, even at a distance, we can learn something
about the object. We may know the approximate size, shape,
and color. We may also know whether the object is stationary
or mobile, living or inanimate. If we attempt to guess what
the object is at a distance, we are likely to be wrong, but
we have narrowed the range of possibilities. As the distance
decreases, more of the details become visible. Eventually, all
distortions and distractions have been removed, and we can
accurately identify the object. This form of perception, which
relies on the notion of approximation followed by sequential
refinement, is what we call fine-tuned learning.
There are many examples of complex problems that are
solved by focusing on the forest before the trees. In part, this is
due to the inability of humans to process large amounts of data
at a time. The need to structure a complex problem in order
to understand it is a natural human tendency. On the other
hand, since computers can process vast amounts of data at a
time, it is not surprising that computer algorithms are typically

Manuscript received August 27, 1996, revised December 7, 1997.
S. P. Coy and B. L. Golden are with the College of Business and
Management, University of Maryland, College Park, MD 20742 USA (e-mail:
bgolden@umdacc.umd.edu).
G. C. Runger is with the College of Engineering and Applied Sciences,
Arizona State University, Tempe, AZ 85287 USA.
E. A. Wasil is with the Kogod College of Business Administration,
American University, Washington, DC 20016 USA.
Publisher Item Identifier S 1083-4427(98)04347-1.

not implemented with fine-tuned learning in mind (exceptions
would include state-of-the-art computer chess programs).
Our interest in fine-tuned learning stems from an application
in which the concept was used successfully to train a neural
network model. In this application, Sun et al. [13] built a neural
network model to predict the quality of wire bonds that connect
two surfaces found in microcircuits. They gathered data on
16 different predictor variables (these correspond to the input
nodes in their neural network) for a set of wire bonds and
then built a neural network model to predict the quality of the
wire bond (that is, the force at which a wire bond broke was
the response variable and this corresponded to the output node
in their neural network model). Thus, each wire bond had 17
measurements associated with it. Sun et al. gathered data for
214 wires on which they trained their neural network.
Sun et al. trained their neural network in two ways. The
first was quite standard: it used all of the original data (that is,
the 17 measurements for each of the 214 wire bonds) to train
the network in one pass using the backpropagation method.
The second way used fine-tuned learning in several sequential
rounds of training using the backpropagation method. In the
first round, the network was trained using averaged data (that
is, they split the 214 wires into 16 groups of roughly 13 wires
each, averaged the values for the predictor and response variables within each group, and trained the neural network on the
16 groups). In the second round of training, the network was
trained using the averaged data for 31 groups of about seven
wires each. Training continued for three more rounds with the
last round consisting of the original data on all 214 wire bonds.
It is well known that the backpropagation method has
limitations. The gradient descent procedure that is used in
backpropagation is a local search heuristic that is prone to
early convergence to a local optimum. Training with backpropagation can be very time-consuming. Sun et al. proposed the
fine-tuned learning enhancement as a way of overcoming early
convergence and reducing training time. In computational
experiments with the wire bond data, they found that the neural
network trained with fine-tuned learning produced slightly
more accurate predictions than the neural network trained in
the standard way (without fine-tuned learning) and required
much less training time. The notion of averaging the data followed by sequential refinement appeared to them to have merit.
The neural network model learned the main characteristics
of the original data from the averaged patterns and gradually
learned more details from the less aggregated patterns.

1083–4427/98$10.00  1998 IEEE

COY et al.: FINE-TUNED LEARNING AND ITS APPLICATION

Fig. 1. The concept of fine-tuned learning.

Building on the work of Sun et al., Coy et al. [6] performed
an extensive set of computational experiments using neural
networks with varying degrees of data nonlinearity. They
evaluated the efficacy of fine-tuned learning in reducing the
time required to train a network and avoiding early convergence. They observed that fine-tuned learning took slightly
longer than backpropagation to train a neural network. They
also found that fine-tuned learning generated results that were
5%–6% better than the results produced by backpropagation.
A third neural network application of fine-tuned learning
is reported by Vakhutinsky and Golden [14]. They developed
and analyzed a hierarchical elastic net algorithm for solving
the traveling salesman problem (TSP). Initially, nodes are
aggregated, based on proximity considerations, in order to
obtain a smaller and, thus, simpler TSP. This simpler TSP is
solved as an approximation to the original problem. Gradually,
nodes are disaggregated and the resulting TSP’s are solved
until the disaggregation process yields the original TSP. The
hierarchical elastic net algorithm dramatically outperforms the
traditional elastic net method.
The notion of fine-tuned learning has also been applied
to improve the performance of more traditional local search
heuristics that are designed to solve combinatorial optimization
problems. Gu and Huang [9] developed a search space smoothing algorithm that generated good solutions to TSP’s. Their
algorithm starts with distances between cities that have been
smoothed according to a transformation (more about this later).
A local search heuristic is then applied to produce a locally
optimal tour. The distances between the cities are smoothed
again (to a lesser extent) and a local improvement heuristic is
applied using the locally optimal tour from the previous step
as the starting tour. As the algorithm continues to run, the
smoothed distances begin to look more like the original distances until, at the final step, the local improvement heuristic is
run on the original data set (that is, the distances between the
cities have not been smoothed). The search space smoothing
algorithm of Gu and Huang uses the notion of data smoothing
followed by sequential refinement (in other words, fine-tuned
learning, although Gu and Huang did not use our terminology)
to escape poor local minima when solving the TSP. We present
the general concept of fine-tuned learning in Fig. 1.
Based on our own research (see [5], [6], [13], and [14]) and
what we have read in the literature (see [3], [4], and [9]), the
concept of fine-tuned learning appears to have worked well on
the few problems to which it has been applied. However, the
computational evidence needed to establish its overall merit is
sparse and the case has yet to be made convincingly that fine-

455

tuned learning is a “smart” concept to employ when trying to
solve difficult problems.
The remainder of this paper is devoted to determining
whether or not a case can be made for fine-tuned learning
as a viable solution concept in the context of combinatorial
optimization problems. In a large computational experiment,
we study two solution procedures that use the concept of finetuned learning to solve the TSP. We compare the performance
of the fine-tuned-learning-based procedures to a standard solution procedure and provide important insight into the behavior
of fine-tuned learning. In particular, we show that fine-tuned
learning avoids becoming trapped in local optimal solutions by
making some moves that increase tour length and not making
some moves that decrease tour length.
In Section II, we present background information on heuristics for the TSP, including the search space smoothing algorithm of Gu and Huang. In Section III, we describe our
computational experiments including details about test problems, our experimental procedure, and results. In Section IV,
we explain how fine-tuned learning works. In Section V, we
present our conclusions.
II. SMOOTHING AND NOISING HEURISTICS FOR THE TSP
A. Heuristics for the TSP
The TSP is easy to state (find the shortest tour that visits
each city only once), and can be difficult to solve optimally,
especially for problems containing a large number of cities.
The algorithmic and computational aspects of the TSP are
provided by Johnson and McGeoch [10] and Jünger et al. [12].
Over the last 30 years or so, a wide variety of heuristics
have been developed to find approximate solutions to the TSP.
Heuristics for the TSP fall into three broad categories: tour
construction methods, improvement methods, and composite
methods. Jünger et al. [12] provide descriptions of ten commonly used tour construction heuristics and three improvement
heuristics.
A tour construction heuristic uses a set of rules to construct
cities are visited exactly once. To
a tour in which all
illustrate, consider the greedy heuristic. It begins with a group
of unconnected cities. The distances between the cities (known
as arc lengths) are sorted in ascending order of length (the
shortest arc length is at the top of the list). The greedy tour
is constructed by taking an arc length off the top of the list
and inserting the corresponding arc into the partial tour. If an
insertion creates a subtour (a subtour is a tour that does not
include all cities), the insertion is not allowed. The heuristic
arcs have been inserted.
stops when
An improvement heuristic tries to improve the quality of an
existing TSP tour by making moves that favorably alter the
tour. To illustrate, the two-opt heuristic alters the existing tour
by exchanging two arcs that are in the tour for two arcs that are
not in the tour as long as the exchange decreases the distance
traveled by the salesman and no subtours are produced. This
type of exchange is shown in Fig. 2. The two-opt heuristic
stops when no favorable exchanges remain. The resulting tour
is said to be two-optimal. There may be many two-optimal

456

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

Fig. 2. A two-opt exchange.

tours for a specific problem, so that a two-optimal tour is only
locally optimal.
It is common practice when solving TSP’s to generate
a starting tour using a tour construction heuristic and then
improve the quality of the starting tour using an improvement
heuristic (this is known as a composite solution method). In
fact, the choice of a starting tour greatly affects the quality of
the final tour produced by an improvement heuristic. Johnson
and McGeoch [10] and Bentley [1] recommend using the
greedy heuristic or a variant known as the randomized greedy
heuristic (more about this later) to produce a starting tour for
the two-opt heuristic.

Fig. 3. Search space smoothing algorithm of Gu and Huang.

B. Search Space Smoothing
As we point out in the introduction, the search space
smoothing algorithm of Gu and Huang [9] uses the fine-tuned
learning concept to solve the TSP. Their algorithm is shown
in Fig. 3.
The search space smoothing algorithm starts with a random
tour in which the distances between the cities have been scaled
and smoothed according to the transformation shown in Step
is set to a high value,
3. Initially, the smoothing factor
so that all of the transformed distances are nearly the same
(see the graph of the smoothing transformation when
in Fig. 4). An improvement heuristic is then applied to this
simplified TSP to produce a locally optimal tour. We call this
the current tour. The value of the smoothing factor is then
decreased, new transformed distances are calculated, and the
improvement heuristic is applied to the current tour with the
new transformed distances. As the smoothing factor decreases
3, 2, 1 in Fig. 4), the transformed
in value (see
distances begin to look more like the original distances. When
the value of the smoothing factor is set to one, the transformed
distances are the original distances.
Gu and Huang apply their search space smoothing algorithm
(using the two-opt heuristic as the local improvement method)
and a two-opt algorithm to ten randomly generated, nonEuclidean TSP’s with 50 cities each. The distance between
each pair of cities is a number taken from a uniform distribution on the interval 0 to 1. Both algorithms are executed
25 times (started from 25 randomly generated tours) on each
problem. Gu and Huang find that their search space smoothing
algorithm produces solutions that are, on average, 7% to 31%
better than the two-opt solutions.

Fig. 4. The smoothing transformation.

Gu and Huang conduct additional experiments in which
three search space smoothing algorithms (using two-opt, Oropt, and city swap as the local improvement heuristic) and
three local improvement heuristics (two-opt, Or-opt, and city
swap) are applied to randomly generated, non-Euclidean problems with 50, 60, 70, 80, 90, and 100 cities. As the number
of cities increases, Gu and Huang report that the search space
smoothing algorithm typically performs better than its local
improvement counterpart by a wider margin (for example,
for 100-city problems, on average, search space smoothing
with Or-opt as the local improvement heuristic is about 6%
better than standard Or-opt, whereas for 60-city problems, on
average, the margin is only 4.7%).
C. Noising
The noising method of Charon and Hudry [3] can be viewed
as a heuristic using the fine-tuned learning concept to solve

COY et al.: FINE-TUNED LEARNING AND ITS APPLICATION

Fig. 5. Our noising algorithm for the TSP.

combinatorial optimization problems (in particular, the clique
partitioning problem). The noising method starts with a data
set to which some noise has been added. A descent method is
then applied until a local minimum is found. At each iteration
of the process, less noise is added to the data set until no noise
is added at the final iteration. The best solution found during
this process becomes the final solution.
We incorporate Charon and Hudry’s noising method into
an algorithm for solving the TSP. Our noising algorithm is
shown in Fig. 5. In Fig. 5, we see that the noising algorithm
begins with a starting tour in which the distances between
the cities have been perturbed according to the transformation
is set
shown in Step 3. Initially, the perturbation parameter
to a high value. We generate a random number between
and 1 and then transform the original distance between two
cities by performing the multiplication shown in Step 3. To
a large original distance between
illustrate, when
can be transformed to a distance
two cities, say
or as small as
as large as
A small original distance
can be transformed to a
between two cities, say
distance between 0.01 and 0.19. We illustrate the effect of our
transformation on distances for a small value of in Fig. 6.
Next, a two-opt heuristic is applied to the resulting TSP
to produce a two-optimal tour. The value of the perturbation
parameter is decreased so that less noise is added to the
data set, new transformed distances are calculated, and the
two-opt heuristic is applied to the current tour with the new
transformed distances. As the perturbation parameter decreases
in value, the transformed distances begin to look more like the
equals zero (that is,
original distances. When the value of
no noise is added to the distances), the transformed distances
become the original distances.
In a recent paper, Charon and Hudry [4] use the noising
method to produce a variant of the simulated annealing heuristic for the TSP (they do not add noise to the distances as
be the difference in length between the
we do). Letting
current tour and a new tour resulting from a proposed two-

457

Fig. 6. The noising transformation with P = 0:20: The diagonal lines above
and below the middle line show the maximum and minimum that a distance
can vary as a function of P:

opt exchange, Charon and Hudry add a random amount of
noise (call it , where is a number taken from a uniform
to
to
and accept the
distribution on the interval
is positive. Charon and Hudry
proposed exchange when
begin their procedure with a relatively large and then reduce
periodically until it equals 0. When no two-opt exchanges
are possible, the procedure ends.
Charon and Hudry test three noising variants and seven
other methods on two TSP’s that have random, non-Euclidean
distances and 100 cities and six problems that have Euclidean
distances (four problems are taken from TSPLIB; see Jünger et
al. [12]). Based on solution accuracy, Charon and Hudry conclude that their noising variants produce results that compare
favorably to results produced by simulated annealing.

III. COMPUTATIONAL EXPERIMENTS
In this section, we describe the experiments that we conducted to compare the performance of two algorithms that
use fine-tuned learning (that is, smoothing and noising) to a
standard TSP heuristic (that is, two-opt).
A. Test Problems
The description of our test problem sets is given in Table I.
We generate three different size problems (100, 316, and 1000
cities) with two different types of distances, thereby giving us
six test problem sets. To illustrate, from Table I, there are 25
problems that have 100 cities with Euclidean distances (we
call these Euclidean problems) and 25 problems that have 100
cities with random distances (we call these random problems).
We generate the Euclidean problems by randomly locating
cities in a unit square. The
coordinate and
coordinate
for each city are produced by a random number generator
started from different seeds. In the random problems, the
distance between two cities is a number taken from a uniform
distribution on the interval 0 to 1.

458

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

TABLE I
TEST PROBLEM SETS

TABLE III
AVERAGE RESULTS FOR THE 100-CITY PROBLEMS

TABLE II
EXPERIMENTAL SETTINGS FOR EUCLIDEAN AND RANDOM PROBLEMS

TABLE IV
AVERAGE RESULTS FOR THE 316-CITY PROBLEMS

B. Implementation Details
Each algorithm is written in C for a 32 bit operating
environment and is run on a 60 MHz Pentium-based personal
computer. Our two-opt algorithm is based on the tour-order,
two-opt algorithm described by Bentley et al. [2].
To make the two-opt algorithm run quickly, we use a
neighbor-list implementation in which we store the 40 nearest
neighbors of each city and, instead of looking at all possible
moves, we only look at exchanges with the 40 neighbors that
are nearest to a city. Our choice of 40 nearest neighbors for
both Euclidean and random problems is based on the work
of Johnson and McGeoch [10] and our own computational
testing. Neighbor lists only have to be constructed once for
a problem and they can be reused in successive runs of an
algorithm on the same problem. (We point out that using
a single neighbor list for noising is a rough approximation
since the noising transformation does not maintain the original
ranking of the distances.)
C. Experimental Procedure
In order to run our smoothing, noising, and two-opt algorithms, we must specify a starting tour, the number of times
that we will execute each algorithm, and the schedules for
various parameters. Our settings, which improve slightly upon
those of Gu and Huang, are given in Table II.
For each Euclidean and random problem, we generate
25 random starting tours (we call this a random start) and
then run each algorithm (using the smoothing or noising
schedules), thereby producing 25 solutions. We also generate
25 randomized greedy starting tours (we call this a greedy
start) and then run each algorithm, thereby producing another
25 solutions to each problem.
Bentley [1] and Johnson and McGeoch [10] have observed
that using a starting tour that is not “too good” allows local
optimization algorithms to make substantial improvements.
Bentley [1] points out that, for the two-opt algorithm, a
greedy starting heuristic performs better than other starting
heuristics, while Johnson and McGeoch [10] find that a
randomized greedy heuristic performs well. The randomized
greedy heuristic that we use follows Johnson and McGeoch’s
implementation: the shortest legal arc that can be added to a

partial tour has a 2/3 probability of being selected, while the
next shortest legal arc has a 1/3 probability of being selected.
D. Results
In Tables III, IV, and V, we report results for the 100-,
316-, and 1000-city problems, respectively. Since there are
multiple problems in each test set and we make multiple
runs of each algorithm, we provide average results in three
categories: tour length, percent above the Held–Karp (HK)
lower bound, and run time. To illustrate, in Table III for
the 100-city problems, we have 25 problems with Euclidean
distances and each problem is solved from 25 different random
starting solutions. This gives a total of 625 solutions. (We
note that there are 250 solutions for the 316-city problems
and 125 solutions for the 1000-city problems with Euclidean
distances and random starts.) In the first line of results in
Table III (Euclidean problems), we see that, for the smoothing
algorithm, the average tour length of the 625 solutions is
8.0037 and an average of 0.152 s is spent to find one solution.
Before each algorithm is run, there is some problem preprocessing that takes place. The time spent on preprocessing is
the sum of the CPU times for calculating distances, generating
the neighbor list, and producing a starting tour. The average
time spent on preprocessing is also recorded in Tables III–V.
To illustrate, in Table III for the Euclidean problems with
a random start, we see that the average preprocessing time
is 0.054 s. The average total processing time is the sum of
the average preprocessing time and the average algorithm
processing time. To illustrate, the average total processing time
to solve a 100-city Euclidean problem using smoothing with

COY et al.: FINE-TUNED LEARNING AND ITS APPLICATION

TABLE V
AVERAGE RESULTS FOR THE 1000-CITY PROBLEMS

459

TABLE VII
PERCENTAGE ABOVE THE AVERAGE TOUR LENGTH OF THE
BEST HEURISTIC FOR EACH PROBLEM SIZE AND TYPE

TABLE VI
APPROXIMATE HELD–KARP LOWER BOUNDS

a random start is
s. We provide an
indication of solution quality by comparing tour length (TL)
to an approximate HK lower bound. We use the approximate
HK bounds shown in Table VI (these are taken from Johnson
et al. [11]) to compute the percent over the HK lower bound,
TL HK HK In the first line of results in
that is,
Table III (Euclidean problems), we see that, for the smoothing
algorithm, the average percent over the HK lower bound of
the 625 solutions is 3.91.
E. Summary of Results
In this section, we summarize our observations about
the results of the computational experiments displayed in
Tables III–V. The results that are displayed in these three
tables provide average statistics. In Table VII, we present
percentage statistics based on the tour length for each size
problem (100, 316, and 1000 cities) and for each type of
problem (Euclidean and random problems). To illustrate, for
the 100-city Euclidean problems in Table VII, we see that
the average tour length produced by the noising algorithm
is 0.17%
above the
average tour length produced by the smoothing algorithm
The average tour length produced by the two-opt algorithm
is 3.34% above the average tour length produced by
the smoothing algorithm
1) Euclidean Problems: If we restrict our attention to the
Euclidean problems of the three problem sets (that is, the lefthand sides of Tables III–V and the upper portion of Table VII),
the results of six experiments are reported. We make the
following observations about the performance of the three
algorithms.
• In five experiments, the noising algorithm produces the
shortest average tour length while, in one experiment, the
smoothing algorithm produces the shortest average tour
length.

• In all six experiments, the noising algorithm produces
average tour lengths that are virtually the same as the average tour lengths produced by the smoothing algorithm.
• In all six experiments, the noising algorithm and the
smoothing algorithm produce average tour lengths that
are shorter than the average tour lengths produced by the
two-opt algorithm.
• When a random start is used, the two-opt algorithm
produces average tour lengths that are about 3% above the
average tour lengths produced by noising and smoothing.
When a greedy start is used, two-opt is about 1% above
noising and smoothing.
• Noising has the slowest average running times, while twoopt has the fastest average running times. The differences
in running time grow substantially as the number of cities
is increased, so that, for a 1000-city problem with a
random start, a single run of noising averages nearly 73
s (without preprocessing), while smoothing takes about
12 s and two-opt takes about 9 s. In fact, for 316- and
1000-city problems, smoothing produces tour lengths that
are nearly as good as the tour lengths produced by noising,
but smoothing takes much less run time.
2) Random Problems: If we restrict our attention to the
random problems of the three problem sets (that is, the righthand side of Tables III–V and the lower portion of Table VII),
the results of six experiments are reported. We make the
following observations about the performance of the three
algorithms.
• In five experiments, the smoothing algorithm produces
the shortest average tour length while, in one experiment,

460

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

the noising algorithm produces the shortest average tour
length.
• In all six experiments, the noising algorithm and the
smoothing algorithm produce average tour lengths that
are shorter than the average tour lengths produced by the
two-opt algorithm.
• When a random start is used, the noising algorithm produces average tour lengths that are substantially greater
than the average tour lengths produced by the smoothing
algorithm, especially for the 1000-city problems. When
a greedy start is used, the average tour lengths produced
by noising are nearly 2% above the average tour lengths
produced by smoothing on 100-city problems, but the
two algorithms produce virtually the same average tour
lengths on the remaining problems.
• When a random start is used, the two-opt algorithm produces average tour lengths that are substantially greater
than the average tour lengths produced by noising and
smoothing. The range is 8% to nearly 45% higher. When
a greedy start is used, two-opt performs much better with
a range of 2% to nearly 6% higher.
• Noising has the slowest average running times, while twoopt has the fastest average running times. The differences
in running time grow substantially as the number of cities
is increased, so that, for a 1000-city problem with a
random start, a single run of noising averages nearly 23 s
(without preprocessing), while smoothing takes about 8 s
and two-opt takes about 2 s.
3) Random Start Versus Greedy Start: In Table VIII, we
present the percentage that the average tour length with a
random start exceeds the average tour length with a greedy
start. To illustrate, for the 100-city Euclidean problems in
Table VIII, we see that the average tour length produced by
the noising algorithm using a random start is 0.82% above the
average tour length produced by the noising algorithm with
a greedy start (that is,
Using the results reported in Tables III–V, and VIII, we make
the following observations about the impact of a random
starting tour and a greedy starting tour on an algorithm’s
performance.
• For each algorithm, regardless of problem size or problem
type, the average tour length with a greedy start is shorter
than the average tour length with a random start.
• For Euclidean problems, a greedy start produces an
average tour length that is slightly better than the average
tour length produced by a random starting tour for both
noising and smoothing. For both algorithms, the average
tour length with a random start is about 1% higher than
the average tour length with a greedy start. For the twoopt algorithm a greedy start is clearly better. The average
tour length with a random start is about 2% to 4% higher
than the average tour length with a greedy start.
• For random problems, a greedy start produces an average
tour length that is dramatically better than the average
tour length produced by a random starting tour for all
three algorithms, except for the smoothing algorithm on
100-city problems. Excluding the exception, for all three

TABLE VIII
PERCENTAGE THAT THE AVERAGE TOUR LENGTH FROM A RANDOM
START EXCEEDS THE AVERAGE TOUR LENGTH FROM A GREEDY START

algorithms, the average tour length with a random start
is 10% to 75% higher than the average tour length with
a greedy start. We point out that there is a great deal of
variability in the percentages that we show in Table VIII
for the random problems.
F. Summary of Computational Experiments
Based on our computational experiments, the noising and
smoothing algorithms produce results that are quite similar
in terms of solution accuracy for the three sizes of problems
and the two types of problems that we tested. Noising and
smoothing produce better solutions than the two-opt algorithm,
although two-opt runs faster than both. We would recommend
using a greedy start for all three algorithms.
Based on solution accuracy and computation time, the slight
edge goes to the smoothing algorithm. Our implementation
of smoothing is faster than noising on these problems and it
produces tour lengths that, on average, are nearly the same as
those produced by noising.
Finally, we point out that we have applied our smoothing
heuristic to 29 problems with sizes ranging from 105 to 1432
cities taken from the TSPLIB library. This computational
experiment is described in [5]. On average, our smoothing
heuristic produced solutions that were 5% above the optimal
solutions for these 29 problems.
IV. HOW FINE-TUNED LEARNING WORKS: INSIGHT INTO THE
BEHAVIOR OF THE SMOOTHING ALGORITHM FOR THE TSP
Our computational experiments indicate that the smoothing
algorithm is an effective algorithm for solving TSP’s. In this
section, we provide detailed insight into the behavior of the
smoothing algorithm that reveals how and why it works as
well as it does.
A. Intuition Provided by Gu and Huang
Recall that the search space smoothing algorithm of Gu and
Huang [9] begins with distances between cities that have been
smoothed according to the transformation shown in Step 3
is set to a high
of Fig. 3. Initially, the smoothing factor
value, so that all of the transformed distances are virtually the
same. Gu and Huang argue that, when the value of is high,

COY et al.: FINE-TUNED LEARNING AND ITS APPLICATION

Fig. 7. Local minima in the smoothed and original search spaces.

there are a small number of local optima that can be produced
by an improvement heuristic (such as the two-opt heuristic).
We illustrate this in Fig. 7. In the top panel, we see that in
the smoothed search space there are fewer local optima than
there are in the original search space (where the distances are
). According to Gu and Huang,
not transformed, that is,
some of the local minima in the original search space (that is,
the valleys) are “filled” by the smoothing transformation, so
that a local improvement heuristic will not become “trapped”
at these points and, therefore, the chance of finding the global
optimum should increase.
B. Insight from Our Computational Experiments
In order to gain some insight into how the smoothing
algorithm works, we plot the actual tour lengths produced by
smoothing in the order in which it produced them for a 316city Euclidean problem started from a greedy tour [Fig. 8(a)]
and a 100-city random problem started from a random tour
[Fig. 8(b)]. We see that, in both figures, the tour lengths are
not (strictly) monotonically decreasing in value.
From what we see in Fig. 8, it is clear that the smoothing
algorithm makes uphill moves in which arc exchanges are
made that increase the value of the tour length. (In Fig. 8, we
have included actual tour lengths from two-opt and noising
for comparison purposes.) In fact, what we observe from our
computational experiments is that, for values of the smoothing
the smoothing algofactor greater than one (that is,
rithm makes uphill moves and often does not make favorable
downhill moves. More specifically, in the smoothed search

461

space, it is clear that, when the two-opt heuristic is applied,
only arc exchanges are made that decrease the value of the
tour length. However, when we evaluate the arc exchanges
and the resulting tour lengths in the original search space
(using the original distances), it is possible that a downhill
move we make in the smoothed search space is actually an
uphill move in the original search space. Also, it is possible
that an uphill move we do not make in the smoothed search
space is actually a downhill move in the original search space.
Of course, downhill moves and uphill moves in the smoothed
search space can also be downhill moves and uphill moves in
the original search space.
Recall from Fig. 4 the shape of the smoothing transformation when
We now examine the behavior of the
smoothing algorithm in the convex, concave, and transitional
regions of the smoothing transformation. In Fig. 9, we illustrate how a downhill move in the smoothed search space is
actually an uphill move in the original search space when
we are in the convex region of the smoothing transformation.
When the mean of two transformed distances associated with
two arcs that are not in a tour is less than the mean of two arcs
that are in a tour, the two-opt heuristic will make the exchange.
In Fig. 9, arcs (1, 2) and (3, 4) are in the tour and arcs (2, 3)
and (1, 4) are being considered for exchange (see Fig. 2 for
an illustration of this type of exchange). When transformed
distances are used, the mean of arcs (1, 2) and (3, 4) is about
0.58, the mean of arcs (2, 3) and (1, 4) is about 0.55, and,
two-opt would make the exchange since
since
this is a downhill move in the smoothed search space. When
untransformed distances are examined, the mean of arcs (1, 2)
and (3, 4) is about 0.74 and the mean of arcs (2, 3) and (1,
4) is about 0.80. In the original search space, we would not
exchange the arcs since we would be making an uphill move
We point
that increases the tour length by
out that if the roles of the arcs are reversed, that is, arcs (1,
2) and (3, 4) are not in the tour and arcs (2, 3) and (1, 4) are
in the tour, then the exchange in the smoothed search space
would not be made (it is an uphill move), even though it is a
downhill move in the original search space.
In Fig. 10, we illustrate how an uphill move in the smoothed
search space is actually a downhill move in the original search
space when we are in the concave region of the smoothing
transformation. In the smoothed search space, arcs (1, 2) and
(3, 4) are in the tour with a mean distance of about 0.41 and
arcs (2, 3) and (1, 4) are not in the tour with a mean distance of
two-opt would not perform the
about 0.43. Since
exchange in the smoothed search space and the uphill move
would not be made. However, in the original search space, the
mean of arcs (1, 2) and (3, 4) is about 0.31 and the mean of arcs
(2, 3) and (1, 4) is about 0.25, so that, in the original search
space, by exchanging the arcs, we would be making a downhill
move that decreases the tour length by
In the smoothed search space we have “missed” a favorable
opportunity to exchange arcs and reduce the tour length.
In Fig. 11, we show that, in the transitional region of the
smoothing transformation (in the transitional region arcs fall
on both sides of the inflection point), there is a downhill move
that we would make in the smoothed search space that is an

462

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

(a)

(b)

Fig. 8. Sequence of tour lengths (original distances) produced by the smoothing algorithm on two TSP’s. (a) The 316-city Euclidean problem and (b)
the 100-city random problem.

uphill move in the original search space. We point out that if
the roles of the arcs in the tour and the arcs not in the tour
are reversed, then the exchange in the smoothed search space
would not be made (it is an uphill move), even though it is a
downhill move in the original search space.
In our computational experiments, we observed that most of
the uphill moves and missed downhill moves occurred in the
transitional region of the smoothing transformation. A few of
these moves took place in the concave region, while almost no
moves of any type occurred in the convex region as there were
very few opportunities to evaluate a group of four arcs that had
distances greater than the average distance (some opportunities
existed when there were many large arcs in a random start
tour). In each region of the smoothing transformation, we
observed that the number of missed downhill moves exceeded

the number of uphill moves. In addition, as the smoothing
parameter decreased in value, the number of uphill moves and
missed downhill moves also decreased.
Based on our computational work with the TSP, it is clear
that the smoothing algorithm escapes from local minima by
moving uphill occasionally and by occasionally not taking
a downhill move. Two natural questions arise with respect
to the behavior of smoothing: How important are the uphill
moves? How important are the missed downhill moves? To
answer the first question, we evaluated a variant of smoothing
called restricted smoothing that did not allow uphill moves
but continued to allow missed downhill moves. We point out
that in our restricted smoothing algorithm an exchange of arcs
was accepted only if it resulted in a downhill move in both
the smoothed and original search spaces. (This additional test

COY et al.: FINE-TUNED LEARNING AND ITS APPLICATION

Fig. 9. A downhill move in the smoothed search space ( = 2, convex
portion of the smoothing transformation) is really an uphill move in the
original search space. Arcs (1, 2) and (3, 4) are in the current TSP tour
and arcs (2, 3) and (1, 4) are not in the current tour but are being considered
for exchange.

463

Fig. 11. A downhill move in the smoothed search space ( = 2, transitional
portion of the smoothing transformation) is really an uphill move in the
original search space. Arcs (1, 2) and (3, 4) are in the current TSP tour
and arcs (2, 3) and (1, 4) are not in the current tour but are being considered
for exchange.

and 0.37% better on the random problems. The results of this
experiment indicate that the uphill moves do not appear to be
very important as the average tour lengths produced by both
smoothing algorithms are nearly the same.
Finally, we point out that we cannot answer the second question by examining the performance of a restricted smoothing
algorithm that makes all downhill moves and allows uphill
moves. In this complementary variant, uphill moves that are
taken would be undone by two-opt and the algorithm would
not converge.

V. CONCLUSIONS

Fig. 10. An uphill move in the smoothed search space ( = 2, concave
portion of the smoothing transformation) is really a downhill move in the
original search space. Arcs (1, 2) and (3, 4) are in the current TSP tour and
arcs (2, 3) and (1, 4) are not in the current tour but are being considered
for exchange.

makes restricted smoothing more computationally burdensome
than smoothing itself.)
We ran our restricted smoothing algorithm on 20 problems (ten Euclidean problems—five 100-city problems, three
316-city problems, and two 1000-city problems; ten random
problems with the same size breakdown) drawn from the test
problem sets shown in Table I. We performed 25 executions
of restricted smoothing started from a randomized greedy tour
on each problem. We found that, on average, the tour lengths
produced by smoothing were 0.16% better than the tour lengths
produced by restricted smoothing on the Euclidean problems

In this paper, we introduce the concept of fine-tuned learning. As mentioned, the notion has already been successfully
applied to prediction as well as optimization problems. This
paper builds upon the earlier work by Sun et al. [13] and
Vakhutinsky and Golden [14] in that it presents a general finetuned learning approach. This general approach seems to be
widely applicable.
In several recent papers, smoothing and noising are proposed as new approaches for obtaining near-optimal solutions
to combinatorial optimization problems. We argue that both
of these ideas are examples of fine-tuned learning. In our
computational experiments, we demonstrate that while both
approaches are effective, smoothing shows greater promise.
That is, with respect to accuracy and running time, smoothing
outperforms noising. In addition, smoothing clearly dominates
two-opt with respect to accuracy, although it requires slightly
more running time. Based on our extensive computational
experiments with the TSP, fine-tuned learning is a meritorious
concept that deserves serious consideration by researchers
solving difficult problems.

464

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 28, NO. 4, JULY 1998

[12] M. Jünger, G. Reinelt, and G. Rinaldi, “The traveling salesman problem,” in Network Models, M. Ball, T. Magnanti, C. Monma, and G.
Nemhauser, Eds. Amsterdam, The Netherlands: North-Holland, 1995,
pp. 225–330.
[13] X. Sun, B. Golden, and E. Wasil, “The fine-tuned learning enhancement
to the standard backpropagation algorithm,” in Intelligent Engineering
Systems Through Artificial Neural Networks: Volume 5, C. Dagli, M.
Akay, C. Chen, B. Fernández, and J. Ghosh, Eds. New York: ASME
Press, 1995, pp. 125–133.
[14] A. Vakhutinsky and B. Golden, “A hierarchical strategy for solving
traveling salesman problems using elastic nets,” J. Heuristics, vol. 1,
no. 1, pp. 67–76, 1995.

Fig. 12.

A new classification scheme for improvement-based TSP heuristics.

In our computational experiments, we observed that smoothing and noising made some uphill moves and did not make
some downhill moves. Many well-known TSP heuristics (including two-opt, three-opt, and simulated annealing) accept all
downhill moves. These observations serve as motivation for a
new classification scheme for TSP heuristics as displayed in
Fig. 12. (For descriptions of the various heuristics listed, see
[7], [8], [10], and [12].) In future work, we expect researchers
to develop new and more effective heuristics for a host of
combinatorial optimization problems based on this scheme.
REFERENCES
[1] J. Bentley, “Fast algorithms for geometric traveling salesman problems,”
ORSA J. Comput., vol. 4, no. 4, pp. 307–411, 1992.
[2] J. Bentley, D. Johnson, L. McGeoch, and E. Rothberg, “Near optimal
solutions to very large traveling salesman problems,” unpublished.
[3] I. Charon and O. Hudry, “The noising method: A new method for combinatorial optimization,” Oper. Res. Lett., vol. 14, no. 3, pp. 133–137,
1993.
[4] I. Charon and O. Hudry, “Mixing different components of metaheuristics,” in Meta-heuristics: Theory and Applications, I. Osman and J.
Kelly, Eds. Boston, MA: Kluwer, 1996, pp. 589–603.
[5] S. Coy, B. Golden, G. Runger, and E. Wasil, “Solving the TSP
with sequential smoothing,” in Proc. 2nd Int. Conf. Computational
Intelligence and Neuroscience, P. Wang, Ed., Research Triangle Park,
NC, Mar. 1997, pp. 280–283.
[6] S. Coy, B. Golden, E. Wasil, and G. Runger, “Evaluating the effectiveness of the fine-tuned learning enhancement to backpropagation,”
in Intelligent Engineering Systems Through Artificial Neural Networks:
Volume 7, C. Dagli, M. Akay, O. Ersoy, B. Fernández, and A. Smith,
Eds. New York: ASME Press, 1997, pp. 105–111.
[7] G. Dueck, “New optimization heuristics. The great deluge algorithm and
the record-to-record-travel,” J. Comput. Phys., no. 104, pp. 86–92, 1993.
[8] G. Dueck and T. Scheuer, “Threshold accepting: A general purpose
algorithm appearing superior to simulated annealing,” J. Comput. Phys.,
no. 90, pp. 161–175, 1990.
[9] J. Gu and X. Huang, “Efficient local search with search space smoothing:
A case study of the traveling salesman problem,” IEEE Trans. Syst.,
Man, Cybern., vol. 24, pp. 728–735, May 1994.
[10] D. Johnson and L. McGeoch, “The traveling salesman problem: A
case study in local optimization,” in Local Search in Combinatorial
Optimization, E. Aarts and J. Lenstra, Eds. London, U.K.: Wiley, 1997,
pp. 215–310.
[11] D. Johnson, L. McGeoch, and E. Rothberg, “Asymptotic experimental
analysis for the Held-Karp traveling salesman bound,” in Proc. 7th Annu.
ACM/SIAM Symp. Discrete Algorithms, Atlanta, GA, 1996, pp. 341–350.

Steven P. Coy received the B.S. degree in business administration from the University of Vermont,
Burlington, and the M.S. degree in operations research from the University of Maryland, College
Park. He is pursuing the Ph.D. degree in management science at the University of Maryland.
His research interests are the design and analysis
of heuristics, data modeling, and neural networks.

Bruce L. Golden is a Professor and Distinguished
Faculty Research Fellow in the College of Business
and Management, University of Maryland, College
Park. His research interests include neural networks,
heuristic search, and applied operations research.
He is currently Editor-in-Chief of the INFORMS
Journal on Computing.
Dr. Golden has received numerous awards, including the 1994 Thomas L. Saaty Prize.

George C. Runger holds degrees in industrial engineering and statistics.
He is a Faculty Member in Industrial and Management Systems Engineering, Arizona State University, Tempe. His research interests are in process
control and experimental design strategies. He is
coauthor of the text Applied Statistics and Probability for Engineers. He was a Faculty Member in the
College of Business and Management, University
of Maryland, College Park, and also at Rensselaer
Polytechnic Institute, Troy, NY. He was previosuly
a Senior Engineer at IBM.
Mr. Runger received an Ellis Ott Award and a Brumbaugh Award for his
research.

Edward A. Wasil is a Professor of Management
Science, Kogod College of Business Administration,
American University, Washington, DC, where he
has taught graduate courses in operations research
and applied statistics since 1985. His research interests include the application of neural networks to
business decision problems, network optimization,
and the process of implementing management science models.
He is Feature Article Editor for the INFORMS
Journal on Computing.

Data Min Knowl Disc (2007) 14:409–431
DOI 10.1007/s10618-006-0062-6

Using metarules to organize and group discovered
association rules
Abdelaziz Berrado · George C. Runger

Received: 26 April 2005 / Accepted: 20 November 2006 / Published online: 10 February 2007
Springer Science+Business Media, LLC 2007

Abstract The high dimensionality of massive data results in the discovery of
a large number of association rules. The huge number of rules makes it difficult to interpret and react to all of the rules, especially because many rules are
redundant and contained in other rules. We discuss how the sparseness of the
data affects the redundancy and containment between the rules and provide a
new methodology for organizing and grouping the association rules with the
same consequent. It consists of finding metarules, rules that express the associations between the discovered rules themselves. The information provided by
the metarules is used to reorganize and group related rules. It is based only
on data-determined relationships between the rules. We demonstrate the suggested approach on actual manufacturing data and show its effectiveness on
several benchmark data sets.
Keywords Item sets · Data sparseness · Clustering rules · Classification ·
Rules pruning
1 Introduction
Mining association rules from massive data often results in a massive set of
rules. The large number of rules makes it overwhelming to extract the desired

Responsible editor: M. J. Zaki.
A. Berrado (B)· G. C. Runger
Department of Industrial Engineering, Arizona State University, Tempe, AZ, USA
e-mail: berrado@asu.edu
G. C. Runger
e-mail: runger@asu.edu

410

A. Berrado, G. C. Runger

information by simply analyzing the rules individually. The rules are often highly
redundant. Organizing the rules is a key to summarizing them and making them
easily understandable and interpretable by the data analyst. In this paper, we
suggest a new approach to reorganizing and grouping the redundant rules with
the same consequent by unveiling their mutual relationship and their containment in other more general rules. We find new rules that we call metarules which
reveal relationships between the discovered association rules. We use the information provided by the metarules to reorganize and group the related rules.
The metarules are also useful for pruning the more specific, possibly overfitting, rules. We apply the same association algorithm used to generate the rules
to derive the metarules. This approach postpones ad hoc, analyst preferences
for rules until an organized summary of the rules is generated. The organized
collection presents only data-determined relationships between the rules. Then
preferences can later be applied to the organized collection.
Our focus is on summarizing a subset of association rules with the same consequent. Our basic approach could likely be extended to general association
application, but we do not explore that extension here. The rest of this paper
is organized as follows. A brief introduction to association rules is given in
Section 2. We also cite and summarize published work related to the topic at
hand. Containment and overlap of rules is discussed in Section 3. In Section 4,
we explain how metarules are generated and used to organize the discovered
rules. An example in Section 5, illustrates the application of the suggested
methodology to actual manufacturing data and shows its advantage over existing approaches to clustering rules. In Section 6, experimental results concerned
with the effectiveness of the suggested approach with several well known data
sets are given. Section 7 is a conclusion.

2 Background for association rules mining and related work
Association rules mining emerged as a technique for finding interesting rules
from transactional databases (Agrawal et al. 1993). More specifically, it was
initially used to reveal associations in commercial data from a database of
transactions each representing the set of items purchased by a customer. The
association analysis identifies items purchased together.
An association rule is an expression of the form: A → C, where A and C are
subsets of the set of items. Here A is referred to as the set of antecedents and
C as the set of consequents. The subsets A and C are disjoint. The importance
of a rule is evaluated by its support and confidence. The support of a rule is
the fraction of all transactions where the set of antecedents A and the set of
consequents C apply simultaneously. The support of a rule is a measure of its
importance in terms of the number of transactions where the rule applies. The
confidence of a rule is the fraction of the set of transactions containing the set
of antecedents A which contain the set of consequents C. Furthermore, the
confidence of a rule quantifies the strength of the association between the set of
antecedents and the set of the consequents. Minimum support and confidence

Using metarules to organize and group discovered association rules

411

thresholds are usually pre-specified before mining for association rules. Using
a low-support threshold uncovers all the underlying regularities in the data but
also results in a high number of association rules, most of which are redundant
and/or contained in other rules.
Several efforts have been deployed to tackle the problem of summarizing
and pruning the huge number of rules. Klemettinen et al. (1994) suggested a
method using templates which allow the user to retrieve only the rules that
are of interest. Other approaches, (Liu and Hsu 1996; Silberschatz and Tuzhilin
1996; Liu et al. 1997; Padmanabhan and Tuzhilin 1998) find those unexpected
rules by comparing the discovered rules to the pre-defined user’s knowledge
about the domain. These methods allow the user to view only rules that are of
interest but they don’t prune or summarize the rules. Ng et al. (1998) and Srikant
et al. (1997) require the user to specify constraints or restrictions regarding the
items that are associated simultaneously in the mined association rules. Using
these item constraints certainly reduces the number of the resulting rules but
the mined rules still need to be pruned and summarized.
Another approach to pruning the rules is called Minimum Improvement
(Bayardo et al. 2000). A rule is pruned if the difference between its confidence and the confidence of any of its proper subrules (a proper subrule is a
simplification of the rule formed by removing one or more conditions from its
antecedents) is less than the pre-specified Minimum Improvement. This method
would not be effective when the data is sparse because many overlapping rules
won’t be sensitive to a low-Minimum Improvement threshold and thus won’t
be pruned. Furthermore, this method does not perform a summarization of the
rules that remain after pruning.
Statistical tests have been utilized for pruning potentially uninteresting rules
generated due to sampling. Bay and Pazzani (2001) provided a search algorithm
for mining contrast sets with pruning rules. Contrast sets refer to conjunctions of
attributes and values that differ meaningfully in their distribution across groups
or classes of interest. They applied a significance test to remove the insignificant
contrast sets. Huang and Webb (2005a) also developed an insignificance filter
for automatically discarding insignificant rules during rule discovery with the
OPUS search algorithm from data with undiscretized quantitative attributes.
Huang and Webb (2005b) suggested a new derivative rule filter using a t-test
for pruning a class of insignificant rules called Derivative Rules that are not
successfully removed using existing rule pruning techniques.
A chi-squared test was also used as the basis for pruning Liu et al. (1999). The
dependence between the antecedents and the consequent of a rule is evaluated
by a χ 2 test. A rule is pruned if its χ 2 statistic is lower than the pre-specified
threshold value which corresponds to a desired significance level. In sparse
domains this technique will fail to prune many rules. Furthermore Liu et al.
(1999) suggested a method for summarizing the remaining rules into a special
subset of associations called the direction setting rules which give a global picture
of the underlying relationships in the domain. Clearly the completeness of the
set of direction setting rules depends on the results of the previous pruning step.

412

A. Berrado, G. C. Runger

Toivonen et al. (1995) introduced a technique for pruning the discovered
rules by forming rule covers. A cover is a subset of rules that covers the entire
database. Although this method reduces the number of rules significantly, it does
not conserve all the information embedded in the discovered rules because a
greedy algorithm is used to find a good cover.
Chawla et al. (2004) proposed an adaptive local pruning method for association rules using directed hypergraphs. The method uses Association Rules
Networks as a graphical method to prune rules by associating with hypercycle
and reverse hyperedges. The pruning is local because it takes place in the context of a goal node. That is, a rule that is considered redundant for a particular
goal node may become important for another goal node. Unlike our approach
presented herein, this method does not provide a global organization of the
discovered association rules.
Several methods use distance-based clustering to group association rules.
Toivonen et al. (1995) defined the distance between two association rules as
the number of rows where the rules differ. A new normalized distance metric
was presented by Gupta et al. (1999) to cluster the rules. Lent et al. (1997),
clustered association rules with a geometric-based algorithm. This application
was limited to rules with two attributes in the set of antecedents. Defining a
distance metric between two rules is sensitive to the asymmetric relationship
between the rules. That is, if a rule is viewed as the set of rows that satisfy the
rule, there is often considerable containment and overlap between the sets of
rows that satisfy each rule.
Association rule mining is also used in classification. The approach of integrating classification and association rules mining (known as associative classification) is not new to the machine learning community. CBA (Liu et al. 1998)
generates association rules with consequent restricted to a class attribute. Such
class association rules (CARs) were also found using CBA-RG (Liu et al. 1998)
which is adapted from Apriori (Agrawal et al. 1993). The discovered CARs are
then used for classification. The CARs are sorted based on their confidence
then support and finally based on the order in which they were discovered.
Then a subset C of high precedence rules from CARs are chosen to cover
the data set. This subset of rules is augmented with a default class (majority
class of uncovered data) and then used for classification. A rule is chosen to
classify a data instance based on high precedence. CMAR (Li et al. 2001) or
classification based on multiple association rules, is more efficient than CBA
in terms of finding the association rules, it adopts a variant of an efficient frequent pattern mining, FP-growth (Han et al. 2000), to find the rules and stores
them efficiently in a CR-tree which is a prefix tree structure. More specific and
lower confidence rules are then pruned. The rules are thereafter pruned based
on database coverage. CMAR uses a coverage threshold to select database
coverage. After pruning the rules, CMAR determines the class label of a data
instance based on a weighted χ 2 analysis of multiple rules and not by the rule
with highest precedence like in CBA. Both CBA and CMAR use the pessimistic-error-rate pruning in C4.5 (Quinlan et al. 1992) to prune the discovered

Using metarules to organize and group discovered association rules

413

association rules. In this paper, we suggest using metarules as an alternative to
pruning and organizing the discovered class association rules.
Association rules were also considered in the case were the data set contains
quantitative attributes. Srikant and Agrawal (1996) suggested the first algorithm of the quantitative case, it uses discretization of the quantitative data.
Fukuda et al. (1999) also provided an efficient algorithm using computational
geometry and sampling methods for efficiently mining quantitative association
rules. Their solution was however, limited to rules with a categorical consequent. Aumann and Lindell (2003) introduced a new definition of quantitative
association rules based on statistical inference theory. Their work focuses on
situations with rules of one quantitative attribute or categorical attributes in
the antecedent set and one quantitative consequent.
Mining association rules with multiple quantitative antecedents without prior
discretization still remains an active research problem. Significant efforts have
been made to develop discretization methods. Dougherty et al. (1995) supplied
a review of several existing methods and classified them according to three
major axes: global versus local, supervised versus unsupervised, and static versus dynamic. Yang and Webb (2002) conducted a thorough comparison of several of the existing methods when employed for naive-Bayes classifiers. Liu
et al. (2002) provided a description of existing discretization methods and suggested some guidelines on how to choose a discretization method under various
settings. In our example, we use a naive discretizer on the continuous attributes.
The focus of our work is to organize generated rules regardless of the methodology used within rule discovery.
3 Containment and overlap of rules
One cause of the large number of rules is the redundancy among the rules
due to the sparseness of the data in high-dimensional spaces. The sparseness of
the data is due essentially to the curse of dimensionality. Scott and Thompson
(1983), showed that the data gets sparser as the dimensionality gets higher, that
local neighborhoods of points in high dimensions are mostly empty, and that
even in the case of uniform distributions, data is concentrated at the borders
of the volume of interest. We illustrate through the following example how the
sparseness of the data results in a redundancy among the rules.
Let A, B, and C denote three categorical variables of interest: A with two
categories, A1 and A2 , B with three categories, B1 , B2 , and B3 and C with two
categories C1 or C2 . Furthermore, assume that the distribution of the data is
summarized in Tables 1 and 2.
Table 1 Distribution of the
data when C is at the level C1

B

A

A1
A2

B1

B2

B3

1
0

100
100

0
1

414

A. Berrado, G. C. Runger

Table 2 Distribution of the
data when C is at the level C2

Table 3 Discovered
association rules with C1
as a consequent

B
B1

B2

B3

0
0

15
10

A

A1
A2

0
0

Rule index

Antecedents

Consequent

Support Frequency

r1
r2
r3
r4
r5

A1
A1 and B2
A2
A2 and B2
B2

C1
C1
C1
C1
C1

101
100
101
100
200

The numerical values in the cells in Tables 1 and 2, indicate the number of
observations in each cell. Note that some cells have few (or zero) observations.
Suppose that we are interested in the association rules that have C1 as a consequent. It follows that we find five rules with C1 as a consequent and support
frequency of at least 100. The rules are summarized in Table 3 where the first
column, labeled rule index, represents an arbitrary index that refers to each
rule. In the case under study, all the rules have the same consequent C1 . The
effect of sparseness is the generation of several rules.
The distribution of the data suggests that the categories of the variable A do
not affect the density of the data in the different cells of Table 1. Nevertheless,
the category A1 appears as an antecedent of the rules r1 and r2 , and the category
A2 appears as an antecedent of the rules r3 and r4 . Notice that the rules r1 and r2
are derived from nearly the same rows (or examples). It is clear then that they
convey the same information. The same thing is also true for rules r3 and r4 .
In contrast, the rule r5 resulted from the high density of category B2 and the
sparseness of the data in the other categories of B.
Hence, the sparseness of the data in some regions of the space results in a
high number of redundant association rules, most of which are not important.
This makes it difficult to find important or good rules among the resulting rules
especially in a high-dimensional space. Detecting those regions where the rules
are redundant and/or contained in other rules is then a key to grouping and
pruning the discovered rules. This type of relationship between rules is difficult
to capture with a clustering algorithm based on a distance metric. Examples are
presented in Sect. 5.5.
4 Finding metarules to organize and prune the discovered rules
4.1 Finding metarules
We propose a novel technique for grouping and then pruning the discovered
rules which share the same consequent. It consists of finding one-way associa-

Using metarules to organize and group discovered association rules

415

tions between those discovered rules. One-way association rules refer to rules
with one antecedent and one consequent. We call these associations between
the rules metarules. The approach simply and easily summarizes the asymmetric
relationships between the rules. We apply the same association algorithm in a
new role.
Let I = {i1 , i2 , . . . , ip } be a set of p items, and D = {d1 , d2 , . . . , dn } be the set
of all n data rows, where each data row contains a subset of items from I. Let
R = {r1 , r2 , . . . , rm } represent the set of the m discovered association rules with
the same consequent of interest obtained from D.
We say that a rule ri from R is supported by the data row dj or that data row
dj supports rule ri if all the antecedents of the rule ri are items of dj and we
refer to this relationship by the following expression: ri ⊆ dj . Define a new set
of transactions Q = {q1 , q2 , . . . , ql } where l ≤ n such that every element qj of Q
is a subset of rules from R such that:
qj = {ri ∈ R | ri ⊆ dj }.
In other words, each rule is considered an item. Also, each data row from D is
mapped to the subset of rules from R which it supports and this subset of rules
(when nonempty) corresponds to a transaction in Q. Note that if every data
row supports at least one rule then l = n.
If we find one-way association rules from the set Q, we will determine all
the one-way associations between the rules from R. The metarules take the
form ri → rj where ri and rj are rules from R. The resulting metarules provide
a summary of the containment and overlap between the rules. The confidence
threshold for mining metarules is defined similarly to an ordinary association
rule but applied to the set Q. The support threshold for metarules mining can
be set to 0% in order to uncover all the relationship between all the rules.
Because metarules only calculate rules with one antecedent and consequent
the calculations are simpler than a full association analysis and the support is
easily set to 0%. Let MR = {mr1 , mr2 , . . . , mrk } refer to the set of the k discovered metarules, obtained from Q. If we analyze the metarules from MR, we can
understand the relationship between the association rules from R.
Also, a graphical presentation of the metarules is defined here. Each rule
from R is represented by a node and each metarule connects two nodes with a
directed arc, such that the originating node is the antecedent of the metarule
and the destination node is its consequent.
We illustrate the suggested approach to mining metarules through the same
example that we used in Sect. 3. Recall that we found five rules so that R =
{r1 , r2 , . . . , r5 }. Also, n = 227. Note that l = 227 and it corresponds to the number of data rows which support the antecedents of at least one of five rules from
R. Mining for association rules from the data cases for the corresponding Q
results in the eight metarules summarized in Table 4. The minimum confidence
and support thresholds used for this example are, respectively, 85 and 0%.
In general the support and confidence thresholds would be set for metarules in identically the same manner as the usual rules. Because many rules

416

A. Berrado, G. C. Runger

are redundant, confidences as high as 90% or more can be used. Our following
examples use 100%. A higher confidence threshold results in a smaller set of
metarules. Large numbers of metarules indicate many relationships between
the rules. The support threshold for metarules mining that we use is specified
to 0% in order to uncover all the relationships between all the rules.
Note that our approach could likely be extended by using other metrics
instead of or in addition to support and confidence to assess the interestingness
of the discovered rules or metarules (Tan et al. 2002), but we do not explore
that extension here.
The metarules from Table 4 reveal the dependency between the rules of
interest: r1 , . . . , r5 . The graph in Fig. 1 illustrates the relationship between the
five rules. The graph is composed of five nodes, one for each rule and eight
directed arcs, one for each metarule. For example, the metarule r1 → r2 creates

Table 4 Discovered
metarules

Fig. 1 Relationship between
the rules

Metarule
index

Antecedents

Consequent

Support
(%)

Confidence
(%)

mr1
mr2
mr3
mr4
mr5
mr6
mr7
mr8

r2
r1
r2
r4
r3
r4
r1
r3

r1
r2
r5
r3
r4
r5
r5
r5

44
44
44
44
44
44
44
44

100
86
100
100
90
100
86
90

Using metarules to organize and group discovered association rules

417

an arc originating from the node r1 and ending at the node r2 . Figure 1 shows
the complete relationship between the five rules.
Figure 1 shows that the rules r1 and r2 show a mutual relationship with each
other. We conclude that the rules r1 and r2 are approximately equivalent. That
is, the two rules are supported by the same data rows. The same thing is true
for the rules r3 and r4 . We also notice that there are arcs originating from the
nodes r1 –r4 toward the node r5 and none in the other direction, we say that the
rule r5 is less specific than the other rules. That is, the data rows that support
each of the rules r1 –r4 are contained in the rows that support the rule r5 . In
Sect. 4.3, formal definitions for specificity of a rule and equivalence between
rules are provided.
4.2 Finding independent subgroups of rules
Using the metarules to build a graphical representation can divide the rules
from R into disjoint subgroups of rules, if they exist. That is, there might be
no arcs that link the rules spanning a subgroup to the rules spanning the other
subgroups. We illustrate this by considering a more complex example in Sect. 5.
Each cluster of rules explains the data in a local region of the high-dimensional
space.
Using the metarules for the organization of the discovered rules into clusters
or subgroups of rules differs from the distance-based clustering algorithms used
for the same purpose (Toivonen et al. 1995; Gupta et al. 1999). Our method
does not need a distance metric to find the subgroups of rules. The asymmetric
relationships generated from the containment and overlap between the rules
makes it challenging to define an appropriate distance metric to be used for
clustering the rules.
4.3 Reorganizing the equivalent and the more specific rules
Let ri be a rule from R and let OUT(ri ) refer to the subset of rules rj from R
defined as follows:
OUT(ri ) = {rj ∈ R | ri → rj ∈ MR}.
Also, let IN(ri ) refer to the subset of rules rj from R such that rj → ri is a
metarule from MR:
IN(ri ) = {rj ∈ R | rj → ri ∈ MR}.
Definition 1 Let ri and rj be two rules from R, we say that ri is more specific
than rj or that rj is more general than ri , if the following condition is met:
1. rj ∈ OUT(ri ) and rj ∈
/ IN(ri ).

418

A. Berrado, G. C. Runger

Fig. 2 Relationship between
the rules after combining the
equivalent rules

Definition 2 Let ri and rj be two rules from R, and let cij and cji denote respectively the confidences of the metarules ri → rj , rj → ri .
we say that ri and rj are equivalent if the following three conditions are satisfied:
1. ri ∈ OUT(rj ) and ri ∈ IN(rj ),
2. OUT(ri ) \ {rj } = OUT(rj ) \ {ri },
3. IN(ri ) \ {rj } = IN(rj ) \ {ri },
Where OUT(ri ) \ {rj } refers to the subset of OUT(ri ) excluding the rule rj , that
is OUT(ri ) \ {rj } ∪ {rj } = OUT(ri ). Note that if ri and rj are equivalent and
cij = cji = 100% then the two rules are supported by exactly the same data
rows. That is, they are different representations of the same relationship.
If we group each pair of equivalent rules and represent them by the same
node, the graph from Fig. 1 is reduced to the graph in Fig. 2. This more clearly
explains the relationships.
The computational overhead in the suggested approach could be broken
down into the following three components:
• The first one is concerned with creating the new set of transactions Q, which
takes mn computations to create.
• The second is concerned with metarules mining, which is simply the same as
the computational complexity of a priori because it is used to find metarules
from Q. We note that only one-way association rules (rules with a single
antecedent and single consequent) are calculated so that this step is much
simpler than a full association rule analysis.
• The third is concerned with the computational complexity of simplifying
metarules, which is quadratic to the number of metarules.

4.4 Pruning the discovered rules using metarules
Although the objective of the metarules approach is to group and organize
rules, the approach can also be directly used to prune the rules. First consider
equivalent rules. Suppose that rules ri and rj are found equivalent using metarules. Now, if the antecedent itemset of rj is contained within the antecedent set
of ri , then rj can be pruned because it is more complex than ri . For the example
at hand, this means that r2 with antecedent {A1 , B2 } can be pruned because it

Using metarules to organize and group discovered association rules

419

is contained in r1 with antecedent {A1 }. Similarly, r4 can be pruned since it is
contained in r3 . This leads to the following definition.
Definition 3 Let ri and rj be two rules from R, and let cij and cji denote the
confidences of the metarules ri → rj and rj → rj , respectively. We say that rj is
more complex than ri if the following two conditions are satisfied:
1. ri and rj are equivalent according to Definition 2, and,
2. The antecedents of ri is a proper subset of the antecedents of rj .
Note that Definition 3 implies that cji = 100%, because the support of rj is
completely included in the support of ri . The examples that follow show that
a large number of rules are related through this definition of equivalence and
complexity. Consequently, pruning based on this simple definition of equivalence and complexity is useful. This might be expected from sparse data in high
dimensions.
More generally, metarules can indicate clearly which rules are more specific,
and they can be directly used for pruning. However, for rules that do not satisfy
Definition 3, the decision to prune has some consequences. Even equivalent
rules may result in ambiguous pruning decisions. For example, when r1 has
antecedent {A1 , B2 } and r2 has antecedent {C1 , D2 } our solution is to merge
the rules to a common node, but not make a pruning decision. More importantly, the fact that a rule is more specific than another rule does not imply
that it should be pruned. It may or may not be overfitting the data, because
it has a smaller support than the more general one. Subject knowledge can be
used instead to decide which of the more specific rules should be pruned. We
recommend that this task be delayed until after the pruning of the equivalent
rules has taken place. For the example at hand, if subject knowledge indicates
that the two nodes with more specific rules, {r1 and r2 } and {r3 and r4 }, can be
pruned, then we are left only with the rule r5 . Here r5 is B2 → C1 and it is a
reasonable summary of the data in Table 1.
Although metarules directly apply to the issue of pruning specific rules in
the presence of more general ones, this topic is not further explored in this
paper. Instead, we propose to simplify the rules as much as possible before
subject matter expertise is invoked. Consequently, the rule reductions that we
present in the following examples are conservative. They are based only on
our summary graphics and with nodes joined based on equivalence defined in
Definition 2, without the pruning of more specific rules.
5 Application
Assume that we are interested in determining which process variables are
responsible for a defective product from a manufacturing process. Also, assume that the variables being investigated are all categorical. If we gather data
about the outcome of the process (good or defective output), given certain
process settings (operating conditions), and if we consider each observation
as a transaction, we can use association rules mining to associate defects with

420

A. Berrado, G. C. Runger

certain process settings. Process settings refer to the process variables being
set at certain levels. Because we are trying to explain a defective output, the
consequent of interest for the rules is “defective output.” Each element in the
set of antecedents represents a process variable set at some level. Note that
looking for a specific consequent such as a defective output, narrows down the
pool of rules that need to be analyzed. This subset of rules can however still be
massive and the rules highly redundant and thus they still need to be pruned
and summarized.
We now illustrate the advantage of our approach by organizing the association rules discovered from actual data provided by a major manufacturer. The
data has been coded and no actual variables or process names are used. The
data was collected from a manufacturing process with 35 numerical variables
and a binary response indicates whether the output is good, G, or rejected, R.
The number of observations is approximately 10,000.
5.1 Discretization of the numerical variables
Because the data provided is composed of 35 numerical variables and a
binary response, we had to discretize the numerical feature space before generating the association rules. As discussed in Sect. 2, several discretization methods are available. For initial results, we adopted a simple and naive method
known as Equal Frequency Discretization to partition each of the 35 continuous attributes individually. For simplicity, each attribute was divided into
four intervals. To discretize a variable xi , we determined its minimum and
maximum values, then sorted all values in ascending order and divided the
range into four intervals so that each contains 25% of the data. As mentioned previously, although better quality rules might be obtained with an
alternative discretizer, our objective is to summarize rules generated in any
manner.
We identified each variable by a number between 1 and 35. For example, the number 1 refers to the variable x1 . Because each variable was partitioned into four intervals, we used the following indexing to refer to the
intervals of each variable: the intervals corresponding to variable xi are, respectively, i_1–i_4. Because the class label (column 36) was already categorical, it
remained unchanged. As an example of the discretized data, consider the first
data row: the measured values of the 35 variables were represented by the
following record where the last element, R, on the record is the class label.
1_4, 2_1, 3_1, 4_1, 5_2, 6_1, 7_2, 8_4, 9_2, 10_2, 11_2, 12_2, 13_1, 14_3, 15_2, 16_4,
17_3, 18_1, 19_4, 20_1, 21_2, 22_4, 23_2, 24_3, 25_3, 26_1, 27_1, 28_1, 29_4, 30_4,
31_4, 32_3, 33_4, 34_2, 35_3, R.
5.2 Rule generation
A search was made for all possible rules with R as a consequent and the complete set of rules that resulted from this analysis are displayed in Table 5. We

Using metarules to organize and group discovered association rules
Table 5 Generated
association rules

421

Rule Antecedents
index

Consequent Support Confidence
(%)
(%)

r1
r2
r3
r4
r5
r6
r7
r8
r9
r10
r11
r12
r13
r14
r15
r16
r17
r18
r19
r20
r21
r22
r23
r24
r25

R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R

1_2, 13_2, 11_1
3_2, 13_2, 11_1
3_2, 11_1, 14_4
20_2, 3_2, 11_1, 5_1
20_2, 3_2, 11_1, 10_1
20_2, 4_3, 11_1, 5_1
20_2, 4_3, 11_1, 10_1
17_2, 3_2, 11_1, 5_1
17_2, 3_2, 11_1, 10_1
12_3, 4_3, 2_2, 11_1
1_3, 4_3, 2_2, 11_1
1_2, 3_2, 11_1, 5_1
1_2, 3_2, 11_1, 10_1
1_2, 13_2, 11_1, 5_1
1_2, 13_2, 11_1, 10_1
1_2, 13_2, 11_1, 33_4
1_2, 11_1, 14_4, 5_1
1_2, 11_1, 14_4, 10_1
1_2, 11_1, 14_4, 8_3
3_2, 13_2, 11_1, 5_1
3_2, 13_2, 11_1, 10_1
3_2, 13_2, 11_1, 8_3
3_2, 13_2, 11_1, 33_4
4_3, 2_2, 11_1, 5_1
4_3, 2_2, 11_1, 10_1

1.2
1.6
1.1
1.1
1.1
1.1
1.1
1.0
1.0
1.1
1.2
1.7
1.7
1.2
1.2
1.0
1.4
1.4
1.1
1.1
1.2
1.1
1.3
1.5
1.5

81.8
82.1
80.6
80.5
80.6
85.4
85.0
81.0
81.0
80.6
80.0
80.9
81.0
87.8
87.9
82.9
86.8
86.8
83.7
91.0
91.1
84.1
81.1
82.0
82.0

used a confidence threshold of 80%. Note that the support of the discovered
rules is in the order of 1%. It is low because we are interested in explaining the
occurrences of a rare consequent R.
The discovered rules are redundant and some of them are contained in others. The 35 variables are not all independent from each other. Consider, for
example, the rules r1 and r14 . All the antecedents of r1 are also antecedents of
r14 . The set of rows that support r14 is included in the set of rows that supports
r1 so that r14 is more specific than r1 .

5.3 Finding metarules and rule organization
After finding the rules, we followed the method in Sect. 4 to generate the metarules and then used them to organize the discovered association rules. The
metarules discovered from the rules from Table 5 are summarized in Table 6.
Note that we used a confidence threshold of 80%. Using a higher confidence
threshold, for example 90%, would result in fewer metarules. The support
threshold used to generate the metarules is 10%. These metarules explain the
dependency between the 25 rules from Table 5. For example mr1 and mr2 show
a mutual dependency between the rules r6 and r7 .

422

A. Berrado, G. C. Runger

Table 6 Discovered
metarules

Rule
index

Antecedents

Consequent

Support
(%)

Confidence
(%)

mr1
mr2
mr3
mr4
mr5
mr6
mr7
mr8
mr9
mr10
mr11
mr12
mr13
mr14
mr15
mr16
mr17
mr18
mr19
mr20
mr21
mr22
mr23
mr24
mr25
mr26
mr27
mr28
mr29
mr30
mr31
mr32
mr33
mr34
mr35
mr36
mr37
mr38
mr39
mr40
mr41
mr42
mr43

r6
r7
r25
r24
r19
r19
r8
r9
r22
r4
r5
r17
r18
r16
r14
r16
r15
r16
r1
r23
r2
r14
r15
r20
r21
r14
r1
r20
r21
r15
r1
r20
r21
r20
r20
r20
r20
r21
r21
r21
r21
r12
r13

r7
r6
r24
r25
r17
r18
r9
r8
r2
r5
r4
r18
r17
r14
r16
r15
r16
r1
r16
r2
r23
r15
r14
r14
r14
r1
r14
r15
r15
r1
r15
r21
r20
r1
r2
r12
r13
r1
r2
r12
r13
r13
r12

19.6
19.6
28.3
28.3
19.6
19.7
19.9
19.9
20.7
21.9
21.9
24.8
24.8
17.8
17.8
17.9
17.9
19.2
19.2
26.2
26.2
21.5
21.5
16.3
16.3
21.5
21.5
16.3
16.4
21.7
21.7
20.1
20.1
16.3
20.1
16.3
16.3
16.4
20.2
16.3
16.4
33.6
33.6

100
99.2
100
100
92.2
93
100
100
100
100
99.3
100
92.3
82.4
92.3
93.2
82.6
100
81.8
100
81.5
100
99.2
81.1
80.5
100
91.6
81.1
81.3
100
92.3
100
99.2
81.1
100
81.1
81.1
81.3
100
80.5
81.3
100
99.5

Refer to Fig. 3 for the graphical presentation of the metarules. Figure 3 shows
that the discovered association rules are organized into several subgroups as
follows:
•
•
•

A cluster grouping the following eleven rules: (r1 , r16 , r14 , r15 , r12 , r13 , r20 , r21 ,
r2 , r23 , r22 ).
A cluster of three rules: (r17 , r18 , r19 ).
Four clusters that group two rules each: (r6 , r7 ), (r4 , r5 ), (r8 , r9 ), and (r24 , r25 ).

Using metarules to organize and group discovered association rules

423

Fig. 3 Discovered metarules

•

And finally three individual rules that did not participate in any metarule:
r3 , r10 , and r11 .

With this new organization of the rules into independent subgroups, it is much
easier to analyze and understand the rules.
5.4 Grouping the equivalent rules and pruning the more specific ones
Our ability to analyze the rules is enhanced further after more processing of the
rules within each subgroup. Within each subgroup, we simplified the graphical
presentation by grouping into the same node the equivalent rules. The reorganized rules are plotted in Fig. 4. The three individual rules which don’t belong
to any subgroup were not affected by this step. The subgroups with two nodes
were reduced to one node. The subgroup with three nodes was reduced to a
subgroup with two nodes. Finally, the subgroup with ten nodes was reduced to
seven nodes.
Pruning the contained rules according to Definition 3 would result in pruning
rules r14 and r15 since they are more complex than r1 . Notice that some of the
subgroups could be simplified further. For instance, if we combine knowledge
about the process with information about the more specific rules we might
decide to prune rule r19 and possibly prune the node containing the rules r20
and r21 as well. We might also decide to group the rules (r1 , r14 , r15 ) with rule

424

A. Berrado, G. C. Runger

Fig. 4 Metarules after
grouping equivalent rules

Fig. 5 Metarules after
pruning the more specific
rules

r16 into the same node. The final set of subgroups of rules after grouping and
pruning several rules is described in Fig. 5. Each group of rules could then be
further analyzed by looking at the variables involved in each of the rules within
the same group.
This application justifies our original thesis that finding metarules enhances
our understanding of the discovered association rules by braking them down
into independent subgroups and pruning some overfitting rules within the subgroups. The reduced subgroups of rules are more actionable than the complete
set of discovered rules.

Using metarules to organize and group discovered association rules

425

5.5 Comparison with rules clustering approaches using distance metrics
In this section, we compare the groups formed using the metarules approach in
Fig. 3 with the clusters formed using two different distance metrics to group association rules. Let d1 and d2 refer to the distance metric suggested in (Toivonen
et al. 1995) and (Gupta et al. 1999), respectively. We used hierarchical clustering
algorithms with both distance metrics and experimented with different linkages.
Figures 6 and 7 illustrate, respectively, the dendrograms obtained with Ward
linkage using the distance metrics d1 and d2 , respectively.
First, note that cluster assignment differs when using different distance metrics. Take, for instance, rules 20 and 21. When using d1 they were initially
grouped with rules 1 and 14–16, and then with rules 2, 23, and 22. When using
d2 , rules 20 and 21 where grouped with rules 2, 23, and 22 before grouping them
with rules 1 and 14–16. Furthermore, Figs. 6 and 7 agree that rules 12 and 13 are
first clustered with rules 4, 5, 8, and 9 while the metarules approach suggests that
rules 20 and 21 have greater than 80% confidence to rules 12 and 13. We reduced
the confidence level to understand the clustering results between rules 12, 13
and 4, 5 and we found the following metarules: r4 → r12 with confidence 72.9%
and r5 → r12 with confidence 72.4% and r4 → r13 with confidence 72.9% and
r5 → r12 with confidence 73.1%, the confidence levels of these metarules are
all inferior to those of the metarules mr36 , mr37 , mr40 , and mr41 relating rules
12, 13, 20, and 21 found in Table 6.
Another major difference between the metarules in Fig. 3 and the clusters
of Figs. 6 and 7 is that the latter do not indicate the nature of the relationships between the clustered rules. Greater detail is obtained from the ability
of the metarules to provide insights into containment. For example, mr18 indicates 100% confidence for r16 → r1 , but mr19 shows only 81.8% confidence

Similarity

−296.58

−164.39

− 32.19

100.00

1 14 15 16 20 21 2 23 22 3 17 18 19 4

5

8

9 12 13 6

7 10 11 24 25

Rules

Fig. 6 Dendrogram of clustering association rules with Ward linkage and distance metric d1

426

A. Berrado, G. C. Runger

−259.56

Similarity

−139.71

−19.85

100.00

1 15 14 16 2 23 22 20 21 3 17 18 19 4

5

8

9 12 13 6

7 10 11 24 25

Rules

Fig. 7 Dendrogram of clustering association rules with Ward linkage and distance metric d2

for r1 → r16 . The clustering results blend these two implications and place r1
and r16 close to each other in the dendrograms, without a clear description of
the 100% one-way relationship. Although the distance metrics give a sense of
proximity between the rules, they do not indicate the level of containment or
overlap of rules. Furthermore, the conclusions above continue to hold when
linkages such as complete, average and single were used instead of Ward in the
comparisons above.

6 Experimental evaluation
We show the organization provided by metarules on the following six benchmark data sets obtained from the UC Irvine ML repository (Blake and Merz
1998) and a Microarray data set obtained from the Kent Ridge Bio-medical
data set repository (Li and Liu 2002).
•
•
•
•
•
•
•

Iris Plants data (iris),
Johns Hopkins University Ionosphere data (ion),
Statlog Project Heart Disease data (hea),
Thyroid Disease data (thy),
Attitudes Toward Workplace Smoking Restrictions data (smo), and
Mushroom data (mush),
Ovarian Cancer (OvaCan).

The data sets, as described in Table 7, are of different sizes and varying number and types of attributes. The continuous attributes in the data sets used were
discretized using a 4-bin equal-frequency discretization. After discretization we
reduced the number of attributes of the Ovarian Cancer data set from 15,154 to

Using metarules to organize and group discovered association rules

427

Table 7 Major properties of the data sets considered in the experimentation
Properties

No of classes
No of examples
No of attributes
No of continuous attributes

Data sets
Iris

Thy

Ion

Smo

Hea

Mush

Ovacan

3
150
4
4

3
7,200
21
6

2
351
34
32

3
2,855
13
2

2
270
13
6

2
8,124
22
0

2
253
500
500

500 by random selection. The a priori algorithm itself is known to consume too
much memory with sparse data sets and this prevents it from returning any rules.
The experimental results are summarized in Table 8 where each row describes
the results of one experiment that consisted of the following steps:
•

•
•

Find the set of rules with the consequent matching the class labels specified
in column 2. The support and confidence thresholds used to find association
rules are reported in the third column. It should be noted that the thresholds used differ between class labels and data sets since the class labels were
not distributed evenly on the data cases. The number of rules found in each
case can be found in column 4. We used the default maximum itemset size
for a priori except for the Ovarian Cancer data set where we reduced it to
four in order to avoid too much memory consumption and enable a priori
to return rules.
Metarules were then mined following the procedure described in Sect. 4.1
with a support threshold of 0% and a confidence threshold of 100%. The
number of metarules discovered is reported in column 5.
After mining the metarules, the definition of rule equivalence given in Sect.
4.3 Definition 2 was applied and the number of remaining nodes after grouping the equivalent rules is reported in column 7. The number of remaining
metarules in the simplified graph can be found in column 6. Column 8 is the
ratio of the reduced number of metarules to the initial number of metarules.
Finally column 9 indicates the ratio of the reduced number of rules to their
initial number.

It should be noted that we do not prune the contained or the more specific
rules. Our objective is to illustrate the effect of grouping equivalent rules on
the metarule graphs.
The experimental results indicate that grouping the equivalent rules in the
same node leads to the simplification of the metarules graph. The extent of simplification of the metarules graph varies between the different cases considered
both in terms of number of remaining rules and number of remaining metarules;
in some cases, such as for both class labels of the mushroom data set, both class
labels of the ovarian cancer data set, for the first two class labels of the thyroid
disease (thy) data set and for the first two class labels of the smoking (smo)
data set, the metarule graphs were reduced more compared to the remaining experimental cases. The greatest simplification of rules was noted for the

Absence(1)
Presence(2)
Unrestricted(0)
Prohibited(1)
Restricted(2)
Setosa
Virginica
Versicolor
Good
Bad
Class (1)
Class (2)
Class (3)
Poisonous
Edible
Normal
Cancer

Hea

Ovacan

Mush

Thy

Ion

Iris

Smo

Class labels

Data
sets

Table 8 Experimental results

10/80
10/90
0.05/85
0.01/85
1/85
1/95
1/95
1/95
10/95
10/95
0.01/95
0.01/95
25/95
20/95
20/95
20/80
20/85

%Supp/%Conf
of rules

276
95
326
267
204
51
59
46
1,115
100
139
669
634
846
528
56
1,651

No of
rules

1,321
250
7,093
1,683
546
318
334
138
9,032
206
7,532
39,920
6,277
3,64,522
85,273
336
59,572

No of
metarules

421
96
17
46
84
102
155
57
1,369
52
7
53
2,638
95
858
0
945

No of
simplified
metarules
177
69
50
129
137
32
42
33
469
33
17
60
490
27
93
8
292

No of
simplified
rules
31.87
38.40
0.24
2.73
15.38
32.08
46.41
41.30
15.16
25.24
0.09
0.13
42.03
0.03
1.01
0
1.58

% of
remaining
metarules (%)

64.13
72.63
15.34
48.31
67.16
62.75
71.19
71.74
42.06
33.00
12.23
8.97
77.29
3.19
17.61
14.28
17.68

% of
remaining
rules (%)

428
A. Berrado, G. C. Runger

Using metarules to organize and group discovered association rules

429

Table 9 Effect of confidence threshold for metarules mining on the metarules graph representing
the rules predicting the poisonous category from the mushroom data set
Confidence

No of
metarules

No of Simplified
metarules

No of Simplified
rules

Remaining
metarules(%)

Remaning
rules (%)

100%
90%
80%

3,64,522
5,19,275
5,26,514

95
19
18

27
10
8

0.03
0.00
0.00

3.19
1.18
0.95

category Poisonous for the mushroom data set, where the number of rules
went down from 846 to 27 rules. The highest simplification of metarules was,
however, noted for the category Normal for the Ovarian Cancer data set, the
number of metarules was reduced from 336 metarules to none. Finding the
equivalent rules makes it easier to process the rules and the reduced metarules graph indicates the relationships between these groups of rules. Note that
these results were achieved with a conservative metarule confidence of 100%.
Relaxing the confidence threshold for metarules mining would further simplify
the metarules graph. The following experiment illustrates how the metarules
graph is simplified with the reduction of the confidence threshold for metarules
mining. Consider the mushroom data with the class label “Poisonous.” As the
confidence threshold was reduced from 100 to 90% then 80%, the number of
simplified rules and simplified metarules decreased as reported in Table 9.
Reducing the confidence threshold of metarules results in more metarules
which in turn leads to more equivalent rules. This naturally leads to a decrease in
the percentage of rules and metarules after grouping the equivalent rules. Once
all the equivalent rules have been grouped, no further simplification occurs
with the reduction of the confidence threshold of metarules mining. Consequently, the analyst can decide on a convenient confidence threshold in order
to make the metarules graph match the redundancy to discover in the rules
used in the application. A default of 100% confidence still resulted in dramatic
reductions in these examples. Each group of equivalent rules in the metarules
graph can then be further investigated individually to understand the structure
of its representative rules. The simplifications in these examples did not yet use
rule pruning based on antecedent itemsets or other criteria. We consider metarules as a first step to learn of the attribute masking and redundancy generated
from sparse data in high dimensions that can be followed by further pruning.

7 Conclusion
Finding association rules is an efficient way to uncover all the hidden regularities in massive high-dimensional data. The huge number of discovered rules
is a handicap for the human user to assimilate all the information provided by
the rules. Since the rules express relationships that exist indeed in the data it
is not prudent to arbitrarily prune and eliminate some of the rules. This paper introduces a new approach based on metarules and a graphical display

430

A. Berrado, G. C. Runger

to partition the association rules with the same consequent into independent
subgroups of rules without assumptions about relevance of rules. These subgroups of rules are simplified even more after grouping the equivalent rules.
The organized collection presents only data-determined relationships between
the rules. Domain knowledge can also be applied as appropriate to possibly
prune the more specific ones. The data analyst can analyze each of the formed
subgroups of rules individually and therefore understand the relationships that
hold between the data in different regions of the measurement space.

References
Agrawal R, Imielinski T, Swami A (1993) Mining association rules between sets of items in large
relational databases. In: Proceedings of ACM SIGMOD international conference on management of data, Washington, DC, USA, pp 207–216
Aumann Y, Lindell Y (2003) A statistical theory for quantitative association rules. J Intell Inf Syst
20(3):255–283
Bay SD, Pazzani MJ (2001) Detecting group differences: mining contrast sets. Data Min Knowl
Discov 5(3):213–246
Bayardo R, Agrawal R, Gunopulos D (2000) Constraint-based rule mining in large, dense databases.
Data Min Knowl Discov J 4:217–240
Blake CL, Merz CJ (1998) UCI repository of machine learning databases. UC Irvine, Deptatment
Information and Computer Science. http://www.ics.uci.edu/ mlearn/MLRepository.html
Chawla C, Davis, J Pandey G (2004) On local pruning of association rules using directed hypergraphs. In: Proceedings of the 20th international conference on data engineering, Boston, MA,
USA, p 832
Dougherty J, Kohavi R, Sahami M (1995) Supervised and unsupervised discretization of continuous
features. In: Proceedings of the international conference on machine learning, Tahoe City, CA,
USA, pp 194–202
Fukuda T, Morimoto Y, Morishita S, Tokuyama T (1999) Mining optimized association procedures.
Rules for numeric attributes. J Comput Syst Sci 58:1–12
Gupta KG, Strehl A, Ghosh J (1999) Distance based clustering of association rules. In: Proceedings of ANNIE, intelligent engineering systems through articial neural networks, St. Louis, MO,
USA, pp 759–764
Han J, Pei J, Yin Y (2000) Mining frequent patterns without candidate generation. In: Proceedings
of ACM SIGMOD international conference on management of data, Dallas, TX, USA, pp 1–12
Huang S, Webb GI (2005a) Discarding insignificant rules during impact rule discovery in large databases. In: Proceedings of the SIAM 2005 data mining conference, Newport Beach, CA, USA,
pp 541–545
Huang S, Webb GI (2005b) Pruning derivative partial rules during impact rule discovery. In: Proceedings of PAKDD the 9th Pacific-Asia conference on advances in knowledge discovery and
data mining. Lecture notes in computer science, vol 3518, Hanoi, Vietnam, pp 71–80
Klemettinen M, Mannila H, Ronkainen P, Toivonen H, Verkamo AI (1994) Finding interesting rules
from large sets of discovered association rules. In: Proceedings of the international conference
on information and knowledge management, CIKM, Gaithersburg, MD, USA, pp 401–407
Lent B, Swami A, Widom J (1997) Clustering association rules. In: Proceedings of the 13th international conference on data engineering, IEEE Computer Society, Birmingham, UK, pp 220–231
Li J, Liu H (2002) Kent ridge bio-medical data set repository, Institute for Infocomm Research.
http://sdmc.lit.org.sg/GEDatasets/Datasets.html
Li W, Han J, Pei, J (2001) Accurate and efficient classification based on multiple classassociation rules. In: International conference on data mining, ICDM, San Jose, CA, USA,
pp 369–376
Liu B, Hsu W (1996) Post analysis of the learned rules. In: Proceedings of the 13th national
conference on artificial intelligence, Portland, OR, pp 828–834

Using metarules to organize and group discovered association rules

431

Liu B, Hsu W, Chen S (1997) Using general impression to analyze discovered classification rules.
In: Proceedings of the 3rd international conference on knowledge discovery and data mining,
Newport Beach, CA, USA, pp 31–36
Liu B, Hsu W, Ma Y (1998) Integrating classification and association rule mining. In: Proceedings
of the 4th international conference on knowledge discovery and data mining, KDD, New York,
USA, pp 80–86
Liu B, Hsu W, Ma Y (1999) Pruning and summarizing the discovered associations. In: Proceedings of
ACM SIGKDD international conference on knowledge discovery and data mining, San Diego,
CA, USA, pp 125–134
Liu H, Hussain F, Tan CL, Dash M (2002) Discretization: an enabling technique. Data Min Knowl
Discov 6(4):393-423
Ng R, Lakshmanan LVS, Han J, Pang A (1998) Exploratory mining and pruning optimizations of
constrained associations rules. In: Proceedings of ACM SIGMOD international conference on
management of data, Seattle, WA, pp 13–24
Padmanabhan B, Tuzhilin A (1998) A belief-driven method for discovering unexpected patterns.
In: Proceedings of the 4th international conference on knowledge discovery and data mining,
KDD, New York, USA, pp 94–100
Quinlan JR (1992) C4.5: program for machine learning. Morgan Kaufmann, Los Atlos, CA
Scott DW, Thompson JR (1983) Probability density estimation in higher dimensions. In: Proceedings of the 15th symposium on the interface, North-Holland, pp 173–179
Silberschatz A, Tuzhilin A (1996) What makes patterns interesting in knowledge discovery systems.
IEEE Trans Knowl Data Eng 8(6):970–974
Srikant R, Agrawal R (1996) Mining quantitative association rules in large relational tables. In:
Proceedings of the ACM SIGMOD conference on management of data, Montreal, Quebec,
Canada
Srikant R, Vu Q, Agrawal R (1997) Mining association rules with item constraints. In: Proceedings
of the 3rd international conference on knowledge discovery and data mining, KDD, Newport
Beach, CA, USA, pp 67–73
Tan P-N, Kumar V, Srivastava J (2002) Selecting the right interestingness measure for association
patterns. In: Proceedings of the 8th ACM SIGKDD, international conference on knowledge
discovery and data mining, Madison, WI, USA, p 183
Toivonen H, Klemetinen M, Ronkainen P, Hatonen K, Mannila H (1995) Pruning and grouping discovered association rules. In: Proceedings of the Mlnet workshop on statistics, machine learning,
and discovery in databases, Heraklion, Crete, Greece, pp 47–52
Yang Y, Webb GI (2002) A comparative study of discretization methods for naive-bayes classifiers.
In: Proceedings of the Pacific Rim knowledge acquisition workshop, PKAW, Tokyo, pp 159–173

JMLR: Workshop and Conference Proceedings 16 (2011) 59–69 Workshop on Active Learning and Experimental Design

Active Batch Learning with Stochastic Query-by-Forest
(SQBF)
Alexander Borisov

alexander.boriosv@intel.com

Intel, Nizhniy Novgorod, Russia

Eugene Tuv

eugene.tuv@intel.edu

Intel, Chandler, AZ, USA

George Runger

runger@asu.edu

Arizona State University, Tempe, AZ, USA

Editor: I. Guyon, G. Cawley, G. Dror, V. Lemaire, and A. Statnikov

Abstract
In a conventional machine learning approach, one uses labeled data to train the model.
However, often we have a data set with few labeled instances, and a large number of
unlabeled ones. This is called a semi-supervised learning problem. It is well known that
often unlabeled data could be used to improve a model. In real world scenarios, labeled data
can usually be obtained dynamically. However, obtaining new labels in most cases requires
human effort and/or is costly. An active learning (AL) paradigm tries to direct the queries
in such way that a good model can be trained with a relatively small number of queries.
In this work we focus on so-called pool-based active learning, i.e., learning when there is a
fixed large pool of unlabeled data, and we can query the label for any instance from this
pool at some cost. Existing methods are often based on strong assumptions for the joint
input/output distribution (i.e., a mixture of Gaussians, linearly separable input space, etc.),
or use a distance-based approach (such as Euclidean or Mahalanobis distances). That makes
such methods very susceptible to noise in input space, and they often work poorly in high
dimensions. Also, such methods assume numeric inputs only. In addition, for most methods
relying on distance computations and/or linear models, computational complexity scales at
least quadratically with respect to the number of unlabeled samples, rendering them useless
on large datasets. In real world applications data is often large, noisy, contains irrelevant
inputs, missing values, and mixed variable types. Often queries should be arranged in
groups or batches (this is called batch AL). In batch querying one should consider both
the ’usefulness’ of individual queries within a batch, and the batch diversity. Batch AL,
although being very practical by nature, is rarely addressed by existing AL approaches.
Here we propose a new non-parametric approach to the AL problem called Stochastic Query
by Forest (SQRF), that effectively addresses the challenges described above. Our algorithm
is based on a QBC algorithm applied to an RF ensemble, and our main contribution is the
batch diversification strategy. We describe two different strategies for batch selection, the
first of which achieved the highest average score on the AISTATS 2010 active learning
challenge and ranked top on one of the challenge datasets. Our work focuses on binary
classification problems, but our method can be directly applied to regression or multi-class
problems with minor modifications.
Keywords: tree ensembles, query by committee, random forest

c 2011 A. Borisov, E. Tuv & G. Runger.


Borisov Tuv Runger

1. Introduction
The basic idea behind active learning (AL) is that a regression or classification algorithm
can achieve better performance on limited data when it is allowed to choose the data for
learning. In pool-based active learning, we are given a large fixed pool of unlabeled data,
and are allowed to query the target value for each unlabeled instance at a given cost. Here
we assume equal unit cost for all queries. The model is first built on all labeled instances,
then we query an instance that is considered the most useful and update the model. The
goal is to achieve a better learning curve (error vs. total query cost) for a model, compared
to querying labels at random. The main challenge for an AL algorithm is computing the
“utility” on unlabeled instances. The usual intuition behind the utility function is to select
instances in dense regions of input distribution, or in regions of low sampled density, or
where the is the most “uncertainty” in the model. For a comprehensive review of AL
approaches see Settles (2009). Below we outline the most commonly used AL approaches.
Uncertainty sampling (see Lewis and Gale (1994) for example). Suppose we have a model
that can report class probabilities pi , i = 1 . . . K, where K is number of classes. Then all
unlabeled instances are ranked according to the current model uncertainty measure (for a
classification problem this is usually computed as 1 − max(pi )). The next instance queried
is the instance with the largest uncertainty, thus avoiding the instances that are predicted
with high confidence.
Query-by-committee, or QBC (see Seung et al. (1992), Freund et al. (1997)). This
approach first constructs an ensemble (committee) of diverse base learners, then ranks
all unlabeled instances with respect to a committee disagreement measure. Disagreement
can be computed as the entropy of predicted class probabilities over committee members,
or in various other ways. Here one tries to select instances that represent regions of input
space that are not covered by existing learners in the committee. QBC discourages querying
instances from the same region of the input distribution where good prediction is impossible
by nature (an inherent weakness of uncertainty sampling).
However, methods like uncertainty sampling and QBC do not take global properties
of the input distribution into account, and can spend too much time querying outliers or
sparsely populated regions. Density based methods try to overcome this issue by incorporating the input data density into the utility function. The resulting utility function for a
data point x is computed as U (x) · D(x)p , where U (x) is an expected utility, and D(x) is an
estimated input density. The parameter p controls the influence of the density factor. This
encourages querying from more densely populated regions of input space. See, for example,
Xu et al. (2007) for a density-based method in relevance feedback.
Another approach that uses global input distribution information directly minimizes the
expected model generalization error (expected risk). This is similar in nature to a Bayesian
experimental design Chaloner and Verdinelli (1996). However, the expected risk can be
computed in closed form only for a limited class of models, such as a mixture of Gaussians,
or SVMs. Such methods often have high computational complexity because the model has
to be rebuilt with each query, and the utility recalculated. For an example of an algorithm
that solves those problems with a fast model update and utility recalculation strategy using
Gaussian Random Fields (GRF) model see Zhu et al. (2003). GRF also naturally uses
unlabeled data for learning (performs semi-supervised learning).

60

Active Batch Learning with Tree Ensembles

Expected variance reduction tries to reduce the expected variance of the model prediction. It is well known that model error can be decomposed into noise (term independent
from model), bias (a term specific to the selected model class that estimates the difference
between the best target function in the model class and the real underlying target), and
variance. Given a model class (for example, linear functions or trees), noise and bias cannot
be influenced by the model. Expected variable reduction selects instances that minimize the
expected variance for the model. However, an estimate of the expected variance is also only
available for a very limited class of models. Expected model change tries to select instances
that maximize the model change with respect to the addition of a selected instance to the
training set. This, however, does not guarantee model error improvement, just diversity of
queries. Estimation of model change is also only possible for a limited number of model
classes.
However an issue with most of the approaches described above is that they do not work
with large and/or noisy data, or use very limiting assumptions on the model class. For example, methods relying on any distance metrics are susceptible to the curse of dimensionality
(usually do not work well for more than 10-20 inputs), are sensitive to feature scaling and
incur the additional complexity of calculating distances (although the later problem can be
partially solved by clustering). Linear models or EM with a mixture of Gaussians rarely
fit complex distributions in real world data. Any kernel-based method like SVM and GRF,
also requires an approximately correct estimation of the kernel width parameter, and that
is in itself a complex task for high-dimensional noisy data.
As stated earlier, querying more than one instance at time (batch learning) often can
greatly reduce the labeling effort and computation time. For example, one does not need
to rebuild the model for each queried instance, and parallel labeling is possible. In real
problems labeling and/or querying is often done by human experts and processing unlabeled
instances one-by-one is more costly than in groups (batches). However, batch learning
introduces an additional challenge compared to single instance queries. In addition to
optimizing individual queries, one must make sure that instances in the batch are diverse
enough. That is the reason why a greedy selection of instances with the highest utility
does not work. In addition, a batch learning algorithm should be fast enough, compared to
querying instances one-by-one, to be useful. Several approaches to batch learning (Brinker
(2003), Xu et al. (2007)) also use a greedy selection algorithm with a modified utility criteria.
In the first work, for example, the authors use a linear combination of utility and diversity
measures with a SVM model. Diversity for each sample measures how far it is from the
other samples in the batch. The second article Xu et al. (2007) introduces a “Relevance,
Diversity and Density” batch learning framework. Relevance considers individual instance
utility, density promotes sampling from more populated regions of the distribution, and
diversity ensures that samples within the batch are not close to each other.
But, in practice, one often deals with very large datasets (with potentially hundreds to
thousands of inputs and/or up to millions of instances), especially given the fact that AL
deals with large amounts of unlabeled data. Also the data are usually very noisy and contain
categorical variables, so that approaches based on linear models or Euclidean/Mahalanobis
distance metrics are not computationally feasible. This also prohibits usage of “global”
methods like empirical risk minimization because they usually rely on distance-based models
like SVM or GRF. Neural Networks also rarely perform well with large and noisy data with
61

Borisov Tuv Runger

an unknown distribution. Mixture models and clustering approaches fail when the data do
not contain easily separable clusters. Many prospective AL approaches, for example GRF,
are not well suited for batch learning, and each query involves quadratic complexity in the
current number of labeled samples for the model update, resulting in a total of (O(N 3 )
complexity for queries and model update with N instances in the initial unlabeled data
pool (given that the initial number of labeled instance is very small compared to N ). So
most of the methods described above can only handle several thousand samples and tens of
variables, severely limiting their practical application.
In this work we propose a nonparametric batch AL method using tree ensembles that
works with huge data sets and overcomes most of the problems described above. We introduce decision tree ensembles in Section 2. Section 3 contains a detailed description of
our algorithm. Then we describe successful application of our method to AISTATS 2010
active learning challenge problems that provide a very good representation of real life active
learning tasks.

2. Decision tree models and tree ensembles
As stated above, to effectively deal with active learning problem one needs to impose some
reasonable assumptions on the joint input/output data distribution. Those assumptions
are represented by a particular data model. Among models used with huge, noisy and heterogeneous data, a very popular choice is the decision tree–because trees are fast to learn,
resistant to outliers and noise and provide good predictive accuracy. Trees are usually
induced in a recursive, greedy fashion. For each node the best split (split with greatest
impurity reduction) is selected, then the process is repeated in the child nodes. For example, the CART algorithm described in Hastie et al. (2001) can be used. Commonly used
node impurity measures are the variance of the target (for regression) or the Gini index (for
classification). However, trees often suffer from instability, or low predictive power if the
underlying data model is complex. Significant improvements over single tree models can be
achieved with tree ensembles, sequential (Gradient Boosting Trees (GBT), Multi-class Logistic Regression Trees (MCLRT), Adaboost, see Hastie et al. (2001)) and parallel (Random
Forest (RF), see Breiman (2001)). We briefly describe RF, because it is used extensively in
our active learning algorithm, although we use a GBT model as the final predictor (using
samples queried by our AL algorithm). We refer to Hastie et al. (2001) for details on a
GBT algorithm for the multi-class case and omit it here.
The idea behind RF is to combine many diverse trees into an ensemble. This allows
for more stable model, reduces over-fitting, and improves predictive accuracy over a single
tree. RF constructs a number of independent trees, each tree is built on a random portion
(60% for example) of the training (labeled) samples. Additional diversity within the tree
is added via split randomization. Instead of selecting the single best split among the best
splits on each variable like CART does, a random,
small subset of variables is selected at
√
each tree node (a commonly used setting is M , where M is the total number of variables).
Then the best split is selected only within this subset. Prediction from an RF model is
obtained with averaging for regression, and voting in classification (where the number of
trees that predict each target class are counted and the most frequent class is selected as
the predictor). RFs are usually applied to classification problems, because combining many

62

Active Batch Learning with Tree Ensembles

weak trees via averaging does not always result in a better model in regression settings.
RF is especially attractive for use with QBC, because it naturally introduces base learner
diversity, and each tree has an intrinsic prediction probability estimate computed from the
class proportions in the terminal node of the selected instance. RF can also be used to
estimate various other properties of the joint data distribution, such as density, outlier
scores, variable importance measures, or (supervised) distance metrics (Breiman (2001)).

3. Tree ensemble approach for active learning
Here we describe two our algorithms for batch AL. Denote N0 , N1 as the counts of the
target classes in the labeled data, pc = Nc /N, c = 1, 2 as the target class proportions in
the currently labeled data, and the i-th tree in ensemble G as Ti = Ti (G), i = 1 . . . R, where
R is number of trees in the ensemble. Denote the terminal node of the i-th tree containing
instance x as Ti (x), and pic (x) as the predicted class probabilities in the i-th tree for x,
computed as the target class proportions in node Ti (x). Denote the same probabilities
weighted with class priors
pic (x)/pc
p0ic (x) = P
, c = 1, 2
c pic (x)/pc
Algorithm 1. Stochastic query by forest.
1. Build an RF ensemble G (we used 700 shallow trees with depth = 2-6, depending on
the current labeled data size).
2. For each sample, compute the committee disagreement q(x) = sd(p0ic (x)) as the standard deviation of the weighted rare class probability among the ensemble of trees. Then
sort all remaining unlabeled instances with respect to q(x), so that q(x1 ) > q(x2 ) >
. . . > q(xnu ), where nu is the number of remaining unlabeled instances.
3. Sample the next batch randomly from x1 , . . . xαnu . Parameter α controls the discarded
fraction of unlabeled instances, and indirectly introduces a tradeoff between randomness and high utility. We set α = 2/3 in our experiments. Sampling probabilities
are computed from utility scores as following. Denote the threshold q0 = q(xαnu ),
and L(x) = (q(x) − q0 )/(q(xP
1 ) − q0 ). Then the sampling probability of instance x is
computed as psel (x) = L(x)/ x L(x).
4. Sample the batch from the remaining unlabeled instances with the computed sampling
probabilities. Rebuild model G and return to step 2 (until no unlabeled instances are
left in the pool).
This method addresses both the uncertainty score and the input density (as random
sampling selects more instances from the dense regions of the input distribution). At the
same time we enforce diversity within the batch through randomization. Instead of scoring
uncertainty with a single class probability estimate, the tree ensemble allows an embedded
uncertainty score to be calculated directly from the differences between the individual learners (as the standard deviation). This approach is distinct from other QBC approaches, and
this provides one of our contributions. Also, the standard deviation is simple uncertainty
63

Borisov Tuv Runger

score and alternatives (such as more robust measures) could be useful. We would not expect
the results to change substantially with alternative measures.
Our perspective is that the tree ensemble is useful for this uncertainty score, but that
in addition the tradeoff between utility and randomness is a key component of our strategy. The importance of a random element was illustrated by Cawley (2010) who concluded
that a simple, random baseline method was competitive with more complex strategies. He
conjectured that uncertainty sampling does not sufficiently explore the feature space and,
instead, tends to expend samples to exploit the current knowledge of the likely decision
boundary. Our tradeoff between uncertainty and randomness is easily managed by our
alpha parameter, and in the challenge data we used a sizeable proportion (alpha = 2/3)
of random sampling, but not entirely random. Also, our implementation of step 3 of the
algorithm 1 uses rejection sampling to avoid a quadratic complexity in the number of unlabeled instances. When the number of rejected instances becomes too large, we just select
the remaining instances with highest utility (although it occurs very rarely in practice).
Our second approach directly addresses diversity and density in a way similar to Brinker
(2003). Suppose we present all labeled and unlabeled data through RF model G (resulting
in each instance being assigned to a terminal node for each tree). Denote the labeled data
count in a node T as l(T ), and the total count as s(T ). Then we can estimate the expected
proportion d(x) of the labeled
instance P
density to the total density in the neighborhood
P
R
l(T
(x))/
of instance x as d(x) = R
i
i=1 s(Ti (x)). The inverse proportion of labeled
i=1
instances in the neighborhood can be used instead of local density as a multiplier for the
utility measure, because it promotes both queries in dense regions, and in regions with few
labeled points. Below is the detailed description of the algorithm that uses this modified
utility function.
Algorithm 2. Local density based query-by-committee.
1. Build an RF ensemble G.
PR
P
2. Compute l(x) = R
i=1 s(Ti (x)) for each unlabeled instance.
i=1 l(Ti (x)) and s(x) =
3. Compute a modified utility score q 0 (x) = q(x)/d(x) = q(x)s(x)/l(x) for all unlabeled
data. Then sort all remaining instances with respect to q 0 (x), so that q 0 (x1 ) > q 0 (x2 ) >
. . . > q 0 (xnu ). Initialize query count Q = 0
4. Select an instance with the highest value of q(x)/l(x) from x1 , . . . xn0 , where n0  nu
is a predefined number of lookup instances. We set n0 = min(1000, nu ).
5. Mark it as labeled and propagate through all the trees in G (resulting in updated counts
l(Ti (x) in each tree). Increase Q.
6. If Q < Q0 , where Q0 is predefined number of queries that can be completed without
new sorting (say 20 − 50), return to step 4. Else return to step 3 (and sort again).
7. Rebuild model G and return to step 2.
Additional tricks in steps 4 and 5 are introduced to avoid sorting unlabeled instances
with respect to the utility score after each query. Reasonable values for Q0 and N0 can
prevent a large time complexity in the number of unlabeled instances, while selecting the
64

Active Batch Learning with Tree Ensembles

top-scored instances with respect to utility. Computation of q(x)/l(x) in step 4 has complexity O(RD), where D is the maximum tree depth, as q(x) are never updated. However,
for a small tree depth (this is important for a more robust estimation of q(x) and d(x)) this
is not a major problem. Batch selection complexity is still negligible compared to RF and
GBT model building complexities. We can use shallow trees because RF is used for AL
only, and high predictive accuracy is not an issue. We use default settings for the RF count
of attributes scored at a node (equal to the square root of the total number of attributes).
As a classifier we used GBT with embedded feature selection (see Borisov et al. (2006)
for details), or RF when the number of labeled samples was small. Model selection and
GBT parameter optimization (a simple grid search for tree depth and regularization over a
predefined set of values) used two-fold CV error estimates. One could potentially use RF in
all cases, but a GBT can improve predictions in some cases and we allowed this alternative
in our strategy.
The robustness of tree-based ensembles allowed for a straight-forward approach. There
was no preprocessing of the features, no feature generation, no data cleaning, and no
preliminary data analysis. Missing values were handled with traditional tree-based approaches. Missing attribute values were ignored to score splits. To assign instances, surrogates (Breiman et al. (1984)) were used for GBT, while the majority child node was assigned
for RF.

4. Experiments
We applied both our algorithms to the twelve AISTATS 2010 AL challenge datasets (six
development datasets which were larger on average, and six test datasets). Below we briefly
describe the challenge datasets and ranking measure. Data came from diverse real world
domains, for example, marketing, ecology, and text processing. The largest datasets in
the development group were (16969 x 9733), (216 x 72626), where the first number is
the number of inputs, and the second is number of instances in the training data. The
largest test datasets were (92 x 17535), (12000 x 10000) and (12 x 67628). Also, four of
the development datasets had very unbalanced target distributions (1.8% - 6.15% as the
proportions of the rare class).
The task was to achieve the best learning curve while querying data in arbitrary batches
and updating the model after each query. The score was estimated as the area under the
learning curve (model error versus number of labels queried), after all unlabeled instances
are queried. The X-axis (number of labels) was log2 and this was scaled to favor good
performance on a small number of labeled instances. Model error was calculated as the
area under the ROC curve (AUC), to account for an unbalanced class distribution. The
target was binary in all problems. The model error was estimated on separate test data, that
had the same size as the training data, but with unknown labels. For detailed descriptions,
see the challenge site http://www.causality.inf.ethz.ch/activelearning.php.
In small preliminary experiments with the test datasets both proposed AL approaches
performed significantly better than random sampling, uncertainty sampling, or QBC . But
because of small rare class proportions, the model error variation is very high, especially on
a small number of labeled samples. There were small differences in performance between
our algorithms 1 and 2 in these preliminary studies. Although the computational time for

65

Borisov Tuv Runger

algorithm 2 with n0 = 1000, Q0 = 20 was not significantly higher than for algorithm 1, we
chose to apply the first algorithm because of its simplicity, and it proved to be more robust
for very unbalanced classes.
As mentioned previously, data preprocessing was not applied and the process to estimate
the parameters was not complex. The predictive model, tree depth (over the range 2-6),
and the GBT regularization parameter were selected via two-fold cross validation and alpha
was fixed at 2/3 based on our preliminary experiments. For RF, the number of trees in an
ensemble was fixed (at 700) and the number of the attributes scored at a node used the
default (equal to the square root of the total number of attributes). Performance was not
sensitive to either these serial or parallel ensemble parameters.
In our preliminary studies there were only minor differences between batches from 5-15
instances. Because it was necessary to submit and process each query manually, we limited
ourselves to 15 queries per data set. The initial batch size was chosen as 5-10 depending on
the number of inputs, then for each query the batch size was scaled exponentially with the
exponent chosen in a way so that 15 queries covered all unlabeled data.
Our first algorithm (SQBF), had the top average rank on all six test datasets and had
the first rank on one of the datasets. Figure 1 shows the ALC performance of SQBF
(IdealAnalytics) and selected competitors over all the datasets where results were provided.
Some competitors did not consider all the datasets. The selected competitors achieved a
top-two result on at least one dataset. Table 1 supplements the ALC scores with additional
results for the top-two competitors on each dataset. Further details of the challenge results
were provided by Guyon. et al. (2010). SQBF provided consistent performance across these
datasets. Figure 2 shows the ALC performance of SQBF (IdealAnalytics) and baseline
methods over all the datasets. Details of the baselines methods were provided by Cawley
(2010). SQBF (IdealAnalytics) was also a consistently strong performer compared to the
baseline methods.
Our AL strategy is also very fast. It has the same asymptotic computational complexity
as building an RF model, i.e O(T N log(N ) log(M )), where T is number of trees, M is
number of features, N is number of samples. The total run time (for either of the two
algorithms) on all six development or test datasets on one machine was approximately 68 hours (Zeon workstation 3 GHz with 4 GB RAM, 2 processors with hyper-threading,
Windows XP system) depending on the model optimization settings.

5. Acknowledgments
This research was partially supported by ONR grant N00014-09-1-0656. We thank the
reviewers for helpful comments that improved this work.

6. Conclusions
We introduced a novel approach for pool-based, batch active learning using tree ensembles.
We described two algorithms for batch selection that optimize both the query utility function
and within batch diversity. Both algorithm are very fast, and can work with very large
datasets. Both methods were successfully applied to real datasets from the AISTATS 2010
AL challenge. However, we are planning more experiments on artificial datasets where the

66

Active Batch Learning with Tree Ensembles

Figure 1: The ALC performance of SQBF (IdealAnalytics) and selected competitors over
all the datasets where results were provided. The competitors selected achieved
a top-two result on at least one dataset.

Figure 2: The ALC performance of SQBF (IdealAnalytics) and baseline methods (described
by Cawley (2010) over all the datasets.

underlying joint distribution is known, to investigate the relative strengths and weaknesses
of the proposed approaches, and to compare them to other AL methods. We are also
considering some form of semi-supervised learning (for example with auto-regressive trees or

67

Borisov Tuv Runger

Table 1: Summary of the results from AISTATS 2010 Active Learning Challenge. Results
are shown for the top-two competitors on each data set with our algorithm denoted
as SQBF.
Data
Set A
Set B
Set C
Set D
Set E
Set F
Overall

Algorithm
FLYINGSKY
pipifuyj

AUC
0.8622

Ebar
0.0049

ALC
0.6289

Rank
1

SQBF
ROFU

IdealAnalyticsIntel
scan33scan33

0.9520
0.7327

0.0045
0.0034

0.5273
0.3757

5
1

SQBF
BRAIN

IdealAnalyticsIntel
chrisg

0.7544
0.7994

0.0044
0.0053

0.3173
0.4273

5
1

SQBF
DATAM1N

IdealAnalyticsIntel
datam1n

0.8333
0.9641

0.0050
0.0033

0.3806
0.8610

2
1

SQBF
DSL

IdealAnalyticsIntel
yukun

0.9730
0.8939

0.0030
0.0039

0.6397
0.6266

7
1

SQBF
SQBF

IdealAnalyticsIntel
IdealAnalyticsIntel

0.9253
0.9990

0.0037
0.0009

0.4731
0.8018

5
1

NDSU
SQBF

NDSU
IdealAnalyticsIntel

0.9634
0.9062

0.0018
0.0015

0.7912
0.5233

2
4.1667

ROFU

scan33scan33

0.8774

0.0014

0.5072

4.8333

Gaussian Random Field models in tree terminal nodes). We do not currently use unlabeled
data for learning in any way and results of some participants on AISTATS 2010 challenge
show that on some datasets one can substantially benefit from semi-supervised learning.

References
A. Borisov, V. Eruhimov, and E. Tuv. Tree-based ensembles with dynamic soft feature
selection. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, editors, Feature Extraction
Foundations and Applications: Studies in Fuzziness and Soft Computing. Springer, 2006.
L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees.
Wadsworth, Belmont, MA, 1984.
K. Brinker. Incorporating diversity in active learning with support vector machines. In
Proceedings of the Twentieth International Conference on Machine Learning, pages 59–
66. AAAI Press, 2003.
G.C. Cawley. Some baseline methods for the active learning challenge. In N. Lawrence,
editor, JMLR: Workshop and Conference Proceedings, volume 1, 2010.

68

Active Batch Learning with Tree Ensembles

K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science,
3:273–304, October 1996.
Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by
committee algorithm. Machine Learning, 28(2):133–168, 1997.
I. Guyon., G. Cawley, G. Dror, and V. Lemaire. Results of the active learning challenge.
In N. Lawrence, editor, JMLR: Workshop and Conference Proceedings, volume 1, 2010.
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,
2001.
D.D. Lewis and W.A. Gale. A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, page 12. Springer-Verlag New York, Inc., 1994.
B. Settles. Active learning literature survey. Technical report, Computer Sciences Technical
Report, 2009.
H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of the
fifth annual workshop on Computational learning theory, pages 287–294. ACM, 1992.
Z. Xu, R. Akella, and Y. Zhang. Incorporating diversity and density in active learning for
relevance feedback. Advances in Information Retrieval, pages 246–257, 2007.
X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised
learning using gaussian fields and harmonic functions. In ICML 2003 workshop on The
Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining, pages
58–65, 2003.

69

