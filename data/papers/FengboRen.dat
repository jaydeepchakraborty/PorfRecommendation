IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

255

Data-Driven Sampling Matrix Boolean Optimization
for Energy-Efficient Biomedical Signal
Acquisition by Compressive Sensing
Yuhao Wang, Xin Li, Senior Member, IEEE, Kai Xu, Fengbo Ren, Member, IEEE, and Hao Yu, Senior Member, IEEE

Abstract—Compressive sensing is widely used in biomedical
applications, and the sampling matrix plays a critical role on
both quality and power consumption of signal acquisition. It
projects a high-dimensional vector of data into a low-dimensional
subspace by matrix-vector multiplication. An optimal sampling
matrix can ensure accurate data reconstruction and/or high compression ratio. Most existing optimization methods can only produce real-valued embedding matrices that result in large energy
consumption during data acquisition. In this paper, we propose an
efficient method that finds an optimal Boolean sampling matrix
in order to reduce the energy consumption. Compared to random
Boolean embedding, our data-driven Boolean sampling matrix can
improve the image recovery quality by 9 dB. Moreover, in terms of
sampling hardware complexity, it reduces the energy consumption
by 4.6× and the silicon area by 1.9× over the data-driven realvalued embedding.
Index Terms—Compressive sensing, low power sensor, quantization, resistive random-access memory (RRAM), sampling
matrix optimization.

I. I NTRODUCTION

B

IOMEDICAL wireless circuits for applications such as
health telemonitoring [1], [2] and implantable biosensors
[3], [4] are energy sensitive. To prolong the life-time of their
services, it is essential to perform the dimension reduction
while acquiring original data. The compressive sensing [5] is
a signal processing technique that exploits signal sparsity so
that signal can be reconstructed under lower sampling rate than
that of Nyquist sampling theorem. The existing works that
apply compressive sensing technique on biomedical hardware
focus on the efficient signal reconstruction by either dictionary
learning [4], [6] or more efficient algorithms of finding the
Manuscript received February 6, 2016; revised July 5, 2016; accepted
July 14, 2016. Date of publication November 14, 2016; date of current version
March 22, 2017. This work is sponsored by grants from Singapore MOE Tier-2
(MOE2015-T2-2-013), NRF-ENIC-SERTD-SMES-NTUJTCI3 C-2016 (WP4)
and NRF-ENIC-SERTD-SMES-NTUJTCI3 C-2016 (WP5). This paper was
recommended by Associate Editor A. Bermak.
Y. Wang and H. Yu are with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798 (e-mail: ywang29@
e.ntu.edu.sg; haoyu@ntu.edu.sg).
X. Li is with Department of Electrical and Computer Engineering, Carnegie
Mellon University, Pittsburgh, PA 15213 USA (e-mail: xinli@cmu.edu).
K. Xu and F. Ren are with the School of Computing, Informatics, and
Decision Systems Engineering, Arizona State University, Tempe, AZ 85281
USA (e-mail: kaixu@asu.edurenfengbo@asu.edu; renfengbo@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBCAS.2016.2597310

sparsest coefficients [1], [2], [7]–[9]. However, these works, by
improving the reconstruction on mobile/server nodes instead of
data acquisition on sensor nodes, can only indirectly reduce the
number of samples for wireless transmission with lower energy.
In this work, we aim to achieve both high performance signal
acquisition and low sampling hardware cost at sensor nodes
directly.
In compressive sensing, the sampling is performed by multiplying the original signal vector with a linear embedding
matrix, which projects the high-dimensional data vector into a
low-dimensional subspace with preserved intrinsic information.
The concise representation is called a low-dimensional embedding. The sampling matrix can be either random, generally
optimized or optimized towards specific dataset. The random
sampling matrices, Bernoulli or Gaussian, though easier to
construct, have two major limitations. Firstly, the guarantee
on signal recoverability using random sampling matrices is
only probabilistic and therefore large recovery error may be
incurred. Secondly, its construction is independent on the data
under investigation, and therefore the geometric information of
dataset cannot be exploited. The generally optimized sampling
matrices, such as Reed-Muller code [10], [11] or Puffer transformation [4], have deterministic recoverability by constructing
matrices with minimized mutual coherence. However, they are
still independent on data type of interest so that the performance
cannot be maximized. The data-driven optimized embedding,
on the other hand, can leverage geometric structure of dataset in
particular application with additional learning phase, which is
especially beneficial for biomedical sampling hardware where
the target data type is predetermined. Signal acquisition by a
data-driven optimized sampling matrix can preserve more intrinsic information of original signal, and therefore ensure more
accurate signal reconstruction and/or higher compression ratio.
Most existing data-driven optimization methods only produce real-valued embedding matrices [12]. However, the hardware mapping of real-valued sampling matrix is much more
power consuming than that of Boolean matrix. The reason is
that for real-valued embedding operation, the required hardware resources, primarily full-adders, are quadratically depending on the precision required, while only linearly for Boolean
embedding mapping. Therefore, a Boolean sampling matrix is
preferred as the significantly reduced hardware resources can
contribute to higher energy- and area-efficiency. In fact, the
random Bernoulli matrix is the most widely used sampling
matrix in existing CMOS based implementations for low power

1932-4545 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

256

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

Fig. 2. The proposed flow for data-driven Boolean sampling matrix
optimization.
Fig. 1. The comparison of information loss of NuMax through truncation
quantization versus proposed quantization. This work achieves both high signal
recovery performance as well as low sampling hardware cost by proposed
quantization algorithm.

consumption [4], [7], [13]. In addition, the recent emerging
resistive random-access memory (RRAM) [14]–[16] in crossbar
(or cross-point) structure [14] can provide intrinsic fabric for
matrix-vector multiplication, which potentially enables both
energy- and area-efficient hardware implementation of linear
embedding. However, the limited RRAM programming resolution also favors only Boolean embedding matrices to be mapped
to RRAM crossbar structure.
Therefore, the hardware realization of sampling matrix faces
a dilemma. On one hand, if data-driven optimized real-valued
embedding, such as NuMax [12], is mapped for better recovery quality, large power overhead will be expected. On the
other hand, if non-data-driven random Boolean embedding or
Boolean Reed-Muller [10], [11] embedding is mapped for better hardware energy-efficiency, high signal recovery accuracy
cannot be accomplished. Such trade-off is illustrated in Fig. 1.
Fig. 1 also reveals that quantization of NuMax by straightforward truncation works only above precision of 16-bit, below
which it will incur significant performance degradation. Therefore, without data-driven optimized Boolean embedding, the
advantages of both sides cannot be achieved simultaneously.
The challenge to perform data-driven Boolean embedding optimization is that, with large amount of dataset involved in the
sampling matrix optimization, only convex methods with realvalued optimized matrices are feasible.
In this paper, towards high performance (data-driven) and
low power (Boolean) sampling, instead of optimizing Boolean
embedding on original dataset, we propose an optimizing algorithm that transforms a data-driven optimized real-valued
sampling matrix to a Boolean sampling matrix. The proposed
optimization flow is illustrated in Fig. 2. As the input optimized
real-valued embedding matrix is optimized towards the specific dataset, and the proposed algorithm seeks least intrinsic
information loss, so the resulting Boolean embedding matrix is
still optimized towards the same training dataset. In addition,
we have discussed the corresponding hardware implementations based on both CMOS technology and emerging nonvolatile resistive random-access-memory (RRAM) technology
for obtained optimized Boolean embedding. Such capability

was first exploited in our preliminary work [17]. The numerical
experiments demonstrate that the proposed data-driven Boolean
embedding can combine both high signal quality and also low
sampling power. Specifically, it can improve image recovery
quality (RSNR) by 9 dB compared to the non-data-driven
Bernoulli embedding, and improve energy efficiency by 4.6×
than that of data-driven real-valued sampling circuit.
The contributions of this paper are summarized as below:
• To our best knowledge, the data-driven optimized
Boolean sampling matrix is constructed for the first time.
Being Boolean and optimized towards dataset, we achieve
both highest signal recovery quality and best hardware
energy efficiency among all existing schemes.
• We formulate the problem of finding best transformation
that quantizes real-valued sampling matrix into Boolean
matrix with minimal information loss.
The rest of this paper is organized as follows. Section II introduces the background of compressive sensing and near-isometric
embedding. Section III presents the sampling hardware for
Boolean embedding with the corresponding optimization problem formulated. Sections IV and V detail the two proposed
Boolean embedding optimization algorithms. Numerical results
are presented in Section VI with conclusion in Section VII.
II. BACKGROUND
A. Compressive Sensing and Isometric Distortion
Recently, the emerging theory of compressive sensing has
enabled the recovery of undersampled signal, if the signal is
sparse or has sparse representation on certain basis, such as
wavelet transformation and Fourier transformation. And the
recovery can be achieved by solving
minimize x1
x∈RN

subject to

y = ΨΩx

(1)

where x ∈ RN is the sparse coefficients vector and Ω ∈ RN ×N
is the basis on which the original signal is sparse; Ψ ∈ RM×N
is the sensing matrix and y ∈ RM (M  N ) the undersampled
data in low dimension. To ensure a successful recovery, the

WANG et al.: DATA-DRIVEN SAMPLING MATRIX BOOLEAN OPTIMIZATION

257

TABLE I
N OTATION TABLE OF U SED M ATHEMATICAL S YMBOLS

Fig. 3. The embedding circuit by CMOS matrix-vector multiplier.

sensing matrix (Ψ) must meet the restricted isometry property
(RIP), which is defined as: if there exists a δ ∈ (0, 1) such that
the following equation is valid for every vector v ∈ RN
(1 −

δ)v22

≤

Ψv22

≤ (1 +

δ)v22

(2)

then Ψ has the RIP with isometric distortion constant δ. The
notations of all used symbols are summarized in Table I.
B. Optimized Near-Isometric Embedding
The easiest way to construct a matrix with RIP is to generate
a random matrix. The work [18] proves that random matrix is
of a very high possibility to satisfy RIP, yet not deterministic.
Different from the random Bernoulli sampling matrix that the
RIP is probabilistic, a data-driven sampling matrix can ensure
the RIP of the finite given data points. One recent work in
[12] proposed the NuMax framework to construct a nearisometric embedding matrix with deterministic RIP. Given a
dataset χ = {x1 , x2 , . . . , xi } ∈ RN , the NuMax produces an
optimized continuous-valued embedding matrix Ψ so that every
pairwise distance vector v for χ can preserve its norm after
embedding up to a given distortion tolerance δmax .
Once the optimized NuMax sampling matrix Ψ is obtained, the signal acquisition y = ΨΩx can be performed by
multiplying the embedding matrix Ψ with signal vector Ωx.
The conventional CMOS circuit based data acquisition frontend that performs real-valued embedding is shown in Fig. 3.
The sampling circuit has two major components, the SRAM
memory that stores the embedding matrix, and the multiplieraccumulators (MAC) that perform multiplication and addition.
For an embedding matrix Ψ ∈ Rm×n (m  n), in each cycle,

Fig. 4. The implementation of (a) MAC with multiplier in 4-bit resolution
for real-valued embedding matrix and (b) MAC for {−1, 1}m×n embedding
matrix (c) MAC for {−1, 1}m×n embedding matrix with power optimization.

m MACs multiply one element of input vector Ωx with one
column of Ψ, and then add with previously accumulated results.
Therefore, it requires n cycles to obtain the embedded signal y.
As NuMax produces real-valued Ψ, the precision of Ψ substantially determines the hardware complexity. For example, a
multiplier in MAC with 4-bit resolution, shown in Fig. 4(a),
requires 16 full-adders. In fact, the number of required fulladders generally depends quadratically on the precision of both
Ωx and Ψ. For the typical precision of real-valued elements in
sampling matrix, 16-bit (Fig. 1) resolution may lead to as many
as hundreds of full-adders for each MAC, which makes the realvalued NuMax embedding less appealing for signal acquisition
hardware mapping.
III. B OOLEAN E MBEDDING FOR S IGNAL
ACQUISITION F RONT-E ND
A. CMOS-Based Boolean Embedding Circuit
The mapping of a Boolean embedding matrix can eliminate
the usage of multipliers. For a {0, 1}m×n Boolean embedding
matrix, the MAC only accumulate signal data when Boolean
multiplicand is 1. For a more general {−1, 1}m×n Boolean matrix, the Boolean multiplicand indicates addition or subtraction
for the signal data. That is to say, the required resources of fulladders are only linearly depending on the precision of signal Ωx.
As such, the hardware resource can be significantly reduced.
Specifically for the {−1, 1}m×n embedding matrix mapping,
the multiplication by −1 requires the calculation of 2’s complement. The intuitive approach is illustrated in Fig. 4(b). The

258

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

TABLE II
C OMPATIBILITY OF D IFFERENT S AMPLING M ATRICES
ON VARIOUS H ARDWARE P LATFORMS

Fig. 5. The embedding circuit by emerging non-volatile RRAM crossbar.

In2 signal is 1 for multiplying −1 and 0 for multiplying 1.
To obtain the 2’s complement, the XOR logic is used to get
the complement of each bit and the C0 (carry 0) is applied as
well. However, the 2’s complement calculation close to input
will incur substantial dynamic power for the combinational
logic. Instead, the circuit diagram in Fig. 4(c) first accumulates
all data to be multiplied by 1 and −1 separately, and the
subtraction is performed at the very last cycle. Therefore, the
2’s complement circuit is only active every n cycles, which can
greatly improve the power efficiency.
To be compatible with optimized sampling matrices such as
Reed-Muller code [10], [11] and NuMax [12], SRAM block is
required to store the matrix and provide reconfigurability. For
non-optimized sampling matrices such as random Gaussian and
Bernoulli, apart from storing the matrix in SRAM, the matrix
can also be generated at runtime, which can improve hardware
efficiency. In practice, the pseudo-random number generator
(PRNG) is used [7], [13], which avoids the power-consuming
SRAM arrays. As PRNG produces 0/1 sequences with a predetermined pattern, one of the issue for PRNG is the selfcoherence. For example, a 8-bit per cycle PRNG has a period of
256 (28 ), and when filling a n × 256 sampling matrix by rows
with such 0/1 sequences, all rows of the matrix will be identical.
This can be overcome by increasing the number of bits the
PRNG produces per cycle at a cost of higher hardware complexity. With limited hardware resources, the pseudo-random
numbers generated by PRNG usually lead to performance
degradation of signal acquisition compared to stored random
sampling matrix.
B. RRAM Crossbar Based Boolean Embedding Circuit
The emerging resistive random-access-memory (RRAM)
crossbar [14], [15] provides an intrinsic in-memory fabric
of matrix-vector multiplication, which is proposed in Fig. 5.
Compared to CMOS embedding circuit, RRAM crossbar based
approach can provide three major advantages: 1) embed the
sensing matrix [Ψ in (1)] in-memory without the need for
loading it externally each cycle, 2) perform the matrix-vector
multiplication in single cycle, and 3) minimize the leakage
power due to its non-volatility. A RRAM crossbar structure is
composed of three layers: horizontal wires at top layer, vertical
wires at bottom layer and RRAM devices in the middle layer
at each cross-point. For a m × n RRAM crossbar, assume the
input signal of ith row is VIi and the conductance of RRAM
device on ith row jth column is Gij , then the output current
i
flowing down jth column Ij = Σm
i=1 VI Gij . In other words,

crossbar structure intrinsically supports in-memory embedding
operation
⎡
⎡ 1⎤
⎤ ⎡ 1⎤
G11
VI
VO
G12
···
G1n
⎢ G21
⎢ VO2 ⎥
⎥ ⎢ VI2 ⎥
G
·
·
·
G
22
2n
⎢
⎢
⎥
⎥⎢ ⎥
⎢ .. ⎥ = Z ⎢ ..
..
.. ⎥ ⎢ .. ⎥ (3)
..
⎣ .
⎣ . ⎦
.
.
. ⎦⎣ . ⎦
m
Gm2
···
Gmn
VO
Gm1
VIn
where Z is the transimpedance of the transimpedance amplifier
(TIA) and VOi the output voltage of ith column. It must be
ensured that input ||VI ||∞  Vth to avoid accidental value
changes of G, in which the Vth is the device programming
threshold voltage.
The mapping of embedding matrix is accomplished by the
resistance programming of RRAM crossbar according to Ψ.
Intended for memory usage, RRAM devices are commonly
bistable with on-resistance and off-resistance ratio as high as
103 ∼ 104 [15], [19]. Resistance programming with higher
resolution has been demonstrated in 4 or 5 levels at most
[19], [20]. Therefore, resistance programming in continuous
(or close-continuous) value resolution is practically challenging
due to large process variation under current manufacture technology. In other words, the real-valued sampling matrix does
not comply with RRAM crossbar and Boolean sampling matrix
is preferred.
As the RRAM crossbar is essentially (0,1) binary in terms of
conductance, the mapping of (0,1) Boolean matrix follows: 0
corresponds to high resistance state (HRS) and 1 maps to low
resistance state (LRS). To map Ψ̂ ∈ {−1, 1}m×n, simple linear
transformation needs to be considered: Ψ̂x = (2Θ − J)x =
2Θx − Jx, where Θ ∈ {0, 1}m×n, J all-ones matrix and x
input vector. The Jx is implemented by an additional all-LRS
column that generates Σx as current offset for other columns,
as shown in Fig. 5. The sampling matrices that each type of
hardware supports are illustrated in Table II.
C. Problem Formulation
For an sampling matrix Ψ that satisfies RIP with distortion
of δΨ , the following equation will also hold true:
(1 − δΨ )x22 ≤ T Ψx22 ≤ (1 + δΨ )x22

(4)

if T is an orthonormal rotation matrix. In other words, if we
can find an orthonormal rotation matrix that transforms realvalued NuMax embedding matrix Ψ into a matrix that is close
enough to a Boolean matrix Ψ̂, then the Boolean embedding
of Ψ̂ can preserve original distortion δΨ . In other words, the
resulting Ψ̂ is still optimized towards the same training dataset

WANG et al.: DATA-DRIVEN SAMPLING MATRIX BOOLEAN OPTIMIZATION

259

as NuMax embedding Ψ, and meanwhile it can be efficiently
mapped to circuits in Figs. 4(c) and 5 with greatly reduced
power consumption.
The Boolean sampling matrix optimization can be then formulated as the following optimization problem:
minimize T Ψ −
T,Ψ̂

subject to

As Ψ and Ψ̂ are given matrices, Tr(ΨT Ψ) and Tr(Ψ̂T Ψ̂) are
therefore two constants. Consider k as constant first, the formulated optimization problem in (6) can be rewritten as
maximize Tr(T T Ψ̂ΨT )
T

subject to T T · T = I

Ψ̂2F

TT · T = I
Ψ̂ ∈ {−1, 1}m×n

(5)

where Ψ ∈ Rm×n (m < n) is the optimized real-valued sampling matrix learned from dataset, that projects data from high
n-dimension to low m-dimension; T ∈ Rm×m is an orthonormal rotation matrix that attempts to transform Ψ to a Boolean
matrix. Ψ̂ ∈ Rm×m is the closest Boolean matrix solution
where closeness is defined by the Frobenius norm.
Ideally, if an orthonormal transformation matrix T can rotate
Ψ to an exact Boolean matrix, i.e., the optimal value of (5) is
zero, then the distortion δ of optimized Boolean embedding will
be exactly the same as the NuMax real-valued embedding. In
practice, with a non-zero optimal value, the closeness of T Ψ to
Ψ̂ indicates information loss degree from Ψ to Ψ̂. Alternatively,
it can be interpreted as an equivalent near-orthogonal rotation
T  transforming the real-valued Ψ to an exact Boolean Ψ̂. The
degree of orthogonality implies the information loss of Ψ.

and with the singular value decomposition Ψ̂ΨT = U ΣV T
where Σ = diag(σ1 , . . . , σn ), the cost function of (8) can be
rewritten as
Tr(T T Ψ̂ΨT ) = Tr(T T U ΣV T )
= Tr(V T T T U Σ) ≤

n


σi .

(9)

i=1

The inequality holds as V , T , and U are all orthonormal
matrices. As such, the trace is maximized when V T T T U = I,
which leads to
T = UV T .

(10)

To optimize k, let ∂f /∂k = 0 in which f is the cost function of
(7), and the best scaling factor can be obtained by
k=

Tr(T T Ψ̂ΨT )
.
Tr(ΨT Ψ)

(11)

B. Quantization

IV. I TERATIVE H EURISTIC A LGORITHM
It is intractable to solve the problem formulated in (5) considering the orthogonal constraint T T · T = I and the integer
constraint Ψ̂ ∈ {−1, 1} simultaneously, as both constraints are
non-convex. When one constraint is considered at one time, (5)
can be split into two manageable problems: if the orthogonal
constraint is considered for T , and Ψ̂ a given Boolean matrix,
the problem becomes the search of an orthogonal rotation matrix for maximal matrix agreement; if the integer constraint is
considered for Ψ̂, and T a given orthogonal matrix, the problem
turns to a Boolean quantization problem. In this section, a
heuristic approach is proposed that iteratively solves orthogonal rotation problem and Boolean quantization problem, and
gradually approximates the optimal solution of Ψ̂ in each round.
A. Orthogonal Rotation

T is a known orthogonal transformation matrix, and Ψ is a
given real-valued optimized projection matrix, the problem to
find its closest Boolean matrix can be formulated as
minimize ||kT Ψ − Ψ̂||2F
Ψ̂

subject to

Ψ̂ ∈ {−1, 1}.

It is obvious that the solution for (12) is
	
1,
(kT Ψ)ij ≥ 0
Ψ̂ij =
−1, (kT Ψ)ij < 0.

minimize ||kT Ψ − Ψ̂||2F
T,k

(6)

The cost function can be represented by trace function as
||kT Ψ − Ψ̂||2F = k 2 Tr(ΨT Ψ) + Tr(Ψ̂T Ψ̂) − 2kTr(T T Ψ̂ΨT ).
(7)

(12)

(13)

This can be seen as Boolean quantization. The quantization
error can be defined as
e = ||kT Ψ − Ψ̂||2F .

The problem of finding an orthogonal transformation matrix
T that can rotate a given real-valued projection matrix Ψ to
another given Boolean matrix Ψ̂ can be formulated as

subject to T T · T = I.

(8)

(14)

In ideal case, the error would be zero which means an orthogonal transformation T on optimized real-valued projection
matrix Ψ finds an exact Boolean matrix Ψ̂. Therefore, the
distortion δΨ̂ caused by Ψ̂ will be the same as δΨ . With e 
= 0, it
can be inferred that δΨ̂ > δΨ . To reduce the quantization error,
it is an intrinsic idea to increase the level of quantization.
Consider a modified problem formulation
⎧
⎪
(kT Ψ)ij ≥ 1/2
⎨1,
Ψ̃ij = 0,
(15)
−1/2 ≤ (kT Ψ)ij < 1/2
⎪
⎩
−1, (kT Ψ)ij < −1/2

260

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

with each element of the matrix Ψ normalized within the
interval of [−1, 1]. It is important to keep matrix Boolean
so that it can be mapped to RRAM crossbar structure efficiently, thus it requires that the matrix Ψ̃ can be split into two
Boolean matrices Ψ̃ = (1/2)(Ψ̂1 + Ψ̂2 ) where Ψ̃ ∈ {−1, 0, 1}
and Ψ̂1 , Ψ̂2 ∈ {−1, 1}. With Boolean quantization, only one
projection RRAM crossbar is needed. Two RRAM crossbars
are needed for the three-level quantization case, as a result of
trade-off between error and hardware complexity.
C. Overall Optimization Algorithm
The heuristic optimization process is summarized in
Algorithm 1. Given some initial guess of Ψ̂, the inner loop
of Algorithm 1 tries to find the local close-optimal solution
by improving Ψ̂ through iterations. Within each iteration, (6)
and (12) are solved by singular vector decomposition and
quantization as concluded in (10) and (15), respectively. The
iterations terminate when the Ψ̂ stops improving and converges.

only problem in small size can be managed. For the embedding
matrix in compressive sensing, the transformation matrix T
could have dozens of rows while matrix Ψ̂ may have thousands
of Boolean variables, so current solvers may fail in such scale.
In this section, we proposed a row generation algorithm that
also can efficiently tackle the problem.
A. Elimination of Norm Equality Constraint
The orthonormality of T in (5) implies two specific constraints, the orthogonality of rows of T that
tTi · tj = 0

∀ i, j that i 
= j

(16)

and the norm equality that
ti 22 = 1 ∀ i

(17)

where ti is the ith row of T . Both imply numerous quadratic
equality constraints (non-convex) and therefore hard to manage
simultaneously. The non-convex quadratic norm equality constraint of rows of T indicates the normalization of rows after
orthogonality is satisfied. In the following, we show how the
norm equality constraint can be eliminated without affecting
the solution accuracy of problem in (5).
Assume we only impose orthogonal constraint on T rather
than more strict orthonormal constraint, the original problem
can be then relaxed to
minimize T Ψ − Ψ̂2F
T,Ψ̂

subject to

T T · T = D2
Ψ̂ ∈ {−1, 1}m×n

(18)

where D = diag(d1 , d2 , . . . , dm ) is a diagonal matrix, and di
is the norm of ith row of T . That is to say, an additional row
scaling operation is introduced during the sensing stage
y = D−1 Ψ̂Ωx
As both integer constraint and orthogonal constraint are nonconvex, the local optimum in most cases is not optimal globally.
In other words, the solution strongly depends on the initial
guess that leads to the local close-optimum. Therefore, the outer
loop of Algorithm 1 increases the search width by generating
numerous initial guesses that are scattered within orthogonal
matrices space. For each initial guess it will gradually converge
to a local optimum, thus the increase of search width will
compare numerous local optimal solutions and approximate the
global optimum.
V. ROW G ENERATION A LGORITHM
The formulated problem in (5) is a mixed-integer nonlinear programming (MINLP) problem, as it has both nonlinear
orthogonal constraint T T · T = I and the integer constraint
Ψ̂ ∈ {−1, 1}m×n. Although such mixed-integer non-linear programming (MINLP) problem can be solved by existing algorithms such as genetic algorithm [21], it lacks efficiency and

(19)

where Ψ̂  T Ψ is the optimized Boolean embedding matrix
that can be efficiently realized in hardware, Ω is the orthonormal sparse basis of original signal, and x is the sparse
coefficients.
In fact, the row scaling operation during signal acquisition
is unnecessary and can be transferred to recovery stage if an
implicit sensing is performed
ŷ = Ψ̂Ωx

(20)

with corresponding signal reconstruction by
minimize x1
x∈RN

subject to

|D−1 ŷ − D−1 T ΨΩx| ≤ 

(21)

where  is the tolerance for noise on sampled signal data
ŷ. As such, the norm equality constraint is eliminated while
the compressive sensing signal acquisition front-end hardware
complexity stays the same and recovery quality is not affected.

WANG et al.: DATA-DRIVEN SAMPLING MATRIX BOOLEAN OPTIMIZATION

B. Convex Relaxation of Orthogonal Constraint
To construct a transformation matrix T with orthogonal rows
and minimize the cost function at the same time is challenging.
In the following, we propose a convex row generation algorithm
that seeks local optimal solution. The idea is to construct each
row of T at one time while minimize the cost function. Assume
t1 , t2 , . . . , ti−1 are first i − 1 rows that are already built with
orthogonality, to construct the ith row ti
minimize ti Ψ −
ti ,ψ̂i

⎡

⎢
⎢
subject to ⎢
⎣

t1
t2
..
.

2
ψ̂i 2

for each sub-problem will be extremely complicated. In addition, the linearization by row generation significantly reduces
the number of Boolean variables thus reduces the worst-case
complexity from 2m×n to m · 2n . As such, the row generation
together with widely available integer programming solvers can
find solution for problem formulated in (18).

VI. N UMERICAL R ESULTS
A. Experiment Setup

⎤
⎥
⎥ T
⎥ · ti = 0
⎦

ti−1
ψ̂i ∈ {−1, 1}n.

261

(22)

In other words, each time to construct a new row ti , it
has to be orthogonal with previously built t1 , t2 , . . . , ti−1 . The
iterative row generation algorithm is shown in Algorithm 2.
From the geometric perspective, Algorithm 2 seeks to find
an orthogonal basis in the m-dimensional space iteratively.
Initially T is empty so the first direction vector has the greatest
freedom to minimize the cost function. After the first basis vector is chosen, the algorithm finds the best basis vector in the left
(m-1)-dimensional subspace that best minimizes the target
function. This is iteratively performed until the last direction
is selected in only 1-dimensional subspace with freedom for
length only.
As T is a square matrix, there always exists a solution
for Algorithm 2. The MINLP problem with m × n integer
variables in (18) is therefore relaxed to m MINLP sub-problems
each with only n integer variables.

C. Overall Optimization Algorithm
The overall algorithm to solve(18) is illustrated in Algorithm 2.
The 0–1 programming problem in (22) within the loop can be
readily solved by branch-and-cut method, under the condition
that the number of Boolean variable is kept small. The branchand-cut method is widely implemented in solvers such as
MOSEK [22] and BARON [23].
Without the linearization by row generation, the branch-andcut method cannot be applied as the orthogonal constraint is
strongly nonlinear thus evaluation of lower and upper bounds

In this part, we evaluate different compressive sensing sampling matrices from both software and hardware perspectives.
The numerical experiments are performed within Matlab on a
desktop with 3.6 GHz Intel i7 processor and 16 GB memory.
The software performance of sampling matrices is mainly characterized by the signal recovery quality of sampling matrices.
For this purpose, both LFW image data [24] and biomedical
ECG data [25] are used. For both types of data, the NuMax
[12] optimization is first applied with varied training parameter
δ values ([0.05, 0.1, . . . , 0.35]), NuMax produces optimized
real-valued sampling matrices with different ranks. As depicted in the flow chart in Fig. 2, the proposed algorithms are
then applied to Booleanize NuMax sampling matrices. Apart
from above data-driven sampling matrices, random Gaussian,
Bernoulli, and Reed-Muller [10], [11] (non-data-driven optimization) sampling are also compared. The reconstructed
signal-to-noise ratio (RSNR) is used as signal recovery quality
metric, which is defined as

RSNR = 20 log10

x2
x − x̂2


(23)

where x is the original signal and x̂ is the reconstructed signal.
With respect to hardware cost consideration, above all sampling matrices can be mapped to three different sampling
hardware configurations. Specifically, the MAC/SRAM, MAC/
PRNG and RRAM crossbar configurations with their variations
are evaluated to examine the hardware friendliness of all the
sampling schemes. For real-valued MAC, 16-bit resolution is
used as we find that resolution higher than 16-bit will not
improve accuracy, as shown in Fig. 1. For RRAM crossbar,
the resistance of 1 KΩ and 1 MΩ are used for RRAM on-state
resistance and off-state resistance according to [19]. The area
of the RRAM crossbar is evaluated by multiplying the cell area
(4F 2 ) with sampling matrix size plus one additional column to
calculate current offset as discussed in Section III-B. Dynamic
power of the RRAM crossbar is evaluated statistically under
1000 random input patterns following an uniform distribution
with voltage ranging from −0.5 V to 0.5 V (|V | < |Vset | =
0.8 V and |V | < |Vreset | = 0.6 V [19]) and the duration of
operation is 5 ns [19]. Both the real-valued and Boolean digital
CMOS matrix multiplier designs are implemented in Verilog
and synthesized with GlobalFoundries 65 nm low power PDK.
A pseudo-random number generator design [7] is also implemented and synthesized. The SRAM that stores sampling
matrix is evaluated by CACTI [26] memory modeling tool with
the setting of 65 nm low standby power fabrication process.

262

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

Fig. 6. The algorithm efficiency for (a) local search convergence and (b) global
search convergence.

Table II shows all valid combinations of sampling matrix and
hardware configuration that will be compared in the section.
Among all the combinations, we will show in this section that
our proposed sampling matrix can achieve both best signal
recovery quality and hardware efficiency.
B. Iterative Heuristic Algorithm on High-D ECG Signals
For training stage (NuMax), 1,000 ECG periods in dimension
of 256 are randomly picked from database [25] as the dataset χ,
which leads to around 1 million of pairwise distance vectors
in set S(χ). For testing phase, another 1,000 ECG periods are
selected as dataset χ , which have no overlap with learning
dataset χ. The ECG signal reconstruction is performed on
unseen data set χ by solving (1) with Battle-Lemarie wavelet
bases used.
a) Algorithm convergence and effectiveness: The efficiency of Algorithm 1 can be examined from two aspects,
finding both local and global optima. The efficiency of finding
local optimum is assessed by convergence rate. The local search
terminates when the approximation error ||T Ψ − Ψ̂||2F stops
improving.
Given specific RIP upper-bounds, NuMax [12] provides Ψ
with different ranks. With RIP constraint of 0.1, the NuMax
produces a Ψ ∈ R19×256 sampling matrix. Algorithm 1 is applied to Ψ with total 10 000 repeated local search and the
convergence is illustrated in Fig. 6(a). It can be observed that
the relative error reduces dramatically within the first few iterations. The zoomed sub-figure shows that local search on average converges within 50 iterations, where convergence is
defined as less than 1e-6 error reduction in two consecutive
iterations. Generally, the local optimum can be considered
found in less than 100 iterations.
The global search is achieved by scattering many initial
guesses in the orthogonal matrices space for T , and comparing

Fig. 7. The recovery quality comparison among different sampling matrices.
(a) Examples of recovered ECG signals at γ = 19/256 and (b) RSNR for
1000 ECG periods.

the corresponding local optima. The errors under varying number of initial guesses are shown in Fig. 6(b). Considering the
Boolean constraint and the orthogonal constraint, the problem
formulated in (5) is generally NP-hard. Therefore, the relative
error can be improved by scattering exponentially more initial
guesses, yet no convergence is observed. Hence an efficient
search policy should be designed in a way scattering as many
initial points as possible and limiting the local search for each
initial guess within 100 iterations.
b) ECG recovery quality comparison: The ECG signal
recovery examples at the undersampling ratio γ = 19/256 are
shown in Fig. 7. For non-data-driven sampling matrices, both
the random Bernoulli and Gaussian show similar reconstruction quality. In other words, the increase of bits in random
numbers will not improve recovery quality. This is because, the
increase of bits of random number will not gain any additional
information.
The pseudo-random number generator (PRNG) based
Bernoulli exhibits the lowest reconstructed signal quality. This

WANG et al.: DATA-DRIVEN SAMPLING MATRIX BOOLEAN OPTIMIZATION

Fig. 8. The isometric distortion on the unseen dataset χ for different
embeddings.

is because, as PRNG produces 0/1 sequences with a predetermined pattern, it has self-coherence issue. For example, a 8-bit
per cycle PRNG have a period of 256 (28 ), and when filling a
sampling matrix by rows with such 0/1 sequences, all rows of
the matrix will be identical.
The Reed-Muller code optimizes the sampling matrix by
minimizing the correlations among different rows/columns,
which helps to improve sampling performance. Being a generic
sampling matrix that works with all data types, it cannot exploit
the isometric property of ECG signal, which limits its performance in particular applications. Specifically, it only shows
1 dB improvement compared to random Bernoulli sampling.
The data-driven NuMax real-valued sampling exhibits the
best recovery quality (highest RSNR) as shown in Fig. 7(b). The
proposed iterative heuristic (IH) algorithm quantizes the realvalued NuMax sampling with slight quality loss. Specifically,
at the undersampling ratio γ = 19/256, the IH (lv2) exhibits
8 dB, 9 dB, and 10 dB higher RSNR than that of Reed-Muller,
Bernoulli, and pseudoBernoulli samplings, respectively. Also,
the level-3 quantization through (15) can preserve more information than level-2 quantization through (13). The RSNR
of IH (lv3) shows marginal 0.48 dB higher RSNR than IH
(lv2). The IH (lv3) sampling matrix Ψ ∈ {−1, 0, 1}m×n will
incur additional hardware overhead compared to IH (lv2) Ψ ∈
{−1, 1}m×n.
Fig. 7(a) gives a visual effect of quality of recovered ECG
signal segments with different sampling matrices. The datadriven sampling matrices, i.e., NuMax, IH (lv2), and IH (lv3),
can recover signals that tightly coincide with original signals.
C. Row Generation Algorithm on Low-D Image Patches
For training stage (NuMax), 6,000 patches with size of 8 × 8
are randomly picked throughout all images as the dataset χ,
which leads to around 18 millions of pairwise distance vectors
in set S(χ). For testing phase, another 6,000 patches with size
of 8 × 8 are selected as dataset χ with no overlap with learning
dataset χ. The image reconstruction is performed on unseen
data set χ by solving (1) with 2D DCT bases.
The genetic algorithm [21] is adopted as the baseline solver
for the mixed-integer nonlinear programming (MINLP) in (5),
which is compared with the proposed algorithm in Algorithm 2.
Both algorithms are run given same amount of time, i.e.,

263

m × 500 seconds where m is the rank of Ψ that indicates the
size of the problem.
a) Algorithm effectiveness: The idea behind the proposed real-valued matrix Booleanization is to preserve the RIP
of NuMax sampling matrix, which differs from the truncation
based quantization. The information loss during the quantization is directly related to the RIP preservation. The algorithm
effectiveness in this part will be examined by the isometric
distortion δ, defined in (2).
The distortions of the all embeddings are tested on unseen
dataset χ . The isometric distortions of both random embeddings are almost invariant. Being optimized on image dataset
χ, both the NuMax and proposed (quantized NuMax) are
significantly better than random embeddings. With focus on
the Boolean sampling matrices that are hardware friendly, the
isometric distortion of optimized Boolean embedding is 3.0×
better than random Boolean embedding on average.
Due to the near-orthogonal rotation, the optimized Boolean
embedding experiences some penalty on isometric distortion δ
compared to NuMax approach. For genetic algorithm, as it experiences higher distortion than that of the proposed algorithm,
it can be inferred that Algorithm 2 can find a more precise solution. In addition, it can be observed that the genetic algorithm
fails when undersampling ratio m/n increases, and this is because the proposed row generation based algorithm requires linearly more time when the number of row m increases, while the
genetic algorithm needs exponentially more time. Moreover,
the solution provided by genetic algorithm is stochastic, which
has no guarantee on its effectiveness while the proposed algorithm is deterministic.
b) Image recovery quality comparison: The recovery
examples under γ = 25/64 are shown in Fig. 9(a). The reconstructed images in blue box correspond to Boolean embeddings
that have low power hardware implementations, and images in
red box are from optimization based approaches which show
lower recovery errors. The genetic algorithm is also optimization based, but the effectiveness is inconsiderable. Therefore,
only the proposed can achieve both low power and high recovery performance. The numerical image reconstruction quality
is shown in Fig. 9(b). The two random embeddings show
similar reconstruction RSNR, which is averagely 8.3 dB lower
than that of the proposed optimized Boolean sampling matrix.
The RSNR of optimized Boolean embedding is close to that
of NuMax embedding, which is 2.5 dB lower as a result of
information loss by near-orthogonal rotation.
On the other hand, the genetic algorithm shows no obvious
effectiveness of improving recovery quality even though it
optimizes a Boolean embedding matrix. The main reason is that
during the conversion from Ψ to Ψ̂ too much information loss
leads Ψ̂ to be close to a random Boolean matrix. In other words,
the genetic algorithm is ineffective to solve the problem in (5).
In addition, the stochastic nature of genetic algorithm makes it
necessary to perform the algorithm considerably many times.
The proposed algorithm, on the contrary, guarantees to produce
a Boolean matrix with high performance with single execution.
The proposed iterative heuristic algorithm is also compared
with the row generation (RG) algorithm. The signal recovery
quality of RG algorithm outperforms heuristic algorithm by

264

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

TABLE III
H ARDWARE P ERFORMANCE C OMPARISON A MONG D IFFERENT
S AMPLING M ATRICES (19 × 256) ON VARIED
H ARDWARE C ONFIGURATIONS

Fig. 9. The recovery quality comparison among different embedding matrices.
(a) Examples of recovered images under γ = 25/64 and (b) RSNR on 6000
8 × 8 image patches.

2.1 dB on average. Though RG algorithm is more effective, it
involves binary programming (22), and is unscalable to applications with high-dimensional signal. To conclude, heuristic algorithm is efficient for applications of both low/high-dimensional
signals, and RG algorithm provides the best performance for
low-dimension signal applications.
D. Hardware Performance Evaluation
In this part, the hardware performance benefits of Boolean
embedding will be investigated in details. The evaluation only
focuses on the embedding hardware as indicated by red dashlined boxes in Figs. 3 and 5.
a) Hardware comparison: The matrix-vector multiplier
is composed of multiple multiplier-accumulator (MAC) in parallel. To multiply the signal vector with a 19 × 256 sampling
matrix, 19 MACs are needed and each MAC requires 256 cycles
to perform the inner-product with each cycle (1 ns). To store the
NuMax real-valued sampling matrix, 16 kB SRAM with 64-bit
I/O bus-width is used.
The proposed Boolean optimization quantizes NuMax sampling matrix into a {−1, 1}m×n Boolean matrix. The size of
SRAM to store sampling matrix is therefore reduced from
16 kB to 1 kB. Compared to a Bernoulli {0, 1}m×n matrix, a
{−1, 1}m×n multiplication requires calculations of 2’s complement of input signal vector, which incurs additional hardware
cost for MACs. To minimize the overhead of 2’s complement,

the MAC design in Fig. 4(c) is used, which calculates 2’s
complement only once every 256 cycles.
RRAM crossbar supports both {0, 1}m×n and {−1, 1}m×n
Boolean matrices. As the sampling matrix is embedded into the
RRAM crossbar which also performs the matrix multiplication,
no separate memory is required.
The performance of four hardware schemes that support
different types of sampling matrices is compared in Table III.
Compared to the NuMax real-valued embedding on 16-bit
MAC and 16 kB SRAM hardware, the proposed quantized
−1/1 Boolean embedding on 1-bit MAC and 1 kB SRAM
consumes 4.6× less operation energy per embedding, 1.8×
smaller leakage power, and 1.9× smaller area. This is because,
as mentioned in Section III, the real-valued multiplier generally
requires quadratically increasing number of full-adders when
resolution increases, while Boolean multiplier only needs linearly more full-adders.
When the proposed quantized −1/1 Boolean embedding is
performed on RRAM crossbar, it further improves the hardware performance significantly. Specifically, for the operation
energy per embedding, the RRAM crossbar based embedding
outperforms the CMOS circuit based real-valued embedding by
117×. The area of the RRAM crossbar based embedding is
nearly 1000× better than that of CMOS circuit based realvalued embedding. In addition, the RRAM crossbar will not
experience the leakage power which is at the scale of hundreds
of μW for the CMOS circuit based approach. For the operation
speed, the RRAM crossbar embedding executes in single cycle
while the CMOS circuit requires 256 cycles due to the reuse of
hardware. The overall performance for different sampling matrices on varied hardware platforms is summarized in Table IV.
b) Impact of RRAM variation: One non-negligible issue
of mapping Boolean embedding matrix to RRAM crossbar is
the RRAM RHS and LHS variations. With high resistance
variation, the embedding matrix will deviate from expected
values to be represented by RRAM resistance, and hence the
recovery quality may degrade. The sensitivity study of recovery
quality on the resistance variations of RRAM is shown in
Fig. 10. The resistance of RRAM is assumed to follow lognormal distribution with the mean to be RLRS and RHRS , and
standard deviation σLRS and σHRS for LRS and HRS cells
respectively.
With varied σLRS and σHRS , it can be observed from Fig. 10
that the performance degradation is more susceptible to resistance variation of LRS, while less sensitive on variation of RHS.

WANG et al.: DATA-DRIVEN SAMPLING MATRIX BOOLEAN OPTIMIZATION

265

TABLE IV
C OMPARISON OF A LL VALID S AMPLING M ATRIX AND H ARDWARE C OMBINATIONS

rotations. As such, the embedding not only can be effectively
mapped to both CMOS circuit and RRAM crossbar with much
lower power consumption, also high performance of optimized
real-valued embedding can be well preserved. Numerical results show that, in terms of signal acquisition quality, the
proposed data-driven optimized Boolean sampling matrix outperforms the random Bernoulli matrix by 2.9× and 3.2× with
RRAM crossbar and CMOS MAC circuits with similar energy
efficiency, respectively. Compared to real-valued data-driven
sampling matrix, the proposed Boolean sampling can achieve
117× and 4.6× better energy efficiency on RRAM crossbar
and CMOS MAC implementations with similar signal quality,
respectively. Overall, the proposed data-driven Boolean sampling matrix combines both the high performance advantage
of real-valued sampling and low power advantage of Boolean
sampling.
R EFERENCES

Fig. 10. The sensitivity of recovery quality of (a) image signal and (b) ECG
signal on the resistance standard deviation σ of RRAM for both low resistance
state (LRS) and high resistance state (HRS) following Log-normal distribution.

In practice, the RHS variation σHRS is approximately 0.3 [27],
and LHS variation σLRS roughly 0.1 [27]. The real-world σ
is annotated in Fig. 10 and it can be concluded that the proposed Boolean embedding on RRAM crossbar is robust against
RRAM device variations when on/off ratio is high (GLRS 
GHRS ≈ 0). To further suppress the performance degradation,
material engineering [28] and verification programming method
[27] can help achieve higher LHS uniformity.
VII. C ONCLUSION
In this work, towards energy efficient and high performance
hardware implementation of data acquisition by compressive
sensing, a novel embedding algorithm is proposed to transform
a given optimized real-valued embedding matrix into an optimized Boolean embedding matrix under (near-) orthogonal

[1] F. Ren and D. Markovic, “A configurable 12-to-237 ks/s 12.8 mw sparseapproximation engine for mobile ExG data aggregation,” in Proc. IEEE
Int. Solid-State Circuits Conf., San Francisco, CA, USA, Feb. 2015,
pp. 1–3.
[2] Z. Zhang et al., “Compressed sensing of eeg for wireless telemonitoring
with low energy consumption and inexpensive hardware,” IEEE Trans.
Biomed. Eng., vol. 60, no. 1, pp. 221–224, Jan. 2013.
[3] A. Dixon et al., “Compressed sensing system considerations for ecg and
emg wireless biosensors,” IEEE Trans. Biomed. Circuits Syst., vol. 6,
no. 2, pp. 156–166, Apr. 2012.
[4] Y. Suo et al., “Energy-efficient multi-mode compressed sensing system
for implantable neural recordings,” IEEE Trans. Biomed. Circuits Syst.,
vol. 8, no. 5, pp. 648–659, Oct. 2014.
[5] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289–1306, Apr. 2006.
[6] J. Pant and S. Krishnan, “Compressive sensing of electrocardiogram signals by promoting sparsity on the second-order difference and by using
dictionary learning,” IEEE Trans. Biomed. Circuits Syst., vol. 8, no. 2,
pp. 293–302, Apr. 2014.
[7] F. Ren and D. Markovic, “A configurable 12–237 ks/s 12.8 mw sparseapproximation engine for mobile data aggregation of compressively sampled physiological signals,” IEEE J. Solid-State Circuits, vol. 51, no. 1,
pp. 68–78, Jan. 2016.
[8] D. Malioutov and M. Malyutov, “Boolean compressed sensing: LP relaxation for group testing,” in Proc. IEEE Int. Conf. Acoust., Speech and
Signal Process., Kyoto, Japan, Mar. 2012, pp. 3305–3308.
[9] M. Fatemi and M. Vetterli, “Randomized recovery for boolean compressed sensing,” in Proc. IEEE Int. Symp. Inform. Theory, Istanbul,
Turkey, Jul. 2013, pp. 469–473.
[10] R. Calderbank and S. Jafarpour, “Reed Muller sensing matrices and the
lasso,” in Sequences and Their Applications SETA 2010, C. Carlet and
A. Pott, Eds. Berlin, Germany: Springer, 2010, pp. 442–463.

266

IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS, VOL. 11, NO. 2, APRIL 2017

[11] S. Jafarpour, “Deterministic Compressed Sensing,” dissertation, Dept.
Comput. Sci., Princeton Univ., Princeton, NJ, USA, Aug. 2011.
[12] C. Hegde et al., “Numax: A convex approach for learning near-isometric
linear embeddings,” IEEE Trans. Signal Process., vol. 63, no. 22,
pp. 6109–6121, Nov. 2015.
[13] F. Chen, A. Chandrakasan, and V. Stojanovic, “Design and analysis of
a hardware-efficient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3,
pp. 744–756, Mar. 2012.
[14] D. B. Strukov et al., “The missing memristor found,” Nature, vol. 453,
no. 7191, pp. 80–83, May 2008.
[15] S. Kim and Y.-K. Choi, “Resistive switching of aluminum oxide for flexible memory,” Appl. Phys. Lett., vol. 92, no. 22, pp. 223 508.1–223 508.3,
Jun. 2008.
[16] Y. Wang, H. Yu, and W. Zhang, “Nonvolatile cbram-crossbar-based
3-D-integrated hybrid memory for data retention,” IEEE Trans. Very
Large Scale Integr. (VLSI) Syst., vol. 22, no. 5, pp. 957–970, May 2014.
[17] Y. Wang et al., “Optimizing boolean embedding matrix for compressive
sensing in RRAM crossbar,” in Proc. ACM/IEEE Int. Symp. Low Power
Electron. Design, Rome, Italy, Jul. 2015, pp. 13–18.
[18] R. Baraniuk et al., “A simple proof of the restricted isometry property
for random matrices,” Constr. Approx., vol. 28, no. 3, pp. 253–263,
Dec. 2008.
[19] H. Lee et al., “Low power and high speed bipolar switching with a thin
reactive ti buffer layer in robust HfO2 based RRAM,” in Proc. IEEE Int.
Electron Devices Meeting, San Francisco, CA, USA, Dec. 2008, pp. 1–4.
[20] S.-S. Sheu et al., “A 5 ns fast write multi-level non-volatile 1 k bits RRAM
memory with advance write scheme,” in Proc. Symp. VLSI Circuits,
Kyoto, Japan, 2009. pp. 82–83.
[21] L. Costa and P. Oliveira, “Evolutionary algorithms approach to the solution of mixed integer non-linear programming problems,” Comput. Chem.
Eng., vol. 25, no. 2, pp. 257–266, Mar. 2001.
[22] M. ApS, The MOSEK optimization toolbox for MATLAB manual.
Version 7.1 (Revision 28), 2015. [Online]. Available: http://docs.mosek.
com/7.1/toolbox/index.html
[23] N. Sahinidis, “Baron: A general purpose global optimization software
package,” J. Global Optim., vol. 8, no. 2, pp. 201–205, Mar. 1996.
[24] G. B. Huang et al., “Labeled faces in the wild: A database for studying
face recognition in unconstrained environments,” Univ. Massachusetts,
Amherst, MA, Tech. Rep. 07-49, Oct. 2007.
[25] G. B. Moody and R. G. Mark, “The impact of the mit-bih arrhythmia database,” IEEE Eng. Med. Biol., vol. 20, no. 3, pp. 45–50, May–Jun. 2001.
[26] S. J. Wilton and N. P. Jouppi, “Cacti: An enhanced cache access and cycle
time model,” IEEE J. Solid-State Circuits, vol. 31, no. 5, pp. 677–688,
May 1996.
[27] Y. Chen et al., “Highly scalable hafnium oxide memory with improvements of resistive distribution and read disturb immunity,” in Proc. IEEE
Int. Electron Devices Meeting, Baltimore, MD, USA, Dec. 2009, pp. 1–4.
[28] H.-S. Wong et al., “Metal-oxide RRAM,” Proc. IEEE, vol. 100, no. 6,
pp. 1951–1970, May 2012.

Yuhao Wang received the B.S. degree in microelectronics engineering from Xi’an Jiao Tong University,
Xi’an, China, and the Ph.D. degree from the School
of Electrical and Electronic Engineering, Nanyang
Technological University, Singapore, in 2011 and
2015, respectively.
Currently, he is a Senior R&D Engineer at Synopsys, Mountain View, CA, USA. His research interests
include EDA topics related to emerging nonvolatile
memory design flow and hardware optimization with
emphasis on energy efficiency.

Xin Li (S’01–M’06–SM’10) received the Ph.D. degree in electrical and computer engineering from
Carnegie Mellon University, Pittsburgh, PA, USA,
in 2005.
Currently, he is an Associate Professor in the
Department of Electrical and Computer Engineering,
Carnegie Mellon University. His research interests
include integrated circuit and signal processing.
Dr. Li was a recipient of the National Science Foundation Faculty Early Career Development
Award in 2012, the IEEE Donald O. Pederson Best
Paper Award in 2013, the Best Paper Award from Design Automation Conference in 2010, two IEEE/ACM William J. McCalla International Conference on
Computer-Aided Design Best Paper Awards in 2004 and 2011, and the Best
Paper Award from the International Symposium on Integrated Circuits in 2014.

Kai Xu was born in Taian, China, in 1990. He received the B.S. degree in electrical engineering from
Shandong University, Shandong, China, in 2011, and
the M.S. degree in electrical engineering from the
University of Electronic Science and Technology of
China, Chengdu, China, in 2014.
Currently, he is working toward the Ph.D. degree
in computer engineering at Arizona State University, Tempe, AZ, USA. His research interests include machine learning and, convex optimization
for designing low power framework in Internet of
Things (IoTs).

Fengbo Ren (S’10–M’15) received the B.Eng. degree from Zhejiang University, Hangzhou, China,
in 2008, and the M.S. and Ph.D. degrees from the
University of California, Los Angeles, Los Angeles,
CA, USA, in 2010 and 2014, respectively, all in
electrical engineering.
In 2015, he joined the faculty of the School
of Computing, Informatics, and Decision Systems
Engineering, Arizona State University, Tempe, AZ,
USA. His doctoral research involved designing
energy-efficient VLSI systems, accelerating compressive sensing signal reconstruction, and developing emerging memory
technology. His research interests are focused on hardware acceleration and
parallel computing solutions for data analytics and information processing,
with emphasis on compressive sensing, sparse coding, and deep learning
frameworks.
Dr. Ren received the 2012–2013 Broadcom Fellowship.

Hao Yu (M’06–SM’14) received the B.S. degree
from Fudan University, Shanghai, China, in 1999,
and the M.S. and Ph. D degrees in integrated circuit and embedded computing from the Electrical
Engineering Department, University of California,
Los Angeles, Los Angeles, CA, USA, in 2007.
He was a Senior Research Staff at Berkeley Design Automation. Since October 2009, he has been
an Assistant Professor in the School of Electrical
and Electronic Engineering, Nanyang Technological
University, Singapore. His primary research interest
is in emerging CMOS technologies such as 3DIC and RFIC designs at nanotera scale. He has 195 top-tier peer-reviewed publications, five books, and six
book chapters.
Dr. Yu received the Best Paper Award from the ACM Transactions on Design
Automation of Electronic Systems in 2010, Best Paper Award nominations
at DAC’06, ICCAD’06, and ASP-DAC’12, and the Inventor Award from the
Semiconductor Research Cooperation. He is an associate editor and technical
program committee member of several journals and conferences.

A Compressive-sensing based Testing Vehicle for 3D TSV
Pre-bond and Post-bond Testing Data
Hantao Huang, Hao Yu

Cheng Zhuo

Fengbo Ren

School of Electrical and
Electronic Engineering
Nanyang Technological
University
50 Nanyang Avenue,
Singapore

Intel Corporation
Hillsboro, OR 97124 U.S.A

School of Computing,
Informatics, Decision Systems
Engineering
Arizona State University
699 S Mill Ave, Tempe,
Arizona, U.S.A

cheng.zhuo@intel.com

renfengbo@asu.edu

haoyu@ntu.edu.sg

(with TSV density of 10, 000/mm2 or more) [7, 8], it poses
a grand challenge for TSV pre-bond and post-bond testing
because the bandwidth of TSV testing data becomes the
primary bottleneck [5, 9] for online TSV testing vehicle. As
such, one needs to develop an eﬃcient yet low-loss method
to compress TSV testing-data with preserved fault detection
information.

ABSTRACT
Online testing vehicle is required for 3D TSV pre-bond and
post-bond testing due to high probability of TSV failures.
It has become a challenge to deal with large sets of generated testing data with limited probing when transmitting
the data out. In this paper, a lossless compressive-sensing
based testing vehicle is developed for online testing of TSVs.
By exploring sparsity of the testing data under constraint of
failure bound of TSV, sparse-representation based encoding can be deployed by XOR and AND network on chip to
deal with large volume of testing data. Experimental results
(with benchmarks) have shown that 89.70% pre-bond data
compression rate can be achieved under 0.5% probability of
failures; and 88.18% post-bond data compression rate can
be achieved with 5% probability of failures.

1.

Probe

TSV
Probe
Pad

FEOL/BEOL
Adhesive

Carrier
(a)

Functional
TSV
TSV
elevator

Front end
of line/Back
end of line

Layer
(b)

Figure 1: (a) Probing on extra DFT pads in pre-bond testing
(b) TSV elevator in post-bond testing

INTRODUCTION

Conventional testing data compression is widely used with
according simple compaction circuits. It can be categorized
as space compression and time compression. For space compression, XOR network is used to reduce many testing bits
into single bit, which is however, limited by high aliasing rate
because any even number of faulty responses could cancel
error with a ’correct’ response. For time compression such
as multiple input signature register (MISR) method, it can
achieve lower aliasing eﬀect for conventional testing but cannot be directly deployed in the TSV testing because of the
clustering eﬀect [10, 11]. Furthermore, such a many-to-one
compression method provides less online diagnosis information for TSV self-healing, which is a critical to improve the
TSV yield.
In this paper, to perform online testing with improved
bandwidth, we propose a lossless data compression method
based on compressive-sensing for both pre-bond and postbond TSV testing. It has no aliasing rate with on-chip data
compression and oﬀ-chip lossless data recovery with according online hardware implementation for testing data compression. The problem of compressive-sensing based TSV
testing data compression is formulated to ﬁnd the maximum
compression rate with lossless recovery under the constraint
of TSV failure probability. For the pre-bond TSV testing,
the failure probability refers to TSV failure probability due
to variation of manufacturing [7]; whereas for the post-bond
TSV testing, the failure probability is the faulty IC probability resulting in error output during functional testing

With the introduction of vertical through silicon via (TSV),
3D-IC provides energy eﬃcient interconnect for memory and
logic integration [1]. However, 3D-IC has low yield with high
probability of TSV manufacturing defects [2]. TSV may
be shorted to substrate due to pin-hole or open because of
micro-void or partial ﬁlling [3]. To increase the yield, online
TSV testing vehicle is thereby required to provide just-intime diagnosis to detect faulty TSVs and replace them with
redundant TSVs [4, 5] to achieve self-healing (TSV repair).
Due to the stacking nature of 3D-IC, TSV testing is needed
for every layer before stacking. However, the testing data
can be only collected from specially designed probe pad as
shown in Fig. 1a, which are limited in number due to area
limitation. After bonding, the bottom die can communicate
with the external tester. Therefore, as shown in Fig. 1b,
the testing data has to be transferred from bottom die to
upper die through the so-called TSV elevator [6]. As the
3D-stacking is mainly applied for high-volume I/O circuits
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

ISPD ’16, April 3–6, 2016, Santa Rosa, California, USA.
c 2016 ACM. ISBN 978-1-4503-4039-7/16/04. . . $15.00

DOI: http://dx.doi.org/10.1145/2872334.2872351

19

die area and reduce the functional TSV densities. Therefore, data compression is needed to reduce the bandwidth
requirement of TSV elevator and save testing time.

[12]. Experiment results show that with the testing data
compression, 89.70% data compression rate can be achieved
under 0.5% failure probability and 88.18% post-bond data
compression rate with 5% failure probability.
The rest of the paper is organized as follows. Section II
provides the TSV online testing vehicle architecture with
the according problem formulation. Section III discusses
the compressive-sensing based testing data compression and
recovery. Section IV shows the application in pre-bond and
post-bond TSV testing. Section V presents the numerical
results with conclusion drawn in Section VI.

As previously discussed, lossless testing data compression
is required to provide diagnostic information for TSV repair. The main proposed solution here is to fully utilize the
compressive-sensing to compress the TSV testing data with
high data compression rate under the given failure probability while being able to losslessly recover the original signal.
Here, we assume that the TSV failure probability is known
prior. As such, the TSV testing data compression problem
can be formulated.
Problem: Find minimum number of output bits OB to
locate faulty TSV/IC Ef ault ∈ RN

COMPRESSIVE 3D-IC TESTING PROBLEM

In this section, we will present the 3D-IC testing architecture for pre-bond and post-bond online TSV testing with
the probe pad and TSV elevator. Moreover, the problem
of the testing data compression is formulated based on this
testing architecture.

M in. < OB = M Log2 (M ax(Y )) >
S.T.(i) Xr = (I + Ef ault )Xe

(1)

(ii) Y = Φ(Xr − Xe )
(iii) Ef ault 0 ≤ K

2.1 TSV Testing Vehicles
The online TSV testing vehicle architecture can support
pre-bond die testing, post-bond stack testing and board level
interconnect testing based on [6]. Pre-bond testing is mainly
designed to detect TSV manufacturing faults; while postbond testing is not only for TSV interconnect testing but
also for scan-chain and functional testing. As shown in Fig.
2, the testing wrapper will provide testing access mechanism (TAM) and send/receive testing signal to/from I/Os
of external stack. Testing data will be injected from probe
pad for pre-bond testing or from bottom die to upper die
through TSV elevator for post-bond testing. Decoder will
decode the input testing data and function as Automatic
Testing Pattern Generator (ATPG) to cover the sequential
and combinational circuits. The testing data is collected
from the scan chain and compressed by the testing data
compression (TDC) block. Then, such compressed testing
data will be transported from the upper die to the bottom
and eventually collected by the tester or probe pad. For prebond TSV testing, the expected output is the same as the
input testing data, whereas for post-bond TSV testing the
expected output is collected from output bandwidth based
on time-division multiplexing technique, in which the ﬁrst
slot is used for expected output and the remaining slots are
for compressed testing data output [13]. Therefore, no additional bandwidth is required for expected output.
For the pre-bond TSV testing, testing each TSV is time
consuming and impractical [5, 7], groups of TSVs are tested.
The TSV groups are formed based on the pitch of probe
head such that the probe can contact the whole TSV group
at once. The faulty TSV interconnect can be detected by
the diﬀerence between the received signal and expected signal. The target of pre-bond TSV testing data compression
is to minimize the output bits to save testing time while be
able to lossless recover original signal to provide diagnostic
information for TSV repair. Probe pad is used to collect the
pre-bond TSV testing data.
For the post-bond TSV testing, as there are only TSV
elevators and probe pads to provide input and output data
for non-bottom die, the testing time is constrained by the
bandwidth, given a large volume of testing data. Moreover, increasing the number of TSV elevator will incur more

where Xr ∈ RN and Xe ∈ RN denote the received and expected testing data through TSV for pre-bond testing or
scan chain for post-bond testing and Y ∈ RM is the compressed output testing data. Φ ∈ RM ×N is the compressivesensing matrix generated from TDC block using XOR-AND
networks. M and N is the compressed testing data length
and original testing data length respectively. Ef ault ∈ RN
are the defective TSV location in the pre-bond testing, while
for the post-bond testing, Ef ault represents the error bits
introduced by faulty ICs. Sparsity K represents the maximum number of non-zero values in Ef ault , which can be
estimated by the TSV failure probability or IC failure probability. The lossless compression thereby means that when
given the compressed result Y and sensing matrix Φ, we can
losslessly recover Ef ault such that no TSV testing data information is lost. By making use of sparsity of Ef ault , an
unique sparse solution can be found for the undetermined
linear system [14, 15]. Please note that only compression
process performed as Y = Φ(Xr − Xe ) is on-chip. The rest
computation including testing data recovery is performed

Core Wrapper

Core Wrapper

Scan Chain
Decod
Decoder
cod
der

Decoder

Scan Chain
Scan Chain
Scan Chain

Scan Chain

Decoder

Scan Chain

Scan Chain
Scan Chain

TDC

Core Wrapper

Scan Chain
Scan Chain

Scan Chain

Scan Chain

Buss
T
S
V

PAD

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

Testing
elevator
Additional
probe pad
for prebond
testing

Scan Chain

Decoder

Scan Chain

Decoder

PAD

Core Wrapper

Core Wrapper
Scan Chain
Scan Chain

Scan Chain
Scan Chain

Bus

Input
testing
data

PAD

T
S
V

T
S
V

T
S
V

T
S
V

T
S
V

TAM

TDC

2.

2.2 Problem Formulation

T
S
V

T
S
V

IEEE 1149.1 Boundary Scan

T
S
V

T
S
V

T
S
V

T
S
V

PAD

Output
testing
data

TEST BOARD
Figure 2: TSV testing vehicle with compressive-sensing
based testing circuits

20

algorithm, we assume the aﬀected output signal (error bit
probability) is proportional to faulty component probability. For pre-bond TSV testing, TSV testing data is directly
related to faulty TSV location. The clustering eﬀect is considered and there exists a spatial correlation between defective TSVs. This indicates that presence of a defective TSV
increases the probability of defective TSV nearby. Based on
[11, 17], the probability of defect for i-th TSV Pi can be
modeled as below.

computation including testing data recovery is performed
oﬀ-chip.

3.

COMPRESSIVE-SENSING BASED COMPRESSION ALGORITHM

In this section, we discuss the sparsity of the testing data
which is the foundation to perform compressive-sensing. Moreover, orthogonal matching pursuit is introduced here to solve
undetermined equations. Please note that this section focuses on testing data analysis and recovery.

Pi = P (1 +

BwImp =

N
,
M ∗ log2 (M ax(Y ))

NC = 1 −

1
BwImp

(5)

where M ax(Y ) is maximum digital value of the compressed
testing data, and N is the original data size. Please note that
for a given bandwidth, data compression is equivalent to improve bandwidth, which has the same eﬀect on throughput.
We select the maximum code bit size log2 (M ax(Y )) to ease
the decoding after receiving them from probe pad or tester.

3.2 Lossless Compression and Recovery in L1
Norm
The lossless compression is performed on-chip; whereas
recovery is done oﬀ-chip. The recovery of compressed testing
data can be formulated as L0 norm minimization problem
given below:
argmin
subject to

Ef ault 0
Y = ΦEf ault

(6)

where Ef ault is N dimensional sparse testing data (Ef ault ∈
RN ), Φ is the sensing matrix (Φ ∈ RM ×N ) generated by
TDC from XOR-AND network, and Y is the compressed
testing data in low dimension (Y ∈ RM and M  N ).
Note the solution of L0 norm is equivalent to L1 norm with
overwhelming probability [14].

Signal Difference

(3)

where Nf aulty is the number of faulty components. Ncom
and Pcom are the number of components and the probability of being faulty components respectively. The IC failure
probability is Ppost = 1 − Ypost (Nf ault = 0).
For post-bond, due to the application of ATPG and diﬀerent functional testing algorithms, the testing data may not
directly reﬂect the faulty component location. Therefore,
the clustering eﬀect is not considered. As the aﬀected output bits due to faulty IC components depends on the testing

1

Signal Difference

(2)

where NT SV is the number of TSVs. The yield of pre-bond
TSV can be calculated as x = 0. Similarly for post-bond
TSV testing, the fault-free IC probability can be estimated
using Poisson distribution [18].
(Ncom ∗ Pcom )Nf ault −Ncom ∗Pcom
e
Nf ault !

(4)

where P is the single TSV failure rate, dic is the distance
between the T SVi and cluster center, and α is the clustering coeﬃcients indicating cluster extent. A large α indicates
higher clustering eﬀect. In our simulations, we assume the
cluster center is injected randomly and only forms a proportion of total defective TSV number. The rest of failure
TSV is generated with the combination of failure probability
and clustering eﬀect as mentioned in (4). The testing data
compression rate NC and output bandwidth improvement
for both pre-bond and post-bond circuit can be deﬁned as
follows.

Traditional lossless testing data compression methods such
as length run (LR) coding and Golomb coding (GLC) are
limited by the hardware implementation complexity for high
volume of testing data. Compressive-sensing is one recently
developed compression technique with sparse-representation
of data by ﬁnding the most sparse solution of undetermined
linear systems [14, 16]. Such a compression method results
in simple encoding with comparable hardware implementation complexity as MISR since only XOR and AND network
is required on-chip with data reconstruction oﬀ-chip.
To perform compressive-sensing, sparsity of testing data
is estimated based on its yield. For TSV testing, signal is
sparse in nature as only 1 − yield portion of data is nonzero
if we compare the received result to expected outcome using XOR operation. For a common TSV yield such as 99%
under pre-bond testing, there are 99% interconnects functioning properly. The defects are sparse in this sense indicating that the diﬀerence between the received and expected
outputs are sparse with only 1% non-zero value at locations
of faulty TSV interconnects. Similar to the pre-bond testing, the post-bond functional testing data can also be sparse
by taking the diﬀerence between the received and expected
data [12]. In order to know the sparsity, we need to estimate
the TSV yield Ypre and IC fault-free probability Ypost . Fig.
3 shows the testing data by XOR the expected and received
expected output for 1024 TSV under diﬀerent yields. Signal diﬀerence  1 indicates an defective TSV. Thus, we can
conclude that the higher yield, the more sparse the testing
data will be.
Similar to [11, 17], we assume a uniform failure probability
p for TSV under testing following binomial distribution. The
overall probability of having x defective TSVs is

Ypost =

(1/dic )α )

j=1

3.1 Sparsity of Testing data

x
Ypre = CN
px (1 − p)NT SV −x
T SV

Nc


0.5

0
0

500

TSV Number

(a)

1000

1

0.5

0
0

500

1000

TSV Number

(b)

Figure 3: (a) Sparse data for yield (Ypre ) of 95% (b) sparse
data for yield (Ypre ) of 99%

21

Algorithm 1 Orthogonal Matching Pursuit Algorithm

To ensure a successful recovery without loss, the sensing
(or sampling) matrix must satisfy the restricted isometry
property (RIP) [14]. In this paper, we use random Bernoulli
matrix that can be implemented from pseudo number generator in hardware with well recognized RIP property. For
the online TSV testing vehicles in Section 2, we can implement a counter to record non-zero values for lossless data
compression and reconstruction. The minimum dimension
M of compressed testing data [14] is
M = O(Klog(N/K))

Require: An MxN TDC generated sensing matrix Φ =[ ϕ1 , ϕ2 ,
...,ϕN ], an M-dimensional compressed testing data Y and
yield Yyield
Ensure: An accurate testing data Ef aulty
1: Initialize the residual r0 = Y , the index set Λ0 = ∅ , ΦΛ0 =
∅ and iteration counter t = 1.
2: Calculate sparsity K = Ceil(N − N Yyield )
3: While (t ≤ K)
4: Find column index λt of Φ correlates Y most as below
λt = argmaxj=1,...N | < rt−1 , ϕj > |,
5: Update column index set and matrix of chosen columns
Λt = Λt−1 ∪ λt
ΦΛt =[ΦΛt−1 ϕλt ]
6: Solve a least squares problem to obtain new signal
xt = argmin|| Y -ΦΛt x ||2
7: Calculate the new approximation and residual
at =ΦΛt xt , rt = Y − at , t = t + 1
8: End While
9: Ef aulty =xt

(7)

where K is the sparsity of data that can be estimated as (8)
and N is the total length of original testing data. N has
relationship with number of TSV NT SV by N = NT SV ∗
Ndata , where Ndata is the number signal sent from each TSV.
From (7), we can sample the signal based on its sparsity. The
sparsity K can be estimated as below.
K = Ceil(N − N Ypre )

(8)

where Ypre is the yield of pre-bond TSV testing. If we replace Ypre by Ypost , the sparsity K for the post-bond TSV
testing can also be calculated.

The scan chain output and the expected output from the
probe pad are XORed to obtain the testing data, Ef aulty ,
which is normally sparse with  1 to denote the failure. The
Bernoulli function is realized using a linear feedback shift
register (LFSR) with M measurements collected after performing M shifts. Here, M is the dimension of compressed
testing data Y . Furthermore, the Bernoulli matrix is multiplied with the testing data using AND gates, where bits
are added using the adder and fed to the probe pad. This
implementation reduces the number of outputs from N dimensions to M dimensions. Note that the above-mentioned
TDC implementation is suitable for both pre-bond and postbond TSV testing on-chip.

3.3 OMP based Compressed Data Recovery
As discussed in Section 3.1, L0 norm solution can be applied to solve (6). In this paper, we deploy the Orthogonal
Matching Pursuit (OMP) solver for the L0 norm solution,
which is a heuristic solver based on greedy algorithm to ﬁnd
the most sparse solution [19]. More details on compressed
testing data recovery from OMP are provided in Algorithm
1. The residual is initialized as compressed testing data Y .
Index set Λ0 and chosen matrix Φ0 are empty. The largest
correlated column is found from Step 4, while index and chosen matrix will be updated as Step 5. The new estimated
testing data is reconstructed in Step 6 via L2 minimization. The residual is updated from the estimated testing
data and compressed testing data. The iteration will stop
after K iterations. In summary, OMP performs two functions as follows. Firstly, it ﬁnds the most correlated column
from the sensing matrix Φ by comparing simple dot multiplication. Secondly, the largest correlated column is added
to the selected column and by solving a L2 norm minimization, the most ﬁtted new estimate testing data is generated.
This procedure will repeat K times to ﬁnd the recovered
testing data. Note that K is the sparsity of the testing data
Ef aulty , which can be estimated from the Ypre and Ypost .

4.

4.2 Pre-bond TSV Testing
For pre-bond TSV testing, short or open defects will lead
to receive incorrect testing data, which can be used to detect faulty TSVs after comparing to the expected data. Fig.
5a shows an example of TSV failure. In general, the size of
probe heads is large compared to the pitch of TSVs. Hence,
the probe head will contact a group of TSVs instead of each
individual TSV. As shown in Fig. 5b, among the group of
TSVs, only a few can have the I/O driving ability, which
can be used to output the testing data and the rest can only

XOR Network Bernoulli Matrix 

COMPRESSIVE-SENSING BASED HARDWARE IMPLEMENTATION

In this section, we will discuss the hardware implementation of compressive-sensing with XOR and AND network
followed by the application of compressive-sensing to prebond and post-bond TSV testing.

Mask
controller

LFSR

Scan Chain
Output
Scan Chain
Output
Scan Chain
Output

4.1 Compressive-sensing based Testing Circuit
To perform the data compression using the proposed algorithm, outputs from the scan chain and expected output are
provided as inputs for the testing data compression (TDC),
as shown in Fig.2. Note that X-states (unknown states) will
be masked to ’0’ based on the information from testing data
controlled by mask controller. The TDC testing can be implemented using adders and XOR gates as shown in Fig. 4.

Adder

Expected
TD-multiplexer
TDC
output/
Compressed
data Y
TSV Elevator or Probe Pad
Figure 4: Compressive-sensing based testing circuit diagram
for testing data compression

22

0001

Received

Short Defects

SF

0010

0000

0100

Connected
1000

1100
1100

No signal
transferred

Two TSV
Shorted

(a)
Probe Head

Probe Head

SF

TSV

SF

TSV

SF

TSV

TSV with I/O

SF
DIE

TSV

TSV without I/O

(b)

Therefore, no additional bandwidth is required. The testing data compression (TDC) will compress the testing data
through XOR-AND network described in Fig. 4. The compressed testing data will be sent to oﬀ-chip computational
resource for recovery. Note that as for the existing MISRbased compression method, faults are detected from one-inmany testing based on single signature. Insuﬃcient diagnostic information is available to designers. Moreover, detection
failure rate of MISR increases due to the error-cancelling effect under the clustering eﬀect and the non-uniform TSV
failure probability. In contrast, the proposed method performs the lossless compression without the error-cancelling
eﬀect. Moreover, it results in pin-level output of testing data
for TSV self-repair or the other debugging capability.

TSV

1111/0000
Open Defects

Probe Head

Sent

(c)

Figure 5: Conceptual diagram for pre-bond TSV testing
receive data from probe. As multiple TSVs are connected
to one probe head, it is important to diﬀerentiate each TSV
in the receiving end. Similar to [20], a scan ﬂip-ﬂop (SF)
controlled by digital enable circuit is utilized to diﬀerentiate the input from the TSVs as described in Fig. 5c. This
received data from top SF is provided as the input to the
XOR network of TDC (in Fig. 4) and then the compressed
testing data Y is sent through top TSV with I/O. The original testing data Ef ault can be recovered from compressed
testing data Y based on the proposed Algorithm 1.

Die 1

5. SIMULATION RESULTS
In this section, we discuss the experiments set-up, the testing data recovery and compression for pre-bond and postbond TSV testing. The compressive-sensing based TSV testing simulation platform is implemented in Matlab 2014a on
a computer with 3.2 GHz core and 8.0G memory.

5.1 Experiment Set-up
Firstly, we present the compressed testing data of TSVs
under diﬀerent yields and the corresponding reconstructed
data. The experiment is performed for 1024 number of TSVs
with input as ’1’ for each TSV to verify whether ’1’ is received. Faulty TSV is inserted based on (2).
Secondly, for pre-bond TSV testing, 4096, 16384 and 65536
TSVs are tested with a scan signal and a 3-bit testing output [20]. To model defective TSV distribution, 10% defective
TSVs are inserted as center in a TSV map; and the rest is
generated based on clustering eﬀect presented in (4).
Finally, for post-bond TSV testing, the proposed testing
data compression method is applied for ISCAS-85 benchmarks [21] in Verilog based on stuck-at fault model. The
testing pattern is generated using Mintest [22], which provides 100% fault coverage. An 8-bit output signal after scan
testing is assumed and error bit probability is modeled based
on (3) for stuck-at fault model. We select length-run (LR)
coding and Golomb coding (GLC) coding for performance
comparison as they both perform lossless compression and
close to the entropy of information [23]. The area overhead of proposed TDC is synthesized using DC Synopsys

Connector
Lines
TSV Lines
Driver/Sampler
Source/Sink

Die 2

Router
BIST and
d SScheduler

Figure 6: Conceptual diagram for post-bond TSV testing

4.3 Post-bond TSV Testing
For post-bond functional testing, our proposed compression technique can work independently or co-work with conventional MISR techniques to further compress the testing
data depending on the lossless/lossy testing requirement.
Our proposed compression algorithm provides general solutions to digital signal lossless compression. Fig. 6 shows
a conceptual post-bond TSV testing diagram where builtin-self-test (BIST) circuit is shared between diﬀerent TSV
groups. The signal generated from BIST is scheduled and
sent to router. Router will send the signal to TSV group
driver for testing purposes by a sequence of digital bits similar as in the pre-bond test. Testing data compression (TDC)
will receive the expected testing data from the router and
compress them. Similarly ATPG can also be performed
where the expected output is known. As shown in Fig. 4,
the compressed testing data Y is collected and original testing data Ef ault can be recovered from Y as shown in the
proposed Algorithm 1.

Input Patterns
Probe
/Pad

Data collection
via probe/PAD
Pre-bond

XOR-AND
MATRIX 

DUT
Xe

4.4 TSV Testing Flow

Xr

Compressed
data Y
Boundary
Scan

Pre-bond
Flow

Proposed
Algorithm 1

XOR-AND
network

Fig. 7 depicts the overall testing ﬂow for the proposed
TSV online testing vehicle. The input patterns and the expected output will be sent to device under test (DUT) and
XOR-AND network respectively through input and output
testing pads or elevators as shown in Fig. 2. Note that
the expected output will be sent through output testing
pads based on time-division multiplexer (TD-multiplexer).

Post-bond
Flow

Postbond

Off-chip data
recovery
Efaulty
Identify Faulty
TSV/circuits

On-chip
Compression

Off-chip
Recovery

Figure 7: Testing ﬂow for proposed testing vehicle

23

Table 1: Testing data Compression Rate and Bandwidth Improvement in Pre-bond Testing with 0.5 % (upper row) and 1% (lower row)
Failure Rate
TSV
Number

4096

16384

65536

Proposed

Cluster
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α
α

=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=
=

0
1
2
0
1
2
0
1
2
0
1
2
0
1
2
0
1
2

Comp. Rate
89.45%
89.70%
89.32%
65.29%
65.03%
66.48%
89.16%
89.23%
89.35%
64.80%
64.29%
64.86%
89.17%
89.21%
89.24%
65.32%
64.59%
64.88%

Bw Imp.
9.48 x
9.71 x
9.36 x
2.88 x
2.86 x
2.98 x
9.22 x
9.28 x
9.39 x
2.84 x
2.80 x
2.85 x
9.24 x
9.27 x
9.30 x
2.88 x
2.82 x
2.85 x

Length Run
Coding
Comp. Rate
Bw Imp.
81.80%
5.49 x
81.14%
5.30 x
81.52%
5.41 x
50.99%
2.04 x
50.73%
2.03 x
51.52%
2.06 x
80.37%
5.09 x
80.15%
5.04 x
80.47%
5.12 x
49.42%
1.98 x
46.93%
1.88 x
50.62%
2.03 x
79.77%
4.94 x
79.73%
4.93 x
79.38%
4.85 x
50.03%
2.00 x
48.57%
1.94 x
47.08%
1.89 x

Golomb Coding
GS= 8
Comp. Rate
Bw Imp.
76.83%
4.32 x
77.29%
4.40 x
76.68%
4.29 x
35.57%
1.55 x
35.57%
1.55 x
37.16%
1.59 x
75.95%
4.16 x
73.99%
3.84 x
75.59%
4.10 x
34.08%
1.52 x
29.49%
1.42 x
34.51%
1.53 x
73.48%
3.77 x
73.09%
3.72 x
73.49%
3.77 x
34.27%
1.52 x
34.19%
1.52 x
26.68%
1.36 x

Recovered−Original

(D-2010.03-SP2) and estimated 4522um2 using 65nm process technology from Globalfoundries.

5.2 Compressive-sensing in TSV Testing
As shown in Fig. 4, the XOR of received and expected
data is multiplied by a Bernoulli based sensing matrix Φ,
which is generated from a binary pseudo number generator. The TSV with defects will not receive ’1’ and therefore the XOR result will be ’1’, indicating the failure of this
TSV. Furthermore, one encoded output is generated from
the adder based on the row of sensing matrix Φ multiplied
by the XOR result. As an example, we collected 200 output
measurements and plotted the adder output under diﬀerent
yields in Fig. 8. From Fig. 8(a), one can observe that the
adder results are smaller for TSV with 99% yield, compared
to adder results shown in Fig. 8(b) for TSV with yield 95%.
It indicates that high yield testing data will required less
bits for encoding, which results in higher compression rate
as per Equation (5).
Measurements

5

0
0

100

Cycles

(a)

200

35

Adder Results

Adder Results

10

(a)

Golomb Coding
GS =32
Comp. Rate
Bw Imp.
81.23%
5.33 x
81.30%
5.35 x
80.98%
5.26 x
48.02%
1.92 x
47.29%
1.90 x
49.51%
1.98 x
80.38%
5.10 x
78.63%
4.68 x
80.27%
5.07 x
47.35%
1.90 x
42.03%
1.73 x
47.73%
1.91 x
78.26%
4.60 x
78.12%
4.57 x
78.26%
4.60 x
47.17%
1.89 x
47.28%
1.90 x
45.34%
1.83 x

0.01
0.005
0
−0.005
−0.01
0

500

TSV Index

1000

(b)

Figure 9: (a) Maximum diﬀerence vs. number of output (b)
signal reconstruction with 60 measurements for yield of 99%
nearly 190 measurements needed to fully recover the testing data for yield 95%. Since the sparse solution is unique
[14], we can conﬁrm the correctness of reconstruction by
performing OMP solver twice. For lossless reconstruction,
the maximum diﬀerence between the recovered and original
data should be suﬃciently small. As Fig. 9(b) shows, for
99% yield, the maximum diﬀerence between the recovered
and original data is as small as E −2 for 60 measurements
to represent 1024 TSVs, corresponding to a 82.42% data
compression rate (1 − 60 × 3/1024).

Measurements

30
25

5.3 Compressive-sensing in Pre-bond Testing

20
15
0

Golomb Coding
GS= 16
Comp. Rate
Bw Imp.
79.98%
5.00 x
80.18%
5.05 x
79.76%
4.94 x
44.38%
1.80 x
43.80%
1.78 x
45.85%
1.85 x
79.11%
4.79 x
77.28%
4.40 x
78.89%
4.74 x
43.39%
1.77 x
38.37%
1.62 x
43.84%
1.78 x
76.86%
4.32 x
76.63%
4.28 x
76.86%
4.32 x
43.35%
1.77 x
43.41%
1.77 x
40.21%
1.67 x

100

Cycles

In Fig. 10, the red square is the defective TSV cluster
center and black circle is the defective TSV generated from
failure probability and clustering eﬀect based on (4). X-axis
and Y-axis represent the location of TSV. The average failure probability is 20 % and due to the clustering eﬀect, the
failure probability can be as high as 81.52% for the TSVs
close to the center as Fig. 10c. Table 1 shows the output
testing data compression rate for diﬀerent clustering factor
α and the number of TSVs. A compression of nearly 89% is
achieved for 4096 TSVs with failure rate of 0.5%, but is reduced to nearly 66% with failure rate of 1%. This indicates
that more number of measurements is required for the lossless compression when the failure probability increases. It
also shows that despite the existence of clustering eﬀect,the
compression rate will be maintained almost the same.
In addition, as shown in Table 1, we compare our compression algorithm with length-run (LR) coding and Golomb
coding (GLC) based compression algorithms [23]. Note that
Golomb coding is greatly aﬀected by the tunable group size
GS. For example, if we consider the case of 16384 TSVs with

200

(b)

Figure 8: (a) Compressed output for yield of 99% (b) compressed output for yield of 95%
Furthermore, the minimum number of required measurements (dimension of compressed testing data) under different yields to achieve a lossless recovery is presented in
Fig. 9. As illustrated in Fig. 9(a), the diﬀerence between the recovered and original data decreases dramatically as the measurements (M ) increases, as expected from
the compressive-sensing theory [14] , indicating a least number of measurements are required for lossless compression.
As such, the higher the yield is, the less number of measurements (M ) is required. For example, when the yield
is as high as 99%, there are nearly 60 measurements good
enough to fully recover the testing data; whereas, there are

24

10

10

10

sensing based method. By exploring the sparsity of the
testing data, one can achieve on-chip data compression and
lossless oﬀ-chip data recovery. The encoding for compression
can be easily implemented on-chip using XOR and AND networks with signiﬁcantly improved bandwidth for the output
of the testing data. As such, it can result in an eﬃcient implementation of online TSV testing vehicle to improve TSV
yield with TSV self-repair capability. Experiment results
with benchmarks have shown that 89.70% pre-bond data
compression rate can be achieved under 0.5% failure probability; and 88.18% post-bond data compression rate can be
achieved with 5% failure probability.

81.52%

61.12%

5 41.44%

5

5
20.45%

0
0

5

10

0

5

10

0
0

57.56%
21.43%

5

10

(a)
(b)
(c)
Figure 10: (a) No clustering eﬀect (b) clustering eﬀect with
α = 1 (c) clustering eﬀect with α = 2
failure probability 1% and clustering factor α as 1, our proposed algorithm can successfully compress 64.29%; whereas
LR can only compress 46.93%; GLC with GS = 8, 16, 32
can only compress 29.49%, 38.37% and 42.03%, respectively.
The bandwidth improvement can also be derived from (5) as
shown in Table 1. Note that with increasing GS, GLC compression rate of testing-data will converge to the information
entropy. However, hardware complexity and decoding time
also increase dramatically, which limits us to compare until
GS = 32.
Table 2: Testing Data Compression in Post-bond Testing
Failure
Prob.

5%

10%

Benchmark
c499
c432
c1908
c2670
c3540
c5315
c6288
c7552
c499
c432
c1908
c2670
c3540
c5315
c6288
c7552

Output
(bits)
1696
196
2475
6300
1870
4674
416
7992
1696
196
2475
6300
1870
4674
416
7992

Proposed
88.15%
88.18%
86.89%
82.56%
87.76%
81.80%
85.65%
80.30%
73.82%
82.19%
70.57%
61.42%
70.82%
63.71%
76.06%
60.99%

LR
Coding
78.22%
77.96%
73.69%
73.81%
80.55%
75.31%
81.15%
74.27%
59.53%
68.67%
56.00%
55.17%
56.56%
52.38%
58.51%
54.45%

GLC
GS =8
73.01%
76.38%
72.83%
69.77%
75.75%
71.13%
82.21%
68.26%
50.17%
63.06%
45.86%
44.25%
46.24%
40.88%
49.83%
43.95%

7. REFERENCES
[1] J. R. Cubillo et al., “Interconnect design and analysis for
through silicon interposers (TSIs),” in IEEE 3DIC, 2012.
[2] L. Jiang, Q. Xu, and B. Eklow, “On eﬀective TSV repair for
3D-stacked ICs,” in DATE. ACM/IEEE, 2012, pp. 793–798.
[3] C. Zhang and et.al., “Novel crack sensor for TSV-based 3D
integrated circuits: design and deployment perspectives,” in
IEEE ICCAD, 2013.
[4] F.-W. Chen, H.-L. Ting, and T. Hwang, “Fault-tolerant tsv by
using scan-chain test tsv,” in IEEE ASP-DAC, 2014.
[5] B. Zhang and V. D. Agrawal, “An optimal probing method of
pre-bond TSV fault identiﬁcation in 3D stacked ICs,” in IEEE
S3S, 2014.
[6] E. J. Marinissen, “Challenges and emerging solutions in testing
TSV-based 1/2D-and 3D-stacked ICs,” in IEEE DATE, 2012.
[7] B. Noia and K. Chakrabarty, “Pre-bond probing of TSVs in 3D
stacked ICs,” in IEEE ITC, 2011.
[8] E. J. Marinissen and Y. Zorian, “Testing 3D chips containing
through-silicon vias,” in IEEE ITC, 2009.
[9] J. Xie, Y. Wang, and Y. Xie, “Yield-aware time-eﬃcient testing
and self-ﬁxing design for TSV-based 3D ICs,” in IEEE
ASPDAC, 2012.
[10] P. H. Bardell, W. H. McAnney, and J. Savir, Built-in test for
VLSI: pseudorandom techniques. Wiley-Interscience, 1987.
[11] M. Tahoori, “Defects, yield, and design in sublithographic
nano-electronics,” in IEEE Defect and Fault Tolerance in
VLSI Systems, 2005.
[12] W. Maly, “Realistic fault modeling for VLSI testing,” in IEEE
DAC, 1987.
[13] B. Sklar, Digital communications. Prentice Hall NJ, 2001,
vol. 2.
[14] D. L. Donoho and M. Elad, “Optimally sparse representation in
general (nonorthogonal) dictionaries via L1 minimization,”
Proceedings of the National Academy of Sciences, 2003.
[15] E. J. Candes and T. Tao, “Near-optimal signal recovery from
random projections: Universal encoding strategies?” IEEE
Transactions on Information Theory,, vol. 52, no. 12, pp.
5406–5425, 2006.
[16] D. L. Donoho, “For most large underdetermined systems of
linear equations the minimal L1-norm solution is also the
sparsest solution,” Communications on pure and applied
mathematics, vol. 59, no. 6, pp. 797–829, 2006.
[17] Y. Zhao and et.al., “Cost-eﬀective TSV grouping for yield
improvement of 3D-ICs,” in IEEE ATS, 2011.
[18] G. S. May and C. J. Spanos, Fundamentals of semiconductor
manufacturing and process control. John Wiley & Sons, 2006.
[19] J. A. Tropp and A. C. Gilbert, “Signal recovery from random
measurements via orthogonal matching pursuit,” IEEE
Transactions on Information Theory,, vol. 53, no. 12, pp.
4655–4666, 2007.
[20] B. Noia, S. Panth, K. Chakrabarty, and S. K. Lim, “Scan test
of die logic in 3D ICs using TSV probing,” in IEEE ITC, 2012.
[21] M. C. Hansen, H. Yalcin, and J. P. Hayes, “Unveiling the
ISCAS-85 benchmarks: A case study in reverse engineering,”
IEEE Design and Test of Computers, vol. 16, no. 3, pp. 72–80,
1999.
[22] I. Hamzaoglu and J. H. Patel, “Testset compaction algorithms
for combinational circuits,” in IEEE ICCAD, 1998.
[23] A. Chandra and K. Chakrabarty, “Test data compression for
system-on-a-chip using golomb codes,” in IEEE VTS, 2000.

GLC
GS =16
76.42%
78.57%
76.34%
73.74%
78.72%
74.97%
84.13%
72.49%
57.17%
66.94%
53.27%
51.80%
53.76%
49.14%
56.49%
51.55%

5.4 Compressive-sensing in Post-bond Testing
We ﬁnally discuss the post-bond testing data compression. We assume 5% and 10% probabilities of failure IC
for an 8-bit output signal (signature), which mean 0.639%
and 1.308% failure probabilities for each bit. Similar to prebond testing, we compare our proposed compression algorithm with LR and GLC coding based compression algorithms, and is presented in Table 2. It also shows that the
data compression rate outperforms length-run coding and
Golomb coding. The testing data compression for 5% failure
probability varies from 80.03% to 88.18%, whereas 74.27% to
68.26% for LR coding; and 76.42% to 72.49% for GLC with
GS = 16. For circuit c7552, our proposed algorithm has a
6.03% and 7.81% improvement compared to LR and GLC,
respectively. However, as failure probability increases, our
proposed testing data compression rate outperforms further
by 6.55% and 9.45% compared to LR and GLC with GS =
16 respectively. Moreover, since the proposed algorithm is
lossless compression, it can also co-work with MISR or other
conventional compression techniques to further compressed
the testing data with shared circuit implementations.

6.

CONCLUSION

In this paper, the testing data compression is discussed
for pre-bond and post-bond TSV testing via compressive-

25

68

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

A Configurable 12–237 kS/s 12.8 mW
Sparse-Approximation Engine for Mobile Data
Aggregation of Compressively Sampled
Physiological Signals
Fengbo Ren, Member, IEEE, and Dejan Marković, Member, IEEE

Abstract—Compressive sensing (CS) is a promising technology
for realizing low-power and cost-effective wireless sensor nodes
(WSNs) in pervasive health systems for 24/7 health monitoring.
Due to the high computational complexity (CC) of the reconstruction algorithms, software solutions cannot fulfill the energy
efficiency needs for real-time processing. In this paper, we present
a 12—237 kS/s 12.8 mW sparse-approximation (SA) engine chip
that enables the energy-efficient data aggregation of compressively
sampled physiological signals on mobile platforms. The SA engine
chip integrated in 40 nm CMOS can support the simultaneous
reconstruction of over 200 channels of physiological signals while
consuming <1% of a smartphone’s power budget. Such energyefficient reconstruction enables two-to-three times energy saving
at the sensor nodes in a CS-based health monitoring system as
compared to traditional Nyquist-based systems, while providing
timely feedback and bringing signal intelligence closer to the user.
Index Terms—Application-specific integrated circuits (ASICs),
biomedical signal processing, compressed sensing, digital integrated circuits, energy efficiency, low-power design, minimization
methods, parallel architecture, real-time systems, reconfigurable
architecture, signal reconstruction.

I. I NTRODUCTION

D

IGITAL electronic industry today relies on Nyquist
sampling theorem, which requires to double the size
(sampling rate) of the signal representation on the Fourier
basis to avoid information loss. However, most natural signals
have much sparser representation on some other, non-Fourier,
orthogonal basis. This implies a large amount of redundancy in
Nyquist-sampled data, making compression a necessity prior to
storage or transmission [1], [2]. Recent advances in compressive sensing (CS) theory suggest an alternative data acquisition
framework that can directly access the signal information in its
sparse domain [3], [4]. Compared to the conventional Nyquist
Manuscript received May 09, 2015; revised August 23, 2015; accepted
September 11, 2015. Date of publication October 15, 2015; date of current
version December 30, 2015. This paper was approved by Guest Editor Edith
Beigne. This work was supported in part by Broadcom Fellowship Program
and in part by TSMC University Program.
F. Ren was with the University of California, Los Angeles, CA 90095 USA.
He is now with Arizona State University, Tempe, AZ 85281 USA (e-mail:
renfengbo@asu.edu).
D. Marković is with the University of California, Los Angeles, CA 90095
USA (e-mail: dejan@ee.ucla.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JSSC.2015.2480862

framework, the CS framework has several intrinsic advantages.
First, random encoding is a universal compression method that
can effectively apply to all compressible signals regardless of
what their sparse domain is. This is a desirable merit for the
data fusion across multiple signal sources. Second, sampling
and compression can be performed at the same stage in CS,
allowing for a sampling rate that is significantly lower than the
Nyquist rate. Therefore, CS has a potential to greatly impact
the data acquisition devices that are sensitive to cost, energy
consumption, and portability, such as wireless sensor nodes
(WSNs) in mobile and wearable applications [5].
Especially, CS is a promising solution for realizing the onbody WSNs in pervasive health systems toward 24/7 health
monitoring [6]. Electrocardiogram (ECG), electromyography
(EMG), and electroencephalogram (EEG) signals (collectively
referred to as ExG) contain critical information about human
body status and are therefore the main targets in health monitoring applications. As shown in Fig. 1, a CS-based wireless health
monitoring system includes the on-body WSNs that utilize a
unified random encoding scheme to compress different physiological signals to reduce the data size for transmission (thereby
saving transmit energy), and a mobile data aggregator that performs real-time signal reconstruction to promote on-site analysis and processing for real-time applications. Such a system has
numerous benefits. First, it brings the signal intelligence closer
to the user for timely prediction and decision-making. This
is particularly important for real-time tasks such as arrhythmia and seizure detection, EMG-driven machine actuation, and
brain–computer interface. Second, by reconstructing the sparse
coefficients of the original signal only, the data size for on-site
storage or transmission to the cloud can be further reduced. For
practical use, the data aggregator is desired to have a sufficient
throughput for reconstructing > 50 channels of physiological
signals (sampled at ≤ 1 kHz) in real time [7]. Additionally, to
minimize the overhead of adding such a function to a mobile
device, the power consumption of the data aggregator is desired
to be bounded within 1% of a mobile device’s 2 W power
budget. This implies a sparse-approximation (SA) engine that
can support > 50 kS/s throughput in <20 mW of power (see
Fig. 2). It is also desirable to have flexibility for varying sparsity parameters, orthogonal basis, and the number of channels.
Such a set of specifications imposes significant challenges to
the hardware implementation.

0018-9200 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

REN AND MARKOVIĆ: CONFIGURABLE 12–237 kS/s 12.8 mW SA ENGINE

Fig. 1. (a) CS-based wireless health monitoring system with desired system
requirements: on-body WSNs that utilize a unified random encoding scheme
to compress data for low energy and a mobile data aggregator that performs
real-time signal reconstruction for timely prediction and proactive prevention.
To further reduce the data size for storage or processing, only the sparse
coefficients of the signal are reconstructed. Reconstruction takes <1% of a
mobile device’s power budget, allowing 2−3× energy saving of the sensors
[5]. (b) Amplitude and frequency characteristics of ExG signals.

The first challenge is the complexity of SA algorithms. SA
is an optimization problem that involves complex operations in
an iterative process with intensive memory access. Compared
to the orthogonal transformations used in the Nyquist framework, SA algorithms have greater computational complexity
(CC) and higher data dependency (DD). The second challenge
stems from the intricacies of physiological signals. ExG signals can span three orders of magnitude in both amplitude
(10 µV to 10 mV) and frequency (0.1–500 Hz) (see Fig. 1).
In addition, due to the difference in physiological activity of
the signal sources, these signals could have sparse representations on completely different orthogonal bases. Furthermore,
their sparsity is time-varying depending on the subject’s activity [8]. For the best reconstruction results, the hardware design
must be able to handle a high dynamic range and flexible problem settings, such as reconstruction basis (Ψ), error tolerance
(), signal and measurement dimensions (n and m), and signal
sparsity level (k).
So far, there has been very limited work and demonstration
of dedicated SA solver chips [9]–[11]. The application-specific
integrated circuit (ASIC) implementations of three greedy algorithms are first presented in [9] for the long-term evolution
(LTE) channel estimation in wireless communication applications. These implementations in 180 nm CMOS feature
a target throughput of 2 kS/s with the power consumptions
of 88–209 mW. A 65 nm generic solver chip implementing
the approximate message passing (AMP) algorithm is demonstrated in [10] for an audio restoration application. This chip
achieves a target throughput of 397 kS/s at the power consumption of 177.5 mW for processing audio signals that have
a relatively lower sparsity. Prior designs mainly focused on
achieving the target throughputs, with much less emphasis on
power/energy and area efficiency. Besides, prior designs were
optimized for a limited dynamic range and a fixed problem
setting, making them unsuitable for biosensing applications.

69

In this paper, we present a configurable and energy-efficient
SA engine chip in 40 nm CMOS that addresses above challenges and makes the CS technology accessible to mobile users.
The chip testing results illustrate a reconstruction throughput
of 66–237 kS/s and a power consumption of 12.8 mW when
operating at VDD = 0.7 V. Such level of performance can support the simultaneous reconstruction of over 200 channels of
compressively sampled ExG signals in real time while consuming <1% of a smartphone’s power budget. The high energyefficiency of our chip results from an algorithm-architecture
codesign approach that facilitates the tight interactions between
1) algorithm reformulations that reduce the algorithm complexity by an order of magnitude; 2) a configurable system
architecture that leads to nearly 100% utilization of computing resources; and 3) an efficient memory control scheme that
cuts down the memory usage by half. The system architecture of the SA engine chip is optimized toward mapping the
orthogonal matching pursuit (OMP) algorithm and its variants [12], [13]. Because human body is expected to have a
low activity on average where ExG signals feature a high
sparsity, especially when dynamic thresholding schemes are
used [8], this is where OMP has better complexity–accuracy
tradeoff than other SA algorithms [14]. The SA engine chip
implements domain transformation by explicit matrix multiplication thereby supporting signal reconstruction on arbitrary
basis. Additionally, the SA engine adopts the single-precision
floating-point data format to achieve a large dynamic range
and can be configured at run time to handle flexible problem
settings and accurately recover a wide range of physiological
signals.
II. A LGORITHM R EFORMULATION T OWARD E NERGY
E FFICIENCY
Energy efficiency is the metric indicating how much computing can be performed with a finite energy source. For
dedicated algorithms running on hardware, energy efficiency
is usually defined as the energy consumption per algorithmic
execution, which can be measured by the ratio of power (J/s)
and processing throughput (S/s). From the hardware perspective, both the CC and the DD characteristics of an algorithm
impact the energy efficiency. A high CC indicates a large
amount of computations per algorithmic execution, implying
more switching energy from the logic gates. On the other hand,
a high loop-carried DD indicates low concurrency of computations, generally implying increased memory usage and longer
execution time that leads to higher leakage energy.
OMP is a fast and heuristic algorithm that can recover a
k-sparse signal in exact k iterations given the constraints in
the context of CS [3], [4], [12]. The pseudocode of the original OMP algorithm is shown in Table I (see Appendix for
notations). Note that Cholesky factorization is favored over QR
factorization as the numerical method for solving the leastsquares (LS) problem since QR factorization requires three
times more memory for storing the factorization matrices,
which is undesired for memory-leakage-limited design. In each
iteration, three tasks are performed: 1) atom searching (AS)
for updating the active set; 2) LS solving for computing the

70

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

TABLE I
P SEUDOCODE OF THE OMP A LGORITHM

TABLE II
P SEUDOCODE OF THE R EFORMULATED OMP A LGORITHM

∗ Assuming
# Refer

that all atoms in A are normalized.
to Appendix and [12] for notations.

∗ Assuming

that all atoms in A are normalized.
operation only.
# Refer to Appendix and [12] for notations.
† Memory

Fig. 2. Complexity characteristic of the OMP algorithms. The impact of the
reformulation techniques is making OMP more energy-efficient for hardware
implementations by simplifying the LS task.

estimation; and 3) estimation update (EU). The complexity
characteristic of the OMP algorithms is illustrated in Fig. 2.
Note that the LS task is a minimization problem that involves
both high CC and loop-carried DD [14]. For energy-efficient
mapping, three algorithm reformulation techniques are previously introduced to break down the LS task into four basic
linear algebra (BLA) operations (at each iteration) [14]–[16].
The pseudocode of the reformulated OMP algorithm is shown
in Table II (see Appendix for notations). Such simplification
not only reduces the total CC of the LS task from O(mk 3 )
to O(mk 2 ) but also cuts the number of data-dependent loops
involved from k 3 down to k 2 (see Appendix B for definitions
of n, m, k in the context of CS), making the reformulated OMP
algorithm much more suitable for an energy-efficient hardware
implementation [14].
In the reformulated OMP, the AS task that features high
CC but low DD has the biggest impact on throughput.
Parallelization is applied in our architecture design to relax the
transistor switching speed for gaining energy efficiency. On the
other hand, the LS task plays a pivotal role on hardware utilization. Note that any hardware resource designed exclusively
for the LS task will have a very low utilization rate due to the
low CC. Consequently, resource sharing is applied in the architecture design to improve hardware utilization and gain energy
efficiency from reduced area and leakage costs.

Fig. 3. System architecture of the SA engine chip.

III. A RCHITECTURE D ESIGN
A. System Architecture
The system architecture of the SA engine chip is shown in
Fig. 3. The computing resources include the vector and scalar
processing cores (VC and SC). In order to support a realtime throughput with high energy efficiency, 128 processing
elements (PEs) are coordinated in parallel through the interconnect block (IB) in the VC. These PEs can process independent
data in a single-instruction-multiple-data (SIMD) fashion or
interconnected by the IB to perform pipelined operations. The
large parallelism of PEs allows the SA engine to achieve the
target throughput at a scaled supply voltage and reduced operating frequency. Therefore, additional energy efficiency can be
gained from the relaxed transistor performance. Depending on
the top-level data-path configuration, SC can either postprocess a selective result from the VC through the VC-multiplexer
(VC-MUX) or process independent data from memories in
parallel.

REN AND MARKOVIĆ: CONFIGURABLE 12–237 kS/s 12.8 mW SA ENGINE

For efficient local memory access, a dedicated cache is
assigned to each PE in the VC and the SC, respectively. To
facilitate the data communication between VC and SC in long
delay lines, such as carrying over intermediate results between
different tasks or different iterations of the algorithm, a shared
cache can be accessed by all the PEs in the VC and the SC. In
addition, a core-level shift-register logic (SRL) unit (core-SRL)
is used to connect all the PEs with the SC. This customized
feedback path minimizes the loop latency between VC and SC
by avoiding any memory access, thereby accelerating the iterative BLA operations such as forward and backward substitution
(FS and BS). A dedicated memory unit stores the index of the
active set. The controller of this memory unit is also responsible for accessing the data in the sampling matrix from external
memories.
To allow the processing of different signal representations,
a parallelized fixed-to-floating-point conversion interface is
available at the external input of the VC. Note that there are
several data-paths bridging the VC and SC in the system architecture. The complex data flow of the reformulated OMP is
enforced by the customized local memory controllers (“PE-$CTRL” and “SC-$-CTRL” in Fig. 3), which are coordinated by
a global controller. All these controllers are dedicated finitestate machines (FSMs) with programmable state transition
contingent upon the values of the configuration bits (“configbit” in Fig. 3). The configuration bits set the problem size
(m, n) and error tolerance (ε) for each run of the algorithm.
Therefore, the SA engine can be configured with a different
problem setting for each reconstruction. Note that the memorybased data-flow-control schemes are efficient in handling data
reordering operations, such as matrix transpose, since most
of the data movements can be realized by pointer manipulations. The dynamic configuration of the computation cores is
also controlled by dedicated FSMs in a similar fashion. The
SA engine uses first-in-first-out (FIFO) interfaces to handle the
flow control at the data I/Os. When a reconstruction is done, the
chip loads new random samples (y) and unloads reconstructed
sparse coefficients (“xvalue ” in Fig. 3) with their index (“xloc ”
in Fig. 3) simultaneously. Once the loading and unloading are
complete, the next reconstruction is kicked off.
B. Computation Cores
The block diagram of the PE in the VC is shown in Fig. 4.
The PE integrates two basic arithmetic units in a pipeline: a
multiplier and an adder. Flexible data-path connections are realized by inserting multiplexers at each input of the arithmetic
units. Therefore, the PE can be dynamically configured to execute different operations or take different operands through the
control bits of the multiplexers. Note that the multiplier can be
bypassed by setting one of its inputs to 1, and the adder can
be bypassed by resetting the SRL output to 0. Therefore, the
PE can perform a selective set of operations including multiplication, recursive multiplication, power operation, addition,
accumulation, and MAC.
On the VC level, the 128 PEs can perform vector operations
in an SIMD fashion including vector addition, element-wise
multiplication, element-wise MAC, and vector–scalar product.

71

Fig. 4. Block diagram of PE and PE interconnections provided by IB.

Fig. 5. Block diagram of SC.

To enable folded processing of long vectors, an SRL unit is
inserted into the feedback path of each PE. The folding factor is dictated by the latency of the SRL unit. In addition, the
PEs can be coordinated through the IB to compute vector inner
products (Fig. 4). The IB connects the adders distributed in different PEs into a pipelined adder tree through the registers and
multiplexers inside the PEs, so that the element-wise products
can be added up to a scalar. The inner product computation
in this mode is highly scalable: for a different vector length,
the corresponding result can be selected by the VC-MUX at a
different pipeline stage. The folded inner product computation
in this mode needs an additional accumulator at the output of
VC-MUX, which is carried by the SC.
The block diagram of the SC is shown in Fig. 5. The SC
integrates a comparator, a sequential divider, and two adders
with configurable data-paths. Similar to the VC, the SC can also
be configured to perform a variety of operations through the
control bits of the multiplexers. When the SC is cascaded with
the VC, complex operations such as correlation sorting, FS, and
BS can be performed.
The first stage adder in the SC plays a critical role in two
tasks. First, it accumulates the result from the VC to support
folded inner product. Second, it adds the RHS of a linear equation to the LHS for performing FS and BS. The sequential
divider is used to handle the inverse of a diagonal matrix as
in op. 5 of Table II. Note that the division in op. 5 is not part of
the data-dependent loops in solving FS. Therefore, the latency

72

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

Fig. 7. Data folding scheme of PE caches in the (a) mirror and (b) shuffle mode
for handling Cholesky factorization (k = 192).

Fig. 6. Data mapping scheme of PE caches in the (a) mirror and (b) shuffle
mode for handling Cholesky factorization.

of the divider has no impact on the throughput of FS execution.
Five pipelined stages are inserted (through retiming) to remove
the divider from critical path. The comparator is used to perform sorting tasks, such as sorting the correlation coefficients
in the AS task. The second-stage adder can be used to update
the results of folded inner product as in op. 6a of Table II or to
update independent data as in op. 8 of Table II.
C. Memory Control Scheme
In the LS task of the reformulated OMP, the parallel column
and row access of the triangular Cholesky factorization matrix
L ∈ Rk×k are required for performing FS and BS, respectively
[17]. As accessing a row of L is equivalent to accessing a column of LT , a straightforward memory mapping scheme of PE
caches is to store both L and LT in a square matrix as illustrated in Fig. 6(a). We refer to this data mapping scheme as
the mirror mode. In the mirror mode, columns of L and LT
can be accessed at the same address of each PE cache in an
ascending and descending order, respectively. For instance (see
Fig. 6), the column vector l1 , l2 , and l3 can be accessed in parallel by reading the data at address 0, 1, and 2 of each PE cache,
respectively. The row vector l1T , l2T , and l3T can be accessed in
parallel by reading the data at address 4, 3, and 2 of each PE
cache, respectively. An advantage of the mirror mode is that a
large square matrix can be easily folded into smaller sub-blocks
so that a large-size Cholesky factorization can be computed in a
folded fashion by utilizing the PE-SRL and the core-SRL units.
An example data folding scheme in the mirror mode is illustrated in Fig. 7(a), where a folding factor of 1.5 is presented
with 128 parallel PEs and k = 192.
The down side of the mirror mode is that it doubles memory space for storing L. Since the SA engine is a memoryleakage-limited design, where memory leakage has significant
impact on the system’s energy efficiency, the mirror mode is
highly undesired. To avoid such an overhead, we propose a

shuffle-mode scheme that is more efficient in utilizing memory
space in our design. In the shuffle mode, the row elements of L
are stored across adjacent PE caches in a shuffle order as illustrated in Fig. 6(b). The data are shuffled such that the row access
pattern remains the same, but the column of L can be accessed
from the same memory space by using an incremental address
pattern across PE caches. Note that a circular position shift must
be performed at the memory output in order to recover the correct data order. For instance (see Fig. 6), the column vectors
l1 , l2 , and l3 can be accessed in parallel by reading the data at
address 0, 1, and 2 of each PE cache with a position up-shift by
0, 1, and 2, respectively. Differently, the row vector l1T , l2T , and
l3T can be accessed in parallel by reading the data at the address
set of [0, 1, 2, 3], [X, 0, 1, 2], and [X, X, 0, 1] with a position
up-shift by 0, 1, and 2, respectively.
By adopting the shuffle-mode scheme, a 2× memory size
reduction is achieved as compared to the mirror-mode case.
According to the postlayout simulation results, this leads to
another 40% saving in total power consumption of the chip due
to the reduced memory leakage. The corresponding data folding
scheme of the shuffle mode is illustrated in Fig. 7(b).
D. Dynamic Configuration of System Architecture
Taking advantages of the reformulated OMP algorithm with a
simplified LS task, we manage to reuse computing resources to
perform all the three tasks through dynamic configuration. Due
to the intrinsic DD between the six BLA operations in Table II,
the proposed resource sharing scheme maximizes the hardware utilization rate and area efficiency without introducing
throughput overhead.
Fig. 8 illustrates the dynamic configuration of the system
architecture in three tasks. In the AS task, the VC is cascaded with the SC in pipeline. The VC accesses ai and rt−1 in
parallel from the external memory and the PE caches, respectively. The PEs are configured to compute their inner product as
c (i) = ai rt−1 . The SC accumulates the result when folding is
enabled and compares the absolute values of c (i) with that of
c (i − 1). The smaller value is dropped, while the larger value
and the associated column index is buffered for the next comparison. After all the correlation coefficients are compared, the

REN AND MARKOVIĆ: CONFIGURABLE 12–237 kS/s 12.8 mW SA ENGINE

73

Fig. 9. Data-path configuration of the computing resources in the VC and the
SC for computing FS and BS.

Fig. 8. Dynamic configuration of the system architecture in the (a) AS, (b) LS,
and (c) EU tasks.

column index of the maximum component is written into the
active set memory.
In the LS task, a series of matrix–vector multiplications, FS,
divisions, and BS need to be executed (see ops. 4–7 in Table II).
For computing matrix–vector multiplications in ops. 4 and 6a,
the same configuration as in the AS task is used. Differently,
in order to compute FS and BS using recursive vector operations, the core-SRL is enabled to link the adder in SC with the
PEs in the VC into parallel loops. The SRL units in the PEs are
also enabled to support the folded computation of large-size FS
and BS. Fig. 9 illustrates the data-path configuration of computing resources in the VC and the SC for computing the FS
in op. 5 (Table II). Note that performing FS and BS in an iterative fashion has little impact on the system throughput. This

is because 1) FS and BS are intrinsically an iterative process
that has loop-carried DD and 2) the LS task is not the throughput bottleneck in the reformulated OMP as shown in Fig. 2. In
addition, computing FS and BS using vector-based operations
allows for the reutilization of the VC and improves the hardware utilization rate. When the FS in op. 5 (Table II) executes
iteratively using the configuration shown in Fig. 9, the subsequent divisions can be then scheduled to the SC and executed
by the pipelined sequential divider cascaded.
In the EU task, the two computation cores are configured to
update the estimation results separately. The VC accesses AΛt
and d (Λt ) from the external memory and the shared cache,
respectively. Note that the active atoms AΛt are accessed by
using the contents from the active set memory as the read
address. One should also note that the matrix–vector multiplication c (i) = Art−1 in the AS task is executed by computing
the independent inner products as ai rt−1 . Differently, the
matrix–vector multiplication v = AΛt d (Λt ) in the EU task is
computed in a column-wise fashion by configuring the PEs into
an element-wise MAC mode. Each clock cycle, one column of
AΛt and a single element of d (Λt ) are accessed and multiplied,
and the results are accumulated element-wise in the SRL units
in PEs. After t × F cycles, where F is the folding factor, the
result v will be available in the SRLs. Then, the residual rt is
updated by the PEs in parallel as rt = rt−1 + v. Meanwhile,
the SC updates xt element-wise as xt (i) = xt−1 (i) + d (i)
whenever d (i) is read out from the shared cache. The overall dynamic configuration scheme of the SA architecture is
summarized in Table III.
IV. C HIP I MPLEMENTATION
The die photo and chip summary are shown in Fig. 10. The
SA engine chip is implemented in a 40 nm 1P8M CMOS process using a standard-cell-based design flow. The RTL codes are
synthesized in synopsys design compiler (DC). To achieve the
target throughput, a clock period of 60 ns (16.7 MHz) evaluated
at the worst-case process, voltage, and temperature (PVT) corner is targeted throughout the chip implementation. Taking into
account the overhead to be introduced by the subsequent physical design, a 22% timing slack is used during the synthesis.
Specifically, the SA engine is synthesized with a target clock
frequency of 16.7/(1 − 0.22) = 21.4 MHz. To reduce leakage

74

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

TABLE III
S UMMARY OF DYNAMIC C ONFIGURATION S CHEME OF THE S YSTEM A RCHITECTURE

∗ R/W

= read/write.

Fig. 10. Die photo and summary of the SA engine chip.

power, the SA engine is first synthesized using high-threshold
(HVT) standard cells only.1 Then, standard-threshold (SVT)
standard cells are selectively inserted to the critical paths for
timing improvement. This is carried out by switching ON the
leakage optimization tool in DC.
The PE cache, SC cache, and shared cache in the SA engine
have a memory size of 1.2 KB, 1.5 KB, and 768 B, respectively. Note that there are a total of 128 instances of the PE
cache in the design. To reduce area cost, the PE caches are
realized using dual-port SRAM hard macros. Differently, the
SC cache and the shared cache are realized using synthesized
RAMs mainly because they can be flattened during the physical
design to facilitate floorplanning. For voltage scaling purposes,
the SA engine is split into two power domains. The PE caches
realized by SRAM macros are under the memory (high voltage) domain, while the rest of the design is under the logic (low
voltage) domain. Lever shifters are placed along the boundary
of SRAM macros to handle signaling across the two voltage
domains.
1 The HVT option is not available in our memory complier. The transistors used in the memory macros are still SVT. Therefore, the memory leakage
dominates the system leakage power.

The physical design of the SA engine is performed in
Cadence Encounter. To reduce the run time, a bottom-up hierarchical design method is adopted. Specifically, the PE and the
PE cache are first placed and routed separately at the block
level. During the chip-level floorplanning, these two blocks are
treated as hard macros. For the best implementation results, the
rest of the design is flattened during the top-level placement and
routing. Note that the PE macro is routed using M1–M4 only
so that the 128 instances will not block the routing channels on
M5–M8 during the top-level placement and routing.
To facilitate the top-level routing, the PE and PE cache
instances are grouped into 128 pairs, which are then placed into
16 rows. In each row, eight pairs of the PE group are placed
evenly with a 50 µm space in between. To enhance power delivery, a global power grid is routed across both of the voltage
domains over the entire chip. To minimize IR drops, the global
power stripes are routed using the redistribution (RDL) and M8
layers that have smaller resistance and support higher current
density.
Overall, the SA engine chip occupies a core area of 5.13 mm2
with an aspect ratio of 0.99 and integrates 61 M transistors. For
the leakage reduction purpose, HVT devices are used in 99.89%

REN AND MARKOVIĆ: CONFIGURABLE 12–237 kS/s 12.8 mW SA ENGINE

75

Fig. 12. Averaged RSNR performance of the SA engine chip for ExG signal
reconstruction. The ECG, EEG, and EMG signals are reconstructed on the Haar
DWT, DCT, and DWT–DCT joint basis, respectively.

Fig. 11. Testing environment of the SA engine chip.

of the logic cells. The SA engine chip has 42 digital inputs, 58
digital outputs, and 156 power pads supplying three different
power domains. The I/O domain has a constant supply voltage
of 2.5 V. The logic and memory domain both have a nominal
supply voltage of 0.9 V, while each operates up to 1 V and down
to 0.5 and 0.7 V, respectively.
V. C HIP T ESTING
A. Testing Environment
The chip testing environment is illustrated in Fig. 11. A
Kintex-7 KC705 FPGA board is used as the testbed for mapping hardware test benches. A customized printed circuit board
(PCB) is designed to host the SA engine chip for testing. The
SA engine chip is wire-bonded to a 256-pin pin grid array
(PGA) package and then mounted to the host PCB through a
zero insertion force (ZIF) socket. The host PCB is connected
to the KC705 board through two high-speed FPGA Mezzanine
Card (FMC) connectors. A clock generator is used as the external clock source for both the FPGA and the SA engine chip.
The clock is injected to the host PCB through an SMA connector and then passed to the FPGA board through the dedicated
clock pins in the FMC connector. In order to control and monitor the chip testing process on a computer, Xilinx ChipScope
IPs are utilized in the test bench design. Specifically, ChipScope
virtual I/O (VIO) is used as the soft registers both to store the
static control bits of the SA engine chip and the test bench and
to monitor the static outputs indicating the chip status. In addition, ChipScope integrated logic analyzer (ILA) is used as the
probes to capture the dynamics of all the digital I/Os of the SA
engine chip.
B. Testing Results
Several 1 min recordings of real ExG signals downloaded
from the PhysioBank database are used in the signal reconstruction test [18]. Specifically, the original ExG signals are encoded

by random Bernoulli matrices with a 5% overlapping window applied at different signal dimensions and under-sampling
ratios (see Appendix B). Then, the random samples are fed into
the SA engine chip to reconstruct the signal coefficients on a
specific sparsifying basis. The original signal can then be recovered by back projecting the reconstructed sparse coefficients
into time domain. In order to observe the raw signal sparsity,
no thresholding scheme is applied in our test. To measure the
reconstruction accuracy, we use the metric of reconstruction
SNR (RSNR) defined as


 x2
(1)
RSNR = 20 log10
 x − x̂2
where x is the original signal and x̂ is the recovered estimation of x. The averaged RSNR performance measured on
the SA engine chip is shown in Fig. 12. The best orthogonal
basis for reconstructing the chosen ECG, EEG, and EMG signals is found to be Haar discrete wavelet transform (DWT),
discrete cosine transform (DCT), and DWT–DCT joint basis,
respectively. It is also found that the RSNR performance is sensitive to the error tolerance (ε) setting of the chip. Dynamically
configuring ε to 3%–5% of the energy of random samples
results in the best RSNR performance. In general, higher
under-sampling ratio improves the RSNR performance at the
cost of higher data rate for radio transmission. In addition, at
the same under-sampling ratio, using a higher signal dimension in compressive sampling improves RSNR slightly at the
cost of reduced throughput and increased energy consumption.
Therefore, given a target RSNR, there exists an optimal chip
setting for achieving the maximum throughput. For reconstructing the ECG, EMG, and EEG with a target RSNR of > 15 dB,
the preferred chip setting is found to be {n = 256, m ≥ 90},
{n = 128, m ≥ 58}, and {n = 512, m ≥ 205}, respectively.
These settings indicate that an under-sampling ratio (m/n)
of 0.35, 0.45, and 0.4 can be achieved (for > 15 dB RSNR)
on the ECG, EMG, and EEG sensor nodes through compressive sampling, which corresponding to an approximate sensor
energy saving of 2.8×, 2.5×, and 2.2×, respectively, due to
the reduced data size (m/n) for wireless transmission [5].
Example ExG signals reconstructed at the preferred settings are

76

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

Fig. 15. Power breakdown at different supply voltages. At the MEP (Vlogic =
Vmem = 0.7 V), memory and logic leakage power contribute to 64% and 23%
of the total power consumption, respectively.
Fig. 13. Examples of ExG signals reconstructed on the SA engine chip
with a > 15 dB RSNR performance. The ECG, EEG, and EMG signals are
reconstructed on the Haar DWT, DCT, and DWT–DCT joint basis, respectively.

TABLE IV
M EASURED T HROUGHPUT AND E NERGY E FFICIENCY OF THE SA
E NGINE C HIP

∗ Measured

Fig. 14. Measured power and operating frequency at different supply voltages.

at the MEP for ExG signal reconstruction.
highlighted numbers are the best performance at an RSNR of
>15 dB.

illustrated in Fig. 13. Note that the chip is flexible enough to
accommodate various physiological signals and reconstruction
quality requirements through the configurable settings of m, n,
and ε and the support for different reconstruction bases.
The measured power and operating frequency of the SA
engine chip at different supply voltages are shown in Fig. 14.
The memory and logic domain (Vmem and Vlogic ) of the SA
engine chip can operate down to 0.7 and 0.5 V, respectively.
The minimum energy point (MEP) for operation is found at
Vlogic = Vmem = 0.7 V, which is the minimum supply voltage
the SRAM macros can operate at. At the MEP, the chip has
an operating frequency of 12.2 MHz and a power consumption of 12.8 mW. The breakdown of total power consumption
by dynamic power, logic leakage power, and memory leakage
power is shown in Fig. 15. Note that the SA engine chip is
a memory-leakage-limited design. At the MEP, memory and
logic leakage contribute to 64% and 23% of the total power consumption, respectively. As we further scale down Vlogic while
keeping Vmem at 0.7 V (for functionality), the memory leakage
power becomes increasingly dominant. At the minimum supply voltages, 84% of the total power is consumed by memory
leakage. This indicates that lowering Vlogic below 0.7 V will
reduce the operating frequency without making much impact
on the total power consumption, thereby degrading the energy
efficiency. Compared to the MEP, a two-time higher operating

frequency can be achieved at VDD = 1 V with a six-time higher
power consumption.
The measured throughput and energy efficiency of the SA
engine chip when operating at the MEP for ExG signal reconstruction are summarized in Table IV. At the MEP, the chip
achieves a throughput of 237, 123, and 66 kS/s and an energy
efficiency of 54, 104, and 194 nJ/sample for reconstructing
ECG, EMG, and EEG signals at RSNR > 15 dB, respectively.
Such level of performance is sufficient to support the simultaneous reconstruction of 237, 61, and 132 channels of ECG,
EMG, and EEG signals, respectively. Operating at VDD = 1 V,
the chip can achieve a two-time higher throughput at the cost of
a three-time lower energy efficiency.
The SA engine chip is compared to an Intel Core i7-4700MQ
processor and two existing SA solver chips [9], [10] designed
for different applications in Fig. 16. For fair comparison, the
designs that implement fast algorithms (such as FFT) for a dedicated sampling matrix are not considered for our comparison,
since the SA engine chip implements domain transformation
explicitly and supports arbitrary sampling matrices. In addition,
we apply the same problem settings used in the reference design
when making the comparison. While the reference designs targeted a fixed problem setting and a limited dynamic range, our
chip handles flexible problem settings at run time and supports
a large dynamic range. Overall, the SA engine chip achieves

† The

REN AND MARKOVIĆ: CONFIGURABLE 12–237 kS/s 12.8 mW SA ENGINE

77

Fig. 16. Comparison to state-of-the-art.

a two-time higher throughput with up to 14,100 times better energy efficiency for ExG signal reconstruction than the
software solver running on the CPU. For high-sparsity signal
reconstruction nk ≤ 4% , the SA engine chip is 76–350 times
more energy efficient than the referencedesign [9]. For lowsparsity signal reconstruction nk ≥ 16% , the SA engine chip
is less energy efficient than the reference design implementing
the AMP algorithm [10] since the number of iterations required
by AMP is less dependent on the signal sparsity level.

VI. C ONCLUSION
In this paper, we present a 12–237 KS/s 12.8 mW SA engine
chip for enabling the energy-efficient data aggregation of compressively sampled physiological signals on mobile platforms.
Taking an algorithm-architecture codesign approach, we apply
a combination of techniques to optimize the SA engine chip
toward high energy efficiency. First, by applying algorithm
reformulations, we reduce the CC and the data-dependent loops
involved in the LS task by an order of magnitude. This saves
dynamic and leakage energy from eliminated computation and
reduced execution time per functionality, respectively. Second,
we propose a configurable system architecture, in which all
the computing resources are shared across different tasks. This
maximizes the hardware utilization and minimizes the overhead
of LS computation. Third, by introducing the shuffle-mode
memory control scheme, we effectively cut down the memory
usage for handling Cholesky factorization by half and saves
another 40% of the total power from reduced memory leakage.
The SA engine chip integrated in 40 nm CMOS is able to
support the simultaneous reconstruction of over 200 channels
of physiological signals by consuming <1% of a smartphone’s
power budget. In a CS-based health monitoring system, the
SA engine chip can enable a two-to-three-time lower energy
at the sensor nodes through compressive sampling [5], while

providing timely feedback and bringing signal intelligence
closer to the user.
A PPENDIX
A. Notations
The following conventions apply to the notations in this
paper. A matrix is denoted as an upper-case bold letter (e.g.,
A). A vector is denoted as a lower case letter (e.g., a). ai ,
when bolded, represents the ith column vector of matrix A.
ai , when not bolded, represents an arbitrary vector indexed by
i. x (i) represents the ith element of vector x. A set of index
is denoted by an upper case Greek letter (e.g., Λ). AΛ , when
bolded, represents the set of column vectors of A that are
indexed by Λ, and x (Λ) represents the set of elements of x that
are indexed by set Λ.
B. Compressive Sensing
Let α ∈ Rn be a compressible signal that has a sparse representation x ∈ Rn on a certain orthogonal basis Ψ ∈ Rn×n ,
given as
α = Ψx

(2)

where x is a k-sparse vector that contains only k nonzero elements, denoted as x ∈ Snk . Then, the compressive sampling is
performed by applying a linear mapping on α through a random
matrix Θ ∈ Rm×n , expressed as
y = Θα + β

(3)

where β is an additive noise imposed by the sampling process.
According to (2), the linear mapping in (3) is as if encoding
the sparse coefficient x through another random matrix A ∈
Rm×n as
y = Ax + β

(4)

78

IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 51, NO. 1, JANUARY 2016

where A is uniquely defined by A = ΘΨ. CS theorem tells us
that as long as Θ satisfies the null space property (NSP) and the
restricted isometry property (RIP) of order 2k [3], [4], the signal
information x (in the sparse domain) can be well preserved by
the random encoding scheme in (3) or (4). This holds true even
when the sampling matrix Θ is a underdetermined matrix with
m < n (so does A), which represents a dimensionality reduction from Rn to Rm . In this case, the random measurement y is
a compressed representation of the signal’s sparse coefficient x
that is encoded by A. It is proven that Θ randomly generated
from sub-Gaussian distributions, such as random Bernoulli or
random Gaussian matrices, can easily satisfy both the NSP and
the RIP of order 2k given the condition of
n
(5)
m ≥ C · k · log
k
where C is a constant. In the context of this paper, n, m,
k denote the signal dimension, measurement dimension, and
signal sparsity level, respectively. In addition, k/n and m/n
denotes the signal sparsity ratio and the under-sampling ratio
that indicate the data size reduction achievable by conventional
orthogonal transformation-based compression methods and the
compressive sampling method, respectively.
To recover the original signal α, or equivalently its sparse
coefficient x, we need to solve the linear equation in (4). Note
that (4) is an underdetermined system equation with infinite
possible solutions. However, it can be proven that by utilizing the sparsity condition x ∈ Snk as prior knowledge, x can be
robustly estimated by solving the 0 pseudonorm minimization
problem, defined as
min  x0 ,

subject to  y − Ax2 ≤ ε

[7] I. Goncharova, D. J. McFarland, T. M. Vaughan, and J. R. Wolpaw, “EMG
contamination of EEG: Spectral and topographical characteristics,” Clin.
Neurophysiol., vol. 114, no. 9, pp. 1580–1593, 2003.
[8] A. M. Dixon, E. G. Allstot, D. Gangopadhyay, and D. J. Allstot,
“Compressed sensing system considerations for ECG and EMG wireless biosensors,” IEEE Trans. Biomed. Circuits Syst., vol. 6, no. 2,
pp. 156–166, Apr. 2012.
[9] P. Maechler, P. Greisen, B. Sporrer, S. Steiner, N. Felber, and A. Burg,
“Implementation of greedy algorithms for LTE sparse channel estimation,” in Proc. Conf. Record 44th Asilomar Conf. Signals Syst. Comput.
(ASILOMAR), 2010, pp. 400–405.
[10] P. Maechler et al., “VLSI design of approximate message passing for signal restoration and compressive sensing,” IEEE J. Emerging Sel. Topics
Circuits Syst., vol. 2, no. 3, pp. 579–590, Sep. 2012.
[11] D. E. Bellasi, L. Bettini, C. Benkeser, T. Burger, Q. Huang, and C. Studer,
“VLSI design of a monolithic compressive-sensing wideband analogto-information converter,” IEEE J. Emerging Sel. Topics Circuits Syst.,
vol. 3, no. 4, pp. 552–565, Dec. 2013.
[12] J. A. Tropp and A. C. Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53,
no. 12, pp. 4655–4666, Dec. 2007.
[13] D. L. Donoho, Y. Tsaig, I. Drori, and J.-L. Starck, “Sparse solution of
underdetermined systems of linear equations by stagewise orthogonal
matching pursuit,” IEEE Trans. Inf. Theory, vol. 58, no. 2, pp. 1094–1121,
Feb. 2012.
[14] F. Ren, “A scalable VLSI architecture for real-time and energy-efficient
sparse approximation in compressive sensing systems,” Ph.D. dissertation, Dept. Electr. Eng., Univ. California, Los Angeles, CA, USA,
2015.
[15] F. Ren, C. Zhang, L. Liu, W. Xu, V. Owall, and D. Markovic, “A modified
square-root-free matrix decomposition method for efficient least square
computation on embedded systems,” Embedded Syst. Lett., vol. 6, no. 4,
pp. 73–76, 2014.
[16] F. Ren, W. Xu, and D. Markovic, “Scalable and parameterised VLSI
architecture for efficient sparse approximation in FPGAs and SoCs,”
Electron. Lett., vol. 49, no. 23, pp. 1440–1441, 2013.
[17] L. Vandenberghe, “Applied numerical computing,” LA, USA: UCLA
Academic Publishing, 2013.
[18] A. L. Goldberger, et al., “PhysioBank, Physiotoolkit, and Physionet:
Components of a new research resource for complex physiologic signals,”
Circulation, vol. 101, no. 23, pp. e215–e220, 2000.

(6)

where ε is the error tolerance that should be greater than the
noise’s energy level given as  β2 ≤ ε. The formulation in (6),
a.k.a. the SA problem, is the optimization problem of finding
the sparest vector out of the solution space constrained by the
linear mapping in (4). Thanks to the rich research in the field
of CS, the SA problem in (6) can be either solved by heuristic
methods such as OMP [12] and stage-wise OMP (StOMP) [13],
or be relaxed to a 1 -norm minimization problem and solved by
linear programming [3], [4].

Fengbo Ren (S’10–M’15) received the B.Eng.
degree from Zhejiang University, Hangzhou, China,
in 2008, and the M.S. and Ph.D. degrees from the
University of California, Los Angeles, CA, USA,
in 2010 and 2014, respectively, all in electrical
engineering.
In 2015, he joined the Faculty of the School
of Computing, Informatics, and Decision Systems
Engineering, Arizona State University, Tempe, AZ,
USA. His research interests include hardware acceleration and parallel computing solutions for data
analytics and information processing, with emphasis on compressive sensing,
sparse coding, and deep learning frameworks.
Dr. Ren was the receipt of the 2012–2013 Broadcom Fellowship.

R EFERENCES
[1] C. Christopoulos, A. Skodras, and T. Ebrahimi, “The JPEG2000 still
image coding system: An overview,” IEEE Trans. Consum. Electron.,
vol. 46, no. 4, pp. 1103–1127, Nov. 2000.
[2] D. Marpe, T. Wiegand, and G. J. Sullivan, “The H.264/MPEG4 advanced
video coding standard and its applications,” IEEE Commun. Mag.,
vol. 44, no. 8, pp. 134–143, Aug. 2006.
[3] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289–1306, Apr. 2006.
[4] E. J. Candès and M. B. Wakin, “An introduction to compressive sampling,” IEEE Signal Process. Mag., vol. 25, no. 2, pp. 21–30, Mar.
2008.
[5] F. Chen, A. P. Chandrakasan, and V. M. Stojanovic, “Design and analysis
of a hardware-efficient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3,
pp. 744–756, Mar. 2012.
[6] U. Varshney, “Pervasive healthcare and wireless health monitoring,”
Mobile Netw. Appl., vol. 12, no. 2–3, pp. 113–127, 2007.

Dejan Marković (S’96–M’06) received the Ph.D.
degree in electrical engineering from the University
of California, Berkeley, CA, USA, in 2006.
He is a Professor of Department of Electrical
Engineering at the University of California, Los
Angeles (UCLA), Los Angeles, CA, USA. He is also
affiliated with UCLA Bioengineering Department
as a Co-Chair of the Neuroengineering field.
His research interests include implantable neuromodulation systems, domain-specific architectures,
embedded systems, energy harvesting, and design
methodologies.
Dr. Marković was the recipient of an NSF CAREER Award in 2009, the 2014
ISSCC Lewis Winner Award for Outstanding Paper, the 2007 David J. Sakrison
Memorial Prize, and also the corecipient of an ISSCC Jack Raper Award for
Outstanding Technology Directions in 2010.

Analysis of STT-RAM Cell Design with Multiple
MTJs Per Access
Henry Park, Richard Dorrance, Amr Amin, Fengbo Ren, Dejan Markoviü, and C.K. Ken Yang
Department of Electrical Engineering
University of California, Los Angeles
Los Angeles, CA 90095
henrypark@ucla.edu
Abstract— Density of STT-RAMs is limited by the area cost and
width of the access device in a cell since it needs to support the
programming currents. This paper explores a cell structure that
shares each cell’s access transistor with multiple MTJ memory
elements. Feasibility and limitations of such a cell structure is
explored for both reading and writing of the memory. The
analytical and simulation results indicate that only small amount
of sharing is possible and having MTJs that can handle a high
read current without disturbing the cell is needed.

Normolized Cell Area (Norm. to MTJ device size)

4

Keywords-component; STT-RAM, Magnetic Tunnel Junction,
1T-3MTJ

I.

INTRODUCTION

Aggressive integration of on-chip memory arrays raises the
need for high-density memories. Advances in spin-torque
transfer RAM (STT-RAM) enable storing data in a magnetic
tunneling junction (MTJ) comprising a thin insulator and
ferromagnetic sheets. The technology has shown promise as
dense arrays of MTJs have been fabricated with very small cell
sizes [1, 2]. However, as shown in Fig. 1, due to the required
junction current density for switching, the access transistor
must be sized to support such currents. For technology nodes
down to 32nm, the actual cell size is determined by the access
transistor. This paper analyzes the effectiveness and challenges
in building an STT-RAM memory cell that shares multiple
MTJs in a cell with a single access transistor.
II.

10

Jc0=2 MA/cm2
Jc0=4 MA/cm2
Jc0=6 MA/cm2
Jc0=8 MA/cm2

3

10

2

10

1

10

0

10

32-nm

45-nm

65-nm

90-nm

Technology node

Fig. 1. Required normalized NFET cell area at different technology nodes
(RA = 4.9 ȍ·μm2, TMR = 120%, MTJ size = 70×120nm2

remains stable during a read operation (we assume minimum
write current at 99.99% switching probability while maximum
read current is at 0.01%). Such reliable read operation is traded
with scalability of the memory cell due to minimum NFET size
(Fig. 1). This paper explores possibility of sharing multiple
MTJs with a single access device to reduce its impact on the
cell size.

BACKGROUND
III.

Similar to most memories, an STT-RAM typically employs
a structure with one MTJ switched/accessed by a single access
transistor (1T-1MTJ). The organization of a conventional STTRAM is shown in Fig. 2(a). The access transistor is sized [3] to
meet the minimum write current requirement for the cell to
switch either in the parallel to anti-parallel, min(IW(APĺP)), or
vice versa, min(IW(PĺAP)). Directions of each switching
current are depicted in Figure 3. Switching from AP to P is
equivalently moving operating point from A to B in Fig. 3(a)
while switching in reverse direction (P to AP) is from C to D in
Fig. 3(b). In both cases current density can be improved by
boosting voltages above the nominal supply level [4]. Since
each cell has a dedicated access device, a cell’s value is not
perturbed by the writing of other cells. As long as the
maximum read current, max(IR(AP)) or max(IR(P)), provides
sufficient margin from the minimum write currents, the cell

Structure of multiple MTJs per cell is shown in Fig. 2(b).
The figure shows a simple case of two MTJs per single access
NFET, but theoretically more MTJ devices could share a single
NFET. While improving array density, this structure introduces
additional complexity and limitations in both reading and
writing. In general, when one cell is accessed, current can flow
between the bit-lines through resistive paths formed by nonaccessed cells. Fig. 4 shows such an example. This parasitic
current results in both degraded read and unstable non-accessed
cells during writes. It may also limit the number of cells in a
column hence leading to a finer sub-array partitioning of the
memory. Similar analysis can be found from papers of crosspoint (CP) cell MRAM (Magnetic Random Access Memory),
where the authors removed the access device to minimize the

Sponsored by the DARPA/MTO STT-RAM program

c
978-1-4577-0995-1/11/$26.00 2011
IEEE

DESIGN CONSIDERATIONS

53

SL

BL

SL

BL1 BL2

Row[0]

Row[0]

Row[1]

Row[1]
...

...

Row [n]

Row [n]

WL[n]

WL[n]

VAC1

VAC
MTJ
...

...

Row[N]

Row[N]

EN

EN

Source-Line
Driver

Bit-Line
Driver

EN1

EN

EN2

Source-Line
Driver

Bit-Line
Driver

Fig. 2. (a) Standard 1T-1MTJ Structure (left). (b) Suggested 1T-2MTJ
Structure with separated bit lines (right).

Write ‘0’ (RP)
(same direction
as read access)

Current
NFET

B

VWL

A
GP

GAP

VBL

C

I

MTJ
VSL

VAC

GAP

VSS
D

VAC
Fig. 3. Load line of NFET and MTJ device (G denotes conductance) in case
of (a) reading or writing RP (above) and (b) writing RAP (below).

cell area smaller than 6F2 but they also revealed limitations on
the array size and MTJ characteristics [5~7].
A. Impact of MTJ Sharing on Reads
Fig. 4 can be used to illustrate when a single MTJ in a cell
is being read. The total amount of read current is the sum of the
main MTJ current (IMTJ through MTJ1 in the Fig. 4) and the
parasitic current through all other MTJs attached to its bit-line
(BL1). If the number of MTJ devices per cell is NMTJ and the
number of cells per column is NC, then the equivalent parasitic
resistance between BL1 and VAC1 are expressed as [7]


R PAR

 N MTJ
 N MTJ

 N C  1
R MTJ
 1   N C  1

APAR  R MTJ  

RMTJ is a discrete random variable bounded by RP and RAP.
In comparison to a 1T-1MTJ cell, the parasitic resistive paths
reduce the resistance difference when reading either RP or RAP
of MTJ1. The parasitic paths effectively reduce the TMR seen

54

1

20

0

0

-20

-20

-40
-60

-40
-60

20

0

1

1.5

2.52.
5

20

10

1

32

2
2

VWL

1.5

4
3.5

40

20

VWL

P o AP
W

40

4.5

1.5

NFET

1

GP

Effective TMR (%)
2

5
1.5

Current
Write ‘1’ (RAP)

APAR

VAC

1.5

2

3
4
Number of MTJs per Cell

15
5

-20

VAC

oP
IAP
W

APAR

VBL

Number of Cells per Column

MTJ
VSS

Fig. 4. Illustration of parasitic current disturbance. IMTJ is the wanted current
through the selected device.

0.5
50

100
150
TMR (%)

-40
-60

200

Fig. 5. (a) Load line of NFET and MTJs during read access (upper). (b) APAR
as function of the number of MTJs per cell and the number of cells per
column (lower left). (c) Effective TMR as a function of APAR and TMR
(lower right).

by the read sense amplifier and an effective TMR can be
expressed as

TMReff

R AP & APAR  R P  R P & APAR  R AP
R P & APAR  R AP



A
TMR
1
 PAR 

A

PAR



 1  TMR   1

Fig. 5(a) illustrates the impact of the parasitic resistance on
current load lines from an MTJ resistance loading the nonlinear
switching characteristics of an NFET. RP and RAP result in
different load lines. Since RAP has a higher resistance, the
operating point during a read (point A) shows a lower current
and lower voltage across the access device (VAC). A region of
uncertainty can be overlaid onto the load lines. When the
regions overlap due to RPAR, incorrect sensing can occur. The
overlap condition occurs when min(IR(P)) < max(IR(AP))
which corresponds to APAR < 1. Fig. 5(b) shows that APAR
becomes smaller as number of MTJs per cell or number of cells

2011 IEEE/ACM International Symposium on Nanoscale Architectures

Current
NFET
GP

VMTJ

A

IPWo AP

B

GPAR
+
GP

GPAR
+
GP

GPAR
+
GAP

GPAR GP
+
C
GAP GAP

Current

D

VMTJ
NFET
oP
IAP
W

GAP

GPAR

IPRo AP

IPRo AP

GPAR

VAC

Fig. 6. Resistor network. Access transistors and bit-line drivers control every
node voltage to properly control leakage current. However, this concept is
practically unrealizable due to circuit complexity.

RP to RAP

RAP to RP
1

VSS

VSS
IMTJ

VWL

IMTJ

VWL

VBL1

R1
R2

VAC

IPAR_MTJ

0.9

VAC

R3

R3

R4

R4

A column of cells with multi-MTJ cells can be ideally
modeled as a network of resistors (MTJs) as shown in Fig. 6.
Each node can have an optimal voltage set such that the write
current, IMTJ, through the accessed MTJ is maximized and the
IPAR_MTJ through any of the non-accessed MTJs is minimized.
Since IPAR_MTJ must be smaller than max IR(AP) or IR(P) to
avoid any probability of false switching, the ratio of the
IPAR_MTJ/IMTJ determines the minimum ratio of max(IR)/min(IW)
required for the MTJ in order for a multi-MTJ cell to be
possible. Actual implementation deviates from this lower
bound depending on the ability to accurately set the individual
node voltages. Section III addresses these cases. Analysis made
here provides a lower bound.

IW(APo P)/IW(Po AP) = 0.4
IW(APo P)/IW(Po AP) = 0.5
IW(APo P)/IW(Po AP) = 0.6

0.7

IW(APo P)/IW(Po AP) = 0.6

0.6

IR P 

0.5

Fig. 7. Equivalent simplified model of memory array for (a) RP to RAP and
(b) RAP to RP write access. Node-to-node impedance is max; therefore the
leakage current is minimum. This optimistic case study sets the lower
boundary of NFET size.

B. Impact of MTJ Sharing on Writes
Designing a cell with multiple MTJs for programming has
two primary issues. First, writing a cell can result in an
unintentional write of a non-accessed MTJ due to the parasitic
current. Second, the parasitic paths can reduce the current
through the accessed MTJ hence requiring a larger access
transistor to provide the current.

IW(APo P)/IW(Po AP) = 0.4

0.8

IPAR_MTJ

per column increases. Fig. 5(c) plots equation (2) to relate APAR
and the MTJ’s TMR with the TMReff. Intuitively, APAR>1 can
be maintained for small values of #cells per column and #MTJs
per cell. Section III describes these limits in greater detail. For
TMR greater than 100%, a safe margin for successful read
access can be achieved when the sub-array size (i.e. total
number of MTJs per column) is kept below 9.

IR  AP 
IW  AP o P 

IW(APo P)/IW(Po AP) = 0.5

R1
R2
IREAD/IWRITE

VSL1

VAC

Fig. 8. (a) Switching current of ‘1’ (RP to RAP) and Leakage current. At
operating point A, the current through the selected MTJ should be greater
than the minimum write current while the leakage current through any nonaccessed device should be smaller than the maximum read current. (b)
Programming current of ‘0’ (RAP to RP) and Leakage current.

IW P o AP 

0.4

0

50

100
TMR (%)

150

200

Fig. 9. Required ratio of minimum read current and maximum write current for
AP to P and P to AP switching. As switching current is more asymmetric
(0.6ĺ 0.4), the required current ratio of AP to P switching (dotted lines) is
higher. The reverse relation can be seen from P to AP switching (solid lines).

Determining the minimum ratio for Fig. 6 is equivalent to a
simplified model of 2 cells per column and 2 MTJs per cell as
shown in Fig. 7. The worst-case conditions for writing PĺAP
and APĺP are shown. Writing a “1” (PĺAP) can be
visualized graphically in Fig. 8(a) when the IMTJ at point A >
min(IW(PĺAP)).
TABLE I.

THE WORST-CASE LEAKAGE DURING WRITE ACCESS
Switching from RP to RAP

R1

R2

R3

R4

RP

RP

RP

RP

RP

RP

RP

RAP

Requirement

1 P o AP
P
 I W R _ M IN  I READ
_ MAX
3
1
 I P o AP  I AP
 3  TMR  W R _ M IN READ _ MAX

Switching from RAP to RP
R1

R2

R3

R4

RAP

RP

RP

RP

RAP

RP

RAP

RP

Requirement

1  TMR   I AP o P

3
1  TM R 

 3  TMR 

2011 IEEE/ACM International Symposium on Nanoscale Architectures

WR _ MIN

P
 I READ
_ MAX

P
AP
 I WAPR o
_ M IN  I READ _ MAX

55

With this IMTJ through the accessed MTJ, the worst-case
parasitic current can be defined as listed in Table I where R2
and R3 in the first case and R4 in the second case have a
chance to be flipped if IPAR_MTJ > max(IR). When resistances
through the parasitic path are equal, the ratio of
max(IR)/min(IW) is roughly 1/3. As shown in the Table I, the
value can depend TMR since the programmed value of the
non-accessed MTJ can vary between RP or RAP. Fig. 8(b) and
Table I show the worst case and ratio for writing a “0”
(APĺP). Fig. 9 visualizes requirements listed in Table I. As
IW becomes highly asymmetric, the minimum ratio of read and
write current should be higher. This plot indicates that the
worst-case condition is generally dominated by maximum read
current of RAP.
The current through the parasitic resistance paths determine
the additional amount of current that must be sourced by the
access transistor. The lower bound of roughly 1/3 indicates that
the access device must be increased by at least 1/3 of
min(IW(PĺAP)) for each additional MTJ/cell.

Fig. 10. NFET Area (normalized to 1T-1MTJ device size) as a function of
number of MTJs per cell. Number of cells per column is fixed at 2. Higher
tolerable read current lowers sensitivity of design space.

Depending on the ability to control the voltages at each
node, a higher ratio may be required which implies that the
given MTJ has very narrow state transition region over control
current [8]. The results in the next section assume the least
complex implementation where nodes VAC2 in Fig. 6 are not
driven to a specific voltage when the cell is not accessed.
IV.

SIMULATION RESULTS

Feasibility and performace of using multi-MTJ cells that
share a single access transistor depends on MTJ characteristics.
Our analysis optimizes the area of the cell in a multi-MTJ
design using an MTJ with characteristics as indicated in Table
II. Values in the table are based on measured results of recent
publications [8, 9] with adjustable read/write current ratio. The
cell area is minimized based on reducing the required access
device size for writing. The minimum device size is determined
by adjusting bit-line voltages to meet the design targets given
in Table II. Maximum parasitic current of non-accessed MTJ is
carefully tested under various worst case conditions to remain
within the MTJ specification. For this simulation, a 45nm
CMOS technology is used with HSPICE.
TABLE II.

SIMULATED MTJ SPECS

Fig. 11. NFET Area (normalized to 1T-1MTJ device size) as a function of
number of MTJs per cell and number of cells in the sub-array. The number of
cells per column has relatively small impact on the NFET size as long as bitline drivers have controls on the leakage current. The max(read)/min(write)
current ratio used in this plot is 0.7.

MTJ specifications

56

Terms

Value

RA

4.9 ȍ·μm2

RP

750 ȍ

TMR

120 (%)

Switching Current
(IW(PĺAP))
Switching Current
(IW(APĺP))
Max(IR(P)
/min(IW(PĺAP))
Max(IR(AP)
/min(IW(APĺP))

500 μA
375 μA
0.55
0.6~0.8

Our analysis explores the impact of the shared design on
cell area and effective TMR for various numbers of MTJs per
cell and numbers of cells per column. The simulation results of
the shared structure are normalized by the 1T-1MTJ cell area.
A. Normalized NFET Size
Figure 10 shows normalized device area of the access
transistor as a function of # MTJ per cell with different ratios
of max(IR(AP))/min(IW(APĺP)). With a large number of
MTJs per cell, the parasitic path has low impedance leading to
higher parasitic currents. Hence, the bit-lines associated with
the non-accessed cells should be driven as strongly as possible
to avoid sizing up the access NFET unnecessarily. An inherent
tradeoff is present with the max(IR(AP))/min(IW(APĺP) ratio

2011 IEEE/ACM International Symposium on Nanoscale Architectures

however, the performance worsens considerably where the
parasitic current cannot be properly controlled. Equivalently,
if the non-accessed bit-lines are floating, the amount of NFET
current is roughly proportional to the number of MTJs per cell
and is very sensitive to the number of cells per column.

Infeasible
B. Simulated Differential Read Current

Fig. 12. Differential Read Current as a function of array structure. The
max(read)/min(write) current ratio used in this plot is 0.7.

As indicated by the effective TMR in Equation (2),
parasitic resistors desensitize resistance variation of target
MTJ. Fig. 12 shows that the differential read current is always
less than 60% of 1T-1MTJ case. In this simulation, the access
device size has been scaled as in Fig. 11. The results indicate
that the differential current drops substantially with increasing
number of MTJs per cell and hence realistically only 2 or 3
MTJs per cell is functionally feasible. With larger numbers of
cells per column, we anticipate the differential read current to
be <20% of the 1T-1MTJ case putting more demand on the
sensing circuitry or longer time for sensing.
C. Area Saving
Assuming that access transistor size is a dominant factor of
memory density, cell area reduction rate per MTJ is shown in
Fig. 13. Note that this ratio is normalized to cell area of 1T1MTJ case. This figure clearly indicates that higher memory
density can be attained though the cost comes from reduced
sensing margin and increased risk of data loss. Table III
summarizes simulation results.
TABLE III.

Fig. 13. Cell area reduction per MTJ compared to 1T-1MTJ device size. The
max(read)/min(write) current ratio used in this plot is 0.7.

and the amount of parasitic current. As shown in the figure, if
the node at the access device of a non-accessed cell is not
driven, the ratio must increase to be greater than 0.5 for the
multi-MTJ structure to be feasible. The slope of each curve
increases with larger numbers of MTJs/cell because the nonaccessed nodes (nodes VAC2 in Fig. 6) are not simultaneously
driven. The larger number of parasitic paths in parallel with
increasing number of MTJs/cell results in greater parasitic
current through the non-accessed devices. Hence, the area
efficiency degrades.
Fig. 11 sweeps the NFET size as a function of the number
of cells per column (i.e. sub-array size). The sub-array size has
little impact on the NFET size as long as net parasitic
resistance is greater than the source impedance of bit-line
driver. Once bit-line driver loses control over idle bit-lines,

SIMULATION RESULTS

Terms

value

Number of MTJs per Cell

3

Number of Cells

3

Switching Current (PĺAP)

500 μA

Switching Current (APĺP)

375 μA

Max Read Current/ Min
Switching Current
Differential Read Current
(Normalized)
Cell area per MTJ
(Normalized)

70%

V.

15%
56%

CONCLUSION

This paper describes the impact of using multiple MTJs that
shares a single access device in a cell. Because of the resistive
nature of the MTJs, device sharing has substantial limitations
due to the parasitic resistive paths that degrade both the read
differential current and write stability. The technique promises
modest improvements in the array density but can only be
applied to small number of MTJs per cell and limited column
size. Furthermore, MTJ characteristics must have a high
max(IR)/min(IW) ratio in order to avoid instability of nonaccessed MTJs.

2011 IEEE/ACM International Symposium on Nanoscale Architectures

57

ACKNOWLEDGMENT
This work was supported by the DARPA/MTO STT-RAM
program. The authors would like to thank UCLA STT-RAM
team for helpful discussions and for providing measurement
data.

[4]
[5]

[6]

REFERENCES
[1]
[2]

[3]

58

C. Chappert, et al., "The Emergence of Spin Electronics in Data
Storage", Nature Materials, June 2007, pp. 813-823.
M. Hosomi, et al., "A Novel Nonvolatile Memory with Spin Torque
Transfer Magnetization Switching: Spin-RAM", IEEE IEDM, Dec.
2005, pp. 459-462.
A. Raychowdhury, et al., “Design Space and Scalability Exploration of
1T-1STT MTJ Memory Arrays in the Presence of Variability and

[7]
[8]

[9]

Disturbances,” in Electron Devices Meeting (IEDM), 2009 IEEE
International, December 2009, pp. 1-4.
J. DeBrosse, et al., "1 16 Mb RAM featuring bootstrapped write
drivers", Symp. VLSI Technol., p. 454, 2004.
F. Z. Wang, “Diode-free magnetic random access memobyr using spindependent tunneling effect,” Appl. Phys. Lett., vol. 77, no. 13, pp. 2036–
2038, 2000.
N. Sakimura, et al., “A 512Kb Cross-Point Cell MRAM,” in IEEE
ISSCC Dig. Tech. Papers, 2003, pp. 278–279.
C.J. Amsinck, et al., “Scaling constraints in nanoelectronic randomaccess memories,” Nanotechnol., vol. 16, no. 10, pp. 2251–2260, 2005.
P.K. Amiri, et al., "Low Write-Energy Magnetic Tunnel Junctions for
High-Speed Spin-Transfer-Torque RAM", IEEE Electron Device
Letters, January 2011, Vol. 32, Issue 1, pp. 57-59
Z. M. Zeng, et al., "Effect of resistance-area product on spin-transfer
switching in MgO-based magnetic tunnel junction memory cells", Appl.
Phys. Lett. 98, 072512, 2011

2011 IEEE/ACM International Symposium on Nanoscale Architectures

A Binary Convolutional Encoder-decoder Network
for Real-time Natural Scene Text Processing

arXiv:1612.03630v1 [cs.CV] 12 Dec 2016

Zichuan Liu1 zliu016@e.ntu.edu.sg
Fengbo Ren2 renfengbo@asu.edu
1

Yixing Li2 yixingli@asu.edu
Hao Yu1 haoyu@ntu.edu.sg

Nanyang Technological University, 2 Arizona State University

Abstract
In this paper, we develop a binary convolutional encoder-decoder network (B-CEDNet)
for natural scene text processing (NSTP). It converts a text image to a class-distinguished
salience map that reveals the categorical, spatial and morphological information of characters.
The existing solutions are either memory consuming or run-time consuming that cannot
be applied to real-time applications on resource-constrained devices such as advanced
driver assistance systems. The developed network can process multiple regions containing
characters by one-off forward operation, and is trained to have binary weights and binary
feature maps, which lead to both remarkable inference run-time speedup and memory usage
reduction. By training with over 200, 000 synthesis scene text images (size of 32 × 128), it
can achieve 90% and 91% pixel-wise accuracy on ICDAR-03 and ICDAR-13 datasets. It
only consumes 4.59 ms inference run-time realized on GPU with a small network size of
2.14 MB, which is up to 8× faster and 96% smaller than it full-precision version.

1

Introduction

The success of convolutional neuron network (CNN) has resulted in a potential general machine
learning engine for various computer vision applications LeCun et al. [1998], Krizhevsky et al. [2012]
as well as for the natural scene text processing (NSTP), such as text detection, recognition and
interpretation from images. Applications, such as Advanced Driver Assistance System (ADAS) for
road signs with text, however require a real-time processing capability that is beyond the existing
approaches Jaderberg et al. [2014a] from processing functionality, efficiency and latency.
A general NSTP system is based on a two-phase end-to-end pipeline that firstly segments the text
region from the original image and then performs recognition of the cropped image. In recognition
phase, the cropped image is transformed into text sequence that will be further processed by the
Natural Language Processing (NLP) module Chowdhury [2003]. There are two categories of methods
that can be applied, character-level method Wang et al. [2012], Bissacco et al. [2013] and word-level
method Jaderberg et al. [2014a,b]. The character-level method performs an individual character
detection and recognition. It relies on a multi-scale sliding window strategy combined with a
convolutional neural network to localize characters, resulting in long processing latency in detection
Wang et al. [2012]. On the other hand, the word-level method Jaderberg et al. [2014a] has shorter
inference time but large memory consumption due to the huge size of the fully-connected layer.
For a real-time NSTP application targeted for ADAS, one needs a method with memory efficiency,
fast processing time as well as low power. Recent advancement in binary-constrained deep learning
Courbariaux and Bengio [2016] has opened up new opportunities for highly efficient CNN realization
for the real-time NSTP. In this paper, we propose a binary convolutional encoder-decoder network
(B-CEDNet) that can perform a real-time one-shot text interpretation. Instead of detecting characters
sequentially Bissacco et al. [2013], Wang et al. [2012], Shi et al. [2015], our proposed network detects
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

multiple characters simultaneously. It can distinguish different classes of characters and recovers
their spatial and morphologic information by one forward pass. The output is a set of salience maps
with the same size as the input image, indicating pixel-wise posterior probability distribution over a
category space that composes 26-character and a background class, which allows paralleled character
prediction with significant speedup. More importantly, different from traditional CNN engine, by
applying binary constraints in training, it results in a B-CEDNet with massive computing parallelism
with binarized weights and activations. The experiment shows that our proposed network achieves up
to 91% pixel-wise accuracy on public dataset Lucas et al. [2003], Karatzas et al. [2013]. Furthermore,
we observe an impressive 4.59 ms average forward time for processing a 32 × 128 grayscale scene
text image with a small network size of 2.14 MB, which has up to 8× faster run-time and 96%
smaller memory usage than its full-precision version.

2

B-CEDNet architecture

The conventional NSTP architecture Bissacco et al. [2013] ignores the spatial information of features,
which leads to a slow sequential character detection. In contrast, the proposed B-CEDNet shown in
Figure 1 makes use of deconvolution techniques Badrinarayanan et al. [2015] to recover this spatial
information, allowing paralleled character processing. Specifically, it takes a 32 × 128 grayscale
image as input and converts it to a compact high-level feature, which is further decoded into a set of
salience maps that indicate the categorical, spatial and morphologic information of the characters.
The B-CEDNet consists of three main modules, adapter module, binary encoder module and binary
decoder module. The adapter module (block-0) contains a full-precision convolutional layer, followed
by a batch-normalization (BN) layer and binarization (Binrz) layer. It transforms the input data
into binary format before feeding the data into binary encoder module. The binary encoder module
consists of 4 blocks (block-1 to -4), each of which has one binary convolutional (BinConv) layer,
one batch-normalization (BN) layer, one pooling layer and one binarization (Binrz) layer. The
BinConv layer takes binary feature maps abk−1 ∈ {0, 1}Wk−1 ×Hk−1 ×Dk−1 as input and performs
binary convolution operation which is illustrated as follows:
sk (x, y, z) =

wk X
hk X
dk
X

XN OR(wkb (i, j, l, z), abk−1 (i + x − 1, j + y − 1, l)),

(1)

i=1 j=1 l=1

where XN OR(·) is defined as bit-wise XNOR operation, wkb ∈ {0, 1}wk ×hk ×dk are the binary
weights in k-th block and sk ∈ RWk ×Hk ×Dk is the output of the spatial convolution. Then sk
is normalized by the BN layer before pooling and binarization. The output of k-th BN layer
ak ∈ RWk ×Hk ×Dk is represented by
sk (x, y, z) − µ(x, y, z)
p
ak (x, y, z) =
γ(x, y, z) + β(x, y, z),
(2)
σ 2 (x, y, z) + 
where µ and σ 2 are the expectation and variance over the mini-batch, while γ and β are learnable
parametersIoffe and Szegedy [2015]. The output of the BN layer is subsequently down-sampled by
the pooling layer. Here we apply 2 × 2 max-pooling to filter out the strongest response which will be
binarized by the Binrz layer. The binarized activations abk of k-th block can be represented as

0, ak (x, y, z) ≤ 0
abk (x, y, z) =
(3)
1, ak (x, y, z) > 0
What is more for the decoder module, it translates the compact high-level representation ab5 ∈
{0, 1}2×8×512 generated by the encoder into a set of salience maps p ∈ R32×128×27 that indicates the
spatial probability distribution over category space including 26 characters and a background class.
The decoder module is composed of 6 convolutional blocks (block-5 to -10). Block-5 to -8 are formed
by one un-pooling layer, one BinConv layer, one BN layer and one Binrz layer. Note that there
exists a symmetric structure along block-1 to -8. Thus the un-pooling layers Badrinarayanan et al.
[2015] within block-5 to -8 simply assign the input pixels back to their original position according to
the index generated by the corresponding max-pooling layer and pad the remains with zeros. The
up-sampled feature maps then go through the binary convolution, normalization and binarization.
The output of block-8 is a 32 × 128 × 512 tensor which will be processed by block-9 and -10 to
generate spatial salience map. Block-9 and -10 form a 2-D spatial classifier with 1 × 1 convolution
window and softmax output. It produces the posterior probability distribution over the category space
for each pixel in the original image.
2

Full Precision Feature Maps

32x128x27
Salience Map

Binary Feature Maps

32x128x100

32x128x1
Input Image

Block-0

32x128x512

16x64x512
8x32x512
Block-1

16x64x512
8x32x512
Block-8
4x16x512
4x16x512
2x8x512
Block-7

Block-2

Conv

BN

Binrz

BinConv BN

Adaptor

32x128x512

Block-9

Block-10

Block-3 Block-4 Block-5 Block-6

Pooling Binrz Unpooling BinConv

Binary Encoder

BN

Binrz Softmax

Binary Decoder

Figure 1: (a) Conventional architecture of NSTP system; (b) Proposed B-CEDNet NSTP system.

3

Training

The B-CEDNet can be trained and optimized under binary constraints proposed in Courbariaux and
Bengio [2016], which can significantly reduce memory usage and also improve parallelism. In the
existing binary CNN method Courbariaux and Bengio [2016], hinge loss function is used for training,
which is however unsuitable for our application because it fails to provide a probability interpretation
from the input image. In this paper, we apply cross-entropy error as the loss function by removing
the Binrz layer in block-10. For our application, the prediction error J is represented as follows:
J(w) = −

1
N · W10 · H10

H10 X
C
N W
10 X
X
X

ea10 (m,n,c)
[1{Y (i) (x, y) = c} ln PC
],
a10 (m,n,l)
l=1 e
i=1 m=1 n=1 c=1

(4)

where N is the number of training sample in a mini-batch, C is the number of classes, w is the filter
weights, Y (i) ∈ {1, ..., C}H10 ×W10 is the 2-D label of i-th training image, a10 ∈ RH10 ×W10 ×C 1 is
the output of the BN layer in block-10.
To achieve generality of trained model, it usually needs a large amount of labeled data for training.
However, the existing datasets are limited to word-level annotation Veit et al. [2016] or cannot provide
enough pixel-wise labeled data Karatzas et al. [2013]. Inspired by Jaderberg et al. [2014a], we create
a text rendering engine that generates texts with different fonts, colors and projective distortions. The
labeled image has the same size with the corresponding text image and provides a pixel-wise labeling
over the category space. Additionally, our model is trained by AdaMax optimizer Kingma and Ba
[2014] with initial learning rate of 0.002, learning rate decay of 0.9 and mini-batch size of 20.

4

Experiment results and discussion

4.1

Experiment setup

The model is built based on MatConvNetVedaldi and Lenc [2015] on Dell Precision T7500 server
with Intel Xeon 5600 processor, 64GB memory and TITAN X GPU. To evaluate the performance,
we train the model on synthesis scene text dataset with 200, 000 images and test it on the standard
datasets ICDAR-03 and ICDAR-13 Lucas et al. [2003], Karatzas et al. [2013].
4.2

Pixel-wise accuracy

Table 1 reports the pixel-wise accuracy on the testing datasets mentioned above. We achieve notable
90% and 91% pixel-accuracy on ICDAR-03 and ICDAR-13. Figure 2 (a) shows the ideal cases,
where our binary model is capable to precisely recognize each character and recover their outline
information under normal contrast and even illumination condition. While, in the non-ideal cases,
the salience maps produce low confidence value in the area with uneven illumination or low contrast
as shown in Figure 2 (b). It indicates that the model feels confused (unconfident) when interpreting
the characters. It is worth noting that the robustness can be improved by extending the training set
to include the missed cases. Therefore we believe B-CEDNet will be promising in fast scene text
detection and natural text interpretation.
1

In the proposed network C, H10 , W10 are 27, 32 and 128, respectively.

3

Table 1: Pixel-wise accuracy our proposed network on ICDAR-03 and ICDAR-13.
Binary
Non-binary

ICDAR-03
0.90
0.90

ICDAR-13
0.91
0.91

Image
Sailence map
Prediction

(a)
Image
Sailence map
Prediction

(b)

Figure 2: (a) Testing samples with high prediction accuracy rate; (b) Testing samples with low
prediction accuracy rate.
4.3

Run-time and memory usage

Inference Time (ms)

Figure 3 compares the inference time for B-CEDNet running on baseline kernel and XNOR kernel
Courbariaux and Bengio [2016]. Baseline kernel is an optimized matrix multiplication kernel, while
the XNOR kernel is tailored for bit-count operation in binary network. We measure the inference
time with a batch of input images (size of 16) to obtain higher utilization of GPU. Due to the
bit-count operation and huge memory access reduction, the B-CEDNet achieves an average of 4.59
ms inference time and 8× speedup with XNOR kernel on TITAN X GPU compared with baseline
kernel. Since XNOR kernel introduces run-time overhead concatenating 32 binary values into a
32-bit register, the speedup is more remarkable when the convolutional operation becomes more
intensive. Accordingly, the most computation-intensive block, block-8 has the highest speedup of
17.7×. On the other hand, the memory usage is reduced by over 96% (2.14 MB) compared with the
full-precision version (66.12 MB).
25
20

Non-binary
Binary

15
10

Network Size (MB)
66.12
2.14

Total Inference Time (ms)
36.32
4.59

8ｘ

5

7.5ｘ
4.2ｘ

0

17.7ｘ

4.2ｘ
1.7ｘ

0.7ｘ

3.8ｘ

1.7ｘ

1ｘ

block-1 block-2 block-3 block-4 block-5 block-6 block-7 block-8 block-9 block-10

Binary

Non-binary

Speed-up

Figure 3: Inference run-time and memory consumption of B-CEDNet and its full-precision version.

5

Conclusion

In this paper, we have developed a binary convolutional encoder-decoder network (B-CEDNet) for
real-time NSTP applications. The B-CEDNet can effectively capture the categorical, spatial and
morphologic information from the text image and is extremely computationally efficient with binary
weights and activations trained from binary constraints. By training with over 200, 000 synthesis
scene text image, it can achieve up to 91% pixel-wise accuracy. It only consumes 4.59 ms inference
time realized on GPU with a small network size of 2.14 MB, which is up to 8× faster and 96%
smaller than its non-binary version.

4

References
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoderdecoder architecture for image segmentation. arXiv preprint arXiv:1511.00561, 2015.
Alessandro Bissacco, Mark Cummins, Yuval Netzer, and Hartmut Neven. Photoocr: Reading text
in uncontrolled conditions. In Proceedings of the IEEE International Conference on Computer
Vision, pages 785–792, 2013.
Gobinda G Chowdhury. Natural language processing. Annual review of information science and
technology, 37(1):51–89, 2003.
Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights
and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and
artificial neural networks for natural scene text recognition. arXiv preprint arXiv:1406.2227,
2014a.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for text spotting. In European
conference on computer vision, pages 512–528. Springer, 2014b.
Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda,
Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere
de las Heras. Icdar 2013 robust reading competition. In 2013 12th International Conference on
Document Analysis and Recognition, pages 1484–1493. IEEE, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105,
2012.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Simon M Lucas, Alex Panaretos, Luis Sosa, Anthony Tang, Shirley Wong, and Robert Young. Icdar
2003 robust reading competitions. In ICDAR, volume 2003, page 682. Citeseer, 2003.
Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based
sequence recognition and its application to scene text recognition. arXiv preprint arXiv:1507.05717,
2015.
A. Vedaldi and K. Lenc. Matconvnet – convolutional neural networks for matlab. In Proceeding of
the ACM Int. Conf. on Multimedia, 2015.
Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text:
Dataset and benchmark for text detection and recognition in natural images. In arXiv preprint
arXiv:1601.07140, 2016. URL http://vision.cornell.edu/se3/wp-content/uploads/
2016/01/1601.07140v1.pdf.
Tao Wang, David J Wu, Adam Coates, and Andrew Y Ng. End-to-end text recognition with
convolutional neural networks. In Pattern Recognition (ICPR), 2012 21st International Conference
on, pages 3304–3308. IEEE, 2012.

5

Scalability and Design-Space Analysis of a
1T-1MTJ Memory Cell
Richard Dorrance, Fengbo Ren, Yuta Toriyama, Amr Amin, C.-K. Ken Yang, Dejan Marković
Department of Electrical Engineering, University of California, Los Angeles, CA, USA
Abstract—This paper introduces a design-space feasibility
region as a function of MTJ characteristics and memory target
speciﬁcations. The sensitivity of the design space is analyzed for
scaling of both MTJ and underlying transistor technology. Design
points for improved yield, density, and memory performance
can be extracted for 90nm down to 32nm processes based
on measured MTJ devices. To achieve ﬂash-like densities in
upcoming 22nm and 16nm technology nodes, scaling of the
critical switching current density is required.
Index Terms—STT-RAM, Design Space, Magnetic Tunnel
Junction, Variability

120

2.5

TMR [%]

RAP [kȍ]

110

2

X
Y
Z

1.5

1
0.5

1

100
90
X

1.5

80

4

4.5

I. I NTRODUCTION

II. M ODELING MTJ VARIABILITY AND S CALING
This section describes the MTJ model and characteristics
that is used in the subsequent sections to explore the design
space for several scaled CMOS technologies.
A. MTJ Device Variability
While statistical variation of CMOS is generally well understood, similar characteristics for MTJs have not been well
documented. This paper uses a combination of fundamental
equations and measured device characteristics to model the
statistical behavior of MTJs. Fig. 1(a) contains a plot of measured RP vs. RAP for 105 MTJ nanopillars of varying size and
target resistance-area (RA) products. Variations in resistance
and tunnel magnetoresistance (TMR) are due to a combination
of lithographic variations in the physical dimensions of the
nanopillar and minute variations in the thicknesses of the up
to 20 different layers in state-of-the-art MTJ processes [3].
The cumulative effects of these variations on RA and TMR
can be calculated [4], as shown in Fig. 1(b) and Table I.

c
978-1-4577-0995-1/11/$26.00 2011
IEEE

5

Z
5.5

6

RA [ȍÂȝm2]

RP [kȍ]

Spin-Torque Transfer RAM (STT-RAM) is a candidate
for next-generation memory, with Magnetic Tunnel Junctions
(MTJs) as the non-volatile storage element. However, questions remain unanswered for such a memory to succeed. The
design of the MTJ depends considerably on the underlying
transistor technology since a given CMOS technology constrains the design space due to the overhead and impact of the
access transistor in each memory cell. Previous work, such
as [1], has not addressed this dependency or provided the
necessary framework with measured CMOS and MTJ characteristics. The feasibility and yield of the memory depend on
the design space and the variation of the MTJs [2]. This paper
presents such a design space as a function of the variation for
technologies down to 32nm to show the scalability of STTRAM.

Y

(a) RAP vs. RP (measured).

(b) TMR vs. RA (calculated).

Fig. 1: Measured (a) RAP vs. RP and (b) TMR vs. RA for MTJ
nanopillars measuring 150 × 45nm2 (X), 130 × 50nm2 (Y), and
170 × 45nm2 (Z) at room temperature (300K).
TABLE I: Measured Device Statistics

T M R [%]
σT M R [%]
RA [Ω · μm2 ]
σRA [Ω · μm2 ]

X

Y

Z

105.7
4.7
4.88
0.342

107.3
2.7
5.51
0.297

105.3
4.6
5.22
0.311

B. Scaling of MTJ Current and Resistance
The resistance and switching current can be modeled using
a precessional-based switching model, modiﬁed to include
thermally-activated switching [5]. The switching current of an
MTJ, for a constant pulse of duration τ , is given by:


ln (τ /τ0 )
,
(1)
IC = IC0 1 −
Δ
where Δ is the thermal stability of the MTJ, τ0 is the natural
time constant, and IC0 is the critical switching current. This
critical switching current [6] is given by:
IC0 =

α4πe 2
MS V,
ηh̄

(2)

where α is the Gilbert damping constant, η is the factor of
spin polarization, h̄ is the reduced Planck’s constant, e is
the elemental charge of an electron, MS is the magnetization
saturation of the free layer, and V is the volume of the free
layer.
For an MTJ with free layer dimensions l > w >> d
[7], as shown in Fig. 2, the thermal stability of an MTJ is

32

3

RAP,MAX (Write)

d

C
2.5
5ı

l

2

BL
IC(APĺ3)

IC(Pĺ$3)

WL

RAP

3ı

1.5

B

1
N
MI
P,

0.5

SL

(a) IC (AP → P ).

(b) IC (P → AP ).

A
0
0

Fig. 3: 1T-1MTJ cell architecture showing MTJ switching current for
(a) AP to P and (b) P to AP.

Δ=

E
H K MS
=
V ≈d
kB T
2kB T



1
1
−
w
l



MS2
V,
kB T

0.5

III. D ESIGN S PACE
The analysis in this paper uses a conventional 1T-1MTJ
cell architecture as shown in Fig. 3. The writing currents
for ﬂipping the cell resistance are deﬁned as IC (P → AP )
and IC (AP → P ). The design space of a single STT-RAM
memory cell can be illustrated using an RAP vs. RP plot
as is shown in Fig. 4. The feasibility region is indicated by
the shaded region. It contains all points (RP , RAP ) in the
design space so that a memory cell made with such an MTJ
is functional. In the design space, the two lower bounds are
set by the read margin of the cell, while the two upper bounds
are set by the write margin of the cell.
The lower bound RP,M IN is dependent on the implementation of the sense ampliﬁer, and represents the minimum
resistance required for reliable circuit operation. Additionally,
RAP,M IN is determined by T M RM IN (Fig. 5), the minimum
TMR required for the read ampliﬁer to differentiate between
RP and RAP . Regardless of the speciﬁcs of the implementation, all sense ampliﬁers are either a voltage- or current-

1.5

Fig. 4: Design space, in a 65nm process, for WN = 2.0μm, IC (P →
AP ) = 500μA, IC (AP → P ) = 375μA, with an overlay of device
X from Table I.
300

(3)

where kB is Boltzmann’s constant, T is the absolute temperature in Kelvin, HK is the out-of-plane uniaxial anisotropy,
and E is the energy of anisotropy [8], [9].
In this paper, dimensional scaling is performed to maintain
a constant Δ in order to ensure the long-term non-volatility
of the STT-RAM. Therefore, dimensions l and w of the MTJ
are scaled by a factor λ to manipulate IC0 and RP/AP , then
to keep Δ constant, d must scale by λ−1/2 . This results in
IC0 ∝ lwd → λ3/2 and RP/AP ∝ l−1 w−1 → λ−2 .

1

RP [kȍ]

250

TMRMIN [%]

SL

approximately:

)
ad

RA

RP

WL

e
(R

RP,MAX (Write)

BL

RAP [kȍ]

Fig. 2: MTJ free layer dimensions.

RP,MIN (Read)

w

200
150
100
50
0

0

0.2

0.4

0.6

'Iref/Iref
Fig. 5: Design space lower bound T M RM IN vs. ΔIref /Iref for a
current-sensing read circuit with ideal reference resistance 2(RP 
RAP ).

sensing topology. For a generic current-sensing read circuit,
T M RM IN can be expressed as:
T M RM IN =

2ΔIref /Iref
.
1 − ΔIref /Iref

(4)

For Iref ﬂowing through the reference resistance Rref ,
Iref + ΔIref,1 ﬂows through RP and Iref − ΔIref,2 through
RAP . When ΔIref,1 = ΔIref,2 = ΔIref , T M RM IN is
minimized. Under this condition, Rref = 2(RP  RAP ) and
we can express T M RM IN as a function of the normalized
fractional sensing current (ΔIref /Iref ). In Eq. 4, ΔIref must
be chosen so that the read ampliﬁer correctly evaluates across

2011 IEEE/ACM International Symposium on Nanoscale Architectures

33

8

200
200

2

0

0.5

0

4

800

1

600

40

1.5

WN [Pm]
(a) RP,M AX vs. WN .

0

600

800

1000

2

2

400

2

400

400

0

Increasing
Variability

200

0

0

0.5

1

1000

1.5

2

WN [Pm]
(b) RAP,M AX vs. WN .

Fig. 6: (a) RP,M AX and (b) RAP,M AX at nominal VDD for a 65nm
process (IC contours are in μA).

RAP [kȍ]

100

4

2.5

6
20

100

6

3
100

8

RAP,MAX [kȍ]

10

RP,MAX [kȍ]

10

5ı
3ı

1.5

1
SS
TT

0.5

all transistor PVT variations. Similarly, T M RM IN can be
derived for voltage-sensing topologies. It should be noted that
the lower bounds RP,M IN and RAP,M IN , while critical to the
readability of the cell, are almost completely independent of
the MTJs used. The only requirement is that sensing time and
current (Iref ) be small enough so as not to disturb the cell
during the read operation.
The upper bounds, RP,M AX and RAP,M AX , are the maximum allowable resistances such that the access transistor, in a
1T-1MTJ conﬁguration, is still able to provide the minimum
critical writing currents IC (P → AP ) and IC (AP → P ).
These upper bounds are subsequently very sensitive to the
speciﬁc characteristics of the MTJ device used. Transistorlevel simulations are used to determine the relationship between RM AX , IC , and cell size (transistor width WN ) for a
technology. Fig. 6 shows an example of such a simulation in
a 65nm process. Using the conventional conﬁguration from
Fig. 3, WN is swept along with RM AX . The contours of the
simulated current are shown.
Fig. 4 shows a speciﬁc MTJ cell and its associated statistical
variation (the concentric ovals around point B) overlaid on
the design space. The design-space margin can be deﬁned as
the number of σ’s of MTJ variation before crossing any of
the previously deﬁned bounds. Deﬁning design-space margin
(DSM) in terms of σ simpliﬁes feasibility characterization to a
single variable and thus allows yield to be quickly calculated.
To a ﬁrst order, 3σ, 4σ, 5σ, and 6σ of design margin roughly
correspond to being able to reliably produce 1kbit, 32kbit,
4Mbit, and 1Gbit memory arrays.
Fig. 7 highlights the effects of CMOS variability on the
design space bounds. To illustrate the effects more clearly, a
35F 2 cell in a 65nm process is chosen. As expected, the more
stringent constraints of the SS corner causes the design space
to shrink. This shift is caused by an increase in the threshold
voltage of the access transisor. Enviormental variables, like
temperature, also have a signiﬁcant effect in the design space.
A consumer grade STT-RAM is expected to operate over a
range of more than 100°C in which TMR can drop by more
than 30% [10], degrading the DSM for readability. These
sources of technological and environmental variability must

34

FF

0
0

0.5

1

1.5

RP [kȍ]
Fig. 7: Design space, in a 65nm process, for WN = 750nm,
IC (P → AP ) = 300μA, IC (AP → P ) = 300μA, with an overlay
of device X from Table I, for SS, TT, and FF corners.

also be considered in the design process.
IV. S ENSITIVITY A NALYSIS AND D ESIGN E XAMPLE
Many variables, at both the circuit and device levels, affect
the design space. In order to optimize a variable for a target
memory speciﬁcation, we must determine how such a variable
impacts the design space. This section introduces a designspace sensitivity (DSS) as a metric to quantify the behavior
of the change in design space as a function of various design
parameters (VDD , λ, JC , RA, TMR, WN , etc.).
A. Design-Space Sensitivity Analysis
First consider the points A, B, and C in Fig. 4. Points A
and C correspond to the corner values of RP and RAP in the
feasible design space. Point B represents the nominal MTJ at
the center of the MTJ device distribution. For a positive design
margin to exist, point B must fall somewhere between points
A and C.
A “better” design space can be achieved from altering a
design parameter, when a larger distribution of the MTJs
(the number of σ) falls within the feasible region. Note that
the improved design space is not simply increasing the area
of the feasibility region, since the motion of point B must
be considered as well. Recall that point A depends only
slightly on the MTJ parameters. Therefore, the improvement
(or deterioration) of the design space depends mostly on the
change in DSM between points B and C as a function of a
particular design variable.
Therefore, we deﬁne the design-space sensitivity to the

2011 IEEE/ACM International Symposium on Nanoscale Architectures

8

8

VDD = 1.0V
Ȝ= 1.0
DSM = 0ı

6

6

RAP [k:]

RAP [k:]

5

VDD = 1.4V
Ȝ= 1.0
DSM = 3ı

7

4

5

7
6

RAP [k:]

7

8

4

5
4

3

3

3

2

2

2

1

1

1

0
0

0.5

1

1.5

2

2.5

3

0
0

0.5

1

RP [k:]

1.5

2

2.5

3

VDD = 1.4V
Ȝ= 0.7
DSM = 5ı

0
0

0.5

1

RP [k:]

1.5

2

2.5

3

RP [k:]

(a) VDD = 1.0V and λ = 1.0. 0σ design margin. (b) VDD = 1.4V and λ = 1.0. 3σ design margin. (c) VDD = 1.4V and λ = 0.7. 5σ design margin.

Fig. 8: Design space, in a 65nm process, for a 30F 2 cell (WN = 0.65μm) for device X from Table I: IC (P → AP ) = 450μA,
IC (AP → P ) = 300μA. Inner red oval represents 3σ of MTJ device variation. Dashed, black oval corresponds to 5σ of MTJ variation.
120

20
0

VDD = 1.4V

40 RAP
20
0

-20

RP

-20

1

1.1

1.2

1.3

1.4

-40

0.8

VDD [V]

1

1.2

O

(a) Sensitivity to VDD vs. VDD .

(b) Sensitivity to λ vs. λ.

Design-Space Margin [V@

DSS (O)

DSS (VDD)

60

RP

JC Constant

9

JC Scaled

~250

8
2

120F

7

~240
~230

6
2

60F

5

~222

4

~215

3

~210
30F2

2

0
32

~25
~23

1

Fig. 9: Design-space sensitivity of parameters (a) VDD and (b) λ in
a 65nm technology.

~263

Approximate Memory Size [bits]

80

60

-40
0.9

VDD = 1.0V

100

RAP

80

40

10

120

100

45

65

90

Technology [nm]

Fig. 10: Design margin vs. technology node. For constant Jc (P →
AP ) = 6 × 106 A/cm2 and Jc (P → AP ) scaling by 4.5% each
technology generation.

parameter X as:
B
∂( RC −R
)P/AP
σ
DSS(X) =
,
∂X

B. Design Example
(5)

where RB and RC are taken as either RP or RAP at points
B and C, thus deﬁning the DSS along each dimension of
B
is the normalized distance between
the design space. RC −R
σ
points B and C in the design space along the RP/AP dimension. Intuitively, the DSS(X) describes the instantaneous rate
of change in DSM to a particular design parameter X. The
derivative loses positional information, and so we used the
DSS in conjunction with the original plot of the design space
to determine the beneﬁt of tuning the design parameter X.
For both the RP and RAP dimensions, if DSS(X) > 0, then
the DSM is improved by increasing X, and if DSS(X) < 0,
then DSM is improved by decreasing X. When the designspace sensitivities for the two dimensions conﬂict, the size of
the design space in each dimension should then be taken into
account.

In this section we use the sensitivity analysis to design
a 4Mbit STT-MRAM with a 30F 2 cell size (comparable to
eDRAM) in a 65nm technology. Device X from Table I with
IC (P → AP ) = 450μA and IC (AP → P ) = 300μA is the
nominal MTJ and can be scaled by λ. Also, approximately 5σ
of design margin is required for reasonable yield.
Fig. 8(a) shows the design space for a nominal VDD = 1.0V
and λ = 1.0. The inner red oval is the 3σ variation of the MTJ,
while the dashed, black oval represents the 5σ variation of the
MTJ. Clearly, with nominal VDD and λ, the memory is not
functional. Fig. 9 shows that the design space is much more
sensitive to VDD than it is to λ. Therefore, we choose to scale
VDD to 1.4V . Fig. 8(b) shows the new design space, with the
3σ bound at the edge of the design boundary.
Scaling VDD alone proves insufﬁcient to meet the 5σ design
margin required, and so we simultaneously scale λ. Fig. 9(b)
shows that scaling λ results in conﬂicting DSS. The RAP
margin improves more by scaling λ up, while the RP margin

2011 IEEE/ACM International Symposium on Nanoscale Architectures

35

TABLE II: JC (P → AP ) for an RA of 5 Ω · μm2
Equivalent
Cell Size
FLASH (6F 2 )
eDRAM (30F 2 )
SRAM (120F 2 )

JC vs. Technology Node

(106

90nm

65nm

45nm

32nm

2.72
4.60
6.67

2.62
4.58
6.76

2.65
4.57
6.86

2.20
4.31
7.07

A/cm2 )
22nm‡

16nm‡

2.04
1.86
4.23
4.12
7.17
7.30
‡ Predicted

TABLE III: JC (P → AP ) for an RA of 10 Ω · μm2
Equivalent
Cell Size
FLASH (6F 2 )
eDRAM (30F 2 )
SRAM (120F 2 )

JC vs. Technology Node (106 A/cm2 )
90nm

65nm

45nm

32nm

2.22
3.27
4.27

2.14
3.28
4.33

2.19
3.33
4.44

1.82
3.20
4.59

22nm‡

16nm‡

1.69
1.56
3.18
3.16
4.68
4.78
‡ Predicted

TABLE IV: JC (P → AP ) for an RA of 15 Ω · μm2
Equivalent
Cell Size
FLASH (6F 2 )
eDRAM (30F 2 )
SRAM (120F 2 )

JC vs. Technology Node (106 A/cm2 )
90nm

65nm

45nm

32nm

1.83
2.56
3.17

1.81
2.59
3.21

1.82
2.63
3.33

1.67
2.58
3.47

22nm‡

VI. C ONCLUSION
In this paper we have shown that the joint optimization
of multiple design parameters is essential in the design of
a STT-RAM memory array. We have derived the necessary
framework to allow for such a systematic design procedure.
Additionally, the analytical methodology presented in this
work has been utilized to show that mild scaling of MTJ JC
is required to enable ﬂash-like memory densities in upcoming
CMOS technologies. Such densities, coupled with low write
energies and its non-volatility, make STT-RAM a possible
contender for next-generation memories.
ACKNOWLEDGMENTS

16nm‡

1.62
1.56
2.59
2.61
3.55
3.65
‡ Predicted

improves by scaling λ down. However, Fig. 8(b) indicates that
RAP has considerable margin and we can trade off some of
that margin for improved margin in RP . Therefore, we choose
to scale λ down to 0.7. As we can see in Fig. 8(c), the desired
5σ bound on MTJ variation is essentially enclosed within the
design space.
V. F UTURE S CALABILITY
Scalability is an important feature for the success of a memory technology. Fig. 10 shows how scaling of the transistor
technology impacts the design margin when using an MTJ
that has a current density (JC ) of 6 × 106 A/cm2 for 10ns
P to AP switching and RA of 5Ω · μm2 . The ﬁgure shows
that SRAM equivalent sizes (120F 2 ) scale well, with a design
margin more than sufﬁcient to construct gigabit memories
(> 6σ). However, as we decrease cell size, the design margin
begins to degrade below 45nm. The design margin practically
disappears once we reach an eDRAM-equivalent cell size
(30F 2 ). However, if the MTJ current density scales with
technology, also shown in Fig. 10, then this trend is reversed.
By scaling JC by as little as 4.5%, a constant design margin
can be achieved between each technology node below 45nm.
Tables II, III, and IV contain critical switching current densities for ﬂash-, eDRAM-, and SRAM-equivalent cell sizes for
RAs of 5, 10, and 15Ω · μm2 , respectivly. All table values correspond to 5σ of design margin and sub-10ns switching times.

36

It should be noted that while larger RAs require smaller current
densities (in order to meet voltage headroom constraints), they
scale much better between successive technology nodes. We
can also see that ﬂash-like cell sizes (6F 2 ) requires current
densities below 3×106 A/cm2 in current technologies and less
than 2 × 106 A/cm2 in upcoming 22nm and 16nm technology
nodes. Aggressive scaling of MTJ currents will be required to
achieve ﬂash-like densities in future technologies.

This work was supported by the DARPA STT-RAM program. The authors would also like to thank Pedram Khalili,
Henry Park, and Henry Chen of UCLA and Ilya Krivorotov
of UCI for their contributions.
R EFERENCES
[1] A. Raychowdhury, D. Somasekhar, T. Karnik, and V. De, “Design Space
and Scalability Exploration of 1T-1STT MTJ Memory Arrays in the
Presence of Variability and Disturbances,” in Electron Devices Meeting
(IEDM), 2009 IEEE International, December 2009, pp. 1–4.
[2] M. Motoyoshi et al., “A study for 0.18 μm high-density MRAM,” in
Symposium on VLSI Technology, 2004. Digest of Technical Papers., June
2004, pp. 22–23.
[3] S. R. Min et al., “Etch Characteristics of Magnetic Tunnel Junction Stack
with Nanometer-Sized Patterns for Magnetic Random Access Memory,”
Thin Solid Films, Proceedings of the International Symposium on Dry
Process, 2006. (DPS 2006)., vol. 516, no. 11, pp. 3507–3511, November
2008.
[4] R. Beach et al., “A Statistical Study of Magnetic Tunnel Junctions for
High-Density Spin Torque Transfer-MRAM (STT-MRAM),” in Electron
Devices Meeting, 2008. IEDM 2008. IEEE International, December
2008, pp. 1–4.
[5] T. Moriyama et al., “Tunnel Magnetoresistance and Spin Torque Switching in MgO-based Magnetic Tunnel Junctions with a Co/Ni Multilayer
Electrode,” Applied Physics Letters, vol. 97, no. 7, p. 072513, 2010.
[6] J. Z. Sun, “Spin-Current Interaction with a Monodomain Magnetic Body:
A Model Study,” Phys. Rev. B, vol. 62, no. 1, pp. 570–578, July 2000.
[7] H. Chang and J. Burns, “Demagnetizing and Stray Fields of Elliptical
Films,” Journal of Applied Physics, vol. 37, no. 8, pp. 3240–3245, July
1966.
[8] V. Korenivski and R. Leuschner, “Thermally Activated Switching in
Nanoscale Magnetic Tunnel Junctions,” IEEE Transactions on Magnetics, vol. 46, no. 6, pp. 2101–2103, June 2010.
[9] J. Sun and D. Ralph, “Magnetoresistance and Spin-Transfer Torque
in Magnetic Tunnel Junctions,” Journal of Magnetism and Magnetic
Materials, vol. 320, no. 7, pp. 1227–1237, 2008.
[10] K. Lee and S. Kang, “Design Consideration of Magnetic Tunnel
Junctions for Reliable High-Temperature Operation of STT-MRAM,”
Magnetics, IEEE Transactions on, vol. 46, no. 6, pp. 1537–1540, June
2010.

2011 IEEE/ACM International Symposium on Nanoscale Architectures

INTEGRATION, the VLSI journal 47 (2014) 487–498

Contents lists available at ScienceDirect

INTEGRATION, the VLSI journal
journal homepage: www.elsevier.com/locate/vlsi

Statistical timing and power analysis of VLSI considering
non-linear dependence
Lerong Cheng a, Wenyao Xu b,n, Fengbo Ren a, Fang Gong a, Puneet Gupta a, Lei He a
a
b

Electrical Engineering Department, University of California, Los Angeles, CA 90095, United States
Computer Science and Engineering, State University of New York at Buffalo, NY 14260, United States

art ic l e i nf o

a b s t r a c t

Article history:
Received 15 January 2012
Received in revised form
29 December 2013
Accepted 29 December 2013
Available online 4 February 2014

Majority of practical multivariate statistical analysis and optimizations model interdependence among
random variables in terms of the linear correlation. Though linear correlation is simple to use and
evaluate, in several cases non-linear dependence between random variables may be too strong to ignore.
In this paper, we propose polynomial correlation coefﬁcients as simple measure of multi-variable nonlinear dependence and show that the need for modeling non-linear dependence strongly depends on the
end function that is to be evaluated from the random variables. Then, we calculate the errors in
estimation resulting from assuming independence of components generated by linear de-correlation
techniques, such as PCA and ICA. The experimental results show that the error predicted by our method
is within 1% error compared to the real simulation of statistical timing and leakage analysis. In order to
deal with non-linear dependence, we further develop a target-function-driven component analysis
algorithm (FCA) to minimize the error caused by ignoring high order dependence. We apply FCA to
statistical leakage power analysis and SRAM cell noise margin variation analysis. Experimental results
show that the proposed FCA method is more accurate compared to the traditional PCA or ICA.
Published by Elsevier B.V.

Keywords:
Statistical modeling
VLSI
Yield analysis

1. Introduction
With the CMOS technology scaling down to the nanometer
regime, process as well as operating variations have become a
major limiting factor for integrated circuit design. These variations
introduce signiﬁcant uncertainty for both circuit performance and
leakage power. Statistical analysis and optimization, therefore, has
generated lot of interest in the VLSI design community.
Existing work has studied statistical analysis and optimization
for timing [1–5], power [6–9], and spatial correction extraction [10].
Most of these papers assume independence between random
variables when performing statistical analysis. In order to obtain
independence, most existing works use linear transformations, such
as principal component analysis (PCA) or independent component
analysis (ICA), to de-correlate the data. However, when there is nonlinear dependence between the random variables under consideration, both PCA and ICA cannot completely remove the dependence
between random variables. PCA can only remove linear correlation1
between random variables but cannot remove the high order
n

Corresponding author.
E-mail address: wenyaoxu@buffalo.edu (W. Xu).
1
Two random variables X1 and X2 are uncorrelated if and only if
E½X 1  X 2  ¼ E½X 1   E½X 2 . The linear correlation measures how likely one random
variable may increase when the other one increases.
http://dx.doi.org/10.1016/j.vlsi.2013.12.004
0167-9260 Published by Elsevier B.V.

dependence. On the other hand, ICA tries to minimize the mutual
information between the random variables.2 However being a linear
operation, ICA often cannot completely remove the dependence
between random variables.
In practice, the dependence between different variation sources is
rarely linear (e.g., Vth is exponentially related to Leff). Therefore,
ignoring such non-linear dependencies can cause signiﬁcant error
in analyses. There are some existing techniques for handling arbitrary
dependence, such as Copula [11] and total correlation [12]. However,
the complexity of using Copula is exponentially related to the
number of random variables. Mutual information [12] and total
correlation [12] measure the dependence between random variables,
however, it is not easy to apply them in the statistical analysis.
Moreover, there is little work in removing dependence using such
measures as is readily done using PCA for linear correlation.
There exists some nonlinear algorithms to decompose nonlinear
dependent variation sources to independent components, such as
nonlinear PCA [13] (or Kernel PCA) and nonlinear ICA [14]. Applying
such algorithms may completely (or almost completely) remove
2
The mutual information between two random variables X1 and X2, IðX 1 ; X 2 Þ, are
deﬁned as IðX 1 ; X 2 Þ ¼ ∬ 11 f 12 ðx1 ; x2 Þ  log ðf 12 ðx1 ; x2 Þ=f 1 ðx1 Þ  f 2 ðx2 ÞÞ dx1 dx2 , where
f 1 ðx1 Þ and f 2 ðx2 Þ are the marginal probability density function (PDF) of X1 and X2,
respectively and f 12 ðx1 ; x2 Þ are the joint PDF of X1 and X2. IðX 1 ; X 2 Þ measures the
dependence between X1 and X2, IðX 1 ; X 2 Þ ¼ 0 if and only if X1 and X2 are independent.

488

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

dependence between variation sources and results in independent
components. However, such algorithms either express the variation
sources as a very complicate function of independent components
or do not give close form expressions to express variation source
using independent components. Hence, such nonlinear transformations are not easy to use in statistical analysis and optimization.
Compared to the previous work [15], we analyzed the impact of
nonlinear dependence on statistical analysis and evaluated the
performance of the algorithm with more experiments in this
paper. In sum, key contributions of this work are as follows:

 We propose polynomial correlation coefﬁcients as a simple
measure of non-linear dependence among random variables.

 We show that the importance of modeling non-linear depen-





dence strongly depends on what is to be done with the random
variables, i.e., the end function of random variables that is to be
estimated.
We develop closed form expressions to calculate error in the
estimation of arbitrary moments (e.g., mean, variance, skewness) of the to-be estimated function as a result of assuming
true independence of components generated by PCA or ICA
techniques.
We develop a target function driven component analysis
algorithm (we refer to as FCA) which minimizes the error
caused by ignoring non-linear dependence without increasing
the computational complexity of statistical analysis.

The methods developed in this paper can be used to check
whether linear de-correlation techniques like PCA will sufﬁce for
particular analysis problem. To the best of our knowledge, this is
the ﬁrst work to propose a systematic method to evaluate the need
for complex non-linear dependence modeling for statistical analysis in VLSI design or otherwise. We apply our error estimation
formula to the typical examples from computer aided VLSI design:
statistical timing and leakage analysis. Experimental result shows
that our estimation is within 1% error of simulation. Further we
give two example applications of FCA algorithm: statistical leakage
analysis and SRAM cell noise margin variation analysis. The
experimental results show that the FCA is more accurate than
regular PCA or ICA.
The rest of the paper is organized as follows: Section 3
theoretically calculates the impact of high order correlation,
Section 4 applies the formula to statistical timing and leakage
analysis and presents some experimental results, ﬁnally Section 5
presents the target function driven ICA algorithm to minimize the
error caused by ignoring non-linear dependence and Section 6
concludes this paper.

between X1 and X2 should be considered. Theoretically, the mean of f
should be E½f  ¼ 9. However, if we ignore the dependence between X1
and X2 and assume that they are independent, then we would
calculate the mean of f as E½f  ¼ 5. From the above example, we can
see that ignoring high order dependence can cause large error even
when computing the mean. Moreover, notice that in the above
example, we know that variation source is a function of independent
random variables S1 and S2 (i.e., we know the mixing function).
However, in many real applications [16–19], this assumption does
not hold true, which makes the problem of higher order dependence
difﬁcult to handle. ICA tries to minimize the mutual information
between the random variables.
When IðX 1 ; X 2 Þ exists, X1 and X2 are independent if and only if
IðX 1 ; X 2 Þ ¼ 0. Since it is still a linear operation, it cannot completely
remove the dependence between random variables. Let us observe
another simple example: let S1 and S2 be two independent random
variables with standard normal distribution and X 1 ¼ S1 þ S2 ,
X 2 ¼ S1 S2 . Then there will be no linear operations to decompose
X1 and X2 to independent random variables.

3. Analysis of impact of nonlinear dependence
As discussed above, commonly used PCA and ICA techniques
cannot provide fully independent random variable decomposition.
In this section, we are going to study the impact of non-linear
dependence on statistical analysis. We deﬁne the ijth order
polynomial correlation coefﬁcient between two random variables
X1 and X2 as
E½X i X j   E½X i E½X j 

1 2
1
2
ﬃ:
ρij ¼ qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ

E½ðX i1  E½X i1 Þ2   E½ðX j2  E½X j2 Þ2 

ð1Þ

ρij's provide us with simple and good measures to estimate the
impact of nonlinear dependence. Note that 1 r ρij r 1 and that
ρ11 is simply the linear correlation coefﬁcient. In rest of this paper,
we assume that the ρij's are known. In practice, ρij can be
computed from the samples of variation sources.
With the above deﬁnition, we will show how to evaluate the
impact of non-linear dependence on statistical analysis. Let us
consider the two random variable case ﬁrst. Let f be a polynomial
function (or Taylor expansion of an arbitrary function) of two
random variables X ¼ ðX 1 ; X 2 ÞT :
f ðXÞ ¼ ∑aij X i1 X j2 :

ð2Þ

ij

Then
E½f ðXÞ ¼ ∑aij mij ;

ð3Þ

ij

2. Motivation and preliminaries
In this section, we show the limitations of using PCA and ICA to
obtain independent random variables and propose the polynomial
correlation measure.
PCA can only remove linear correlation between random variables
but cannot remove the high order dependence. Independent random
variables must be uncorrelated, but uncorrelated random variables are
not necessarily independent. If we assume that the uncorrelated
random variables are independent (as is done by most VLSI statistical
analysis techniques), errors in the statistical calculations can be
signiﬁcantly large. Consider the following simple example. Let S1 and
S2 be two independent random variables with standard normal distributions. Let X 1 ¼ S1 þ S2 , X 2 ¼ S1 S2 . It is easy to ﬁnd that X1 and X2 are
uncorrelated, but certainly not independent. Let f ðX 1 ; X 2 Þ ¼ X 21 þ X 1
X 2 þ X 21 X 22 þ X 22 . We can see that in order to compute the mean of f ðÞ,
not only the linear correlation but also the 4th order joint moments

where mij ¼ E½X i1  X j2  is the ijth joint moment of X1 and X2. If we
ignore mij, then the error of mean estimation will be
ai;j ðmi;j  mi;0 m0;j Þ. That is, the importance of the ijth joint moment
depends on the coefﬁcient of the ijth joint moment in the Taylor
expansion, ai;j and mi;j  mi;0 m0;j . We deﬁne
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð4Þ
Q ij ¼ ai;j  m2i;0  m0;2j :
Then the mean can be expressed as
E½f ðX 1 ; X 2 Þ ¼ ∑ρi;j  Q i;j ;

ð5Þ

ij

where ρi;j is the ijth order polynomial correlation coefﬁcient
between X1 and X2 as deﬁned in (1). From the above equation, we
ﬁnd that the importance of the ijth order dependence depends on
Q i;j . The above equations illustrate the two random variable case.
In practice, principal component analysis (PCA) or independent
component analysis (ICA) is used to obtain principal components

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

or independent components, respectively. Assume that
T

P ¼ ðP 1 ; P 2 Þ ¼ W  X

ð6Þ

are the principal components (or independent components)
obtained from PCA [20], where W is the transform matrix. Then
the function f can be written as the function of P1 and P2:
f ðXÞ ¼ f ðW  1  PÞ ¼ ∑cij P i1 P j2 :

ð7Þ

ij

Besides mean and variance, skewness is also an important
characteristic of statistical distributions. In order to estimate the
error of skewness calculation, we ﬁrst estimate the error of the
third order raw moment Δ3 in a similar way to

Δ3 ¼ E½f 3   E½f 03  ¼
γ

T ij ¼ uij 

Because P is a linear combination of X, it is easy to obtain the
coefﬁcients cij, from aij and the transform matrix W.
In practice, when high order dependence exists, P1 and P2 are
not completely independent. In this section, we try to estimate the
error caused by ignoring the high order dependence. We focus on
mean, variance, and skewness calculation.
We express mean of f as

489

∑

i Z 1;j Z 1

ρPij  T γij ;

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m2i;0  m0;2j ;

ð16Þ

ð17Þ

where the coefﬁcients uij can be calculated from cij. Then the error
of skewness can be calculated as

Δγ ¼

03

E½f  þ Δ3
ðs

0þ

Δs Þ


3

03

E½f 

s03



Δ3
:
s03

ð18Þ

μ

E½f ðXÞ ¼ f ðW  1  PÞ ¼ ∑ρp;i;j  T i;j
ij

4. Case study of statistical leakage and timing analysis

¼ ∑cij P i1 P j2
ij

Statistical analysis is widely used in integrated circuit design. In
the section, we apply our error estimation techniques on the
statistical timing and leakage power analysis.

μ

¼ ∑ρp;i;j  T i;j ;
ij

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
T i;j ¼ ci;j  mP2i;0  mP0;2j ;
μ

ð8Þ

where mPij is the ijth joint moment of P1 and P2, and ρPij is the ijth
order correlation coefﬁcient between P1 and P2. Since P is a linear
combination of X, it is easy to obtain joint moments mPij and
correlation coefﬁcients ρPij can be easily calculated from the
moments of Xi's mij and the transform matrix W. If we assume
that these components are independent, i.e., we assume all the ρPij
to be zero, then total error in mean estimation is

Δμ ¼

∑

i Z 1;j Z 1

ρPij  T μi;j :

ð9Þ

Similar to the estimation of the error in mean, we may estimate
the error in variance calculation. We ﬁrst estimate the error of
2
second order raw moment of f ðÞ. f ðÞ can be expressed as a
polynomial function of Pi's as
2

f ðP 1 ; P 2 Þ ¼ ∑dij P i1 P j2 ;

ð10Þ

ij

where the coefﬁcients dij can be calculated from cij's. Then we may
estimate the error of the second order raw moment of f ðÞ
02

Δ2 ¼ E½f   E½f  ¼
2

T si;j ¼ di;j 

∑

s

ρ  T i;j ;

i Z 1;j Z 1

P
ij

pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m2i;0  m0;2j ;

ð11Þ

4.1. Statistical leakage analysis
4.1.1. Single cell leakage
Generally, the leakage variation of a single cell is expressed as
an exponential function of variation sources [21,8,7]
2

2

P leak ¼ P 0  ec11 X 1 þ c12 X 1 þ c21 X 2 þ c22 X 2 ;

ð19Þ

where X1 and X2 are the variation sources, P0 is the nominal
leakage value, cij's are the sensitivity coefﬁcients for variation
sources X1 and X2, respectively. Performing Nth order Taylor
expansion to the above equation, we have
1

P leak ¼ P 0 ∑ ai;j X i1 X j2
i;j ¼ 0
N

 P 0 ∑ ai;j X i1 X j2 :
i;j ¼ 0

ð20Þ

Now we have the to-be estimated function in a polynomial form of
variation sources. Then we may apply the method in Section 3 to
estimate the error of mean, variance, and skewness when ignoring
the high order dependence.

ð12Þ

0

where f is the function ignoring the dependence. Then the error
of variance calculation if high order dependence is ignored is

4.1.2. Full chip leakage
Full chip leakage power is calculated as

Δs2 ¼ Δ2  2μ0 Δμ  Δ2μ
¼ s2f  s2f 0

P chipleak ¼ ∑ P rleak  ∑ qi;j X i1 X j2 ;

ð21Þ

qi;j ¼ ∑ ari;j ;

ð22Þ

N

rAC

02

2

0

¼ E½f   ðE½f Þ2  E½f  þ ðE½f Þ2
¼ Δ2  2μ0 Δμ  Δμ ;
2

ð13Þ

where μ0 is the mean calculated by ignoring the high order
dependence and Δμ is the error of mean calculation which is
calculated in (9). In practice Δμ is much smaller compared to μ0 ,
therefore, we have

Δs2  Δ2  2μ0 Δμ :

ð14Þ

With the error of variance, we may also calculate the error of
standard deviation:

Δs ¼

qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Δ 2
s02 þ Δs2  s0  s 0 :
2s

i;j ¼ 0

ð15Þ

rAC

where C is the set of all circuit elements in the chip and ari;j is the
ijth order coefﬁcient for the rth circuit element. From the above
equation, we can see that the full chip leakage can be expressed as
the Taylor expansion of the variation sources. Therefore, we may
estimate the error of mean, variance, and skewness calculation as
mentioned previously.

4.2. Statistical timing analysis
Next, we calculate the error in statistical timing analysis.

490

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

4.2.1. Gate delay
The delay of a single gate is usually expressed as a quadratic
function of variation sources [4,22–28]
D ¼ a11 X 21 þ a22 X 22 þ2a12 X 1 X 2 þb1 X 1 þ b2 X 2 þ d0 :

ð23Þ

A ¼ ðaij Þ is the matrix of the second-order sensitivity coefﬁcients of
delay with respect to the variation sources, B ¼ ðbi Þ is the vector of
the linear delay sensitivity coefﬁcients, and d0 is the nominal
delay. We can apply the method in Section 3 to estimate the error
of mean, variance, and skewness variation.
From the above equation, we see that the mean of delay
variation is affected by the linear correlation between Xi's and
does not depend on the high order joint moments. However, the
delay variance and skewness are affected by high order covariances. This is because D is a quadratic function of Xi's, then the
variance is a 4th order polynomial and the skewness is a 6th order
polynomial of Xi's.
4.2.2. Full chip statistical static timing analysis (SSTA)
Due to many works on SSTA making a generic analysis of errors
is difﬁcult. For block based SSTA, there are two major operations,
MAX and ADD. The ADD operation is straightforward because we
may obtain the coefﬁcients of the sum Ds ¼ D1 þ D2 by adding up
the coefﬁcients of D1 and D2.
For the MAX operation, the problem is more involved because
there is no closed-form expression for the max of two random
variables. There are several algorithms to approximate the MAX
operation. The error of the MAX operation depends on which
algorithm we use. As an example, we consider a commonly used
algorithm namely, moment matching [23,27,28]. In the moment
matching technique, to compute the max of two delay value
Dm ¼ maxðD1 ; D2 Þ with the second order canonical form similar
to (23)
Dm ¼ am11 X 21 þam22 X 22 þ 2am12 X 1 X 2 þ bm1 X 1 þ bm2 X 2 þ dm0 :

ð24Þ

The joint moments between variation sources and the max,
E½X i maxðD1 ; D2 Þ, are ﬁrst computed and MAX is expressed in the
second order canonical form
E½Dm  ¼ am22 m20 þ 2am12 m11 þ am11 m02 þ bm1 m10 þ bm2 m01 þ dm0 ;
E½X 1  Dm  ¼ am22 m30 þ2am12 m21 þ am11 m12 þ bm1 m20
þ bm2 m11 þ dm0 m10 ;
E½X 2  Dm  ¼ am22 m21 þ2am12 m12 þ am11 m03 þ bm1 m11
þ bm2 m02 þ dm0 m01 ;
E½X 21

 Dm  ¼ am22 m40 þ2am12 m31 þ am11 m22 þ bm1 m30
þ bm2 m21 þ dm0 m20 ;

E½X 22

 Dm  ¼ am22 m22 þ2am12 m13 þ am11 m04 þ bm1 m12
þ bm2 m03 þ dm0 m02 ;

E½X 1 X 2  Dm  ¼ am22 m31 þ 2am12 m22 þ am11 m13 þ bm1 m21
þbm2 m12 þ dm0 m11 :

ð25Þ

These equations are solved to obtain the coefﬁcients amij and bmij
for Dm. Notice that the above equations contain the high order
joint moments between X1 and X2, if the high order dependence is
ignored, there will be error in the coefﬁcients amij and bmij. In order
to estimate the error, we may use the correct dependence to
compute the correct amij and bmij and then compare to those
calculated by ignoring the high order dependence.
4.3. Experiments
In this section, we show experimental results on some small
benchmark circuits to validate our estimation techniques.

4.3.1. Dependent variation sources generation
In our experiment, we assume two variation sources: effective
channel length Leff and threshold voltage Vth. Since these two
variation sources are dependent, to generate the dependent
variation sample, we assume that the variation of gate length Lgate
and dopant density Nbulk are independent.3 We ﬁrst generate
samples of Lgate and Nbulk then we use ITRS 2005 MASTAR4 (Model
for Assessment of CMOS Technologies And Roadmaps) tool [29–31]
to obtain dependent samples of Leff and Vth from the samples of
Lgate and Nbulk. By applying PCA (or ICA) to the samples of Leff and
Vth, we obtain the marginal distribution for each principal component (or independent component).
In the experiment, we use the samples of Leff and Vth with the
exact dependence to perform SPICE Monte-Carlo simulation to
calculate the exact distribution of leakage power (or delay), which
is the golden result for comparison. We also assume each principal
component (or independent component) from PCA (or ICA) to be
independent. Then we calculate the leakage power (or delay)
under such assumption and compare the result to that of the
Golden case.

4.3.2. Experimental results
In our experiments, for Lgate and Nbulk, we assume a Gaussian
distribution with 3s of 5% of the nominal value. We use 10,000
Monte-Carlo simulations to calculate the golden case leakage
power. For SPICE Monte-Carlo simulation, we assume BPTM
45 nm technology. Moreover, in our experiment, we only consider
inter-die variation.
Table 1 illustrates the mean, standard deviation, and skewness
of different cell delays. In the table, we compare the result of
Monte-Carlo (MC) simulation, the result after ﬁtting (after ﬁtting),
and result after applying PCA (PCA). Then we calculate the error
caused by curve ﬁtting (ﬁtting error), the error when ignoring the
nonlinear dependence (PCA error), and the error predicted by our
algorithm above (predicted error). In the table, we also compare
the result for two different delay (leakage) models, linear model
(Lin) and quadratic delay (leakage) model (Quad). For the linear
leakage model, we just ﬁt the leakage power as the exponential of
the linear function of variation sources, that is, not the square term
in the power in (19). For the linear delay model, we just ﬁt the gate
delay as a linear function of variation sources, that is, no second
order terms in (23).
From the table, we see that, as expected, the linear delay model
leads to larger ﬁtting error but almost does not depend on high
order correlation. However, the quadratic delay model has smaller
ﬁtting error, but there is error (about 5%) of standard deviation if
we ignore the non-linear correlation. Moreover, we see that error
predicted by our algorithm (predicted error) is very close to the
experimental result (PCA error). Table 2 illustrates the mean,
standard deviation, and skewness of different cell leakage power.
From the table, we can ﬁnd a similar trend as delay except that in
both linear and quadratic delay models, ignoring high order
dependence may cause error in both mean and standard deviation.
We also show some full chip delay and leakage analysis for few
ISCAS85 benchmarks in Tables 3 and 4. In the tables, we compare
the result of Monte-Carlo (MC) simulation, the result of SSTA
(SSTA) or statistical leakage analysis (stat leak), and delay after
applying PCA (PCA). Then we calculate the error of SSTA (SSTA
error) or statistical leakage analysis (stat leak error), the error
when ignoring the nonlinear dependence (PCA error), and the
3
Notice that in practice, Lgate and Nbulk cannot be easily measured in silicon.
The only parameters we can measure is Leff and Vth. That is, we can only extract the
dependence between Leff and Vth from the measured samples without knowing the
exact variation of Lgate and Nbulk.

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

491

Table 1
Cell delay.
Gate

Fitting type

After ﬁtting

MC
μ

3s

γ

μ

3s

γ

Inv

Lin
Quad

5.12
5.12

1.12
1.12

0.12
0.12

5.10
5.14

1.03
1.14

0.10
0.12

Nand

Lin
Quad

9.29
9.29

1.95
1.95

0.14
0.14

9.20
9.33

1.84
1.98

0.11
0.15

Nor

Lin
Quad

12.32
12.32

2.78
2.78

0.14
0.14

12.12
12.38

2.52
2.85

0.12
0.14

Gate

Fitting type

PCA
μ

Fitting error
3s

γ

μ

3s

γ

Inv

Lin
Quad

5.09
5.15

1.02
1.02

0.10
0.11

 0.03
0.02

 0.09
0.02

 0.02
0

Nand

Lin
Quad

9.18
9.36

1.82
1.89

0.11
0.13

 0.09
0.04

 0.11
0.03

 0.03
0.01

Nor

Lin
Quad

12.11
12.41

2.51
2.69

0.12
0.13

 0.20
0.06

 0.22
0.07

 0.02
0

Gate

Fitting type

PCA error

Predicted error

μ

3s

γ

μ

3s

γ

Inv

Lin
Quad

 0.01
0.01

 0.01
 0.12

0
 0.01

0
0

0
 0.10

0
 0.01

Nand

Lin
Quad

 0.02
0.03

 0.02
 0.09

0
 0.02

0
0

0
 0.11

0
 0.02

Nor

Lin
Quad

 0.01
0.03

 0.01
 0.16

0
 0.01

0
0

0
 0.14

0
 0.01

Note:delay value is in ps.
Table 2
Cell leakage.
Gate

Fitting type

After ﬁtting

MC
μ

3s

γ

μ

3s

γ

Inv

Lin
Quad

7.12
7.12

2.55
2.55

0.35
0.35

6.44
7.15

2.13
2.61

0.31
0.36

Nand

Lin
Quad

11.28
11.28

3.84
3.84

0.34
0.34

10.18
11.69

3.19
4.09

0.31
0.35

Nor

Lin
Quad

17.18
17.18

4.82
4.82

0.30
0.30

15.91
17.72

4.08
5.13

0.27
0.32

Gate

Fitting type

PCA
μ

Fitting error
3s

γ

μ

3s

γ

Inv

Lin
Quad

6.32
7.05

2.02
2.49

0.27
0.31

 0.68
0.03

 0.42
0.06

 0.04
0.01

Nand

Lin
Quad

9.97
11.34

3.01
3.91

0.27
0.31

 1.10
0.41

 0.65
0.25

 0.03
0.01

Nor

Lin
Quad

15.69
17.42

3.94
4.96

0.24
0.28

 1.27
0.54

 0.74
0.31

 0.03
0.02

Gate

Fitting type

PCA error

Predicted error

μ

3s

γ

μ

3s

γ

Inv

Lin
Quad

 0.08
 0.10

 0.11
 0.12

 0.04
 0.05

 0.11
 0.12

 0.13
 0.14

 0.03
 0.04

Nand

Lin
Quad

 0.21
 0.35

 0.18
 0.18

 0.04
 0.04

 0.19
 0.30

 0.16
 0.17

 0.03
 0.04

Nor

Lin
Quad

 0.22
 0.30

 0.14
 0.17

 0.03
 0.04

 0.19
 0.31

 0.16
 0.19

 0.03
 0.05

Note:leakage value is in nW.

492

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

Table 3
Chip delay.
Bench mark

SSTA type

MC

SSTA
3s

μ

γ

3s

μ

γ

C17

Lin
Quad

42.2
42.2

14.7
14.7

0.10
0.10

41.3
42.7

13.9
15.2

0.09
0.12

C499
0

Lin
0
Quad

320.2
 0.01
320.2

105.5

0.14

318.2

102.9

0.13

105.5

0.14

321.1

106.2

0.15

C880

Lin
Quad

674.4
674.4

221.3
221.3

0.12
0.12

671.2
679.5

215.2
227.2

0.11
0.14

C3540

Lin
Quad

1241.2
1241.2

413.2
413.2

0.13
0.13

1237.1
1249.7

410.1
420.2

0.12
0.15

C7522

Lin
Quad

1919.3
1919.3

635.4
635.4

0.13
0.12

1911.3
1931.2

631.2
638.9

0.11
0.13

Bench mark

SSTA type

PCA

SSTA error
3s

μ

γ

μ

3s

γ

C17

Lin
Quad

41.2
42.0

13.8
15.0

0.09
0.11

 0.9
0.5

 0.8
0.5

 0.01
0.02

C499

Lin
Quad

317.9
319.2

102.6
104.9

0.12
0.13

 2.0
0.9

 2.6
0.7

 0.01
0.01

C880

Lin
Quad

671.5
676.7

214.8
225.3

0.11
0.13

 3.2
5.1

 6.1
5.9

 0.01
0.2

C3540

Lin
Quad

1236.5
1247.1

409.7
418.2

0.11
0.13

 4.1
8.5

 3.1
7.0

 0.01
0.2

C7522

Lin
Quad

1907.1
1929.4

631.9
639.5

0.12
0.13

 8.0
11.1

 4.2
3.5

 0.02
0.01

Bench mark

SSTA type

PCA error

Predicted error

μ

3s

γ

μ

3s

γ

C17

Lin
Quad

 0.1
 0.7

 0.1
 0.2

0
 0.01

0
 0.6

0
 0.2

 0.01
 0.01

C499

Lin
Quad

 0.3
 1.9

 0.3
 1.3

 0.01
 0.02

0
 2.1

0
 1.1

 0.01
 0.02

C880

Lin
Quad

0.3
 2.8

 0.4
 1.9

0
 0.01

0
 2.9

0
 1.8

 0.0
 0.01

C3540

Lin
Quad

 0.6
 2.6

 0.4
 2.0

 0.01
 0.02

 0.5
 2.7

 0.5
 1.9

 0.01
 0.01

C7522

Lin
Quad

 3.2
 1.8

0.7
0.6

0.01
0.00

 2.9
 1.7

0.6
0.5

0.01
0.01

Note:delay value is in ps.

error predicted by our algorithm (predicted error). Notice that
SSTA error and the statistical leakage analysis error are caused by
both curve ﬁtting and analysis algorithm. Similar to the single gate
case, we see that error predicted by our algorithm (predicted
error) is very accurate compared to the experimental result (PCA
error). From the tables, we see that the error caused by non-linear
dependence is not signiﬁcant in the ISCAS85 circuit bench.4

5. Target function driven component analysis
In the previous section, we introduced the method to estimate
the error caused by ignoring non-linear dependence and showed
that it depends on the target function being estimated. As discussed in Section 1, linear operations cannot completely remove the
4
Especially for statistical timing analysis in this experiment, such error is less
than 2%.

dependence between variation sources. However, due to simplicity
of application, linear operation is preferred. Therefore, in this section,
we try to ﬁnd an optimum linear transform to minimize the error of
ignoring the non-linear dependence. The proposed algorithm, function driven component analysis (FCA), decomposes dependent variation sources into components so as to minimize error in the
estimation of certain statistical measures of the target function.
In the rest of this section, we ﬁrst present our algorithm and
then apply it to statistical leakage analysis and SRAM cell noise
margin variation analysis. Note that the method can also be applied
to the variation analysis of emerging memory technologies, such as
STT-RAM.
5.1. FCA algorithm
Let f(X) be a polynomial function (or Taylor expansion of
an arbitrary function) of an n-dimensional random vector X ¼
ðX 1 ; X 2 ; …X n ÞT . The objective of the FCA is to ﬁnd an n  n transfer
matrix W and independent components P ¼ ðP 1 ; P 2 ; …; P n Þ ¼ W  X

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

493

Table 4
Chip leakage. Note: leakage value is in μW for C17, and in mW for others.
Gate

Fitting type

C17

Lin
Quad

C499

Lin
Quad

C880

Lin
Quad

C3540

MC

Stat leak

μ

3s

γ

μ

3s

γ

430.2
430.2

120.3
120.3

0.28
0.28

415.3
437.2

113.3
126.3

0.25
0.32

3.21
3.21

0.32
0.32

22.3
22.3

7.58
7.58

0.34
0.34

Lin
Quad

89.2
89.2

29.23
29.23

C7522

Lin
Quad

162.2
162.2

56.21
56.21

Gate

Fitting type

C17

Lin
Quad

C499

Lin
Quad

C880

Lin
Quad

C3540

9.14
9.14

2.92
3.58

0.29
0.35

20.2
23.8

6.95
7.92

0.29
0.39

0.41
0.41

84.5
92.5

27.78
30.29

0.39
0.43

0.38
0.38

155.3
170.1

55.51
57.11

0.35
0.40

PCA

8.54
9.45

Stat Leak error

μ

3s

γ

μ

3s

γ

413.2
431.5

109.2
122.2

0.23
0.30

 14.9
7.0

 7.0
6.0

 0.03
0.02

2.79
3.40

0.27
0.33

 0.60
0.41

 0.29
0.37

 0.03
0.03

19.1
23.3

6.63
7.76

0.26
0.37

 2.1
1.5

 0.63
0.34

 0.05
0.05

Lin
Quad

82.1
90.7

27.15
30.01

0.37
0.42

 4.7
3.3

 1.45
1.04

 0.02
0.02

C7522

Lin
Quad

152.1
168.2

55.13
56.72

0.31
0.39

 6.9
7.9

 0.80
0.90

 0.03
0.02

Gate

Fitting type

PCA error

8.32
9.21

Predicted error

μ

3s

γ

μ

3s

γ

C17

Lin
Quad

 2.1
 5.7

 4.1
 4.1

 0.02
 0.02

 2.3
 5.3

 3.9
 3.8

 0.02
 0.03

C499

Lin
Quad

 0.22
 0.24

 0.13
 0.18

 0.02
 0.02

 0.19
 0.28

 0.12
 0.17

 0.02
 0.02

C880

Lin
Quad

 1.1
 0.5

 0.32
 0.16

 0.03
 0.02

 0.9
 0.4

 0.36
 0.18

 0.02
 0.02

C3540

Lin
Quad

 2.4
 1.8

 0.53
 0.28

 0.02
 0.01

 2.2
 1.5

 0.59
 0.27

 0.02
 0.01

C7522

Lin
Quad

 3.2
 1.9

 0.38
 0.39

 0.04
 0.01

 2.9
 1.7

 0.36
 0.37

 0.03
 0.01

Note:leakage value is in μW for C17, and in mW for others.

to minimize the error of f(WP) when assuming that all Pi's are
independent. In statistical analysis, the error of f(WP) is usually
measured by mean, variance, and skewness. In this work, we
consider the ﬁrst-order analysis by matching the mean of f(X).
Those are
W ¼ arg min Δ;

ð26Þ

Δ ¼ Δs þ εΔγ ;

ð27Þ

Δμ ¼ μf  μf 0 ;

ð28Þ

Δs ¼ s f  s f 0 ;

ð29Þ

Δγ ¼ γ f  γ f 0 ;

ð30Þ

Δμ ¼ 0

where μf, sf, and γf are the mean, standard deviation, and
skewness of f(X), respectively; μf 0 , sf 0 , and γ f 0 are the mean,
standard deviation, and skewness of f(WP) when assuming that

all Pi's are independent; ε is the weight factor for the skewness
error. Since f(X) is a polynomial function of X, similar to (9), (15),
and (18), μf, sf, and γf can be expressed as a function of joint
moments of Xi's, which are known; and μf 0 , sf 0 , and γ f 0 can be
expressed as a function of joint moments of Pi's. Considering
P¼ WX, the joint moments of Pi's can be expressed as functions of
W and the joint moments of Xi's. Hence, the error Δ can be
expressed as a function of W and joint moments of Xi's. Therefore
(26) becomes a non-linear programming problem. We use a nonlinear programming solver to obtain the transfer matrix W.
A more general objective is5
W ¼ arg minðΔμ þ ε1 Δs þ ε2 Δγ Þ:
Unlike the regular PCA or ICA, our FCA algorithm presented
above tries to minimize the error for a target function f. That is, for
different target function f, we may have different transfer matrix
5

This is especially useful in cases where μ ¼ 0 has no solution.

494

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

W. In FCA, we need to obtain an n  n transfer matrix W, that is, we
need to solve a n2 variable non-linear programming problem.
However, for any statistical analysis, FCA needs to be run only
once. Moreover, FCA still uses linear operation to decompose the
variation sources. Therefore, applying FCA does not increase the
computational complexity of the statistical analysis compared to
regular PCA or ICA.
In order to validate our algorithm, let us ﬁrst take a look at the
simple example we introduced in Section 1: let S1 and S2 be two
independent random variables with standard normal distributions
and X 1 ¼ S1 þS2 , X 2 ¼ S1 S2 . Estimate the mean of f ðX 1 ; X 2 Þ ¼
X 21 þ X 1 X 2 þ X 21 X 22 þ X 22 . As discussed in Section 1, the correct value
is E½f ðÞ ¼ 9. If we apply PCA, because X1 and X2 are uncorrelated,
they are just principal components. If we assume that principal
components X1 and X2 are independent, we have E½f ðÞ ¼ 5. If we
apply fast kernel ICA [32] to obtain independent components, we
will have E½f ðÞ ¼ 5:78. If we use FCA, we have E½f ðÞ ¼ 8:54. That is,
FCA works better than PCA and ICA.

Table 5
Logic cell leakage power comparison.
Gate

μ

3s

γ

90%

95%

99%

INV
Exact
PCA
ICA
FCA

7.15
7.05
7.07
7.10

2.61
2.49
2.47
2.56

0.36
0.31
0.32
0.34

10.12
9.85
9.91
10.02

10.75
10.34
10.41
10.59

11.37
11.01
11.09
11.21

NAND
Exact
PCA
ICA
FCA

11.69
11.34
11.32
11.68

4.09
3.91
3.93
4.05

0.35
0.31
0.34
0.35

15.95
15.51
15.60
15.83

16.82
16.41
16.45
16.68

17.76
17.41
17.50
17.62

NOR
Exact
PCA
ICA
FCA

17.72
17.42
17.51
17.71

5.13
4.96
5.01
5.09

0.32
0.28
0.30
0.31

23.89
23.44
23.51
23.74

24.95
24.61
24.64
24.85

26.11
25.94
25.99
26.01

Note:leakage value is in nW.

5.2. Experimental results
In this section, we show some examples to validate the FCA
algorithm. As discussed in Section 4.3, non-linear dependence
does not have signiﬁcant impact on statistical timing analysis. In
this section we show three examples of FCA in VLSI design:
statistical leakage analysis, differential Opamp amplitude, and
SRAM noise margin variation analysis.
5.2.1. Statistical leakage analysis
We ﬁrst discuss statistical leakage analysis. Similar to Section
4.3, we assume two variation sources, effective Leff and Vth and we
only consider inter-die variation for the variation sources. We
generate dependent variation samples of Leff and Vth in the same
way as Section 4.3.1. With the dependent samples, we use FCA
(PCA or ICA) to decompose the variation sources and obtain the
marginal distribution of each component. Then we generate
sample of each component according to its marginal distribution.
Assuming that the components are independent, we generate the
samples of Leff and Vth. Finally, we use these samples to run SPICE
Monte-Carlo simulation to obtain leakage power. We use BPTM
45 nm technology in the experiment and assume supply voltage to
be 1.0 V. For Lgate and Nbulk, we assume that they follow Gaussian
distribution and the 3-sigma value is 5% of the nominal value.
In order to validate the accuracy of FCA we deﬁne three
comparison cases: (1) samples generated from Mastar4 with the
exact dependence, which is the golden case for comparison,
(2) samples generated from PCA, and (3) samples generated from
fast kernel ICA [32].
Table 5 illustrates the mean, standard deviation, skewness, 90%,
95%, and 99% percentile point of leakage of different logic cells.
From the table, we see that the value obtained from FCA is closer to
the exact value than PCA and ICA.6 Table 6 illustrates the leakage
comparison for full chip leakage power analysis. For full chip
leakage power analysis, FCA may give out different decomposition
matrices for different cells. In this experiment, we apply the
decomposition matrix obtained from the inverter for all logic cells
in the chip. From the table, we see that even FCA works well in full
chip leakage analysis.
Table 7 illustrates the exact error and the estimated error
(using the method in Section 4) of mean, standard deviation,
and skewness for logic cells. From the table, we can ﬁnd that the
6
The run time for PCA and ICA is less than 0.1 s, and the run time for FCA is
0.4 s. However, because FCA needs to be run only once in the statistical analysis,
such run time overhead is a non-issue.

Table 6
Chip leakage power comparison.
Gate

μ

3s

γ

90%

95%

99%

C17
Exact
PCA
ICA
FCA

437.2
431.5
432.8
437.3

126.3
122.2
123.5
124.9

0.32
0.30
0.30
0.31

612.4
592.8
599.2
604.2

675.3
653.5
654.4
664.3

721.5
701.8
704.6
711.2

C499
Exact
PCA
ICA
FCA

9.45
9.21
9.25
9.44

3.58
3.40
3.45
3.51

0.35
0.33
0.34
0.35

10.31
10.02
10.12
10.16

11.15
10.98
11.01
11.06

12.01
11.78
11.81
11.91

C880
Exact
PCA
ICA
FCA

23.8
23.3
23.2
23.9

7.92
7.76
7.72
7.81

0.39
0.37
0.38
0.38

26.11
25.56
25.32
25.74

29.15
28.41
28.35
28.67

31.31
29.10
29.01
29.65

C3540
Exact
PCA
ICA
FCA

92.5
90.7
90.9
92.1

30.29
30.01
30.12
30.13

0.43
0.42
0.42
0.42

107.2
103.1
104.1
105.6

119.5
114.3
116.3
116.9

131.2
124.6
125.6
128.4

C7522
Exact
PCA
ICA
FCA

170.1
168.2
167.9
169.7

57.11
56.72
56.81
56.86

0.40
0.39
0.39
0.40

193.3
184.6
185.2
187.1

218.6
209.3
211.3
213.5

231.2
221.5
223.3
225.9

Note:leakage value is in.μW for C17, and in mW for others.

estimated error is close to the exact error and that FCA has a lower
error than PCA or ICA.

5.2.2. Differential ampliﬁer analysis
The second application example for FCA is the simple one stage
differential operation ampliﬁer amplitude. We use the same device
setting as the statistical leakage analysis in Section 5.2.1. The only
difference is that in this experiment, we consider the mismatch
of Leff and Vth of the two input transistors. We assume that the
3-sigma of both mismatch variation is 5% of the nominal value.
Table 8 illustrates the mean, standard deviation, skewness, 90%,
95%, and 99% percentile point of amplitude of the Opamp. From the
table, we see that the value obtained from FCA is closer to the exact

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

Table 7
Estimated error for the logic cell leakage power.
Exact error

Table 10
SNM comparison.
Est error

Name

μ

3s

γ

μ

3s

γ

INV
PCA
ICA
FCA

 0.10
 0.08
 0.05

 0.12
 0.14
 0.05

 0.05
 0.04
 0.02

 0.11
 0.10
0

 0.14
 0.16
 0.14

 0.04
 0.04
 0.03

NAND
PCA
ICA
FCA

 0.35
 0.37
 0.01

 0.18
 0.16
 0.04

 0.04
 0.01
 0.00

 0.39
 0.34
0

 0.17
 0.13
 0.06

 0.03
 0.02
 0.01

NOR
PCA
ICA
FCA

 0.30
 0.21
 0.01

 0.17
 0.12
 0.04

 0.04
 0.02
 0.01

 0.28
 0.24
0

 0.19
 0.14
 0.05

 0.04
 0.03
 0.02

s (mV)

γ

90% (mV)

95% (mV)

99% (mV)

148
141
140
147

29.4
30.7
30.3
28.9

0.091
0.077
0.079
0.094

105
107
109
106

84.4
89.2
88.4
85.8

65.5
69.0
68.1
67.2

Table 11
Estimated error for the SNM variation analysis.
Name

Exact error

Est error

μ

s

γ

μ

s

γ

 4.7
 5.0
 1.4

1.3
0.9
 0.5

 0.014
 0.012
0.003

 4.9
 4.8
0.0

1.2
0.9
 0.4

 0.015
 0.014
0.005

Note:the error of μ and s is in mV.

Table 8
Opamp amplitude comparison.
Name

μ

3s

γ

90%

95%

99%

Exact
PCA
ICA
FCA

742.4
749.7
748.0
743.7

65.0
61.4
62.1
62.6

0.96
0.84
0.85
0.88

682.2
687.4
686.2
685.9

646.3
648.6
648.7
647.7

712.8
615.3
615.2
615.0

Table 9
Estimated error for the Opamp amplitude.

PCA
ICA
FCA

0.1678

μ (mV)

PCA
ICA
FCA

Note:leakage value is in nW.

Name

Exact
PCA
ICA
FCA

Nominal value

Exact error

10
Exact

8
Redundancy %

Gate

495

PCA
ICA

6

FCA

4
2

Est error

μ

3s

γ

μ

3s

γ

7.3
5.6
1.3

 3.4
 2.9
 2.4

 0.12
 0.11
 0.08

6.9
5.2
0

 3.3
 3.1
 2.1

 0.10
 0.09
 0.05

value than PCA and ICA, which is similar to the leakage power
variation analysis. In Table 9, we compare the exact error and the
error estimated by the method in Section 3. We also observe that
the estimated error is very close to the exact value as expected.

0
30

40

50

60

70

(a) SNM cut off %
Fig. 1. Redundancy for Non-ECC scheme to achieve 99% yield rate.

15
Exact
PCA
ICA
FCA

5.2.3. SRAM noise margin variation analysis
The third application example for FCA is the 6 T-SRAM cell
noise margin (SNM). We use similar setting to the statistical
leakage analysis in Section 5.2.1. In order to highlight the ﬂexibility
of FCA, in this experiment, we consider only within-die variation.
That is, each transistor has its own variation. In practice, SNM is
mainly affected by within die variation of the 4 transistors which
make two inverters, and inter-die variation and variation of the
two pass-transistor has little impact on SNM. Therefore, in our
experiment, we only consider within-die variation for those
4 transistors. In this case, because we consider 4 transistors in
an SRAM cell, there are 8 variation sources in an SRAM (Leff and Vth
for all 4 transistors). Notice that PCA and ICA provide the same
transfer matrix for Leff and Vth for all transistors, however because
FCA tries to handle 8 variation sources together, it may provide
different transfer matrices for different transistors.
Table 10 illustrates the mean, standard deviation, skewness,
90%, 95%, and 99% percentile point of noise margin of an SRAM.
From the table, we ﬁnd that the value obtained from FCA is closer
to the exact value than PCA and ICA. Table 11 compares the exact

10

5

0
0.05

0.1

0.15

0.2

SNM (V)
Fig. 2. SNM PDF comparison.

error and the error estimated by the method in Section 3. We also
ﬁnd that the estimated error is very close to the exact value.
With noise margin variation analysis, we may further estimate
a number of redundant SRAM cells needed to ensure error correct
SRAM array. We assume that the variation of all SRAM cells in the

496

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

Table 12
SRAM cell noise margin comparison assuming Lgate and Nbulk to be with skewnormal distribution.
Name

μ (mV)

s (mV)

γ

90% (mV)

95% (mV)

14
Exact
PCA

12

ICA

99% (mV)

10
Exact
PCA
ICA
FCA

158
161
160
159

32.8
31.3
31.8
33.1

0.345
0.297
0.307
0.335

112
120
115
114

94.2
99.4
97.3
96.6

74.8
80.2
76.3
75.5

FCA

8
6
4

Table 13
Estimated error for the SNM variation analysis assuming Lgate and Nbulk to be with
skew-normal distribution.
Name

Exact error

PCA
ICA
FCA

0
0.05

Est error

μ

s

γ

μ

s

γ

3.4
2.1
1.2

 1.5
 1.0
0.3

 0.048
 0.038
 0.010

3.9
1.4
0.0

 1.1
 0.7
0.2

 0.031
 0.027
 0.019

Note:the error of μ and s is in mV.

Exact

Redundancy %

0.1

0.15

0.2

0.25

SNM (V)
Fig. 4. SNM PDF comparison assuming Lgate and Nbulk to be with skew-normal
distribution.

illustrates the mean, standard deviation, skewness, 90%, 95%, and
99% percentile point of noise margin of an SRAM under such
setting. Table 13 illustrates the estimated error for PCA, ICA, and
FCA when assuming that all variation sources follow skew-normal
distribution. Fig. 3 illustrates redundancy and Fig. 4 compares the
PDFs. From the table and ﬁgure, we ﬁnd that FCA works better
than PCA and ICA under different variation settings.

8

6

2

PCA
ICA
FCA

4

6. Conclusion

2

0
30

40

50

60

70

(a) SNM cut off %
Fig. 3. Redundancy for Non-ECC scheme to achieve 99% yield rate assuming Lgate
and Nbulk to be with skew-normal distribution.

array are independent and an SRAM cell is faulty when the noise
margin is less than a cut off value. For non-ECC architecture, for
simplicity, we calculate the number of redundant SRAM cells
needed to achieve a certain percent yield. For the ECC scheme,
the number of redundant SRAM cells depends on the coding. For
simplicity, we estimate the Shannon Channel limit [33], which is
the lower bound of the redundancy required to achieve no error
coding.
Fig. 1 illustrates the percentage SRAM redundancy under
different cut off SNM values. In the ﬁgure, the x-axis is the cut
off SNM value, which is calculated as a certain percentage of the
nominal value (0.152 V). The y-axis is the percentage redundancy.
For the non-ECC scheme, we assume that the redundancy is to
achieve 99% yield rate.7 Fig. 2 compares the PDFs predicted by ICA,
PCA, and FCA to the exact PDF. From the ﬁgures, we see that FCA
predicts the redundancy more accurately than PCA or ICA.
We also ran experiments for different variation settings. In
stead of assuming Lgate and Nbulk to be Gaussian, we assume that
they follow skew-normal distribution with α ¼ 10 [34]. Table 12

7
This is just a simple estimation. In practice, because redundancy is needed for
each row and column of SRAM array, the actual redundancy may be much higher.

In this paper, we have proposed the ﬁrst method to estimate
the error of statistical analysis when ignoring the non-linear
dependence using polynomial correlation coefﬁcients. Such a
method can be used to evaluate the accuracy of the linear decorrelation techniques like PCA for a particular analysis problem.
As examples, we apply our technique to statistical timing and
power analysis. Experimental result shows that the error predicted
by our method is within 1% compared to the real simulation. We
have further proposed a novel target function driven component
analysis (FCA) algorithm to minimize the error caused by ignoring
high order dependence. We apply such a technique to two
applications of statistical analysis, statistical leakage power analysis and SRAM cell noise margin variation analysis. Experimental
results show that the proposed FCA method is more accurate
compared to the traditional PCA or ICA. In the future work, we will
evaluate our work with larger-scale industrial circuits. Also, sparse
approximation based parameter estimation methods [35–37] will
be considered to reduce the need of measurements in the
statistical model.
References
[1] J.-J. Liou, A. Krstic, L.-C. Wang, K.-T. Cheng, False-path-aware statistical timing
analysis and efﬁcient path selection for delay testing and timing validation, in:
Design Automation Conference, Proceedings, June 2002, pp. 566–569.
[2] J. Jess, K. Kalafala, S. Naidu, R. Otten, C. Visweswariah, Statistical timing for
parametric yield prediction of digital integrated circuits, in: DAC 03, June
2003.
[3] M. Orshansky, A. Bandyopadhyay, Fast statistical timing analysis handling
arbitrary delay correlations, in: DAC 04, June 2004.
[4] S. Bhardwaj, P. Ghanta, S. Vrudhula, A framework for statistical timing analysis
using non-linear delay and slew models, in: ICCAD, November 2006.
[5] W. Xu, J. Wang, Y. Hu, J.-Y. Lee, F. Gong, L. He, M. Sarrafzadeh, In-place FPGA
retiming for mitigation of variational single-event transient faults, IEEE Trans.
Circuits Syst. Part I (TCAS-I) 58 (June) (2011) 1372–1381.
[6] L. Cheng, Y. Lin, L. He, Tracebased framework for concurrent development
of process and FPGA architecture considering process variation and

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

[7]

[8]
[9]

[10]

[11]
[12]
[13]
[14]

[15]

[16]
[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]
[25]

[26]

[27]

[28]

[29]
[30]
[31]

[32]

[33]
[34]
[35]

[36]

[37]

reliability, in: Proceedings of the International Symposium on FPGAs, February
2008.
C.H. Tan, J. Allen, Minimization of power in VLSI circuits using transistor sizing,
input ordering, and statistical power estimation, in: Proceedings of the
International Workshop on Low Power Design, April 1994, pp. 75–79.
H.-Y. Wong, L. Cheng, Y. Lin, L. He, FPGA device and architecture evaluation
considering process variations, in: ICCAD, November 2005.
L. Cheng, W. Xu, F. Gong, Y. Lin, H.-Y. Wong, L. He, Statistical timing and power
optimization of architecture and device for FPGAs, ACM Trans. Reconﬁgurable
Technol. Syst. (TRTS) 5 (June) (2012).
J. Xiong, V. Zolotov, L. He, Robust extraction of spatial correlation,
in: Proceedings of the International Symposium on Physical Design, April
2006.
R.B. Nelsen, An Introduction to Copulas, Springer, New York City, USA, 2007.
T.M. Cover, J.A. Thomas, Elements of Information Theory, John Wiley and Sons,
Hoboken, New Jersey, USA, 2006.
Mark A. Kramer, nonlinear principal component analysis using autoassociative
neural networks, AlChE J. 37 (2) (1991) 233–243.
Christian Jutten, Juha Karhunen, Advances in blind source separation (BSS)
and independent component analysis (ICA) for nonlinear mixtures, Int. J.
Neural Sys. 14 (5) (2004) 267.
L. Cheng, P. Gupta, L. He, Accounting for non-linear dependence using function
driven component analysis, in: The 2009 Asia and South Paciﬁc Design
Automation Conference, 2009.
L. Cheng, J. Xiong, L. He, Non-linear statistical static timing analysis for nonGaussian variation sources, in: DAC, June 2007.
Fengbo Ren, Henry Park, Chih-Kong Ken Yang, Dejan Markovic, Reference
Calibration of Body-Voltage Sensing Circuit for High-Speed STT-RAMs, IEEE
Transactions on Circuits And Systems—I: Regular Papers (TCAS-I) 60 (11)
(2013) 2932–2939.
L. Cheng, F. Gong, W. Xu, J. Xiong, L. He, M. Sarrafzadeh, Fourier series
approximation for max operation in non-Gaussian and quadratic statistical
static timing analysis, IEEE Trans. Very Large Scale Integr. Syst. (TVLSI) 20
(August) (2012) 1383–1391.
F. Ren, D. Markovic, True energy-performance analysis of the MTJ-based logicin-memory architecture (1-bit full adder), IEEE Trans. Electron Devices 57
(May) (2010) 1023–1028.
K. Esbensen, P. Geladi, Principle component analysis, in: Proceedings of the
Multivariate Statistical Workshop for Geologists and Geochemists, August
1987.
S. Borkar, T. Karnik, S. Narendra, J. Tschanz, A. Keshavarzi, V. De, Parameter
variations and impacts on circuits and microarchitecture, in: DAC 03, June
2003.
L. Zhang, W. Chen, Y. Hu, J.A. Gubner, C.C.-P. Chen, Correlation-preserved nonGaussian statistical timing analysis with quadratic timing model, in: DAC, June
2005, pp. 83–88.
Y. Zhan, A.J. Strojwas, X. Li, L.T. Pileggi, Correlation-aware statistical timing
analysis with non-Gaussian delay distribution, in: DAC, Anaheim, CA, June
2005, pp. 77–82.
V. Khandelwal, A. Srivastava, A general framework for accurate statistical
timing analysis considering correlations, in: DAC, June 2005, pp. 89–94.
J. Singh, S. Sapatnekar, Statistical timing analysis with correlated
non-Gaussian parameters using independent component analysis, in:
ACM/IEEE International Workshop on Timing Issues, February 2006,
pp. 143–148.
H. Chang, V. Zolotov, C. Visweswariah, S. Narayan, Parameterized block-based
statistical timing analysis with non-Gaussian and nonlinear parameters, in:
DAC, Anaheim, CA, June 2005, pp. 71–76.
Y. Zhan, A.J. Strojwas, D. Newmark, M. Sharma, Generic statistical timing
analysis with non-Gaussian process parameters, in: Austin Conference on
Integrated Systems and Circuits, May 2006.
L. Cheng, J. Xiong, L. He, Non-Gaussian statistical timing analysis using secondorder polynomial ﬁtting, in: Proceedings of the Asia South Paciﬁc Design
Automation Conference, January 2008.
International Technology Roadmap for Semiconductor, 2005, 〈http://public.
itrs.net/〉.
International Technology Roadmap for Semiconductor, A User's Guide to
MASTAR4, 2005, http://www.itrs.net/models.html.
T. Skotnicki, Heading for decananometer CMOS—is navigation among icebergs
still a viable strategy?, in: Solid-State Device Research Conference, September
2000, pp. 19–33.
H. Shen, S. Jegelka, A. Gretton, Fast kernel ICA using an approximate Newton
method, in: 11th International Conference on Artiﬁcial Intelligence and
Statistic, 2007.
R.E. Blahut, Algebraic Codes for Data Transmission, Cambridge University
Press, Cambridge, UK, 2002.
A. Azzalini, The skew-normal distribution and related multivariate families,
Scand. J. Stat. 32 (June) (2005) 159–188.
X. Li, Finding deterministic solution from underdetermined equation: largescale performance modeling by least angle regression, in: IEEE/ACM Design
Automation Conference (DAC), June 2009, pp. 364–369.
W.X.F. Ren, D. Markovic, Scalable and parameterized VLSI architecture for
efﬁcient sparse approximation in FPGAs and SOCs, IET Electron. Lett. 49
(November) (2013) 1440–1441.
W. Xu, C. Gu, C. Li, M. Sarrafzadeh, Robust doppler radar demodulation via
compressed sensing, IET Electron. Lett. 48 (October) (2012) 1428–1430.

497

Lerong Cheng received the B.S. degree in electronics
and communications engineering from Zhongshan University, Guangzhou, China, in2001, the M.S. degree in
electrical and computer engineering from Portland
State University, Portland, OR, in 2003, and the Ph.D
degree in electrical engineering from the University of
California at Los Angeles, Los Angeles, in 2009. He is
currently a Computer-Aided Design (CAD) Engineer
with SanDisk Corporation, San Jose, CA. His current
research interests include CAD of very large-scale
integrated circuits and systems, programmable fabrics,
low-power and high-performance designs, and statistical timing analysis.

Wenyao Xu received the Ph.D. degree from the Electrical Engineering Department, University of California,
Los Angeles, CA, USA, in 2013. Currently, he is an
Assistant Professor with the Computer Science and
Engineering Department, University at Buffalo, the
State University of New York (SUNY), New York, NY,
USA. His current research interests include embedded
system design, computational modeling, algorithm
design, human computer interaction, integrated circuit
design technologies, and their applications in medical
and health applications. He received the Best Medical &
Performance Application Paper Award of the IEEE
Conference on Implantable and Wearable Body Sensor
Networks in 2013 and the Best Demonstration Award of ACM Wireless Health
Conference in 2011.

Fengbo Ren received the B.Eng. degree in electrical
engineering from Zhejiang University, Hangzhou,
China, in 2008, and the M.S. degree from University
of California, Los Angeles, CA, USA, in 2010, where he is
currently a Ph.D. candidate, specializing in circuit and
embedded systems. In 2006, he studied as exchange
student with the Department of Electronic & Computer
Engineering, Hong Kong University of Science and
Technology, Kowloon, Hong Kong. During the fall of
2009, he worked as an Engineer Intern with the Digital
ASIC Group, Qualcomm Inc., San Diego, CA, USA, where
he contributed to the low power design ﬂow of a smart
phone SOC. During the summer of 2012, he worked as a
Ph.D. Intern with the Data Center Group, Cisco Systems Inc., San Jose, CA, USA,
where he was involved in the FPGA emulation of a data center switch ASIC. His
current research interests include circuit design and design optimization for STTRAM and efﬁcient DSP architectures for the sparse signal processing in compressivesensing applications.

Fang Gong received his B.S. degree from the Computer
Science Department at the Beijing University of Aeronautics and Astronauticsin 2005. Also, he graduated
from Computer Science Department at the Tsinghua
University with M.S. degree in 2008. After that, he
continues to work toward his Ph.D. degree in the
Electrical Engineering Department at the University of
California,Los Angeles. His research interests mainly
focus on stochastic modeling and simulation for CAD.
He also works on healthcare embedded system, parallel
and distributed computing.

Puneet Gupta received the B.Tech. degree in electrical
engineering from the Indian Institute of Technology
Delhi, New Delhi, India, in 2000, and the Ph.D. degree
from the University of California at San Diego (UCSD),
San Diego, in 2007. He co-founded Blaze DFM, Inc.,
Sunnyvale, CA (acquired by Tela, Inc.) in 2004 and was
its Product Architect until 2007. He is currently a
Faculty Member with the Department of Electrical
Engineering, UCLA. He has authored over 80 papers,
ten U.S. patents, and a book chapter. His current
research interests include building high-value bridges
across application–architecture–implementation–fabrication interfaces for loweredcost and power, increased
yield, and improved predictability of integrated circuits and systems. Dr. Gupta was

498

L. Cheng et al. / INTEGRATION, the VLSI journal 47 (2014) 487–498

a recipient of the NSF CAREER Award, the ACM/SIGDA Outstanding New Faculty
Award, the SRC Inventor Recognition Award, the European Design Automation
Association Outstanding Dissertation Award, and the IBM Ph.D. Fellowship. He has
given tutorial talks at DAC, ICCAD, the International VLSI Design Conference, and
the SPIE Advanced Lithography Symposium. He was the Technical Program
Committee Member of DAC, ICCAD, ASPDAC, ISQED, ICCD, SLIP, and VLSI Design.
He wasthe Program Chair of the IEEE DFM&Y Workshop in 2009 and 2010.

Lei He is a professor at electrical engineering department, University of California, Los Angeles (UCLA) and
was a faculty memberat the University of Wisconsin,
Madison between 1999 and 2002. He also held visiting
or consulting positions with Cadence, Empyrean Soft,
Hewlett-Package, Intel, and Synopsys, and was technical advisory board member for Apache Design Solutions and Rio Design Automation. Dr. He obtained Ph.D.
degree in computer sciencefrom UCLA in 1999. His
research interests include modeling and simulation,
VLSI circuits and systems, and cyber physical systems.
He has published on ebook and over 200 technical
papers with 12 best paper nominations mainly from

Design Automation Conference and International Conference on Computer-Aided
Design and ﬁve best papers or best contribution awards including the ACM
Transactions on Electronic System Design Automation 2010 Best Paper Award.

arXiv:1612.05203v4 [cs.CV] 5 Apr 2017

CSVideoNet: A Real-time End-to-end Learning Framework for
High-frame-rate Video Compressive Sensing
Kai XU
Arizona State University

Fengbo Ren
Arizona State University

kaixu@asu.edu

renfengbo@asu.edu

April 6, 2017

Abstract

ing in physical and biological science. Some high-end
frame-rate cameras, such as Photron SA1, SA3, are capable of recording high resolution still images of ephemeral
events such as a supersonic flying bullet or an exploding
balloon with negligible motion blur and image distortion
artifacts. However, due to the complex sensor hardware
designed for high sampling frequency, these equipments
are extremely expensive. The high cost (over tens of
thousand dollars for one camera) limits the field of their
applications. Furthermore, the high transmission bandwidth and the large storage space associated with the
high frame rate challenges the manufacture of affordable
consumer devices. For example, true high-definitionresolution (1080p) video cameras at a frame rate of 10k
fps can generate about 500 GB data per second, which
imposes significant challenges for existing transmission
and storing techniques. In addition, the high throughput raises energy efficiency a big concern. For example,
“GoPro 5” can capture videos at 120 fps with 1080p resolution. However, the short battery life (1-2 hours) has
significantly narrowed their practical applications.

This paper addresses the real-time encoding-decoding
problem for high-frame-rate video compressive sensing (CS). Unlike prior works that perform reconstruction using iterative optimization-based approaches, we
propose a non-iterative model, named “CSVideoNet”.
CSVideoNet directly learns the inverse mapping of CS
and reconstructs the original input in a single forward
propagation. To overcome the limitations of existing CS
cameras, we propose a multi-rate CNN and a synthesizing RNN to improve the trade-off between compression
ratio (CR) and spatial-temporal resolution of the reconstructed videos. The experiment results demonstrate that
CSVideoNet significantly outperforms the state-of-theart approaches. With no pre/post-processing, we achieve
25dB PSNR recovery quality at 100x CR, with a frame
rate of 125 fps on a Titan X GPU. Due to the feedforward and high-data-concurrency natures of CSVideoNet,
it can take advantage of GPU acceleration to achieve
three orders of magnitude speed-up over conventional
iterative-based approaches. We share the source code at
https://github.com/PSCLab-ASU/CSVideoNet.

1

Traditional video encoder, e.g., H.264/MPEG-4, is composed of motion estimation, frequency transform, quantization, and entropy coding modules. From both speed
and cost perspectives, the complicated structure makes
these video encoder unsuitable for high-frame-rate video
cameras. Alternatively, compressive sensing (CS) is a
much more hardware-friendly acquisition technique that
allows video capturing with a sub-Nyquist sampling rate.
The advent of CS has led to the emergence of new image devices, e.g., single-pixel cameras [6]. CS has also

Introduction

High-frame-rate cameras are capable of capturing videos
at frame rates over 100 frames per second (fps). These
devices were originally developed for research purposes,
e.g., to characterize events which occur at a rate that is
higher than traditional cameras are capable of record1

been applied in many practical applications, e.g., accelerating magnetic resonance imaging (MRI) [12]. While
traditional signal acquisition methods follow a samplethen-compress procedure, CS could perform compression along with sampling. The novel acquisition strategy has enabled low-cost on-sensor data compression,
relieving the pain for high transmission bandwidth and
large storage space. In the recent decade, many algorithms have been proposed [3, 15, 1, 4, 20, 2] to solve
the CS reconstruction problem. Generally, these reconstruction algorithms are based on either optimization or
greedy approaches using signal sparsity as prior knowledge. As a result, they all suffer from high computational
complexity, which requires seconds to minutes to recover
an image depending on the resolution. Therefore, these
sparsity-based methods cannot satisfy the real-time decoding need of high-frame-rate cameras, and they are not
appropriate for the high-frame-rate video CS application.
The slow reconstruction speed of conventional CS approaches motivates us to directly model the inverse mapping from compressed domain to original domain, which
is shown in Figure 1. Usually, this mapping is extremely
complicated and difficult to model. However, the existence of massive unlabeled video data gives a chance to
learn such a mapping using data-driven methods. In this
paper, we design an enhanced Recurrent convolutional
neural network (RCNN) to solve this problem. RCNN has
shown astonishingly good performance for video recognition and description [5, 21, 23, 19]. However, conventional RCNNs are not well suited for video CS application, since they are mostly designed to extract discriminant features for classification related tasks. Simultaneously improving compression ratio (CR) and preserving
visual details for high-fidelity reconstruction is a more
challenging task. In this work, we develop a special
RCNN, called “CSVideoNet”, to extract spatial-temporal
features, including background, object details, and motions, to significantly improve the compression ratio and
recovery quality trade-off for video CS application over
existing approaches.
The contributions of this paper are summarized as follows:

y = Φf

f = Ψθ
Synthesize DL

y

θ

f
Analysis DL

Compressed
Domain

?

Signal
Domain

θ = Ωf

Coefficient
Domain

Figure 1: The illustration of bridging the gap between
compressed domain and signal domain.
videos to the original input with no pre/postprocessing. To the best of our knowledge, there has
been no published work that addresses this problem
using similar methods.
• We propose a multi-level compression strategy to
improve CR with the preservation of high-quality
spatial resolution. Besides, we perform implicit motion estimation to improve temporal resolution. By
combining both spatial and temporal features, we
further improve the compression ratio and recovery
quality trade-off without increasing much computational complexity.
• We demonstrate CSVideoNet outperforms the reference approaches not only in recovery quality but
also in reconstruction speed because of its noniterative nature. It enables real-time high-fidelity
reconstruction for high-frame-rate videos at high
CRs. We achieve state-of-the-art performance on
the large-scale video dataset UCF-101. Specifically,
CSVideoNet reconstructs videos at 125 fps on a Titan X GPU and achieves 25dB PSNR at a 100x CR.

2

Related Works

There have been many recovery algorithms proposed for
CS reconstruction, which can generally be categorized as
follows:
Conventional Model-based CS Recovery: In [16],
the authors model the evolution of scenes as a linear dynamical system (LDS). This model comprises two submodels: the first is an observation model that models
frames of video lying on a low-dimensional subspace;
the second predicts the smoothly varied trajectory. The

• We propose an end-to-end and data-driven framework for video CS. The proposed network directly
learns the inverse mapping from the compressed
2

model performs well in stationary scenes, however, inadequate for non-stationary scenes.
In [25], the authors use Gaussian mixture model
(GMM) to recover high-frame-rate videos, and the reconstruction can be efficiently computed as an analytical solution. The hallmark of the algorithm is that it adapts
temporal compression rate based upon the complexity of
the scene. The parameters in GMM are trained offline and
tuned during the recovery process.
In [17], the authors propose a multi-scale video recovery framework. It first obtains a low-resolution video
preview with very low computational complexity, then it
exploits motion estimates to recover the full-resolution
video using an optimization algorithm. In a similar
work [8], the authors propose a motion-compensated
and block-based CS with smooth projected Landweber (MC-BCS-SPL) reconstruction algorithm, which estimates a motion vector from a reference and a reconstructed frame. The reconstructed video is derived by
the combination of the low-resolution video and the motion vector. The drawback of the two works is the need
to specify the resolution at which the preview frame is
recovered, which requires the prior knowledge of object speed. Also, the recovery performance is highly
dependent on the quality of motion estimation, which
is difficult to obtain accurately. Furthermore, the explicit motion estimation has a high runtime cost which
makes this model inadequate for reconstructing highframe-rate videos.
Neural Network Based CS Recovery: In [14], the
authors propose a stacked autoencoder to learn a representation of the training data and to recover test data
from their sub-sampled measurements. Compared to the
conventional iterative approaches, which usually need
hundreds of iterations to converge, the feed-forward
deep neural network runs much faster in the inference
stage.
In [11], the authors propose a convolutional neural
network, which takes CS measurements of an image as
input and outputs an intermediate reconstruction. The
intermediate output is fed into an off-the-shelf denoiser
to obtain the final reconstructed image. The author
shows the network is highly robust to sensor noise and
can recover visually higher quality images than competitive algorithms at low CRs of 10 and 25. Both [14] and
[11] are designed for image reconstruction, which only

focus on spatial feature extraction. For video applications, temporal features between adjacent frames are also
important. Therefore, the overlook of temporal correlation makes the image reconstruction algorithms inadequate for video applications.
In [9] 1 , the authors propose a Video CS reconstruction
algorithm based on a fully-connected neural network.
This work focuses on temporal CS where multiplexing
occurs across the time dimension. A 3D volume is reconstructed from 2D measurements by a feed-forward process. The author claims the reconstruction time for each
frame can be reduced to about one second. The major
drawback of this work is that the algorithm is based on
a plain fully-connected neural network, which is not efficient in extracting temporal features.

3
3.1

Methodology
Overview of the Proposed Framework
for Video CS

Two kinds of CS cameras are being used today. Spatial multiplexing cameras (SMC) take significantly fewer
measurements than the number of pixels in the scene to
be recovered. SMC has low spatial resolution and seeks
to spatially super-resolve videos. In contrast, temporal
multiplexing cameras (TMC) have high spatial resolution
but low frame-rate sensors. Due to the missing of inter frames, extra computation is needed for motion estimation. For these two sensing systems, either spatial
or temporal resolution is sacrificed for achieving a better spatial-temporal trade-off. To solve this problem, we
propose a new sensing and reconstruction framework,
which combines the advantage of the two systems. The
random video measurements are collected by SMC with
very high temporal resolution. To compensate for the
low spatial resolution problem in SMC, we propose a
multi-CR strategy. The first key frame in a group of pictures (GOP) is compressed with a low CR, and the remaining non-key frames are compressed with a high CR.
The spatial features in the key frame are reused for the
recovery of the entire GOP due to the high inter-frame
correlation in high-frame-rate videos. The spatial reso1 This work is submitted to arXiv.org, not peer reviewed and published yet.

3

lution is hence improved. The RNN extrapolates motion
from high-resolution frames and uses it to improve the
temporal resolution. Therefore, a better compression ratio and spatial-temporal resolution trade-off are obtained
by the proposed framework.
The overall architecture of the proposed video CS reconstruction framework is shown in Figure 2. The network contains three modules: 1) an encoder (sensing matrix) for simultaneous sampling and compression; 2) For
each compressed frame data, there is a dedicated CNN for
spatial features extraction; 3) An LSTM for motion estimation and video reconstruction. As mentioned earlier,
to improve the spatial resolution, the random encoder encodes the first frame in a GOP with more measurements
and the remaining with less. In addition, a recent research [24] shows that sensing matrix can be trained with
raw data to better preserve the Restricted Isometry Property (RIP). Therefore, the encoder can also be integrated
into the entire model and trained with the whole network to improve reconstruction performance. Besides,
as the proposed algorithm eliminates the sparsity prior
constraint, the direct optimization of RIP preservation
in [24] is not necessary. Instead, we can use the reconstruction loss to train the sensing matrix along with the
model. For simplicity, we still use a random Bernoulli
matrix for information encoding in the experiment. Different from the prior work that extracts motion from
low-resolution previews, the proposed LSTM network
infers motion from high-resolution frames generated by
the multi-rate CNNs. The resolution of the reconstructed
video is further improved with the incorporation of highquality motion estimation.
3.1.1

lates the missing information to obtain high-dimension
data. Instead, we design a special CNN suitable for CS
reconstruction, which has the best recovery performance
among all the tested structures mentioned above. The
overall network structure is shown in Figure 3. All feature maps have the same size as the reconstructed video
blocks, and the number of feature maps decrease monotonically. This process resembles the sparse coding stage
in CS, where a subset of dictionary atoms is combined to
form the estimation of the original input. There is a fullyconnected (FC) layer, denoted in gray color in Figure 3,
which connects CNN layers to convert vectorized mdimensional video data to 2D features maps. To reduce
the latency of the system and to simplify the network architecture, we use video blocks as input and set the block
size n to 32×32. All convolutional layers are followed
by a ReLU layer except for the final layer. We pre-train
the eight-layer key CNN for processing the first frames in
GOPs that are compressed with a low CR. For other nonkey frames compressed with a high CR, we use 3-layer
small non-key CNNs to handle them since they carry information of low entropy. All the hyper-parameters in
the model are carefully selected to better fit the dataset.
All the weights of non-key CNNs are shared. Hence the
proposed framework can be easily generalized for other
high-frame-rate video applications that require a larger
number of non-key frames. One should note that the pretraining of the key CNN is critical for improving the reconstruction performance. We tested a case where the
whole network is trained from scratch without any pretraining, in which case the convergence performance is
bad. The reason is in large part because of the vanishing gradient problem, since we have a long path from
the CNNs to the LSTM. The pre-training greatly alleviate
this problem.

Multi-rate CNN Variant for Compression
Ratio Enhancement

Typical CNN architectures used for recognition, classification, and segmentation that map input to rich hierarchical visual features is not applicable to the reconstruction problem. The goal of the CNN is not only to
extract spatial visual features, but also to preserve details as much as possible. Therefore, we eliminated the
pooling layer which causes information loss. In addition,
we discard the convolution-deconvolution architecture,
widely used for segmentation tasks, which first encodes
salients into low-dimension features and then interpo-

3.1.2

Motion-estimating Synthesizing LSTM for
Spatial-temporal Resolution Enhancement

The proposed framework is end-to-end trainable, computationally efficient, and requires no pre/post-processing.
This is achieved by performing motion estimation implicitly, which is different from prior works [17, 25, 8].
We utilize an LSTM network to extract motion features
from the reconstructed frames of CNNs that are critical
for improving temporal resolution. Due to the informa4

32

32
32

32
3
3

Key Frame 1

Random Encoder
CR=M

Re
LU

3
3

32

32

32

32

3
Re 3
LU

3
Re 3
LU

3
3
Re
LU

32

.
..

Random Encoder
CR=N (N>>M)

Random Encoder
CR=N (N>>M)

1

3
3

16*32*32

6*32*32

6*32*32

1*32*32

32
32

3
3

ReLU

.
.
.

32
32

3
3

32

32
ReLU

64

16*32*32

32
32

3
3

ReLU

3
3

32
ReLU

3
3

6*32*32

1*32*32

2

.
.
.

32
3
3

6*32*32

1~T

...

Non-key
Frame T

.

Re 3 Re
LU
LU

...

.

.

32
3

Key CNN
32

Non-key
Frame K+1

32

32

32

3
Re 3

LU

32

32

32

32

16*32*32

6*32*32

6*32*32

1*32*32

T

32

Synthesizing LSTM

Non-key CNN

Figure 2: The overall architecture of the proposed framework. The compressed video frames are acquired by compressive sensing. The reconstruction is performed by the CSVideoNet consisting of a key CNN, multiple non-key
CNNs, and a synthesizing LSTM. In every T frames, the first one and the remaining (T-1) frames are compressed
with a low and high CR, respectively.
Using MSE as the loss function favors high PSNR.
PSNR is a commonly used metric to quantitatively evaluate recovery quality. From the experiment results, we
illustrate that PSNR is partially correlated to the perceptual quality. To derive a better perceptual similarity metric will be a future work. The proposed framework can
be easily adapted to a new loss function.
Three training algorithms, i.e., SGD, Adagrad [7] and
Adam [10] are compared in the experiment. Although
consuming the most GPU memory, Adam converges towards the best reconstruction results. Therefore, Adam
is chosen to optimize the proposed network.

tion flow from the first LSTM node to the remaining, the
LSTM will implicitly infer representations for the hidden
motion from the key frames with relatively high-quality
reconstructed by CNNs. Therefore, the recovery quality of the GOP is improved by the aggregation of motion
and spatial visual features. That is why we call this network the motion-estimation synthesizing LSTM. For simplicity, each input LSTM node in the experiment accepts
input data with equal length. In fact, since the non-key
frames carry less information than the key frame, the
LSTM network can be designed to accept inputs with
variable lengths. Hence, we can reduce the model size
and get a faster reconstruction speed. From the experiment results, we find the utilization of the LSTM network
is critical to improving recovery fidelity. As a result, our
model outperforms the competitive algorithms by a significant margin.

4

Experiment

As there is no standard dataset designed for video CS,
we use UCF-101 dataset introduced in [18] to benchmark
3.2 Learning Algorithm
the proposed framework. This dataset consists of 13k
Given the ground-truth video frames x{1,··· ,T } and the clips and 27 hours of video recording data collected from
corresponding compressed frames y{1,··· ,T } , we use YouTube, which belong to 101 action classes. Videos in
mean square error (MSE) as the loss function, which is the dataset have a resolution of 320×240 and are sampled
at 25 fps. We retain only the luminance component of
defined as:
the extracted frames and crop the central 160×160 patch
T
from each frame. These patches are then segmented into
1 X
L(W, b) =
kf (yi ; W, b) − xi k22 ,
(1) 32×32 non-overlapping image blocks. We get 499,760
2N i
GOPs for training and testing in total. We set three test
where W and b are the network weights and biases, re- cases with three CRs of 25, 50 and 100, respectively. Since
spectively.
the size of each input image block is 32 × 32, the cor5

Table 1: Summary of all baselines and the proposed approach.

tional iterative algorithms developed for CS, e.g., matching pursuit, orthogonal mating pursuit, iterative hardthresholding. It offers state-of-the-art recovery perforDenoising-based
Iterative Based
D-AMP [13] mance and operates tens of times faster compared to
approximate message passing
Image CS
Stacked denoising autoencoder
SDA [14]
other iterative methods [13]. Both SDA and ReconNet
Non-iterative Based
Convolutional neural network
ReconNet [11]
are DNN-based reconstruction approaches for images,
Motion-compensated blockbased CS with smooth
MC-BCS-SPL [8] which are developed recently. ReconNet is based on a
Iterative Based
projected Landweber
Video CS
CNN model and has state-of-the-art performance among
Gaussian mixture model
GMM [25]
Fully-connected neural network
VCSNet [9]
all image CS reconstruction algorithms [11]. We tested
Non-iterative Based
Proposed approach
CSVideoNet
both frame-based and block-based D-AMP, which reconstructs the entire frame and an image block at a time,
responding length of measurements is 40, 20 and 10, re- respectively. Other approaches are tested in a blockspectively. The dimension of data for pre-training the key based pattern since it offers much higher throughput.
CNN is (N × C × H × W ), where N =100 is the batch The quantized results of average PSNR, SSIM, and MAE
size, C=1 is the channel size, and H=32 and W =32 is the for each method under different CRs are shown in Taheight and width of each image block, respectively. The ble 2. It is shown that CSVideoNet outperforms the refdimension of the data used for training the entire model erence approaches on all the three metrics by a meanis (N 0 × T × C × H × W ), where T =10 is the sequence ingful margin, especially at the CR of 100. The MAE of
length for one GOP, and N 0 =20 is the batch size. The CSVideoNet is 4.59 at a 100x CR which means the averother dimensions are the same. We shrink the batch size aged pixel-wise distortion is only 4.59/255 = 1.2% comhere because of the GPU memory limitation. In every 10 pared to the ground-truth video. The PSNR drop from the
consecutive video frames, we define the first one as the CR of 25 to 100 is also calculated. We found the proposed
approach suffers from the least performance degradation
key frame, and the remaining as non-key frames.
among all. This is in large part due to the feature sharing between the key and non-key frames when the com4.1 Comparison with the State-of-the-art pressed input carries limited information.
An example reconstructed frame by each approach is
We compare our algorithm with six reference work for
shown
in Figure 4 for visual quality assessment. The
CS reconstruction: [25, 8, 14, 13, 11, 9]. We summarize
ground-truth
frame is shown in Figure 5. We can
all baseline approaches and our approach in Table 1. For
see
that
CSVideoNet
provides the finest details among
a fair comparison, we also re-train reference algorithms
all
approaches.
The
edges
produced by CSVideoNet is
using UCF-101 dataset. Three metrics: Peak signal-tomuch
sharper,
while
such
details
are no longer preserved
noise ratio (PSNR), structural similarity (SSIM) [22], and
by
other
methods
after
reconstruction.
This comparipixel-wise mean absolute error (MAE) are applied for
son
demonstrates
that
the
temporal
correlation
is critical
performance evaluation. Note that MAE is the averaged
for
video
reconstruction,
the
overlook
of
such
features
absolute error of each pixel value within the range of
[0,255], which gives a straightforward measure of the will significantly degrade the recovery quality of videos.
pixel-wise distortion. The authors of VCSNet only offer a Therefore, the conventional image CS approaches are not
model pre-trained with the CR of 16, without providing suitable for video applications.
sufficient training details to reproduce the experiment.
Therefore, we train the proposed model and compare it 4.2 Comparison with Video CS Apwith CVSNet at a single CR of 16 only.
proaches
4.1.1

We compare the proposed CSVideoNet with existing
video CS approaches. MC-BCS-SPL estimates motion directly from the current and the reference frame. GMM
models the spatial-temporal correlation by assuming all

Comparison with Image CS Approaches

We first compare with the algorithms used for image CS
reconstruction. D-AMP is a representative of the conven6

32

32
32

3
3

32

3
3

3
3

128

32

64

32

32

32

ReLU

ReLU

Random Encoder
CR=M

32

32

3
3
ReLU

32

ReLU

Key CNN

32
32

32

32

3
3

3

3
ReLU

32

3
3
ReLU

16

32
32
3
3

ReLU

16

Figure 3: Process for the pre-training of the key CNN.
Table 2: Performance comparison with image CS reconstruction approaches.

PSNR

SSIM

MAE
PSNR↓

CR
25
50
100
25
50
100
25
50
100
25 → 100

D-AMP(F)
25.34
12.49
7.17
0.76
0.08
0.03
4.65
64.30
92.12
72%

D-AMP(B)
15.1494
9.1719
8.0942
0.0934
0.0249
0.0067
24.92
81.67
86.04
13%

SDA
23.39
21.96
20.40
0.69
0.65
0.61
5.76
6.60
8.50
47%

ReconNet
24.27
22.47
20.44
0.73
0.67
0.61
5.02
5.67
7.42
16%

Table 3: Performance comparison with video CS reconstruction approaches.

CSVideoNet
26.87
25.09
24.23
0.81
0.77
0.74
3.38
4.31
4.59
10%

PSNR

SSIM

MAE
PSNR↓

pixels within a video patch are drawn from a GMM distribution. GMM has the state-of-the-art performance
among conventional model-based video CS approaches
[25]. To the best of our knowledge, [9] is the only DNNbased work proposed for video CS. The quantized results
of average PSNR, SSIM, and MAE for each method under different CRs are shown in Table 3. It is observed
that the proposed approach improves PSNR by 3 to 5dB
over the reference methods. Specifically, we find MCBCS-SPL and GMM have similar performance and perform much better than the model-based image CS approach, D-AMP. However, their performance are similar
to SDA and ReconNet, which are designed for processing images. This implies that the conventional modelbased methods suffer from limited performance due to
the limited model capacity when dealing with large-scale
problem. Even though they consider the temporal correlation among video frames, the model capacity is insufficient for visual patterns. To improve performance,
one could increase the size of the conventional models.
However, the computational complexity for these methods will also increase substantially, inhibiting their application to video CS.
Deep neural networks (DNNs) provide a viable solution. Both CSVideoNet and VCSNet are DNNs that are

CR
25
50
100
25
50
100
25
50
100
25 → 100

MC-BCS-SPL
22.41
20.59
19.67
0.37
0.30
0.19
11.88
16.03
28.86
26%

GMM
23.76
21.26
19.64
0.72
0.61
0.54
5.14
7.50
9.37
17%

CSVideoNet
26.87
25.09
24.23
0.81
0.77
0.74
3.38
4.31
4.59
10%

Table 4: Performance comparison with VCSNet at a CR
of 16.

PSNR
SSIM
MAE

VCSNet
25.07704
0.817669
3.887867

CSVideoNet
28.078
0.8431
2.9452

designed for video CS reconstruction. For reasons explained earlier, we compare the two approaches at a CR
of 16. The results are shown in Table 4 and Figure 5.
Both the two approaches achieve high recovery quality
compared to other baselines. However, VCSNet is a plain
fully-connect network that has limited capability for processing sequential data. As a result, it suffers from a lowquality motion estimation, which explains why it has inferior performance compared to the proposed solution.
To illustrate that the performance improvement of the
proposed approach comes from integrating temporal features through the LSTM network rather than simpling
increasing the model size, we set another experiment, in
which we compare the performance of two CNNs with
7

28.49dB

21.43dB

22.04dB

23.57dB

23.43dB

28.86dB

24.52dB

18.26dB

20.52dB

23.59dB

21.19dB

27.92dB

5.78dB
5.78dB

17.04dB

18.05dB

23.49dB

18.43dB

27.84dB

D-AMP

SDA

MC-BCS-SPL

GMM

CSVideoNet

(a)

(b)

(c)

ReconNet

Figure 4: Reconstruction result for each method at the CR of (a) 25, (b) 50, and (c) 100, respectively.

8

Table 6: The structure of CNN1 and CNN2.

27.14dB

29.46dB
# Layer
CNN1
CNN2
*

VCSNet

CSVideoNet

Table 5: Comparison of CSVideoNet and CNN-based approaches.

PSNR

SSIM

MAE

CNN1
24.27
22.47
20.44
0.73
0.67
0.61
5.02
5.67
7.42

CNN2
23.74
22.17
20.10
0.69
0.65
0.58
6.46
6.23
8.92

3
64
256

4
32
256

5
32
128

6
16
128

7
16
64

8
1
64

9

10

11

12

13

32

32

16

16

1

Performance Under Noise

To demonstrate that the robustness of CSVideoNet to
sensor noise, we conduct a reconstruction experiment
with input videos contaminated by random Gaussian
noise. In this experiment, the architecture of all DNNbased frameworks remains the same as in the noiseless
case. We test the performance at three levels of SNR 20dB, 40dB, and 60dB. For each noise level, we evaluate
all approaches at three CRs of 25, 50, and 100. The average PSNR achieved by each method at different CRs and
noise levels are shown in Figure 6. It can be observed that
CSVideoNet can reliably achieve a high PSNR across at
different noise levels and outperform the reference methods consistently.

Figure 5: Comparison with VCSNet at the CR of 16.

CR
25
50
100
25
50
100
25
50
100

2
128
512

CNN1 is used in CSVideoNet. The dimension of all feature maps in both CNNs are 32×32.

4.3

Ground Truth

1
1
1

CSVideoNet
26.87
25.09
24.23
0.81
0.77
0.74
3.38
4.31
4.59

4.4

Time Complexity

We benchmark the runtime performance of different
methods. Due to the iterative nature of conventional
CS algorithms (D-AMP, MC-BCS-SPL, GMM), they suffer
from high data-dependency and low parallelism, which
is not suitable for GPU acceleration. Due to the lack of
GPU solvers for these algorithms, we run these reference algorithms on an octa-core Intel Xeon E5-2600 CPU.
Benefiting from the feedforward data-path and high data
concurrency of DNN-based approaches, we accelerate
CSVideoNet and other DNN-based baselines using a
Nvidia GTX Titan X GPU. The time cost for fully reconstructing a video frame in the size of (160×160) are compared in Table 7. It is shown that CSVideoNet consumes 8
milliseconds (125 fps) to reconstruct a frame at the CR of
100. This is three orders of magnitude faster than the reference methods based on iterative approaches. The time
cost of VCSNet and CSVideoNet at the CR of 16 is 3.5 and
9.7 milliseconds, respectively. Through further hardware
optimization, we believe CSVideoNet has the potential
to be integrated into CS cameras to enable the real-time
reconstruction of high-frame-rate video CS.

different sizes. The structure of the two CNNs are shown
in Table 6, and the performance comparison is shown
in Table 5. We can see that simply increasing the size
of CNN does not provide meaningful improvement for
reconstruction. This can be explained by the incapability of CNN to capture temporal features. The incorporation of the LSTM network improves the PSNR by up
to 4 dB, which represents more than twice of error reduction. Specifically, the performance improvement increases with the CR and achieves the maximum when CR
is 100. This explains that the implicit motion estimation
by LSTM is critical to the video CS reconstruction especially at high CRs.
9

Table 7: Runtime comparison for reconstructing a
160×160 video frame at different CRs.

CR = 25
27

Model
D-AMP(F)
D-AMP(B)
SDA
ReconNet
MC-BCS
GMM
CSVideoNet

26
25

24
23
22
20

40

CR = 50

60

Inf

5

20

15

40

CR = 100

60

Inf

25
20
15
10
20

40
D-AMP
MC-BCS-SPL

60
SDA
GMM

Inf
ReconNet
CSVideoNet

Figure 6: PSNR comparison at different SNRs.

CR=50
41.20
8.5498
0.027
0.063
8.03
10.54
0.0085

CR=100
31.74
8.4433
0.023
0.061
9.00
18.34
0.0080

Conclusion

In this paper, we present a real-time, end-to-end, and
non-iterative framework for high-frame-rate video CS.
A multi-rate CNN variant and a synthesizing LSTM network are developed to jointly extract spatial-temporal
features. This is the key to enhancing the compression ratio and recovery quality trade-off. The magnificent model capacity of the proposed deep neural network allows to map the inverse mapping of CS without
exploiting any sparsity constraint. The feed-forward and
high-data-concurrency natures of the proposed framework are the key to enabling GPU acceleration for realtime reconstruction. Through performance comparison,
we demonstrate that CSVideoNet has the potential to
be extended as a general encoding-decoding framework
for high-frame-rate video CS applications. In the future work, we will exploit the effective learning methods to decode high-level information from compressed
videos, e.g., object detection, action recognization, and
scene segmentation.

25

20

CR=25
38.37
8.4652
0.0278
0.064
7.17
8.87
0.0094

10

References

[15] D. Needell and J. A. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. ACM Communications, 53(12):93–100, 2010. 2

[1] A. Beck and M. Teboulle. A fast iterative shrinkagethresholding algorithm for linear inverse problems. SIAM
Journal on Imaging Sciences, 2(1):183–202, 2009. 2

[16] A. Sankaranarayanan, P. Turaga, R. Chellappa, and
R. Baraniuk. Compressive acquisition of linear dynamical systems. SIAM Journal on Imaging Sciences, 6(4):2109–
2133, 2013. 2

[2] T. Blumensath and M. E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational
Harmonic Analysis, 27(3):265 – 274, 2009. 2

[17] A. C. Sankaranarayanan, L. Xu, C. Studer, Y. Li, K. F. Kelly,
and R. G. Baraniuk. Video compressive sensing for spatial
multiplexing cameras using motion-flow models. SIAM
Journal on Imaging Sciences, 8(3):1489–1518, 2015. 3, 4

[3] E. J. Candès, J. Romberg, and T. Tao. Robust uncertainty
principles: exact signal reconstruction from highly incomplete frequency information. IEEE TIT, 52(2):489–509,
Feb 2006. 2

[18] K. Soomro, A. Roshan Zamir, and M. Shah. UCF101: A
Dataset of 101 Human Actions Classes From Videos in The
Wild. In CRCV-TR-12-01, 2012. 5

[4] I. Daubechies, R. DeVore, M. Fornasier, and C. S. Gntrk. Iteratively reweighted least squares minimization for
sparse recovery. Communications on Pure and Applied
Mathematics, 63(1):1–38, 2010. 2

[19] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsupervised learning of video representations using lstms. In
ICML, 2015. 2

[5] J. Donahue, L. A. Hendricks, M. Rohrbach, S. Venugopalan, S. Guadarrama, K. Saenko, and T. Darrell. Longterm recurrent convolutional networks for visual recognition and description. In CVPR, 2015. 2

[20] J. A. Tropp and A. C. Gilbert. Signal recovery from random
measurements via orthogonal matching pursuit. IEEE TIT,
53(12):4655–4666, 2007. 2

[6] M. F. Duarte, M. A. Davenport, D. Takbar, J. N. Laska,
T. Sun, K. F. Kelly, and R. G. Baraniuk. Single-pixel imaging via compressive sampling. IEEE Signal Processing Magazine, 2008. 1

[21] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney,
T. Darrell, and K. Saenko. Sequence to sequence - video
to text. In ICCV, 2015. 2

[7] J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient
methods for online learning and stochastic optimization.
JMLR, 12:2121–2159, 2011. 5

[22] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600–612, April 2004. 6

[8] J. E. Fowler, S. Mun, and E. W. Tramel. Block-based compressed sensing of images and video. Foundations and
Trends in Signal Processing, 4(4):297–416, 2012. 3, 4, 6

[23] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In ICML,
2015. 2

[9] M. Iliadis et al. Deep Fully-Connected Networks for Video
Compressive Sensing. CoRR, abs/1603.04930, 2016. 3, 6, 7

[24] K. Xu, Y. Li, and F. Ren. A Data-Driven Compressive
Sensing Framework Tailored For Energy-Efficient Wearable Sensing. In ICASSP, 2017. 4

[10] D. P. Kingma and J. Ba. Adam: A Method for Stochastic
Optimization. In ICLR, 2015. 5

[25] J. Yang, X. Yuan, X. Liao, P. Llull, D. J. Brady, G. Sapiro,
and L. Carin. Video compressive sensing using gaussian
mixture models. IEEE TIP, 23(11):4863–4878, 2014. 3, 4, 6,
7

[11] K. Kulkarni, S. Lohit, P. Turaga, R. Kerviche, and A. Ashok.
Reconnet: Non-iterative reconstruction of images from
compressively sensed measurements. In CVPR, 2016. 3,
6
[12] S. Ma, W. Yin, Y. Zhang, and A. Chakraborty. An efficient
algorithm for compressed mr imaging using total variation and wavelets. In CVPR, 2008. 2
[13] C. A. Metzler, A. Maleki, and R. G. Baraniuk. From denoising to compressed sensing. IEEE TIT, 62(9):5117–5144,
Sept 2016. 6
[14] A. Mousavi et al. A Deep Learning Approach to Structured
Signal Recovery. CoRR, abs/1508.04065, 2015. 3, 6

11

AN ENERGY-EFFICIENT COMPRESSIVE SENSING FRAMEWORK INCORPORATING
ONLINE DICTIONARY LEARNING FOR LONG-TERM WIRELESS HEALTH MONITORING
Kai XU, Yixing Li, Fengbo Ren
Parallel Systems and Computing Laboratory (PSCLab)
Arizona State University
ABSTRACT

storage and data analysis. These technology advancements
will eventually transform the existing model of health related
services to continuous monitoring for disease prediction and
prevention [2]. Such a wireless health revolution will make
healthcare systems more effective and economic, beneﬁting
billions of individuals and the society they live in.
One of the key challenges faced by the long-term wireless health monitoring is the energy efﬁciency of sensing and
information transfer. Due to the limited battery capacity of
WSNs, continuous sensing inevitably increases the frequency
of battery recharging or replacement, making it less convenient for practical usage. In the WSNs for bio-sensing applications, the energy cost of wireless transmission is about 2 orders of magnitude greater than other operations (e.g., analogto-digital conversion (ADC)). State-of-the-art radio transmitters exhibit energy efﬁciency in the nJ/bit range while every
other component consumes at most tens of pJ/bit [3]. Therefore, reducing the data size for information transfer is the key
to improve energy efﬁciency.
The CS framework [1, 4] offers a universal and simple
data encoding scheme that can compress a variety of physiological signals, providing a viable solution to realizing
energy-efﬁcient WSNs for long-term wireless health monitoring. However, the compression ratio (CR) demonstrated
by existing frameworks is limited given a signal recovery
quality required for diagonosis purposes. In [5, 6] percent
root-mean-square difference (PRD) of 8.5% and 9% is reported at a CR of 5x and 2.5x for ECG signals, respectively.
These frameworks all deal with the sparsity of physiological
signals on pre-determined bases and fail to take into account
the individual variability in signals that is critical to exact
signal recovery.
In this paper, we propose an energy-efﬁcient data acquisition framework, customized for the long-term electrocardiogram (ECG) monitoring, which exploits online dictionary
learning (ODL) on server nodes to train personalized bases
that capture the individual variability for further improving
the sparsity of ECG signals. By incorporating such prior
knowledge into signal recovery, the CS performance in terms
of accuracy-CR trade-off is signiﬁcantly enhanced, leading to
further data size reduction and energy saving on sensor nodes.
Additionally, the proposed framework does not require any

Wireless body area network (WBAN) is emerging in the mobile healthcare area to replace the traditional wire-connected
monitoring devices. As wireless data transmission dominates
power cost of sensor nodes, it is beneﬁcial to reduce the data
size without much information loss. Compressive sensing
(CS) is a perfect candidate to achieve this goal compared to
existing compression techniques. In this paper, we proposed
a general framework that utilize CS and online dictionary
learning (ODL) together. The learned dictionary carries individual characteristics of the original signal, under which
the signal has an even sparser representation compared to
pre-determined dictionaries. As a consequence, the compression ratio is effectively improved by 2-4x comparing
to prior works. Besides, the proposed framework ofﬂoads
pre-processing from sensor nodes to the server node prior
to dictionary learning, providing further reduction in hardware costs. As it is data driven, the proposed framework has
the potential to be used with a wide range of physiological
signals.
Index Terms— Compressive sensing, online dictionary
learning, wireless sensor nodes (WSNs), wireless health.
1. INTRODUCTION
The existing heathcare model of the medical system is based
on episodic examination or short-term monitoring for disease diagnosis and treatment. The major issues in such a
system are the overlook of individual variability and the lack
of personal baseline data, due to limited frequency of measurements. Continuous or non-intermittent monitoring is the
key to create big data of individual health record for studying
the variability and obtaining the personal baseline. Recent
advancements in wireless body area networks (WBAN) and
bio-sensing techniques has enabled the emergence of miniaturized, non-invasive, cost-effective wireless sensor nodes
(WSNs) that can be deployed on the human body for personal health and clinical monitoring [1]. Through WBAN,
the monitored data can be transmitted to a near-ﬁeld mobile
aggregator for on-site processing. Through Internet infrastructures, the data can be uploaded to remote servers for

,(((



,&$663

2.3. ODL

pre-processing stages on sensor nodes. Alternatively, high
reconstruction quality is enforced by pre-processing training
data prior to the dictionary learning stage, to eliminate the impact of noise and interference on trained bases, enabling simpler and more cost-effective sensor structures. Experimental
results based on MIT-BIH database show that our framework
is able to achieve an average PRD of 9% at a CR of 10x.
This indicates that our framework can achieve 2-4x additional
energy saving on sensor nodes (for the same reconstruction
quality) compared to the reference designs [1, 5, 6, 7]. Due
to the training and personalization of the dictionary, the proposed framework has the potential to be generally applied to
a wide range of physiological signals.

Assuming the training set is composed of i.i.d. samples following a distribution p(x), ODL draws one sample xt at a
time and alternates between the sparse coding stage and dictionary update stage.
2.3.1. Sparse Coding
The sparse coding problem is a l1 -regularized least-squares
problem deﬁned as
1
αt = arg min xt − Dt−1 α22 + λα1 .
n
2
α∈R

2. BACKGROUND

Due to the high correlations between columns of the
dictionary, a Cholesky-based implementation of the LARSLasso algorithm, which provides the whole regularization
path, is chosen here to solve the sparse coding problem [16].

2.1. Compressive Sensing
Assuming a signal f ∈ Rn can be well represented by a sparse
vector x ∈ Rk on a certain basis Ψ ∈ Rn×k as f = Ψx, then
the signal information can be well preserved by projecting f
onto a random domain through a sensing matrix Φ ∈ Rm×n
(m<n) [8], given as
y = Φf = ΦΨx.

2.3.2. Dictionary Updating
At this stage, the objective is to ﬁnd a dictionary D that satisﬁes:

(1)

t

11
xi − Dαi 22 + λαi 1 .
Dt = arg min
t
2
D
i=1

Candes and et al. [9] has proven that one has a high probability to recover the sparse coefﬁcient x by solving the basis
pursuit (BP) problem deﬁned as
min x1

x∈Rk

s.t.

y − ΦΨx2 ≤ ε,

(3)

(4)

The problem in (4) can be solved by the block coordinate
descent algorithm [16]. Overall, the detailed procedure for
ODL algorithm is summarized in Algorithm 1.

(2)

where ε is an error tolerance term for enhancing the accuracy
of the solution considering noise.

Algorithm 1 Pseudocode for ODL
Input: Input data x ∈ Rn ∼ p(x), initial dictionary D0 ∈
Rn×k , number of iterations t.
Output: Learned dictionary Dt .
Steps:
1: Set A0 ← 0, B0 ← 0.
2: For t=1:T
3: Draw a new sample xt from p(x).
4: Sparse coding: ﬁnd a sparse coefﬁcient of xt under
current dictionary Dt−1 .
5: At ← At−1 + αt αtT .
6: Bt ← Bt−1 + xt αtT .
7: Dictionary update: update dictionary Dt−1 column
by column, the j-th column is given by
8: For j=1:k
9:
dj ← (At1)jj (Bt (:, j) − DAt (:, j)) + dj .
if dj 2 > 1, then normalize it to unit form.
10: end for
11. end for
12. Return Dt .

2.2. Dictionary Learning
Learning dictionaries from data instead of using off-the-shelf
bases has been proved effective in improving signal reconstruction performance for images [10]. The most recent dictionary learning algorithms [11, 12, 13] are second-order iterative batch procedures that access the whole training set
at each iteration in order to minimize a cost function under
certain constraints. Although these algorithms [11, 12, 13]
have been shown experimentally faster than ﬁrst-order gradient descent methods, they cannot effectively handle very
large training sets [14], because of the involved matrix factorization upon the entire training data. To be able to deal with
large data sets for long-term monitoring, the ODL algorithm
is adopted in our framework. Compared to the methods mentioned above, ODL has a higher training speed and requires
less storage space [15] because of its elimination of large matrix factorizations. With ODL, it is possible to add new features into the dictionary without stalling the reconstruction,
which offers a mechanic of amelioration when a distinctive
input is received.



Fig. 1: Block diagram of the proposed framework. The parameter sweeping and dictionary training procedure are executed
on servers. The reconstruction process is performed on mobile platform for providing timely feedback. The random encoding
process using random Bernoulli matrix is embedded into the sensor node for effective data compression and energy saving.

3. THE PROPOSED FRAMEWORK

prior to random encoding on the sensor node. Instead, a simple segmentation module is sufﬁcient for clean reconstruction.
The initialization in dictionary learning is important. A
poorly initialized dictionary may contain bad atoms that are
never used [16]. Generally, the dictionary can be initialized
by random numbers or input data. For more difﬁcult and regularized problem, it is preferable to start from a less regularized
case and gradually increase the regularization coefﬁcients. In
our framework, the dictionary is initialized by randomly chosen columns from the input data set for simplicity.
The most notable advantage of ODL over other dictionary learning algorithms, such as K-SVD, is that ODL does
not rely on the matrix factorization upon the entire training
data. As a result, the time cost is much less compared to the
non-online versions when handling large training datasets. So
a speciﬁc input ECG signal that carries new features, such as
disease information, can be quickly processed by the dictionary learning module to update the dictionary when necessary. As dictionary update does not depend on the previous
samples, the framework also eliminates the demand of large
storage space for prior inputs.
BP algorithm, running on the mobile node, is used in our
framework to reconstruct high-quality signals. As ODL is
compatible with other reconstruction algorithms, more computation efﬁcient algorithms (e.g., fast iterative shrinkagethresholding algorithm (FISTA) can be implemented to improve accuracy-complexity trade-off).

The most recent frameworks on ECG monitoring [17, 18, 19]
adopt a QRS detection process, such as the Pan-Tompkins algorithm, prior to the sensing stage in order to locate the period
information of ECG signals. However, integrating the QRS
detection process into the sensor nodes not only occupying
CPU cycles but also burning excessive power. For wearable
applications, an energy-efﬁcient framework must get rid of
such pre-processing stages on sensor nodes.
The block diagram of the proposed framework is shown
in Fig. 1. It is composed of three functional modules (i.e.,
dictionary learning, random encoding, and CS signal reconstruction, performed on a server node, a sensor node, and a
mobile node, respectively).
The dictionary learning module is used to train personalized bases to capture the individual variability that is critical to exact signal recovery. As dictionary learning directly
extracts features from the segmented raw data, the learned
dictionary contains critical temporal and spatial information
needed for reconstruction. As a result, there is hardly a need
for signal alignment. To search for an optimum setup, we
ﬁrst sweep each parameter used in dictionary learning, including signal dimension, batch size for training, regularization coefﬁcient, and dictionary size. The derived parameters
are then applied to the dictionary learning module. As the reconstructed signals are the linear composition of atoms in the
trained dictionary, a “clean” dictionary thereby have the denoising effect on signal reconstruction. To get a “clean” dictionary, the training data is ﬁrst ﬁltered by a notch ﬁlter to remove power-line inference. Then the signal is passed through
a band-pass ﬁlter to remove baseline wandering and highfrequency inference. Enabled by the pre-processing in the
dictionary learning stage, the proposed framework eliminates
the need of employing complicated pre-processing methods

4. EXPERIMENT RESULTS
Experiments are conducted to compare the performance of the
proposed framework in terms of recovery quality and CR with
the conventional CS frameworks adopting pre-determined basis for the reconstruction of ECG signal. All frameworks em-



Table 1: Performance Comparison of CS frameworks
Framework
Proposed
Ansari-Ram et al. [5]
Casson el al. [7]
Mamaghanian el al. [1]
Chae et al. [6]

CR
10
5
4
3.4
2.5

PRD (%)
9
9
9
9
9

ploys the same random Bernoulli matrix Φ (0/1 only) as the
sensing matrix, so the hardware cost of the acquisition module, i.e., the sensor nodes, are the same.

Fig. 2: Comparison of our proposed framework with conventional CS framework in term of CR.

4.1. Performance Metrics
The compression ratio (CR) and percent root-mean-square
difference (PRD) are used as the performance metrics.
1) Compression Ratio (CR): CR is a measurement of the
reduction of the data required to represent the original signal
f . If m measurements are required to recover the signal with
dimension n, then
n
(5)
CR = .
m
2) Percent Root-mean-square Difference (PRD): PRD is
a measurement of the difference between the original signal
f and the reconstructed signal f  . As arbitrarily low PRD can
be achieved by selecting a high DC level in signal f , a more
appropriate metric is to remove the DC bias in signal f as
P RD =

f − f  2
× 100,
f − f̄ 2

Fig. 3: Reconstruction result for a segment of ECG signal
when CR=10. (a) Original ECG signal; (b) Reconstructed
signal using pre-determined DCT-DWT joint basis; (c) Reconstructed signal using online trained dictionary.

(6)
of 10x. This represents a 6.5x more sample size reduction
(engergy saving) than the reference framework [20]. Table 1
compares the proposed framework with existing CS frameworks [1, 5, 6, 7] that adopt pre-determined basis in signal
recovery. In general, our framework is able to further improve
the CR by 2-4x for achieving an average PRD of 9%. Fig.3
demonstrates the high reconstruction quality of the proposed
framework in comparison to the reference framework [20]
when CR=10.

where f̄ is the mean of signal f .
4.2. Experiment Settings and Results
Through parameter sweeping, the dimension of the signal n is
set to 256, size of the dictionary k is set to 512. Experiments
are carried out based on the MIT-BIH Arrhythmia Database.
In the experiments, 649984 samples are divided into 2539
epochs. Each epoch contains 256 samples. Among all the
data sets, 512 epochs are randomly chosen to initialize the
dictionary, 1621 epochs are used to train the dictionary, and
the remaining is used as the testing set. For performance
comparison, the pre-determined basis used in the reference
framework is a joint basis composed by both discrete cosine transform (DCT) and descrete wavelet transform (DWT)
bases [20]. This is because the periods components (e.g. QS
waves) and the spike components (e.g. R wave) have sparse
representations on DCT and DWT basis, respectively.
Figure 2 shows the performance comparison results.
Overall, the proposed framework outperform the reference
framework signiﬁcantly due to the use of personlized basis
in reconstruction . Speciﬁcally, an average PRD of 9%, required for diagnosis purposes [21], can achieved at a high CR

5. CONCLUSIONS
In this paper, we propose an energy-efﬁcient data acquisition
framework combining the notion of CS and ODL for longterm ECG monitoring. The framework signiﬁcantly enhances
CS performance by learning personalized basis to inform signal recovery. Experiment results show that by moving preprocessing to the dictionary learning stage, a simple segmentation process in the sensor nodes is sufﬁcient to recover highquality signals. In the future work, we will add sub-basis
onto which the abnormal ECG signal is projected, when the
“healthy” sub-basis is unable to model the original signal accurately.



6. REFERENCES

[13] L. Honglak et al., “Efﬁcient sparse coding algorithms,”
in Advances in Neural Inform. Process. Syst., pp. 801–
808. MIT Press, 2007.

[1] H. Mamaghanian et al., “Compressed sensing for realtime energy-efﬁcient ecg compression on wireless body
sensor nodes,” IEEE Trans. Biomed. Eng, vol. 58, no. 9,
pp. 2456–2466, Sep. 2011.

[14] L. Bottou and O. Bousquet, “The tradeoffs of large scale
learning,” in Advances in Neural Inform. Process. Syst.,
2008, pp. 161–168.

[2] U. Varshney, “Pervasive healthcare and wireless health
monitoring,” Mob. Netw. Appl., vol. 12, no. 2-3, pp.
113–127, Mar. 2007.

[15] J. Mairal et al., “Online dictionary learning for sparse
coding,” in Proc. of the 26th Ann. Int. Conf. on Mach.
Learn., 2009, pp. 689–696.

[3] F. Chen et al., “Design and analysis of a hardwareefﬁcient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3, pp. 744–756, Mar. 2012.

[16] J. Mairal et al., “Online learning for matrix factorization
and sparse coding,” J. Mach. Learn. Res., vol. 11, pp.
19–60, Mar. 2010.

[4] Y. Wang et al., “Optimizing boolean embedding matrix
for compressive sensing in rram crossbar,” in Proc. 2015
ACM/IEEE Int. Symp. Low Power Electron. and Design,
Rome, Italy, Jul. 2015, pp. 13–18.

[17] S. Lee et al., “A new approach to compressing ecg signals with trained overcomplete dictionary,” in EAI 4th
Int. Conf. on Wireless Mobile Commun. and Healthcare,
Nov. 2014, pp. 83–86.

[5] F. Ansari-Ram et al., “Ecg signal compression using
compressed sensing with nonuniform binary matrices,”
in 16th CSI Int. Symp. on Artiﬁcial Intell. and Signal
Process, May 2012, pp. 305–309.

[18] L. F. Polania et al., “Compressed sensing based method
for ecg compression,” in IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
May 2011, pp. 761–764.

[6] D.H. Chae et al., “Performance study of compressive
sampling for ecg signal compression in noisy and varying sparsity acquisition,” in IEEE Int. Conf. on Acoust.,
Speech and Signal Process, May 2013, pp. 1306–1309.

[19] M. Abo-Zahhad et al., “Compression of ecg signal
based on compressive sensing and the extraction of signiﬁcant features,” Int. J. of Commun., Network and Syst.
Sciences, vol. 8, pp. 97–117, 2015.

[7] A.J. Casson and E. Rodriguez-Villegas, “Signal agnostic compressive sensing for body area networks: Comparison of signal reconstructions,” in Annu. Int. Conf. of
the IEEE Eng. in Med. and Biol. Soc., Aug. 2012, pp.
4497–4500.

[20] F. Ren and D. Markovic, “18.5 a conﬁgurable 12-to237ks/s 12.8mw sparse-approximation engine for mobile exg data aggregation,” in IEEE Int. Solid-State Circuits Conf., Feb. 2015, pp. 1–3.
[21] Y. Zigel et al., “The weighted diagnostic distortion
(wdd) measure for ecg signals compression,” IEEE
Trans. Biomed. Eng., pp. 1422–1430, 2000.

[8] E.J. Candes et al., “An introduction to compressive sampling,” Signal Process. Mag., vol. 25, pp. 21 – 30, 2008.
[9] E.J. Candes et al., “Robust uncertainty principles: exact
signal reconstruction from highly incomplete frequency
information,” IEEE Trans. Inf. Theory, vol. 52, no. 2,
pp. 489–509, Feb. 2006.
[10] M. Elad and M. Aharon, “Image denoising via sparse
and redundant representations over learned dictionaries,” IEEE Trans. on Image Process., vol. 15, no. 12,
pp. 3736–3745, Dec. 2006.
[11] M. Aharon et al., “k-svd: An algorithm for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. on Signal Process., vol. 54, no. 11, pp.
4311–4322, Nov. 2006.
[12] B. A. Olshausen and D. J. Field, “Sparse coding with an
overcomplete basis set: A strategy employed by v1?,”
Vision Res., vol. 37, pp. 3311–3325, Dec. 1997.



AN ENERGY-EFFICIENT COMPRESSIVE SENSING FRAMEWORK INCORPORATING
ONLINE DICTIONARY LEARNING FOR LONG-TERM WIRELESS HEALTH MONITORING
Kai XU, Yixing Li, Fengbo Ren

arXiv:1606.01557v1 [cs.IT] 5 Jun 2016

Parallel Systems and Computing Laboratory (PSCLab)
Arizona State University
ABSTRACT
Wireless body area network (WBAN) is emerging in the mobile healthcare area to replace the traditional wire-connected
monitoring devices. As wireless data transmission dominates
power cost of sensor nodes, it is beneficial to reduce the data
size without much information loss. Compressive sensing
(CS) is a perfect candidate to achieve this goal compared to
existing compression techniques. In this paper, we proposed
a general framework that utilize CS and online dictionary
learning (ODL) together. The learned dictionary carries individual characteristics of the original signal, under which
the signal has an even sparser representation compared to
pre-determined dictionaries. As a consequence, the compression ratio is effectively improved by 2-4x comparing
to prior works. Besides, the proposed framework offloads
pre-processing from sensor nodes to the server node prior
to dictionary learning, providing further reduction in hardware costs. As it is data driven, the proposed framework has
the potential to be used with a wide range of physiological
signals.
Index Terms— Compressive sensing, online dictionary
learning, wireless sensor nodes (WSNs), wireless health.
1. INTRODUCTION
The existing heathcare model of the medical system is based
on episodic examination or short-term monitoring for disease diagnosis and treatment. The major issues in such a
system are the overlook of individual variability and the lack
of personal baseline data, due to limited frequency of measurements. Continuous or non-intermittent monitoring is the
key to create big data of individual health record for studying
the variability and obtaining the personal baseline. Recent
advancements in wireless body area networks (WBAN) and
bio-sensing techniques has enabled the emergence of miniaturized, non-invasive, cost-effective wireless sensor nodes
(WSNs) that can be deployed on the human body for personal health and clinical monitoring [1]. Through WBAN,
the monitored data can be transmitted to a near-field mobile
aggregator for on-site processing. Through Internet infrastructures, the data can be uploaded to remote servers for

storage and data analysis. These technology advancements
will eventually transform the existing model of health related
services to continuous monitoring for disease prediction and
prevention [2]. Such a wireless health revolution will make
healthcare systems more effective and economic, benefiting
billions of individuals and the society they live in.
One of the key challenges faced by the long-term wireless health monitoring is the energy efficiency of sensing and
information transfer. Due to the limited battery capacity of
WSNs, continuous sensing inevitably increases the frequency
of battery recharging or replacement, making it less convenient for practical usage. In the WSNs for bio-sensing applications, the energy cost of wireless transmission is about 2 orders of magnitude greater than other operations (e.g., analogto-digital conversion (ADC)). State-of-the-art radio transmitters exhibit energy efficiency in the nJ/bit range while every
other component consumes at most tens of pJ/bit [3]. Therefore, reducing the data size for information transfer is the key
to improve energy efficiency.
The CS framework [1, 4] offers a universal and simple
data encoding scheme that can compress a variety of physiological signals, providing a viable solution to realizing
energy-efficient WSNs for long-term wireless health monitoring. However, the compression ratio (CR) demonstrated
by existing frameworks is limited given a signal recovery
quality required for diagonosis purposes. In [5, 6] percent
root-mean-square difference (PRD) of 8.5% and 9% is reported at a CR of 5x and 2.5x for ECG signals, respectively.
These frameworks all deal with the sparsity of physiological
signals on pre-determined bases and fail to take into account
the individual variability in signals that is critical to exact
signal recovery.
In this paper, we propose an energy-efficient data acquisition framework, customized for the long-term electrocardiogram (ECG) monitoring, which exploits online dictionary
learning (ODL) on server nodes to train personalized bases
that capture the individual variability for further improving
the sparsity of ECG signals. By incorporating such prior
knowledge into signal recovery, the CS performance in terms
of accuracy-CR trade-off is significantly enhanced, leading to
further data size reduction and energy saving on sensor nodes.
Additionally, the proposed framework does not require any

pre-processing stages on sensor nodes. Alternatively, high
reconstruction quality is enforced by pre-processing training
data prior to the dictionary learning stage, to eliminate the impact of noise and interference on trained bases, enabling simpler and more cost-effective sensor structures. Experimental
results based on MIT-BIH database show that our framework
is able to achieve an average PRD of 9% at a CR of 10x.
This indicates that our framework can achieve 2-4x additional
energy saving on sensor nodes (for the same reconstruction
quality) compared to the reference designs [1, 5, 6, 7]. Due
to the training and personalization of the dictionary, the proposed framework has the potential to be generally applied to
a wide range of physiological signals.
2. BACKGROUND
2.1. Compressive Sensing
Assuming a signal f ∈ Rn can be well represented by a sparse
vector x ∈ Rk on a certain basis Ψ ∈ Rn×k as f = Ψx, then
the signal information can be well preserved by projecting f
onto a random domain through a sensing matrix Φ ∈ Rm×n
(m<n) [8], given as
y = Φf = ΦΨx.

(1)

Candes and et al. [9] has proven that one has a high probability to recover the sparse coefficient x by solving the basis
pursuit (BP) problem defined as
min kxk1

x∈Rk

s.t. ky − ΦΨxk2 ≤ ε,

(2)

where ε is an error tolerance term for enhancing the accuracy
of the solution considering noise.
2.2. Dictionary Learning
Learning dictionaries from data instead of using off-the-shelf
bases has been proved effective in improving signal reconstruction performance for images [10]. The most recent dictionary learning algorithms [11, 12, 13] are second-order iterative batch procedures that access the whole training set
at each iteration in order to minimize a cost function under
certain constraints. Although these algorithms [11, 12, 13]
have been shown experimentally faster than first-order gradient descent methods, they cannot effectively handle very
large training sets [14], because of the involved matrix factorization upon the entire training data. To be able to deal with
large data sets for long-term monitoring, the ODL algorithm
is adopted in our framework. Compared to the methods mentioned above, ODL has a higher training speed and requires
less storage space [15] because of its elimination of large matrix factorizations. With ODL, it is possible to add new features into the dictionary without stalling the reconstruction,
which offers a mechanic of amelioration when a distinctive
input is received.

2.3. ODL
Assuming the training set is composed of i.i.d. samples following a distribution p(x), ODL draws one sample xt at a
time and alternates between the sparse coding stage and dictionary update stage.
2.3.1. Sparse Coding
The sparse coding problem is a l1 -regularized least-squares
problem defined as
1
αt = arg min kxt − Dt−1 αk22 + λkαk1 .
n
2
α∈R

(3)

Due to the high correlations between columns of the
dictionary, a Cholesky-based implementation of the LARSLasso algorithm, which provides the whole regularization
path, is chosen here to solve the sparse coding problem [16].
2.3.2. Dictionary Updating
At this stage, the objective is to find a dictionary D that satisfies:
t

Dt = arg min
D

1X1
kxi − Dαi k22 + λkαi k1 .
t i=1 2

(4)

The problem in (4) can be solved by the block coordinate
descent algorithm [16]. Overall, the detailed procedure for
ODL algorithm is summarized in Algorithm 1.
Algorithm 1 Pseudocode for ODL
Input: Input data x ∈ Rn ∼ p(x), initial dictionary D0 ∈
Rn×k , number of iterations t.
Output: Learned dictionary Dt .
Steps:
1: Set A0 ← 0, B0 ← 0.
2: For t=1:T
3: Draw a new sample xt from p(x).
4: Sparse coding: find a sparse coefficient of xt under
current dictionary Dt−1 .
5: At ← At−1 + αt αtT .
6: Bt ← Bt−1 + xt αtT .
7: Dictionary update: update dictionary Dt−1 column
by column, the j-th column is given by
8: For j=1:k
9:
dj ← (At1)jj (Bt (:, j) − DAt (:, j)) + dj .
if kdj k2 > 1, then normalize it to unit form.
10: end for
11. end for
12. Return Dt .

Fig. 1: Block diagram of the proposed framework. The parameter sweeping and dictionary training procedure are executed
on servers. The reconstruction process is performed on mobile platform for providing timely feedback. The random encoding
process using random Bernoulli matrix is embedded into the sensor node for effective data compression and energy saving.

3. THE PROPOSED FRAMEWORK
The most recent frameworks on ECG monitoring [17, 18, 19]
adopt a QRS detection process, such as the Pan-Tompkins algorithm, prior to the sensing stage in order to locate the period
information of ECG signals. However, integrating the QRS
detection process into the sensor nodes not only occupying
CPU cycles but also burning excessive power. For wearable
applications, an energy-efficient framework must get rid of
such pre-processing stages on sensor nodes.
The block diagram of the proposed framework is shown
in Fig. 1. It is composed of three functional modules (i.e.,
dictionary learning, random encoding, and CS signal reconstruction, performed on a server node, a sensor node, and a
mobile node, respectively).
The dictionary learning module is used to train personalized bases to capture the individual variability that is critical to exact signal recovery. As dictionary learning directly
extracts features from the segmented raw data, the learned
dictionary contains critical temporal and spatial information
needed for reconstruction. As a result, there is hardly a need
for signal alignment. To search for an optimum setup, we
first sweep each parameter used in dictionary learning, including signal dimension, batch size for training, regularization coefficient, and dictionary size. The derived parameters
are then applied to the dictionary learning module. As the reconstructed signals are the linear composition of atoms in the
trained dictionary, a “clean” dictionary thereby have the denoising effect on signal reconstruction. To get a “clean” dictionary, the training data is first filtered by a notch filter to remove power-line inference. Then the signal is passed through
a band-pass filter to remove baseline wandering and highfrequency inference. Enabled by the pre-processing in the
dictionary learning stage, the proposed framework eliminates
the need of employing complicated pre-processing methods

prior to random encoding on the sensor node. Instead, a simple segmentation module is sufficient for clean reconstruction.
The initialization in dictionary learning is important. A
poorly initialized dictionary may contain bad atoms that are
never used [16]. Generally, the dictionary can be initialized
by random numbers or input data. For more difficult and regularized problem, it is preferable to start from a less regularized
case and gradually increase the regularization coefficients. In
our framework, the dictionary is initialized by randomly chosen columns from the input data set for simplicity.
The most notable advantage of ODL over other dictionary learning algorithms, such as K-SVD, is that ODL does
not rely on the matrix factorization upon the entire training
data. As a result, the time cost is much less compared to the
non-online versions when handling large training datasets. So
a specific input ECG signal that carries new features, such as
disease information, can be quickly processed by the dictionary learning module to update the dictionary when necessary. As dictionary update does not depend on the previous
samples, the framework also eliminates the demand of large
storage space for prior inputs.
BP algorithm, running on the mobile node, is used in our
framework to reconstruct high-quality signals. As ODL is
compatible with other reconstruction algorithms, more computation efficient algorithms (e.g., fast iterative shrinkagethresholding algorithm (FISTA) can be implemented to improve accuracy-complexity trade-off).
4. EXPERIMENT RESULTS
Experiments are conducted to compare the performance of the
proposed framework in terms of recovery quality and CR with
the conventional CS frameworks adopting pre-determined basis for the reconstruction of ECG signal. All frameworks em-

Table 1: Performance Comparison of CS frameworks
Framework
Proposed
Ansari-Ram et al. [5]
Casson el al. [7]
Mamaghanian el al. [1]
Chae et al. [6]

CR
10
5
4
3.4
2.5

PRD (%)
9
9
9
9
9

ploys the same random Bernoulli matrix Φ (0/1 only) as the
sensing matrix, so the hardware cost of the acquisition module, i.e., the sensor nodes, are the same.

Fig. 2: Comparison of our proposed framework with conventional CS framework in term of CR.

4.1. Performance Metrics
The compression ratio (CR) and percent root-mean-square
difference (PRD) are used as the performance metrics.
1) Compression Ratio (CR): CR is a measurement of the
reduction of the data required to represent the original signal
f . If m measurements are required to recover the signal with
dimension n, then
n
CR = .
(5)
m
2) Percent Root-mean-square Difference (PRD): PRD is
a measurement of the difference between the original signal
f and the reconstructed signal f 0 . As arbitrarily low PRD can
be achieved by selecting a high DC level in signal f , a more
appropriate metric is to remove the DC bias in signal f as
P RD =

kf − f 0 k2
× 100,
kf − f̄ k2

Fig. 3: Reconstruction result for a segment of ECG signal
when CR=10. (a) Original ECG signal; (b) Reconstructed
signal using pre-determined DCT-DWT joint basis; (c) Reconstructed signal using online trained dictionary.

(6)

where f̄ is the mean of signal f .
4.2. Experiment Settings and Results
Through parameter sweeping, the dimension of the signal n is
set to 256, size of the dictionary k is set to 512. Experiments
are carried out based on the MIT-BIH Arrhythmia Database.
In the experiments, 649984 samples are divided into 2539
epochs. Each epoch contains 256 samples. Among all the
data sets, 512 epochs are randomly chosen to initialize the
dictionary, 1621 epochs are used to train the dictionary, and
the remaining is used as the testing set. For performance
comparison, the pre-determined basis used in the reference
framework is a joint basis composed by both discrete cosine transform (DCT) and descrete wavelet transform (DWT)
bases [20]. This is because the periods components (e.g. QS
waves) and the spike components (e.g. R wave) have sparse
representations on DCT and DWT basis, respectively.
Figure 2 shows the performance comparison results.
Overall, the proposed framework outperform the reference
framework significantly due to the use of personlized basis
in reconstruction . Specifically, an average PRD of 9%, required for diagnosis purposes [21], can achieved at a high CR

of 10x. This represents a 6.5x more sample size reduction
(engergy saving) than the reference framework [20]. Table 1
compares the proposed framework with existing CS frameworks [1, 5, 6, 7] that adopt pre-determined basis in signal
recovery. In general, our framework is able to further improve
the CR by 2-4x for achieving an average PRD of 9%. Fig.3
demonstrates the high reconstruction quality of the proposed
framework in comparison to the reference framework [20]
when CR=10.
5. CONCLUSIONS
In this paper, we propose an energy-efficient data acquisition
framework combining the notion of CS and ODL for longterm ECG monitoring. The framework significantly enhances
CS performance by learning personalized basis to inform signal recovery. Experiment results show that by moving preprocessing to the dictionary learning stage, a simple segmentation process in the sensor nodes is sufficient to recover highquality signals. In the future work, we will add sub-basis
onto which the abnormal ECG signal is projected, when the
“healthy” sub-basis is unable to model the original signal accurately.

6. REFERENCES
[1] H. Mamaghanian et al., “Compressed sensing for realtime energy-efficient ecg compression on wireless body
sensor nodes,” IEEE Trans. Biomed. Eng, vol. 58, no. 9,
pp. 2456–2466, Sep. 2011.
[2] U. Varshney, “Pervasive healthcare and wireless health
monitoring,” Mob. Netw. Appl., vol. 12, no. 2-3, pp.
113–127, Mar. 2007.

[13] L. Honglak et al., “Efficient sparse coding algorithms,”
in Advances in Neural Inform. Process. Syst., pp. 801–
808. MIT Press, 2007.
[14] L. Bottou and O. Bousquet, “The tradeoffs of large scale
learning,” in Advances in Neural Inform. Process. Syst.,
2008, pp. 161–168.
[15] J. Mairal et al., “Online dictionary learning for sparse
coding,” in Proc. of the 26th Ann. Int. Conf. on Mach.
Learn., 2009, pp. 689–696.

[3] F. Chen et al., “Design and analysis of a hardwareefficient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3, pp. 744–756, Mar. 2012.

[16] J. Mairal et al., “Online learning for matrix factorization
and sparse coding,” J. Mach. Learn. Res., vol. 11, pp.
19–60, Mar. 2010.

[4] Y. Wang et al., “Optimizing boolean embedding matrix
for compressive sensing in rram crossbar,” in Proc. 2015
ACM/IEEE Int. Symp. Low Power Electron. and Design,
Rome, Italy, Jul. 2015, pp. 13–18.

[17] S. Lee et al., “A new approach to compressing ecg signals with trained overcomplete dictionary,” in EAI 4th
Int. Conf. on Wireless Mobile Commun. and Healthcare,
Nov. 2014, pp. 83–86.

[5] F. Ansari-Ram et al., “Ecg signal compression using
compressed sensing with nonuniform binary matrices,”
in 16th CSI Int. Symp. on Artificial Intell. and Signal
Process, May 2012, pp. 305–309.

[18] L. F. Polania et al., “Compressed sensing based method
for ecg compression,” in IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
May 2011, pp. 761–764.

[6] D.H. Chae et al., “Performance study of compressive
sampling for ecg signal compression in noisy and varying sparsity acquisition,” in IEEE Int. Conf. on Acoust.,
Speech and Signal Process, May 2013, pp. 1306–1309.

[19] M. Abo-Zahhad et al., “Compression of ecg signal
based on compressive sensing and the extraction of significant features,” Int. J. of Commun., Network and Syst.
Sciences, vol. 8, pp. 97–117, 2015.

[7] A.J. Casson and E. Rodriguez-Villegas, “Signal agnostic compressive sensing for body area networks: Comparison of signal reconstructions,” in Annu. Int. Conf. of
the IEEE Eng. in Med. and Biol. Soc., Aug. 2012, pp.
4497–4500.

[20] F. Ren and D. Markovic, “18.5 a configurable 12-to237ks/s 12.8mw sparse-approximation engine for mobile exg data aggregation,” in IEEE Int. Solid-State Circuits Conf., Feb. 2015, pp. 1–3.

[8] E.J. Candes et al., “An introduction to compressive sampling,” Signal Process. Mag., vol. 25, pp. 21 – 30, 2008.
[9] E.J. Candes et al., “Robust uncertainty principles: exact
signal reconstruction from highly incomplete frequency
information,” IEEE Trans. Inf. Theory, vol. 52, no. 2,
pp. 489–509, Feb. 2006.
[10] M. Elad and M. Aharon, “Image denoising via sparse
and redundant representations over learned dictionaries,” IEEE Trans. on Image Process., vol. 15, no. 12,
pp. 3736–3745, Dec. 2006.
[11] M. Aharon et al., “k-svd: An algorithm for designing overcomplete dictionaries for sparse representation,”
IEEE Trans. on Signal Process., vol. 54, no. 11, pp.
4311–4322, Nov. 2006.
[12] B. A. Olshausen and D. J. Field, “Sparse coding with an
overcomplete basis set: A strategy employed by v1?,”
Vision Res., vol. 37, pp. 3311–3325, Dec. 1997.

[21] Y. Zigel et al., “The weighted diagnostic distortion
(wdd) measure for ecg signals compression,” IEEE
Trans. Biomed. Eng., pp. 1422–1430, 2000.

A SINGLE-PRECISION COMPRESSIVE SENSING SIGNAL RECONSTRUCTION ENGINE
ON FPGAS
Fengbo Ren, Richard Dorrace, Wenyao Xu, Dejan Markoviü
Department of Electrical Engineering
University of California, Los Angeles
Los Angeles, CA 90095, U.S.A
E-mail: fren@ee.ucla.edu
ABSTRACT
Compressive sensing (CS) is a promising technology for
the low-power and cost-effective data acquisition in
wireless healthcare systems. However, its efficient realtime signal reconstruction is still challenging, and there is
a clear demand for hardware acceleration. In this paper,
we present the first single-precision floating-point CS
reconstruction engine implemented a Kintex-7 FPGA
using the orthogonal matching pursuit (OMP) algorithm.
In order to achieve high performance with maximum
hardware utilization, we propose a highly parallel
architecture that shares the computing resources among
different tasks of OMP by using configurable processing
elements (PEs). By fully utilizing the FPGA recourses,
our implementation has 128 PEs in parallel and operates
at 53.7 MHz. In addition, it can support 2x larger problem
size and 10x more sparse coefficients than prior work,
which enables higher reconstruction accuracy by adding
finer details to the recovered signal. Hardware results
from the ECG reconstruction tests show the same level of
accuracy as the double-precision C program. Compared to
the execution time of a 2.27 GHz CPU, the FPGA
reconstruction achieves an average speed-up of 41x.
1. INTRODUCTION
The compressive sensing (CS) framework offers a
universal approach for compressive data sampling at subNyquist rates [1], which promises to lower the cost and
power consumption of data acquisition systems for a
variety of signals [2]. However, the computational
complexity of CS reconstruction algorithms is usually 1-2
orders of magnitude higher than orthogonal transforms [2].
This leads to inefficient real-time processing on general
purpose processors, limiting the application of CS
techniques on mobile platforms. Therefore, an efficient
hardware solution is critically needed as it will benefit a
variety of CS applications in terms of cost, portability and
battery-life.

978-1-4799-0004-6/13/$31.00 ©2013 IEEE

There have been a few related works on the VLSI
design for CS signal reconstruction. The first FPGA
implementation of the orthogonal matching pursuit (OMP)
algorithm is presented in [3]. The application-specific
integrated circuit (ASIC) implementations of several
greedy algorithms are introduced in [4-7]. However, most
of the existing work suffer from two major deficiencies
that limit their practical usage: 1) they are all fixed-point
implementations optimized for a particular input pattern
and lack the flexibility to accommodate different
applications. 2) The number of sparse coefficients
supported by these implementations is so limited that the
reconstruction accuracy cannot be guaranteed, especially
when the signal is not ideally sparse on the selective basis.
Contributions: In this paper, we present the first
single-precision floating-point CS reconstruction engine
implemented a Kintex-7 XC7K325T FPGA using the
OMP algorithm. In order to achieve high performance
with maximum hardware utilization, we propose a highly
parallel architecture and take into account resource
sharing among different tasks of OMP by using
configurable processing elements (PEs). The FPGA
implementation has 128 PEs in parallel and operates at
53.7 MHz. It can support 2x larger problem size with 10x
more sparse coefficients in signal reconstructions than
prior VLSI designs. This allows more signal details to be
recovered and leads to higher reconstruction accuracy.
Emulation results from the ECG reconstruction tests show
that the FPGA implementation can achieve the same level
of accuracy as the double-precision C program with over
40x less reconstruction time.
2. PRELIMINARIES
The compressive sampling system is often modeled as a
linear system given by
‫ ݕ‬ൌ ࡭‫ ݔ‬൅ ߚ,
(2.1)
where ‫ॺ א ݔ‬௡௞ is a ݇ -sparse signal given ॺ௡௞ ൌ
ሼ‫ א ݔ‬Թ௡ ȁ ‫צ ݔ צ‬଴ ൑ ݇ሽ, ࡭ ‫ א‬Թ୫ൈ୬ is the sampling matrix,
ߚ ‫ א‬Թ௠ is a random noise with bounded energy ‫צ ߚ צ‬ଶ ൑
ߝ , and ‫ א ݕ‬Թ௠ is the linear measurement or the sampled
data. It has been proved that a ‫ॺ א ݔ‬௡௞ can be exactly

Table 1. Pseudo-Code of the OMP Algorithm.
Step

Operation

1

ሬറǡ ݀ ൌ ሬറ
Ͳǡ ߉଴ ൌ ‫׎‬ǡ ‫ ݐ‬ൌ ͳ.
‫ݎ‬଴ ൌ ‫ݕ‬ǡ ‫ݔ‬଴ ൌ Ͳ

2*

ܿ ൌ ࡭் ‫ݎ‬௧ିଵ ,߮ ൌ  ௜ ȁܿሺ݅ሻȁǡ ߉௧ ൌ ߉௧ିଵ ‫߮ ׫‬.

3

݀ሺ߉௧ ሻ ൌ ఈ ሼ࡭௸೟ ் ࡭௸೟ ߙ ൌ ܿሺ߉௧ ሻሽ,

4

‫ݎ‬௧ ൌ ‫ݎ‬௧ିଵ െ ࡭௸೟ ݀ሺ߉௧ ሻǡ ‫ݔ‬௧ ൌ ‫ݔ‬௧ିଵ ൅ ݀ǡ ‫ ݐ‬ൌ ‫ ݐ‬൅ ͳ.

5

If ‫ݎ צ‬௧ ‫צ‬ଶ ൑ ߝ, stop, otherwise, go to step 2
* Assuming that all atoms in ࡭ are normalized.

recovered by solving the κ଴ pseudo-norm minimization
given as
min ‫ Ͳצ ݔ צ‬ǡ  ‫ ݕ צ‬െ ࡭‫ ʹצ ݔ‬൑ ߝ, (2.2)
as long as ࡭ satisfies the Null Space Property (NSP) and
the Restricted Isometry Property (RIP) of order ʹ݇ [1].
The minimization problem in (2.2) is NP-hard. Another
approach to attack this problem is to relax the objective
function in (2.2) to κଵ  minimization [1] as
min ‫ ͳצ ݔ צ‬ǡ  ‫ ݕ צ‬െ ࡭‫ ʹצ ݔ‬൑ ߝ. (2.3)
The problem in (2.3) is convex and can be solved through
general convex optimization methods.
3. RECONSTRUCTION ALGORITHM
The concept of OMP [8] is built upon the following
observation. When ‫ॺ א ݔ‬௡௞ has only ݇ non-zeros, the
linear measurement ‫ ݕ‬ൌ ࡭‫ ݔ‬can also be represented as
‫ ݕ‬ൌ ࡭௸ ‫ݔ‬ሺ߉ሻ, where ߉ is the index set of the non-zero
elements in ‫ ݔ‬, called the active set. Note that given
࡭௸ ‫ א‬Թ௠ൈ௞ with ݉ ൐ ݇ǡ ‫ݔ‬ሺ߉ሻ can be best approximated
by solving the least square (LS) problem given as
(3.1)
min ‫ ݕ צ‬െ ࡭௸ ‫ݔ‬ሺ߉ሻ ‫צ‬ଶ ,
as long as the active set ߉ is identified.
The pseudo-code of the OMP algorithm is described in
Table 1. There are three main tasks performed in each
iteration: atom searching (AS) for identifying the active
set (Step 2), least square (LS) solving for computing the
updating direction (Step 3), and estimation update (EU)
(Step 4). At iteration ‫ ݐ‬, the number of floating point
operations (FLOPs) required by each task is ʹ݊݉ ,
ʹ݉‫ ݐ‬൅ ͵‫ ݐ‬ଶ , and ʹ݉‫ݐ‬, respectively. Given ݊ ൐ ݉ ‫[ ݇ ب‬1]
and ‫ ݐ‬൑ ݇ [8], the AS task is found to be the performance
bottleneck as it contributes the most computation in OMP.
On the other hand, the LS task plays a pivotal role on the
hardware utilization efficiency. This is because the matrix
factorization in the LS task involves a variety of
operations with complex data flows, introducing extra
hardware complexity in terms of control and scheduling.
However, due to the proper use of factorization techniques
(see Section 4.3), the LS task only contributes a small part
of the total computation. As a result, any hardware

Fig. 1. VLSI architecture of the reconstruction engine.

resource dedicated to performing the LS task will have a
very low utilization rate.
4. HARDWARE IMPLEMENTATION
In order to achieve high performance with maximum
hardware utilization, we design a highly parallel VLSI
architecture with configurable PEs, where all the
computing resources for performing the AS task are also
reused by other tasks. Such resource sharing strategy
improves the hardware utilization, allowing for more PEs
to be mapped onto the FPGA for further speed-up.
4.1. Architecture Design
The top-level block diagram of the proposed VLSI
architecture is shown in Fig. 1. The vector core integrates
multiple PEs in parallel with a flexible data-path. Both the
PE and the data-path connection can be dynamically
configured through different microcode. This enables the
vector core to support a selected set of vector operations.
A shift register logic (SRL) unit is used in the feedback
path of each PE for providing the folding capability to
process long vectors.
The scalar core supports scalar comparison, addition,
accumulation, and division. Depending on the top-level
data-path configuration, the scalar core can either postprocess a selected result from the vector core (e.g. inner
product) or process independent data separately. When the
two cores are connected as a processing group, more
advanced operations, such as correlation sorting, forward
substitution (FS), and backward substitution (BS), can be
executed. Note that the global feedback path with SRL
unit is dedicated for performing FS and BS with iterative
vector operations.
The computation cores are linked by seven data-path
memories. Note that in our design, the sampling matrix
࡭ ‫ א‬Թ௠ൈ௡ is explicitly stored in order to accommodate
the CS reconstructions on different basis. The complex
data flow of OMP is enforced by customized local
memory controllers, which are synchronized by a top-

level finite-state machine (FSM). Using data-path
memories helps improve the power efficiency especially
when the supported problem size is large, as most of the
data movement can be replaced by pointer manipulations.
4.2. Data Fromat
In order to figure out the word-length required to preserve
the software-level accuracy for different CS problems, we
investigate the dynamic range (DR) requirement of each
computing unit in our design using double-precision
MATLAB simulations. The worst-case DR requirement
of the vector core is found to be over 70 bits. Such a large
DR is mainly due to the solution searching characteristic
of OMP that the scales of most variables are gradually
scaled down as the residue approaches zero. Besides,
sharing the computing resources also tends to enlarge the
DR as it has to cover the worst case of all different tasks.
As a result, a fixed-point implementation would incur
large area overhead for preserving the software-level
accuracy in different CS problems. Therefore, we adopt
the single-precision floating-point data format in our
design. It can provide the required DR with a 32-bit wordlength, leading to significant area reduction especially in
the data-path memories.
Nonetheless, ࡭ is still stored in a fixed-point data
format. Since it needs not to be updated during the
operations, thereby having a limited DR. A parallelized
fixed-point to floating-point conversion interface is
inserted at the memory output (Fig. 1).
4.3. Square-Root-Free OMP
In our design, the infrequent square root operations are
avoided through algorithm reformulation. For solving the
LS problem in (3.1), we adopt an alternative Cholesky
factorization method [9] given by
ࢶ ൌ ࡸࡰࡸࢀ
(4.1)
்
where ࢶ ൌ ࡭௸೟ ࡭௸೟ is a positive definite matrix, ࡸ is a
lower triangular matrix with normalized diagonal
elements, and ࡰ is a diagonal matrix that absorbs the
square-rooted factors from both ࡸ and ࡸࢀ and cancels the
square root operations. The stopping criterion ‫ݎ צ‬௧ ‫צ‬ଶ ൑ ߝ
can be reformulated as ‫ݎ‬௧ ் ‫ݎ‬௧ ൑ ߝ ଶ and computed with
doubled dynamic range.

Table 2. Implementation Results on FPGA*
Usage Breakdown†
System
VC
SC
DMs
Frequency

53.7 MHz

LUTs

186,799 (91%)

90%

3%

7%

DSP48s

258 (31%)

99%

0%

1%

Block RAMs

382 (86%)

0%

0%

100%

* Xilinx Kintex-7 XC7K325T-FBG900
† VC: vector core, SC: scalar core, DMs: data-path memories

is the largest problem size supported by VLSI designs to
date. In addition, our implementation can support up to
݇ ൑ ͵ʹͲ sparse coefficients in signal reconstructions,
which is almost 10x more than previous work [3-7].
Reconstructing more coefficients adds finer details to the
recovered signal. This is critical to the reconstruction
accuracy, especially when the signal is not ideally sparse
on the selective basis.
Table 2 summarizes the system performance and the
resource utilization profile of the FPGA implementation.
After performing global retiming, the critical path is found
to pass through the floating-point multiplier. A system
frequency of 53.7 MHz is realized. The implementation
uses 91% of the LUTs, 31% of the DSP48 slices, and 98%
of the BRAMs on the FPGA device. Note that all the
BRAMs are used to construct data-path memories, where
65% is used for storing ࡭ in a 10-bit fixed-point data
format. Resulting from the resource sharing efforts, the
vector core has a constant 100% temporal utilization rate
during the operations. Moreover, 90% of the LUT usage
is dedicated to the parallelization of the vector core for
achieving higher performance.
In order to evaluate the reconstruction accuracy, we
use the ECG data from MIT-BIH database [10] to perform
CS sampling and reconstruction. In our experiment, the
ECG signals are segmented with a fixed window size of
݊ ൌ ͳͲʹͶ and sampled by Bernoulli random matrices
with different compression (݉Τ݊) ratios. For comparison
purposes, the signals are reconstructed on the DWT basis

5. EXPERIMENT RESULTS
The single-precision CS reconstruction engine is
implemented on a Xilinx Kintex-7 XC7K325T-FBG900
FPGA (speed grade -2). By efficiently utilizing the
resources on the FPGA, the reconstruction engine has 128
PEs in parallel and can support a flexible problem size of
݊ ൑ ͳ͹ͶͲ and ݉ ൑ ͸ͶͲ. To the best of our knowledge, it

Fig. 2. Average RSNR performance measured from the ECG
reconstruction in C program and on FPGA at different compression
ratio (݉Ȁ݊).

configurable PEs for enabling the resource sharing
between different tasks of OMP. By fully utilizing the
recourses on FPGA, our implementation has 128 PEs in
parallel and operates at 53.7 MHz. In addition, it can
support twice the problem size and nearly 10x more
sparse coefficients in signal reconstructions than prior
VLSI designs. Emulation results from the ECG
reconstruction tests show that the proposed hardware
solution can achieve the same level of accuracy as the
software counterpart. Compared to the execution time of a
2.27 GHz CPU, an average speed-up of 41x is achieved
by the FPGA implementation.
Fig. 3 An ECG signal recovered by FPGA with a compression ratio of
(a) ݉Ȁ݊ ൌ ͲǤʹ, (b) ݉Ȁ݊ ൌ ͲǤ͵, and (c) ݉Ȁ݊ ൌ ͲǤͶ.

using both C program and the FPGA implementation. The
reconstruction accuracy is measured by the reconstruction
SNR (RSNR) defined as
‫צ‬௫‫צ‬
RSNR=ʹͲ ȉ ଵ଴ ሺ ො‫צ‬మ ሻ.
(5.1)
‫צ‬௫ି௫ మ

where ‫ א ݔ‬Թ௡ is the original signal, and ‫ݔ‬ො ‫ א‬Թ௡ is the
reconstructed signal. Figure 2 shows the average RSNR
performance measured from the C program (doubleprecision) and the FPGA reconstructions (singleprecision). As the floating-point data format is used, the
reconstruction engine on FPGA is able to achieve the
same level of accuracy as the C program in this test.
Figure 3 illustrates an ECG signal recover by the FPGA
implementation versus the original signal at different
compression ratio. Note that as the RSNR performance is
improved with more measurements, the number of sparse
coefficients ( ݇ ) that needs to be reconstructed also
increases. This clearly shows the importance of
supporting a large ݇ for achieving a good reconstruction
quality. In this sense, our implementation that offers 10x
larger such capability than prior work is a more viable
computing solution for the accuracy-driven CS problems.
On the other hand, reconstructing more coefficients
requires more iterations of the OMP algorithm thereby
more reconstruction time. However, this overhead can be
disregarded as long as the real-time requirement can be
met by the FPGA acceleration in most applications. With
128 PEs operating at 53.7 MHz in parallel, the ECG
reconstruction (Fig. 3) on FPGA takes 39.9 ʅs per
iteration. Compared to the execution time of the C
program powered by a 2.27 GHz CPU, over 40x speed-up
is achieved on average.
6. CONCLUSION
In this paper, we present the first single-precision floatingpoint CS reconstruction engine implemented on a Kintex7 XC7K325T FPGA. In order to achieve high
performance with maximum hardware utilization, we
design a highly parallel VLSI architecture with

7. REFERENCES
[1]

E. Candès, et al., “An Introduction to Compressive
Sampling”, IEEE Signal Process. Mag., vol. 25, no. 2, pp.
21-30, Mar. 2008.

[2]

M. Duarte, et al., “Single Pixel Imaging via Compressive
Sampling”, IEEE Signal Process. Mag., vol. 25, no. 2, pp.
83-91, Mar. 2008.

[3]

A. Septimus, et al., “Compressive Sampling Hardware
Reconstruction”, in Proc. Int. Symp. Circuits and Systems
(ISCAS), Paris, Fance, May 2010, pp. 3316-3319.

[4]

P. Maechler, et al., “Matching Pursuit: Evaluation and
Implementation for LTE Channel Estimation”, in Proc.
44th Asilomar Conf. Signals, Systems and Computers,
Pacific Grove, CA, Nov. 2010, pp. 400-405.

[5]

J. Stanislaus, et al., “High Performance Compressive
Sensing Reconstruction Hardware with QRD Process”, in
Proc. Int. Symp. Circuits and Systems (ISCAS), Seoul,
Korea, May 2012, pp. 29-32.

[6]

M. Patrick, et al., “VLSI Design of Approximate Message
Passing for Signal Restoration and Compressive Sensing”,
IEEE J. Emerg. Sel. Topic Circuits Syst., vol. 2, no. 3, pp.
579-590, Sep. 2012.

[7]

L. Bai, et al., “High-Speed Compressed Sensing
Reconstruction on FPGA Using OMP and AMP”, in Proc.
19th Int. Conf. Electronics, Circuits and Systems (ICECS),
Seville, Spain, Dec. 2012, pp. 53-56.

[8]

J. Tropp, et al., “Signal Recovery from Random
Measurements via Orthogonal Matching Pursuit”, IEEE
Trans. Inform. Theory, vol. 53, no. 12, pp 4655-4666, Dec.
2007.

[9]

D. Yang “Turbo Bayesian Compressed Sensing”, Ph.D.
dissertation, Dept. Elect. Eng., Univ. of Tennessee,
Knoxville, Knoxville, TN, 2011.

[10] G. Moody, et al., “The impact of the MIT-BIH Arrhythmia

Database,” IEEE Eng. in Med. and Biol., vol. 20, no. 3, pp.
45-50, May 2001.

ISSCC 2015 / SESSION 18 / SoCs FOR MOBILE VISION, SENSING, AND COMMUNICATIONS / 18.5
18.5

A Configurable 12-to-237KS/s 12.8mW
Sparse-Approximation Engine for Mobile
ExG Data Aggregation

Fengbo Ren, Dejan Markovic´
´
University of California, Los Angeles, CA
Compressive sensing (CS) is a promising solution for low-power on-body
sensors for 24/7 wireless health monitoring [1]. In such an application, a mobile
data aggregator performing real-time signal reconstruction is desired for timely
prediction and proactive prevention. However, CS reconstruction requires
solving a sparse approximation (SA) problem. Its high computational complexity
makes software solvers, consuming 2–50W on CPUs, very energy inefficient for
real-time processing. This paper presents a configurable SA engine in a 40nm
CMOS technology for energy-efficient mobile data aggregation from
compressively sampled biomedical signals. Using configurable architecture, a
100% utilization of computing resources is achieved. An efficient data-shuffling
scheme is implemented to reduce memory leakage by 40%. At the
minimum-energy point (MEP), the SA engine achieves a real-time throughput for
reconstructing 61-to-237 channels of biomedical signals simultaneously with
<1% of a mobile device’s 2W power budget, which is 76–350× more energyefficient than prior hardware designs.
Electrocardiography
(ECG),
electroencephalography
(EEG),
and
electromyography (EMG) signals, collectively referred to as ExG, contain critical
information about the human body status. ExG signals can span 3 orders of
magnitude in amplitude (10μV–10mV) and frequency (0.1Hz–500Hz), and can
have a large difference in sparsity depending on the subject’s activity. For
instance, a low and high activity can produce a signal sparsity (k/n) of <2% and
>15%, respectively [2], where k is the signal sparsity level and n is the signal
dimension (# of samples). As a result, prior chip implementations of SA solvers
[3-4] that are optimized to handle limited dynamic range and fixed problem size
are not suitable for the intended application. We designed a flexible SA
architecture (Fig. 18.5.1), which uses a single-precision floating-point data
format to achieve a large dynamic range. It can be configured to handle different
problem settings on-the-fly, including signal and measurement dimensions
(n and m), signal sparsity level (k), reconstruction basis (A) and error tolerance
(∈).
The human body is expected to have a low activity on average, where ExG
signals feature a high sparsity, especially when thresholding schemes are used
[2]. This is where orthogonal matching pursuit (OMP) has a better
complexity-accuracy trade-off than other SA algorithms [4]. For efficient
mapping, 3 algorithm reformulation techniques have been introduced to further
break down the OMP algorithm into 6 basic linear algebra (BLA) operations at
each iteration [5], which are grouped into 3 tasks: atom searching (AS),
least-squares solving (LS), and estimation update (EU). Taking advantage of the
algorithm-architecture co-design, most hardware resources in the SA
architecture are shared across different tasks.
The computing resources in the SA architecture include the vector and scalar
processing cores (VC and SC), Fig. 18.5.2. To support flexible problem sizes
with high energy efficiency, 128 processing elements (PEs) in the VC are
coordinated through an interconnect block (IB). When the IB is enabled, PEs are
configured to perform inner product. Otherwise, PEs can be configured to
support element-wise addition, multiplication, multiply-accumulation, and
vector-scalar product. A shift-register logic (SRL) unit is used in the feedback
path of each PE to enable folded processing of long vectors. The SC supports
scalar comparison, addition, accumulation, and division. Depending on the
top-level data-path configuration, the SC can either post-process a selective
result from the VC or process independent data in parallel. For efficient local
memory access, a 1.2KB and 1.5KB cache is dedicated to each PE and the SC,
respectively (Fig. 18.5.1). To facilitate data communication between the VC and
SC in long delay lines, a 768B cache is shared between all the PEs and the SC.
In addition, a core-level SRL unit is used to connect the SC with all the PEs. This
feedback path effectively reduces the loop latency between VC and SC to 4–8
cycles, thereby greatly accelerating the iterative BLA operations such as forward
and backward substitution (FS and BS). The SA engine uses FIFO interfaces to
handle the flow control at the data inputs and outputs. A fixed-to-floating-point
conversion interface is also available at the input to allow the processing of
different signal representations. When executing the OMP algorithm, the VC is
dynamically configured in a SIMD fashion. Similarly, the SC and data-paths are
also dynamically configured to perform the BLA operations in different tasks. As

334

• 2015 IEEE International Solid-State Circuits Conference

a result, a 100% utilization of the computing resources is achieved, maximizing
area efficiency (Fig. 18.5.2).
In the LS task, column and row access of a triangular matrix (L) is required for
performing FS and BS, respectively, for realizing Cholesky factorization (CF). As
accessing a row of L is equivalent to accessing a column of LT, a straightforward
memory-mapping scheme for PE caches is to store both L and LT as a regular
square matrix (Fig. 18.5.3). We refer to this scheme as a mirror mode, where
columns of L and LT can be accessed at the same address of each PE cache in
an ascending and descending order, respectively. The mirror mode allows the
square matrix to fold into 128×128 blocks to perform a larger-size (k>128) CF at
the cost of doubled memory size. As memory leakage dominates the total power,
a data-shuffling scheme that is more efficient in utilizing memory space is
realized. In the shuffle mode, each row of L is stored in a shuffled order across
adjacent PE caches. As a result, the rows and columns of L can be accessed at
the same and at a different address of each PE-cache, respectively. To recover
the data order correctly, a shuffler performing circular position-shift is
implemented. Compared to the mirror-mode implementation, a 2× memory size
reduction results in a 40% lower total power.
Reconstruction signal-to-noise ratios (RSNR) from 1-minute recordings of real
ExG signals downloaded from the PhysioBank database are measured on the SA
engine (Fig. 18.5.4). CS samples of ExG signals are encoded by random
Bernoulli matrices, with a 5% overlapping window applied. In order to observe
the raw signal sparsity, no thesholding scheme is applied. The best orthogonal
basis for reconstructing ECG, EEG, and EMG are found to be the Haar discrete
wavelet transform (DWT), the discrete cosine transform (DCT), and a DWT-DCT
joint basis, respectively. It is also found that the RSNR performance is sensitive
to ∈. Dynamically configuring ∈ to 3–5% of the energy of each CS sample
results in the best RSNR performance. In general, using higher n for
reconstruction improves RSNR at the cost of lower throughput and higher
energy. To achieve RSNR>15dB with maximum throughput, the preferred n for
reconstructing the chosen ECG, EMG, and EEG recordings is found to be 256,
128, and 512, respectively.
The power and operating frequency of the SA engine are measured at different
supply voltages (Fig. 18.5.5). The MEP is found at VDD=0.7V, which corresponds
to a 12.8mW power and a 12.2MHz operating frequency. At the MEP, the SA
engine achieves a throughput of 237, 123, and 66KS/s and an energy efficiency
of 54, 104, and 194nJ/sample for reconstructing ECG, EMG, and EEG signals,
respectively, at RSNR>15dB. This throughput corresponds to the simultaneous
reconstruction of 237, 61, and 132 channels of ECG, EMG, and EEG data,
respectively. At VDD=1V, the maximum operating frequency of the SA engine is
25.3MHz. Compared to the MEP, a 2× higher throughput can be achieved at the
cost of 3× lower energy efficiency.
The SA engine is compared to an Intel Core i7-4700MQ processor and prior chip
implementations [3-4] of generic SA solvers (Fig. 18.5.6). Overall, the SA engine
achieves a 2× higher throughput with up to 14,100× better energy efficiency for
ExG signal reconstruction than the software solver running on the CPU. For
high-sparsity signal reconstruction, the SA engine is 76–350× more energy
efficient than prior hardware designs. With a <1% power budget of mobile
devices, the 5.13mm2 SA engine in 40nm CMOS (Fig. 18.5.7) can provide a
2–3× energy saving at CS-based sensor nodes, while providing timely feedback
and bringing signal intelligence closer to the user.
Acknowledgements:
We thank Broadcom Foundation for funding support and TSMC University
Program for chip fabrication.
References:
[1] F. Chen, et al., “Design and Analysis of a Hardware-Efficient Compressed
Sensing Architecture for Data Compression in Wireless Sensors,” IEEE J. SolidState Circuits, vol. 47, no. 3, pp. 744-756, 2012.
[2] A. Dixon, et al., “Compressed Sensing System Considerations for ECG and
EMG Wireless Biosensors,” IEEE Trans. on Biomedical Circuis and Syst., vol. 6,
no. 2, pp. 156-166, 2012.
[3] P. Maechler, et al., “VLSI Design of Approximate Message Passing for Signal
Restoration and Compressive Sensing,” IEEE J. on Emerging and Selected
Topics in Circuits and Syst., vol. 2, no. 3, pp. 579-590, 2012.
[4] P. Maechler, et al., “Implementation of Greedy Algorithms for LTE Sparse
Channel Estimation,” Asian Solid-State Circuits Conf, pp. 400-405, 2010.
[5] F. Ren, et al., “A Scalable and Parameterized VLSI Architecture for
Compressive Sensing Sparse Approximation,” Electronics Letters, vol. 49, no.
23, pp. 1440-1441, 2013.

978-1-4799-6224-2/15/$31.00 ©2015 IEEE

ISSCC 2015 / February 25, 2015 / 10:45 AM
CoreͲSRL Shared$ PEͲ$ “1”
IB IB

PipelinedAdderTree

SCͲ$ͲCTRL

ActiveSet

Xvalue Xloc

configbit

Controller

SCͲ$

PE

IB
On
On
Off
On
Off
Off

VCMUX
Static
Static
Dyn.
Static
Dyn.
Off

Core
-SRL
Off
Off
On
Off
On
Off

value
index



abs

T/F

SC
SCͲ$

SC

PE-$

SC-$

ACC, CMP R/ Residue Off
ACC
Off
Off
ACC, DIV
R/W L
R/ D
ACC
R/ L
W/ D
ACC, DIV
R/ LT
Off
ACC, ADD R/W residue R/W x

Share $ Active Set
Off
W/ IR
R/W IR
R IR
W/ IR
R/ IR

W/ Index
R/ Index
Off
Off
Off
R/ Index

IR = Intermediate Result, R/W = read/write, x = sparse coefficient

data
valid
ready

Memory

PESRL
Inner prod. Off
AS
Op 1 Inner prod. Off
Op 2
MAC
On
LS
Mult
Off
Op 3
Op 4
MAC
On
EU
MAC
On
Task

SCͲ$
÷

VCͲMUX



Share$

“0”

੣

PEinterconnectionwithIBactivated
2x
PE0
4x
PE1
PE2
8x
PE3
……
……
128x
Pipe.
PE124
PE125
PE126
PE127

SC

PEͲ$

PE

Selectiveresults

±

VC
PEͲ$ͲCTRL+Shuffler

Share$
VCͲMUX
PEͲ$

IB

÷

VCͲMUX

FPͲtoͲFLPConv.

PE

y

“1”
PEͲ$
A

CoreͲSRL

IB

A

IB
SRL

Shared$

Figure 18.5.2: PE and SC block diagrams and the dynamic configuration
scheme of the SA engine for executing the AS, LS, and EU tasks of the OMP
algorithm.

Figure 18.5.1: System architecture of the SA engine.

25
DOUTto
PEͲ0

addr 0 1
PEͲ$0 L41 x

2
x

3
x

0 1
L41 x

2
x

3
x

DOUTto
PEͲ0

PEͲ$1 L21 L22 L32 L42

L21 L22 L32 L42

PEͲ1

PEͲ$1 L31 L42 x

x

L31 L42 x

x

PEͲ1

PEͲ$2 L31 L32 L33 L43

L31 L32 L33 L43

PEͲ2

PEͲ$2 L21 L32 L43 x

L21 L32 L43 x

PEͲ2

PEͲ$3 L41 L42 L43 L44 L41 L42 L43 L44
1 2 3 4
1 2 3 4
Col.Access
RowAccess
0 0 0 0
0 0 0 0
Cir.Shift

PEͲ3

PEͲ$3 L11 L22 L33 L44 L11 L22 L33 L44
1 2 3 4
1 2 3 4
Col.Access
RowAccess
0 1 2 3
3 2 1 0
Cir.Shift

5

2

PEͲ$127

6

4

Power(mW)

60
40

132
{0.5,0.7}

69
{0.6,0.7}

2

5

2
3 4

5

0

103
{0.9,0.9}
72
{0.8,0.8}

10

15

0
0.2

Energy
Efficiency
(nJ/sample)
{Vlogic,Vmem}(V)

20

25

30

Frequency(MHz)
at MEP
ECG
Throughput
EMG
(KS/s)
EEG
ECG
Energy
Efficiency EMG
(nJ/sample) EEG

128
387
213
331
33
60
39

256
237
123
201
54
104
63

10

Signal Dimension (n)
384 512 640 768
102 79 64
38
54 41 24
20
87 66 54
32
125 163 200 337
238 312 536 640
147 194 238 399

n=512
m/nш0.4
k/nу0.13

6

159
{1,1}

59
91
{0.55,0.7} {0.65,0.7}

5

n=128x2
m/nш0.45
k/nу0.19

5

ShuffleMode

54
{0.7,0.7}

20

15

1

6

Figure 18.5.3: The memory access and folding scheme of PE caches for
handling Cholesky factorization in mirror and shuffle mode. Column and row
access is needed for solving FS and BS, respectively.
80

n=256
m/nш0.35
k/nу 0.13

1

3

$127
2xmem.size
reduction

MirrorMode

RSNR(dB)

ĂĂ

>>

6

4

k

Ă Ă

3

3

4

1

addr
$0

5

1
2

Ă Ă

addr
PEͲ$0

20

PEͲ3

ĂĂ

>>

ĂĂ

0 1 2 3
L11 L21 L31 L41

ĂĂ

addr 0 1 2 3
PEͲ$0 L11 L21 L31 L41

896 1024
33
29
13
12
27
19
390 444
954 1087
466 685

Figure 18.5.5: Power vs. frequency measured at different VDD, measured
throughput and energy efficiency for ExG reconstruction when operating at the
MEP (the highlighted numbers are the best performance for achieving
RSNR>15dB).

0.3

ECG
EEG
EMG

0.4
0.5
0.6
CompressionRatio(m/n)

0.7

Figure 18.5.4: Measured RSNR of ECG, EEG, and EMG signals reconstructed
on the DWT, the DCT, and joint DWT-DCT basis, respectively, with different
signal dimensions (n), compression ratios (m/n), and signal sparsity (k/n).
Design
Intel i7-4700MQ
[3]
Technology
22nm
180nm
General
Target app
LTE channel estimation
Algorithm
OMP/AMP
MP
GP
OMP
Signal dimension
Large
256
Measurement dim.
Large
200
Sparsity level
Large
50
18
10
Core area (mm2)
174.4
0.73
1.21
2.42
Dynamic range
High (float-pt)
Low (fix-pt)
PE parallelism
SSE4
2
8
Local memory (KB)
7,424
Freq. (MHz)
2,394
140
128
Throughput (KS/s)
5 to 98(3)
2
Power (mW)
47,000
88
209
200
451,322
2,444 5,778 11,222
Energy Efficiency*
(nJ/sample)
503,028
-

[4]
65nm
Audio
AMP
512(1)
512
0.629
Low (fix-pt)
32
5.76
333
397
177.5
223

This work
40nm
Biomedical sensing
OMP, K-OMP
up to 1024
up to 512
up to 192
5.13
High (float-pt)
128
147
27.4
12 to 237(2)
8.6 to 78
32(4) (high sparsity)
389(5) (low sparsity)

* Technology scaling to 40nm: Delay~1/S, Power~1/U2, where S=L/40nm, U=VDD/0.9V.
1 The supported problem size is 1024x512, but only half of the sampling matrix is generic.
2 ExG reconstruction throughput measured at MEP.
3 ExG reconstruction throughput measured in MATLAB simulation.
4,5 For fair comparison, the numbers are measured for the same problem size (m,n,k) as in [3,4].

Figure 18.5.6: Comparison with an Intel Core i7-4700MQ processor and prior
chip implementations of the generic SA solver.

DIGEST OF TECHNICAL PAPERS •

335

18

ISSCC 2015 PAPER CONTINUATIONS

2.28mm
7HVWLQJ0HPRU\
3(
9&3(
3(
9&3(

Technology

40nm 1P8M
CMOS
FO4 16.3ps (TT)

Transistor
Flavor

SVT 0.11%
HVT 99.89%

3(
9&3(

2.25mm

3(
9&3(
Active Set 6KDUHG

3(
6&
3(

6&

9&3(

Transistor
Count

61 M

I/Os

Digital 42/58
Power 156

Core VDD

Logic: 0.5 to 1 V
Mem: 0.7 to 1V

3(
9&3(
3(
9&3(
3(

I/O VDD

2.5 V

Core Size

2.28 x 2.25 mm

9&3(
3(

7HVWLQJ0HPRU\

Figure 18.5.7: Die photo and chip summary.

• 2015 IEEE International Solid-State Circuits Conference

978-1-4799-6224-2/15/$31.00 ©2015 IEEE

A DATA-DRIVEN COMPRESSIVE SENSING FRAMEWORK TAILORED FOR
ENERGY-EFFICIENT WEARABLE SENSING
Kai XU, Yixing Li, Fengbo Ren

arXiv:1612.04887v2 [cs.LG] 16 Dec 2016

Arizona State University
ABSTRACT
Compressive sensing (CS) is a promising technology for
realizing energy-efficient wireless sensors for long-term
health monitoring. However, conventional model-driven CS
frameworks suffer from limited compression ratio and
reconstruction quality when dealing with physiological
signals due to inaccurate models and the overlook of
individual variability. In this paper, we propose a data-driven
CS framework that can learn signal characteristics and
personalized features from any individual recording of
physiologic signals to enhance CS performance with a
minimized number of measurements. Such improvements
are accomplished by a co-training approach that optimizes
the sensing matrix and the dictionary towards improved
restricted isometry property and signal sparsity, respectively.
Experimental results upon ECG signals show that the
proposed method, at a compression ratio of 10x, successfully
reduces the isometry constant of the trained sensing matrices
by 86% against random matrices and improves the overall
reconstructed signal-to-noise ratio by 15dB over
conventional model-driven approaches.
Index Terms— Data-driven compressive sensing, mobile
healthcare, wearable sensing, Internet of things (IoT)
1. INTRODUCTION
1.1. Background
The existing healthcare model based on episodic
examination or short-term monitoring for disease diagnosis
and treatment suffers from the overlook of individual
variabilities and the lack of personal baseline data.
Long-term or non-intermittent monitoring is the key to
creating the big data of individual health record for studying
the variability and obtaining the personal baseline. Recent
advances in wireless body area network (WBAN) and
bio-sensing techniques have enabled the emergence of
miniaturized, non-invasive, cost-effective wireless sensors
that can be placed on human bodies for personal health
monitoring [1]. Through WBAN and Internet, the monitored
data can be transmitted to a near-field mobile device for
on-site processing, as well as to remote servers for storage
and data analysis. These technology advancements will

eventually revolutionize the health related services to
become more efficient and economical, benefiting billions of
individuals.
One of the key challenges faced by the long-term
wireless health monitoring is the energy efficiency of sensing
and information transfer. Due to the limited battery capacity
of wireless sensors, non-intermittent sensing inevitably
increases the frequency of battery recharging or replacement,
making it less convenient for practical use. In bio-sensing
applications, the energy cost of wireless transmission is
about two orders of magnitude greater than other
components [2]. This implies that reducing the data size for
information transfer is the key to improving the energy
efficiency of wearable sensors.
Compressive sensing (CS) [3] offers a universal and
straightforward data encoding scheme that can compress a
variety of physiological signals, providing a promising
solution to the problem.
However, most existing CS
frameworks are model-driven and suffer from very limited
performance when dealing with physiological signals
[4, 5, 6]. The reasons are two-fold. First, conventional CS
frameworks employ random Gaussian or Bernoulli sensing
matrices that are generated independently from any data,
thereby they fail to leverage any particular geometric
structure embedded in the signals of interest. This limits the
rank of the sensing matrix required for preserving the
Restricted Isometry Property (RIP), leading to limited
compression ratio (CR). On the other hand, conventional CS
frameworks [7, 8, 5] that adopt predetermined basis for
reconstruction underestimate the intricacy of philological
signals and overlook the criticality of individual variability to
signal fidelity, which results in very limited reconstruction
performance especially at high CR [6]. Our previous study
[9] has shown that learned dictionaries can better
approximate the underlying statistical model of input data.
Therefore, they can significantly improve the sparsity of
physiological signals as well as reconstruction performance.
1.2. Relation to Prior Work
There have been some recent work on exploiting data
structures for compressive sensing [10, 11, 12]. In [10], the
authors aim to minimize the averaged mutual coherence

between sensing matrix and dictionary. The major limitation
of this work is that the mutual coherence is not a direct
indicator of RIP, so the optimization result is not suitable for
sensor applications. In [11], the authors aim to find a sensing
matrix Φ and a dictionary Ψ such that the Gram matrix of the
product ΦΨ is as close to the identity matrix as possible. The
problem is that the Gram matrix can hardly be the identity
matrix in practice as Ψ is usually over-complete, so the result
is sub-optimal. In [12], the authors aim to preserve the
pairwise distance between sample vectors. However, since
the NuMax formulation minimizes the transformation
distortion against the original signal rather than its sparse
coefficient, the trained sensing matrix is not compatible with
any over-complete dictionaries. Therefore, these existing
approaches are not ideally suitable for the CS of
physiological signals in wearable sensing applications.
In this paper, we propose a data-driven CS framework
that co-optimizes the sensing matrix and the dictionary
towards improved restricted isometry property (RIP) and
signal sparsity, respectively, by exploiting the intrinsic data
structure of physiological signals.
Specifically, online
dictionary learning (ODL) [13] is first adopted to train a
personalized basis that further improves signal sparsity by
capturing the characteristics and individual variability of
physiological signals. Based on the learned dictionary, a
distortion minimization problem is formulated to construct a
near-isometry and low-rank sensing matrix to guarantee a
satisfactory recovery performance at improved compression
ratios. Overall, the proposed framework keeps the promise to
significantly enhance the reconstruction quality and CR
trade-off for the CS of physiological signals.
The data-driven nature of the proposed CS framework is
very appealing because it fills the gap between the massive
medical data and how to utilize them to improve the quality
of sensing. The key insight from this study is that the sensor
energy efficiency can be enhanced by learning the intrinsic
signal structures from big data through cost-effective
computation on server systems, rather than doing costly
circuit-level development.
Moreover, the proposed
data-driven framework is equally applicable to a variety of
physiological signals and has the potential to be consistently
improved as more and more data is collected for training.
2. PRELIMINARIES
When fully implemented in the digital domain, CS can be
considered as a dimensionality reduction technique for signal
compression. Assuming a signal f can be represented by a
sparse vector θ ∈ Rk on a certain basis Ψ ∈ Rn×k , i.e.,
f = Ψθ, the signal information x can be well preserved by
projecting f onto a low-dimension space through a sensing
matrix Φ ∈ Rm×n , (m ≤ n and Φ should satisfy 2), given as
y = Φf + z = ΦΨθ + z,

(1)

where z is a noise term.
For robust reconstruction, the matrix Φ should satisfy the
RIP [14] for all k-sparse signal x, defined as
(1 − δK kxk22 ≤ kΦxk22 ≤ (1 + δK kxk22 )).

(2)

When the RIP holds, Φ approximately preserves the
Euclidean norm of all k-sparse signals. Then the sparse
coefficient can be solved the following `-1 minimization
problem with a relaxed constraint,
min kθk1

θ∈Rk

s.t. ky − ΦΨθk2 ≤ ε.

(3)

If matrix
A ∈ Rm×n satisfies the RIP of order 2k with
√
δ2k < 2 − 1, the solution to 3 is equivalent to the original
signal with overwhelming probability [15]. In addition, we
have
kx∗ − x̂k`2 ≤ C ·

kx − xk k`1
√
,
k

(4)

where x ∈ Rn is the input signal, xk is the k-sparse
approximation, and x̂ is the solution to 3, and C is a constant
which is proportional to the isometry constant δ2k . Eq. 4
means a smaller isometry constant guarantees a smaller
recovery error, which is suitable for target applications.
3. FRAMEWORK ARCHITECTURE
3.1. Architecture Overview
The architecture of the proposed framework is shown in Fig.
1. It is composed of three functional units, including a
training unit, a CS sampling unit and signal recovery unit
performed on server, sensor and mobile nodes, respectively.
Since physiological signals can vary among different
patients, a generic basis for all patients usually perform
poorly. The dictionary learning module trains personalized
basis that captures individual-specific features that are
critical to CS recovery, which guarantees a higher sparsity
than predetermined basis. Here we employ ODL as the
method for dictionary learning. The most notable advantage
of ODL is that it does not rely on the matrix factorization
upon the entire training data. As a result, the computational
complexity is much less compared to the non-online
approaches especially for handling large training data.
Before ODL is performed, the raw physiological signals
must be pre-processed to remove baseline wandering and
high-frequency interference. This is essential to achieving a
high signal reconstruction quality. Once the dictionary is
learned, it can be downloaded to the mobile node to perform
accurate signal recovery.
In the proposed framework, the sensing matrix training
(SMT) generates a data-specific sensing matrix with
minimized rank and a small isometry constant. A small rank
further reduces the data size for transmission, and a smaller

Training Database
Individual recordings of
physiological signal

Sensing Matrix Training
Improve compression ratio
and recovery quality

Data Preprocessing



Online Dictionary Learning
Improve recovery quality at
high compression ratios

Over-complete and
sparsifying dictionary 

Near-isometry and lowrank sensing matrix 

Server Node

EEG

EEG

Compressive Sampling
Data compression

ECG

EMG

Sensor Node

 y1
 

 y M


 



 f1

 f N

 

Sparse
Approximation






min 

x R K

1

s . t ., y  

Signal
Reconstruction
 fˆ1 
 x1 




       
 fˆ N 
 x K 



ECG

Mobile
Node

EMG

Fig. 1: Block diagram of the proposed data-driven compressive sensing framework.

isometry enhances reconstruction quality denoted by 4. Once
the sensing matrix is trained, it can be downloaded to the
sensor node to perform effective compression of
physiological signals for energy-efficient sensing and
information transfer.

Candès and Tao prove that if the sensing matrix Φ satisfies
the RIP, then `-1 minimization algorithms can successfully
recover a sparse signal from noisy measurements [14]. Here
we formulate an optimization problem that directly optimizes
the RIP towards lower isometry constant δ and lower rank of
the sensing matrix Φ in 5.
(1 − δ)kθk2 ≤ kΦΨθk2 ≤ (1 + δ)kθk2 ,

(5)

where θ is the sparse coefficient vector under the dictionary
Ψ. 5 is equivalent to
|kΦΨθk2 − kθk2 | ≤ δ,

(6)

when θ is normalized.
Suppose we have L sparse coefficients, θi , i = 1, . . . , L
, the optimization problem is essentially to guarantee each of
them will satisfy 6, which can be then reformulated as

i = 1, . . . , L.

(7)

Assume A = ΦΨ, Y = AT A, 7 can be represented as
|θiT (Y − I)θi | ≤ δ,

i = 1, . . . , L.

(8)

As the rank of the sensing matrix implies the data size for
transmission after compression, we also aim to minimize the
rank of the sensing matrix in 9. Since the rank minimization
problem is not convex, we use the nuclear norm as a proxy to
relax the problem to 10.
min {|θiT (Y − I)θi |, rank(Y)},
Y

s.t.

i = 1, . . . , L
(9)

Y  0,
T

diag(Y) = [1, 1 , . . . , 1] .

(δ + βkYk∗ )

s.t.

Y  0,

Y

diag(Y) = [1, 1 , . . . , 1]T
|θiT (Y − I)θi | ≤ δ,

3.2. Sensing Matrix Training (SMT)

|θiT (ΨT ΦT ΦΨ − I)θi | ≤ δ,

min

(10)

i = 1, . . . , L.

where β is the penalty parameter for the nuclear norm. Then,
we perform an Cholesky decomposition to obtain the matrix
A, and a singular value decomposition (SVD) to derive the
sensing matrix Φ, as defined in 11 and 12, respectively.
Y = AT A = USUT , A = (Usqrt(S))T

(11)

Ψ = USVT , Ψ† = VS−1 UT , Φ = AΨ†

(12)

3.3. Online Dictionary Learning (ODL)
We seek the dictionary that gives the best representation of
every item in the training dataset under the sparsity
constraint. The advantage of learning dictionaries from
individual recordings of physiological signals is that it
provides much better sparse representations than
model-driven approaches by exploiting the rich information
embedded in the training data. ODL offers faster training
speed and fewer storage requirements because of the online
processing nature. It is also possible to add new features to
the dictionary without stalling the reconstruction using ODL,
which offers a mechanic of melioration when a distinctive
input is received. Due to the page limit, we would like to
refer the readers to [13] for details of ODL.
3.4. Co-training of sensing matrix and dictionary
(CTSMD)
We aim to jointly improve signal sparsity and isometry
constant through a co-training approach. The proposed
CTSMD algorithm is described in Algorithm 1. One should
note that the proposed CTSMD algorithm is a non-iterative
process. Empirical results show that one round of CTSMD is
sufficient to obtain a well-defined results.

0.7

Algorithm 1 Pseudocode for CTSMD

0.6

t

2)

1X1
Ψt = arg min
kxi − Ψθi k22 + λkθi k1 ,
t i=1 2
Ψ

Sensing matrix training:
3) min(δ + βkYk∗ )

The proposed SMT method
Gaussian random sensing matrix

0.5

Isometry constant δ

Input: x ∈ Rn , Ψ0 ∈ Rn×k , λ, β,
Output: Φ, Ψ,
Online dictionary learning:
1
1) θt = arg min kxt − Ψt−1 θk22 + λkθk1 ,
θ∈Rn 2

0.4
0.3
0.2
0.1
0

2

3

4

8

9

10

Fig. 2: Isometry constant under different compression ratios.

Y

25

s.t. Y  0,
diag(Y) = [1 1 . . . 1],

Proposed method (SMT+ODL)
RandomSensing+ODL
SMT+DCT-DWT
RandomSensing+DCT-DWT

20

i = 1, . . . , L,

4)Y = AT A = USUT , A = (Usqrt(S))T ,
5)Ψ = USVT , Ψ† = VS−1 UT , Φ = AΨ† .

4. EXPERIMENTS
4.1. Experimental setup
Real electrocardiogram (ECG) data from the MIT-BIH
arrhythmia ECG database [16] is used to benchmark the
proposed framework. The customized solver is used for
ODL problem and CVX solver [17] is used to solve the SMT
problem. Due to the large memory requirement of CVX, our
experiments are subjected to limited problem size, which has
cost a certain performance degradation across our algorithm.
Here we extract 3600 samples, and each sample has a
dimension of 128. 3000 and 600 samples are used for
training and testing, respectively. The training data is first
used with the CTSMD algorithm to construct the sensing
matrix and the reconstruction dictionary, which are then used
to perform CS measurement and signal reconstruction on the
testing data. Three reference approaches are compared in our
experiments, i.e. random Gaussian sensing matrix with
trained dictionary by ODL, trained sensing matrix by SMT
with predetermined discrete cosine and wavelet transform
(DCT-DWT) dictionary, and random Gaussian sensing
matrix with a predetermined DCT-DWT dictionary.
CR = n/m and reconstructed signal-noise ratio (RSNR)
= kxk2 /kx − x0 k2 are used as the performance metrics,
where n is the dimension of original signal x, m is the
number of measurements, and x0 is the reconstructed signal.
4.2. Experiment Results
The isometry constant of the trained sensing matrix with
respect to CR is shown in Fig. 2. Note that the sensing
matrices produced by the proposed framework have reduced

RSNR (dB)

|θiT (Y − I)θi | ≤ δ,

5
6
7
Compression ratio n/m

15
10
5
0

2

3

4

5
6
7
Compression ratio

8

9

10

Fig. 3: RSNR under different compression ratios.

the isometry constant by over 80% over the Gaussian
random matrices across all the CRs. The reduced isometry
constant implies better preservation of the signal’s geometry
structure in the compressed domain. According to 4, such
improvement will lead to a higher reconstruction accuracy.
The RSNR results at different CR are shown in Fig. 3.
By using SMT and ODL, RSNR is increased about 5dB and
10dB, respectively. Overall, the proposed data-driven method
achieves a 15dB improvement of RSNR over the model-based
approach across all different CRs.
5. CONCLUSION
In this paper, we propose a data-driven CS framework
tailored for the energy-efficient wearable sensing of
physiological signals. Exploiting the structure of data is the
key to enhancing CS performance. Specifically, the SMT
reduces the isometry constant in RIP, and the ODL improves
signal sparsity, which are both critical to providing a better
recovery performance under improved compression ratios. In
future works, we plan to develop customized solver for the
SMT problem to handle large dataset. We also need to add
binary constraint to SMT for efficient sensor hardware
implementations. This will benefit the hardware and energy
cost of mobile sensors, which enables the data-driven
technique to be used in practical IoTs applications.

6. REFERENCES
[1] Guy Pare et al.,
“Systematic review of home
telemonitoring for chronic diseases: The evidence
base,” Journal of the American Medical Informatics
Association, 2007.
[2] F. Chen et al., “Design and analysis of a hardwareefficient compressed sensing architecture for data
compression in wireless sensors,” IEEE Journal of
Solid-State Circuits, vol. 47, no. 3, pp. 744–756, Mar.
2012.
[3] E. J. Cands, “Compressive sampling,” Proceedings of
the International Congress of Mathematicians., 2006.
[4] L. F. Polania et al., “Compressed sensing based method
for ecg compression,” in IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
May 2011, pp. 761–764.
[5] M. Abo-Zahhad et al., “Compression of ecg signal
based on compressive sensing and the extraction
of significant features,”
International Journal of
Communications, Network and System Sciences, vol. 8,
pp. 97–117, 2015.
[6] F. Ren and D. Markovic, “18.5 a configurable 12to-237ks/s 12.8mw sparse-approximation engine for
mobile exg data aggregation,” in IEEE International
Solid State Circuits Conference (ISSCC), Feb. 2015, pp.
1–3.
[7] L.F. Polania et al., “Compressed sensing based method
for ecg compression,” in IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
May 2011, pp. 761–764.
[8] Seungjae Lee, Jun Luan, and P.H. Chou, “A new
approach to compressing ecg signals with trained
overcomplete dictionary,” in EAI 4th International
Conference on Wireless Mobile Communication and
Healthcare (Mobihealth), Nov. 2014, pp. 83–86.
[9] K. Xu et al., “An energy-efficient compressive sensing
framework incorporating online dictionary learning for
long-term wireless health monitoring,” in 2016 IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), Mar. 2016, pp. 804–808.
[10] M. Elad,
“Optimized projections for compressed
sensing,” IEEE Transactions on Signal Processing, vol.
55, no. 12, pp. 5695–5702, Dec. 2007.
[11] J.M. Duarte-Carvajalino and G. Sapiro, “Learning to
sense sparse signals: Simultaneous sensing matrix and
sparsifying dictionary optimization,” Image Processing,
IEEE Transactions on, vol. 18, no. 7, pp. 1395–1408,
July 2009.

[12] C. Hegde et al., “Numax: A convex approach for
learning near-isometric linear embeddings,” Signal
Processing, IEEE Transactions on, vol. 63, no. 22, pp.
6109–6121, Nov. 2015.
[13] Julien Mairal et al.,
“Online learning for matrix
factorization and sparse coding,” J. Mach. Learn. Res.,
vol. 11, pp. 19–60, 2010.
[14] E. Candes, J. Romberg, and T. Tao,
“Stable
Signal Recovery from Incomplete and Inaccurate
Measurements,” ArXiv Mathematics e-prints, 2005.
[15] Emmanuel J. Cands, “The restricted isometry property
and its implications for compressed sensing,” Comptes
Rendus Mathematique, vol. 346, no. 9, pp. 589 – 592,
2008.
[16] Goldberger et al., “PhysioBank, PhysioToolkit, and
PhysioNet: Components of a new research resource for
complex physiologic signals,” Circulation, vol. 101, no.
23, pp. e215–e220, June 2000.
[17] Michael Grant and Stephen Boyd, “CVX: Matlab
software for disciplined convex programming, version
2.1,” http://cvxr.com/cvx, 2014.

2932

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 60, NO. 11, NOVEMBER 2013

Reference Calibration of Body-Voltage Sensing
Circuit for High-Speed STT-RAMs
Fengbo Ren, Student Member, IEEE, Henry Park, Student Member, IEEE, Chih-Kong Ken Yang, Fellow, IEEE,
and Dejan Marković, Member, IEEE

Abstract—With the continuing scaling of MTJ, the high-speed
reading of STT-RAM becomes increasingly difficult. Recently,
a body-voltage sensing circuit (BVSC) has been proposed for
boosting the sensing speed. This paper analyzes the effectiveness
of using the reference calibration technique to compensate for
the device mismatches and improve the read margin of BVSC.
HSPICE simulation results show that a 2-bit reference calibration
can improve the worst-case read margin in a 1-Mb memory by
over 3 times. This leads to up to 30% higher yield across all
process corners. In order to maintain the yield improvement even
in the worst-case corner, independent calibration circuitry has to
be deployed for each memory array.
Index Terms—Body-voltage sensing, CMOS, magnetic tunnel
junction (MTJ), nonvolatile memory, read margin, reference
calibration, sensing margin, spin-transfer torque random access
memory (STT-RAM).
Fig. 1. Scaling trend of MTJ switching current according to the Grand is
STT-RAM roadmap [7].

I. INTRODUCTION

S

PIN-TORQUE transfer RAMs (STT-RAMs) have been
the subject of extensive research in the past several years.
STT-RAM is often perceived as the “universal memory” due
to its potential for high density, low energy, and high speed.
Prototypes incorporating smaller cell size than SRAM, better
performance than DRAM, non-volatility of Flash, and the
endurance on the order of
read/write cycles have been
reported [1]–[8]. Moreover, the switching current reduction,
driven by the dimension and critical current density
scaling of the magnetic tunnel junction (MTJ), has been
pushing down the power consumption of STT-RAM toward
embedded and mobile applications [9]–[15].
With the continuing scaling of MTJ, the high-speed reading
of STT-RAM becomes increasingly difficult, not only because
both the CMOS and MTJ variability keep increasing, but also
the switching current of MTJ will reach the order of 10
,
which can be very challenging for reliable high-speed sensing
(Fig. 1) [16]. To improve the sensing, our previous work [17]
implements the concept of short pulse reading (SPR) [16] to
allow a higher read current for better sensing speed. This paper

Manuscript received September 27, 2012; revised January 24, 2013; accepted
February 15, 2013. Date of publication April 02, 2013; date of current version October 24, 2013. This work was supported by the DARPA STT-RAM
(HR0011-09-C-0114) program. This paper was recommended by Associate Editor M. M. Khellah.
The authors are with the Department of Electrical Engineering, University of
California, Los Angeles, CA 90095 USA (e-mail: fren@ee.ucla.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TCSI.2013.2252653

enhances the reliability of the proposed body-voltage sensing
circuit (BVSC) [17] by adding the capability of calibrating the
reference voltage level. The enhanced BVSC features shorter
sensing time for higher sensing margin and less read disturbance
as compared to prior sensing circuits, which makes the scheme
suitable for future technology scaling.
The key component in the BVSC approach is the body-connected load [17], [18]. The body-connected load utilizes bodyvoltage modulation (BVM) to adjust the sensing voltage according to the sensing current. While the other types of sensing
circuits that adopt the diode-connected or the current source
load suffer from deficiency either in sensing margin or speed,
the BVSC is optimized to support both features [17]. However, BVM is more sensitive to the threshold voltage variation
as compared to gate-voltage modulation (GVM) [17]. As a result, a shifting of the sensing voltage in the worst case corner
would deteriorate the effective sensing margin of BVSC. If the
reference voltage level is to be fixed, the corresponding read
margin would also be degraded. This paper explores the feasibility of using a reference calibration technique to recover the
read margin loss due to process variations for BVSC. The main
motivation is to improve the stability of BVSC for better yield
of STT-RAMs. The next section briefly reviews the SPR concept and the BVSC with the definitions of sensing margin and
read margin. Section III describes the main idea of the reference calibration in details. Section IV discusses simulation results showing that a simple 2-bit reference calibration improves
the worst-case read margin in a 1-Mb memory by over 3 times,
leading to a significant yield increase (up to 30%) across all corners. Conclusions are presented in Section V.

1549-8328 © 2013 IEEE

REN et al.: REFERENCE CALIBRATION OF BODY-VOLTAGE SENSING CIRCUIT FOR HIGH-SPEED STT-RAMs

2933

Fig. 2. Basic MTJ structure. Switching current from the fixed (free) to the free
(fixed) layer switches the MTJ into a parallel (anti-parallel) state.

II. HIGH-SPEED READING OF STT-RAM THROUGH BVSC
A. STT-RAM and SPR
MTJ is the storage element of STT-RAM. It consists of two
ferromagnetic layers separated by a thin nonconductive tunneling barrier (e.g., MgO) as shown in Fig. 2. The thicker ferro
magnet with fixed magnetic orientation is called the fixed layer
or the pinned layer. The thinner layer with flexible magnetic
orientation is called the free layer. The MTJ exhibits two resistive states determined by the relative magnetization directions
of the fixed and free layers: a parallel (P) orientation produces a
low resistance
and an anti-parallel (AP) orientation results
in a high resistance
. The resistance difference between
the two states is measured by the tunnel magneto-resistance
ratio (TMR), defined as
. A higher TMR indicates better readability and is thereby preferred by the reading
operation.
In STT-RAMs, data is stored in MTJs in a magnetic form: “0”
and “1” are represented by magnetization direction of the free
layer. The switching of the MTJ can be controlled by a bi-directional writing current as shown in Fig. 2: the current in the
direction from the fixed (free) to the free (fixed) layer writes the
MTJ into the AP (P) state. As shown in Fig. 3, the switching
probability of MTJ can be characterized as a function of both
the switching current and the switching time (duration of the
switching current) [19]. In STT-RAM design, the sensing current distribution has to be kept within the 0% switching probability region in order to avoid destructive read [17]. Note that 0%
switching probability corresponds to low read currents for long
read durations and high read currents for short read durations.
Different from the low current reading (LCR) scheme, the SPR
scheme senses the cell orientation with a current that is close
in amplitude to the writing current but with much shorter pulse
to improve the sensing speed without risking read disturbance
[17].
B. BVSC
A schematic of the sensing stage of BVSC is shown in Fig. 4.
In the sensing circuit, the resistance difference between the
and
states is captured by a sensing current difference
,
which is converted into a voltage difference
through a
load transistor
. The conversion ratio from
to
is
given by the small-signal resistance
of the load transistor as
(1)

Fig. 3. The switching characteristic of the MTJ [19] with illustrations of the
sensing current distribution in the short pulse reading (SPR) and the low current
reading schemes.

Fig. 4. The schematic of the sensing stage of BVSC.

Note that the two sensing voltages
and
are strongly affected by the process and parametric variations in CMOS and
MTJ devices. Using the sensing voltage statistics, we define the
worst-case margin between
and
as the sensing margin
(SM), given by
(2)
According to our previous study [17], a small
(as in a
diode-connected load) leads to a high sensing speed but low
sensing margin. On the other hand, a large
(e.g., a current source load) results in slow speed and large variation of the
sensing voltage, leading to limited sensing margin. In order to
provide large sensing margin while maintaining sensing speed,
BVSC uses a body-connected load, which has an effective
5–6 times bigger than that of the diode-connected load,
but 2–3 orders of magnitude smaller than that of the current
source load. As a result, BVSC offers balanced sensing speed
vs. sensing margin tradeoff.

2934

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 60, NO. 11, NOVEMBER 2013

The sensing margin defined in (2) statistically characterizes
the quality of resistance sensing in presence of device variability, but it does not show the exact voltage difference seeing
at the sense amplifier input when reading a memory cell. In order
to capture that, we define the read margin (RM) as the readability of the sensing voltage given a reference voltage
and the input-referred offset
of a sense amplifier. According to the resistance state of the memory cell being read,
we have
and
defined as
(3)
and
(4)
Respectively. The overall RM is defined as the worst case of the
two,
(5)
should be equal to the common-mode level of the
Ideally,
sensing voltage, as
, and it can be generated by a voltage divider network that connects the
of
two sensing stages that sense
and
, respectively. However, for the memory cells in an array sharing the same sense
amplifier,
and
are subject to process variations and could
be independent variables, whereas,
and
have to be
common factors. As a result, the optimal
for each array
is subject to the actual distribution of the sensing voltages and
should be determined on a case-by-case basis. In order to find
the optimal
for maximizing the read margin, a reference
calibration method is proposed and discussed in the following
sections.
III. DEVICE VARIATION AND REFERENCE CALIBRATION

Fig. 5. I-V curves of the load and clamp transistors in the sensing stage of
BVSC illustrating the impact of process variations on the sensing behavior.

between the source and the body regions of the PMOS transistor
can be turned on if
is well below
.
This junction leakage would cause BVM to haveweaker control and the effective
to increase (Fig. 5). As a result,
the effective sensing margin would be reduced if the operating
point of
is shifted beyond the inflection point
shown in Fig. 5 due to process variations. This effect is the most
prominent in the fast-NMOS slow-PMOS (FS) corner. Fig. 6
shows the histograms of
in a 1 Mb memory at typical
and FS corners. During
sensing,
is in deep saturation
such that its
drop will vary substantially if the sensing current changes due to process variation. On the other hand, during
sensing, the sensing current at FS corner is sufficiently high
such that
may enter the linear region. As a result, the
distribution shows relatively larger shift between the two corners (typical and FS) than that of the
distribution as shown
in Fig. 6, and the effective sensing margin is degraded in the FS
corner. Such shifting would cause read errors if a fixed
is
used. Therefore, a reference calibration scheme that can adaptively adjust
according to the sensing voltage statistics is
desired in order to best utilize the sensing margin, especially at
the FS corner.

A. Impact of CMOS and MTJ Variations

B. Reference Calibration

The device variation of MTJ can be lumped into independent Gaussian variations of
and TMR [20], [21]. The effect
of such variations together with the variation of access transistors on the sensing behavior can be visualized as shown in
Fig. 5. The right hand side of Fig. 5 shows the I-V curves of
the clamp transistor
and the MTJ cell. Depending on the
resistance state of MTJ, the cross-point of the two curves determines the sensing current,
. The left hand side of Fig. 5
presents the I-V curve of the load transistor
. Thus, by projecting the same
onto the I-V curve of
, the sensing
voltage,
, can be obtained. The variation of each device
contributes to the variation of
, yet at different levels.
In memory design, different memory arrays are usually driven
by independent sensing stages. For each array, the variations of
and
tend to affect its
statistics globally—they
shift the mean of
distribution of the whole array. On the
other hand, the MTJ device variation tends to populate
of each single cell around the mean locally. These combined
global and local deviations may cause substantial yield loss if a
fixed global
is used.
Another potential problem which may reduce the effectiveness of the body-connected load is that the P-N junction formed

As multiple MTJ cells in the same array share a sensing
circuit, the variations of
and
can be compensated by
calibrating the
level at the sense amplifier input. The
generation of multiple reference levels can be implemented by
using resistor taps as shown in Fig. 7. By choosing the optimal
level through the configuration bit (SEL), the read
margin does not degrade as much from the device mismatch
and hence the chance of reading errors reduces. This concept is
illustrated in Fig. 8. The distributions of
and
for reading
the whole memory array are modeled as Gaussian distributions
with different mean and standard deviation. The band bounded
by dashed lines around
represents the zone where
reading errors would occur, if
falls within the band, due
to device mismatch of the sense amplifier and random noises.
The width of the error zone can be characterized by
of
the sense amplifier and the target noise margin (NM). Note
that since
is usually generated from sensing a separate
reference array other than regular memory arrays,
may
vary independently from
. Fig. 8(a) illustrates an example
where the
distribution of the whole array is shifted
toward the right-hand side of
. In this case, the worst-case
read margin of the whole array might be tiny or even negative

REN et al.: REFERENCE CALIBRATION OF BODY-VOLTAGE SENSING CIRCUIT FOR HIGH-SPEED STT-RAMs

2935

Fig. 8. Illustration of reference calibration used to improve the worst-case read
margin. (a) Before calibration, (b) after calibration.

Fig. 6. The
distribution of a 1-Mb memory with BVSC at: (a) the typical and (b) the FS corners.

become more balanced such that the worst case read margin is
significantly improved.
For analytical purpose, the reading error is modeled mathematically as follows. Suppose that ,
and
,
are
the mean and standard deviation of
and
distributions,
respectively, and
and
represent the distance from the
boundary of the error zone to
and
, respectively. Then,
the probability for a read error to occur when reading a memory
array with N MTJ cells is the complement probability of all the
to be distributed out of the error zone, as given by

(6)
Fig. 9 plots the read error probability color map as a function
of
and
. As
and
increase,
which implies less device variability, the error probability goes
down exponentially. For practical design, the sum of and is usually limited by the sensing margin and is assumed to be fixed.
In this case, the minimum is achieved when
(7)

Fig. 7. Proposed reference calibration scheme. Resistor taps generate the
levels for digital calibration.

and reading errors are very likely to occur. However, this can
be fixed by calibrating the
level as shown in Fig. 8(b). As
increases, the read margin for reading P and AP states

as indicated in Fig. 9. Therefore, the primary goal of reference
calibration is to provide the best available read margin for the
level that satworst-case reading by choosing the optimal
isfies (7). Ideally, this can be done with continuous tuning of
the reference levels. In practice, there is a tradeoff between increasing the number of configuration bits (granularity of
)
and chip area.
The detailed calibration algorithm is shown in Fig. 10. The
algorithm begins by setting
to the middle node of the
resistor taps,
. The lower
and upper
bounds of the preferred
is determined as the
boundary between a successful and a failed read-after-write operation on data pattern “0” and “1,” respectively. Therefore, the

2936

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 60, NO. 11, NOVEMBER 2013

Fig. 11. Block diagram of the STT-RAM architecture with the reference
calibration technique applied. SS: Sensing Stage, CC: Calibration Circuitry,
SR: Shift Register.
Fig. 9. Read error probability for a 512-cell memory array as a function of the
and
). Numbers
sensing margin to device variability ratio (
on the contour lines represent error exponent (see vertical bar on the right).

TABLE I
SUMMARY OF MTJ PARAMETERS

EEPROM which automatically loads control datainto the local
registers for selecting the optimal
during reset or initialization. Therefore, the calibration process only needs to be performed by once after the fabrication. In the case of the device
having time-varying characteristics, the calibration control bits
may also be updated by software periodically.
IV. SIMULATION RESULTS
A. Simulation Setup

Fig. 10. Flow diagram of the reference calibration method. The upper and
is searched by sweeping the control
lower boundaries of the proffered
is set as selected by
bits (SEL). At the end of the calibration process,
.

read and write of both data patterns are required to exercise all
the cells in an array. In the algorithm,
is first searched
with data pattern “0.” Since the first SEL value may lead to either a successful or a failed read, the searching of
may
follow different directions according to the first result. Differently, as long as
is determined, we only need to search for
in the other direction. Theoretically, all the
levels
within the bounds are error free. To maximize read margin, the
optimum
is selected by
.
A block diagram that illustrates the STT-RAM architecture
with the reference calibration technique applied is shown in
Fig. 11. The calibrated control bits can be stored in an off-chip

In order to analyze the effectiveness of the reference calibration technique and determine the number of configuration bits
for effective calibration, we simulate the reading of a 512-cell
memory array using HSPICE Monte Carlo (MC) simulations
with a 65-nm CMOS model. Both the across-chip variations
and the chip-to-chip variations are enabled in the simulations.
At each process corner,
MC runs are conducted for statistics parameter extraction purposes [22]. The MTJ model and its
variation parameters used in the simulations are summarized in
Table I. The MTJ variation is modeled by the standard deviation
of resistance-area (RA) ratio
and TMR
extracted
from measurements [20]. A total
of the MTJ variation is
considered.
B. Read Margin and Yield Improvements
Fig. 12 shows the read margin statistics of a 512-cell memory
array extracted from
MC runs in the nominal case. For
Fig. 12(b)–(d), the optimal
in the simulation is determined using the algorithm shown in Fig. 10. The simulation
results show that a simple 2-bit reference calibration can effectively improve the worst case read margin by 3 times.
With one extra calibration bit, another 30% improvement
can be achieved. The improvement margin nearly saturates
at 4 calibration bits. Clearly, the amount of improvement by
reference calibration becomes much less significant when

REN et al.: REFERENCE CALIBRATION OF BODY-VOLTAGE SENSING CIRCUIT FOR HIGH-SPEED STT-RAMs

2937

Fig. 12. The read margin statistics of a 512-cell memory array extracted from
MC runs in the nominal case with: (a) no calibration, (b) 2-bit calibration,
(c) 3-bit calibration, (d) 4-bit calibration.

the calibration resolution exceeds 2 bits. This is because the
reference calibration actually re-distributes the sensing margin
around
rather than enlarging it. Fig. 12 indicates that a
2-bit reference calibration is sufficient for the nominal case.
Fig. 13 presents the yield of a 512-cell memory array calculated from (6) using parameters extracted from the MC simulations in different process corners. One sigma of the
of the
sense amplifier is assumed to be 11 mV. Note that the
level that maximizes the read margin in the nominal case is
chosen as the nominal operating point. However, in the worstcase corner (FS corner) the array yield is around 70% without
any reference calibration. This indicates only tuning the operation point of
is not sufficient for compensating the variations in the worst-case corner. If a 1-Mb memory is built using
multiple such arrays, the overall yield would be lower than 1%
in the FS corner. However, with a 2-bit reference calibration, the
yield of a 1-Mb memory can be improved to over 99.7% across
all corners. This result indicates that reference calibration can
bevery effective in compensating within-die variations.
In addition, reference calibration can also be used to compensate for the device mismatch of sense amplifiers by shifting
against the
. Fig. 14 illustrates the yield improvement by reference calibration of a 512-cell memory array as a
function of the variability of the sense amplifier. In the nominal
case, the yield drops rapidly as the standard deviation of
increases beyond 15 mV without reference calibration. With reference calibration, the yield stays nearly constant. This trend becomes more prominent in the FS corner. These results show that
the reference calibration technique is able to relax the device
matching requirements of the sense amplifier design without
sacrificing the yield. Similar results have also been reported by
a study on a self-reference scheme [23]. However, the self-reference scheme improves the reading robustness at the cost of
lowering the sensing speed [23], while our technique does not
affect the sensing speed at all.

Fig. 13. Yield of a 512-cell memory array at different process corners with:
(a) no calibration, (b) 2-bit calibration.

Fig. 14. Yield of a 512-cell memory array as a function of the sense amplifier
variability at: (a) the typical and (b) the FS corners.

2938

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 60, NO. 11, NOVEMBER 2013

potential area saving from using smaller devices and may mitigate the area overhead. According to our estimation, the overall
area overhead of reference calibration circuitry is limited to 10%
of the peripheral circuitry. Such impact reduces as the column
mux ratio and memory area utilization rate increases.
V. CONCLUSION
This paper presents a technique of using reference calibration as an enhancement to BVSC to enable fast and reliable
reading of STT-RAM. The simulation results show that by applying a simple 2-bit reference calibration, the worst case read
margin due to process variations can be improved by over 3
times, leading to a significant yield increase (up to 30%) across
all corners. Moreover, the reference calibration technique improves the yield in the presence of device mismatch in the sense
amplifier design. In practical design, where the yield loss due to
the worst-case corner is critical to the designers, dedicated reference calibration circuitry should be employed to each memory
array.
ACKNOWLEDGMENT
The authors thank Prof. Jianping Wang and Dr. Hui Zhao
form the University of Minnesota for providing MTJ measurement data. This work was supported by the DARPA STT-RAM
program.
REFERENCES
Fig. 15. (a) Yield of an 1-Mb memory when multiple arrays are sharing the
at: (a) the typical and (b) the FS corners.
same calibrated

C. Reference Sharing
For all the above-mentioned results, we assume each array of
the memory has its own control bits for independent reference
calibration. In order to minimize the area overhead of the calibration circuitry, we also study the feasibility of sharing a single
calibrated
across multiple arrays. Fig. 15 illustrates how
the yield of a 1-Mb memory is affected by sharing the calibrated
. As one would expect, sharing across more than 32 arrays
results in a yield drop in the typical corner. In the FS corner, the
yield drops rapidly as more than 1 array are sharing the same
calibrated
. Although increasing the calibration resolution helps to alleviate the drop rate, the yield loss is still significant. As the array size covered by the same calibrated
increases, the chance of the tail of the sensing voltage distribution exceeding the calibrated
also increases (Fig. 6).
As a result, the effectiveness of the reference calibration technique diminishes. This indicates that dedicated calibration circuitry has to be deployed for each memory array if the yield loss
due to the worst-case corner is critical to the designers.
The area overhead of calibration circuitry may depend on the
column mux ratio and the area utilization rate of the memory.
In the case of a 2-bit calibration, the calibration circuitry introduced to each array only includes a short resistor ladder, a few
transmission gates, and a few control bit registers. In addition,
applying the reference calibration relaxes the device matching
requirements of the sense amplifier (Fig. 14), which allows for

[1] G. De Sandre et al., “A 4 Mb LV MOS-selected embedded phase
change memory in 90 nm standard CMOS technology,” IEEE J.
Solid-State Circuits, vol. 46, no. 1, pp. 52–63, Jan. 2011.
[2] W. Otsuka et al., “A 4Mb conductive-bridge resistive memory with 2.3
GB/s read-throughput and 216 MB/s program-throughput,” in Proc.
Int. Solid-State Circuits Conf., San Francisco, CA, USA, 2011, pp.
210–211.
[3] K. Tsuchida et al., “A 64 Mb MRAM with clamped-reference and adequate-reference schemes,” in Proc. Int. Solid-State Circuits Conf., San
Francisco, CA, USA, 2010, pp. 258–259.
[4] Y. Pan et al., “Quasi-nonvolatile SSD: Trading flash memory nonvolatility to improve storage system performance for enterprise applications,” in Proc. 18th Int. Symp. High Perform. Comput. Archit.
(HPCL’12), New Orleans, LA, USA, 2012, pp. 1–10.
[5] Y. Pan et al., “On the case of using quasi-EZ-NAND flash memory
to build future solid-state drives,” IEEE Trans. Comput., 2012, to be
published.
[6] G. Dong et al., “Estimating information-theoretical NAND flash
memory storage capacity and its implication to memory system design
space exploration,” IEEE Trans. Very Large Scale Integr. (VLSI) Syst.,
vol. 20, no. 9, pp. 1705–1714, 2012.
[7] D. Smith et al., “Latest advances and roadmap for in-plane and perpendicular STT-RAM,” in Proc. 3rd Int. Memory Workshop (IMW),
Dallas, TX, USA, 2011, pp. 1–3.
[8] Y. Pan, “Exploring the use of emerging nonvolatile memory technologies in future FPGAs,” IEEE Trans. Very Large Scale Integr. (VLSI)
Syst., 2012, to be published.
[9] F. Ren and D. Marković, “True energy-performance analysis of the
MTJ-based logic-in-memory architecture (1-Bit full adder),” IEEE
Trans. Electron Devices, vol. 57, no. 5, pp. 1023–1028, May 2010.
[10] C. Zhang et al., “Mapping channel estimation and MIMO detection
in LTE-advanced on a reconfigurable cell array,” in Proc. Int. Symp.
Circuits Syst. (ISCAS), Seoul, Korea, 2012, pp. 1799–1802.
coupler
[11] H. Wu et al., “A 60 GHz on-chip RF-interconnect with
for 5 Gbps Bi-directional communication and multi-drop arbitration,”
in Proc. IEEE Custom Integr. Circuits Conf. (CICC), San Jose, CA,
USA, 2012, pp. 9–12.
[12] W. Xu et al., “In-place FPGA retiming for mitigation of variational
single-event transient faults,” IEEE Trans. Circuits Syst. I, Reg. Papers,
vol. 58, no. 6, pp. 1372–1381, Jun. 2011.

REN et al.: REFERENCE CALIBRATION OF BODY-VOLTAGE SENSING CIRCUIT FOR HIGH-SPEED STT-RAMs

[13] C. Zhang et al., “Energy efficient MIMO channel pre-processor using
a low complexity on-line update scheme,” in Proc. IEEE NORCHIP,
Copenhagen, Denmark, 2012, pp. 1–4.
[14] N. Amini et al., “Experimental analysis of IEEE 802.15.4 for on/off
body communications,” in Proc. 22nd IEEE Symp. Personal Indoor
Mobile Radio Commun. (PIMRC), Toronto, ON, Canada, 2011, pp.
2138–2142.
[15] W. Xu et al., “eCushion: An eTextile device for sitting posture monitoring,” in IEEE Int. Conf. Body Sensor Networks (BSN), Dallas, TX,
USA, 2011, pp. 194–199.
[16] K. Ono et al., “A disturbance-free read scheme and a compact
stochastic-spin-dynamics-based MTJ circuit model for Gb-scale
SPRAM,” in Proc. IEEE Int. Electron Devices Meet., Baltimore, MD,
USA, 2009, pp. 1–4.
[17] F. Ren et al., “A body-voltage-sensing-based short pulse reading circuit for spin-torque transfer RAMs (STT-RAMs),” in Proc. 13th Int.
Symp. Quality Electron. Design (ISQED’12), Santa Clara, CA, USA,
pp. 275–282.
[18] M. F. Chang et al., “A 0.5 V 4 Mb logic-process compatible embedded
resistive RAM (ReRAM) in 65 nm CMOS using low-voltage currentmode sensing scheme with 45 ns random read time,” in Proc. Int. SolidState Circuits Conf., San Francisco, CA, USA, 2012, pp. 434–435.
[19] H. Zhao et al., “Spin-torque driven switching probability density function asymmetry,” IEEE Tran. Magn., vol. 48, no. 11, pp. 3818–3820,
Nov. 2012.
[20] R. Dorrance et al., “Scalability and design-space analysis of a 1T-1
MTJ memory cell for STT-RAMs,” IEEE Trans. Electron Devices, vol.
59, no. 4, pp. 878–887, Apr. 2012.
[21] R. Dorrance et al., “Scalability and design-space analysis of a
1T-1MTJ memory cell,” in Proc. ACM/IEEE Int. Symp. Nanoscale
Arch. (NANOARCH’ll), San Diego, CA, USA, Jun. 2011, pp. 32–36.
[22] R. Kanj et al., “Mixture importance sampling and its application to
the analysis of SRAM designs in the presence of rare failure events,”
in IEEE/ACM Proc. 43rd Design Autom. Conf., Anaheim, CA, USA,
2006, pp. 69–72.
[23] Z. Sun et al., “Variation tolerant sensing scheme of spin-transfer torque
memory for yield improvement,” in IEEE/ACM Int. Conf. Comput.Aid-Design, Brooklyn, OH, USA, 2010, pp. 432–437.
Fengbo Ren (S’10) was born in Shenyang, China. He
received the B.Eng. degree in electrical engineering
from Zhejiang University, Hangzhou, China, in 2008,
and the M.S. degree from University of California,
Los Angeles, CA, USA, in 2010, where he is currently a Ph.D. candidate, specializing in circuit and
embedded systems.
In 2006, he studied as exchange student with the
Department of Electronic & Computer Engineering,
Hong Kong University of Science and Technology,
Kowloon, Hong Kong. During the fall of 2009, he
worked as an Engineer Intern with the Digital ASIC Group, Qualcomm Inc.,
San Diego, CA, USA, where he contributed to the low power design flow of
a smart phone SOC. During the summer of 2012, he worked as a Ph.D. Intern
with the Data Center Group, Cisco Systems Inc., San Jose, CA, USA, where he
was involved in the FPGA emulation of a data center switch ASIC. His current
research interests include circuit design and design optimization for STT-RAM
and efficient DSP architectures for the sparse signal processing in compressive
sensing applications.

2939

Henry Park (S’07) was born in Los Angeles, CA,
USA. He received the B.S.E.E. degree (summa
cum laude) from Seoul National University, Seoul,
Korea, in 2003, and the M.S.E.E. degree from
UCLA, Los Angeles, CA, USA, in 2009, where
he is currently working toward the Ph.D. degree in
electrical engineering.
From 2003 to 2006, he was with Hunter
Technology, Seoul, Korea, where he developed
microprocessor embedded systems with digital and
analog interface. During the summer and fall of
2009, he was with Broadcom, Irvine, CA, USA, where he was involved in
high precision data converters and noise chopper design. His research interests
include data converters and statistical analysis of memory cell stability.

Chih-Kong Ken Yang (S’94–M’98–SM’07–F’10)
was born in Taipei, Taiwan. He received the B.S.
and M.S. degrees in 1992 and the Ph.D. degree in
1998 from Stanford University, Stanford, CA, USA,
in electrical engineering.
He joined University of California at Los Angeles,
CA, USA, as an Assistant Professor in 1999 and has
been a Professor since 2009. His current research
area is high-performance mixed-mode circuit design for VLSI systems such as clock generation,
high-performance signaling, low-power digital
functional blocks, and analog-to-digital conversion.

Dejan Marković (S’96–M’06) received the
Dipl.Ing. degree from the University of Belgrade,
Serbia, in 1998 and the M.S. and Ph.D. degrees
from the University of California, Berkeley, CA,
USA, in 2000 and 2006, respectively, all in electrical
engineering.
In 2006, he joined the faculty of the Electrical
Engineering Department at the University of California, Los Angeles, CA, USA, as an Assistant
Professor. Since 2009, he has been affiliated with
the Biomedical Engineering Interdepartmental
Program at UCLA as a co-chair of the Neuroengineering field. He is also a
director of the Integrated Circuits track within the UCLA Master of Science
in Engineering Online Program. His current research is focused on integrated
circuits for emerging radio and healthcare systems, programmable ICs, design
with post-CMOS devices, optimization methods and CAD flows.
Dr. Marković was awarded the CalVIEW Fellow Award in 2001 and 2002
for excellence in teaching and mentoring of industry engineers through the UC
Berkeley distance learning program. In 2004, he was a co-recipient of the Best
Paper Award at the IEEE International Symposium on Quality Electronic Design. In recognition of the impact of his Ph.D. work, he received the 2007 David
J. Sakrison Memorial Prize at UC Berkeley. He received an NSF CAREER
Award in 2009. In 2010, he was a co-recipient of ISSCC Jack Raper Award for
Outstanding Technology Directions and a winner of the DAC/ISSCC Student
Design Contest.

934

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 63, NO. 10, OCTOBER 2016

An 8-Bit Compressive Sensing ADC With 4-GS/s
Equivalent Speed Utilizing Self-Timed
Pipeline SAR-Binary-Search
Boyu Hu, Fengbo Ren, Zuow-Zun Chen, Xicheng Jiang, Fellow, IEEE, and Mau-Chung Frank Chang, Fellow, IEEE

Abstract—This brief presents a 65-nm CMOS single-channel
8-bit ADC compatible for energy-efficient high-speed compressive
sensing (CS) and Nyquist sampling (NS). A self-timed pipeline
two-stage SAR-binary-search architecture is proposed and integrated with a 4-GHz random-matrix clock generator, enabling
a physical sampling speed up to 500 MS/s with 40.2-dB SNDR
in NS-mode and an equivalent speed up to 4 GS/s with 36.2-dB
SNDR in CS-mode, leading to FOMs of 239 fJ/conversion-step and
71 fJ/conversion-step, respectively. A passive-charge-sharing with
open-loop residue-amplifier technique is proposed to boost the
maximum physical sampling speed and the equivalent CS acquisition bandwidth. A reference-voltage fitting calibration scheme is
applied to predistort interstage errors.
Index Terms—ADC, compressive sensing (CS), SAR-binarysearch (BS), self-timed pipeline.

I. I NTRODUCTION

C

ONVENTIONAL signal acquisition follows Nyquistsampling (NS) theorem: the sampling rate should be at
least twice the maximum frequency presented in a signal. Many
of the natural signals have more compact, or so-called “sparser,”
representations on a certain basis, which means that a small
portion of coefficients in the sparse domain is sufficient to carry
a significant portion of the signal energy. Compressive sensing
(CS) theory [1] takes advantage of such a fact and suggests an
alternative data acquisition framework that can indirectly access
the signal information in its sparse domain at sub-Nyquist rate.
Spectral sparse signals, which have a sparse coefficient representation on Fourier basis, are widely encountered in signal
processing. Concrete examples include sparse spectrum sensing
in cognitive radio [2], transmitter localization in intelligent
communication, narrow-band modulated signals with unknown
carrier frequency located in wide-band, slowly varying chirps,
smooth/piecewise smooth signals, and so on [3]. By incorporating randomness into the sampling process in CS framework,
spectral sparse signal information can be well encoded into
much fewer samplers than that of NS, and the original signal

Manuscript received September 19, 2015; revised December 11, 2015;
accepted February 29, 2016. Date of publication March 4, 2016; date of current
version September 22, 2016. This brief was recommended by Associate Editor
P. Rombouts.
B. Hu, Z.-Z. Chen, and M.-C. F. Chang are with the Department of Electrical
Engineering, University of California, Los Angeles, CA 90095 USA (e-mail:
boyuhu@ucla.edu).
F. Ren is with the School of Computing, Informatics and Decision Systems
Engineering, Arizona State University, Tempe, AZ 85281 USA.
X. Jiang is with Broadcom Corporation, Irvine, CA 92617 USA.
Color versions of one or more of the figures in this brief are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TCSII.2016.2538378

can be robustly recovered in digital domain by applying sparsity as prior knowledge given the random-matrix. In addition,
recent research [2], [4] shows promising and powerful signal
processing techniques to solve such problems as detection, classification, and filtering directly on the compressed data domain
with low-complexity computations instead of resorting to a
computation-consuming full-scale signal recovery. Such a trend
makes the CS framework a potentially power- and hardwareefficient alternative solution to its NS framework counterpart
for sparse signal processing.
Inspired by the encouraging progress from the signal processing community, hardware implementation of a CS-based
signal acquisition system has recently attracted attention from
the circuit design community [5]–[8]. However, there still lacks
combining the power of CS with high-speed CMOS ADC,
which is essential in the CS signal processing toolkit.
This brief presents a CMOS ADC silicon prototype for
energy-efficient multigigahertz range spectral sparse CS operation. A self-timed pipeline scheme is proposed to combine
the randomness embedded sampling and conversion with highspeed pipeline architecture. A passive-charge-sharing (PCS)
technique together with open-loop (OL) residue amplifier (RA)
is exploited for high-speed power-efficient interstage residue
transferring. A two-stage architecture hybriding a SAR-ADC
and binary-search (BS) ADC is proposed to both shorten the
pipeline cycle and absorb interstage gain error and nonlinearity
with corresponding calibration techniques. These, together with
the integration of an on-chip single pulse random-matrix clock
generator, lead to an 8-bit 500-MS/s ADC in Nyquist-mode and
equivalent 4-GS/s ADC in CS-mode.
II. ADC A RCHITECTURE AND I MPLEMENTATION
A. Hardware Mapping of CS-ADC System
A conceptual explanation and its math description for the
acquisition of spectral sparse signals with CS-ADC are shown
in Fig. 1(a) and (b). For a sparse spectrum on n-bin discrete
Fourier basis, represented by a vector fn×1 , only k bins (k 
n)f1 , . . . , fk contain significant coefficients. According to CS
theory, m random measurements from the signal, where m
satisfies [5]
m > k × log2 (n)

(1)

would be sufficient to recover the original k coefficients if the
sampling process adopts a known random-matrix Am×n . The
time-domain representation of fn×1 is
xn×1 = Ψn×n × fn×1

1549-7747 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

(2)

HU et al.: 8-BIT CS-ADC WITH 4-GS/s EQUIVALENT SPEED UTILIZING SELF-TIMED PIPELINE SAR-BS

935

Fig. 1. (a) Spectral sparse signal. (b) Math equation describing CS procedure.
(c) Random sampling in time-domain. (d) Mapping from math equation to
hardware.

where Ψn×n is an orthogonal basis projection from frequencydomain to time-domain. Combining Am×n and Ψn×n leads to
the sampling matrix in frequency-domain as
Φm×n = Am×n × Ψn×n .

(3)

Equation (3) encodes f1 , . . . , fk into the compressed measurement as ym×1 in CS operation instead of xn×1 in conventional
NS operation.
Fig. 1(c) and (d) shows the sampling procedure in timedomain and its hardware mapping. The procedure can be
considered as first sampling the input signal uniformly with
“virtual” samplers at time intervals of a minimum time grid
Δtmin and then picking up the physical samplers from them
following the random-matrix Am×n . Δtmin corresponds to the
predefined physical time intervals between any two neighboring
elements in a same row of Am×n . The equivalent CS acquisition
bandwidth is given by the inverse of Δtmin , while the compression rate, which relates to the tolerable input signal sparsity
levels, is defined by the average physical sampling period over
Δtmin . As shown in Fig. 2(a), each row of Am×n is designed to
contain only one sampling time-window. For row k, its timewindow is composed of a fixed part of 8Δtmin from pk to
pk + 7 and a subsequent variable part randomly being qΔtmin ,
where q is uniformly distributed among 0 to 7. “1” indicates
a physical sampling initiated by a single-pulse trigger, while
“0” indicates no physical action. The starting point pk+1 of
the next row k + 1 begins right after row k’s sampling point.
Since there is no overlapping between any of the sampling timewindows, the two-dimension Am×n can be flattened onto the
one-dimension time axis. A math expression for the consecutive
sampling time points is
tk+1 = tk + (8 + q) × Δtmin .

(4)

Fig. 2(b) shows the system diagram at chip level. The
ADC is integrated with a UART controller, a SRAM, and a
random-matrix clock generator. The circuit implementation of
the random-matrix clock generator providing the single-pulse
trigger includes a TSPC-logic-based length variable looped
shift register chain controlled by a digital synthesized 16-bit
Fibonacci linear-feedback-shifter-register [6]. Δtmin is set as
250 ps, and thus, the average physical sampling time period for
collecting one measurement is between 2 and 3.75 ns, leading
to a 4-GS/s equivalent CS-ADC with the maximum physical
sampling speed of 500 MS/s. The NS operation is enabled by
providing the ADC with a fixed-time-period trigger of 8Δtmin .

Fig. 2. (a) Random-matrix. (b) System diagram at chip level.

The SRAM is controlled by the UART controller and serves as
a high-speed buffer to cache both ADC’s output and random
sampling time intervals q. Am×n is reconstructed with the
recorded q values, and the spectral sparse signal is recovered
with ADC’s output, Am×n , and DFT basis Ψn×n in a blockby-block way. A block size of a maximum n of 2048 and a
corresponding measurement number m of 178 on average are
supported for one block recovery in the prototype. Signals that
have sparse coefficients on Fourier basis within the time interval
of such a block can be acquired and recovered. This claim
also applies to short-time spectral sparse signals with different
sparse coefficient locations and amplitudes within different
time intervals [3].
B. Self-Timed Pipeline SAR-Binary-Search Architecture
A higher maximum physical sampling speed enables a wider
equivalent CS acquisition bandwidth for a given compression
rate. An energy-efficient high-speed ADC architecture being
able to follow random-matrix sampling is essential for wideband CS acquisition. The proposed ADC evolves from twostage pipeline SAR-ADC. While inheriting its good balancing
between power and speed, several new techniques at both
architecture and circuit level are exploited to fulfill the request
of high-speed CS operation.
Fig. 3(a) shows the proposed ADC architecture, which
hybrids a 5-bit comparator-interleaved SAR-ADC [9] with a
4-bit BS-ADC with 1-bit overlap for over-range correction.
Instead of conventional closed-loop-based interstage residue
amplification, a PCS scheme together with OL amplifier is exploited here.
Fig. 3(b) shows the proposed self-timed pipeline scheme.
Unlike the conventional multiphase driven pipeline scheme, the
proposed pipeline operation is self-propagated. The initiation of
one operation is pulse-triggered by the completion of its previous event, which makes the proposed ADC fully compatible
with randomness embedded single-pulse sample and conversion. Fig. 3(c) explains the timing diagram of the proposed

936

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 63, NO. 10, OCTOBER 2016

Fig. 3. (a) Proposed self-timed pipeline SAR-BS ADC architecture. (b) Proposed self-timed pipeline scheme. (c) Timing diagram.

Fig. 5. (a) Comparator in SAR-ADC. (b) PCS with OL amplifier. (c) Comparator in BS-ADC. (d) Reference-voltage fitting calibration.

Fig. 4. Waveform of the first-stage ADC Cap-DAC and second-stage ADC
input in the conventional and proposed architectures.

ADC. A single pulse from the random-matrix clock generator
initiates the pipeline cycle. For the SAR-ADC, the two comparators alternatively trigger each other with their ready signals,
while their compare decisions are latched to flip the split
Cap-DAC, which has a unit capacitor size of 2 fF. After
five cycles, the control logic holds the operation of the SARADC. For the BS-ADC, a delayed version of the random-matrix
clock generator’s pulse first resets and then sets the conversion
clock of the first layer of the BS tree. The clock propagates
through the four-layer BS tree, and its ready signal resets the
PCS capacitor in front of the OL amplifier and also sends it into
the control logic. After receiving a ready signal from both the
SAR- and BS-ADC, the control logic sets up the PCS capacitor
sample signal and then resets the split Cap-DAC to set up for
the next pipeline cycle.
The benefits on both speed enhancement and calibration
feasibility brought by the proposed architecture and techniques
over the conventional two-stage SAR-ADC are demonstrated in
Fig. 4. In the conventional approach, the Cap-DAC of the firststage ADC is unavailable for the next pipeline cycle until the
RA is fully settled. The settling time occupies the conversion
period of both two stages and severely delays the total pipeline

cycle. In the proposed approach, the Cap-DAC shares its
residue passively with the PCS capacitor and then is free for the
next pipeline cycle. Since the charge-sharing time is decided by
the R − C time constant of the switch and the capacitor instead
of the gain-bandwidth-product of the RA in close-loop configuration, the time needed for interstage residue transferring is
significantly shortened for the first stage. Notice that the second
stage of the proposed architecture is a BS-ADC which directly
compares the output of the RA with its preset voltage levels on
a resistor ladder. Its conversion speed is considerably faster than
that of a conventional SAR-ADC due to the elimination of sampling and adding/subtracting voltage residue on a Cap-DAC.
By proper design, the total time of OL amplifier settling plus
4-bit BS conversion can be comparable to 5-bit SAR conversion
plus PCS, which makes the pipeline cycle of the proposed
two-stage architecture well-balanced and shortened compared
to the conventional approach. The interstage gain error and
nonlinearity of the RA can be absorbed into this architecture
due to the feasibility of BS-ADC’s inherent analog predistortion
with its reference resistor ladder. Expensive high-order polynomial fitting in digital domain would be required for the similar
predistortion in SAR-ADC.
C. Circuit Implementation and Calibration Techniques
Fig. 5(a) shows the comparator in the SAR-ADC. Each comparator is composed of an offset-calibrated preamplifier, a selfreset strong-arm latch, and a ready detector. Fig. 5(b) shows

HU et al.: 8-BIT CS-ADC WITH 4-GS/s EQUIVALENT SPEED UTILIZING SELF-TIMED PIPELINE SAR-BS

Fig. 6. Die micrograph of the ADC.

937

Fig. 9. Measured output spectrum for an input signal of 0.73 MHz in NS-mode
with a decimation of 64.

Fig. 7. Measured DNL and INL performance before and after calibration.

Fig. 10. Measured output spectrum of a 3% spectral sparse signal.

Fig. 8. Measured SNDR and SFDR performance as the input frequency
changes in NS-mode and CS-mode (with postprocessing and recovery).

Fig. 11. Measured output spectrum of a spectral sparse two-band AM signal.

the PCS and OL amplifier block. It is composed of a pair of
top-plate-sample capacitors and a two-stage resistor-load-based
amplifier, of which the input offset and output common mode
are calibrated with the help of a shared auxiliary comparator.
The output of the amplifier is directly connected to the 15 comparators of the 4-bit BS-ADC. It serves the role of both amplifying the residue and protecting the residue from the kick-back
noise of the BS-ADC. As shown in Fig. 5(c), each of the comparators has a dual differential input, one pair of which connects
to the input and the other connects to the adjustable reference
voltages from the reference resistor ladder. For a four-layer BS
tree of 1–2– 4–8 comparators, respectively, the decision result
from one layer decides the propagation direction of the clock
signal to its next layer.
The foreground calibrations are carried out by the UART
controller. For offset calibration of both SAR comparators and
OL amplifier, the output of SAR’s Cap-DAC is connected
to the common-mode voltage, and the reference voltage to
the calibration differential pair of the comparator or amplifier
is swept step by step and frozen when the polarity of the
comparator/auxiliary comparator’s output flips. For commonmode calibration of the OL amplifier, the analog mux redirects
the input of the auxiliary comparator to the amplifier’s common
mode and the common mode of the BS-ADC’s resistor ladder.

Fig. 12. Ideal and demodulated two baseband signals.

The calibration tail current of its second stage is adjusted until
these two voltages match. Fig. 5(d) shows the applying of the
interstage reference-voltage fitting calibration scheme. For each
of the 15 comparators in the BS-ADC, its reference voltage
can be adjusted +2/−2 LSB around the original voltage with
a step of 1/4 LSB. During calibration, a combined usage of
the calibration capacitor in the Cap-DAC and a 4-bit resistorDAC generates 15 calibration voltages from −7 LSB to 7 LSB,
one corresponding to calibration of BS-ADC’s comparator. The
reference voltage of each comparator is swept from low to high
until its output polarity flips. Since the calibration voltages are
generated on the signal path and experience the same PCS and

938

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—II: EXPRESS BRIEFS, VOL. 63, NO. 10, OCTOBER 2016

TABLE I
S UMMARY OF P ERFORMANCE C OMPARISON

OL amplifying process as a normal input signal, the signal path
imperfectness, including the interstage gain error, offset, and
nonlinearity, and the comparator’s offset of the BS-ADC, is
analog predistorted through this calibration procedure [15].
III. E XPERIMENTAL R ESULTS
The proposed ADC has been designed and fabricated in
standard 65-nm CMOS technology. The output signals are postprocessed and recovered based on orthogonal matching pursuit
[10], [11].
Fig. 6 presents the die micrograph. Fig. 7 shows the DNL and
INL performance. The DNL is −0.6/+1.5 LBS, and the INL
is −0.8/+1.4 LBS after calibration. Fig. 8 presents the SFDR
and SNDR performance in NS-mode and in CS-mode with
postprocessing and recovery, as the input frequency changes.
Fig. 9 plots the decimated-by-64 output spectrum for an input
signal of 0.73 MHz in NS-mode with an SNDR of 40.2 dB and
an SFDR of 47.7 dB.
Fig. 10 shows the detection of a 3% sparsity input signal
composed of 30 active frequency bins randomly located across
0–2-GHz frequency band. The detection accuracy is 100%.
Fig. 11 shows the acquired spectral sparse two-band
amplitude-modulated (AM) signal with carriers located at 404
and 1186 MHz, respectively. Fig. 12 shows the demodulated
two baseband signals from acquisition and the ideal ones
generated in MATLAB. Each baseband signal is represented
by six coefficients randomly located on discrete Fourier basis
within 30 MHz. The time-domain recovery SNDRs, which are
defined by the ratio of the normalized ideal signal amplitude
over the normalized maximum deviation of recovered signal
from the ideal signal, are 37 and 36 dB, respectively, including
all of the errors from the arbitrary waveform generator, cables,
connectors, and PCB.
Table I summarizes the performance of the prototype and
its comparison with state-of-the-art NS and CS-ADCs. A 40×
wider acquisition bandwidth is achieved compared with its
prior art CS counterpart. It also shows significant power and
area reduction compared with its NS-ADC counterparts by
exploring CS for wideband located spectral sparse signals.
IV. C ONCLUSION
A single-channel 8-bit CS/NS ADC silicon prototype integrated with a high-speed random-matrix clock generator has
been presented. With the proposed self-timed pipeline SAR-BS

architecture, the ADC achieves a physical sampling speed up
to 500 MS/s and an equivalent speed up to 4 GS/s for spectral
sparse signal processing applications. A PCS with OL amplifier
technique is proposed to effectively shorten the pipeline cycle
and improves the conversion energy efficiency. The application
of the interstage reference-voltage fitting calibration scheme
into the proposed architecture is also investigated.
R EFERENCES
[1] E. J. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,”
IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489–509, Feb. 2006.
[2] Z. Tian and G. B. Giannakis, “Compressed sensing for wideband cognitive
radios,” in Proc. IEEE ICASSP, Apr. 2007, vol. 4, pp. 1357–1360.
[3] J. Tropp et al., “Beyond Nyquist: Efficient sampling of sparse bandlimited signals,” IEEE Trans. Inf. Theory, vol. 56, no. 1, pp. 520–544,
Jan. 2010.
[4] M. Davenport, P. T. Boufounos, M. B. Wakin, and R. G. Baraniuk, “Signal processing with compressive measurements,” IEEE J. Emerging Sel.
Topics Signal Process., vol. 4, no. 2, pp. 445–460, Apr. 2010.
[5] M. Trakimas, R. D. Angelo, S. Aeron, T. Hancock, and S. Sonkusale, “A
compressed sensing analog-to-information converter with edge-triggered
SAR ADC core,” IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 60, no. 5,
pp. 1135–1148, May 2013.
[6] D. E. Bellasi et al., “VLSI design of a monolithic compressive-sensing
wideband analog-to-information converter,” IEEE J. Emerging Sel. Topics
Circuits Syst., vol. 3, no. 4, pp. 552–565, Apr. 2013.
[7] X. Chen, Z. Yu, S. Hoyos, B. M. Sadler, and J. Silva-Martinez, “A
sub-Nyquist rate sampling receiver exploiting compressive sensing,”
IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 58, no. 3, pp. 507–520,
Mar. 2011.
[8] F. Chen, A. P. Chandrakasan, and V. M. Stojanovic, “Design and analysis
of a hardware-efficient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3,
pp. 744–756, Mar. 2012.
[9] L. Kull et al., “A 3.1 mW 8 b 1.2 GS/s single-channel asynchronous SAR
ADC with alternate comparators for enhanced speed in 32 nm digital SOI
CMOS,” IEEE J. Solid-State Circuits, vol. 48, no. 12, pp. 3049–3058,
Dec. 2013.
[10] J. A. Tropp and A. C. Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53,
no. 12, pp. 4655–4666, Dec. 2007.
[11] F. Ren and D. Markovic, “A configurable 12-to-237 KS/s 12.8 mW sparseapproximation engine for mobile ExG data aggregation,” in Proc. IEEE
ISSCC, 2016, pp. 68–78.
[12] H. Wei, P. Zhang, B. D. Sahoo, and B. Razavi, “An 8-bit 4-GS/s 120-mW
CMOS ADC,” in Proc. IEEE Custom Integr. Circuits Conf., Sep. 2013,
pp. 1–4.
[13] J.-I. Kim, B.-R.-S. Sung, W. Kim, and S.-T. Ryu, “A 6-b 4.1-GS/s flash
ADC with time-domain latch interpolation in 90 nm CMOS,” IEEE J.
Solid-State Circuits, vol. 48, no. 6, pp. 1429–1441, Jun. 2013.
[14] Y. Ramy, M.-S. Chen, M.-C. F. Chang, and C.-K. K. Yang, “An
architecture-reconfigurable 3 b-to-7 b 4 GS/s-to-1.5 GS/s ADC using
subtractor interleaving,” in Proc. IEEE Asian Solid-State Circuits Conf.,
Nov. 2013, pp. 285–288.
[15] S. Hashemi and B. Razavi, “A 7.1-mW 1-GS/s ADC with 48-dB SNDR
at Nyquist rate,” in Proc. IEEE Custom Integr. Circuits Conf., Sep. 2013,
pp. 1–4.

	
  

	
  
	
  

mCOPD: Mobile Phone Based Lung Function Diagnosis
and Exercise System for COPD
Wenyao Xu, † Ming-Chun Huang, † Jason J. Liu, ‡ Fengbo Ren, ‡ Xinchen Shen,
‡
Xiao Liu, † Majid Sarrafzadeh
†
Wireless Health Institute, University of California, Los Angeles, California, 90095, USA
‡
Electrical Engineering, University of California, Los Angeles, California, 90095, USA
†

	
  

	
  

ABSTRACT

	
  

	
  

	
  

	
  

	
  

COPD (Chronic Obstructive Pulmonary Disease) is a serious lung disease that causes difficulty in breathing. COPD
patients require lung function examinations and perform
breathing exercises on a regular basis in order to manage
and be more aware of their health status. In this paper,
we designed and developed a mobile-phone based system for
lung function diagnosis, called mCOPD. Besides enabling
accurate COPD examinations at home, the mCOPD system
also offers a video-game based guidance system for breathing
exercises. We evaluated mCOPD in controlled and uncontrolled environments with 40 subjects. The experimental
results show that our system is a promising tool for remote
medical treatment of COPD.

	
  
Figure 1: Spirometric classification of chronic obstructive
pulmonary disease (COPD) [5]

Keywords

	
  

COPD detection, Android, Spirometer, Wireless health

Categories and Subject Descriptors
J.3 [Health and Medical information systems]: LIFE
AND MEDICAL SCIENCE

General Terms
Design, Economics, Reliability, Experimentation

1. INTRODUCTION
1.1 COPD Introduction
Chronic obstructive pulmonary disease, although incurable,
is a treatable illness that affects both lungs and other important body systems. The leading cause of COPD is smoking,
and COPD has become more prevalent during the past twenty years [1]. It is reported that COPD and related conditions
are the fourth leading cause of death in United States, after heart disease, cancer and stroke [2]. More specifically,
COPD kills more than 126,000 Americans annually [3]. The
mortality rate of COPD and related conditions rose nearly
40% from 1979 to 2009, and may surpass stroke to become
the third leading disease worldwide by 2020 [4].
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers
or to redistribute to lists, requires prior specific permission and/or a fee.
Request permissions from Permissions@acm.org.
PETRA ’13, May 29 - 31 2013, Island of Rhodes, Greece
Copyright 2013 ACM 978-1-4503-1973-7/13/05 $15.00.
http://dx.doi.org/10.1145/2504335.2504383

	
  

Spirometry is the most common and effective instrument for
diagnosis of COPD. In fact, spirometry is as important in diagnosing COPD as blood pressure measurements in the diagnosis of hypertension. Spirometry is a simple test to measure
the amount of air a person can breathe out, as well as the
amount of time taken to do so. There are three important
parameters in spirometry testing: forced expiratory volume
in one second (FEV1), forced vital capacity (FVC), and the
ratio of the previous two features (FEV1/FVC). FVC indicates the maximum volume of air that can be exhaled during
a forced maneuver. FEV1 is the volume exhaled in the first
second of maximal expiration after a maximal inspiration.
This parameter is an indicator of how quickly the lungs can
be emptied. FEV1/FVC, the ratio of the above two features,
gives a clinically useful index of airflow limitation. By analyzing these three features, physicians can track changes in
patients’ lung functions. As shown in Figure 1, FEV1/FVC
≤ 0.7 can confirm non-reversible airflow limitation.

	
  

Spirometric classification has proven to be useful in various
areas of COPD [5]. The major areas in which we are interested are predicting health status [6, 7] and mortality [8]. It
has been proven that spirometry is not only useful for initial
diagnosis but also for long term monitoring. It is also one of
the motivations for a long term COPD monitoring system.

	
  

The Global initiative for Obstructive Lung Disease (GOLD)
classifies COPD into four stages: mild, moderate, severe,
very severe. These stages are classified based on the value of FEV1/FVC. There are some common ways that can
alleviate the symptoms such as smoking cessation, healthy
nutrition and exercise. Another way to greatly improve the
condition is to change the way we breathe. There are several

	
  

	
  

	
  

	
  

	
  

	
  

useful breathing exercises: pursed-lip breathing, diaphragmatic breathing, comfortable breathing, and forceful coughing [9]. For most of the exercises, patients need to breathe
as smoothly and slowly as possible.
This requirement illustrates the need for an assisted method [10, 11] to guide
people in performing the correct breathing exercise.

1.2 Our Contribution
In this paper, a new mobile phone based system that is
both economic and accurate is proposed and designed for
daily COPD evaluation and treatment guidance. In contrast to traditional COPD diagnostic devices, our mCOPD
system can be entirely integrated in the mobile phone. By
leveraging the built-in microphone, we developed efficient
computational modeling to estimate lung functions through
sound signals.
There are four major advantages of mCOPD compared to
the traditional COPD diagnostic instruments. Firstly, the
mCOPD system will dramatically reduce the cost of spirometry systems since it is only based on the existing smart
phone platform without any extra sensors. Secondly, the
portability of smart phone makes it ideal for daily COPD
monitoring and long term reporting for lung function. With
the mobile phone network, patients can upload their COPD
related data anywhere and get evaluation results and feedback effectively. It will increase their interest in using the
system in long term due to the convenience. Thirdly, the
extensible features of smart phone [12] technology make it
easier to update the functions in the future. According to
the demands of physicians and patients, the system can be
easily personalized for better status monitoring of COPD.
Finally, novel guidance can be implemented on the mobile
phone. More specifically, the mCOPD system has been integrated with one or more games [13, 14] to help patients with
breathing exercises during their treatment stage. It will be
a unique feature different from the traditional COPD treatment.
The remainder of this paper is organized as follows. Section
2 discusses existing solutions and related research work on
the instruments for COPD diagnosis. Section 3 introduces
the system architecture in our mCOPD system. Section
4 presents, in detail, the design of the mCOPD diagnostic
subsystem structure. Section 5 elaborates the phone-based
gaming for COPD treatment. Section 6 discusses the experimental evaluation of the mCOPD system. The future work
and conclusion are concluded in Section 7.

2. BACKGROUND AND RELATED WORK
2.1 Existing Products and Solutions
In the current market, there are a variety of different spirometers. Most of them are not designed exclusively for COPD
detection but they can all be used to detect FEV1 and
FVC. Here we classify the current products into three categories: traditional spirometers, portable spirometers, and
computer-aided spirometers.

2.1.1

Traditional Spirometers

Traditional spirometers are capable of accurately measuring
more than 50 different spirometric features. They are often
purchased by hospitals for professional diagnosis of different

	
  

Figure 2: Current examples of existing spirometers. From
left to right: Traditional, Computer-aided, and Portable
Spirometers

	
  

	
  
kinds of lung disease. The advantage of this kind of traditional spirometer is the accuracy and range of functionality.
However, the trade off is size and price. This kind of system
is far from affordable to the average person. Moreover, it is
apparently not suitable for regular monitoring by patients
and are usually stand alone devices.

	
  

2.1.2

Portable Spirometers

This kind of spirometer is in a dominant position in today’s
home-based COPD lung function detection and monitoring
market. Portable spirometers are handled easily. Compared
with the traditional spirometer, many unused functions are
not implemented, leaving just the core features like FEV1
and FVC. This greatly simplifies the design and cost of the
system. With a simple processor and circuit, typically it can
not process and record data after measurement. Most of
the portable spirometers are also stand alone systems. This
restricts the applications in the future, especially in long
term monitoring. Moreover, it has some guidance features
to help patients in treatment stage.

	
  

2.1.3

Computer-aided Spirometers

Computer-aided spirometers are the newer models on the
market. This kind of spirometer commonly utilizes the USB
interface of a computer to transmit the sensor data onto the
computer. Unlike the portable spirometer, USB spirometer can use high-performance computers to do complex data
analysis, data collection, date management, and data sharing. The main problem of this kind of device is that it
is still not convenient to move from one place to another.
Even though the cost has been greatly reduced, the spirometer and software is still quite expensive. We need a more
economical and general solution.

	
  

A comparison between different off-the-shelf spirometers is
summarized in Figure 3.

	
  

2.2 Related Work
Mobile phone based COPD detection is a relatively new application. However, our work gains motivation from some
previous work. SpiroSmart [15] provides a good approach
for mobile phone based lung function detection. It also tries
to utilize the internal microphone for lung capacity detection. Moreover, the sound based medical sensing is developed in several other related areas. [16, 17] did wide research

	
  

	
  

	
  

	
  
	
  

	
  
	
  
	
  
	
  
	
  
	
  
	
  

	
  

	
  

	
  

	
  

Figure 3: The comparison among different kinds of spirometers

	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  

on coughing sound and breath detection. Wheeze detection
for asthma diagnosis has also lead to positive results [18, 19].
Compared to the above work, the mCOPD system provides
a different way in audio based lung function sensing. Moreover, the game based rehabilitation system is novel for this
application.

3. SYSTEM ARCHITECTURE

Figure 5: COPD Diagnostic System User Interface

detects the airflow based on air noise sound. Figure 5 shows
the COPD diagnostic system user interface. It contains two
major parts: the signal chart and the data presentation. For
the data presentation part, three major features FVC, Lung
FEV1 and FEV1/FVC are shown. Time field indicates the
total time of a patient’s exhalation at the current detection.
The strength value shows the direct sampled microphone
value. This is an important index for determining the airflow
value.

	
  

Figure 4: System Structure and the User Interface
There are two functions in the mCOPD system: COPD diagnostic system and COPD gaming treatment system, as
shown in Figure 4. The system can support two major operations: lung function measurement and exercise game guidance. Patients are able to create their own records based on
their name, height, age and sex. The record of lung function
information can be transferred to doctors for diagnosis.
From a user’s point of view, there are three parts in mCOPD. 1) The COPD diagnostic system mainly focuses on
calibration of FEV1, FVC and other related medical parameters. These parameters can by calculated by capturing the
input airflow signal from internal mobile device microphone.
2) COPD gaming treatment system provides a novel way for
patients with COPD to finish their daily treatment exercise.
A video game is developed and implemented in the system in
order to increase the patients’ interest in rehabilitation exercise. In this way, the patient compliance can be improved.
3) A stand-alone statistics module is also implemented in
the phone. This part can visualize the history data, which
benefits both patients and physicians to be easier in tracking
the diagnostic and exercise history.

4. DIAGNOSTIC SYSTEM DESIGN AND IMPLEMENTATION
4.1 System Overview
The COPD diagnostic system is the most important part
in the mCOPD system. As mentioned above, our system

The COPD diagnostic system consists of four procedures:
sensing, signal pre-processing, modeling and calculation, and
signal post-processing. The sensing part serves as the input
collection of the system. As shown in Figure ???, the airflow
signal is captured by microphone membrane and then transformed into an analog signal. In the signal pre-processing
procedure, the analog signal is sampled at 2.4kHz. The sampled signal will then be converted into 8-bit digital signal via
internal ADC. In order to analyze the frequency domain features, 64-point FFT is performed tho the signal. This choice
balances the resolution requirements of the analysis and the
performance of the current mobile device. The transformed
frequency signal will also need to go through low-pass filters
to eliminate noise. In the modeling and calculation procedure, two different machine learning regressions are used on
the low-frequency signal. As a result, the low-frequency signal will be matched into specific airflow speed. This instant
airflow speed will then be transmitted into the final stage.
In the signal post-processing procedure, the airflow speed is
integrated via certain cross section area. The medical parameters will then be obtained and output for future use.

	
  

We will discuss the details of each part in the remaining part
in this section.

	
  

4.2 Sensing Procedure
There are three kinds of off-the-shelf sensors which are widely used for lung function instruments: pressure sensor, mass
air flow sensor and air volume sensor.

	
  

There are special requirements on the mCOPD system. In
order to achieve maximum compatibility, we tend to choose
the internal sensors based on performance, price and availability. The mobile device MEMS microphone would be a

	
  

	
  

	
  

	
  

	
  
	
  
	
  

good choice. The MEMS microphone has a similar structure as the electret microphone, as shown in Figure 6, the
membrane on the microphone will vibrate with the coming
air pressure. The vibration will then be transformed into a
voltage signal.
Several related work have shown that there are two major
components in the wind noise: Natural flow turbulence in
the wind and turbulence generated during the interaction
between the microphone membrane and wind. [20] The natural flow turbulence can be well eliminated via specially selected wind generation. The interaction turbulence will then
serve as the main source of voltage output from microphone.
This makes the direct airflow sensing possible in theory.
Research has shown that a decreasing function of frequency
can be obtained in the recorded wind noise. [21] Most of the
energy will be conserved in the low frequency area. Thus,
the airflow coming from the mouth will perform as a low frequency sound pressure. This sound pressure measurement
will present as noise in the output sound signal. Richard et
al. [22] have shown that this wind noise on the bare microphone could be approximated well by fluctuating stagnation
pressure. The stagnation pressure is the pressure measured
at the sphere’s zero velocity position. The one-dimensional
model has proved to be effective:

p(t) = (1/2) ∗ ρ ∗ v + ρ ∗ V ∗ v(t) + (1/2) ∗ ρ ∗ v(t)2

(1)

	
  

	
  

	
  

	
  

where ρ is the ambient density, V is the average flow velocity, and v(t) is the fluctuating velocity magnitude. The
MEMS microphone is suitable also because of its low price
and widely availability. No more extra accessories need to
be bought for the basic mCOPD system. This will greatly
extend the system accessibility.

4.3 Signal Pre-Processing Procedure
Frequency domain computational model is established based
on the MEMS microphone. Frequency domain analysis has
certain advantages compared with time domain analysis in
airflow calibration. First, the time domain analysis leads to
saturation problems. Unlike the sound pressure, the airflow
will easily saturate the membrane vibration. This leads to
maximum output from microphone in the time domain. In
practical, most of the variational characteristics will be lost
in the time domain. The frequency domain analysis overcomes this limitation. Due to the feature of airflow, most of
the frequency components lie in the relatively low frequency
area, specifically below 1.2kHz. A low-pass filter could be
applied to eliminate the background noise. Moreover, the
resolution requirement of the system is also relatively low.
A 64-point FFT is sufficient in practical application. This
requirement could be satisfied in most of today’s mobile devices.
The output of the computational model will first be the
airflow speed. The output serves as the basic component
for computing expected medical requirements like FEV1 and
FVC. Post processing will be applied in the later stage.

4.4 Modeling and Calibration Procedure

	
  

	
  

Figure 6: MEMS Microphone Structure[17]

	
  

Two machine learning regressions are applied for solving the
computational model. Frequency response will serve as the
feature selection component.

	
  

Nearest neighbour matching is first used to match the
input frequency response to wind speed. The training samples are first collected via our test platform. These training
samples will be stored in the system as a reference table.
Both the geometric and average distance between the input
frequency response and different sets of reference points will
be calculated. The reference set with the nearest distance
will be chosen as the output wind speed. Nearest neighbour matching will be a good alternate when mathematical
analysis is not available. The resolution of the result depends on the quality of the training samples. In practice,
the resolution can be reduced to 0.2m/s in certain controlled
environments. However, the input frequency response can
not exceed the maximum and minimum value of the training
samples. This leads to high quality requirements of collecting procedure.

	
  

Mathematical Analysis is the second and better method
for the frequency response wind speed model. A secondorder mathematical regression model is established based
on the training samples and historical research. The input
frequency response will then serve as the input of the mathematical model. The mathematical regression overcomes the
limitation of nearest neighbour matching. After the mathematical relation is established, the output wind speed can
be well predicted.

	
  

4.5 Signal Post-Processing Procedure
After wind speed is obtained through computational model
regressions, post processing is needed to obtain the actual
required medical features. In mCOPD, we assume that the
interface area of the mouth when doing the lung capacity
test is the same. Integration of wind speed via certain fixed
area will be done in the final stage. FEV1 will first be obtained in the first second. After that, a threshold will be set
to calculate the FVC.

	
  

5. COPD BREATHING EXERCISE GAME
5.1 Exergaming and Design Criteria

	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  
	
  

	
  
Figure 8: Dyson Air Multiplier used in our experiments
	
  

	
  

	
  

	
  

	
  

	
  

	
  

	
  

Figure 7: Game Exercises for COPD Patients
Exergaming, short for exercise games, is a recent development in entertainment gaming. The goal of today’s video
games is not only to have fun, but also to provide benefits
to your health. Based on this concept, we designed an interesting game which not only provides fun, but also includes
rehabilitation exercises while playing it.
COPD patients are required to do certain kinds of rehabilitation exercises in order to learn how to get the most air out
of every breath. Dyspnea, or shortness of breath, is a result
of air hunger that causes people to feel like they cannot catch
enough breath. It is primarily due to the lack of oxygen in
the bloodstream and directly related to disturbances in the
lungs caused by COPD. Using pursed-lip breathing technique could slow their exhalations and make their breaths
longer and using diaphragmatic breathing technique could
increase the amount of inhaling and exhaling air.
Pursed-Lip Breathing is a breathing technique designed
to help control dyspnea, the hallmark symptom of COPD.
The following demonstrates the simple technique of pursedlip breathing: patients need to draw the lips together as
if they were going to whistle and blow out through pursed
lips slowly and evenly, and try to make the exhalation time
longer than the inhalation time. Practicing pursed-lip breathing on a daily basis 3 to 4 times a day will allow patients to
use it effectively during times of need, especially when their
shortness of breath worsens [23].
Diaphragmatic Breathing is a breathing technique slightly more complicated than pursed-lip breathing. Diaphragmatic breathing helps strengthen the diaphragm and
the abdominal muscles allowing more air to move in and out
of lungs without tiring the chest muscles [24].
Our gaming challenges are designed based on these breathing criteria. Users need to follow one of these two breathing
techniques to pass some of a day’s challenges and follow the
other to pass the rest.

5.2 Key components in the game
In this designed exergaming, user controls the movement of
a colored ball through blowing to the microphone. The ball

moves vertically high or low proportionally to the strength of
exhalation. COPD patients are required to using Pulsed-Lip
Breathing to exhale into the microphone slowly and evenly
in order to generate stable sound signal. We require players
to try their best to control their breath and make the colored ball move in the range of the background road. It is
especially hard for people with chronic lung disease to continuously provide a strong enough airflow at the latter stage
of one exhalation. If patients can’t control their breathing
during this set of period, which means they probably will
either make a very strong exhalation at the first half stage
or only can provide a very weak airflow coming to an end,
the colored ball will either hit the upper bound of the road
or falls below the lower bound. Both situations will lead to
the failure of the game. The speed of the ball moving from
screen left to the right is configurable. Since the speed is
a reflection of the length of expiration time, players can set
the speed at the beginning of the game according to their
current ability.

	
  

Game difficulties can be increased through lowering the speed
of the colored ball which requires longer stable exhaling
time in succeeding the game. Game difficulties can also
be increased through requiring patients to use Pursed-Lip
Breathing when inhaling and use Diaphragmatic Breathing
when exhaling which in another way increases their exercise
tolerance.

	
  

Patients can finish daily exercise through completing certain
amount of tasks in the game. mCOPD will record exercise
data which includes number and type of exergames have
been played and success rate and send it to doctors via internet. In this way, doctors can monitor patients’ in-home
rehabilitation exercise through real data.

	
  

6. EXPERIMENTS AND RESULTS
We evaluate the effectiveness of the mCOPD system for two different aspects. First, we validate the accuracy of the
microphone model. Second, we test the overall performance
of mCOPD while measuring lung function parameters.

	
  

6.1 Model Verification
In this part, we try to validate the accuracy of the microphone model. In order to find corresponding relations between the frequency response of microphone and different

	
  

air volumes together then we get the total lung capacity of
the user.

	
  

	
  
	
  

Figure 9: Experimental Setup

	
  
	
  

	
  

	
  

	
  

Figure 10: Frequency Response of Microphone to Airflow
with Speed 2.1m/s
airflow rate, we attached our microphone to the place near
the air intake of a digital portable anemometer. Thus, the
speed of air which causes sound signals on microphone is the
same of that goes through the anemometer. We put the microphone together with the anemometer 10-15cm away from
the air multiplier in order to catch the strongest airflow generating from it. The air multiplier as shown in Figure 8 is
a blade-less fan producing speed-stable airflow with continuous airflow speed adjustment.
We did 12 training tests at the first place. Experimental
setup is shown in Figure 9. The speed of wind in our training ranges from 2.1m/s to 4.4m/s with 0.2m/s increments.
Experimental result shows that there exists a clear relationship between the airflow rate and the corresponding frequency responses of microphone. The frequency response of our
testing microphone to airflow with speed 2.1m/s looks like
this (Figure 10). Figure 11 shows all the frequency response
curves of these 12 tests. We can see from Figure 11 that as
the airflow rate increases, the frequency response strength
increases as well. We can see the curve moves upward along
with the Y axis in Figure 11. Figure 12 shows the change
in response intensity at a specific frequency point with the
increase of airflow rate. From Figure 12 we find that almost
every frequency point within the response region increases along with the increase of airflow speed and they are of
certain mathematical quadratic relation. The red curve in
Figure 12 is a quadratic mathematical fitting curve we use
to approach the increasing trend we get from our tests.
Frequency responses of mobile phone based microphone to
various airflow rates are mainly concentrating on the lower
frequency part. We pick the first 16 frequency points and got
their fitting quadratic mathematical equations based on our
12 training tests. When patients use our mCOPD spirometry, we assume in a very small time interval the airflow rate
is stable, so the frequency response of microphone within
that time interval is also stable. We choose the responses intensity of the first 16 frequency points. Put them into
their quadratic equations respectively and we get 16 predicted airflow rates in total. The mean of the 16 numbers is our
predicted speed for that small time interval. The instantaneous predicted speed multiplies the cross-section area of a
mouthpiece that we provide in our tests to control airflow
multiplies dt is the instantaneous air volume of that time
interval. At the end, we integrate all these instantaneous

	
  

	
  

	
  

Figure 11: Frequency Response Curves on different airflow
speed
	
  

	
  

	
  

Figure 12: Change in Frequency Response Intensity with
the increase of airflow speed
	
  

	
  

6.2 mCOPD Performance on FEV1, FVC Calculation
After we verified our microphone model, we transferred our
mathematical model into our mobile phone application mCOPD and did 40 concurrent tests using a mobile phone
and a digital spirometer. Test results (Figure 14) show that

	
  

	
  

the average deviation between the data FVC tested by mCOPD and the data tested by the digital spirometer is about
6.5%; the average deviation between the date FEV1 tested
by mCOPD and the data tested by the digital spirometer
is about 3.6% (Figure 15); the average deviation of the data FEV1/FVC when compared to the clinical spirometer is
3.9% (Figure 16) for common measures of lung function.

	
  

Figure 15: FEV1 difference between mCOPD Data and Actual Spirometer Data (average deviation is 3.6%)
	
  

	
  

	
  
	
  

	
  

	
  

Figure 13: One Subject with traditional spirometer and mCOPD

6.2.1

Test Setup and Procedure

To evaluate and inform the design of mCOPD, we find 40
volunteers to participate in our comparable test. Our custom lung function diagnosis application for Android recorded subjects’ exhalation sound using the built-in microphone
and provided feedback to the user based on testing data.
We also obtained measurements during the same session using a standard clinical spirometer. The spirometer is a fully
electronic spirometer. It measures the speed of the airflow
by measuring pressure difference in the channel. Figure 13
shows the use procedure of mCOPD.

	
  

Figure 16: FEV1/FVC difference between mCOPD Data
and Actual Spirometer Data (average deviation is 3.9%)
	
  

	
  
	
  
up, after one fully exhalation of tester, we can get two results
simultaneously from the mobile phone and from the clinical
spirometer. Spirometry measurements rely largely on effort.
Each participant is coached how the test is conducted and
is asked to practice using the spirometer first before we feel
they are able to perform an acceptable test.
	
  

	
  

	
  

	
  

Figure 14: FVC difference between mCOPD Data and Actual Spirometer Data (average deviation is 6.5%)
Participants hold the clinical spirometer and exhale to the
mouthpiece. Since the cylindrical mouthpiece is hollow, we
fix the mobile phone in front of the mouthpiece. In this way,
all air going through the mouthpiece will produce signals on
the built-in microphone. The distance between the mobile
phone and tester’s mouth is very close and there is no resistance inside the mouthpiece, so the decrease of airflow speed
between this short distance is very limited. Using this set

	
  

Figure 17: Stages of COPD [25]

6.3 Diagnostic Recommendations for Different Stages

	
  

	
  

mCOPD will give out diagnostic recommendations at the
end of every breathing test based on testing results (FEV1,
FVC, FEV1/FVC) and existing guidelines from the Global
Initiative for Chronic Obstructive and Lung Disease (GOLD)
for COPD Diagnosis and Management (see Figure (Figure
17)). mCOPD will also provide different treatment recommendations for different stages. For example, after a breathing test, mCOPD detect that the tester may possibly be at
stage 1 of COPD. mCOPD then gives out a list of treatment
recommendations to the tester, like: quit smoking, get flu
and pneumonia vaccine, take Bronchodilators, eat healthy
and do daily exercise, etc.

7. CONCLUSION AND FUTURE WORK
As the third leading cause of death in United States, COPD
is now gaining more and more attention by both physicians and patients. The mCOPD lung function diagnosis and
exercise system successfully combines novel airflow sensing
method with smart phone based exergaming which explores
a new way in long term COPD monitoring and gives out a
brand new method in rehabilitation exercising with high user compliance. Future extensions are also possible in either
remote data transferring or adding gaming features. More
research will be done in the future in order to improve calibration and network data sharing.

8. REFERENCES
[1] NHLBI morbidity and mortality chartbook. 2008.
[2] A. L. Association, COPD Fact Sheet. 2007.
[3] M. S. Niederman, “Introduction: Mechanisms and
management of copd: We can do better - it’s time for
a re-evaluation,” Chest, vol. 113, 1998.
[4] C. J. Murray and A. D. Lopez, “Evidence-based health
policy–lessons from the global burden of disease
study,” Science, vol. 274, pp. 740–3, Nov 1996.
[5] B. R. Celli and W. MacNee, “Standards for the
diagnosis and treatment of patients with copd: a
summary of the ats/ers position paper,” Eur Respir J,
vol. 23, pp. 932–46, Jun 2004.
[6] A. J. F. M, “Chronic obstructive pulmotary disease
and health related quality of life,” Ann Intern Med,
vol. 127, pp. 1072–1079, 1997.
[7] J. Liu, M.-C. Huang, W. Xu, N. Alshursfa, and
M. Sarrafzadeh, “On-bed Monitoring for Range of
Motion Exercises with a Pressure Sensitive Bedsheet,”
in IEEE Conference on Body Sensor Network, 2013.
[8] N. R. Anthonisen, E. C. Wright, and J. E. Hodgkin,
“Prognosis in chronic obstructive pulmonary disease,”
Am Rev Respir Dis, vol. 133, pp. 14–20, Jan 1986.
[9] S. HandiHaler, Breathing Exercises.
[10] W. Xu, M.-C. Huang, N. Amini, J. Liu, L. He, and
M. Sarrafzadeh, “Smart Insole: A Wearable System
for Gait Analysis,” in International Conference on
Pervasive Technologies Related to Assistive
Environments, Jun. 2012.
[11] M.-C. Huang, W. Xu, J. Liu, L. He, Y. Su, and
M. Sarrafzadeh, “Inconspicuous Personal Computer
Protection with Touch-Mouse,” in International
Conference on Human Computer Interaction, 2013.

	
  

[12] Wenyao Xu and Ming-Chun Huang and Navid Amini
and Lei He and Majid Sarrafzadeh, “Smart cushion:
Design and calibration of textile sensors for sitting
posture analysis,” IEEE Sensors Journal, 2013.
[13] M.-C. Huang, W. Xu, Y. Su, C.-Y. Chang, B. Lange,
and M. Sarrafzadeh, “Smart Glove for Upper
Extremities Rehabilitative Gaming Assessment,” in
International Conference on Pervasive Technologies
Related to Assistive Environments, Jun. 2012.
[14] M.-C. Huang, E. Chen, W. Xu, M. Sarrafzadeh, B.
Lange, and C.-Y. Chang, “Gaming for Upper
Extremities Rehabilitation,” in ACM/EMS Conference
on Wireless Health, Oct.
[15] M. G. E. C. Larson, G. Boriello, S. Heltshe,
M. Rosenfeld, and S. N. Patel, “Spirosmart: Using a
microphone to measure lung function on a mobile
phone,” in 2012 ACM Conference on Ubiquitous
Computing, pp. 280–289, ACM, 2012.
[16] W. T. Goldsmith, A. M. Mahmoud, J. S. Reynolds,
W. G. McKinney, A. A. Afshari, A. A. Abaza, and
D. G. Frazer, “A system for recording high fidelity
cough sound and airflow characteristics,” Ann Biomed
Eng, vol. 38, pp. 469–77, Feb 2010.
[17] M. H. J. Kroutil, “Detection of breathing,” in
Advanced Semiconductor Devices and Microsystems,
2008. ASDAM 2008, The Seventh International
Conference on Advanced Semiconductor 167Devices
and Microsystems, pp. 167–170, 2008.
[18] K. Anderson, Y. Qiu, A. R. Whittaker, and M. Lucas,
“Breath sounds, asthma, and the mobile phone,”
Lancet, vol. 358, pp. 1343–4, Oct 2001.
[19] J. A. a. F. Homs-Corbera, “Time-frequency detection
and analysis of wheezes during forced exhalation,”
IEEE Transaction, vol. 51, 2004.
[20] J. B. S. Bradley, S. von Hunerbein, and T. Wu, “The
mechanisms creating wind noise in microphones,” in
Acoustic Engineering Society Convention, 2003.
[21] W. L. E. Nemer, “Single-microphone wind noise
reduction by adaptive postfiltering,” in IEEE
Workshop on Applications of Signal Processing to
Audio and Acoustics, 2009.
[22] J. W. R. Raspet and K. Dillion, “Framework for wind
noise studies,” Acoustical Society of America, vol. 43,
p. 9, 2005.
[23] R. D. Leader, How Do I Perform Pursed Lip
Breathing? 2010.
[24] R. D. Leader, How To Perform Diaphragmatic
Breathing With Pursed Lips. 2009.
[25] G. I. f. C. O. L. Disease, GOLD Guidelines COPD
Diagnosis and Management: At-A-Glance Desk
Reference. 2012.

A Body-Voltage-Sensing-Based Short Pulse Reading Circuit for
Spin-Torque Transfer RAMs (STT-RAMs)
Fengbo Ren, Henry Park, Richard Dorrance, Yuta Toriyama, C.-K. Ken Yang, Dejan Marković
Department of Electrical Engineering, University of California, Los Angeles, CA, USA
E-mail: fren@ee.ucla.edu
sensing current that is strictly bounded by the long duration
switching current (IC) of the MTJ. Consequently, the scaling
of JC will eventually challenge the viability of the LCR
sensing scheme for a high-speed reading.
To solve the problem, a short-pulse reading (SPR) scheme
has been proposed in [9], where a sensing current that is
similar in magnitude to the writing current is used to read
the MTJ, but with a much shorter pulse width. However, no
circuit implementation of the SPR scheme has been
published thus far. Naturally, there has been no clear answer
to what the best circuit structure to implement the SPR is.
In this work, we propose an SPR circuit structure with a
body-voltage sensing circuit. To study its suitability for the
SPR, we analyze the read margin (RM) and performance of
the proposed sensing circuit and compare them to those of
the two reference designs [7], [8] under the proposed SPR
structure. The analysis is validated by Monte-Carlo
simulations in HSPICE using a 65-nm CMOS technology,
considering both CMOS and MTJ variations. Results show
that the proposed sensing circuit outperforms the reference
designs by a large margin in sensing speed for the same
energy. In the worst case of PVT variations, the proposed
circuit can achieve a RM as high as 300 mV under a 1 V
supply with only 0.78 ns of sensing time, while the reference
designs require 4.3 ns and 2.3 ns to achieve a RM of 200
mV, respectively.
The remainder of the paper is organized as follows.
Section 2 introduces MTJ basics and discusses the SPR
scheme. Implementation of the proposed SPR scheme is
described in Section 3. Section 4 reviews the reference
designs, introduces analysis metrics and the simulation setup.
The comparison results and discussions are presented in
Section 5. Section 6 concludes the paper.

Abstract
With scaling of CMOS and Magnetic Tunnel Junction
(MTJ) devices, conventional low-current reading techniques
for STT-RAMs face challenges in achieving reliability and
performance improvements that are expected from scaled
devices. The challenges arise from the increasing variability
of the CMOS sensing current and the reduction in MTJ
switching current. This paper proposes a short-pulse reading
circuit, based on a body-voltage sensing scheme to mitigate
the scaling issues. Compared to existing sensing techniques,
our technique shows substantially higher read margin (RM)
despite a much shorter sensing time. A narrow current pulse
applied to an MTJ significantly reduces the probability of
read disturbance. The RM analysis is validated by MonteCarlo simulations in a 65-nm CMOS technology with both
CMOS and MTJ variations considered. Simulation results
show that our technique is able to provide over 300 mV RM
at a GHz frequency across process-voltage-temperature
(PVT) variations, while the reference designs require 4.3 ns
and 2.3 ns sensing time for a 200 mV RM, respectively. The
effective read energy per bit required by the proposed
sensing circuit is around 195 fJ in the nominal case.

Keywords
Emerging memory, STT-RAM, sensing circuit, short-pulse
reading, body-voltage sensing, read margin.

1. Introduction
Over the last decade, extensive research has been carried out
in the search of a scalable “universal memory.” PhaseChange RAM (PC-RAM) has been shown to be a viable
replacement for Flash [1]. Resistive RAM (RRAM) is in its
initial stage of exploration [2] and its benefits are yet to be
seen. Recently, STT-RAM has been regarded as the front
runner, because it can achieve a smaller cell size than
SRAM, better performance than DRAM, the non-volatility
of Flash, and better endurance (on the order of 1016
read/write cycles) than Magnetoresistive RAM (MRAM)
[3]-[5]. Compared to MRAM, another advantage of STTRAM is that the switching current scales with device size [5]
due to the nature of spin-torque transfer. With future scaling,
the variation in CMOS devices is likely to continue to
increase, and the critical current density (JC) of the MTJ
devices will decrease. These two effects combined will
greatly impede the reliability of the MTJ read operation
unless the reading is to be performed with levels of current
that are comparable to those used for the write operation.
Most existing STT-RAM reading schemes use a lowcurrent reading (LCR) in which a sensing current smaller
than the writing current is applied on the selected MTJ to
avoid read disturbance [3], [6]-[8]. This approach leads to a
978-1-4673-1036-9/12/$31.00 ©2012 IEEE

2. Towards High-Speed Reading of STT-RAM

Figure 1: The basic MTJ structure illustrating parallel and
anti-parallel states and switching current.
275

13th Int'l Symposium on Quality Electronic Design

60

%
40

to

%
80

%

AP

10
0%

Switching Current (uA)

%

20%
20%
0%
10

80% 60%
%
40

g
hin
itc
Sw rob.
P

0%
10

P
oA

20%

Pt

0%

40
%

Switching Current (uA)

P

0%

0%

Current Density (MA/cm2)

0
10

80%

60%

40%
0%
ing
itch
Sw rob.
P

Current Density (MA/cm2)

2.1. MTJ Switching Characteristics
MTJ is the storage element of STT-RAM. It consists of two
ferromagnetic layers separated by a thin nonconductive
tunneling barrier (e.g. MgO) as shown in Fig. 1. The thicker
ferromagnet, whose layer-stack structure fixes its magnetic
orientation, is called the fixed layer or the pinned layer. The
thinner layer, of which magnetic orientation can be changed,
is called the free layer. The MTJ exhibits two resistive states
determined by the relative magnetization directions of the
fixed and the free layers: a parallel (P) orientation produces
a low resistance (RP) and an anti-parallel (AP) orientation
results in a high resistance (RAP). The resistance difference
between the two states is measured by tunnel magnetoresistance ratio (TMR), defined as (RAP − RP)/RP. A higher
TMR indicates better readability and is thereby preferred by
the reading operation.
Similar to MRAMs, STT-RAMs store information in
MTJs in a magnetic form—“0” and “1” are represented as
different magnetization directions of the free layer. The
switching of the MTJ is made by applying a bi-directional
writing current to the device as shown in Fig. 1: the current
in the direction from the fixed (free) to the free (fixed) layer
writes the MTJ into the AP (P) state. A typical switching
characteristic of the MTJ is depicted in Fig. 2. The contours

Figure 2: The switching characteristic of the MTJ with a
free-layer stack of Co60Fe20B20 and a size of 50 nm by 130
nm. (a) AP to P, (b) P to AP.

show that the current density required for achieving a certain
switching probability is a function of the switching time.
This graph indicates that there is a tradeoff between the
amplitude and the pulse width of the sensing/writing current
in STT-RAM. Note also that the AP to P and the P to AP
switching are asymmetric—P to AP usually requires a
higher current density for the same switching probability. In
this paper, JC refers to the critical current density required
for 100% switching probability, and the critical current (IC)
is calculated as JC times the MTJ junction area.
2.2. The Need for SPR
In STT-RAM design, the writing current distribution should
stay above the 100% region (Fig. 2) to guarantee successful
writing. Similarly, the sensing current distribution has to be
kept below the 0% region to avoid accidental switching
(read disturbance). The conventional LCR scheme avoids
the read disturbance by keeping the read-current amplitude
substantially below IC. Typical writing currents today are in
the 300-500 µA range and reading with 1/3 or 1/5 of the
write current is still feasible. However, scaled MTJs would
need to work with writing currents on the order of 10s of µA
[4], making the LCR impractical for fast reading.
Alternatively, the SPR scheme uses a higher sensing current
amplitude with a shorter duration [9] to effectively improve
the sensing speed without risking the read disturbance. As a
result, the SPR scheme is of great interest for designing the
fast and reliable reading circuit for future STT-RAMs.
This paper contributes an architecture and a circuit design
of the SPR scheme. The proposed SPR circuit will be
discussed in the following sections.

3. Design of High-Speed SPR Circuit
3.1. SPR Architecture
The key idea of SPR is to perform reading with a short pulse
of the sensing current applied on the MTJ device. Generally,
the shorter the pulse, the shorter the sensing time and the
lower the chance for a read disturbance. Figure 3 shows the
proposed SPR scheme, which includes a sensing circuit that
compares the cell value with a reference followed by a
capturing latch.
The sensing circuit uses a voltage sensing scheme. The
data sensor converts the MTJ resistance into a voltage signal
(VDATA) that has two levels, VH and VL corresponding to
sensing RAP and RP, respectively. The reference sensor

Figure 3: Proposed SPR scheme.

averages VH and VL to generate the reference voltage VREF.
The difference of VDATA and VREF is further amplified by the
2nd-stage amplifier so that the resulting differential output
VOUT reflects the sensed resistance by its polarity, namely
VOUT > 0 (< 0) for reading RAP (RP).
The capturing latch regenerates VOUT into a full-swing
signal. Theoretically, the latch utilizing positive feedback
has an infinite gain and is capable of resolving an arbitrarily
small voltage difference. Practically, the minimum
resolvable voltage is limited by mismatch and noise. Using a
strong positive feedback allows for early and quick data
regeneration.
Figure 4 illustrates the timing diagram of the proposed
SPR scheme. Instead of waiting for VOUT to completely
settle, the capturing latch is enabled to regenerate the final
output, once VOUT with sufficient RM has been established
(RM is analyzed in Section 4). Then the sensing circuit is
disabled for the rest of the reading operation to cut off the
sensing current thereby minimizing its pulse width.
3.2. Body-Voltage Sensing Circuit (BVSC)
In the sensing circuit, the sensing signal VDATA is converted
from the sensing current (IMTJ) through a load network. The
swing of VDATA, which has to be large enough to suppress
device mismatches and noise, is given by
VH − VL = (IMTJ,P − IMTJ,AP)·RLOAD ,
(1)
where the conversion gain (RLOAD) is the small signal
resistance of the load device. According to Eq. (1),
increasing RLOAD increases the signal swing, but only to a
certain extent. As the MTJ resistance varies due to the
geometry and randomness in the tunnel barrier thickness due
to manufacturing, VDATA (and VREF) also has variation. We
define a statistical measure of the worst-case margin
between VH and VL as the sensing margin (SM), given by
SM = μ(VH − VL) − 3σ(VH − VL) .
(2)
One should note that the primary design objective is to
maximize SM, not simply its mean. Further increasing
RLOAD beyond some point would eventually deteriorate SM,
since a higher RLOAD also amplifies the variance of the
sensing signal. In addition, a load device with large RLOAD is
not desirable for SPR as it introduces a large RC time
constant, limiting the sensing speed [10]. Therefore,
choosing an optimum RLOAD, that should be neither too big
nor too small, is critical to the quality of sensing.
Figure 5 (a)-(c) shows different types of loads that are
commonly used for resistance-sensing circuits. From the
above discussion, it is noted that none of them are ideally

Figure 4: Timing diagram of the SPR circuit.

Figure 5: Different types of transistor loads and their RLOAD.
(a) diode-connected load, (b) current source load, (c) current
mirror load, (d) body-connected load.
suited for SPR. The diode-connected load (Fig. 5 (a)) has a
small RLOAD (1/gm), resulting in a small SM. The currentsource load (Fig. 5 (b)) has a large RLOAD (rO), making it
sensitive to the MTJ resistance variation. Its bandwidth at
the sensing node (VDATA) is also limited. The current-mirror
load (Fig. 5 (c)) allows for the sensing and amplification to
be performed in the same stage, but it has imbalanced load
impedance and a limited bandwidth at the sensing node. To
account for both SM and speed, we propose a bodyconnected load as shown in Fig. 5 (d). This load connects
the body (n-well) and the drain terminal of a PMOS
transistor. Its effective RLOAD is 1/gmb [10]. As compared to
the diode-connected load (RLOAD = 1/gm), the bodyconnected load has a larger output impedance due to the fact
that the body voltage is weaker at tuning the current than the
gate voltage is (gmb < gm). On the other hand, 1/gmb is still
much smaller than the RLOAD of the current-source load (rO).
As a result, the body-connected load properly trades off
speed to effectively increase SM, instead of choosing an
extreme as in the cases from Fig. 5 (a)-(c).
Figure 6 shows the schematic of the proposed BVSC.
Besides a body-connected PMOS load, an NMOS clamp
transistor cascading with the column mux device is used in
the sensor circuits for controlling the sensing current as well
as for shielding the BL voltage from the voltage variation at
node VDATA. The reference sensors connecting to the
reference cells constantly sense RP and RAP to generate VL
and VH, respectively. A voltage divider network in between
generates VREF by averaging VL and VH. The 2nd-stage
amplifier uses two differential pairs, each generating one of
the differential outputs (VOUT+ or VOUT-). This differential
amplifier provides extra signal swing at the outputs,
enabling a more reliable and early data regeneration. The
VOUT is finally regenerated to a full-scale signal by the
dynamic latch. The proposed BVSC has a significant speed
advantage due to two key factors: 1) the body-connected
loads guarantee a large bandwidth of the sensor circuits 2)
the amplification stage, which is completely decoupled from
the sensing stage, has more freedom in tuning the current to
trade off power with performance.

Dyn.
Latch

Body-Voltage Sensing Circuit

2nd Stage
Amp

Data
Sensor

Reference
Sensor

Col. Mux
Device

Memory
Cell

Col. Mux
Device

RP

RDATA

Col. Mux
Device

Reference
Cell

Access
Device

RAP

Access
Device

Memory Core

Figure 6: Schematic of the proposed body-voltage-sensing-based SPR circuit.

4.1. Reference Designs
We compare the proposed BVSC with two recent designs
that use current-sensing scheme [7], [8] for reading. The
first reference design is an improved current-mirror-based
sensing circuit (CMSC) presented in [7]. This design adds
an equalizer to the sense amplifier outputs to mitigate the
issues of the imbalanced output impedance and the skewed
sensing time of reading RP and RAP, of the original currentmirror sense-amplifier based design [6]. The second
reference design is the split-path sensing circuit (SPSC) [8].
This design implements a double current-mirror based
differential amplifier by splitting the sensing current into
two paths and mirroring them differentially to improve the
output signal swing and RM.
4.2. Read Margin (RM)
The timing of enabling the data regeneration phase is critical
to the sensing integrity. To avoid reading errors caused by
the device mismatch and noise, the regeneration phase
cannot be activated until the target signal amplitude of VOUT
has been established. This condition is given by
|VOUT| ≧ NM + VOS-DL ,
(3)
where VOS-DL is the input-referred offset voltage of the
dynamic latch, and NM is the noise margin. Similar to SM,
the VOUT fluctuation, due to CMOS and MTJ variations, is
statistically characterized by defining RMP and RMAP as
RMP = μ(VOUT,P) + 3σ(VOUT,P) ,
(4)
and

RMAP = μ(VOUT,AP) − 3σ(VOUT,AP) ,
(5)
for reading RP and RAP, respectively. Fig. 7 illustrates the
definition shown in Eqs. (4) and (5). Note that RMP and
RMAP have different polarities, however it is the absolute
value that represents the actual read margin. So the overall
RM is defined as the smaller of the two as
RM = min (|RMP|, |RMAP|) .
(6)
It is important to note the difference between SM defined
in Eq. (2) and RM defined here: SM characterizes the worstcase signal swing of the single-ended output of sensor
circuits, while RM measures the worst-case signal amplitude
of the differential outputs of the 2nd-stage amplifier, for
VOUT,P
Distribution

Frequency

4. Comparison Method

VOUT,AP
Distribution

+3σ

-3σ

RMP RMAP
μ
-VDD

μ
0
Voltage (V)

Figure 7: Illustration of the definition of RM.

VDD

12000

Table I: Summary of MTJ parameters.
50 nm × 130 nm
14.8 Ω·um2
110%
4%
5%

reading RP and RAP, respectively. With the RM definition in
Eq. (6), the condition in Eq. (3) can be expressed as
RM > NM + VOS-DL .
(7)
Eq. (7) indicates that higher RM allows better noise margin,
but it also requires more sensing time to achieve the noise
margin. Consequently, a reliable reading with fast access
demands proper tradeoff between noise margin and sensing
time. In general, the higher the RM a sensing circuit is able
to achieve, the shorter the sensing time it needs for meeting
the same noise margin target. Therefore, one of the main
objectives in designing the SPR circuit is to maximize RM
of the sensing circuit.

8000
6000

BVSC
SM = 195 mV

4000
2000
0

0

0.1

0.2

0.3

Voltage (V)

0.4

0.5

Figure 8: Distribution of sensing signal (VDATA) swing
(VH − VL) of SPSC (diode-connected load) and BVSC
(body-connected load). SM is calculated using Eq. (2).
optimization tool in HSPICE for the targets of maximizing
the RMs at both 1 ns and 10 ns sensing time, under the same
sensing current. The target sensing current (IMTJ,P) through
the selected MTJ device was set at 50 µA.

5. Comparison Results

Frequency

Frequency

Frequency

Frequency

Frequency

Figure 8 shows the sensing signal (VDATA) swing (VH-VL)
comparison between SPSC that uses a diode-connected load
and the proposed BVSC with a body-connected load. Using
Eq. (2), SM is extracted from this plot. It can be seen that
the body-connected load provides a better sensing quality
due to a larger signal swing—it outperforms the diodeconnected load with over a 3.5x higher SM. Such an
improvement greatly relaxes the device matching constraints

Frequency

4.3. Simulation Setup
The three sensing circuits are compared under the same SPR
structure (Fig. 3) by performance, RM, and reliability
through HSPICE simulations in a 65-nm CMOS technology.
The MTJ model used in our simulations is summarized in
Table I and Fig. 2. Both the chip-to-chip and across-chip
local variation of CMOS device, and MTJ variations are
implemented in our Monte Carlo simulations. The MTJ
variation is modeled by the standard deviation of RA (σRA)
and TMR (σTMR) extracted from measurements [11]. A total
±5 σ of the MTJ variation is considered. For all circuits, the
key design parameters such as the geometry and the bias
voltage of critical transistors are optimized using the built-in

SPSC
SM = 55 mV

10000

Frequency

Size
RA
TMR
σRA
σTMR

Figure 9: The single-ended output (VOUT+, VOUT-) distribution of (a) CMSC, (b) SPSC, and (c) BVSC, and the differential
outputs (VOUT) distribution of (d) CMSC, (e) SPSC, and (f) BVSC in the nominal case. RM is calculated using Eqs. (4) and (5).

800

effective RM at short sensing times. SPSC has an improved
RM as compared to CMSC, but the limited bandwidth at
output nodes restricts its RM with a short sensing time also.
The results in Fig. 10 show that, with a 1 ns sensing time,
the proposed BVSC is able to achieve over 600 mV RM in
the nominal case. This RM is 2.2 and 1.3 times higher than
that of CMSC and SPSC with sensing times of 5 ns,
respectively.
The effect of temperature variation on the RM and
performance is shown in Fig. 11, with RM calculated using
Eqs. (4), (5) and (6). Temperature variation has little impact
on performance for all the sensing circuits. In the worst case
of temperature, the performance degradation for CMSC,
SPSC, and BVSC (for a RM level of 200 mV) is about 0.33
ns, 0.12 ns, and 0.01 ns, and the RM reduction is 6%, 4%,
and 3%, respectively.
Figure 12 shows the effect of supply voltage variation.
Decreasing voltage has negative impact on performance, as
expected. The RM of CMSC and SPSC gets reduced
accordingly, so does the common mode (CM) level of the

400

SPSC RMAP

200

CMSC RMAP

0
CMSC RMP

-200
-400
-600

BVSC RMP

1

Sensing Time (ns)

SPSC RMP

Read Margin (mV)

Read Margin (mV)

Figure 11: RM versus sensing time with temperature
variation. RM is calculated using Eqs. (4), (5) and (6).

BVSC RMAP

600

-800
0.4

Read Margin (mV)

of the following amplifier stage.
Figure 9 shows the output voltage distribution of the three
sensing circuits after VOUT is completely settled. RM is
extracted according to Eqs. (4) and (5). Note in Fig. 9 (a)
that CMSC has the identical VOUT- distribution for both
reading RP and reading RAP, as the common VOUT- is
generated directly from the reference cell without
differential amplification [7]. Consequently, its maximum
range of VOUT is limited to ±VDD/2, and so is RM. The
simulation result (Fig. 9 (d)) shows that CMSC is able to
achieve an RM of 285 mV under the nominal supply voltage
(1V). Alternatively, the VOUT- can be generated from a
differential amplification stage in SPSC and BVSC circuits.
This method can produce a VOUT- in complement to VOUT+,
thereby effectively doubling RM. However, as SPSC
performs sensing and amplification in the same stage,
transistors in SPSC are placed in series with the memory
cells. As a result, the voltage headroom consumed by these
devices limits its output swing and subsequently RM. Figure
9 (e) shows that SPSC is able to achieve over 520 mV RM
under 1V VDD. The proposed BVSC implements the sensing
and the amplification in separate stages. Its RM is
proportional to the output swing of the 2nd stage amplifier
and is intrinsically large. So the BVSC design should be
optimized with more emphasis on the power budget rather
than large RM. As shown in Fig. 9 (f), BVSC has the largest
RM that is over 620 mV under 1V VDD. With respect to
CMSC and SPSC, BVSC has a RM improvement of 335
mV and 100 mV, respectively.
The sensing time required for a certain RM is of practical
interest to SPR. RM comparison measured at different
sensing times is shown in Fig. 10. The SPR technique
demands for a faster and wider separation of the RM curves.
From this perspective, BVSC clearly shows the best
suitability to SPR with great advantages in both RM and
performance. CMSC has more balanced RMP and RMAP,
which results from the equalizer used at the outputs as
suggested in [7]. However, the equalization phase also
brings about 1 ns overhead in delay, reducing CMSC’s

10

Figure 10: RM versus sensing time in the nominal case.
BVSC has the best RM and performance due to bodyvoltage sensing and the 2nd-stage amplifier.

Figure 12: RM versus sensing time with supply voltage
variation. RM is calculated using Eqs. (4), (5) and (6).

Table II: Sensing time required for different RM levels
in the worst case across PVT variations.

Power (μW)

135

80

300

Read Energy/bit
(fJ)

585

178.5

195.5

%

80%

0%

40
0.2

1

10

Sensing Time (ns)
120

IMTJ,P

110

CMSC
SPSC
BVSC

100
90
80

(b)

70

10
0%

AP

BVSC

100%

P

SPSC

60

to

CMSC

60

(a)

to

Table III: Average power and read energy per bit

CMSC
SPSC
BVSC

80

P

sensing signal (VDATA) in BVSC. For BVSC, the output
swing of the 2nd stage amplifier is not only inversely related
to the CM level of VDATA, but also proportional to the supply
voltage. As a result, these two factors cancel each other, and
BVSC is less sensitive to the supply voltage variation. In the
worst case of supply voltage, the performance degradation
for CMSC, SPSC, and BVSC (for a RM level of 200 mV) is
about 1.5 ns, 0.23 ns, and 0.1 ns, and the RM reduction is
28%, 9%, and 6%, respectively.
Table II summarizes the sensing time required for
achieving different RM levels in the worst case across PVT
variations. As the condition in Eq. (7) suggests, RM must be
large enough before launching the regeneration phase, in
order to overcome the device variations and noise.
Simulation results show that a small input-referred offset
voltage (σ(VOS-DL) < 10 mV) can be achieved by properly
sizing up the dynamic latch. By considering ±3σ of the
input-referred offset, a 200 mV RM can guarantee a noise
margin of around 170 mV. The worst-case sensing time
required by CMSC, SPSC, and BVSC for achieving such
level of noise margin is 4.33 ns, 2.25 ns, and 0.65 ns,
respectively. The proposed BVSC has significant speed
advantage over CMSC and SPSC. It is able to provide over
300 mV RM at a GHz speed, which enables the practical
application of the SPR scheme.
Table III summarizes the average power of the sensing
circuits in the nominal case, and the effective read energy
per bit based on the worst-case sensing time required for a
RM level of 200 mV (Table II). BVSC consumes higher
power than CMSC and SPSC, resulting from the use of the
2nd-stage sense amplifier. However, due to the higher speed,
the effective read energy per bit required by BVSC is close
to that of SPSC and much lower than that of CMSC. This
indicates that the proposed BVSC is able to greatly boost the
read performance without sacrificing energy efficiency.
Note that body-voltage sensing requires isolated N-wells
for the PMOS transistors in sensor circuits (Fig. 6). Besides,
BVSC has more transistors due to the 2nd-stage amplifier.

100

AP

100
200
300
400
500
600

IMTJ,AP
0%
10

RM (mV)

Sensing Time (ns)
CMSC SPSC BVSC
1.96
1.48
0.57
4.33
2.25
0.65
N/A
3.16
0.78
N/A
4.41
1.0
N/A
N/A
1.72
N/A
N/A
3.64

120

60

60 80%
%
40
%

40

50
0.2

1

%

10

Sensing Time (ns)

Figure 13: The sensing current distribution, (a) IMTJ,AP, (b)
IMTJ,P, with scaled switching characteristic of the MTJ
(geometry from Table I and JC scaled by 0.5).
These result in a certain area overhead on the peripheral
circuitry. However, its impact on the overall area diminishes
in proportion to the utilization rate of the memory.
Figure 13 shows the sensing current distribution of the
three circuits when operating with a 170 mV noise margin,
plotted with the MTJ switching characteristic. To project the
advantage of BVSC-based SPR circuit in future scaled STTRAMs, we assume both JC and the size of the MTJ are
scaled down by a factor of two. Clearly, the conventional
reading techniques tend to be more destructive at such a
level of scaling. With a similar amount of the sensing
current (IMTJ,P = 50 µA), BVSC is able to operate with 3-7
times shorter sensing time, which significantly reduces the
probability of read disturbance. Therefore, BVSC has better
support for future advanced STT-RAMs in terms of
performance and reliability.

6. Conclusions
A body-voltage sensing based short-pulse reading circuit is a
viable solution for high-speed and reliable reading of future
scaled STT-RAMs. The proposed body-connected load
properly trades off sensing speed with over 3x improvement
in sensing margin, as compared to the conventional diode-

connected load. A 2nd-stage differential amplifier further
enhances the read margin, which allows earlier data latching
with the same level of noise margin. As a result, the
proposed SPR circuit is able to perform high-speed readings
with the shortest current pulse reported to date. Such a short
pulse (~ 1ns) has great promise to eliminate read disturbance
and to support the aggressive scaling required of future lowpower STT-RAM memories.

Acknowledgments
This work was supported by the DARPA STT-RAM
(HR0011-09-C-0114) program. The authors thank Prof.
Jianping Wang and Dr. Hui Zhao form the University of
Minnesota for providing MTJ measurement data.

References
[1]

[2]

G. De Sandre, et al., “A 4 Mb LV MOS-Selected Embedded
Phase Change Memory in 90 nm Standard CMOS
Technology,” IEEE J. Solid-State Circuits, vol. 46, no. 1, pp.
52–63, Jan. 2011.
W. Otsuka, et al., “A 4Mb Conductive-Bridge Resistive
Memory with 2.3GB/s Read-Throught put and 216MB/s
Program-Throughput,” in Proc. Int. Solid-State Circuits
Conf., 2011, pp. 210–211.

[3]

K. Tsuchida, et al., “A 64Mb MRAM with ClampedReference and Adequate-Reference Schemes,” in Proc. Int.
Solid-State Circuits Conf., 2010, pp. 258–259.
[4] D. Smith, et al., “Latest Advances and Roadmap for In-Plane
and Perpendicular STT-RAM,” in 3rd Int. Memory Workshop
(IMW), May 2011, pp. 1–3.
[5] F. TAbrizi, “Non-volatile STT-RAM: A True Universal
Memory,” Grandis Inc, Aug, 2009.
[6] G.D. Arndt, et al., “A 16-Mb MRAM Featuring
Bootstrapped Write Drivers,” IEEE J. Solid-State Circuits,
Vol. 40, No. 4, pp. 902-908, Apr. 2005.
[7] J.P. Kim, et al., “A 45nm 1Mb Embedded STT-MRAM with
Design Techniques to Minimize Read-Disturbance,” in Proc.
VLSI Symp., 2011, pp. 296-297.
[8] S.O. Jing, et al., “Split Path Sensing Circuit,” U.S. Patent
2010/0321976 A1, filled June 17, 2009.
[9] K. Ono, et. al., “A Disturbance-Free Read Scheme and A
Compact Stochastic-Spin-Dynamics-Based MTJ Circuit
Model for Gb-Scale SPRAM,” in Proc. IEEE Int. Electron
Devices Meet., 2009, pp. 1–4.
[10] B. Razavi, Design of Analog CMOS Integrated Circuits,
Chapter 6, Tata McGraw-Hill, 2002.
[11] R. Dorrance, et. al., “Scalability and Design-Space Analysis
of a 1T-1MTJ Memory Cell,” in Proc. ACM/IEEE Int. Symp.
on Nanoscale Arch., June 2011, pp. 32–36.

A Scalable Sparse Matrix-Vector Multiplication Kernel for
Energy-Efficient Sparse-Blas on FPGAs
Richard Dorrance

Fengbo Ren

Dejan Marković

EE Department, UCLA
Los Angeles, CA 90095 USA

EE Department, UCLA
Los Angeles, CA 90095 USA

EE Department, UCLA
Los Angeles, CA 90095 USA

rdorrance@ucla.edu

renfengbo@ucla.edu

dejan@ee.ucla.edu

ABSTRACT

1. INTRODUCTION

Sparse Matrix-Vector Multiplication (SpMxV) is a widely used
mathematical operation in many high-performance scientific and
engineering applications. In recent years, tuned software libraries
for multi-core microprocessors (CPUs) and graphics processing
units (GPUs) have become the status quo for computing SpMxV.
However, the computational throughput of these libraries for
sparse matrices tends to be significantly lower than that of dense
matrices, mostly due to the fact that the compression formats
required to efficiently store sparse matrices mismatches
traditional computing architectures. This paper describes an
FPGA-based SpMxV kernel that is scalable to efficiently utilize
the available memory bandwidth and computing resources.
Benchmarking on a Virtex-5 SX95T FPGA demonstrates an
average computational efficiency of 91.85%. The kernel achieves
a peak computational efficiency of 99.8%, a >50x improvement
over two Intel Core i7 processors (i7-2600 and i7-4770) and
showing a >300x improvement over two NVIDA GPUs (GTX
660 and GTX Titan), when running the MKL and cuSPARSE
sparse-BLAS libraries, respectively. In addition, the SpMxV
FPGA kernel is able to achieve higher performance than its CPU
and GPU counterparts, while using only 64 single-precision
processing elements, with an overall 38-50x improvement in
energy efficiency.

Sparse matrices arise in a wide variety of computational
disciplines, including image reconstruction, circuit and economic
modeling, industrial engineering, compressive sensing, neural
networks, and algorithms for least squares and eigenvalue
problems [1-3]. As such, Sparse Matrix-Vector Multiplication
(SpMxV) is the main computational kernel that dominates the
performance of many of the aforementioned applications.
Unfortunately, the performance of SpMxV algorithms tends to be
much lower than that of dense matrices, mostly due to the
mismatch between the memory access patterns of sparse matrices
and the compression formats required to efficiently store them
[3,4].
Numerous efforts have been made to accelerate the performance
of SpMxV on multi-core microprocessors (CPUs) [5,6] and
graphics processing units (GPUs) over the years [7-9]. Recently,
field-programmable gate arrays (FPGAs) have become an
attractive option for accelerating SpMxV [1-4,10-14]. FPGAs
have high floating-point performance, large amounts of on-chip
memory, and an abundant number of high-speed I/O pins capable
of providing large amounts of off-chip memory bandwidth. The
flexible nature of FPGAs also allows architectural adaptations to
satisfy the needs of different problems.
In this paper, we propose a scalable architecture for SpMxV with
higher computational efficiency than traditional CPU/GPU-based
approaches. Computational efficiency is a measure of the
percentage of the total hardware resources available that are
actively being used by an algorithm. An implementation of an
algorithm with a higher computational efficiency will therefore be
more energy-efficient. We leverage the structure of conventional
sparse matrix compression formats for general sparse matrices in
order to regularize their memory access patterns. The
benchmarking results based on the FPGA implementation show
that the proposed SpMxV kernel can reach significantly higher
computational efficiency than state-of-the-art solutions using
CPUs and GPUs, with more than a 50x and 300x improvement
respectively. Even for very large, irregular, sparse matrices, our
design can achieve performance comparable to that of dense
matrices.

Categories and Subject Descriptors
B.7.1 [Integrated Circuits]: Types and Design Styles—
Algorithms implemented in hardware; G.1.3 [Numerical
Analysis]: Numerical Linear Algebra—Sparse, structured, and
very large systems (direct and iterative methods)

General Terms
Algorithms, performance

Keywords
SpMxV, sparse-BLAS, FPGA, CPU, GPU, energy-efficiency,
computational efficiency, benchmarking
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
FPGA’14, February 26–28, 2014, Monterey, California, USA.
Copyright © 2014 ACM 978-1-4503-2671-1/14/02…$15.00.
http://dx.doi.org/10.1145/2554688.2554785

The remainder of the paper is organized as follows. Section 2
introduces SpMxV and discusses the inefficiencies present in
existing software (powered by CPUs and GPUs) and hardware
(FPGA) implementations. Section 3 details the proposed
architecture to address these short comings. Benchmarking results
on computational throughput and energy efficiency are presented
in Section 4. Section 5 concludes the paper.

161

(a)

1
0
A
5

0

0

4

0

0

2

3

0

0

0

0

7

8

0

9

0

6




(b)

data   1 4 2 3 5 7 8 9 6 

COO  row   0 0 1 1 2 2 2 3 3 
 col   0 1 1 2 0 3 4 2 4


(c)

 data   1 4 2 3 5 7 8 9 6 

CSR  ptr   0 2 4 7 9 
 col   0 1 1 2 0 3 4 2 4


(d)

data   1 5 4 2 3 9 7 8 6 

CSC  row   0 2 0 1 1 3 2 2 3 
 ptr   0 2 4 6 7 9 


Figure 2. A graphical representation of how SpMxV is
performed using the CSR format on CPUs and GPUs. Each
element in y is calculated as the dot product between the
appropriate row of A and the vector x.
Unlike the COO format, the row indices are not explicitly stored,
but rather as an array of row pointers, ptr. The ith element of ptr
corresponds to the offset of the ith row into the col and data
arrays. For example, in Fig.1(c) the first element of ptr is 0,
indicating that the first element in row 0 is 1 and is located in
column 0; the second element of ptr is 2, indicating that the first
element in row 1 is 2 and is located in column 1; the third element
of ptr is 4, indicating that the first element in row 2 is 5 and is
located in column 5; and so on. For an M×N matrix, ptr has M+1
elements in the CSR format, with the final element indicating the
total number of nonzero entries in the matrix. The compressed
sparse column (CSC) format, used in our SpMxV kernel, is a
variation of the CSR format (Fig. 1(d)). Instead of storing the
column indices and an array of row pointers, the CSC stores the
row indices and an array of column pointers. For any matrix A,
the CSR storage of A is exactly the same as the CSC storage of
AT.

Figure 1. The sparse matrix representation for (a) an example
matrix A in the (b) COO, the (c) CSR, and the (d) CSC
formats.

2. SPARSE MATRIX-VECTOR
MULTIPLICATION
SpMxV is a mathematical kernel that takes the form of:
y  Ax,

(1)

where A is an M×N sparse matrix (the majority of the elements
are zero), y is an M×1 vector, and x is an N×1 vector. More
generally, SpMxV can be represented as:

y   Ax   ,

(2)

where α and β are scalars.

2.2 Existing SpMxV Architectures

The performance of sparse-matrix algorithms tends to be much
lower than that of dense matrices due to two key factors: (1) the
way the sparse matrix is represented in memory and (2) the
computation architecture of the target platform.

Specialized software libraries for solving dense and sparse linear
algebra problems are very popular for high-performance
computing. These libraries, such as MKL [5] for CPUs, and
cuBLAS [7] and cuSPARSE [8] for GPUs, provide a standardized
programming interface, with subroutines optimized for the target
platform.

2.1 Sparse Matrix Representation
There are a variety of ways to represent the sparse matrix for
storage purposes. However, the few computationally efficient
formats are restricted to highly structured matrices, such as
diagonal or banded matrices. In this paper, we focus on boosting
the efficiency of SpMxV for generic sparse matrices. Therefore,
we only present general sparse storage schemes in this section.

For SpMxV on CPUs and GPUs, the ith element of y is typically
calculated as the dot-product of the ith row of A and the vector x
(Fig. 2). This is because each computing core usually contains only
a handful of general purpose registers and a single floating-point
unit (FPU). Therefore, CSR is one of the most computationally
efficient storage options for sparse matrices on CPUs and GPUs. It
has the added benefit of being easily parallelizable: each
computing core can be independently assigned a different value of
y to calculate.

Figure 1 illustrates a sample sparse matrix and three different
schemes to represent it. The simplest storage scheme, shown in
Fig. 1(b), is the coordinate (COO) format. The row indices,
column indices, and values of the nonzero matrix entries are
explicitly stored in 3 separate arrays: row, col, and data. The
compressed sparse row (CSR) format (Fig. 1(c)) is the most
commonly used sparse storage scheme, which also stores the
column indices and nonzero values into the arrays: col and data.

Improving the parallel performance of SpMxV via blocking
(splitting up the matrix into several sub-matrices) and
modifications to the CRS format is a very active area of study

162

Figure 3. A graphical representation of how SpMxV is
performed using the CSC format. The entire vector y is
calculated as a series of vector additions of the columns of A
weighted by the appropriate element from x.
[3,6,9]. Unfortunately, the use of CSR, and its variants, for SpMxV
have several drawbacks on CPUs and GPUs that hurts its overall
computational efficiency [3]:

Figure 4. Schematic of a single PE using a simple dual-port
RAM, floating-point adder, and a floating-point multiplier.

(1) The SpMxV kernel is memory-bounded. CPUs and GPUs
typically have much larger computational throughput than
available memory bandwidth. This leads to a very low
utilization rate for the computing resources, and
subsequently, poor energy efficiency.

adders and their resource usage in the design. As such, they only
average less than 50% of their theoretical peak processing
throughput and memory bandwidth [1-4,10-14].

3. PROPOSED ARCHITECTURE
Our architecture abandons the idea of calculating each element of
y separately as the row-wise dot product between A and x. Instead,
the entirety of y is calculated as the column-wise vector additions
of A weighted by each element of x, as shown in Fig. 3.
Fundamentally, this allows us to directly address the major
limitations present in the SpMxV algorithm when implemented on
an FPGA:

(2) The indirect (global) memory references for the vector x
present in col adds uncertainty to the memory access pattern,
ultimately delaying the computation. Each element of col
must first be loaded from memory and added to the address
of x as an offset. Only then can the correct value of x be
loaded into the FPU for computation.
(3) Irregular memory access of vector x causes a large number of
cache misses. In CPUs, this cache miss can add tens of
cycles of latency. In GPUs, a cache miss can add hundreds of
cycles of latency. GPUs typically try to hide these large
latencies by interleaving dozens of threads on a single
computational core. This works well for computationbounded algorithms, but not memory-bounded algorithms
like SpMxV.

(1) A dedicated co-processor allows for much better balancing
of system resources. The number of processing elements
(PEs) can be efficiently scaled to match the available
memory bandwidth.
(2) What used to be indirect (global) memory references for x in
col vector (for the CSR format) are now direct (local)
memory references for y in the row vector. In other words,
when a column of A is multiplied by an element of x, in the
manner shown in Fig. 3, we know exactly which elements of
y the partial product contributes to. This allows us to halve
the number of require memory accesses, the largest
bottleneck in the SpMxV algorithm.

(4) Short row lengths (i.e. very few nonzero elements per row)
can cause serious performance degradation. When rows are
short, the overhead associated with calculating each element
of y becomes significant.
Due to these drawbacks, CPUs and GPUs reach less than 5% of
their theoretical peak processing throughput and utilize less than
50% of their available memory bandwidth for SpMxV [6,9].

(3) Memory access to the x vector is no longer irregular, but
sequential. By using the CSC format to store A, both A and x
can be placed in a large off-chip memory and sequentially
streamed into the DSP co-processor (eliminating the time
and energy overheads of a cache miss).

Previous FPGA implementations have attempted to alleviate these
inefficiencies by introducing several architectural changes. In
some designs, several processing elements (PEs) work together to
compute a single element of y in parallel [1,4,12]. These designs
employ various reduction circuits in order to combine the
intermediary results. Other designs have each PE calculate several
elements of y in a sequential manner in order to mitigate the effect
of short rows [2,10,12]. In both cases, the entirety of the x vector,
or a large subsection (in the case of blocking), is buffered in onchip Block RAM (BRAM) to reduce the effects of irregular
memory accesses [1-4,10-13]. However, these prior
implementations primarily focus on reducing the total number of

(4) Short row or column lengths have much less impact on the
performance of SpMxV, since the PEs are rarely idled thanks
to the balanced memory bandwidth and computing
capability. However, performance is degraded as the
memory bandwidth of x approaches that of A for extremely
sparse matrices. In the rare case of M ‫ ب‬N, the performance
of CSC is no better than that of CSR.

163

Table 1. Resource usage for the SpMxV kernel (64 PEs).

FPGA

Processing Element

Resource

Processing Element

SpMxV
Controller

DRAM

Percent

Registers

31,621

58,880

53.70%

Processing Element

LUTs

27,958

58,880

47.48%

BRAMs

160

244

65.57%

DSP48Es

320

640

50.00%

Processing Element

First, the reduction circuit adds a large amount of overhead in
terms of latency and additional hardware (even if existing adders
are used, more resources are needed for configurability). Second,
by splitting up computation along the columns, we lose some of
the sequential nature of the memory accesses we had gained with
the CSC format. The memory accesses for each PE are still
sequential, but globally the memory accesses for all PEs are
irregular. To mitigate this, a more complicated memory controller
is required to ensure a balanced load across all of the PEs. Third,
if there are fewer columns than PEs, this approach is effectively
no different than prior FPGA implementations.

Memory Controller

QDR0

Available

Processing Element

Processing Element

Scheduling &
Hazard Detecti on

Used

QDR1

Figure 5. Top-level schematic of the SpMxV kernel, with 8
processing elements, running on the ROACH FPGA platform.
The SpMxV kernel acts as a coprocessor for a networked
computer running a MATLAB environment.

The second option for computing the final vector is to assign a
subset of rows of A to each PE (i.e. blocking along the rows of A).
Each PE computes a subset of the final vector, which are then
concatenated together at the end (requiring no additional latency).
Additionally, this preserves the property of sequential memory
accesses across all PEs, allowing for a much simpler memory
controller (at the cost of a slightly more complicated SpMxV
controller to handle additional scheduling and hazard detection).
Finally, with minor modifications to the SpMxV and memory
controllers, our SpMxV kernel can also support a sparse matrix
dense matrix multiplication (SpMxM): each column of the dense
matrix is assigned to a PE, which each PE computing a single
column of the resulting matrix. Our SpMxV kernel uses this
option, with the modifications to support SpMxM, in its
implementation.

Computing the SpMxV column-wise also allows for an extremely
simple PE (Fig. 4). Each PE contains a single-precision floatingpoint adder and multiplier, as well as a simple dual-port RAM.
Each simple dual-port RAM utilizes the large amounts of BRAM
resources available on FPGAs and can accommodate several
hundred to several thousand elements of y. Figure 5 shows the
overall experiment setup, in which the SpMxV kernel
(implemented on an FPGA) serves as a co-processor attached to
an external computer. A dedicated memory controller allows the
elements of A and x to be continuously streamed into the FPGA,
while the SpMxV controller’s primary function is scheduling and
hazard detection. Hazard detection avoids the conflict between
two partial products that contribute to the same element of y
overlapping due to the latency of the floating-point adder. If such
a hazard is detected, we must either stall or provide alternative
data to ensure that the result of y is calculated correctly.

3.2 SpMxV and Memory Controller
The primary purposes of the SpMxV controller are scheduling
and hazard detection. The memory controller acts as a slave to the
SpMxV controller, ensuring a continuous stream of data into the
PEs. Hazards arise when two or more partial products want to
write to the same memory address of the dual-port RAM in a
short period of time. Due to the latency of the floating-point adder
in Fig. 4, the existing sum of the partial products in the dual-port
RAM must be prefetched. If two partial products that contribute
to the same term are allowed to proceeded, the second product
will prefetch a sum that does not include the first product. The
result is that the final sum of products will not include the first
conflicting partial product. The SpMxV controller detects these
hazards and corrects them in one of two ways. It first attempts to
shuffle the partial products to increase their distance in time. If
this is not possible, or would result in additional hazards, the
SpMxV controller issues a stall command to the PE (by
deasserting the Valid signal, and holding the values of Aij, Xj, and
i).

3.1 Processing Element
As stated previously, each PE (Fig. 4) contains a single-precision
floating-point adder and multiplier, as well as a simple dual-port
RAM. To perform the SpMxV, each PE multiplies an element of
the data vector (Aij) and the corresponding element of the x vector
(Xj) together. The resulting partial product is then added to
address in the row vector (i), before being stored back into the
BRAM of the dual-port RAM. Due to the latency of the
multiplication and addition operations, a Valid signal is used to
prevent data corruption due to hazards. Using this strategy, data
can be continuously streamed into each PE (directly from the
CSC format) with a small startup overhead latency equal to that of
the adder and the multiplier.
Since each PE has its own working copy of the vector being
computed, there are two possible strategies for assembling the
final vector. The first option is to assign a subset of the x vector to
each PE (i.e. blocking along the columns of A). Each PE
computes a partial sum of the final vector and an adder tree
(which can be built from the existing adders in each PE) is used to
combine them at the end. Similar to prior FPGA implementations
[1-4,10-13], this straightforward approach has several drawbacks.

Figure 6 shows the stalling behavior for a single processing
element performing SpMxV between the example A matrix from
Fig. 1(a) and the vector x = [0.1 0.2 0.3 0.4 0.5]T. In the example,
the latency of the floating-point multiplier and adder are both 2

164

1

2

0

1

3

4

5

6

7

8

9

10

11

12

5

6

13

14

15

16

17

18

19

20

21

CLK
Count
Ptr
Aij

1

5

0

4

2

4

4

0.1

Valid

Y

3

0

Xj
i

2

2

2

9

0.3

0

1

Hazard

Hazard

1

6

3

0.2

0

1

7

8

9

7

7

9

8

0.4

6

X

0.5

3

X

2

3

X

Hazard

0

1

0

1

0

0
0

0.1
0

0.1
0

0.9
0

0.9
0.4

0.9
1.3

0.9
1.3

0.9
1.3

0.9
1.3

0.9
1.3

0
0

0
0

0.5
0

0.5
0

0.5
0

0.5
0

0.5
2.7

3.3
2.7

7.3
2.7

7.3
5.7

5

6

9

10

14

15

16

20

21

1

2

3

4

7

8

11

12

13

17

18

19

Figure 6. Timing diagram for calculating the SpMxV of the example A matrix from Fig. 1(a) and x = [0.1 0.2 0.3 0.4 0.5]T using a
single PE. For this example, the floating-point adder and multiplier both have a latency of 2 clock cycles and the simple dual-port
memory (Y) has a latency of 1 clock cycle.
clock cycles. Additionally, the simple dual-port memory (Y) has a
latency of 1 clock cycle. In cycle 2, a hazard is detected due to the
proximity of the partial product of 1×0.1 and the partial product
of 4×0.2. We must stall for 2 cycles—by deasserting Valid and
holding the values of Aij, Xj, and i—to ensure that partial product
of 1×0.1 is added to Y before continuing. If co-processor had not
stalled, the partial product of 4×0.2 would have added itself to the
current value of Y, 0, producing an incorrect final value of 0.8
(instead of 0.9). Computation resumes in cycle 5, after the hazard
has passed. Additional hazards are detected in cycles 6 and 12,
with each hazard resulting in 3 clock cycles of stalling.

Table 1 show the total resource usage of the SpMxV kernel
implemented with 64 PEs using a single-precision floating-point
format. The SX95T FPGA can support up to 96 single-precision
PE (using 98.4% of the available BRAM), but is ultimately
limited by our available memory bandwidth. The CSC A matrix is
stored in the SDRAM, while the x and y vectors are stored in the
two QDRs. The QDRs are accessed in parallel to effectively
create a single memory with twice the bit width. The ROACH
board can operate up to 150MHz (limited by the QDR memory
controller), resulting in a peak performance of 19.2 GFLOP/s with
a thermal design power (TDP) of 25W.

4. PERFORMANCE EVALUATION

4.1 Comparison to CPUs and GPUs

The SpMxV kernel was evaluated on the open-source academic
research platform “ROACH” (Reconfigurable Open Architecture
Computing Hardware) [15]. The ROACH platform is equipped
with a Virtex-5 SX95T FPGA for DSP applications, a PowerPC
running Linux, two 36Mb QDRII+ SRAMs, and 2GB of DDR2
SDRAM for a combined peak memory bandwidth of 35.74 GB/s.
The PowerPC allows a computer running MATLAB to interface
with “software registers,” BRAMs, and FIFOs on the FPGA, as
well as to load data in and out of the board-level QDRs and
DRAM (Fig. 5).

We use a collection of 10 unstructured matrices used by both
Williams et al. [6] and Bell et al. [9] in our performance
benchmarking study. Table 2 details the size and overall sparsity
structure of each matrix. All of the matrices are publically
available online from the University of Florida Sparse Matrix
Collection [16]. For comparison, the same benchmarks are run on
both a 64-bit Linux machine and a 64-bit Windows machine.
The Linux machine has 16GB of memory and an Intel Core i72600 processor (4 physical cores with hyper-threading, for a total
of 8 virtual cores), using the MKL sparse-BLAS library [5]. The
Core i7-2600 processor has a peak memory bandwidth of 21GB/s

165

Table 2. Summary of unstructured matrices used for benchmarking performance (publically available from [16]).

Matrix

Rows

Dense
Protein
FEM/Spheres
FEM/Cantilever
Wind Tunnel
FEM/Harbor
QCD
FEM/Ship
Economics
FEM/Accelerator

2,000
36,417
83,334
62,451
217,918
46,835
49,152
140,874
206,500
121,192

Columns

Nonzeros

2,000
36,417
83,334
62,451
217,918
46,835
49,152
140,874
206,500
121,192

4,000,000
4,344,765
6,010,480
4,007,383
11,524,432
2,374,001
1,916,928
3,568,176
1,273,389
2,624,331

Nonzeros/Column
2000.00
119.31
72.13
64.17
52.88
50.69
39.00
25.33
6.17
21.65

Density
100.00000%
0.32761%
0.08655%
0.10275%
0.02427%
0.10823%
0.07935%
0.01798%
0.00299%
0.01787%

FEM/Accelerator. These matrices had a significantly higher rate
of cache misses due to their large size and overall sparsity. The
relativity small number of nonzero elements per row (especially
for the Economics matrix) also added significant overhead by
having to flush the pipeline more often.

with a peak performance of 108.8GFLOP/s and a TDP of 95W
[17]. The benchmarks were also run on an NVIDIA GeForce
GTX 660 graphics card (960 CUDA cores), installed on the same
Linux machine, using the cuSPARSE library [8]. The GPU has a
peak memory bandwidth of 144.2GB/s and a peak performance of
1881.6GFLOP/s with a TDP of 140W [18].

Similarly, the GTX 660 and GTX Titan GPUs achieved an
average performance of 5.79 and 14.86 GFLOP/s, respectively
across all 10 matrices. The resulting computational efficiencies
were 0.31% and 0.33%. Overall, the computational efficiency of
both GPUs was between 0.2-0.5% for all of the test matrices. The
GTX Titan had an average speed up of 2.57x over the GTX 660
GPU, which is consistent with the GTX Titan having 2x the
memory bandwidth and 2.39x the number of processing cores as
the GTX 660. Because individual process threads are organized
differently on the GPU, each CUDA core had to be flushed far
less often than the CPU for the FEM/Ships, Economics, and
FEM/Accelerator matrices, leading to more even performance.

The Windows machine has 32GB of memory and an Intel Core
i7-4770 processor (4 physical cores with hyper-threading, for a
total of 8 virtual cores), using the MKL sparse-BLAS library [5].
The Core i7-4770 processor has a peak memory bandwidth of
25.6GB/s and a peak performance of 217.6GFLOP/s with a TDP
of 84W [19]. The benchmarks were also run on an NVIDIA
GeForce GTX Titan graphics card (2688 CUDA cores), installed
on the same Windows machine, using the cuSPARSE library [8].
The GPU has a peak memory bandwidth of 288.4GB/s and a peak
performance of 4,500GFLOP/s with a TDP of 250W [20].
Figure 7 compares the raw computational performance (in
GFLOP/s) of the CPU, GPU, and FPGA SpMxV kernels for all of
the matrices tested. SpMxV on the two CPUs showed a
performance drop of 20-50% compared to dense matrices, while
the two GPUs showed a performance drop of 30-60%. Figure 8
compares the computational efficiency of the CPU, GPU, and
FPGA SpMxV kernels for all of the matrices tested. For a
memory bound algorithm like SpMxV, the computational
efficiency is strongly determined by the memory hierarchy (i.e.
the cache structure and size). The computational efficiency is
calculated as the ratio of the measured SpMxV performance, in
GFLOP/s, over the theoretical peak GFLOP/s achievable for each
platform.

On the Linux test setup, the GTX 660 had an average speedup of
2.93x over the i7-2600 processor, and on the Windows setup, the
GTX Titan had a 3.23x speedup over the i7-4770 processor.
Despite the roughly 3x performance increase by the GPUs (Fig.
6), the CPUs are about 6x more computationally efficient than the
GPUs (Fig. 7). The GPUs use 100-300x more processing cores to
achieve a total, theoretical peak performance roughly 20x greater
than that of the CPU, but only have about 8x more memory
bandwidth. The cache structure of a GPU is smaller and has
higher latency that of a CPU [18,19]. GPUs are designed to mask
random memory accesses for computationally intensive
algorithms, leading to much larger penalties in efficiency for
cache misses when compared to a CPU.

The Core i7-2600 and Core i7-4770 processors achieved an
average performance of 2.01 and 4.59GFLOP/s, respectively,
across all 10 test matrices. The resulting computational
efficiencies were 1.84% and 2.11%. Overall, the computational
efficiency of both CPUs was 1-2% for all of the test matrices. The
Core i7-4770 processor was able to achieve a 2.28x speedup over
the Core i7-2600, despite only having 22% more memory
bandwidth, due to its more efficient memory accesses with its
larger vector processing cores.

The SpMxV kernel running on the Virtex-5 SX95T FPGA
achieved an average performance of 17.64GFLOP/s, for a
computational efficiency of 91.85%, across all 10 matrices. The
Dense matrix achieved a peak performance of 19.16GFLOP/s for
a computational efficiency of 99.8%. This performance represents
an average speedup of 9.55x and 4.18x over the i7-2600 and i74770 CPUs and a 3.31x and 1.28x speedup over the GTX 660 and
GTX Titan GPUs. Moreover, the computational efficiency of the
FPGA SpMxV kernel had an average improvement of 54x and
322x over the CPUs and GPUs, respectively.

The largest drops in performance for the CPUs were recorded
using the sparsest matrices: FEM/Ships, Economics, and

166

25

20
FPGA (SX95T)

GFLOP/s

GPU (GTX TITAN)
15

10

GPU (GTX 660)

5
CPU (i7-4770)
CPU (i7-2600)
0

Figure 8. Computational efficiency of the CPU, GPU, and FPGA SpMxV kernels.

167

or
el
er
at

FE
M
/A
cc

on
om
ic
s
Ec

D

/S
hi
p
FE
M

M
/H
FE

Q
C

ar
bo
r

l
nn
e
Tu

W
in
d

an
til
ev
er

es

M
/C

he
r
FE

M
/S
p

FE

Pr
ot
ei
n

D
en

se

Computational Efficiency [%]

Figure 7. Raw computational performance of the CPU, GPU, and FPGA SpMxV kernels.

Table 3. Comparison of FPGA SpMxV Architectures

FPGA
Frequency [MHz]
Memory Bandwidth [GB/s]
Number of PE
Peak Performance [GFLOP/s]
Matrix Format
Sparse Test Matrix Density
MIN-MAX [%]
Average [%]
Computational Efficiency
MIN-MAX [%]
Average [%]

[1]

[2]

[11]

[12]

[13]

This Work

Virtex-5
LX155T
100
6.5
16
3.2
CVBV†

Stratix-III
EP3SE260
100
8.5
6
1.2
COO

Virtex-II
Pro 70
200
8
4
1.6
CSR

Virtex-II
Pro 100
170
8.5
5
1.7
CSR

Virtex-II
6000
95
1.6
3
0.57
SPAR*

Virtex-5
SX95T
150
35.74
64
19.2
CSC

0.01-5.48
1.41

0.51-11.49
3.34

0.04-4.17
0.87

0.04-0.39
0.16

0.01-1.10
--

0.003-0.33
0.09

1-7
4.48

5-7
5.63

20-79
42.6

50-98.4
79.4

1-74
55.6

69-99.8
91.9

†Variant of CSR.

*Variant of CSC.

the sparsity structure of the matrix. The computational efficiency
ranges from 20% to 79%, performing particularly poorly for
extremely sparse matrices (<0.1% density). Zhang et al. [12]
improves upon this design by having each PE calculate a different
element of y. Their accumulator requires a minimum row length
of 8, zero padding when necessary, but can switch between any of
several different rows if it encounters a data hazard. This roughly
doubles the computational efficiency of the design as compared to
Zhuo et al. [11]. However, it still performs quite poorly for
extremely sparse matrices.

The average power consumption of the i7-2600 and i7-4770
processors was measured to be 77.2W and 66.3W, respectively.
The resulting power efficiencies are 26MFLOP/s/W and
69MFLOP/s/W. Similarly, the average power of the GTX 660 and
GTX Titan were measured to be 99W and 163W, respectively.
The resulting power efficiencies are 58MFLOP/s/W and
91MFLOP/s/W. The worst case power of the SX95T FPGA was
measured to be 5.1W, resulting in a power efficiency of 3,460
MFLOP/s/W. This represents more than a 50x and 38x
improvement in energy efficiency over the CPU and GPU
implementations, respectively.

The final approach is to use the method outlined is Section 3 to
stream CSC matrix data through several PEs. Gregg et al. [13] use
a variant of CSC called the sparse matrix architecture and
representation (SPAR) format. In the SPAR format, row and ptr
are combined into a single vector with zero padding introduced at
the start of each column of the data vector. Each PE only buffers
a small portion of the y vector (32 elements vs. 3,456 elements in
our design) in a local cache. A y-cache miss requires a write-back
to high latency DRAM, incurring a 109 clock cycle penalty. With
such a small cache, the design is very highly sensitive to the
sparsity structure of the matrix. Computational efficiency ranges
from as low as 1% to as high as 74%.

4.2 Comparison to Existing FPGA Art
In Table 3, we compare our proposed architecture to several
published SpMxV FPGA architectures. The architectures can be
categorized into 3 distinct groups. The first is to either re-encode,
reorder, or preprocess the sparse matrix in such a way as to reduce
data hazards [1,2]. Kestur et al. [1] re-encode the matrix into the
Compressed Variable-Length Bit Vector (CVBV) format (a
variation of the CSR format) on the fly to reduce the required
memory bandwidth. However, re-encoding the matrix insures a
significant overhead penalty, resulting in marginal efficiency
improvements over CPU and GPU implementations (only 3-10x).
Sun et al. [2] preprocess the matrix to reorganize and optimize the
datapath to eliminate hazards. After preprocessing, the matrices
achieve a computational efficiency of 96-99% on the FPGA.
Unfortunately, the preprocessing overhead (datapath optimization,
FPGA reconfiguration, and buffering the matrix on BRAM) is
about 20 times greater than that of the SpMxV calculation. This
results in an effective computational efficiency of only 5-7%.

Our architecture improves upon Gregg et al. [13] by buffering
much larger sections of the y vector (removing the need for a
costly write-back scheme) and the elimination of unnecessary
stalling due to zero padding (due to the SPAR format). Our design
is able to handle matrix densities below 0.01% (10x sparser than
prior FPGA designs) with computational efficiencies as high as
99.8%. Our drop in performance (and efficiency) for the
Economics matrix on the ROACH platform is due to the very
small number of nonzeroes per column, causing the x vector to
need more memory bandwidth than the QDRs can deliver. This
results in about 25% of the PEs being idled during any given
cycle. The FEM/Accelerator matrix, while similar in density to
the FEM/Ship matrix, alternates between being extremely dense
and extremely sparse along its columns, causing about 30% of the
PEs to be idled during any given clock cycle. This particular issue
can be alleviated by buffering the x vector on-chip like previous

The second approach is to use several PE (each with its own
working copy of x) in parallel with a reduction circuit or adder
tree to accumulate a single element of y [11,12]. In Zhuo et al.
[11], the partial product of 4 multipliers are added together via a
tree of 3 adders. The resulting sum is then fed into a novel
reduction circuit (accumulator) that handles the potential readafter-write data hazards. The drawback of this approach is that it
requires zero padding to achieve a minimum row size. Combined
with blocking along the columns of A, the design is sensitive to

168

FPGA designs. We estimate that this would increase the sustained
(average) computational efficiency from 91.85% to 98.29%.

[7] “Nvidia cuBLAS.” [Online]. Available:
http://developer.nvidia.com/cublas

5. CONCLUSIONS

[8] “Nvidia cuSPARSE.” [Online]. Available:
http://developer.nvidia.com/cusparse

This paper describes a SpMxV kernel using a CSC sparse-matrix
format, and demonstrates its computational efficiency using an
FPGA. The efficiency advantage of the kernel results from
transforming irregular random memory accesses into a
regularized stream of serial memory accesses. The benchmarking
results show that the proposed architecture achieves a peak
computational efficiency of 99.8% when performing SpMxV,
which is over 54 and 322 times more efficient than an Intel Core
i7-4770 processor and over an NVIDA GTX Titan GPU
performing the same tasks, respectively. Implemented on a
Virtex-5 SX95T FPGA, our design is able to achieve higher
performance than its CPU and GPU counterparts running
optimized sparse-BLAS software libraries, while only using 64
single-precision processing elements, with a 38-50x improvement
in energy efficiency.

[9] N. Bell and M. Garland, “Implementing sparse matrix-vector
multiplication on throughput-oriented processors,” in Proc.
ACM/IEEE Conf. Supercomputing (SC 2009), pp. 18:1–
18:11, Nov. 2009.
[10] G. Kuzmanov and M. Taouil, “Reconfigurable sparse/dense
matrix-vector multiplier,” Int. Conf. Field-Programmable
Tech. (FPT 2009), pp. 483–488, Dec. 2009.
[11] L. Zhuo and V.K. Prasanna, “Sparse Matrix-Vector
multiplication on FPGAs,” in Proc. ACM/SIGDA Int. Symp.
Field-Programmable Gate Arrays (FPGA ‘05), pp. 63-74,
Feb. 2005.
[12] Yan Zhang, Y.H. Shalabi, R. Jain, K.K. Nagar, and J.D.
Bakos, “FPGA vs. GPU for sparse matrix vector multiply,”
Int. Conf. Field-Programmable Tech. (FPT 2009), pp. 255–
262, Dec. 2009.

6. ACKNOWLEDGMENTS

7. REFERENCES

[13] D. Gregg, C. McSweeney, C. McElroy, F. Connor, S.
McGettrick, D. Moloney, and D. Geraghty, “FPGA Based
Sparse Matrix Vector Multiplication using Commodity
DRAM Memory,” Int. Conf. Field Programmable Logic
Applicat. (FPL 2007), pp. 786-791, Aug. 2007.

[1] S. Kestur, J.D. Davis, and E.S. Chung, “Towards a Universal
FPGA Matrix-Vector Multiplication Architecture,” Int.
Symp. Field-Programmable Custom Comp. Mach. (FCCM
2012), pp. 9–16, May 2012.

[14] C.Y. Lin, H. K.-H. So, and P.H.-W. Leong, “A Model for
Matrix Multiplication Performance on FPGAs,” Int. Conf.
Field Programmable Logic Applicat. (FPL 2011), pp.305–
310, Sept. 2011.

[2] S. Sun, M. Monga, P.H. Jones, and J. Zambreno, “An I/O
Bandwidth-Sensitive Sparse Matrix-Vector Multiplication
Engine on FPGAs,” IEEE Trans. Circuits Syst. I, Reg.
Papers, vol. 59, no. 1, pp. 113–123, Jan. 2012.

[15] “ROACH.” [Online]. Available:
https://casper.berkeley.edu/wiki/ROACH

The authors would like to thank Yuta Toriyama and Fang-Li
Yuan of UCLA for their helpful discussions.

[16] T. A. Davis and Y. Hu., “The university of Florida sparse
matrix collection.,” ACM Trans. Math. Softw., vol. 38, no. 1,
pp. 1:1–1:25, Dec. 2011.

[3] G. Goumas, K. Kourtis, N. Anastopoulos, V. Karakasis, and
N. Koziris, “Understanding the Performance of Sparse
Matrix-Vector Multiplication,” Euromicro Conf. Parallel,
Distributed and Network-Based Process. (PDP 2008), pp.
283–292, Feb. 2008.

[17] P. Gepner, D. L. Fraser, and V. Gamayunov, “Evaluation of
the 3rd generation Intel Core Processor focusing on HPC
applications,” Int. Conf. Parallel Distrib. Process. Techn.
Applicat. (PDPTA 2012), pp. 818–823, July 2009.

[4] J. Sun, G. Peterson, and O. Storaasli, “Mapping Sparse
Matrix-Vector Multiplication on FPGAs,” Reconfigurable
Systems Summer Institute (RSSI 2007), July 2007.
[5]

[18] “NVIDIA GeForce GTX 680: The fastest, most efficient
GPU ever built.” [Online]. Available:
http://www.geforce.com/Active/en_US/en_US/pdf/GeForceGTX-680-Whitepaper-FINAL.pdf

“Intel Math Kernel library.” [Online]. Available:
http://software.intel.com/en-us/intel-mkl

[19] “Intel® Core™ i7-4770 Processor.” [Online]. Available:
http://ark.intel.com/products/75122/Intel-Core-i7-4770Processor-8M-Cache-up-to-3_90-GHz

[6] S. Williams, L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J.
Demme, “Optimization of sparse matrix-vector
multiplication on emerging multicore platforms,” in Proc.
ACM/IEEE Conf. Supercomputing (SC 2007), pp.1–12, Nov.
2007.

[20] “Introducing the GeForce GTX TITAN.” [Online].
Available: http://www.geforce.com/whatsnew/articles/introducing-the-geforce-gtx-titan

169

A GPU-Outperforming FPGA Accelerator Architecture for
Binary Convolutional Neural Networks
YIXING LI, Arizona State University
ZICHUAN LIU, Nanyang Technological University
KAI XU, Arizona State University
HAO YU, Nanyang Technological University
FENGBO REN, Arizona State University
FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great
attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based
solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA
acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is
trained with binary constraints on weights and activations. Specifically, we propose an optimized FPGA
accelerator architecture tailored for bitwise convolution and normalization that features massive spatial
parallelism with deep pipelines stages. A key advantage of the FPGA accelerator is that its performance is
insensitive to data batch size, while the performance of GPU acceleration varies largely depending on the
batch size of the data. Experiment results show that the proposed accelerator architecture for binary CNNs
running on a Virtex-7 FPGA is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing
online individual requests in small batch sizes. For processing static data in large batch sizes, the proposed
solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy
efficiency.
CCS Concepts: • Computer Systems Organization → Hardware accelerators • Computer Systems
Organization → Neural networks;
Additional Key Words and Phrases: FPGA, hardware acceleration, deep learning, convolutional neural
network, binary neural network, high-throughput, energy efficiency

1 INTRODUCTION
Convolutional neural network (CNN) has become a popular machine learning engine for many
image-related data analytics [15-16] [20] [27], such as image classification, face detection, object
tracking, etc. CNNs outperform traditional feature selection based approaches especially for
learning from big data. For a conventional CNN, high computation complexity and large
memory footprint are the two main throughput bottlenecks for hardware acceleration.
Therefore, the unmet throughput need of CNNs calls for the development of more efficient
hardware acceleration solutions for driving real-time applications.
Several methods have been proposed to alleviate the computation complexity and memory
footprint by reducing the redundancy of CNN models. These methods include pruning [18] [26],
reduced-precision CNNs [4], and binary CNNs (BCNNs) [9]. The pruning technique [18] prunes
the “useless” weights of a trained network based on sensitivity analysis, which can effectively
reduce the CNN weight count (usually referred to as network size) for a ten-class classification
problem by 75% [18]. Ref. 4 demonstrates that reducing the numerical precision of a CNN from
32 to 16 bits has very limited impact on classification accuracy. This can result in a network size
reduction of 50%. However, a numerical precision below 8 bits resulted from quantization in the
post-training stage often suffers from unacceptable accuracy drop [4]. Alternatively, recent
advancement in binary-constrained deep learning has opened up new opportunities for efficient
hardware acceleration. BinaryConnect [5] and the work in Ref. 6 demonstrate the successful use

2
of binary and ternary (-1, 0, +1) weights in a CNN, respectively. But, they both have non-binary
activations. As one step forward, EBP [7], Bitwise DNNs [8], and the BCNN in Ref. 9 successfully
exploit both binary weights and activations. In particular, the BCNN in Ref. 9 shows a 0.96%
classification error rate on the MNIST database [17], which is comparable to a full-precision
state-of-the-art CNN. Overall, BCNNs have been shown with up to 96.8% reduced network sizes
with minimum accuracy loss when comparing to their full-precision counterparts. Therefore, it
is believed that BCNN is a more hardware-friendly model with superior accuracy-complexity
trade-off.
Thus far, GPU-based CNN accelerator is still dominant due to its improved throughput over
CPUs. However, the high power consumption of GPUs has brought up cooling concerns in data
center computing. On the other hand, FPGA-based CNN accelerator has been widely
investigated due to its energy efficiency benefits. As the system throughput is proportional to
the computing parallelism and operating frequency, the theoretical throughput of GPU-based
and FPGA-based CNN accelerators can be estimated on the 1st order based on device
specifications. A Titan X GPU has 3,072 CUDA cores, while a Virtex-7 FPGA has 3,600 DSP48
slices. For implementing a full-precision CNN, the computing parallelism of GPUs and FPGAs
can be approximately the same. But, GPUs offer 5-10x higher frequency. As a result, FPGAs can
hardly match up the throughput of GPUs for accelerating full-precision CNNs. Differently, for a
BCNN, the operations in the convolution layers become bitwise XNORs and bit-count logic. A
direct impact is that one can use LUTs instead of DSP48 slices to implement the bitwise
operations on an FPGA. Hundreds of thousands of LUTs make it possible for a high-end FPGA to
match up or surpass the throughput of a GPU, even considering the bitwise operation capability
of CUDA cores. Moreover, FPGAs benefit from much higher energy efficiency, which makes it a
superior solution for accelerating BCNN in a data center setting. Early research effort [9] shows
that GPU can get 7x speedup using a binary kernel for MNIST classification task on a binary
multilayer perceptron (MLP). However, there have been very few studies on exploring FPGAbased accelerator architecture for binary neural networks.
In this paper, we propose an optimized FPGA accelerator architecture tailored for BCNN. The
proposed architecture was adopted to implement a 9-layer BCNN on a Xilinx Virtex-7
XC7VX690 FPGA, which achieves nearly state-of-the-art classification accuracy on CIFAR-10.
The experiment results show that the FPGA implementation outperforms its optimized GPU
counterpart with 75x higher energy efficiency and 8.3x higher throughput for processing a small
batch size of 16 images (e.g. from individual online request). For processing a large batch size of
512 images (e.g. from static data), the FPGA implementation achieves comparable throughput
with 9.5x higher energy efficiency compared with the GPU counterpart.
The contributions of this paper are summarized as follows:
• We propose a throughput optimization model for the end-to-end mapping of general
BCNNs.
• We demonstrate a 7.663-TOPS 8.2-W FPGA accelerator for a BCNN that highly
outperforms the GPU counterpart especially for processing individual online requests in
small batch size for the 1st time.
• We reveal the impact of applying binary constraints in CNN training on FPGA
acceleration is the enablement of massive computing parallelism of bitwise operations based
on abundant LUT resources.
• We optimize the accelerator architecture to fully exploit both spatial and temporal
parallelism across all the layers using architectural unfolding, pipelining, and data-flow
control with memory channels. Compared with GPU implementations that only have spatial
parallelism, the proposed architecture offers superior throughput and energy efficiency
performance regardless of the size of workload.

3

dog
cat
lion
bird

Convolution

Pooling Convolution Pooling Fully-connected
Fig. 1. Convolutional neural network.

2 BACKGROUND AND MOTIVATION
2.1 CNN
A CNN is a trained neural network model with high-level features extracted from input images
[13]. A typical CNN model contains convolutional, pooling, and fully-connected layers as shown
in Fig. 1. The first few layers usually capture regional information such as edges and curves, and
the last few layers interpret these low-level features into high-level abstractions with the
posterior probability assigned for classification.
2.1.1 Convolution. The convolution layer is the core layer of a CNN. Taking an RGB image
as an example, the input of each convolutional layer is a 3D feature map with the size of
𝑾𝑰𝑫′ × 𝑯𝑬𝑰′ × 𝑫𝑬𝑷′ as shown in Fig. 2. Each filter has a size of 𝑭𝑾 × 𝑭𝑯 × 𝑭𝑫, where 𝑭𝑾
and 𝑭𝑯 is the width and height of the reception field, respectively, and 𝑭𝑫 is equal to the depth
𝑫𝑬𝑷′ of the input feature maps. N filters are constructed as a 4D tensor. The output feature
maps 𝒀 in the size of 𝑾𝑰𝑫 × 𝑯𝑬𝑰 × 𝑫𝑬𝑷 are obtained from the spatial convolution along the
1st and the 2nd dimensions of the input feature maps with the 3D-filter 𝑾[𝒏]. The operation in

DEP’

FH
FD

-0.2 0.8 1.3
1.4 -0.6 1.7
1.1 -0.9 -2.8

FW

Weights

1

-1 1 -1 1 1
-1 1 -1 1 1
1 -1 1 -1 1 1
1 -1 1 -1 1 1
1 -1 1 -1 1 1
1 -1 1 -1 1 1
1

1 -1 1
-1 -1 1
1 1 1

ReLU

WID
HEI
0.6

1

DEP

Output feature maps
CNN

BCNN

Fig. 2. A single layer in CNN and BCNN.

Binarization

Mult & add

WID’
0.7
1 3.1
0 -2.5
1 -0.6
0 -1.9
1 0.2
1
0.5
1 -2.3
0 2.9
1 1.7
0 2.1
1 0.5
1
1 1.8
0 -1.8
1 -0.5
0 0.8
1 -1.7
1
HEI’ 1.3
-0.2
1 -0.1
0 1.9
1 -2.4
0 0.3
1 3.1
1
2.2
1 1.1
0 -1.5
1 -0.5
0 0.9
1 -1.2
1
-0.2
1 -0.1
0 1.9
1 -2.4
0 0.3
1 3.1
1

XNOR & bitcount

Input feature maps

4
convolutional layers is defined as
𝐹𝑊−1 𝐹𝐻−1 𝐹𝐷−1

𝑌[𝑛][𝑤′][ℎ′] = ∑ ∑ ∑ 𝑊[𝑛][𝑤][ℎ][𝑑] × 𝑓𝑚𝑎𝑝[𝑤′ + 𝑤][ℎ′ + ℎ] [𝑑].
𝑤=0

(1)

ℎ=0 𝑑=0

One should note that there is no data dependency for the calculation of each pixel across the
entire output feature maps. Therefore, spatial parallelism can be applied in the hardware
architecture to improve throughput. Differently, within the convolution operation for
calculating each pixel, data dependency exists among the nested loops of summation in (1).
These data-dependent operations can be unfolded and pipelined in the hardware architecture to
gain temporal parallelism and improve throughput.
2.1.2 Pooling. The pooling layer performs subsampling across a K×K contiguous region on
the output feature map of convolutional layers. Pooling is used to pool out sensitive information
critical to classification and eliminate insensitive information that is irrelevant. Also, pooling
layers reduce an amount of trainable parameters in the network. There are two kinds of pooling
methods which are commonly used in CNNs. One is max-pooling, which picks the maximum
value of the pooling region. The other is average-pooling, which picks the mean value of the
pooling region.
2.1.3 Normalization. Normalization is a powerful technique that stabilizes and accelerates the
training process [11]. In the inference stage, normalization needs to be applied to match the
training process. Statistical reference values are counted across the whole training set as
𝑧=

𝑦 −𝜇
√𝜎 2 + 𝜖

γ + 𝛽,

(2)

where 𝜇 is the mean value, and 𝜎 2 is the variance with very a small constant 𝜖 to ensure a nonzero denominator. Note that γ and 𝛽 scales and shifts the normalized values, respectively. Since
𝜇, 𝜎 2 , 𝜖, γ and 𝛽 are all constants in the inference stage, they can be precomputed to reduce the
computation complexity of normalization.
2.1.4 Nonlinear function. Nonlinear function is an element-wise operation that performs on
each neuron after the normalization in the convolutional layers and the fully-connected layers.
Two common nonlinear functions used in CNNs are Sigmoid and Rectified Linear Unit (ReLU)
[13].
2.2 Binary CNN (BCNN)
A BCNN is a CNN trained with binary constraints that results in binary weights and activations,
and a significant reduction in computation complexity. The convolution operation is the most
time-consuming and computation-intensive part of a CNN. In a BCNN, as shown in Fig. 2, both
the weights and activations are constrained to a binary set of values, e.g. [+1, -1]. As such, the
multiplications in convolution is simplified to a bitwise exclusive NOR (XNOR). From a vector
operation perspective, the convolution can be expressed as an XNOR dot-product operation as
𝐹𝑊−1 𝐹𝐻−1 𝐹𝐷−1

̅̅̅̅̅̅̅̅[𝑤′ + 𝑤][ℎ′ + ℎ] [𝑑].
̅ [𝑛][𝑤][ℎ][𝑑]⨁𝑓𝑚𝑎𝑝
𝑌[𝑛][𝑤′][ℎ′] = ∑ ∑ ∑ 𝑊

(3)

𝑤=0 ℎ=0 𝑑=0

Comparing to a real-valued CNN with a single–precision data format, the FPGA
implementation of a BCNN requires much reduced logic and memory resources. Although, one
should note that neither the inputs nor the outputs of the normalization and the pooling layers are
binarized. The BCNN adopts a max-pooling scheme, which is thought to be more hardwarefriendly than average-pooling [14]. Since the weights and activations are constrained to either +1

5
or -1, the nonlinear function becomes an adjusted sign function, a.k.a. a Binarize function defined
as
1
𝑖𝑓 𝑧 ≥ 0,
𝐵𝑖𝑛𝑎𝑟𝑖𝑧𝑒(𝑧) = {
0 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.

(4)

2.3 Compression ratio and accuracy of compact CNNs
Table 1 shows some popular techniques for neural network compression. The baseline is a
standard CNN trained by conventional techniques resulting in a full precision network for
inference. Experiment results show that simply quantizing the network parameters below 10 bits
in the post-training stage will cause significant accuracy drop on CIFAR-10 classification task
using the CNN model in Ref. 9. Although, pruning the network has limited accuracy loss, the
pruned network is still based on full-precision operations. The compression ratio achieved by
pruning can be up to 5x [18], but the hardware resources needed for computing the remaining
full-precision operations still have the same logic complexity.
Differently, the BCNN trained with binary constraints features the best compression ratio
with superior accuracy performance. [9] shows that BCNN can achieve same accuracy as the
full-precision CNN on a ten-class classification task on CIFAR-10 dataset. Ref. 19 demonstrates
that with improved training technique, the BCNN only suffers from a 5% accuracy drop in terms
of both top-1 and top-5 error for a 1000-class classification task based on ImageNet dataset. In
addition, the hardware resources needed for realizing the bitwise convolutions in BCNNs are
just simple logic gates rather than multipliers. All of these suggest that BCNNs offer much
superior trade-off between complexity and accuracy and are ideal for efficient hardware
implementation.
2.4 Impact of binarization on hardware acceleration
A Titan X GPU has 3,072 CUDA cores (one ALU per core) and can run at up to 1 GHz, while a
midrange Virtex-7 FPGA has 3,600 DSP48 slices and 433,200 LUTs and typically runs at around
100-200 MHz. For mapping a full-precision or reduced-precision CNN, the two devices are on a
par in terms of the level of computing parallelism considering that a CUDA core and a DSP48
slice can map a floating- and a fixed-point multiplication accumulator (MAC), respectively. But,
FPGAs run at a 5-10x lower frequency in general. As a result, the existing FPGA
implementations of reduced-precision CNNs can hardly achieve comparable throughput to their
GPU counterparts.
A BCNN offers large room for throughput improvement for both GPU-based and FPGA-based
implementations. When using a tailored binary kernel on a GPU, a fully-pipelined ALU in one
CUDA core can process 32 bitwise operations per clock cycle. This increases the equivalent
computing parallelism of a Titan X GPU to 3,072×32=98,304 for running a BCNN. On the other
hand, for an FPGA-based BCNN, the bitwise operation can be efficiently mapped onto the
Table 1. Methods for neural network compression
Methods

Execution stage

Standard
Quantizing
Pruning
BNN

Training
post-training
training
training

Compression
ratio
1x
Up to 3x
Up to 5x
Up to 32x

Inference

Accuracy

full precision + full network
reduced precision + full network
full precision + pruned network
binary + full network

lossless
lossy
lossless
lossless

6
abundant LUT resources. Since one 6-input LUT can map 2.5 XNORs on average, the computing
parallelism of a Virtex-7 FPGA is on the order of 433,200×2.5≈1,000,000. Given the operation
frequency difference, GPU- and FPGA-based BCNN implementations should have a similar level
of throughput performance in a 1st order estimation. The FPGA-based solution features much
higher energy-efficiency. It is also worth mentioning that GPUs can only achieve the theoretical
peak throughput when the data batch size is large enough to hide the computation and memory
access latency. Thus, in the application scenarios such as processing online classification
requests from individual users where small batches of data must be processed on the fly, FPGAbased solution will keep the promise to outperform GPU counterparts in terms of both
throughput and energy efficiency. In the following sections, we present an FPGA-based BCNN
accelerator and a benchmarking study that validate our hypothesis.
2.5 A BCNN on CIFAR-10
In order to assess the practical performance of the proposed architecture, we use the BCNN on
CIFAR-10 [9] as an example model for the FPGA implementation. The overall architecture of
BCNN is shown in Table 2 [9]. It takes an RGB image with a size of 3 × 32 × 32 as the input of
the first layer. For each convolutional layer, the filter size is fixed as 3 × 3 with a stride and zero
padding of 1 pixel each. The filter specification of each convolutional layer in Table 2 is denoted
as the WID×HEI×DEP. Max-pooling is performed over a 2 × 2 window with a stride size of 2
followed by the convolutional layers of 2, 4 and 6. The last three layers are fully connected
layers. Normalization is applied to all the layers, which is followed by binarization except for the
last layer.
3 ALGORITHM REFORMULATION FOR EFFICIENT FPGA MAPPING
3.1 Binary-encoded Convolution
When training the BCNN in [9], the weights and activations are constrained to either +1 or -1.
For efficient FPGA mapping, we encode +1/-1 as 1/0 in our design. In this way, it only takes 1 bit
to store a weight or an activation value. Moreover, the convolution operation in layer 𝑙 is
𝑏
simplified into an XNOR dot product of the input feature map 𝑎𝑙−1
and the weight 𝑤𝑙𝑏 , given as
𝑏
𝑦𝑙 = 𝑋𝑛𝑜𝑟𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡(𝑎𝑙−1
, 𝑤𝑙𝑏 ).

(5)

Equation (5) sums up 1s and 0s, which is different from the original BCNN that sums up -1s and
+1s in (3). The relation between the original output feature map 𝑦𝑙𝑜 and the revised 𝑦𝑙 in our

Table 2. BCNN configurations
Name
Filter/weight
# of filters
Output size
Name
Filter/weight
# of filters
Output size

CONV-1
3×3×3
128
128×32×32
CONV-6
512×3×3
512
512×4×4

CONV-2
128×3×3
128
128×16×16
FC-1
8192×1024
1024

CONV-3
128×3×3
256
256×16×16
FC-2
1024×1024
1024

CONV-4
256×3×3
256
256×8×8
FC-3
1024×10
10

CONV-5
256×3×3
512
512×8×8

7
design can be expressed as
𝑦𝑙𝑜 = 1 × 𝑦𝑙 + (−1) × (𝑐𝑛𝑢𝑚𝑙 − 𝑦𝑙 ) = 2𝑦𝑙 − 𝑐𝑛𝑢𝑚𝑙 ,

(6)

where 𝑐𝑛𝑢𝑚𝑙 = 𝐹𝑊 × 𝐹𝐻 × 𝐷𝐸𝑃 is the total number of bitwise XNOR operations needed for
each 𝑦𝑙𝑜 . The difference between 𝑦𝑙𝑜 and 𝑦𝑙 is compensated in the normalization module in our
design.
Note that all the layers take the binary feature map of its previous layer as the input except
for the first layer. In our design, we rescale the input data within the range of [-31,31] and use a
6-bit fixed-point data format, which helps to reduce the resource utilization of non-binary
operations at the cost of a limited classification accuracy loss of <0.5%. Since the input image
size is 3 × 32 × 32, the computational complexity of the first layer is not a dominating factor. The
fixed-point dot product of a 6-bit signed input 𝑎0 and a 2-bit signed weight 𝑤1 is denoted as
𝑦1 = 𝐹𝑝𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡(𝑎0 , 𝑤1 ).

(7)

3.2 Comparator-based Normalization
The parameters subject to training can be considered as constant values in the inference stage.
Therefore, we can combine the binarization in (4), the normalization function in (2) and the
value compensation in (6) into a modified sign function defined as
1
𝑁𝑜𝑟𝑚𝐵𝑖𝑛𝑎𝑟𝑖𝑧𝑒(𝑦𝑙 , 𝑐𝑙 ) = {
0

𝑖𝑓 𝑦𝑙 ≥ 𝑐𝑙 ,
𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒,

(8)

where 𝑐𝑙 is a constant threshold derived by 𝑐𝑙 = (𝑐𝑛𝑢𝑚𝑙 + 𝜇 − 𝛽√𝜎2 + 𝜖/γ) × 0.5 , and it is
rounded to the nearest integer for hardware implementation.
The impact of the proposed reformulation on hardware implementation is that both the
reformulated normalization and binarization functions can be efficiently implemented as a single
LUT-based comparator. In addition, one only needs to store one threshold value 𝑐𝑙 for each
output value rather than a set of training parameters 𝜇, 𝜎 2 , 𝛽 and γ.
{1. The first layer}
𝑦1 ← 𝐹𝑝𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡(𝑎0, 𝑤1 )
𝑎1 ← 𝑁𝑜𝑟𝑚𝐵𝑖𝑛𝑎𝑟𝑖𝑧𝑒(𝑦1 , 𝑐1 )
{2. Remaining hidden layers}
for 𝑙 = 2 to 8 do
𝑏
𝑦𝑙 ← 𝑋𝑛𝑜𝑟𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡(𝑎𝑙−1
, 𝑤𝑙𝑏 )
If (𝑙 = 2,4,6) then
𝑦𝑙 ← 𝑀𝑃(𝑦𝑙 )
end if
𝑎𝑙 ← 𝑁𝑜𝑟𝑚𝐵𝑖𝑛𝑎𝑟𝑖𝑧𝑒(𝑦𝑙 , 𝑐𝑙 )
end for
{3. Output layers}
𝑏
𝑦𝑙 ← 𝑋𝑛𝑜𝑟𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡(𝑎𝑙−1
, 𝑤𝑙𝑏 )
)
𝑎𝑙 ← 𝑁𝑜𝑟𝑚(𝑦𝑙 , 𝑐𝑙
Fig. 3. Pseudo code of the BCNN algorithm.

8
3.3 BCNN Model Overview
We summarize the inference flow for the reformulated BCNN algorithm in Fig. 3. The
st
convolution in the 1 layer involves fixed-point dot product operations (𝐹𝑝𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡).
Differently, bitwise XNOR dot product operations (𝑋𝑛𝑜𝑟𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡) are used in all the other
layers. Max-pooling (𝑀𝑃) is applied in layers 2, 4 and 6. Normalization and binarization are
combined as a single function (𝑁𝑜𝑟𝑚𝐵𝑖𝑛𝑎𝑟𝑖𝑧𝑒), which is applied in all layers except for the
output layer. The output layer ends with the normalization function 𝑁𝑜𝑟𝑚 for classification.
4 ARCHITECTURE DESIGN AND OPTIMIZATION
4.1 Architecture Overview
The binary nature of the BCNN enables us to map all the weights, feature maps, and reference
values (for normalization) onto the on-chip block RAMs (BRAMs) in a single FPGA. This
eliminates any DRAM access latency and dramatically reduces the energy consumption of the
system comparing to the existing work relying on off-chip storage [1] [3] [12] [21].
Fig. 4 shows the overall architecture of the proposed BCNN accelerator. The binary
convolutional kernel in each layer is followed by a NormBinarize (NB) kernel with or without a
Max-pooling (MP) kernel. All of the kernels are highly parallelized with an optimized number of
processing elements (PEs) and operate in a single instruction multiple data (SIMD) fashion. A
streaming architecture is enabled by using double-buffering-based memory channels to handle
the data flow between adjacent layers. Each PE in the binary convolutional kernel handles an
XNOR dot product operation, which is the core operation in both convolutional and fullyconnected layers. The PEs interface with the BRAMs in parallel to read the weights concurrently.
4.2 Architectural parameters
4.2.1 Loop Unrolling. Note that the three nested loops in (3) that accumulate the XNOR output
values along the three dimensions of a convolutional filter has loop-carried data dependency.
Unrolling data-dependent loops is the same as architectural unfolding, which will improve
throughput by increasing the level of temporal parallelism. This trades off more hardware
NormBinarize Kernel

1
1
1
0

1
1
0
1

PE1 PE1 PE1
PE1 PE1 PE1
PE1 PE1 PE1

Layer 1

Memory channels

Memory channels

Input image

1
0
0
0

Max-pooling Kernel

Weights W2

Weights W1
0
1
0
1

Binary Conv. Kernel

1
0
1
0

1
1
1
1

0
0
0
0

1
1
1
1

Weights W3
1
1
0
1

0
0
1
0

Memory channels

Fixed-Point Conv. Kernel

PE2 PE2 PE2 PE2
PE2 PE2 PE2 PE2
PE2 PE2 PE2 PE2
PE2 PE2 PE2 PE2

1
0
1
0

1
1
1
1

0
0
0
0

1
1
1
1

PE3 PE3
PE3 PE3
PE3 PE3

Layer 2

Fig. 4. Overview of the proposed accelerator architecture for BCNN.

Layer 3

9
resource with improved computing parallelism. The unfolding factor is a critical architectural
parameter in our design, denoted as UF. UF has a maximum value of 𝑊𝐼𝐷 × 𝐻𝐸𝐼 × 𝐷𝐸𝑃 in each
layer.
Differently, the calculation of the pixel values along the three dimensions of an output
feature map has no loop-carried data dependency. Unrolling independent loops is equivalent to
creating spatial parallelism in the architecture to improve throughput. In our design, we fully
unroll these independent loops to maximize the throughput. We denote the unrolling factor of
independent loops as P. Maximizing P generates a massively parallelized PE array by utilizing
the abundant LUT resources on the FPGA. Note that the PEs in the same layer are identical, but
they could be different in size across different layers.
4.2.2 Pipelining. Loop pipelining is applied in the proposed architecture to further enhance
the temporal parallelism and maximize the system throughput. Note that the queuing time to
feed in the next data is the inversely proportional to throughput, which is referred to as initial
interval 𝐼 in this paper. If there is a loop existing in the data path, the minimum initial interval
will be limited by the loop latency of the recursive architecture. With loop pipelining, we can
feed in the next data whenever possible with the minimum initial interval. In the case of a fully
pipelined implementation, we can feed in new data every clock cycle (𝐼 = 1).
4.3 Throughput Modeling and Optimization
If we only perform one XNOR operation and one accumulation in each clock cycle, the total
execution time 𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣 in terms of clock cycles of a convolutional layer can be model as
𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣 = 𝑊𝐼𝐷 × 𝐻𝐸𝐼 × 𝐷𝐸𝑃 × 𝐹𝑊 × 𝐹𝐻 × 𝐹𝐷,

(9)

where WID, HEI, and DEP denotes the width, height, and depth of a convolutional filter, and FW,
FH, and FD denotes the width, height and, depth of an output feature map, respectively.
When architectural unfolding is applied in performing the XNOR dot product operation in
each PE, 𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣 will be divided by UF. Similarly, when spatial parallelism is applied to create
PE arrays for processing P output pixels in parallel, 𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣 will be further reduced by P times.
The same PE array is reused to calculate the output feature maps with pipelining applied, which
contributes to an 𝐼-cycle initial interval for the most inner loop. Thus, the throughput of the
convolutional kernel with architectural optimization can be formulated as
𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡𝐶𝑂𝑁𝑉 =

𝑈𝐹 × 𝑃
𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣

×

1
𝐼

× 𝑓𝑟𝑒𝑞,

(10)

where 𝑓𝑟𝑒𝑞 is the system frequency. Note that 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡𝐶𝑂𝑁𝑉 is inversely proportional to the
estimated cycle count 𝐶𝑦𝑐𝑙𝑒𝑒𝑠𝑡 in a convolutional layer, defined as
𝐶𝑦𝑐𝑙𝑒𝑒𝑠𝑡 =

𝐶𝑦𝑐𝑙𝑒𝑐𝑜𝑛𝑣
× 𝐼.
𝑈𝐹 × 𝑃

(11)

In the proposed accelerator architecture, we use a double buffering scheme to further
enhance the spatial parallelism of the system as shown in Fig. 4. The computation of each layer
is triggered at the same time and alternates between two phases. Specifically, one channel of
th
th
𝑓𝑚𝑎𝑝𝐿−1 is used as the input of the L layer while the L-1 layer is writing new outputs into
the other 𝑓𝑚𝑎𝑝𝐿−1 channel. When both layers finish processing, the memory buffers swap, and
the next processing phase is triggered. Therefore, the overall system-level throughput can be
formulated as

10

𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 =

𝑚𝑎𝑥(𝐶1 , 𝐶2, 𝐶3 … , 𝐶𝑘 )
,
𝑓𝑟𝑒𝑞

(12)

th

where 𝐶𝐿 is the execution time of the L layer in the proposed accelerator architecture. 𝐶𝐿 can
be either 𝐶𝑦𝑐𝑙𝑒𝑒𝑠𝑡 for throughput modeling or 𝐶𝑦𝑐𝑙𝑒𝑟 for evaluating real execution throughput.
One should note that the system throughput can be maximized with the optimal hardware
utilization when all the layers have equal execution time (𝐶1 = 𝐶2 = 𝐶3 = ⋯ = 𝐶𝑘 ). In the case
that the Lth layer has longer execution time than other layers, one can always increase the
parallelism of the Lth layer while decreasing that of other layers to gain throughput with
minimum overhead in resource usage. Since the convolutional layers take up over 95% of the
computation, we only emphasize the optimization of convolutional layers in this section. The
fully-connected layer can be easily optimized to match up the system throughput using the same
principle.
5 FPGA IMPLEMENTATION
In this section, we present the strategy of mapping different computing units to maximize the
FPGA resource utilization.
5.1 PE Unit
The block diagram of a PE unit is shown in Fig. 5. A PE unit handles the XNOR dot product
operation of a weight vector and a feature map vector from the previous layer. The vectors are
fed into an array of 2-input XNOR gates followed by a parallelized bit-count logic for
accumulation. Since both the XNOR gates and the bit-count logic take binary values as input, the
PEs can be efficiently implemented using the abundant LUT resources. This is the key to
enabling massive computing parallelism on an FPGA. Note that the number of XNOR gates in
each PE is the same as the unfolding factor UF of the current layer. By accumulating the PE
output, the pixel value of an output feature map can be computed by the bit-count logic.
5.2 Computing Kernels
Fig. 6 shows the architecture of the convolutional kernel followed by the Max-pooling and
NormBinarize kernels. Each convolutional kernel has an array of PEs implemented using LUTs
followed by an array of accumulators implemented using DSP48 slices. The number of PEs and
DSP slices is equal to the spatial parallelism factor P. Each convolutional kernel thereby
computes P pixel values of the output feature map in parallel. Besides the weight arrays, only
Parallelized bit-count logic

Weights

=1

=1

=1

1-bit XNOR
Gate

=1
=1

fmap
=1

Fig. 5. Processing element (PE).

11

BRAM
Weights
0
1
0
1

1
0
0
0

1
1
1
0

Fmaps

PE

Acc.

PE

Acc.

PE

Acc.

PE

Acc.

PE

Acc.

MP NB
kernelkernel

Fmaps

Buffer

Conv.
kernel

0 0 1 0
1 0 1 0
0 0 0 0

0 0 1 0
1 0 1 0
0 0 0 0

Register

LUT DSP48

LUT

Register

Fig. 6. The architecture of computing kernels and their FPGA mapping schemes.

intermediate results of the accumulator outputs (bit-count results) within a single feature map
are stored in BRAMs. Feature maps are mapped onto distributed RAMs.
For the convolutional layers 1, 3 and 5 without max-pooling, the outputs of accumulators are
directly connected to the NB kernels. The hardware kernel of fully-connected layers is similar to
Fig. 6. Note that the max-pooling is performed in pipeline with the computation of feature maps
in our implementation.
5.3 Memory
To read and write a large number of bits in the same clock cycle, we have to partition and
reshape the memory arrays in the BCNN model. Partition essentially breaks down a large data
array into smaller ones to fit in multiple BRAMs for parallel access. Reshaping basically
redefines the depth and width of a single BRAM by grouping multiple words into a wider one. In
our design, the weight and 𝑓𝑚𝑎𝑝 arrays are mapped onto BRAMs and distributed RAMs
(registers), respectively. Since the maximum word length of a BRAM in a Virtex-7 FPGA is
limited to 32 bits, we first reshape the weight array by 32 and then partition the weight arrays
into several BRAMs to guarantee enough memory bandwidth for the required system
throughput.
6 EXPERIMENT RESULTS

Table 3. Optimized parameters for each layer
Layer
Conv 1
Conv 2
Conv 3
Conv 4
Conv 5
Conv 6

UF
27
384
384
768
768
1536

P
32
32
16
16
8
8

𝑪𝒚𝒄𝒍𝒆𝒄𝒐𝒏𝒗
3538944
150994944
75497472
150994944
75497472
150994944

𝑪𝒚𝒄𝒍𝒆𝒆𝒔𝒕
4096
12288
12288
12288
12288
12288

𝑪𝒚𝒄𝒍𝒆𝒓
5233
12386
12296
13329
12386
14473

12

Table 4. FPGA resource utilization summary
Resource
Used
Available
Utilization/%

LUTs
342126
433200
78.98

BRAMs
1007
2060
48.88

Registers
70769
607200
14.30

DSP
1096
2800
39.14

We implement the proposed accelerator architecture for the BCNN in Ref. 9 using the optimal
architectural parameters shown in Table 3. We optimize the parameters of UF and P to make
𝐶𝑦𝑐𝑙𝑒𝑒𝑠𝑡 of each layer approximately the same based on the throughput model in (12). Each
layer is also fully pipelined with an initial interval of 𝐼 = 1. Note that the operations along the
FW and the FD dimensions are fully unfolded for maximizing the throughput.
6.1 Design Environment
We use C language to describe the accelerator architecture. Vivado HLS is used to produce the
RTL codes. The Vivado Design Suite is used to map the design onto a Xilinx Virtex-7 XC7VX690
FPGA. The execution time in terms of clock cycles is reported by Vivado HLS and the system
frequency is reported by Vivado Design Suite after the implementation stage. We notice a large
discrepancy of LUTs usage between the synthesis reports in Vivado HLS and Vivado Design
Suite. For accurate results, the resource utilization and power consumption are reported in
Vivado Design Suite after the implementation stage.
6.2 FPGA Implementation results
As shown in Table 3, the real execution time 𝐶𝑦𝑐𝑙𝑒𝑟 given by the synthesis report for each layer
is well aligned with 𝐶𝑦𝑐𝑙𝑒𝑒𝑠𝑡 estimated by our model in (11). The throughput bottleneck is layer
6 in this case. Running at a system frequency of 90 MHz, the FPGA-accelerated BCNN achieves
an image processing throughput of 6,218 frames per second (FPS), which is the highest
throughput for the same dataset reported by far. The top-1 accuracy rate is 87.8%, which is only
0.3% lower compared to the software model in Theano.

Table 5. Results in comparison with FPGA-based accelerators

Device
[3]
[1]
[12]
[4]
[22]
[23]
[24]
[21]
Ours

Virtex 6
Virtex 7
Zynq-7000
Stratix-V
Arria-10
Intel QuickAssist
QPI FPGA
Arria-10
Zynq-7000
Virtex 7

Clock Bit(MHz) width

GOPS

200
100
150
120
150

16
32 float
16
8 ~ 16 b
8 ~ 16 b

147
62
137
117.8
645.25

Energy
Performance
Power
Efficiency Density
(W)
(GOPS/W) (GOPS/kLUT)
10
14.7
0.98
18.7
3.3
0.14
9.6
14.3
0.75
25.8
4.56
0.45
21.2
30
4.01

200

32 float

123.48

13.18

9.37

0.62

385
143
90

fixed
1~2b
1

1790
207.8
7663

37.46
4.7
8.2

47.78
44
935

4.19
4.43
22.40

13
To reduce runtime, we adopt a bottom-up design strategy by synthesizing our design layer by
layer in Vivado HLS and implementing the entire system in Vivado Design Suite. The overhead
introduced by initialization is negligible. Table 4 shows the resource utilization summary for the
entire BCNN implementation. LUTs are used for mapping all the computing kernels, including
binary convolution, MP and NB kernels. Feature maps of convolutional layers are mapped onto
distributed RAMs result in additional LUT consumption. The BRAM usage is mostly consumed
by all the weight matrices. Flip-flops are used for storing feature maps and constructing a deep
st
pipeline. Around 30% of the DSP slices are used by the 1 layer to perform fixed-point
multiplication. For the rest of convolutional layers, DSP slices are used for accumulating PE
outputs as shown in Fig. 6.
Existing FPGA-based CNN implementations are compared in Table 5. To minimize the impact
of different FPGA models on throughput, energy efficiency and performance density defined as
throughput normalized to resource utilization are used as the performance metrics for
comparison. Compared with the FPGA implementations of floating-point or reduced-precision
CNNs, our BCNN implementation achieves 4-124x higher GOPS, 20-283x better energyefficiency and 5-160x better performance density. Even compared with the BCNN
implementation in Ref. 21, our work achieves 5x better performance density in terms of
GOPS/kLUT. The work in Ref. 21 implements three kinds of computing kernels in hardware:
floating-point convolution, binary convolution and fully-connected kernels. Since this reference
work maps a single layer of the BCNN at a time, only one kind of computing kernels is active at
a time. Such a time multiplexing scheme limits the system throughput due to the low hardware
utilization. In our design, all the layers of the BCNN are mapped into a streaming architecture
with optimized architectural parameters, and the data is flowing throughout the entire
architecture in a deep pipeline. Therefore, the kernels are constantly active, and the utilization
rate of the hardware resources is high. In addition, Ref. 21 consumes extra power for loading the
weights from off-chip memory layer by layer in addition to the FPGA power reported. On the
contrary, there is no such overhead in our architecture since we fully map the network and
trained parameters on chip.
6.3 FPGA-based verse GPU-based BCNN
Fig. 7 compares the performance of the BCNN accelerated by a Titan X GPU and our FPGAbased design. For GPU acceleration, the baseline kernel is designed for floating-point
computation, and the XNOR kernel is optimized for bitwise operations [9]. In the XNOR kernel,
it concatenates 32 1-bit values into a 32-bit value. At the peak performance, each CUDA core can
execute 32 bitwise operations per clock cycle. That is the reason why BCNN can also gain
remarkable speedup on a GPU when using the XNOR kernel for compilation.
GPU acceleration is apparently sensitive to the size of workload (batch size here). One of the
keys to achieving high performance in GPU computing is to hide the long latency of functional
units by data-level interleaving especially when there are loop-carried data dependency existed
in the algorithm. Only when the workload is large enough, a GPU is able to maintain high
thread-level parallelism to achieve a high throughput. Differently, the FPGA-based solution is
invariant to the batch size of data. Experiment results show that our design significantly
outperforms the GPU acceleration using the baseline kernel in terms of both throughput and
energy efficiency. Even compared with the GPU acceleration using the XNOR kernel, which is
reported as the best GPU-based CNN performance by far, our design achieves a 75x better
energy efficiency and an 8.3x better throughput for processing data in a small batch size of 16.
For processing data in a large batch size of 512 (the maximum size that fit into the GPU memory),
our design can match the throughput of the GPU acceleration with a 9.5x better energy
efficiency.

14

* Titan X GPU has used up ~80% memory for batch size of 512
Fig. 7. Throughput and energy efficiency comparison with GPU implementations.

Therefore, the FPGA-based BCNN solution is a clearly better choice for accelerating the data
center applications that process online individual requests in small batch sizes. In a recent study
conducted by Baidu, a dominant Internet company in China with 600 million active users, it is
reported that the typical on-line prediction workload in terms of batch size is around 8 to 16 [25].
Such small workload is not enough for GPU to achieve its peak throughput performance. Thus,
the FPGA-based solution is more superior in handling this kind of requests from individual users.
For processing static data in large batch sizes, the proposed solution is on a par with a Titan
X GPU in terms of throughput while delivering much higher energy efficiency. This renders the
FPGA-based solution a better choice for energy constrained applications, such as mobile-based
advanced driver assistance systems (ADAS). In the ADAS application, a large batch of data
needs to be processed for monitoring real-time road condition. In this case, both throughput and
energy efficiency are essential and the FPGA-based solution can be deployed.
7 CONCLUSION
In this paper, we propose an optimized accelerator architecture tailored for BCNNs. We
demonstrate for the 1st time that the FPGA-based BCNN solution can greatly outperform a Titan
X GPU in terms of both throughput and energy efficiency for processing accurate image
classification tasks. The proposed BCNN accelerator running on a Virtex-7 FPGA is 8.3x faster
and 75x more energy-efficient than a Titan X GPU for processing individual online requests in
small batch sizes. For processing static data in large batch sizes, the proposed solution is on a par
with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency. Thus,
BCNNs are ideal for efficient hardware implementations on FPGAs regardless of the size of
workload. The bitwise operations in BCNNs allow for the efficient hardware mapping of
convolution kernels using LUTs, which is the key to enable massive computing parallelism on
an FPGA. Applying the optimal levels of architectural unfolding, parallelism, and pipelining
based on the proposed throughput model is the key to maximizing the system throughput.

15
Building memory channels across layers with data-flow control is the key to constructing a
streaming architecture to further improve the throughput.
ACKNOWLEDGMENTS
This work by Arizona State University and Nanyang Technological University is supported by
Cisco Research Center (CG#594589) and Singapore MOE Tier-2 (MOE2015-T2-2-013),
respectively. We acknowledge Mr. Skip Booth and Mr. Hugo Latapie from Cisco for fruitful
research discussions. We also thank Xilinx University Program for donating the FPGA boards.

REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]

Zhang, C., Li, P., Sun, G., Guan, Y., Xiao, B., and Cong, J. 2015. Optimizing FPGA -based accelerator design for
deep convolutional neural networks. In Proceedings of the 2015 ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays, 161–170.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. 2012. ImageNet classification with deep convolutional neural
networks. In Advances in neural information processing systems, 1097–1105.
Farabet, C., Martini, B., Corda, B., Akselrod, P., Culurciello, E., and LeCun, Y. 2011. Neuflow: A runtime
reconfigurable dataflow processor for vision. In Conference on Computer Vision and Pattern Recognition 2011
Workshops, 109–116.
Suda, N., Chandra, V., Dasika, G., Mohanty, A., Ma, Y., Vrudhula, S., Seo, J.S., and Cao, Y. 2016. Throughputoptimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks. In Proceedings of the
2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 16–25.
Courbariaux, M., Bengio, Y., and David, J. P. 2015. Binaryconnect: Training deep neural networks with binary
weights during propagations. In Advances in Neural Information Processing Systems, 3123–3131.
Sung, W., Shin, S., and Hwang, K. 2015. Resiliency of deep neural networks under quantization. arXiv preprint
arXiv:1511.06488.
Cheng, Z., Soudry, D., Mao, Z., and Lan, Z. 2015. Training binary multilayer neural networks for image
classification using expectation backpropagation. arXiv preprint arXiv:1503.03562.
Kim, M., and Smaragdis, P. 2016. Bitwise neural networks. arXiv preprint arXiv:1601.06071.
Courbariaux, M. and Bengio, Y. 2016. Binarynet: Training deep neural networks with weights and activations
constrained to + 1 or -1. arXiv preprint arXiv:1602.02830.
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. 2016. XNOR -Net: ImageNet classification using binary
convolutional neural networks. In European Conference on Computer Vision, 525–542.
Ioffe, S., and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. In Proceedings of the 32nd International Conference on Machine Learning.
Qiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J., Tang, T., Xu, N., Song, S., and Wang, Y. 2016. Going
deeper with embedded FPGA platform for convolutional neural network. In Proceedings of the 2016 ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays, 26–35.
LeCun, Y., Bengio, Y., and Hinton, G. 2015. Deep learning. Nature, 521(7553), 436–444.
Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT Press, 2016.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. 2012. ImageNet classification with deep convolutional neural
networks. In Advances in neural information processing systems, 1097–1105.
Simonyan, K., and Zisserman, A. 2015. Very deep convolutional networks for large-scale image recognition. In
Proceedings of the 32nd International Conference on Learning Representations.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. 1998. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11), 2278–2324.
Anwar, S., Hwang, K., and Sung, W. 2017. Structured Pruning of Deep Convolutional Neural Networks. ACM
Journal on Emerging Technologies in Computing Systems, Vol. 13, Issue 3, Article 32 (February 2017), 18 pages.
Tang W., Hua G., and Wang L. 2017. How to Train a Compact Binary Neural Network with High Accuracy? In
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2625–2631.
Panda, P., Sengupta, A., and Roy, K. 2017. Energy-Efficient and Improved Image Recognition with Conditional
Deep Learning. ACM Journal on Emerging Technologies in Computing Systems. 13, 3, Article 33 (February 2017), 21
pages.
Zhao, R., Song, W., Zhang, W., Xing, T., Lin, J.H., Srivastava, M., Gupta, R. and Zhang, Z. 2017. Accelerating
Binarized Convolutional Neural Networks with Software-Programmable FPGAs. In Proceedings of the 2017
ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 15–24.
Ma, Y., Cao, Y., Vrudhula, S. and Seo, J.S. 2017. Optimizing Loop Operation and Dataflow in FPGA Acceleration of
Deep Convolutional Neural Networks. In Proceedings of the 2017 ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays, 45–54.

16
[23]
[24]
[25]
[26]
[27]

Zhang, C. and Prasanna V. 2017. Frequency Domain Acceleration of Convolutional Neural Networks on CPUFPGA Shared Memory System. In Proceedings of the 2017 ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays, 35–44.
Zhang, J. and Li. J. 2017. Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional
Neural Network. In Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate
Arrays, 25–34.
Ouyang, J., Lin, S., Qi, W., Wang, Y., Yu, B. and Jiang, S. 2016. SDA: Software-defined accelerator for large-scale
DNN systems. In Proceedings of the Hot Chips 28.
Han, S., Pool J., Tran, J., and Dally, W. 2015. Learning both weights and connections for efficient neural network.
In Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS'15), 1135–1143.
Wang, L., Ouyang, W., Wang, X. and Lu, H., 2015. Visual tracking with fully convolutional networks.
In Proceedings of the IEEE International Conference on Computer Vision, 3119–3127.

IEEE EMBEDDED SYSTEMS LETTERS, VOL. 6, NO. 4, DECEMBER 2014

73

A Square-Root-Free Matrix Decomposition Method
for Energy-Efficient Least Square Computation on
Embedded Systems
Fengbo Ren, Chenxin Zhang, Liang Liu, Wenyao Xu, Viktor Öwall, and Dejan Marković

Abstract—QR decomposition (QRD) is used to solve leastsquares (LS) problems for a wide range of applications. However,
traditional QR decomposition methods, such as Gram–Schmidt
(GS), require high computational complexity and nonlinear
operations to achieve high throughput, limiting their usage on
resource-limited platforms. To enable efficient LS computation
on embedded systems for real-time applications, this paper
presents an alternative decomposition method, called QDRD,
which relaxes system requirements while maintaining the same
level of performance. Specifically, QDRD eliminates both the
square-root operations in the normalization step and the divisions
in the subsequent backward substitution. Simulation results show
that the accuracy and reliability of factorization matrices can be
significantly improved by QDRD, especially when executed on
precision-limited platforms. Furthermore, benchmarking results
on an embedded platform show that QDRD provides constantly
better energy-efficiency and higher throughput than GS-QRD
in solving LS problems. Up to 4 and 6.5 times improvement in
energy-efficiency and throughput, respectively, can be achieved
for small-size problems.
Index Terms—Computational complexity, energy efficiency,
least-squares problem, matrix factorization, QR decomposition.

I. INTRODUCTION

Q

R DECOMPOSITION (QRD) is one of the most
widely used matrix factorization techniques for solving
least-squares (LS) problems in various applications.
Because of its practical importance, enormous algorithmic efforts have been made to improve energy efficiency and routine
performance of QRD over the past few decades, especially
from a very large scale integration (VLSI) design point of
view. Two hardware-friendly methods for computing QRD
are commonly used in practice. Givens rotation (GR) offers
a low-cost computation method by handling one element of
Manuscript received May 30, 2014; accepted August 18, 2014. Date of publication August 22, 2014; date of current version November 20, 2014. This manuscript was recommended for publication by A. Coskun
F. Ren and D. Marković are with the Department of Electrical Engineering, University of California, Los Angeles, CA 90095 USA (e-mail:
renfengbo@ucla.edu; dejan@ee.ucla.edu).
C. Zhang, L. Liu, and V. Öwall are with the Department of Electrical and Information Technology, Lund University, Lund 221 00, Sweden (e-mail: chenxin.
zhang@eit.lth.se; Liang.Liu@eit.lth.se; Viktor.Owall@eit.lth.se).
W. Xu is with the Department of Computer Science and Engineering, State
University of New York at Buffalo, Buffalo, NY 14260 14214 USA (e-mail:
wenyaoxu@buffalo.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/LES.2014.2350997

a matrix at a time with only additions and table look-ups
involved. However, the inherent sequential processing of GR
limits its applications with stringent real-time requirements
[1]. Alternatively, the Gram–Schmidt (GS) algorithm allows
parallel computations with higher processing throughput by
factorizing a matrix at the vector-level. Therefore, GS-QRD
is often in favor for throughput-driven applications, where the
parallelization of computations is desired [2], [3].
However, neither of the above methods are ideal for real-time
implementations on embedded systems. Due to limited memory
and computation resources available on such platforms,
GR-QRD will have limited throughput while GS-QRD will
face design challenges on precision loss and energy-efficiency
[1]–[4]. Prior analysis has revealed that nonlinear operations
in GS-QRD, namely square-root operations and divisions,
are the primary cause of the aforementioned design issues
[2]. Nonlinear operations are not cost-effective for an explicit
implementation, and they are usually the accuracy bottleneck
of the overall system. Most existing work implements nonlinear operations using iterative approximation methods, such
as Newton–Raphson, with word-length optimization applied
[2]–[4]. Nonetheless, these methods trade computational complexity with throughput or accuracy, which is not desired for
real-time applications.
In this paper, we tackle the abovementioned issues jointly by
deriving an alternative QDR decomposition (QDRD) method
that has both the throughput advantage of GS-QRD and the intrinsic square-root-free feature of GR-QRD. Specifically, a new
is introduced as the scaling
diagonal factorization matrix
factor to exclude the normalization process of orthogonal basis.
As a result, both the square-root operations and the divisions
in the backward substitution for solving LS problems are eliminated. To the best of our knowledge, a similar idea has been
suggested in [5] while this is the first paper that analyzes and
evaluates its performance for LS computation based on hardware implementation results.
II. THE QDRD METHOD FOR SOLVING LS PROBLEMS
A LS problem is formulated as finding an

that gives
(1)

where
(usually
) and
. If has a
zero-null space, the solution to (1) can be computed by solving
the normal equation given as

1943-0663 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

(2)

74

According to QRD, a matrix
space can be decomposed as

IEEE EMBEDDED SYSTEMS LETTERS, VOL. 6, NO. 4, DECEMBER 2014

with a zero-null
(3)

where

is an orthonomal matrix with
(
is identity matrix) and
is an upper-triangular
matrix with positive diagonal elements. By substituting with
(3), (2) becomes
(4)

Since is an upper-triangular matrix, (4) can be solved by
using backward substitution.
Note that in GS-QRD, square-root operations are required for
, as Euclidean
computing the diagonal elements of ,
column of and the
row of
norms [2]. In addition, the
(except for the diagonal elements) is normalized by the
element of
, respectively. Therefore, there must exist
with
, and
a diagonal matrix
and
that satisfy
unnormalized matrices
and
, where serves as a normalizer
for both and . According to this, (4) can be formulated as
(5)
appears on both sides of (5)
Note that the normalizer
and cancels each other, which indicates that the normalization
of factorization matrices in QRD is essentially redundant for
solving LS problems. Therefore, the square-root operations
involved in the normalization process are also redundant.
A. Square-Root-Free QDRD
Based on the above analysis, we propose an alternative factorization method that essentially eliminates the square-root operations by removing the normalizer from both and in (3)
. By defining
as
,
, and
, we
that has a zero-null
derive the QDRD of a matrix
space as

Fig. 1. Illustrations of (a) the QRD and (b) the QDRD of a

matrix.

where

is the column vector of
and
,
is a row vector of ,
is a scalar element
, respectively. Given that
and
of
are known submatrices from the
,
, and
previous iteration, the unknown variables
can be derived as follows. According to (7), we know that
(10)
and
(11)
In addition, by expanding (9) we have
(12)
and
(13)
Therefore, from (10) and (12),
can be derived as
, and
can be calculated as
.
can be derived by multiplying both sides
of (13) by
as
(14)
Inserting (10) and (11), (14) can be simplified to
. As indicated by (13), the QDRD of the next iteration is
given by

(6)

(15)

where
is an orthogonal matrix,
is a
is an upper-triangular matrix.
diagonal matrix, and
The new factorization matrices have several important properties. Since is a diagonal matrix, we can derive

Note that by introducing the new factorization matrix
to avoid the normalization process, the proposed QDRD
method completely eliminates the square-root operations. To
better explain this concept in comparison to QRD, the two difmatrix are illustrated
ferent matrix decompositions of a
using 2-D vectors in Fig. 1. Note that the basis vectors in QRD
( ) are always orthonormal, while those in QDRD ( ) are
and no longer normalized.
scaled by

(7)
Equation (7) indicates that
as
can be normalized by

is an orthogonal matrix, which
(8)

Additionally, from
and
,
, meaning the diagonal elements of
we must have
must be all unit ones.
shown in (6) can be computed
The QDRD of
column-wise in iterations. In each iteration, (6) can be partitioned as
(9)

B. Solving LS Problems using QDRD
To solve LS problems using the proposed QDRD method, the
normal equation in (2) can be substituted by (6) and simplified
according to (8) as
(16)
then solved through backward substitution. Equation (16) also
indicates that solving LS problems only requires finding the
directions of the orthogonal basis of , regardless of the normalization of their Euclidean norms. Therefore, the introduced

REN et al.: A SQUARE-ROOT-FREE MATRIX DECOMPOSITION METHOD

75

scaling factor
in QDRD can be disregarded in the backward
are
substitution. Moreover, since the diagonal elements of
all unit ones in QDRD, the divisions required in the backward
substitution in (16) can be safely eliminated.
III. MATRIX FACTORIZATION PERFORMANCE ANALYSIS
In general, two characteristics of the factorization matrices
are critical to the numerical accuracy of the solutions to LS
problems. One is how well the factorization matrices can
reconstruct the original matrix . The other is how close the
transpose of the orthogonal matrix is to its inverse. In this study,
we define two new generic metrics to quantify both of the criteria. The reconstruction signal to noise ratio (RSNR) for QRD
,
and QDRD is defined as
and

, respec-

denotes the Frobenius
tively, where the operator
norm of a matrix. Note that RSNR measures the reconstruction error of the factorization matrices relative to the
original matrix as the ratio of ’s energy over that of the
reconstruction error. Using a similar concept, the orthogonality signal to noise ratio (OSNR) for QRD and QDRD
, and
is defined as
, respectively.
Ideally, if infinite precision can be preserved during matrix
factorization, both OSNR and RSNR should approach positive
infinity. In practice, both metrics are subject to quantization
noise. Given a fixed machine precision, higher OSNR and
RSNR performance indicates the superiority of the factorization method in terms of accuracy and reliability.
Matrix factorization experiments are carried out in MATLAB
using double precision in order to compare the performance of
are
QDRD against GS-QRD. The input matrices
randomly generated for different sizes ranging from 10 to 100
. For each matrix size, 1000
with an aspect ratio of
trials are performed using Gaussian random matrices. The impact of the precision of nonlinear operations is investigated by
injecting rounding errors at different precision—their results are
rounded to different number of significant bits (ranging from
10 to 52 bits). Besides, all linear operations are performed with
the highest precision to exclude their effects in the analysis.
Note that the upper bound of the precision is limited by the
word-length of the mantissa in double precision.
Fig. 2 shows the comparison results measured from general
matrix factorizations. The RSNR and OSNR gain of QDRD
(over GS-QRD) is calculated as
and
, respectively. Note that both
results are averaged across all matrix sizes. Figure 2(a) shows
that QDRD provides nearly the same RSNR performance as
GS-QRD, indicating the independence of RSNR performance
on the precision of square-root operations. On the other hand,
QDRD is able to achieve significant OSNR gain, especially
when the precision of square-root operations dominates the
). It is because the
overall OSNR performance (
rounding errors of square-root operations can be accumulated
in GS-QRD over iterations, which deteriorates its OSNR as a
dominating factor. On the contrary, QDRD is square-root-free,
and thereby its OSNR will not have such precision loss.
Overall, QDRD is guaranteed to provide more accurate results

Fig. 2. Average (a) RSNR and (b) OSNR gain of QDRD over GS-QRD with
) and divisions (
).
respect to the word-length of square-root (

than GS-QRD when nonlinear operations are performed given
precision constraints.
IV. EVALUATION

OF

LS COMPUTATION
SYSTEMS

ON

EMBEDDED

In order to evaluate the performance of LS computation
on embedded systems, we implemented the LS solver on an
ultra-low-power MSP430 microprocessor using GS-QRD and
QDRD, respectively. Based on the results, we analyze and
compare the computational complexity and the throughput of
the two methods. Potential applications are also discussed.
A. Experimental Platform
The embedded platform is equipped with an ultra-low-power
16-bit RISC MSP430F2274 microprocessor. The system has a
supply voltage of 3.3 V and an operation frequency of 1 MHz.
The computational resources in the hardware include a customized 16-bit multiplier and an arithmetic logical unit (ALU)
that contains a 16-bit full adder. The microprocessor has a 256
Kb local register file with external access to a 1 KB RAM and
a 32 KB Flash memory. Overall, 27 instructions and seven addressing modes are supported.
B. Computational Complexity Analysis
To simplify the analysis, we approximate the computational
complexity by normalizing different operations to that of a 1-bit
addition. Note that the complexity here refers to the hardware
cost and indicates the energy consumption rather than the speed
of the operation. Specifically, our analysis assumes that a -bit
addition has a complexity of and a -bit multiplication has
one of . Since only adder and multiplier are available, a -bit
, assuming the
division is given a complexity of
by
Newton-Raphson method is used to approximate
in iterations with a given
computing
initial value . Similarly, a -bit square-root operation has a
assuming
is apcomplexity of
using the same
proximated by computing
method.
Based on the above models, Table I summarizes the overall
computational complexity of solving a LS problem using
GS-QRD and QDRD, respectively, on the embedded platform.
Both methods have the same operation count in terms of linear
additions and
operations and require
multiplications for computing the matrix factorization shown
additions
in (3) and (6), respectively. In addition,
and multiplications are involved in calculating the backward
substitution shown in (4) and (16). However, QDRD cuts down
significantly on the nonlinear operations required. Solving a
LS problem using GS-QRD requires square-root operations
divisions, while only divisions with no square-root
and

76

IEEE EMBEDDED SYSTEMS LETTERS, VOL. 6, NO. 4, DECEMBER 2014

TABLE I
THE COMPUTATIONAL COMPLEXITY OF SOLVING LS PROBLEMS

TABLE II
THE EXECUTION TIME OF SOLVING A SINGLE LS PROBLEM

D. Potential Applications

Fig. 3. Computational complexity of solving LS problems using QDRD with
different matrix sizes. The result is normalized to that of using GS-QRD assuming different iterations of the Newton–Raphson method.

operations are necessary for QDRD. Therefore, QDRD always
has a lower computational complexity than QRD. Moreover,
since the complexity also reflects the total energy consumption
of the hardware platform, QDRD is more energy-efficient for
solving LS problems.
Fig. 3 shows the computational complexity of solving LS
problems of different sizes by using QDRD in the case of
and
. The complexity is normalized to that of GS-QRD
for comparison. Note that when higher precision is required,
more iterations of the Newton-Raphson method have to be
performed in GS-QRD. In contrast, QDRD is free of such precision loss, thereby leading to a larger complexity reduction as
increases. In addition, complexity reduction for QDRD becomes more prominent for small matrices, where the overall
complexity is less dominated by additions and multiplications.
C. Throughput Evaluation from Hardware Emulation
To investigate the algorithm performance in practice, we
implement both algorithms in C language and compile them
with the IAR Embedded Workbench tool [6] on MSP430F2274
for emulation. In the compiler settings, the optimization level
is set to high for both the C codes (function in-lining, instruction scheduling, common subexpression elimination, etc.,
are performed). Due to the loop-carried data dependency in
both algorithms, the computation in different loops cannot be
pipelined. Consequently, the hardware throughput is inversely
proportional to the execution time of the algorithm. Table II
summarizes the measured execution time of solving a single
and
. In
LS problem at different sizes when
this case, the number of clock cycles required to execute an
addition, multiplication, division, and square-root operation is
4, 6, 128, and 1056, respectively. Clearly, executing nonlinear
operations is orders of magnitude slower, and such overhead
degrades the performance. As shown in Table II, by eliminating
the square-root operations and alleviating divisions, QDRD is
able to provide constantly higher throughput than GS-QRD in
solving LS problems. Particularly, a speedup of 6.5 times can
.
be achieved for small-size problems

Many real-life applications of LS problems can benefit
from using QDRD, especially when implemented on resourceor energy-limited platforms. Multiple-input–multiple-output
(MIMO) communication is one of such applications, where a
small-size LS problem (depending on the antenna count) needs
to be solved for signal detection [1]. As the MIMO technique
has been adopted in modern wireless standards, using QDRD
can benefit the low-power communication systems. For in, up to
stance, in an 4-antenna MIMO system
4 times better energy-efficiency and higher throughput can be
achieved compared to GS-QRD (see Fig. 3 and Table II). Other
application examples include the real-time curve-fitting used
in control systems, such as motion controllers [7]. In these
applications, the size of LS problems is limited by the number
of fitting parameters and data samples. Adopting QDRD can
potentially improve the battery life and real-time performance
of such applications.
V. CONCLUSION
Square-root operations required in QRD are intrinsically redundant for solving LS problems. To enable efficient LS computation on embedded systems for real-time applications, this
paper presents an alternative QDRD method that relaxes the
system requirements to achieve high throughput. Simulation results confirm that QDRD provides more accurate and reliable
results than GS-QRD for general matrix factorizations. Furthermore, benchmarking results based on an MSP430 microprocessor testbed show that, due to complexity reduction of nonlinear operations, QDRD provides constantly better energy-efficiency and higher throughput than GS-QRD in solving LS problems. More specifically, up to 4 and 6.5 times improvement in
energy efficiency and throughput, respectively, can be achieved
for small-size problems.

REFERENCES
[1] Y.-H. Zheng et al., “Efficient implementation of QR decomposition
for gigabit MIMO-OFDM systems,” IEEE Trans. Circuits Syst. I, Reg.
Papers, vol. 58, no. 10, pp. 2531–2542, Oct. 2011.
[2] P. Luethi et al., “Gram-Schmidt-Based QR decomposition for MIMO
detection: VLSI implementation and comparison,” in Proc. IEEE Asia
Pacific Conf. Circuits Syst., Nov. 2008, pp. 830–833.
[3] S. Aslan, “Realization of area efficient QR factorization using unified division, square-root, and inverse square-root hardware,” in Proc.
IEEE Int. Conf. Electr. Inf. Technol., Jun. 2009, pp. 245–250.
[4] B. Gestner, “VLSI implementation of a lattice reduction algorithm for
low-complexity equalization,” in Proc. IEEE Int. Conf. Circuits Syst.
Commun., May 2008, pp. 643–647.
[5] Å. Björck, “Numerical methods for least square problems,” SIAM, p.
62, 1996.
[6] “IAR Embedded Workbench for TI MSP430,” [Online]. Available: http://www.iar.com/Products/IAR-Embedded-Workbench/TIMSP430/
[7] J.-B. Wang et al., “Universal Real-Time NURBS interpolator on a
PC-Based Controller,” Int. J. Adv. Manufacturing Technol., vol. 71,
no. 1–4, pp. 297–507, Mar. 2014.

