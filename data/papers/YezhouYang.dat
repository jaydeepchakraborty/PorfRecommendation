Learning the Semantics of Manipulation Action
Yezhou Yang† and Yiannis Aloimonos† and Cornelia Fermüller† and Eren Erdal Aksoy‡
†
UMIACS, University of Maryland, College Park, MD, USA
{yzyang, yiannis, fer}@umiacs.umd.edu
‡
Karlsruhe Institute of Technology, Karlsruhe, Germany
eren.aksoy@kit.edu

arXiv:1512.01525v1 [cs.RO] 4 Dec 2015

Abstract
In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with
a robotic mechanism (e.g. a humanoid
robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs λ-calculus; (2) enable a probabilistic semantic parsing schema to learn the
λ-calculus representation of manipulation
action from an annotated action corpus of
videos; (3) use (1) and (2) to develop a
system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom
schemata. The experiments conducted on
a public available large manipulation action dataset validate the theoretical framework and our implementation.

1

Introduction

Autonomous robots will need to learn the actions
that humans perform. They will need to recognize
these actions when they see them and they will
need to perform these actions themselves. This requires a formal system to represent the action semantics. This representation needs to store the semantic information about the actions, be encoded
in a machine readable language, and inherently be
in a programmable fashion in order to enable reasoning beyond observation. A formal representation of this kind has a variety of other applications such as intelligent manufacturing, human

robot collaboration, action planning and policy design, etc.
In this paper, we are concerned with manipulation actions, that is actions performed by agents
(humans or robots) on objects, resulting in some
physical change of the object. However most of
the current AI systems require manually defined
semantic rules. In this work, we propose a computational linguistics framework, which is based
on probabilistic semantic parsing with Combinatory Categorial Grammar (CCG), to learn manipulation action semantics (lexicon entries) from annotations. We later show that this learned lexicon
is able to make our system reason about manipulation action goals beyond just observation. Thus
the intelligent system can not only imitate human
movements, but also imitate action goals.
Understanding actions by observation and executing them are generally considered as dual problems for intelligent agents. The sensori-motor
bridge connecting the two tasks is essential, and
a great amount of attention in AI, Robotics as well
as Neurophysiology has been devoted to investigating it. Experiments conducted on primates have
discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti
et al., 2001; Gazzola et al., 2007). This suggests
that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able
to first build up a semantic structure from observations, and then the decomposition of that same
structure should occur when the intelligent agent
executes commands.
Additionally, studies in linguistics (Steedman,
2002) suggest that the language faculty develops
in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in
the world by composing affordances of tools and
consequences of actions. It is this more primitive

apparatus that is our major interest in this paper.
Such an apparatus is composed of a “syntax part”
and a “semantic part”. In the syntax part, every linguistic element is categorized as either a function
or a basic type, and is associated with a syntactic
category which either identifies it as a function or a
basic type. In the semantic part, a semantic translation is attached following the syntactic category
explicitly.
Combinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can
be used to represent such structures with a small
set of combinators such as functional application
and type-raising. What do we gain though from
such a formal description of action? This is similar to asking what one gains from a formal description of language as a generative system. Chomskys contribution to language research was exactly
this: the formal description of language through
the formulation of the Generative and Transformational Grammar (Chomsky, 1957). It revolutionized language research opening up new roads for
the computational analysis of language, providing researchers with common, generative language
structures and syntactic operations, on which language analysis tools were built. A grammar for
action would contribute to providing a common
framework of the syntax and semantics of action,
so that basic tools for action understanding can be
built, tools that researchers can use when developing action interpretation systems, without having
to start development from scratch. The same tools
can be used by robots to execute actions.
In this paper, we propose an approach for learning the semantic meaning of manipulation action
through a probabilistic semantic parsing framework based on CCG theory. For example, we want
to learn from an annotated training action corpus
that the action “Cut” is a function which has two
arguments: a subject and a patient. Also, the action consequence of “Cut” is a separation of the
patient. Using formal logic representation, our
system will learn the semantic representations of
“Cut”:
Cut :=(AP \N P )/N P : λx.λy.cut(x, y) → divided(y)

Here cut(x, y) is a primitive function. We will further introduce the representation in Sec. 3. Since
our action representation is in a common calculus
form, it enables naturally further logical reasoning
beyond visual observation.

The advantage of our approach is twofold: 1)
Learning semantic representations from annotations helps an intelligent agent to enrich automatically its own knowledge about actions; 2) The
formal logic representation of the action could be
used to infer the object-wise consequence after a
certain manipulation, and can also be used to plan
a set of actions to reach a certain action goal. We
further validate our approach on a large publicly
available manipulation action dataset (MANIAC)
from (Aksoy et al., 2014), achieving promising experimental results. Moreover, we believe that our
work, even though it only considers the domain of
manipulation actions, is also a promising example
of a more closely intertwined computer vision and
computational linguistics system. The diagram in
Fig.1 depicts the framework of the system.

Figure 1: A CCG based semantic parsing framework for manipulation actions.

2

Related Works

Reasoning beyond appearance: The very small
number of works in computer vision, which aim to
reason beyond appearance models, are also related
to this paper. (Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques,
we could possibly infer implicit information (such
as functional objects) from video, and they call
them “Dark Matter” and “Dark Energy”. (Yang
et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance. (Joo et al., 2014)
used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who
captured an image. More recently, (Pirsiavash et
al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in

a large corpus using natural language processing
techniques. Different from these fairly general investigations about reasoning beyond appearance,
our paper seeks to learn manipulation actions semantics in logic forms through CCG, and further
infer hidden action consequences beyond appearance through reasoning.
Action Recognition and Understanding: Human activity recognition and understanding has
been studied heavily in Computer Vision recently,
and there is a large range of applications for this
work in areas like human-computer interactions,
biometrics, and video surveillance. Both visual
recognition methods, and the non-visual description methods using motion capture systems have
been used. A few good surveys of the former can
be found in (Moeslund et al., 2006) and (Turaga et
al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz
and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such
as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual
frames e.g. (Saisan et al., 2001; Chaudhry et al.,
2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain
(SEC) representation to model and learn the semantic segment-wise relationship transition from
spatial-temporal video segmentation.
There also have been many syntactic approaches to human activity recognition which used
the concept of context-free grammars, because
such grammars provide a sound theoretical basis
for modeling structured processes. Tracing back
to the middle 90’s, (Brand, 1996) used a grammar
to recognize disassembly tasks that contain hand
manipulations. (Ryoo and Aggarwal, 2006) used
the context-free grammar formalism to recognize
composite human activities and multi-person interactions. It is a two level hierarchical approach
where the lower-levels are composed of HMMs
and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with
errors from low-level processes such as tracking,
stochastic grammars such as stochastic CFGs were
also used (Ivanov and Bobick, 2000; Moore and
Essa, 2002). More recently, (Kuehne et al., 2014)
proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works

proved that grammar based approaches are practical in activity recognition systems, and shed
insight onto human manipulation action understanding. However, as mentioned, thinking about
manipulation actions solely from the viewpoint
of recognition has obvious limitations. In this
work we adopt principles from CFG based activity recognition systems, with extensions to a CCG
grammar that accommodates not only the hierarchical structure of human activity but also action
semantics representations. It enables the system
to serve as the core parsing engine for both manipulation action recognition and execution.
Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a
minimalist generative grammar, similar to the one
of human language, also exists for action understanding and execution. The works closest related
to this paper are (Pastra and Aloimonos, 2012;
Summers-Stay et al., 2013; Guha et al., 2013).
(Pastra and Aloimonos, 2012) first discussed a
Chomskyan grammar for understanding complex
actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of
such a grammar using as perceptual input only
objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al.,
2015) applied it on unconstrained instructional
videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction.
Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing
originally was used mainly to translate natural
language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins,
2007). (Mooney, 2008) presented a framework
of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and
actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek
et al., 2011), forklift operation (Tellex et al., 2014)
and of human-robot interaction (Matuszek et al.,
2014). In this work, instead of grounding natural
language sentences directly, we ground information obtained from visual perception into seman-

tically informed structures, specifically in the domain of manipulation actions.

3

A CCG Framework for Manipulation
Actions

Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete
background reading, we would like to refer readers to (Steedman, 2000). We will first give a brief
introduction to CCG and then introduce a fundamental combinator, i.e., functional application.
The introduction is followed by examples to show
how the combinator is applied to parse actions.
3.1

Manipulation Action Semantics

The semantic expression in our representation of
manipulation actions uses a typed λ-calculus language. The formal system has two basic types:
entities and functions. Entities in manipulation
actions are Objects or Hands, and functions are
the Actions. Our lambda-calculus expressions are
formed from the following items:
Constants: Constants can be either entities or
functions. For example, Knife is an entity (i.e., it
is of type N) and Cucumber is an entity too (i.e., it
is of type N). Cut is an action function that maps
entities to entities. When the event Knife Cut Cucumber happened, the expression cut(Knife, Cucumber) returns an entity of type AP, aka. Action
Phrase. Constants like divided are status functions
that map entities to truth values. The expression
divided(cucumber) returns a true value after the
event (Knife Cut Cucumber) happened.
Logical connectors: The λ-calculus expression
has logical connectors like conjunction (∧), disjunction (∨), negation(¬) and implication(→).
For example, the expression
connected(tomato, cucumber)∧
divided(tomato) ∧ divided(cucumber)

represents the joint status that the sliced tomato
merged with the sliced cucumber. It can be
regarded as a simplified goal status for “making a cucumber tomato salad”. The expression
¬connected(spoon, bowl) represents the status
after the spoon finished stirring the bowl.
λx.cut(x, cucumber) → divided(cucumber)

represents that if the cucumber is cut by x, then
the status of the cucumber is divided.
λ expressions: lambda expressions represent
functions with unknown arguments. For example,
λx.cut(knif e, x) is a function from entities to entities, which is of type NP after any entities of type
N that is cut by knife.
3.2

Combinatory Categorial Grammar

The semantic parsing formalism underlying our
framework for manipulation actions is that of
combinatory categorial grammar (CCG) (Steedman, 2000). A CCG specifies one or more logical forms for each element or combination of elements for manipulation actions. In our formalism,
an element of Action is associated with a syntactic “category” which identifies it as functions, and
specifies the type and directionality of their arguments and the type of their result. For example, action “Cut” is a function from patient object phrase
(NP) on the right into predicates, and into functions from subject object phrase (NP) on the left
into a sub action phrase (AP):
Cut := (AP \N P )/N P.

(1)

As a matter of fact, the pure categorial grammar is a conext-free grammar presented in the accepting, rather than the producing direction. The
expression (1) is just an accepting form for Action “Cut” following the context-free grammar.
While it is now convenient to write derivations as
follows, they are equivalent to conventional tree
structure derivations in Figure. 3.2.
Knife
N
NP

Cut

Cucumber
N
NP

(AP \NP )/NP

AP \NP

>

<

AP
AP

AP

NP
N

A

Knife Cut

NP
N
Cucumber

Figure 2: Example of conventional tree structure.
The semantic type is encoded in these categories, and their translation can be made explicit

in an expanded notation. Basically a λ-calculus
expression is attached with the syntactic category.
A colon operator is used to separate syntactical
and semantic expressions, and the right side of the
colon is assumed to have lower precedence than
the left side of the colon. Which is intuitive as any
explanation of manipulation actions should first
obey syntactical rules, then semantic rules. Now
the basic element, Action “Cut”, can be further
represented by:
Cut :=(AP \N P )/N P : λx.λy.cut(x, y) → divided(y).

(AP \N P )/N P denotes a phrase of type AP ,
which requires an element of type N P to specify
what object was cut, and requires another element
of type N P to further complement what effector
initiates the cut action. λx.λy.cut(x, y) is the λcalculus representation for this function. Since the
functions are closely related to the state update,
→ divided(y) further points out the status expression after the action was performed.
A CCG system has a set of combinatory rules
which describe how adjacent syntatic categories
in a string can be recursively combined. In the
setting of manipulation actions, we want to point
out that similar combinatory rules are also applicable. Especially the functional application rules
are essential in our system.
3.3

Functional application

The functional application rules with semantics
can be expressed in the following form:
A/B : f B : g => A : f (g)

(2)

B : g A\B : f => A : f (g)

(3)

Rule. (2) says that a string with type A/B can be
combined with a right-adjacent string of type B to
form a new string of type A. At the same time, it
also specifies how the semantics of the category A
can be compositionally built out of the semantics
for A/B and B. Rule. (3) is a symmetric form of
Rule. (2).
In the domain of manipulation actions, following derivation is an example CCG parse. This
parse shows how the system can parse an observation (“Knife Cut Cucumber”) into a semantic representation (cut(knif e, cucumber) →
divided(cucumber)) using the functional application rules.

Knife
N
NP
knife
knife

Cut
(AP \NP )/NP
λx .λy.cut(x , y)
→ divided (y)

Cucumber
N
NP
cucumber
cucumber

AP \NP
λx .cut(x , cucumber )
→ divided (cucumber )
<
AP
cut(knife, cucumber )
→ divided (cucumber )

4

>

Learning Model and Semantic Parsing

After having defined the formalism and application rule, instead of manually writing down all the
possible CCG representations for each entity, we
would like to apply a learning technique to derive them from the paired training corpus. Here
we adopt the learning model of (Zettlemoyer and
Collins, 2005), and use it to assign weights to the
semantic representation of actions. Since an action may have multiple possible syntactic and semantic representations assigned to it, we use the
probabilistic model to assign weights to these representations.
4.1

Learning Approach

First we assume that complete syntactic parses of
the observed action are available, and in fact a manipulation action can have several different parses.
The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one
given by (Zettlemoyer and Collins, 2007). We assume a probabilistic categorial grammar (PCCG)
based on a log linear model. M denotes a manipulation task, L denotes the semantic representation
of the task, and T denotes its parse tree. The probability of a particular syntactic and semantic parse
is given as:
ef (L,T,M )·Θ
f (L,T,M )·Θ
(L,T ) e

P (L, T |M ; Θ) = P

(4)

where f is a mapping of the triple (L, T, M ) to
feature vectors ∈ Rd , and the Θ ∈ Rd represents
the weights to be learned. Here we use only lexical features, where each feature counts the number
of times a lexical entry is used in T . Parsing a manipulation task under PCCG equates to finding L
such that P (L|M ; Θ) is maximized:
argmaxL P (L|M ; Θ)
X
= argmaxL
P (L, T |M ; Θ).
T

(5)

We use dynamic programming techniques to
calculate the most probable parse for the manipulation task. In this paper, the implementation from
(Baral et al., 2011) is adopted, where an inverse-λ
technique is used to generalize new semantic representations. The generalization of lexicon rules
are essential for our system to deal with unknown
actions presented during the testing phase.

5

Experiments

5.1

Manipulation Action (MANIAC) Dataset

(Aksoy et al., 2014) provides a manipulation action dataset with 8 different manipulation actions
(cutting, chopping, stirring, putting, taking, hiding, uncovering, and pushing), each of which consists of 15 different versions performed by 5 different human actors1 . There are in total 30 different objects manipulated in all demonstrations. All
manipulations were recorded with the Microsoft
Kinect sensor and serve as training data here.
The MANIAC data set contains another 20 long
and complex chained manipulation sequences
(e.g. “making a sandwich”) which consist of a total of 103 different versions of these 8 manipulation tasks performed in different orders with novel
objects under different circumstances. These serve
as testing data for our experiments.
(Aksoy et al., 2014; Aksoy and Wörgötter,
2015) developed a semantic event chain based
model free decomposition approach. It is an unsupervised probabilistic method that measures the
frequency of the changes in the spatial relations
embedded in event chains, in order to extract the
subject and patient visual segments. It also decomposes the long chained complex testing actions
into their primitive action components according
to the spatio-temporal relations of the manipulator. Since the visual recognition is not the core
of this work, we omit the details here and refer
the interested reader to (Aksoy et al., 2014; Aksoy
and Wörgötter, 2015). All these features make the
MANIAC dataset a great testing bed for both the
theoretical framework and the implemented system presented in this work.

in the format of observed triplets (subject action
patient) and a corresponding semantic representation of the action as well as its consequence. The
semantic representations in λ-calculus format are
given by human annotators after watching each action clip. A set of sample training pairs are given
in Table.1 (one from each action category in the
training set). Since every training clip contains
one single full execution of each manipulation action considered, the training corpus thus has a total
of 120 paired training samples.
Snapshot

Training Corpus

We first created a training corpus by annotating
the 120 training clips from the MANIAC dataset,
1

Dataset available for download at https:
//fortknox.physik3.gwdg.de/cns/index.
php?page=maniac-dataset.

semantic representation

cleaver chopping carrot

chopping(cleaver, carrot)
→ divided(carrot)

spatula cutting pepper

cutting(spatula, pepper)
→ divided(pepper)

spoon stirring bucket

stirring(spoon, bucket)

cup take down bucket

take down(cup, bucket)
→ ¬connected(cup, bucket)
∧moved(cup)

cup put on top bowl

put on top(cup, bowl)
→ on top(cup, bowl)
∧moved(cup)

bucket hiding ball

hiding(bucket, ball)
→ contained(bucket, ball)
∧moved(bucket)

hand pushing box

pushing(hand, box)
→ moved(box)

box uncover apple

uncover(box, apple)
→ appear(apple)
∧moved(box)

Table 1: Example annotations from training corpus, one per manipulation action category.
We also assume the system knows that every
“object” involved in the corpus is an entity of its
own type, for example:
Knif e := N : knif e
Bowl := N : bowl
......

Additionally, we assume the syntactic form of
each “action” has a main type (AP \N P )/N P
(see Sec. 3.2). These two sets of rules form the
initial seed lexicon for learning.
5.3

5.2

triplet

Learned Lexicon

We applied the learning technique mentioned in
Sec. 4, and we used the NL2KR implementation from (Baral et al., 2011). The system learns
and generalizes a set of lexicon entries (syntactic
and semantic) for each action categories from the
training corpus accompanied with a set of weights.

We list the one with the largest weight for each action here respectively:
Chopping :=(AP \N P )/N P : λx.λy.chopping(x, y)
→ divided(y)
Cutting :=(AP \N P )/N P : λx.λy.cutting(x, y)
→ divided(y)
Stirring :=(AP \N P )/N P : λx.λy.stirring(x, y)
T ake down :=(AP \N P )/N P : λx.λy.take down(x, y)
→ ¬connected(x, y) ∧ moved(x)
P ut on top :=(AP \N P )/N P : λx.λy.put on top(x, y)
→ on top(x, y) ∧ moved(x)
Hiding :=(AP \N P )/N P : λx.λy.hiding(x, y)
→ contained(x, y) ∧ moved(x)
P ushing :=(AP \N P )/N P : λx.λy.pushing(x, y)
→ moved(y)
U ncover :=(AP \N P )/N P : λx.λy.uncover(x, y)
→ appear(y) ∧ moved(x).

The set of seed lexicon and the learned lexicon
entries are further used to probabilistically parse
the detected triplet sequences from the 20 long
manipulation activities in the testing set.
5.4

Deducing Semantics

Using the decomposition technique from (Aksoy
et al., 2014; Aksoy and Wörgötter, 2015), the reported system is able to detect a sequence of action triplets in the form of (Subject Action Patient) from each of the testing sequence in MANIAC dataset. Briefly speaking, the event chain
representation (Aksoy et al., 2011) of the observed
long manipulation activity is first scanned to estimate the main manipulator, i.e. the hand, and manipulated objects, e.g. knife, in the scene without
employing any visual feature-based object recognition method. Solely based on the interactions
between the hand and manipulated objects in the
scene, the event chain is partitioned into chunks.
These chunks are further fragmented into subunits to detect parallel action streams. Each parsed
Semantic Event Chain (SEC) chunk is then compared with the model SECs in the library to decide
whether the current SEC sample belongs to one
of the known manipulation models or represents a
novel manipulation. SEC models, stored in the library, are learned in an on-line unsupervised fashion using the semantics of manipulations derived
from a given set of training data in order to create
a large vocabulary of single atomic manipulations.
For the different testing sequence, the number
of triplets detected ranges from two to seven. In total, we are able to collect 90 testing detections and

they serve as the testing corpus. However, since
many of the objects used in the testing data are not
present in the training set, an object model-free approach is adopted and thus “subject” and “patient”
fields are filled with segment IDs instead of a specific object name. Fig. 3 and 4 show several examples of the detected triplets accompanied with a
set of key frames from the testing sequences. Nevertheless, the method we used here can 1) generalize the unknown segments into the category of
object entities and 2) generalize the unknown actions (those that do not exist in the training corpus)
into the category of action function. This is done
by automatically generalizing the following two
types of lexicon entries using the inverse-λ technique from (Baral et al., 2011):
Object [ID] :=N : object [ID]
U nknown :=(AP \N P )/N P : λx.λy.unknown(x, y)

Among the 90 detected triplets, using the
learned lexicon we are able to parse all of them
into semantic representations. Here we pick the
representation with the highest probability after
parsing as the individual action semantic representation. The “parsed semantics” rows of Fig. 3 and
4 show several example action semantics on testing sequences. Taking the fourth sub-action from
Fig. 4 as an example, the visually detected triplets
based on segmentation and spatial decomposition
is (Object 014, Chopping, Object 011). After semantic parsing, the system predicts that
divided(Object 011). The complete training corpus and parsed results of the testing set will be
made publicly available for future research.
5.5

Reasoning Beyond Observations

As mentioned before, because of the use of λcalculus for representing action semantics, the obtained data can naturally be used to do logical reasoning beyond observations. This by itself is a
very interesting research topic and it is beyond this
paper’s scope. However by applying a couple of
common sense Axioms on the testing data, we can
provide some flavor of this idea.
Case study one: See the “final action consequence and reasoning” row of Fig. 3 for case one.
Using propositional logic and axiom schema, we
can represent the common sense statement (“if an
object x is contained in object y, and object z is
on top of object y, then object z is on top of object
x”) as follows:

Figure 3: System output on complex chained manipulation testing sequence one. The segmentation
output and detected triplets are from (Aksoy and Wörgötter, 2015)
.

Figure 4: System output on the 18th complex chained manipulation testing sequence. The segmentation
output and detected triplets are from (Aksoy and Wörgötter, 2015)
.
Axiom (1):
∃x, y, z, contained(y, x) ∧
on top(z, y) → on top(z, x).
Then it is trivial to deduce an additional final action consequence in this scenario that
(on top(object 007, object 009)). This matches
the fact: the yellow box which is put on top of the
red bucket is also on top of the black ball.
Case study two: See the “final action consequence and reasoning” row of Fig. 4 for a more
complicated case. Using propositional logic and
axiom schema, we can represent three common
sense statements:
1) “if an object y is contained in object x, and
object z is contained in object y, then object z is
contained in object x”;
2) “if an object x is contained in object y, and
object y is divided, then object x is divided”;
3) “if an object x is contained in object y, and
object y is on top of object z, then object x is on
top of object z” as follows:

Axiom (2):
∃x, y, z, contained(y, x) ∧
contained(z, y) → contained(z, x).
Axiom (3):
∃x, y, contained(y, x) ∧
divided(y) → divided(x).
Axiom (4):
∃x, y, z, contained(y, x) ∧
on top(y, z) → on top(x, z).
With these common sense Axioms, the system
is able to deduce several additional final action
consequences in this scenario:
divided(object 005) ∧ divided(object 010)
∧ on top(object 005, object 012)
∧ on top(object 010, object 012).

From Fig. 4, we can see that these additional
consequences indeed match the facts: 1) the bread
and cheese which are covered by ham are also divided, even though from observation the system
only detected the ham being cut; 2) the divided
bread and cheese are also on top of the plate, even
though from observation the system only detected
the ham being put on top of the plate.

We applied the four Axioms on the 20 testing
action sequences and deduced the “hidden” consequences from observation. To evaluate our system
performance quantitatively, we first annotated all
the final action consequences (both obvious and
“hidden” ones) from the 20 testing sequences as
ground-truth facts. In total there are 122 consequences annotated. Using perception only (Aksoy
and Wörgötter, 2015), due to the decomposition
errors (such as the red font ones in Fig. 4) the system can detect 91 consequences correctly, yielding
a 74% correct rate. After applying the four Axioms and reasoning, our system is able to detect
105 consequences correctly, yielding a 86% correct rate. Overall, this is a 15.4% of improvement.
Here we want to mention a caveat: there are definitely other common sense Axioms that we are
not able to address in the current implementation.
However, from the case studies presented, we can
see that using the presented formal framework, our
system is able to reason about manipulation action
goals instead of just observing what is happening
visually. This capability is essential for intelligent
agents to imitate action goals from observation.

scenarios, based on whether the action is transitive or intransitive, the main types of action can be
extended to include AP \N P .
Moreover, the logical expressions can also be
extended to include universal quantification ∀ and
existential quantification ∃. Thus, manipulation
action such as “knife cut every tomato” can be
parsed into a representation as ∀x.tomato(x) ∧
cut(knif e, x) → divided(x) (the parse is given
in the following chart). Here, the concept “every”
has a main type of N P \N P and semantic meaning of ∀x.f (x). The same framework can also
extended to have other combinatory rules such as
composition and type-raising (Steedman, 2002).
These are parts of the future work along the line of
the presented work.

6

The presented computational linguistic framework enables an intelligent agent to predict and
reason action goals from observation, and thus has
many potential applications such as human intention prediction, robot action policy planning, human robot collaboration etc. We believe that our
formalism of manipulation actions bridges computational linguistics, vision and robotics, and
opens further research in Artificial Intelligence
and Robotics. As the robotics industry is moving
towards robots that function safely, effectively and
autonomously to perform tasks in real-world unstructured environments, they will need to be able
to understand the meaning of actions and acquire
human-like common-sense reasoning capabilities.

Conclusion and Future Work

In this paper we presented a formal computational framework for modeling manipulation actions based on a Combinatory Categorial Grammar. An empirical study on a large manipulation action dataset validates that 1) with the introduced formalism, a learning system can be devised
to deduce the semantic meaning of manipulation
actions in λ-schema; 2) with the learned schema
and several common sense Axioms, our system is
able to reason beyond just observation and deduce
“hidden” action consequences, yielding a decent
performance improvement.
Due to the limitation of current testing scenarios, we conducted experiments only considering a
relatively small set of seed lexicon rules and logical expressions. Nevertheless, we want to mention that the presented CCG framework can also
be extended to learn the formal logic representation of more complex manipulation action semantics. For example, the temporal order of manipulation actions can be modeled by considering a seed
rule such as AP \AP : λf.λg.bef ore(f (·), g(·)),
where bef ore(·, ·) is a temporal predicate. For
actions in this paper we consider seed main type
(AP \N P )/N P . For more general manipulation

Knife
N
NP
knife
knife

Cut

every

(AP \NP )/NP
λx .λy.cut(x , y)
→ divided (y)

N P \N P
∀x .f (x )
∀x .f (x )

Tomato
N
NP
tomato
tomato

NP
∀x .tomato(x )

>

>

AP \NP
∀y.λx .tomato(y) ∧ cut(x , y) → divided (y)
<
AP
∀y.tomato(y) ∧ cut(knife, y) → divided (y)

7

Acknowledgements

This research was funded in part by the support of the European Union under the Cognitive Systems program (project POETICON++),
the National Science Foundation under INSPIRE
grant SMA 1248056, and by DARPA through
U.S. Army grant W911NF-14-1-0384 under the
Project: Shared Perception, Cognition and Reasoning for Autonomy.

References
E E. Aksoy and F. Wörgötter. 2015. Semantic decomposition and recognition of long and complex manipulation action sequences. International Journal
of Computer Vision, page Under Review.
E.E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen,
and F. Wörgötter. 2011. Learning the semantics of
object–action relations by observation. The International Journal of Robotics Research, 30(10):1229–
1249.

Jungseock Joo, Weixin Li, Francis F Steen, and SongChun Zhu. 2014. Visual persuasion: Inferring communicative intents of images. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 216–223. IEEE.
A. Kale, A. Sundaresan, AN Rajagopalan, N.P. Cuntoor, A.K. Roy-Chowdhury, V. Kruger, and R. Chellappa. 2004. Identification of humans using
gait. IEEE Transactions on Image Processing,
13(9):1163–1173.

E E. Aksoy, M. Tamosiunaite, and F. Wörgötter. 2014.
Model-free incremental learning of the semantics
of manipulation actions. Robotics and Autonomous
Systems, pages 1–42.

Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014.
The language of actions: Recovering the syntax
and semantics of goal-directed human activities. In
Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 780–787. IEEE.

Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Jiayu Zhou. 2011. Using inverse λ and generalization to translate english to formal languages. In
Proceedings of the Ninth International Conference
on Computational Semantics, pages 35–44. Association for Computational Linguistics.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2011. A joint
model of language and perception for grounded attribute learning. In International Conference on Machine learning (ICML).

Jezekiel Ben-Arie, Zhiqian Wang, Purvin Pandit, and
Shyamsundar Rajaram.
2002.
Human activity recognition using multidimensional indexing.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 24(8):1091–1104.

Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and
Dieter Fox. 2014. Learning from unscripted deictic
gesture and language for human-robot interactions.
In Twenty-Eighth AAAI Conference on Artificial Intelligence.

Matthew Brand. 1996. Understanding manipulation in
video. In Proceedings of the Second International
Conference on Automatic Face and Gesture Recognition, pages 94–99, Killington,VT. IEEE.

T.B. Moeslund, A. Hilton, and V. Krüger. 2006. A
survey of advances in vision-based human motion
capture and analysis. Computer vision and image
understanding, 104(2):90–126.

R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal.
2009. Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for
the recognition of human actions. In Proceedings
of the 2009 IEEE Intenational Conference on Computer Vision and Pattern Recognition, pages 1932–
1939, Miami,FL. IEEE.

Raymond J Mooney. 2008. Learning to connect language and perception. In AAAI, pages 1598–1601.

N. Chomsky. 1957. Syntactic Structures. Mouton de
Gruyter.
Noam Chomsky. 1993. Lectures on government and
binding: The Pisa lectures. Walter de Gruyter.
V Gazzola, G Rizzolatti, B Wicker, and C Keysers.
2007. The anthropomorphic brain: the mirror neuron system responds to human and robotic actions.
Neuroimage, 35(4):1674–1684.
Anupam Guha, Yezhou Yang, Cornelia Fermüller, and
Yiannis Aloimonos. 2013. Minimalist plans for interpreting manipulation actions. Proceedings of the
2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5908–5914.
Yuri A. Ivanov and Aaron F. Bobick. 2000. Recognition of visual activities and interactions by stochastic
parsing. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(8):852–872.

Darnell Moore and Irfan Essa. 2002. Recognizing
multitasked activities from video using stochastic
context-free grammar. In Proceedings of the National Conference on Artificial Intelligence, pages
770–776, Menlo Park, CA. AAAI.
K. Pastra and Y. Aloimonos. 2012. The minimalist grammar of action. Philosophical Transactions of the Royal Society B: Biological Sciences,
367(1585):103–117.
Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. 2014. Inferring the why in images. arXiv
preprint arXiv:1406.5472.
Giacomo Rizzolatti, Leonardo Fogassi, and Vittorio
Gallese. 2001. Neurophysiological mechanisms underlying the understanding and imitation of action.
Nature Reviews Neuroscience, 2(9):661–670.
Michael S Ryoo and Jake K Aggarwal. 2006. Recognition of composite human activities through contextfree grammar based representation. In Proceedings
of the 2006 IEEE Conference on Computer Vision
and Pattern Recognition, volume 2, pages 1709–
1718, New York City, NY. IEEE.

P. Saisan, G. Doretto, Y.N. Wu, and S. Soatto. 2001.
Dynamic texture recognition. In Proceedings of the
2001 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 2, pages
58–63, Kauai, HI. IEEE.
Mark Steedman. 2000. The syntactic process, volume 35. MIT Press.
Mark Steedman. 2002. Plans, affordances, and combinatory grammar. Linguistics and Philosophy, 25(56):723–753.
D. Summers-Stay, C.L. Teo, Y. Yang, C. Fermüller,
and Y. Aloimonos. 2013. Using a minimal action grammar for activity understanding in the real
world. In Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4104–4111, Vilamoura, Portugal. IEEE.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy.
2014.
Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning, 94(2):151–167.
P. Turaga, R. Chellappa, V.S. Subrahmanian, and
O. Udrea. 2008. Machine recognition of human activities: A survey. IEEE Transactions on Circuits
and Systems for Video Technology, 18(11):1473–
1488.
Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. 2013.
Inferring “dark matter” and “dark energy” from
videos. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2224–2231. IEEE.
Yezhou Yang, Cornelia Fermüller, and Yiannis Aloimonos. 2013. Detection of manipulation action
consequences (MAC). In Proceedings of the 2013
IEEE Conference on Computer Vision and Pattern
Recognition, pages 2563–2570, Portland, OR. IEEE.
Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos.
2014. A cognitive system for understanding human manipulation actions. Advances in Cognitive
Sysytems, 3:67–86.
Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis
Aloimonos. 2015. Robot learning manipulation action plans by “watching” unconstrained videos from
the world wide web. In The Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI-15).
A. Yilmaz and M. Shah. 2005. Actions sketch: A
novel action representation. In Proceedings of the
2005 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 1, pages
984–989, San Diego, CA. IEEE.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Structured classification with probabilistic categorial
grammars. In UAI.
Luke S Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678–687.

LightNet: A Versatile, Standalone Matlab-based
Environment for Deep Learning
[Simplify Deep Learning in Hundreds of Lines of Code]

arXiv:1605.02766v3 [cs.LG] 2 Aug 2016

Chengxi Ye, Chen Zhao*, Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos
Computer Science Department, University of Maryland, College Park, MD 20740, USA.
{cxy, yzyang, fer, yiannis}@umiacs.umd.edu *chenzhao@umd.edu
ABSTRACT
LightNet is a lightweight, versatile, purely Matlabbased deep learning framework. The idea underlying its
design is to provide an easy-to-understand, easy-to-use and
efficient computational platform for deep learning research.
The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks
(MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports
both CPU and GPU computation, and the switch between
them is straightforward. Different applications in computer
vision, natural language processing and robotics are demonstrated as experiments.
Availability: the source code and data is available at:
https://github.com/yechengxi/LightNet

Categories and Subject Descriptors
D.0 [Software]: General; I.2.10 [Artificial Intelligence]:
Vision and Scene Understanding

Keywords
Computer vision; natural language processing; image understanding; machine learning; deep learning; convolutional
neural networks; multilayer perceptrons; recurrent neural
networks; reinforcement learning

1.

INTRODUCTION

Deep neural networks [8] have given rise to major advancements in many problems of machine intelligence. Most
current implementations of neural network models primarily
emphasize efficiency. These pipelines (Table 1) can consist
of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2]. It requires
extensive efforts to thoroughly understand and modify the
models. A straightforward and self-explanatory deep learning framework is highly anticipated to accelerate the understanding and application of deep neural network models.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

MM ’16 October 15-19, 2016, Amsterdam, Netherlands
c 2016 Copyright held by the owner/author(s).

ACM ISBN 978-1-4503-3603-1/16/10.
DOI: http://dx.doi.org/10.1145/2964284.2973791

Table 1: Deep Neural Network Packages
Framework
Language
Native Models
Lines of Code
Caffe
C++
CNN
74,903
Theano
Python, C
MLP/CNN/RNN
148,817
Torch
Lua, C
MLP/CNN/RNN
458,650
TensorFlow
C++
MLP/CNN/RNN
335,669
Matconvnet
Matlab, C
CNN
43,087
LightNet
Matlab
MLP/CNN/RNN
951 (1,762)*
* Lines of code in the core modules and in the whole package.

We present LightNet, a lightweight, versatile, purely
Matlab-based implementation of modern deep neural network models. Succinct and efficient Matlab programming
techniques have been used to implement all the computational modules. Many popular types of neural networks,
such as multilayer perceptrons, convolutional neural networks, and recurrent neural networks are implemented in
LightNet, together with several variations of stochastic gradient descent (SDG) based optimization algorithms.
Since LightNet is implemented solely with Matlab, the
major computations are vectorized and implemented in hundreds of lines of code, orders of magnitude more succinct
than existing pipelines. All fundamental operations can be
easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can
focus on the mathematical modeling part rather than the
engineering part. Application oriented users can easily understand and modify any part of the framework to develop
new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following
features: 1. LightNet contains the most modern network
architectures. 2. Applications in computer vision, natural
language processing and reinforcement learning are demonstrated. 3. LightNet provides a comprehensive collection of
optimization algorithms. 4. LightNet supports straightforward switching between CPU and GPU computing. 5. Fast
Fourier transforms are used to efficiently compute convolutions, and thus large convolution kernels are supported. 6.
LightNet automates hyper-parameter tuning with a novel
Selective-SGD algorithm.

2.

USING THE PACKAGE

An example of using LightNet can be found in (Fig. 1):
a simple template is provided to start the training process.
The user is required to fill in some critical training parameters, such as the number of training epochs, or the training
method. A Selective-SGD algorithm is provided to facilitate
the selection of an optimal learning rate. The learning rate is

n_epoch=20; %training epochs
dataset_name='mnist'; %dataset name
network_name='cnn'; %network name
use_gpu=1; %use gpu or not

∂z
, which are the gradients that guide the gradient descent
∂b
process.

%function handle to prepare your data
PrepareDataFunc=@PrepareData_MNIST_CNN;
%function handle to initialize the network
NetInit=@net_init_cnn_mnist;
%automatically select learning rates
use_selective_sgd=1;
%select a new learning rate every n epochs
ssgd_search_freq=10;
learning_method=@sgd; %training method: @sgd

Main_Template(); %call training template

Figure 1: A basic example, which shows how to train
a CNN on the MNIST dataset with LightNet.
selected automatically, and can optionally be adjusted during the training. The framework supports both GPU and
CPU computation, through the opts.use gpu option. Two
additional functions are provided to prepare the training
data and initialize the network structure. Every experiment
in this paper can reproduced by running the related script
file. More details can be found on the project webpage.

3.

BUILDING BLOCKS

The primary computational module includes a feed forward process and a backward/back propagation process. The
feed forward process evaluates the model, and the back propagation reports the network gradients. Stochastic gradient
descent based algorithms are used to optimize the model
parameters.

3.1

Core Computational Modules

LightNet allows us to focus on the mathematical modeling
of the network, rather than low-level engineering details. To
make this paper self-contained, we explain the main computational modules of LightNet. All networks ( and related
experiments) in this paper are built with these modules. The
notations below are chosen for simplicity. Readers can easily
extend the derivations to the mini-batch setting.

3.1.1

∂z
∂z ∂y
=
·
= f 0 (y) · xT
∂W
∂y ∂W

(2)

∂z
∂z ∂y
=
·
= f 0 (y)
∂b
∂y ∂b

(3)

Convolutional Layer

A convolutional layer maps Nmap in input feature maps to
Nmap out output feature maps with a multidimensional filter
bank kio . Each input feature map xi is convolved with the
corresponding filter bank kio . The convolution results are
summed, and a bias
P value bo is added, to generate the o-th
output map: yo = 1≤i≤Nmap in kio ∗ xi +bo . To allow using
large convolution kernels, fast Fourier transforms (FFT) are
used for computing convolutions (and correlations). According to the convolution theorem [10], convolution in the spatial domain is equivalent to point-wise multiplication in the
frequency domain. Therefore, ki ∗ xi can be calculated using
the Fourier transform as: ki ∗ xi = F −1 {F{ki } · F {xi }}.
Here, F denotes the Fourier transform and · denotes the
point-wise multiplication operation. The convolution layer
supports both padding and striding.
The mapping from the o-th output feature map to the
network output can be expressed as: z = f (yo ). Here f is
the non-linear mapping from the o-th output feature map yo
∂z
to the final network output. As before (in Sec. 3.1.1), ∂x
,
i
∂z
∂z
need
to
be
calculated
in
the
backward
process,
,
and
∂ki
∂bo
as follows:
∂z
∂z ∂yo
=
·
= f 0 (yo ) ? ki ,
(4)
∂xi
∂yo ∂xi
where ? denotes the correlation operation. Denoting the
complex conjugate as conj, this correlation is calculated in
the frequency domain using the Fourier transform as: x?k =
F −1 {F{x} · conj(F{k})}.
∂z ∂yo
∂z
=
·
= f 0 (yo ) ? xi ,
∗
∗
∂kio
∂yo ∂kio

(5)

where k∗ represents the flipped kernel k. Thus, the gradient
∂z
is calculated by flipping the correlation output. Finally,
∂kio
∂z
∂z ∂yo
=
·
= 1T · vec(f 0 (yo ))
∂bo
∂yo ∂bo

Linear Perceptron Layer

A linear perceptron layer can be expressed as: y = W x+b.
Here, x denotes the input data of size input dim × 1, W
denotes the weight matrix of size output dim × input dim,
b is a bias vector of size output dim × 1, and y denotes the
linear layer output of size output dim × 1.
The mapping from the input of the linear perceptron to
the final network output can be expressed as: z = f (y) =
f (W x + b), where f is a non-linear function that represents
the network’s computation in the deeper layers, and z is the
network output, which is usually a loss value.
∂z
The backward process calculates the derivative ∂x
, which
∂z
is the derivative passing to the shallower layers, and ∂W
,

(1)

The module adopts extensively optimized Matlab matrix
operations to calculate the matrix-vector products.

3.1.2
%sgd parameter
%(unnecessary if selective-sgd is used)
%sgd_lr=5e-2;

∂z
∂z ∂y
=
·
= f 0 (y)T · W
∂x
∂y ∂x

(6)

∂z
In words, the gradient ∂b
can be calculated by point-wise
o
summation of the values in f 0 (yo ).

3.1.3

Max-pooling Layer

The max pooling layer calculates the largest element in
Pr × Pc windows, with stride size Sr × Sc . A customized
im2col ln function is implemented to convert the stridden
pooling patches into column vectors, to vectorize the pooling
computation in Matlab. The built-in max function is called
on these column vectors to return the pooling result and the
indices of these maximum values. Then, the indices in the

original batched data are recovered accordingly. Also, zero
padding can be applied to the input data.
Without the loss of generality, the mapping from the maxpooling layer input to the final network output can be expressed as: z = f (y) = f (Sx), where S is a selection matrix,
and x is a column vector which denotes the input data in
this layer.
∂z
is calculated and passed to
In the backward process, ∂x
∂z
∂z
the shallower layers: ∂x = ∂y · S = f 0 (y)T S.
When the pooling range is less than or equal to the stride
∂z
size, ∂x
can be calculated with simple matrix indexing techniques in Matlab. Specifically, an empty tensor dzdx of the
same size with the input data is created. dzdx(f rom) =
dzdy, where f rom is the pooling indices, and dzdy is a tensor recording the pooling results. When the pooling range
is larger than the stride size, each entry in x can be pooled
multiple times, and the back propagation gradients need to
be accumulated for each of these multiple-pooled entries. In
∂z
is calculated using the Matlab function:
this case, the ∂x
accumarray().

3.1.4

Optimization Algorithms

Stochastic gradient descent (SGD) algorithm based optimization algorithms are the primary tools to train deep
neural networks. The standard SGD algorithm and several
of its popular variants such as Adagrad [3], RMSProp [12]
and Adam [6] are also implemented for deep learning research. It is worth mentioning that we implement a novel
Selective-SGD algorithm to facilitate the selection of hyperparameters, especially the learning rate. This algorithm selects the most efficient learning rate by running the SGD
process for a few iterations using each learning rate from a
discrete candidate set. During the middle of the neural net
training, the Selective-SGD algorithm can also be applied to
select different learning rates to accelerate the energy decay.

4.
4.1

EXPERIMENTS
Multilayer Perceptron Network

A multilayer perceptron network is constructed to test
the performance of LightNet on MNIST data [9]. The network takes 28 × 28 inputs from the MNIST image dataset
and has 128 nodes respectively in the next two layers. The
128-dimensional features are then connected to 10 nodes to
calculate the softmax output. See Fig. 2 for the experiment
results.

4.2

Figure 2: Loss and error rates during training
and testing phases using LightNet on the MNIST
dataset.

(a)

(b)

Figure 3: Loss and error rates of training and testing
with LightNet on the CIFAR-10 dataset.

Loss function

Usually, a loss function is connected to the outputs of the
deepest core computation module. Currently, LightNet supports the softmax log-loss function for classification tasks.

3.3

(b)

Rectified Linear Unit

The rectified linear unit (ReLU ) is implemented as a major non-linear mapping function, some other functions including sigmoid and tanh are omitted from the discussion
here. The ReLU function is the identity function if the input
is larger than 0 and outputs 0 otherwise: y = relu(x) = x ·
ind(x > 0). In the backward process, the gradient is passed
to the shallower layer if the input data is non-negative. Otherwise, the gradient is ignored.

3.2

(a)

Convolutional Neural Network

LightNet supports using state-of-the-art convolutional network models pretrained on the ImageNet dataset. It also
supports training novel network models from scratch. A convolutional network with 4 convolution layers is constructed
to test the performance of LightNet on CIFAR-10 data [7].
There are 32, 32, 64, 64 convolution kernels of size 5 × 5 in
the first three layers, the last layer has kernel size 4 × 4.
relu functions are applied after each convolution layer as
the non-linear mapping function. LightNet automatically
selects and adjusts the learning rate and can achieve stateof-the-art accuracy with this architecture. Selective-SGD
leads to better accuracy compared with standard SGD with
a fixed learning rate. Most importantly, using Selective-SGD
avoids manual tuning of the learning rate. See Fig. 3 for the
experiment results. The computations are carried out on a
desktop computer with an Intel i5 6600K CPU and a Nvidia
Titan X GPU with 12GB memory. The current version of
LightNet can process 750 images per second with this network structure on the GPU, around 5× faster than using
CPU.

4.3

LSTM Network

The Long Short Term Memory (LSTM) [4] is a popular recurrent neural network model. Because of LightNet’s
versatility, the LSTM network can be implemented in the
LightNet package as a particular application. Notably, the
core computational modules in LightNet are used to perform time domain forward process and back propagation for
LSTM.
The forward process in an LSTM model can be formulated

as:

By using a least squared loss function:
it = sigmoid(Wih ht−1 + Wix xt + bi ),

(7)

ot = sigmoid(Woh ht−1 + Wox xt + bo ),

(8)

ft = sigmoid(Wf h ht−1 + Wf x xt + bf ),

(9)

gt = tanh(Wgh ht−1 + Wgx xt + bg ),

(10)

ct = ft  ct−1 + it  gt , ht = ot  tanh(ct ),

(11)

z = (y − Qcurrent (stateold , act))2
= (Qnew (stateold , act) − Qcurrent (stateold , act))2 ,

(16)

the Q-Network can be optimized using the gradient:
∂z
∂Qcurrent
∂z
=
.
∂θ
∂Qcurrent
∂θ

(17)

Here θ denotes the parameters in the Q-Network.

5.

CONCLUSION

T
X

LightNet provides an easy-to-expand ecosystem for the
zt = f (ht ), z =
zt .
(12)
understanding and development of deep neural network modt=1
els. Thanks to its user-friendly Matlab based environment,
Where it /ot /ft denotes the response of the input/output/forget the whole computational process can be easily tracked and
visualized. This set of the main features can provide unique
gate at time t. gt denotes the distorted input to the memory
convenience to the deep learning research community.
cell at time t. ct denotes the content of the memory cell at
time t. ht denotes the hidden node value. f maps the hidden
nodes to the network loss zt at time t. The full network loss
6. REFERENCES
[1] Barto, A. G., Sutton, R. S., and Anderson, C. W.
is calculated by summing the loss at each individual time
Neuronlike adaptive elements that can solve difficult
frame in Eq. 12.
learning control problems. Systems, Man and Cybernetics,
To optimize the LSTM model, back propagation through
IEEE Transactions on, 5 (1983), 834–846.
time is implementedP
and the most critical value to calculate
[2]
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J.,
∂zt
∂z
= Tt=s ∂c
.
in LSTM is: ∂c
Goodfellow, I., Bergeron, A., Bouchard, N.,
s
s
A critical iterative property is adopted to calculate the
Warde-Farley, D., and Bengio, Y. Theano: new features
and speed improvements. arXiv preprint arXiv:1211.5590
above value:
(2012).
∂z ∂cs
∂zs−1
∂z
[3] Duchi, J., Hazan, E., and Singer, Y. Adaptive
=
+
.
(13)
∂cs−1
∂cs ∂cs−1
∂cs−1
subgradient methods for online learning and stochastic
A few other gradients can be calculated through the chain
rule using the above calculation output:
∂zt
∂zt ∂ht
∂z
∂z
∂ct
=
,
=
.
∂ot
∂ht ∂ot ∂{i, f, g}t
∂ct ∂{i, f, g}t

(14)

The LSTM network is tested on a character language modeling task. The dataset consists of 20, 000 sentences selected
from works of Shakespeare. Each sentence is broken into 67
characters (and punctuation marks), and the LSTM model is
deployed to predict the next character based on the characters before. 30 hidden nodes are used in the network model
and RMSProp is used for the training. After 10 epochs,
the prediction accuracy of the next character is improved to
70%.

4.4

Q-Network

[4]
[5]

[6]
[7]
[8]

[9]

As an application in reinforcement learning, We created
a Q-Network [11] with the MLP network. The Q-Network
is then applied to the classic Cart-Pole problem [1]. The
dynamics of the Cart-Pole system can be learned with a twolayer network in hundreds of iterations. One iteration of the
update process of the Q-Network is:
Qnew (stateold , act) = reward+γQcurrent (statenew , actbest )
= reward + γmaxa Qcurrent (statenew , a)
= reward + γV (statenew ).

[10]
[11]

[12]

(15)

The action is randomly selected with probability epsilon,
otherwise the action leading to the highest score is selected.
The desired network output Qnew is calculated using the
observed reward and the discounted value γV (statenew ) of
the resulting state, predicted by the current network through
Eq. 15.

[13]

optimization. The Journal of Machine Learning Research
12 (2011), 2121–2159.
Hochreiter, S., and Schmidhuber, J. Long short-term
memory. Neural computation 9, 8 (1997), 1735–1780.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S.,
Long, J., Girshick, R., Guadarrama, S., and Darrell,
T. Caffe: Convolutional architecture for fast feature
embedding. In Proceedings of the ACM International
Conference on Multimedia (2014), ACM, pp. 675–678.
Kingma, D., and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 (2014).
Krizhevsky, A., and Hinton, G. Learning multiple layers
of features from tiny images, 2009.
Krizhevsky, A., Sutskever, I., and Hinton, G. E.
Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing
systems (2012), pp. 1097–1105.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86, 11 (1998), 2278–2324.
Mallat, S. A wavelet tour of signal processing: the sparse
way. Academic press, 2008.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A.,
Veness, J., Bellemare, M. G., Graves, A., Riedmiller,
M., Fidjeland, A. K., Ostrovski, G., et al. Human-level
control through deep reinforcement learning. Nature 518,
7540 (2015), 529–533.
Tieleman, T., and Hinton, G. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine
Learning 4 (2012), 2.
Vedaldi, A., and Lenc, K. Matconvnet: Convolutional
neural networks for matlab. In Proceedings of the 23rd
Annual ACM Conference on Multimedia Conference
(2015), ACM, pp. 689–692.

2013 IEEE International Conference on Robotics and Automation (ICRA)
Karlsruhe, Germany, May 6-10, 2013

Robots with Language: Multi-Label Visual Recognition Using NLP
Yezhou Yang, Ching L. Teo, Cornelia Fermüller and Yiannis Aloimonos

Abstract— There has been a recent interest in utilizing
contextual knowledge to improve multi-label visual recognition
for intelligent agents like robots. Natural Language Processing
(NLP) can give us labels, the correlation of labels, and the
ontological knowledge about them, so we can automate the
acquisition of contextual knowledge. In this paper we show
how to use tools from NLP in conjunction with Vision to
improve visual recognition. There are two major approaches:
First, different language databases organize words according
to various semantic concepts. Using these, we can build special
purpose databases that can predict the labels involved given a
certain context. Here we build a knowledge base for the purpose of describing common daily activities. Second, statistical
language tools can provide the correlations of different labels.
We show a way to learn a language model from large corpus
data that exploits these correlations and propose a general
optimization scheme to integrate the language model into the
system. Experiments conducted on three multi-label everyday
recognition tasks support the effectiveness and efficiency of our
approach, with significant gains in recognition accuracies when
correlation information is used.

I. I NTRODUCTION
Recognition tasks for robots are not independent. They
are correlated [6]. Researchers from computer vision and
robotics disciplines have conducted a great amount of work
utilizing the correlation between recognition tasks to boost
individual performance, e.g. human identification and action
[13], object and action [31]. However, correlation learned
from hard-coded boolean charts or human labeling limits the
possibility of deploying the approach into an real intelligent
agent, aka a robot. In this paper, we show that the field
of Natural Language Processing (NLP) has produced many
tools that we can use to obtain the correlation. We discuss
the different usages of language tools and how to combine
natural language and vision tools in a robot.
Computational linguistics have created large text corpora
and statistical tools so that we can obtain probability distributions for the co-occurrence of any two words, such as how
likely a certain noun co-occurs with a certain verb. Here we
present a framework to learn the correlation from corpus
and apply it to several correlated multi-label recognition
tasks. While we do not claim credits for introducing corpus
statistics, we introduce a general way to combine it with
vision. Experimental results on three different multi-label
recognition tasks support our conjecture that the corpus
guided method is able to boost recognition performance.
Classical and computational linguists are interested in
modeling lexical semantics and have created resources where
Computer
Vision
Lab,
UMIACS,
University
of
land
College
Park,
College
Park,
MD
20740,

{yzyang,cteo,fer,yiannis}@cs.umd.edu
978-1-4673-5643-5/13/$31.00 ©2013 IEEE

MaryUSA

information about the conceptual meaning of lexical items
and how these items relate to each other [5], such as
“cause-effect” or “performs-functions”, is organized. For
example, the WordNet database [20] relates words through
synonymy (words having the same meaning, like argue and
contend) and hypernymy (“is-a” relationships, as between
car and vehicle), among many others [21].
From another perspective, it is widely known that recognition tasks, like many other Artificial Intelligence tasks,
require a high level knowledge base. Fanya Montalvo defined
any such AI task as an AI-complete problem [18]. Tremendous amount of work has been devoted into building such a
knowledge base, from early experts systems, to IBM’s recent
Watson project. In this work, we build a small database of
common daily activities using linguistic tools. We show here
how one can exploit the relationships in such a database to
predict reasonable language labels for mining a large textual
corpus dynamically.
In this paper, we focus on using as labels “nouns” (objects)
and “verbs” (actions). Language and vision extract different
information about the same entity, and we integrate them
at the last computational stage in the recognition. However,
we believe that the true power of using language and vision
in combination will come through an integration at earlier
stages. Both statistical and classical linguistic tools can
provide in addition to correlation of labels, other information
useful for visual recognition, such as spatial and temporal
relations, and information on the visual appearance of objects and actions. We collectively call these information the
entities’ attributes. In some sense, language creates a multilayer, hierarchical representation. Using this information, we
can potentially address visual recognition as a problem of
reasoning about the scene, instead of just classifying labels.
For example, instead of recognizing a hammer using a
classifier, we can verify that the segmented image patch
“contains a wooden handle”, “has a metal part” and “is next
to a nail”. Although we do not have all the tools yet to fully
demonstrate these ideas, we include here some preliminary
work on utilizing visual attributes for recognition.
II. M OTIVATION
Human perception has a crucial but straightforward principle: Principle of Totality [6], which states that the conscious
experience must be considered globally (by taking into
account all the physical and mental aspects of the individual
simultaneously) because the nature of the mind demands
that each component be considered as part of a system of
dynamic relationships. If any individual recognition task can
be regarded as “some part of the system” here, then where

4256

Fig. 1. Overview of framework for two sequences: (above) drinking, (below) cleaning. Hands, tools and action features (1) are extracted. From the visual
detection scores (2), a correlation matrix (3) learned from a textual corpus (4) is used to improve the final label predictions (5).

do the “dynamic relationships” come from? Obviously for
human beings, such dynamic relationships are from knowledge we accumulate through either learning or experience.
We argue here that for a robot, such dynamic relationships
can be obtained from linguistic resources, either from lexical
databases and/or estimated by mining a corpus.
Fig. 1 shows a sketched overview of the proposed framework that uses the above mentioned principle. Consider the
following scenario: A robot is observing a human being, e.g.
drinking a cup of coffee (Fig. 1 left), and it is required
to recognize the action and the tool. Current Computer
Vision applications require that all entities, that the robot can
distinguish, are predefined. Using our Knowledge database,
which stores information about common daily activities, the
robot can obtain the possible entities (objects and activities
in a kitchen setting). Then the agent may be challenged
from the uncertainty between a white cup or a white towel.
However, by tracking the hand trajectories, it is confident that
the action is “drinking” rather than “cleaning”. If the agent
has knowledge that humans normally use a cup to drink but
not a towel, it should be easy to resolve the uncertainty. The
knowledge needed here can be achieved by “teaching” the
agent with several hard-coded propositional facts, such as
[drink use cup], [clean use towel]. However, if we
want to extend the knowledge, we have to enumerate all
the possible facts required. On the other hand, such kind
of knowledge is not deterministic: sometimes we do use a
cup to clean by pouring water onto a dirty desk. The system
should also assign a small possibility to [clean use cup].
Another way of learning is to learn the correlation from
a large corpus automatically. Sentences like “Wellington’s
first chance to sip from the cup” are indicative of a strong
correlation between cup and drink. Using NLP we can
examine synonyms from WordNet to find out that sip is a
form of drink. Sec. IV-A reports an experiment inspired
from this scenario. We extend the framework to show how
one can similarly exploit the correlation between scenes and
objects in sec. IV-B. Finally, we know that the appearance
of objects and actions can vary greatly, and visual object
classifiers don’t appear to be scalable. The problem is even
harder for action recognition; descriptors robust to viewpoint and variation in movement have not been developed

yet. Instead, as was advocated in the last part of sec. I, if
we can obtain knowledge about the attributes of labels that
do not change under environmental influences, we can use
them in a reasoning process within the framework. This is
demonstrated here for the case of object attributes in sec. IVC.
In general, it is reasonable to expect the robot to learn the
correlation between labels from the corpus and use it as an
approximation of knowledge to guide the recognition task.
Our proposed framework (sec. III) details how the corpus
is mined, as well as the optimization method for integrating
information from language and visual processing so as to
help the robot achieve higher recognition accuracies when it
is doing several correlated recognition tasks.
III. T HE M ULTI -L ABEL R ECOGNITION F RAMEWORK
Consider a multi-label recognition task with two sets of
possible labels: t1 ∈ T1 and t2 ∈ T2 , m and n is the size of
the two label sets. Our framework first predicts the possible
labels given some initial knowledge of the domain. From
these labels, we show how we compute corpus statistics,
essentially label correlations, to guide recognition in both
T1 and T2 . We then show how this framework can be
generalized to a situation with three or more tasks.
A. Predicting Language Labels
An important prerequisite before corpus statistics can be
computed is that we need to determine the relevant task
labels that are appropriate for the dataset. The simplest and
most direct approach, for e.g. in [28], is to predefine all the
labels prior to any computation of the corpus statistics. While
this may work for standalone evaluations of the proposed
framework, a system that is fully autonomous should be
able to make reasonable predictions of labels, given some
knowledge of the domain. In this work, inspired by [22]
we build a small ontology of relations inherent in common
daily activities, with bidirectional relations all organized in
a symmetric, labeled graph. Fig. 2 shows an example result
of querying the knowledge base with the prior knowledge
“kitchen” corresponding to one of our experimental datasets.
The output is a list of relevant kitchen related tools, together
with the possible associated actions linked to each tool.

4257

In Fig. 2(a), “kitchen” is one of the subclasses (aka “isa” relationship) under the superclass “scenes”, and every
associated tool is a subclass of “tools” in purple lines. In
Fig. 2(b), the visualization shows every associated action
with each tool in dotted yellow lines. Every concept in this
knowledge base starts from the superclass “Thing”. Although
querying the knowledge base does not provide probabilities
that we need directly, the list of possible tool and action
labels can then be used as seeds to create the corresponding
language model.

to grossly underestimate PL (t1 |t2 ). For example, in the
Gigaword Corpus counting how often drink co-occurs with
cup where the actual words are used will not be significantly
larger than pick and cup. The reason is that cup can mean
a normal drinking cup or a trophy cup. In order to ensure that
PL captures the correct sense of the word: nouns or verbs,
we use WordNet to determine the synonyms and hyponymns
of the words considered.
We then recompute PL using these enlarged word classes
to capture more meaningful relationships between the cooccurring words. Fig. 3(a) shows the m × n co-occurrence
matrix of likelihood scores over the set of tools and actions
considered in the UMD Sushi-Making dataset (sec. IV-A.1),
denoted as PL (T1 |T2 ) when we normalize the correlation
scores over all labels in T2 . Similarly we obtain PL (T2 |T1 )
when we normalize over all labels in T1 .

(a)

(a)

(b)
Fig. 2. Label prediction using the Embodied Knowledge Base. Purple
lines indicate “subclass” relationships (aka “is-a”) while dotted yellow
lines indicate “association” relationships (one class is related to another
class semantically). (a) The “kitchen” concept with its associated tools. (b)
Various tools classes with their associated actions.

B. Correlation Mining
The key component of our approach is the language model
that predicts the correlation between T1 and T2 . We use the
Gigaword Corpus [12] as a large text resource that contains
the information. We do this by training a language model
PL (T1 , T2 ) that returns the maximum likelihood estimates
of any label t1 given the other label t2 . This can be done
by counting the number of times t1 co-occurs with t2 in
1 ,t2 )
sentences in the corpus: PL (t1 |t2 ) = #(t
#(t2 ) .
As many English words share common meanings, a simple
count of the words (labels) defined in T1 or T2 is likely

(b)
Fig. 3. (a): Gigaword co-occurrence matrix over tools and actions. (b):
Gigaword co-occurrence matrix over objects and scenes

A quick analysis shows that for most of the tool classes,
the predicted actions are correct (large values along the
diagonals): e.g peeler predicts peeling with high probability (0.94). However, there are many co-occurrences which
we could not anticipate: e.g. sprinkling has some synonyms such as drizzle moisten splash splosh

4258

which have uses that are also close to cup, resulting in
a higher score (0.29) versus drinking (0.17). Other misselected tools-action are also due to the confusion at the synonyms/hyponymns levels. We also notice that more general
actions such as picking have a more uniform distribution
across the tools, which is expected. Despite this simplistic
model, most of the entries in PL make sense – and it properly
reflects the innate complexity of language. As will be shown
in sec. IV-A.4, although the prior from language is weak, it
is still helpful for the task of action and tool recognition. We
used the same approach to extract the relationship between
objects and scenes. Fig. 3(b) shows the m × n co-occurrence
matrix of likelihood scores over the set of objects T1 and
scenes T2 considered in Sec. IV-B.2.

(Fig. 4(a)) where approximate inference methods, such as
message passing [7] can be applied. This occurs, for e.g.
when a third task, such as attributes of tools and actions,
is added into the framework in which we solve a joint
likelihood over the entire graphical model.
Further simplification can be achieved if some of the task
labels are mutually independent, for e.g. object attributes and
scenes, which allows us to solve the inference problem via
a dynamical programming approach using Hidden Markov
Models (HMM). Fig. 4(b) shows the HMM that combines
PL and PV in a straightforward manner: the emissions
correspond to PV from visual processes (scenes, objects and
attributes) and the transition probabilities PL are obtained
from the language model.

C. Correlation Guidance
We use state-of-the-art features and machine learning
techniques (specifically SVM) to train two classifiers on
both recognition tasks that take in the labels and return a
recognition confidence score on every label. The confidence
scores are converted to probabilities using Platt’s method
[17]. We then normalize the scores over all possible labels
to get PV (T1 ) and PV (T2 ), where the subscript PV is used
to denote probabilities from visual processing.
Together with PL (T1 |T2 ) and PL (T2 |T1 ), the joint probability of the multi-recognition task can be modeled as:
P (t1 , t2 ) = PV (t1 )PL (t2 |t1 ) = PV (t2 )PL (t1 |t2 ).

(1)

We first focus on predicting t1 . We can get an estimated
probability from language correlation, PL (t1 ), by marginalizing over T2 :
X
X
PL (t1 ) =
P (t1 , t2 ) =
PV (t2 )PL (t1 |t2 ). (2)
t2 ∈T2

t2 ∈T2

By introducing λ ∈ [0, 1] as a regularization factor that controls the balance between the influence of visual detections
PV and corpus statistics PL , as well as taking logs on both
PV (t1 ) and PL (t1 ), we obtain the log-likelihood L of the
labeling task by:
L(t1 ) = log(PV (t1 )) + λ log(PL (t1 )).

(3)

We can derive L(t2 ) in a similar manner. The final label
prediction pair (tf1 , tf2 ) is then obtained by:
tf1 = arg max L(t1 ) and tf2 = arg max L(t2 )
t1 ∈T1

t2 ∈T2

(4)

As |T1 | and |T2 | are usually small, an exhaustive search over
every label t1 or t2 is practical which guarantees a global
optimal solution.
D. Generalizing to ≥ 3 Multi-Label Tasks
When generalizing the framework over three or more
labels tasks, (T1 , T2 . . . Tn ), the cost of the marginalization
step in eq. (2) increases exponentially and a naive brute force
summation over all possible labels in Tn becomes impractical. In this case, we cast the problem of determining the
optimal solution into a general graphical model optimization

(a)

(b)

Fig. 4. (a): General case for a three labels task: PV denotes probabilities
from visual detection and PL denotes correlation probabilities from corpus
mining. (b): HMM model for three specific task labels: Scene-ObjectAttribute are denoted as T1 , T2 , T3 respectively.

IV. E XPERIMENTS
A. Tools and Actions
In this set of experiments, we validate our proposed
framework introduced in sec. III using a scenario where our
robot observes humans making sushi. We collect the UMD
Sushi-Making Dataset where we have computed the language
models from initial seed labels predicted by our Knowledge
Base.
1) The UMD Sushi-Making Dataset: The UMD SushiMaking Dataset1 [28] consists of 12 actions, performed by
4 actors using 10 different kitchen tools. This results in
48 video sequences each of around 1000 frames (30 seconds long). Other well known datasets such as the KTH,
Weizmann or Human-EVA datasets [25], [11], [27] do not
involve hand-tools. The dataset by Messing et al. [19] has
only 4 actions with tool use. The CMU Kitchen Dataset
[15] has many tool interactions for 18 subjects making 5
recipes, but many of the actions are blocked from view due
to the placements of the 4 static cameras. Our Sushi-Making
dataset provides a clear view of the actions and tools. The
12 actions are: cleaning, cutting, drinking, flipping, peeling,
picking (up), pouring, pressing, sprinkling, stirring, tossing,
turning. The tools are: tissue, knife, cup, rolling-mat, fruitpeeler, water-pitcher, spoon, shaker, spatula, mixing-bowl.

4259

1 http://www.umiacs.umd.edu/research/POETICON/umd sushi

As was discussed in sec. III-B, some of the actions such
as picking or flipping are extremely general and are
easily confused. We made this choice to ensure that the
language prediction PL is not perfect and to show that our
approach works even under noisy data.
2) Active tool detection strategy: We pursue an active
strategy for detecting the relevant tools (denoted by T1 ) in
the video as illustrated in Fig. 5. This approach has two
important benefits. By focusing our processing only on the
relevant regions of the video frame, we dramatically reduce
the chance that the tool detector will misfire. At the same
time, by detecting the hand locations, we obtain the action
trajectory, which is used to describe the action as shown in
the next section.

in recognition accuracies when eq. (3) and eq. (4) were
applied over various values of λ ∈ [0, 0.5] as shown in
Fig. 6(a). Using corpus statistics, we obtained a relative
improvement of 6% in recognition accuracy in both action
and tool recognition compared to their baselines which had
a combined average of 4.5%. As λ represents our confidence
on the accuracy of the corpus-statistics versus the visual
detections, different values of λ are expected to have different
effects on the labels (tools or actions) considered. In this
case, the language model is more biased towards tools and
gives the largest improvement when λ is larger, compared
to actions which have the opposite effect. Such seemingly
divergent results can be explained from the inherent bias of
language itself, where tools (and objects in general) have
stronger correlations to specific (and limited) actions while
many similar actions can be performed by numerous different
tools, which results in a weaker (and hence more limited)
effect on the action recognition accuracy.
B. Objects and Scenes

Fig. 5. Overview of the tool detection strategy: (1) Optical flow [2] is first
computed from the input video frames. (2) We train a CRF segmentation
model [24] based on optical flow + skin color. (3) Guided by the flow
computations, we segment out hand-like regions (and removed faces if
necessary) to obtain the hand regions that are moving (the active hand that
is holding the tool). (4) The active hand region is where the tool is localized.
Using the PLS detector [26] (5), we compute a detection score PV (t1 ), the
probability that a tool t1 ∈ T1 exists given the video.

3) Action Recognition: Action labels are denoted as T2 in
this dataset. Tracking the hand regions in the video provides
us with two sets of (left and right) hand trajectories as
shown in Fig. 1. We then construct for every video a feature
vector Fd that encodes the hand trajectories. Fd encodes the
frequency and velocity components. Frequency is encoded by
using the first 4 real components of the Fourier transforms of
the position space in x- and y- direction which gives a 16dim vector over both hands. Velocity is encoded by averaging
the difference in hand positions between two adjacent frames
hδxi, hδyi which gives a 4-dim vector. These features are
then combined to yield a 20-dim vector Fd . A SVM classifier
is trained over these feature vectors to obtain the recognition
score PV (t2 ).
4) Results: A 4-fold cross validation was performed over
the 48 videos of the Sushi-Making dataset in order to
evaluate the effectiveness of our proposed approach. We first
obtained the recognition accuracy using PLS (for the 10
tools) and Action Features + SVM (for the 12 actions) alone
and used them as a baseline to highlight the improvement

In this set of experiments, we further evaluated our proposed framework for the scenario in which our robot is
observing natural scenes with objects. We use a general
large scale image dataset as testbed. As the images were
taken from numerous domains, we found that it was easier
to use the given ground truth labels of scenes and objects
for this task, and we focus on showing the usefulness of the
proposed approach in improving object and scene recognition
accuracies.
1) SUN 20 scenes dataset: We evaluated the proposed
approach using a subset of the SUN 20 scenes dataset [3].
The dataset comprises of 20 scenes and 127 objects from
which we selected 1000 images. The large number of scenes
and objects over a large variety of domains make this an
extremely challenging dataset to evaluate the effectiveness
of our approach in more general situations.
2) UIUC Pascal sentence dataset: In addition, we performed evaluations of our framework using the UIUC Pascal
sentence dataset, first introduced in [10]. It contains 1000 images taken from a subset of the Pascal-VOC 2008 challenge
image dataset, which are hand annotated with sentences that
describe the image. The ground truth labels for objects and
scenes are extracted from these sentences using the Berkeley
Parser [23]. We divided the images into 8 distinct scenes
[29] with 20 object classes defined in the Pascal-VOC 2008
challenge.
3) Results: For the SUN 20 scenes dataset, we randomly
divided the 1000 images into a training set of 600 images
with the remaining 400 as the testing set. Results are summarized in Fig. 6(b). We first extracted GIST features and
used a SVM classifier over the 20 scene classes to obtain the
baseline scene recognition accuracy. For objects, we chose
the top 50 object classes from the original 127 which yielded
the best detection scores over the training set, and determined
the existence of the object in the test set by comparing it with
annotated ground truth labels to obtain the baseline object
recognition accuracies. The same parameters and trained

4260

(a)

(b)

(c)

Fig. 6. Experimental results: (a) Action and Tool recognition vs individual recognition baselines on the UMD Sushi-Making dataset. (b) Scene and Object
recognition vs individual recognition baselines on the SUN 20 scenes dataset. (c) Scene and Object recognition vs individual recognition baselines on the
UIUC Pascal sentences dataset.

object models provided by the authors of the dataset were
used in all experiments. We repeat the same experimental
procedure with the UIUC Pascal sentence dataset (Fig. 6(c)),
using the same train-test splits, GIST+SVM classifiers over
8 scene classes and pre-trained object models over the 20
object classes provided by the authors.
We evaluated our proposed framework and compared it
to the baseline accuracies in the two datasets by varying
λ. For the SUN 20 scenes dataset, we obtain a relative
improvement of 2.6% (objects) and 1% (scenes) and an
overall improvement of 1.4% (objects + scenes) over the
baselines. The results are more significant in the UIUC
Pascal sentence dataset where the relative improvements
range from 10% (objects) to 3% (scenes) with an overall
improvement of 8.1%. Although the task for these two
datasets are the same (objects+scenes), the vastly different
improvements from our approach highlights the need to
define the domain of the task prior to computing the relevant
corpus statistics. In the SUN 20 scenes dataset, the large
number of scenes and object classes meant that the corpus
statistics is diluted over all object/scene classes, limiting the
improvement of our approach over the baselines. This effect
is mitigated in the UIUC Pascal sentences dataset which has
fewer object and scene classes. This further emphasized the
importance of predicting the relevant task labels given the
domain knowledge (sec. III-A) in order to maximize the
benefits of using language. Finally, by comparing the results
task in sec. IV-A.4, we note that the divergence due to λ
between objects and scenes labels is not as significant. This
indicates that objects and scenes have stronger correlations
compared to actions and tools, which makes intuitive sense.
C. Scenes, Objects and Attributes
In the last experimental scenario, we require our robot to
recognize scenes, objects as well as object attributes. The
importance of recognizing objects through their attributes
has recently received attention in Computer Vision [9]. Our
proposed framework lends itself naturally to the use of such
high-level knowledge since mining the correlation between
attributes and objects is still doable from a large corpus,
under the condition that the attribute labels are commonly
used in daily life. By invoking the assumption that object
attributes are independent of the scenes in which they occur,

Fig. 7. Object recognition performance on the UIUC Pascal sentence
dataset when object attributes are used.

we are able to model the problem using the HMM introduced
in sec. III-D. We extend the experiments conducted over
the UIUC Pascal sentence dataset in sec. IV-B.3 by adding
human annotated attributes described in [9] as the third task
label T3 in the framework. As our focus is not on designing
attributes detectors in this paper, we have allowed PV (t3 ) to
be the human annotated attributes. From the original list of
64 attributes, we first select the top 20 semantically meaningful attributes by hand (discarding those that have little or no
relevance to the 20 object classes) and then compute a new
language model PL (T2 |T3 ) that relates these attributes to the
object classes. By considering the human annotated attributes
as a form of “attribute corpus” we compute PLA (T2 |T3 )
which represents the upper-bound on the language model
that we can expect from mining the corpus. The object
recognition accuracy results are summarized in Fig. 7. The
first two plots (in green) are from Fig. 6(c), where no attribute
information was used. By adding attributes in the framework,
we obtained an additional improvement of 10% and 13.6%
for PL (T2 |T3 ) and PLA (T2 |T3 ), respectively compared to the
case when no attributes were used. The strength of using
attributes for object recognition are clear here: we are able
to consistently improve upon the baseline (when no attributes
are used) and when a cleaner corpus that relates objects and
attributes directly are used.
V. R ELATED W ORKS
Our work is mostly related to Yang et al. [30], in which
they showed that correlation learned from corpus can guide
descriptions of natural images. Kulkarni et al. [14] proposed
to generate simple image descriptions by designing various

4261

Conditional Random Field models. Both studies focused
on generating descriptions of images, while in our work,
we focus on using correlation and graphical models to
study fundamental recognition tasks. Additionally, advances
in Natural Language Processing and Computer Vision have
lead to several works that have focused on using sources
of data that are readily available “in the wild” to analyze
static images. The seminal work of Duygulu et al. [8]
showed how nouns can provide constraints that improve
image segmentation. Berg et al. [1] processed news captions
to discover names associated with faces in the images, and
Jie et al. [13] extended this work to associate poses detected
from images with the verbs in the captions. Some studies
also considered dynamic scenes. [4] studied the aligning of
screen plays and videos, [16] learned and recognized simple
human movement actions. These recent works had shown
that exploiting co-occurring text information from scripts and
captions aids in the visual labeling task. Our paper takes this
further by using generic text obtained from the Gigaword
corpus [12]. As was shown in the preceding sections, by
using NLP tools, we can still derive useful correlations for
multi-label recognition tasks.
VI. C ONCLUSION
A framework has been introduced which integrates NLP
tools for guiding multi-label visual recognition tasks. We
applied it to three different real world tasks for our robot:
1) tools+actions recognition on the UMD Sushi-Making
video dataset, 2) objects+scenes recognition on the SUN 20
scenes and UIUC Pascal sentences image datasets and 3)
scenes+objects+attributes recognition on the UIUC Pascal
sentences dataset. The experimental results reported support
the effectiveness of our framework compared to baselines
using visual processing alone. We also explored how much
we should trust corpus statistics by adjusting a regularization
parameter that balances our confidence on the accuracy of
the corpus statistics versus the visual detections. In addition,
the experiments also highlighted the need to have a predefined domain to constrain the corpus statistics and the usefulness of adding attributes in improving object recognition
performance. In future work, we intend to explore extracting information about the spatial and temporal relations of
objects in the scene (the prepositions) and about attributes
of objects and verbs (adjectives, part descriptions of nouns
and adverbs) from language. Using these information we can
explore improving visual recognition at earlier stages in the
computation. For example, we can use this information in
segmentation, and for learning labels from both vision and
language information, and reformulate visual recognition as
a reasoning process.
VII. ACKNOWLEDGEMENTS
The support of the European Union under the Cognitive
Systems program (project POETICON++) and the National
Science Foundation under the Cyberphysical Systems Program is gratefully acknowledged. Y. Yang and C. Teo are
supported in part by the Qualcomm Innovation Fellowship.

R EFERENCES
[1] T. L. Berg, A. C. Berg, J. Edwards, and D. A. Forsyth. Who’s in the
picture? In NIPS, 2004.
[2] T. Brox, C. Bregler, and J. Malik. Large displacement optical flow.
In CVPR, pages 41–48. IEEE, 2009.
[3] M. J. Choi, A. T. J. Lim, and A. S. Willsky. Exploiting hierarchical
context on a large database of object categories. CVPR, 2010.
[4] T. Cour, C. Jordan, E. Miltsakaki, and B. Taskar. Movie/script:
Alignment and parsing of video and text transcription. 2008.
[5] D. A. Cruse. Lexical semantics. Cambridge, England: University
Press, 1986.
[6] A. Desolneux, L. Moisan, and J. M. Morel. From Gestalt Theory to
Image Analysis, volume 34. 2008.
[7] J. Domke. Parameter learning with truncated message-passing. CVPR,
2011.
[8] P. Duygulu, K. Barnard, J. F. G. de Freitas, and D. A. Forsyth. Object
recognition as machine translation: Learning a lexicon for a fixed
image vocabulary. In ECCV, 2002.
[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects
by their attributes. CVPR, 2009.
[10] A. Farhadi, S. M. M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian,
J. Hockenmaier, and D. A. Forsyth. Every picture tells a story:
Generating sentences from images. In ECCV, 2010.
[11] L. Gorelick, M. Blank, E. Shechtman, R. Basri, and M. Irani. Actions
as space-time shapes. PAMI, 29(12):2247–2253, 2007.
[12] D. Graff. English gigaword. In Linguistic Data Consortium, Philadelphia, PA, 2003.
[13] L. Jie, B. Caputo, and V. Ferrari. Who’s doing what: Joint modeling
of names and verbs for simultaneous face and pose annotation. NIPS.
NIPS, December 2009.
[14] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. Berg, and T. Berg.
Baby talk: Understanding and generating simple image descriptions.
CVPR, 2011.
[15] F. D. la Torre, J. Hodgins, J. Montano, S. Valcarcel, R. Forcada, and
J. Macey. Guide to the carnegie mellon university multimodal activity
(cmu-mmac) database. Technical report, CMU-RI-TR-08-22, Robotics
Institute, Carnegie Mellon University, July 2009.
[16] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning
realistic human actions from movies. In CVPR, 2008.
[17] H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on platt’s probabilistic
outputs for support vector machines. Mach. Learn., 68:267–276,
October 2007.
[18] J. C. Mallery. Thinking about foreign policy: Finding an appropriate
role for artificially intelligent computers. 1988.
[19] R. Messing, C. Pal, and H. Kautz. Activity recognition using the
velocity histories of tracked keypoints. In ICCV 09, 2009.
[20] G. A. Miller. Wordnet: A lexical database for english. Communications
of the ACM, 38:39–41, 1995.
[21] G. A. Miller and C. Fellbaum. WordNet then and now, volume 41 of
Language Resources and Evaluation, pages 209–214. Springer, 2007.
[22] K. Pastra. Praxicon: a grounded, compositional & generative concept world. the 4th International Conference in Cognitive Systems
(CogSys), 2010.
[23] S. Petrov and D. Klein. Improved inference for unlexicalized parsing.
In Proceedings of HLT-NAACL, 2007.
[24] C. Rother, V. Kolmogorov, and A. Blake. ”grabcut”: interactive
foreground extraction using iterated graph cuts. ACM Trans. Graph.,
23(3):309–314, 2004.
[25] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A
local svm approach. In ICPR, 2004.
[26] W. Schwartz, A. Kembhavi, D. Harwood, and L. Davis. Human detection using partial least squares analysis. In International Conference
on Computer Vision, 2009.
[27] L. Sigal, A. O. Balan, and M. J. Black. Humaneva: Synchronized video
and motion capture dataset and baseline algorithm for evaluation of
articulated human motion. International Journal of Computer Vision,
87(1-2):4–27, 2010.
[28] C. L. Teo, Y. Yang, H. Daume, C. Fermuller, and Y. Aloimonos. A
corpus-guided framework for robotic visual perception. Workshops at
the Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011.
[29] A. Torralba, K. P. Murphy, W. T. Freeman, and M. A. Rubin. Contextbased vision system for place and object recognition. In ICCV, pages
273–280. IEEE Computer Society, 2003.
[30] Y. Yang, C. Teo, H. Daume, and Y. Aloimonos. Corpus-guided
sentence generation for natural images. EMNLP, 2011.
[31] B. Yao and L. Fei-Fei. Modeling mutual context of object and human
pose in human-object interaction activities. In CVPR, June 2010.

4262

IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 3, JULY 2017

1397

Unsupervised Linking of Visual Features to Textual
Descriptions in Long Manipulation Activities
Eren Erdal Aksoy, Ekaterina Ovchinnikova, Adil Orhan, Yezhou Yang, and Tamim Asfour

Abstract—We present a novel unsupervised framework, which
links continuous visual features and symbolic textual descriptions
of manipulation activity videos. First, we extract the semantic
representation of visually observed manipulations by applying a
bottom-up approach to the continuous image streams. We then employ a rule-based reasoning to link visual and linguistic inputs. The
proposed framework allows robots 1) to autonomously parse, classify, and label sequentially and/or concurrently performed atomic
manipulations (e.g., “cutting” or “ stirring”), 2) to simultaneously
categorize and identify manipulated objects without using any
standard feature-based recognition techniques, and 3) to generate
textual descriptions for long activities, e.g., “breakfast preparation.” We evaluated the framework using a dataset of 120 atomic
manipulations and 20 long activities.
Index Terms—Cognitive human-robot interaction, learning and
adaptive systems, semantic scene understanding.

I. INTRODUCTION
NTEGRATION of textual descriptions and visual features
has gained an increasing attention in natural language processing, computer vision, and robotics ([1]–[6]). The main challenge consists in bridging the gap between perceived continuous
visual features and discrete symbolic linguistic constructions. In
the context of robot learning from demonstration, this problem
is called symbol grounding [7] referring to the grounding of the
observed high-level symbolic action or object concepts into the
low-level sensory-motor data.
In this work, we introduce a novel unsupervised method allowing the discretization of visually observed continuous manipulations into the high-level symbolic object-action concepts
which can be directly mapped to their counterparts in linguistic video descriptions. This helps robots to ground language in
vision, i.e. makes the symbol grounding problem treatable. For
instance, in human-robot interaction tasks, e.g. dinner preparation, robots can autonomously parse individual atomic actions

I

Manuscript received September 10, 2016; accepted January 20, 2017. Date
of publication February 15, 2017; date of current version March 10, 2017.
This paper was recommended for publication by Associate Editor B. Argall
and Editor D. Lee upon evaluation of the reviewers’ comments. This work was
supported by the EU FET Proactive Grant (GA: 641100) TIMESTORM.
E. E. Aksoy, E. Ovchinnikova, A. Orhan, and T. Asfour are with the
High Performance Humanoid Technologies Laboratory, Karlsruhe Institute
of Technology, 76131 Karlsruhe, Germany (e-mail: eren.aksoy@kit.edu;
e.ovchinnikova@gmail.com; ubdnw@student.kit.edu; asfour@kit.edu).
Y. Yang is with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281 USA (e-mail:
yz.yang@asu.edu).
Color versions of one or more of the figures in this letter are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/LRA.2017.2669363

and the role of each manipulated tool and associate them with
human instructions. Thus, robots can learn how each visual entity in the continuous demonstration is described in language,
e.g. “a big knife”, or what a symbolic action command “cut”
means in terms of interactions between visual entities. Likewise,
given a set of human demonstrations, robots can further learn
the most probable atomic action sequence, e.g. a long-term action plan to prepare the dinner. As robots perform the planned
actions, they can also verbally inform the user about each
action.
State of the art methods approach this problem by applying
pre-trained object and action detectors to the observed demonstrations. We, however, do not employ object or action recognition, since it is not feasible to introduce every possible tool or
action type that robots may encounter. Instead, our framework
allows robots to autonomously categorize a) demonstrated actions based on their semantic similarities and b) manipulated
objects based on their roles in actions. Thus, our framework
neither requires prior object or action knowledge, nor performs
action or object recognition in the traditional sense. The framework assigns linguistic labels to the acquired action and object
concepts by extracting them from textual descriptions of the
demonstrations.
Our framework has two phases: learning and testing. In
the learning phase, we employ the “Semantic Event Chain”
(SEC) concept [8], which captures the semantic representation of a continuous manipulation by discretizing it into states
(Section III-A2). A pattern of SEC states forms symbolic action
concepts and suggests object categories based on their roles in
the manipulation. Learning is concluded by labeling these symbolic object and action concepts with linguistic labels extracted
from textual video descriptions. In the testing phase, we analyze chained manipulation sequences, which are decomposed
into sequentially and/or concurrently performed manipulation
instances. We compute object and action concepts for each instance and match them with the learned labeled concepts to
generate textual descriptions.
We applied our framework to a large publicly available manipulation action (ManiAc) dataset [9] with 120 demonstrations
of 8 manipulation types (e.g. “cutting”) and 20 long activities
(e.g. “breakfast preparation”). We used the Mechanical Turk
(MTurk) crowdsourcing platform to acquire 5 textual descriptions for each video. We also obtained promising results on the
description generation for long activities.
Contributions: (1) We introduce a novel unsupervised method
for grounding language in vision by linking low-level visual
sensory data to their symbolic descriptions. (2) We label visual
object and action concepts by using their textual descriptions

2377-3766 © 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1398

IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 3, JULY 2017

only, without employing any standard feature-based recognition
techniques. (3) We generate a textual description for an unseen
complex activity, in which various manipulations are chained
sequentially or concurrently. (4) We evaluate our framework on
a large dataset.

II. STATE OF THE ART
Related literature on linking natural language and vision
mostly focuses on either generating textual descriptions for
static scenes (e.g. [10], [11]) or learning linguistic descriptions only for object concepts (e.g. [4]), without considering
action information. In [1]–[3], [5], [12], textual descriptions
of videos are generated by employing supervised visual detectors. Semantic hierarchies are used to generate the descriptions
by detecting pre-trained subject, object, action triplets [1]. A
similar data-driven approach in [2] considers the best two visual object detector results as subject and object candidates. In
[12], a more accurate probabilistic factor-graph model is created by including the scene (location) information. The recent
method along these lines uses Hidden Markov Models (HMMs)
to learn a statistical relation between followed motions, manipulated objects, and their manually assigned linguistic descriptions [5]. In [13], HMMs are also used for recognizing events
based on motion profiles. In [14], discriminative models (e.g.
Conditional Random Fields) predict semantic representations
of the visual content, which are then translated into textual
descriptions.
All these approaches analyze short videos and rely on visual
feature-based object and action descriptors (e.g. SIFT, STIP, improved dense trajectories), which require large sets of labeled
training data. Such methods are limited to known object-action
classes and suffer from the generalization problem if training
data are limited. Unlike these methods, our unsupervised SEC
approach considers only the roles of the manipulated objects
to learn object and action categories, without employing any
conventional visual object or action detector. In our framework, structured linguistic information is further employed to
label these learned categories, whereas the aforementioned approaches use linguistic information to tackle the problem of
inaccurate or missing visual detections. In this respect, our approach bears similarities to [15] which introduces a hyperfeature coding of videos to be aligned with linguistic data using
unsupervised hand and object detection.
In the context of robotics, HMMs are used to represent human whole-body motion data by considering the joint angle
or position data only, without incorporating object information
[6]. Although this method supports a bi-directional mapping
between human whole-body motion and linguistic utterances,
using only motion data limits the application of the method
to manipulations, in which the motion profiles of objects and
hands vastly vary between demonstrations. In addition, classical
HMM-based approaches are not suitable for recognizing parallel streams of actions [16] and cannot easily describe motions
with repetitions or recursions [17]. SECs obey the Markovian
assumption, but in contrast to HMM-based generative frameworks, the states in SECs are observable and represent topological changes in the scene.
Another line of work uses deep networks for generating video
descriptions. In [3], recurrent neural networks are used to map

image sequences to word sequences, after being trained on raw
images and optical flow data. In [18], video descriptions are
generated by fusing appearance and optical flow cues extracted
from a spatio-temporal convolutional network. Although these
approaches show promising results on unconstrained videos,
they omit the action semantics and require tuning of a large set
of hyper-parameters. Since they depend on the motion features,
they require a large training set, because optical flow features
can vary significantly even between the same action demonstrations. In contrast, our approach extracts only descriptive
spatio-temporal discontinuities in relations between objects and
subjects in the scene, which remain stable for a given manipulation type, even with large motion and object variations.
III. OUR APPROACH
We refer to our system as “ViLaRob” standing for “coupling
Vision and Language for Robotics”. The proposed framework
shown in Fig. 1 consists of two processing phases: learning and
testing, results of which are combined in a robot memory. In
the learning phase (Fig. 1 green box), we process visual and
linguistic features extracted from an input atomic manipulation
video annotated with textual descriptions. In the visual process,
the scene content is encoded by graphs derived from tracked
unique image segments. Graphs are converted into SECs which
are used to learn action concepts, i.e. SEC models, in an
unsupervised way. Given the SEC, the framework estimates
an action concept index by comparing it with the learned
SEC models and categorizes the tracked image segments as
manipulator, primary and secondary objects. These action
and object information are stored as a visual quadruple. In the
linguistic process, we use the MTurk platform to obtain textual
descriptions for each video. We parse the textual descriptions
and extract labels of the action and its participants (semantic
role fillers). The extracted label set for each video is converted
into a linguistic quadruple. Learning is concluded by combining
visual and linguistic quadruples. The combined data is stored
as Object-Action Concepts (OACs) in the robot memory (Fig. 1
blue box). OACs are co-joint symbolic representations of the
robot’s sensorimotor experience, which is very much related to
the object-action complexes introduced in [19].
In the testing phase (Fig. 1 orange box), we process videos
showing novel long manipulation activities, e.g. “making a
sandwich”. The process starts with extracting the long SEC representation, which is decomposed into chunks by considering
the interaction between the estimated manipulator and objects
in the scene. Each SEC chunk is compared to the learned SEC
models to estimate an action concept index. After applying the
object role categorization, we generate visual quadruples for
each chunk. The framework then compares these quadruples
with the learned OACs in the robot memory, assigns labels to
objects and actions, and generates a linguistic description for
the novel long activity.
In the following, we detail processing steps in Fig. 1, where
black headers represent modules inherited from our previous
works [8], [9], [20], [21], whereas red headers indicate our
main contributions in the ViLaRob framework.
A. Learning Stage: Visual Process
1) Image Segmentation, Tracking, and Graphs: The input of
our framework is an RGB-D image stream of a human manip-

AKSOY et al.: UNSUPERVISED LINKING OF VISUAL FEATURES TO TEXTUAL DESCRIPTIONS

Fig. 1.

1399

Overview of the proposed framework. Headers written in red indicate the main contribution of this work.

Fig. 2. A sample cutting action with (a) original images and (b) segments
and main graphs. Numbers represent graph nodes and edges are given by blue
lines. (c) The SEC matrix in which N represents two disjoint object segments,
T shows a touching relation between two objects, and A is for the absence
of an object. (d) Extracted manipulator, primary, and secondary objects with
corresponding linguistic labels (shown in blue boxes).

ulation demonstration. After applying a color and depth-based
image segmentation [22], we track all objects and hands in the
scene. Each segmented image is converted into a graph, in which
nodes show segment centers and edges represent the contact
(i.e. touching) relation between segment pairs. In Fig. 2, some
images with the tracked image segments and graphs are shown
for a sample cutting action.
2) Semantic Event Chains (SECs): Given a continuous
graph sequence, an exact graph matching method is applied
to extract a set of main graphs representing topological changes
in the scene. Main graphs are used to construct the SEC matrix
(ψ). SEC rows describe spatial relations between two objects
and columns encode the topological scene structure at each main
graph. Spatial relations in the SEC rows are Not touching (N),
Touching (T), and Absence (A). Fig. 2 shows the SEC matrix for
a cutting example. For instance, the third SEC row represents
the spatial relations between nodes 9 and 7, i.e. the left hand
and the knife. The third SEC column indicates the state when

Fig. 3. (a) SEC with [N, T , . . . , T , N ] colored blocks, i.e., unique SEC
chunks. (b) Visual manipulator, primary, and secondary object segments in
SEC chunks. (c) Linguistic object labels. (d) Mapped manipulation labels.

the left hand starts grasping the knife. The SEC matrix encodes
object pairs that produce at least one relational change (e.g.
from N to T ) and ignores those with static relations (e.g. between the left and right hands). The SEC concept was introduced
in [8].
3) Learning SEC Models: Given a set of human demonstrations, we employ the unsupervised learning framework introduced in [9] to learn an action concept, i.e. a SEC model
matrix (ψ m ). The learning method works in an on-line fashion and is initiated once a new manipulation is observed. For
instance, when the first atomic manipulation is demonstrated,
the extracted SEC sample ψ1 is treated as the first model ψ1m
and stored in a library. Once the i-th demonstration is shown,
we encode it again by a SEC ψi and measure the semantic similarities ζ(·, ·) with all existing SEC models in the library. The similarity between a SEC sample and a model, i.e.

1400

ζ(ψi , ψjm ), is computed by comparing rows and columns as described in [8]. If the computed maximum similarity is higher
than an automatically estimated threshold [9], the new SEC
sample ψi is assigned to the most similar model ψΛm , where
Λ = arg max1≤j ≤η (ζ(ψi , ψjm )) and η is the total model number
in the library. The model ψΛm is then updated with additional rows
or columns that might exist in ψi . In this way, the SEC models
will only consist of those rows and columns that are frequently
observed in all assigned SEC samples. If the maximum similarity is lower than the threshold, the SEC sample ψi is introduced
as a new action model, i.e. ψηm+1 . All learned SEC models are
stored in the library as a set of action concepts Ψm = {ψ1m , · · · ,
ψηm+1 }.
4) Visual Quadruples: We assign an action concept index
to a SEC sample and categorize SEC graph nodes, i.e. image
segments, according to their roles in the action. The action
concept index, Λ, of the SEC sample is the index number of
the best fitting SEC model computed in Section III-A3. For
instance, the SEC sample in Fig. 2 has the highest similarity
with the first model ψ1m , thus, Λ = 1.
To categorize SEC graph nodes, we employ the method in
[20], which assumes that each manipulation involves three main
participants: manipulator (M ), primary (P ) and secondary (S )
objects. The manipulator, M , e.g. a hand, frequently interacts
with other objects in the scene and is estimated as the graph node
that participates in most of the relational changes (e.g. from N
to T ) and has the longest touching relations in the SEC. The
object, which has the longest touching relations with M , is then
considered as the primary object P , e.g. a knife in the cutting
action. Secondary objects, S , represent those nodes that interact,
i.e. have touching relations, with P , e.g. a cucumber to be cut.
In Fig. 2, nodes 9, 7, and 8 are respectively categorized as M ,
P , and S . This categorization method does not rely on object
recognition, but applies a rule-based reasoning to identify the
naked graph nodes based on changes in their spatio-temporal
contact relations (see [20]). Although here we focus on unimanual manipulations only, the framework can be extended to
bi-manual manipulations.
The estimated SEC action concept index and object categories
from each manipulation video are stored as a visual quadruple,
i.e. QV = (Λ, M , P , S ), to be later matched with linguistic
counterparts. In Fig. 2, the obtained visual quadruple is QV =
(Λ = 1, M = 9, P = 7, S = 8).
B. Learning Stage: Linguistic Process
We now describe the fully automatic unsupervised generation
of linguistic quadruples. We assume that the conceptualization
of the visual input expressed in natural language corresponds
to our visual encoding of the action by visual quadruples, i.e.
elements of visual quadruples are described by certain syntactic
structures in language.
1) Mechanical Turk Textual Descriptions: For action videos,
we obtain textual descriptions (e.g. Fig. 5), which are processed
to generate symbolic labels for the action, manipulator, and
manipulated objects. The descriptions can be grammatically
incorrect and contain typos. Therefore, we do not rely on one
description per video, but obtain several of them and extract
most frequent labels as described below.

IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 3, JULY 2017

Fig. 4.

Sample frames for eight different actions in the ManiAc dataset.

Fig. 5.

Collected descriptions and extracted linguistic quadruple.

2) Description Parsing: First, each textual description is
parsed in order to abstract from syntactic variations. In the experiment described below, we use the Boxer parser [23].
3) Semantic Roles: In language, verbs often refer to actions.
We extract verbs from the parsed description as potential labels
for the action shown in the video. Each action has participants,
e.g. an agent, a manipulated object, an instrument. In language,
the participants are often described by noun phrases linked to
the verb syntactically. Each participant plays a certain role in
the action called a semantic role [24].
For each verb, we extract fillers (noun phrases) of the semantic
roles of
1) agent (deliberately performing the action, e.g. person),
2) patient (undergoing the action, e.g. carrot in cutting),
3) instrument (the main tool, e.g. knife in cutting), and
4) location (where the action occurs, e.g. bowl in stirring).
We assume that agents are expressed by syntactic subjects, patients—by direct objects, instruments—by prepositional phrases with the instrument prepositions (e.g. cutting with a knife) or by the verb use and its synonyms
(using a knife for cutting), and locations – by prepositional
phrases with location prepositions (e.g. on, from, at). We also
handle constructions like PART of NOUN (e.g. piece of cucumber), where PART is any noun referring to a part of an
object (e.g. piece, slice), and NOUN as the corresponding object
label (e.g. cucumber). Based on these assumptions, we formulate rules for automatically extracting nouns as potential labels
for objects participating in the action. For finding synonyms
and location/instrument prepositions, we use lexical databases
WordNet and FrameNet.1 The mentioned patterns cover most
frequent syntactic constructions, but are not exhaustive. Corpora
mining can be employed to further expand patterns.
A role filler may be expressed by a pronoun or not linked to
the action verb syntactically, e.g. knife is not linked to cut in
“A person grasps a knife and cuts a cucumber”. To resolve the
1 https://wordnet.princeton.edu,

https://framenet.icsi.berkeley.edu

AKSOY et al.: UNSUPERVISED LINKING OF VISUAL FEATURES TO TEXTUAL DESCRIPTIONS

reference or detect missing role fillers which are not covered
by the above patterns, we select the noun in the sentence that
is most likely to be a filler of the corresponding role. For this
purpose, we use a database of dependency tuples extracted from
a large corpus [25]. A dependency tuple consists of a syntactic
relation type, the fillers of the relation, and the frequency of
their co-occurrence in a corpus, e.g. prep phrase, cut, with,
knife, 7399. For each action verb without a role filler, we select
nouns with the highest frequencies to be the role fillers in the
database.
We also extract action and object descriptors (slow, gently,
red, long etc.) expressed by adverbs and adjectives. As a result,
for each textual description, we obtain several semantic structures, each containing a name and descriptors for action, agent,
patient, instrument, and location.
4) Linguistic Quadruples: For each video, the semantic
structures are converted into quadruples in three steps. First, we
calculate frequencies of each linguistic action label (Λ ) in the
semantic structures, so that synonyms in the WordNet database
are considered to be the same label, e.g. cut and slice. Second,
we merge semantic structures that share the action label and
calculate frequencies of each role filler and their descriptors.
Third, we map the fillers of the roles agent, patient, instrument,
and location to manipulator (M  ), primary (P  ) and secondary
(S  ) objects. Agents are directly mapped to M  . If an instrument
filler has a higher frequency than a location filler, the instrument
filler is mapped to P  and the patient filler is mapped to S  ,
e.g. in “cut a cucumber with a knife”, knife and cucumber are
mapped to P  and S  , respectively. Otherwise, the patient filler
is mapped to P  and the location is mapped to S  , e.g. in “put a
box on a cup”, box and cup are linked to P  and S  , respectively.
We finally store the action label and the role fillers
for each video description as a linguistic quadruple, i.e.
QL = (Λ , M  , P  , S  ). For the example in Fig. 2 the obtained quadruple is QL = (Λ = “cut”, M  = “person”, P  =
“knif e”, S  = “cucumber”). Note that each element of QL is
assigned a normalized frequency value and attributes.
C. Robot Memory: Object-Action Concepts (OACs)
We now match the elements of quadruples QL and QV in
order to find the most probable linguistic labels for the learned
SEC models and graph nodes, i.e. image segments in SECs. The
most frequent action label Λ in QL is directly mapped to the
SEC action index Λ in QV , e.g. in Fig. 2, the label “cut” is
mapped to the first SEC model ψ1m . Likewise, M  , P  , and S 
in QL are respectively mapped to their visual counterparts in
QV , cf. Fig. 2. Thus, without applying object recognition, we
can label, for instance, the pink segment, i.e. node 7, in Fig. 2
as “knife” with the attributes “medium sized” and “red”.
The final matched data between QL and QV form
a set of Object-Action Concepts (OACs) as ΩOAC =
{· · · , (Λi , Λi ), (Mi , Mi ), (Pi , Pi ), (Si , Si ), · · · } where i is the
observed manipulation number in the learning stage. The
learned ΩOAC is stored in the robot memory to identify new
visual quadruples detected in the testing stage.
D. Testing Stage
1) SEC Decomposition: The testing phase starts with an
analysis of an unseen long manipulation video, e.g. showing

1401

“making a sandwich”, which needs to be first temporally decomposed into chunks to detect each sequentially or concurrently
performed atomic action such as cutting or stirring.
First, we extract the event chain of a long manipulation sequence. The decomposition process is triggered with the estimation of the manipulator M (see Section III-A4) from the
long SEC. Next, we apply the method introduced in [20], which
searches for [N, T ] and [T, N ] relational changes in SEC rows
that involve the manipulator M . These changes are cutting
points; a change from N to T indicates the start point, whereas
a change from T to N defines the end point of the manipulation.
For instance, when a hand grasps a knife, the relation in the
corresponding SEC row switches from N to T . These semantic
relational changes indicate potential temporal borders, in which
atomic actions take place.
Fig. 3(a) depicts the event chain for a sample manipulation
sequence, in which a hand is removing a cup, putting an apple
down, and hiding it with the cup. In this example, the segment
7 is correctly estimated as the manipulator. Colored blocks in
the SEC highlight sequences of [N, T, · · · , T, N ] relations that
belong to the manipulator. These blocks represent the start and
end points of the three unique SEC chunks.
2) Parsing Visual Quadruples: Once the long SEC Ψ is decomposed, we obtain sequentially performed k different SEC
chunks as Ψ = {ψ1 , · · · , ψk }. In each chunk ψi , we employ the
technique described in Section III-A4 and search for the primary
(Pi ) and secondary (Si ) objects. We assume that each chunk ψi
involves at most one primary object, since a hand can grasp and
manipulate only one object at a time. However, there might be n
different secondary objects, i.e. Si = {sij : j ∈ [1, · · · , n]}. We
treat each sij as an indicator of a potential parallel action stream
because, if different manipulations share the same temporal interval, each has to have a unique secondary object. We apply the
brute force combinatorial process in [20], which assesses each
set of {M , Pi , sij } as one manipulation hypothesis ψij . Each
hypothesis is compared with the learned SEC models in Ψm to
explore the best matching action concept index Λji .
Finally, we construct visual quadruples for each hypothesis
ψj

as QVi = (Λji , M , Pi , sij ). To identify object and action labels,
ψj

we compare each QVi with the already labeled visual quadruples, i.e. the learned ΩOAC (see Section III-C). The comparison of objects is based on matching the color and texture
ψj

features of the image segments in QVi with that of stored
in ΩOAC . The labels of the best matched elements in ΩOAC
ψj
are assigned to the corresponding elements in QVi . We emphasize that this feature comparison step is not employed for
any object detection purpose, but to search for the most similar segments in the memory. This step is not a contribution of
the framework; any other matching method can be employed
instead.
Fig. 3(b) shows the estimated Pi and Si in each SEC chunk.
For instance, in the temporal interval of the blue block in
Fig. 3(a), segment 2 is estimated as P1 , since it has the most
touching relations with the previously detected M (segment 7).
Segments 5 and 8 are estimated as secondary objects s11 and
s12 , since they are the only segments touching to P1 in the blue
block. Thus, the framework returns two parallel manipulation
hypotheses: ψ11 = {7, 2, 5} and ψ12 = {7, 2, 8}, which are most

1402

IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 3, JULY 2017

similar to models ψΛm1 and ψΛm2 . Fig. 3(c) shows the matched
linguistic labels obtained by comparing these object segments
with those in ΩOAC . M is labeled as “person” and P1 as “yellow cup”. Secondary objects s11 and s12 are labeled as “green
apple” and “big red cup”. Fig. 3(d) shows the best matched action labels (Λi ) for each chunk, e.g. the framework detects two
parallel actions labeled as “take” and “reveal” in the blue SEC
chunk.
3) Generating Textual Descriptions: The labeled visual
quadruples are used for generating a textual description. For
each visual quadruple, we generate a sentence “Determiner (A,
The) + manipulator + action verb (3rd person present) + primary object description + preposition + secondary object description”. Object description is given as “determiner (a, the)
attributes (adjectives) object noun”. The positions of primary
and secondary object descriptions can be interchangeable depending on whether a location or an instrument preposition is
more frequently used with the action verb in the MTurk descriptions. For example, the following sentence is generated for the
action sequence in Fig. 3: “A person takes a yellow cup from a
big red cup and reveals a green apple hidden under the yellow
cup. The person takes the green apple from the big red cup.
The person hides the green apple with the yellow cup.” Unknown actions and objects are labeled with “manipulate” and
“something”, respectively.

TABLE I
PERCENTAGE OF DESCRIBED ACTION LABELS FOR EACH SEC MODEL
Λ

ψ 1m

ψ 2m

ψ 3m

ψ 4m

ψ 5m

ψ 6m

ψ 7m

Λ

cut: 88
chop: 4
slice: 4
saw: 4

stir: 100

move: 79
slide: 7
put: 7
push: 7

hide: 76
close: 8
cover: 8
put: 8

take: 76
move: 8
put: 8
remove: 8

put: 92
place: 8

reveal: 68
uncover: 8
take: 8
remove: 8
lift: 8

Fig. 6. Learned object labels for actions cut (left) and stir (right). Each box
indicates a different concept, labels of which are given beneath. Attributes of
each object are given under the corresponding image, if there are any.

IV. EXPERIMENTAL EVALUATION
For the evaluation, we used the publicly available manipulation action dataset ManiAc2 [9] with eight atomic manipulation
types: stirring, cutting, chopping, hiding, putting, taking, pushing, and uncovering. In contrast to other datasets, MainAc focuses on human-object interactions recorded as RGB-D image
streams and has high intra-class variations. For each type, there
are 15 videos, with five human subjects manipulating 30 objects.
Fig. 4 shows a sample frame for each manipulation type. We
used these, in total, 120 manipulations for the learning phase of
our framework.
A. Learning Phase
To obtain textual descriptions for atomic action videos, we
employed the Amazon Mechanical Turk platform. In each
MTurk task description, we showed a video and the instruction:
Watch the video and describe actions and manipulated objects
in the video and their aspects. Example description: A person
is gently cutting a green long cucumber with a red knife. We
collected five descriptions per video, i.e. 600 descriptions for
120 videos in total. We obtained a significant variance between
acquired textual descriptions. Fig. 5 shows example descriptions
and the extracted linguistic quadruple QL for a cutting video.
We obtained one QL per video.
Given the 120 atomic videos, we extracted the corresponding
SEC representations and learned SEC models using the online learning approach (Section III-A3). Our learning method
retrieved 7 SEC models, i.e. action concepts, for 8 manipulation types, such that Ψm = {ψ1m , · · · , ψ7m }. This is because the
2 https://fortknox.physik3.gwdg.de/cns/index.php?page=maniac-dataset

learning approach merged cutting and chopping samples and
generated one single model for both types. Those two manipulations are indeed similar since both yield the same consequence,
i.e. splitting objects into parts. Primitives, i.e. SEC columns, in
both action types are the same. Differences are the followed
motion and velocity profiles of the movements, which are not
captured by SECs.
All 120 SEC samples were compared with these 7 models
in Ψm to predict the action concept index Λ. After categorizing the manipulated objects as M , P , and S , we extracted the
QV representation of each video and matched them with the
QL counterparts to create ΩOAC . Table I shows the distribution of the matched linguistic labels (Λ ) for each SEC model
in Ψm .
For example, model ψ1m was learned from both cutting and
chopping video samples which were mostly (88%) described
as “cut”, but rarely as “chop”,“slice”, or “saw” by the MTurk
annotators. Having mostly the same symbolic labels for the cutting and chopping videos supports our claim that these actions
are semantically similar.
Fig. 6 shows the labeled manipulator, primary and secondary
objects for actions: “cut” and “stir”. For instance, our approach
learned that the primary object for cutting is mostly labeled
as “knife” with various attributes, e.g. “medium sized red” or
“black”. The same label was also learned for the primary object in stirring. Indeed, in the ManiAc dataset, some subjects
selected knives as the tool for stirring. Note that such a topdown reasoning plays a vital role for cognitive robots to explore
grounded object affordances.
We also manually validated the generation of linguistic and
visual quadruples in the learning phase, see Fig. 7. For the visual

AKSOY et al.: UNSUPERVISED LINKING OF VISUAL FEATURES TO TEXTUAL DESCRIPTIONS

1403

TABLE II
AVERAGE ACCURACY OF GENERATED DESCRIPTIONS
One

Sentence description
Action verb
Primary object noun
Secondary object noun

Fig. 7. Accuracy measures in the visual and linguistic quadruple generation
in the learning phase. Note that for linguistic quadruples, we validated verb and
noun labels only; attributes (adjectives and adverbs) were ignored.

quadruples, the accuracy indicates the percentage of correctly
estimated (true-positive) action concept indices and object roles.
For the linguistic quadruples, the accuracy is the percentage of
linguistic labels that we validated as acceptable for describing
objects and actions in the video.
B. Testing Phase
ManiAc also provides 20 long chained activities, e.g. “making
a sandwich” or “preparing a breakfast”. These activities are
composed of over 100 different versions of the 8 atomic actions
and some novel action types, e.g. pouring, which were not seen
in the learning phase. Atomic actions are presented sequentially
or concurrently in different scene contexts. We used these 20
long activities for testing.
First, the 20 activities were converted into SECs, each of
which was decomposed into smaller chunks (Section III-D1).
In each SEC chunk, we parsed all possible action hypotheses by
categorizing the objects as M , P , and S (Section III-D2). An
action concept index Λ was assigned to each hypothesis after
comparing the hypotheses with the 7 models in Ψm . The predicted action indices and temporal borders of SEC chunks were
then compared with the ground truth provided in the dataset to
measure the action decomposition and classification accuracies,
i.e. the average true positive rates. The average decomposition
and classification accuracies are 90.9% and 84.9%, respectively.
These results are comparable with our previous findings reported
in [20], [21].
To generate textual descriptions for the 20 long activities,
we computed the QV representation of each hypothesis and
matched them with the already labeled data in ΩOAC . This
matching process returns linguistic labels for the action and
objects in the hypotheses. Note that novel objects that were
unseen in the atomic videos but appeared in the long videos were
manually appended to the OACs with their linguistic labels. By
employing the matched linguistic descriptors in all hypotheses
(Section III-D3), we generated in total 104 textual descriptions.
Fig. 8 shows the qualitative results of automatically generated
descriptions for three long activities.
We asked three validators to label each sentence, action verb,
and object noun phrase as correct (all description parts are correct), partially correct (some parts are correct), and wrong (all
parts are wrong), cf. Fig. 8. Table II shows percentages of the

Two

All Three

C

C+P

C

C+P

C

C+P

.61
.92
.70
.74

.95
.94
.82
.89

.56
.91
.64
.70

.93
.93
.77
.84

.42
.75
.61
.68

.91
.91
.73
.80

descriptions that are correct (C) and correct or partially correct
(C + P) according to any one, any two, or all three validators. The
inter-annotator agreement Fleiss’ kappa is 0.76. The pairwaise
unweighted Cohen’s kappa is 0.73, 0.83, and 0.71 for each pair
of validators. These values show that the three validators have a
high level of agreement. We also asked the validators to indicate
the number of atomic actions that were not described, which is
measured as 0.26 per video.
Most errors in the generated descriptions resulted from the
errors in the object matching. Lack of context sensitivity was
also an issue. The same manipulation action might need to be
labeled differently depending on the manipulated objects. For
example, if a bowl is used to cover an apple, it can be described
as hiding an apple with a bowl. But sandwich making cannot
be described as hiding the bread with cheese. Another issue
concerns composite entities. If a piece of cheese is put on the
bread, the resulting object is a sandwich, while our method still
calls it cheese.
The runtime of the SEC model learning is about 25 mins,
although the segmentation, tracking, and SEC extraction run in
real-time (25 Hz) on a PC with Intel Core i7 3.33 GHz CPU with
11.8 GB RAM and an Nvidia card GTX 295. Linguistic semantic roles are generated in around 6 ms per textual description
on average, while linguistic quadruples are generated in about
45 ms per video on average. In the testing phase, the average
temporal decomposition and recognition time is about 8 secs
per video.
V. CONCLUSION
We introduced a novel framework for grounding language
in vision by bridging the gap between continuous visual information and discrete linguistic descriptions of manipulation
activities. The framework allows robots to identify and learn
co-joint object-action concepts without prior knowledge.
The proposed framework identifies three action participants:
manipulator, primary and secondary objects. This is clearly a
limitation from a cognitive point of view, because humans distinguish between more action components, e.g. instrument, location, destination, which are reflected in the linguistic descriptions of the scenes. We plan to extend our visual and linguistic
perception to cover more action components for a better cognitive approximation. Another limitation concerns the accurate
segment tracking and object matching methods. Since both are
not in the focus of this study, their naive implementations inject
noise to the generated video descriptions. We plan to employ
more advanced computer vision methods for the improvement.
Context sensitivity and composite entities pose challenges for

1404

IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 2, NO. 3, JULY 2017

Fig. 8. Three long activities together with the automatically generated textual descriptions. Black bars represent start and end points of each detected atomic
action. Green, blue, and red boxes respectively highlight correct, partially correct, and wrong descriptions. Words in red indicate false labels.

the linguistic process (Section IV-B). We will address these issues by (a) using more training data to learn object-dependent
action labels and (b) applying world knowledge resources to
relabel composite entities. Future work also concerns the execution of learned action concepts with robots by employing
generic execution skills, cf. [26]. We also plan to introduce an
interactive dialog allowing robots to ask human demonstrators
for help in identifying unknown object or action concepts not
stored in the OACs. Such human-robot interaction would boost
the performance of our framework in novel scenes, e.g. any
mismatched concept in the new scene can be directly corrected
by the human. To evaluate the scalability of our framework, we
plan to benchmark it with more manipulation datasets (e.g. [15])
from various domains.
REFERENCES
[1] S. Guadarrama et al., “YouTube2Text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition,” in
Proc. 14th Int. Conf. Comput. Vision, 2013, pp. 2712–2719.
[2] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney, K. Saenko, and
S. Guadarrama, “Generating natural-language video descriptions using
text-mined knowledge,” in Proc. 27th AAAI Conf. Artif. Intell., 2013,
pp. 541–547.
[3] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and
K. Saenko, “Sequence to sequence-video to text,” in Proc. Int. Conf.
Comput. Vision, 2015, pp. 4534–4542.
[4] T. Nakamura, T. Nagai, K. Funakoshi, S. Nagasaka, T. Taniguchi, and
N. Iwahashi, “Mutual learning of an object concept and language model
based on MLDA and NPYLM,” in Proc. Intell. Robots Syst., 2014,
pp. 600–607.
[5] Y. Yamada, W. Takano, and Y. Nakamura, “Statistical behavioral understanding by motion, object, and language,” in Proc. Int. Fed. Promotion
Mech. Mach. Sci., 2015, pp. 1–6.
[6] W. Takano and Y. Nakamura, “Statistical mutual conversion between
whole body motion primitives and linguistic sentences for human motions,” Int. J. Robot. Res., vol. 34, no. 10, pp. 1314–1328, 2015.
[7] S. Harnad, “The symbol grounding problem,” Phys. D, Nonlinear Phenomena, vol. 42, pp. 335–346, 1990.
[8] E. E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and F. Wörgötter,
“Learning the semantics of object-action relations by observation,” Int. J.
Robot. Res., vol. 30, no. 10, pp. 1229–1249, 2011.
[9] E. E. Aksoy, M. Tamosiunaite, and F. Wörgötter, “Model-free incremental
learning of the semantics of manipulation actions,” Robot. Auton. Syst.,
vol. 71, pp. 118–133, 2015.

[10] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S. C. Zhu, “I2T: Image parsing
to text description,” Proc. IEEE, vol. 98, no. 8, pp. 1485–1508, Aug. 2010.
[11] G. Kulkarni et al., “Baby talk: Understanding and generating simple image
descriptions,” in Proc. Conf. Comput. Vision Pattern Recognit., 2011,
pp. 1601–1608.
[12] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko, and R. Mooney,
“Integrating language and vision to generate natural language descriptions
of videos in the wild,” in Proc. 25th Int. Conf. Comput. Linguistics, 2014,
pp. 1218–1227.
[13] H. Yu, S. Narayanaswamy, A. Barbu, and J. M. Siskind, “A compositional
framework for grounding language inference, generation, and acquisition
in video,” J. Artif. Intell. Res., vol. 52, pp. 601–713, 2015.
[14] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele,
“Translating video content to natural language descriptions,” in Proc. Int.
Conf. Comput. Vision, 2013, pp. 433–440.
[15] Y. C. Song et al., “Unsupervised alignment of actions in video with text
descriptions,” in Proc. Int. Joint Conf. Artif. Intell., 2016, pp. 2025–2031.
[16] J. Graf, S. Puls, and H. Wörn, “Recognition and understanding situations
and activities with description logics for safe human-robot cooperation,”
in Proc. COGNITIVE, 2010, pp. 90–96.
[17] K. Lee, Y. Su, T.-K. Kim, and Y. Demiris, “A syntactic approach to robot
imitation learning using probabilistic activity grammars,” Robot. Auton.
Syst, no. 12, pp. 1323–1334, 2013.
[18] L. Yao et al., “Video description generation incorporating spatio-temporal
features and a soft-attention mechanism,” CoRR, 2015.
[19] N. Krüger et al., “Object-action complexes: Grounded abstractions
of sensory-motor processes,” Robot. Auton. Syst, vol. 59, no. 10,
pp. 740–757, 2011.
[20] E. E. Aksoy, M. J. Aein, M. Tamosiunaite, and F. Wörgötter, “Semantic parsing of human manipulation activities using on-line learned
models for robot imitation,” in Proc. Intell. Robots Syst., 2015,
pp. 2875–2882.
[21] E. E. Aksoy, A. Orhan, and F. Wörgötter, “Semantic decomposition and
recognition of long and complex manipulation action sequences,” Int. J.
Comput. Vision, vol. 122, no. 1, pp. 84–115, 2017.
[22] A. Abramov, K. Pauwels, J. Papon, F. Wörgötter, and B. Dellen, “Depthsupported real-time video segmentation with the kinect,” in Proc. IEEE
Workshop Appl. Comput. Vision, 2012, pp. 457–464.
[23] J. Bos, “Wide-coverage semantic analysis with boxer,” in Proc. Conf.
Semantics Text Process., 2008, pp. 277–286.
[24] C. J. Fillmore, “The case for case,” in Universals in Linguistic Theory,
E. Bach and R. T. Harms, Eds. New York, NY, USA: Holt, Rinehart and
Winston, 1968.
[25] E. Ovchinnikova, V. Zaytsev, S. Wertheim, and R. Israel, “Generating conceptual metaphors from proposition stores,” arXiv preprint
arXiv:1409.7619, 2014.
[26] M. Wächter, S. Ottenhaus, M. Kröhnert, N. Vahrenkamp, and T. Asfour,
“The armarX statechart concept: Graphical programing of robot behavior,”
Frontiers Robot. AI, vol. 3, p. 33, 2016.

2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems
October 7-12, 2012. Vilamoura, Algarve, Portugal

Using a Minimal Action Grammar for Activity Understanding in the
Real World
Douglas Summers-Stay, Ching L. Teo, Yezhou Yang, Cornelia Fermüller and Yiannis Aloimonos
Abstract— There is good reason to believe that humans
use some kind of recursive grammatical structure when we
recognize and perform complex manipulation activities. We
have built a system to automatically build a tree structure
from observations of an actor performing such activities.
The activity trees that result form a framework for search
and understanding, tying action to language. We explore and
evaluate the system by performing experiments over a novel
complex activity dataset taken using synchronized Kinect and
SR4000 Time of Flight cameras. Processing of the combined
3D and 2D image data provides the necessary terminals and
events to build the tree from the bottom-up. Experimental
results highlight the contribution of the action grammar in: 1)
providing a robust structure for complex activity recognition
over real data and 2) disambiguating interleaved activities from
within the same sequence.

I. I NTRODUCTION
How do humans come to understand, recognize, and
replicate complex actions? Even if we have witnessed several
occurrences of the same activity, each will be unique in terms
of the order actions are performed, the explicit motion of the
limbs involved, and the appearance of the objects involved.
Somehow, the sensory data must be stored in a greatly
compressed representation that captures relevant information
while discarding what is irrelevant. This representation must
be capable of handling actions of any complexity, where
activities are composed of previously known actions and subactions.
This suggests that the brain uses a similar method for
understanding both language and actions. This idea has
support on neuroscientific, anthropological, and behavioral
grounds. In the early 1950’s, psychologist Karl Lashey
suggested that syntax may apply to goal-directed actions
as well as to language [28]. Archaeologist Leroi-Gourhan
argued that tool making and use reflects a capability for
compositionality of structures, linking language and action
[5]. Two-year old children have been shown to have the
ability to recognize and reproduce hierarchically organized
actions [2], [37]. Additionally, the same parts of the brain that
have long been understood to be used in language production
(such as Broca’s Area) have been found to be crucial to the
process of action planning [8], [9].
This material is partly based upon work presented at the 2011 Telluride
Neuromorphic Cognition Engineering Workshop, supported by the National
Science Foundation. In addition, the support of the European Union under the Cognitive Systems program (project POETICON++) is gratefully
acknowledged.
The authors are from Department of Computer Science,
University of Maryland, College Park, MD 20742, USA
{dss,cteo,yzyang,fer,yiannis}@umiacs.umd.edu

978-1-4673-1736-8/12/S31.00 ©2012 IEEE

If such a representation is taken to be an innate, central
aspect of both language and activity understanding, it must be
simpler and more fundamental than the grammars we learn
for each individual language. It also must be a generative
grammar (rather than one used purely for recognition) in
order to allow an individual to learn to perform actions by
example. Chomsky’s minimalist program is an attempt to
discover such a universal generative grammar for language,
with the expectation that it plays a more general cognitive
role. A generative grammar consists of a set of elements
and a set of production rules that allow the formation of
grammatical sentences. Context-free grammars are generative grammars which have recursive rules which allow
nesting of elements in the same type of elements. Although
context-free grammars are sufficiently expressive to handle
the complexity of language, they cannot account for what we
actually see in natural languages, such as agreement (in case,
number, or gender) and reference (such as relative clauses.)
These long-distance dependencies cannot be captured by
context-free grammars. The Chomskyan Minimalist Program
deals with this through a number of transformations on the
output of context-free grammars [6].
In [25], Pastra and Aloimonos introduce a minimalist
grammar of action which defines the set of terminals, features, non-terminals and production rules for such a grammar
in the sensorimotor domain. However, this was a purely
theoretical description. The action grammars used in our
experiments are an implementation of such a grammar in a
system that is capable of sensing and interpreting real-world
situations under a wide range of natural conditions. Such a
representation is a natural summary of the important aspects
of an activity, which abstracts away such details as who is
performing the action, where it is being performed, how the
objects involved are spatially located, and the appearance of
the objects and body parts. What is left is a tree structure
that captures the order in which tools, objects and hands are
brought together and separated, a structure which is easy to
store, search, and compare to other such trees.
In order to make the use of the grammar practical, we
have designed the system so that it will be able to handle and
abstract away a wide range of realistic conditions, such as
varying viewpoint, lighting, surrounding environment, object
and actor appearance.
The activities we are attempting to recognize and understand are complex, concrete human activities. Actions like
“stirring” or “tightening a bolt,” the traditional purview of
action recognition techniques, are represented by a single
node in the action tree. (For this reason, we refer to what

4104

we are doing as “activity recognition” rather than “action
recognition.”) Abstract actions, like “doing research,” or
“playing soccer” contain important steps which take place
in terms of mental or data structures, which we have no
way to detect or estimate with the current setup. Instead
we are looking at multi-step activities which involve the
manipulation of physical objects towards some goal state.
This is basically any form of manual labor: the physical
work of craftsmen, home builders, factory workers, chefs,
janitors, and so forth. These are also largely the kinds of
activities which we would hope for a general purpose robot
to be able to perform.
“Action recognition” could interact with activity recognition in two important ways, both assisting with and being
assisted by activity recognition. First, activity recognition
provides important context for action recognition. One of
the main difficulties in action recognition is finding when
the action begins and ends. Forming an action tree provides
natural endpoints for individual actions: these actions occur
between the time a tool (including the hands) comes into
contact with an object and the time when it breaks such
contact. Knowing what the tool and object are provides
significant constraints on what the action might be, reducing
it to a handful of possibilities with any significant probability
of occurring. Second, when action recognition is performed,
the action can be used as a label on part of the activity tree,
which improves our ability to match with similar activities.
II. R ECENT W ORKS
The problem of action recognition and human activity has
been an active research area in Computer Vision, motivated
by several promising applications, such as human-computer
interface, video indexing and retrieval and video surveillance,
etc. Several excellent surveys on the topic of visual recognition are available [22], [34]. But non-visual descriptions,
using motion capture systems, have also been of interest
in Computer Vision and Graphics. Many of those studies
are concerned with dimensionality reduction techniques, that
provide a good characterization for classification [3], [36],
[20]. Most of the focus in visual action analysis was on the
study of human actions that were characterized by movement
and change of posture, such as walking, running, jumping
etc. The dominant approaches to the recognition of single
actions compute statistics of spatio-temporal interest points
[18], [38], [7] and flow in video volumes as descriptors,
or represent short actions by stacks of silhouettes [11],
[39]. Approaches to more complex, longer actions employ
parametric approaches, such as Hidden Markov Models [15],
Linear Dynamical Systems [30] or Non-linear Dynamical
Systems [4], which are defined on tracked features or optic
flow presentations.
To capture the semantics of complex activities, higher level
reasoning methods are required. A number of approaches
use stochastic context free grammars with the primitives
beings body parts [29] or trajectories [14], and some also
include the interaction with objects [23]. To model the
temporal constraints, several approaches have used Hidden

Markov Models, which allow to exploit the relation between
specific objects and actions [13], [24]. A related class of
approaches use dynamic Bayesian networks to divide the
temporal sequence into sub-sequence and define relative
temporal relations [27], [10], [19].
Most closely related to our work are a few recent studies
on hand manipulation actions. In [35] manipulation actions
are represented as a sequences of motion primitives. The
process is modeled using a combination of discriminative
support vector machines and generative hidden Markov models. In [16] hands and objects segmented from the video and
shape-based hand/object features and manipulation features
are defined to provide a sequence of interrelated manipulations and object features. Semantic manipulation object
dependencies are extracted using conditional random fields.
In [33] manipulations in a breakfast scenario are analyzed.
The image sequence is represented by an activity graph that
codes spatiotemporal object interactions. Event classes are
extracted from the activity graphs, where each event class
encodes a similar pattern of spatiotemporal relations between
corresponding objects, but the objects are known beforehand.
While all these approaches use task-dependent primitives,
our approach is general; its basics are simply the merging and
parting of objects. A similar idea was pursued by [1] for the
analysis of short stereo video sequences of of hand motions
manipulating a number of objects. Relations between object
at decisive time points during manipulation, such as when
two objects touch or overlap, are stored in a transition matrix.
Using simple sub-string search algorithms different matrices
are compared for recognition. The objects in these sequences
are however easily visually recognized, and the approach was
only applied to short activities, such as putting two objects
on a plate.
III. A PPROACH
We describe the overall approach of using the action
grammar for activity understanding (see Fig. 1) by first
introducing the experimental dataset in sec. III-A. Next, we
define the action grammar and how it is created in sec. IIIB. We then detail how the important subcomponents: hand
state determination and object recognition are achieved in
secs. III-C and III-D respectively. With these detections,
we illustrate how an activity tree can be built (sec. III-E)
and be used for comparing the similarity between different
trees (sec. III-F) and how the action grammar is useful for
separating complex interleaved activities in sec. III-G.
A. Kinect+SR4000 Complex Activity Dataset
We introduce a novel dataset that contains five complex
hand manipulation activities performed by four different
human actors. Each activity is defined by the completion of
a complex object or entity: for example, making a sandwich.
The task of creating this entity is further comprised of
nine specific actions which may involve the use of different
kinds of hand-tools and objects. Different actions could be
concatenated to form novel activities: Cooking vegetables +
making a sandwich. Other well known datasets such as the

4105

the supplementary material1 .

Fig. 2. Data collection setup. The Kinect is mounted on an Erratic mobile
robot base, the SR4000 is mounted on the side nearer to the table where
the actions are performed.

B. The Action Grammar
Fig. 1. Overview of the approach: (1) Pointcloud data and RGB-Depth
data are extracted and processed from the SR4000 and Kinect cameras
respectively. (2) Hands and objects are detected from pointclouds and the
human pose is extracted from Kinect. (3) The detected objects are then
combined using an action grammar treebank to produce an activity tree.

KTH, Weizmann or Human-EVA datasets [31], [11], [32] do
not involve hand-tools. The human-object interaction dataset
by Gupta et al. [12] has only four objects with extremely
simple actions. The dataset by Messing et al. [21] has only
four simple actions with tool use. The CMU Kitchen Dataset
[17] has several actions performed by 18 subjects for five
recipes, but many of the actions are blocked from view due
to the placements of the four static cameras.
The Complex Activity Dataset extends beyond these
datasets by considering the compositional aspect of the activity in terms of the entities created by the actions involved
in each step. The activities are classified into two general
categories: Kitchen and Crafts, each with eight separate
video sequences captured from two externally synced and
calibrated active sensors: 1) the Kinect which provides RGBDepth and 2) a Swissranger SR4000 Time of Flight camera
which provides Intensity(Grayscale)-Depth. The Kinect camera is positioned frontal-parallel at a distance of ≈ 6m from
the actor so that we can track the entire body motion, while
the SR4000 is positioned ≈ 1.5m from the actor on the side
so that hand-actions and objects can be clearly seen (see
Fig. 2). In total, there are 16 video sequences made from
different combinations of activities and objects. The list of
activities, actions and objects considered are summarized in
Table I. Sample sequences from the dataset are available in

The grammar has one simple rule, which is applied
repeatedly:
An activity consists of
1) Using a tool to bring two objects together, resulting in
a new object or a tool or
2) Using a tool together with a single object, resulting in
a transformed object or tool
These tools or objects can themselves be the result of an
activity, which gives rise to the tree structure of activities.
The new object can be a collection like “a piece of bread on
a plate” formed by bringing together a slice of bread and a
plate. The point is that after they have been brought together,
they are treated as one combined object, temporarily, as the
bread moves together with the plate.
There are theoretically two ways to use the grammar.
In this paper we parse the actions that take place, starting
with recognizing simple actions (of type 1 or 2, above) and
building them up into an activity tree, an example is shown
in Fig. 5 (right). Every non-terminal node of this tree is
an action. The other way to use the grammar would be in
a generative way: starting with an activity one wanted to
perform, and working out how to do it. One would simply
look for an activity one has observed resulting in the final
object one wanted to have, find out what the input objects and
actions to attain that are needed, and what activities result in
those objects, and so on, breaking it down to the point that
the objects and tools needed are the ones available.
C. Extracting Hand Locations from 3D Pointclouds
Since we are concerned with manipulative actions, the
terminals in the action grammar trees are the objects/tools
that are currently manipulated by the hands. An approach
1 More information on the dataset and how the data is collected can
be found at: http://www.umiacs.umd.edu/research/POETICON/umd_

complex_activities/

4106

Activity Class

Name
Cooking Vegetables

Actions
{slice, cut}

Making Sandwich
Sewing a Toy

{spread, slice}
{cut, pin, thread, sew}

Card Making

{cut, paste, write,
fold}
{screw, push, twist}

Kitchen

Crafts

Assemble a Machine

Objects/tools
{cucumbers, carrots, tomatoes, apple, chopper}
{bagel, ham, cheese, knife}
{cloth, paper, needle, thread,
scissors}
{paper,
glue,
scissors,
marker}
{wrench, nut, bolt, frames}

TABLE I
L IST OF MANIPULATION ACTIVITIES CONSIDERED .

that passively searches for objects and tools in the video
will not be sufficient as many objects are visible on the table
but are not participating in the activity. Instead, we actively
search for hands in each frame, and determine if the hand is
currently occupied with an object or free directly from 3D
pointclouds – a binary hand state Hs = {occ, f ree}. Once
we know the approximate location of each hand and its state,
a trained object classifier can then be used only on these
regions, which reduces processing time and false positives.
Note that we process pointclouds from the SR4000 since it
is nearer to the actor than the Kinect and provides a clearer
view for object recognition.

in the SR4000 camera coordinates. However, relying solely
on the Kinect for hand tracking is unreliable since it may
fail, especially when half the body is blocked by a table
(see Fig. 2). Our proposed approach is to 1) robustly extract
potential hand regions from SR4000 pointclouds, and 2)
combine it with the predicted locations from Kinect, so as
to determine the final locations of the hands in SR4000 and
its hand state: occupied or free. We use PCL 1.43 as the
main pointcloud processing library to first remove obvious
outliers by filtering out points that have low confidence
values or those does not belong to any obvious cluster. A
plane estimation procedure using RANSAC is then applied
to estimate a planar model that represents the table surface.
The estimated coefficients are then used to reproject the
remaining points so that a convex hull is created from
which points in the original cloud that are within the convex
hull (table points) are removed. A 3D Euclidean clustering
is then applied to obtain reasonable pointcloud clusters.
The predicted hand locations from Kinect are then used
to extract the hand pointclouds if the location is within a
fixed distance threshold of the cluster’s centroid. Finally, the
extrema of each hand pointcloud is computed from which we
use a region growing segmentation algorithm using nearest
neighbors to extract the hand and any associated objects/tools
that are in contact with the hand (Fig. 3(d)). The current hand
state Hs is obtained from the difference in the pointcloud
sizes against a running average of previous pointcloud sizes.
A significant deviation beyond a ratio threshold will indicate
that the hand is occupied (ratio > 1) or empty (ratio < 1). In
addition, if only a single hand pointcloud is detected, and its
current size is approximately equal to the combine sizes of
the the left and right hand in previous frames, a merge event
is raised. This will be important for building the activity tree
(sec. III-E).

Fig. 3. Detecting hand locations from SR4000 pointclouds. (1) Outliers
are first removed, (2) Table surface is then estimated, (3) Objects and hands
are extruded and clustered from reprojected convex hull, and (4) Predicted
hand pointcloud locations.

D. Object Recognition

The procedure is summarized on Fig. 3. The inputs are
the pointclouds obtained from the SR4000 and the tracked
skeleton from the Kinect2 . Since both cameras are calibrated,
the approximate 3D locations of the tracked hands are known
2 PrimeSense

The activity tree is built when there are changes Hs for
each hand (left and right), and the terminals are the objects
(if any) on each hand when Hs changes. Using the results
of the predicted hand locations and states described in the
previous section, we crop out a rectangular region slightly
larger than the hand point cloud size to obtain an intensity
image of the potential objects/tools should Hs = occ (Fig. 4).
3 http://www.pointclouds.org

OpenNI implementation was used to obtain the skeleton.

4107

We also extract the cropped region whenever a merge event
starts or ends.

Fig. 4. Merging occupied hand pointclouds with intensity image for object
recognition.

We extract Histogram of Gradient (HoG) features from the
cropped image from which an object classifier is then used to
predict the object label. Classifiers for each object/tool class
are trained over a separate training set of labeled data using
a degree three polynomial SVM. We select the object labels
from the classifier that gives the highest response for the case
when Hs = occ. Due to the large amounts of occlusions
when a merge event occurs, we select from the top N =
4 detection responses the most consistent object label from
the previous frames (since it is unlikely that an object label
changes when a merge occurs).
E. Building the Activity Tree

when objects are brought together and begin to be treated
as a single object (a merge event). This provides enough
information to build the activity tree, as shown in Fig. 5. The
parser creates a new leaf node whenever one of the actor’s
hands (or tools held in the hands) come into contact with a
new object. These nodes keep track of the time the object
was first and last seen, and what the object was recognized as
(using the HoG object recognition described in sec. III-D.) If
these objects are brought together, a new node is created with
each of the original object nodes as children. This process
gradually builds up tree structures.
Detecting when objects are brought together is not a
foolproof method of recognizing when the objects begin to
be treated as one combined object. One may, for instance,
pick up two unrelated objects in one hand just to clear away
a working space. In videos where this kind of event happens
frequently, a better means of recognizing a significant, meaningful contact would be needed, or a better way of tracking
where the objects ended up during a merge event.
To build a robust system, it would be useful to have several
examples of each activity one wanted to recognize, and
measure whether the activity tree from an unknown activity
fell into this cluster. There are many ways to perform any
object manipulation activity. Certain aspects proved to be
highly variable. Objects were frequently handled, set down,
and picked up again, or passed from one hand to another,
in an irregular way. A cutting action might be followed by
a second cutting action if the results of the first cut were
deemed unacceptable. The order of some of the actions
differed from one trial to another.
Other aspects of the activities however, were much more
consistent. In order to correctly perform the tasks, particular
objects needed to come together and not be separated again
before the end of the activity. In the Sewing a Toy activity, for
example, the pieces of felt needed to be joined together with
the thread. Recognizing these critical merge events is crucial
for understanding the activities, and is sufficient for these
activities which all consist of assembling objects. In a richer
grammar, sub-actions trees could be merged based on other
criteria. For example, when disassembling an object, subactions may arise when certain objects are separated from
each other, rather than only when they come together.
F. Comparing Activity Trees

Fig. 5. Creating an Activity Tree: (Left) Events and objects detected from
SR4000 intensity images. (Right) Formation of an activity tree that parallels
the events and objects occurrence, based on the defined action grammar.

The previous steps tell us what objects or tools are
grasped by each hand at each frame of the recording, and

In order to compare the trees, we use a measure of
tree edit distance. The edit distance between two trees is
the minimal-cost sequence of edit operations on labeled
nodes that transforms one tree into the other. The following
operations are possible, each with its own cost:
• inserting a node
• deleting a node
• renaming a node (changing the label)
Efficient algorithms exist to find the minimal edit distance.
The most recent advancement was made by Pawlik et. al
[26] which has O(n3 ) time with n > m and O(mn) space
complexity for the worst case. For the small sizes of trees
we encounter, solving this takes negligible time and memory.

4108

Using the tree edit distance is critical for discriminating
situations where the same objects are used in different ways.
For example, if one were to put drafting instruments into
a case to bring to school, the tree would consist of each
instrument being merged with the case one by one. However,
if one were to use the instruments for drafting, a more
complex tree would be created, where the triangle is used
with the pencil and paper, then moved out of the way, and so
forth. Forming activity trees allows us to capture the structure
of this interaction.
G. Separating Interleaved Activities
In many cases, an activity is an uninterrupted sequence
of related events. In this case, segmenting the activity in
a recording means simply finding the beginning and end
point of the activity. However, there may be interruptions,
in which the actor is trying to deal with more than one
goal simultaneously. This results in actions that are mixed
together. By parsing these actions, we are able to get a fine
grained segmentation of the recording, identifying which actions belong to each activity, even when they are thoroughly
mixed. To demonstrate this, several activities in the Complex
Activity Dataset contain interleaved actions of different activities combined together. For example, the actor was asked
to perform a cutting and pasting task (Making Card) and the
Assemble a Machine task, interleaving the actions for each
activity. Because the activities involved separate objects, we
were able to use the grammar to successfully separate out
the actions for each task. This is shown in Fig. 6.

count how many trees are correctly matched and report the
accuracy score per level.
The next experiment evaluates the action grammar over
twelve activities from the Complex Activity Dataset. In this
part, we used a leave-one-out training procedure to train the
object classifiers – for each test sequence, the remaining
eleven sequences were used for training. Note that four
sequences involving Sewing and Assembling a Machine are
left out of the evaluation due to the fact that the object
recognition simply failed as the objects of interests: pins,
bolts, etc. are too small4 . We then report the normalized tree
edit distances of the resulting activity trees when they are
compared with the ground truth, together with the amount
of terminal corruption per sequence. As a comparison to
highlight the contribution of the action grammar in building
the activity tree, we also report the activity recognition
performance when only the terminals are used to build a
degenerate tree of depth 1 only (a flattened tree).
B. Results over Artificial Noisy Data
The accuracy scores over increasing degree of terminal
corruption are summarized in Fig. 7.

IV. E XPERIMENTS
We report the results of two experiments that evaluate the
performance of the action grammar in recognizing complex
manipulation activities. We first derive theoretical bounds
of the expected performance by inducing artificial noise
in the terminals (sec. IV-B) and then evaluate the performance of recognizing activity trees over real data from the
Kinect+SR4000 Complex Activity Dataset (sec. IV-C).
A. Experimental Procedure
For the experiment that explores the theoretical performance of the action grammar, we manually induced corruption in the input terminals of each activity tree from the
Complex Activity Dataset in two ways: 1) by sampling from
a uniform distribution of all possible object labels considered
(except the ground truth) and 2) by consistently selecting
the object labels from only one but a different activity tree
for each associated object: e.g, if the activity was Card
Marking, we will replace object labels consistently from
another activity such as Cooking Vegetables. We considered
corruption of the input ranging from 10% (almost correct)
to 90% (almost all wrong) and report the accuracy scores
in interpreting the activity using the corrupted activity tree
using the following procedure: for each level of corruption,
we compute the edit distances for each tree, and take the
ground truth identity of the smallest edit distance. We then

Fig. 7. Accuracy scores with varying degrees of terminal corruption: 1)
Randomly replaced object labels (red solid line) and 2) Replaced object
labels consistently from another (incorrect) tree (blue dotted line).

The activity trees are robust enough to handle the fact
that no object detection method is completely accurate. In
an attempt to characterize the behavior of tree matching in
the presence of noise, we considered two possible causes
of terminal corruption as described in the previous section.
In the first case where the missed detections are completely
random (the red solid line), the trees perform fairly well,
accurately matching the true tree to the partially mislabeled
tree in all cases until 40% of the labels have been replaced. In
the second case (the blue dotted line), all the incorrect labels
come from a single incorrect tree and so are consistent with
each other. In this worst case scenario, the performance does
worse, and errors in recognition show up when 20% of the
labels are incorrect.
C. Results over Complex Activity Dataset
We summarize the matching performance for the twelve
test activity trees in Fig. 8 and compare it against the baseline
4 the specific sequences used and left out can be found at http://www.
umiacs.umd.edu/research/POETICON/umd_complex_activities/

4109

Fig. 6. A complex interleaved sequence Making Card + Assemble a Machine can be cleanly separated into its component activity trees using the action
grammar.

method of using terminals alone (Fig. 9).

Activity
Card Making
Card Making+Assemble
Machine
Making Sandwich
Cooking Vegetables
Cutting Applesa

aA

Label
Card(1)
Card+Assbly(1)

Corruption
0.48
0.55

Label
Card(2)
Card+Assbly(2)

Corruption
0.49
0.48

SandW(1)
Veg(1)
Veg(3)
Apple(1)

0.55
0.42
0.54
0.55

SandW(2)
Veg(2)
Veg(4)
Apple(2)

0.36
0.62
0.47
0.42

Fig. 9. Confusion matrix of normalized tree edit distances when terminals
are used alone. Lower values along the diagonals are better. Boxes indicate
the diagonal blocks of interest for each set.

subset of the Cooking Vegetables activities

Fig. 8. (Above) Confusion matrix of normalized tree edit distances for
each of the twelve test sequences. Lower values along the diagonals are
better. Boxes indicate the diagonal blocks of interest for each set. (Below)
Amount of corrupted terminals [0, 1] per testing sequence. A value closer
to 1 means more corruption.

In order to measure how well the activity trees could be
used for activity recognition in real data, we computed the
tree edit distance between each test tree and the ground
truth for each of the activities. Each activity comes as a set
containing at least two similar sequences. For example, Card
Making has two sequences: Card(1) and Card(2), performed
by two different actors which introduces a small amount
of variation within each activity set itself. In the confusion
matrix above, the blocks of low edit distances along the
diagonal for each activity set and higher distances elsewhere
indicate that the activity trees are finding fairly good matches
among the correct set of activities (Fig. 8 (above)). This

performance is achieved in spite of the high levels of
corruption in the terminals (Fig. 8 (below)) of between 36%
to 62% that are sufficient to degrade performance (shown in
the first experiments), which is indicative of the robustness
of the approach in noisy real data.
By way of comparison, we flattened the trees so that all the
nodes were at the same level (depth 1) and repeated the same
experiment (Fig. 9). This effectively eliminates the effect of
using the action grammar. In this case, the diagonal structure
is much less evident. This is especially true for activity sets
that contains complex interleaved activities such as Card
Making + Assemble a Machine and Cooking Vegetables. As
was explained in sec. III-G and illustrated in Fig. 6, the
ability of the action grammar in disambiguating complex
interleaved activities is shown by the fact that the block
diagonals for such activities display lowered performance
when flattened trees are used (the tree edit distances are much
higher within each block) compared to the ones when the full
action grammar is used in the previous experiment (Fig. 8).

4110

V. C ONCLUSION AND F UTURE W ORK
Using a grammar of action to build activity trees appears
to be a practical way to begin to parse complex activities.
While the results show that we are able to discriminate these
particular activities, the variability between performance at
recognizing various activities seems to indicate that for a
larger or more general selection of activities, the simple rules
of this grammar might be insufficient. We would like to
include the ability to build up trees based on disassembly
and transformation as well as assembly. This will probably
require keeping track of the history of each of the objects
involved in the activity.
The grammar described here could easily be extended
to include more specific patterns to be matched, creating a
richer grammar that provides immediate information about
actions and subactions. More traditional action recognition
techniques could also be incorporated. For example, when
a tool touches an object, we could determine whether the
tool is actually performing its function on the object and
transforming it, or just coming into contact with it. Object
recognition could be improved by feedback from the tree
structure, increasing the probability of detection for an object
consistent with the rest of the tree. The activity trees could
also be used by a robot to emulate activities it has seen
performed previously by humans, or even generated based
on goals the robot is trying to attain.
R EFERENCES
[1] E. E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and
F. Wörgötter. Learning the semantics of object-action relations by
observation. International Journal of Robotics Research, 30(10):1229–
1249, 2011.
[2] P. Bauer. Recalling past events: from infancy to early childhood.
Annals of Child Development, 11:25–71, 1995.
[3] R. Chalodhorn, D. Grimes, R. R. Gabriel, and M. Asada. Learning humanoid motion dynamics through sensory-motor mapping in reduced
dimensional spaces. In ICRA, 2006.
[4] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal. Histograms
of oriented optical flow and binet-cauchy kernels on nonlinear dynamical systems for the recognition of human actions. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition, 2009.
[5] J. Chavaillon. Andr leroi-gourhan, le geste et la parole. L’Homme,
7(3):122–124, 1967.
[6] N. Chomsky. Lectures on Government and Binding: The Pisa Lectures.
Mouton de Gruyter, 1993.
[7] P. Dollár, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In VS-PETS, October 2005.
[8] L. Fadiga, L. Fogassi, V. Gallese, and G. Rizzolatti. Visuomotor
neurons: ambiguity of the discharge or motor perception? International
Journal of Psychophysiology, 35:165–177, 2000.
[9] L. Fogassi, P. F. Ferrari, B. Gesierich, S. Rozzi, F. Chersi, and
G. Rizzolatti. Parietal lobe: From action organization to intention
understanding. Science, 308:662 – 667, 2005.
[10] S. Gong and T. Xiang. Recognition of group activities using dynamic
probabilistic networks. In Proc. International Conference on Computer
Vision, 2003.
[11] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions
as space-time shapes. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 29(12):2247–2253, 2007.
[12] A. Gupta and L. S. Davis. Objects in action: An approach for
combining action understanding and object perception. In CVPR. IEEE
Computer Society, 2007.
[13] S. Hongeng and R. Nevatia. Large-scale event detection using
semi-hidden markov models. In Proc. International Conference on
Computer Vision, 2003.
[14] Y. Ivanov and A. Bobick. Recognition of visual activities and
interactions by stochastic parsing. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2000.

[15] A. Kale, A. Sundaresan, A. N. Rajagopalan, N. P. Cuntoor, A. K. RoyChowdhury, V. Kruger, and R. Chellappa. Identification of humans
using gait. IEEE Transactions on Image Processing, 13(9):1163–1173,
2004.
[16] H. Kjellstrom, J. Romero, and D. Kragic. Simultaneous visual
recognition of manipulation actions and manipulated objects. In Proc.
European Conference on Computer Vision, 2008.
[17] F. D. la Torre, J. Hodgins, J. Montano, S. Valcarcel, R. Forcada, and
J. Macey. Guide to the carnegie mellon university multimodal activity
(cmu-mmac) database. Technical report, CMU-RI-TR-08-22, Robotics
Institute, Carnegie Mellon University, July 2009.
[18] I. Laptev. On space-time interest points. International Journal of
Computer Vision, 64(2–3):107–123, 2005.
[19] B. Laxton, J. Lim, and D. Kriegman. Leveraging temporal, contextual
and ordering constraints for recognizing complex activities in video. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition,
2007.
[20] Y. Li, C. Fermuller, Y. Aloimonos, and H. Ji. Learning shiftinvariant sparse representation of actions. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, 2010.
[21] R. Messing, C. Pal, and H. Kautz. Activity recognition using the
velocity histories of tracked keypoints. In ICCV ’09: Proceedings
of the Twelfth IEEE International Conference on Computer Vision,
Washington, DC, USA, 2009. IEEE Computer Society.
[22] T. B. Moeslund, A. Hilton, and V. Krüger. A survey of advances in
vision-based human motion capture and analysis. Computer Vision
and Image Understanding, 104:90–126, 2006.
[23] D. Moore and I. Essa. Recognizing multitasked activities using
stochastic context-free grammar from video. In Proceedings of AAAI
Conference,, 2002.
[24] N. Oliver, E. Horvitz, and A. Garg. Layered representations for human
activity recognition. In ICMI, 2003.
[25] K. Pastra and Y. Aloimonos. The minimalist grammar of action. Phil.
Trans. R Soc. B, 367(1585):103–117, 2012.
[26] M. Pawlik and N. Augsten. Rted: a robust algorithm for the tree edit
distance. Proc. VLDB Endow., 5(4):334–345, 2011.
[27] C. Pinhanez and A. Bobick. Human action detection using pnf
propagation of temporal constraints. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, 1998.
[28] D. A. Rosenbaum, R. G. Cohen, S. A. Jax, D. J. Weiss, and R. Van
Der Wel. The problem of serial order in behavior: Lashleys legacy.
Human Movement Science, 26(4):525–554, 2007.
[29] M. Ryoo and J. Aggarwal. Recognition of composite human activities
through context-free grammar based representation. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition, 2006.
[30] P. Saisan, G. Doretto, Y. N. Wu, and S. Soatto. Dynamic texture
recognition. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, 2001.
[31] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A
local svm approach. In ICPR, 2004.
[32] L. Sigal, A. O. Balan, and M. J. Black. Humaneva: Synchronized video
and motion capture dataset and baseline algorithm for evaluation of
articulated human motion. International Journal of Computer Vision,
87(1-2):4–27, 2010.
[33] M. Sridhar, A. G. Cohn, and D. C. Hogg. Learning functional objectcategories from a relational spatio-temporal representation. In Proc.
18th European Conference on Artificial Intelligence,, pages 606–610,
2008.
[34] P. Turaga, R. Chellappa, V. S. Subrahmanian, and O. Udrea. Machine
recognition of human activities: A survey. IEEE Transactions on
Circuits and Systems for Video Technology, 18(11):1473–1488, 2008.
[35] I. Vicente, V. Kyrki, and D. Kragic. Action recognition and understanding through motor primitives. Advanced Robotics, 21:1687–1707,
2007.
[36] J. Wang, D. Fleet, and A. Hertzmann. Gaussian process dynamical
models for human motion. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2008.
[37] A. Whiten, E. Flynn, K. Brown, and T. Lee. Imitation of hierarchical
action structure by young children. Developmental Science, 9:574–
582, 2006.
[38] G. Willems, T. Tuytelaars, and L. J. V. Gool. An efficient dense
and scaleinvariantspatio-temporal interest point detector. In Proc.
European Conference on Computer Vision, 2008.
[39] A. Yilmaz and M. Shah. Actions sketch: A novel action representation. In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 984–989, 2005.

4111

Language-Action Tools for Cognitive Artificial Agents: Papers from the 2011 AAAI Workshop (WS-11-14)

A Corpus-Guided Framework for Robotic Visual Perception
Ching L. Teo, Yezhou Yang, Hal Daumé III, Cornelia Fermüller, Yiannis Aloimonos
University of Maryland Institute for Advanced Computer Studies, College Park, MD 20742-3275
{cteo, yzyang, hal, fer, yiannis}@umiacs.umd.edu

the visual input. A more sophisticated RPU should be able
to 1) fuse (noisy) information from various sensors and processing inputs, 2) perform inference and predictions and 3)
eventually generate a useful output or command that show
that the robot has truly perceived the world with all its complexity and richness. From a systems point of view, this
represents a further step of reﬁnement over the basic RPU:
where we endow it with a control unit which we will term as
the Robot Perception Control Unit (RPCU) in the rest of the
paper. From Fig. 1, we see that the RPCU is the central component of the overall Robotic Perception System, consisting
of a bottom-layer that feeds it with input detections (visual,
sonar or inertial etc.), and a top-layer that uses the outputs of
the RPCU to perform other tasks (feedback or response). In
this paper, we focus on the RPCU’s design and implementation speciﬁcally for video inputs of human actions.
There are obviously numerous ways to realize the RPCU,
all which entail numerous challenges. The most crucial challenge is grounding the visual inputs to semantically meaningful units: getting from a patch of pixel values to identifying the object itself. We show that by adding language,
learned from a large generic corpus, we are able to produce a
reasonable grounding that enables the RPCU to handle noisy
detections and make reasonable predictions. Our proposed
framework for the RPCU contains the following key novel
elements (summarized in Fig. 1):

Abstract
We present a framework that produces sentence-level summarizations of videos containing complex human activities that
can be implemented as part of the Robot Perception Control
Unit (RPCU). This is done via: 1) detection of pertinent objects in the scene: tools and direct-objects, 2) predicting actions guided by a large lexical corpus and 3) generating the
most likely sentence description of the video given the detections. We pursue an active object detection approach by focusing on regions of high optical ﬂow. Next, an iterative EM
strategy, guided by language, is used to predict the possible
actions. Finally, we model the sentence generation process as
a HMM optimization problem, combining visual detections
and a trained language model to produce a readable description of the video. Experimental results validate our approach
and we discuss the implications of our approach to the RPCU
in future applications.

Introduction
Robot perception has been a well researched problem both
in Robotics and Artiﬁcial Intelligence. In this paper, we
focus on the visual perception problem: how can one enable a robot to make sense of its visual environment. During the past few years, with the development of statistical
machine learning techniques, several data-driven detection
methods were used as a basic Robot Perception Unit (RPU)
– a place in the robot’s Operating System (OS) that performs
visual processing such as detecting/recognizing objects, actions and scenes.

• Using Language: we use language as a prior in guiding the
RPCU so as to handle noisy inputs and make reasonable
predictions. We believe this approach mimics how we as
humans perceive the world, where vast amounts of highlevel knowledge acquired over our lives allows us to infer
and recognize complex visual inputs. This knowledge is
encoded in various forms, of which language is clearly the
most predominant. Language manifests itself in the form
of text which is also extremely accessible from various
large research corpora.

Figure 1: Overview of the proposed robot perception system,
which includes the bottom-layer inputs module, RPCU (in
the dashed box) and the top-layer output module.

• Information Fusion: we use current state of the art object detectors to detect hands, tools and direct-objects
(objects that are manipulated by the tool) as initial hypothesis to predict actions in an iterative manner using
an Expectation-Maximization (EM) formulation, with the
noisy visual detections supporting the (equally noisy) proposed action model.

This RPU, however, is lacking in some aspects, more
speciﬁcally in providing high-level information concerning
c 2011, Association for the Advancement of Artiﬁcial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

36

• Sentence (Output) Generation: the proposed RPCU is able
to generate a human-readable sentence that summarizes
the visual inputs that it has received, essentially grounding the visual inputs to a high-level semantic description.
This is achieved by modeling the sentence generation process as a Hidden Markov Model (HMM) and performing Viterbi decoding to produce the most likely sentence,
given the visual detections and a language model. Such
summaries are extremely useful and can be used by further downstream modules for further processing, analysis,
storage or even enable the robot to interact with us in a
more natural way (via speech).
We ﬁrst describe how the hand, tool and direct-objects are
detected actively. We then describe how the three key elements of the proposed RPCU described above are implemented and combined. We validate the proposed RPCU
by generating sentence-level summarizations of input test
videos containing daily human activities. Finally, we conclude with a discussion of the results and its implications for
robotic perception in general.

Figure 2: Overview of the tool detection strategy: (1) Optical ﬂow is ﬁrst computed from the input video frames. (2)
We train a CRF segmentation model based on optical ﬂow
+ skin color. (3) Guided by the ﬂow computations, we segment out hand-like regions (and removed faces if necessary)
to obtain the hand regions that are moving (the active hand
that is holding the tool). (4) The active hand region is where
the tool is localized. Using the PLS detector (5), we compute
a detection score for the presence of a tool. Direct object detection follows a similar strategy (see text).

Hand, Tool and Object Detections
Active object detection strategy
We pursue the following active strategy as illustrated in
Fig. 2 for detecting relevant tools and direct-objects ni ∈ N
from the input video, md ∈ M . M is the set of all videos
considered and N1 ⊂ N and N2 ⊂ N are the sets of possible tools and direct-objects respectively with N as the set
of all objects considered. First, a trained person detector
(Felzenszwalb et al. 2010) is used to determine the location
of the human actor in the video frame. The location of the
face is also detected using (Viola and Jones 2004). Optical
ﬂow is then computed (Brox, Bregler, and Malik 2009) and
we focus on human regions which have the highest ﬂow, indicating the potential locations of the hands. We then apply
a variant of a CRF-based color segmentation (Rother, Kolmogorov, and Blake 2004) using a trained skin color+ﬂow
model to segment the hand-like regions which are moving.
This is justiﬁed by the fact that the moving hand is in contact with the tool that we want to identify. In some cases,
the face may be detected (since it may be moving) but they
are removed using the face detector results. We then apply
a trained object detector (Schwartz et al. 2009) near the detected active hand region that returns a detection score at
each video frame. In order to detect the direct-object, we
compute the midpoint of the two hand regions where the
direct-object is likely to be found and applied same trained
object detector as above to determine the direct-object’s
identity. Averaging out the detection yields PI (ni |md ), the
probability that a tool or direct-object ni exists given the
video md . Specially for tools, we denote PI (N1 |M ) as the
set of likelihood scores over all tools in N1 and all videos in
M.
This active approach has two important beneﬁts. By focusing our processing only on the relevant regions of the
video frame, we dramatically reduce the chance that the object detector will misﬁre. At the same time, by detecting the
hand locations, we obtain immediately the action trajectory,

which is used to describe the action as shown in the next
section.

Action features
Tracking the hand regions in the video provides us with two
sets of (left and right) hand trajectories as shown in Fig. 3.
We then construct for every video a feature vector Fd that
encodes the hand trajectories. Fd encodes the frequency and
velocity components. Frequency is encoded by using the
ﬁrst 4 real components of the 1D Fourier Transform in both
the x and y directions, fx , fy , which gives a 16-dim vector over both hands. Velocity is encoded by averaging the
difference in hand positions between two adjacent frames
δx, δy which gives a 4-dim vector. These features are
then combined to yield a 20-dim vector Fd .

Figure 3: Detected hand trajectories. x and y coordinates are
denoted as red and blue curves respectively.
We denote FM as the set of of action features Fd over all
videos in M .

The Robot Perception Control Unit
The Language Model
The ﬁrst key component of the RPCU is the language model
that predicts the most likely verb (action) that is associated

37

j ∈ V , d ∈ M , Aijd indicates whether an action j is performed using tool i during video clip d.

1 j is performed using i during d
Aijd =
(3)
0 otherwise
and A is a 3D indicator matrix over all tools, actions and
videos. Denoting the parameters of the model as C = {Cj }
which speciﬁes the grounding of each action j, we seek to
determine from eq. (2) the maximum likelihood parameter:

L(D, A|C)
(4)
C ∗ = arg max

with a noun (tool or direct-object) trained from a large text
corpus: the English Gigaword (Graff 2003). We view the Gigaword Corpus as a large text resource that contains the information we need to make correct predictions of actions
given the detected tools from the video and the associated
direct-object given the action. Denoting vj ∈ V as an action label from the set of admissible actions V , we train two
related language models. The ﬁrst model returns the maximum likelihood estimates of an action vj given the tool
ni ∈ N1 : PL (vj |ni ), and the second model returns the mostlikely direct-object ni ∈ N2 given the action vj : PL (ni |vj ).
This can be done by counting the number of times vj cooccurs with ni in a sentence:
#(vj , ni )
#(vj , ni )
PL (vj |ni ) = 
, PL (ni |vj ) = 
#(n
)
i
i
j #(vj )

C

L(D, A|C) = log P (D, A|C)
= log P (A|D, C)P (D|C)
(5)
with the data D comprised of the tool detection likelihoods
PI (N1 |M ), the tool-action likelihoods PL (V |N1 ) and action features FM under the current model parameters C. Geometrically, we can view C as the superset of the |V | action
label centers that deﬁnes our current grounding of each action j in the action feature space.
Using these centers, we can write the assignment given
each video d, tool i and action j, P (Aijd |D, C) as:
P(Aijd = 1|D, C) = PI (i|d)PL (j|i)P en(d|j)
(6)
where P en(d|j) is an exemplar-based likelihood function
deﬁned between the associated action feature of video d, Fd
and the current model parameter for action j, Cj as:
2
1
(7)
P en(d|j) = exp−||Fd −Cj ||
Z
where Z is a normalization factor. What eq. (7) encodes is
the penalty that we score against the assignment when there
is a large mismatch between Fd and Cj , the cluster center of
action j.
Rewriting eq. (6) over all videos M , tools N1 and actions
V we have:
P(A = 1|D, C) = PI (N1 |M )PL (V |N1 )P en(FM |C) (8)
where we use the set variables to represent the full data
and assignment model parameters. In the derivation that follows, we will simplify P(A = 1|D, C) as P(A = 1) and
P(A = 0) = 1 − P(A = 1). We detail the Expectation and
Maximization steps in the following sections.

(1)

Figure 4: The Gigaword co-occurrence matrix for tools and
predicted actions.
Fig. 4 shows the set of the |N1 |×|V | co-occurrence matrix
of likelihood scores over all tools and actions considered in
the experiments, denoted as PL (V |N1 ).

Information Fusion: Predicting Actions
Given the noisy video detections described in the previous
section, the second key capability of the proposed RPCU
is to combine them in some reasonable manner, using the
trained language model to predict the action that is occurring. Formally, our goal is to label each video with their most
likely action, along with the tool1 that is associated with the
action using an EM formulation. That is, we want to maximize the likelihood:

Expectation step We compute the expectation of the latent variable A, denoted by W, according to the probability
distribution of A given our current model parameters C and
data (PI , PL , and FM ):
W = EP(A) [A]

L(D; A) = EP(A) [L(D|A)]
= EP(A) [logP(FM , PI (·), PL (·)|A)]

(2)

where A is the current (binary) action label assignments of
the videos (see eq. (3)). D is the data computed from the
video that consists of: 1) the language model PL (·) that predicts an action given the detected tool, 2) the tool detection
model PI (·) and 3) the action features, FM , associated with
the video.
We ﬁrst deﬁne the latent assignment variable A. To simplify our notations, we will use subscripts to denote tools
i = ni , actions j = vj and videos d = md . For each i ∈ N1 ,
1

A

Where,

= P(A = 1) × 1 + (1 − P(A = 1)) × 0
= P(A = 1)
(9)
According to Eq. 6, the expectation of A is:
W = P(A = 1) ∝ PI (N1 |M )PL (V |N1 )P en(FM |C) (10)
Speciﬁcally, for each i ∈ N1 , j ∈ V, d ∈ M :
Wijd ∝ PI (i)PL (j|i)P en(d|j)
(11)
Here, W is a |N1 |×|V |×|M | matrix. Note that the constant
of proportionality does not matter because it cancels out in
the Maximization step.

Direct-objects can be used as well but are not considered here.

38

Given the computed conditional probabilities: PI (n1 |m),
PI (n2 |m) and PI (v|m) (eq. (16)) which are observations
from the input video with the parameters of the trained language model: PL (v|n1 ), PL (n2 |v) (eq. (1)), we seek to ﬁnd
the most likely sentence structure T ∗ = (n1 , v, n2 ) by:

Maximization step The maximization step seeks to ﬁnd
the updated parameters Cˆ that maximize eq. (5) with respect
to P(A):
Cˆ = arg max EP(A) [log P(A|D, C)P(D|C)]
C

(12)

T ∗ = arg max P(T |n1 , v, n2 )

Where D = PI , PL , FM . EM replaces P(A) with its expectation W. As A, PI , PL are independent of the model
parameters C, we can simplify eq. (12) to:
Cˆ = arg max P(FM |C)
C
⎛
⎞

= arg max ⎝−
Wijd ||Fd − Cj ||2 ⎠
C

n1 ,v,n2

= arg max{PI (n1 |m)PI (n2 |m)PI (v|m)×
n1 ,v,n2

PL (v|n1 )PL (n2 |v)}

(17)

where the last equality holds by assuming independence between the visual detections and corpus predictions. Obviously a brute force approach to try all possible combinations
to maximize eq. (17) will not be feasible due to the potentially large number of possible combinations. A better solution is needed.

(13)

i,j,d

where we had replaced P(FM |C) with eq. (7) since the
relationship between FM and C is the penalty function
P en(FM |C). This enables
 us to deﬁne a target maximization function as F(C) = i,j,d Wijd ||Fd − Cj ||2 .
According to the Karush-Kuhn-Tucker conditions, we can
solve the maximization problem by the following constraint:

∂F
(Wijd (Fd − Cj )) = 0
(14)
= −2
∂C
i,j,d

Thus, for each j ∈ V , we have:

i∈N ,j∈V,d∈M Wijd Fd
ˆ
Cj =  1
i∈N1 ,j∈V,d∈M Wijd

(15)

We then update C = Cˆ within each iteration until convergence.
Action Prediction Using the learned model C ∗ , the conditional probability distribution of each action j given the
input video d, PI (j|d), can be computed by:
 	


PI (i|d)PL (j|i)P en(Ft |Cj∗ )
Z=
j∈V i∈N1

Figure 5: The HMM used for optimizing T . The relevant
transition and emission probabilities are also shown. See text
for more details.
Our proposed strategy is to pose the optimization of T as
a dynamic programming problem, akin to a Hidden Markov
Model (HMM) where the hidden states are related to the sentence structure we seek: T , and the emissions are related to
the observed detections: {n1 , v, n2 } in the video if they exist. The hidden states are therefore denoted as: {N1, V, N2}



∗
i∈N1 PI (i|d)PL (j|i)P en(Ft |Cj )
PI (j|d) =
(16)
Z
where Ft is the action features extracted from d and Cj∗ is
the j th action center from the learned model. Replacing our
short-hand notations for vj = j and d = md , the pdf computed above is denoted as PI (vj |md ).


	

Tools n1 ∈ N1
’towel’ ’knife’
’fork’ ’spoon’

Sentence Generation
The ﬁnal key component of the proposed RPCU is to generate a reasonable sentence that summarizes the input video.
In order to do this we deﬁne the sentence to be generated in
terms of its core components by a triplet, T = {n1 , v, n2 }
where n1 ∈ N1 , n2 ∈ N2 refer to any tools and directobjects detected previously from the input video m with v
as the predicted action label from the EM formulation described above. We have dropped all subscripts i, j, d as we
are not concerned with any particular object, action or video
here. Using T , we generate a sentence that summarizes the
input video using a pre-deﬁned sentence template.

Actions v ∈ V
’clean’ ’cut’
’toss’

Direct-objects n2 ∈ N2
’table’ ’cheese’
’tomato’ ’salad’

Table 1: The set of tools, actions and direct-objects considered.
with values taken from their respective word classes from
Table 1. The emission states are {n1,v,n2} with binary values: 1 if the detections occur or 0 otherwise. The full HMM
is summarized in Fig. 5. The rationale for using a HMM is
that we can reuse all previous computation of the probabilities at each level to compute the required probabilities at the
current level. From START, we assume all tool detections
are equiprobable: P(N1|START) = |N11 | . At each N1, the

39

HMM emits a detection from the video and by independence
we have: P(n1|N1) = PI (n1 |m). After N1, the HMM transits to the corresponding verb at state V with P(V|N1) =
PL (v|n1 ) obtained from the ﬁrst language model. Similarly,
V has emissions P(v|V) = PI (v|m). The HMM then transits from V to N2 with P(N2|V) = PL (n2 |v) computed from
the second language model which emits the direct-object detection score from the video: P(n2|N2) = PI (n2 |m).
Comparing the HMM with eq. (17), one can see that all
the corpus and detection probabilities are accounted for in
the transition and emission probabilities respectively. Optimizing T is then equivalent to ﬁnding the best (most likely)
path through the HMM given the video observations using
the Viterbi algorithm which can be done signiﬁcantly faster
than the naive approach.
The computed T ∗ is then used to generate a sentence of
the form V-N2-N1 which represents the generation template common in standard language generation work. To
form readable sentences, standard English grammar rules
and syntax are used to ensure that the words produced are
grammatically and syntactically coherent – for example, we
impose that V be of the present gerund form, and the preposition {with} is the only admissible preposition used with
N1, the tool. We show in the experiment section that this
simple approach is sufﬁcient for the RPCU to generate concise sentences that summarizes the videos.

the highly structured domain of baseball videos The work
of (Farhadi et al. 2010) attempts to “generate” sentences by
ﬁrst learning from a set of human annotated examples, and
producing the same sentence if both images and sentence
share common properties in terms of their triplets: (NounsVerbs-Scenes). No attempt was made to generate novel sentences from images beyond what has been annotated by humans.
Natural language generation (NLG) is a long-standing
problem. Classic approaches (Traum, Fleischman, and Hovy
2003) are based on three steps: selection, planning and realization. A common challenge in generation problems is the
question of: what is the input? Recently, approaches for generation have focused on formal speciﬁcation inputs, such as
the output of theorem provers (McKeown 2009) or databases
(Golland, Liang, and Klein 2010). Most of the effort in those
approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy
inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and
the content selection and realization needs to take this uncertainty into account.

Experiments
In order to validate the proposed RPCU, we extracted 24
video clips from a subset of the POETICON video dataset2
of four everyday scenarios: 1) Clean table with towel, 2)
Cut cheese with knife, 3) Cut tomato with knife, and 4)
Toss salad with fork and spoon. Each scenario have 6 video
clips of the action performed by 2 different pairs of actors
(3 videos per pair), with intraclass variations in the location and appearance of the tools, and how the actions are
performed by the actors in the videos. There are also multiple actions occurring at the same time, which makes this
dataset extremely challenging with occlusions and constant
human interactions. All the videos evaluated are taken from
the same viewpoint.
We ﬁrst detected the tools and direct-objects n1 and n2 actively and extracted the hand trajectories as action features
as described previously. At the same time, the RPCU’s language models are trained using the Gigawords corpus from
the deﬁned tool, objects and action sets (Table 1).
Next we invoke the RPCU’s action prediction module that
uses an EM formulation to predict the most-likely action
given the tools and action trajectories. Among the 24 video
clips, we are able to predict the correct action labels for 21
of them (87.5% accuracy). We normalize the EM output for
each label using eq. (16) to obtain PI (v|m).
Using the predicted action probability distribution and the
language model, we use RPCU’s sentence generation module to produce descriptive sentences for each of the 24 video
clips. Sample results from each scenario are shown in Fig. 6.
We are able to generate 22 correct sentences from the 24
videos (91.6% accuracy) by comparing the ground truth core
sentence structure with each of the video’s predicted T ∗ .

Related Works
The proposed RPCU combines two important computational
problems: action recognition and sentence generation and
solves them in an integrated manner guided by language. As
such, related works spans both the Computer Vision (action
recognition) and Computational Linguistics (sentence generation) domains.
Action recognition research spans a long history. Comprehensive reviews of recent state of the art can be found
in (Turaga et al. 2008; Weinland, Ronfard, and Boyer 2010;
Lopes et al. ). Most of the focus was on studying human
actions that were characterized by movement and change
of posture, such as walking, running, jumping etc. Our approach is more closely related to the use of language for
object detection and image annotation. With advances on
textual processing and detection, several works recently focused on using sources of data readily available “in the wild”
to analyze static images. The seminal work of (Duygulu
et al. 2002) showed how nouns can provide constraints
that improve image segmentation. (Gupta and Davis 2008)
(and references herein) added prepositions to enforce spatial constraints in recognizing objects from segmented images. (Berg et al. 2004) processed news captions to discover
names associated with faces in the images, and (Jie, Caputo,
and Ferrari 2009) extended this work to associate poses detected from images with the verbs in the captions. Some
studies also considered dynamic scenes. (Cour et al. 2008)
studied the aligning of screen plays and videos, (Laptev et
al. 2008) learned and recognized simple human movement
actions in movies, and (Gupta, Kembhavi, and Davis 2009)
studied how to automatically label videos using a compositional model based on AND-OR-graphs that was trained on

2

40

http://poeticoncorpus.kyb.mpg.de/

The observed improvement in the ﬁnal accuracy of the generated sentence shows that the HMM is able to correct, given
the video detections (emissions), initially wrong action predictions thanks to the trained language models.

even more complicated sentences that involves, for example,
adjectives and adverbs.

Acknowledgments
The support of the European Union under the Cognitive Systems program (project POETICON) and the National Science Foundation under the Cyberphysical Systems Program,
is gratefully acknowledged.

References
Berg, T. L.; Berg, A. C.; Edwards, J.; and Forsyth, D. A.
2004. Who’s in the picture? In NIPS.
Brox, T.; Bregler, C.; and Malik, J. 2009. Large displacement optical ﬂow. In CVPR, 41–48. IEEE.
Cour, T.; Jordan, C.; Miltsakaki, E.; and Taskar, B. 2008.
Movie/script: Alignment and parsing of video and text transcription. In ECCV.
Duygulu, P.; Barnard, K.; de Freitas, J. F. G.; and Forsyth,
D. A. 2002. Object recognition as machine translation:
Learning a lexicon for a ﬁxed image vocabulary. In ECCV,
volume 2353, 97–112.
Farhadi, A.; Hejrati, S. M. M.; Sadeghi, M. A.; Young, P.;
Rashtchian, C.; Hockenmaier, J.; and Forsyth, D. A. 2010.
Every picture tells a story: Generating sentences from images. In Daniilidis, K.; Maragos, P.; and Paragios, N., eds.,
ECCV (4), volume 6314 of Lecture Notes in Computer Science, 15–29. Springer.
Felzenszwalb, P. F.; Girshick, R. B.; McAllester, D. A.; and
Ramanan, D. 2010. Object detection with discriminatively
trained part-based models. IEEE Trans. Pattern Anal. Mach.
Intell. 32(9):1627–1645.
Golland, D.; Liang, P.; and Klein, D. 2010. A game-theoretic
approach to generating spatial descriptions. In Proceedings
of EMNLP.
Graff, D. 2003. English gigaword. In Linguistic Data Consortium, Philadelphia, PA.
Gupta, A., and Davis, L. S. 2008. Beyond nouns: Exploiting
prepositions and comparative adjectives for learning visual
classiﬁers. In ECCV, volume 5302, 16–29.
Gupta, A.; Kembhavi, A.; and Davis, L. S. 2009. Observing human-object interactions: Using spatial and functional compatibility for recognition. IEEE Trans on PAMI
31(10):1775–1789.
Jie, L.; Caputo, B.; and Ferrari, V. 2009. Who’s doing what:
Joint modeling of names and verbs for simultaneous face
and pose annotation. In NIPS., ed., Advances in Neural Information Processing Systems, NIPS. NIPS.
Laptev, I.; Marszalek, M.; Schmid, C.; and Rozenfeld, B.
2008. Learning realistic human actions from movies. In
CVPR.
Lopes, A. P. B.; do Valle Jr., E. A.; de Almeida, J. M.; and
de Albuquerque Arajo, A. Action recognition in videos:
from motion capture labs to the web. CoRR.
McKeown, K. 2009. Query-focused summarization using
text-to-text generation: When information comes from multilingual sources. In Proceedings of the 2009 Workshop on

Figure 6: Four frames (left) from the input videos and results. (Right-upper): Sentence structure T ∗ predicted using
Viterbi and (Right-lower): Generated sentences.

Conclusion and Future Work
In this paper, we have proposed a crucial enhancement to
the standard Robot Perception Unit (RPU): the Robot Perception Control Unit (RPCU), which 1) Combines detection results in a iterative way to improve different sources
of detection; 2) Uses language to help improve and combine
the detection results and 3) Provide a concise output in the
form a descriptive sentence. This is achieved by learning a
language model from the Gigaword corpus, formulating an
iterative EM approach to predict actions and generating a
verbal description of the video using a HMM. Experimental results over 24 videos from a subset of the POETICON
dataset shows that our approach is able to predict the associated action and generate a descriptive sentence with high
accuracy.
The proposed RPCU has provided a viable framework
that we believe is an important step forward for robotic perception to progress from simple detection based strategies of
the RPU to a real cognitive agent – one that is able to perceive, reason and act accordingly to its various inputs. The
key contribution is the semantic grounding afforded by language which our framework exploits. The proposed RPCU
is also generalizable to different detection inputs: e.g. low to
mid-level visual features or even other sensors (e.g. sonar).
A more reﬁned language model can also be used to handle
a larger variety of inference tasks: e.g. predicting the next
likely action given something as happened. A major limitation of the approach is that the set of tools, objects and
actions (the vocabulary) must be pre-deﬁned, future work
will focus on discovering from language, the co-located set
of such tools, objects and actions via attributes. Finally, we
can also extend the language generation module to generate

41

Language Generation and Summarisation (UCNLG+Sum
2009), 3. Suntec, Singapore: Association for Computational
Linguistics.
Rother, C.; Kolmogorov, V.; and Blake, A. 2004. ”grabcut”:
interactive foreground extraction using iterated graph cuts.
ACM Trans. Graph. 23(3):309–314.
Schwartz, W.; Kembhavi, A.; Harwood, D.; and Davis, L.
2009. Human detection using partial least squares analysis.
In ICCV.
Traum, D.; Fleischman, M.; and Hovy, E. 2003. Nl generation for virtual humans in a complex social environment.
In In Proceedings of the AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue,
151–158.
Turaga, P. K.; Chellappa, R.; Subrahmanian, V. S.; and
Udrea, O. 2008. Machine recognition of human activities: A survey. IEEE Trans. Circuits Syst. Video Techn.
18(11):1473–1488.
Viola, P., and Jones, M. 2004. Robust real-time face detection. ICCV 57(2):137–154.
Weinland, D.; Ronfard, R.; and Boyer, E. 2010. A survey
of vision-based methods for action representation, segmentation and recognition. Computer Vision and Image Understanding. Article in Press, Accepted Manuscript.

42

What Can I Do Around Here? Deep Functional
Scene Understanding for Cognitive Robots

arXiv:1602.00032v2 [cs.RO] 2 Feb 2016

Chengxi Ye, Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos
Computer Vision Lab, University of Maryland, {cxy, yzyang, fer, yiannis}@umiacs.umd.edu
Abstract—For robots that have the capability to interact with
the physical environment through their end effectors, understanding the surrounding scenes is not merely a task of image
classification or object recognition. To perform actual tasks, it
is critical for the robot to have a functional understanding of
the visual scene. Here, we address the problem of localizing
and recognition of functional areas from an arbitrary indoor
scene, formulated as a two-stage deep learning based detection
pipeline. A new scene functionality testing-bed, which is complied
from two publicly available indoor scene datasets, is used for
evaluation. Our method is evaluated quantitatively on the new
dataset, demonstrating the ability to perform efficient recognition
of functional areas from arbitrary indoor scenes. We also
demonstrate that our detection model can be generalized onto
novel indoor scenes by cross validating it with the images from
two different datasets.

I. I NTRODUCTION
Every day, human beings interact with the surrounding environments via perception and action. Through interacting with
our environments, we gradually draw on our understanding of
the functions that areas of the scene bear. For example, a round
knob like object indicates that an action of spherical grasping
and perhaps turning could be applied onto, and a faucet like
entity with metallic texture indicates that an action of turning
on or off water could be applied onto. As human beings,
we recognize such functional areas in our daily environments
with vision, so we can perform various of actions in order to
finish a task. As robots (such as Baxter) begin to collaborate
with humans in domestic environments, they will also need
to recognize functional areas in order to develop a functional
understanding of the scene.

Fig. 1. The purpose of our system. Through supervised learning, the robot
can take appropriate actions to interact with the world by visually observing
the environment.

Imagine a Baxter robot with a mobile base enters an
arbitrary kitchen, trying to clean up the mess from a dinner
party (Fig. 1). Before the robot can perform any dexterous

movements, the robot needs to recognize functional areas of
the scene such as where it can indicate actions like getting
power for a vacuum machine, getting water, etc. Modern
Computer Vision technologies allows robots to recognize
objects from a known set of hundreds of categories. However,
in the previous situation, a brutal applying of object detectors
either does not suffice (for example, detecting cabinet does
not indicate where and how to open), or is an overkill (for
example, it is not necessary to differentiate between paper
towel with towel as long as the robot understands a pinch
grasp could be applied to get them). Instead, we argue that
more importantly the robot needs to know which part of the
scene corresponds to which functionality, or to address the
inevitable self-questioning: what can I do around here? As
Gibson remarked, “If you know what can be done with a[n]
object, what it can be used for, you can call it whatever you
please” [8].
In this paper, we address the novel problem of localizing
and identifying functional areas of an arbitrary indoor
scene, so that a robot can have a functional understanding
of the visual scene in order to perform actions accordingly,
and generalize this knowledge into novel scenes. Example
outputs of the presented system are shown in Fig. 1: without
recognizing the big salad bowl, the robot understands that
a two-hand raise-and-move action (black bounding boxes)
can be applied in a specific area of the kitchen; without
seeing a specific type of handle bar before. However, from
its appearance it should be able to infer that a wrap grasp
could be applied to pull the bar (green bounding boxes), etc.
Knowing “what can I do here” can serve as the first step
for many applications in Robotics. For example, the presented
pipeline could serve as a functional attention mechanism at
the first glance of a novel scene for the robot. The output of
the system can guide robot motion planning to move towards
target functional area. Dexterous actions can then be planned
in these attended areas. Also the usage of our system is not
limited in the field of robotics. One potential application could
be to provide verbalized functional instructions for the blind.
From a computer vision and robotic vision perspective,
recognizing functional areas from a static scene (in our experiments, we use static images to simulate the scene seen
by a robot), is a challenging task since areas with unique
visual appearance may indicate the same functionality. Also,
unlike concepts such as objects, functionality itself can only
be meaningful while a certain action could be applied on
or leads to a certain consequence. Therefore we need an

ontology for scene functionality. Furthermore, our goal is to
provide a general functional scene understanding model for
unconstrained indoor scene from real scenarios, and we have
to overcome the huge variance naturally imposed from real
world image.
The main contributions of this paper are as follows: 1)
a scene functional area ontology for indoor domain; 2) the
first two stage, deep network based recognition approach is
developed and presented for scene functional understanding,
which is proved to be effective and efficient on two scene
functional area datasets that are augmented from publicly
available datasets; 3) the first scene functionality dataset is
compiled and made publicly available, which contains more
than 100,000 annotated training samples and two sets of testing
images from different indoor scenes1 . .
II. R ELATED W ORK
We will focus our review on studies of three major concepts,
which we consider are most closely related to this work: a)
studies that evolve around the idea of the concept of affordance
in Robotics; b) studies aiming to infer or reason beyond
appearance, such as physics or action intention and c) deep
network based visual recognition approaches.
Affordance related: The affordance of objects has been
studied in Computer Vision and Robotics communities. Early
work sought a function-based approach to object recognition
for 3D CAD models of objects, for example chairs [25]. From
the computer vision community, [13] classifies human hand
actions considering the objects being used, Grabner et al.
[10] detects “sittable” areas from 3D data. [22] represents
object directly in terms of their interaction with human hands.
More recently, [19] attempts to learn the affordance of tool
parts from geometric features obtained from RBGD images
using different classification approaches. They showed that the
concept of affordance has generalization potential.
Affordances might also be considered as a subset of object
attributes, which have been shown to be powerful for object
recognition tasks as well as transferring knowledge to new
categories. In the robotics community, the authors of [26]
identify color, shape, material, and name attributes of objects
selected in a bounding box from RGB-D data. [12] explored,
using active manipulation of different objects, the influence of
the shape, material and weight in predicting good “pushable”
locations. [1] used a full 3D mesh model to learn so-called 0ordered affordances that depend on object poses and relative
geometry. Koppula et al. [14] view affordance of objects
as a function of interactions, and jointly model both object
interactions and activities via a Markov Random Field using
3D geometric relations (‘on top’, ‘below’ etc.) between the
tracked human and object as features.
Reason Beyond Appearance: Many recent approaches in
Robotics and Computer Vision aim to infer physical properties
beyond appearance models from visual inputs. [30] proposes
that implicit information, such as functional objects, can be
1 Available

from https://www.umiacs.umd.edu/∼yzyang/DFSU

inferred from videos. [31] used stochastic tracking and graphcut based segmentation to infer manipulation consequences
beyond appearance. [33] takes a task-oriented viewpoint and
models objects using a simulation engine. [21] is the first
work in Computer Vision that simulates action forces. More
recently, [32] proposes that the grasp type, which is recognized
using CNNs, reveals the general category of action the human
is intending to do next.
Deep Network based Visual Recognition: Recently,
studies in neural network have found that convolutional neural
networks (CNN) can produce near human level image classification accuracy[16], and related work has been used to various
visual recognition tasks such as scene labeling [6] and object
recognition[9]. For other closely related technical work that
our approach builds upon, we embed the survey along the
presentation of our approach in Sec. IV
III. S CENE F UNCTIONALITY O NTOLOGY

Fig. 2.

Functionality ontology considered in this work.

A number of functionality ontologies have been proposed
in several areas of research, including robotics, developmental medicine, and biomechanics, each focusing on different
aspects of the action. In a recent survey, Worgotter et al.
[28] attempts to categorize manipulation actions into less than
30 fundamental types based on hand-object relations. In this
work, we study the common set of actions a robot with end
effectors could perform in a domestic indoor environment
and use hierarchical categorization of functionality into eleven
functional area end categories.
First we distinguish, according to the most commonly
used classification (based on the functional area location),
into “Small part of furniture/appliance/wall”, “Objects (vessels and tools)” and “Furniture”. For the “Small part of
furniture/appliance/wall” category, we first consider functional
areas that could be used to physically open a container, such
as a cabinet handle or an oven handle. Based on the main
grasping type, we further differentiate the “to open” category
into two end categories, which are “spherical grasp (turn)

to open” and “wrap grasp (drag) to open”. For the “to turn
on/off” sub-category, according to intended media, we further
differentiate it into three end-categories: “turn on/off fire”,
“turn on/off water” and “turn on/off electricity”.
Secondly, for the “Objects (vessels and tools)” sub-category,
we first consider functional areas that are used to move around
the appliance. Based on the shape of the functional area and
number of hands needed, we further differentiated it into
four end categories: “two-hands raise-and-move”, “cylindrical
grasp to move”, “hook grasp to move” and “pinch grasp to
move”. Also, for areas contain elongated tools, we categorize
it into end category, which is “manipulate elongated tools”.
Lastly, we consider the use of other whole furniture, and
categorize it into an end category “to sit, to place and etc.”,
which contains a heavily studied functionality in Computer
Vision “sittable” [10]. Fig. 2 illustrates the considered functionality hierarchy and end-categories. We also design different
color code for each functional area to facilitate visualization
in our detection pipeline.

(a)

(b)

IV. O UR A PPROACH

(c)

In this section, we first briefly summarize the basic concepts
involved in an end-to-end visual detection pipeline through
Convolutional Neural Networks (CNN), and then we present
our implementation of the recognition pipeline which is tailored for the task of scene functional understanding.
The input to our system is a static image of an indoor scene
(we use kitchen scenes for training), which simulates a scene
that a domestic robot assistant, equipped with a normal RGB
camera, could observe. Fig. 3 depicts the key steps of our
system. First, the system applies an attention mechanism to
guide the robot to a set of areas (represented as bounding
boxes) that potentially contain functional meaningful areas as
depicted in Fig. 3 (a). Then, a deep neural network is trained to
take each of these areas as input and output a belief distribution
over the ontology of scene functional areas as depicted in
Fig. 3 (b). In Fig 3 (c), the final output of our system is
visualized. Bounding boxes are used to indicate the detected
location of the functional area and different color is used to
indicate the function predicted. For example, the blue boxes
around the cabinet’s door handles indicate the robot could
apply “spherical grasp to open” and the green boxes around
the oven handle indicates the robot could apply “wrap grasp
to open”. Please refer to Fig. 2 for a full list of functionality
considered and their color codes.

Fig. 3.
(a) An area proposal pipeline provides high quality category
independent candidate regions. (b) A neural network takes each of the regions
and predict the most likely function. (b) System output following color code
from Fig. 2.

A. Functional Region Proposal

Biologically inspired learning algorithms such as the artificial neural network were heavily used in the 1990s [17].
Artificial neural networks computationally simulate the activities of human neurons. Each node in a network acts as
an artificial neuron that takes as inputs the outputs of other
nodes, and applies a non-linear mapping of the combined
inputs as its own output. The human cerebral cortex system
was estimated to have ∼100 billion neuron cells organized in
6 layers[11, 18] to conduct different perceptual activities. The
complex and deep layered structure may be a key factor to the

Given an input image, there exists an astronomical amount
of possible candidate regions that could contain functional
areas. But human visual system has limited processing power.
It allocates its resources by limiting to a small amount of
attention areas based on low level vision features such as
texture, contrast and color.
Recently, there is a variety of algorithms are presented that
use low level vision features to train attention models and
output category independent region proposals[5, 2, 27]. These

pipelines combine a diverse amount of visual features, such
as color, intensity, texture and edge information, to select
areas from static images that are likely to be semantically
meaningful. In the first stage of our system, we adopted one
of the state-of-the-art visual attention methods called selective
search[27] to propose a set of bounding boxes from an input
static scene.
Selective search over-segments an input image and then
hierarchically merges similar neighboring regions to reduce the
amount of candidate regions. The similarity measure is based
on a variety of criteria, such as size, color, texture. Different
from exhaustive search, selective search aim to provide higher
quality but a small number of candidate regions. These regions
serve as candidates to the next stage processing. Though
selective search has been proven to be effective, it may still
output background areas. Thus in the following steps, together
with the eleven functional categories introduced in Sec. III, we
also include a “background” category.
B. CNN for feature extraction

human perceptual capability. In the early days, due to limited
computational resources and the inherent difficulty in training
a layered non-linear system, artificial neural networks used
to have much fewer elements (∼103 ) and layers (2-3). With
recent advancements in hardware development and progress
in training large scale non-linear functions, developments of
artificial neural network witness much improved number of
elements(∼ 107 ) and depths(> 5). As a result, the recognition
power of the corresponding networks have been improved to
get closer to human performance[15].
Convolutional neural network (CNN), which is a specific
type of neural network, was developed for computers to simulate the human vision system. Each convolutional element in
the network spatial-invariantly
combines neighboring feature
P
maps as y =
X
∗
w
+ b, where Xi is the input
i
i
1≤i≤N
feature map from the i − th neuron from the previous layer.
wi is a convolution kernel corresponding to that i − th node
that simulates the human receptive field. ∗ is a convolution
operator and b is a bias to the linear output. The convoluted
results using different kernels are then linearly combined as
inputs to a non-linear mapping as:
z = f (y) = f (

X

Xi ∗ wi + b).

(1)

1≤i≤Inputs

The neural network may have other elements such as
max/average pooling (locally calculate the max/average elements) to simulate human response to the non-linearly mapped
features. The non-linear mapping function f is usually a
sigmoid-like function or the rectified linear unit (ReLU)
function[15]. In this work, we adopt the ReLU function.
To predict the most likely action of an input region, we pass
the image patch through a trained neural network. The output
features are used as inputs to a softmax function to model the
probability pj of a specific functionality j.
pj = P

exj

1≤i≤K

exi

(2)

In this work we adopt two convolutional neural network
architectures[3](VGG-F and VGG-S, differing in the total
number of parameters and filter size), which both are pretrained on the ImageNet dataset [4]. Each of them has five
convolution layers and three fully connected layers.
C. Network Training
Training a large network with only a small amount of data
could be problematic. Training with insufficient data may
lead to a network that overfits the data which has limited
generalization power. In the early developments of the human
brains, humans gain cognitive capability by being exposed to
a vast amount of varied information. The connections between
the neurons are pruned during the learning process to adapt to
the data and improve efficiency [20].
One way to adapt a large neural network (pre-trained on
millions of samples) to a relatively smaller dataset (with
tens of thousands of training samples) is to fix the network

Fig. 4. Convergence rates of a 1-d damped spring mass system. Different
learning rates lead to different convergence rates. High learning rate causes
underdamping, in which case oscillations and slow convergence occur, while
low learning rate causes overdamping and slow convergence.

structure and only update the weights of the final mapping
to the softmax functions. An even more biologically plausible
strategy[9, 3], that significantly improves the performance, is
to simultaneously fine-tune all the parameters in the original
neural network.
In our system, we adopt the fine-tuning strategy and connect
the 4096-dimensional output feature vector from pre-trained
networks to a (11+1) class (11 functional category introduced
in Sec.III and one “background” category) fully connected
prediction layer.
There have been many studies on how to train a deep neural
network. One simplest approach is to the stochastic gradient
descent (SDG) method. For each iteration the algorithm takes
a small batch of samples, and evaluate an averaged gradient
based on these samples. Parameters are locally optimized in
the negative gradient direction to reduce the training loss.
A momentum term is introduced to combine the previous
estimated gradient to smooth the current gradient estimate.
Approaches fall into this paradigm are termed the first order
gradient descent approach. This gradient descent approach is a
practical and efficient way to reduce the loss when the step size
is small enough and when the underlying gradient is non-zero.
Remarkably, this approach works well even for non-convex
optimization problems such as training the neural network. In
practice the training process will converge at a local minimum
when proper parameters are chosen.
It can be shown the convergence process with the local
convex approximation is equivalent to a high dimensional
spring-mass system in a viscous medium [23]. The momentum
term amounts to the mass size and accelerates the convergence
when set in a proper range. Larger step size amounts to less
viscous medium and less damping, in which case the system
can be under-damped and will oscillate (shown in Fig 4).
In the final stage of training when the parameters are already
located near a local minimum, it is practically effective to
reduce the step size or reduce the so-called learning rate, in
order to magnify the damping amplitude and accelerates the
speed of convergence along directions with larger curvatures.
V. E XPERIMENTS
A. Dataset and Protocols
Since there does not exist a publicly available dataset for
scene functionality analysis, it is necessary to compile a new
one to serve as a testing-bed for our pre-mentioned system.

Without the loss of generality, we set our main testing scenario
in a kitchen domain, and we first adopted the subset of kitchen
images from a well-known publicly available database (the
SUN dataset[29]).
Following the functionality ontology given from Sec. III, we
manually labeled over 10,000 functional regions from over 500
kitchen photos, each with a category from the eleven entries.
We further split the dataset into 90% for training and 10% for
testing. Additionally, for each training image, we randomly
generated 100 bounding boxes of varied sizes, and treated
the patches as “background” samples if the Intersection-OverUnion (IOU = A∩B
A∪B ) values of the patch with regard to
each labeled functional area are smaller than 0.5. Following
this procedure, we generated around 90,000 “background”
samples. Overall, we compiled a new dataset which contains
more than 100,000 training samples to serve as initial training
data. During the training process, we reserved 2% of the data
for model validation.
The first testing set includes the reserved 10% of the
kitchen images from SUN dataset. To see how well the trained
model could generalize onto another dataset, we also randomly
selected additional 100 images from anther publicly available
dataset (the NYU indoor scene dataset [24]). Note that in NYU
testing images, there are not only kitchen scenes, but also other
indoor scenes, such as office and bedroom.
B. Training and Testing Details
We applied the first order gradient descent approach to train
the network. The learning rate was initially set to be 0.005 for
the first 7 layers of the network and 0.05 for the final layer,
which generates input to the softmax function. As mentioned
before, we reduced the learning rate by a magnitude of 10 after
40 epochs. The significant drop in training loss may be well
explained by the fact that smaller learning rate corresponds
to higher damping rate, and results in faster convergence in
certain high curvature directions. After another 20 epochs, we
further reduced the learning rate by another magnitude of 10.
We then early stopped the training at 70 epochs. All parameters
were chosen from previous literature and empirical testing.
In the region proposal stage of the testing phase, we used
the selective search “quality” option and set minimum segment
size to be the 1/100*max(height,width). With this minimum
segment size, selective search algorithm generates thousands
of candidate regions for the classification stage of the pipeline.
It is also worth to mention that the system’s output after one
round of training has large number of false positives (shown in
Fig. 5 (b)) and the precision rate is around 5%. To improve the
performance, we adopted the “hard negative mining” technique
which is widely used for training an SVM [7]. Specifically,
the initial trained network is first applied onto the training
images, and the falsely detected regions (false positives and
false negatives) were collected as “hard” examples. These
“hard” samples were included as training data into the next
round of training. At the end, around 250, 000 images were
used in the third round of training. This strategy significantly

(a)

(b)

(c)

(d)

Fig. 5. (a) Ground-truth labeling of a kitchen scene. Detection result with
VGG-S network after (b) one round training; (c) two rounds of training; (d)
three rounds of training.

reduces the false positive rate, and yields a model with the
best performance (Fig. 8).
C. Evaluation metrics
To compare the performances from different training
procedures, we adopted the following evaluation metrics:
precision, recall rate and the F-1 score. When calculating these metrics, we treated a detected bounding box
as true positive if 1) it has a correct prediction and 2)
its IOU value with a labeled box is bigger than 0.5.
The metrics were further calculated following their defini#truepositive
tions: precision = #f alsepositive+#truepositive
, recall =
precision·recall
#truepositive
#truepositive+#f alsenegative and F 1 = precision+recall .
D. Experimental results
The ROC curves of different models trained with different
rounds can be found in Fig 8. In Table I we report the
precision, recall rates and the F1 score of each model on the
two testing sets with one to three rounds of training. From
Fig. 8 and Table I, we can see that, comparing with the
slightly faster but slim VGG-F network, the VGG-S network,
whose network architecture is more complex, achieves better
performance at the same recall rate. We also plot the loss
curves, top 1-class/5-class error rates curves in the third round
training in Fig. 7. The corresponding confusion matrix on the
validation dataset is plotted in Fig 6.
From the Table I, we can also see that the inclusion
of “hard” examples from previous rounds of training can
significantly improve the detection performance. As a matter
of fact, we observe a boost of precision from 5% to 30% after
three rounds of training. Even though the recall rates may drop
after several rounds of training, the F1 score, which takes a
trade-off between precision and recall, shows that the VGGS network with three rounds of training performs best while

TABLE I
P RECISION AND R ECALL FOR D IFFERENT N ETWORKS

VGG-S One round
VGG-S Two rounds
VGG-S Three rounds
VGG-F One round
VGG-F Two rounds
VGG-F Three rounds

Fig. 6.

Precision (SUN)
5.26%
16.18%
31.52%
4.78%
19.89%
29.74%

Recall (SUN)
23.92%
15.05%
11.58%
25.08%
9.08%
11.54%

F1 score (SUN)
0.0862
0.1559
0.1694
0.0803
0.1247
0.1663

Confusion matrix of the VGG-S network after third round training.

objective
objective

10
1000

Recall (NYU)
50.25%
15.58%
15.53%
49.75%
15.59%
10.05%

F1 score (NYU)
0.0327
0.1098
0.188
0.0251
0.1438
0.1149

(a)

error
error

0.1
0.1
train
train
val
val

Precision (NYU)
1.69%
8.48%
23.82%
1.29%
13.34%
13.41%

traintop1e
traintop1e
traintop5e
traintop5e
valtop1e
valtop1e
valtop5e
valtop5e

0.09
0.09

0.08
0.08
10
10-1-1
0.07
0.07

error
error

energy
energy

0.06
0.06

10
10-2-2

0.05
0.05

0.04
0.04

0.03
0.03
10
10-3-3
0.02
0.02

(b)

0.01
0.01

-4-4

10
10

00

20
20

40
40

training
trainingepoch
epoch

(a)

60
60

80
80

00
00

20
20

40
40

60
60

80
80

training
trainingepoch
epoch

Fig. 8. (a) ROC curves of different models on the SUN kitchen dataset. (b)
ROC curves of the same set of trained models on NYU indoor scene dataset.

(b)

Fig. 7. The loss (a) and the prediction accuracy on validation set (b) of the
VGG-S neural network during the third round of training.

compared to the VGG-F network. The ROC curves from Fig.
8 also support this claim. Thus, in the rest of experiments, we
mainly adopted the model trained using VGG-S architecture
after three rounds of training.
Here we want to mention a caveat. Though we had a
high labeling standard for compiling the new dataset, there
inevitably still exists functional areas that the human labeler
failed to label out, such as the ones shown in Fig 5 (a).
However, from Fig 5 (c,d), we can see that the system
is still able to detect these areas as meaningful functional
parts. For example, the blender (second left) on the table
was not labeled as “two hands to raise” in the ground truth,
but was successfully detected after three rounds of training
on VGG-S network. Using pre-mentioned evaluation metrics,

this example was treated as a “false-positive”, which hurts the
model’s precision score.
Furthermore, real world functional areas can be of different
shapes and sizes. The labeled area usually deviate from the
system proposed area. This phenomenon affected the IOU
values, biased them towards lower value and consequently
compromised the performance metrics of the system. This indicates that the actual performance of our system is potentially
beyond the evaluation metrics reported.
To better demonstrate the system’s performance, we use
bounding boxes with different colors to indicate the functional
areas detected. The correspondence between the functionality
category and the color code can be found in Fig 2.
As mentioned before, we observed that the system trained
using VGG-S network after three rounds of training performs
best. We used this trained model to demonstrate detection
results on the testing images in the SUN kitchen dataset for

(a)

(e)
Fig. 9.

(b)

(f)

(c)

(g)

(d)

(h)

Sample detection results on the test images from the SUN kitchen dataset, using our system with the VGG-S network after three rounds of training.

analysis. From Fig. 9, we can see that our system is able
to detect meaningful functional areas and output high quality
bounding box detections. It can correctly predict intended
functionality even when the areas are tiny and ambiguous. It
is also interesting to note that the system is able to detect
meaningful functionality at some challenging areas of the
scene. For example, a bulb (Fig. 9)(b) is recognized as a
“spherical grasp to open” functional area even though the
bulb is not labeled with any specific functionality in the
training data. Moreover, a round table in the same scene is
recognized as “sittable”, which is physically correct but a
rarely used functionality of it. The performance of the system
from another aspect supports our intuition that the concept of
functional visual areas share similar visual appearance among
the same category and is one of the prerequisites for the
generalization capability.
Since the functionality ontology is innately general to other
indoor domains, we could expect the recognition capability
of the trained model to be generalized to different indoor
scenes. To validate that the trained model is actually capable of
generalizing, we randomly selected 100 scenes from another
publicly available NYU indoor dataset and also labeled them
with ground-truth functional areas to serve as the second
testing set. The new testing set includes static scenes such
as office, living room, bedroom and bathroom. We applied the
pre-trained detection model from SUN kitchen dataset directly
onto this new set of scenes.
The ROC curves and evaluation scores on the new NYU
testing set can be found in Fig 8 (b) and parts of Table I. After
applying our detection pipeline using the pre-trained model,
we achieved comparable performance on the new testing data.
The metric reported are competitive with the ones in the
kitchen scenario. More significantly, the VGG-S model after
three rounds of training performs even better on the NYU

testing set than on the SUN testing set.
We also demonstrate the detection results on the NYU
testing data using bounding boxes with the same color code
(shown in Fig. 10). We can see that, for example, though some
objects such as sofa and bed never exist in the kitchen scene,
the meaningful functionality “sittable” can still be correctly
detected (Fig. 10(a,b,d,e)). These experimental results on the
NYU testing data support our hypothesis that the functional
recognition model learned in the kitchen environment is well
generalizable to other indoor scenarios.
To further validate the applicability of the presented system,
we deployed the trained model (VGG-S after three rounds of
training) onto a humanoid platform (Baxter shown in Fig. 1
in a real world kitchen scene (a computer science student
lounge). Fig. 11 lists example visualizations of our system’s
output. From the visualization, we can see that the system
successfully detects “to sit or to place” areas around chairs,
“turn on/off electricity” around power outlets, “turn on/off
water” areas around faucet and “two hands raise” areas around
containers. We also notice an interesting failure case: the lids
of the water bottles do not exist in the training data, but
our system detects them as “turn on fire”. Even though the
action consequence of turning the lid is predicted wrong, the
intended movement (“turn to open”) is conceptually correct.
These experimental results and observations also support our
intuition that the trained functionality detection model has
generalization capability and can be applied directly onto a
humanoid platform.
VI. C ONCLUSION AND F UTURE W ORK
In this work, we studied the problem of scene functional
understanding for cognitive robots and presented a deep neural
networks based two stage detection pipeline for the task. First
of all, the problem itself is a brand new problem. We compiled
a new scene functionality dataset from two existing publicly

Fig. 10.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Sample detection results from the test images of NYU indoor dataset, using our system with the VGG-S network after three rounds of training.

Fig. 11.

(a)

(b)

(c)

(d)

(e)

(f)

Sample detection results of a dynamic kitchen scene using our system with the VGG-S network after three rounds of training.

available dataset as testing beds and conducted experiments on
it. The experimental results show our method is effective and
efficient in detecting functional areas in an arbitrary indoor
scene. We also conducted a cross validation experiment, and
the experimental result supports our intuition that the presented
functionality detection model has generalization potential.
We found that due to the inherited ambiguity of functionality, the problem itself is more challenging than the
traditional object recognition task. This partially explains why
the experimental results still has potential space to improve.
We also observed that during training stage, the classification
accuracy is almost perfect on validation set, but during testing
phase the detection performance is still far from perfect. One
main reason for the performance drop is believed to be that the
proposed regions contain novel variations or surroundings that

the trained model fails to capture. It is generally considered as
an open problem in vision tasks when using a general region
proposal algorithm as the first stage of the detection pipeline.
To tackle this problem, we are currently experimenting with
one joint training approach, which aims to jointly train the
region proposal and classification stages at the same time.
Moreover, note that the functional categories we consider
in the ontology are only direct functions, which are those can
be directly applied onto objects, such as “to open” and “to
move”. It is much more complicated to categorize secondorder or even third-order functions, such as “to cut”, which
requires first “to move” a tool (knife) and the function is then
applied onto the second object (such as bread). This remains
as an open problem for future research.

R EFERENCES
[1] Aitor Aldoma, Federico Tombari, and Markus Vincze.
Supervised learning of hidden and non-hidden 0-order
affordances and detection in real scenes. In Robotics and
Automation (ICRA), 2012 IEEE International Conference
on, pages 1732–1739. IEEE, 2012.
[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari.
Measuring the objectness of image windows. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on, 34(11):2189–2202, 2012.
[3] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. Return of the devil in the details:
Delving deep into convolutional nets. arXiv preprint
arXiv:1405.3531, 2014.
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages
248–255. IEEE, 2009.
[5] Ian Endres and Derek Hoiem. Category independent
object proposals. In Computer Vision–ECCV 2010, pages
575–588. Springer, 2010.
[6] Clement Farabet, Camille Couprie, Laurent Najman, and
Yann LeCun. Learning hierarchical features for scene
labeling. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 35(8):1915–1929, 2013.
[7] Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. Object detection with
discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32
(9):1627–1645, 2010.
[8] James J Gibson. The theory of affordances. Hilldale,
USA, 1977.
[9] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jagannath Malik. Rich feature hierarchies for accurate object
detection and semantic segmentation. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference
on, pages 580–587. IEEE, 2014.
[10] Helmut Grabner, Juergen Gall, and Luc Van Gool. What
makes a chair a chair? In Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on, pages
1529–1536. IEEE, 2011.
[11] Suzana Herculano-Houzel. The human brain in numbers:
a linearly scaled-up primate brain. Frontiers in human
neuroscience, 3, 2009.
[12] Tucker Hermans, Fuxin Li, James M Rehg, and Aaron F
Bobick. Learning contact locations for pushing and
orienting unknown objects. In Humanoid Robots (Humanoids), 2013 13th IEEE-RAS International Conference
on, pages 435–442. IEEE, 2013.
[13] Hedvig Kjellström, Javier Romero, and Danica Kragić.
Visual object-action recognition: Inferring object affordances from human demonstration. Computer Vision and
Image Understanding, 115(1):81–90, 2011.
[14] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

Saxena. Learning human activities and object affordances
from rgb-d videos. The International Journal of Robotics
Research, 32(8):951–970, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
Imagenet classification with deep convolutional neural
networks. In NIPS, 2012.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing
systems, pages 1097–1105, 2012.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.
Jan H Lui, David V Hansen, and Arnold R Kriegstein.
Development and evolution of the human neocortex.
Cell, 146(1):18–36, 2011.
Austin Myers, Ching L Teo, Cornelia Fermüller, and
Yiannis Aloimonos. Affordance detection of tool parts
from geometric features. In IEEE International Conference on Robotics and Automation (ICRA), 2015.
Rosa C Paolicelli, Giulia Bolasco, Francesca Pagani,
Laura Maggi, Maria Scianni, Patrizia Panzanelli, Maurizio Giustetto, Tiago Alves Ferreira, Eva Guiducci, Laura
Dumas, et al. Synaptic pruning by microglia is necessary
for normal brain development. Science, 333(6048):1456–
1458, 2011.
Tu-Hoa Pham, Abderrahmane Kheddar, Ammar Qammaz, and Antonis A Argyros. Towards force sensing
from vision: Observing hand-object interactions to infer
manipulation forces. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE, 2015.
Alessandro Pieropan, Carl Henrik Ek, and Hedvig Kjellstrom. Functional object descriptors for human activity
modeling. In Robotics and Automation (ICRA), 2013
IEEE International Conference on, pages 1282–1289.
IEEE, 2013.
Ning Qian. On the momentum term in gradient descent
learning algorithms. Neural networks, 12(1):145–151,
1999.
Nathan Silberman and Rob Fergus. Indoor scene segmentation using a structured light sensor. In Computer Vision
Workshops (ICCV Workshops), 2011 IEEE International
Conference on, pages 601–608. IEEE, 2011.
Louise Stark and Kevin Bowyer. Function-based generic
recognition for multiple object categories. CVGIP: Image
Understanding, 59(1):1–21, 1994.
Yuyin Sun, Liefeng Bo, and Dieter Fox. Attribute based
object identification. In Robotics and Automation (ICRA),
2013 IEEE International Conference on, pages 2096–
2103. IEEE, 2013.
Jasper RR Uijlings, Koen EA van de Sande, Theo Gevers,
and Arnold WM Smeulders. Selective search for object
recognition. International journal of computer vision,
104(2):154–171, 2013.
Florentin Worgotter, Eren Erdal Aksoy, Norbert Kruger,

[29]

[30]

[31]

[32]

[33]

Justus Piater, Ales Ude, and Minijia Tamosiunaite. A
simple ontology of manipulation actions based on handobject relations. Autonomous Mental Development, IEEE
Transactions on, 5(2):117–134, 2013.
Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva,
Antonio Torralba, et al. Sun database: Large-scale scene
recognition from abbey to zoo. In Computer vision and
pattern recognition (CVPR), 2010 IEEE conference on,
pages 3485–3492. IEEE, 2010.
Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. Inferring “dark matter” and “dark energy” from videos.
In Computer Vision (ICCV), 2013 IEEE International
Conference on, pages 2224–2231. IEEE, 2013.
Yezhou Yang, Cornelia Fermüller, and Yiannis Aloimonos. Detection of manipulation action consequences
(MAC). In Proceedings of the 2013 IEEE Conference on
Computer Vision and Pattern Recognition, pages 2563–
2570, Portland, OR, 2013. IEEE.
Yezhou Yang, Cornelia Fermüller, Yi Li, and Yiannis
Aloimonos. Grasp type revisited: A modern perspective
on a classical feature for vision. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2015.
Yixin Zhu, Yibiao Zhao, and Song-Chun Zhu. Understanding tools: Task-oriented object modeling, learning
and recognition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
2855–2864, 2015.

Neurocomputing 119 (2013) 222–231

Contents lists available at SciVerse ScienceDirect

Neurocomputing
journal homepage: www.elsevier.com/locate/neucom

Color-to-gray based on chance of happening preservation
Mingli Song a, Dapeng Tao b,n, Chun Chen a, Jiajun Bu a, Yezhou Yang c
a
b
c

College of Computer Science, Zhejiang University, Hangzhou 310027, China
School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510640, China
Department of Computer Science, University of Maryland, College Park, USA

art ic l e i nf o

a b s t r a c t

Article history:
Received 17 October 2012
Received in revised form
5 February 2013
Accepted 15 March 2013
Communicated by Pingkun Yan
Available online 20 June 2013

It is important to convert color images into grayscale ones for both commercial and scientiﬁc applications,
such as reducing the publication cost and making the color blind people capture the visual content and
semantics from color images. Recently, a dozen of algorithms have been developed for color-to-gray
conversion. However, none of them considers the visual attention consistency between the color image
and the converted grayscale one. Therefore, these methods may fail to convey important visual information
from the original color image to the converted grayscale image. Inspired by the Helmholtz principle
(Desolneux et al. 2008 [16]) that “we immediately perceive whatever could not happen by chance”, we
propose a new algorithm for color-to-gray to solve this problem. In particular, we ﬁrst deﬁne the Chance of
Happening (CoH) to measure the attentional level of each pixel in a color image. Afterward, natural image
statistics are introduced to estimate the CoH of each pixel. In order to preserve the CoH of the color image
in the converted grayscale image, we ﬁnally cast the color-to-gray to a supervised dimension reduction
problem and present locally sliced inverse regression that can be efﬁciently solved by singular value
decomposition. Experiments on both natural images and artiﬁcial pictures suggest (1) that the proposed
approach makes the CoH of the color image and that of the converted grayscale image consistent and
(2) the effectiveness and the efﬁciency of the proposed approach by comparing with representative
baseline algorithms. In addition, it requires no human–computer interactions.
& 2013 Elsevier B.V. All rights reserved.

Keywords:
Color-to-gray (C2G)
Chance of happening (CoH)
Visual attention

1. Introduction
Color-to-gray algorithms convert color images to grayscale ones
for different purposes, such as, publishing less expensive alternatives
to full color printings, e-ink based book reader, and producing
reading materials for color blind people. By converting a color image
to a grayscale one, we will inevitably lose the visual information, and
thus a dozen of color-to-gray algorithms [1] have been presented to
remain the visual information as much as possible.

1.1. Related work
Color-to-gray through a linear combination of R, G and B
channels is the most popular and efﬁcient strategy to convert a
color image into a grayscale one. Wyszecki and Stiles [2] have
combined the R, G, B channels by using a group of weighted linear
functions. Wu and Rao [3] have linearly combined the R, G, and B
channels by deﬁning a series of policies to separate luminance
value from the chrominance value so as to construct the gray-scale
image based on the luminance value.
n

Corresponding author. Tel.: +86 13 560476207.
E-mail address: dtao.scut@gmail.com (D. Tao).

0925-2312/$ - see front matter & 2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.neucom.2013.03.037

Color-to-gray can also be treated as a dimension reduction
problem. Therefore, unsupervised dimension reduction algorithms
can be employed to transform the three dimensional color-space
to one dimensional grayscale. In particular, principal component
analysis (PCA) [5] constructs the covariance matrix based on the
statistics of the R, G, and B values of pixels and projects color
pixels to the leading principle component. The method assumes
that observations are drawn from a single Gaussian and ignores
the spatial distribution of color pixels. Although its kernelization
[6] can deal with the non-Gaussian distribution property of color
pixels, the computational complexity is too expensive to be
applicable in practice, let alone the sensitivity of the kernel
parameter to different images.
Bala and Braun [7] have sorted all of the colors in an image
according to their lightness and arranged the colors to grayscales
by using a series of weights which are proportional to their color
distance. It performs well for images with simple structures, e.g.,
color bins and blocks, but fails for images with high frequency, e.g.,
a rasterized graphic. Afterward, Bala and Eschbach [8] have
applied the high frequency chrominance information to the
luminance channel to preserve the difference between adjacent
colors locally in the converted grayscale image.
Rasche et al. [10] have deﬁned an objective function that
enforces the proportional color differences across the mapping
between color and grayscale based on a subset of pixels in a color

M. Song et al. / Neurocomputing 119 (2013) 222–231

223

image. By minimizing an objective function, a linear mapping is
obtained for color-to-gray. However, it ignores the spatial distribution of different colors and the different perceiving effects of such
spatial distribution. Therefore, it fails to deal well with images
containing small splashes of colors. To reduce this problem, they
[11] have introduced multidimensional scaling (MDS) [12] to the
CIELAB color space [2] for color-to-gray. Unfortunately, its computational cost is not acceptable for images with abundant colors.
By considering the spatial distribution of colors, Socolinsky and
Wolff [9] have utilized image gradient information to describe the
color difference between neighboring pixels and applied such a
difference to implement color-to-gray. This method cannot deal
with long-scale different regions. Especially it is challenging for
this method to convert the pseudo-isochromatic plates to grayscale images while preserving the semantic content.
Gooch et al. [13] have presented an interactive Color2Gray
method that tuned three parameters: the difference in color
chrominance, the contrast between colors, and the size of a pixel's
neighborhood. Both the color difference and the spatial distribution of colors are taken into account. By using this algorithm, users
can preserve their preferred visual cues via tuning the above three
parameters manually. Smith et al. [14] have presented a color-togray method by combining the global Helmholtz–Kohlrausch color
appearance mapping and the multiscale local contrast enhancement. Human–computer interactions (HCIs) are also required to
obtain the multiscale local contrast enhancement, which makes
the scheme suited for natural images.
Most recently, Song et al. [15] have deﬁned three visual cues by
considering the color spatial distribution, gradient information
and the perception priority of hue, chroma and lightness. By
casting color-to-gray as a visual cue preservation procedure
(VCP), a probabilistic graphical model and an efﬁcient optimization have been presented and achieved top level performance
compared with representative conventional algorithms without
HCIs.

visual cue preservation (VCP) [15], all fail to transfer the visual
attention from the color image to the converted grayscale one. In
the original color image, the yellow bush attracts observers'
attention but the bush in the converted grayscale images cannot.
Therefore, it is essential to develop a new color-to-gray algorithm
to analyze the attentional importance of pixel and make the
important pixels being distinct in the color-to-gray operation.
According to the Helmholtz principle in Gestalt psychological
theory [16] “Whenever some large deviation from randomness
occurs, a structure is perceived”. As a commonsense statement, the
Helmholtz principle means that “we immediately perceive whatever could not happen by chance”. This description inspires us
with two cues: on one hand, the importance of a region depends
heavily on the Chance of Happening (CoH), which is estimated
based on the previous experience of HVS; on the other hand, only
the region which “could not happen by chance” tends to be
“immediately perceived” by HVS, where “could not happen by
chance” means the CoH of the region is small.
Inspired by the Helmholtz principle above, we employ the CoH
to evaluate the attentional importance of pixels in the color image
for the subsequent color-to-gray. In particular, we estimate CoH
based on the natural image statics and develop locally sliced
inverse regression (LSIR) to transfer CoHs of pixels in the color
image (and visual attention of the color image accordingly) to the
converted grayscale image. Thorough empirical evaluations show
the effectiveness of this CoH preserving based color-to-gray by
comparing with representative color-to-gray baseline algorithms.
The rest of the paper is organized as follows. Section 2
introduces the LSIR based CoH preserving for color-to-gray.
Section 3 deﬁnes CoH and presents its computation based on
natural image statistics. Afterward, LSIR for CoH preserving based
color-to-gray is described in Section 4. Experimental results in
Section 5 thoroughly demonstrate the effectiveness of the new
algorithm. Section 6 concludes the paper.

1.2. Motivation of the proposed approach

2. LSIR based CoH preserving for color-to-gray

Although many representative methods have been proposed as
aforementioned, it is obvious that these existing methods fail to
consider the importance of distribution of visual information from
the view of human visual system. It has been widely acknowledged that human visual system is usually sensitive to the
important visual cues and neglects unimportant ones in an image
[4]. For example, as shown in the converted grayscale image in
Fig. 1, the grayscale renderings of the color image by linearly
combining R, G, and B according to [3], by principal component
analysis (PCA) [5], by multi-dimensional scaling (MDS) [11] and by

Given a color image Ic, the color-to-gray conversion can be
treated as a dimension reduction process by seeking a proper
projection vector β to map Ic to a grayscale image Ig,
I g ¼ βT I c :

ð1Þ

Different from the existing approaches, the new color-to-gray
algorithm preserves the CoH of each pixel in Ic. The CoHs of pixels
in a color image indicate particular properties of the image that
attract receivers' attention. Fig. 2 shows that the proposed scheme
for color-to-gray by using LSIR for preserving CoH or transferring

Fig. 1. Existing color-to-gray approaches [3,5,11,15] fail to transfer the visual attention from the color image to the converted grayscale one. In the original color image, the
yellow bush attracts observers' attention but the bush in the converted grayscale images cannot. (For interpretation of the references to color in this ﬁgure caption, the reader
is referred to the web version of this article.)

224

M. Song et al. / Neurocomputing 119 (2013) 222–231

CoHs of pixels in the color image to the converted grayscale image.
The proposed scheme contains two stages: CoH computation and
LSIR based dimension reduction for color-to-gray by preserving
CoH.

from a set of natural images, we present a Bayesian framework to
obtain the CoH of a pixel.

 CoH computation: The CoH of a pixel stands for how likely this

Color statistics have been widely used in image processing and
computer vision. In digital image coding systems, YCbCr is a preferable
color space, because of three factors: (1) YCbCR takes the property of
HVS into account. Study shows human eyes are sensitive to luminance,
but not so sensitive to chrominance. YCbCR color space makes use of
this fact to achieve more efﬁcient representation of images. It
separates the luminance and chrominance components of an image.
(2) YCbCR is used for digital encoding of color information in
computing systems. Some other color spaces, like YUV, are traditionally used for analog encoding of color information in television system.
(3). The conversion between RGB and YCbCR is linear. Although Lnanbn
and HSV are a color space that also matches the human visual system.
However, the conversion between Lnanbn, HSV and RGB are nonlinear,
so their conversions are more time consuming than that of YCbCr [17].
Therefore, we adopt YCbCr in learning CSP.
Given a collection of natural color images, Fig. 3 shows
distributions of Y, Cb and Cr. In particular, the distribution of Y
tends to be uniform (cf. Fig. 3(a)), so it is not helpful for calculating
CoH. In contrast to Y, distributions of Cb (cf. Fig. 3(b)) and Cr
(cf. Fig. 3(c)) are Laplace-like, and thus they are important for the
modeling of CoH. In this paper, distributions of Cb and Cr are
modeled by the generalized normal distributions, i.e., for a pixel

3.1. CSP: color statistics based priors

pixel will appear at a speciﬁc location in an image. According to
understandings to the Helmholtz principle [16], CoH is mainly
determined by two groups of priors, i.e., color statistics and
spatial correlations. In this paper, we learn these priors in a
computationally plausible way and use them to estimate the
CoH of a pixel based on a Bayesian framework.
LSIR based dimension reduction for color-to-gray: We cast the
process of color-to-gray to a supervised dimension reduction
problem, locally sliced inverse regression (LSIR) that extends
the sliced inverse regression (SIR) [19] by considering the local
geometry of the data distribution. LSIR can be efﬁciently and
effectively solved by singular value decomposition.



3. On calculating the chance of happening
The CoH of a pixel represents how likely this pixel will appear
at a speciﬁc location in an image. It is mainly determined by two
groups of priors, which are color statistics based priors (CSP) and
spatial correlations based priors (SCP). After obtaining CSP and SCP

D = 10

Single Feature Prior

Surrounding Feature Prior

Bayesian Framework

Chance of Happening

LSIR based dimension reduction

0.14

0.12

0.12

0.10

0.10

0.08
0.06

0.06
0.04

0.02

0.02
0

50

100
150
200
Value of Feature Y

250

0
-100 -50 0

0.25
0.20

0.08

0.04

0

c

Probability

0.14

Probability

Probability

Fig. 2. LSIR based CoH preserving for color-to-gray contains two stages: CoH computation and LSIR based dimension reduction for color-to-gray by preserving CoH.

0.15
0.10
0.05

50 100 150 200 250 300 350
Value of Feature Cb

0
-100 -50 0

50 100 150 200 250 300 350
Value of Feature Cr

Fig. 3. (a) CSP of Y: the distribution of Y in natural images; (b) CSP of Cb: the distribution of Cb in natural images; (c) CSP of Cr: the distribution of Cr in natural images.
The blue bars are histograms of Y/Cb/Cr values obtained from a collection of natural images and two red curves are the ﬁtted generalized normal distributions for histograms
of Cb and Cr, respectively. (For interpretation of the references to color in this ﬁgure caption, the reader is referred to the web version of this article.)

M. Song et al. / Neurocomputing 119 (2013) 222–231

compute the CoH of a pixel CoHðx; yÞ by taking of both CSP and
SCP into account based upon the Bayesian theorem

(x,y), we have
PðCbðx; yÞ; μb ; sb ; θb Þ
 
 
θb
 Cbðx; yÞ−μb θb
¼
exp −

2sb τð1=θb Þ
sb
PðCrðx; yÞ; μr ; sr ; θr Þ
 
 
θr
 Crðx; yÞ−μr θr
exp −
¼
 ;
2sr τð1=θr Þ
sr

ð2Þ

ð3Þ

where τ is a gamma function, sðÞ is the scale parameter that describes
the standard deviation of the density, μðÞ is the mean of the density,
and θðÞ is the shape parameter that is inversely proportional to the
decreasing rate of the peak. Given a collection of natural color images,
the model parameters ðμðÞ ; θðÞ ; sðÞ Þ can be estimated by maximum
likelihood estimation, μb ¼ 130, μr ¼ 130, sb ¼ 0:23, sr ¼ 0:041,
θb ¼ 0:26, and θr ¼ 0:22. Fig. 3 shows the estimated distributions.

CoHðx; yÞ ¼ Pðhðx; yÞjΩðx; yÞÞ
¼ PðΩðx; yÞ; hðx; yÞÞ=PðΩðx; yÞÞ;

PðΩðx; yÞ; hðx; yÞÞ ¼
¼
¼

SCP reﬂects the local contrast of an image. Given a surrounding
window of size ð2w−1Þ by ð2w−1Þ, we have two distances, i.e., the
intensity distance and the spatial distance, are deﬁned between
pixel (x,y) and one of its surrounding pixels ðxj ; yj Þ inside the
window, j∈½1; ð2w−1Þnð2w−1Þ. As the dynamic ranges of Y, Cb, and
Cr are same, it is natural to treat them equivalently instead of
adding weights on each component to balance their numerical
values. The intensity distance in our approach is
DF i ðx; yÞ ¼ jYðx; yÞ−Yðxj ; yj Þj þ jCbðx; yÞ−Cbðxj ; yj Þj
ð4Þ

and the spatial distance is
DLj ðx; yÞ ¼ maxðjx−xj j; jy−yj jÞ;

ð5Þ

where j  j is the absolute operation and maxð; Þ returns the largest
value of the two inputs.
SCP reﬂects the local contrast distribution of natural images. In
order to ﬁgure out the local contrast distribution of natural image, we
learn the SCP by counting the number of pixels for each DL varies from
1 to ðw−1Þ in the range of DF based on the collected natural images.
For example, in order to learn the SCP on Y, we count the number of
pixels for each DL value in Y channel, respectively, in the range of
½0; 255 by scanning all the pixels in all the collected natural images.
Such counting phase helps us to learn the local contrast distribution in
the natural images. The counting results on Y, Cb and Cr are given in
Fig. 3, respectively. By observing Fig. 3, it is noticeable that the three
distributions of intensity distance for different location distances are
different from each other, but they are all exponential like. So we use
exponential function to ﬁt these distributions. Parameters of these
distributions are estimated by the maximum likelihood estimation.
Given a pixel (x,y), the existence probability of a pixel ðxj ; yj Þ in the
surrounding window is given by
Pððxj ; yj Þjðx; yÞÞ ¼ expð−ϕðDLj ðx; yÞÞDF j ðx; yÞÞ;

ð6Þ

ϕðÞ is an enumerate function that produces the unique coefﬁcient for
each intensity distance DF j ðx; yÞ corresponding to each location
distance DLj ðx; yÞ by exponential regression. In our approach, we set
w for the surrounding window to 41, and the estimated value of ϕðÞ
ranges from 0.955 to 0.472 while DL increases from 1 to 40. The
original and approximated distributions are illustrated in Fig. 4.

ð7Þ

where hðx; yÞ is the value of pixel ðxw ; yw Þ, and Ωðx; yÞ represents
the values of pixels in the surrounding window of pixel (x,y).
As the CSP value and the SCP value of each pixel do not depend
on those of their neighbors, we suppose that the ð2w−1Þnð2w−1Þ
surrounding pixels are independent to each other. Since
PðΩðx; yÞ; hðx; yÞÞ is the joint probability that represents the cooccurrence of the pixel (x,y) and its surrounding pixels, it is given by

3.2. SCP: spatial correlations based priors

þjCrðx; yÞ−Crðxj ; yj Þj;

225

ð2w−1Þnð2w−1Þ

∏

j¼1
ð2w−1Þnð2w−1Þ

∏

j¼1
ð2w−1Þnð2w−1Þ

∏

j¼1

Pððxj ; yj Þ; ðx; yÞÞ
Pððxj ; yj Þjðx; yÞÞPðx; yÞ
Pððxj ; yj Þjðx; yÞÞPðCbðx; yÞÞPðCrðx; yÞÞ:
ð8Þ

Since PðΩðx; yÞÞ is the probability representing the occurrence of
the surrounding pixels, it is given by
ð2w−1Þnð2w−1Þ

PðΩðx; yÞÞ ¼

∏

j¼1

¼

ð2w−1Þnð2w−1Þ

∏

j¼1

Pðxj ; yj Þ
PðCbðxj ; yj ÞÞPðCrðxj ; yj ÞÞ:

ð9Þ

Substituting (8) and (9) to (7), we have

CoHðx; yÞ ¼ Pðhðx; yÞΩðx; yÞÞ
¼

nð2w−1Þ
∏jð2w−1Þ
Pððxj ; yj Þjðx; yÞÞPðCbðx; yÞÞPðCrðx; yÞÞ
¼1
nð2w−1Þ
PðCbðxj ; yj ÞÞPðCrðxj ; yj ÞÞ
∏jð2w−1Þ
¼1

:

ð10Þ

For computation, we rewrite (10) as
CoH′ðx; yÞ-log CoHðx; yÞ


 
 

 Cbðxj ;yj Þθb  Crðxj ;yj Þ θr
nð2w−1Þ
j
j
¼ ∑jð2w−1Þ
ϕðD
ðx;
yÞÞD
ðx;
yÞ−
−




sb
sr
¼1
l
f


 

 Cbðx; yÞθb  Crðx; yÞ θr
þð2w−1Þnð2w−1Þn 
:
 þ

sb
sr

ð11Þ

For each image, we rearrange the value of CoH′ðxw ; yw Þ between
0 and 1. We summarize the computation of CoH in Algorithm 1.
CSP and SCP are learned from 1000+ natural color images
collected from the Internet. Speciﬁcally, we adopt both look-up
table and integral image [18] techniques to speed up the computation of CoH value. Therefore, only searching, adding and subtracting are involved in the computation according to Eq. (11). These
operations make the computation simple and efﬁcient.
Algorithm 1. Chance of happening computation.
1: Given a color image dataset, convert the color images from
the RGB color space to the YCbCr color space;
2: Learning the Color Statistics based Priors (CSP) based on
Eqs. (2) and (3);
3: Learning the Spatial Correlations based Priors (SCP) based
on Eq. (6);
4: Compute the Chance of Happening (CoH) value of each point
numerically in a given color image according to Eq. (11).

3.3. A Bayesian framework for CoH computation
For a pixel (x,y) in a color image, its CoH should reﬂect the
happening probability of this pixel given a surrounding window
Ωðx; yÞ, and thus its CoH should consider both the color value itself
and the visual context in the surrounding window. Thus, we

4. LSIR for CoH preserving
We cast the process of color-to-gray to a supervised dimension
reduction problem. Given a color image of size W  H, we can

Probability

226

M. Song et al. / Neurocomputing 119 (2013) 222–231

0.7

0.7

0.7

0.7

0.6

0.6

0.6

0.6

0.6
0.5

0.7

0.5

0.5

0.5

0.5

0.4

0.4

0.4

0.4

0.4

0.3

0.3

0.3

0.3

0.3

0.2

0.2

0.2

0.2

D =1

0.1
0

D =2

0.1

0

0

0

5 10 15 20 25 30 35 40 45 50

0

5 10 15 20 25 30 35 40 45 50

D = 10

0.1

0.2

D = 39

0.1

0

5 10 15 20 25 30 35 40 45 50

D = 40

0.1
0

0
0

5 10 15 20 25 30 35 40 45 50

0

5 10 15 20 25 30 35 40 45 50

Intensity Distance
Fig. 4. Surrounding feature prior. The blue bars represent the origin distribution and the red lines are estimated exponential probability distribution. (For interpretation of
the references to color in this ﬁgure caption, the reader is referred to the web version of this article.)

obtain a set of labelled samples D ¼ fIðiÞ; LðiÞj1≤i≤W  Hg, wherein I
(i) is the RGB vector of the i-th pixel and L(i) is the corresponding
label obtained by separating CoHs of pixels into two groups
according to a predeﬁned threshold ε

1 CoH′ðx; yÞ oε
LðiÞ ¼ Lðx; yÞ ¼
ð12Þ
0 Otherwise
We introduce locally sliced inverse regression (LSIR) that
extends sliced inverse regression (SIR) [20] by preserving the local
geometry of conditional distributions. LSIR ﬁnds a suitable projection vector based on the label set L to transfer the CoHs of pixels in
the color image to the converted grayscale image.
4.1. Sliced inverse regression
Since LSIR is an extension of SIR, we ﬁrst brieﬂy review SIR. SIR
models the dependency between the label L and the RGB pixels I
as
L ¼ f ðβT I; ϵÞ;

ð13Þ

where β is the linear independent projection vector and ϵ is the
independent noise. Eq. (13) implies that βT I is sufﬁcient for the
prediction of L, i.e., the conditional independence L⊥IjβT I.
SIR uses the conditional expectation EðIjLÞ to form a curve in R3
by varying L. Under mild conditions, it has been proved that the
inverse regression curve falls in β. Thus, β can at least be partially
estimated by performing the principle component analysis on the
inverse regression curve EðLjIÞ. In SIR, β is the eigen-vector of the
covariance matrix Γ ¼ covðEðIjLÞÞ associated with the largest eigenvalue. By taking the data covariance Σ ¼ covðIÞ into account, the
objective function of SIR is
maxðβT ΣβÞ−1 βT Γβ:
β

ð14Þ

A main limitation of SIR is that it ignores the local geometry of
data distribution. The inverse regression curve EðIjLÞ globally
encodes the conditional distribution PðIjLÞ, i.e., SIR loses important
discriminative information when PðIjLÞ is complex, such as asymmetric and nonlinear.
4.2. Locally sliced inverse regression
LSIR remains the local geometry of the conditional distribution
PðIjLÞ, i.e., close data from the same class are still close to each
other in β. To this end, we minimize the following item to maintain
the local geometry:
RðβjLÞ ¼ EIðiÞ;IðjÞ;PðIðiÞjLðiÞÞ ½wði; jÞ∥βT IðiÞ−βT IðjÞ∥2 ;
where wði; jÞ is an indicator function given by
(
1; ∥IðiÞ−IðjÞ∥o δ
wði; jÞ ¼
0; ∥IðiÞ−IðjÞ∥4 δ;

ð15Þ

ð16Þ

and the parameter δ is used to specify the closeness between data.

Intuitively, RðβjLðiÞÞ measures the average distance of close data
from PðIðiÞjLðiÞÞ after being projected onto β. Thus, the smaller
RðβjLðiÞÞ is, the better the local geometry of PðIðiÞjLðiÞÞ can be
preserved. Since there are 2 classes for CoH preserving based
color-to-gray, we have
2

RðβÞ ¼ ∑ Pðy ¼ cÞRðβjy ¼ cÞ;
c¼1

ð17Þ

where Pðy ¼ cÞ is the prior probability of the class c. By combining
(16) with (14), we obtain LSIR:
maxðβT ΣβÞ−1 βT Γβ=RðβÞ:
β

ð18Þ

It is not trivial to solve LSIR (18). We initially need the empirical
^
estimators Σ^ , Γ^ and RðβÞ.
Given a training set D, the following
estimators can be obtained:
8
1
>
>
>
> μ^ c ¼ n LðiÞ∑¼ cIðiÞ
>
c
>
>
>
<
1
μ^ ¼ ∑ IðiÞ
ð19Þ
N IðiÞ∈I
>
>
>
>
>
>
1
>
>
^
μÞ
^ T
: Σ^ ¼ ∑ ðIðiÞ−μÞðIðiÞ−
NIðiÞ∈I
where nc is the number of samples in class c. Since μ^ c is an
empirical estimator of EðIðiÞjLðiÞ ¼ cÞ, a point on the inverse
regression curve EðIðiÞjLðiÞÞ, the matrix Γ can be estimated by
1 2
∑ nc ðμ^ c −μÞð
^ μ^ c −μÞ
^ T
Γ^ ¼
Nc¼1

ð20Þ

For RðβjLðiÞ ¼ cÞ, we have the following estimator from (15):
1
^
∑
wði; jÞ∥βT IðiÞ−βT IðjÞ∥2
RðβjLðiÞ
¼ cÞ ¼ 2
nc LðiÞ ¼ LðjÞ ¼ c

ð21Þ

By some algebra, (21) can be simpliﬁed as
1
^
RðβjLðiÞ
¼ cÞ ¼ 2 βT I c ðDc −W c ÞI Tc β
nc

ð22Þ

where I c ∈R2nc contains the training data from class c, W c ∈Rnc nc is
the indicator matrix with its entry wði; jÞ for class c, and Dc is a
c
diagonal matrix with the entry Dc ði; iÞ ¼ ∑nj ¼
1 W c ði; jÞ.
^
Then, we have RðβÞ:
1 2 1 T
^
∑
β I c ðDc −W c ÞI Tc β
RðβÞ
¼
N c ¼ 1 nc
¼ βT Aβ

ð23Þ

where A ¼ ð1=NÞ∑2c ¼ 1 ð1=nc ÞI c ðDc −W c ÞI Tc .
In order to preserve the saturation of the color image in the
grayscale result, the intensity scale variance is taken into account
in our approach. Speciﬁcally, we rescale Γ^ according to
1=2
1=2
Γ^ ←Σ S Γ^ Σ S ;

ð24Þ
T

^ ðI−μÞ
^ is the intensity scale variance.
where Σ S ¼ ðI−μÞ

M. Song et al. / Neurocomputing 119 (2013) 222–231

227

Fig. 5. For this case, the most attentional area is the grass in the middle of the image. The proposed CoH based approach achieved the best performance. Though MDS and
VCP produce more visual details, e.g., the cloud, the attentional area is not distinguished as well as the proposed CoH based approach.

Fig. 6. For this case, the most attentional area centers at the pink rope. The proposed CoH based approach performed best and VCP performed comparably to PCA and
Photoshop. MDS produced sharper results but failed to preserve the visual attention consistency to the rope. And the attentional area shifted to the cage on the top left in the
results from VCP, PCA and Photoshop. (For interpretation of the references to color in this ﬁgure caption, the reader is referred to the web version of this article.)

228

M. Song et al. / Neurocomputing 119 (2013) 222–231

Therefore, substituting (19), (23) and (24) into (18), we obtain
the empirical LSIR for converting color images to the corresponding grayscale ones:
1=2
1=2
maxðβT Σ^ βÞ−1 βT Σ S Γ^ Σ S β=ðβT AβÞ:
β

ð25Þ

Since only the direction of β is of interest, we restrict ðβT AβÞ ¼ 1
and introduce a new variable β1 ¼ A1=2 β, by which (25) becomes
1=2
1=2
maxðβT1 A−1=2 Σ^ A−1=2 β1 Þ−1 βT1 A−1=2 Σ S Γ^ Σ S A−1=2 β1 :
β1

subjects sat at a distance of approximately 60 cm from a 19 in.
computer screen of resolution 1280  1024 in a dark room and
used a chin rest to stabilize their head. The eye tracker records the
gaze path on a separate computer as they viewed each image at
full resolution for 3 s separated by 1 s of viewing a gray screen. The
longest dimension of each image was 1024 pixels and the other
dimension ranged from 405 to 1024.

ð26Þ
5.1. Comparison with existing techniques

Algorithm 2. LSIR based color-to-gray algorithm for CoH preservation.
1:
2:
3:
4:
5:
6:

Separate pixels into two groups according to (12);
Calculate Σ^ and Γ^ by (19) and (20);
Calculate Wc, c¼ 1,2, and get A by (23);
Calculate Σ S by (24);
Compute the projection vector β1 according to (27);

Get β1 ¼ A1=2 β;
7: Obtain the grayscale image by (1).

According to [21,22], the optimal solution β1 of (26) is the ﬁrst
eigenvector of the following generalized eigen-decomposition as
Eq. (27). This eigen-decomposition property makes it easier and
faster to carry out computation than some state-of-the-art locality
preserving dimensional reduction [26–28]:
1=2
1=2
ðA−1=2 Σ s Γ^ Σ S A−1=2 Þβ1 ¼ λðA−1=2 Σ^ A−1=2 Þβ1 :

In Figs. 5–8, we in turn show the original color image, the
converted grayscale images by Photoshop, PCA, MDS, VCP and
CoH, and their corresponding saliency maps obtained from the eye
tracking records. For the proposed CoH preserving based color-togray, we set λ ¼ 0:3 for all the experiments. We show how this
parameter affects the performance in the next experiment. For fair
comparison, parameters in algorithms are tuned to the best
settings to achieve the best performance for VCP and MDS based
color-to-gray.
Figs. 5–8 suggest that Photoshop and PCA fail to preserve the
attention in the converted grayscale image. Although MDS and

ð27Þ

Given the projection vector β obtained from the empirical LSIR
(25), we can project RGB color vectors to gray scale values. This
projection transfers the CoHs of pixels in the color image to the
converted grayscale image. Algorithm 2 details the implementation procedure of the proposed LSIR for CoH preserving based
color-to-gray algorithm.

5. Experimental results
In this section, we ﬁrst evaluate the proposed CoH preserving
based color-to-gray algorithm by comparing with well-known
automatic color-to-gray techniques, which are the color-to-gray
function provided by Adobe Photoshop [1], the principle component analysis based color-to-gray (PCA) [5], the multidimensional
scaling based color-to-gray (MDS) [11], and the visual cue preserving based color-to-gray (VCP) [15] in terms of effectiveness and
efﬁciency. We do not use the interactive approaches as baselines,
because they can usually achieve the wanted results by introducing complex HCIs. We then discuss the inﬂuence of the sampling
window size, the only free parameter used in the proposed
approach for color-to-gray. Finally, we provide additional examples to show the effectiveness of the proposed color-to-gray
algorithm.
We carry out eye tracking on a set of color images and the
corresponding converted grayscale images for color-to-gray conversion quality assessment. Results of eye tracking evaluation are
directly given by human observers, so it is probably the best way
to compare different color-to-gray algorithms for attention pres
servation. In our experiments, we use EyeLink II eye tracker to
record the observers visual attention, which reﬂects the CoH of
s
each point on an image. The EyeLink II has the fastest data rate
and highest resolution in head mounted video-based eye trackers,
and thus it is ideal for saccade analysis and smooth pursuit studies.
We have 16 subjects with age between 22 and 35 involved in the
eye tracking experiment for color-to-gray quality assessment.
All the subjects have normal or corrected-to-normal vision. All

Fig. 7. For this case, the most attentional area centers at the head of the boat. The
proposed CoH based approach distinguishes the attentional area most. PCA, VCP
and the proposed method are comparable. (For interpretation of the references to
color in this ﬁgure caption, the reader is referred to the web version of this article.)

M. Song et al. / Neurocomputing 119 (2013) 222–231

229

Fig. 8. For this case, the most attentional area is the yellow bush on the top left. The proposed CoH based approach distinguishes the attentional area consistently while the
attentional level to the bush is weakened in the results from other approaches. (For interpretation of the references to color in this ﬁgure caption, the reader is referred to the
web version of this article.)

Fig. 9. The impact of the size of sampling windows for the proposed color-to-gray algorithm: The source image was obtained from [19].

VCP can preserve more visible details in the converted grayscale
image, they cannot transfer the attention information from the
color image to the converted grayscale image, e.g., the grassland in
the color image in Fig. 6 and the pink rope in the color image in
Fig. 7. In contrast, CoH always preserves the visual attention area
better than other algorithms. For example, CoH distinguishes the
grassland well in Fig. 6. The corresponding saliency maps obtained
from the eye tracking records validate CoH based color-to-gray can
effectively preserve the visual attention area.
The computational cost of the proposed method is comparable
to several popular approaches. It usually takes around 60 s to
convert a color image of size 800  600. MDS costs more than
120 s under the same condition (but using the color quantization
for speedup). Other methods are slightly faster but fail to properly
preserve the visual attention consistency.

5.2. Experiments with different sampling window sizes
In this experiment, we study how free parameters affect the
performance of the proposed scheme and how to choose suitable
parameters to achieve a good synthesized grayscale image. In the
proposed scheme, we have three model parameters only, which
are the size of the sampling window coefﬁcient w in Eq. (11), the
threshold ε in Eq. (12), and the threshold δ in Eq. (16). Based on our
experience, we can always achieve reasonable results by setting
ε ¼ 0:2 and δ ¼ 200. At current stage, the only free parameter is the
size of sampling windows.
To analyze the impact of the size of sampling windows on the
performance of the proposed approach, we set up a set of
experiments by varying the window size ð2w−1Þnð2w−1Þ continuously. In Fig. 9, we display three transformed gray-scale images

230

M. Song et al. / Neurocomputing 119 (2013) 222–231

Fig. 10. Further examples for the proposed approach: The top row shows the original color images, the second row shows the grayscale images converted by the Photoshop
color-to-gray function and the bottom row shows the grayscale images converted by the proposed algorithm. Source images are from [19] and the Internet.
(For interpretation of the references to color in this ﬁgure caption, the reader is referred to the web version of this article.)

corresponding to different w. Based on this ﬁgure, we have the
following observations: (1) according to the gray-scale images in
Fig. 9, if the sampling window is too small, the computed grayscale
image cannot preserve the CoH very well; and (2) if the sampling
window becomes larger, the CoH can be preserved better, but
noise is brought at the same time. In addition, larger sampling
window leads to much more time consumption to produce the
result. In our approach, we choose w¼ 43 for 800  600 images.
5.3. Further examples for the proposed approach
Further experimental results are given in Fig. 10. In the ﬁrst
column (from the left), the most attentional area is the orange,
which is well preserved in the grayscale image by the proposed
method. The result shown in the middle row, obtained by Photoshop, cannot distinguish the attentional area very well. We also
test the new method on an artiﬁcial color dot pattern in the
second column. The grayscale image from the proposed method
demonstrates that the proposed approach successfully distinguishes the number depicted in the color image. Moreover, in
the third column, the red extinguisher is distinguished well to be
noticed. And from the fourth to the sixth column, the most
attentional areas, i.e., the deer, the red pimiento, and the gray
bird, successfully capture attention in the grayscale images as they
do in the color image.

6. Conclusion and future work
Conventional color-to-gray algorithms have been successfully
applied in practice but fail to preserve the visual attention
consistency between the color image and the converted grayscale
image. To solve this problem, this paper proposed an algorithm
that transfers the Chance of Happening (CoH) of the pixels in the
original color image to the converted grayscale image. CoH is
deﬁned according to the Helmholtz principle in Gestalt psychological theory that “we immediately perceive whatever could not
happen by chance” and reﬂects the attentional level of pixels in the

color image. We estimate CoH from a collection of natural images
and develop locally sliced inverse regression (LSIR) to transfer
CoHs of pixels in the color image (and visual attention of the color
image accordingly) to the converted grayscale image. Thorough
empirical studies over a wide range of color images demonstrate
the effectiveness for attention preservation of the new algorithm
in converting grayscale images from color images in comparison
with popular algorithms.
In the future, we will pay more efforts on the application of the
proposed method in object tracking [23], image segmentation
[24], image classiﬁcation [25], etc.

Acknowledgments
This work was supported in part by National Natural Science
Foundation of China (61170142), National Key Technology R&D
Program (2011BAG05B04),the Program of International S&T Cooperation under Grant 2013DFG12840, and the Fundamental Research
Funds for the Central Universities.
References
[1] R. Brown, Photoshop tips: converting color to black-and-white. 〈 http://www.
russellbrown.com/tips_tech.html〉, 2002.
[2] G. Wyszecki, W.S. Stiles, Color Science: Concepts and Methods, Quantitative
Data and Formulae, Wiley Interscience, 2000.
[3] H.R. Wu, K.R. Rao, Digital Video Image Quality and Perceptual Coding, CRC
Press, 2001.
[4] W. James, The Principle of Psychology, Holt, New York, 1890.
[5] C. Archambeau, N. Delannay, M. Verleysen, Mixtures of robust probabilistic
principal component analyzers, Neurocomputing 71 (7–9) (2008) 1274–1282.
[6] S. Mika, B. Schölkopf, A. Smola, K.-R. Müller, M. Scholz, G. Räsche, Kernel PCA
and de-noising in feature spaces, in: Advances in Neural Information Processing Systems, vol. 11, MIT Press, 1999, pp. 536–542.
[7] R. Bala, K. Braun, Color-to-grayscale conversion to maintain discriminability,
in: Proceedings of the SPIE, 2004, pp. 196–202.
[8] R. Bala, R. Eschbach, Spatial color-to-grayscale transformation preserving
chrominance edge information, in: Proceedings of the IS&T/SID 12th Color
Imaging Conference, 2006, pp. 82–86.
[9] D. Socolinsky, L. Wolff, Multispectral image visualization through ﬁrst-order
fusion, IEEE Trans. Image Process. 11 (8) (2002) 923–931.

M. Song et al. / Neurocomputing 119 (2013) 222–231

[10] K. Rasche, R. Geist, J. Westall, Detail preserving reproduction of colour images
for monochromats and dischromats, IEEE Comput. Graph. Appl. 25 (3) (2005)
22–30.
[11] K. Rashce, R. Geist, J. Westall, Re-colouring images for gamuts of lower
dimension, Comput. Graph. Forum 24 (3) (2005) 423–432.
[12] S. Flori, Visualization of Riemannian-manifold-valued elements by multidimensional scaling, Neurocomputing 74 (6) (2011) 983–992.
[13] A. Gooch, J. Tumblin, B. Gooch, Color2Gray: salience-preserving color removal,
ACM Trans. Graph. 24 (3) (2005) 634–639.
[14] K. Smith, P.-E. Landes, J. Thollot, K. Myszkowski, Apparent grayscale: a simple
and fast conversion to perceptually accurate images and video, Comput.
Graph. Forum 27 (2) (2008) 193–200.
[15] M. Song, D. Tao, C. Chen, X. Li, C.-W. Chen, Color to gray: visual cue
preservation, IEEE Trans. Pattern Anal. Mach. Intell. 32 (9) (2010) 1537–1552.
[16] A. Desolneux, L. Moisan, J. Morel, From Gestalt Theory to Image Analysis:
A Probabilistic Approach, Springer Science and Business Media, Heidelberg,
2008.
[17] G. Zhai, Q. Chen, X. Yang, W. Zhang, Scalable visual sensitivity proﬁle
estimation, in: Proceedings of the ICASSP, 2008, pp. 876–879.
[18] P. Viola, M. Jones, Rapid object detection using a boosted cascade of simple
features, in: IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 2001, pp. 511–518.
[19] N.D.B. Bruce, J.K. Tsotsos, Saliency based on information maximization, Adv.
Neural Inform. Process. Syst. 18 (2006) 155–162.
[20] K.-C. Li, Sliced inverse regression for dimension reduction, J. Am. Stat. Assoc.
86 (414) (1991) 316–327.
[21] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd ed., Academic
Press, 1990.
[22] D. Tao, X. Li, X. Wu, S.J. Maybank, Geometric mean for subspace selection, IEEE
Trans. Pattern Anal. Mach. Intell. 31 (2) (2009) 260–274.
[23] X. Cao, R. Lin, P. Yan, X. Li, Visual attention accelerated vehicle detection in
low-altitude airborne video of urban environment, IEEE. Trans. Circ. Syst.
Video Technol. 22 (3) (2012) 366–378.
[24] Y. Cao, Y. Yuan, X. Li, B. Turkbey, P.L. Choyke, Segmenting images by combining
selected atlases on manifold, in: Medical Image Computing and ComputerAssisted Intervention, 2011, pp. 272–279.
[25] Y. Luo, D. Tao, B. Geng, C. Xu, S.J. Maybank, Manifold regularized multi-task
learning for semi-supervised multi-label image classiﬁcation, IEEE Trans.
Image Process. 22 (2) (2013) 523–536.
[26] X. Wang, Z. Li, D. Tao, Subspaces indexing model on Grassmann manifold for
image search, IEEE Trans. Image Process. 20 (9) (2011) 2627–2635.
[27] X. Wang, Z. Li, L. Zhang, J. Yuan, Grassmann hashing for approximate nearest
neighbor search in high dimensional space, in: IEEE International Conference
on Multimedia and Expo, 2011, pp. 1–6.
[28] T. Zhou, D. Tao, Shrinking sparse dimension reduction, IEEE Trans. Image
Process. 22 (1) (2013) 244–257.

Mingli Song received the Ph.D. degree incomputer
science from Zhejiang University, China, in 2006. He
is currently an associate professor in the College of
Computer Science and Microsoft Visual Perception
Laboratory, Zhejiang university. His research interests
include visual perception analysis, image enhancement,
and face modeling. He is a Senior Member of the IEEE.

231
Dapeng Tao received the B.S. degree in Electronics and
Information Engineering from Northwestern Polytechnical University, Xi'an, China. He is currently a Ph.D.
candidate in Information and Communication Engineering at South China University of Technology,
Guangzhou, China. His research interests include machine learning, computer vision and cloud computing.

Chun Chen is a professor in the College of Computer
Science at Zhejiang University, China. His research
interests include computer vision, computer graphics,
and embedded technology.

Jiajun Bu received the B.S and Ph.D. degrees in Computer Science from Zhejiang University, China, in 1995
and 2000, respectively. He is a professor in College of
Computer Science and the Director of Embedded System and Software Center at Zhejiang University. His
research interests include video coding in embedded
system, data mining, and mobile database.

Yezhou Yangis a Ph.D. student at Department of Computer Science at University of Maryland, College Park.
He received the B.S degree in Computer Science from
Zhejiang University, China, in 2010. His research interests include computer vision and robot vision.

arXiv:1609.03619v2 [cs.CV] 24 Oct 2016

1

Reliable Attribute-Based Object Recognition
Using High Predictive Value Classifiers
Wentao Luan1 , Yezhou Yang2 , Cornelia Fermüller2 , John S. Baras1
1

Institute for Systems Research, University of Maryland, College Park, USA
2
Computer Vision Lab, University of Maryland, College Park, USA
wluan@umd.edu, yzyang@cs.umd.edu, fer@umiacs.umd.edu, baras@umd.edu

Abstract. We consider the problem of object recognition in 3D using
an ensemble of attribute-based classifiers. We propose two new concepts
to improve classification in practical situations, and show their implementation in an approach implemented for recognition from point-cloud
data. First, the viewing conditions can have a strong influence on classification performance. We study the impact of the distance between the
camera and the object and propose an approach to fusing multiple attribute classifiers, which incorporates distance into the decision making.
Second, lack of representative training samples often makes it difficult
to learn the optimal threshold value for best positive and negative detection rate. We address this issue, by setting in our attribute classifiers
instead of just one threshold value, two threshold values to distinguish
a positive, a negative and an uncertainty class, and we prove the theoretical correctness of this approach. Empirical studies demonstrate the
effectiveness and feasibility of the proposed concepts.

Introduction

Reliable object recognition from 3D data is a fundamental task for active agents
and a prerequisite for many cognitive robotic applications, such as assistive
robotics or smart manufacturing. The viewing conditions, such as the distance of
the sensor to the object, the illumination, and the viewing angle, have a strong
influence on the accuracy of estimating simple as well as complex features, and
thus on the accuracy of the classifiers. A common approach to tackle the problem of robust recognition is to employ attribute based classifiers, and combine
the individual attribute estimates by fusing their information [1],[2],[3].
This work introduces two concepts to robustify the recognition by addressing
common issues in the processing of 3D data, namely the problem of classifier
dependence on viewing conditions, and the problem of insufficient training data.
We first study the influence of distance between the camera and the object on
the performance of attribute classifiers. Unlike 2D image processing techniques,
which usually scale the image to address the impact of distance, depth based
object recognition procedures using input from 3D cameras tend to be affected
by distance-dependent noise, and this effect cannot easily be overcome [4].

2

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

We propose an approach that addresses effects of distance on object recognition. It considers the response of individual attribute classifiers’ depending on
distance and incorporates it into the decision making. Though, the main factor studied here is distance, our mathematical approach is general, and can be
applied to handle other factors affected by viewing conditions, such as lighting,
viewing angle, motion blur, etc.
To implement the attribute classifiers, usually, the standard threshold method
is used to determine the boundary between positive and negative examples. Using this threshold the existence of binary attributes is determined, which in turn
controls the overall attribute space. However, there may not be enough training samples to accurately represent the underlying distributions, which makes it
more difficult to learn one good classification threshold that minimizes the number of incorrect predictions (or maximizes the number of correct predictions).
Here we present an alternative approach which applies two thresholds with
one aiming for a positive predictive value (PPV), giving high precision for positive classes, and the other aiming for a negative predictive value (NPV), giving
high precision for negative classes. Each classifier can then have three types of
output: “positive” when above the high PPV threshold, “negative” when below
the high NPV threshold and “uncertain” when falling into the interval between
the two thresholds. Recognition decisions, when fusing the classifiers, are then
made based on the positive and negative results. More observations thereby are
needed for drawing a conclusion, but we consider this trade-off affordable, since
we assume that our active agent can control the number of observations. Note
that two threshold approaches have previously been used for the purpose of
achieving results of high confidence, for example in [5], and in probability ratio
tests.
The underlying intuition here is that it should be easier to obtain the high
PPV and NPV thresholds than the classical Bayes threshold (minimizing the
classification error), when the number of training samples is too small to represent well the underlying distribution. Fig. 1a illustrates the intuition. The top
figure shows the ground truth distributions (of the classification score) of the
positive and negative class. The lower figure depicts the estimated distributions
from training samples, which are biased due to an insufficient amount of data.
Furthermore, as our experiment revealed, even the ground truth distribution
could be dependent on viewing conditions, which makes it more challenging to
learn a single optimal threshold. In such a case, the system may end up with
an inaccurate Bayes threshold. However, it is still possible to select high PPV
(NPV) thresholds by setting these thresholds (at a safe distance) away from the
negative (positive) distribution.
For each basic (attribute) classifier, we can also define a reliable working
region indicating a fair separation of the distributions of positive and negative
classes. Hence our approach can actively select “safe” samples and discard “unsafe” ones in unreliable regions. We prove the asymptotic correctness of this
approach in section 3.3.

Reliable Attribute-Based Object Recognition

(a)

3

(b)

Fig. 1. (a): Illustration of common conditional probability density functions of the
positive and negative class. Top: ground truth distribution of the two classes; bottom:
a possible distribution represented by the training data. Blue line: positive class; red
line: negative class. dashed line: (estimated) Bayes threshold; solid line: high PPV and
NPV thresholds. (b): The relationship of Objects (O), attributes (Fi ), environmental
variables (Ek ) and observations (Zik ) in our model.

Integrating both concepts, our complete approach to 3D object recognition
works as follows: Offline we learn attribute classifiers, which are distance dependent. In practice, we discretize the space into n distance intervals, and for each
interval we learn classifiers with two thresholds. Also, we decide for each attribute classifier a reliable range of distance intervals. During the online process
our active system takes RGBD images as it moves around the space. For each
input image, it first decides the distance interval in order to use the classifiers
tuned to that interval. Classifier measurements from multiple images are then
combined via maximum a posteriori probability (MAP) estimation.
Our work has three main contributions: 1) We put forward a practical framework for fusing component classifiers’ results by taking into account the distance,
to accomplish reliable object recognition. 2) We prove our fusion framework’s
asymptotic correctness under certain assumptions on the attribute classifier and
sufficient randomness of the input data. 3) The benefits of introducing simple
attributes, which are more robust to viewing conditions, but less discriminative,
are demonstrated in the experiment.

2

Related Work

Creating practical object recognition systems that can work reliably under different viewing conditions, including varying distance, viewing angle, illumination
and occlusions, is still a challenging problem in Computer Vision. Current single source based recognition methods have robustness to some extent: features
like SIFT [6] or the multifractal spectrum vector (MFS) [7] in practice are invariant to a certain degree to deformations of the scene and viewpoint changes;
geometric-based matching algorithms like BOR3D [8] and LINEMOD [9] can
recognize objects under large changes in illumination, where color based algo-

4

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

rithms tend to fail. But in complicated working environments, these systems
have difficulties to achieve robust performance.
One way to deal with variations in viewing conditions is to incorporate different sources of information (or cues) into the recognition process [16]. However,
how to fuse the information from multiple sources, is still an open problem.
Early fusion methods have tried to build more descriptive features by combining features from sources like texture, color and depth before classification.
For example, Asako et al. builds voxelized shape and color histogram descriptors [1] and classifies objects using SVM, while in [10] information from color,
depth, SIFT and shape distributions is described by histograms and objects are
recognized using K-Nearest Neighbors.
Besides early fusion, late fusion also has gained much attention and achieves
good results. Lutz at al. [3] proposes a probabilistic fusion approach, called
MOPED [11], to combine a 3D model matcher, color histograms and feature
based detection algorithm, where a quality factor, representing each method’s
discriminative capability, is integrated in the final classification score. Meta information [12] can also be added to create a new feature. Ziang et al. [2] blends
classification scores from SIFT, shape, and color models with meta features providing information about each model’s fitness from the input scene, which results
in high precision and recall on the Challenge and Willow datasets. Considering
influences due to viewing conditions, Ahmed [13] applies an AND/OR graph
representation of different features and updates a Bayes conditional probability
table based on measurements of the environment, such as intensity, distance and
occlusions. However, these methods may suffer from inaccurate estimation of the
conditional probabilities involved, because of insufficient training data.
In our work, we propose a framework for object recognition using multiple
attribute classifiers, which considers both, effects due to viewing conditions and
effects due to biased training data that systems face in practice. We implement
our approach for an active agent that takes advantage of multiple inputs at
various distances.

3

Assumptions and Formulation

Before going into the details and introducing the notation, let us summarize
this section. Section 3.1 defines the data fusion of the different classification
results through MAP estimation. Section 3.2 proves that MAP estimation will
classify correctly under certain requirements and assumptions. The requirements
are restrictions on the values of the PPV and NPV. The assumptions are that
our attribute classifiers perform correctly in the following sense: A ground truth
positive value should be classified as positive or uncertain and a ground truth
negative value should be classified as negative or uncertain. Finally section 3.3
proves asymptotic correctness of MAP estimation. The estimation will converge,
even if the classifiers don’t perform correctly, under stronger requirements on the
values of the PPV and NPV.

Reliable Attribute-Based Object Recognition

5

Let the objects in the database be described by the set O = {oj } (j =
1, 2, ..., |O|). Each object oj ∈ O is represented by a attribute vector F j =
[f1j , f2j , ..., fM j ]T , where M is the number of attributes. For the i-th attribute Fi ,
there is a corresponding component classifier to identify it. Denote its observation
as Zik , where i is the index for the classifier and k is the observation number. Here
we consider binary attributes fij ∈ Range(Fi ) = {0, 1}, ∀i ∈ {1, 2, ..., M }, and
there are three possible values for the observation : Zik = {0, 1, u} k ∈ 1, 2, , , , K,
where u represents uncertainty for the case that the classification score falls in
the interval between the high PPV and NPV thresholds.
The model also encodes effects due to viewing conditions (or environmental
factors). In this work, we study the effect of distance. Thus, E is the distance
between the object and the camera. However, in future work, other environmental factors can be encoded as additional components. Fig. 1b illustrates the
relationship between objects, attributes, environmental factors and observations
in a graphical model.
In our notation EK = {E 1 , E 2 , ..., E K } represents the environmental vari1
2
K
able at each observation, and ZK
i = {Zi , Zi , ..., Zi } is the set of observation
results from the i-th classifier. We assume that an observation of an attribute Zik
only depends on the ground truth attribute variable Fi and the environmental
variable E k . Because we assume that each object oj can be
 representedj by an
1 if F = F ,
j
M -dimension attribute vector F , we have P (F |O = oj ) =
0 o.w.

3.1

Inference

K
With K observation results ZK = {ZK
1 , ..., ZM } and corresponding environmenK
tal conditions E , we want to obtain the posterior probability of the target
object being object oj ∈ O. i.e. P (O = oj |ZK , EK ). Based on our graphical
model we have:

P (O = oj |ZK , EK ) =
=

P (O = oj , ZK , EK )
P (O = oj )P (ZK |F = F j , EK )P (EK )
=
K
K
P (Z , E )
P (ZK , EK )
K M
P (EK )P (O = oj ) Y Y
P (Zik |Fi = fij , E k )
P (ZK , EK )
i=1
k=1

= λP (O = oj )

K Y
M
Y
k=1

P (Fi = fij |Zik , E k )
P (Fi = fij )
i=1
(1)

where λ ,

Q
QM
P (EK ) K
P (Z k ,E k )
k=1
QK i=1
QM i
.
K
K
P (Z ,E ) k=1 i=1 P (E k )

P (Fi = fij ) =

X
t

Because

P (O = ot )P (Fi = fij |O = ot ) =

X
{t|fit =fij }

P (O = ot )

(2)

6

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

Finally, we have
P (O = oj |ZK , EK ) = λP (O = oj )

K Y
M
Y
k=1

P (F = fij |Zik , E k )
P i
.
{t|fit =fij } P (O = ot )
i=1

(3)

The recognition A then is derived using MAP estimation as:
A , argmax P (O = oj |ZK , EK ).

(4)

oj

In our framework, we use the high positive and negative predictive value observations (Z = 0, 1) to determine the posterior probability.
We also take into account the influence of environmental factors. That is,
only observations from a reliable working region are adopted in the probability calculation. When the environmental factor is distance, the reliable working
region is defined as a range of depth values where the attribute classifier work
reasonably well. We treat a range of distance values as a reliable working region for a classifier, if the detection rate for this range is larger than a certain
threshold, and the PPV meets the system requirement.
This requirement for the component classifiers is achievable if the positive
conditional probability density function of the classification score has a nonoverlapping area with the negative one. Then we can tune the classifier’s PPV
threshold towards the positive direction (towards left in Fig. 1a) to achieve a
high precision with a guarantee of minimum detection rate.
Formally speaking, our P (Fi = fij |Zik , E k ) is defined as:
 +
 pi
1 − p−
P (Fi = 1|Zik , E k ) = P
i


if ek ∈ Ri & zik = 1,
if ek ∈ Ri & zik = 0,
o.w.
t|fit =fij P (O = ot )

(5)

where Ri is the set of environmental values for which the i-th classifier can
achieve a PPV p+
i with a detection rate lower bound. As before, k denotes the
k-th observation. If the above condition is not met, either the recognition is
done in an unreliable region or the answer is uncertain. Now equation (3) can
be rewritten as:
P (O = oj |ZK , EK ) = λP (O = oj )

K Y
Y
k=1 i∈Ik

P (F = fij |Zik , E k )
P i
,
{t|fit =fij } P (O = ot )

(6)

where Ik = Ik+ ∪ Ik− is the index set of recognized attributes at the k-th observation with Ik+ = {i|ek ∈ Ri & zik = 1} and Ik− = {i|ek ∈ Ri & zik = 0}.
Intuitively, it means that we only use a component classifier’s recognition
result when 1) it works in its reliable range; 2) the result satisfies high PPV or
NPV thresholds. In Section 3.2, we introduce the predictive value requirements
for the component classifiers.

Reliable Attribute-Based Object Recognition

3.2

7

System Requirement for the Predictive Value

Here we put forward a predictive value requirement for each component classifier
to have correct MAP estimations assuming there do not exist false positives or
false negatives from observations.
To simplify our notation, we define the prior probability of object πj ,
P (O = oj ), j = (1, 2, ..., No ) and the prior probability of attribute Fi being
P
positive as wi , {t|fit=1} πt , (i = 1, 2, ..., M ). For each attribute, the following
max{t|fit =0} πt
min{t|fit =1} πt ),

ratios are calculated: ri+ , max(1,

ri− , max(1,

max{t|fit =1} πt
min{t|fit =0} πt ).
j

−
I+
Fj and IFj are the index sets of positive and negative attributes in F , and
the reliably recognized attributes’ indexes at the k-th observation are denoted
as I = {I1 , I2 , ..., IK } (Ik as defined in section 3.1). We next state the conditions
for correct MAP estimation.
S
Theorem 1. S
If the currentlySrecognized attributes Sk Ik can uniquelySidentify
object oj , i.e. k Ik+ ⊆ IF + , k Ik− ⊆ IF − , ∀t 6= j, k Ik+ * IF + or k Ik− *
j

t

j

IF − , and if ∀i ∈ {1, 2, ..., M } the classifiers’ predictive values satisfy p+
i ≥
t

ri+ wi
1+(ri+ −1)wi

and p−
i ≥

ri− (1−wi )
,
wi +ri− (1−wi )

then the MAP estimation result A = {oj }.

This requirement means that if 1) the attributes can differentiate an object
from others, and 2) the component classifiers’ predictive values satisfy the requirement, then for the correct observation input, the system is guaranteed to
have a correct recognition result.
Proof. Based on (6) and the definition above, the posterior probability of oj is,
K  Y
Y
Y p− 
p+
i
i
P (O = oj |Z , E ) = λπj
.
wi k− 1 − wi
k+
K

K

k=1

i∈I

(7)

i∈I

S k
Because the current observed
k I can uniquely identify oj , we
S attributes
will have ∀og ∈ O/{oj }, ∃Ig ⊆ k Ik and Ig =
6 ∅, s.t. ∀i ∈ Ig , fgi = 0 if i ∈ Ik+
or fgi = 1 if i ∈ Ik− . Thus, ∀og ∈ O/{oj },
K

K

P (O = og |Z , E ) = λπg

K 
Y
k=1

Y
i∈Ik− /Ig

p−
i

i∈Ik−

T

Ig

1−
wi

p+

1−p+

p−

1−p−

p−

p+
i
wi

Y
i∈Ik+

T

Ig

1 − p+
i
1 − wi
(8)

.

max{t|fit =0} πt
ri+ wi
and ri+ = max(1, min{t|f
π ),
1+(ri+ −1)wi
it =1} t
+
p+
1−p
i
i
wi ≥ 1 ≥ 1−wi . For similar reasons, we have
−
1−p
1 ≥ wii . Also since Ig 6= ∅, we can have (7) >

Since for each classifier, p+
i ≥
we have πj wii ≥ πg 1−wij and

i∈Ik+ /Ig


p−
i

Y

1 − wi

Y

i
i
πj 1−w
≥ πg wji and 1−w
≥
i
i
(8), an thus the conclusion is reached.

8

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

From the proof, we can extend the result to a more general case: if the
currently recognized attributes cannot uniquely determine
an object,
S
S i.e. there
exists a non-empty set O0 = {oj |oj ∈ O, IF + ⊇ k Ik+ & IF − ⊇ k Ik− }, the
j
j
final recognition result A = argmax πj . Furthermore, if an equal prior probability
oj ∈O0

is assumed, then A = O0 .
Theorem 1 proves the system’s correctness under correct observations. Next,
for the general case section 3.3 proves that MAP estimation asymptotically converges to the actual result under certain assumptions.
3.3

Asymptotic Correctness of the MAP Estimation

Now we are going to prove that MAP estimation will converge to the correct
result when 1) the attribute classifiers’ PPV and NPV are high enough in their
reliable working region, where a lower bound of detection rate exists, and 2) the
inputs are sampled randomly.
Denote di as the detection rate and qi as the false-positive rate of the i-th
attribute classifier when applying the high PPV threshold in its reliable working
region. Similarly, for the high NPV threshold, si denotes the true negative rate
and vi denotes the false negative rate.
Theorem 2. We assume that the inputs are sampled sufficiently random such
that each attribute classifier gets the same chance to work in its reliable region
where a lower bound exists for its detection rate, 0 < A < di ≤ 1 and all the
objects have different positive attributes, i.e. ∀i, j, i 6= j s.t. IF + * IF + . If
i

j

−
the component classifiers’ predictive values p+
i and pi are high enough, MAP
estimation will converge to the correct result asymptotically with an increasing
number of observations.

Proof. Consider the worst case, where only two candidates O = {o1 , o2 } exist.
Without loss of generality, assume o1 has positive attributes IF + = {1, 2, ..., M1 }
1
and o2 has all the remaining positive attribute IF + = {M1+1 , M1+2 , ..., M },
2
where M1 ≥ 1. Also assume o1 is the ground truth object. In this case all the
false-positive and false-negatives will drive the estimation toward o2 .
Based on (6), the posterior probability distributions of o1 and o2 can be
written as:
P (O = o1 |ZK , EK ) = λπ1

M1
Y
p+ + 1 − p−
i n−
) i
( i )ni (
wi
wi
i=1

M
Y

(

i=M1 +1

−
1 − p+
p−
i n+
) i ( i )ni
1 − wi
1 − wi

(9)
M1
Y
−
1 − p+
p−
i n+
P (O = o2 |ZK , EK ) = λπ2
(
) i ( i )ni
1 − wi
1 − wi
i=1

M
Y
i=M1 +1

(

p+
i n+
i
wi

)

(

p−
i n−
i

1−
wi

)

,

(10)
−
where n+
i and ni are the number of positive and negative recognition results of
the i-th attribute. Denote n as the number of times the i-th classifier works in its

Reliable Attribute-Based Object Recognition

9

di
reliable region Ei . Based on the central limit theorem, we have P (n+
i >nα)=1
and P (n−
i < nαvi ) = 1 for i = 1, 2, ..., M1 when n goes to infinity and α can be
any positive constant larger than 1.
For the same reason, we have P (n+
i < nαqi ) = 1 for i = M1 + 1, ..., M
when n goes to infinity. We use the same n here assuming same likelihood of
reliable working regions for each classifier. Actually it does not matter if there is
a constant positive factor on n, which means that the chances for the classifiers’
reliably working region may be proportional.
Dividing (9) by (10), we obtain:

QM1
−
(1−p−
p+
n+
i )/(wi ) ni
i /wi
i (
)
−
i=1 ( (1−p+ )/(1−wi ) )
P (O = o1 |ZK , EK )
π1
p
/(1−w
)
i
i
i
=
−
+ (1−p− )/wi
p+
P (O = o2 |ZK , EK )
π2 QM
i
i /wi
)ni
)ni (
(
p−
i /(1−wi )

i=M1 +1 (1−p+ i)/(1−wi )

π1
≥
π2

QM1

i=1 (

p+
i /wi
(1−p+
i )/(1−wi )

)

d
n αi

(

(1−p−
i )/(wi )
p−
i /(1−wi )

)nαvi

p+
i /wi
nαqi
i=M1 +1 ( (1−p+ i)/(1−wi ) )

QM

−
(p+
i , pi larger than the threshold in theorem 1)
d1
p+
i
α
i=1 ( 1−p+ )
i

QM1


= c1 c2

1−p−
( p−i )αvi
i

n

p+
i
αqi
i=M1 +1 ( 1−p+ )

QM

i


≥ c1 c2

A 1−p− α
p+
i
i
α
i=1 ( 1−p+ ) ( p− )
i
i

QM1

QM

p+

i
i=M1 +1 ( 1−p+ )

α

−
1−p
i
wi

n

+
(1−p )
i
1−wi

i

(for the upper bound of qi and vi see (Eq. 12))
(11)
p
p 1−p
Because lim 1−p
= ∞ and lim ( 1−p
)
= 1, the division will be larger than
p→1

p→1

1 when the predictive value of each classifier is high enough, which means the
MAP estimation will yield o1 asymptotically.
The proof of upper bound of qi and vi :
1 − p+
P (Zi = 1)(1 − p+
i )
i
≤
1 − wi
1 − wi
1 − p−
P (Zi = 0)(1 − p−
i )
i
≤
vi = P (Zi = 0|Fi = 1) =
wi
wi
qi = P (Zi = 1|Fi = 0) =

(12)
(13)

Beyond providing theoretical background, in the next section we perform
experiments on a real object recognition task to first demonstrate the influence
of the environment, and then to validate our framework’s performance.

4

Experiments

In this section, we demonstrate our framework on the task of recognizing objects
on a table top. We first build a pipeline to collect our own data1 . The reason
1

The dataset is available from http://ece.umd.edu/~wluan/ECCV2016.html

10

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

for collecting our own data is that other available RGBD datasets [14],[15] focus
on different aspects, usually pose or multiview recognition, and do not contain
a sufficient amount of samples from varying observation distances.
Three experiments are conducted to show 1) the necessity of incorporating
environmental factors (the recognition distance in our case) for object recognition; 2) the performance of the high predictive value threshold classifier in
comparison to the single threshold one; and 3) the benefits of incorporating less
discriminative attributes for extending the working range of classifiers.
4.1

Experimental Settings

The preprocessing pipeline is illustrated in Fig. 2a. After a point cloud is grabbed
from a 3D camera such as Kinect or Xtion PRO LIVE, we first apply a passthrough
filter to remove points that are too close or too far away from the camera. Then
the table surface is located by matching the point could to a 3D plane model
using random sample consensus (RANSAC), and only points above the table are
kept. Finally, on the remaining points, Euclidean clustering is employed to generate object candidates, and point clouds with less than 600 points are discarded.

(a)

(b)

Fig. 2. (a) Illustration of the preprocessing pipeline. Left: input; Middle: point cloud
after passthrough filtering; Right: segmented candidate object and removed table surface. (b) The objects we use in the task and their IDs.

For the segmented point clouds, three categories of classifiers are applied,
which are tuned to attributes of fine shape, coarse shape, and color.
Fine shape is recognized by the Viewpoint Feature Histogram (VFH) descriptor, which encodes a point cloud into a 308 dimensional vector. Radu [17]
provides a pipeline of computing VFH features and retrieving the minimum feature distance matching by fast approximate K-Nearest Neighbors, implemented
in the Fast Library for Approximate Nearest Neighbors (FLANN) [18]. However, this approach tends to generate false positives when matching different
point clouds with very different distances to the camera. Thus, we adapt the
original recognition pipeline to a two step matching. We first pick up model
point clouds from our database with similar distance to test input point cloud.
Among the nearby template point clouds, we use the minimum VFH feature

Reliable Attribute-Based Object Recognition

11

matching distance as the classification score. Both steps use FLANN to accelerate neighbor retrieval, where the former step uses the Euclidean distance and
the latter the Chi-Square distance.
As another type of attribute, we use coarse shape, which is less selective than
the fine shape attribute. Our experiments later on demonstrate its advantage of
having a larger working region, thence it can help increase the system’s recognition accuracy over a broader range of distance. Two coarse shapes, cylinders
and planar surfaces, are recognized by fitting a cylindrical and a plane model,
whose coefficients are estimated by RANSAC. The percentage of outlying points
is counted as the classification score for the shape. Thus, a lower score indicates
better coarse attribute fitting in our experiment.
The last type of attribute in our system is color, which is used to augment
the system’s recognition capability. To control the influence of illumination, all
samples are collected under one stable lighting condition. The color histogram
is calculated on point clouds after Euclidean clustering, where few background
or irrelevant pixels are involved. The Hue and Saturation channels of color are
discretized into 30 bins (5 × 6), which works well for differentiating the major
colors.
As shown in Fig. 2b, there are 9 candidate objects in our dataset. To recognize
them, we use 5 fine shape attributes: shape of cup, bottle, gable top carton, wide
mouse bottle, and box; 2 coarse shape attributes: cylinder and plane surface; 3
major colors: red, blue and yellow. The attributes for all objects are listed in
Table 1. In the following experiments, we fix the pose of objects, and set the
recognition distance as the only changing factor.

Object
ID
1
2
3
4
5
6
7
8
9

gable
plane
top
cylinder
surface
carton
shape
X
X
X
X
X
X
X
X
X
X
X
X
-

box
shape
X
-

wide
mouth
bottle
shape
X
-

cup
shape

bottle
shape

red
color

blue
color

yellow
color

X
-

X
X
X

X
X
X
-

X
X
X

X
X
-

Table 1. Object IDs and their list of attributes

4.2

Experimental Results

EXPERIMENT ONE: The first experiment is designed to validate our claim
that the classifiers’ response score distributions are indeed distance variant.
Therefore, it is necessary to integrate distance in a robust recognition system.

12

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

Taking the fine shape classifier of bottle shapes as example, we divide the
distance range between 60 cm and 140 cm into 4 equally separated intervals
and collect positive samples (object id 7, 8, 9) and negative samples from the
remaining 9 objects in each distance interval. The number of positive samples in
each interval is 120 with 40 objects from each positive instance, while the number
of negative samples is 210 with 35 from each instance. The distribution of the
bottle classifier’s response score is approximated by Gaussian kernel density
estimation with a standard deviation of 3, and plotted in Fig. 3.

Recognition distance from 60 to 80cm

0.07

positive class
negative class

0.06

Probability density

Probability density

0.06

Recognition distance from 80 to 100cm

0.07

positive class
negative class

0.05
0.04
0.03
0.02
0.01

0.05
0.04
0.03
0.02
0.01

0

0
0

50

100

150

200

0

50

Response score

(a)

positive class
negative class

0.06

Probability density

Probability density

200

Recognition distance from 120 to 140cm

0.07

positive class
negative class

0.06

150

(b)

Recognition distance from 100 to 120cm

0.07

100

Response score

0.05
0.04
0.03
0.02
0.01

0.05
0.04
0.03
0.02
0.01

0

0
0

50

100

Response score

(c)

150

200

0

50

100

150

200

Response score

(d)

Fig. 3. Estimated distribution of bottle shape classifier’s response score under 4 recognition distance intervals.

We observe that the output score distribution depends on the recognition
distance interval. Therefore, relying on one single classification threshold across
all the distance intervals would introduce additional error. More importantly, we
observe that with a larger distance, the area of overlap between the positive and
negative distributions, becomes wider, which makes classification more difficult.
EXPERIMENT TWO: Experiment one demonstrated the difficulty of
learning a distance-variant ground truth distribution and corresponding classification thresholds. Therefore, we propose to use two high predicative value thresholds when multiple inputs are available. The second experiment is designed to
validate this idea by comparing the classification accuracy of an estimator that
1) uses two high predicative value thresholds, with an estimator that uses 2) one
optimal Bayes threshold minimizing the error in the training data.

Reliable Attribute-Based Object Recognition

13

To have a fair comparison, we set our task as recognizing 5 objects (id
1, 4, 5, 6, 9 ) with 5 fine shape attributes such that each object contains one
positive attribute that uniquely identifies it. Both training and testing point
clouds are collected at a distance of 100 cm to 120 cm . To learn the classification threshold, we sample 26 point clouds for each object and uniformly select
20 for training. The testing data for each object consists of 22 point clouds that
we can randomly choose from to simulate the scenario of an active moving observer gathering multiple inputs. Here we want to mention a special case. When
our framework is uncertain based on the current input, it randomly select (with
equal probability) one of the possible objects. The classification accuracy between using a single threshold and using two high predicative value thresholds
are shown in Fig. 4a respectively.
We can see that both methods’ error rates decrease when the number of observations increases. The approach using two thresholds (the red line) has lower
error rate than the one using a single threshold (the blue line). The green line
shows the error introduced by random selection, when our framework cannot
make a sole decision. The major part of the error in the two thresholds method
is due to this error. It is worth mentioning that under theoretical conditions, the
classical Bayes single threshold should still be the best in minimizing the classification error. Our method provides an alternative for cases when the training
data in real world scenarios does not represent well the underlying distribution.

1

0.45

Single optimal threshold
Two high predictive value thresholds
Two thresholds w.o. certain decision

0.4
0.35

0.8

Accuracy

Error rate

0.3
0.25
0.2

0.6

0.4

0.15
0.1

Fine shape, color
Coarse shape, color
Fine, coarse shape, color

0.2

0.05

0

0
0

2

4

6

8

10

Number of observation

(a)

12

14

16

60-80

80-100 100-120 120-140 140-160
Distance interval (cm)

(b)

Fig. 4. (a): Error rate using classification with a single threshold (blue) and two high
predictive value thresholds (red). The green line depicts the error component due to
the cases where the two thresholds method has to randomly select. (b) Three systems’
recognition accuracy for different working distance intervals.

EXPERIMENT THREE: The third experiment demonstrates the benefits of using less discriminative attributes for extending the system’s working
range. To recognize the 9 objects in Fig. 2b, we build three recognition systems
utilizing attributes of fine shape and color, coarse shape and color, and all of the
three attributes, respectively. Considering the influence of the recognition distance on the response score distribution, the complete range of distances from
60 cm to 160 cm is split into 5 equal intervals. We then learn the classifica-

14

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

tion thresholds and predictive values accordingly. Both, the training and the
testing data, consist of around 100 samples from each object across recognition
distances from 60 cm to 160 cm. We learn the PPV and NPV by counting the
training data w.r.t. the thresholds and select thresholds satisfying a predictive
value larger than 0.96. The minimum detection rate for the reliable working
distance interval is 0.09. This means if 1) an attribute classifier cannot find a
threshold with PPV larger than 0.96, and 2) detection rate larger than 0.09 in
a certain distance interval, the output of this attribute classier in this interval
will not be adopted for decision making. During the testing phase, for fair comparison, we constrain the number of input point clouds collected from the same
distance interval in each working region. Around 120 point clouds are collected
for each object. Once more, random selection is applied when multiple objects
are found as possible candidates.
Fig. 4b displays the systems’ recognition accuracy after observing three times
in each distance interval. As expected, the classification performance starts to
decrease for larger distances. At 120 cm to 160 cm, the system using fine shape
attributes (blue) performs even worse than the system using less selective coarse
attributes (green). This validates that the coarse shape based classifier has a
larger working region, though its simple mechanisms allows for less discrimination than the fine grain attribute based classifier. Finally, due to the complementary properties, the system using all attributes (yellow) achieves the best
performance at each working region.

5

Conclusions

In this work we put forward a practical framework for using multiple attributes
for object recognition, which incorporates recognition distance into the decision making. Considering the difficulties of finding a single best classification
threshold and the availability of multiple inputs at testing time, we propose to
learn a high PPV and a high NPV threshold and discard uncertain values during
decision making. The framework’s correctness was proven and a fundamental experiment was conducted to demonstrate our approach’s feasibility and benefits.
Additionally, we showed that less selective shape attributes (compared to the
sophisticated ones) can have advantages, because their simple mechanism can
lead to high reliability when the system is working at a large range of distances.
In future work, we plan to extend the approach to a variety of environmental
factors such as lighting conditions, blur, and occlusions. Furthemore, additional
attribute classifiers will be incorporated to improve the system’s recognition
performance.
Acknowledgment: This work was funded by the support of DARPA
(through ARO) grant W911NF1410384, by NSF through grants CNS-1544787
and SMA-1540917 and Samsung under the GRO program (N020477, 355022).

Reliable Attribute-Based Object Recognition

15

References
1. Kanezaki, A., Marton, Z.C., Pangercic, D., Harada, T., Kuniyoshi, Y., Beetz, M.:
Voxelized Shape and Color Histograms for RGB-D. In: IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), Workshop on Active Semantic Perception and Object Search in the Real World, San Francisco, CA, USA
(September, 25–30 2011)
2. Xie, Z., Singh, A., Uang, J., Narayan, K.S., Abbeel, P.: Multimodal blending
for high-accuracy instance recognition. In: Proceedings of the 26th IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). (2013)
3. Lutz, M., Stampfer, D., Schlegel, C.: Probabilistic object recognition and pose
estimation by fusing multiple algorithms. In: Robotics and Automation (ICRA),
2013 IEEE International Conference on. (May 2013) 4244–4249
4. Salih, Y., Malik, A.S., Walter, N., Sidibé, D., Saad, N., Meriaudeau, F.: Noise
robustness analysis of point cloud descriptors. In: 15th International Conference
on Advanced Concepts for Intelligent Vision Systems - Volume 8192. ACIVS 2013,
New York, NY, USA, Springer-Verlag New York, Inc. (2013) 68–79
5. Wu, T., Zhu, S.C.: Learning near-optimal cost-sensitive decision policy for object
detection. In: IEEE International Conference of Computer Vision. (2013)
6. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision 60 (2004) 91–110
7. Xu, Y., Ji, H., Fermüller, C.: A projective invariant for textures. In: Computer
Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on.
Volume 2. (2006) 1932–1939
8. Bertsche, M., Fromm, T., Ertel, W.: Bor3d: A use-case-oriented software framework for 3-d object recognition. In: Technologies for Practical Robot Applications
(TePRA), 2012 IEEE International Conference on. (April 2012) 67–72
9. Hinterstoisser, S., Lepetit, V., Ilic, S., Holzer, S., Bradski, G.R., Konolige, K.,
Navab, N.: Model based training, detection and pose estimation of texture-less 3d
objects in heavily cluttered scenes. In: Computer Vision - ACCV 2012 - 11th Asian
Conference on Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised
Selected Papers, Part I. (2012) 548–562
10. Attamimi, M., Mizutani, A., Nakamura, T., Nagai, T., Funakoshi, K., Nakano, M.:
Real-time 3d visual sensor for robust object recognition. In: Intelligent Robots
and Systems (IROS), 2010 IEEE/RSJ International Conference on. (Oct 2010)
4560–4565
11. Collet Romea, A., Martinez Torres, M., Srinivasa, S.: The MOPED framework:
Object recognition and pose estimation for manipulation. International Journal of
Robotics Research 30(10) (September 2011) 1284 – 1306
12. Fromm, T., Staehle, B., Ertel, W.: Robust multi-algorithm object recognition using
machine learning methods. In: Multisensor Fusion and Integration for Intelligent
Systems (MFI), 2012 IEEE Conference on. (Sept 2012) 490–497
13. Naguib, A., Lee, S.: Adaptive bayesian recognition with multiple evidences. In:
Multimedia Computing and Systems (ICMCS), 2014 International Conference on.
(April 2014) 337–344
14. Lai, K., Bo, L., Ren, X., Fox, D.: A large-scale hierarchical multi-view rgb-d
object dataset. In: Robotics and Automation (ICRA), 2011 IEEE International
Conference on. (May 2011) 1817–1824
15. Singh, A., Sha, J., Narayan, K.S., Achim, T., Abbeel, P.: Bigbird: A large-scale 3d
database of object instances. In: Robotics and Automation (ICRA), 2014 IEEE
International Conference on. (May 2014) 509–516

16

W. Luan, Y. Yang, C. Fermüller, J. S. Baras

16. Liu, X., Baras, J.S.: Trust-aware crowdsourcing with domain knowledge. In: 2015
54th IEEE Conference on Decision and Control (CDC). (Dec 2015) 2913–2918
17. Rusu, R.B., Bradski, G., Thibaux, R., Hsu, J.: Fast 3d recognition and pose using
the viewpoint feature histogram. In: Proceedings of the 23rd IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Taipei, Taiwan
(10/2010 2010)
18. Muja, M., Lowe, D.G.: Scalable nearest neighbor algorithms for high dimensional
data. Pattern Analysis and Machine Intelligence, IEEE Transactions on 36 (2014)

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Manipulation Action Tree Bank: A Knowledge Resource for Humanoids
Yezhou Yang1 , Anupam Guha1 , Cornelia Fermüller2 , and Yiannis Aloimonos1

In the field of computational linguistics, parsed text
corpora with annotations presented as compositional tree
structures, also called tree banks [2] have proven to be the
most effective way to organize syntactic and even semantic
structures of natural languages. The typical way of building a
tree bank involves: 1) first humans annotate each sentence in
the corpus extensively, 2) then parse them into grammar trees
(or other more sophisticated structures). These annotated tree
banks can serve as learning bases for automatic sentence part
of speech (POS) tagging and bottom up parsing models.
Later these models can be used to automatically tag and
parse more natural language resources in order to bootstrap
the whole tree bank. Prediction and reasoning based on
tree banks has been extensively studied in the field of
computational linguistics.
Similarly, in this paper, we propose that tree banks are an
effective and practical way to organize semantic structures
of manipulation actions for robotics. They could be used as
basis for 1) automatic manipulation action understanding and
execution and 2) doing reasoning and prediction during both
observation and execution, as illustrated in Fig. 1.
The rest of the paper is organized as follows: In Sec. II
a brief survey of related work is provided. In Sec. III we
will briefly introduce the manipulation action context-free
grammar that is used to parse semantic tree structures in
this work. In Sec. IV the technical details for building the
manipulation action tree banks are presented. In Sec. V two
sets of publicly available manipulation action datasets are
used to build different levels of tree banks, three experiments
are conducted, and one example of using the knowledge
resource is given. Sec. VI describes conclusion and future
works. The tree banks can be downloaded from http:
//www.umiacs.umd.edu/˜yzyang/MA_Tree_Bank/.

Abstract— Our premise is that actions of manipulation are
represented at multiple levels of abstraction. At the high
level a grammatical structure represents symbolic information
(objects, actions, tools, body parts) and their interaction in a
temporal sequence, and at lower levels the symbolic quantities
are grounded in perception. In this paper we create symbolic
high-level representations in the form of manipulation action
tree banks, which are parsed from annotated action corpora.
A context free grammar provides the grammatical description
for the creation of the semantic trees. Experiments conducted
on the tree banks show that they allow to 1) generate so-called
visual semantic graphs (VSGs), 2) compare the semantic distance between steps of activities and 3) discover the underlying
semantic space of an activity. We believe that tree banks are
an effective and practical way to organize semantic structures
of manipulation actions for humanoids applications. They
could be used as basis for 1) automatic manipulation action
understanding and execution and 2) reasoning and prediction
during both observation and execution. The knowledge resource
follows the widely used Penn Tree Bank format.

I. Introduction
Autonomous humanoid robots need to have the capability
to understand, learn and execute actions of manipulation.
These actions involve objects and tools, are composed of
primitive sub-actions, and are performed using the robots’
effectors. Recent studies on humans have shown that grammatical structures underlie our representation of manipulation actions, which are used both to understand and to
execute these actions. Understanding manipulation actions
is like understanding language, while executing them is like
generating language.
If a household robot is innately wired with the capability
of understanding and generating language, it should be able
to apply a similar mechanism to understand and execute
manipulation actions. It should be able to learn how to do an
action from either observing a human doing it (learn from
observation) or from parsing human instructions (learn from
language). However, given that there is effectively an infinite
number of ways to do a certain manipulation action, such as
for example, making a peanut butter and jelly sandwich [1],
the robot should be able to store and organize all the semantic
structures effectively, rather than simply an enumeration of
all possible ways. Moreover, assuming that the semantic
structure has been stored properly in the robot memory,
further problems of doing the prediction and reasoning over
them effectively also need to be solved.

II. Related Work
Robotic manipulation has attracted a great amount of interest due to its direct applications in intelligent manufacturing.
With the recent development of advanced robotic manipulators, work on robust and accurate manipulation techniques
has followed quickly. For example, [3] developed a method
for the PR2 to fold towels, which is based on visual processes
of grasp point detection and multiple-view geometry. [4]
and [5] developed for their humanoid robot, ARMAR-III,
manipulation and perception capabilities based on imitation
learning for human environments. [6] proposed learning
object affordances models in multi-object manipulation tasks.
[7] investigated robots searching for objects using reasoning
about both perception and manipulation. A good survey on
humanoid dual arm manipulation can be found in [8]. These

1 Y. Yang, A. Guha, and Y. Aloimonos, are from the Department of
Computer Science, and 2 C. Fermüller is from UMIACS, University of Maryland, College Park, MD 20742, USA {yzyang, aguha, yiannis} at

cs.umd.edu and fer at umiacs.umd.edu
978-1-4799-7174-9/14/$31.00 ©2014 IEEE

987

works achieved promising results on robotic manipulation,
but they focused on specific actions, and do not allow for
generalization. Here we propose that manipulation action tree
banks, by capturing the syntactic structure of manipulation
actions, provide a general solution to organize the underlying
semantics behind specific actions and allow the robot to do
more sophisticated tasks like reasoning and prediction.

used by the robot to infer actions from natural language
commands. Attempts to use natural language tree like structures is less common but present in works like [20] where
parsing of natural language instructions plays an important
part in generating tree like plans for the robot. The logical
extension of these works is to make tree banks of the
actions themselves and use them to reason about actions in a
language like manner. Also, in [21] a probabilistic first-order
knowledge base was built as action-specific knowledge for
robots acting in household environments. In this paper we
further focus on manipulation actions, and propose to build
tree banks for manipulation actions to serve as a hierarchical
knowledge base for autonomous humanoids to effectively
perform reasoning and prediction over the learned semantic
structures. We did experiments on two sets of datasets at
different levels of granularity.
III. A Grammatical Representation of Manipulation
Actions
In [16] a context-free grammar was proposed for robots
to understand human manipulation actions. Every complex
activity is built from smaller blocks, where a block is a
“Subject”, “Action” and “Patient” triplet. Here, a “Subject”
can be either a hand or an object, and the same holds for the
“Patient”. Furthermore, a complex activity also has a basic
recursive structure, and can be decomposed into simpler
actions. For example, the typical manipulation activity “sawing a plank” is described by the top level triplet “handsaw
saw plank”, and has two lower level triplets (which in time
proceed the top level action), namely “hand grasp saw”
and “hand grasp plank”. The following Manipulation Action
Context-Free Grammar (MACFG) (Table. I) was proposed
to parse manipulation actions, and in this work we follow
the same grammatical rules.

Fig. 1: A robotic system using a manipulation tree bank.
Human activity recognition and understanding has been
studied heavily in Computer Vision recently and there is
a large range of applications for this work in areas like
human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual
description methods using motion capture systems have been
used. Surveys of the former can be found in [9] and [10].
There has been some work on the way instructions are
stored and analyzed for manipulation actions, generally as
sequences. Work done by [11] among others investigates how
these sequence datasets can be processed in order to reason
about manipulation actions. Sequence alignment borrows
techniques from informatics. Our paper deals with grammar
trees, a more detailed representation of manipulation actions.
Chomsky in [12] suggested that a minimalist generative
grammar, similar to the one of human language, also exists
for action understanding and execution. The works closest
related to this paper are [13], [14], [15], [16]. [13] first
discussed a Chomskyan grammar for understanding complex
actions as a theoretical concept, [14] provided an implementation of such a grammar using as perceptual input only objects, and Guha et al. [15] suggested a set of atomic symbols
for describing movement.[16] proposed a set of context-free
grammar rules for manipulation action understanding, as well
as a set of operations to parse the subject, action, object
triplets into grammatical tree structures. Since the tree banks
are created using the methods from [16], we will briefly
introduce the grammar and parsing rules in Sec. III.
In the field of computational linguistics, there have been
attempts to train robots using natural language [17] by
identifying robot primitives from natural language utterances.
The most explored area has been robot navigation using
natural language instructions in works like [18], [19] where
semi-structured environment, grounded knowledge etc. are

AP
HP
H
A
O

→
→
→
→
→

A O | A HP
H AP | HP AP
h
a
o

(1)
(2)
(3)

TABLE I: Manipulation Action Context-Free Grammar.
The nonterminals H, A, and O represent hand, action and
object (tools and objects under manipulation), respectively,
and the terminals, h, a and o are the observations. AP stands
for Action Phrase and HP for Hand Phrase.
The design of this grammar is motivated by these observations: First, the main and only driving force in manipulation
actions are the hands. Thus, a specialized nonterminal symbol H is used for their representation. Second, an Action
(A) can be applied to an Object (O) directly or to a Hand
Phrase (HP), which in turn contains an Object (O). This is
encoded in Rule (1), which builds up an Action Phrase (AP).
Finally, an Action Phrase (AP) can be combined either with
the Hand (H), or a Hand Phrase. This is encoded in rule
(2), which recursively builds up the Hand Phrase. The rules
above form the syntactic rules of the grammar used in the
parsing algorithms.
988

To automatically parse the key semantic triplets (subject,
action, patient) into tree structures, two operations, CONSTRUCTION() and DESTRUCTION() were proposed also.
For more details, please refer to [16].

2) TACoS Cooking Dataset: The Saarbrücken Corpus
of Textually Annotated Cooking Scenes (short: TACoS)
contains a set of video descriptions (in natural language)
and timestamp-based alignment with the videos. The videos
along with a low-level activity annotation is available as part
of the corpus. It contains 127 kitchen sequences containing
41 different activities ranging from “cut a cucumber” to
“cook carrot soup”. 60 different action labels are used for
annotation (e.g. PEEL, STIR, TRASH). Also a location tag
is accompanied with each (subject, action, patient) triplet
to further modify actions. A straightforward extension of
the grammar (Table. I) by adding in new rules (Table. III)
with a nonterminal “ADV” (adverb), is used to accommodate
the extra information (From location A To location B,
or At location A). Also, since there is no annotation of
destructive actions, we adapt the parsing schema without the
DESTRUCTIVE() routine.

IV. Manipulation Action Tree Bank
The concept of a “tree bank” is widely used in the field
of computational linguistics, such as the Penn Treebank [2].
Different from natural language tree banks, where the basic
unit is a parsed grammar tree for each tagged sentence, each
tree for manipulation actions, represents the current status
of the action. Since every primitive action in a complex
manipulation activity comes in a temporal order, for each
sequence, a list of trees are created. For example, Fig. 2
shows a typical set of trees that represent the action “Cutting
a Tomato and Put Tomato into Bowl”.
Thus for each manipulation action S n , with n ∈ (1...N) in
the dataset Dm , a list of trees T n is created. T n = {t1n , t2n ...tkn }
where k is the number of key semantic triplets involved in
the activity. In Sec. V, two tree banks at different levels of
semantic granularity are created from two publicly available
and annotated manipulation action datasets.

A
ADV

→
→

A ADV
From/At T o

(1)
(2)

TABLE III: Extra MACFG rules for location information.
Final grammatical trees built from the first sequence in
the TACoS dataset are illustrated in Fig. 3. Note that all the
non-terminals are emitted to save space.

V. Datasets and Experiments

B. From Manipulation Action Trees to VSGs
Manipulation action trees can be used to generate Visual
Semantic Graphs (VSGs). A visual semantic graph (VSG),
proposed in the work of [24], provides a plausible formalism
for semantic activity representation. This formalism takes as
input computed object segments, their spatial relationship,
and temporal relationship over consecutive frames.
Every frame is described by a Visual Semantic Graph
(VSG), which is represented by an undirected graph
G(V, E, P). The vertex set |V| represents the set of semantically meaningful segments, the edge set |E| represents the
spatial relations between any of the two segments. Two
segments are connected when they share parts of their
borders, or when one of the segments is contained in the
other. If two nodes v1 , v2 ∈ V are connected, E(v1 , v2 ) = 1,
otherwise, E(v1 , v2 ) = 0.
The VSG can be generated from the manipulation action
tree sequences by following two rules: 1) replace constructive
actions and destructive actions with connected lines and
disconnected lines; 2) keep the connected lines if the action
is a MERGE action, such as PLACE or PUT. Fig. 2 shows
corresponding manipulation action trees with their VSGs at
each key frame.

A. Datasets
Cooking actions are a typical group of manipulation
actions, and we use such actions here as testing-bed for both
action understanding and execution. To test manipulation
action tree banks, we use two state-of-the-art fully annotated
cooking datasets, namely the 50 Salad Dataset [22] and the
TACoS Cooking Dataset [23]. Though they are both cooking
datasets, each of them has its own focused set of actions,
which makes them a great complement to each other. A brief
introduction for both datasets is given below. Table. II lists
the technical details for each of them.
1) 50 Salad Dataset: The 50 Salad dataset captures 25
people preparing 2 mixed salads each and contains over four
hours of annotated accelerometer and RGB-D video data.
Including detailed annotations, multiple sensor types, and
two sequences per participant, the 50 Salads dataset can
be used for research in areas such as activity recognition,
activity spotting, sequence analysis, progress tracking, sensor
fusion, transfer learning, and user-adaptation.
The 50 Salad dataset has its specific focus on salad
making, capturing the different variations of one typical
manipulation action. Before parsing the (subject, action,
patient) annotations into trees, we first automatically extend
the original annotations with context sub-actions like “hand
grasp knife” and “hand ungrasp knife”. Since the annotations
provided from the 50 salad dataset comes with “pre” and
“post” tags, it can be directly used to indicate constructive
and destructive actions required by the parsing algorithm.
Examples of grammatical trees built from sequences of this
dataset are illustrated in Fig. 2.

C. Tree Edit Distance Analysis
Manipulation action trees can be used to compare the
semantic distance between different steps of the activity. In
order to compare the trees, we use a measure of tree edit
distance. The edit distance between two trees is the minimalcost sequence of edit operations on labeled nodes that
transforms one tree into the other. The following operations
are possible, each with its own cost:
989

#S eq
54
127

Dataset
50 S alad
T ACoS

#Events
3820
5530

Ave. #Events/S eq
71
44

#Activities
1 (salad making)
41

#Actions
17
60

#People
25
22

Ave. Length
10703 f rames
4.5 min

TABLE II: Technical details of the 50 Salad and TACoS datasets.
DUMMY.0
DUMMY.0

knife

DUMMY.0
HP.75

AP.79
hand

AP.76
A.77

HP.78

bowl
HP.75

bowl tomato1

A.80 tomato1
cut1

grasp1

hand

AP.76
A.77

hand

AP.76

knife

A.77

grasp1

tomato1

HP.75

bowl

knife

DUMMY.0

grasp1

bowl tomato1 hand knife

DUMMY.0
DUMMY.0

AP.82
A.83
grasp2

HP.84

knife
HP.81

bowl knife

tomato1

DUMMY.0
HP.81

AP.85
hand

A.86
place1

bowl

AP.82
A.83

hand

AP.82

tomato1

A.83

grasp2

bowl

HP.81

knife

grasp2

hand

tomato1

DUMMY.0
knife bowl hand tomato1

Fig. 2: Example grammatical trees and visual semantic graphs (VSG). The sample sequence is from the 50 Salad dataset.

Fig. 3: Examples of grammatical trees from the TACoS dataset. The original annotation is given in the table.
inserting a node (between a node and its children)
deleting a node (and connecting its children to its parent)
• renaming a node (changing the label)
Efficient algorithms exist for finding the minimal edit
distance. The algorithm used in this paper is from the original
paper by Zhang and Shasha [25]. For the small sizes of
trees we encounter, comparing two trees takes negligible
time and memory. However, because the number of trees are
considerable large, a fast algorithm is desirable. For each
pair of manipulation actions (S n , S m ) n, m ∈ (1...N) in the 50
Salad dataset, two lists of trees T n and T m are parsed in the
tree bank. T n = {t1n , t2n ...tkn }, T m = {t1m , t2m ...tlm } where k, l are
the number of key semantic triplets involved in S n and S m
respectively. A tree edit distance matrix D(n,m) with a size
of k × l is computed by finding the minimal edit distance
between each tree pair T in and T nj , i ∈ (1...k), j ∈ (1...l),

 n m
D(t1 , t1 ) D(t1n , t2m ) · · · D(t1n , tlm )
D(t2n , t1m ) D(t2n , t2m ) · · · D(t2n , t1m )
 .
D(n,m) = 
(1)

...
...
...



 n m

D(tk , t1 ) D(tkn , t2m ) · · · D(tkn , tlm )

Fig. 4 shows four typical pairs of distance matrices from
the 50 salad tree bank. The X and Y axis indicate the
temporal order of each sequence respectively.

•
•

D. Action Tree Embedding Analysis
The tree bank of manipulation actions can be used to discover the underlying semantic space of the activity. In order
to show that the tree bank generated from the manipulation
action grammar captures the underlying semantic space of
the manipulation action, we apply multidimensional scaling
(MDS) on the distance matrix D (which was computed using
the minimal edit distance from Sec. V-C.) Specifically, D
is constructed by concatenating the D(n,m) for each pair of
n, m ∈ (1...N),


 D1,1 D1,2 · · · D1,N 
 D

 2,1 D2,2 · · · D2,N 
D =  .
(2)
..
..  .
 ..
.
. 

DN,1 DN,2 · · · DN,N
The goal of MDS is, given D, to find I vectors x1 , . . . , xI ∈
RN such that kxi − x j k ≈ Di, j , for all i, j ∈ I, where k · k is a
990

(a)
(a)

(c)

(b)

(c)

(b)

(d)

(e)

(f)

(g)

(d)

Fig. 4: 50 Salad Dataset, Sequence 3 trail 2. (a) Self distance
(b) With Sequence 22 Trail 1, a similar way of making
a salad (c) With Sequence 24 Trail 1, making salad in a
different order; (d) With Sequence 25 Trail 2, large variation,
a total different way of making salad.

Fig. 5: Manipulation action tree embedding analysis and
semantic space discovery.

vector norm, and I is the total number of action trees in the
tree bank. In this work we simply applied the most widely
used classical MDS, using the Euclidean distance as norm.
After applying MDS on D from the 50 Salad tree bank, the
embedded space is plotted in Fig. 5. Each dot in the figure is
a manipulation action tree in the tree bank, and trees from the
same manipulation action share the same color. The top three
dimensions are used since they best capture the underlying
semantic space for the 50 Salad tree bank.
Fig. 5a shows a top-down view of the space. The data
points form a ring. This is desirable, because each salad
making activity in the dataset starts from an empty tree
(no object or tool is used) and ends with an empty tree
(both objects and tools are released). Each intermediate step
involves the hand(s) using object(s) or tool(s) to apply an
action onto another object. The corresponding data points
are located on the path of the ring. Fig. 5b shows a side
view of the space. The data forms three clusters. This is also
desirable, because each intermediate step in salad making
involves a START step (no object or tool grasped), a GRASP
step (object or tool grasped) and an ACT step (action applied
onto object). In fact, from another viewpoint, Fig. 5c shows
that the underlying semantic space of salad making in the
50 Salad tree bank indeed consists of three rings.
A trace connecting dots in the space represents each salad
making scenario in the 50 salad tree bank. Fig. 5d, 5e, 5f
and 5g respectively plot the traces of four typical scenarios
discussed in Sec. V-C, which are sequence 3 trail 2, sequence
22 trail 1, sequence 24 trail 1 and sequence 25 trail 2.

such as “Tregex” and “tgrep”. These tools are widely used in
the computational linguistics community to search tree banks
using regular expressions. Thus, humanoid robots that are
equipped with the action knowledge in a tree bank fashion
have the advantage to do action-centric reasoning easily.
In fig. 6, we show an example using the “Tregex” GUI
program to reason in the 50 salad-making tree bank. After
observing a “knife” and a “tomato” on the table, the humanoid robot can initialize a search to find sub-trees that
contain both “knife” and “tomato” using regular expression.
The program returns all the sub-tree matches from the action
tree bank. Apparently, in the salad-making scenario, when
both “knife” and “tomato” are observed, “hand grasp knife
cut tomato” is the most common action to do. This action
command can be further used to drive the humanoid. A
similar search, which is to find a sub-tree contains both
“tomato” and “bowl”, returns an action command “hand
grasp tomato place (into) bowl” from the same tree bank.
Moreover, since the system is able to return not only the
matching sub-trees, but also their temporal locations in each
sequences, the robot can easily figure out that “cut tomato”
precedes “ tomato place (into) bowl” in this scenario.
In this section we describe one example of using the Penn
style tree bank to do reasoning for humanoids. More complicated searches can be conducted by cooperating “and”,
“or” or “neglect” logics in the regular expressions. A regular
expression generation engine, which can be driven by lowlevel visual systems such as object recognition and tracking,
can generate these searches automatically. Also, it is worth
pointing out that there are other more sophisticated tools to

E. A Reasoning Example using Action Tree Bank
We organize our tree banks following the Penn tree bank
format, which makes it plausible to apply off-the-shelf tools
991

[3] J. Maitin-Shepard, M. Cusumano-Towner, J. Lei, and P. Abbeel, “Cloth
grasp point detection based on multiple-view geometric cues with
application to robotic towel folding,” in Robotics and Automation
(ICRA), 2010 IEEE International Conference on. IEEE, 2010.
[4] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, “Imitation learning
of dual-arm manipulation tasks in humanoid robots,” International
Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183–202, 2008.
[5] T. Asfour, P. Azad, N. Vahrenkamp, K. Regenstein, A. Bierbaum,
K. Welke, J. Schröder, and R. Dillmann, “Toward humanoid manipulation in human-centred environments,” Robotics and Autonomous
Systems, vol. 56, pp. 54–65, 2008.
[6] B. Moldovan, P. Moreno, M. van Otterlo, J. Santos-Victor, and
L. De Raedt, “Learning relational affordance models for robots in
multi-object manipulation tasks,” in Robotics and Automation (ICRA),
2012 IEEE International Conference on. IEEE, 2012, pp. 4373–4378.
[7] M. R. Dogar, M. C. Koval, A. Tallavajhula, and S. Srinivasa, “Object
search by manipulation,” in Robotics and Automation (ICRA), 2013
IEEE International Conference on. IEEE, 2013.
[8] C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D. V.
Dimarogonas, and D. Kragic, “Dual arm manipulation: A survey,”
Robotics and Autonomous Systems, 2012.
[9] T. Moeslund, A. Hilton, and V. Krüger, “A survey of advances in
vision-based human motion capture and analysis,” Computer vision
and image understanding, vol. 104, no. 2, pp. 90–126, 2006.
[10] P. Turaga, R. Chellappa, V. Subrahmanian, and O. Udrea, “Machine
recognition of human activities: A survey,” Circuits and Systems for
Video Technology, IEEE Transactions on, vol. 18, no. 11, pp. 1473–
1488, 2008.
[11] M. Tenorth, J. Ziegltrum, and M. Beetz, “Automated alignment of
specifications of everyday manipulation tasks,” in IROS. IEEE, 2013.
[12] N. Chomsky, Lectures on government and binding: The Pisa lectures.
Walter de Gruyter, 1993, vol. 9.
[13] K. Pastra and Y. Aloimonos, “The minimalist grammar of action,”
Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 367, no. 1585, pp. 103–117, 2012.
[14] D. Summers-Stay, C. Teo, Y. Yang, C. Fermuller, and Y. Aloimonos,
“Using a minimal action grammar for activity understanding in the
real world,” in IROS, 2013.
[15] A. Guha, Y. Yang, C. Fermüller, and Y. Aloimonos, “Minimalist plans
for interpreting manipulation actions,” IROS, 2013.
[16] Y. Yang, C. Fermuller, and Y. Aloimonos, “A cognitive system for
human manipulation action understanding,” in the Second Annual
Conference on Advances in Cognitive Systems (ACS), 2013.
[17] S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and A. Klein, “Training
personal robots using natural language instruction,” Intelligent Systems, IEEE, vol. 16, no. 5, pp. 38–45, Sep 2001.
[18] M. MacMahon, B. Stankiewicz, and B. Kuipers, “Walk the talk:
Connecting language, knowledge, and action in route instructions,”
Def, vol. 2, no. 6, p. 4, 2006.
[19] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J.
Teller, and N. Roy, “Understanding natural language commands for
robotic navigation and mobile manipulation.” in AAAI, 2011.
[20] M. Stenmark and P. Nugues, “Natural language programming of industrial robots,” in Robotics (ISR), 2013 44th International Symposium
on. IEEE, 2013, pp. 1–5.
[21] D. Nyga and M. Beetz, “Everything robots always wanted to know
about housework (but were afraid to ask),” in Intelligent Robots and
Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE,
2012, pp. 243–250.
[22] S. Stein and S. J. McKenna, “Combining embedded accelerometers
with computer vision for recognizing food preparation activities,”
in Proceedings of the 2013 ACM International Joint Conference on
Pervasive and Ubiquitous Computing (UbiComp 2013). ACM, 2013.
[23] M. Regneri, M. Rohrbach, D. Wetzel, S. Thater, B. Schiele, and
M. Pinkal, “Grounding action descriptions in videos,” Transactions
of the Association for Computational Linguistics (TACL), vol. 1, pp.
25–36, 2013.
[24] E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and F. Wörgötter,
“Learning the semantics of object–action relations by observation,”
The International Journal of Robotics Research, vol. 30, no. 10, pp.
1229–1249, 2011.
[25] K. Zhang and D. Shasha, “Simple fast algorithms for the editing
distance between trees and related problems,” SIAM journal on computing, vol. 18, no. 6, pp. 1245–1262, 1989.

Fig. 6: An example of using “tregex” to achieve action
commands from the tree bank knowledge resource.
do reasoning and learning from tree banks. These techniques
can also be applied onto autonomous humanoid robots.
VI. Conclusion and Future Works
In this paper, we presented manipulation action tree banks
for robot action understanding and execution. We created the
knowledge resource using the manipulation action contextfree grammar and its associated parsing algorithms. We
conducted three experiments on the proposed tree bank.
We showed that the tree bank is able to generate visual
semantic graphs, and we compared semantic distances to
discover the underlying semantic space. Additionally, we
showed an example of querying the tree bank to do reasoning
for humanoids robot under a specific scenario.
In current work we are integrating the presented linguistic
resource with low-level vision tools on a humanoid platform.
We are developing a cognitive robot system that equipped
with the proposed tree banks deals with the uncertainty as
well as the complexity of everyday human manipulation
activities. We are also working on an analysis of the different levels of granularity of manipulation tasks in order
to possibly take shallow parses of the grammatical trees to
find a coarse similarity between different domains of every
manipulation actions.
VII. Acknowledgements
This research was funded in part by the support of
the European Union under the Cognitive Systems program
(project POETICON++), the National Science Foundation
under INSPIRE grant SMA 1248056, and support from
the US Army, Grant W911NF-14-1-0384 under the Project:
Shared Perception, Cognition and Reasoning for Autonomy.
References
[1] W. Yi and D. Ballard, “Recognizing behavior in hand-eye coordination
patterns,” International Journal of Humanoid Robotics, vol. 6, no. 03,
pp. 337–359, 2009.
[2] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a
large annotated corpus of english: The penn treebank,” COMPUTATIONAL LINGUISTICS, vol. 19, no. 2, pp. 313–330, 1993.

992

2012 IEEE International Conference on Robotics and Automation
RiverCentre, Saint Paul, Minnesota, USA
May 14-18, 2012

Towards a Watson That Sees: Language-Guided Action Recognition for
Robots
Ching L. Teo, Yezhou Yang, Hal Daumé III, Cornelia Fermüller and Yiannis Aloimonos
Abstract— For robots of the future to interact seamlessly with
humans, they must be able to reason about their surroundings
and take actions that are appropriate to the situation. Such
reasoning is only possible when the robot has knowledge of how
the World functions, which must either be learned or hardcoded. In this paper, we propose an approach that exploits
language as an important resource of high-level knowledge
that a robot can use, akin to IBM’s Watson in Jeopardy!. In
particular, we show how language can be leveraged to reduce
the ambiguity that arises from recognizing actions involving
hand-tools from video data. Starting from the premise that
tools and actions are intrinsically linked, with one explaining
the existence of the other, we trained a language model over a
large corpus of English newswire text so that we can extract
this relationship directly. This model is then used as a prior
to select the best tool and action that explains the video. We
formalize the approach in the context of 1) an unsupervised
recognition and 2) a supervised classification scenario by an EM
formulation for the former and integrating language features
for the latter. Results are validated over a new hand-tool action
dataset, and comparisons with state of the art STIP features
showed significantly improved results when language is used. In
addition, we discuss the implications of these results and how
it provides a framework for integrating language into vision on
other robotic applications.

Fig. 1. (Top) Ambiguities in action recognition: similar trajectories for
different actions. Tools considered in isolation can only suggest possible
actions. (Below) Language can predict, given the tool and action trajectories,
the most likely action label.

recognizes what actions occurred and the second component
creates an internal representation so that the action can be
recreated by the robot. Unlike the LfD challenge where the
robot is supposed to recreate the one task it was taught (the
second component), we address a variant of the first component where the robot labels the correct action associated with
a set of unlabeled action data (with repetitions) performed
by different human teachers (or actors). Posing our problem
in terms of unlabeled data over different actors is also closer
to reality as the robot needs to learn how to generalize the
action so that it can discover, in the second component, an
optimal representation to recreate this action on its own.
In this paper, we consider the task of recognizing human
activities from videos sequences – specifically, activities
that involve hand-tools. Action or activity recognition has
remained one of the most difficult problems in computer
vision. The main reason is that detections of key objects that
define the action in video – tools, objects, hands and humans
are still unreliable even using current state of the art object
detectors [10], [26]. Descriptions based on tracking trajectories of local features such as STIP [19] and modeling velocity
as suggested in [22], are strongly viewpoint dependent and
may be confused when similar movements are used for
different actions, e.g. drinking from a cup vs. peeling. Both
actions involve large up-down hand movements. The main
challenge of action recognition is the ambiguity when these
two components: objects and trajectories, are considered
in isolation. From Fig. 1(top), detecting a cup or action
trajectory in isolation can only suggest some likely actions. A
more reliable prediction can be achieved when we combine

I. I NTRODUCTION
Humans display an uncanny ability to perceive the world
that far surpasses current vision only based systems both
in terms of precision and accuracy. This is largely due to
the vast amount of high-level knowledge that humans have
acquired over their lives that enables humans to infer and
recognize complex visual inputs. In the same way, service
and personal robots of the future must be endowed with
such knowledge in order to interact and reason with humans
and their environments effectively. This knowledge can be
encoded in various forms, of which language is clearly the
most predominant. Language manifests itself in the form of
text which is also extremely accessible from various large
research corpora.
The ability to reason is an essential faculty for service
and personal robots. A typical scenario is when the robot
needs to understand an action or task which the human
performs for the purpose of learning. Such Learning from
Demonstration (LfD) [1] paradigm is gaining popularity in
robotics as shown by the recently concluded Learning by
Demonstration Challenge at AAAI 20111 . There are two
interconnected components in LfD. The first component
The authors
University of

are from Department of Computer Science,
Maryland, College Park, MD 20742, USA

{cteo,yzyang,hal,fer,yiannis}@umiacs.umd.edu
1 http://www.lfd-challenge.org/

978-1-4673-1405-3/12/$31.00 ©2012 IEEE

374

both object detection and trajectories.
A missing component in the above approach is how we
can combine object detection with actions in a logical and
intuitive way. To do this, we propose to model language as
a resource of prior knowledge that essentially encodes how
actions and objects (tools) are related. This language model
allows us to weigh the video detections so that the objects
and action features that best explain the video are eventually
selected (Fig. 1(below)).

tools, we still can derive interesting relationships to guide the
visual task of action recognition.
III. O UR A PPROACH
The input is a set of |M | videos, M = {md }, d ∈
{1, 2, · · · , |M |} containing some actions, with each video
containing exactly one unique action. The |V | actions are
drawn from the set V = {vj }, j ∈ {1, 2, · · · , |V |}. This
means that we have assumed that the task of segmenting
actions from a long video sequence has been done. In
addition, we assume that every action must have an actor
that uses a particular hand-tool. The same tool can be
used in multiple actions. The |N | tools comes from the set
N = {ni }, i ∈ {1, 2, · · · , |N |}. All labels (both actions and
tools) must be known in advance, which is a requirement for
learning the appropriate language model. A summary of the
approach is shown in Fig. 2.

II. R ELATED W ORK
Action recognition research spans a long history. Comprehensive reviews of recent state of the art can be found
in [28], [30], [21]. Most of the focus was on studying
human actions that were characterized by movement and
change of posture, such as walking, running, jumping etc.
Many studies represent actions by modeling body poses,
body joints and body parts [24]. Depending on the extent of
the features used, the literature distinguishes between local
and global action models. The former use spatio-temporal
interest points and descriptors based on intensity, gradients
and flow within spatio-temporal cuboids centered on these
interest points [19], [7]. The latter compute descriptors over
the whole video frame or an extracted human skeleton or
silhouette. For example, [9] used histograms of optical flow
and Gorelick et al. [11] and Yilmaz et al. [31] represented
human activities by 3-D space-time shapes. Another class
of approaches model the evolution of actions over time. For
example Bissacco et al. [3] used joint-angle as well as joint
trajectories from motion-capture data and features extracted
from silhouettes to represent action profiles. Chaudhry et al.
[5] employed non linear dynamical systems to model the
temporal evolution of optical flow histograms.
Our approach is more closely related to the use of language
for object detection and image annotation. With advances
on textual processing and detection, several works recently
focused on using sources of data readily available “in the
wild” to analyze static images. The seminal work of Duygulu
et al. [8] showed how nouns can provide constraints that improve image segmentation. Gupta et al. [14] (and references
herein) added prepositions to enforce spatial constraints in
recognizing objects from segmented images. Berg et al.[2]
processed news captions to discover names associated with
faces in the images, and Jie et al.[17] extended this work to
associate poses detected from images with the verbs in the
captions. Some studies also considered dynamic scenes. [6]
studied the aligning of screen plays and videos, [20] learned
and recognized simple human movement actions in movies,
and [15] studied how to automatically label videos using
a compositional model based on AND-OR-graphs that was
trained on the highly structured domain of baseball videos .
These recent works had shown that exploiting co-occurring
text information from scripts and captions aids in the visual
labeling task. Our paper takes this further by using generic
text obtained from the English Gigaword corpus [12], which
is a large corpus of English newswire text from which we
learn a language model. As we will show, using general NLP

Fig. 2. Key components of the approach.: (a) Training the language model
from a large text corpus. (b) Detected tools are queried into the language
model. (c) Language model returns prediction of action. (d) Action features
are compared and beliefs updated.

The high level overview of the approach is as follows (see
Fig. 2): 1) We first detect potential tools from the input video.
2) For each identified tool, we query a trained language
model to determine the most likely verbs (actions) associated
with the tool. 3) We then confirm the predicted action using
the action features obtained from the video to update our
confidence on the current action label of the video. This
process is repeated until our belief on the action labels is
maximized over all tools and action features. Note that our
approach is symmetric, that is, we could have started off with
action features and inquire the language model in exactly the
same way. Our choice of starting with objects is based on
the fact that object detectors are better researched and are
generally more accurate than action detectors.
For the purpose of this discussion, we shall assume the
most general case where we only have unlabeled video data.
This means that the best that we can do is to perform some
form of clustering to discover automatically what is the best
action label that describes the cluster. Intuitively, we want to
learn the “meaning” of actions by grounding them to visual
representations obtained from video data. Hence if we knew
this grounding, we can assign action labels to the videos. On
the other hand, if we know the action labels of the video data,
we can estimate this grounding. This leads naturally to an
iterative Expectation-Maximization (EM) formulation where
375

we attempt to determine the optimal grounding parameters
that will assign action labels to videos with the highest
probability.
More formally, our goal is to label each video with their
most likely action, along with the tool that is associated with
the action. That is, we want to maximize the likelihood:
L(D; A) = EP(A) [L(D|A)]
= EP(A) [logP(FM , PI (·), PL (·)|A)]

(1)

where A is the current (binary) action label assignments of
the videos (see eq. (3)). D is the data computed from the
video that consists of: 1) the language model PL (·) that
predicts an action given the detected tool (sec. III-A), 2)
the tool detection model PI (·) (sec. III-B) and 3) the action
features, FM , associated with the video (sec. III-C). We
describe how these 3 data components are computed in the
following paragraphs and detail how we optimize eq. (1)
using EM in sec. IV-A.

Fig. 3.
Enlarging the word class to contain synonyms yields more
reasonable counts: cup only connects weakly with drink. By clustering
other closely related words together, their combined counts increase the
desired association between cup and drink.

A. Language as a predictor of actions
The key component of our approach is the language model
that predicts the most likely verb (action) that is associated
with a noun (tool) trained from a large text corpus. We
view the Gigaword Corpus as a large text resource that
contains the information we need to make correct predictions
of actions given the detected tools from the video. We do
this by training a language model PL (vj , ni ) that returns
the maximum likelihood estimates of an action vj given the
tool ni . This can be done by counting the number of times
vj co-occurs with ni in a sentence:
PL (vj |ni ) =

#(vj , ni )
#(ni )

Fig. 4.

Gigaword co-occurrence matrix of tools and predicted actions.

predict the expected action easily. However, there are
many co-occurrences which we could not anticipate. For
e.g, using synonyms of cup makes it more selective to
drinking (0.17) but it is sprinkling that has the
highest prediction score (0.29). Investigating further reveals that sprinkling has some synonyms such as
drizzle moisten splash splosh which have uses
that are also close to cup. Other mis-selected tools-action are
also due to the confusion at the synonyms/hyponymns levels.
We also notice that more general actions such as picking
have a more uniform distribution across the tools, which is
expected. In the same way, the tool mat is also very general
in its use such that it displays no significant selectivity to
any of the actions. Despite this simplistic model, most of
the entries in PL make sense – and it properly reflects the
innate complexity of language. As will be shown in sec. V,
although the priors from language are weak, they are still
helpful for the task of action recognition.

(2)

As many English words share common meanings, a simple
count of the action words (verbs) defined in V is likely
to grossly underestimate the relationship between the tool
and the action it is associated with. For example, in the
Gigaword Corpus, counting the number of times drink
co-occurs with cup where the actual words are used will
not be significantly larger than pick and cup. The reason
is that cup can mean a normal drinking cup or a trophy
cup. In order to ensure that PL captures the correct sense
of the word (nouns, verbs), we use WordNet to determine
the synonyms and hyponymns of the tools and actions
considered. As illustrated in Fig. 3, extending cup to include drinking_glass, wine_glass, mug captures
the expected action drink in a larger part of the corpus.
We then recompute PL using these enlarged word classes
to capture more meaningful relationships between the cooccurring words. Fig. 4 shows the |N | × |V | co-occurrence
matrix of likelihood scores over all the tools and actions
considered in the experiments, denoted as PL (V |N ).
For most of the tool classes, the predicted actions
are correct (large values along the diagonals): for e.g.
peeler predicts peeling with high probability (0.94)
and shaker predicts sprinkling at 0.66. This shows
that for tools that have a unique use, our approach can

B. Active tool detection strategy
We pursue the following active strategy for detecting,
and subsequently recognizing, relevant tools in the video as
illustrated in Fig. 5. First, a trained person detector [10] is
used to determine the location of the human actor in the
video frame. The location of the face is also detected using
[29]. Optical flow is then computed [4] and we focus on
human regions which have the highest flow, indicating the
376

potential locations of the hands. We then apply a variant of
a CRF-based color segmentation [23] using a trained skin
color+flow model to segment the hand-like regions which
are moving. This is justified by the fact that the moving
hand is in contact with the tool that we want to identify.
In some cases, the face may be detected (since it may be
moving) but they are removed using the face detector results.
We then apply a trained Partial Least Squares (PLS) object
detector similar to [26] near the detected active hand region
that returns a detection score at each video frame. Averaging
out the detection yields PI (ni |md ), the probability that a tool
ni ∈ N exists given the video md . We denote PI (N |M ) as
the set of detection scores (essentially the likelihood) over
all tools in N and all videos in M .

Fig. 6. Detected hand trajectories. x and y coordinates are denoted as red
and blue curves respectively.

We denote FM as the set of of action features Fd over all
videos in M .
IV. U SING L ANGUAGE TO GUIDE RECOGNITION
In this section, we formalize our EM approach to learn a
joint tool-action model that assigns the most likely action
label associated with a set of unlabeled video. We first
derive from eq. (1) an expression that allows EM to estimate
the parameters of this model, followed by details of the
Expectation and Maximization steps. We then show how to
use the learned model to perform testing. Finally, we consider
the case where we have labeled data in which we formulate
a simpler supervised approach.
A. Unsupervised learning of a joint tool-action model
We first define the latent assignment variable A. To
simplify our notations, we will use subscripts to denote tools
i = ni , actions j = vj and videos d = md . For each
i ∈ N , j ∈ V , d ∈ M , Aijd indicates whether an action
j is performed using tool i during video clip d.

1 j is performed using i during d
(3)
Aijd =
0 otherwise

Fig. 5. (Best viewed in color) Overview of the tool detection strategy: (1)
Optical flow is first computed from the input video frames. (2) We train a
CRF segmentation model based on optical flow + skin color. (3) Guided
by the flow computations, we segment out hand-like regions (and removed
faces if necessary) to obtain the hand regions that are moving (the active
hand that is holding the tool). (4) The active hand region is where the tool
is localized. Using the PLS detector (5), we compute a detection score for
the presence of a tool.

and A is a 3D indicator matrix (or a 3D array) over all tools,
actions and videos. Denoting the parameters of the model as
C = {Cj } which specifies the grounding of each action j,
we seek to determine from eq. (1) the maximum likelihood
parameter:
X
L(D, A|C)
(4)
C ∗ = arg max

This active approach has two important benefits. By focusing our processing only on the relevant regions of the
video frame, we dramatically reduce the chance that the tool
detector will misfire. At the same time, by detecting the hand
locations, we obtain immediately the action trajectory, which
is used to describe the action as shown in the next section.

C

A

Where,
L(D, A|C) = log P (D, A|C)
= log P (A|D, C)P (D|C)

C. Action features
Tracking the hand regions in the video provides us with
two sets of (left and right) hand trajectories as shown in
Fig. 6. We then construct for every video a feature vector Fd
that encodes the hand trajectories. Fd encodes the frequency
and velocity components. Frequency is encoded by using the
first 4 real coefficients of the Fourier transform in both the
x and y directions, fx , fy , which gives a 16-dim vector over
both hands. Velocity is encoded by averaging the difference
in hand positions between two adjacent frames hδxi, hδyi
which gives a 4-dim vector. These features are then combined
to yield a 20-dim vector Fd .

(5)

with the data D comprised of the tool detection likelihoods
PI (N |M ), the tool-action likelihoods PL (V |N ) and action
features FM under the current model parameters C. Geometrically, we can view C as the superset of the |V | action label
centers that defines our current grounding of each action j
in the action feature space.
Using these centers, we can write the assignment given
each video d, tool i and action j, P (Aijd |D, C) as:
P(Aijd = 1|D, C) = PI (i|d)PL (j|i)P en(d|j)
377

(6)

where P en(d|j) is an exemplar-based likelihood function
defined between the associated action feature of video d, Fd
and the current model parameter for action j, Cj as:
P en(d|j) =

2
1
exp−||Fd −Cj ||
Z

According to the Karush-Kuhn-Tucker conditions, we can
solve the maximization problem by the following constraint:
X
∂F
= −2
(Wijd (Fd − Cj )) = 0
(14)
∂C

(7)

i,j,d

Thus, for each j ∈ V , we have:
P
i∈N,j∈V,d∈M Wijd Fd
Cˆj = P
i∈N,j∈V,d∈M Wijd

where Z is a normalization factor. What eq. (7) encodes is
the penalty that we score against the assignment when there
is a large mismatch between Fd and Cj , the cluster center of
action j.
Rewriting eq. (6) over all videos M , tools N and actions
V we have:
P(A = 1|D, C) = PI (N |M )PL (V |N )P en(FM |C)

We then update C = Cˆ within each iteration until convergence.
3) Testing the learned model: The learned model C ∗ can
be used to classify new videos from a held-out testing set.
Denoting the input test video as mt , we predict the most
likely action label, vt∗ by:
X

vt∗ = arg max
PI (i|mt )PL (j|i)P en(Ft |Cj∗ )
(16)

(8)

where we use the set variables to represent the full data
and assignment model parameters. In the derivation that
follows, we will simplify P(A = 1|D, C) as P(A = 1) and
P(A = 0) = 1 − P(A = 1). We detail the Expectation and
Maximization steps in the following sections.
1) Expectation step: We compute the expectation of the
latent variable A, denoted by W, according to the probability
distribution of A given our current model parameters C and
data (PI , PL , and FM ):

j∈V

B. Supervised action classification
If we have labeled video data of actions, a supervised
approach will be the most straightforward. Every video, md ,
is represented by a set of features Fd that combines Fd
(action features), PI (tool detection) and PL together in the
following manner:

(9)

According to Eq. 6, the expectation of A is:
W = P(A = 1) ∝ PI (N |M )PL (V |N )P en(FM |C)

Fd = [Fd ; PI (N |md ); PI (N |md ) × PL (V |N )]
= [Fd ; PI (N |md ); PL (V |md )]

(10)

(11)

Here, W is a |N | × |V | × |M | matrix. Note that the constant
of proportionality does not matter because it cancels out in
the Maximization step.
2) Maximization step: The maximization step seeks to
find the updated parameters Cˆ that maximize eq. (5) with
respect to P(A):
Cˆ = arg max EP(A) [log P(A|D, C)P(D|C)]

V. E XPERIMENTS

(12)

We performed a series of experiments using a new dataset
of human actions performed with hand-tools to show quantitatively how language aids in action recognition. We first
describe the dataset, and report recognition results on both
the unsupervised and supervised scenarios.

C

Where D = PI , PL , FM . EM replaces P(A) with its
expectation W. As A, PI , PL are independent of the model
parameters C, we can simplify eq. (12) to:
Cˆ = arg max P(FM |C)
C


X
= arg max −
Wijd ||Fd − Cj ||2 
C

(17)

where ; means a concatenation of the features vectors.
This yields a 20 + |N | + |V |-dim vector. What eq. (17)
means is that for every video md , we concatenate its Fd
together with PI (N |md ), the distribution over all |N | tools,
and together with the verb prediction: PL (V |md ), obtained
from the text corpus. Given labeled training videos from all
possible actions in V, we can proceed to train discriminative
classifiers (SVM, Bayes Net and Naive Bayes) to predict the
action in the testing video.

Specifically, for each i ∈ N, j ∈ V, d ∈ M :
Wijd ∝ PI (i)PL (j|i)P en(d|j)

i∈N

where Ft is the action features extracted from mt and Cj∗ is
the j action center from the learned model.

W = EP(A) [A]
= P(A = 1) × 1 + (1 − P(A = 1)) × 0
= P(A = 1)

(15)

A. The UMD Sushi-Making Dataset
The UMD Sushi-Making Dataset2 consists of 12 actions,
performed by 4 actors using 10 different kitchen tools, for
the purpose of preparing sushi. This results in 48 video
sequences each of around 1000 frames (30 seconds long).
Other well known datasets such as the KTH, Weizmann or

(13)

i,j,d

where we had replaced P(FM |C) with eq. (7) since the
relationship between FM and C is the penalty function
P en(FM |C). This enables
us to define a target maximization
P
function as F(C) = i,j,d Wijd ||Fd − Cj ||2 .

2 http://www.umiacs.umd.edu/research/POETICON/umd_
sushi/

378

Human-EVA datasets [25], [11], [27] do not involve handtools. The human-object interaction dataset by Gupta et al.
[13] has only 4 objects. The dataset by Messing et al. [22]
has only 4 actions with tool use. The CMU Kitchen Dataset
[18] has many tool interactions for 18 subjects making 5
recipes, but many of the actions are blocked from view
due to the placements of the 4 static cameras. The head
mounted camera gives a limited and shaky top-down view
which cannot be processed easily.
Our Sushi-Making dataset provides a clear view of the
action in use with the tools and it simulates an active robotic
agent observing the human actor performing the action in a
realistic environment: a kitchen, with a real task: making
sushi, that is made up of several actions that the robot
must identify. See Fig. 5 for an example. The 12 actions
are: cleaning, cutting, drinking, flipping, peeling, picking
(up), pouring, pressing, sprinkling, stirring, tossing, turning.
The tools used are: tissue, knife, cup, rolling-mat, fruitpeeler, water-pitcher, spoon, shaker, spatula, mixing-bowl.
As was discussed in sec. III-A, some of the actions such
as picking or flipping are extremely general and are
easily confused. We made this choice to ensure that the
language prediction PL is not perfect and to show that our
approach works even under noisy data.

labels and 3) Supervised classification using trained SVM,
Bayes Net and Naive Bayes classifiers. 36 videos were used
for training the joint tool-action model using EM and 12
videos were held out for testing. For the supervised part,
the same parameters as the baseline were used. We report
our unsupervised and supervised results in Figs. 8(a) and
8(b) respectively. More detailed results (confusion matrices
for each action) can be found online3 . In addition, we show
in Fig. 7 the improving recognition accuracy of the trained
model at each iteration. The evolution of the action labels
versus the ground truth is also presented. Testing on the held
out video set using the trained model yields a recognition
accuracy of 83.33%.
D. Comparison with state of art action features
We compared our approach with a bag-of-words (BoW)
representation built upon state of the art STIP [19] features
clustered using K-means with k = 50. We trained three
classifiers: SVM, Naive Bayes and Bayesian Net using the
same procedure and parameters as the baseline and we summarize the results in Table I. The BoW representation using
Feature
STIP+Bag of Words

B. Baseline: Vision-only Recognition

Action Features+Language

As a baseline, we perform experiments without the language component, that is PL in eq. (17) is not considered as
part of Fd . Two experiments: 1) clustering using K-means
and 2) supervised classification are performed.
K-means clustering results: For the case of unlabeled
videos, we performed K-means clustering with k = 12. We
used 36 videos in this experiment. As labels, we took the
majority ground truth labels from each cluster to be the
predicted labels of the cluster. We then counted the number
of videos that are correctly placed in the right cluster to
derive a measure of accuracy, which is reported in Fig. 8(a).
Supervised classification results: From the 48 videos
from the UMD Sushi-Making dataset, we used 36 videos
from 3 actors to train a degree 3 polynomial SVM classifier
for the 12 actions. We set the cost parameter to 1.0 with
a tolerance termination value at 0.001. These parameters
were chosen from a separate development set of 8 videos.
The remaining 12 videos were then used for testing. 4fold cross validation was performed and the classification
accuracy is reported. In addition, we trained a Naive Bayes
(NB) classifier and a Bayes Net (BN) classifier over the
training data. The BN is initialized as a NB with at most
1 parent. We then apply a simple estimator to estimate the
conditional probability tables. All classifiers are tested using
WEKA [16]. We summarize the results in Fig. 8(b).

Action Features+Language

Method
Naive Bayes
Bayes Net
SVM
Naive Bayes
Bayes Net
SVM
Unsupervised EM
Semi-supervised EM

Accuracy
56.25%
75%
77.08%
66.67%
85.41%
91.67%
77.78%
91.67%

TABLE I
C LASSIFICATION ACCURACY: STIP VERSUS OUR APPROACH .

STIP achieves a maximum classification rate of 77.08% with
trained SVM classifiers. Our approach which uses comparatively simpler video features: Fourier coefficients + velocity
eq. (17) outperforms the state of the art significantly. This
gain is possible due to the addition of language prediction
in the action feature.
E. Discussion: the effects of adding language
Comparing the unsupervised recognition results, K-means
clustering on the action features alone (with PL ) achieves
only 69.44% recognition rate. The clustering accuracy, with
the addition of PL and using the EM formulation described
in sec. IV-A achieves 77.78% with random initialization of
the model C. We show further that with correctly initialized
parameters from 12 labeled videos, is enough to increase
the accuracy to 91.67%, which is just as good as the SVM
classifier (which is supervised). This result shows that once
again, even with no or limited labeled data, our proposed EM
formulation is able to leverage the predictive power of PL to
find the optimal action and its corresponding tool that best
explains the video. Fig. 9 shows some video frames with the
predicted action and corresponding tool using EM.

C. Adding Language
In this section, we performed experiments with the aid of
the language component PL . Three separate experiments are
performed: 1) Unsupervised EM, 2) Semi-Supervised EM
where we initialized the model parameters C with 12 known

3 http://www.umiacs.umd.edu/research/POETICON/umd_
sushi/res_ICRA2012

379

Fig. 7. (Best viewed in color) (Left) Unsupervised EM: accuracy at each iteration. (Right) Scatterplots of action label assignments at selected iterations.
We see that with each iteration, the assignment label clusters approaches the ground truth label (boxed in red). Note that we used PCA to reduce the action
feature dimensions to 2 for visualization.

For the classification results in Fig. 8(b) using the three
classifiers, we clearly see that the addition of PL increases
the classification accuracy, with the most dramatic increase
when SVM or Naive Bayes are used: from 87.5% to 91.67%
(SVM) and 62.5% to 66.67% when language is added.
This shows that even with a simple model, PL is able to
provide additional discriminatory features which improve the
classification. The most important result is that these features
are estimated directly from a generic text corpus and the
method is not limited to a particular domain such as cooking.
This fact alone highlights the strength of language in aiding
action classification.
Besides improving action recognition accuracy in both
supervised and unsupervised scenarios, another key observation from our results is that language is complementary in
aiding many vision related tasks where the use of high-level
knowledge is required. Previous works described in sec. II
have shown that language (in a restricted sense) can be used
to simplify ill-posed image problems like segmentation and
annotation. We showed here that the difficult problem of
recognizing actions involves high-level knowledge as well.
This is because of the strong relationship between the actions
and the tools that were used to perform these actions. Instead
of learning from a huge amount of training image data on
how tools correlate with actions, we showed that it is possible
to obtain such information directly from a text corpus. Such
a text corpus, although noisy, is much easier to obtain than
annotated image data; and we showed that with the right
EM formulation, the noisy predictions from PL provides us
appreciable gains in recognition rates on unlabeled video.

Fig. 9. Some predicted action and tools using EM. The wrong prediction (in
red and italicized) of the sprinkle action is due to a high co-occurrence
with bowl in PL (V |N ).

of using language which encodes the intrinsic relationships
between tools and actions, leveraging it to aid in the action
recognition task. As was advocated in the introduction, our
approach has important implications for robots that acquire
new skills via LfD, and we are currently evaluating the proposed approach on a mobile robotic agent over an even larger
set of actions V and tools N , and developing strategies to
detect the tools and actions better across differing viewpoints
as the robot moves. We are also investigating the utility of
tracking the hand and detecting the tools better by exploiting
depth information captured using a Kinect camera.
Our approach, however, goes beyond action recognition
and can be extended to other vision problems faced in
robotics with a more careful treatment of language. Progress
in the areas of object recognition, image segmentation and
general scene understanding have been slow as these problems require semantic grounding. Language, when exploited
properly, provides for this. For e.g. using shallow-parsing
or Named-Entity Recognition to improve PL predictions
and subsequently Fd ; or performing a dependency parse to
reduce the need to use synonyms to extract the tool and
related verb from a sentence more accurately. An important
limitation of our current approach is that we need to know
in advance the action labels and tools of the video. We are
currently working on approaches to discover, using attributes
of the potential tool and action features obtained from the
video, a prediction of the tool and action labels directly from
language. The potential world-knowledge embedded in lan-

VI. S UMMARY AND F UTURE W ORK
This paper has shown a principled approach of integrating
large scale language corpora for the purpose of action
recognition in videos involving hand-tools. We validated
our approach in both supervised and unsupervised scenarios
and out-performed the current state-of-the-art STIP+BoW
features significantly. These results demonstrate the strength
380

Fig. 8. (a) Unsupervised recognition accuracy: no language (K-Means) versus language (EM). (b) Classification accuracy: no language versus language.
All reported results have variances within ±0.5%.

guage, along with its complexities, was clearly demonstrated
with Watson in Jeopardy! which set a milestone in AI. We
believe it will do the same for vision and robotics in the near
future.

[14] A. Gupta and L. S. Davis. Beyond nouns: Exploiting prepositions and
comparative adjectives for learning visual classifiers. In D. A. Forsyth,
P. H. S. Torr, and A. Zisserman, editors, ECCV (1), volume 5302 of
Lecture Notes in Computer Science, pages 16–29. Springer, 2008.
[15] A. Gupta, A. Kembhavi, and L. S. Davis. Observing human-object
interactions: Using spatial and functional compatibility for recognition.
IEEE Trans on PAMI, 31(10):1775–1789, 2009.
[16] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten. The weka data mining software: An update. SIGKDD
Explorations, Volume 11, Issue 1, 2009.
[17] L. Jie, B. Caputo, and V. Ferrari. Who’s doing what: Joint modeling
of names and verbs for simultaneous face and pose annotation. In
NIPS, December 2009.
[18] F. D. la Torre, J. Hodgins, J. Montano, S. Valcarcel, R. Forcada, and
J. Macey. Guide to the carnegie mellon university multimodal activity
(cmu-mmac) database. Technical report, CMU-RI-TR-08-22, Robotics
Institute, Carnegie Mellon University, July 2009.
[19] I. Laptev and T. Lindeberg. Space-time interest points. In ICCV, pages
432–439. IEEE Computer Society, 2003.
[20] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning
realistic human actions from movies. In CVPR, 2008.
[21] A. P. B. Lopes, E. A. d. Valle, J. M. de Almeida, and A. A. de Arajo.
Action recognition in videos: from motion capture labs to the web.
(arXiv:1006.3506), Jun 2010. Preprint submitted to CVIU.
[22] R. Messing, C. Pal, and H. Kautz. Activity recognition using the
velocity histories of tracked keypoints. In ICCV ’09: Proceedings
of the Twelfth IEEE International Conference on Computer Vision,
Washington, DC, USA, 2009. IEEE Computer Society.
[23] C. Rother, V. Kolmogorov, and A. Blake. ‘grabcut’: interactive
foreground extraction using iterated graph cuts. ACM Trans. Graph.,
23(3):309–314, 2004.
[24] B. Sapp, A. Toshev, and B. Taskar. Cascaded models for articulated
pose estimation. In ECCV, 2010.
[25] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: A
local svm approach. In ICPR, 2004.
[26] W. Schwartz, A. Kembhavi, D. Harwood, and L. Davis. Human detection using partial least squares analysis. In International Conference
on Computer Vision, 2009.
[27] L. Sigal, A. O. Balan, and M. J. Black. Humaneva: Synchronized video
and motion capture dataset and baseline algorithm for evaluation of
articulated human motion. International Journal of Computer Vision,
87(1-2):4–27, 2010.
[28] P. K. Turaga, R. Chellappa, V. S. Subrahmanian, and O. Udrea.
Machine recognition of human activities: A survey. IEEE Trans.
Circuits Syst. Video Techn., 18(11):1473–1488, 2008.
[29] P. Viola and M. Jones. Robust real-time face detection. International
Journal of Computer Vision, 57(2):137–154, 2004.
[30] D. Weinland, R. Ronfard, and E. Boyer. A survey of visionbased methods for action representation, segmentation and recognition.
Computer Vision and Image Understanding, 115(2):224–241, 2010.
[31] A. Yilmaz and M. Shah. Actions sketch: A novel action representation.
In CVPR, pages 984–989, 2005.

VII. ACKNOWLEDGEMENTS
The support of the European Union under the Cognitive
Systems program (project POETICON) and the National Science Foundation under the Cyberphysical Systems Program
is gratefully acknowledged. Ching Teo and Yezhou Yang are
supported in part by the Qualcomm Innovation Fellowship.
R EFERENCES
[1] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of
robot learning from demonstration. Robot. Auton. Syst., 57:469–483,
May 2009.
[2] T. L. Berg, A. C. Berg, J. Edwards, and D. A. Forsyth. Who’s in the
picture? In NIPS, 2004.
[3] A. Bissacco, A. Chiuso, and S. Soatto. Classification and recognition
of dynamical models: The role of phase, independent components,
kernels and optimal transport. IEEE Trans. Pattern Anal. Mach. Intell.,
29:1958–1972, November 2007.
[4] T. Brox, C. Bregler, and J. Malik. Large displacement optical flow.
In CVPR, pages 41–48. IEEE, 2009.
[5] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal. Histograms of
oriented optical flow and binet-cauchy kernels on nonlinear dynamical
systems for the recognition of human actions. In CVPR, 2009.
[6] T. Cour, C. Jordan, E. Miltsakaki, and B. Taskar. Movie/script:
Alignment and parsing of video and text transcription. In ECCV.
2008.
[7] P. Dollár, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In VS-PETS, October 2005.
[8] P. Duygulu, K. Barnard, J. F. G. de Freitas, and D. A. Forsyth.
Object recognition as machine translation: Learning a lexicon for a
fixed image vocabulary. In A. Heyden, G. Sparr, M. Nielsen, and
P. Johansen, editors, ECCV (4), volume 2353 of Lecture Notes in
Computer Science, pages 97–112. Springer, 2002.
[9] A. Efros, A. Berg, G. Mori, and J. Malik. Recognizing action at a
distance. In ICCV, pages 726–733, 2003.
[10] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and D. Ramanan.
Object detection with discriminatively trained part-based models.
IEEE Trans. Pattern Anal. Mach. Intell., 32(9):1627–1645, 2010.
[11] L. Gorelick, M. Blank, E. Shechtman, R. Basri, and M. Irani. Actions
as space-time shapes. PAMI, 29(12):2247–2253, 2007.
[12] D. Graff. English gigaword. In Linguistic Data Consortium, Philadelphia, PA, 2003.
[13] A. Gupta and L. S. Davis. Objects in action: An approach for
combining action understanding and object perception. In CVPR. IEEE
Computer Society, 2007.

381

Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence

Robot Learning Manipulation Action Plans by
“Watching” Unconstrained Videos from the World Wide Web
Yezhou Yang

Yi Li

Cornelia Fermüller

Yiannis Aloimonos

University of Maryland
yzyang@cs.umd.edu

NICTA, Australia
yi.li@nicta.com.au

University of Maryland
fer@umiacs.umd.edu

University of Maryland
yiannis@cs.umd.edu

Abstract

Kyriazis, and Argyros 2011). The acquisition of grasp information from video (without 3D information) is still considered very difficult because of the large variation in appearance and the occlusions of the hand from objects during
manipulation.
Our premise is that actions of manipulation are represented at multiple levels of abstraction. At lower levels the
symbolic quantities are grounded in perception, and at the
high level a grammatical structure represents symbolic information (objects, grasping types, actions). With the recent
development of deep neural network approaches, our system
integrates a CNN based object recognition and a CNN based
grasping type recognition module. The latter recognizes the
subject’s grasping type directly from image patches.
The grasp type is an essential component in the characterization of manipulation actions. Just from the viewpoint
of processing videos, the grasp contains information about
the action itself, and it can be used for prediction or as a feature for recognition. It also contains information about the
beginning and end of action segments, thus it can be used to
segment videos in time. If we are to perform the action with
a robot, knowledge about how to grasp the object is necessary so the robot can arrange its effectors. For example, consider a humanoid with one parallel gripper and one vacuum
gripper. When a power grasp is desired, the robot should
select the vacuum gripper for a stable grasp, but when a precision grasp is desired, the parallel gripper is a better choice.
Thus, knowing the grasping type provides information for
the robot to plan the configuration of its effectors, or even
the type of effector to use.
In order to perform a manipulation action, the robot also
needs to learn what tool to grasp and on what object to perform the action. Our system applies CNN based recognition modules to recognize the objects and tools in the video.
Then, given the beliefs of the tool and object (from the output of the recognition), our system predicts the most likely
action using language, by mining a large corpus using a
technique similar to (Yang et al. 2011). Putting everything
together, the output from the lower level visual perception
system is in the form of (LeftHand GraspType1 Object1 Action RightHand GraspType2 Object2). We will refer to this
septet of quantities as visual sentence.
At the higher level of representation, we generate a symbolic command sequence. (Yang et al. 2014) proposed a

In order to advance action generation and creation in robots
beyond simple learned schemas we need computational tools
that allow us to automatically interpret and represent human
actions. This paper presents a system that learns manipulation action plans by processing unconstrained videos from
the World Wide Web. Its goal is to robustly generate the sequence of atomic actions of seen longer actions in video in
order to acquire knowledge for robots. The lower level of the
system consists of two convolutional neural network (CNN)
based recognition modules, one for classifying the hand grasp
type and the other for object recognition. The higher level
is a probabilistic manipulation action grammar based parsing module that aims at generating visual sentences for robot
manipulation. Experiments conducted on a publicly available unconstrained video dataset show that the system is able
to learn manipulation actions by “watching” unconstrained
videos with high accuracy.

Introduction
The ability to learn actions from human demonstrations is
one of the major challenges for the development of intelligent systems. Particularly, manipulation actions are very
challenging, as there is large variation in the way they can
be performed and there are many occlusions.
Our ultimate goal is to build a self-learning robot that is
able to enrich its knowledge about fine grained manipulation
actions by “watching” demo videos. In this work we explicitly model actions that involve different kinds of grasping,
and aim at generating a sequence of atomic commands by
processing unconstrained videos from the World Wide Web
(WWW).
The robotics community has been studying perception
and control problems of grasping for decades (Shimoga
1996). Recently, several learning based systems were reported that infer contact points or how to grasp an object from its appearance (Saxena, Driemeyer, and Ng 2008;
Lenz, Lee, and Saxena 2014). However, the desired grasping type could be different for the same target object, when
used for different action goals. Traditionally, data about the
grasp has been acquired using motion capture gloves or hand
trackers, such as the model-based tracker of (Oikonomidis,
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

3686

only objects. (Yang et al. 2014) proposed a set of contextfree grammar rules for manipulation action understanding.
However, their system used data collected in a lab environment. Here we process unconstrained data from the internet.
In order to deal with the noisy visual data, we extend the manipulation action grammar and adapt the parsing algorithm.
The recent development of deep neural networks based
approaches revolutionized visual recognition research. Different from the traditional hand-crafted features (Lowe
2004; Dalal and Triggs 2005), a multi-layer neural network
architecture efficiently captures sophisticated hierarchies describing the raw data (Bengio, Courville, and Vincent 2013),
which has shown superior performance on standard object
recognition benchmarks (Krizhevsky, Sutskever, and Hinton
2013; Ciresan, Meier, and Schmidhuber 2012) while utilizing minimal domain knowledge. The work presented in this
paper shows that with the recent developments of deep neural networks in computer vision, it is possible to learn manipulation actions from unconstrained demonstrations using
CNN based visual perception.

context-free grammar and related operations to parse manipulation actions. However, their system only processed
RGBD data from a controlled lab environment. Furthermore, they did not consider the grasping type in the grammar. This work extends (Yang et al. 2014) by modeling manipulation actions using a probabilistic variant of the context
free grammar, and explicitly modeling the grasping type.
Using as input the belief distributions from the CNN
based visual perception system, a Viterbi probabilistic parser
is used to represent actions in form of a hierarchical and
recursive tree structure. This structure innately encodes the
order of atomic actions in a sequence, and forms the basic
unit of our knowledge representation. By reverse parsing it,
our system is able to generate a sequence of atomic commands in predicate form, i.e. as Action(Subject, P atient)
plus the temporal information necessary to guide the robot.
This information can then be used to control the robot effectors (Argall et al. 2009).
Our contributions are twofold. (1) A convolutional neural
network (CNN) based method has been adopted to achieve
state-of-the-art performance in grasping type classification
and object recognition on unconstrained video data; (2) a
system for learning information about human manipulation
action has been developed that links lower level visual perception and higher level semantic structures through a probabilistic manipulation action grammar.

Our Approach
We developed a system to learn manipulation actions from
unconstrained videos. The system takes advantage of: (1)
the robustness from CNN based visual processing; (2) the
generality of an action grammar based parser. Figure1 shows
our integrated approach.

Related Works
Most work on learning from demonstrations in robotics has
been conducted in fully controlled lab environments (Aksoy
et al. 2011). Many of the approaches rely on RGBD sensors
(Summers-Stay et al. 2013), motion sensors (Guerra-Filho,
Fermüller, and Aloimonos 2005; Li et al. 2010) or specific
color markers (Lee et al. 2013). The proposed systems are
fragile in real world situations. Also, the amount of data used
for learning is usually quite small. It is extremely difficult to
learn automatically from data available on the internet, for
example from unconstrained cooking videos from Youtube.
The main reason is that the large variation in the scenery will
not allow traditional feature extraction and learning mechanism to work robustly.
At the high level, a number of studies on robotic manipulation actions have proposed ways on how instructions are stored and analyzed, often as sequences. Work
by (Tenorth, Ziegltrum, and Beetz 2013), among others,
investigates how to compare sequences in order to reason
about manipulation actions using sequence alignment methods, which borrow techniques from informatics. Our paper
proposes a more detailed representation of manipulation actions, the grammar trees, extending earlier work. Chomsky
in (Chomsky 1993) suggested that a minimalist generative
grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos 2012;
Summers-Stay et al. 2013; Guha et al. 2013; Yang et al.
2014). (Pastra and Aloimonos 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, (Summers-Stay et al. 2013) provided an implementation of such a grammar using as perceptual input

CNN based visual recognition
The system consists of two visual recognition modules, one
for classification of grasping types and the other for recognition of objects. In both modules we used convolutional neural networks as classifiers. First, we briefly summarize the
basic concepts of Convolutional Neural Networks, and then
we present our implementations.
Convolutional Neural Network (CNN) is a multilayer
learning framework, which may consist of an input layer,
a few convolutional layers and an output layer. The goal
of CNN is to learn a hierarchy of feature representations.
Response maps in each layer are convolved with a number
of filters and further down-sampled by pooling operations.
These pooling operations aggregate values in a smaller region by downsampling functions including max, min, and
average sampling. The learning in CNN is based on Stochastic Gradient Descent (SGD), which includes two main operations: Forward and BackPropagation. Please refer to (LeCun and Bengio 1998) for details.
We used a seven layer CNN (including the input layer and
two perception layers for regression output). The first convolution layer has 32 filters of size 5 × 5, the second convolution layer has 32 filters of size 5 × 5, and the third convolution layer has 64 filters of size 5 × 5, respectively. The first
perception layer has 64 regression outputs and the final perception layer has 6 regression outputs. Our system considers
6 grasping type classes.
Grasping Type Recognition A number of grasping taxonomies have been proposed in several areas of research, in-

3687

Figure 1: The integrated system reported in this work.
cluding robotics, developmental medicine, and biomechanics, each focusing on different aspects of action. In a recent
survey (Feix et al. 2013) reported 45 grasp types in the literature, of which only 33 were found valid. In this work, we use
a categorization into six grasping types. First we distinguish,
according to the most commonly used classification (based
on functionality) into power and precision grasps (Jeannerod
1984). Power grasping is used when the object needs to be
held firmly in order to apply force, such as “grasping a knife
to cut”; precision grasping is used in order to do fine grain
actions that require accuracy, such as “pinch a needle”. We
then further distinguish among the power grasps, whether
they are spherical, or otherwise (usually cylindrical), and
we distinguish the latter according to the grasping diameter, into large diameter and small diameter ones. Similarly,
we distinguish the precision grasps into large and small diameter ones. Additionally, we also consider a Rest position
(no grasping performed). Table 1 illustrates our grasp categories. We denote the list of these six grasps as G in the
remainder of the paper.
Grasping
Types

Small Diameter

Large Diameter

subtract the global mean obtained from the training data.
For each testing video with M frames, we pass the target hand patches (left hand and right hand, if present) frame
by frame, and we obtain an output of size 6 × M . We sum
it up along the temporal dimension and then normalize the
output. We use the classification for both hands to obtain
(GraspType1) for the left hand, and (GraspType2) for the
right hand. For the video of M frames the grasping type
recognition system outputs two belief distributions of size
6 × 1: PGraspT ype1 and PGraspT ype2 .
Object Recognition and Corpus Guided Action Prediction The input to the object recognition module is an RGB
image patch around the target object. We resize each patch to
32 × 32 × 3 pixels, and we subtract the global mean obtained
from the training data.
Similar to the grasping type recognition module, we also
used a seven layer CNN. The network structure is the same
as before, except that the final perception layer has 48 regression outputs. Our system considers 48 object classes,
and we denote this candidate object list as O in the rest of
the paper. Table 2 lists the object classes.

Spherical
& Rest

apple, blender, bowl, bread, brocolli, brush, butter, carrot,
chicken, chocolate, corn, creamcheese, croutons, cucumber,
cup, doughnut, egg, fish, flour, fork, hen, jelly, knife, lemon,
lettuce, meat, milk, mustard, oil, onion, pan, peanutbutter,
pepper, pitcher, plate, pot, salmon, salt, spatula, spoon,
spreader, steak, sugar, tomato, tongs, turkey, whisk, yogurt.

Power

Precision

Table 2: The list of the objects considered in our system.
For each testing video with M frames, we pass the target object patches frame by frame, and get an output of size
48×M . We sum it up along the temporal dimension and then
normalize the output. We classify two objects in the image:
(Object1) and (Object2). At the end of classification, the object recognition system outputs two belief distributions of

Table 1: The list of the grasping types.
The input to the grasping type recognition module is a
gray-scale image patch around the target hand performing
the grasping. We resize each patch to 32 × 32 pixels, and

3688

AP
HP
H
G1
G2
O1
O2
A

size 48 × 1: PObject1 and PObject2 .
We also need the ‘Action’ that was performed. Due to the
large variations in the video, the visual recognition of actions
is difficult. Our system bypasses this problem by using a
trained language model. The model predicts the most likely
verb (Action) associated with the objects (Object1, Object2).
In order to do prediction, we need a set of candidate actions
V . Here, we consider the top 10 most common actions in
cooking scenarios. They are (Cut, Pour, Transfer, Spread,
Grip, Stir, Sprinkle, Chop, Peel, Mix). The same technique,
used here, was used before on a larger set of candidate actions (Yang et al. 2011).
We compute from the Gigaword corpus (Graff 2003) the
probability of a verb occurring, given the detected nouns,
P (Action|Object1, Object2). We do this by computing the
log-likelihood ratio (Dunning 1993) of trigrams (Object1,
Action, Object2), computed from the sentence in the English
Gigaword corpus (Graff 2003). This is done by extracting
only the words in the corpus that are defined in O and V (including their synonyms). This way we obtain a reduced corpus sequence from which we obtain our target trigrams. The
log-likelihood ratios computed for all possible trigrams are
then normalized to obtain P (Action|Object1, Object2).
For each testing video, we can compute a belief distribution
over the candidate action set V of size 10 × 1 as :
PAction =

X

X

G1 O1 | G2 O2 | A O2 | A HP
H AP | HP AP
‘Lef tHand0 | ‘RightHand0
‘GraspT ype10
‘GraspT ype20
‘Object10
‘Object20
‘Action0

0.25
0.5
0.5
PGraspT ype1
PGraspT ype2
PObject1
PObject2
PAction

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

Table 3: A Probabilistic Extension of Manipulation Action
Context-Free Grammar.
Rule (1), which builds up an action phrase (AP ); (iii) an action phrase (AP ) can be combined either with the hand (H)
or a hand phrase, as encoded in rule (2), which recursively
builds up the hand phrase. The rules discussed in Table 3
form the syntactic rules of the grammar.
To make the grammar probabilistic, we first treat
each sub-rule in rules (1) and (2) equally, and assign
equal probability to each sub-rule. With regard to the
hand H in rule (3), we only consider a robot with
two effectors (arms), and assign equal probability to
‘LeftHand’ and ‘RightHand’. For the terminal rules
(4-8), we assign the normalized belief distributions
(PObject1 , PObject2 , PGraspT ype1 , PGraspT ype2 ,PAction )
obtained from the visual processes to each candidate object,
grasping type and action.

P (Action|Object1, Object2)

Object1∈O Object2∈O

× PObject1 × PObject2 .

→
→
→
→
→
→
→
→

Parsing and tree generation We use a bottom-up variation of the probabilistic context-free grammar parser that
uses dynamic programming (best-known as Viterbi parser
(Church 1988)) to find the most likely parse for an input visual sentence. The Viterbi parser parses the visual sentence
by filling in the most likely constituent table, and the parser
uses the grammar introduced in Table 3. For each testing
video, our system outputs the most likely parse tree of the
specific manipulation action. By reversely parsing the tree
structure, the robot could derive an action plan for execution. Figure 3 shows sample output trees, and Table 4 shows
the final control commands generated by reverse parsing.

(1)

From Recognitions to Action Trees
The output of our visual system are belief distributions of
the object categories, grasping types, and actions. However,
they are not sufficient for executing actions. The robot also
needs to understand the hierarchical and recursive structure
of the action. We argue that grammar trees, similar to those
used in linguistics analysis, are a good representation capturing the structure of actions. Therefore we integrate our
visual system with a manipulation action grammar based
parsing module (Yang et al. 2014). Since the output of our
visual system is probabilistic, we extend the grammar to a
probabilistic one and apply the Viterbi probabilistic parser
to select the parse tree with the highest likelihood among
the possible candidates.

Experiments
The theoretical framework we have presented suggests two
hypotheses that deserve empirical tests: (a) the CNN based
object recognition module and the grasping type recognition
module can robustly recognize input frame patches from
unconstrained videos into correct class labels; (b) the integrated system using the Viterbi parser with the probabilistic
extension of the manipulation action grammar can generate
a sequence of execution commands robustly.
To test the two hypotheses empirically, we need to define a set of performance variables and how they relate to
our predicted results. The first hypothesis relates to visual
recognition, and we can empirically test it by measuring the
precision and recall metrics by comparing the detected object and grasping type labels with the ground truth ones. The
second hypothesis relates to execution command generation,
and we can also empirically test it by comparing the generated command predicates with the ground truth ones on
testing videos. To validate our system, we conducted experi-

Manipulation Action Grammar We made two extensions from the original manipulation grammar (Yang et al.
2014): (i) Since grasping is conceptually different from other
actions, and our system employs a CNN based recognition
module to extract the model grasping type, we assign an additional nonterminal symbol G to represent the grasp. (ii) To
accommodate the probabilistic output from the processing
of unconstrained videos, we extend the manipulation action
grammar into a probabilistic one.
The design of this grammar is motivated by three observations: (i) Hands are the main driving force in manipulation actions, so a specialized nonterminal symbol H is used
for their representation; (ii) an action (A) or a grasping (G)
can be applied to an object (O) directly or to a hand phrase
(HP ), which in turn contains an object (O), as encoded in

3689

ments on an extended version of a publicly available unconstrained cooking video dataset (YouCook) (Das et al. 2013).

Dataset and experimental settings
Cooking is an activity, requiring a variety of manipulation
actions, that future service robots most likely need to learn.
We conducted our experiments on a publicly available cooking video dataset collected from the WWW and fully labeled, called the Youtube cooking dataset (YouCook) (Das
et al. 2013). The data was prepared from 88 open-source
Youtube cooking videos with unconstrained third-person
view. Frame-by-frame object annotations are provided for
49 out of the 88 videos. These features make it a good empirical testing bed for our hypotheses.
We conducted our experiments using the following protocols: (1) 12 video clips, which contain one typical kitchen
action each, are reserved for testing; (2) all other video
frames are used for training; (3) we randomly reserve 10%
of the training data as validation set for training the CNNs.
For training the grasping type, we extended the dataset by
annotating image patches containing hands in the training
videos. The image patches were converted to gray-scale and
then resized to 32×32 pixels. The training set contains 1525
image patches and was labeled with the six grasping types.
We used a GPU based CNN implementation (Jia 2013) to
train the neural network, following the structures described
above.
For training the object recognition CNN, we first extracted annotated image patches from the labeled training
videos, and then resized them to 32 × 32 × 3. We used the
same GPU based CNN implementation to train the neural
network, following the structures described above.
For localizing hands on the testing data, we first applied
the hand detector from (Mittal, Zisserman, and Torr 2011)
and picked the top two hand patch proposals (left hand and
right hand, if present). For objects, we trained general object
detectors from labeled training data using techniques from
(Cheng et al. 2014). Furthermore we associated candidate
object patches with the left or right hand, respectively depending on which had the smaller Euclidean distance.

Figure 2: Confusion matrices. Left: grasping type; Right: object.
training data, such as “Tofu”. The performance in the classification of grasping type goes up, because we sum up the
grasping types belief distributions over the frames, which
helps to smooth out wrong labels. The performance metrics
reported here empirically support our hypothesis (a).

Visual Sentence Parsing and Commands
Generation for Robots
Following the probabilistic action grammar from Table 3, we
built upon the implementation of the Viterbi parser from the
Natural Language Processing Kit (Bird, Klein, and Loper
2009) to generate the single most likely parse tree from the
probabilistic visual sentence input. Figure 3 shows the sample visual processing outputs and final parse trees obtained
using our integrated system. Table 4 lists the commands
generated by our system on the reserved 12 testing videos,
shown together with the ground truth commands. The overall percentage of correct commands is 68%. Note, that we
considered a command predicate wrong, if any of the object, grasping type or action was recognized incorrectly. The
performance metrics reported here, empirically support our
hypothesis (b).

Discussion
The performance metrics reported in the experiment section empirically support our hypotheses that: (1) our system
is able to robustly extract visual sentences with high accuracy; (2) our system can learn atomic action commands with
few errors compared to the ground-truth commands. We believe this preliminary integrated system raises hope towards
a fully intelligent robot for manipulation tasks that can automatically enrich its own knowledge resource by “watching”
recordings from the World Wide Web.

Grasping Type and Object Recognition
On the reserved 10% validation data, the grasping type
recognition module achieved an average precision of 77%
and an average recall of 76%. On the reserved 10% validation data, the object recognition module achieved an average
precision of 93%, and an average recall of 93%. Figure 2
shows the confusion matrices for grasping type and object
recognition, respectively. From the figure we can see the robustness of the recognition.
The performance of the object and grasping type recognition modules is also reflected in the commands that our
system generated from the testing videos. We observed an
overall recognition accuracy of 79% on objects, of 91% on
grasping types and of 83% on predicted actions (see Table
4). It is worth mentioning that in the generated commands
the performance in the recognition of object drops, because
some of the objects in the testing sequences do not have

Conclusion and Future Work
In this paper we presented an approach to learn manipulation
action plans from unconstrained videos for cognitive robots.
Two convolutional neural network based recognition modules (for grasping type and objects respectively), as well as
a language model for action prediction, compose the lower
level of the approach. The probabilistic manipulation action
grammar based Viterbi parsing module is at the higher level,
and its goal is to generate atomic commands in predicate
form. We conducted experiments on a cooking dataset which
consists of unconstrained demonstration videos. From the

3690

Snapshot
HP
AP

HP

LeftHand

HP

A3

AP

H

Spread

O1

A1

PowerSmall- Brush
Grasp

AP

H
RightHand

O1

A1

PrecisionSmall- Corn
Grasp

HP
AP

HP

Grip Steak

O1

A1

O2

A3

AP

H
LeftHand

PowerSmall- Tongs
Grasp

HP
AP

HP

LeftHand

A3

AP

H

O1

A1

PowerSmall- Knife
Grasp

HP

Cut

AP

H
RightHand

O1

A1

PrecisionSmall- Lemon
Grasp

HP

LeftHand

O1

PowerSmall- Knife
Grasp

Cut

Overall
Recognition
Accuracy

HP

A3

AP
A1

Learned Commands
Grasp PoS(LH, Knife)
Grasp PrS(RH, Bowl)
Action Cut(Knife, Bowl)
Grasp PoS(LH, Bowl)
Grasp PoL(RH, Bowl)
Action Pour(Bowl, Bowl)

Grasp PoS(LH, Tongs)
Action Grip(Tongs, Chicken)

Grasp PoS(LH, Chicken)
Action Cut(Chicken, Chicken)

Grasp PoS(LH, Brush)
Grasp PrS(RH, Corn)
Action Spread(Brush, Corn)

Grasp PoS(LH, Brush)
Grasp PrS(RH, Corn)
Action Spread(Brush, Corn)

Grasp PoS(LH, Tongs)
Action Grip(Tongs, Steak)

Grasp PoS(LH, Tongs)
Action Grip(Tongs, Steak)

Grasp PoS(LH, Spreader)
Grasp PrL(RH, Bread)
Action Spread(Spreader, Bread)
Grasp PoL(LH, Mustard)
Grasp PrS(RH, Bread)
Action Spread(Mustard, Bread)
Grasp PoS(LH, Spatula)
Grasp PrS(RH, Bowl)
Action Stir(Spatula, Bowl)
Grasp PoL(LH, Pepper)
Grasp PoL(RH, Pepper)
Action Sprinkle(Pepper, Bowl)
Grasp PoS(LH, Knife)
Grasp PrS(RH, Lemon)
Action Cut(Knife, Lemon)
Grasp PoS(LH, Knife)
Grasp PrS(RH, Broccoli)
Action Cut(Knife, Broccoli)
Grasp PoS(LH, Whisk)
Grasp PrL(RH, Bowl)
Action Stir(Whisk, Bowl)

Grasp PoS(LH, Spreader)
Grasp PrL(RH, Bowl)
Action Spread(Spreader, Bowl)
Grasp PoL(LH, Mustard)
Grasp PrS(RH, Bread)
Action Spread(Mustard, Bread)
Grasp PoS(LH, Spatula)
Grasp PrS(RH, Bowl)
Action Stir(Spatula, Bowl)
Grasp PoL(LH, Pepper)
Grasp PoL(RH, Pepper)
Action Sprinkle(Pepper, Pepper)
Grasp PoS(LH, Knife)
Grasp PrS(RH, Lemon)
Action Cut(Knife, Lemon)
Grasp PoS(LH, Knife)
Grasp PoL(RH, Broccoli)
Action Cut(Knife, Broccoli)
Grasp PoS(LH, Whisk)
Grasp PrL(RH, Bowl)
Action Stir(Whisk, Bowl)

Object: 79%
Grasping type: 91%
Action: 83%

Overall percentage of
correct commands: 68%

AP

HP
H

Ground Truth Commands
Grasp PoS(LH, Knife)
Grasp PrS(RH, Tofu)
Action Cut(Knife, Tofu)
Grasp PoS(LH, Blender)
Grasp PrL(RH, Bowl)
Action Blend(Blender, Bowl)

AP

H
RightHand

A1

O1

PrecisionSmall- Bowl
Grasp

Table 4: LH:LeftHand; RH: RightHand; PoS: Power-Small;
PoL: Power-Large; PoP: Power-Spherical; PrS: PrecisionSmall; PrL: Precision-Large. Incorrect entities learned are
marked in red.
used in this work is still a syntax grammar. We are currently investigating the possibility of coupling manipulation
action grammar rules with semantic rules using lambda expressions, through the formalism of combinatory categorial
grammar developed by (Steedman 2002).

Figure 3: Upper row: input unconstrained video frames;
Lower left: color coded (see lengend at the bottom) visual
recognition output frame by frame along timeline; Lower
right: the most likely parse tree generated for each clip.

Acknowledgements
This research was funded in part by the support of the European Union under the Cognitive Systems program (project
POETICON++), the National Science Foundation under INSPIRE grant SMA 1248056, and support from the US Army,
Grant W911NF-14-1-0384 under the Project: Shared Perception, Cognition and Reasoning for Autonomy.

performance on this challenging dataset, we can conclude
that our system is able to recognize and generate action commands robustly.
We believe that the grasp type is an essential component
for fine grain manipulation action analysis. In future work
we will (1) further extend the list of grasping types to have
a finer categorization; (2) investigate the possibility of using
the grasp type as an additional feature for action recognition;
(3) automatically segment a long demonstration video into
action clips based on the change of grasp type.
Another line of future work lies in the higher level of
the system. The probabilistic manipulation action grammar

References
Aksoy, E.; Abramov, A.; Dörr, J.; Ning, K.; Dellen, B.; and
Wörgötter, F. 2011. Learning the semantics of object-action relations by observation. The International Journal of Robotics Research 30(10):1229–1249.
Argall, B. D.; Chernova, S.; Veloso, M.; and Browning, B. 2009.

3691

A survey of robot learning from demonstration. Robotics and Autonomous Systems 57(5):469–483.
Bengio, Y.; Courville, A.; and Vincent, P. 2013. Representation
learning: A review and new perspectives. Pattern Analysis and
Machine Intelligence, IEEE Transactions on 35(8):1798–1828.
Bird, S.; Klein, E.; and Loper, E. 2009. Natural language processing with Python. ” O’Reilly Media, Inc.”.
Cheng, M.-M.; Zhang, Z.; Lin, W.-Y.; and Torr, P. H. S. 2014.
BING: Binarized normed gradients for objectness estimation at
300fps. In IEEE CVPR.
Chomsky, N. 1993. Lectures on government and binding: The Pisa
lectures. Berlin: Walter de Gruyter.
Church, K. W. 1988. A stochastic parts program and noun phrase
parser for unrestricted text. In Proceedings of the second conference on Applied natural language processing, 136–143. Association for Computational Linguistics.
Ciresan, D. C.; Meier, U.; and Schmidhuber, J. 2012. Multi-column
deep neural networks for image classification. In CVPR 2012.
Dalal, N., and Triggs, B. 2005. Histograms of oriented gradients
for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on,
volume 1, 886–893. IEEE.
Das, P.; Xu, C.; Doell, R. F.; and Corso, J. J. 2013. A thousand
frames in just a few words: Lingual description of videos through
latent topics and sparse object stitching. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition.
Dunning, T. 1993. Accurate methods for the statistics of surprise
and coincidence. Computational Linguistics 19(1):61–74.
Feix, T.; Romero, J.; Ek, C. H.; Schmiedmayer, H.; and Kragic,
D. 2013. A Metric for Comparing the Anthropomorphic Motion
Capability of Artificial Hands. Robotics, IEEE Transactions on
29(1):82–93.
Graff, D. 2003. English gigaword. In Linguistic Data Consortium,
Philadelphia, PA.
Guerra-Filho, G.; Fermüller, C.; and Aloimonos, Y. 2005. Discovering a language for human activity. In Proceedings of the
AAAI 2005 Fall Symposium on Anticipatory Cognitive Embodied
Systems. Washington, DC: AAAI.
Guha, A.; Yang, Y.; Fermüller, C.; and Aloimonos, Y. 2013. Minimalist plans for interpreting manipulation actions. In Proceedings of the 2013 International Conference on Intelligent Robots
and Systems, 5908–5914. Tokyo: IEEE.
Jeannerod, M. 1984. The timing of natural prehension movements.
Journal of motor behavior 16(3):235–254.
Jia, Y. 2013. Caffe: An open source convolutional architecture for
fast feature embedding. http://caffe.berkeleyvision.org/.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. 2013. Imagenet classification with deep convolutional neural networks. In NIPS 2012.
LeCun, Y., and Bengio, Y. 1998. The handbook of brain theory
and neural networks. Cambridge, MA, USA: MIT Press. chapter
Convolutional networks for images, speech, and time series, 255–
258.
Lee, K.; Su, Y.; Kim, T.-K.; and Demiris, Y. 2013. A syntactic approach to robot imitation learning using probabilistic activity
grammars. Robotics and Autonomous Systems 61(12):1323–1334.
Lenz, I.; Lee, H.; and Saxena, A. 2014. Deep learning for detecting robotic grasps. International Journal of Robotics Research to
appear.
Li, Y.; Fermüller, C.; Aloimonos, Y.; and Ji, H. 2010. Learning
shift-invariant sparse representation of actions. In Proceedings of

the 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2630–2637. San Francisco, CA: IEEE.
Lowe, D. G. 2004. Distinctive image features from scale-invariant
keypoints. International journal of computer vision 60(2):91–110.
Mittal, A.; Zisserman, A.; and Torr, P. H. 2011. Hand detection
using multiple proposals. In BMVC, 1–11. Citeseer.
Oikonomidis, I.; Kyriazis, N.; and Argyros, A. 2011. Efficient
model-based 3D tracking of hand articulations using Kinect. In
Proceedings of the 2011 British Machine Vision Conference, 1–11.
Dundee, UK: BMVA.
Pastra, K., and Aloimonos, Y. 2012. The minimalist grammar of
action. Philosophical Transactions of the Royal Society: Biological
Sciences 367(1585):103–117.
Saxena, A.; Driemeyer, J.; and Ng, A. Y. 2008. Robotic grasping of
novel objects using vision. The International Journal of Robotics
Research 27(2):157–173.
Shimoga, K. B. 1996. Robot grasp synthesis algorithms: A survey.
The International Journal of Robotics Research 15(3):230–266.
Steedman, M. 2002. Plans, affordances, and combinatory grammar. Linguistics and Philosophy 25(5-6):723–753.
Summers-Stay, D.; Teo, C.; Yang, Y.; Fermüller, C.; and Aloimonos, Y. 2013. Using a minimal action grammar for activity understanding in the real world. In Proceedings of the 2013 IEEE/RSJ
International Conference on Intelligent Robots and Systems, 4104–
4111. Vilamoura, Portugal: IEEE.
Tenorth, M.; Ziegltrum, J.; and Beetz, M. 2013. Automated alignment of specifications of everyday manipulation tasks. In IROS.
IEEE.
Yang, Y.; Teo, C. L.; Daumé III, H.; and Aloimonos, Y. 2011.
Corpus-guided sentence generation of natural images. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, 444–454. Association for Computational Linguistics.
Yang, Y.; Guha, A.; Fermuller, C.; and Aloimonos, Y. 2014. A
cognitive system for understanding human manipulation actions.
Advances in Cognitive Sysytems 3:67–86.

3692

Active Scene Recognition with Vision and Language
Xiaodong Yu∗ , Cornelia Fermüller† , Ching Lik Teo‡ , Yezhou Yang‡ , Yiannis Aloimonos‡
Computer Vision Lab, University of Maryland, College Park, MD 20742, USA
xdyu@umiacs.umd.edu∗ , fer@cfar.umd.edu† , {cteo, yzyang, yiannis}@cs.umd.edu‡

Abstract
This paper presents a novel approach to utilizing high
level knowledge for the problem of scene recognition in an
active vision framework, which we call active scene recognition. In traditional approaches, high level knowledge
is used in the post-processing to combine the outputs of
the object detectors to achieve better classification performance. In contrast, the proposed approach employs high
level knowledge actively by implementing an interaction between a reasoning module and a sensory module (Figure 1).
Following this paradigm, we implemented an active
scene recognizer and evaluated it with a dataset of 20
scenes and 100+ objects. We also extended it to the analysis of dynamic scenes for activity recognition with attributes. Experiments demonstrate the effectiveness of the
active paradigm in introducing attention and additional
constraints into the sensing process.

Figure 1. Overview of the active approach for scene recognition.

standing in the paradigm of active vision. Central to the approach is a bio-inspired attention mechanism. Human perception is active and exploratory. We continuously shift our
gaze to different locations in the scene. After recognizing
objects, we will fixate again at a new location, and so on.
Humans interpret visual input by using their knowledge of
actions and objects, along with the language used for representing this information. It is clear that when we analyze a
complex scene, visual processes continuously interact with
our high-level knowledge, some of which is represented in
the form of language. In some sense, perception and language are engaged in an interaction, as they exchange information that leads to meaning and understanding.
This idea is applied to the simplest interpretation problem, scene recognition, in this paper. The proposed system
consists of two modules: (1) the reasoning module, which
obtains higher level knowledge about scene and object relations, proposes attentional instructions to the sensory module and draws conclusions about the contents of the scene;
(2) the sensory module, which includes a set of visual operators responsible for extracting features from images, detecting and localizing objects and actions. The novelty of the
proposed active paradigm is that the sensory module does
not passively process the image; instead, it is guided by the
reasoning module, which decides what and where the sen-

1. Introduction
The paradigm of Active Vision [1, 2, 3, 17, 19] had invigorated Computer Vision research in the early 1990s. The
ideas were inspired by the observation that in nature vision is used by systems that are active and purposive. By
studying visual perception in isolation, we often end up
with more complicated formulations and under-constrained
problems. Thus, the Active Vision paradigm proposed that
visual perception should be studied as a dynamic and purposive process for active observers that can control their
imaging mechanism. Most previous work in this paradigm
was concerned with low level robot vision problems, and
applied the ideas to shape reconstruction and navigational
problems, such as motion estimation, obstacle avoidance,
surveillance and path planning. Higher level tasks of scene
understanding and recognition have not been sufficiently
studied in this framework. These problems require combining high level knowledge and reasoning procedures with
low-level image processing and a systematic mechanism for
doing so.
In this paper we propose a new approach to scene under1

2011 IEEE International Conference on Computer Vision
c
978-1-4577-1102-2/11/$26.00 
2011
IEEE

810

sory module should process next. Thus the sensory module
shifts the focus of attention to a small number of objects
at selected locations of the scene. This leads to faster and
more accurate scene recognition.
Figure 1 illustrates the interaction between the two modules, which is modeled as an iterative process. Within each
iteration, the reasoning module decides on what and where
to detect next and expects the sensory module to reply with
some results after applying the visual operators. The reasoning module thus provides a focus of attention for the
sensory module, which can be an object to be detected and
a place to be examined. For the problem of scene recognition, the interaction between the two modules is simple.
(See Figure 6 for examples of the interaction over a given
image.) However, our framework is more general. In Section 5 we discuss the extension of the framework to dynamic
scene understanding. In this case the goal is to interpret the
activity in the video. An activity is described by a set of
quantities: the human, the tools, the objects, the motion,
and the scene involved in the activity. Each of the quantities has many possible instances which can be described
by their attributes (e.g., adjectives of nouns and adverbs of
verbs). Thus the reasoning module at every iteration has to
decide which quantity and which attribute to compute next.
This procedure can be implemented in a hierarchical model
of the proposed active scheme.
The rest of the paper is organized as follows: in the next
section, we review related work; Section 3 describes an implementation of the proposed paradigm; in Section 4 we
experimentally evaluate the active scene recognizer; in Section 5 we discuss how our framework can be generalized to
object recognition and dynamic scene interpretation, and we
demonstrate the ideas on the problem of recognizing hand
activities on a small dataset; finally we draw conclusions in
Section 6.

2. Related Works
Recognition by Components: The methodology for object, scene and activity recognition in this paper follows the
idea of “recognition by components”, which can be traced
back to early work by Biederman [4]. In this methodology,
scenes are recognized by detecting the inside objects [13],
objects are recognized by detecting their parts or attributes
[11], and activities are recognized by detecting the motions,
objects and contexts involved in the activities [10]. However, all previous works employ passive approaches. As
a result, they need to run through all object/attribute detectors over the testing images and videos before making
the final conclusion. In this paper we explore an active
approach, which aims at greatly reducing the number of
object/attribute detectors needed for recognition of objects,
scenes and activities.
Active Learning and Active Testing: Our work is a

type of active testing and is closely related to the visual
“20 question” game described in [5]. While the approach in
[5] needs human annotators to answer the questions posed
by the computer, our approach is fully automated without a
human in the loop.
To select the optimal objects/attributes, we use the criterion of Maximum Information Gain, which have been
widely used for active learning of objects and scenes [21,
25]. Information theory also have been used for object localization in application of face detection [22].
Employing Ontological Knowledge in Computer Vision System for Scene Interpretation: Ontological knowledge plays an important role in the reasoning and learning
system of human. For example, in the problem of scene
recognition, if we know that coast is a type of outdoor scene
and also know that it is unlikely to find bookshelves therein.
Hence, we do not need to apply the bookshelves detectors in the possible coast scene image. The work in [23]
takes advantage of this type of knowledge in object detection. Similarly, the knowledge about objects and attributes
is employed in [11]. Extending the knowledge about object hierarchy is employed in [15]. In this paper, we further explore the ontological knowledge about activities and
attributes and present a pilot study using a hand activity
dataset.

3. The Approach
3.1. System Overview
The proposed active scene recognizer classifies a scene
by iteratively detecting the objects inside it. In the k-th
iteration, the reasoning module provides an attentional instruction to the sensory module to search for an object Ok
within a particular region Lk in the image. Then the sensory
module runs the corresponding object detector and returns
a response, which is the highest detection score dk and the
object’s location lk . The reasoning module receives this response, analyses it and starts a new iteration. This iteration
continues until some terminating criteria are satisfied. To
implement such an active scene recognizer, we need to implement the following components: (1)a sensory module for
object detection; (2) a reasoning module for predicting the
scene class based on the sensory module’s responses; (3) a
strategy for deciding which object and where in the scene
the sensory module should process in the next iteration; and
(4) a strategy for initializing and terminating the iteration.
We will describe these components in the rest of this section.

3.2. Scene Recognition by Object Detection
In the proposed framework, the reasoning module decides the scene class S based on the responses X from the
sensory module, which we call Scene Recognition by Ob-

811

ject Detection (SROD). The optimal scene class of the given
image belongs to the one that maximizes the probability:
S ∗ = arg max p(S|X),

(1)

S∈[1:M ]

where M is the number of scene classes.
The responses from the sensory module are a detection
score and a detection bounding box. We only consider the
objects’ vertical positions, since they are more consistent
within the images of the same scene class [23]. An object’s vertical position is represented by a profile of the
mask formed by the object’s bounding box (see Figure 2
for an example). The object’s mask formed by the object’s
bounding box is normalized to 256 × 256 pixels, and the
profile is the histogram of pixels within the object’s mask
along the vertical axis. By this compact representation, we
not only record the object’s vertical location, but also record
the object’s scales along the horizontal and vertical axes. In
the following, we denote this representation of an object’s
location as lk .
As described above, in each iteration, the sensory module returns a detection score di and a detected location li
for the expected object Oi . Thus at step k, we have accumulated a list of detected score d1:k and corresponding
locations l1:k . Given X = (d1:k , l1:k ), the probability of a
scene S is :
P (S|X) = p(S|d1:k , l1:k )
∝ p(d1:k , l1:k |S)
= p(d1:k |S)p(l1:k |S).

(2)

In the above equation, we assume d1:k and l1:k are independent given S. We approximate p(d1:k |S) by the inner product of d1:k and d˜S1:k , where d˜S1:k is the mean d1:k of training
examples for scene class S. Similarly, p(l1:k |S) is approxiS
mated by the inner product of l1:k and ˜l1:k
. The advantage
of this approximation is its simplicity and flexibility. We
need to update the list of selected object in each iteration.
If we adopt a parametric model for p(d1:k |S) and p(l1:k |S),
we would need to learn the parameters for all permutations
of O1:k , k = 1, ..., N , where N is the total number of object
categories in the dataset. For large N 1 , such scheme would
not work simply because of the computational constraints.
Using a parameter-free approach, we avoid this difficulty.

3.3. Detecting Objects by The Sensory Module
The task of the sensory module is to detect the object
required by the reasoning module and return a response. In
this paper, we applied three object detectors: a Spatial Pyramid Matching object detector [12], a latent SVM object detector [8] and the texture classifier by Hoiem [9]. For each
1 In

(a)

(b)

(c)

Figure 2. Representation of the object’s location: (a) an object’s
bounding box; (b) the binary mask formed by the bounding box;
(a) the profile of the object’s binary mask along the vertical direction.

object class, we train all three object detectors and then select the one with the highest detection accuracy on a validation dataset to use in the test. Given a test image, the object
detector will find a few candidates with corresponding detection scores. The one with the highest score is selected
and sent to the reasoning module. The detection scores are
normalized by Platt scaling [18] to obtain probabilistic estimates.

3.4. Attentional Instructions by The Reasoning
Module
The interaction between the reasoning and sensory modules at iteration k starts from an attentional instruction issued by the reasoning module, based on its observation history. In this paper, the attentional instruction in iteration k
includes what to look for, i.e., the object to detect (denoted
as Ok ) and where to look, i.e., the regions to detect (denoted
as Lk ). The criterion to select Ok and Lk is to maximize the
expected information gain about the scene in the test image
due to the response of this object detector:
{Ok∗ , L∗k } = arg max I(S; dk , lk |d1:k−1 , l1:k−1 ),

(3)

Ok ∈Ñk−1 ,
Lk ∈Lk

where Ñk−1 denotes the set of indices of objects that have
not been detected until iteration k, Lk denotes the search
space of Ok ’s location. The global optimization procedure
is approximated by two local optimization procedures. In
the first step, we select Ok based on the maximum expected
information gain criterion:
Ok∗ = arg max I(S; dk , lk |d1:k−1 , l1:k−1 ).

(4)

Ok ∈Ñk−1

S
Then L∗k is selected by thresholding ES [˜lO
∗ ], the expected
k
∗
location of object Ok across all scene classes.

The expected information gain of Ok given the previous

our dataset, N > 100

812

response d1:k−1 and l1:k−1 is defined as:

hk . We model these parameters by Gaussian distributions

I(S;dk , lk |d1:k−1 , l1:k−1 )
X
=
p(dk , lk |d1:k−1 , l1:k−1 )
dk ∈D,lk ∈Lk

× KL[p(S|d1:k , l1:k ), p(S|d1:k−1 , l1:k−1 )].

(5)

The KL divergence on the right side of Equation 5 can easily
be computed after applying Equation 2. To compute the
first term on the right side of Equation 5, we factorize it as
follows:
p(dk , lk |d1:k−1 , l1:k−1 )
= p(dk |d1:k−1 , l1:k−1 )p(lk |d1:k , l1:k−1 ).

(6)

The two terms on the right side of the above equation can be
efficiently computed by their conditional probability with
respect to S:
p(dk |d1:k−1 , l1:k−1 )
=

M
X

p(dk |S, d1:k−1 , l1:k−1 )p(S|d1:k−1 , l1:k−1 )

S=1

=

M
X

p(dk |S)p(S|d1:k−1 , l1:k−1 ),

(7)

yk ∼N (µyk , σy2k ),

(10)

hk ∼N (µhk , σh2 k ),
2
wk ∼N (µwk , σw
).
k

(11)
(12)

The means and variances of these Gaussian distributions are
estimated from the training set. Thus the problem of drawing a sample of lk becomes the problem of drawing a sample
of yk , hk , wk from three Gaussian distributions.
After drawing samples of dk and lk , we substitute them
into Equation 5 to compute the expected information gain
for Ok . Then among all possible Ok ’s, we select the object
that yields the maximum expected information gain, Ok∗ .
The distribution of Ok∗ ’s location in a particular scene S is
S
approximated by ˜lO
∗ , which is computed as follows: first,
k
we aggregate lOk∗ in all training samples of scene class S in
the training stage; then we normalize the accumulated valS
ues into [0, 1]. Thus the expectation of ˜lO
∗ across all scene
k
S
classes, ES [˜lO∗ ], represents the distribution of Ok∗ ’s locak
tion in an image of any scene class. Finally, we threshold
this value by 0.5 and obtain a binary L∗k , which provides the
focus of attention for the sensory module in the next iteration.

3.5. Initializing and Terminating the Iteration

S=1

where we assume dk is independent of d1:k−1 and l1:k−1
given S. p(dk |S) can be computed by introducing the binary variable ek , which indicates whether object Ok appears
in the scene or not:
X
p(dk |S) =
p(dk |ek , S)p(ek |S)
(8)
ek ∈{0,1}

=

X

p(dk |ek )p(ek |S).

(9)

ek ∈{0,1}

p(ek |S) encodes the high-level knowledge about the relationship between scene S and object Ok . One way to obtain it is to count the object labels in the training image set.
Otherwise, we can obtain it from textual corpus. p(dk |ek )
encodes the information about the accuracy of different object detectors. The method to estimate its value is discussed
in Section 4.1. The procedures described above are also employed to compute p(lk |d1:k , l1:k−1 ) in a similar fashion.
Finally, we note that the expectation in Equation (5)
needs to be computed at a set of sampling points of dk (denoted as D) and a set of sampling points of lk (denoted as
Lk ). D is within a one dimensional space between 0 and 1
and we draw samples of dk uniformly. Lk can be parameterized by three parameters: the center position of Ok , yk ; the
horizontal extent of Ok , wk ; and the vertical extent of Ok ,

The interaction between two modules starts from the first
object and its expected location, which are provided by the
reasoning module. We select the object O1 that maximizes
the mutual information
O1∗ = arg max I(S; d1 , l1 ).

(13)

O1 ∈[1:N ]

To terminate the iteration, we can either stop after a fixed
number of iterations (e.g., the 20 question game), or stop
when the expected information gain at each iteration is below a threshold. In our experiments, we followed the former
approach and found that 30 iterations are sufficient to produce competitive recognition results.

4. Experiments
4.1. Image Datasets
We evaluate the proposed approach using a subset of the
SUN image set from [7]. Overall, the SUN dataset[7] contains 12K images, 1K scene classes and more than 200 object classes. We sort the scene classes by the number of
examples and select the top 20 that have more than 50 examples per scene class. The remaining scene classes are
discarded since they do not have sufficient number of examples to evaluate our algorithms. For each of the 20 selected
scene classes, we randomly select 50 examples, where 30

813

of them are used for training and the rest 20 for testing. At
the end, there are 127 object classes within our subset of
SUN image set but only a handful of object classes appear
in a particular scene class. As discussed in [7], a typical
scene image contains seven object classes. The object detectors are trained using an additional dataset of 26,000 objects that is disjoint from the training/testing scene images
as described in [7]. The obtained object detectors are then
evaluated in the 600 scene training examples. The detection
score, dk , is normalized into [0, 1] and evenly quantized into
10 discrete values. For each ek (0 or 1), we accumulate the
counts of dk for each of its 10 values. Due to its discrete nature, p(dk |ek ) can be modeled as a multinomial distribution.
Since Dirichlet is the conjugate prior for multinomial, we
use a Dirichlet distribution Dir(α) as the prior for p(dk |ek ),
where α represents the number of prior observations of dk
given a particular ek . Through all experiments in this paper,
we set the parameter α = 1. We also tried other values of α
and found no significant performance impact.

Figure 3. Comparison of scene classification accuracy of different
approaches (GIST+SVM vs. BoW+SVM vs. CART vs. SROD).
We also illustrate the “ideal” performance of CART and SROD,
where we use the object ground truths as the outputs of object
detectors. They are represented as “(CART)” and “(SROD)” in
the figure respectively.

4.2. Performance of the Scene Recognizer
In the first experiment, we evaluate the scene recognizer
(SROD) as described in Equation 2 while all objects are detected. The “ideal” SROD, where we use the object ground
truths as the outputs of object detectors, is also evaluated
to illustrate the upper limit of the performance of SROD.
Three baseline algorithms are evaluated as listed below:
• SVM using GIST [16] features.
• SVM using Bag-of-Words (BoW). We used two types
of local features, SIFT [14] and opponent SIFT [24],
and the size of visual word dictionary is set as 500 for
each of them.
• Classification and Regression Tree (CART) [6] that
uses the object detection scores as attributes to predict
the scene classes of a given image. The “ideal” CART,
where the object ground truth is used as attributes, is
also evaluated to illustrate the upper limit of the performance of CART.
Figure 3 compares the scene classification accuracy of
these baseline algorithms and the SROD approach. The
SROD approach significantly outperforms all the baseline
algorithms. This result confirms the effectiveness of objectbased approaches in interpreting complex scenes and the
robustness of the SROD approach to the errors in object detection. It is worth to emphasize that there is still a lot of
room to improve the current object-based scene recognizer,
as suggested by the performance of the ideal SROD.
In addition, we evaluate the robustness of these scene
recognition approaches with respect to the size of training
samples. We randomly select a number of training examples from the training image set for each scene class and
repeat the experiments three times. The mean and standard
deviation of the average accuracy when using 5, 10, 15, 20,

Figure 4. Classification accuracies of different approaches
(GIST+SVM vs. BoW+SVM vs. CART vs. SROD) with respect
to the number of training images.

25 and 30 training examples are reported in Figure 4. The
proposed SROD method achieves substantially better performance than all baseline algorithms, including the CART
algorithm that uses the same outputs of object detectors.

4.3. Comparison of the Active Scene Recognizer vs.
the Passive Scene Recognizer
In this experiment, we compare the proposed active
scene recognizer with two baseline algorithms and the results are presented in Figure 5. Both baseline algorithms
recognize scene class by iterative object detection, which
is similar to the proposed SROD method. But they employ different strategies to select the to-be-detected object
in each iteration. The first baseline (denoted as “DT” in
Figure 5) follows a fixed object order, which is provided by
the CART algorithm; while the second baseline (denoted as
“Rand” in Figure 5) just randomly selects an object from
the remaining object pool. Object selection obviously has a
big impact on the performance of scene recognition, since

814

Figure 7. Hierarchical active scheme for dynamic scene recognition, where each iteration invokes four steps. Section 5 discusses
the details.
Quantity
Tools
Figure 5. Comparison of classification accuracy among different
object selection strategies (active vs. passive vs. random) in
the object-based scene recognizers, with respect to the number of
training images.

both the proposed active approach and the “DT” approach
significantly outperform the “Rand” approach. The result
also shows that the active approach is superior to the “DT”
approach that is passive: the active approach can achieve
stable performance after selecting 30 objects while the passive “DT” approach needs 60 objects. Furthermore, the object’s expected location provided by the reasoning module
in the active approach not only reduces the spatial search
space to be about 1/3 to 1/2 of the whole image but also reduces the false positives in the sensory module’s response.
As a result, the proposed active approach achieves 3% to
4% performance gain compared to the passive approaches.

4.4. Visualization of the Interaction between the
Sensory Module and the Reasoning Module
Figure 6 illustrates a few iterations of the active scene
recognizer performed on a test image. It shows that after
detecting twenty objects, the reasoning module is able to
decide the correct scene class with high confidence.

5. Dynamic Scene Recognition
There are two key premises in the proposed active
scheme: (1) a quantity can be recognized by accumulating
evidences from its components; (2) the components can be
assumed to be independent given the quantity. Given these
two premises, the active scheme can be applied to select a
small number of components to recognize the quantity without impairing the performance. In the previous section, we
have applied this active scheme to recognize static scenes.
However, this active scheme can also be applied to recognize objects by their parts and recognize activities by their
motion and object properties.
In this section, we will demonstrate the application of
the active scheme in an activity recognition problem. A big
challenge in this problem is that the components are hetero-

Motion

Attribute
Color
Texture
Elongation
Convexity
Frequency
Motion variation
Motion spectrum
Duration

e=1
silver
bristle
yes
yes
high
large
sparse
long

e=0
other colors
non-bristle
no
no
low
small
non-sparse
short

Table 1. Activity attributes in the hand activity dataset.

geneous. While static scenes only involve a single quantity, i.e., objects, activities are described by different quantities, including motion, objects and tools, scenes, temporal properties, etc. To alleviate this problem, we propose a
hierarchical active scheme for dynamic scene recognition.
Figure 7 presents this method. In this scheme, each iteration performs the following four steps: (1) using the maximum information gain criterion, the activity-level reasoning module sends an attentional instruction to the quantitylevel reasoning module that indicates the desired quantity
(e.g., motion or objects); (2) the quantity-level reasoning
module then sends an attentional instruction to the sensory module that indicates the desired attributes (e.g., object color/texture, motion properties); (3) the sensory module applies the corresponding detectors and returns the detector’s response to the quantity-level reasoning module; (4)
finally, the quantity-level reasoning module returns the likelihood of the desired quantity to the activity-level reasoning
module.
To demonstrate this idea, we used 30 short video sequences of 5 hand actions from a dataset collected from the
commercially available PBS Sprouts craft show for kids (the
hand activity data set). The activities are coloring, drawing,
cutting, painting, and gluing. 20 sequences were used for
training and the rest for testing. Two quantities are considered in recognizing an activity: the characteristics of tools
and the characteristics of motion. Four attributes are defined for the characteristics of tools, including color, texture, elongation, and convexity; and four attributes are defined for the characteristics of motion, including frequency,
motion variation, motion spectrum, and duration. The details of these quantities and attributes are described in Table
1.
The sensory module includes detectors for the 8 at-

815

Expected
Object Ok :

O1
wall

O2
person

O3
books

Expected
Location
Lk :

Sensory
Module’s
Response
(dk , lk ):

Reasoning
module’s
Belief
P (S|d1:k , l1:k )
and S ∗

...
...

O10
sink

...
...

O20
toilet

...
...

...

...

...

...

...

...

...

...

...

Figure 6. Visualization of the iterations between the reasoning module and the sensory module in an active scene recognition process. The
detected regions with detection score greater than 0.5 are highlighted with a red bounding box.

tributes of tools/motion. To detect these attributes, we need
to segment the hand and tools from the videos. Figure 8
illustrates these procedures, which are described as follows:
1. Hand regions Sh are segmented by applying a variant
of the color segmentation approach based on Conditional Random Fields (CRF) [20] using a trained skin
color model. Similarly, moving regions of hands and
tools, Sf , are segmented by applying another CRF
over the optical flow fields.

Figure 8. Procedures to extract hands and tools from the hand activity video sequence. Please refer to the text for details.

2. A binary XOR operation is applied on Sh and Sf to
remove the moving hand regions and produce a segmentation of tools, ST .
3. Apply a threshold tf to remove regions with flows that
are different from the hand regions and obtain a candidate region for tool, Ŝr .

Figure 9. Sample frames for 10 testing videos in the hand action
dataset: (from left to right) coloring, cutting, drawing, gluing,
painting. The detected tool is fit with an ellipse.

4. Detect edges in Ŝr .
5. Fitting a minimum volume ellipse over the edge map
of Ŝr , which estimates the region of the detected tool.
Figure 9 shows the estimated ellipse enclosing the detected tool over some sample image frames from the dataset.
This ellipse, together with Ŝr , is then used as a mask to
compute object-related attributes. The color and texture
attributes were computed from histograms of color and
wavelet-filter outputs, and the shape attributes were derived

from region properties of the convex hull of the object and
the fitted ellipse. The motion attributes were computed from
the spectrum of the average optical flow over the sequence
and the variation of the flow.
Table 2 shows the interactions between the reasoning
modules and the sensory modules for one of the testing
videos. Here the sensory module only needed to detect two
attributes before the reasoning module arrived at the correct
conclusion. Overall, 8 out of 10 testing videos were recog-

816

Iteration
Expected quantity
Expected
attribute
Sensory module’s
response
Reasoning module’s conclusion
Reasoning module’s confidence

1
Tools

2
Tools

3
Tools

4
Motion

Elongation

Color

Texture

Duration

0.770

1.000

0.656

0.813

Coloring

Painting

Painting

Painting

0.257

0.770

0.865

0.838

Table 2. An example of interactions between the reasoning module
and the sensory module for hand activity recognition, where the
ground truth of the activity class is painting.

nized correctly after detecting two to three attributes, while
the remaining two testing videos could not be recognized
correctly even after detecting all the attributes. This is because of errors in the segmentation, the choice of attributes
and the small set of training samples.

6. Conclusion and Future Work
We proposed a new framework for scene recognition
within the active vision paradigm. In our framework, the
sensory module is guided by attentional instructions from
the reasoning module and employs detectors of a small set
of objects within selected regions. The attention mechanism
is realized using an information theoretic approach, with the
idea that every detected object should maximize the added
information for scene recognition. Our framework is evaluated in a static scene dataset and shows the advantage over
the passive approach. Also we discussed how it can be generalized to object recognition and dynamic scene analysis,
and gave a proof of concept by implementing it for attribute
based activity recognition.
In the current implementation, we have assumed that objects are independent given the scene class. Though this
assumption simplifies the formulation, this is not necessarily true in general. In the future, we plan to remove this
assumption and design a scene recognition model that better represents the complex scenes in the real world. Also,
we will perform a comprehensive study of the proposed approach using larger image/video datasets to investigate the
impact of the active paradigm.

References
[1] J. Aloimonos, I. Weiss, and A. Bandopadhay. Active Vision.
IJCV, 2:333–356, 1988. 1
[2] R. Bajcsy. Active Perception. Proceedings of the IEEE,
76:996–1005, 1988. 1
[3] D. H. Ballard. Animate Vision. Artificial Intelligence,
48:57–86, 1991. 1
[4] I. Biederman. Recognition-by-Components: A Theory
of Human Image Understanding. Psychological Review,
94:115–147, 1987. 2

[5] S. Branson, C. Wah, B. Babenko, F. Schroff, P. Welinder,
P. Perona, and S. Belongie. Visual Recognition with Humans
in the Loop. In ECCV, 2010. 2
[6] L. Breiman, J. Friedman, C. J. Stone, and R. Olshen. Classification and Regression Trees. Chapman and Hall/CRC,
1984. 5
[7] M. J. Choi, J. Lim, A. Torralba, and A. S. Willsky. Exploiting Hierarchical Context on a Large Database of Object Categories. In CVPR, 2010. 4, 5
[8] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object Detection with Discriminatively Trained Part
Based Models. PAMI, 32(9):1627 – 1645, 2010. 3
[9] D. Hoiem, A. Efros, and M. Hebert. Automatic Photo Popup. In ACM SIGGRAPH, 2005. 3
[10] N. Ikizler-Cinbis and S. Sclaroff. Object, Scene and Actions:
Combining Multiple Features for Human Action Recognition. In ECCV, 2010. 2
[11] H. N. Lampert, C. H. and S. Harmeling. Learning To Detect
Unseen Object Classes by Between-Class Attribute Transfer.
In CVPR, 2009. 2
[12] S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural
Scene Categories. In CVPR, 2006. 3
[13] L.-J. Li, H. Su, E. P. Xing, and L. Fei-Fei. Object Bank: A
High-Level Image Representation for Scene Classification &
Semantic Feature Sparsification. In NIPS, 2010. 2
[14] D. G. Lowe. Distinctive Image Features from Scale-invariant
Keypoints. IJCV, 20:91–110, 2004. 5
[15] M. Marszalek and C. Schmid. Semantic Hierarchies for Visual Object Recognition. In CVPR, 2007. 2
[16] A. Oliva and A. Torralba. Modeling the Shape of the Scene:
a Holistic Representation of the Spatial Envelope. IJCV,
42:145–175, 2001. 5
[17] J. olof Eklundh, P. Nordlund, and T. Uhlin. Issues in Active
Vision: Attention and Cue Integration/Selection. In BMVC,
pages 1–12, 1996. 1
[18] J. C. Platt. Probabilities for SV Machines. In Advances in
Large Margin Classifiers, 1999. 3
[19] R. D. Rimey and C. M. Brown. Control of Selective Perception Using Bayes Nets and Decision Theory. IJCV, 12:173–
207, 1994. 1
[20] C. Rother, V. Kolmogorov, and A. Blake. GrabCut: interactive Foreground Extraction using Iterated Graph Cuts. ACM
Trans. Graph., 23(3):309–314, 2004. 7
[21] B. Siddiquie and A. Gupta. Beyond Active Noun Tagging: Modeling Contextual Interactions for Multi-Class Active Learning. In CVPR, 2010. 2
[22] R. Sznitman and B. Jedynak. Active Testing for Face Detection and Localization. PAMI, 2010. 2
[23] A. Torralba. Contextual Priming for Object Detection. IJCV,
53(2):153–167, 2003. 2, 3
[24] van de Sande, K. E. A., T. Gevers, and C. G. M. Snoek. Evaluating Color Descriptors for Object and Scene Recognition.
PAMI, 32(9):1582–1596, 2010. 5
[25] S. Vijayanarasimhan and K. Grauman. Cost-Sensitive Active
Visual Category Learning. IJCV, 2010. 2

817

Corpus-Guided Sentence Generation of Natural Images
Yezhou Yang † and Ching Lik Teo † and Hal Daumé III and Yiannis Aloimonos
University of Maryland Institute for Advanced Computer Studies
College Park, Maryland 20742, USA
{yzyang, cteo, hal, yiannis}@umiacs.umd.edu

Abstract
We propose a sentence generation strategy
that describes images by predicting the most
likely nouns, verbs, scenes and prepositions
that make up the core sentence structure. The
input are initial noisy estimates of the objects
and scenes detected in the image using state of
the art trained detectors. As predicting actions
from still images directly is unreliable, we use
a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns,
scenes and prepositions. We use these estimates as parameters on a HMM that models
the sentence generation process, with hidden
nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.

1

Figure 1: The processes involved for describing a scene.

Introduction

What happens when you see a picture? The most
natural thing would be to describe it using words:
using speech or text. This description of an image is
the output of an extremely complex process that involves: 1) perception in the Visual space, 2) grounding to World Knowledge in the Language Space and
3) speech/text production (see Fig. 1). Each of these
components are challenging in their own right and
are still considered open problems in the vision and
linguistics fields. In this paper, we introduce a computational framework that attempts to integrate these
†

indicates equal contribution.

components together. Our hypothesis is based on
the assumption that natural images accurately reflect
common everyday scenarios which are captured in
language. For example, knowing that boats usually
occur over water will enable us to constrain the
possible scenes a boat can occur and exclude highly
unlikely ones – street, highway. It also enables us to predict likely actions (Verbs) given the
current object detections in the image: detecting a
dog with a person will likely induce walk rather
than swim, jump, fly. Key to our approach is
the use of a large generic corpus such as the English
Gigaword [Graff, 2003] as the semantic grounding
to predict and correct the initial and often noisy visual detections of an image to produce a reasonable
sentence that succinctly describes the image.
In order to get an idea of the difficulty of this
task, it is important to first define what makes up

444
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454,
c
Edinburgh, Scotland, UK, July 27–31, 2011. 
2011
Association for Computational Linguistics

Figure 2: Illustration of various perceptual challenges for
sentence generation for images. (a) Different images with
semantically the same content. (b) Pose relates ambiguously to actions in real images. See text for details.

a description of an image. Based on our observations of annotated image data (see Fig. 4), a descriptive sentence for an image must contain at minimum: 1) the important objects (Nouns) that participate in the image, 2) Some description of the actions (Verbs) associated with these objects, 3) the
scene where this image was taken and 4) the preposition that relates the objects to the scene. That is, a
quadruplet of T = {n, v, s, p} (Noun-Verb-ScenePreposition) that represents the core sentence structure. Generating a sentence from this quadruplet is
obviously a simplification from state of the art generation work, but as we will show in the experimental results (sec. 4), it is sufficient to describe images. The key challenge is that detecting objects, actions and scenes directly from images is often noisy
and unreliable. We illustrate this using example images from the Pascal-Visual Object Classes (VOC)
2008 challenge [Everingham et al., 2008]. First,
Fig. 2(a) shows the variability of images in their raw
image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz
et al., 2009] to reliably detect important objects in
the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010]
manages around 42% for humans and only 11% for
boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically
similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi,
2004] have proposed that inferring the action from
static images (known as an “implied action”) is of445

ten achieved by detecting the pose of humans in the
image: the position of the limbs with respect to one
another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption
is weak as 1) similar actions may be represented by
different poses due to the inherent dynamic nature of
the action itself: e.g. walking a dog and 2) different
actions may have the same pose: e.g. walking a dog
versus running (Fig. 2(b)). The missing component
here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei,
2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy
respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state
of the art scene detectors [Oliva and Torralba, 2001;
Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined
scene classes for a classification to be successful –
with a reported average precision of 83.7% tested
over a dataset of 2600 images.
Addressing all these visual challenges is clearly
a formidable task which is beyond the scope of this
paper. Our focus instead is to show that with the
addition of language to ground the noisy initial visual detections, we are able to improve the quality of the generated sentence as a faithful description of the image. In particular, we show that it
is possible to avoid predicting actions directly from
images – which is still unreliable – and to use the
corpus instead to guide our predictions. Our proposed strategy is also generic, that is, we make no
prior assumptions on the image domain considered.
While other works (sec. 2) depend on strong annotations between images and text to ground their predictions (and to remove wrong sentences), we show
that a large generic corpus is also able to provide
the same grounding over larger domains of images.
It represents a relatively new style of learning: distant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled”
data containing images and captions but only separate data from each side. Another contribution is
a computationally feasible way via dynamic programming to determine the most likely quadruplet
T ∗ = {n∗ , v ∗ , s∗ , p∗ } that describes the image for
generating possible sentences.

2

3

Related Work

Recently, several works from the Computer Vision
domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used
predefined production rules to describe actions in
videos. [Berg et al., 2004] processed news captions
to discover names associated with faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs
in the captions. Both approaches use annotated examples from a limited news caption corpus to learn
a joint image-text model so that one can annotate
new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of
objects and poses makes it nearly impossible to learn
a more general model. In addition, no attempt was
made to generate a descriptive sentence from the
learned model. The work of [Farhadi et al., 2010] attempts to “generate” sentences by first learning from
a set of human annotated examples, and producing the same sentence if both images and sentence
share common properties in terms of their triplets:
(Nouns-Verbs-Scenes). No attempt was made to
generate novel sentences from images beyond what
has been annotated by humans. [Yao et al., 2010]
has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids.
Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al.,
2003] are based on three steps: selection, planning
and realization. A common challenge in generation
problems is the question of: what is the input? Recently, approaches for generation have focused on
formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization.
We address a tangential problem that has not received much attention in the generation literature:
how to deal with noisy inputs. In our case, the inputs
themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty
into account.
446

Our Approach

Our approach is summarized in Fig. 3. The input is a
test image where we detect objects and scenes using
trained detection algorithms [Felzenszwalb et al.,
2010; Torralba et al., 2003]. To keep the framework
computationally tractable, we limit the elements of
the quadruplet (Nouns-Verbs-Scenes-Prepositions)
to come from a finite set of objects N , actions V,
scenes S and prepositions P classes that are commonly encountered. They are summarized in Table. 1. In addition, the sentence that is generated
for each image is limited to at most two objects occurring in a unique scene.

Figure 3: Overview of our approach. (a) Detect objects
and scenes from input image. (b) Estimate optimal sentence structure quadruplet T ∗ . (c) Generating a sentence
from T ∗ .

Denoting the current test image as I, the initial
visual processing first detects objects n ∈ N and
scenes s ∈ S using these detectors to compute
Pr (n|I) and Pr (s|I), the probabilities that object
n and scene s exist under I. From the observation
that an action can often be predicted by its key objects, Nk = {n1 , n2 , · · · , ni }, ni ∈ N that participate in the action, we use a trained Language model
Lm to estimate Pr (v|Nk ). Lm is also used to compute Pr (s|n, v), the predicted scene using the corpus given the object and verb; and Pr (p|s), the predicted preposition given the scene. This process is
repeated over all n, v, s, p where we used a modified HMM inference scheme to determine the most
likely quadruplet: T ∗ = {n∗ , v ∗ , s∗ , p∗ } that makes
up the core sentence structure. Using the contents
and structure of T ∗ , an appropriate sentence is then
generated that describes the image. In the following
sections, we first introduce the image dataset used
for testing followed by details of how these components are derived.

Objects n ∈ N
’aeroplane’ ’bicycle’ ’bird’
’boat’ ’bottle’ ’bus’ ’car’
’cat’ ’chair’ ’cow’ ’table’
’dog’ ’horse’, ’motorbike’
’person’ ’pottedplant’
’sheep’ ’sofa’ ’train’
’tvmonitor’

Actions v ∈ V
’sit’ ’stand’ ’park’
’ride’ ’hold’ ’wear’
’pose’ ’fly’ ’lie’ ’lay’
’smile’ ’live’ ’walk’
’graze’ ’drive’ ’play’
’eat’ ’cover’ ’train’
’close’ ...

Scenes s ∈ S
’airport’
’field’
’highway’
’lake’ ’room’
’sky’ ’street’
’track’

Preps p ∈ P
’in’ ’at’ ’above’
’around’ ’behind’
’below’ ’beside’
’between’
’before’ ’to’
’under’ ’on’

Table 1: The set of objects, actions (first 20), scenes and preposition classes considered

(a)

Figure 4: Samples of images with corresponding annotations from the UIUC scene description dataset.

3.1

Object and Scene Detections from Images

We use the Pascal-VOC 2008 trained object detectors [Felzenszwalb et al., 2008] of 20 common everyday object classes that are defined in N . Each of
the detectors are essentially SVM classifiers trained
on a large number of the objects’ image representations from a large variety of sources. Although
20 classes may seem small, their existence in many
1

Figure 5: (a) [Top] The part based object detector from
[Felzenszwalb et al., 2010]. [Bottom] The graphical
model representation of an object, for e.g. a bike. (b)
Examples of GIST gradients: (left) an outdoor scene vs
(right) an indoor scene [Torralba et al., 2003].

Image Dataset

We use the UIUC Pascal Sentence dataset, first introduced in [Farhadi et al., 2010] and available online1 . It contains 1000 images taken from a subset of the Pascal-VOC 2008 challenge image dataset
and are hand annotated with sentences that describe
the image by paid human annotators using Amazon Mechanical Turk. Fig. 4 shows some sample
images with their annotations. There are 5 annotations per image, and each annotation is usually
short – around 10 words long. We randomly selected
900 images (4500 sentences) as the learning corpus
to construct the verb and scene sets, {V, S} as described in sec. 3.3, and kept the remaining 100 images for testing and evaluation.
3.2

(b)

http://vision.cs.uiuc.edu/pascal-sentences/

447

natural images (e.g. humans, cars and plants) makes
them particularly important for our task, since humans tend to describe these common objects as well.
As object representations, the part-based descriptor
of [Felzenszwalb et al., 2010] is used. This representation decomposes any object, e.g. a cow, into
its constituent parts: head, torso, legs, which are
shared by other objects in a hierarchical manner.
At each level, image gradient orientations are computed. The relationship between each parts is modeled probabilistically using graphical models where
parts are the nodes and the edges are the conditional
probabilities that relate their spatial compatibility
(Fig. 5(a)). For example, in a cow, the probability
of finding the torso near the head is higher than finding the legs near the head. This model’s intuition lies
in the assumption that objects can be deformed but
the relative position of each constituent parts should
remain the same. We convert the object detection scores to probabilities using Platt’s method [Lin
et al., 2007] which is numerically more stable to obtain Pr (n|I). The parameters of Platt’s method are
obtained by estimating the number of positives and
negatives from the UIUC annotated dataset, from

which we determine the appropriate probabilistic
threshold, which gives us approximately 50% recall
and precision.
For detecting scenes defined in S, we use the
GIST-based scene descriptor of [Torralba et al.,
2003]. GIST computes the windowed 2D Gabor filter responses of an input image. The responses of
Gabor filters (4 scales and 6 orientations) encode the
texture gradients that describe the local properties
of the image. Averaging out these responses over
larger spatial regions gives us a set of global image properties. These high dimensional responses
are then reprojected to a low dimensional space via
PCA, where the number of principal components are
obtained empirically from training scenes. This representation forms the GIST descriptor of an image
(Fig. 5(b)) which is used to train a set of SVM classifiers for each scene class in S. Again, Pr (s|I) is
computed from the SVM scores using [Lin et al.,
2007]. The set of common scenes defined in S is
learned from the UIUC annotated data (sec. 3.3).
3.3

Corpus-Guided Predictions

(sec. 3.1) as a learning corpus that allows us to determine the appropriate target verb set that is amenable
to our problem. We first apply the CLEAR parser
[Choi and Palmer, 2010] to obtain a dependency
parse of these annotations, which also performs
stemming of all the verbs and nouns in the sentence.
Next, we process all the parses to select verbs which
are marked as ROOT and check the existence of a
subject (DEP) and direct object (PMOD, OBJ) that
are linked to the ROOT verb (see Fig. 6(a)). Finally,
after removing common “stop” verbs such as {is,
are, be} we rank these verbs in terms of their occurrences and select the top 50 verbs which accounts
for 87.5% of the sentences in the UIUC dataset to be
in V.
Object class n ∈ N
bus

chair

bicycle

Synonyms, hni
autobus charabanc
double-decker jitney
motorbus motorcoach omnibus
passenger-vehicle schoolbus
trolleybus streetcar ...
highchair chaise daybed
throne rocker armchair
wheelchair seat ladder-back
lawn-chair fauteuil ...
bike wheel cycle velocipede
tandem mountain-bike ...

Table 2: Samples of synonyms for 3 object classes.

Figure 6: (a) Selecting the ROOT verb from the dependency parse ride reveals its subject woman and direct
object bicycle. (b) Selecting the head noun (PMOD)
as the scene street reveals ADV as the preposition on

Predicting Verbs: The key component of our approach is the trained language model Lm that predicts the most likely verb v, associated with the objects Nk detected in the image. Since it is possible that different verbs may be associated with varying number of object arguments, we limit ourselves
to verbs that take on at most two objects (or more
specifically two noun phrase arguments) as a simplifying assumption: Nk = {n1 , n2 } where n2 can
be NULL. That is, n1 and n2 are the subject and
direct objects associated with v ∈ V. Using this assumption, we can construct the set of verbs, V. To
do this, we use human labeled descriptions of the
training images from the UIUC Pascal-VOC dataset
448

Next, we need to explain how n1 and n2 are
selected from the 20 object classes defined previously in N . Just as the 20 object classes are defined visually over several different kinds of specific objects, we expand n1 and n2 in their textual descriptions using synonyms. For example,
the object class n1 =aeroplane should include
the synonyms {plane, jet, fighter jet,
aircraft}, denoted as hn1 i. To do this, we expand each object class using their corresponding
WordNet synsets up to at most three hyponymns levels. Example synonyms for some of the classes are
summarized in Table 2.
We can now compute from the Gigaword corpus [Graff, 2003] the probability that a verb exists given the detected nouns, Pr (v|n1 , n2 ). We do
this by computing the log-likelihood ratio [Dunning,
1993] , λnvn , of trigrams (hn1 i , v, hn2 i), computed
from each sentence in the English Gigaword corpus
[Graff, 2003]. This is done by extracting only the
words in the corpus that are defined in N and V (in-

cluding their synonyms). This forms a reduced corpus sequence from which we obtain our target trigrams. For example, the sentence:

ity that a scene co-occurs with the object and action,
Pr (s|n, v) by:
Pr (n, v|s)Pr (s)
Pr (n, v)
Pr (n|s)Pr (v|s)Pr (s)
=
Pr (n)Pr (v)
∝ Pr (s|n) × Pr (s|v)

Pr (s|n, v) =

the large brown dog chases a small young cat
around the messy room, forcing the cat to run
away towards its owner.

will be reduced to the stemmed sequence dog
chase cat cat run owner2 from which we obtain the target trigram relationships: {dog chase
cat}, {cat run owner} as these trigrams respect the (n1 , v, n2 ) ordering. The log-likelihood ratios, λnvn , computed for all possible (hn1 i , v, hn2 i)
are then normalized to obtain Pr (v|n1 , n2 ). An example of ranked λnvn in Fig. 7(a) shows that λnvn
predicts v that makes sense: with the most likely
predictions near the top of the list.
Predicting Scenes: Just as an action is strongly
related to the objects that participate in it, a
scene can be predicted from the objects and verbs
that occur in the image. For example, detecting Nk ={boat, person} with v={row} would
have predicted the scene s={coast}, since boats
usually occur in water regions. To learn this relationship from the corpus, we use the UIUC dataset
to discover what are the common scenes that should
be included in S. We applied the CLEAR dependency parse [Choi and Palmer, 2010] on the UIUC
data and extracted all the head nouns (PMOD) in
the PP phrases for this purpose and excluded those
nouns with prepositions (marked as ADV) such as
{with, of} which do not co-occur with scenes in
general (see Fig. 6(b)). We then ranked the remaining scenes in terms of their frequency to select the
top 8 scenes used in S.
To improve recall and generalization, we expand
each of the 8 scene classes using their WordNet
synsets hsi (up to a max of three hyponymns levels).
Similar to the procedure of predicting the verbs described above, we compute the log-likelihood ratio
of ordered bigrams, {n, hsi} and {v, hsi}: λns and
λvs , by reducing the corpus sentence to the target
nouns, verbs and scenes defined in N , V and S. The
probabilities Pr (s|n) and Pr (v|n) are then obtained
by normalizing λns and λvs . Under the assumption
that the priors Pr (n) and Pr (v) are independent and
applying Bayes rule, we can compute the probabil2

stemming is done using [Choi and Palmer, 2010]

449

(1)

where the constant of proportionality is justified under the assumption that Pr (s) is equiprobable for all
s. (1) is computed for all nouns in Nk . As shown
in Fig. 7(b), we are able to predict scenes that colocate with reasonable correctness given the nouns
and verbs.
Predicting Prepositions: It is straightforward to
predict the appropriate prepositions associated with
a given scene. When we construct S from the UIUC
annotated data, we simply collect and rank all the associated prepositions (ADV) in the PP phrase of the
dependency parses. We then select the top 12 prepositions used to define P. Using P, we then compute
the log-likelihood ratio of ordered bigrams, {p, hsi}
for prepositions that co-locate with the scene synonyms over the corpus. Normalizing λps yields
Pr (p|s), the probability that a preposition co-locates
with a scene. Examples of ranked λps are shown in
Fig. 7(c). Again, we see that reasonable predictions
of p can be found.

Figure 7: Example of how ranked log-likelihood values
(in descending order) suggest a possible T : (a) λnvn for
n1 = person, n2 = bus predicts v = ride. (b) λns
and λvs for n = bus, v = ride then jointly predicts
s = street and finally (c) λps with s = street predicts p = on.

3.4

Determining T ∗ using HMM inference

Given the computed conditional probabilities:
Pr (n|I) and Pr (s|I) which are observations
from an input test image with the parameters of the trained language model, Lm :

Pr (v|n1 , n2 ), Pr (s|n, v), Pr (p|s), we seek
find the most likely sentence structure T ∗ by:

to

T ∗ = arg max Pr (T |n, v, s, p)
n,v,s,p

= arg max{Pr (n1 |I)Pr (n2 |I)Pr (s|I)×
n,v,s,p

Pr (v|n1 , n2 )Pr (s|n, v)Pr (p|s)} (2)
where the last equality holds by assuming independence between the visual detections and corpus predictions. Obviously a brute force approach to try all
possible combinations to maximize eq. (2) will not
be feasible due to the large number of possible combinations: (20 ∗ 21 ∗ 8) ∗ (50 ∗ 20 ∗ 20) ∗ (8 ∗ 20 ∗ 50) ∗
(12 ∗ 8) ≈ 5 × 1013 . A better solution is needed.

Figure 8: The HMM used for optimizing T . The relevant
transition and emission probabilities are also shown. See
text for more details.

Our proposed strategy is to pose the optimization of T as a dynamic programming problem, akin
to a Hidden Markov Model (HMM) where the hidden states are related to the (simplified) sentence
structure we seek: T = {n1 , n2 , s, v, p}, and the
emissions are related to the observed detections:
{n1 , n2 , s} in the image if they exist. To simplify our notations, as we are concerned with object pairs we will write NN as the hidden states for
all n1 , n2 pairs and nn as the corresponding emissions (detections); and all object+verb pairs as hidden states NV. The hidden states are therefore denoted as: {NN, NV, S, P} with values taken from
their respective word classes from Table 1. The
450

emission states are {nn, s} with binary values: 1
if the detections occur or 0 otherwise. The full
HMM is summarized in Fig. 8. The rationale for
using a HMM is that we can reuse all previous computation of the probabilities at each level to compute the required probabilities at the current level.
From START, we assume all object pair detections
1
are equiprobable: Pr (NN|START) = |N |∗(|N
|+1)
where we have added an additional NULL value for
objects (at most 1). At each NN, the HMM emits
a detection from the image and by independence
we have: Pr (nn|NN) = Pr (n1 |I)Pr (n2 |I). After NN, the HMM transits to the corresponding verb
at state NV with Pr (NV|NN) = Pr (v|n1 , n2 ) obtained from the corpus statistic3 . As no action detections are performed on the image, NV has no emissions. The HMM then transits from NV to S with
Pr (S|NV) = Pr (s|n, v) computed from the corpus
which emits the scene detection score from the image: Pr (s|S) = Pr (s|I). From S, the HMM transits
to P with Pr (P|S) = Pr (p|s) before reaching the
END state.
Comparing the HMM with eq. (2), one can see
that all the corpus and detection probabilities are
accounted for in the transition and emission probabilities respectively. Optimizing T is then equivalent to finding the best (most likely) path through
the HMM given the image observations using the
Viterbi algorithm which can be done in O(105 ) time
which is significantly faster than the naive approach.
We show in Fig. 9 (right-upper) examples of the top
viterbi paths that produce T ∗ for four test images.
Note that the proposed HMM is suitable for generating sentences that contain the core components
defined in T which produces a sentence of the form
NP-VP-PP, which we will show in sec. 4 is sufficient for the task of generating sentences for describing images. For more complex sentences with
more components: such as adjectives or adverbs, the
HMM can be easily extended with similar computations derived from the corpus.
3.5

Sentence Generation

Given the selected sentence structure T
=
{n1 , n2 , v, s, p}, we generate sentences using the
3

each verb, v, in NV will have 2 entries with the same value,
one for each noun.

entire UIUC testing dataset which are scored lower
in our evaluation metrics (sec. 4.1) since they do not
fully describe the image content in terms of the objects and actions.
Some examples of sentences generated using this
strategy are shown in Fig. 9(right-lower).

4 Experiments

Figure 9: Four test images (left) and results. (Rightupper): Sentence structure T ∗ predicted using Viterbi
and (Right-lower): Generated sentences. Words marked
in red are considered to be incorrect predictions. Complete results are available at http://www.umiacs.umd.
edu/˜yzyang/sentence_generateOut.html.

following strategy for each component:
1) We add in appropriate determiners and cardinals: the, an, a, CARD, based on the content
of n1 ,n2 and s. For e.g., if n1 = n2 , we will use
CARD=two, and modify the nouns to be in the plural form. When several possible choices are available, a random choice is made that depends on the
object detection scores: the is preferred when we
are confident of the detections while an, a is preferred otherwise.
2) We predict the most likely preposition inserted
between the verbs and nouns learned from the Gigaword corpus via Pr (p|v, n) during sentence generation. For example, our method will pick the preposition at between verb sit and noun table.
3) The verb v is converted to a form that agrees
with in number with the nouns detected. The
present gerund form is preferred such as eating,
drinking, walking as it conveys that an action is being performed in the image.
4) The sentence structure is therefore of the form:
NP-VP-PP with variations when only one object
or multiple detections of the same objects are detected. A special case is when no objects are detected (below the predefined threshold). No verbs
can be predicted as well. In this case, we simply generate a sentence that describes the scene
only: for e.g. This is a coast, This is
a field. Such sentences account for 20% of the
451

We performed several experiments to evaluate our
proposed approach. The different metrics used for
evaluation and comparison are also presented, followed by a discussion of the experimental results.
4.1

Sentence Generation Results

Three experiments are performed to evaluate the effectiveness of our approach. As a baseline, we simply generated T ∗ directly from images without using
the corpus. There are two variants of this baseline
where we seek to determine if listing all objects in
the image is crucial for scene description. Tb1 is a
baseline that uses all possible objects and scene detected: Tb1 = {n1 , n2 , · · · , nm , s} and our sentence
will be of the form: {Object 1, object 2 and
object 3 are IN the scene.} and we simply
selected IN as the only admissible preposition. For
the second baseline, Tb2 , we limit the number of objects to just any two: Tb2 = {n1 , n2 , s} and the
sentence generated will be of the form {Object
1 and object 2 are IN the scene}. In the
second experiment, we applied the HMM strategy
described above but made all transition probabilities
equiprobable, removing the effects of the corpus,
and producing a sentence structure which we denote
as Teq∗ . The third experiment produces the full T ∗
with transition probabilities learned from the corpus.
All experiments were performed on the 100 unseen
testing images from the UIUC dataset and we used
only the most likely (top) sentence generated for all
evaluation.
We use two evaluation metrics as a measure of the
accuracy of the generated sentences: 1) ROUGE-1
[Lin and Hovy, 2003] precision scores and 2) Relevance and Readability of the generated sentences.
ROUGE-1 is a recall based metric that is commonly
used to measure the effectiveness of text summarization. In this work, the short descriptive sentence of
an image can be viewed as summarizing the image

content and ROUGE-1 is able to capture how well
this sentence can describe the image by comparing it
with the human annotated ground truth of the UIUC
dataset. Due to the short sentences generated, we
did not consider other ROUGE metrics (ROUGE-2,
ROUGE-SU4) which captures fluency and is not an
issue here.
Experiment
∗
Baseline 1, Tb1
∗
Baseline 2, Tb2
HMM no cor∗
pus, Teq
Full HMM, T ∗
Human Annotation

R1 ,(length)
0.35,(8.2)
0.39,(6.8)
0.42,(6.5)

Relevance
2.84 ± 1.40
2.14 ± 1.13
2.44 ± 1.25

Readability
3.64 ± 1.20
3.94 ± 0.91
3.88 ± 1.18

0.44,(6.9)
0.68,(10.1)

2.51 ± 1.30
4.91 ± 0.29

4.10 ± 1.03
4.77 ± 0.42

Table 3: Sentence generation evaluation results with human gold standard. Human R1 scores are averaged over
the 5 sentences using a leave one out procedure. Values
in bold are the top scores.

A main shortcoming of using ROUGE-1 is that
the generated sentences are compared only to a finite set of human labeled ground truth which obviously does not capture all possible sentences that
one can generate. In other words, ROUGE-1 does
not take into account the fact that sentence generation is innately a creative process, and a better recall metric will be to ask humans to judge these
sentences. The second evaluation metric: Relevance and Readability is therefore proposed as an
empirical measure of how much the sentence: 1)
conveys the image content (relevance) in terms of
the objects, actions and scene predicted and 2) is
grammatically correct (readability). We engaged the
services of Amazon Mechanical Turks (AMT) to
judge the generated sentences based on a discrete
scale ranging from 1–5 (low relevance/readability
to high relevance/readability). The averaged results
of ROUGE-1, R1 and mean length of the sentences
with the Relevance+Readability scores for all experiments are summarized in Table 3. For comparison,
we also asked the AMTs to judge the ground truth
sentences as well.
4.2

Discussion

The results reported in Table 3 reveals both the
strengths and some shortcomings of the approach
which we will briefly discuss here. Firstly, the R1
452

scores indicate that based on a purely summarization (unigram-overlap) point of view, the proposed
approach of using the HMM to predict T ∗ achieves
the best results compared to all other approaches
with R1 = 0.44. This means that our sentences are
the closest in agreement with the human annotated
ground truth, correctly predicting the sentence structure components. In addition sentences generated by
T ∗ are also succinct: with an average length of 6.9
words per sentence. However, we are still some way
off the human gold standard since we do not predict
other parts-of-speech such as adjectives and adverbs.
Given this fact, our proposed approach performance
is comparable to other state of the art summarization
work in the literature [Bonnie and Dorr, 2004].
Next, we consider the Relevance+Readability
metrics based on human judges. Interestingly, the
first baseline, Tb1∗ is considered the most relevant description of the image and the least readable at the
same time. This is most likely due to the fact that
this recall oriented strategy will almost certainly describe some objects but the lack of any verb description; and longer sentences that average 8.2 words per
sentence, makes it less readable. It is also possible
that humans tend to penalize less irrelevant objects
compared to missing objects, and further evaluations
are necessary to confirm this. Since Tb2∗ is limited
to two objects just like the proposed HMM, it is a
more suitable baseline for comparison. Clearly, the
results show that adding the HMM to predict the optimal sentence structure increases the relevance of
the produced sentence. Finally, in terms of readability, T ∗ generates the most readable sentences,
and this is achieved by leveraging on the corpus to
guide our predictions of the most reasonable nouns,
verbs, scenes and prepositions that agree with the
detections in the image.

5

Future Work

In this work, we have introduced a computationally
feasible framework that integrates visual perception
together with semantic grounding obtained from a
large textual corpus for the purpose of generating a
descriptive sentence of an image. Experimental results show that our approach produces sentences that
are both relevant and readable. There are, however,
instances where our strategy fails to predict the ap-

propriate verbs or nouns (see Fig. 9). This is due
to the fact that object/scene detections can be wrong
and noise from the corpus itself remains a problem.
Compared to human gold standards, therefore, much
work still remains in terms of detecting these objects
and scenes with high precision. Currently, at most
two object classes are used to generate simple sentences which was shown in the results to have penalized the relevance score of our approach. This can
be addressed by designing more complex HMMs to
handle larger numbers of object and verb classes.
Another interesting direction of future work would
be to detect salient objects, learned from training
image+corpus or eye-movement data, and to verify
if these objects aid in improving the descriptive sentences we generate. Another potential application

Figure 10: Images retrieved from 3 verbal search terms:
ride,sit,fly.

of representing images using T ∗ is that we can easily sort and retrieve images that are similar in terms
of their semantic content. This would enable us to
retrieve, for example, more relevant images given a
verbal search query such as {ride,sit,fly}, returning images where these verbs are found in T ∗ .
Some results of retrieved images based on their verbal components are shown in Fig. 10: many images
with dissimilar visual content are correctly classified
based on their semantic meaning.

453

6 Acknowledgement
This material is based upon work supported by
the National Science Foundation under Grant No.
1035542. In addition, the support of the European Union under the Cognitive Systems program (project POETICON) and the National Science Foundation under the Cyberphysical Systems
Program, is gratefully acknowledged.

References
Berg, T. L., Berg, A. C., Edwards, J., and Forsyth, D. A.
(2004). Who’s in the picture? In NIPS.
Bonnie, D. Z. and Dorr, B. (2004). Bbn/umd at duc-2004:
Topiary. In In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL
2004, pages 112–119.
Choi, J. D. and Palmer, M. (2010). Robust constituentto-dependency conversion for english. In Proceedings
of the 9th International Workshop on Treebanks and
Linguistic Theories, pages 55–66, Tartu, Estonia.
Dunning, T. (1993). Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61–74.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn,
J., and Zisserman, A. (2008). The PASCAL Visual
Object Classes Challenge 2008 (VOC2008) Results.
Farhadi, A., Hejrati, S. M. M., Sadeghi, M. A., Young, P.,
Rashtchian, C., Hockenmaier, J., and Forsyth, D. A.
(2010). Every picture tells a story: Generating sentences from images. In Daniilidis, K., Maragos, P.,
and Paragios, N., editors, ECCV (4), volume 6314
of Lecture Notes in Computer Science, pages 15–29.
Springer.
Felzenszwalb, P. F., Girshick, R. B., and McAllester, D.
(2008). Discriminatively trained deformable part models, release 4. http://people.cs.uchicago.edu/ pff/latentrelease4/.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A.,
and Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627–1645.
Golland, D., Liang, P., and Klein, D. (2010). A gametheoretic approach to generating spatial descriptions.
In Proceedings of EMNLP.
Graff, D. (2003). English gigaword. In Linguistic Data
Consortium, Philadelphia, PA.
Jie, L., Caputo, B., and Ferrari, V. (2009). Who’s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation. In NIPS, editor,
Advances in Neural Information Processing Systems,
NIPS. NIPS.
Kojima, A., Izumi, M., Tamura, T., and Fukunaga, K.
(2000). Generating natural language description of human behavior from video images. In Pattern Recognition, 2000. Proceedings. 15th International Conference on, volume 4, pages 728 –731 vol.4.
Kourtzi, Z. (2004). But still, it moves. Trends in Cognitive Sciences, 8(2):47 – 49.

454

Liang, P., Jordan, M. I., and Klein, D. (2009). Learning
from measurements in exponential families. In International Conference on Machine Learning (ICML).
Lin, C. and Hovy, E. (2003). Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
NAACLHLT.
Lin, H.-T., Lin, C.-J., and Weng, R. C. (2007). A note
on platt’s probabilistic outputs for support vector machines. Mach. Learn., 68:267–276.
Mann, G. S. and Mccallum, A. (2007). Simple, robust,
scalable semi-supervised learning via expectation regularization. In The 24th International Conference on
Machine Learning.
McKeown, K. (2009). Query-focused summarization using text-to-text generation: When information comes
from multilingual sources. In Proceedings of the 2009
Workshop on Language Generation and Summarisation (UCNLG+Sum 2009), page 3, Suntec, Singapore.
Association for Computational Linguistics.
Oliva, A. and Torralba, A. (2001). Modeling the shape
of the scene: A holistic representation of the spatial
envelope. International Journal of Computer Vision,
42(3):145–175.
Schwartz, W., Kembhavi, A., Harwood, D., and Davis,
L. (2009). Human detection using partial least squares
analysis. In International Conference on Computer Vision.
Torralba, A., Murphy, K. P., Freeman, W. T., and Rubin,
M. A. (2003). Context-based vision system for place
and object recognition. In ICCV, pages 273–280. IEEE
Computer Society.
Traum, D., Fleischman, M., and Hovy, E. (2003). Nl generation for virtual humans in a complex social environment. In In Proceedings of he AAAI Spring Symposium
on Natural Language Generation in Spoken and Written Dialogue, pages 151–158.
Urgesi, C., Moro, V., Candidi, M., and Aglioti, S. M.
(2006). Mapping implied body actions in the human
motor system. J Neurosci, 26(30):7942–9.
Yang, W., Wang, Y., and Mori, G. (2010). Recognizing
human actions from still images with latent poses. In
CVPR.
Yao, B. and Fei-Fei, L. (2010). Grouplet: a structured
image representation for recognizing human and object interactions. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, San
Francisco, CA.
Yao, B., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C.
(2010). I2t: Image parsing to text description. Proceedings of the IEEE, 98(8):1485 –1508.

2014 14th IEEE-RAS International Conference on
Humanoid Robots (Humanoids)
November 18-20, 2014. Madrid, Spain

Learning Hand Movements from Markerless Demonstrations for
Humanoid Tasks
Ren Mao1 , Yezhou Yang2 , Cornelia Fermüller3 , Yiannis Aloimonos2 , and John S. Baras1
Abstract— We present a framework for generating trajectories of the hand movement during manipulation actions from
demonstrations so the robot can perform similar actions in
new situations. Our contribution is threefold: 1) we extract and
transform hand movement trajectories using a state-of-the-art
markerless full hand model tracker from Kinect sensor data; 2)
we develop a new bio-inspired trajectory segmentation method
that automatically segments complex movements into action
units, and 3) we develop a generative method to learn task
specific control using Dynamic Movement Primitives (DMPs).
Experiments conducted both on synthetic data and real data
using the Baxter research robot platform validate our approach.

I. I NTRODUCTION

Fig. 1. Overview of learning hand movements for humanoid tasks. Placing
task is an example shown here, and the drawing on hand in demonstration
video is indicating hand tracker.

Developing personalized cognitive robots that help with
everyday tasks is one of the on-going topics in robotics
research. Such robots should have the capability to learn how
to perform new tasks from human demonstrations. However,
even simple tasks, like making a peanut jelly sandwich, may
be realized in thousands of different ways. Therefore, it is
impractical to teach robots by enumerating every possible
task. An intuitive solution is to have a generative model to
enable the robot to perform the task learned from observing a
human. Since the essence of human actions can be captured
by skeletal hand trajectories, and most of the daily tasks
we are concerned with are performed by the hands, learning
new tasks from observing the motion of the human hands
becomes crucial.
There are several previous approaches for learning and
generating hand movements for a robot, but they either use
external markers or special equipments, such as DataGloves,
to capture the example trajectories [4], [15], [3]. Such
approaches are not practical for the kind of actions of daily
living, which we consider here. In this work, our system
makes use of a state-of-the-art markerless hand tracker [16],
which is able to reliably track a 26 degree of freedom
skeletal hand model. Its good performance is largely due to
reliable 3D sensing using the Kinect sensor and a GPU based
optimization. Building on this tool, we propose to develop a
user-friendly system for learning hand movements.
The generation of trajectories from example movements
using data gloves has been a hot topic in the field of
humanoids recently. Krug and Dimitrov [7] addressed the
problem of generalizing the learned model. They showed

that with proper parameter estimation, the robot can automatically adapt the learned models to new situations. Stulp and
Schaal [5] explored the problem of learning grasp trajectories
under uncertainty. They showed that an adaptation to the
direction of approach and the maximum grip aperture could
improve the force-closure performance.
Following the idea that human hand movements are composed of primitives [14], [6], the framework of Dynamic
Movement Primitives (DMPs) has become very popular for
encoding robot trajectories recently. This representation is
robust to perturbations and can generate continuous robot
movements. Pastor et al. [1] further extended the DMPs
model to include capabilities such as obstacle avoidance
and joint limits avoidance. The ability to segment complex
movements into simple action units plays an important role
for the description. With proper segmentation, each action
unit can be well fit into one DMP [19], [21].
This paper proposes an approach for learning the hand
movement from markerless demonstrations for humanoid
robot tasks. Fig. 1 gives an overview of our framework. The
main contributions of the paper are: 1) We demonstrate a
markerless system for learning hand actions from movement
demonstrations. The demonstrations are captured using the
Kinect sensor; 2) Our approach autonomously segments an
example trajectory into multiple action units, each described
by a movement primitive, and forms a task-specific model
with DMPs; 3) We learn a generative model of a human’s
hand task from observations. Similar movements for different
scenarios can be generated, and performed on Baxter Robots.

1 R. Mao and J. Baras are with the Department of Electrical and
Computer Engineering and the ISR, 2 Y. Yang and Y. Aloimonos are with
the Department of Computer Science and the UMIACS, 3 C. Fermüller is
with the UMIACS, University of Maryland, College Park, Maryland 20742,
USA. {neroam, baras} at umd.edu, {yzyang, yiannis} at cs.umd.edu and
fer at umiacs.umd.edu.

978-1-4799-7174-9/14/$31.00 ©2014 IEEE

II. R ELATED W ORK
A variety of methods [17] have been proposed to visually capture human motion. For full pose estimation both
938

appearance-based and model-based methods have been proposed. Appearance-based methods [18] are better suited for
the recognition problem, while model-based methods [16]
are preferred for problems requiring an accurate estimation
pose. To capture hand movement, Oikonomidis et al. [16]
provide a method to recover and track the real world 3D
data from Kinect sensor data using a model-based approach
by minimizing the discrepancy between the 3D structure and
the appearance of hypothesized 3D model instances.
The problem of real-time, goal-directed trajectory generation from a database of demonstration movements has
been studied in many works [8]-[10]. Ude et al. [8] have
shown that utilizing the action targets as a query point in an
example database could generate the learned movement to
new situations. Asfour et al. [9] use Hidden Markov Models
to generalize movements demonstrated to a robot multiple
times. Forte et al. [10] further address the problem of generalization from robots’ learned knowledge to new situations.
They use Gaussian process regression based on multiple
example trajectories to learn task-specific parameters.
Ijspeert et al. [2], [14] have proposed the DMP framework.
They start with a simple dynamical system described by
multiple linear differential equations and transform it into
a weakly nonlinear system. It has many advantages in generating motion: It can easily stop the execution of movement
without tracking time indices as it doesn’t directly rely on
time, and it can generate smooth motion trajectories under
perturbations. In [5], Stulp et al. present an approach to generate motion under state estimation uncertainties. They use
DMP and a reinforcement learning algorithm for reaching
and reshaping. Rather than grasping an object at a specific
pose, the robot will estimate the possibility of grasping based
on the distribution of state estimation uncertainty.
The segmentation of complex movements into a series of
action units has recently received attention due to its importance to many applications in the field of robotics. Meier et
al. [20], [21] develop an expectation maximization method
to estimate partially observed trajectories. They reduce the
movement segmentation problem to a sequential movement
recognition problem. Patel et al. [11] use a Hierachical
Hidden Markov Model to represent and learn complex tasks
by decomposing them into simple action primitives.

the GRASP and RELEASE points to decompose the real
movement into periodical sub-movements. Finally, we train
the model of Dynamical Movement Primitives (DMPs) [15]
to generatively model each sequential movement.

III. A PPROACHES

DMPs [10] are widely used for encoding stereotypical
movements. A DMP consists of a set of differential equations
that compactly represents high dimensional control policies.
As an autonomous representation, they are goal directed and
do not directly depend on time, thus they allow the generation
of similar movements under new situations.
In this paper we use one DMP to describe one segment of
the robot trajectory. The discrete trajectory of each variable,
y, of the robot hand’s Cartesian dimensions, is represented
by the following nonlinear differential equations:

A. Data Acquisition from Markerless Demonstrations
The Kinect FORTH Tracking system [16] has been widely
used as a state-of-the-art markerless hand tracking method
for manipulation actions [23]. The FORTH system takes as
input RGB + depth data from a Kinect sensor. It models
the geometry of the hand and its dynamics using a 26
DOF model, and treats the problem of tracking the skeletal
model of the human hand as an optimization problem using
Particle Swarm Optimization. The hand model parameters
are estimated continuously by minimizing the discrepancy
between the synthesized appearances from the model and
the actual observations.
Unlike most other hand data capturing approaches such
as those using DataGloves, the FORTH system is a fully
markerless approach, which makes it possible to achieve a
natural human-robot interaction in daily life activities, such
as teaching humanoids kitchen actions with bare hands.
In this paper, our humanoid is equipped with the FORTH
system to track the hand. The observed 3D movement
trajectories of the hand, palm, and finger joints are stored
as training data.
B. Pre-processing
Since our goal is to generate human-like hand movement
on humanoids, we first convert the collected data from
Kinect space into Robot space. The robot space is the base
frame which takes the robot body center as origin. Then
we transform the data from absolute trajectories to relative
trajectories with respect to the demonstrator’s body center,
which is fixed during the demonstration. Then we perform a
moving average smoothing on the transformed data to reduce
the noise.
In order to learn movements down to the finger level, we
also compute the distance between the index finger and the
thumb to the DMPs. Without loss of generality, we assumed
the robot gripper would have a fixed orientation which is set
the same as the demonstrator’s.
C. Dynamic Movement Primitives (DMPs) Model

Our hand movement learning method has three steps:
1) acquire trajectories in Cartesian space from demonstration; 2) segment the trajectories using key points and 3)
represent each segment with a generative model. Firstly,
the data collected from observed trajectories of the movements of the palm and the fingertips using the markerless
hand tracker [16] are pre-processed by applying moving
average smoothing to reduce the noise. Next a trajectory
segmentation method is applied to find in a bio-inspired way
the GRASP and RELEASE points that reflect the phases
of movement [22]. Then, because of the complexity of
the hand’s movement when manipulating objects, a second
round of segmentation is applied to the trajectories between
939

τ v̇
τ ẏ

= αv (βv (g − y) − v) + f (x)
= v

(1)
(2)

τ ẋ

= −αx x,

(3)

where (1) and (2) include a transformation system and a
forcing function f , which consists of a set of radial basis
functions, Ψ(x), (equations (4) and (5)), to enable the robot
to follow a given smooth discrete demonstration from the
initial position y0 to the final configuration g. Equation (3)
gives a canonical system to remove explicit time dependency
and x is the phase variable to constrain the multi-dimensional
movement in a set of equations. v is a velocity variable. αx ,
αv , βv and τ are specified parameters to make the system
converge to the unique equilibrium point (v, y, x) = (0, g, 0).
f (x) and Ψ(x) are defined as:
∑N
ωk Ψk (x)
x
f (x) = k=1
(4)
ΣN
k=1 Ψk (x)
Ψk (x) = exp(−hk (x − ck )2 ), hk > 0,
(5)

to run a second round of segmentation on the manipulation
phase. In this phase we segment it at detected key points and
model each segment with a different DMP. The generated
trajectory from these DMPs would best fit the training one.
Next we describe our segmentation algorithm in detail:
1) Grasp & Release Candidates: The first step of our
algorithm is to identify the GRASP and RELEASE points in
the observed trajectories. Given the observed trajectory y, the
velocity v and acceleration v̇ can be computed by deriving
first and second order derivatives followed by a moving
average smoothing. Following the studies on human movement [19], the possible GRASP and RELEASE points are
derived as the minima points in the motion of the palm. We
selected the palm since humans intentionally grasp/release
the objects stably by slowing the hand movement. The
GRASP point occurs after the human closes the hand, and
we find it as the local maxima in the motion of the finger-gap
trajectory. The RELEASE point happens before the human
opens the hand, and it can be found in a similar way. In
this paper, we compute a reference trajectory s(t) for each
Cartesian dimension representing the motion characteristics
as a combination of v and v̇ as s(t) = v(t)2 + v̇(t)2 . We
compute s(t)gap for the finger-gap trajectory. Therefore, for
each dimension, the first local minima of s(t) follows the
first maxima of s(t)gap , and is considered a possible GRASP
point candidate. The last local minima of s(t) succeeds
the last maxima of s(t)gap , and is considered a possible
RELEASE point candidate. We take up to three extrema
for grasping and three for releasing, and put them into the
candidate set Cgrasp and Crelease .
2) Manipulation Segmentation: Given the pair of GRASP
and RELEASE points, we can get the manipulation phase trajectories. We then attempt to segment the manipulation phase
trajectories into subactions. Following the same assumption
that hand movements may change at the local minima of the
velocity and acceleration, we extract the candidates of the
first key points by selecting the first local minima, which
follows the first maxima of s(t) during the manipulation
phase for each Cartesian dimensional trajectory. If there
is no such candidate, our algorithm directly models the
current trajectory’s segment by one DMP and returns the
error between the model-generated and observed trajectories.
If there is one possible key point candidate, we use one
DMP to model the former part of the trajectory segmented
by it and compute the error. Then we recursively apply the
same algorithm for the rest of the trajectories to compute key
points as well as errors. By summing up the errors, we select
the key point with minimal error among all candidates. The
selected key point is added to the key point set. Please refer
to Algorithm 1 for details.
3) Evaluation: We consider the movement segmentation
as
problem with error criterion J(t) =
∑ a minimization
i
i
2
(y(t)
−
y(t)
generated ) . It sums up the errors over
i=1,2,3
all dimensions of the trajectories. For each possible pair of
GRASP and RELEASE points (tgrasp ∈ Cgrasp , trelease ∈
Crelease ), we first use two separate DMPs to model the reach
and withdraw phase trajectories and compute their errors as

where ck and hk are the intrinsic parameters of the radial
basis functions distributed along the training trajectory.
1) Learning from observed trajectory: The parameters ωk
in (4) are adapted through a learning process such that the
nonlinear function f (x) forces the transformation system to
follow the observed trajectory y(t). To update the parameters,
the derivatives v(t) and v̇(t) are computed for each time
step. Based on that, the phase variable x(t) is evaluated by
integrating the canonical system in (3). Then, ftarget (x) is
computed according to (1), where y0 is the initial point and g
is the end point of the training trajectory. Finally, the parameters ωk are computed by linear regression
as a minimization
∑
problem with error criterion J = x (ftarget (x) − f (x))2 .
2) Movement generation: To generate a desired trajectory,
we set up the system at the beginning. The unique equilibrium point condition (v, y, x) = (0, g, 0) is not appropriate
here since it won’t be reached until the system converges
to a final state. The start position is set to be the current
position y0′ , the goal is set to be the target position gtarget ,
and the canonical system is reset by assigning the phase
variable x = 1. By substituting the learned parameters ωk
and adapting the desired movement duration τ , the desired
trajectory is obtained via evaluating x(t), computing f (x),
and integrating the transformation system (1).
D. Movement Segmentation
In human movement learning, a complex action is commonly segmented into simple action units. This is realistic since demonstrations performed by humans can be
decomposed into multiple different movement primitives.
Specifically for most common human hand movements, it is
reasonable to assume that the observed trajectory generally
has three subaction units: 1) A reach phase, during which the
hand moves from a start location till it comes in contact with
the object, just before the grasp action; 2) A manipulation
phase, during which the hand conducts the manipulation
movement on the object; 3) A withdraw phase, which is the
movement after the point of releasing the object.
In both the reach and the withdraw phases, the movements
usually can be modelled well by one DMP. However, the
manipulation movement could be too complicated to model
it with only one or two DMPs. Therefore, our approach is
940

∑tgrasp
∑end
Jreach =
J(t) and Jwithdraw =
t=1
t=trelease J(t).
Given the manipulation phase trajectory, we segment it
further as described above in order to model complex movement, for example chopping. The error for the manipulation
phase trajectory (Jmanipulation ) is then computed. The total
error (Jwhole = Jreach + Jwithdraw + Jmanipulation ) is
used as the target function. The final GRASP and RELEASE points are obtained by solving (t∗grasp , t∗release ) =
arg min Jwhole .

Motion Reference Trajectory s(t)
Local Minimas
Local Maximas

−5

10
Time (s)

15

20

5

10
Time (s)

15

20

0
−1
0

0
0

z (m)

z

1

0

Finger−gap (m)

20

5

10
Time (s)

15

20

5

10
Time (s)

15

20

5

10
Time (s)

15

20

x 10

0
0

5

10
Time (s)

15

10
Time (s)

15

20

0.2
0.1
0
0

15

−4

0.2
−0.2
0

10
Time (s)

x 10

1

0.5

5

20

−5

Finger−gap

y (m)

2

5

−0.5

Preprocess

5
−4

0.6
0.4
0

x 10

2
0
0

0.8

y

x (m)

x

4

4

x 10

2
0
0

(a)

(b)

Fig. 2. Data acquisition and preprocessing for chopping task: (a) Transform
action of Kinect sensor data to trajectories in robot space; (b) Computed
motion reference trajectories s(t) for Cartesian dimensions and finger-gap.

Algorithm 1 Manipulation Phase Segmentation
Input: tstart , tend
Output: Keys,Jerror
procedure S EGMENT
Keysc = ∅, Keys = ∅
for all Cartesian dimension i ∈ (1, 2, 3) do
Setmin ← F INDMINS(s(t)i , tstart , tend )
Setmax ← F INDMAXS(s(t)i , tstart , tend )
if ∃tc ∈ Setmin > Setmax (0) then
Keysc ← Keysc + smallest tc
end if
end for
Jerror ← F ITDMP(y(t), tstart , tend )
if Keysc = ∅ then return Keys, Jerror
end if
for all tc ∈ Keysc do
Jf ormer ← F ITDMP(y(t), tstart , tc )
Keyslatter , Jlatter ← S EGMENT(tc , tend )
if Jerror > Jf ormer + Jlatter then
Jerror ← Jf ormer + Jlatter
Keys ← tc + Keyslatter
end if
end for
return Keys, Jerror
end procedure

Step 1) Generate new key points’ locations during the
movement (⃗y (ti )′ , i = 0, 1, · · · , n + 1). Taking the learned
relative motion vectors, we compute locations of new key
points as ⃗y (ti )′ = ⃗y (ti−1 )′ + M⃗V i , i = 1, · · · , n, where
⃗y (t0 )′ is the new grasping location and ⃗y (tn+1 )′ is the new
releasing location for different scenarios.
Step 2) Scale the duration time of each segment based on
the new total time/speed. Since we have a key points set in
the learned model, the new duration time for each segment
in the manipulation phase τi , i = 0, · · · , n can be computed.
Step 3) Use learned DMPs to generate each of the segments accordingly. The reach and withdraw phases are generated directly with the test inputs, while the segments in the
manipulation phase are generated according to inputs computed from the above steps. For example, (⃗y (ti−1 )′ , ⃗y (ti )′ , τi′ )
would be used as input to the ith DMP for generating the
ith segment trajectory in the manipulation phase.
We then concatenate the generated trajectories into the new
movement trajectory ⃗y (t)′ , which is then used to control the
robot effector. At the same time, we also enforce the learned
grasping finger-gap on the robot’s parallel gripper during the
manipulation phase.

E. Generative Model for Hand Movement
After we have found the best GRASP and RELEASE
points along with the key points set (t1 , t2 , · · · , tn ) during
the manipulation phase, our system now is able to model the
hand movement by:
1) DMPs: Including two DMPs for the reach and withdraw phases and a set of DMPs for each segment in the
manipulation phase, yielding n + 3 DMPs.
2) Key Points Set: A series of best key points
(t0 , t1 , t2 , · · · , tn+1 )
for
movement
segmentation
and their corresponding relative motion vectors
(M⃗V 1 , M⃗V 2 , · · · , M⃗V n ). The relative motion vectors
are computed as M⃗V i = ⃗y (ti ) − ⃗y (ti−1 ), i = 1, · · · , n,
where t0 = tgrasp , tn+1 = trelease . Note that the relative
motion vectors from tn to tn+1 are abundant for our model.
3) Grasping Finger-gap: Given the best GRASP and
RELEASE points, we compute the average of the finger-gaps
during the manipulation phase for representing the distance
a parallel gripper should generate for the same object.

IV. E XPERIMENTS
This section describes experiments conducted to demonstrate that our system can learn from markerless demonstrations and generate similar actions in new situations. We first
had our robot observe demonstrations. The object was placed
on a table, and a human was asked to move his right hand to
grasp the object, manipulate it, then release it and withdraw
his hand. Three typical tasks are considered: Place, Chop
and Saw. In order to validate our method, for each task we
collected two sequences. One was used for learning and the
other was used for testing. The movement was tracked by
the FORTH system [16] at 30 fps and the raw data was
transformed into robot space, as shown in Fig. 2(a).
A. DMPs Model training
We calculated the motion reference trajectories s(t), found
the local minima and maxima (Sec. III), as shown in
Fig. 2(b). Applying the learning algorithms by fixing the
number of basis functions to 30 in each DMP model, our
system generated trajectories (Fig. 3). The learned fingergap for grasping and the error for the whole trajectories are
also reported in Table I.

F. Trajectory Generation
Given the testing inputs: the initial locations of the robots
palm, the new locations of the object to grasp and release,
and the expected movement time, our generative model
generates the motion trajectories using the following 3 steps:
941

20

0
0

−1
0

5

Time (s)

10

−1
0

0.5

5

10
Time (s)

15

−1
0

5

Time (s)

10

0
−0.2
0

15

10
Time (s)

15

20

5

5

(a)

10
Time (s)

15

5

10
Time (s)

15

x (m)

x (m)

5

10
Time (s)

15

0.6
0.4
0

20

5

10
Time (s)

15

−0.5
−1
0

20

5

10
Time (s)

15

10
Time (s)

15

20

5

10
15
20
Time (s)
Key Points
Generated Trajectory
Observed Trajectory

5

10
Time (s)

0.2

0
−0.2
0

10
Time (s)

15

20

0
−1
0

20

0.2

5

5

1

0

(a)

Fig. 3. Generated trajectories for learning different hand tasks: a) Place,
b) Chop, c) Saw.

5

10
Time (s)

15

20

(b)

0
−0.2
0

15

20

(c)

Fig. 5. Generated trajectories with different scenarios for chopping task:
a) object is shifted 5 cm in x direction, b) object is shifted 10 cm in y
direction, c) object is shifted 20 cm in both x and y directions.

TABLE I
H AND MOVEMENT LEARNING FOR DIFFERENT TASKS
Chop
0.0878
0.3538

0.4
0

20

0
−0.2
0

20

(c)

Place
0.0753
0.3887

15

0.2

(b)

Task
Grasp finger-gap (m)
Trajectory error (m2 )

10
Time (s)

−0.5
−1
0

20

−0.1
−0.2
0

20

5

0

0
z (m)

z (m)

0

15

−0.5

20

0.2

−0.5
0

10
Time (s)

0

−0.5

15

5

0.4
0

0.6

y (m)

15

y (m)

−0.5

z (m)

10
Time (s)

0
y (m)

y (m)

0

5

y (m)

0.4
0

15

0.8

0.8

0.6

z (m)

Time (s)

10

y (m)

5

0.8

0.5

z (m)

0.4
0

x (m)

x (m)

x (m)

1

0.6

z (m)

Observed Trajectory
Key Points
Generated Trajectory

0.8

0.6

x (m)

0.8

Saw
0.0736
0.5848

B. Experiments in Simulation
We show how well our approach is able to generalize
movement trajectories for different actions by comparing
with the testing sequences. For the testing sequence, we
applied the same pre-processing to transform it into robot
space. We also extracted the grasping and releasing locations,
as well as their duration times. We passed them as parameters
to the trained model. The trajectories generated are shown
in Fig. 4.
The motion patterns generated by different humans for the
same action largely differ from each other. After comparing
the generated trajectories with the observed trajectories of the
testing sequences of different tasks, we found that in general
their motion patterns are quite similar. Even for relatively
complex tasks for example chop, our generated trajectories
are similar to the observed human trajectories. This shows
that our proposed model is good for learning and generating
hand movements for manipulation tasks.
We further tested our trained model by generating trajectories for different grasping and releasing locations. We offset
the grasping and releasing locations by 5, 10 and 20 cm
on the table away from the location of the demonstration.
The generated trajectories for the Chop task are shown in
Fig. 5. The figure shows that the motion trajectories are still
consistent and the generated movements are still quite similar
to the ones from the demonstrations. Our approach achieves
a certain level of spatial generality while maintaining humanlike trajectories.

2

4
6
Time (s)

8

0.4
0

10

−0.5

2

4
6
Time (s)

8

20

0

2

4
6
Time (s)

(a)

8

10

D. Grammar Induction for Hand Task
A study by [25] suggested that a minimalist generative
grammar, similar to the one in human language, also exists
for action understanding and execution. In this experiment,
we demonstrated the applicability of our generative model
in grammar induction for hand tasks.
With learned DMPs as primitives, we induced a contextfree action grammar for the task as follows. Firstly, we
concatenated the learned parameters from the different dimensions of the DMPs into feature vectors and applied PCA
to transform these vectors into a lower dimensional space.
Then we applied K-means clustering with multiple repetitions to cluster DMPs into groups. Besides the two groups
of DMPs for Reach and Withdraw phases, we considered
two other groups of DMPs for stretching and contraction in
the Manipulation phase.
The labelled data from two trails of the chopping task in
PCA space are shown in Fig. 7(a). Based on clustering labels,
we could label each DMP and generate the primitive labels
for the observed task. For example, the Chop task in Fig. 7(a)
can be represented by the sequence of primitives: “Reach
Chop1 Chop2 Chop1 Chop2 Chop1 Chop2 Chop1 Chop2
Withdraw”. Similar sequences can be found in other Chop
trails. After applying the grammar induction technique [26]

5

Time (s)

10

15

10

15

10

15

0

5

10
Time (s)

15

−0.5
−1
0

20

5

Time (s)

0

0
−0.2
0

In this experiment, we showed that our approach can
be used to teach the Baxter robot to perform a similar
task from demonstrations using the FORTH hand tracking
data. We mounted a Kinect sensor on our Baxter. Given
the object location, using our method, we could generate
the hand movement trajectories and use them to control
Baxter’s gripper movement. Fig. 6 shows the front view and
3D trajectory of the generated movement for the chopping
task running on Baxter.

0.5
0
0

0.2
z (m)

z (m)

15

−0.5
−1
0

10

0.5

−0.5
0

10
Time (s)

y (m)

y (m)

y (m)

0

−1
0

5

0

z (m)

0.4
0

0.6

x (m)

x (m)

x (m)

1

0.6

C. Test on the Robot

Key Points
Generated Trajectory
Observed Trajectory

0.8

0.8

Fig. 6. Baxter Experiment: Front view and 3D trajectory of generated
movement for chopping task.

5

10
Time (s)

(b)

15

20

−0.1
−0.2
0

5

Time (s)

(c)

Fig. 4. Comparison between generated trajectories and observed testing
trajectories for different hand tasks: a) Place, b) Chop, c) Saw.

942

semantics) and develop a method to learn action gram
for hand movements based on action units segmented
presented framework.
3) In this paper, in order to focus on the traj
generation problem, we assumed the object location as
Fig.(a)
7. DMPs
clusters in
2D PCA spacein
for Chop
DMPs
clusters
2Dtask
from perception. Currently, we investigate how to int
PCA space.TABLE II
with additional information about objects, such as
G
C
.
S → Reach A W ithdraw (1)
affordances. The modules evaluating object affordance
A → A Chop1 Chop2
| Chop1 Chop2
(2)
the graspable parts of daily kitchen and workshop
[5] F. Stulp, E.R EFERENCES
Theodorou,
Buchli,
and S. Schaal,
Learning[25].
to grasp
using a J.deep
learning
mechanism
This will enab
(a)
(b) Grammar(b)rules induced
under
IEEE
[1] P. Pastor
and H.uncertainty.
Hoffmann and T. Asfour
and S.International
Shchaal, Learnning Conference on Robotics and
With learnt DMPs as primitives, we induced a context-free
and generalization
of motor
skills by learning fromto
demonstration.
humanoid
know not only how to grasp, but also wh
from observed
action grammar
for the task asmovement.
follows. Firstly, we concateAutomation,
2011.
IEEE International Conference Robotics and Automation, 2009.
Fig. 7. Grammar induction fornated
chopping
task: (a)
clustersof in
2Das [2][6]
learnt parameters
fromDMPs
each dimension
DMPs
A.J. Ijspeert
and J. Nakanishi
and
S. Schaal, Learning
rhythmic A.
move-Ude, and D. Kragic, Learning
V. Kruger,
D.grasp.
Herzog,
S. Baby,
ments by demonstration using nonlinear oscillators. IEEE International
feature
vectors
andand
applied
Principle
Component
Analysis
PCA space; (b) Grammar
induced
from
observed
movement.
Fig. 7. rules
DMPs
clustering
Grammar
Induction
forto chopping
task
RAMMAR RULES INDUCED FROM OBSERVED

[23] show that they generally follow a grammatical, recursive
structure. We would like to further investigate the possibility of combining bottom-up (the trajectory segmentation
algorithms presented here) with top-down processing (action
semantics) and develop a method to learn action grammars
for hand movements based on action units segmented by the
presented framework.
3) In S.
thisCalinon,
paper, in F.
order
to focus on
the Sauser,
trajectory D.G. Caldwell, and A. Billard,
[3]
D’halluin,
E.L.
generation problem, we assumed the object location as input
Learning
and
reproduction
of
gestures
from perception. Currently, we investigate how to integrateby imitation: an approach based
on hidden
Markov
and asGaussian
mixture regression. IEEE
with additional
information
about model
objects, such
their
affordances.
The modules
object affordance
detect
Robotics
andevaluating
Automation
Magazine
7(2), pp. 44-45, 2010.
the graspable parts of daily kitchen and workshop tools
[4] F. Stulp, and S. Schaal, Hierarchical reinforcement learning with
using a deep learning mechanism [25]. This will enable our
learning
movement
primitives.
IEEE
humanoid
to know not
only how to grasp,
but also where
to International Conference on
grasp. Humanoid Robots, 2011.

HOP TASK

actions from observations. IEEE Robotics and Automation Magazine
17(2), pp. 30-43, 2010.
[7] R. Krug, and D. Dimitrov, Representing Movement
Primitives as
R EFERENCES
on the sequences of the primitives, we induced a set of
Implicit Dynamical Systems learned from Multiple Demonstrations.
context-free grammar
rules,
as shown
in table.we
7(b).
S is a context-free
With learnt
DMPs
as primitives,
induced
Interntational Conference on Advanced Robotics (ICAR), 2013.
[1] T.P.Asfour,
Pastor and
andJ.H.
Hoffmann
and T. Asfour
and S. Shchaal, Le
the starting action
non-terminal.
Thisforaction
grammar
enables Firstly,
us to we
[8]concateA. Ude, A. Gams,
Morimoto,
Task-specific
generalgrammar
the task
as follows.
andperiodic
generalization
of
motor skills
by learning
from demon
ization
of
discrete
and
dynamic
movement
primitives.
IEEE
produce generatively
newparameters
Chop actionsfrom
and iteach
shows
that our of DMPs as
nated learnt
dimension
IEEE and
International
Conference
Robotics and Automation, 200
Transactions on Robotics
Automation,
2010.
generative model
well suited
a basis Principle
for furtherComponent
research
[2] A.J.
andR.J.Dillmann,
NakanishiImitation
and S. Schaal,
Learning rhythmi
[9] T. Asfour,
P. Ijspeert
Azad, and
Learning
featureis vectors
andasapplied
Analysis
to F. Gyarfas,
on learningtransform
hand actions
guided
by
semantic
principles.
ments byTasks
demonstration
using
nonlinear
oscillators. IEEE Inter
of Dual-Arm Manipulation
in Humanoid
Robots.
IEEE/RAS
these vectors into a lower dimensional space.
Then
International Conference
on Humanoid
Robots
(Humanoids),
pp. 40-2002.
Conference
Intelligent
Robots
and Systems,
V.weC applied K-meansF clustering
W with multiple repetitions
47, December, 2006.
[3] S. Calinon and F. D’halluin and E.L. Sauser and D.G. C
D. Forte,
Morimoto,
and A.Learning
Ude, On-line
synthesis
A. Billard,
and motion
reproduction
of gestures by im
to cluster
DMPs for
intolearning
groups.hand
Besides
the two[10]
groups
ofA. Gams, J.and
We presented
a framework
movement
and adaptation usingan
a trajectory
database.
Robotics
and
Autonomous
approach
based
on
hidden
Markov
model
and Gaussian
DMPs forfor
Reach
and Withdraw
phases,
we considered
two2012.
from demonstrations
humanoids.
The proposed
method
Systems,
regression. IEEE Robotics and Automation Magazine 7(2), pp
[11]
M.
Patel,
C.H.
Ek,
N.
Kyriazis,
A.
Argyros,
J.V.
Miro,
and
D.
Kragic,
groupsfully
of DMPs
for way
stretching
andhand
contraction in the
provides a other
potentially
automatic
to learn
2010.
Language for learning complex human-object interactions. In Proc.
movements Manipulation
for humanoid phase.
robots from demonstrations, and
[4]
F. Stulp
and S. on
Schaal,
Hierarchical
reinforcement learnin
of the IEEE International
Conference
Robotics
and Automation
V. C ONCLUSION AND F UTURE W ORK
learning movement primitives. IEEE International Confere
it does not require
special hand
motion
capturing
devices.
(ICRA),
2013.
The labelled
data
from
two
trails
of
the
chopping
task
We presented a framework for learning hand movement
Humanoid
Robots, 2011.
C. Smith, Y. Karayiannidis,
L. Nalpantidis,
X. Gratal, P. Qi, D.V. Di1) Due in
to PCA
the limitation
ofshown
Baxter’s
effector,
we
can
for
The proposed
method on[12]
spacefrom
isdemonstration
inhumanoids.
Fig.
7(a).
Based
clustering
F. StulpDual
andarm
E. Theodorou
andsurvey,
J. Buchli
and S. Schaal, Lea
marogonas, and[5]
D. Kragic,
manipulationa
Robotics
provides a potentially fully automatic way to learn hand
only map finger-level
parallel
gripper
byand the primitive
movements
foronto
humanoid
robots from
demonstration,
labels, we movements
could
label
eacha DMP
and
generate
grasp under
and Autonomous Systems,
2012. uncertainty. IEEE International Conference on R
it does
notdistance
require specialbetween
hand motion capturing
devices.
transfering labels
the orientation
and
the
thumb
[13] R. Dillmann, Teaching
learning of 2011.
robot tasks via observation of
andand
Automation,
for the observed
example,
1) Due to the task.
limitation For
of Baxter’s
effector, wethe
can Chop task in
human performance.
pp.A. Ude and D.
[6] V.Robotics
Krugerand
andAutonomous
D. Herzog Systems
and S. 47(2-3),
Baby and
only
map finger-level
movements
onto a to
parallel
gripper by
and the index
finger.
In
future
work
we
want
further
Fig. 7(a) can transfering
be represented
bydistance
thebetween
sequence
the orientation and
the thumb of primitives:
109-116, 2004.
Learning
actions
from
observations.
IEEE
Robotics and Aut
investigate the eligibility of
using
model
to tomap
the indexour
finger.current
In future work
we want
further
[14] A.
Ijspeert, J. Nakanishi,
and 17(2),
S. Schaal,
Movement
imitation with
“Reach Chop1and
Chop2
Chop1
Chop2
Chop1
Chop2
Chop1
Magazine
pp. 30-43,
2010.
investigate the eligibility of using our current model to map
finger-level movements onto
robot
hands with
fingers.fingers.
nonlinear dynamical
systems
humanoid
robots. In Proc.
of the IEEE
finger-levelSimilar
movements onto
robot hands withcan
[7] R.
Krugin and
D. Dimitrov,
Representing
Movement Primi
Chop2 Withdraw”.
sequences
be[13],found in
other Conference
International
on Robotics and Automation (ICRA), 2002.
2) Recent
studies on human manipulation
methods
2) Recent studies on human
manipulation
methods
[13],
Implicit Dynamical Systems learned from Multiple Demons
[15]
A.
Ijspeert,
J.
Nakanishi,
P.
Pastor,
H.
Hoffmann,
and
S.
Schaal,
Chop
Interntational
Conference
on Advanced
(ICAR), 201
[22] show that
theytrails.
generally follow a grammatical, recursive
Dynamical movement
primitives: Learning
attractor
models for Robotics
motor
After
the grammar
induction
technique [27]
on
[8]Computation,
A. Ude andvol.
A. 25,
Gams
T. Asfour
structure. We wouldapplying
like to further
investigate
the possibehaviors. Neural
pp. and
328-373,
2013. and J. Morimoto, Taskgeneralization
discrete Efficient
and periodic
dynamic movement pri
the sequences
of the
we can induce
of
[16]a I.set
Oikonomidis,
N. Kyriazis,
and A.ofArgyros,
model-based
bility of combining
bottom-up
(the primitives,
trajectory segmentation
IEEE Transactions
on Robotics
and Automation,
2010.
3D tracking of hand articulations
using Kinect.
British Machine
Vision
context-free
grammar
rules
in
table.
??.
S
is
the
starting
algorithms presented here) with top-down processing (action
[9] T. Asfour and R. Dillmann, Human-like Motion of a Hu
Conference, 2011.
This action
grammar
enables us [17]
to produce
semantics) non-terminal.
and develop a method
to learn
action grammars
A. Erol, G. Bebis, M.Robot
Nicolescu,
Boyle,
and X. Twombly,
VisionArm R.D.
Based
on Closed-Form
Solution
of the Inverse Kin
Hand Pose Estimation:
revview. Computer
Vision
and Imageon Intelligent Ro
Problem.AIEEE/RSJ
International
Conference
generatively
new Chop
actions
andframework.
it shows that ourbased
generbased on action
units segmented
by the
presented
Understanding, 108(1-2):52-73,
2007. pp. 407-1412, 2003.
Systems (IROS),
3) In this
paper,
in is
order
focus asonbasis
the trajectory
ative
model
welltosuited
for further research
on A. Fitzgibbon,
[18] J. Shotton,
M. F.
Cook,
T. Sharpa,
M. and
Finocchio,
R.
[10] T. Asfour,
Gyarfas,
P. Azad
R. Dillmann,
Imitation L
generation problem,
assumed
object by
location
as input
A. Blake, Real-Time
HumanTasks
Pose Recognition
learning we
hand
actionsthegudied
semantic
principles.Moore, A. Kipman, and
of Dual-Arm
Manipulation
in Humanoid Robots. IEE
in Parts from Single Depth Images. Conference on Computer Vision
from perception. Currently, we investigate how to integrate
International Conference on Humanoid Robots (Humanoids),
and Pattern Recognition, 2011.
V.
C
ONCLUSION
AND
F
UTURE
W
ORK
47, December,
2006.
additional information about objects, such as their affor- [19] D. Weinland, R. Ronfard,
and E. Boyer, A survey of vision-based
[11] representation,
D. Forte andsegmentation
A. Gams and
Morimoto Comand A. Ude, On-line
dances. The modules
evaluating
affordances
detect the
methods for action
andJ.recognition.
We presented
a object
framework
for learning
hand movement
synthesis
and adaptation
using2011.
a trajectory database. Robo
puter
Vision
and
Image
Understanding,
pp.
224-241,
graspable parts
daily kitchen and
tools
fromofdemonstration
for workshop
humanoids.
Theusing
proposed
method
Autonomous
Systems,
2012.
[20] F. Meier, E. Theodorou,
F. Stulp, and
S. Schaal,
Movement Segmentadifferent learning
mechanisms
[24].fully
This automatic
will enableway
our to learntionhand
provides
a potentially
[12] C.Library.
Smith,Intelligent
Y. Karayiannidis,
L. Nalpantidis,
using a Primitive
Robots and Systems
(IROS), X. Gratal, P.
humanoid to
know not only
but alsorobots
where from
to grasp.
2011.and
V. Dimarogonas, and D. Kragic, Dual arm manipulationa
movements
for how
humanoid
demonstration,
[21] F. Meier, E. Theodorou,
and S.and
Schaal,
MovementSystems,
Segmentation
Robotics
Autonomous
2012.and
it does
not require special hand motion capturing devices.
ACKNOWLEDGMENT
Recognition for
Imitation
Learning.Teaching
JMLR, 2012.
[13]
R. Dillmann,
and learning of robot tasks via observ
[22] we
J. Flanagan,
Action plans Robotics
used in action
1) Duesupported
to the limitation
Baxter’s
effector,
can and R. Johansson,
human performance.
and observation.
Autonomous Systems 47(2
Research partially
by DARPAof(through
ARO)
Nature, 424.6950: 769-771,
109-116,2003.
2004.
only
map
finger-level
movements
onto
a
parallel
gripper
by
grant W911NF1410384, by National Science Foundation [23] C. Keskin, K.[14]
Furkan, Y.E. Kara, and L. Akarun, Real time hand
A. Ijspeert and J. Nakanishi and S. Schaal, Movement imitati
estimation using depth sensors. Consumer Depth Cameras for
transfering
the orientation
and distanceandbetween
the pose
thumb
(NSF) grants
CNS-1035655
and SMA-1248056,
by
systems in humanoid robots. In Proc. of th
Computer Vision, pp.nonlinear
119-137,dynamical
2013.
and the
index finger.
In future (NIST)
work we
want[24]
to A.
further
International
Conference
on Robotics
and Automation (ICRA
National Institute
of Standards
and Technology
grant
Myers, A. Kanazawa,
C. Fermuller,
and Y. Aloimonos,
Affordance
Ijspeert Features.
and J. Nakanishi
P. Pastor
and H. Hoffmann
investigate the eligibility of using our current model of
toObject
map Parts[15]
70NANB11H148.
fromA.
Geometric
Workshopand
on Vision
meets
Schaal, Dynamical movement primitives: Learning attractor
Cognition,
CVPR
2014.
finger-level movements onto robot hands with fingers.
foronmotor
behaviors.
vol. 25, pp. 328-37
[25] N. Chomsky, Lectures
government
and Neural
binding:Computation,
The Pisa lectures.
R EFERENCES
2) Recent studies on human manipulation methods
No.[13],
9. Walter [16]
de Gruyter,
1993.
I. Oikonomidis
and N. Kyriazis and A. Argyros, Efficient mod
[1] P. Pastor, H. Hoffmann, T. Asfour, and S. Shchaal, Learnning and
[26]recursive
C.G. Nevill-Manning,
I.H. Witten,
Identifying
Hierarchical
Struc- British Machin
3Dand
tracking
of hand
articulations
using Kinect.
[23] show that they generally follow a grammatical,
generalization of motor skills by learning from demonstration. IEEE
ture in Sequences: Conference,
A linear-time
algorithm. Journal of Artificial
2011.
structure.
WeRobotics
would
to further
possi- Research 7: 67-82, 1997.
International
Conference
andlike
Automation,
2009. investigate theIntelligence
[17] A. Erol and G. Bebis and M. Nicolescu and R.D. Boyle
[2] A. Ijspeert,
J. Nakanishi,
and S. Schaal,
Learning rhythmic
movements segmentation
bility
of combining
bottom-up
(the trajectory
Twombly, Vision-based Hand Pose Estimation: A revview. C
by demonstration using nonlinear oscillators. IEEE International Conalgorithms
presented
here)
with top-down processing (action
Vision and Image Understanding, 108(1-2):52-73, 2007.
ference Intelligent
Robots
and Systems,
2002.
transform these vectors into a lower dimensional space. Then
we applied K-means clustering with multiple repetitions
to cluster DMPs into groups. Besides the two groups of
DMPs for Reach and Withdraw phases, we considered two
other groups of DMPs for stretching and contraction in the
Manipulation phase.
The labelled data from two trails of the chopping task in
PCA space is shown in Fig. 7. Based on clustering labels,
we could label each DMP and generate the primitive labels
for the observed task. For example, the Chop task in Fig. 7
can be represented by the sequence of primitives: “Reach
Chop1 Chop2 Chop1 Chop2 Chop1 Chop2 Chop1 Chop2
Withdraw”. Similar sequences can be found in other Chop
trails.
ONCLUSION
AND UTURE
ORK
After applying the grammar induction technique [27] on
the sequences of the primitives, we can induce a set of
context-free grammar rules in table. II. S is the starting nonterminal. This action grammar enables us to produce generatively new Chop actions and it shows that our generative
model is well suited as basis for further research on learning
hand actions gudied by semantic principles.

Conference Intelligent Robots and Systems, 2002.
[3] S. Calinon and F. D’halluin and E.L. Sauser and D.G. Caldwell
and A. Billard, Learning and reproduction of gestures by imitation:
an approach based on hidden Markov model and Gaussian mixture
regression. IEEE Robotics and Automation Magazine 7(2), pp. 44-45,
2010.
[4] F. Stulp and S. Schaal, Hierarchical reinforcement learning with
learning movement primitives. IEEE International Conference on
Humanoid Robots, 2011.
[5] F. Stulp and E. Theodorou and J. Buchli and S. Schaal, Learning to
grasp under uncertainty. IEEE International Conference on Robotics
and Automation, 2011.
[6] V. Kruger and D. Herzog and S. Baby and A. Ude and D. Kragic,
Learning actions from observations. IEEE Robotics and Automation
Magazine 17(2), pp. 30-43, 2010.
[7] R. Krug and D. Dimitrov, Representing Movement Primitives as
Implicit Dynamical Systems learned from Multiple Demonstrations.
Interntational Conference on Advanced Robotics (ICAR), 2013.
[8] A. Ude and A. Gams and T. Asfour and J. Morimoto, Task-specific
generalization of discrete and periodic dynamic movement primitives.
IEEE Transactions on Robotics and Automation, 2010.
[9] T. Asfour and R. Dillmann, Human-like Motion of a Humanoid
Robot Arm Based on Closed-Form Solution of the Inverse Kinematics
Problem. IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pp. 407-1412, 2003.
[10] T. Asfour, F. Gyarfas, P. Azad and R. Dillmann, Imitation Learning
of Dual-Arm Manipulation Tasks in Humanoid Robots. IEEE/RAS
International Conference on Humanoid Robots (Humanoids), pp. 4047, December, 2006.
[11] D. Forte and A. Gams and J. Morimoto and A. Ude, On-line motion
synthesis and adaptation using a trajectory database. Robotics and
Autonomous Systems, 2012.
[12] C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D.
V. Dimarogonas, and D. Kragic, Dual arm manipulationa survey,
Robotics and Autonomous Systems, 2012.
[13] R. Dillmann, Teaching and learning of robot tasks via observation of
human performance. Robotics and Autonomous Systems 47(2-3), pp.
109-116, 2004.
[14] A. Ijspeert and J. Nakanishi and S. Schaal, Movement imitation with
nonlinear dynamical systems in humanoid robots. In Proc. of the IEEE
International Conference on Robotics and Automation (ICRA), 2002.
[15] A. Ijspeert and J. Nakanishi and P. Pastor and H. Hoffmann and S.
Schaal, Dynamical movement primitives: Learning attractor models
for motor behaviors. Neural Computation, vol. 25, pp. 328-373, 2013.
[16] I. Oikonomidis and N. Kyriazis and A. Argyros, Efficient model-based
3D tracking of hand articulations using Kinect. British Machine Vision
Conference, 2011.

943

LightNet: A Versatile, Standalone Matlab-based
Environment for Deep Learning
[Simplify Deep Learning in Hundreds of Lines of Code]
Chengxi Ye, Chen Zhao*, Yezhou Yang, Cornelia Fermüller and Yiannis Aloimonos
Computer Science Department, University of Maryland
College Park, MD, USA.

cxy@umiacs.umd.edu, chenzhao@umd.edu, yzyang@umiacs.umd.edu,
cornelia@umiacs.umd.edu, yiannis@umiacs.umd.edu
ABSTRACT

Table 1: Deep Neural Network Packages

LightNet is a lightweight, versatile, purely Matlabbased deep learning framework. The idea underlying its
design is to provide an easy-to-understand, easy-to-use and
efficient computational platform for deep learning research.
The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks
(MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports
both CPU and GPU computation, and the switch between
them is straightforward. Different applications in computer
vision, natural language processing and robotics are demonstrated as experiments.
Availability: the source code and data is available at:
https://github.com/yechengxi/LightNet

Framework
Language
Native Models
Lines of Code
Caffe
C++
CNN
74,903
Theano
Python, C
MLP/CNN/RNN
148,817
Torch
Lua, C
MLP/CNN/RNN
458,650
TensorFlow
C++
MLP/CNN/RNN
335,669
Matconvnet
Matlab, C
CNN
43,087
LightNet
Matlab
MLP/CNN/RNN
951 (1,762)*
* Lines of code in the core modules and in the whole package.

work models. Succinct and efficient Matlab programming
techniques have been used to implement all the computational modules. Many popular types of neural networks,
such as multilayer perceptrons, convolutional neural networks, and recurrent neural networks are implemented in
LightNet, together with several variations of stochastic gradient descent (SDG) based optimization algorithms.
Since LightNet is implemented solely with Matlab, the
major computations are vectorized and implemented in hundreds of lines of code, orders of magnitude more succinct
than existing pipelines. All fundamental operations can be
easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can
focus on the mathematical modeling part rather than the
engineering part. Application oriented users can easily understand and modify any part of the framework to develop
new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following
features: 1. LightNet contains the most modern network
architectures. 2. Applications in computer vision, natural
language processing and reinforcement learning are demonstrated. 3. LightNet provides a comprehensive collection of
optimization algorithms. 4. LightNet supports straightforward switching between CPU and GPU computing. 5. Fast
Fourier transforms are used to efficiently compute convolutions, and thus large convolution kernels are supported. 6.
LightNet automates hyper-parameter tuning with a novel
Selective-SGD algorithm.

Keywords
Computer vision; natural language processing; image understanding; machine learning; deep learning; convolutional
neural networks; multilayer perceptrons; recurrent neural
networks; reinforcement learning

1.

INTRODUCTION

Deep neural networks [8] have given rise to major advancements in many problems of machine intelligence. Most
current implementations of neural network models primarily
emphasize efficiency. These pipelines (Table 1) can consist
of a quarter to half a million lines of code and often involve multiple programming languages [5, 13, 2]. It requires
extensive efforts to thoroughly understand and modify the
models. A straightforward and self-explanatory deep learning framework is highly anticipated to accelerate the understanding and application of deep neural network models.
We present LightNet, a lightweight, versatile, purely
Matlab-based implementation of modern deep neural netPermission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

2.

USING THE PACKAGE

An example of using LightNet can be found in (Fig. 1):
a simple template is provided to start the training process.
The user is required to fill in some critical training parameters, such as the number of training epochs, or the training
method. A Selective-SGD algorithm is provided to facilitate
the selection of an optimal learning rate. The learning rate is

MM ’16 October 15-19, 2016, Amsterdam, Netherlands
c 2016 Copyright held by the owner/author(s).

ACM ISBN 978-1-4503-3603-1/16/10.
DOI: http://dx.doi.org/10.1145/2964284.2973791

1156

∂z
The backward process calculates the derivative ∂x
, which
∂z
is the derivative passing to the shallower layers, and ∂W
,
∂z
,
which
are
the
gradients
that
guide
the
gradient
descent
∂b
process.

∂z
∂z ∂y
=
·
= f 0 (y)T · W
∂x
∂y ∂x

(1)

∂z
∂z ∂y
=
·
= f 0 (y) · xT
∂W
∂y ∂W

(2)

∂z
∂z ∂y
=
·
= f 0 (y)
∂b
∂y ∂b

(3)

The module adopts extensively optimized Matlab matrix
operations to calculate the matrix-vector products.

3.1.2

Figure 1: A basic example, which shows how to train
a CNN on the MNIST dataset with LightNet.

selected automatically, and can optionally be adjusted during the training. The framework supports both GPU and
CPU computation, through the opts.use gpu option. Two
additional functions are provided to prepare the training
data and initialize the network structure. Every experiment
in this paper can reproduced by running the related script
file. More details can be found on the project webpage.

3.

BUILDING BLOCKS

The primary computational module includes a feed forward process and a backward/back propagation process. The
feed forward process evaluates the model, and the back propagation reports the network gradients. Stochastic gradient
descent based algorithms are used to optimize the model
parameters.

3.1

where ? denotes the correlation operation. Denoting the
complex conjugate as conj, this correlation is calculated in
the frequency domain using the Fourier transform as: x?k =
F −1 {F{x} · conj(F{k})}.

Core Computational Modules

LightNet allows us to focus on the mathematical modeling
of the network, rather than low-level engineering details. To
make this paper self-contained, we explain the main computational modules of LightNet. All networks ( and related
experiments) in this paper are built with these modules. The
notations below are chosen for simplicity. Readers can easily
extend the derivations to the mini-batch setting.

3.1.1

Convolutional Layer

A convolutional layer maps Nmap in input feature maps to
Nmap out output feature maps with a multidimensional filter
bank kio . Each input feature map xi is convolved with the
corresponding filter bank kio . The convolution results are
summed, and a bias
P value bo is added, to generate the o-th
output map: yo = 1≤i≤Nmap in kio ∗ xi +bo . To allow using
large convolution kernels, fast Fourier transforms (FFT) are
used for computing convolutions (and correlations). According to the convolution theorem [10], convolution in the spatial domain is equivalent to point-wise multiplication in the
frequency domain. Therefore, ki ∗ xi can be calculated using
the Fourier transform as: ki ∗ xi = F −1 {F{ki } · F {xi }}.
Here, F denotes the Fourier transform and · denotes the
point-wise multiplication operation. The convolution layer
supports both padding and striding.
The mapping from the o-th output feature map to the
network output can be expressed as: z = f (yo ). Here f is
the non-linear mapping from the o-th output feature map yo
∂z
,
to the final network output. As before (in Sec. 3.1.1), ∂x
i
∂z
∂z
need
to
be
calculated
in
the
backward
process,
,
and
∂ki
∂bo
as follows:
∂z
∂z ∂yo
=
·
= f 0 (yo ) ? ki ,
(4)
∂xi
∂yo ∂xi

∂z
∂z ∂yo
=
= f 0 (yo ) ? xi ,
·
∗
∗
∂kio
∂yo ∂kio

(5)

where k∗ represents the flipped kernel k. Thus, the gradient
∂z
is calculated by flipping the correlation output. Finally,
∂kio
∂z
∂z ∂yo
=
·
= 1T · vec(f 0 (yo ))
∂bo
∂yo ∂bo

Linear Perceptron Layer

A linear perceptron layer can be expressed as: y = W x+b.
Here, x denotes the input data of size input dim × 1, W
denotes the weight matrix of size output dim × input dim,
b is a bias vector of size output dim × 1, and y denotes the
linear layer output of size output dim × 1.
The mapping from the input of the linear perceptron to
the final network output can be expressed as: z = f (y) =
f (W x + b), where f is a non-linear function that represents
the network’s computation in the deeper layers, and z is the
network output, which is usually a loss value.

(6)

∂z
In words, the gradient ∂b
can be calculated by point-wise
o
summation of the values in f 0 (yo ).

3.1.3

Max-pooling Layer

The max pooling layer calculates the largest element in
Pr × Pc windows, with stride size Sr × Sc . A customized
im2col ln function is implemented to convert the stridden
pooling patches into column vectors, to vectorize the pooling
computation in Matlab. The built-in max function is called

1157

on these column vectors to return the pooling result and the
indices of these maximum values. Then, the indices in the
original batched data are recovered accordingly. Also, zero
padding can be applied to the input data.
Without the loss of generality, the mapping from the maxpooling layer input to the final network output can be expressed as: z = f (y) = f (Sx), where S is a selection matrix,
and x is a column vector which denotes the input data in
this layer.
∂z
is calculated and passed to
In the backward process, ∂x
∂z
∂z
the shallower layers: ∂x = ∂y · S = f 0 (y)T S.
When the pooling range is less than or equal to the stride
∂z
can be calculated with simple matrix indexing techsize, ∂x
niques in Matlab. Specifically, an empty tensor dzdx of the
same size with the input data is created. dzdx(f rom) =
dzdy, where f rom is the pooling indices, and dzdy is a tensor recording the pooling results. When the pooling range
is larger than the stride size, each entry in x can be pooled
multiple times, and the back propagation gradients need to
be accumulated for each of these multiple-pooled entries. In
∂z
is calculated using the Matlab function:
this case, the ∂x
accumarray().

3.1.4

(a)

Figure 2: Loss and error rates during training
and testing phases using LightNet on the MNIST
dataset.

Rectified Linear Unit

(a)

The rectified linear unit (ReLU ) is implemented as a major non-linear mapping function, some other functions including sigmoid and tanh are omitted from the discussion
here. The ReLU function is the identity function if the input
is larger than 0 and outputs 0 otherwise: y = relu(x) = x ·
ind(x > 0). In the backward process, the gradient is passed
to the shallower layer if the input data is non-negative. Otherwise, the gradient is ignored.

3.2

4.2

4.1

Convolutional Neural Network

LightNet supports using state-of-the-art convolutional network models pretrained on the ImageNet dataset. It also
supports training novel network models from scratch. A convolutional network with 4 convolution layers is constructed
to test the performance of LightNet on CIFAR-10 data [7].
There are 32, 32, 64, 64 convolution kernels of size 5 × 5 in
the first three layers, the last layer has kernel size 4 × 4.
relu functions are applied after each convolution layer as
the non-linear mapping function. LightNet automatically
selects and adjusts the learning rate and can achieve stateof-the-art accuracy with this architecture. Selective-SGD
leads to better accuracy compared with standard SGD with
a fixed learning rate. Most importantly, using Selective-SGD
avoids manual tuning of the learning rate. See Fig. 3 for the
experiment results. The computations are carried out on a
desktop computer with an Intel i5 6600K CPU and a Nvidia
Titan X GPU with 12GB memory. The current version of
LightNet can process 750 images per second with this network structure on the GPU, around 5× faster than using
CPU.

Loss function

Optimization Algorithms

Stochastic gradient descent (SGD) algorithm based optimization algorithms are the primary tools to train deep
neural networks. The standard SGD algorithm and several
of its popular variants such as Adagrad [3], RMSProp [12]
and Adam [6] are also implemented for deep learning research. It is worth mentioning that we implement a novel
Selective-SGD algorithm to facilitate the selection of hyperparameters, especially the learning rate. This algorithm selects the most efficient learning rate by running the SGD
process for a few iterations using each learning rate from a
discrete candidate set. During the middle of the neural net
training, the Selective-SGD algorithm can also be applied to
select different learning rates to accelerate the energy decay.

4.

(b)

Figure 3: Loss and error rates of training and testing
with LightNet on the CIFAR-10 dataset.

Usually, a loss function is connected to the outputs of the
deepest core computation module. Currently, LightNet supports the softmax log-loss function for classification tasks.

3.3

(b)

4.3

LSTM Network

The Long Short Term Memory (LSTM) [4] is a popular recurrent neural network model. Because of LightNet’s
versatility, the LSTM network can be implemented in the
LightNet package as a particular application. Notably, the
core computational modules in LightNet are used to perform time domain forward process and back propagation for
LSTM.
The forward process in an LSTM model can be formulated
as:

EXPERIMENTS
Multilayer Perceptron Network

A multilayer perceptron network is constructed to test
the performance of LightNet on MNIST data [9]. The network takes 28 × 28 inputs from the MNIST image dataset
and has 128 nodes respectively in the next two layers. The
128-dimensional features are then connected to 10 nodes to
calculate the softmax output. See Fig. 2 for the experiment
results.

it = sigmoid(Wih ht−1 + Wix xt + bi ),

1158

(7)

ot = sigmoid(Woh ht−1 + Wox xt + bo ),

(8)

ft = sigmoid(Wf h ht−1 + Wf x xt + bf ),

(9)

gt = tanh(Wgh ht−1 + Wgx xt + bg ),

(10)

ct = ft  ct−1 + it  gt , ht = ot  tanh(ct ),

(11)

the Q-Network can be optimized using the gradient:
∂z
∂Qcurrent
∂z
=
.
∂θ
∂Qcurrent
∂θ

(17)

Here θ denotes the parameters in the Q-Network.

5.

CONCLUSION

LightNet provides an easy-to-expand ecosystem for the
T
X
understanding
and development of deep neural network modzt = f (ht ), z =
zt .
(12)
els. Thanks to its user-friendly Matlab based environment,
t=1
the whole computational process can be easily tracked and
Where it /ot /ft denotes the response of the input/output/forget visualized. This set of the main features can provide unique
gate at time t. gt denotes the distorted input to the memory
convenience to the deep learning research community.
cell at time t. ct denotes the content of the memory cell at
time t. ht denotes the hidden node value. f maps the hidden
6. ACKNOWLEDGEMENTS
nodes to the network loss zt at time t. The full network loss
This work was funded by the support of the National
is calculated by summing the loss at each individual time
Science Foundation under grant SMA 1540917 and grant
frame in Eq. 12.
CNS 1544797, and by DARPA through U.S. Army grant
To optimize the LSTM model, back propagation through
W911NF-14-1-0384.
time is implementedP
and the most critical value to calculate
T
∂zt
∂z
=
.
in LSTM is: ∂c
t=s ∂cs
s
A critical iterative property is adopted to calculate the
7. REFERENCES
[1] Barto, A. G., Sutton, R. S., and Anderson, C. W.
above value:
Neuronlike adaptive elements that can solve difficult
∂z
∂z ∂cs
∂zs−1
learning control problems. Systems, Man and Cybernetics,
=
+
.
(13)
∂cs−1
∂cs ∂cs−1
∂cs−1
IEEE Transactions on, 5 (1983), 834–846.
A few other gradients can be calculated through the chain
rule using the above calculation output:
∂zt ∂ht
∂z
∂z
∂ct
∂zt
=
,
=
.
∂ot
∂ht ∂ot ∂{i, f, g}t
∂ct ∂{i, f, g}t

(14)

The LSTM network is tested on a character language modeling task. The dataset consists of 20, 000 sentences selected
from works of Shakespeare. Each sentence is broken into 67
characters (and punctuation marks), and the LSTM model is
deployed to predict the next character based on the characters before. 30 hidden nodes are used in the network model
and RMSProp is used for the training. After 10 epochs,
the prediction accuracy of the next character is improved to
70%.

4.4

Q-Network

As an application in reinforcement learning, We created
a Q-Network [11] with the MLP network. The Q-Network
is then applied to the classic Cart-Pole problem [1]. The
dynamics of the Cart-Pole system can be learned with a twolayer network in hundreds of iterations. One iteration of the
update process of the Q-Network is:
Qnew (stateold , act) = reward+γQcurrent (statenew , actbest )
= reward + γmaxa Qcurrent (statenew , a)
= reward + γV (statenew ).

(15)

The action is randomly selected with probability epsilon,
otherwise the action leading to the highest score is selected.
The desired network output Qnew is calculated using the
observed reward and the discounted value γV (statenew ) of
the resulting state, predicted by the current network through
Eq. 15.
By using a least squared loss function:
z = (y − Qcurrent (stateold , act))2
= (Qnew (stateold , act) − Qcurrent (stateold , act))2 ,

(16)

1159

[2] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J.,
Goodfellow, I., Bergeron, A., Bouchard, N.,
Warde-Farley, D., and Bengio, Y. Theano: new features
and speed improvements. arXiv preprint arXiv:1211.5590
(2012).
[3] Duchi, J., Hazan, E., and Singer, Y. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Research
12 (2011), 2121–2159.
[4] Hochreiter, S., and Schmidhuber, J. Long short-term
memory. Neural computation 9, 8 (1997), 1735–1780.
[5] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S.,
Long, J., Girshick, R., Guadarrama, S., and Darrell,
T. Caffe: Convolutional architecture for fast feature
embedding. In Proceedings of the ACM International
Conference on Multimedia (2014), ACM, pp. 675–678.
[6] Kingma, D., and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 (2014).
[7] Krizhevsky, A., and Hinton, G. Learning multiple layers
of features from tiny images, 2009.
[8] Krizhevsky, A., Sutskever, I., and Hinton, G. E.
Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing
systems (2012), pp. 1097–1105.
[9] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86, 11 (1998), 2278–2324.
[10] Mallat, S. A wavelet tour of signal processing: the sparse
way. Academic press, 2008.
[11] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A.,
Veness, J., Bellemare, M. G., Graves, A., Riedmiller,
M., Fidjeland, A. K., Ostrovski, G., et al. Human-level
control through deep reinforcement learning. Nature 518,
7540 (2015), 529–533.
[12] Tieleman, T., and Hinton, G. Lecture 6.5-rmsprop:
Divide the gradient by a running average of its recent
magnitude. COURSERA: Neural Networks for Machine
Learning 4 (2012), 2.
[13] Vedaldi, A., and Lenc, K. Matconvnet: Convolutional
neural networks for matlab. In Proceedings of the 23rd
Annual ACM Conference on Multimedia Conference
(2015), ACM, pp. 689–692.

2016 IEEE-RAS 16th International Conference on
Humanoid Robots (Humanoids)
Cancun, Mexico, Nov 15-17, 2016

Co-active Learning to Adapt Humanoid Movement for Manipulation
Ren Mao1 , John S. Baras1 , Yezhou Yang2 , and Cornelia Fermüller3

Abstract— In this paper we address the problem of interactive robot movement adaptation under various environmental
constraints. A common approach is to adopt motion primitives
to generate target motions from demonstrations. However,
their generalization capability is weak for novel environments.
Additionally, traditional motion generation methods do not
consider versatile constraints from diﬀerent users, tasks, and
environments. In this work, we propose a co-active learning
framework for learning to adapt the movement of robot endeﬀectors for manipulation tasks. It is designed to adapt the
original imitation trajectories, which are learned from demonstrations, to novel situations with diﬀerent constraints. The
framework also considers user feedback towards the adapted
trajectories, and it learns to adapt movement through humanin-the-loop interactions. Experiments on a humanoid platform
validate the eﬀectiveness of our approach.

I. Introduction
Trajectory learning from human demonstrations has been
studied in the ﬁeld of Robotics for decades because of its
wide range of applications in both industrial and domestic
environments. A popular approach uses so-called Motion
Primitives (MPs) to parameterize the observed human motion
and reproduce similar motions with diﬀerent initial and
target states. However, it is widely known that general MPs
methods, such as Dynamic Movement Primitives (DMPs)
[1], exhibit limited capability for generalizing to new environments involving other constraints. Moreover, the learning
used in standard MPs does not allow incorporating user
preferences, such as preferred movements under geometric
constraints. However, humanoid applications in real world
environments would greatly beneﬁt from a practical robot
movement learning framework that take user preferences and
environmental constraints into consideration.
Let’s start with a common example. A human user teaches
a humanoid how to transfer a bottle in diﬀerent situations.
Using an oﬀ-the-shelf approach, the robot can learn the
motion by acquiring MPs from demonstrations and applying
them to generate new trajectories. However, solely following
the generated trajectories may fail in a slightly altered
environment, such as when a bowl is blocking the path as
illustrated in Fig. 2(a). Here we assume that these constraints
are presented to the robot only during the testing phase, and
not during the training phase. In this work, we propose an
optimization based framework for adapting trained movements to novel environments. The ﬁrst goal of our system is

Fig. 1. System for learning movement adaptation for manipulation tasks.
Dashed lines indicate feedback.

to generate adapted trajectories, as shown in Fig. 2(b), that
can: 1) follow demonstrated trajectories for the purpose of
preserving movement patterns, and 2) fulﬁll novel constraints
perceived from the environment during the testing phase.
Moreover, new environmental constraints perceived during the testing phase could be more complex than simply
encountering an obstacle. Building on the last example, this
time, let’s consider the situation where the target bottle is
leaking. Ideally an intelligent robot that understands the
situation should avoid moving the bottle over the bowl, but
follow the movement path around it. We could simply adjust
the objective function in movement adaptation. But what if
in another scenario the robot is asked to transfer a knife
while avoiding obstacles above them to prevent potential
scratches? Constraints of this nature are not only associated
with the context of the task, i.e, leaking bottle or knife as the
manipulated object, but also with the user’s preference, i.e,
avoiding the bowl in a certain manner. To account for these
preferences, a human-in-the-loop on-line adaptation system
is necessary. In the optimization framework for generating
trajectories presented in this paper, we ﬁrstly treat the reward
weights as adjustable parameters that adapt the quality of
the trajectory. Then based on user feedback, the framework
learns the preferred behavior, that fulﬁlls constraints, by
updating the reward weights. Therefore, the learned behavior
can be generalized to diﬀerent situations with similar constraints. As illustrated in Fig. 2(c), after a few iterations of
on-line learning, the robot is able to generate a trajectory
adapted in accordance with the learned preferences.
This paper proposes an approach for interactive learning

1 R. Mao and J. Baras are with the Department of Electrical and Computer
Engineering and the ISR, 3 C. Fermüller is with the Department of Computer
Science and the UMIACS, University of Maryland, College Park, Maryland,
USA. 2 Y. Yang is with School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, USA. {neroam,
baras} at umd.edu, yz.yang at asu.edu and fer at umiacs.umd.edu.

978-1-5090-4718-5/16/$31.00 ©2016 IEEE

372

(a)

(b)

(c)

Fig. 2. Baxter Transferring Leaking Bottle: (a) Movement imitation, failed to avoid the bowl; (b) Movement adaptation with initial weights, successfully
avoided the bowl with a path above it but spilled water into the bowl; (c) Movement adaptation with weights learned for user preferences, successfully
avoided the bowl with a path around it and avoided spilling water in the bowl.

of movement adaptation for manipulation tasks. Fig. 1 illustrates the proposed system. The main contributions of
this work are: 1) A system to generalize robots’ movements
learned from demonstrations to fulﬁll constraints perceived in
a new environment. It is able to adapt trajectories according
to user preferences; 2) An approach for robot learning to
adapt trajectories by updating reward weights based on
users’ feedback. The user thus can co-actively train the
robot in-the-loop by demonstrating desired trajectories; 3)
An implementation of the optimization schema for the skill
of ”transferring objects,” considering obstacles and diﬀerent
geometric user preferences for the movements. We validate
the implementation on a humanoid platform (Baxter), and
the experimental results support our claims.

In order to enable MPs to adapt to novel environments with
obstacles [11], [12], Kober et al. [4] proposed an augmented
version of DMPs which incorporates perceptual coupling to
an external variable. Ghalamzan et al. [13] proposed a threetiered approach that can generalize noisy task demonstrations
to new situations with obstacles. They generated the nominal
path with DMPs and then adapted the trajectory to avoid obstacles by formulating an optimal control problem regarding
the reward function learned from demonstrations via inverse
optimal control. This approach allows users to teach a robot
the desired response to diﬀerent objects but requires oﬄine
training in the environment containing the obstacles for
the reward function. However, in practice, the human users
often have diﬀerent preferences for various environments and
tasks, while it is extremely challenging to provide the optimal
training trajectories in every situation. To account for this,
in our approach, the human users can interactively provide
sub-optimal suggestions on how to improve the trajectory
and the robot learns the preference for diﬀerent constraints,
and incorporates it to generate more applicable trajectories.
User preferences for a robot’s trajectories have been studied in the ﬁeld of human robot interaction (HRI). Sisbot
et al. [14] proposed to model user speciﬁed preferences
as constraints such as the distance of the robot from the
user. Then a path planner fulﬁlling those user preferences
was provided. Ashesh Jain et al. [15] proposed a co-active
learning method to learn user preferences over generated
trajectories for manipulation tasks by iteratively taking user
sub-optimal feedback, and the optimal trajectory was selected
based on the learned reward function. In our work, we adopt
the co-active learning paradigm and further propose a reward
formulation to model user preferences over constraints for
movement generation. Then we integrate it with movement
adaptation through optimization based planning.

II. Related Work
Various approaches have been proposed in robotics for
learning manipulation movements. A well known approach
is imitation learning [2], which focuses on mimicking human
demonstrations, and this approach works well when learning
from demonstration (LfD) techniques [3], [4] are applicable.
However, it only allows to reproduce learned movements in
similar environments.
Approaches [5] for encoding the trajectory as motion
primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression and Gaussian mixture models [6], [7]. In [8], a
mixture model was used to estimate the entire movement
skill from several sample trajectories. Another class of approaches employs Hidden Markov models [9]. One popular
representation to encode motion from demonstrated trajectories, originally introduced in [1], is Dynamic Movement
Primitives (DMPs). It consists of diﬀerential equations with
a non-linear learnable component that allows modeling of
almost arbitrarily complex motion. Recently, Probabilistic
Movement Primitives (ProMPs) [10] was proposed as an
alternative representation. It learns a trajectory distribution
from multiple demonstrations and modulates the movement
by conditioning on desired target states. Incorporating the
variance of demonstrations, the ProMPs approach handles
noise from diﬀerent demonstrations and provides increased
ﬂexibility for reproducing movement. However, all these
approaches hardly deal with novel environments such as
involving diﬀerent obstacles. In our work, we ﬁrst train the
robot using ProMPs, and then generalize these trained motion
primitives to newly introduced environmental constraints.

III. Co-active Learning for Movement Generalization
For the problem of robot learning from demonstrations [3],
a common practice is to oﬄine learn the skills by encoding
the trajectories with movement patterns such as DMPs [16].
During the testing phase, they can then be used to generalize
the movement to novel situations with slight alterations.
However, this generalization capability does not apply to
novel environments with diﬀerent obstacles or to a new task
contexts with a variety of manipulated objects. In this paper,
we propose a complementary framework for generalizing
oﬀ-line learned movement skills to novel situations, and in
373

the robot’s end eﬀector is represented as a sequence T =
{y(t)}t=0,...,T . We model each dimension i of y(t) using linear
regression with n Gaussian time-dependent basis functions ψ
and a n-dimensional weight vectors wi as yi (t) = ψ(t)T wi + y
where y ∼ N(0, σ2y ) denotes zero-mean i.i.d. Gaussian noise.
With the underlying weight vectors w = [wT1 , . . . , wTd ]T , the
probability of observing a trajectory T can be given by


p(y(t)|w) =
N(y(t)|Ψ(t)T w, Σy ), (1)
p(T |w) =

addition we incorporate on-line learning preferences of how
to generalize from human’s feedback co-actively.
While facing a novel situation, the robot is given a
manipulation task context xc that describes the initial and
target states of the robot, and the locations of relevant
objects in the environment. It could compute an imitation
movement trajectory yD by generalizing oﬄine learned skills
and execute if the new environment does not have obstacles
and there are no other constraints inherited from the task.
To further generalize learned movement skills to more
challenging situations, the robot has to generate an adapted
trajectory y based on the task contexts xc and the computed
imitation trajectory yD . Here we use a reward function
f ∗ (y, xc , yD ) to reﬂect how much reward the adapted trajectory y can achieve for diﬀerent contexts. This way, we can
adapt the movement by solving an optimal control problem
by maximizing the reward function f ∗ . The reward function
consists of a Imitation Reward fD describing the tendency
to follow the imitation trajectory yD , a Control Reward fC
describing the smoothness of executing the adapted trajectory y and a Response Reward fE describing the expected
response given the environment. To learn the reward function
which controls how the robot adapts trajectories under new
contexts, we apply a co-active learning technique [15] in
which the user only corrects the robot by providing an
improved trajectory ȳ and then the robot updates the parameter w of f (·; w) based on the user’s feedback. It is
worth noting that this feedback only indicates f ∗ (ȳ, xc , yD ) >
f ∗ (y, xc , yD ), and ȳ may be non-optimal trajectories. With
iterations of improvement, the robot could learn a function
that approximates the oracle f ∗ (·) tightly.

t

t
d


where Ψ(t) = diag(ψ(t), . . . , ψ(t)) and Σy = σ2y Id×d .
1) Learning from Demonstrations: For each demonstration, the trajectory can be easily represented by a weight vector w. To capture trajectory variations from multiple demonstrations, a Gaussian distribution p(w; θ) = N(w|μw , Σw )
over the weights w is estimated. Therefore, the distribution
of the trajectory p(T |w) can be represented as

p(T ; θ) =
p(T |w)p(w; θ)dw
(2)

=
N(y(t)|Ψ(t)T μw , Ψ(t)T Σw Ψ(t)T + Σy ) (3)
t

We can then estimate the parameters θ = {μw , Σw } by using
maximum likelihood estimation as suggested in [10].
2) Trajectory Generation: In novel situations, the trajectory could be modulated by conditioning with diﬀerent
observed states. By adding an observation vector of Y ∗ =
[y0∗T , yT∗T ]T indicating the desired initial state y0∗ and target
state yT∗ with accuracy Σ∗y , we apply Bayes theorem and
represent conditional distribution for w as

	
p(w|Y ∗ ) = N(w|μw , Σw ) ∝ N Y ∗ |Ψ∗T w, Σ∗Y p(w)

	−1 
	
Y ∗ − Ψ∗T μw
μw = μw + Σw Ψ∗ Σ∗Y + Ψ∗T Σw Ψ∗

	−1
Σw = Σw − Σw Ψ∗ Σ∗Y + Ψ∗T Σw Ψ∗ Ψ∗T Σw
(4)
where Ψ∗ = [Ψ(0), Ψ(T )] and Σ∗Y = diag(Σ∗y , Σ∗y ) are
augmented for observation vector Y ∗ . With a conditional
distribution of w, we can generate conditional trajectory
distribution and easily evaluate the mean yD and the variance
ΣD of the trajectory T for any time point t according to
Eq.(1) and Eq.(2). Therefore, the mean yD (t) can be used
as the imitation trajectory in movement adaptation and the
variance ΣD (t) can be used to indicate which parts of the trajectory are more ﬂexible to adapt. A larger variance reﬂects
higher variations in demonstrations. It means more ﬂexibility
for modifying the corresponding part of the trajectory.
It is worth mentioning that, although we adopt ProMPs for
movement imitation in this work, the proposed Movement
Adaptation framework can be integrated into any other
movement imitation learning technique.

IV. Our System
Overall, after the robot has oﬄine learned the movement
skill from demonstrations, when facing a diﬀerent task
context xc in a novel environment, the testing phase includes
three stages: 1) Movement Imitation, which computes an imitation trajectory yD by generalizing demonstrated movement
to new initial and target states; 2) Movement Adaptation,
which generates an adapted trajectory y under new task
and environment contexts by maximizing the given reward
function; 3) Rewards Learning, which updates the parameters
of estimated reward function according to the user’s feedback
through co-active learning. We formulate each stage in our
framework presented in Fig. 1 as following.
A. Movement Imitation
At the beginning, our system oﬄine learns movement
skills in an environment without obstacle or other constraints.
In this work, we adopt the Probabilistic Movement Primitives
(ProMPs) [10] for imitation learning. It obtains a distribution over trajectories from multiple demonstrations, which
captures the variations, and can be easily generalized to new
initial and target states while imitating the movement.
To be speciﬁc, we consider that a robot’s end-eﬀector has
d degrees of freedom (DOF) along with its arm, with its
state denoted as y(t) = [y1 (t), . . . , yd (t)]T . The trajectory of

B. Movement Adaptation
As mentioned before, if the environment of a new situation
is exactly the same as the one during demonstration when
ProMPs are learned, e.g, no obstacle, safety constraints or
374

other new considerations, the robot can perform the movement optimally by directly following the imitation trajectory
yD ∈ d generated by learned ProMPs in discrete time.
In this work, we want to have a system that can adapt to
an environment with novel constraints. Thus, we model the
movement adaptation as an optimal control problem with
ﬁxed time horizon T in discrete time. The output of the
adaptation system is a new trajectory y ∈ d in discrete
time. The input consists of the task context xc which is
obtained from the perception module, the imitation trajectory
yD which is generated from learned ProMPs, and the reward
function f (y, xc , yD ) which represents the reward of the
adapted trajectory y corresponding to the new situation.
1) Optimization with Constraints: Let’s consider that the
perception module detects Nob j objects in the environment,
which may be obstacles. Each object is abstracted as a sphere
in the space represented by its center location and semidiameter {Ok , dk }, k = 1, . . . , Nob j . We assume that the reward
function can be modeled as accumulated sum of rewards
from each state y(t) at time step t:
f (y, xc , yD ) =

T



ft (y(t), xc , yD ).

changes could be considered in the next decision steps.
i+T p
max
t=i+1 ft (y(t), xc , yD )
(a(i),··· ,a(i+T p −1))

subj. to

(13)
3) Reward Function: In order to adapt robot movements
to perform well in novel situations, considering only hard
constraints such as obstacle avoidance, Eq.(11), does not sufﬁce. Thus, we further model a reward function f (y, xc , yD )
that reﬂects the amount of rewards that an adapted trajectory
y can gain within the context xc and yD . As the reward
function f (y) is assumed temporally discrete in Eq.(5), we
model the reward function ft (y(t)) at t by three parts:
ft (y(t); w) = fD,t (y(t); wD ) + fC,t (y(t); wC ) + fE,t (y(t); wE ),
(14)
where the Imitation Reward fD models the tendency to follow
the imitation trajectory yD , the Control Reward fC models
the smoothness of executing the adapted trajectory y and the
Response Reward fE characterizes the expected response to
the environment. Meanwhile, w = [wDT , wCT , wET ]T are parameters that aﬀect the behavior of the movement adaptation.
Next we describe each reward function in detail.
a) Imitation Reward: The Imitation Reward characterizes how well the adapted trajectory can imitate the demonstrations by the distance between points on y and yD . Recall
that we have the variance ΣD (t) of the imitation trajectory
yD in IV-A.2, which indicates how ﬂexible we can adapt the
trajectory. Considering ΣD (t) = diag(σ21 (t), . . . , σ2d (t)) to be
diagonal for the sake of simplicity, we model the Imitation
Reward by the weighted distance:

(5)

t=0

Because we are only modulating the trajectory, the adaptation
system can be modeled as linear dynamics with the control
signal a ∈ m , as it does not involve real physical dynamics.
Considering the embodiment of robotic end-eﬀectors, we
can compute the end-eﬀector’s position in spatial space
E(y) following the kinematics modeling [17]. Then, considering obstacles avoidance, the target optimal policy π∗ =
{a(t)∗ }t=0,...,T −1 can be deﬁned from Eq. (6) with constraints.

(6)
π∗ = arg max Tt=0 ft (y(t), xc , yD )
π

subj. to

∀t = i, · · · , i + T p − 1
z(t + 1) = Az(t) + Ba(t)
y(t) = Cz(t)
U ≥ y(t) ≥ L
	E(y(t)) − Ok 	2 ≥ dk2 , ∀k = 1, · · · , Nob j
y(T ) = yD (T ).

∀t = 0, · · · , T − 1
(7)
z(t + 1) = Az(t) + Ba(t)
(8)
y(t) = Cz(t)
(9)
U ≥ y(t) ≥ L
(10)
2
2
	E(y(t)) − Ok 	 ≥ dk , ∀k = 1, · · · , Nob j (11)
y(T ) = yD (T ),
(12)

fD,t (y(t); wD ) = −(y(t) − yD (t))T V (t)(y(t) − yD (t))

(15)

−σ21 (t)

(16)

V (t) = diag(wD )diag(e

−σ2d (t)

,...,e

),

where V (t) is a weight matrix consisting of parameters wD
2
and {e−σi (t) } in which the variances learned from demonstrations ΣD (t) are modeled to aﬀect adaptation rewards.
b) Control Reward: The Control Reward fC characterizes the smoothness of executing the adapted trajectory y
using the following formulation:

where A, B, C are system matrices, Eq.(12) constrains the
ﬁnal position of the adapted trajectory, Eq.(10) constrains
it within feasible limits, and Eq.(11) ensures it can avoid
obstacles safely by keeping a minimum distance dk between
the robot’s end-eﬀector and any object.
2) Model Predictive Control: In order to ﬁnd an optimal
solution of such a system with continuous state and action
spaces, we adopt Model Predictive Control which computes
the optimal actions in a ﬁnite prediction horizon. Therefore,
by considering a prediction time horizon T p , the optimal
action a(i)∗ , at time step i = 0, . . . , T − 1, can be solved
by (13). At each step i, the optimal actions {a(i)∗ , · · · , a(i +
T p − 1)∗ } for T p decision steps in the future are computed
but only the action for the current step a(i)∗ is performed.
Therefore, it can deal with changing environments as these

fC,t (y(t); wC ) = −wC 	(y(t) − y(t − 1))	2 ,

(17)

where wC is the parameter to weigh this reward.
c) Response Reward: The Response reward fE describes the expected response to the environment, such as
safety considerations for obstacles and objects under manipulation. Here we give intuitive examples. Although we can
ensure minimum distance to avoid obstacles using Eq.(13),
as human users we still expect the robot to transfer a cup
full of water around a laptop instead of above it, to avoid
potential spills. Another example is that the user would prefer
375

that the robot when manipulating sharp objects, such as
knives, keeps a relatively larger distance from the human for
safe. All the above preferences are speciﬁc to objects under
manipulation and the exact environment. Thus, we set the
Response Reward such that the better the adapted trajectory
fulﬁlls the preferences, the higher the reward is.
To formally represent the Response Reward, let us consider a scenario with Nob j obstacles on the table. The leftmost
and rightmost locations of the table are B1 , B2 and the table
surface is S, we then can formulate it as follows:

by features and weights:
(22)
f (y, xc , yD ; w) = wDT φD + wCT φC + wET φE
T





2
φD = φD,1 , . . . , φD,d T , φD,i = −
yi (t) − yD,i (t) 2 e−σi (t)
t=0

(23)
T


φC = − 	(y(t) − y(t − 1))	2

φE = −

⎞
⎛ Nob j
⎟⎟⎟
⎜⎜⎜

T
⎜
fE,t (y(t); wE ) = − ⎜⎜⎝ wO,k φO,k + wB φB + wS φS ⎟⎟⎟⎠
(18)
k=1


φTO,k = −	E(y(t)) − Ok 	, (E(yD (t)) − E(y(t)))T


2
k	
· exp − 	E(y(t))−O
dk
φB =

2


i=1



	E(y(t)) − Bi 	2
exp −
dmin

φS = 	E(y(t)) − S 	2 ,

(24)

t=1
T 



φTO,1 (y(t)), . . . , φTO,Nob j (y(t)), φB (y(t)), φS (y(t))

T

t=0

(25)
where φD , φC , φE represent features of the entire trajectory
corresponding to Imitation, Control and Response Rewards.
Since the user only provides a feedback trajectory ȳ and
the system can not directly observe the reward function, we
apply the co-active learning technique [15] in which the robot
iteratively updates the parameter w of f (·; w) based on user’s
feedback. Note that this feedback only needs to indicate
f ∗ (ȳ, xc , yD ) > f ∗ (y, xc , yD ) and ȳ could be non-optimal
trajectories. Algorithm 1 gives our learning algorithm.

(19)
(20)
(21)

where φO,k represents the feature vector for preferences in
avoiding obstacle Ok , of which the ﬁrst element denotes
avoiding distance and the second element denotes the deviation direction. The preferred deviation direction is given as
reward weights and the inner product indicates the rewards of
deviation considering the given preference. The exponential
decay function is applied so that the features are only eﬀective when the robot’s end-eﬀector is close to the obstacles. φB
and φS are features related to safety by considering boarders
T
T
, . . . , wO,N
, wB , wS ]T
and surface of the table. wE = [wO,1
ob j
are weights corresponding to the features respectively.
Given a set of parameters w = [wDT , wCT , wET ]T , the
MPC module generates an adapted trajectory by maximizing
f (·; w). The robot could follow the trajectory and execute
the task facing the novel situation. However, the generated
trajectory may not be suﬃciently satisfying from a user’s
perspective, since the given or initialized parameters may not
be accurate for modeling the rewards. To accommodate this
issue, after the movement execution, our system allows the
user to provide a better trajectory as feedback to update the
parameters during the following Rewards Learning section.

Algorithm 1 Rewards Learning for Movement Adaptation
Initialize w(0) = [wD(0)T , wC(0)T , wE(0)T ]T
for Iteration i = 0 to T l do
Task Context and Environment Perception: x(i)
c
Movement Imitation:
(i)
yD(i) , Σ(i)
D ← p(T |xc )
Movement Adaptation:
(i)
(i)
π ∗(i) = arg maxπ f (y, x(i)
c , yD ; w )
(i)
∗(i)
y ←π
Movement Execution: y (i)
if User Provides Feedback: ȳ (i) then
√
α(i) = 1/ i
wD(i+1) = wD(i) + α(i) (φD (ȳ (i) , yD(i) ) − φD (y (i) , yD(i) ))
wC(i+1) = wC(i) + α(i) (φC (ȳ (i) ) − φC (y (i) ))
(i)
(i)
wE(i+1) = wE(i) + α(i) (φE (ȳ (i) , x(i)
c ) − φE (y , xc ))
Weights Projection:
w̄(i+1) = [wD(i+1)T , wC(i+1)T , wE(i+1)T ]T
w(i+1) = arg minw∈C 	w − w̄(i+1) 	2
else w(i+1) = w(i)
end if
end for

C. Rewards Learning

Note that α is a learning rate, which decays along iterations, and C in the weights projection part is a bounded set
to ensure that the updated parameters w are in a feasible
space. After iterations of improvements, the robot can learn
an estimated reward function f (·; w∗ ) that approximates the
oracle reward f ∗ (·) as proven in [18]. By maximizing the
estimated reward function f (y, xc , yD ; w∗ ), the robot can
generate an adapted trajectory y that maximizes the rewards
facing situation xc based on imitation trajectory yD .

In this section, we describe how our system learns the
reward function. Let us assume there is an oracle reward
function f ∗ (y, xc , yD ) that reﬂects exactly how much reward
the adapted trajectory y can gain for each context. The
goal of this module is to estimate such a reward function
f (y, xc , yD ; w), where w are the parameters to be learned,
that approximate the oracle reward f ∗ (·) tightly.
By rewriting Eq.(5) and Eq.(14) for the entire trajectory,
we can have the reward function in a linear form represented
376

V. Experiments

weights. There is no preference speciﬁed in the reward function about how to avoid obstacles. Therefore, even though
the adapted trajectory could avoid the obstacle successfully,
it may be not an ideal trajectory.
To learn the user preference, we then provide feedback
via kinethestic demonstration illustrated in Fig. 4(a) and the
feedback trajectory is shown in Fig. 3(d) as dashed line
to indicate user preferences. Following Algo. 1, the robot
iteratively updates the rewards weights based on the user
feedback. Weights are limited via projection in the feasible
set C where wD ∈ [1, 100]7 , wC ∈ [1, 100], wE ∈ [0, 100] except that the last two parameters in wO,k indicating preferred
deviation direction could be [−100, 100]. To quantitatively
validate the performance of our method in movement adaptation, we consider the metric of cumulative error between
the adapted
trajectory	 and the feedback trajectory e(i) =

2
1 T
(i)
(t)
− y (i) (t) as the learning error at iteration i.
ȳ
t=0
T
Since the metric is aﬀected by diﬀerent situations such as
obstacles’ locations, we consider the feedback trajectory as
ﬁxed and let the robot iteratively learn several times to see
how it performs, and we record the “learning curve” under
the same feedback. From Fig. 4(b), we can see that the error
decreases and converges after several iterations, and it only
requires a few iterations to achieve an adapted trajectory as
desired preference according to the feedback. After learning,
the robot uses the updated weights for movement adaptation
in a diﬀerent situation with novel initial/target states and
obstacles’ locations. Fig. 3(e) shows the adapted trajectory
based on the updated weights after one iteration, where it
successfully avoids the obstacle via the desired direction.
In a second scenario where a robot is transferring a
knife around some fragile obstacle, the user may prefer
the robot to avoid the obstacle above it instead of around
it. With the same methods here, we could also generate
adapted trajectories as shown in Fig. 5(a) and Fig. 5(c) for
initial weights. With the user provided feedback, the robot
successfully learns the speciﬁed preferences for movement
adaptation and generates the improved adapted trajectories
for diﬀerent situations as shown in Fig. 5(b) and Fig. 5(d).

To validate the system described above, we design and
conduct the following experiments on a Baxter humanoid
platform. The Baxter robot is asked to do manipulation
tasks such as cleaning on a table top. The workspace is
deﬁned with the surface as S = (0, 0, −0.1), the leftmost
location as B1 = (0, 0.8, 0) and the rightmost location as
B2 = (0, −0.8, 0) described in meters in robot spatial space,
where the coordination system is shown in Fig. 4(a). It needs
to learn transferring the manipulated object between diﬀerent
locations while avoiding obstacles in desired manners.
A. Movement Imitation
In the ﬁrst stage of the experiments, we have our robot
learn oﬀ-line the movement skill from kinethestic demonstrations with no obstacles on the table. All trajectories are
sampled discretely and normalized to T = 200 steps for
transferring movement in joint space, and the left arm of the
Baxter has d = 7 degrees of freedom for joints from shoulder
to wrist denoted as joint 1 to 7. The training trajectories are
encoded by ProMPs with n = 10 Gaussian basis functions so
that it can be generalized to diﬀerent initial and target states.
Fig. 3(a) shows an example of our generated imitation trajectory in spatial space for new task contexts using ProMPs.
Fig. 3(b) shows the corresponding imitation trajectory of
joint 1 (ﬁrst shoulder joint) in joint space. The blue crosses
here are desired new initial and target states, and the shaded
area is the estimated variance for the imitation trajectory,
which reﬂects the variations of demonstrations. True trajectory here means a trajectory recorded from user demonstration in the testing scenario for comparison. It is not hard to
see that the predicted mean of the imitation trajectory well
generalizes to new initial and target states and follows the
same movement pattern as the prior mean trajectory learned
from demonstrations. Therefore, the robot can perform the
task well by following this imitation trajectory if there are
no obstacles or other safety constraints.
B. Learning Adaptation
We consider the situation, where the robot, while facing
the task of transferring a leaking bottle, ﬁnds a bowl ﬁlled
with food as obstacle on the table. We assume that the
bowl’s center location O1 and minimum safety distance d1
are obtained through perception. For movement adaptation,
we set the prediction horizon T p = 11 in the model predictive
control and select system matrices A = 0.9 · I, B = C = I
to make the system stable in the prediction window as suggested in [13]. The limits of joints could be found from the
Baxter hardware speciﬁcation. The minimum safety distance
to the table boarder is set as dmin = 0.1. And the weights for
reward function are initialized to wD = 30·1, wC = 10, wE =
0. We apply the native Matlab Gradient-based optimization
method fmincon to solve the optimization at each time step.
Fig. 3(c) shows the movement imitation for transferring
the leaking bottle, which failed to avoid the obstacle even
though the trajectory generalizes to a novel initial and target
states. Fig. 3(d) shows the movement adaptation with initial

VI. Conclusion and Future Work
We presented a framework for learning to adapt robot
end eﬀector movement for manipulation tasks. The proposed
method generalizes oﬄine learned movement skills to novel
situations considering obstacle avoidance and other taskdependent constraints. It adapts the imitation trajectory generated from demonstrations, while maintaining the learned
movement pattern and considering variations in the geometry.
Here we considered as variations, avoiding obstacles with
movements in desired directions, and keeping certain distances for a safety margin within a workspace. The methods
also provides a way to learn how to adapt the movement in
on-line interactions with user’s feedback.
Another interesting way to incorporate environmental constraints would be to consider visual information of objects
and the environment as an indication of the preferences for
movement adaptation. For instance, the deviation direction
377

(a)

(b)

(c)

(d)

(e)

Fig. 3. Learning to Transfer a Leaking Bottle: (a) Imitation trajectory in spatial space; (b) Imitation trajectory for joint 1 in joint space, shaded area
indicating the predicted variance.(a) Movement Imitation failed to avoid the obstacle; (b) Movement adaptation with initial weights successfully avoided
the obstacle by a path above it but has a potential danger of spilling water, feedback trajectory is provided afterwards; (c) Movement adaptation for a
diﬀerent situation with updated weights after learning from feedback trajectory, successfully avoids the obstacle through a path around. Corresponding
execution on the Baxter platform is given by Fig. 2.

(a)

[2] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, “Imitation learning
of dual-arm manipulation tasks in humanoid robots,” International
Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183–202, 2008.
[3] P. Pastor, H. Hoﬀmann, T. Asfour, and S. Schaal, “Learning and generalization of motor skills by learning from demonstration,” in Robotics
and Automation, 2009. ICRA’09. IEEE International Conference on.
IEEE, 2009, pp. 763–768.
[4] J. Kober, B. Mohler, and J. Peters, “Learning perceptual coupling for
motor primitives,” in Intelligent Robots and Systems, 2008. IROS 2008.
IEEE/RSJ International Conference on. IEEE, 2008, pp. 834–839.
[5] A. Gams, A. J. Ijspeert, S. Schaal, and J. Lenarčič, “On-line learning
and modulation of periodic movements with nonlinear dynamical
systems,” Autonomous robots, vol. 27, no. 1, pp. 3–23, 2009.
[6] F. Guenter, M. Hersch, S. Calinon, and A. Billard, “Reinforcement
learning for imitating constrained reaching movements,” Advanced
Robotics, vol. 21, no. 13, pp. 1521–1544, 2007.
[7] S. Calinon, F. D’halluin, E. L. Sauser, D. G. Caldwell, and A. G.
Billard, “Learning and reproduction of gestures by imitation,” Robotics
& Automation Magazine, IEEE, vol. 17, no. 2, pp. 44–54, 2010.
[8] S. M. Khansari-Zadeh and A. Billard, “Imitation learning of globally
stable non-linear point-to-point robot motions using nonlinear programming,” in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ
International Conference on. IEEE, 2010, pp. 2676–2683.
[9] T. Inamura, I. Toshima, H. Tanie, and Y. Nakamura, “Embodied symbol emergence based on mimesis theory,” The International Journal
of Robotics Research, vol. 23, no. 4-5, pp. 363–377, 2004.
[10] A. Paraschos, C. Daniel, J. R. Peters, and G. Neumann, “Probabilistic
movement primitives,” in Advances in neural information processing
systems, 2013, pp. 2616–2624.
[11] D.-H. Park, P. Pastor, S. Schaal et al., “Movement reproduction and
obstacle avoidance with dynamic movement primitives and potential
ﬁelds,” in Humanoid Robots, 2008. Humanoids 2008. 8th IEEE-RAS
International Conference on. IEEE, 2008, pp. 91–98.
[12] H. Hoﬀmann, P. Pastor, D.-H. Park, and S. Schaal, “Biologicallyinspired dynamical systems for movement generation: automatic realtime goal adaptation and obstacle avoidance,” in Robotics and Automation, 2009. ICRA’09. IEEE International Conference on. IEEE,
2009, pp. 2587–2592.
[13] A. M. Ghalamzan E., C. Paxton, G. D. Hager, and L. Bascetta,
“An incremental approach to learning generalizable robot tasks from
human demonstration,” in Robotics and Automation (ICRA), 2015
IEEE International Conference on. IEEE, 2015, pp. 5616–5621.
[14] E. A. Sisbot, L. F. Marin, and R. Alami, “Spatial reasoning for human
robot interaction,” in Intelligent Robots and Systems, 2007. IROS 2007.
IEEE/RSJ International Conference on. IEEE, 2007, pp. 2281–2287.
[15] A. Jain, S. Sharma, T. Joachims, and A. Saxena, “Learning preferences
for manipulation tasks from online coactive feedback,” The International Journal of Robotics Research, p. 0278364915581193, 2015.
[16] R. Mao, Y. Yang, C. Fermuller, Y. Aloimonos, and J. S. Baras,
“Learning hand movements from markerless demonstrations for humanoid tasks,” in Humanoid Robots (Humanoids), 2014 14th IEEERAS International Conference on. IEEE, 2014, pp. 938–943.
[17] Z. Ju, C. Yang, and H. Ma, “Kinematics modeling and experimental
veriﬁcation of baxter robot,” in Control Conference (CCC), 2014 33rd
Chinese. IEEE, 2014, pp. 8518–8523.
[18] G. Ciná and U. Endriss, “Proving classical theorems of social choice
theory in modal logic,” Autonomous Agents and Multi-Agent Systems,
pp. 1–27, 2016.

(b)

Fig. 4. Rewards Learning from User Feedback for Transferring Leaking
Bottle: (a) User feedback via kinethestic demonstration; (b) Learning curve
for adaptation under the same feedback.

(a)

(b)

(c)

(d)

Fig. 5. Baxter Learning to Adapt Movement for Transferring Knife: (a) (c)
Movement adaptation with initial weights using a path around the duck doll
successfully avoided it but risked scratches; afterwards feedback trajectory
is provided for preferences; (b) (d) Movement adaptation for diﬀerent
situations, with updated weights after learning from feedback trajectory,
successfully avoided the duck doll using a path above it as desired.

for avoiding a knife could be directly inferred from the
location and orientation of its blade from visual input. In
ongoing work we are further investigating the possibility of
directly learning the preferences to adapt movement from
visual perception for the task context.
ACKNOWLEDGMENT
This work was supported by DARPA (through ARO) grant
W911NF1410384 and by NSF through grants CNS-1544787
and SMA-1540917.
References
[1] A. J. Ijspeert, J. Nakanishi, H. Hoﬀmann, P. Pastor, and S. Schaal,
“Dynamical movement primitives: learning attractor models for motor
behaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.

378

2015 IEEE International Conference on Robotics and Automation (ICRA)
Washington State Convention Center
Seattle, Washington, May 26-30, 2015

Learning the Spatial Semantics of Manipulation Actions through
Preposition Grounding
Konstantinos Zampogiannis1 , Yezhou Yang1 , Cornelia Fermüller2 , and Yiannis Aloimonos1
Abstract— In this paper, we introduce an abstract representation for manipulation actions that is based on the
evolution of the spatial relations between involved objects.
Object tracking in RGBD streams enables straightforward
and intuitive ways to model spatial relations in 3D space.
Reasoning in 3D overcomes many of the limitations of similar
previous approaches, while providing significant flexibility in
the desired level of abstraction. At each frame of a manipulation video, we evaluate a number of spatial predicates
for all object pairs and treat the resulting set of sequences
(Predicate Vector Sequences, PVS) as an action descriptor. As
part of our representation, we introduce a symmetric, timenormalized pairwise distance measure that relies on finding
an optimal object correspondence between two actions. We
experimentally evaluate the method on the classification of
various manipulation actions in video, performed at different
speeds and timings and involving different objects. The results
demonstrate that the proposed representation is remarkably
descriptive of the high-level manipulation semantics.

I. I NTRODUCTION
Intelligent robots built for manipulation tasks need to
learn how to manipulate. However, given that there is an
infinite number of ways to perform a certain manipulation action, such as for example, making a peanut butter
and jelly sandwich [1], the robot should be able to store
and organize compact representations effectively, rather than
simply a large set of examples that are hard to generalize.
Various aspects and levels of representations of manipulation
actions have been studied, such as objects and tools [2], [3],
manipulator trajectories [4], [5], actions and sub-actions [6],
object-wise touch relations [7], action consequences or goals
[8], etc. In this work, we focus on another crucial aspect
of manipulation actions, which is the object-wise spatial
relations in 3D space and the correlation of their temporal
evolution to the underlying manipulation semantics.
Why are spatial relations crucial for interpreting manipulations? First of all, while most primitive spatial relations
can be inferred directly from the perceptual input, their
exploitation allows for more complex types of reasoning
about not only geometric, but also the underlying physical
relations between objects. For example, by perceiving that
an apple is “ABOVE” a plate as well as that these two
“TOUCH”, we can infer that the plate is the supporting
object. Secondly, at a higher level, by grounding the spatial
relations for the most commonly used prepositions in natural
1 K. Zampogiannis, Y. Yang, and Y. Aloimonos are from the Department
of Computer Science, and 2 C. Fermüller is from UMIACS, University
of Maryland, College Park, MD 20742, USA. {kzampog, yzyang,
yiannis} at cs.umd.edu and {fer} at umiacs.umd.edu

978-1-4799-6923-4/15/$31.00 ©2015 IEEE

language, we effectively establish a bridge from observation
to language and then from language to execution.
Additionally, a correct understanding of object-wise spatial
relations for a given action is essential for a robot to perform
the action successfully. For example, when a robot is asked
to “Pour a cup of coffee from the pitcher”, it not only needs
to recognize “cup” and “pitcher” and generate a sequence
of trajectories to perform the “pour” action, but also needs
to perform geometric precondition checks on the 3D spatial
relations between “pitcher” and “cup”: unless the “pitcher”
tip is “ABOVE” the “cup”, “pour” should not be triggered!
In other words, a correct understanding of spatial relations
can provide the premise of an accurate execution.
Few work has touched upon object-wise spatial relations
for manipulation actions, due to the difficulties inherited from
object tracking (inevitable occlusions, etc.). A joint segmentation and tracking technique to reason about “touch” and
“contain” relations from top-down 2D views is used in [7].
The limitations of their approach come from reasoning in 2D.
For example, their system is not able to differentiate between
spatial predicates “above” and “in”. Other approaches on
spatial reasoning [9] require additional equipment, such as
motion capture suits, which makes them impractical for more
general purpose applications.
The goal of our work is to develop a system which equips
the robot with a fundamental and reliable understanding of
3D spatial relations. During both observing and executing
manipulation actions, a robot with our proposed capabilities
will be able to understand and reproduce the evolution of the
spatial relations between involved objects with high accuracy.
We show that a 3D spatial relation based representation is
highly descriptive of the underlying action semantics.
At the lower level of this work, we developed a system
for RGBD object segmentation and tracking that does not
require additional markers on the objects and allows us to
overcome difficulties arising from 2D-only image reasoning.
Given point cloud representations for all tracked objects, we
adopted a straightforward yet intuitive way to model geometric relations for the most commonly used natural language
spatial prepositions by partitioning the space around each
object. For a given object pair, our spatial relation predicates
effectively capture the spatial distribution of the first object
with respect to the second. The temporal evolution of each
such distribution is encoded in a Predicate Vector Sequence
(PVS). At a higher level, we a) propose a novel action
descriptor, which is merely a properly ordered set of PVSes
for all involved object pairs, and b) introduce its associated
distance measure, which relies on finding an optimal, in

1389

terms of PVS similarity, object correspondence between two
given actions in this representation. Experiments on real
manipulation videos for various actions, performed with
significant amounts of variation (e.g., different subjects,
execution speeds, initial/final object placements, etc.), indicate that our proposed abstract representation successfully
captures the manipulation’s high-level semantic information,
while demonstrating high discriminative performance.

space, which enables our system to generate representations
of controlled levels of abstraction and discriminative power.
III. O UR APPROACH
A. Overview of our method
A very brief description of the processing steps involved
in our method is provided here and details are discussed in
the following subsections.
RGBD
stream

II. R ELATED WORK
From the very beginning of robotics research, a great
amount of work has been devoted to the study of manipulation, due to its direct applications in intelligent manufacturing. With the recent development of advanced robotic
manipulators, work on robust and accurate manipulation
techniques has followed quickly. For example, [10] developed a method for the PR2 to fold towels, which is based on
multiple-view geometry and visual processes of grasp point
detection. In [11], they proposed learning object affordance
models in multi-object manipulation tasks. Robots searching
for objects were investigated in [12], using reasoning about
both perception and manipulation. [4] and [5] developed
manipulation and perception capabilities for their humanoid
robot, ARMAR-III, based on imitation learning for human
environments. A good survey on humanoid dual arm manipulation can be found in [13]. These works reached promising
results on robotic manipulation, but they focused on specific
actions without allowing for generalization. Here, we suggest
that the temporal evolution of object-wise 3D spatial relations
is a highly descriptive feature for manipulation actions and
thus can be potentially used for generalizing learned actions.
Several recent works have studied spatial relations in the
context of robot manipulation, either explicitly or implicitly.
From a purely recognition point of view, the prepositions
proposed in [14] can be directly used for training visual
classifiers. Recently, [15] built a joint model of prepositions
and objects in order to parse natural language commands,
following the method introduced in [16]. In [17] and [18], supervised learning techniques were employed to symbolically
ground a specific set of spatial relations, using displacements
of likely object contact points and histograms that encode the
relative position of elementary surface patches as features
respectively. However, in general contexts, most common
spatial prepositions do not have ambiguous geometric meanings and can, in principle, be directly modeled. Instead of
grounding them via learning from large sets of annotated
data, we directly model spatial relations according to their
clear geometric interpretation.
In [7], they developed a 2D video segmentation and
tracking system, through which they proposed a compact
model for manipulation actions based on the evolution of
simple, 2D geometric relations between tracked segments.
Reasoning with only 2D segments enforces an unwanted and
unnecessary type of abstraction in the representation due to
the loss of geometric information. In this paper, instead, we
infer spatial relations between segmented point clouds in 3D

Workspace
plane fitting

Fig. 1.

Object segmentation
and tracking

Spatial relation
evaluation

Action descriptor

Processing steps for our action descriptor extraction.

A schematic of our action descriptor extraction pipeline is
given in Fig. 1. The input to our algorithm is an RGBD
video of a manipulation action. The objects of interest
are segmented in the first frame and tracked in 3D space
throughout the rest of the sequence (Section III-B). At each
time instant, we evaluate a predetermined set of pairwise
spatial predicates for each pair of tracked objects (Section IIIC), thus obtaining a sequence of spatial relation descriptors
for every object pair (Predicate Vector Sequence, PVS).
This set of sequences, arranged in a predetermined order,
constitutes our proposed action descriptor (Section III-D).
Subsequently, we define an appropriate pairwise distance
function for this representation, which relies on standard
dynamic programming techniques for time series similarity
evaluation. Distance computation between two actions is
reduced to finding an optimal, in a sense to be defined, object
correspondence between the actions (Section III-E).
Assumptions. To simplify certain subtasks, we assume
that the action takes place, at least at the beginning, on a
planar surface (e.g., on a table) on which all involved objects
initially lie. Furthermore, we assume that the depth sensor
used to record the manipulation remains still throughout the
action duration (no ego-motion). Since our method mostly
pertains to applications of robots learning by watching humans or other robots and as long as objects remain visible
in order to be reliably tracked, we do not consider the latter
to be too restrictive.
B. Point cloud tracking
The initialization of the tracking procedure involves fitting
a plane to the workspace surface in the first RGBD frame
of the input sequence. This is done reliably using standard
methods, i.e. least squares fitting under RANSAC. The points
that lie a certain threshold distance above the estimated
plane are clustered based on which connected component
of their k-NN graph they belong to. Assuming that the
maximum distance between points that belong to the same
object is smaller than the minimum distance between points
of different objects, this simple procedure yields an effective
segmentation of all the objects in the initial frame.
Treating these initial segments (point clouds) as object
models, we initiate one tracker for each object and perform
rigid object tracking using the KLD-sampling adaptive Particle Filtering algorithm [19] that is implemented in [20].
This results in a point cloud for each object, for each video

1390

frame. We denote this set of point clouds at time index t by
t
{X1t , . . . , XN
}, where No is the number of objects in the
o
action. In the following, we will use the terms Xit and “object
i (at time t)”, for some i = 1, . . . , No , interchangeably. A
sample output of our point cloud tracker for a two object
manipulation recording is depicted in Fig. 2.

Fig. 2. Point cloud tracking for a Pour action. First row: RGB frames
from input video. Second row: workspace plane and point clouds for the
two tracked objects.

We must note that object segmentation and tracking are not
the primary objectives of this study. The approach described
here is the one we used in our experiments (Section IV)
and primarily serves as a showcase for the feasibility of
subsequent processing steps using readily available tools. We
believe that any reasonably performing point cloud tracking
algorithm is adequate for our purposes, as spatial relations
can, in general, be quite insensitive to tracking accuracy.
C. Spatial relations
Here, we describe a straightforward yet intuitive approach
to modeling pairwise spatial relations between objects, given
their point clouds in 3D space. We begin by defining an auxiliary coordinate frame, whose axes will directly determine
the left/right, above/below and front/behind directions. As mentioned previously, we assume a still camera
(robot’s eyes) looking at the planar surface where the manipulation takes place.
The auxiliary axes directions are calculated using general
prior knowledge about the axis orientations of our RGBD
sensor world coordinate frame, in which the object point
clouds are in known positions, and the workspace normal
vector that was estimated during the plane fitting step of
segmentation/tracking. Naturally, since the planar workspace
(initially) supports all the objects involved in the manipulation, we would want the above/below direction to be
directly defined by its normal vector n.

the orthogonal complement of v. Axis u is then uniquely
determined as being orthogonal to both v and w (e.g., so
that the resulting frame is right-handed). Let x̂, ŷ and ẑ be
the unit length vectors that are codirectional with the sensor
frame axes, n̂ be a unit length workspace normal and û, v̂, ŵ
be the unit length vectors codirectional with the “workspacealigned” auxiliary frame axes. The above construction, with
the axis directions chosen as in Fig. 3(b), is captured by the
following equations:
v̂ = sgn (ŷ · n̂) n̂,
ŵ = (ẑ − (v̂ · ẑ) v̂) /kẑ − (v̂ · ẑ) v̂k,
û = v̂ × ŵ,
where a · b is the dot product of vectors a and b, a × b is
their cross product, sgn is the signum function and kxk is the
Euclidean norm of x. Table I defines the 6 spatial relation
defining directions in terms of the auxiliary frame axes.
TABLE I
S PATIAL RELATION DEFINING DIRECTIONS
Direction

left

right

front

behind

above

below

Reference
vector

−û

+û

−ŵ

+ŵ

−v̂

+v̂

To infer spatial relations between objects, we will first
build models for the regions of 3D space relative to an
object. For example, to reason about some object being on
the right of object X at some given time, we first explicitly
model the space region on the right of X. We will consider
7 space regions with respect to a given object: one that
will represent the object interior (for the in relation) and
6 around the object, aligned to the directions of Table I.
We will represent all these relative spaces of X as convex
polyhedra and denote them as Sr (X), for r ∈ {in, left,
right, front, behind, below, above}, so that, for
example, Sright (X) is the space region on the right of X.

(a)
Fig. 4.
(a)

(b)
Fig. 3.

Coordinate frames.

Our sensor coordinate frame is drawn in Fig. 3(a). As we
can see, the z-axis corresponds to perceived depth, in roughly
the front/behind direction, while the x and y axes point
approximately to the right and downwards, respectively. This
frame can be rotated so that the new axes, u, v and w,
are aligned with the workspace and the aforementioned
directions with respect to it (Fig. 3(b)). Axis v can be set
to be parallel to n (e.g., pointing downwards). Axis w can
then be defined as the projection of the original z-axis to

(b)
Directional relative spaces.

We model Sin (X) simply as the smallest bounding box
(cuboid) of X that is aligned with the u, v, w axes (Fig. 3(b)
for the blue point cloud). The 6 “directional” relative spaces
are built upon this one, as indicated in Fig. 4(a). Consider
a uvw-aligned cube, concentric to Sin (X), of edge length
significantly larger than the maximum workspace dimension.
Clearly, each face of Sin (X) with the closest face of the
surrounding cube that is parallel to it can uniquely define an
hexahedron that also has these two as faces. We will use these
6 hexahedra as models for our directional relative spaces. In
Fig. 4(b), we draw Sright (X), where X is represented by
the blue point cloud.

1391

A few remarks are in order. First, all relative spaces are
represented as sets (conjunctions) of linear constraints, so
checking if a point lies in them is very easy. Second, one
would expect the directional relative spaces to be unbounded
(e.g., polyhedral cones). They can be modeled this way
by simply dropping one of their defining linear constraints.
Finally, while more elaborate models can be considered for
Sin (X), like the convex hull of X, the construction of
Fig. 4(a) has the nice property of partitioning the space
around X. This will enable us to easily reason about object
relations in a probabilistic manner, in the sense that our (realvalued) relation predicates will define the spatial distribution
of an object relative to another. Clearly, this provides a
more flexible framework than modeling using binary-valued
predicates, which can be simply inferred anyway.
We are now ready to define our models for a set of
basic pairwise spatial object relations. We will model 7 of
them (the inclusion relation and the 6 “directional” ones)
directly based on their respective relative space, and an
additional one that is indicative of whether two objects are in
physical contact (touch). Let Rf = {in, left, right,
front, behind, below, above, touch} be our full set
of spatial relations. We note that these primitive relations
correspond to the spatial prepositions most commonly used
in natural language and can be used to model more complex
relations (e.g., we can reason about on based on above
and touch). For each r ∈ Rf , we will define a pairwise
predicate Rr (Xit , Xjt ) that quantifies exactly whether Xit
is positioned relatively to Xjt according to relation r. For
example, Rleft (Xit , Xjt ) indicates to what degree object i is
on the left of object j at time t.
Let Rs = Rf {touch} be the set of relations that
can be defined by an explicit space region with respect to
Xjt (reference object). Instead of making hard decisions, i.e.
using binary values, we let the predicates Rr (Xit , Xjt ), for
r ∈ Rs , assume real values in [0, 1] that represent the fraction
of Xit that is positioned according to r with respect to Xjt .
A reasonable way to evaluate Rr (Xit , Xjt ), for r ∈ Rs , is
then simply as the fraction of points of Xit that lie in the
relative space Sr (Xjt ):

  
Rr (Xit , Xjt ) = Xit ∩ Sr (Xjt ) / Xit  ,
(1)
where by |A| we denote the cardinality of set A. Since
Sr (Xjt ), for r ∈ Rs , are mutually disjoint by construction,
the predicates of (1) have the property:
X
Rr (Xit , Xjt ) = 1,
r∈Rs

i.e. they define a distribution of the points of Xit to the
relative spaces of Xjt .
The touch relation can be useful in capturing other, nonprimitive spatial relations. For example, by having models
for both above and touch, one can express the on
relation as their “conjunction”. More expressive power is
desirable, as it can translate to better discriminative performance for our representation. We let our contactual predicate
Rtouch (Xit , Xjt ) assume binary values in {0, 1}. Whenever
either of Rin (Xit , Xjt ) or Rin (Xjt , Xit ) is greater than zero,

we can assume that Xit is touching Xjt and, therefore,
Rtouch (Xit , Xjt ) = 1. If this were the defining condition
for Rtouch , incorporating the contactual predicate to our set
of spatial relations would be redundant, in the sense that
we already model Rin . However, there are cases where both
Rin (Xit , Xjt ) and Rin (Xjt , Xit ) are equal to zero, while Xit
is touching Xjt . We may fail to detect this situation using the
above intersection test for various reasons. For example, the
two objects could simply be extremely close to each other
(touching), without part of one lying inside the other. The
precision of our sensor and the accuracy of our tracker can
also cause the above condition to falsely fail. To compensate
for these situations, whenever both Rin (Xit , Xjt ) = 0 and
Rin (Xjt , Xit ) = 0, in which case Xit and Xjt are guaranteed
to be linearly separable point sets, we perform an additional
“proximity” test. We train a linear binary SVM on Xit ∪ Xjt ,
with the class labels given by the object ownership for each
data point, and use the classifier margin, dm , as a measure
of distance between Xit and Xjt . If dm falls below a preset
threshold dT , we set Rtouch (Xit , Xjt ) = 1. Our complete
model of the contactual predicate
is then given by:

1
if
Rin (Xit , Xjt ) > 0




or Rin (Xjt , Xit ) > 0
Rtouch (Xit , Xjt ) =

or dm < dT ,



0 otherwise.
The value of dT depends, among other things, on point cloud
precision related parameters (e.g., sensor and tracking errors)
and was set to a few millimeters in our trials. We note that,
of all relations in Rf , only touch is symmetric.
Spatial abstraction. At this point, we have defined our
models for all relations r ∈ Rf . Our goal is to define
an action representation using the temporal evolution of
spatial object relations. However, tracking all relations in
Rf can yield a representation that is viewpoint-specific or
unnecessarily execution-specific. For example, disambiguating between left and right or front and behind or,
actually, any two of these is clearly dependent on the sensor
viewpoint and might not be informative about the actual
manipulation semantics. As an example, consider a “stir the
coffee” action that involves a cup and a spoon. Picking up
the spoon from the left of the cup, then stirring the coffee and
finally leaving the spoon on the right of the cup is expected to
have the same high-level semantics as picking up the spoon
from the right of the cup, stirring and then leaving it in
front of the cup. For this reason, we combine the relations
{left, right, front, behind} into one, which we can
simply name around, to obtain a desirable kind of spatial
abstraction that, in most cases we can think of, does not
leave out information that is actually manipulation-specific.
The new relation can be viewed as the disjunction of the 4
ones it replaces and its predicate is given by:
Raround (Xit , Xjt ) =Rleft (Xit , Xjt ) + Rright (Xit , Xjt )+
Rfront (Xit , Xjt ) + Rbehind (Xit , Xjt ).
This makes Ra = {in, around, below, above, touch}
the set of relations upon which we will build our action
descriptors.

1392

D. Action descriptors
Let Φt (i, j) be the |Ra |-dimensional vector of all relation
predicates Rr (Xit , Xjt ), r ∈ Ra , for object i relative to object
j at time t, arranged in a fixed relation order, e.g., (in,
around, below, above, touch), so that:

Φt (i, j) ≡ Rin (Xit , Xjt ), . . . , Rtouch (Xit , Xjt ) ,
(2)
where i, j = 1, . . . , No and i 6= j. Let Φ(i, j) denote the
sequence of the predicate vectors (2), for t = 1, . . . , T :

Φ(i, j) ≡ Φ1 (i, j), . . . , ΦT (i, j) .
The latter captures the temporal evolution of all spatial
relations in Ra of object i with respect to object j throughout
the duration of the manipulation execution. We will call
Φ(i, j) a Predicate Vector Sequence (PVS). PVSes constitute
the building block of our action descriptors and can be
represented as |Ra | × T matrices.
Our proposed action descriptors will contain the PVSes
for all object pairs in the manipulation. As will become
clear in the next subsection, comparing two action descriptors reduces to finding an optimal correspondence between
their involved objects, e.g., infer that object i1 in the first
action corresponds to object i2 in the second. To facilitate
this matching task, we require our proposed descriptors to
possess two properties.
The first has to do with the fact that the spatial relations
we consider are not symmetric. A simple solution to fully
capture the temporally evolving spatial relations between
objects i and j, where none of the objects acts as a reference
point, is to include both Φ(i, j) and Φ(j, i) in our descriptor,
for i, j = 1, . . . , No and i 6= j. This might seem redundant,
but, given our predicate models, there is generally no way
to exactly infer Φt (j, i) from Φt (i, j) (e.g., due to different
object dimensions). Our descriptor will then consist of Nr =
No (No − 1) PVSes, as many as the ordered object pairs.
Finally, we need to be able to identify which (ordered)
object pair a PVS refers to, i.e. associate every PVS in
our representation with a tuple (i, j) of object indices. We
opted to do this implicitly, by introducing a reverse indexing
function and encoding the mapping information in the order
in which the PVSes are stored. Any bijective function INo
from {1, . . . , Nr }, the set of PVS indices, onto {(i, j) | i, j =
1, . . . , No ∧ i 6= j}, the set of ordered object index pairs, is
a valid choice as an indexing function. Our proposed action
descriptors are then ordered sets of the form:
A ≡ (Φ1 , . . . , ΦNr ) ,
where, for k = 1, . . . , Nr , Φk = Φ(i, j) and (i, j) = INo (k).
Function INo can be embedded in the descriptor extraction
process. Utilizing the knowledge of the PVS ordering within
an action descriptor will simplify the formulation of establishing an object correspondence between two actions in the
following subsection.
E. Distance measure
To complete our proposed representation, we now introduce an appropriate distance function d on the descriptors
we defined above. If A1 and A2 are two action descriptors,

we design d(A1 , A2 ) to be a symmetric function that gives
a time-normalized distance between the two actions. Additionally, we allow comparisons between manipulations that
involve different numbers of objects. This will enable us to
reason about the similarity between an action and a subset
(in terms of the objects it involves) of another action. In the
following, for k = 1, 2, let Nrk be the number of PVSes in
Ak and Nok be the number of objects in manipulation Ak ,
so that Nok (Nok − 1) = Nrk .
At the core of our action distance evaluation lies the
comparison between PVSes. Each PVS is a time series of
|Ra |-dimensional feature vectors that captures the temporal
evolution of all spatial relations between an ordered object
pair. Different action executions have different durations and
may also differ significantly in speed during the course of
manipulation: e.g., certain subtasks may be performed at
different speeds in different executions of semantically identical manipulations. To compensate for timing differences,
we use the Dynamic Time Warping (DTW) [21] algorithm
to calculate time-normalized, pairwise PVS distances. We
consider the symmetric form of the algorithm in [21], with
no slope constraint or adjustment window restriction.
Let Φ1r1 and Φ2r2 be PVSes in A1 and A2 , respectively,
where r1 = 1, . . . , Nr1 and r2 = 1, . . . , Nr2 . We form the
Nr1 ×Nr2 matrix C = (cr1 r2 ) of all time-normalized distances
between some PVS in A1 and some PVS in A2 , where:
cr1 r2 = DTW(Φ1r1 , Φ2r2 ).
In the following, we will calculate d(A1 , A2 ) as the total cost
of an optimal correspondence of PVSes between A1 and A2 ,
where the cost of assigning Φ1r1 to Φ2r2 is given by cr1 r2 .
One could simply seek a minimum cost matching of
PVSes between two action descriptors by solving the linear
assignment combinatorial problem, with the assignment costs
given in C (e.g., by means of the Hungarian algorithm [22]).
This would yield an optimal cost PVS correspondence between the two actions. However, it is clear that the latter does
not necessarily translate to a valid object correspondence.
Instead, we directly seek an object correspondence between
A1 and A2 that induces a minimum cost PVS correspondence. The object correspondence between A1 and A2 can
be represented as an No1 × No2 binary-valued assignment
matrix X = (xij ), where xij = 1 if and only if object
i in A1 , i = 1, . . . , No1 , is matched to object j in A2 ,
j = 1, . . . , No2 . We require that every row and every column
of X has at most one nonzero entry and that the sum of
all entries in X is equal to min(No1 , No2 ). This ensures that
X defines an one-to-one mapping from the objects in the
action involving the fewest objects to the objects in the other
action. An object assignment X is then evaluated in terms
of the PVS correspondence it defines. We denote the latter
by YX = (yr1 r2 ), where yr1 r2 = 1 if and only if PVS r1
in A1 is mapped to r2 in A2 . The Nr1 × Nr2 matrix YX has
the same structure as X, with min(Nr1 , Nr2 ) nonzero entries.
The cost of assignment X, in terms of its induced PVS
assignment YX , is then given by the sum of all individual

1393

PVS assignment costs:

IV. E XPERIMENTS
Nr2

Nr1

J(YX ) =

X X

cr 1 r 2 y r 1 r 2 .

(3)

r 1 =1 r 2 =1

According to our descriptor definition in the previous subsection, the PVS with index r1 in A1 refers to the ordered
object pair (o11 , o12 ) = INo1 (r1 ) and, similarly, r2 in A2 refers
to (o21 , o22 ) = INo2 (r2 ) (superscripts indicate action). Clearly,
yr1 r2 is nonzero if and only if object o11 in A1 is assigned
to o21 in A2 and o12 in A1 is assigned to o22 in A2 :
yr1 r2 = xo11 o21 xo12 o22 .
Using the above, we can rewrite and optimize (3) in terms of
the object assignment variables xij , for i = 1, . . . , No1 and
j = 1, . . . , No2 :
1

Minimize
X

where
subject to

J(X) =

2

Nr
Nr
X
X

cr1 r2 xo1 o2 xo1 o2
1 1

2 2

r 1 =1 r 2 =1
= INo1 (r1 ),

(o11 , o12 )
(o21 , o22 ) = INo2 (r2 )
2
PNo
i = 1, . . . , No1
j=1 xij ≤ 1,
PNo1
j = 1, . . . , No2
i=1 xij ≤ 1,
PNo1 PNo2
1
2
j=1 xij = min(No , No )
i=1
xij ∈ {0, 1},

i ∈ {1, . . . , No1 },
j ∈ {1, . . . , No2 }.

The binary quadratic program above encodes an instance
of the quadratic assignment problem (QAP), which is NPhard. QAP instances of size (number of objects) N > 30 are
considered intractable [23]. However, most manipulations of
practical interest involve a number of objects well below that
limit. In our implementation, we used the SCIP (constraint
integer programming) solver [24]. To evaluate the correctness
and running time behavior of our optimization scheme,
we ran a small number of tests. For various numbers of
objects N , we built an action descriptor A1 by randomly
generating N (N − 1) PVSes. We constructed A2 from A1
based on an arbitrary object permutation (assignment), by
rearranging the PVSes of A1 (according to IN and the
known object permutation) and adding Gaussian noise to
them. Minimization of J(X) gave back the correct object
assignment, even for significant amounts of noise variance.
Running time for N = 10 objects was in the order of a few
(≈ 10) seconds on a laptop machine.
The minimum value of J(X), over all possible object
assignments, directly defines the distance between actions
A1 and A2 :
d(A1 , A2 ) = min(J(X)).
X

The function d(A1 , A2 ) is symmetric and, being a sum of
DTW distances, gives a time-normalized measure of action
dissimilarity. As noted before, d(A1 , A2 ) is also defined
when A1 and A2 involve different numbers of objects (No1 6=
No2 ). For example, if No1 < No2 , d(A1 , A2 ) is expected to be
exactly the same as if A2 only involved the No1 of its objects
to which the objects in A1 are assigned. This flexibility can
be useful in sub-action matching scenarios.

A. Data description
All our experiments were performed on a set of 21 RGBD
sequences of manipulation executions. All actions involve 2
objects and are partitioned in 4 distinct semantic classes:
• Pour: water poured from a pitcher into a bowl (8
executions).
• Transfer: small object placed inside a bowl (6 executions).
• Stack: a book placed on top of another (2 executions).
• Stir: bowl content stirred using a ladle (5 executions,
one of them performed by our robot).
Executions within each semantic class were performed by
various individuals and with various initial and final positions
of the manipulated objects. For example, in some instances of
Stir, the ladle was initially picked from the left of the bowl
and was finally placed on its left, in others, it was picked
from the right and then placed on the left, etc. Naturally,
there were significant timing differences across instances
of the same semantic class (different overall durations and
execution speeds at each action phase).
We also included a robot execution for an instance of
Stir (Fig. 5, bottom rows) that took roughly 4 times the
average human execution duration to complete and demonstrated disproportionately long “idle” phases throughout the
manipulation. Our robot platform is a standard Baxter humanoid with parallel grippers. To generate trajectories, we
used predefined dynamic movement primitives [25]. The
trajectory start and end points were given from the point
cloud segmentation and transferred onto the robot via a
standard inverse kinematics procedure. We also used visual
servoing to ensure a firm grasp of the tool.
B. Spatial relations evaluation
We begin with a quick evaluation of the performance of
our spatial relation models. To establish our ground truth, we
sampled 3 time instances from each execution sequence. To
evaluate a rich enough set of spatial relations, we sampled
at roughly the beginning, the middle and the end of each
manipulation. For each sample frame, we picked one of
the objects to act as reference (say X1 ) and picked the
relation r ∈ Rs = {in, left, right, front, behind,
below, above} that best described the position of the
second object, X2 , relative to X1 . For testing, we calculated
the spatial predicates Rr (X2 , X1 ), for all r ∈ Rs for all 63
annotated frames and labeled each according to the relation
of maximum predicate value. This gave a classification error
rate of 3/63 ≈ 4.8%. However, 2 of the errors were due
to tracking issues and not the spatial predicates per se: in
both Stack executions, significant part of the book at the
bottom overlapped the top one (the tracked point clouds were
barely distinguishable), so in dominated above. The third
error was from a Stir instance (during the stirring phase),
where we decided in with a ground truth of above, which
was the second largest predicate. Overall, we believe that, up
to severe tracking inaccuracy, our spatial relation estimation
can be considered reliable.

1394

Fig. 5. Spatial relations evolution: ladle relative to bowl
for two instances of
Stir.

In Fig. 5, we depict part of the temporal evolution of the
spatial predicate vector of the ladle relative to the bowl for
two executions of Stir (samples from the corresponding
PVS for all relations in Rf ): one performed by a human
and one by our robot. From the figure, we can see that our
system can reliably track the temporal evolution of spatial
relations in both observation and execution scenarios.
C. Action classification
To evaluate the discriminative performance of our proposed representation, we begin by forming the matrix D =
(dij ) of all pairwise distances for our Na = 21 manipulation
executions, where dij = d(Ai , Aj ), for i, j = 1, . . . , Na .
We depict the values of D in Fig. 6. As expected, D is
symmetric with zero diagonal. Manipulation executions of
the same semantic class were grouped together to consecutive
indices (e.g., 1-8 for Pour, 9-14 for Transfer, 15-16 for
Stack and 17-21 for Stir). Given this, the evident blockdiagonal structure of low distance values in D (blue regions)
suggests that the proposed representation can be quite useful
in classification tasks.

Fig. 6.
Matrix D = (dij )
of pairwise distances.

To confirm this intuitive observation, we considered a
clustering scenario by applying the Affinity Propagation
algorithm [26] on our data. Affinity Propagation is an unsupervised clustering algorithm that does not assume prior
knowledge of the number of classes. Instead, the resulting
number of clusters depends on a set of real-valued “preference” parameters, one for each data point, that express

Fig. 7. Clustering and
embedding of our action
descriptors in 2 dimensions, based on our similarity/distance measure.

how likely the point is to be chosen as a class “exemplar”
(cluster centroid). A common choice [26] is to use the
same preference value for all points, equal to the median
of all pairwise similarities. A similarity measure between
actions Ai and Aj , for i, j = 1, . . . , Na , is directly given
by sij = −dij . Clustering using this scheme resulted in
4 clusters that correctly corresponded to our 4 semantic
classes and there were no classification errors. In Fig. 7,
we plot a 2-dimensional embedding of the action descriptors
for all executions, where we use the same color for all data
points of the same cluster and mark the cluster centroids.
The outcome of our simple clustering experiment confirms
our intuition about matrix D (Fig. 6), suggesting that our
proposed abstract representation is indeed descriptive of the
actual high-level manipulation semantics.
It is worth noting that the descriptor for the robot stirring scenario was correctly classified as a Stir instance
(marked in Fig. 7). This empirically shows that, even when
the specific movement trajectories are quite different, e.g.,
between human trials and robot executions, our PVS-based
representations remain relatively invariant under the proposed distance function. Thus, our learned from observation
manipulation representations could be used to provide additional constraints for robot control policies.
V. C ONCLUSIONS AND FUTURE WORK
In this paper, we introduced our direct take on grounding
spatial relations, by properly partitioning the space around
an object to a set of relative spaces, and then proposed a
novel compact representation that captures the geometric
object interactions during the course of a manipulation.
Experiments conducted on both human and robot executions

1395

validate that 1) our relative space models successfully capture
the geometric interpretation of their respective relation and
the corresponding predicates can be reliably evaluated; 2)
the temporal evolution of object-wise spatial relations, as
encoded in our abstract representation, is indeed descriptive
of the underlying manipulation semantics.
Knowledge of models for spatial relations for common
natural language prepositions is a very strong capability by
itself. First, it enables certain types of nontrivial spatial
reasoning, which is a fundamental aspect of intelligence.
Second, it narrows the gap between observation and execution, with language acting as the bridge. Particularly, from
the human-robot interaction perspective, a robot equipped
with these models will be able to answer a rich repertoire
of spatial queries and understand commands such as: “pick
up the object on the left of object X and in front of object
Y”. Another such task, closer in spirit to this work, is the
automatic generation of natural language descriptions for
observed actions.
In this work, the matching of our spatial-relation sequence
representations is performed at once on whole sequences.
Currently, we are investigating the possibility of modifying
the matching algorithm into an online one. An online action
matching algorithm is needed if we want a fast system
that observes actions and is able to predict during their
course (e.g., for monitoring the correctness of execution).
Additionally, here, we only took into account one aspect
of manipulation actions: object-wise spatial relations. A
complete action model, that would attempt to bridge the gap
between observation and execution, needs to be a multi-layer
combination of spatial relations and many other aspects, such
as movement trajectories, objects, goals, etc.
Acknowledgements. This research was funded by the
support of the European Union under the Cognitive Systems program (project POETICON++), the National Science
Foundation under INSPIRE grant SMA 1248056, and by
DARPA through U.S. Army grant W911NF-14-1-0384 under
the project: Shared Perception, Cognition and Reasoning for
Autonomy.
R EFERENCES
[1] W. Yi and D. Ballard, “Recognizing behavior in hand-eye coordination
patterns,” International Journal of Humanoid Robotics, vol. 6, no. 03,
pp. 337–359, 2009.
[2] H. Kjellström, J. Romero, D. Martı́nez, and D. Kragić, “Simultaneous
visual recognition of manipulation actions and manipulated objects,”
Proceedings of the 2008 IEEE European Conference on Computer
Vision, pp. 336–349, 2008.
[3] A. Gupta and L. Davis, “Objects in action: An approach for combining
action understanding and object perception,” in Proceedings of the
2007 IEEE International Conference on Computer Vision and Pattern
Recognition, 2007, pp. 1–8.
[4] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, “Imitation learning
of dual-arm manipulation tasks in humanoid robots,” International
Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183–202, 2008.
[5] T. Asfour, P. Azad, N. Vahrenkamp, K. Regenstein, A. Bierbaum,
K. Welke, J. Schrder, and R. Dillmann, “Toward humanoid manipulation in human-centred environments,” Robotics and Autonomous
Systems, vol. 56, pp. 54–65, 2008.

[6] A. Guha, Y. Yang, C. Fermüller, and Y. Aloimonos, “Minimalist plans
for interpreting manipulation actions,” in Proceedings of the 2013
International Conference on Intelligent Robots and Systems. Tokyo:
IEEE, 2013, pp. 5908–5914.
[7] E. E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and
F. Wörgötter, “Learning the semantics of object-action relations by
observation,” I. J. Robotic Res., vol. 30, no. 10, pp. 1229–1249, 2011.
[8] Y. Yang, C. Fermüller, and Y. Aloimonos, “Detection of manipulation
action consequences (MAC),” in Proceedings of the 2013 IEEE
Conference on Computer Vision and Pattern Recognition. Portland,
OR: IEEE, 2013, pp. 2563–2570.
[9] M. Wächter, S. Schulz, T. Asfour, E. Aksoy, F. Wörgötter, and
R. Dillmann, “Action sequence reproduction based on automatic segmentation and object-action complexes,” in IEEE/RAS International
Conference on Humanoid Robots (Humanoids), 2013, pp. 0–0.
[10] J. Maitin-Shepard, M. Cusumano-Towner, J. Lei, and P. Abbeel, “Cloth
grasp point detection based on multiple-view geometric cues with
application to robotic towel folding,” in Proceedings of the 2010 IEEE
International Conference on Robotics and Automation. IEEE, 2010,
pp. 2308–2315.
[11] B. Moldovan, P. Moreno, M. van Otterlo, J. Santos-Victor, and
L. De Raedt, “Learning relational affordance models for robots in
multi-object manipulation tasks,” in Proceedings of the 2010 IEEE
International Conference on Robotics and Automation. IEEE, 2012,
pp. 4373–4378.
[12] M. R. Dogar, M. C. Koval, A. Tallavajhula, and S. Srinivasa, “Object
search by manipulation,” in Robotics and Automation (ICRA), 2013
IEEE International Conference on. IEEE, 2013.
[13] C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D. V.
Dimarogonas, and D. Kragic, “Dual arm manipulation: A survey,”
Robotics and Autonomous Systems, 2012.
[14] A. Gupta and L. S. Davis, “Beyond nouns: Exploiting prepositions
and comparative adjectives for learning visual classifiers.” in ECCV
(1), ser. Lecture Notes in Computer Science, D. A. Forsyth, P. H. S.
Torr, and A. Zisserman, Eds., vol. 5302. Springer, 2008, pp. 16–29.
[15] S. Guadarrama, L. Riano, D. Golland, D. Gohring, Y. Jia, D. Klein,
P. Abbeel, and T. Darrell, “Grounding spatial relations for humanrobot interaction,” in IEEE/RSJ International Conference on Intelligent
Robots and Systems, November 2013.
[16] D. Golland, P. Liang, and D. Klein, “A game-theoretic approach to
generating spatial descriptions,” in Proceedings of the 2010 conference
on empirical methods in natural language processing. Association
for Computational Linguistics, 2010, pp. 410–419.
[17] B. Rosman and S. Ramamoorthy, “Learning spatial relationships
between objects,” I. J. Robotic Res., vol. 30, no. 11, pp. 1328–1342,
2011.
[18] S. Fichtl, A. McManus, W. Mustafa, D. Kraft, N. Kruger, and
F. Guerin, “Learning spatial relationships from 3D vision using
histograms,” in Robotics and Automation (ICRA), 2014 IEEE International Conference on, May 2014, pp. 501–508.
[19] D. Fox, “KLD-Sampling: Adaptive Particle Filters,” in Advances in
Neural Information Processing Systems 14 (NIPS 2001). MIT Press,
2001, pp. 713–720.
[20] R. B. Rusu and S. Cousins, “3D is here: Point Cloud Library (PCL),” in
IEEE International Conference on Robotics and Automation (ICRA),
Shanghai, China, May 9-13 2011.
[21] H. Sakoe and S. Chiba, “Dynamic programming algorithm optimization for spoken word recognition,” Acoustics, Speech and Signal
Processing, IEEE Transactions on, vol. 26, no. 1, pp. 43–49, Feb
1978.
[22] H. W. Kuhn, “The hungarian method for the assignment problem,”
Naval research logistics quarterly, vol. 2, no. 1-2, pp. 83–97, 1955.
[23] E. M. Loiola, N. M. M. de Abreu, P. O. Boaventura-Netto, P. Hahn,
and T. Querido, “A survey for the quadratic assignment problem,”
European Journal of Operational Research, vol. 176, no. 2, pp. 657–
690, 2007.
[24] T. Achterberg, “SCIP: Solving constraint integer programs,” Mathematical Programming Computation, vol. 1, no. 1, pp. 1–41, July 2009.
[25] A. J. Ijspeert, J. Nakanishi, and S. Schaal, “Movement imitation
with nonlinear dynamical systems in humanoid robots,” in Robotics
and Automation, 2002. Proceedings. ICRA’02. IEEE International
Conference on, vol. 2. IEEE, 2002, pp. 1398–1403.
[26] B. J. Frey and D. Dueck, “Clustering by passing messages between
data points,” Science, vol. 315, pp. 972–976, 2007.

1396

arXiv:1611.05896v1 [cs.CV] 17 Nov 2016

Answering Image Riddles using Vision and Reasoning through
Probabilistic Soft Logic
Somak Aditya
Yezhou Yang
Chitta Baral
Arizona State University, Tempe, AZ

Yiannis Aloimonos
University of Maryland, College Park

{saditya1,yz.yang,chitta}@asu.edu

yiannis@cs.umd.edu

Abstract

(described in words) that is invoked by all the images in that
set. Often the common concept is not something that even
a human can observe in her first glance but can come up
with after some thought about the images. Hence the word
“riddle” in the phrase “image riddles”. Figure 1 shows an
example of an image riddle. The images individually connect to multiple concepts such as: outdoors, nature, trees,
road, forest, rainfall, waterfall, statue, rope, mosque etc.
On further thought, the common concept that emerges for
this example is “fall”. Here, the first image represents the
fall season (concept). There is a “waterfall” (region) in the
second image. In the third image, it shows “rainfall” (concept) and the fourth image depicts that a statue is “fall”ing
(action/event). The word “Fall” is invoked by all the images
as it shows logical connections to objects, regions, actions
or concepts specific to each image.
In addition, the answer also connects the most significant1 aspects of the images. Other possible answers like
“nature” or “outdoors” do not demonstrate such properties.
They are too general. In essence, image riddles is a challenging task that not only tests our ability to detect visual
items in a set of images, but also tests our knowledge and
our ability to think and reason.
Based on the above analysis, we argue that a system
should have the following capabilities to answer Image Riddles appropriately: i) the ability to detect and locate the objects, regions, and their properties; ii) the ability to recognize actions; iii) the ability to infer concepts from the detected words; and iv) the ability to rank a concept (described
in words) based on its relative appropriateness; in other
words, the ability to reason with and process background or
commonsense knowledge about the semantic similarity and
relations between words and phrases. These capabilities, in
fact, are also desired of any automated system that aims to
understand a scene and answer questions about it. For example, in VQA dataset [1], “Does this man have children”,
“Is this a vegetarian Pizza?” are some such examples, where
one needs explicit commonsense knowledge.

In this work, we explore a genre of puzzles (“image riddles”) which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and,
knowledge-based or commonsense reasoning. We compile
a dataset of over 3k riddles where each riddle consists of 4
images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an
automatic evaluation metric to track future progress. Our
task bears similarity with the commonly known IQ tasks
such as analogy solving, sequence filling that are often used
to test intelligence.
We develop a Probabilistic Reasoning-based approach
that utilizes probabilistic commonsense knowledge to answer these riddles with a reasonable accuracy. We demonstrate the results of our approach using both automatic and
human evaluations. Our approach achieves some promising
results for these riddles and provides a strong baseline for
future attempts. We make the entire dataset and related materials publicly available to the community in ImageRiddle
Website (http://bit.ly/22f9Ala).

1. Introduction

Figure 1. An Image Riddle Example. Question: “What word connects these images?” .

A key component of computer vision is understanding of
images and it comes up in various tasks such as image captioning and visual question answering (VQA). In this paper,
we propose a new task of “image riddles” which requires
deeper and conceptual understanding of images. In this task
a set of images are provided and one needs to find a concept

1 Formally, an aspect is as significant as the specificity of the information it contains.

1

These riddles can be thought of as a visual counterpart to IQ test question types such as sequence filling
(x1 , x2 , x3 , ?) and analogy solving (x1 : y1 :: x2 : ?)2
where one needs to find commonalities between items. This
task is different from traditional VQA, as in VQA the
queries provide some clues regarding what to look for in the
image in question. Most riddles in this task require both superior detection and reasoning capabilities, whereas a large
percentage (of questions) of the traditional VQA dataset
tests system’s detection capabilities. This task differs from
both VQA and Captioning in that this task requires analysis
of multiple images. While video analysis may require analysis of multiple images, this task of “image riddles” focuses
on analysis of seemingly different images.
Hence, this task of Image Riddles is simple to explain;
shares similarities with well-known and pre-defined types
of IQ questions and it requires a combination of vision and
reasoning capabilities. In this paper, we introduce a promising approach in tackling the problem.
In our approach, we first use state-of-the-art Image Classification techniques [21] to get the top identified classlabels from each image. Given these probabilistic detections, we use the knowledge of connections and relations
of these words to infer a set of most probable words (or
phrases). We use ConceptNet 5 [15] as the source of commonsense and background knowledge that encodes the relations between words and short phrases using a structured
graph. Note, the possible range of candidates are the entire vocabulary of ConceptNet 5 (roughly 0.2 million). For
representation and reasoning with this huge probabilistic
knowledge we use the Probabilistic Soft Logic (PSL) [10, 2]
framework3 . Given the inferred words for each image, we
then infer the final set of answers for each riddle.
Our contributions are threefold: i) we introduce the 3K
Image Riddles Dataset; ii) we present a probabilistic reasoning approach to solve the riddles with reasonable accuracy;
iii) our reasoning module inputs detected words (a closed
set of class-labels) and logically infers all relevant concepts
(belonging to a much larger vocabulary).

to directly adopt topic model-based or Zero-shot learningbased approaches.
Our work is also related to the field of Visual Question
Answering. Very recently, researchers spent a significant
amount of efforts on both creating datasets and proposing
new models [1, 18, 6, 17]. Interestingly both [1] and [6]
adapted MS-COCO [14] images and created an open domain dataset with human generated questions and answers.
Both [18] and [6] use recurrent networks to encode the sentence and output the answer.
Even though some questions from [1] and [6] are very
challenging which actually require logical reasoning in order to answer correctly, popular approaches are still hoping
to learn the direct signal-to-signal mapping from image and
question to its answer, given a large enough annotated data.
The necessity of common-sense reasoning could be easily
neglected. Here we introduce the new Image Riddle problem which is 1) a well-defined cognitively challenging task
that requires both vision and reasoning capability, 2) it is
impossible to model the problem as direct signal-to-signal
mapping, due to the data sparsity and 3) system’s performance could still be bench-marked automatically for comparison. All these qualities make our Image Riddle dataset
a good testbed for vision and reasoning research.

3. Background
In this Section, we briefly introduce the different techniques and Knowledge Sources used in our system.

3.1. Probabilistic Soft Logic (PSL)
PSL is a recently proposed framework for Probabilistic Logic [10, 2]. A PSL model is defined using a set of
weighted if-then rules in first-order logic.
Let C = (C1 , ..., Cm ) be such a collection where each
Cj is a disjunction of literals, where each literal is a variable
yi or its negation ¬yi , where yi ∈ y. Let Ij+ (resp. Ij− ) be
the set of indices of the variables that are not negated (resp.
negated) in Cj . Each Cj can be written as:
wj : ∧i∈I − yi → ∨i∈I + yi
(1)
j
W j
or equivalently, wj : ∨i∈I − (¬yi ) ∨i∈I + yi . Each rule Cj
j
j
is associated with a non-negative weight wj . PSL relaxes
the boolean truth values of each ground atom a (constant
term or predicate with all variables replaced by constants)
to the the interval [0, 1], denoted by I(a). To compute soft
truth values for logical formulas, Lukasiewiczs relaxation
[11] of conjunctions (∧), disjunctions (∨) and negations (¬)
is used :
I(l1 ∧ l2 ) = max{0, I(l1 ) + I(l2 ) − 1}

2. Related Work
The problem of Image Riddles has some similarities to
the genre of topic modeling [3] and Zero-shot Learning
[13]. However, this dataset imposes a few unique challenges: i) the possible set of target labels is the entire Natural Language vocabulary; ii) each image, when grouped
with different set of images can map to a different label; iii)
almost all the target labels in the dataset are unique (3k examples with 3k class-labels). These challenges make it hard

I(l1 ∨ l2 ) = min{1, I(l1 ) + I(l2 )}
2 Examples are:

word analogy tasks (male : female :: king : ?); numeric
sequence filling tasks: (1, 2, 3, 5, ?).
3 PSL is shown to be a powerful framework for high-level Computer
Vision tasks like Activity Detection [16].

(2)

I(¬l1 ) = 1 − I(l1 )
In PSL, the ground atoms are considered as random variables and the distribution is modeled using Hinge-Loss
2

Markov Random Field, which is defined as follows:

NELL [23, 20]; ConceptNet has a more extensive coverage of English language words and phrases. These properties make this Knowledge Graph a perfect source for the
required probabilistic commonsense knowledge.

Definition 3.1. Let y and x be two vectors of n and
n0 random variables respectively, over the domain D =
0
[0, 1]n+n . The feasible set D̃ is a subset of D, defined as:

3.3. Word2vec

 (y,x)=0,∀k∈E 	

D̃ = (y, x) ∈ Dcckk (y,x)≤0,∀k∈I

Word2vec uses the theory of distributional semantics4
to capture word meanings and produce word embeddings
(vectors). The pre-trained word-embeddings have been successfully used in numerous Natural Language Processing
applications and the induced vector-space is known to capture the graded similarities between words with reasonable
accuracy [19]. Throughout the paper, for word2vec-based
similarities, we use the 3 Million word-vectors trained on
Google-News corpus [19].

where c = (c1 , ..., cr ) are linear constraint functions associated with the index sets E and I denoting equality and
inequality constraints. A Hinge-Loss Markov Random Field
P is a probability density, defined as: if (y, x) ∈
/ D̃, then
P(y|x) = 0; if (y, x) ∈ D̃, then:
P(y|x) =

1
exp(−fw (y, x))
Z(w, x)

(3)

R
where Z(w, x) = y|(y,x)∈D̃ exp(−fw (y, x))dy.
The hinge-loss energy function fw is defined as:
m
P
fw (y, x) =
wj (max{lj (y, x), 0})pj , where wj ’s are

4. Approach
Given a set of images (in our case four: {I1 , I2 , I3 , I4 }),
the objective is to determine a set of ranked words (T ) based
on how well the word semantically connects these image.
In this work, we present an approach that uses Probabilistic
Reasoning on top of a probabilistic Knowledge Base (ConceptNet). It also uses additional semantic knowledge of
words from Word2vec. Using these knowledge sources, we
predict the answers to the riddles.

j=1

non-negative free parameters and lj (y, x) are linear constraints over y, x and pj = {1, 2}.
The final inference objective of HL-MRF is:
P(y|x) ≡ arg min

m
X

wj (max{lj (y, x), 0})pj

(4)

y∈[0,1]n j=1

In PSL, each logical rule Cj in the database C is used to
define lj (y, x) i.e. the linear constraints over (y, x). Given
a set of weighted logical formulas, PSL builds a graphical
model defining a probability distribution over the continuous space of values of the random variables in the model.
The final optimization problem is defined in terms of
“distance to satisfaction”. For each rule Cj ∈ C this
distance
satisfaction P
is measured using	 the term wj ×
 toP
max 1 − i∈I + yi − i∈I − (1 − yi ), 0 . This encodes
j
j
the penalty to the system if a rule is not satisfied. The final
optimization problem becomes:
arg min
y∈[0,1]n

X

X
X

	
wj max 1 −
yi −
(1 − yi ), 0

Cj ∈C

i∈Ij+

4.1. Outline of our Framework
Algorithm 1. Solving Riddles
1: procedure U N R IDDLER(I = {I1 , I2 , I3 , I4 }, Kcnet )
2:
for Ik ∈ I do
3:
P̃ (Sk |Ik ) = getClassLabelsNeuralNetwork(Ik ).
4:
for s ∈ Sk do
5:
Ts , Wm (s, Ts ) = retrieveTargets(s, Kcnet );
Wm (s, tj ) = sim(s, tj )∀tj ∈ Ts
6:
end for
7:
Tk = rankTopTargets(P̃ (Sk |Ik ), TSk , Wm );
8:
I(T̂k ) = inferConfidenceStageI(Tk , P̃ (Sk |Ik )).
9:
end for
10:
I(T ) = inferConfidenceStageII([T̂k ]4k=1 , [P̃ (Sk |Ik )]4k=1 ).
11: end procedure

(5)

i∈Ij−

.

As outlined in algorithm 1, for each image Ik (here,
k ∈ {1, ..., 4}), we follow three stages to infer related words
and phrases: i) Image Classification: we get top class labels
and the confidence from Image Classifier (Sk , P̃ (Sk |Ik )),
ii) Rank and Retrieve: using these labels and confidence
scores, we rank and retrieve top related words from ConceptNet (Kcnet ), iii) Probabilistic Reasoning and Inference
(Stage I): using the labels (Sk ) and the top related words
(Tk ), we design an inference model to logically infer final
set of words (T̂k ) for each image. Lastly, we use another
probabilistic reasoning model (Stage II) on the combined
set of inferred words (targets) from all images in a riddle.

3.2. ConceptNet
ConceptNet [22], is a multilingual Knowledge Graph,
that encodes commonsense knowledge about the world and
is built primarily to assist systems that attempts to understand natural language text. The knowledge in ConceptNet
is semi-curated. The nodes (called concepts) in the graph
are words or short phrases written in natural language. The
nodes are connected by edges (called assertions) which are
labeled with meaningful relations (selected from a welldefined closed set of relation-labels). For example: (reptile,
IsA, animal), (reptile, HasProperty, cold blood) are some
edges. Each edge has an associated confidence score. Also,
compared to other knowledge-bases like WordNet, YAGO,

4 The

3

central idea is: “a word is known by the company it keeps”.

4.3.1

This model assigns the final confidence scores on the combined set of targets (T ). The pipeline followed for each
image is depicted with an example in Figure 2.

Retrieve Related Words For a Seed

Visual Similarity: We observe that, for objects, the
ConceptNet-similarity gives a poor result (See Table 1). So,
we define a metric called visual similarity. Let us call the
similar words as targets. In this metric, we represent the
seed and the target as vectors. To define the dimensions,
for each seed, we use a set of relations (HasA, HasProperty,
PartOf and MemberOf). We query ConceptNet to get the related words (say, W1,W2,W3...) under such relations for the
seed-word and its superclasses. Each of these relation-word
pairs (i.e. HasA-W1,HasA-W2,PartOf-W3,...) becomes a
separate dimension. The values for the seed-vector are the
weights assigned to the assertions. For each target, we
query ConceptNet and populate the target-vector using the
edge-weights for the dimensions defined by the seed-vector.
To get the top words using visual similarity, we use the
cosine similarity of the seed-vector and the target-vector to
re-rank the top 10000 retrieved similar target-words using
ConceptNet-similarity. For abstract seed-words, we do not
get any such relations and we use the ConceptNet similarity directly. Table 1 shows the top similar words using

Figure 2. An overview of the framework followed for each Image;
demonstrated using an example image of an aardvark (resembles
animals such as tapir, ant-eater). We run a similar pipeline for
each image and then infer final results using a final Probabilistic
Inference Stage (Stage II).

4.2. Image Classification
Neural Networks trained on ample source of images and
numerous image classes has been very effective. Studies have found that convolutional neural networks (CNN)
can produce near human level image classification accuracy [12], and related work has been used in various visual recognition tasks such as scene labeling [5] and object
recognition [7]. To exploit these advances, we use the stateof-the-art class detections provided by the Clarifai API [21]
and the Deep Residual Network Architecture by [8] (using
the trained ResNet-200 model). For each image (Ik ) we
use top 20 detections (Sk ). Let us call these detections as
seeds. An example is provided in the Figure 2. Each detection is accompanied with the classifier’s confidence score
(P̃ (Sk |Ik )).

ConceptNet
man, merby, misandrous,
philandry, male human,
dirty pig, mantyhose,
date woman,guyliner,manslut

Visual Similarity
priest, uncle, guy,
geezer, bloke, pope,
bouncer, ecologist,
cupid, fella

word2vec
women, men, males,
mens, boys, man, female,
teenagers,girls,ladies

Table 1. Top 10 similar Words for “Men”. More in appendix.

ConceptNet, word2vec and visual-similarity for the word
“men”. Moreover, the ranked list based on visual-similarity
ranks boy, chap, husband, godfather, male person, male in
the ranks 16 to 22.
Formulation: For each seed (s), we get the top words
(Ts ) from ConceptNet using the visual similarity metric
and the similarity vector Wm (s, Ts ). Together for an
image, these constitute TSk and the matrix Wm , where
Wm (si , tj ) = simvis (si , tj )∀si ∈ Sk , tj ∈ TSk . Next
we describe the defined similarity metric.
A large percentage of the error in Image Classifiers are
due to visually similar (or semantically similar) objects or
objects from the same category [9]. In such cases, we use
this visual similarity metric to predict the possible visually
similar objects and then use an inference model to infer the
actual object.

4.3. Rank and Retrieve Related Words
Our goal is to logically infer words or phrases that represent (higher or lower-level) concepts that can best explain
the co-existence of the seeds in a scene. Say, for “hand” and
“care”, implied words could be “massage”, “ill”, “ache” etc.
For “transportation” and “sit”, implied words/phrases could
be “sit in bus”, “sit in plane” etc. The reader might be inclined to infer other concepts. However, to “infer” is to derive “logical” conclusions. Hence, we prefer the concepts
which shares strong explainable connections with the seedwords.
A logical choice would be traversing a knowledge-graph
like ConceptNet and find the common reachable nodes from
these seeds. As this is computationally quite infeasible,
we use the association-space matrix representation of ConceptNet, where the words are represented as vectors. The
similarity between two words approximately embodies the
strength of the connection over all paths connecting the two
words in the graph. We get the top similar words for each
seed, approximating the reachable nodes.

4.3.2

Rank Targets

We use P̃ (Sk |Ik ) as an approximate vector representation
for the image, in which the seed-words are the dimensions.
The columns of Wm provides vector representations for the
target words (t ∈ TSk ) in the space. We calculate cosine
similarities for each target with such a image-vector and
then re-rank the targets. We consider the top θ#t targets
and we call it Tk .
4

4.4. Probabilistic Reasoning and Inference
4.4.1

indicates higher connectivity of a word in the graph. This
yields a higher similarity score to many words and might
give an unfair bias to this target in the inference model.
Hence, the higher the C(.), the word provides less specific
information for an image. Hence, the weight becomes

PSL Inference Stage I

Given a set of candidate targets Tk and a set of weighted
seeds (Sk , P̃ (Sk |Ik )), we build an inference model to infer
a set of most probable targets (T̂k ). We model the joint
distribution using PSL as this formalism adopts Markov
Random Field which obeys the properties of Gibbs Distribution. In addition, a PSL model is declared using rules.
Given the final answer from the system, the set of satisfied
(grounded) rules show the logical connections between the
detected words and the final answer, which demonstrates
the system’s explainability.
The PSL model can be best depicted as an Undirected
Graphical Model involving seeds and targets, as given in
Figure 3.

wtij = θα1 ∗ simcn (sik , tjk )+
θα2 ∗ simw2v (sik , tjk ) + 1/C(tjk ),

(6)

where simcn (., .) is the normalized ConceptNet-based similarity. simw2v (., .) is the normalized word2vec similarity
of two words and C(.) is the eigenvector-centrality score of
the argument in the ConceptNet matrix.
ii) To model dependencies among the targets, we observe
that if two concepts t1 and t2 are very similar in meaning, then a system that infer t1 should infer t2 too, given
the same set of observed words. Therefore, the two rules
wtjm : tjk → tmk and wtjm : tmk → tjk are designed
to force the confidence values of tjk and tmk to be as close
to each other as possible. wtjm is the same as Equation 6
without the penalty for popularity.
The combined PSL model inference objective becomes:
arg min

Figure 3. Joint Modeling of seeds and targets, depicted as a Undirected Graphical Model. We define the seed-target and targettarget potentials using PSL rules. We connect each seed to each
target and the potential depends on their similarity and the target’s
popularity bias. We connect each target to θt-t (1 or 2) maximally
similar targets. The potential depends on their similarity.

I(Tk

)∈[0,1]|Tk |

X

X

X

	

wtij max I(sik ) − I(tjk ), 0 +

sik ∈Sk tjk ∈Tk

X

n

	
wtjm max I(tmk ) − I(tjk ), 0 +

tjk ∈Tk tmk ∈Tjmax


	o
max I(tjk ) − I(tmk ), 0 .

To let the targets compete against each other, we add a constraint
P on the sum of the confidence scores of the targets
i.e. j:tjk ∈Tk I(tjk ) ≤ θsum1 . Here θsum1 ∈ {1, 2} and
I(tjk ) ∈ [0, 1]. As a result of this model, we get an inferred
reduced set of targets [T̂k ]4k=1 .

Formulation: Using PSL, we add two sets of rules: i) to
define seed-target potentials, we add rules of the form wtij :
sik → tjk for each word sik ∈ Sk and target tjk ∈ Tk ; ii)
to define target-target potentials, for each target tjk , we take
the most similar θt-t targets (Tjmax ). For each target tjk and
each tmk ∈ Tjmax , we add two rules wtjm : tjk → tmk and
wtjm : tmk → tjk . Next, we describe the choices in detail.
i) From the perspective of optimization, the rule wtij :
sik → tjk adds the term wtij ∗ max{I(sik ) − I(tjk ), 0}
to the objective. This means that if confidence score of the
target tjk is not greater than I(sik ) (i.e. P̃ (Sk |Ik )), then
the rule is not satisfied and we penalize the model by wtij
times the difference between the confidence scores. We add
the above rule for seeds and targets for which the combined
weighted similarity exceeds certain threshold θsim,psl1 .
We encode the commonsense knowledge of words and
phrases obtained from different knowledge sources into the
weights of these rules wtij . Both the knowledge sources are
considered because ConceptNet embodies commonsense
knowledge and word2vec encodes word-meanings. It is
also important that the inference model is not biased towards more popular targets (i.e. abstract words or words too
commonly used/detected in corpus). We compute eigenvector centrality score (C(.)) for each word in the context of
ConceptNet (a network of words and phrases). Higher C(.)

4.4.2

PSL Inference Stage II

To learn the most probable set of common targets jointly,
we consider the targets and the seeds from all images together. Assume that the seeds and the targets are nodes
in a knowledge-graph. Then, the most appropriate targetnodes should observe similar properties as an appropriate
answer to the riddle: i) a target-node should be connected
to the high-weight seeds in an image i.e. should relate to the
important aspects of the image; ii) a target-node should be
connected to seeds from all images.
Formulation: Here, we use the rules wtij : sik → tjk
for each word sik ∈ Sk and target tjk ∈ T̂k for all
k ∈ {1, 2.., 4}. To let the set of targets compete against each
P4 P
other, we add the constraint
k=1
j:tjk ∈T̂k I(tjk ) ≤
θsum2 . Here θsum2 = 1 and I(tjk ) ∈ [0, 1].
To minimize the penalty for each rule, the optimal solution will try to maximize the confidence score of tjk . To
5

5.2.1

minimize the overall penalty, it should maximize the confidence scores of those targets which will satisfy most of the
rules (or rules with maximum total weight). As the summation of confidence scores is bounded, only a few top inferred
targets should have non-zero confidence.

Systems

We propose several variations of the proposed approach and
compare them with a simple vision-only baseline (hypothesis I). We introduce an additional Bias-Correction stage after the Image Classification, which aims to re-weight the
detected seeds using additional information from other images. The variations then, are created to test the effects of
varying the Bias-Correction stage and the effects of the individual stages of the framework on the final accuracy (hypothesis II). We also vary the initial Image Classification
Method (Clarifai, Deep Residual Network).
Bias-Correction: We experimented with two variations:
i) greedy bias-correction and ii) no bias-correction. We follow the intuition that the re-weighting of the seeds of one
image can be influenced by the others7 . To this end, we develop the “GreedyUnRiddler” (GUR) approach. In this approach, we consider all of the images together to dictate the
new weight of each seed. Take image Ik for example. To reweight seeds in Sk , we calculate
P the weights using the folsimcosine (Vsk ,j ,Vj )
. Vj
lowing equation: W̃ (sk ) = j∈1,..4
4.0
is vector of the weights assigned P̃ (Sj |Ij ) i.e. confidence
scores of each seed in the image. Each element of Vsk ,j [i]
is the ConceptNet-similarity score between the seed sk and
si,j i.e. the ith seed of the j th image. The re-weighted seeds
(Sk , W̃ (Sk )) of an image are then passed through the rest
of the pipeline to infer the final answers.
In the original pipeline (“UnRiddler”,in short UR), we
just normalize the weights of the seeds and pass on to the
next stage. We experiment with another variation (called
BiasedUnRiddler or BUR), the results of which are included in appendix, as GUR achieves the best results.
Effect of Stages: We observe the accuracy after each
stage in the pipeline (VB: Upto Bias Correction, RR: Upto
Rank and Retrieve stage, All: The entire Pipeline). For VB,
we use the normalized weighted seeds, get the weighted
centroid vector over the word2vec embeddings of the seeds
for each image. Then we obtain the mean vector over these
centroids. The top similar words from the word2vec vocabulary to this mean vector, constitutes the final answers.
For RR, we get the mean vector over the top predicted targets for all images. Again, the most similar words from the
word2vec vocabulary constitutes the answers.
Baseline: We create Vision-only Baselines. We directly
use the class-labels and the confidence scores predicted using a Neural Network-based Classifier. For each image,
we calculate the weighted centroid of the word2vec embeddings of these labels and the mean of these centroids for the
4 images. For the automatic evaluation we use this centroid
and for the human evaluation, we use the most similar word
to this vector, from the word2vec vocabulary. The Baseline

5. Experiments and Results
In this section, we provide the results of the validation
experiments of the newly introduced Image Riddle dataset,
followed by empirical evaluation of the proposed approach
against vision-only baselines.

5.1. Dataset Validation and Analysis
We have collected a set of 3333 riddles from the internet (puzzle websites). Each riddle has 4 images (66 × 66,
6KB in size) and a groundtruth label associated with it.
To verify the groundtruth answers, we define the metrics:
i) “correctness” - how correct and appropriate the answers
are, and ii) “difficulty” - how difficult are the riddles. We
conduct an Amazon Mechanical Turker-based evaluation.
We ask them to rate the correctness from 1-65 . The “difficulty” is rated from 1-76 . According to the Turkers, the
mean correctness rating is 4.4 (with Standard Deviation
1.5). The “difficulty” ratings show the following distribution: toddler (0.27%), younger child (8.96%), older child
(30.3%), teenager (36.7%), adult (19%), linguist (3.6%),
no-one (0.64%). In short, the average age to answer the
riddles seems to be closer to 13-17yrs. Also, few of these
(4.2%) riddles seem to be incredibly hard. Interestingly, the
average age perceived reported for the recently proposed
VQA dataset [1] is 8.92 yrs. Although, this experiment
measures “the turkers’ perception of the required age”, one
can conclude that the riddles are comparably harder.

5.2. System Evaluation
The presented approach suggests the following hypotheses that requires empirical tests: I) the proposed approach
(and their variants) attain reasonable accuracy in solving the
riddles; II) the individual stages of the framework improves
the final inference accuracy of the answers. In addition,
we also experiment to observe the effect of using commercial classification methods like Clarifai against a published
state-of-the-art Image Classification method.
5 1: Completely gibberish, incorrect, 2: relates to one image, 3 and 4:
connects two and three images respectively, 5: connects all 4 images, but
could be a better answer, 6: connects all images and an appropriate answer.
6 These gradings are adopted from VQA AMT instructions [1]. 1: A
toddler can solve it (ages:3-4), 2: A younger child can solve it (ages:58), 3: A older child can solve it (ages:9-12), 4: A teenager can solve it
(ages:13-17), 5: An adult can solve it (ages:18+), 6: Only a Linguist (one
who has above-average knowledge about English words and the language
in general) can solve it, 7: No-one can solve it.

7 A person would often skim through all the images at one go and will
try to come up with the aspects that needs more attention.

6

performances are listed in Table 2 in the VB+UR cells.
5.2.2

with a scenario: We have three separate robots that attempted to answer this riddle. You have to rate the answer
based on the correctness and the degree of intelligence (explainability) shown through the answer.. The correctness is
defined as before. In addition, turkers are asked to rate intelligence in a scale of 1-48 . We plot the the percentage of
total riddles per each value of correctness and intelligence
in Figure 4. In these histograms plots, we expect a increase
in the rightmost buckets for the more “correct” and “intelligent” systems.

Experiment I: Automatic Evaluation

We evaluate the performance of the proposed approach on
the 3333 Image Riddles dataset using both automatic and
Amazon Mechanical Turker (AMT)-based evaluations.
As an evaluation metric, we use word2vec similarity measure. An answer to a riddle may have several semantically similar answers. Hence it is reasonable to use such a metric. For each riddle, we calculate the maximum similarity between the groundtruth
and top 10 detections from an approach. To calculate
phrase similarities, we use n similarity method of the
gensim.models.word2vec package. The average of
such maximum similarities is reported in percentage form.

Clarifai

ResNet

VB
RR
All
VB
RR
All

GUR
3.3k
2.8k
65.3
65.36
65.9
65.73
68.8*
68.7
66.8
66.4
66.3
66.2
68.2
68.2

UR
3.3k
65
65.9
68.5
68.3
67
68.53

2.8k
65.3†
65.7
68.57
68†
66.7
68.2

Table 2. Accuracy on the Image Riddle Dataset. Pipeline variants
(VB, RR and All) are combined with Bias-Correction stage variants (GUR, UR). All values are in percentage form. (*- Best, † Baselines).
θ#t
θα1
θα2
θt-t
θsim,psl1
θsum1

Number of Targets
ConceptNet-similarity Weight
word2vec-similarity weight
Number of maximum similar Targets
Seed-target similarity Threshold
Sum of confidence scores in Stage I

2500
1
4
1
0.8
2

Figure 4. AMT Results of The GUR+All
(our), Clarifai (baseline
.
1) and ResidualNet (baseline 2) approaches. Correctness Means
are: 2.6 ± 1.4, 2.4 ± 1.45, 2.3 ± 1.4. For Intelligence: 2.2 ±
0.87, 2 ± 0.87, 1.8 ± 0.8

Table 3. A List of parameters θ used in the approach

5.2.4 Analysis
Experiment I shows that the GUR variant (GUR+All in Table 2) achieves the best results in terms of word2vec-based
accuracy. Similar trend is reflected in the AMT-based evaluations (Figure 4). Our system has increased the percentage
of puzzles for the rightmost bins i.e. produces more “correct” and “intelligent” answers for more number of puzzles. The word2vec-based accuracy puts the performance
of ResNet baseline close to that of the GUR variant. However, as evident from Figure 4, the AMT evaluation of the
correctness shows clearly that the ResNet baseline lags in
predicting meaningful answers. Experiment II also includes
what the turkers think about the intelligence of the systems
that tried to solve the puzzles. This also puts the GUR variant at the top. The above two experiments empirically show

To select the parameters in the parameter vector θ, We
employed a random search on the parameter-space over first
500 riddles over 500 combinations. The final set of parameters used and their values are tabulated in Table 3.
Each of the stage-variants (VB, RR and All) are combined with different variations of the Bias-Correction stage
(GUR and UR respectively). The accuracies on all are listed
in Table 2. We provide our experimental results on this 3333
riddles and 2833 riddles (barring 500 riddles we used for the
parameter search).
5.2.3

Experiment II: Human Evaluation

We conduct an AMT-based comparative evaluation of the
results of the proposed approach (GUR+All using Clarifai)
and two vision-only baselines. We define two metrics: i)
“correctness” and ii) “intelligence”. Turkers are presented

8 1: Not intelligent, 2: Moderately Intelligent, 3: Intelligent, 4: Very
Intelligent.

7

Figure 5. Positive and Negative (in red) results of the “GUR” approach (GUR+All variant) on some of the riddles. The groudtruth labels,
closest label among top 10 from GUR and the Clarifai baseline are provided for all images. For more results, check Appendix and the
ImageRiddle website (http://bit.ly/1Rj4tFc).

that our approach achieves a reasonable accuracy in solving
the riddles (Hypothesis I). In table 2, we observe how the
accuracy varies after each stage of the pipeline (hypothesis
II). The table shows a jump in the accuracy after the RR
stage, which leads us to believe the primary improvement
of our approach is attributed to the Probabilistic Reasoning
model. We also provide our detailed results for the “GUR”
approach using a few riddles in Figure 5.

improves on vision-only baselines and provides a stronger
baseline for future attempts.
The task of “Image Riddles” is equivalent to conventional IQ test questions such as analogy solving, sequence
filling; which are often used to test human intelligence. This
task of “Image Riddles” is also in line with the current trend
of VQA datasets which require visual recognition and reasoning capabilities. However, it focuses more on the combination of both vision and reasoning capabilities. In addition to the task, the proposed approach introduces a novel
inference model to infer related words (from a large vocabulary) given class labels (from a smaller set), using semantic
knowledge of words. This method is general in terms of
its applications. Systems such as [24], which use a collection of high-level concepts to boost VQA performance; can
benefit from this approach.

6. Conclusion and Future Works
In this work, we presented a Probabilistic Reasoning
based approach to solve a new class of image puzzles, called
“Image Riddles”. We have collected over 3k such riddles.
Crowd-sourced evaluation of the dataset demonstrates the
validity of the annotations and the nature of the difficulty
of the riddles. We empirically show that our approach
8

References

[14] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014, pages 740–755. Springer,
2014. 2

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. L. Zitnick, and D. Parikh. Vqa: Visual question
answering. In International Conference on Computer
Vision (ICCV), 2015. 1, 2, 6

[15] H. Liu and P. Singh. Conceptnet - a practical commonsense reasoning tool-kit. BT Technology Journal,
22(4):211–226, Oct. 2004. 2

[2] S. Bach, B. Huang, B. London, and L. Getoor.
Hinge-loss markov random fields: Convex inference for structured prediction.
arXiv preprint
arXiv:1309.6813, 2013. 2

[16] B. London, S. Khamis, S. Bach, B. Huang, L. Getoor,
and L. Davis. Collective activity detection using
hinge-loss markov random fields. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 566–571, 2013. 2

[3] D. M. Blei. Probabilistic topic models. Commun.
ACM, 55(4):77–84, Apr. 2012. 2
[4] M. Brysbaert, A. B. Warriner, and V. Kuperman. Concreteness ratings for 40 thousand generally known
english word lemmas. Behavior research methods,
46(3):904–911, 2014. 10

[17] L. Ma, Z. Lu, and H. Li. Learning to answer questions
from image using convolutional neural network. arXiv
preprint arXiv:1506.00333, 2015. 2

[5] C. Farabet, C. Couprie, L. Najman, and Y. LeCun.
Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1915–1929, 2013. 4

[18] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your
neurons: A neural-based approach to answering questions about images. arXiv preprint arXiv:1505.01121,
2015. 2
[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781, 2013. 3

[6] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and
W. Xu. Are you talking to a machine? dataset and
methods for multilingual image question answering.
arXiv preprint arXiv:1505.05612, 2015. 2

[20] T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya,
A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. Never-ending learning. In Proceedings
of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15), 2015. 3

[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and
semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 580–587. IEEE, 2014. 4
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015. 4

[21] G. Sood. clarifai: R Client for the Clarifai API, 2015.
R package version 0.2. 2, 4

[9] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In European conference
on computer vision, pages 340–353. Springer, 2012. 4

[22] R. Speer and C. Havasi. Representing general relational knowledge in conceptnet 5. 2012. 3

[10] A. Kimmig, S. Bach, M. Broecheler, B. Huang, and
L. Getoor. A short introduction to probabilistic soft
logic. In Proceedings of the NIPS Workshop on Probabilistic Programming: Foundations and Applications,
pages 1–4, 2012. 2

[23] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago:
A core of semantic knowledge. In Proceedings of the
16th International Conference on World Wide Web,
WWW ’07, pages 697–706, New York, NY, USA,
2007. ACM. 3

[11] G. Klir and B. Yuan. Fuzzy sets and fuzzy logic: theory and applications. 1995. 2

[24] Q. Wu, C. Shen, L. Liu, A. Dick, and A. v. d. Hengel. What value do explicit high level concepts have
in vision to language problems?
arXiv preprint
arXiv:1506.01144, 2015. 8

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097–1105, 2012. 4
[13] H. Larochelle, D. Erhan, Y. Bengio, U. D. Montral,
and M. Qubec. Zero-data learning of new tasks. In In
AAAI, 2008. 2
9

Appendices

word v provides similar but more specific information than
word u. Each node has a resource P̃ (u|Ik ), the confidence
assigned by the Neural Network. If there is an edge from the
node, some of this resource should be sent along this edge
until for all edges (u, v) ∈ G, wv becomes greater than wu .
We formulate the problem as a Linear Optimization problem:

A. BiasedUnRiddler (BUR): A Variation of the
BiasCorrection Stage

minimize

w=(w1 ,...w|S | )
k

subject to

X
X
s∈Sk

X

P̃ (sk |Ik )

sk ∈Sk

wu ≥ 0.5P̃ (u|Ik ), ∀u ∈ G

In Figure 6: dinosaur, animal and reptile all provide evidence that the image has an animal. Only the word dinosaur
indicates what kind of animal is in the image. The other
words do not add any additional information. Some highconfidence detections also provide erroneous abstract information. Here, the labels monstrous, monster are some such
detections. Hence, the objective is to re-weight the seeds
so that: i) the more specific seed-words should have higher
weight than the ones which provide similar but more general information; ii) the seeds that are too frequently used
or detected in corpus, should be given lower weights.
Specificity and Popularity: We compute eigenvector
centrality score (ECS) for each word in the context of ConceptNet. Higher ECS indicates higher connectivity and
yields a higher similarity score to many words and might
give an unfair bias to this seed (and words implied by this
seed) in the inference model. Hence, the higher the ECS,
the word provides less specific information for an image.
Additionally, we use the concreteness rating (CR) from
[4]. In this paper, the top 39955 frequent English words
are rated from the scale of 1 (very abstract) to 5 (very concrete). For example, the mean ratings for monster, animal
and dinosaur are 3.72, 4.61 and 4.87 respectively.
Problem Formulation: We formulate the problem as a
resource flow problem on a graph. The directed graph G is
constructed in the following way: we order the seeds based
on decreasing centrality scores (CS). We compute CS as:

To limit the resource a node u can send, we limit the final
minimum value by 0.5 P̃ (u|Ik ). The solution provides us
with the necessary weights for the set of seeds Sk in Ik . We
normalize these weights and get W̃ (Sk ).

B. Intermediate Results for the “Aardvark”
Riddle

Figure 7. The four different Images for the “aardvark” riddle.

From the four figures in Figure 7, we get the top 20 Clarifai detections as given in the Table 4.
Based on the GUR approach (GUR+All in paper), our
PSL Stage I outputs probable concepts (words or phrases)
depending on the initial set of detected class-labels (seeds).
They are provided in Table 5. Note that, these are the
top targets detected from almost 0.2 million possible candidates. Observe the following:
i) the highlighted detected animals have a few visual features in common, such as four short legs, a visible tail, short
height etc.
ii) the detections from the third image does not at all lead
us to an animal and the PSL Stage I still thinks that its a
cartoon of sort.
iii) the detections from second gets affected because of
its close relation to the detections from third image and it
infers that the image just depicts cartoon.
In the final PSL Stage II however, the model figures out
that there is an animal that is common to all these images.
This is mainly because seeds from the three images confidently predict that some animal is present in the images.

(7)

where we normalize ECS and −CR to the scale of 0 to 1.
For each seed u, we check the immediate next node v and
add an edge (u, v) if the (ConceptNet-based) similarity between u and v is greater than θsim,ss 9 . If in this iteration, a
node v is not added in G, we get the most recent predecessor u for which the similarity exceed θsim,ss and add (u, v).
The idea is that if a word u is more abstract than v and if
they are quite similar in terms of conceptual similarity, then
9θ

ws =

wu = P̃ (u|Ik ), u ∈
/G

Figure 6. Clarifai detections and results from different stages for
the aardvark image (for BUR variant).

CS = (ECS + (−CR))/2,

max{wu − wv , 0}

(u,v)∈G

denotes the set of parameters used in the model.

10

Image1
monster
jurassic
monstrous
primitive
lizard
paleontology
vertebrate
dinosaur
creature
wildlife
nature
evolution
reptile
wild
horizontal
illustration
animal
side view
panoramic
mammal

Image2
food
small
vector
dinosaur
wildlife
cartoon
nature
evolution
reptile
outline
cute
sketch
painting
silhouette
horizontal
art
illustration
graphic
animal
panoramic

Image3
fun
retro
clip
halloween
set
border
messy
ink
design
ornate
decoration
ornament
vector
contour
cartoon
cute
silhouette
art
illustration
graphic

Image1
panda
dolphin
african forest elephant
placental mammal
otter
gorilla
wildebeest
chimaera
african savannah elephant
florida panther
liger
rabbit
aardvark
iguana
hippopotamus
hadrosaur
mountain goat
panda bear
velociraptor
whale

Image4
rock
nobody
travel
water
sea
aquatic
outdoors
sand
beach
bird
wildlife
biology
zoology
carnivora
nature
horizontal
animal
side view
panoramic
mammal

Image2
graph toughness
cartography
color paint
graph
spectrograph
revue
linear functional
simulacrum
pen and ink
luck of draw
cartoon
camera lucida
explode view
micrographics
hamiltonian graph
crowd art
depiction
echocardiogram
scenography
linear perspective

Image3
decorative
graph toughness
graph
artwork
spectrograph
kesho mawashi
tapestry
map
arabesque
sgraffito
linear functional
hamiltonian graph
emblazon
pretty as picture
art deco
dazzle camouflage
ecce homo
pointillist
pyrography
echocardiogram

Image3
hamiltonian graph
graph toughness
lacquer
figuration
war paint
graph
spectrograph
map
arabesque
fall off analysis
art collection
statue
delineate
jack o lantern
gussie up
ecce homo
pointillist
art deco
pyrography
scenography

Image4
giraffe
waterbuck
sandy beach
moose
wildebeest
skunk
anteater
echidna
bobcat
mule deer
bison
pygmy marmoset
mongoose
sea otter
squirrel monkey
wolverine
okapi
cane rat
whale
american bison

Table 6. Top 20 detections per each image from PSL Stage I (IUR).

They are provided in the Table 6. Observe that the individual detections are better compared to GUR10 .
Final output from PSL Stage II (for BUR) is comparable to that of the GUR approach. The top detections
are: hadrosaur, sea otter, diagrammatic, panda, iguana, pyrography, mule deer, placental mammal, liger, panda bear,
art deco, squirrel monkey, giraffe, echidna, otter, anteater,
pygmy marmoset, hippopotamus.
Here, the set of output mainly contains the concepts
(words or phrases) that either represents “animals with
some similar visual characteristics to aardvark” or it pertains to “cartoon or art”.

Table 4. Top 20 detections from Calrifai API. The detections that
are completely noisy is colored using red. It can be observed that
the third image does not give any evidence of an animal present.
Image1
dolphin
rhinoceros
komodo dragon
african elephant
lizard
gorilla
crocodile
indian elephant
wildebeest
elephant
echidna
chimaera
chimpanzee
liger
gecko
rabbit
iguana
hippopotamus
mountain goat
loch ness monster

Image2
like paint
projective geometry
diagram
line of sight
venn diagram
hippocratic face
real number line
sight draft
x axis
simulacrum
cartoon
diagrammatic
camera lucida
explode view
crowd art
lottery
depiction
conecept design
infinity symbol
scenography

Image4
bison
american bison
marsupial
gibbon
monotreme
moose
mole
wildebeest
echidna
turtle
mule deer
mongoose
tamarin
chimpanzee
wolverine
prairie dog
western gorilla
anteater
okapi
skunk

C. Detailed Accuracy Histograms For Different Variants
In this section, we plot the accuracy histograms for the
entire dataset for all the variants (using Clarifai API) of our
approach (listed in Table 2 of the paper). We also add the
accuracy histograms for variants using BUR approach. The
plots are shown in the Figure 8. From the plots, the shift towards greater accuracy is evident as we go along the stages
of our pipeline.

D. Visual Similarity: Additional Results

Table 5. Top 20 detections per each image from PSL Stage I
(GUR).

Additional results for Visual Similarity are provided in
Tables 7 and 8.
ConceptNet
man, merby, misandrous,
philandry, male human,
dirty pig, mantyhose,
date woman,guyliner,manslut

That is why most of the top detections correspond to animals and animals having certain characteristics in common.
The top detections from PSL Stage II (GUR)
are: monotreme, gecko, hippopotamus, pyrography,
anteater, lizard, mule deer, chimaera, liger, iguana, komodo dragon, echidna, turtle, art deco, sgraffito, gorilla,
loch ness monster, prairie dog.
BUR: For BUR, PSL Stage I outputs probable concepts
(words or phrases) depending on the current set of seeds.

Visual Similarity
priest, uncle, guy,
geezer, bloke, pope,
bouncer, ecologist,
cupid, fella

word2vec
women, men, males,
mens, boys, man, female,
teenagers,girls,ladies

Table 7. Similar Words for “Men”
10 The

output from the PSL Stage I for BUR, is completely independent
of the other images. In essence, for each image, we are predicting all
relevant concepts from a large vocabulary given a few detections from a
small set of class-labels.

11

(0.029), plate rack, throne, wall clock, face powder,
binder, hair slide,velvet,puck, redbone.
3. hog (0.48), wallaby (0.19), wild boar (0.10), Mexican hairless (0.045), gazelle (0.023), wombat (0.017),
dhole (0.016), hyena (0.015), armadillo (0.009), ibex,
hartebeest, water buffalo, bighorn, kit fox, mongoose,
hare, wood rabbit, warthog, mink, polecat.
These predictions show that for the first and fourth image,
there are some animals detected with some distant visual
similarities. The second and third image has almost no animal mentions. This also shows some very confident detections (such as triceratops for the first image) is quite noisy.
In many cases, due to these high-confidence noisy detections, the PSL-based inference system gets biased towards
them. Compared to that, Clarifiai detections provide quite
a few (abstract but) correct detections about different aspects of the image (for example, for 2nd Image, predicts labels related to “cartoon/art” and “animal” both). This seems
to be one of the reasons, for which the current framework
provide better results for Clarifai Detections. Using Residual Network, the final output from the GUR system for the
“aardvark” riddle is: antelope, prairie dog, volcano rabbit,
marsupial lion, peccary, raccoon, pouch mammal, rabbit,
otter, monotreme, jackrabbit, hippopotamus, moose, tapir,
echidna, gorilla.

Figure 8. The accuracy histograms of the BUR, GUR and UR approaches (combined with the VB, RR and All stage variants).
ConceptNet
saurischian, ornithischian,
protobird, elephant bird,
sauropsid, cassowary,
ibis, nightingale, ceratosaurian,
auk, vulture

Visual Similarity
lambeosaurid, lambeosaur,
bird, allosauroid, therapod, stegosaur,
triceratops, tyrannosaurus rex,
deinonychosaur,dromaeosaur,
brontosaurus

word2vec
dinosaurs, dino, T. rex,
Tyrannosaurus Rex, T rex,
fossil, triceratops, dinosaur species,
tyrannosaurus,dinos,
Tyrannosaurus rex

Table 8. Similar Words for “Dinosaur”

E. More Positive and Negative Results
We provide positive and Negative results in Figures 9
and 10 of the ”GUR+All” variant of the pipeline. We obtain better results with Clarifai detections rather than Residual Network detections. Based on our observations, one of
the key property of the ResidualNetwork confidence score
distribution is that there are few detections (1-3) which are
given the strongest confidence scores and the other detections have very negligible confidence scores. These top detections are often quite noisy.
For example, for the aardvark image 1, the ResidualNetwork detections are: triceratops, wallaby, armadillo, hog,
fox squirrel, wild boar, kit fox, grey fox, Indian elephant, red
fox, mongoose, Egyptian cat, wombat, tusker, mink, Arctic
fox, toy terrier, dugong, lion. Only the first detection has
0.84 score and the rest of the scores are very negligible. For
the second, third and fourth images, the top detections are
respectively:
1. pick (0.236), ocarina (0.114), maraca (0.091), chain
saw (0.06), whistle (0.03), can opener (0.03), triceratops (0.02), muzzle, spatula, loupe, hatchet, letter
opener, thresher, rock beauty, electric ray, tick, gong,
Windsor tie, cleaver, electric guitar
2. jersey (0.137), fire screen (0.129), sweatshirt
(0.037), pick (0.035), comic book (0.030), book jacket
12

Figure 9. More Positive results from the “GUR” approach on some of the riddles. The groudtruth labels, closest label among top 10 from
GUR and the Clarifai baseline are provided for all images. For more results, check http://bit.ly/1Rj4tFc.

13

Figure 10. Some Negative results from the “GUR” approach on some of the riddles. The groudtruth labels, closest label among top 10 from
GUR and the Clarifai baseline are provided for all images. For more results, check http://bit.ly/1Rj4tFc.

14

Grasp Type Revisited: A Modern Perspective on A Classical Feature for Vision
Yezhou Yang1 , Cornelia Fermüller1 , Yi Li2 , and Yiannis Aloimonos1
1

Computer Vision Lab, University of Maryland, College Park 2 NICTA and ANU

Abstract
The grasp type provides crucial information about human action. However, recognizing the grasp type from unconstrained scenes is challenging because of the large variations in appearance, occlusions and geometric distortions.
In this paper, first we present a convolutional neural network to classify functional hand grasp types. Experiments
on a public static scene hand data set validate good performance of the presented method. Then we present two applications utilizing grasp type classification: (a) inference of
human action intention and (b) fine level manipulation action segmentation. Experiments on both tasks demonstrate
the usefulness of grasp type as a cognitive feature for computer vision. This study shows that the grasp type is a powerful symbolic representation for action understanding, and
thus opens new avenues for future research.

1. Introduction
The grasp type contains fine-grain information about human action. Consider the two scenes in Fig. 1 from the
VOC challenge. Current computer vision systems can easily detect that there is one bicycle and one cyclist (human
being) in the image. Through human pose estimation, the
system can further confirm that these two cyclists are riding the bike. But humans can tell that the cyclist on the left
side literally is not “riding” the bicycle since his hands are
posing in a “Rest or Extension” grasp next to the handlebar while the cyclist on the right side is racing because his
hands firmly hold the handlebar with a “Power Cylindrical”
grasp. In other words, the recognition of grasp type is essential for a more detailed analysis of human action, beyond
the processes of current state-of-the-art vision systems.
Moreover, recognizing grasp type can help an intelligent
system predict the human action intention. Consider an intelligent agent looking at the two scenes in Fig. 2(a) and (b).
Current state-of-the-art computer vision techniques can accurately recognize many visual aspects from both of these
scenes, such as the fact that there must be a human being

978-1-4673-6964-0/15/$31.00 ©2015 IEEE

(a)

(b)

Figure 1. (a) Rest or Extension on the handlebar vs. (b) Firmly
power cylindrical grasping the handlebar.

standing in the outdoor garden scene, with a knife in his/her
hand. However, we human beings will react dramatically
different when experiencing the two different scenes, because of our ability to recognize immediately the different
ways the person is handling the knife, i.e., the grasp type.
We can effectively infer the possible activity the man is going to do based on his way of grasping the knife. After
seeing scene Fig. 2(a), we could believe this man is going
to cut something hard, or even might be malicious, since
he is “Power Hook” grasping the knife. After seeing scene
Fig. 2(b), we may react with a movement to acquire the
knife (shown in Fig. 2(c)) since the man is “Precision Lumbrical” grasping the knife indicating a passing action. From
this example we can see that the grasp type is a strong cue
for us to infer the human action intention.

(a)

(b)

(c)

Figure 2. (a) Power Hook Grasp a knife vs. (b) Precision Lumbrical Grasp a knife. (c) A natural reaction when seeing scene (b) is
to open the hand to receive the knife.

These are two examples demonstrating how important it
is for us to be able to recognize grasp types. The grasp type

400

is an essential component in the characterization of human
actions of manipulation ([27]). From the viewpoint of processing videos, the grasp contains information about the action itself, and it can be used for prediction or as a feature
for recognition. It also contains information about the beginning and end of action segments, and thus it can be used
to segment videos in time. If we are to perform the action
with an intelligent agent, such as a humanoid robot, grasp
is one crucial primitive action ([7]). Knowledge about how
to grasp the object is necessary so the robot can arrange
its effectors. For example, consider a humanoid with one
parallel gripper and one vacuum gripper. When a power
grasp is desired, the robot should select the vacuum gripper for a stable grasp, but when a precision grasp is desired,
the parallel gripper is a better choice. Thus, knowing the
grasp type provides information to plan the configuration of
robot’s effectors, or even the type of effector to use ([28]).
Here we present a study centered around human grasp
type recognition and its applications in computer vision.
The goal of this research is to provide intelligent systems
with the capability to recognize the human grasp type in
unconstrained static or dynamic scenes. To be specific, our
system takes in an unconstrained image patch around the
human hand, and outputs which category of grasp type is
used (examples are shown in Fig. 3). In the rest of the paper,
we show that this capability 1) is very useful for predicting
human action intention and 2) helps to further understand
human action by introducing a finer layer of granularity.
Further experiments on two publicly available dataset empirically support that we can 1) infer human action intention
in static scenes and 2) segment videos of human manipulation actions into finer segments based on the grasp type evolution. Additionally, we provide a labeled grasp type image
data set and a human intention data set for further research.

Figure 3. Sample outputs. PoC: Power Cylindrical; PoS: Power
Spherical; PoH: Power Hook; PrP: Precision Pinch; PrT: Precision
Tripod; PrL: Precision Lumbrical; RoE: Rest or Extension

2. Related Work
Human hand related:

One way to recognize grasp

type is through model based hand detection and tracking
[18]. Based on the estimated articulated hand model, a set
of biologically plausible features such as the arches formed
by fingers [23] were used to infer the grasp type involved
[27]. These approaches normally use RGB Depth data and
require a calibration phase, which is not applicable or is
too fragile for real world situations. Also a lot of research
has been devoted to hand pose or gesture recognition with
promising experimental results [16, 29]. The goal of these
works is to recognize poses such as “POINT”, “STOP” or
“YES” and “NO”, not considering the interaction with objects. When it comes to recognizing grasp type from unconstrained visual input, inevitably our system has to deal
with the additional challenges introduced by the interaction
with unknown objects. Later in the paper we will show that
the large variation in the scenery will not allow traditional
feature extraction and learning mechanism to work robustly
on public available hand patch testing beds.
The robotics community has been studying perception
and control problems of grasping for decades [21]. Recently, several learning based systems were reported that
infer contact points or how to grasp an object from its appearance [20, 13]. However, the desired grasping type could
be different for the same target object, when used for different action goals. The acquisition of grasp information from
natural static or dynamic scenes is still considered very difficult because of the large variation in appearance and the
occlusions of the hand from objects during manipulation.
Vision beyond appearance: The very small number
of works in computer vision, which aim to reason beyond
appearance models, are also related to this paper. [25]
proposed that beyond state-of-the-art computer vision techniques, we could possibly infer implicit information (such
as functional objects) from video, and they call them “Dark
Matter” and “Dark Energy”. [26] used stochastic tracking and graph-cut based segmentation to infer manipulation consequences beyond appearance. [10] used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who captured an image. More
recently, [19] seeks to infer the motivation of the person in
the image by mining knowledge stored in a large corpus using natural language processing techniques. Different from
these fairly general investigations about reasoning beyond
appearance, our paper seeks to infer human action intention
from a unique and specific point of view: the grasp type.
Convolutional neural networks: The recent development of deep neural networks based approaches revolutionized visual recognition research. Different from the traditional hand-crafted features [14, 4], a multi-layer neural
network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which has shown superior performance on standard object recognition benchmarks [11, 2] while utilizing minimal domain knowledge.

401

The work presented in this paper shows that with the recent developments of deep neural networks, we can learn
a model to recognize grasp type from unconstrained visual
inputs with robustness. We believe we are among the first
to apply deep learning on grasp type recognition.

3. Our Approach
First, we briefly summarize the basic concepts of Convolutional Neural Networks (CNN), and then we present our
implementations for grasp type recognition, human action
intention prediction and fine level manipulation action segmentation using the change of grasp type over time.

3.1. Human Grasp Types
A number of grasping taxonomies have been proposed in
several areas of research, including robotics, developmental medicine, and biomechanics, each focusing on different
aspects of action. In a recent survey, Feix et al. [6] reported 45 grasp types in the literature, of which only 33
were found valid. In this work, we use a categorization
into seven grasp types. First we distinguish, according to
the most commonly used classification (based on functionality), into power and precision grasps [8]. Power grasping
is used when the object needs to be held firmly in order
to apply force, such as “grasping a knife to cut”; precision
grasping is used in order to do fine grain actions that require
accuracy, such as “pinch a needle”. We then further distinguish among the power grasps, whether they are cylindrical,
spherical, or hook. Similarly, we distinguish the precision
grasps into pinch, tripodal and lumbrical. Additionally, we
also consider a Rest or Extension position (no grasping performed). Fig. 4 illustrates the grasp categories.

Figure 4. The grasp types considered. Grasps which cannot be
categorized into the six types here are considered as the “Rest and
Extension” (no grasping performed).

Humans, when looking at a photograph, can more or less
tell what kind of grasp the person in the picture is using.

The question becomes, whether using the current state-ofthe-art computer vision technique, whether we can develop
a system that learns the pattern from human labeled data
and recognizes grasp type from a patch around each hand?
In the following section, we present our take and show that a
grasp type recognition model with decent robustness can be
learned using Convolutional Neural Network (CNN) techniques.

3.2. CNN for Grasp Type Recognition
Convolutional Neural Network (CNN) is a multilayer
learning framework, which may consist of an input layer,
a few convolutional layers and an output layer. The goal
of CNN is to learn a hierarchy of feature representations.
Response maps in each layer are convolved with a number
of filters and further down-sampled by pooling operations.
These pooling operations aggregate values in a smaller region by down-sampling functions including max, min, and
average sampling. In this work we adopt the softmax loss
function which is given by:
n
C
N
e yk
1 XX n
)
tk log( PC
L(t, y) = −
n
ym
N n=1
m=1 e

(1)

k=1

where tnk is the n-th training example’s k-th ground truth
output, and ykn is the value of the k-th output layer unit in
response to the n-th input training sample. N is the number
of training samples, and since we consider 7 grasp type categories, C = 7. The learning in CNN is based on Stochastic
Gradient Descent (SGD), which includes two main operations: Forward and Back Propagation. The learning rate is
dynamically lowered as training progresses. Please refer to
[12] for details.
We used a five layer CNN (including the input layer and
one fully-connected perception layer for regression output).
The first convolutional layer has 32 filters of size 5 × 5 with
max pooling, the second convolutional layer has 32 filters
of size 5 × 5 with average pooling, and the third convolutional layer has 64 filters of size 5 × 5 with average pooling,
respectively. Convolutional layer convolves its input with
a bank of filters, then applies point-wise non-linearity and
max or average pooling operation.
The final fully-connected perception layer has 7 regression outputs. Fully-connected perception layer applies linear filters to its input, then applies point-wise non-linearity.
Our system considers 7 grasp type classes.
For testing, we pass each target hand patch to the trained
CNN model, and obtain an output of size 7×1: PGraspT ype .
In the action intention and segmentation experiments we use
the classification for both hands to obtain PGraspT ype1 for
the left hand, and PGraspT ype2 for the right hand, respectively. To have a fully automatic fine level manipulation
segmentation approach, we need to localize the input hand

402

patches from videos and then recognize grasp types using
CNN. We use the hand detection method of [15] to detect
hands in the first frame, and then apply a meanshift algorithm based tracking method [3] on both hands to continuously extract the image patch around each hand.

3.3. Human Action Intention
Our ability to interpret other people’s actions hinges crucially on predicting their intentionality. Even 18-month-old
infants behave altruistically when they observe an adult accidentally dropping a marker on the floor but out of his
reach, and they can predict his intention to pick up the
marker [24]. From the point view of machine learning for
intelligent systems and human-robot collaboration, due to
the differences in the embodiment of humans and robots,
a direct mapping of action signals is problematic. One solution is that the robot predicts the intent of the observed
human activity and implements the same intention using its
own sensorimotor apparatus [22].
Previous studies showed that there are several key factors
that affect the grasp type [17]. One crucial deciding factor
for the selection of the grasp type to use is the intended
activity. We choose here a categorization into three human
action intentions, closely related to the functional classification discussed above (Fig. 4). The first category reflects the
intention to apply force onto the physical world, such as for
example “cut down a tree with an ax”, and we refer to it as
“Force-oriented”. The second category reflects fine-grained
activity where sensitivity and dexterity are needed, such as
“tie shoelaces”, and we refer to it as “Skill-oriented”. The
third category has no intention of specific action, such as
“showcasing and posing”, and we call it “Casual”. Fig. 5
illustrates the action intention categories by showing one
typical example of each. We should note that the three categories: “force-oriented”, “skill-oriented” and “casual” are
closely related to the three functional categories “power”
“precision”, and “rest”, respectively (Fig. 4). We used a
different labeling, because we encounter a larger variety of
hand poses in the static images used for intention classification than in the videos of human manipulation activities
used for functional categorization.
We investigate the causal relation between human grasp
type and action intention by training a classifier using grasp
types of both hands as input, and the category of action intention as output. As shown next, our experiment demonstrates a strong link. We want to point out that certainly
a finer categorization is possible. For example, “Force oriented” intention can be further divided into sub classes such
as “Selfish” or “Altruistic” and so on. However, such a classification would require other dynamic observations. Here
we show that from the grasp type in a single image a classification into basic intentions (shown in Fig. 5) is possible.

Figure 5. Human action intention categories.

3.4. From Grasp Type to Action Intention
Our hypothesis is that the grasp type is a strong indicator of human action intention. In order to validate this,
we train an additional classifier layer. The procedure is
as follows. For each training image, we first pass the target hand patches (left hand and right hand, if present) of
the main character in the image to the trained CNN model,
and we obtain two belief distributions: PGraspT ype1 and
PGraspT ype2 . We concatenate these two distributions and
use them as our feature vector for training. We train a support vector machine (SVM) classifier f , which takes as input the grasp type belief distributions and derives as output
an action intention distribution PInt of size 3 × 1:
PInt = f (PGraspT ype1 , PGraspT ype2 |θ),

(2)

where θ are the model parameters learned from labeled
pairs. Fig. 6 shows a diagram of the approach. We need
to point out that in the human action intention recognition
we use belief distributions instead of final class labels of
the two hands as input feature vectors. Thus, a certain category of grasp type does not directly indicate a certain action intention in our model. A further experiment using detected grasp type labels of both hands (the grasp type with
the highest belief score) to infer action intention achieves a
slightly worse performance, which confirms our claim here.

3.5. Grasp Type Evolution
In manipulation actions involving tools and objects, the
details of the small sub actions contain rich semantics. Current computer vision methods do not consider them. Consider a typical kitchen action, as shown in Fig. 7. In most
approaches the whole sequence would be denoted as “sprinkle the steak”, and the whole segment would be considered
an atomic part for recognition or analysis. However, within
this around 15 second long action, there are several finer
segments. The gentleman first “Pinch” grasps the salt to

403

manipulation action clip.

4. Experiments

Figure 6. Inference of human action intention from grasp type
recognition.

sprinkle the beef, then he “Extends” to point at the oil bottle, and later he “Power Spherical” grasps a pepper bottle
to further sprinkle black pepper onto the beef. Here we can
see that the dynamic changes of grasp type characterize the
start and end of these finer actions.

The theoretical framework we presented suggests three
hypotheses that deserve empirical tests: (a) the CNN based
grasp type recognition module can robustly classify input
hand patches into correct categories; (b) hand grasp type is
a reliable cognitive feature to infer human action intention;
(c) the evolution of hand grasp types is useful for fine-grain
segmentation of human manipulation actions.
To test the three hypotheses empirically, we need to define a set of performance variables and how they relate to
our predicted results. The first hypothesis relates to visual
recognition, and we can empirically test it by comparing
the detected grasp type labels with the ground truth ones
using the precision and recall metrics. We further compare
the method with a traditional hand-crafted feature based approaches to show the advantage of our approach. The second hypothesis relates to the inference of human action intention, and we can also empirically test it by comparing the
predicted action intention with the ground truth ones on a
testing set. The third hypothesis relates to manipulation action segmentation, and we can test it by comparing the computed key segment frames with the ground-truth ones. We
used two publicly available datasets: (1) the Oxford hand
dataset [15] and (2) a unconstrained cooking video dataset
(YouCook) [5].

4.1. Grasp Type Recognition in Static Images
Figure 7. Grasp type evolution (right hand) in a manipulation action.

In order to see if grasp type evolution actually can help
with a finer segmentation of manipulation actions, we first
recognize the grasp type of both hands, frame by frame, and
then output a segmentation at the points in time when any
of the hands has a change in grasp type. We design a third
experiment on a public cooking video dataset from Youtube
for validation.

3.6. Finer segment action using grasp type evolution
We adopt a straightforward approach. Let’s denote the
sets of grasp types along the time-line of an action of length
M as Gl = {G1l , G2l ...GM
l } for the left hand and as Gr =
{G1r , G2r ...GM
}
for
the
right
hand. Assuming that during
r
a manipulation action the grasp type evolves gradually, we
first apply a one dimensional mode filter to smooth temporally. Each grasp type detection at time t is replaced by its
most common neighbor in the window of [t − δ/2, t + δ/2],
where δ is the window size.
Then, whenever at a time instance t ∈ [1, M ], if Gtl 6=
t+1
Gl or Gtr 6= Gt+1
r , our system outputs one segment at t,
denoted as St . The set St yields a finer segmentation of the

Dataset and Experimental protocol
The Oxford hand dataset is a comprehensive dataset of
hand images collected from various different public image
data set sources with a total of 13050 annotated hand instances. Hand patches larger than a fixed area of (a bounding box of 1500 sq. pixels) were considered sufficiently
‘large’ and were used for evaluation. This way we obtained 4672 hand patches from the training set and 660 hand
patches from the testing set (VOC images). We then further
augmented the dataset with new annotations. We categorized each patch into one of the seven classes by considering
its functionality given the image context and its appearance
following Fig. 4. We followed the training and testing protocol from the dataset.
For training the grasping type, the image patches were
resized to 64×64 pixels. The training set contains 4672 image patches and was labeled with the seven grasping types.
We used a GPU based CNN implementation [9] to train the
neural network, following the structure described above.
We compared our approach with traditional hand-crafted
feature based approaches. One was the histogram of oriented gradients (HoG) + Bag of Words (BoW) + SVM classification, the other HoG + BoW + Random Forest. The
number of orientations we selected for HoG was 32, and

404

Methods
HoG+BoW+SVM
HoG+BoW+RF
CNN

PoC
P
R
.44 .46
.50 .40
.59 .60

PoS
P
0
0
.38

R
NaN
NaN
.62

PoH
P
0
0
.38

R
NaN
NaN
.58

PrP
P
R
0
0
.03 .17
.62 .60

PrT
P
0
0
.56

PrL

R
NaN
0
.66

P
0
0
.36

R
NaN
0
.40

RoE
P
R
.81 .41
.62 .36
.69 .56

Overall
Accu
.42
.36
.59

Table 1. Precision (P) and Recall (R) for each grasp type category and overall accuracy.

the number of dictionary entries for BoW was 100. The parameters for the baseline methods were tuned to have the
best performance.
Experimental results
We achieved an average of 59% classification accuracy
using the CNN based method. Table 1 shows the performance metrics of each grasp type category and the overall
performance in comparison to baseline algorithms. It can be
seen that the CNN based approach has a decent advantage.
To provide a full picture of our CNN based classification
model, we also show the confusion matrix in Fig. 8. Our
system mainly confused “Power Cylindrical Grasp” with
“Rest or Extension”. We believe that this is mostly because
the fingers form natural curves when resting and this makes
the hand look very similar to a cylindrical grasp with large
diameter. Also our model does not perform well on “Precision Lumbrical” grasp due to the relatively small amount
of training samples in this category. Fig. 9 shows some correct grasp type predictions (denoted by black boxes), and
some failure examples (denoted by red and blue bounding
boxes). Blue boxes denote a correct prediction of the underlying high-level grasp type in either the “Power” or “Precision” category, but incorrect recognition in finer categories.
Red boxes denote a confusion between“Power” and “Precision” grasp. Intuitively, the blue marked errors should be
penalized less than the red marked ones.

Figure 9. Examples of correct and false classification. PoC: Power
Cylindrical; PoS: Power Spherical; PoH: Power Hook; PrP: Precision Pinch; PrT: Precision Tripod; PrL: Precision Lumbrical; RoE:
Rest or Extension.

4.2. Inference of Action Intention from Grasp Type
Dataset and Experimental protocol

(a)

(b)

Figure 10. Clear action intention vs. an ambigous one

Figure 8. Category pairwise confusion matrix for grasp type classification using CNN.

A subset of 200 images from the Oxford hand dataset
serves as testing bed for action intention classification.
Since not every image in the test set contains an action intention that falls into one of the three major categories described above, the subset was selected with the following
rules: (1) at least one hand of the main character can be
seen from the image and (2) the main character has a clear
action intention. For example, we can infer that the character from Fig. 10(a) is going to perform a skill-oriented
actions that requires accuracy, while this is not clear from

405

Methods
GL+SVM
GT+SVM

F-O
P
R
.54 .35
.61 .35

S-O
P
R
.73 .59
.82 .71

C
P
.80
.82

R
.89
.83

Overall
Accu
.63
.65

Table 2. Precision (P) and Recall (R) for each intention category
and overall accuracy. GL: Grasp type Label; GT: Grasp Type belief distribution.

the character in Fig. 10(b) (pull the rope with force or just
posing casually?). We labeled the 200 images into the three
major action intention categories and used them as ground
truth. The grasp type CNN model was used to extract a 14
dimension belief distribution as grasp type feature (which
is due to data from both hands of the main character). A 5
folds cross validation protocol was adopted and we trained
each fold using a linear SVM classifier.
Experimental results

Figure 14. 1st row: sample hand localization on first frame using [15]. 2nd to 5th row: two sample sequences of hand patches
extracted using meanshift tracking [3].

4.3. Manipulation Action Fine Level Segmentation
using Grasp Type Evolution

Figure 11. Correct examples of predicting action intention.

We achieved an average 65% prediction accuracy. Table 2 reports precision and recall metrics for each category
of action intention. We also run the same experiment using
grasp type labels instead of belief distributions (GL+SVM).
We can see that it achieves slightly worse performance than
using belief distributions. Fig. 11 shows some interesting
correct cases, and Fig. 12 shows several failure predictions.
We believe that the failure cases are mostly due to the wrong
grasp type recognition inherited from the previous section.
Because of the small amount of pairs with ground truth, we
were not able to train for comparison a converging CNN
model, that would predict action intention directly from
hand patches.

Figure 12. Failure cases of predicting action intention. The label
at the bottom denotes the human labeling.

In this section we want to demonstrate that the change of
grasp type is a good feature for fine grain level manipulation
action temporal segmentation.
Dataset and Experimental protocol
Cooking is an activity, requiring a variety of grasp types,
that intelligent agents most likely need to learn. We conducted our experiments on a publicly available cooking
video dataset collected from the world wide web and fully
labeled, called the Youtube cooking dataset (YouCook) [5].
The data was prepared from open-source Youtube cooking
videos with a third-person view. These features make it a
good empirical testing bed for our third hypothesis.
We conducted the experiment using the following protocols: (1) 8 video clips, which contain at least two fine
grain activities, were reserved for testing; (2) all other video
frames were used for training; (3) we randomly reserved
10% of the training data as validation set for training the
CNNs. For training the grasp type recognition model, we
extended the dataset by annotating image patches containing hands in the training frames. The image patches were
resized to 64 × 64 pixels. The training set contains image
patches that was labeled with the seven grasp types. We
used the same CNN implementation [9] to train the neural
network, following the same structures described above.
Action Fine Level Segmentation
For each testing clip, we first picked the top two hand
proposals using [15] in the first frame, and then we applied
a meanshift algorithm based tracking method [3] on both
hands to continuously extract an image patch around each
hand (Fig. 14). The image patches were further resized to
64 × 64 and pipelined to the trained CNN model. We then
labeled each hand with the grasp type of highest belief score
in each frame. After applying a one dimensional mode fil-

406

Figure 13. Left and right hand grasp type recognition along timeline and video segmentation results compared with ground truth segments.

tering for temporal smoothing, we computed the grasp type
evolution for each hand and segmented whenever one hand
changes grasp type, as described in Sec. 3.6.
Fig. 13 shows two examples of intermediate grasp type
recognition for the two hands and the detected segmentation. A key frame is considered correct, when a ground truth
key frame lies within 10 frames around it. In the first example, the subject’s right hand at the beginning holds the tofu
using an Extension grasp, and then she cuts the tofu with
a Pinch grasp holding the blade. Then using a precision
Tripod grasp she separates one piece of tofu from the rest,
and at the end using a Lumbrical grasp she further cuts the
smaller piece of tofu. Using the grasp type evolution, our
system can successfully detect two key frames out of the
three ground truth ones. In the second video, the gentleman
using a Cylindrical grasp whisks the bowl at the beginning.
Then his left hand extends to reach a small cup, and then
using a Hook grasp he holds the cup. After that, his right
hand extends to reach a spatula and at the end his right hand
scoops food out of the small cup using a Cylindrical grasp.
Using the grasp type evolution, our system can successfully
detect three key frames out of the four ground truth ones.
In the 8 test clips, there are 18 ground truth segmentation
key frames, and 14 of them are successfully detected, which
yields a recall of 78%. Among the 20 detected segmentation
key frames, 16 are correct, which yields a precision of 80%.

5. Conclusion and Future Work
Our experiments produced three results: (i) we achieved
in average 59% accuracy using the CNN based method for
grasp type recognition from unconstrained image patches;

(ii) we achieved in average 65% prediction accuracy in inferring human intention using the grasp type only; (iii) using the grasp type temporal evolution, we achieved 78%
recall and 80% precision in fine grain manipulation action
segmentation tasks. Overall, the empirical results support
our hypotheses (a-c) respectively.
Recognizing grasp type and its use in inference for human action intention and fine level segmentation of human
manipulation actions, are novel problems in computer vision. We have proposed a CNN based learning framework
to address these problems with decent success. We hope
our contributions can help advance the field of static scene
understanding and human action fine level analysis, and we
hope that they can be useful to other researchers in other
applications. Additionally, we augmented a currently available hand data set and a cooking data set with grasp type
labels, and provided human action intention labels for a subset of them, for future research.
Our experiments indicate that there is still significant
space for improving the recognition of grasp type and inference of human intention. We believe that advances in
understanding high-level cognitive structure underlying human intention can help improve the performance. With the
development of deep learning systems and more data, we
can also expect a robust grasp type recognition system beyond the seven categories used in this paper. Moreover, we
believe that progress in natural language processing, such as
mining the relationship between grasp type and actions, can
advance high-level reasoning about human action intention
to improve computer vision methods.

407

6. Acknowledgements
This research was funded in part by the support of
the European Union under the Cognitive Systems program
(project POETICON++), the National Science Foundation under INSPIRE grant SMA 1248056, and by DARPA
through U.S. Army grant W911NF-14-1-0384 under the
Project: Shared Perception, Cognition and Reasoning for
Autonomy. NICTA is funded by the Australian Government
as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

References
[1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013.
[2] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column
deep neural networks for image classification. In CVPR,
2012.
[3] D. Comaniciu, V. Ramesh, and P. Meer. Real-time tracking
of non-rigid objects using mean shift. In Computer Vision
and Pattern Recognition, 2000. Proceedings. IEEE Conference on, volume 2, pages 142–149. IEEE, 2000.
[4] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 886–893. IEEE, 2005.
[5] P. Das, C. Xu, R. F. Doell, and J. J. Corso. A thousand frames
in just a few words: Lingual description of videos through
latent topics and sparse object stitching. In Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition, 2013.
[6] T. Feix, J. Romero, C. H. Ek, H. Schmiedmayer, and
D. Kragic. A Metric for Comparing the Anthropomorphic Motion Capability of Artificial Hands. Robotics, IEEE
Transactions on, 29(1):82–93, Feb. 2013.
[7] A. Guha, Y. Yang, C. Fermüller, and Y. Aloimonos. Minimalist plans for interpreting manipulation actions. In Proceedings of the 2013 International Conference on Intelligent
Robots and Systems, pages 5908–5914, Tokyo, 2013. IEEE.
[8] M. Jeannerod. The timing of natural prehension movements.
Journal of motor behavior, 16(3):235–254, 1984.
[9] Y. Jia.
Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.
berkeleyvision.org/, 2013.
[10] J. Joo, W. Li, F. F. Steen, and S.-C. Zhu. Visual persuasion:
Inferring communicative intents of images. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 216–223. IEEE, 2014.
[11] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS,
2012.

[12] Y. LeCun and Y. Bengio. The handbook of brain theory and
neural networks. chapter Convolutional networks for images,
speech, and time series, pages 255–258. MIT Press, Cambridge, MA, USA, 1998.
[13] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International Journal of Robotics
Research, 2015.
[14] D. G. Lowe.
Distinctive image features from scaleinvariant keypoints. International journal of computer vision, 60(2):91–110, 2004.
[15] A. Mittal, A. Zisserman, and P. H. Torr. Hand detection using
multiple proposals. In BMVC, pages 1–11. Citeseer, 2011.
[16] Z. Mo and U. Neumann. Real-time hand pose recognition using low-resolution depth images. In CVPR (2), pages 1499–
1505, 2006.
[17] J. R. Napier. The prehensile movements of the human hand.
Journal of bone and joint surgery, 38(4):902–913, 1956.
[18] I. Oikonomidis, N. Kyriazis, and A. Argyros. Efficient
model-based 3D tracking of hand articulations using Kinect.
In Proceedings of the 2011 British Machine Vision Conference, pages 1–11, Dundee, UK, 2011. BMVA.
[19] H. Pirsiavash, C. Vondrick, and A. Torralba. Inferring the
why in images. arXiv preprint arXiv:1406.5472, 2014.
[20] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping
of novel objects using vision. The International Journal of
Robotics Research, 27(2):157–173, 2008.
[21] K. B. Shimoga. Robot grasp synthesis algorithms: A survey.
The International Journal of Robotics Research, 15(3):230–
266, 1996.
[22] D. Song, N. Kyriazis, I. Oikonomidis, C. Papazov, A. Argyros, D. Burschka, and D. Kragic. Predicting human intention in visual observations of hand/object interactions. In
Robotics and Automation (ICRA), 2013 IEEE International
Conference on, pages 1608–1615. IEEE, 2013.
[23] R. Tubiana, J.-M. Thomine, and E. Mackin. Examination of
the Hand and the Wrist. CRC Press, Boca Raton, FL, 1998.
[24] F. Warneken and M. Tomasello. Altruistic helping in human
infants and young chimpanzees. science, 311(5765):1301–
1303, 2006.
[25] D. Xie, S. Todorovic, and S.-C. Zhu. Inferring “dark matter”
and “dark energy” from videos. In Computer Vision (ICCV),
2013 IEEE International Conference on, pages 2224–2231.
IEEE, 2013.
[26] Y. Yang, C. Fermüller, and Y. Aloimonos. Detection of manipulation action consequences (MAC). In Proceedings of
the 2013 IEEE Conference on Computer Vision and Pattern
Recognition, pages 2563–2570, Portland, OR, 2013. IEEE.
[27] Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos. A cognitive system for understanding human manipulation actions.
Advances in Cognitive Sysytems, 3:67–86, 2014.
[28] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learning manipulation action plans by “watching” unconstrained
videos from the world wide web. In The Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI-15), 2015.
[29] X. Zabulis, H. Baltzakis, and A. Argyros. Vision-based hand
gesture recognition for human-computer interaction. The
Universal Access Handbook. LEA, 2009.

408

Noname manuscript No.
(will be inserted by the editor)

Prediction of Manipulation Actions

arXiv:1610.00759v1 [cs.CV] 3 Oct 2016

Cornelia Fermüller · Fang Wang · Yezhou Yang · Konstantinos Zampogiannis · Yi
Zhang · Francisco Barranco · Michael Pfeiffer

the date of receipt and acceptance should be inserted later

Abstract Looking at a person’s hands one often can tell
what the person is going to do next, how his/her hands are
moving and where they will be, because an actor’s intentions
shape his/her movement kinematics during action execution.
Similarly, active systems with real-time constraints must not
simply rely on passive video-segment classification, but they
have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded from subjects performing different
manipulation actions on the same object, such as “squeezing”, “flipping”, “washing”, “wiping” and “scratching” with
a sponge. In psychophysical experiments, we evaluated human observers’ skills in predicting actions from video sequences of different length, depicting the hand movement
in the preparation and execution of actions before and after
contact with the object. We then developed a recurrent neural network based method for action prediction using as input patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training
synchronized video and force data streams. Evaluations on
two new datasets show that our system closely matches human performance in the recognition task, and demonstrate
the ability of our algorithms to predict real-time what and
how a dexterous action is performed.
Keywords Online action recognition · Hand motions ·
Forces on the hand · Action prediction
C. Fermüller, Y. Yang, K. Zamgogiannis, Y. Zhang
University of Maryland, College Park, MD 20742, USA
E-mail: fer@cfar.umd.edu
F. Wang
College of Engineering and Computer Science (CECS), Australian National University
F. Barranco
University of Granada
M. Pfeiffer
Institute of Neuroinformatics, University of Zurich and ETH Zürich

1 Introduction
Human action and activity understanding has been a topic
of great interest in Computer Vision and Robotics in recent years. Many techniques have been developed for recognizing actions and large benchmark datasets have been
proposed, with most of them focusing on full-body actions
(Mandary et al, 2015; Takano et al, 2015; Schuldt et al,
2004; Li et al, 2010; Moeslund et al, 2006; Turaga et al,
2008). Typically, computationally approaches treat action
recognition as a classification problem, where the input is
a previously segmented video, and the output a set of candidate action labels.
However, there is more to action understanding, as demonstrated by biological vision. As we humans observe, we constantly perceive, and update our belief about the observed
action and about future events. We constantly recognize the
ongoing action. But there is even more to it. We can understand the kinematics of the ongoing action, the limbs’ future
positions and velocities. We also understand the observed
actions in terms of our own motor-representations. That is,
we are able to interpret others’ actions in terms of dynamics
and forces, and predict the effects of these forces on objects.
Similarly, cognitive robots that will assist human partners
will need to understand their intended actions at an early
stage. If a robot needs to act, it cannot have a long delay in
visual processing. It needs to recognize in real-time to plan
its actions. A fully functional perception action loop requires
the robot to predict, so it can efficiently allocate future processes. Finally, even vision processes for multimedia tasks
may benefit from being predictive. Interpreting human activities is a very complex task and requires both, low-level vision processes and high-level cognitive processes with knowledge about actions. (Gupta and Davis, 2008; Kulkarni et al,
2013). Considering the challenges in state of the art visual
action recognition, we argue that by integrating closely the

2

Cornelia Fermüller et al.

be building blocks for applications involving hand movement recognition. For example, building on work on fullbody recognition (Shotton et al, 2013), (Keskin et al, 2013)
develops a learning-based approach using depth contrast features and Random Forest classifiers. Oikonomidis et al (2011)
in a model-based approach use a 27-degree of freedom model
of the hand built from geometric primitives and GPU accelerated Particle Swarm Optimization. So far, these trackers
and pose estimators work well on isolated hands, but methods still struggle with hands in interaction with objects (Supancic et al, 2015), although there are efforts underway to
deal with such situations (Panteleris et al, 2015).
Inspiration for our work comes from studies in CogFig. 1 Two examples demonstrate that early movements are strong
nitive Sciences on hand motion. The grasp and the moveindicators of the intended manipulation actions. Inspired by this, our
ment kinematics are strongly related to the manipulation acsystem performs action predictions from early visual cues. Compared
tion (Jeannerod, 1984). It has been shown that an actor’s
to the classification delay, earlier prediction of action significantly reduces the delay in real-time interaction, which is fundamentally imporintention shapes his/her movement kinematics during movetant for a proactive system. (Top row: squeezing a sponge; bottom row:
ment execution, and, furthermore, observers are sensitive to
wiping a table with a sponge.)
this information (Ansuini et al, 2015). They can see early
differences in visual kinematics and use them to discriminate between movements performed with different intenhigh-level with the low-level vision processes, with the highlevel modifying the visual processes (Aloimonos and Fermüller, tions. Kinematic studies have looked at such physical differences in movement. For example, Ansuini et al (2008)
2015), a better recognition may be achieved. Prediction plays
an essential component in this interaction. We can think about found that when subjects grasped a bottle for pouring, the
middle and the ring fingers were more extended than when
the action-perception loop of our cognitive system from the
they grasped the bottle with the intent of displacing, throwviewpoint of a control system. The sensors take measureing, or passing it. Similarly, Crajé et al (2011) found that
ments of the human activity. We then apply visual operations
subjects placed their thumb and index fingers in higher poon this signal and extract (possibly using additional cognisitions when the bottle was grasped to pour than to lift.
tive processes) useful information for creating the control
It appears that the visual information in the early phases
signal in order to change the state of the cognitive system.
of
the
action is often sufficient for observers to understand
Because the processing of the signal takes time, this crethe intention of action. Starting from this intuition, we a.)
ates a delay for the control (Doyle and Csete, 2011). It is
conducted a study to evalute humans’ performance in rectherefore important to compute meaningful information that
ognizing manipulation actions; b.) implemented a computaallows us to predict the future state of the cognitive system.
tional system using state-of the art learning algorithms.
In this work, we are specifically interested in manipulation
actions and how visual information of hand movements can
The psychophysical experiment was designed to evalbe exploited for predicting future action so that the crucial
uate human’s performance in recognizing manipulation acdelay in the control loop can be shortened (for an illustration
tions in their early phases. These include: 1) the grasp prepasee Fig. 1).
ration, which is the phase when the hand moves towards the
object and the fingers shape to touch the object; 2) the grasp,
Hand movements and actions have long been studied in
when the hand comes in contact with the object to hold it in
Computer Vision to create systems for applications such as
a stable position; and 3) the early actual action movement
recognition of sign language (Erol et al, 2007). More recent
of the hand together with the object. Throughout these three
applications include gesture recognition (Molchanov et al,
phases, observers’ judgment of the action becomes more re2015), visual interfaces (Melax et al, 2013), and driver analliable and confident. The study gives us an insight about the
ysis (Ohn-Bar and Trivedi, 2014). Different methods model
difficulty of the task and provides data for evaluating our
the temporal evolution of actions using formalisms such as
computational method.
Hidden Markov models (Starner et al, 1998), Conditional
Our computational approach processes the sensory inRandom Fields (Wang et al, 2006) and 3d Convolutional
put as a continuous signal and formulates action interpretaNeural Networks (Molchanov et al, 2015). While in printion as a continuous updating of the prediction of intended
ciple, some of these approaches, could be used for online
action. This concept is applied to two different tasks. First,
prediction, they are always treated as recognition modules.
from the stream of video input, we continuously predict the
In recent years a number of works have developed tools for
identity of the ongoing action. Second, using as input the
general hand pose estimation and hand tracking, which can

Prediction of Manipulation Actions

video stream, we predict the forces on the fingers applied
to grasped objects. Next, we provide a motivation for our
choice of the two tasks, after which we give an overview of
our approach.
The first task is about action prediction from video. We
humans are able to update our beliefs about the observed action, and predict it before it is completed. This capability is
essential to be pro-active and react to the actions of others.
Robots that interact with humans also need such capability.
Predicting future actions of their counterpart allows them to
allocate computational resources for their own reaction appropriately. For example, if a person is passing a cup to the
robot, it has to understand what is happening well before
the action is completed, so it can prepare the appropriate action to receive it. Furthermore, vision processes have to be
initiated and possibly tuned with predicted information, so
the cup can be detected at the correct location, its pose estimated, and possibly other task-specific processes performed
(for example, the content of the cup may need to be recognized).
The second task is about predicting the tactile signal of
the intended action. Findings of neuroscience on the mirror neuron system (Gallesse and Goldman, 1998; Rizzolatti
et al, 2001) provide evidence for a close relationship between mechanisms of action and perception in primates. Humans develop haptic perception through interaction with objects and learn to relate haptic with visual perception. Furthermore, they develop the capability of hallucinating the
haptic stimulus when seeing hands in certain configurations
interacting with objects (Tiest and Kappers, 2014). This capability of hallucinating force patterns from visual input is
essential for a more detailed analysis of the interaction with
the physical world. It can be used to reason about the current interaction between the hand and the object, and to predict the action consequences driven by the estimated force
pattern.
Furthermore, by associating vision with forces, we expect to obtain better computational action recognition modules. Intuitively, the force vectors, whose dimensions are
much lower than the visual descriptors, should provide useful compact information for classification, especially when
the training data is not large. A first experiment, presented
in Section 6.3.3, confirms this idea.
Most important, the force patterns may be used in robot
learning. A popular paradigm in Robotics is imitation learning or learning from demonstration (Argall et al, 2009), where
the robot learns from examples provided by a demonstrator.
If the forces can be predicted from images, then the force
profiles together with the positional information can be used
to teach the robot with video only. Many researchers are
trying to teach robots actions and skills that involve forces,
e.g. wiping a kitchen table (Gams et al, 2010), pull and flip
tasks (Kober et al, 2000), ironing or opening a door (Kormu-

3

shev et al, 2011). These approaches rely on haptic devices
or force and torque sensors on the robot to obtain the force
profiles for the robot to learn the task. If we can predict the
forces exerted by the human demonstrator, the demonstration could become vision only. This would allow us to teach
robots force interaction tasks much more efficiently.
In order to solve the above two tasks, we take advantage of new developments in machine learning. Specifically,
we build on the recent success of recurrent neural networks
(RNNs) in conjunction with visual features from pre-trained
convolutional neural networks (CNNs) and training from a
limited number of weakly annotated data. For the first task,
we use an RNN to recognize the ongoing action from video
input. A camera records videos of humans performing a number of manipulation actions on different objects. For example, they ‘drink’ from a cup, ‘pour’ from it, ‘pound’, ‘shake’,
and ‘move’ it; or they ‘squeeze’ a sponge, ‘flip’ it, ‘wash’,
‘wipe’, and ‘scratch’ with it. Our system extracts patches
around the hands, and feeds these patches to an RNN, which
was trained offline to predict in real-time the ongoing action.
For the second task, we collected videos of actions and synchronized streams of force measurements on the hand, and
we used this data to train an RNN to predict the forces, using
only the segmented hand patches in video input.
The main contributions of the paper are: 1) we present
the first computational study on the prediction of observed
dexterous actions 2) we demonstrate an implementation for
predicting intended dexterous actions from videos; 3) we
present a method for estimating tactile signals from visual
input without considering a model of the object; 4) we provide new datasets that serve as test-beds for the aforementioned tasks.

2 Related work
We will focus our review on studies along the following concepts: the idea of prediction, including prediction of intention and future events (a), prediction beyond appearance (b),
and prediction of contact forces on hands (c), work on hand
actions (d), manipulation datasets (e) and action classification as a continuous process using various kinds of techniques and different kinds of inputs (f ).
Prediction of Action Intention and Future Events: A
small number of works in Computer Vision have aimed to
predict intended action from visual input. For example, Joo
et al (2014) use a ranking SVM to predict the persuasive
motivation (or the intention) of the photographer who captured an image. Pirsiavash et al (2014) seek to infer the motivation of the person in the image by mining knowledge
stored in a large corpus using natural language processing
techniques. Yang et al (2015) propose that the grasp type,
which is recognized in single images using CNNs, reveals

4

the general category of a person’s intended action. In (Koppula and Saxena, 2016), a temporal Conditional Random
Field model is used to infer anticipated human activities by
taking into consideration object affordances. Other works attempt to predict events in the future. For example, Kitani
et al (2012) use concept detectors to predict future trajectories in a surveillance videos. (Fouhey and Zitnick, 2014)
learn from sequences of abstract images the relative motion
of objects observed in single images. Walker et al (2014)
employ visual mid-level elements to learn from videos how
to predict possible object trajectories in single images. More
recently, Vondrick et al (2016) learn using CNN feature representations how to predict from one frame in the video the
actions and objects in a future frame. Our study also is about
prediction of future events using neural networks. But while
the above studies attempt to learn abstract concepts for reasoning in a passive setting, our goal is to perform online
prediction of specific actions from video of the recent past.
Physics Beyond Appearance: Many recent approaches
in Robotics and Computer Vision aim to infer physical properties beyond appearance models from visual inputs. Xie
et al (2013) propose that implicit information, such as functional objects, can be inferred from video. (Zhu et al, 2015)
takes a task-oriented viewpoint and models objects using a
simulation engine. The general idea of associating images
with forces has previously been used for object manipulation. The technique is called vision-based force measurement, and refers to the estimation of forces according to the
observed deformations of an object (Greminger and Nelson,
2004). Along this idea, recently Aviles et al (2015) proposed
a method using an RNN for the classification of forces due
to tissue deformation in robotic assisted surgery.
Inference of Manipulation Forces: The first work in
the Computer Vision literature to simulate contact forces
during hand-object interactions is (Pham et al, 2015). Using as input RGB data, a model-based tracker estimates the
poses of the hand and a known object, from which then the
contact points and the motion trajectory are derived. Next,
the minimal contact forces (nominal forces) explaining the
kinematic observations are computed from the Newton-Euler
dynamics solving a conic optimization. Humans typically
apply more than the minimal forces. These additional forces
are learned using a neural network on data collected from
subjects, where the force sensors are attached to the object. Another approach on contact force simulation is due
to (Rogez et al, 2015). The authors segment the hand from
RGBD data in single egocentric views and classify the pose
into 71 functional grasp categories as proposed in (Liu et al,
2014). Classified poses are matched to a library of graphically created hand poses, and theses poses are associated
with force vectors normal to the meshes at contact points.
Thus the forces on the observed hand are obtained by finding the closest matching synthetic model. Both of these prior

Cornelia Fermüller et al.

approaches derive the forces using model based-approaches.
The forces are computed from the contact points, the shape
of the hand, and dynamic observations. Furthermore, both
use RGBD data, while ours is an end-to-end learning approach using as input only images.
Dexterous Actions: The robotics community has been
studying perception and control problems of dexterous actions for decades (Shimoga, 1996). Some works have studied grasping taxonomies (Cutkosky, 1989; Feix et al, 2009),
how to recognize grasp types (Rogez et al, 2015) and how
to encode and represent human hand motion (Romero et al,
2013). Pieropan et al (2013) proposed a representation of
objects in terms of their interaction with human hands. Realtime visual trackers (Oikonomidis et al, 2011) were developed, facilitating computational research with hands. Recently, several learning based systems were reported that
infer contact points or how to grasp an object from its appearance (Saxena et al, 2008; Lenz et al, 2015).
Manipulation Datasets: A number of object manipulation datasets have been created, many of them recorded with
wearable cameras providing egocentric views. For example, the Yale grasping dataset (Bullock et al, 2015) contains
wide-angle head-mounted camera videos recorded from four
people during regular activities with images tagged with the
hand grasp (of 33 classes). Similarly, the UT Grasp dataset
(Cai et al, 2015) contains head-mounted camera video of
people grasping objects on a table, and was tagged with
grasps (of 17 classes). The GTEA set (Fathi et al, 2011) has
egocentric videos of household activities with the objects
annotated. Other datasets have egocentric RGB-D videos.
The UCI-EGO (Rogez et al, 2014) features object manipulation scenes with annotation of the 3D hand poses, and
the GUN-71 (Rogez et al, 2015) features subjects grasping
objects, where care was taken to have the same amount of
data for each of the 71 grasp types. Our datasets, in contrast, are taken from the third-person viewpoint. While having less variation in the visual setting than most of the above
datasets, it focuses on the dynamic aspects of different actions, which manipulate the same objects.
Action Recognition as an Online Process: Action recognition has been extensively studied. However, few of the
proposed methods treat action recognition as a continuous
(in the online sense) process; typically, action classification
is performed on whole action sequences (Schuldt et al, 2004;
Ijina and Mohan, 2014). Recent works include building robust action models based on MoCap data (Wang et al, 2014)
or using CNNs for large-scale video classification (Karpathy
et al, 2014; Simonyan and Zisserman, 2014a). Most methods that take into account action dynamics usually operate
under a stochastic process formulation, e.g., by using Hidden Markov Models (Lv and Nevatia, 2006) or semi-Markov
models (Shi et al, 2011). HMMs can model relations between consecutive image frames, but they cannot be applied

Prediction of Manipulation Actions

5

to high-dimensional feature vectors. In (Fanello et al, 2013)
the authors propose an online action recognition method by
means of SVM classification of sparsely coded features on
a sliding temporal window. Most of the above methods assume only short-time dependencies between frames, make
restrictive assumptions about the Markovian order of the underlying processs and/or rely on global optimization over the
whole sequence.

a number of frames). Furthermore, the subject of our study
is novel. The previous approaches consider the classical full
body action problem. Here our emphasis is specifically on
the hand motion, not considering other information such as
the objects involved.

In recent work a few studies proposed approaches to
recognition of partially observed actions under the headings
of early event detection or early action recognition. Ryoo
(2011) creates a representation that encodes how histograms
of spatio-temporal features change over time. In a probabilistic model, the histograms are modeled with Gaussian
distributions, and MAP estimation over all subsequences is
used to recognize the ongoing activity. A second approach
in the paper models the sequential structure in the changing
histogram representation, and matches subsequences of the
video using dynamic programming. Both approaches were
evaluated on full body action sequences. In (Ryoo and Matthies,
2013) images are represented by spatio-temporal features
and histograms of optical flow, and a hierarchical structure
of video-subsegments is used to detect partial action sequences
in first-person videos. Ryoo et al (2015) perform early recognition of activities in first person-videos by capturing special
sub-sequences characteristic for the onset of the main activity. Hoai and De la Torre (2014) propose a maximum-margin
framework (a variant of SVM) to train visual detectors to
recognize partial events. The classifier is trained with all the
video sub-sequences of different length. To enforce the sequential nature of the events, additional constraints on the
score function of the classifier are enforced, for example, it
has to increase as more frames are matched. The technique
was demonstrated in multiple applications, including detection of facial expressions, hand gestures, and activities.

In this section, we first review the basics of Recurrent Neural Networks (RNNs) and the Long Short Term Memory
(LSTM) model. Then we describe the specific algorithms
for prediction of actions and forces used in our approach.

The main learning tools used here, the RNN and the
Long Short Term Memory (LSTM) model, were recently
popularized in language processing, and have been used for
translating videos to language (Venugopalan et al, 2014),
image description generation (Donahue et al, 2015), object
recognition (Visin et al, 2014), and the estimation of object motion (Fragkiadaki et al, 2015). RNNs were also used
for action recognition (Ng et al, 2015) to learn dynamic
changes within the action. The aforementioned paper still
performs whole video classification by using average pooling and does not consider the use of RNNs for prediction. In
a very recent work, however, Ma et al (2016) train a LSTM
using novel ranking losses for early activity detection. Our
contribution regarding action recognition is not that we introduce a new technique. We use an existing method (LSTM)
and demonstrate it in an online prediction system. The system keeps predicting, and considers the prediction reliable,
when the predicted label converges (i.e. stays the same over

3 Our Approach

3.1 Recurrent Neural Networks
Recurrent Neural Networks have long been used for modeling temporal sequences. The recurrent connections are feedback loops in the unfolded network, and because of these
connections RNNs are suitable for modeling time series with
strong nonlinear dynamics and long time correlations.
Given a sequence x = {x1 , x2 , . . . , xT }, a RNN computes
a sequence of hidden states h = {h1 , h2 , . . . , hT } and outputs
y = {y1 , y2 , . . . , yT } as follows:
ht = H (Wih xt +Whh ht−1 + bh )

(1)

yt = O(Who ht + bo ),

(2)

where Wih ,Whh ,Who denote weight matrices, bh , bo denote
the biases, and H (·) and O(·) are the activation functions
of the hidden layer and the output layer, respectively. Typically, the activation functions are defined as logistic sigmoid
functions.
The traditional RNN is hard to train due to the so called
vanishing gradient problem, i.e. the weight updates computed via error backpropagation through time may become
very small. The Long Short Term Memory model (Hochreiter and Schmidhuber, 1997) has been proposed as a solution to overcome this problem. The LSTM architecture uses
memory cells with gated access to store and output information, which alleviates the vanishing gradient problem in
backpropagation over multiple time steps.
Specifically, in addition to the hidden state ht , the LSTM
also includes an input gate it , a forget gate ft , an output gate
ot , and the memory cell ct (shown in Figure 2). The hidden layer and the additional gates and cells are updated as
follows:
it = σ (Wxi xt +Whi ht−1 +Wci ct−1 + bi )

(3)

ft = σ (Wx f xt +Wh f ht−1 +Wc f ct−1 + b f )

(4)

ct = ft ct−1 + it tanh(Wxc xt +Whc ht−1 + bc )

(5)

ot = σ (Wxo xt +Who ht−1 +Wco ct + bo )

(6)

ht = ot tanh(ct )

(7)

6

Cornelia Fermüller et al.

Model learning: We follow the common approach of training the model by minimizing the negative log-likelihood over
the dataset D. The loss function is defined as
|D|

l(D,W, b) = − ∑ log(P(Y = y(i) |x(i) ,W, b)),

(10)

i=0

Fig. 2 A diagram of a LSTM memory cell (adapted from (Graves et al,
2013)).

In this architecture, it and ft are sigmoidal gating functions, and these two terms learn to control the portions of the
current input and the previous memory that the LSTM takes
into consideration for overwriting the previous state. Meanwhile, the output gate ot controls how much of the memory
should be transferred to the hidden state. These mechanisms
allow LSTM networks to learn temporal dynamics with long
time constants.

where W and b denote the weight matrix and the bias term.
These parameters can be learnt using the stochastic gradient
descent algorithm.
Since we aim for the ongoing prediction rather than a
classification of the whole sequence, we do not perform a
pooling over the sequences to generate the outputs. Each
prediction is based only on the current frame and the current
hidden state, which implicitly encodes information about
the history. In practice, we achieve learning by performing
backpropagation at each frame.

3.2 RNN for action prediction
In this section, we describe our proposed model for action
prediction. We focus on manipulation actions where a person manipulates an object using a single hand. Given a video
sequence of a manipulation action, the goal is to generate a
sequence of belief distributions over the predicted actions
while watching the video. Instead of assigning an action label to the whole sequence, we continuously update our prediction as frames of the video are processed.
Visual representation: The visual information most essential for manipulation actions comes from the pose and movement of the hands, while the body movements are less important. Therefore, we first track the hand using a meanshift based tracker (Bradski, 1998), and use cropped image
patches centered on the hand. In order to create abstract representations of image patches, we project each patch through
a pre-trained CNN model (shown in Figure 3). This provides
the feature vectors used as input to the RNN.
Action prediction: In our model, the LSTM is trained using
as input a sequence of feature vectors x = {x1 , x2 , · · · , xT }
and the action labels y ∈ [1, N]. The hidden states and the
memory cell values are updated according to equations (3)(7). Then logistic regression is used to map the hidden states
to the label space as follows:
P(Y = i|ht ,Wu , bu ) = so f tmaxi (Wu ht + bu ).

3.3 RNN for prediction of forces at the fingers
We use a model similar to the one above to predict the forces
on the fingers from visual input. Given video sequences of
actions, as well as simultaneously recorded sequences of
force measurements (see Sec. 4.1), we reformulate the LSTM
model, such that it predicts force estimates as close as possible to the ground truth values.
As before, we use as input to the LSTM features from
pre-trained CNNs applied to image patches. In addition, the
force measurements v = {v1 , v2 , · · · , vT }, vt ∈ RM , are used
as target values, where M is the number of force sensors
attached to the hand. Then the forces are estimated as:
vˆt = Wv ht + bv .

(11)

(8)

To train the force estimation model, we define the loss
function as the least squares distance between the estimated
value and the ground truth, and minimize it over the training
set using stochastic gradient descent as:

(9)

l(D,W, b) = ∑ ∑ kvˆt − vt k22

Then the predicted action label is obtained as:
yˆt = argmaxi P(Y = i|ht ,Wu , bu ).

Fig. 3 The flowchart of the action prediction model, where the LSTM
model is unfolded over time.

|D| T

i=0 t=0

(12)

Prediction of Manipulation Actions

7

4 Data collection
4.1 A device for capturing finger forces during
manipulation actions
We made a force sensing device with four force sensors attached directly to four fingers: the thumb, the pointer, the
middle and the ring finger (see Figure 4(a)). We omitted the
small finger, as the forces on this finger are usually quite
small and not consistent across subjects (as found also by
(Pham et al, 2015)). We used the piezoresistive force sensors
by Tekscan, with a documented accuracy (by the manufacturer) of ±3%. The sensors at the finger tips have a measurement range of 0 to 8.896 N (2 lb), with a round sensing area
of 9.53 mm in diameter. The entire sensing area is treated as
one single contact point.
The raw sensor outputs are voltages, from which we derived the forces perpendicular to the sensor surfaces as:

F = 4.448 ∗ C1 ∗


Vout
−C2 ,
Vin −Vout

(13)

where Vout is the sensor measurement. Vin , C1 , and C2 are
fixed constants of the system. To remove environmental noise,
we applied notch filtering to the raw data, which gave us
clear and smooth force outputs (see Figure 5). The software,
which we designed for the device, will be released as a ROS
package, including data recording and force visualization
modules.

(a)

(b)

Fig. 5 Force data collection. (a) The raw, unfiltered voltage signal from
the fingertip force sensors. (b) The filtered force signal from the fingertip sensors.

4.2.1 Manipulation action dataset (MAD)
We asked five subjects to perform a number of actions with
five objects, namely cup, stone, sponge, spoon, and knife.
Each object was manipulated in five different actions with
five repetitions, resulting in a total of 625 action samples.
Table. 1 lists all the object and action pairs considered in
MAD.
Table 1 Object and Action pairs of MAD
Object
cup
stone
sponge
spoon
knife

Actions
drink, pound,shake,move,pour
pound,move,play,grind,carve
squeeze,flip,wash,wipe,scratch
scoop,stir,hit,eat,sprinkle
cut,chop,poke a hole,peel,spread

Since our aim was to build a system that can predict the
action as early as possible, we wanted to study the prediction
performance during different phases in the action. To facilitate such studies, we labeled the time in the videos when the
hand establishes contact with the objects, which we call the
“touching point.”

4.2 Manipulation Datasets

Two datasets were collected. The first dataset contains videos
of people performing dexterous actions on various objects.
The focus was to have different actions (with significant
variation) on the same object. This dataset was used to validate our approach of visual action prediction.
The second dataset contains simultaneously recorded video 4.2.2 Hand actions with force dataset (HAF)
and force data streams, but it has fewer objects. It was used
To solve the problem of synchronization, we asked subjects
to evaluate our approach of hand force estimation.
to wear on their right hand the force sensing device, leave
their left hand bare, and then perform with both hands the
same action, with one hand mirroring the other (see Figure
4(b) for the setting). We recorded from five subjects performing different manipulation actions on four objects, as
listed in Table 2. Each action was performed with five repetitions, resulting in a total of 500 sample sequences.
5 An experimental study with humans
(a)

(b)

Fig. 4 Illustration of the force-sensing device. (a) The sensors on four
fingers; (b) The data collection.

We were interested in how humans perform in prediction
at different phases during the action. Intuitively, we would
expect that the hand configuration and motion just before

8

Cornelia Fermüller et al.

Table 2 Object and Action pairs of HAF
Object
cup
fork
knife
sponge

Actions
drink, move, pound, pour, shake
eat, poke a hole, pick, scratch, whisk
chop, cut, poke a hole, scratch, spread
flip, scratch, squeeze, wash, wipe

the grasping of the object, when establishing contact, and
shortly after the contact point can be very informative of the
intended action. Therefore, in order to evaluate how early we
can accurately predict, we investigated the prediction performance at certain time offsets with respect to the touching
point.
We picked three objects from the MAD dataset for the
study, namely cup, sponge and spoon. The prediction accuracy at four different time points was then evaluated: 10
frames before the contact point, exactly at contact, 10, and
25 frames after the contact point. Figure 6 shows the interface subjects used in this study.

tions are easily confused. For the cup, ’shake’ and ’hit’ were
even after 25 frames still difficult to recognize, and for the
spoon the early phases of movement for most actions appeared similar, and ’eat’ was most difficult to identify.
To see whether there is additional distinctive information
in the actors’ movement, and subjects can take advantage
of it with further learning, we performed a second study.
Five participating subjects were shown 4 sets of 40 videos
for each object, and this time they were given feedback on
which was the correct action. Figure 7(b) shows the overall
success rate for each object and time offset over the four sets.
If learning occurs, subjects’ should improve from the first to
the fourth set. The graphs show that there is a bit of learning.
The effect is largest for the spoon, where subjects can learn
to better distinguish at 10 frames after contact. The focus
was to have different actions (with significant variation) on
the same object.

Fig. 6 Interface used in the human study.

In a first experiment we asked 18 human subjects to perform the prediction task. For each of the three objects, after
a short “training” phase in which all actions were demonstrated at full length, each subject was shown a set of 40
video segments and was asked to identify the currently perceived action. Each segment ended at one of the four time
points relative to the contact point described above and was
constructed from the same hand patches used in the computational experiments. All actions and all time offsets were
equally represented. Figure 7(a) plots subjects’ average prediction performance for the different objects, actions and
time offsets. With five actions per object, 20% accuracy corresponds to chance level. As we can see, the task of judging
before and even at contact point, was very difficult and classification was at chance for two of the objects, the spoon and
and the cup, and above chance at contact only for the sponge.
At 10 frames after contact human classification becomes
better and reaches in average about 75% for the sponge, 60%
for the cup, but only 40% for the spoon. At 25 frames subjects’ judgment becomes quite good with the sponge going
above 95% for four of the five actions, and the other two
actions in average at about 85%. We can also see which ac-

(a) without feedback

(b) with feedback

Fig. 7 Human prediction performance. (a) First study (without feedback). Success rate for three objects (cup, sponge, and spoon) for five
different actions at four time offsets. (b) Second study (with feedback).
Success rate for three objects averaged over five actions over four sets
of videos at four offsets.

6 Experimental results
The two algorithms have been implemented in a system that
runs in real-time on a GPU. This sections reports three ex-

Prediction of Manipulation Actions

9

Fig. 8 Prediction accuracies over time for the five different objects, and Prediction Uncertainty computed from the entropy. The black vertical
bars show the touching point. For each object we warped and aligned all the sample sequences so that they align at the same touching point. Best
viewed in color.

perimental evaluations. The first experiment evaluates the
prediction performance as an on-going task, the second compares our action recognition algorithm against human performance, and the third evaluates our force estimation.

Then all the results were averaged over the five rounds of
testing.

6.1.1 On-going prediction
6.1 Hand action prediction on MAD
Our approach uses visual features obtained with deep learning, which serve as input to a sequence learning technique.
First we apply the mean-shift based tracker of Comaniciu et al (2000) to obtain the locations of the hand. We crop
image patches of size 224×224 pixels, centered on the hand.
Then our feature vectors are computed by projecting these
patches through a convolutional neural network. To be specific, we employ the VGG network (Simonyan and Zisserman, 2014b) with 16 layers, which has been pre-trained on
the ImageNet. We take the output of layer “fc7” as feature
vector (4096 dimensions), which we then use to train a one
layer LSTM RNN model for action prediction.
Our RNN has hidden states of 64 dimensions, with all
the weight matrices randomly intialized using the normal
distribution. We first learn a linear projection to map the
4096 input features to the 64 dimensions of the RNN. We
use mini-batches of 10 samples and the adaptive learning
rate method to update the parameters. The training stops after 100 epochs in all the experiments.
To evaluate the action prediction performance, we performed leave-one-subject-out cross-validation over the five
subjects. Each time we used the data from one subject for
testing, and trained the model on the other four subjects.

Our goal is to understand how the recognition of action improves over time. Thus, we plot the prediction accuracy as
a function of time, from the action preparation to the end of
the action. Our system performs predictions based on every
new incoming frame as the action unfolds.
The first five plots in Figure 8 show the change in prediction accuracy over time. For a given action video, our system generates for each frame a potential score vector (with
one value for each action) to form a score sequence of same
length as the input video. Since the actions have different
length, we aligned them at the touching points. To be specific, we resampled the sequences before and after the touching points to the same length. For each object, we show the
prediction accuracy curves of the five actions.
The vertical bar in each figure indicates the time of the
touching point. The touching point splits the sequence into
two phases: the “preparation” and the “execution”. It is interesting to see that for some object-action pairs our system yields high prediction accuracy even before the touching point, e.g. the “cup - drink” and “sponge - wash”.
The last plot in Figure 8 shows the change of prediction
uncertainty over time for each of the five objects. This measure was derived from the entropy over the different actions.
As can be seen, in all cases, the uncertainty drops rapidly as
the prediction accuracy rises along time.

10

6.1.2 Classification results
At the end of the action, the on-going prediction task becomes a traditional classification. To allow evaluating our
method on classical action recognition, we also computed
the classification results for the whole video. The estimate
over the sequence was derived as a weighted average over
all frames using a linear weighting with largest value at the
last frame. To be consistent with the above, the classification
was performed for each object over the five actions considered.
Figure 9 shows the confusion matrix of the action classification results. One can see that, our model achieved high
accuracy on various object-action combinations, such as “cup
/drink” and “sponge/wash”, where the precision exceeds 90%.
We used two traditional classification methods as our
baseline: Support Vector Machine (SVM) and Hidden Markov
Model (HMM). For the HMM model, we used the mixture
of Gaussian assumption and we chose the number of hidden states as five. Since the SVM model doesn’t accept input samples of different length, we used a sliding window
(size = 36) mechanism. We performed the classification over
each window, and then combined the results using majority
voting. For both these baseline methods, we conducted a dimension reduction step to map the input feature vectors to
128 dimensions using PCA. To further explore the efficiency
of the LSTM method in predicting actions on our dataset,
we also applied the LSTM model using HoG features as input. The average accuracy was found 59.2%, which is 10%
higher than the HMM and 23% higher than the SVM, but
still significantly lower than our proposed method.

Cornelia Fermüller et al.
Table 3 Comparison of classification accuracies on different objects
Object/Action

SVM

HMM

cup/drink
cup/pound
cup/shake
cup/move
cup/pour
stone/pound
stone/move
stone/play
stone/grind
stone/carve
sponge/squeeze
sponge/flip
sponge/wash
sponge/wipe
sponge/scratch
spoon/scoop
spoon/stir
spoon/hit
spoon/eat
spoon/sprinkle
knife/cut
knife/chop
knife/poke a hole
knife/peel
knife/spread
Avg.

79.1%
20.0%
64.3%
62.7%
60.0%
26.7%
87.8%
64.6%
28.3%
43.3%
41.1%
53.3%
85.9%
46.9%
30.0%
39.0%
45.3%
28.9%
65.0%
60.0%
33.5%
0.0%
33.3%
66.3%
38.2%
48.1%

96.0%
81.7%
56.8%
53.2%
100.0%
73.3%
68.0%
97.1%
45.0%
28.5%
81.7%
91.0%
84.6%
47.5%
0.0%
27.1%
30.0%
20.0%
79.2%
25.0%
33.7%
45.0%
20.0%
28.9%
28.3%
53.7%

LSTM
HOG
82.9%
40.0%
32.6%
51.9%
80.3%
60.0%
90.0%
60.5%
60.0%
66.0%
64.3%
96.0%
91.1%
58.1%
43.3%
53.6%
20.0%
22.4%
78.1%
40.5%
49.6%
43.3%
51.0%
90.0%
54.3%
59.2%

LSTM
VGG16
92.5%
73.3%
83.3%
82.1%
80.8%
73.3%
61.4%
86.7%
46.7%
39.1%
83.4%
71.0%
92.5%
46.3%
15.0%
32.0%
74.3%
56.7%
81.1%
69.1%
75.3%
72.7%
72.0%
72.5%
74.2%
68.3%

6.1.3 Discussion
It should be noted that this is a novel, challenging dataset
with no equivalent publicly available counterparts. Subjects
performed the action in unconstrained conditions, and thus
there was a lot of variation in their movement, and they performed some of the actions in very similar ways, making
them difficult to distinguish, as also our human study confirms.
The results demonstrate that deep learning based continuous recognition of manipulation actions is feasible, providing a promising alternative to traditional methods such
as HMM, SVM and other methods based on hand-crafted
features.

6.2 Action prediction at the point of contact, before and
after
We next compare the performance of our online algorithm
(as evaluated in Section 6.1.1) against those of human subjects. Figure 10 summarizes the prediction performance per

Fig. 9 Confusion matrix of action classification.

object and time offset. As we can see our algorithm’s performance is not significantly behind those of humans. At ten
frames after contact, computer lags behind human performance. However, at 25 frames after the contact point, the
gaps between our proposed model and human subjects are
fairly small. Our model performs worse on the spoon, but
this is likely due to the large variation in the way different
people move this object. Our human study already revealed

Prediction of Manipulation Actions

the difficulty in judging spoon actions, but the videos shown
to subjects featured less actors than were tested with the algorithm. Considering this, we can conclude that our algorithm is already close to human performance in fast action
prediction.

11

6.3.1 Training
The LSTM model ( described in Section 3.3) is used to estimate the hand forces for each frame. Since people have different preferences in performing actions, the absolute force
values can vary significantly for the same action. Therefore,
we first normalize the force samples, which are used for
training, to the range [0, 1]. The visual features in the video
frames are obtained the same way as in the action prediction.
Our LSTM model has one layer with 128 hidden states. To
effectively train the model, we use the adaptive learning rate
method for updating the neural network, and we use a batch
size of 10 and stop the training at 100 epochs.

6.3.2 Results
Fig. 10 Comparison of prediction accuracies between our computational method (C) and data from human observers (H). Actions are
classified at four different time points before, at, and after the touching
point (at -10,0,+10, +25 frames from the touching point). C denotes the
learnt model, H denotes the psychophysical data).

6.3 Hand force estimation on HAF
In the following we demonstrate the ability of the RNN to
predict the forces on the fingers directly from images. We
developed an online force estimation system. While watching a person performing actions in front of the camera, our
system provides the finger forces in real time. Figure 11
shows one example of online force prediction for the “drink”
action. We next describe our training method, and then present
our results.

We first show examples of our force estimation and then report the average errors. Figure 12 shows sample results. For
each of the six pairs, the upper graph shows the estimated
forces, and the lower one shows the ground truth. It can be
seen that our system estimates well the overall force patterns
for different actions. For example, for the “sponge/squeeze”
action, the estimated forces correctly reproduce the three
peaks of the real action, or for the “cup/move” action, the
output forces predict the much smoother changes. Table 4
provides the average error of estimated force for each finger, and Table 5 gives the average estimation error for all
the actions. The errors are in the range of 0.075 to 0.155,
which demonstrates that the method also has good quantitative prediction and potential for visual force prediction.

Table 4 Average errors of estimated force for each finger
Avg.

Ring
0.103

Middle
0.098

Pointer
0.130

Thumb
0.119

Table 5 Average errors of estimated force for each action
Object
Cup
Fork
Knife
Fig. 11 Illustration of the online force estimation system. The video
frames in the top row show samples of the action ’drinking from a
cup.’ The second row shows the estimated forces and the third row the
corresponding ground truth.

Sponge

Action 1
Drink
0.096
Eat
0.106
Chop
0.157
Flip
0.101

Action 2
Move
0.122
Hole
0.090
Cut
0.155
Scratch
0.130

Action 3
Pound
0.108
Pick
0.075
Poke
0.109
Squeeze
0.112

Action 4
Pour
0.107
Scratch
0.094
Scratch
0.123
Wash
0.127

Action 5
Shake
0.110
Whisk
0.100
Spread
0.110
Wipe
0.121

12

Cornelia Fermüller et al.

Fig. 12 Samples of force estimation results. The first and third row show the force estimation. The second and fourth row show the corresponding
ground truth values.

Table 6 Action prediction accuracy. Comparsion of prediction using vision data only (”Vision”) against using vision and force data
(”V+F”).

Object
Vision
V+F

cup
82.4%
88.2%

stone
61.4%
75.1%

sponge
61.6%
59.1%

spoon
62.6%
57.5%

knife
73.3%
72.7%

Avg.
68.3%
70.5%

6.3.3 Why predict forces?
One motivation for predicting forces, is that the additional
data, which we learned through association, may help increase recognition accuracy. There is evidence that human
understand others’ actions in terms of their own motor primitives (Gallesse and Goldman, 1998; Rizzolatti et al, 2001).
However, so far these findings have not been modeled in
computational terms.
To evaluate the usefulness of the predicted forces, we
applied our force estimation algorithm on the MAF dataset
to compute the force values. Then we used the vision data
together with the regressed force values as bimodal information to train a network for action predicton. Table 6 shows
the results of the prediction accuracy with the bimodal information on different objects. Referring to the table, the
overall average accuracy for the combined vision force data
(V+F) was 2.2% higher than for vision information only.

This first attempt on predicting with bimodal data demonstrates the potential of utilizing visually estimated forces for
recognition. Future work will further elaborate on the idea
and explore networks (Hoffman et al, 2016), which can be
trained from both vision and force at the same time to learn
”hallucinate” the forces and predict actions.
As discussed in the introduction, the other advantage is
that we will be able to teach robots through video demonstration. If we can predict forces exerted by the human demonstrator and provide the force profile of the task using vision only, this would have a huge impact on the way robots
learn force interaction tasks. In future work we plan to develop and employ sensors that can also measure the tangential forces, i.e. the frictions, on the fingers. We also will expand the sensor coverage to the whole hand. With these two
improvements, our method could be applied to a range of
complicated task such as screwing or assembling.

7 Conclusion and Future work
In this paper we proposed an approach to action interpretation, which treats the problem as a continuous updating of
beliefs and predictions. The ideas were implemented for two
tasks: the prediction of perceived action from visual input,
and the prediction of force values on the hand. The methods were shown to run in real-time and demonstrated high
accuracy performance. The action prediction was evaluated

Prediction of Manipulation Actions

also against human performance, and shown to be nearly on
par. Additionally, new datasets of videos of dexterous actions and force measurements were created, which can be
accessed from (Fermüller, 2016).
The methods presented here are only a first implementation of a concept that can be further developed along a number of directions. Here, we applied learning on 2D images
only, and clearly, this way we also learn properties of the images that are not relevant to the task, such as the background
textures. In order to become robust to these ‘nuisances’, 3D
information, such as contours and depth features, could be
considered in future work. While the current implementation only considers action labels, the same framework can
be applied for other aspects of action understanding. For example, one can describe the different phases of actions and
predict these sub-actions since different actions share similar small movements. One can also describe the movements
of other body parts, e.g., the arms and shoulders. Finally,
the predicted forces may be used for learning how to perform actions on the robot. Future work will attempt to map
the forces from the human hands onto other actuators, for
example three-fingered hands or grippers.

Acknowledgement
This work was funded by the support of the National Science Foundation under grant SMA 1540917 and grant CNS
1544797, by Samsung under the GRO program (N020477,
355022), and by DARPA through U.S. Army grant W911NF14-1-0384.
References
Aloimonos Y, Fermüller C (2015) The cognitive dialogue: A new
model for vision implementing common sense reasoning. Image and
Vision Computing 35(12):2891–2903
Ansuini C, Giosa L, Turella L, Altoé G, Castiello U (2008) An object for an action, the same object for other actions: effects on hand
shaping. Exp Brain Research 185(1):111–119
Ansuini C, Cavallo A, Bertone C, Becchio C (2015) Intentions in
the Brain: The Unveiling of Mister Hyde. The Neuroscientist
21(2):126–135
Argall BD, Chernova S, Veloso M, Browning B (2009) A survey of
robot learning from demonstration. Robotics and Autonomous Systems 57(5):469–483
Aviles AI, Marban1a A, Sobrevilla P, Fernandez J, Casals A (2015) A
recurrent neural network approach for 3d vision-based force estimation. In: aCoRR, abs/1505.00393
Bradski GR (1998) Computer vision face tracking for use in a perceptual user interface
Bullock IM, Feix T, Dollar AM (2015) The Yale Human Grasping Data
Set: Grasp, Object, and Task Data in Household and Machine Shop
Environments. International Journal of Robotics Research
Cai M, Kitani KM, Sato Y (2015) A scalable approach for understanding the visual structures of hand grasps. In: 2015 IEEE International
Conference on Robotics and Automation (ICRA), IEEE, pp 1360–
1366

13
Comaniciu D, Ramesh V, Meer P (2000) Real-time tracking of nonrigid objects using mean shift. In: Computer Vision and Pattern
Recognition, 2000. Proceedings. IEEE Conference on, IEEE, vol 2,
pp 142–149
Crajé C, Lukos J, Ansuini C, Gordon A, Santello M (2011) The effects of task and content on digit placement on a bottle. Exp Brain
Research 212(1):119–124
Cutkosky MR (1989) On grasp choice, grasp models, and the design of
hands for manufacturing tasks. IEEE Transactions on Robotics and
Automation 5(3):269–279
Donahue J, Anne Hendricks L, Guadarrama S, Rohrbach M, Venugopalan S, Saenko K, Darrell T (2015) Long-term recurrent convolutional networks for visual recognition and description. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp 2625–2634
Doyle J, Csete M (2011) Architecture, constraints, and behavior. Proceedings of the National Academy of Sciences 108(Sup. 3):15,624–
15,630
Erol A, Bebis G, Nicolescu M, Boyle RD, Twombly X (2007) Visionbased hand pose estimation: A review. Computer Vision and Image
Understanding 108(1):52–73
Fanello SR, Gori I, Metta G, Odone F (2013) Keep it simple and sparse:
Real-time action recognition. The Journal of Machine Learning Research 14(1):2617–2640
Fathi A, Ren X, Rehg JM (2011) Learning to recognize objects in
egocentric activities. In: Computer Vision and Pattern Recognition
(CVPR), IEEE Conference On, pp 3281–3288
Feix T, Pawlik R, Schmiedmayer H, Romero J, Kragic D (2009) A
comprehensive grasp taxonomy. In: Robotics, Science and Systems
Conference: Workshop on Understanding the Human Hand for Advancing Robotic Manipulation
Fermüller C (2016) Prediction of Manipulation Actions. http://www.
cfar.umd.edu/{$\sim$}fer/action-prediction/

Fouhey DF, Zitnick C (2014) Predicting object dynamics in scenes. In:
IEEE Conference on Computer Vision and Pattern Recognition
Fragkiadaki K, Levine S, Felsen P, Malik J (2015) Recurrent network
models for kinematic tracking. In: arXiv:1508.00271
Gallesse V, Goldman A (1998) Mirror neurons and the simulation theory of mind-reading. Trends in Cognitive Sciences 2(12):493–501
Gams A, Do M, Ude A, Asfour T, Dillmann R (2010) On-line periodic
movement and force-profile learning for adaptation to new surfaces.
In: IEEE International Conference on Robotics Research (ICRA),
IEEE, pp pp.3192–3199
Graves A, Mohamed Ar, Hinton G (2013) Speech recognition with
deep recurrent neural networks. In: IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp 6645–
6649
Greminger M, Nelson B (2004) Vision-based force measurement.
IEEE Transactions on Pattern Analysis and Machine Intelligence
26(3):290–298
Gupta A, Davis LS (2008) Beyond nouns: Exploiting prepositions and
comparative adjectives for learning visual classifiers. In: European
Conference of Computer Vision (ECCV), pp 26–29
Hoai M, De la Torre F (2014) Max-margin early event detectors. International Journal of Computer Vision 107(2):191–202
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural
computation 9(8):1735–1780
Hoffman J, Gupta S, Darrell T (2016) Learning with side information
through modality hallucination. In: In Proc. Computer Vision and
Pattern Recognition (CVPR)
Ijina E, Mohan S (2014) Human action recognition based on recognition of linear patterns in action bank features using convolutional
neural networks. In: International Conference on Machine Learning
and Applications
Jeannerod M (1984) The timing of natural prehension movements.
Journal of Motor Behavior 16(3):235–254

14
Joo J, Li W, Steen FF, Zhu SC (2014) Visual persuasion: Inferring
communicative intents of images. In: Computer Vision and Pattern
Recognition (CVPR), 2014 IEEE Conference on, IEEE, pp 216–223
Karpathy A, Toderici G, Shetty S, Leung T, Sukthankar R, Fei-Fei
L (2014) Large-scale video classification with convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp 1725–1732
Keskin C, Kıraç F, Kara YE, Akarun L (2013) Real time hand pose
estimation using depth sensors. In: Consumer Depth Cameras for
Computer Vision, Springer, pp 119–137
Kitani KM, Ziebart BD, Bagnell JA, Hebert M (2012) Activity forecasting. In: European Conference on Computer Vision (ECCV)
Kober J, Gienger M, Steil J (2000) Learning movement primitives for
force interaction tasks. In: Proceedings. IEEE Conference on Computer Vision and Pattern Recognition, vol 2, pp 142–149
Koppula H, Saxena A (2016) Anticipating human activities using object affordances for reactive robotic response. IEEE Transactions on
pattern Analysis and Machine Intelligence 38(1)
Kormushev P, Calinon S, Caldwell DG (2011) Imitation learning of
positional and force skills demonstrated via kinesthetic teaching and
haptic input. Advanced Robotics 25(5):581–603
Kulkarni G, Premraj V, Ordonez V, Dhar S, Li S, Choi Y, Berg AC,
Berg T (2013) Babytalk: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence 34:42–44
Lenz I, Lee H, Saxena A (2015) Deep learning for detecting robotic
grasps. The International Journal of Robotics Research
Li Y, Fermüller C, Aloimonos Y, Ji H (2010) Learning shift-invariant
sparse representation of actions. In: Proceedings of the 2009 IEEE
Conference on Computer Vision and Pattern Recognition, IEEE,
San Francisco, CA, pp 2630–2637
Liu J, Feng F, Nakamura YC, Pollard NS (2014) A taxonomy of everyday grasps in action. In: 14th IEEE-RAS International Conf. on
Humanoid Robots, Humanoids
Lv F, Nevatia R (2006) Recognition and segmentation of 3-d human
action using hmm and multi-class adaboost. In: Computer Vision–
ECCV 2006, Springer, pp 359–372
Ma S, Sigal L, Sclaroff S (2016) Learning activity progression in lstms
for activity detection and early detection. In: The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)
Mandary C, Terlemez O, Do M, Vahrenkamp N, Asfour T (2015) The
kit whole-body human motion database. In: International Conferences on Advanced Robotics, pp 329–336
Melax S, Keselman L, Orsten S (2013) Dynamics based 3d skeletal
hand tracking. In: Proceedings of Graphics Interface 2013, Canadian Information Processing Society, pp 63–70
Moeslund T, Hilton A, Krüger V (2006) A survey of advances in
vision-based human motion capture and analysis. Computer Vision
and Image Understanding 104(2):90–126
Molchanov P, Gupta S, Kim K, Kautz J (2015) Hand gesture recognition with 3d convolutional neural networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, pp 1–7
Ng JYH, Hausknecht M, Vijayanarasimhan S, Vinyals O, Monga R,
Toderici G (2015) Beyond short snippets: Deep networks for video
classification. CVPR 2015
Ohn-Bar E, Trivedi MM (2014) Hand gesture recognition in real time
for automotive interfaces: A multimodal vision-based approach and
evaluations. IEEE Transactions on Intelligent Transportation Systems 15(6):2368–2377
Oikonomidis I, Kyriazis N, Argyros AA (2011) Efficient model-based
3d tracking of hand articulations using kinect. In: British Machnine
Vision Conference
Panteleris P, Kyriazis N, Argyros A (2015) 3d tracking of human hands
in interaction with unknown objects. In: British Machine Vision
Conference (BMVC 2015), BMVA Press, pp 123–1

Cornelia Fermüller et al.
Pham TH, Kheddar A, Qammaz A, Argyros AA (2015) Towards force
sensing from vision: Observing hand-object interactions to infer
manipulation forces. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
Pieropan A, Ek CH, Kjellstrom H (2013) Functional object descriptors
for human activity modeling. In: Robotics and Automation (ICRA),
2013 IEEE International Conference on, IEEE, pp 1282–1289
Pirsiavash H, Vondrick C, Torralba A (2014) Inferring the why in images. arXiv preprint arXiv:14065472
Rizzolatti G, Fogassi L, Gallese V (2001) Neurophysiological mechanisms underlying the understanding and imitation of action. Nature
Reviews Neuroscience 2(9):661–670
Rogez G, Khademi M, Supančič III J, Montiel JMM, Ramanan D
(2014) 3d hand pose detection in egocentric rgb-d images. In: Workshop at the European Conference on Computer Vision, Springer International Publishing, pp 356–371
Rogez G, Supancic JS III, Ramanan D (2015) Understanding everyday
hands in action from rgb-d images. In: IEEE International Conference on Computer Vision (ICCV)
Romero J, Feix T, Ek CH, Kjellstrom H, Kragic D (2013) A metric
for comparing the anthropomorphic motion capability of artificial
hands. IEEE Transactions on Robotics 29(6):1342–1352
Ryoo M, Fuchs TJ, Xia L, Aggarwal J, Matthies L (2015) Robotcentric activity prediction from first-person videos: What will they
do to me? In: ACM/IEEE International Conference on HumanRobot Interaction (HRI), ACM, pp 295–302
Ryoo MS (2011) Human activity prediction: Early recognition of
ongoing activities from streaming videos. In: Computer Vision
(ICCV), 2011 IEEE International Conference on
Ryoo MS, Matthies L (2013) First-person activity recognition: What
are they doing to me? In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp 2730–2737
Saxena A, Driemeyer J, Ng AY (2008) Robotic grasping of novel objects using vision. The International Journal of Robotics Research
27(2):157–173
Schuldt C, Laptev I, Caputo B (2004) Recognizing human actions: A
local SVM approach. In: Proc. International Conference on Pattern
Recognition
Shi Q, Cheng L, Wang L, Smola A (2011) Human action segmentation
and recognition using discriminative semi-markov models. International journal of computer vision 93(1):22–32
Shimoga KB (1996) Robot grasp synthesis algorithms: A survey. The
International Journal of Robotics Research 15(3):230–266
Shotton J, Sharp T, Kipman A, Fitzgibbon A, Finocchio M, Blake
A, Cook M, Moore R (2013) Real-time human pose recognition
in parts from single depth images. Communications of the ACM
56(1):116–124
Simonyan K, Zisserman A (2014a) Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:14091556
Simonyan K, Zisserman A (2014b) Very deep convolutional networks
for large-scale image recognition. CoRR abs/1409.1556
Starner T, Weaver J, Pentland A (1998) Real-time american sign language recognition using desk and wearable computer based video.
IEEE Transactions on Pattern Analysis and Machine Intelligence
20(12):1371–1375
Supancic JS, Rogez G, Yang Y, Shotton J, Ramanan D (2015) Depthbased hand pose estimation: data, methods, and challenges. In: Proceedings of the IEEE International Conference on Computer Vision,
pp 1868–1876
Takano W, Ishikawa J, Nakamura Y (2015) Using a human action
database to recognize actions in monocular image sequences: Recovering human whole body configurations. Advanced Robotics
29(12):771–784
Tiest WMB, Kappers AM (2014) Physical aspects of softness perception. In: Luca MD (ed) Multisensory Softness, Springer, London, pp
3–15

Prediction of Manipulation Actions
Turaga P, Chellappa R, Subrahmanian V, Udrea O (2008) Machine
recognition of human activities: A survey. IEEE Transactions on
Circuits and Systems for Video Technology 18(11):1473–1488
Venugopalan S, Xu H, Donahue J, Rohrbach M, Mooney R, Saenko K
(2014) Translating videos to natural language using deep recurrent
neural networks. In: arXiv:1412.4729
Visin F, Kastner K, Cho K, Matteucci M, Courville A, Bengio Y (2014)
A recurrent neural network based alternative to convolutional networks. In: International Conference on Image Processing Theory,
Tools and Applications (IPTA)
Vondrick C, Pirsiavash H, Torralba A (2016) Anticipating visual representations from unlabeled video. In: IEEE Conference on Computer
Vision and Pattern Recognition
Walker J, Gupta A, Hebert M (2014) Patch to the future: Unsupervised visual prediction. In: IEEE Conference on Computer Vision
and Pattern Recognition, pp 3302–3309
Wang J, Liu Z, Wu Y, Yuan J (2014) Learning actionlet ensemble for 3d
human action recognition. IEEE Transactions on Pattern Analysis
and Machine Intelligence 36(5):914–927
Wang SB, Quattoni A, Morency LP, Demirdjian D, Darrell T (2006)
Hidden conditional random fields for gesture recognition. In: IEEE
Conference on Computer Vision and Pattern Recognition, vol 2, pp
1521–1527
Xie D, Todorovic S, Zhu SC (2013) Inferring “dark matter” and “dark
energy” from videos. In: IEEE International Conference on Computer Vision (ICCV), pp 2224–2231
Yang Y, Fermüller C, Li Y, Aloimonos Y (2015) Grasp type revisited:
A modern perspective on a classical feature for vision. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)
Zhu Y, Zhao Y, Zhu SC (2015) Understanding tools: Task-oriented object modeling, learning and recognition. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp 2855–
2864

15

From Images to Sentences through Scene Description Graphs
using Commonsense Reasoning and Knowledge

arXiv:1511.03292v1 [cs.CV] 10 Nov 2015

†

Somak Aditya† , Yezhou Yang*, Chitta Baral† , Cornelia Fermuller* and Yiannis Aloimonos*
School of Computing, Informatics and Decision Systems Engineering, Arizona State University
*
Department of Computer Science, University of Maryland, College Park

Abstract

the scene in view. In this methodology, scenes are recognized by detecting the inside objects [30, 5, 23], objects are recognized by detecting their parts or attributes
[13, 26, 11, 49, 42, 43, 50] and activities are recognized
by detecting the motions, objects and contexts involved in
the activities [27, 32, 45, 17, 33, 46].
Recently, researchers have advanced the viewpoint that
if we are able to develop a semantic understanding of a visual scene, then we should be able to produce natural language descriptions of such semantics. This has given rise
to a new area in the field that integrates vision, knowledge
and natural language. Knowledge becomes especially important, as without background knowledge, it has become
increasingly hard to obtain a desirable level of accuracy in
this problem. And as such knowledge can be often mined
from text, the problem now stands at the intersection between Computer Vision and Natural Language Processing.
Mining such knowledge, storing it in a form that retains the
semantics, and reasoning using this knowledge to develop
a better understanding of scenes are the fundamental issues
that are addressed in this paper.
Current developments [31, 22, 8, 21, 44, 3] in Computer
Vision have shown that deep neural nets can be trained to
generate a caption for an arbitrary scene with decent success. It is indeed an exciting achievement. However, current state-of-the-art image captioning systems still have a
few drawbacks such as: 1) a brute-force image-to-text mapping makes it inconvenient to conduct Logical Reasoning
beyond just doing inferences from annotated data; 2) due
to the lack of intermediate semantic representations, they
are all language-dependent; and 3) most importantly, when
the system produces wrong results, it is almost impossible
to trace back the system and analyze the failure case (See
Figure 1).
Let us consider how humans accomplish this task. Human perception is active, selective and exploratory. We continuously shift our gaze to different locations in the scene.
After recognizing objects, we fixate again at a new location,
and so on. We interpret visual input by using our knowledge
of activities, events and objects. When we analyze a vi-

In this paper we propose the construction of linguistic
descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes
using an automatically constructed knowledge base. SDGs
are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given
images, (b) a “commonsense” knowledge base constructed
using natural language processing of image annotations
and (c) lexical ontological knowledge from resources such
as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show
that in most cases, sentences auto-constructed from SDGs
obtained by our method give a more relevant and thorough
description of an image than a recent state-of-the-art image
caption based approach. Our Image-Sentence Alignment
Evaluation results are also comparable to that of the recent
state-of-the art approaches.

1. Introduction
“Imagine, for example, a computer that could look at an
arbitrary scene, anything from a sunset at a fishing village
to Grand Central Station at rush hour and produce a verbal
description. This is a problem of overwhelming difficulty,
relying as it does to finding solutions to both vision and
language and then integrating them. I suspect that scene
analysis will be one of the last cognitive tasks to be performed well by computers”. This fifteen year old quote,
attributed to A. Rosenfeld [41], one of the founders of the
field of Computer Vision, pointed to the fundamental problem of generating semantics of visual scenes. Since then,
researchers have attempted a few approaches that mostly
centered on asking “what” and “where” questions about
1 Commonsense

reasoning and commonsense knowledge can be of
many types [6]. Commonsense knowledge can belong to different levels
of abstraction [18, 28]. In this paper, we focus on capturing and reasoning
based on knowledge about natural activities.

1

(a)

(b)

Figure 1: Examples from [21]: (a) Positive example annotation:
construction worker in orange safety vest is working on road, (b)
Negative example annotation: a bunch of bananas are hanging
from a ceiling. Such annotations could be infrequent, but it is hard
to logically justify such contrasting outputs.

sual scene, visual processes continuously interact with our
high-level knowledge, some of which is represented in the
form of language. In some sense, perception and language
are engaged in an interaction, as they exchange information
that leads to meaning and understanding. Thus, our problem requires at least two modules for its solution: (a) a vision module and (b) a reasoning module that are interacting
with each other. In this paper we propose to model the early
stages of this process. The available datasets make it impossible to perform experiments that will consider vision as an
active process, although this is the ultimate goal. Thus, our
question becomes: if the vision module produces a number
of (probabilistic) detections, how much the reasoning module can infer about the scene if it possesses common sense
abilities? It turns out that the reasoning module can infer a
great deal.
Motivated by such intuitions, we present here an effort
to integrate deep learning based vision and state-of-the-art
concept modeling from commonsense knowledge obtained
from text. We use a deep learning-based perception system
to obtain the objects, scenes and constituents with probabilistic weights from an input image. To predict how the objects interact in the scene, we build a common-sense knowledge base from image annotations along with a Bayesian
Network capturing dependencies among commonly occurring objects and “Abstract Visual Concepts” (defined later).
These two precomputed resources help us infer the following: 1) the correct set of correlated objects based on
the high-confidence objects detected; 2) the most probable
events that these objects participate in; 3) the role that the
objects play in this event; and 4) given the events, objects
and constituents, the “Concept” that emerges from such information. Based on these inferences, we output a Scene
Description Graph (SDG) that depicts how these different
entities and events interact. In Figure 2, we show a possible
SDG for an example image. SDG is essentially a directed
labeled graph among entities and events2 that enables an
array of possibilities to do further analysis beyond visual
appearance, such as Event-Entity based analysis, question
2 Throughout this paper, we follow definition of Entities and Events
from [16, 2].

answering about the scene and flexible caption generation3 .
The fundamental contribution of this work is a novel
algorithm that uses automatically constructed Knowledge
Base to create an SDG from an image, which facilitates further reasoning and caption generation. SDGs have advantages over ground-truth sentences because : 1) they can be
easily processed by machines/AI systems in comparison to
sentences; 2) the output can be rich in information-content;
3) they are not bounded by specific templates, that are often used by researchers to convert labels into sentences and
4) the SDG also can be used to generate sentence descriptions. We also create a Knowledge Base which captures the
knowledge about the commonly-occurring Concepts, events
and entities. The knowledge base can be used to provide answers to the following queries: 1) the event or set of events
that connect two entities; 2) the role an entity plays in an
event and 3) a subset of all possible concepts involving the
entities and connecting events. Lastly, further inferences
about the scenes such as “Will the player holding the ball
be able to tackle the blocker and under what conditions”
can also be attempted by feeding the SDG output as predicates to Reasoning modules along with additional background knowledge.

2. Related Works
Our work is influenced by various lines of work where
researchers have proposed approaches to extract meaningful information from images and videos. As [21] suggests,
such works can be categorized into 1) dense image annotations, 2) generating textual descriptions, 3) grounding natural language in images and 4) neural networks in visual and
language domains.
According to the above categorization, we share our
roots with the works of generating textual descriptions. This
includes the works that retrieves and ranks sentences from
training sets given an image such as [19], [12],[34], [40].
[10], [24], [25], [47], [48] are some of the works that have
generated descriptions by stitching together annotations or
applying templates on detected image content.
Several works have shown promising efforts to acquire
and apply commonsense in different aspects of Scene Analysis. [52] uses abstraction to discover semantically similar
images. [7] proposes to learn all variations pertaining to all
concepts and [36] uses common-sense to learn actions.
Recently, [20] introduced scene graphs to describe
scenes and [37] creates scene graphs from descriptions.
However, we automatically construct the graph from an image, and we believe, due to the event-entity-attribute based
representation and meaningful edge-labels (borrowed from
KM-ontology[4]) , SDGs are more equipped to facilitate
symbolic-level reasoning.
3 One may note that such structures are also generated by Semantic
parsers such as K-parser(www.kparser.org).

Figure 2: Example Image and a possible corresponding SDG. Note, the SDG should contain a similar event wear2 for person2. We omit it
for space constraints. Note that, it is easy to augment spatial information to the above graph such as (person1,left,person2).

3. State-of-the-art Visual Detections
The recent development of deep neural networks based
approaches revolutionized visual recognition research. Different from the traditional hand-crafted features, a multilayer neural network architecture efficiently captures sophisticated hierarchies describing the raw data [1], which
has shown superior performance on standard scene recognition [51], object recognition [15] and image captioning
[21] benchmarks.
Image Dataset: In this paper, we use three image data
sets, which are popularly referred to as Flickr 8k, Flickr
30k and Coco datasets [19]. These three datasets have
8,092, 31,783 and more than 160K images respectively.
All the images from these datasets are accompanied with
5 hand-annotated sentences that describe the image. For all
datasets, we used the train-test splits from [21] and the 4000
testing images (1000 each from Flickr 8k and 30k; 2000
from MS-COCO validation set; denoted as I) serve as the
testing bed for our reasoning experiments.
Deep Object Recognition: We use the trained
bottom-up region proposals and convolutional neural networks(CNN) object detection method from [15]. It considers 200 common everyday object classes (denoted as N )
and trained on ILSVRC 2013 dataset. We apply the method
on the testing images(I) and then convert the object detection scores to Pr (n|I).
Deep Scene Recognition: We use the trained CNN
scene classification method from [51]. The classification
model is trained on 205 scene categories (denoted as S) and
each of the category has more than 5000 training samples.
We apply the method on the testing images and then convert
the scene classification scores to Pr (s|I).
Constituent Annotation Collection and Deep Constituent Recognition: Images from the wild cannot always be categorized into a limited number of Scene categories. However, scene constituents describing properties
or actions of objects, attributes of scenes occur frequently
across images and can be utilized to describe the image. In
this work, we further augment the Flickr 8K image dataset
with human annotation of constituents using Amazon Mechanical Turks. We specifically ask the human labeler to annotate not only objects, but what objects are doing or properties of objects.
We allow the labelers to use free-form text for describ-

ing constituents to reduce annotation effort. To obtain a
standardized set of constituents from the annotations, we
perform stop-words removal, parts-of-speech processing to
retain nouns, adjectives and verbs. We replace the nouns
with their superclasses such as man, boy, father by person,
and then, we rank the resulting phrases according to their
frequencies. Some of the top phrases are grass, dog run,
dog play, kid play, person wear short4 etc.
For the rest of our processing, we post-process the annotations for each training image and consider them if they
are among the 1000 top constituents (denoted as C). Recent
empirical results from a diverse range of visual recognition
tasks indicate that the generic descriptors extracted from the
CNN are very powerful [9, 35]. In this work, we use a
pre-trained CNN from [23]. For each image in I, we use
this pre-trained model to extract a 4096 dimensional feature
vector using [9, 23]. We then trained a multi-label SVM
to do constituents recognition using the deep features. The
trained model is applied on all the testing images and we
convert the classification scores to Pr (c|I).
The set of Pr (n|I), Pr (s|I), Pr (c|I) makes up the initial
visual perception output.

4. Constructing SDGs from Noisy Visual Detections
Next, we explain the reasoning framework to construct
SDGs from noisy labels with the aid of knowledge from
text. To provide a better understanding of this complex system, we provide a diagram of the architecture explaining the
reasoning process for an example image in Figure 3.
As shown, for each image, the above perception system produces object, scene and constituent detection tuples.
Each detection is provided with a confidence score. For objects, scores are provided for each bounding box. Top five
scene labels and top ten constituent detections are considered for the reasoning framework. Most of these detections
are quite noisy. We develop an elaborate reasoning framework to construct SDGs from such noisy detections, with
the help of pre-processed background knowledge.
4 All the phrases with their corresponding frequencies will be made publicly available in the final version.

dataset-independent and only needs to be augmented when
the set of object classifiers expands.
Scenes-to-AVCs Mapping Table (SM ): For each scene
in S, we added ontological information involving a set of
abstract concepts and a set of synonyms. To obtain the synonyms, we again used WordNet API. We hand-annotated
all the AVCs for each scene and learnt a prior belief for
each AVC in scene from human annotations. For example,
for the scene airport terminal, we add {waiting room, big
glass view, people} as the list of AVCs and terminal as the
synonym; and learn the priors 0.7, 0.6 and 0.9 respectively
for AVCs.
In the following sub-sections, we first introduce the Reasoning Framework briefly, followed by a description of the
construction of the Knowledge Base Kb and the Bayesian
Network Bn . Lastly, we describe our reasoning framework
in detail.

4.2. Reasoning Framework

Figure 3: SDG and Sentence Generation through Reasoning using
Knowledge Base and a Bayesian Network Bn

4.1. Pre-processing Phase Data Accumulation
In this phase, we collect Ontological information about
object classes in Object Meta-data table (OT ) and Scene
classes in Scene Metadata (SM ). We also store scene detection tuples (ST ) and human annotation of images (Ad )
for all training images. We create a Knowledge Base Kb ,
a Bayesian Network Bn and a Scenes to Abstract Visual
Concepts5 (AVC) Mapping Table (SM ).
Scene Detection tuples (ST ): We use the perception
system of the previous section to create scene detection tuples ({(si , Pr (si |Itr ))|i ∈ {1, .., 5}}) of set of training images (Itr ). These are used to learn the Bayes Net Bn .
Image Annotations (Ad ): We collect all textual descriptions of the training images provided with Image Datasets
and use them for building the knowledge base Kb and Bayes
Net Bn . However, both can be built using any repository
of sentences that describe day-to-day concepts.
Object Meta-data (OT ): For each of the 200 object
classes, we collected all synonyms, hyponyms and hypernyms. The list is prepared using Wordnet API. This is
5 Abstract Visual Concepts are higher-level scene constituents and they
describe commonly occurring visual concepts that can be observed across
images. In essence, they are can be compared to phrasal verbs. Like textual
phrases, they do not necessarily follow compositionality of its constituent
words. In case of AVC, this happens in the context of images. For example
waiting room suggests room, seating area, chairs, people waiting etc.
In comparison, Scene Constituents can be compared with phrases that
retain their compositionality such as people play, person wear shorts etc.
In context of an image, person wear shorts can be grounded as the objects
person and shorts; and the action wear.

Equipped with the background knowledge stored in the
form of (Kb , Bn , SM , OT ), we process the objects, scene
and constituent detections for an image to construct an appropriate SDG in the following way: i) we populate synonyms, hypernyms, hyponyms of objects and synonyms,
AVCs (with priors) of scenes; ii) (Scene Constituents:) we
extract entities and events from each constituent. Such as,
the constituent person wear short results in an event wear
with two edges: one labeled agent joining the entity person and another labeled recipient joining the entity short;
iii) (Abstract Visual Concepts:) we choose the AVCs iteratively that maximizes the conditional probability given
high confidence objects; iv) (Objects:) for low-scoring objects, we choose the sibling (in the hyponym-hypernym hierarchy) which maximizes the conditional probability given
high-confidence objects and AVCs; v) (Events:) we search
the Kb to find the most compatible events that connect pairs
of high-confidence objects. We add the events obtained
from Constituents to this set of compatible events; vi)
(Concepts 6 :) given the above events and AVCs, we search
the Kb for Concepts that best suits the events and AVCs, and
we also construct an SDG based on just high-confidence objects, events and AVCs.

4.3. Knowledge Base (Kb )
In Figure 4, we describe how we construct Kb from a set
of Image Annotations (Ad ) using the Stanford Parser and
K-Parser [39]. For each sentence, we first parse using the
6 The idea behind the representation of a Concept is inspired by that of a
Process in AURA [2]. The structure Process is a graph that represents a Biological Process in AURA-KB. and it symbolizes a higher-level event that
encapsulates smaller events, and the entities that participate in such events.
Similarly, a Concept represents a natural activity where a few events occur
and participating entities interact through the events. Our entire approach,
in essence, is about sequentially determining the participants (entities and
events) of a Concept and lastly, the Concept itself.

build the Kb . A visualization of a part of the Kb is given
in Figure 4(b). After parsing the annotated sentences in
Flickr8k, Kb consists of 1102 events, 2500 entities and 1869
traits. The total number of edges and distinct concepts in the
graph are 25271 and 14325.

4.4. Conditional Probability Estimation

(a)

(b)

Figure 4: (a) Constructing Knowledge Base From Annotations.
(b) A snapshot of the Kb . In this figure, Person and bench are
entities, lay is the connecting event. The entity Person can have
trait climber. The sub-graph essentially captures the knowledge of
the activity person laying on a bench. The figure on the left shows
the edge-labels.

Stanford Parser to get a dependency graph. The K-parser
then maps these dependency labels using a set of rules to a
set of meaningful labels from KM-Ontology[4] and the resulting graph is further augmented using ontological and semantic information from different sources (more details on
kparser.org). We then generalize each of these graphs
i.e. replace entities by their superclasses. Then we merge
them based on overlapping entities and events, and create a
single graph (Kb ).
Kb is defined as the tuple (G, C). G = (V, E) denoting
set of vertices V , set of edges E. Each vertex and edge has
a label. Each vertex can be of three types: events, entities
and traits. Events correspond to verbs, entities correspond
to superclasses of nouns that directly interact with events
and traits represent all other nouns. Edge labels in the Kb
are exactly the same as in the K-parser (Figure 4). C is a
set of concepts which corresponds to generalized K-parser
graphs of sentences and is essentially a sub-graph of G.
From Flickr8k annotations, we process nearly 16000
sentences provided for the images (2 per each image7 ) to
7 We do not consider all 5 sentences for an image, as most of the sentences are conceptually same.

In this sub-section, we describe the type of conditional
probabilities we estimate and the Bayesian Network we
learn to estimate such probabilities. We use conditional
probability calculations in two of the steps of our approach:
inferring the most probable collection of Abstract Visual
Concepts and rectifying low-scoring erroneous objects.
For inferring the most probable collection of AVCs, we
first make a list (Cf req ) of all the frequent AVCs (with frequency > 2 in our experiments) from all scenes detected
for a test image. Then we follow Algorithm 1 to get the
set of inferred concepts Cinf from the set of high-scoring
(score > αh 8 ) entities Oimg and the set of scenes Simg detected for image img ∈ I. We iterate till the entropy keeps
decreasing.
Algorithm 1 Infer Abstract-Visual-Concepts
1: procedure INFER M OST P ROBABLE AVC S(Simg , Oimg , Cf req )
2:
prevH ← 1
3:
while Cf req =
6 φ do
4:
Smax ← argmaxs∈Cf req P (s|Cinf , Oimg )
P
5:
H ←
s∈Cf req {−P (s|Cinf , Oimg )∗logP (s|Cinf , Oimg )}
6:
if H > prevH then break;
7:
prevH ← H
8:
Cinf ← Cinf ∪ {Smax }; Cf req ← Cf req \ {Smax }

Next, we attempt to rectify the low-scoring entities based
on high-scoring entities (Oimg ) and the above Cinf . For
each low-scoring entity, we get all its siblings i.e. we get all
the children of its hypernyms. For example, if bathing cap
is assigned a low score, the assigned superclass is headwear
and its children are headband, hat etc. We calculate the
following omax = argmaxo∈siblings P (o|Cinf , Oimg ) and
then add omax to the high-scoring entities list (Oimg ).
As the above paragraphs suggest, we need to estimate the conditional probabilities: P (s|Cinf , Oimg ) and
P (o|Cinf , Oimg ). To estimate the conditional probabilities,
we learn a Bayesian Network Bn using Ad and ST .
4.4.1 Learning the Bayesian Network Bn
To capture the knowledge of naturally co-occurring entities
and Abstract Visual Concepts, we learn a Bayesian Network
that represents the dependencies among them. We create
the training data D which is a set of tuples T = (ti )i , i ∈
1, .., N where N is the total number of entities and AVCs.
Each term ti is binary and denotes 1 if the ith entity (or
AVC) occurs in the tuple. Then, we use the Tabu Search
(tabu) algorithm to learn the structure and then we populate the Conditional Probability Tables using the R-bnlearn
8 The hyper-parameter (α ) are set based on performance on validation
h
data.

package [38]. A subgraph of the learnt Bayesian Network
is shown in the Figure 5.

Figure 5: A subgraph reflecting the dependencies captured in the
Learnt Bayesian Network Bn

To create the training data D, we process each training image (in Itr ), and we automatically detect entities and
AVCs and then output the tuple T . To detect entities, we
parse the image annotations (Ad ) and extract entities from
it. Some of the AVCs such as people and people wear
shorts are detected using rule-based techniques. However,
for scenes such as airport-terminal, it is unlikely that AVCs
such as waiting room can be found in human descriptions
of an image; as we tend to describe only the entities and
their interactions. Keeping this idea in mind, we ran the
scene classifier system from the previous Section 3, and we
consider all the AVCs of the scene with the highest score
(P r(s|Itr )), from the Scene-to-AVC lookup table (SM ).

4.5. Ranking and Inferring Final Concepts
Given the most relevant set of Abstract Visual Concepts
(Cinf ) and entities (Oimg ), we find Concepts that the image
describes. To do this, we use the Kb to search first for events
that these entities (i.e. objects) participate in and then we
use these events and entities together to search for Concepts
in the set of concepts C in Kb .
We rely on two assumptions about the Knowledge Base:
I) Kb reflects a more-or-less complete view of the relevant
world knowledge and hence we can find the most suitable
events from it. This assumption is valid if the images come
from the same domain; such as in our examples, we have
used the Flickr8k dataset and the domain corresponds to
pictures of humans and dogs in natural setting; and II) Kb
contains all concepts possible with the given events and entities. This is a strict assumption, which might not be true
even if we parse the whole Web. To alleviate the problem,
we give two final outputs: i) an SDG involving the entities,
AVCs and events and ii) another SDG of the top Concept
that is obtained from C in Kb .
Search Connecting Events: The motivation behind
building a Knowledge Base was to logically explain why
certain co-occurring events are suitable for the combination
of entities. For example, consider the entities person and
swimming trunks. Note, swimming trunks corresponds to
the vertex trunk in Kb . We get events such as sniff, climb,
wear etc., i.e., some corresponding to tree-trunk and others
to swimming-trunks. To logically find suitable events, we
find all connecting events from G in Kb and then filter spuri-

ous events based on ontological and background knowledge
from OT and C in Kb .
For a pair of entity in Oimg , we traverse the path from
one entity to another in the graph G and consider eventnodes on the path. As shown in Figure 4(b), two entities
can be connected by an event. However, in some cases,
they could be connected by a chain of events and entities.
We employ a greedy breadth-first search over the graph G
for such pairs. We denote the set of entities that are related
to each other by some event, by Oev .
For filtering spurious events, we introduce the notion
of Edge-Compatible Events. An event is edge-compatible
with respect to two entities if they are connected to the
event using edges with compatible labels. As these labels
are well-defined relations between entities and events from
KM-Ontology, the label-compatibility is easy to observe.
For example, (agent,recipient) is a compatible pair and only
an animate entity can be an agent. Based on the rules, the
event wear is edge-compatible with respect to entities person and trunk.
Even after this, we still obtain events like climb etc. To
filter such events, we consult the table OT and the set of
concepts C. We know that the entity swimming trunks belongs to the superclass clothing, and hence we retain only
those events that are connected to an entity trunk which is
of the same superclass, in some concept in C.
SDG Construction: After obtaining a set of suitable
events (such as wear), we construct an SDG using the
following set of rules: i) add has(scene, component, s)
for all AVC s in Cinf ; ii) add has(event, location, scene)
for the top detected events; iii) add all compatible edges
related to the events such as has(wear,agent,person) and
has(wear,recipient,trunk); and iv) for all entities oim in
(Oimg \ Oev ), do the following: if it is an animate entity,
add has(oim , location, scene); Otherwise, find the shortest
path from oI to the top detected event in the Kb and add the
edges on the path to the SDG.
Search Concepts: Given the events and entities (Oev ),
we search the set of Concepts C in Kb . Recall, in the Kb , a
Concept is a generalized K-parser graph of a sentence. We
consider a Concept as candidate if all edges from a detected
Edge-Compatible Event are present in it.
Next, we weight each candidate Concept using the remaining entities in (Oimg \ Oev ) and AVCs; i.e., increase
a counter if an entity or AVC occurs in the graph. We also
calculate a joint confidence-score for each Concept based
on the Pr (n|I), Pr (s|I), Pr (c|I) values of the object, scene
and constituents present in the Concept. Based on the counters and the joint confidence-score, we rank the Concepts.
Template Based Sentence Generation: We generate textual descriptions from the SDG using the
SimpleNLG[14] package. For example, for the edges
has(wear,agent,person) and has(wear,recipient,shorts), we

will generate the sentence “a person is wearing shorts”.
Based on the edge-labels (labels from KM-ontology) we
populate the verb, subject, object and adjectives (including
quantitative9 ) of sentences using simple rules. It should be
noted that these K-Parser labels are a direct mapping from
the set of Stanford Dependencies, and theoretically we can
populate all the parts-of-speeches of a sentence from the
SDG. Herein lies the effectiveness of producing an SDG
from an image.

5. Experiments and Results
The Knowledge-Structure representing a scene should
be rich in information-content and should carry enough semantics to describe the image. We adopted three sets of
experiments. First, we detect the accuracy with which our
system can detect events and entities present in the image.
We perform a qualitative evaluation (“relevance” and “thoroughness”) of the textual descriptions generated from SDGs
with the sentences generated by [21] using the Amazon Mechanical Turkers (AMT). And lastly, to evaluate the imagesentence alignment quality, we design an Image Retrieval
task and report our results on Image Search based on generated annotations. To conclude, we provide a few example
images and their SDGs.
For comparison purposes, we use the implementation
from [21] to generate a textual caption S for each testing
image. The method is based on a combination of CNN over
image regions, bidirectional recurrent neural networks over
sentences, and a structured multimodal embedding. We denote the set of captions as SN N .
Training Phase:
Our model can be represented
by the tuple (Kb , Bn , ST , Ad , OT , SM ). Among these,
ST , OT and SM are collected and stored once, and re-used
for all datasets. For our experiments, we re-use the same
Bayesian Network Bn learnt from Flickr8k data for all the
datasets. Though, we build the Kb each time from the annotated sentences, this can be easily avoided by using the
same Kb for all the datasets. In essence, for the reasoning
part, we donot require any training at all for new datasets.
Entity and Event Detection Accuracy: For this experiment, we extracted entities and events (gold-standard) from
constituent annotations for the 1000 test images of Flickr8k.
We manually checked them to remove noise. To provide a
baseline, we also extracted entities and events from SN N
automatically using K-parser. Subsequently, we compared
the gold-standard with entity-event set from [21] and the
SDG output from our system for each image. The statistics
of our evaluation is given in Table 1.
AMT Evaluation of Generated Sentences: Since sentence generation to describe a scene is innately a creative
process, a good metric is to ask humans to evaluate these
9 For high-scoring detections, we also consider the spatial information
from the bounding-boxes. For N such detections of an object obj, we
generate sentences like N obj’s are in the scene.

Type

Accuracy-SDG(%)

Precision-SDG(%)

Entities
Events

13.6
13.1

21.7
8

Accuracy(%)[21]
16.9
15.3

Precision(%)[21]
34.2
15.2

Table 1: Accuracy and Precision in events and entities prediction

sentences. The evaluation metrics: Relevance and Thoroughness, are therefore proposed as empirical measures of
how much the description conveys the image content (relevance) and how much of the image content is conveyed by
the description (thoroughness)10 . We engaged the services
of AMT to judge the generated descriptions based on a discrete scale ranging from 1–5 (low relevance/thoroughness
to high relevance/thoroughness). The average of the scores
and their deviation are summarized in Table 2 for Flickr8k,
Flickr30k test images and MS-COCO validation images.
For comparison, we asked the AMTs to also judge one random gold-standard description and the output from [21], a
state-of-the-art image captioning system.
In our experiments, we found that Kb from Flickr8k annotations can be used for Flickr30k without much effect
on accuracy. However, for MS-COCO datasets, Kb from
Flickr8k annotations falls short of producing a desired accuracy as the COCO data is much more varied.
Experiment
R ± D(8k)
T ± D(8k)
R ± D(30k)
T ± D(30k)
R±D(COCO)
T±D(COCO)

[21]
2.08 ± 1.35
2.24 ± 1.33
1.93 ± 1.32
2.17 ± 1.34
2.69 ± 1.49
2.55 ± 1.41

Our Method
2.82 ± 1.56
2.62 ± 1.42
2.43 ± 1.42
2.49 ± 1.42
2.14 ± 1.29
2.06 ± 1.24

Gold Standard
4.69 ± 0.78
4.32 ± 0.99
4.78 ± 0.61
4.52 ± 0.93
4.71 ± 0.67
4.37 ± 0.92

Table 2: Sentence generation relevance (R) and thoroughness (T)
human evaluation results with gold standard and [21] on Flickr 8k,
30k and MS-COCO datasets. D: Standard Deviation.

Image-Sentence Alignment Evaluation: Similar to
the experiments in [21, 20], we also evaluate the imagesentence alignment quality using ranking experiments. We
withhold the set of testing images and use the generated sentences as queries.
We process the textual query and construct Gquery =
(Vq , Eq ) using the same procedure by which we construct Kb . For each image, we take the SDG Gimg =
(Vimg , Eimg ) and calculate similarity between the SDG and
the query using the following formula:
P
Sim(Gquery , Gimg ) =

vq ∈Vq

maxvimg ∈Vimg (sim(vq , vimg ))
|Vq |

sim(vq , vimg ) = (wnsim(label(vq ), label(vimg ))+
Jaccard(neighbors(vq ), neighbors(vimg )))/2.

Similarity between two vertices are calculated based on
their word-meaning similarity and neighbor similarity. Here
10 For complete instructions provided to the turkers, please check out
Appendix.

(a)

(b)

(g)

(c)

(h)

(d)

(e)

(i)

(f)

(j)

Figure 6: The SDGs in (d), (e) and (f) corresponds to images (a), (b) and (c) respectively. For more detailed examples, please check
Appendix and http://bit.ly/1NJycKO.

wnsim(., .) is WordNet-Lin Similarity [29] between two
words and Jaccard(., .) is the standard Jaccard coefficient
similarity. Based on the above similarity measure, we give
the image retrieval results compared with few of the stateof-the-art results in Table 3.
Model
[21] BRNN
Our Method-SDG

R@1
11.8
18.1

[21] BRNN
Our Method-SDG

15.2
26.5

[21] BRNN (1k)
Our Method-SDG (1k)
Our Method-SDG (2k)

20.9
19.3
15.4

Flickr8k
R@5 R@10
32.1
44.7
39.0
50.0
Flickr30k
37.7
50.5
48.7
59.4
MS-COCO
52.8
69.2
35.5
49.0
32.5
42.2

the sentences and images can seamlessly converge to such
space of graphical representations. This could have huge
repercussions in search in Image and Textual space and storing knowledge from images and text together in a unified
Knowledge Base.

6. Conclusion
Med r
12.4
10.5
9.2
6.0
4.0
11.0
17.0

Table 3: Image-Search Results: We report the recall@K (for K =
1, 5 and 10) and Med r (Median Rank) metric for Flickr8k, 30k
and COCO datasets. For COCO, we experimented on first 1000
(1k) and random 2000 (2k) validation images.

One of the primary contributions of our work is the
Knowledge-Structure representation that bridges the gap
between semantic information in text and images. From
the results of this experiment, the benefit of having such an
intermediate representation is easy to observe.
Example Images and SDGs: As examples, we pick a
few images which produces objects and scene recognitions
with comparably good confidence scores. The images and
their corresponding SDGs are provided in Figure 6. As we
can observe, the information produced by these SDGs are
easily processed by machines. We can answer questions
such as how entities interact in an event, which possible
events are in the scene and how entities interact in a scene.
We should also mention that the concept-level modeling
provided by SDGs is what separates this work from other recent approaches [20]. Furthermore, comparing these structures with the K-Parser output in Figure 4, we can see how

This paper introduced a reasoning module to generate
textual descriptions from images by first constructing a new
intermediate semantic representation, namely the Scene Description Graph (SDG), which is later used to generate
sentences. The reasoning module uses an automatically
constructed Knowledge Base created from text, to capture
“commonsense” knowledge. Having built the Knowledge
Base, we proposed a method of obtaining such SDGs from
noisy labels using our prediction system. The SDG is a representation of the scene in view that integrates direct visual knowledge (objects and their locations in the scene)
with background commonsense knowledge. In addition,
the SDGs have a structure similar to semantic representations of sentences, thus facilitating the interaction between
Vision and Natural Language. The notion of the SDG has
great potential. Here we used the SDG for the automatic
creation of sentences describing the scene; but, equipped
with background knowledge, it also allows reasoning and
question/answering about the scene 11 .
To demonstrate the effectiveness of the sentences and
constructed SDGs, we performed a number of experiments.
Our AMT evaluations on popular datasets show that our
sentences performs comparatively well with respect to the
state-of-the-art in measures of relevance and thoroughness.
A Gold-Standard based evaluation shows that our output
SDGs can detect events and entities with comparable accuracy as a state-of-the-art system. And lastly, our Image
Retrieval experiment shows that the Image-Sentence alignment quality is comparable with state-of-the-art results.
11 Please

see appendix for an example.

References
[1] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions on,
35(8):1798–1828, 2013. 3
[2] V. K. Chaudhri, B. E. John, S. Mishra, J. Pacheco, B. Porter,
and A. Spaulding. Enabling experts to build knowledge bases
from science textbooks. In Proceedings of the 4th International Conference on Knowledge Capture, K-CAP ’07, pages
159–166, New York, NY, USA, 2007. ACM. 2, 4
[3] X. Chen and C. L. Zitnick. Learning a recurrent visual representation for image caption generation. arXiv preprint
arXiv:1411.5654, 2014. 1
[4] P. Clark, B. Porter, and B. P. Works. Km-the knowledge machine 2.0: Users manual. Department of Computer Science,
University of Texas at Austin, 2004. 2, 5
[5] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference
on, volume 1, pages 886–893. IEEE, 2005. 1
[6] E. Davis and G. Marcus. Commonsense reasoning and commonsense knowledge in artificial intelligence. Commun.
ACM, 58(9):92–103, Aug. 2015. 1
[7] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-supervised visual concept
learning. In 2014 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2014, Columbus, OH, USA, June
23-28, 2014, pages 3270–3277, 2014. 2
[8] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014. 1
[9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings
of the 31st International Conference on Machine Learning
(ICML-14), pages 647–655, 2014. 3
[10] D. Elliott and F. Keller. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1292–1302, 2013. 2
[11] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing
objects by their attributes. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE, 2009. 1
[12] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg,
2010. Springer-Verlag. 2
[13] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model.
In Computer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on, pages 1–8. IEEE, 2008. 1

[14] A. Gatt and E. Reiter. Simplenlg: A realisation engine
for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG
’09, pages 90–93, Stroudsburg, PA, USA, 2009. Association
for Computational Linguistics. 6
[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Computer Vision and Pattern Recognition,
2014. 3
[16] D. Gunning, V. K. Chaudhri, P. Clark, K. Barker, S. yi Chaw,
B. Grosof, A. Leung, D. Mcdonald, S. Mishra, J. Pacheco,
B. Porter, A. Spaulding, D. Tecuci, and J. Tien. Project halo
updateprogress toward digital aristotle. 2
[17] A. Gupta and L. S. Davis. Objects in action: An approach
for combining action understanding and object perception. In
Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pages 1–8. IEEE, 2007. 1
[18] C. Havasi, R. Speer, and J. Alonso. Conceptnet 3: a flexible,
multilingual semantic network for common sense knowledge. In Recent advances in natural language processing,
pages 27–29. Citeseer, 2007. 1
[19] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
description as a ranking task: Data, models and evaluation
metrics. Journal of Artificial Intelligence Research, pages
853–899, 2013. 2, 3
[20] J. Johnson, R. Krishna, M. Stark, J. Li, M. Bernstein, and
L. Fei-Fei. Image retrieval using scene graphs. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2015. 2, 7, 8
[21] A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image descriptions. arXiv preprint
arXiv:1412.2306, 2014. 1, 2, 3, 7, 8
[22] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying
visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014. 1
[23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS
2012, 2013. 1, 3
[24] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg,
and T. L. Berg. Baby talk: Understanding and generating
image descriptions. In Proceedings of the 24th CVPR, 2011.
2
[25] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. Collective generation of natural image descriptions.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume
1, ACL ’12, pages 359–368, Stroudsburg, PA, USA, 2012.
Association for Computational Linguistics. 2
[26] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 951–958. IEEE,
2009. 1
[27] I. Laptev. On space-time interest points. International Journal of Computer Vision, 64(2-3):107–123, 2005. 1
[28] D. B. Lenat. Cyc: A large-scale investment in knowledge
infrastructure. Commun. ACM, 38(11):33–38, Nov. 1995. 1

[29] D. Lin. An information-theoretic definition of similarity. In
ICML, volume 98, pages 296–304, 1998. 8
[30] D. G. Lowe. Object recognition from local scale-invariant
features. In Computer vision, 1999. The proceedings of the
seventh IEEE international conference on, volume 2, pages
1150–1157. Ieee, 1999. 1
[31] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain
images with multimodal recurrent neural networks. arXiv
preprint arXiv:1410.1090, 2014. 1
[32] R. Messing, C. Pal, and H. Kautz. Activity recognition using the velocity histories of tracked keypoints. In Computer
Vision, 2009 IEEE 12th International Conference on, pages
104–111. IEEE, 2009. 1
[33] A. S. Ogale, A. Karapurkar, and Y. Aloimonos. Viewinvariant modeling and recognition of human actions using
grammars. In R. Vidal, A. Heyden, and Y. Ma, editors, WDV,
volume 4358 of Lecture Notes in Computer Science, pages
115–126. Springer, 2006. 1
[34] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In
J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. C. N. Pereira,
and K. Q. Weinberger, editors, NIPS, pages 1143–1151,
2011. 2
[35] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson.
Cnn features off-the-shelf: an astounding baseline for recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512–519.
IEEE, 2014. 3
[36] M. Santofimia, J. Martinez-del Rincon, and J.-C. Nebel.
Common-Sense Knowledge for a Computer Vision System
for Human Action Recognition. In J. Bravo, R. Hervás, and
M. Rodrı́guez, editors, Ambient Assisted Living and Home
Care, volume 7657 of Lecture Notes in Computer Science,
pages 159–166. Springer Berlin Heidelberg, 2012. 2
[37] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D.
Manning. Generating semantically precise scene graphs
from textual descriptions for improved image retrieval. In
Proceedings of the Fourth Workshop on Vision and Language, pages 70–80, Lisbon, Portugal, September 2015. Association for Computational Linguistics. 2
[38] M. Scutari. Learning bayesian networks with the bnlearn R
package. Journal of Statistical Software, 35(3):1–22, 2010.
6
[39] A. Sharma, N. H. Vo, S. Aditya, and C. Baral. Towards addressing the winograd schema challenge - building and using a semantic parser and a knowledge hunting module. In
Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1319–1325, 2015. 4
[40] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y.
Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2:207–218, 2014. 2
[41] D. G. Stork. HAL’s Legacy: 2001’s Computer as Dream and
Reality. MIT Press, 1998. 1
[42] C. L. Teo, C. Fermüller, and Y. Aloimonos. A gestaltist
approach to contour-based object recognition: Combining
bottom-up and top-down cues. The International Journal of
Robotics Research, page 0278364914558493, 2015. 1

[43] C. L. Teo, A. Myers, C. Fermuller, and Y. Aloimonos. Embedding high-level information into low level vision: Efficient object search in clutter. In Robotics and Automation
(ICRA), 2013 IEEE International Conference on, pages 126–
132. IEEE, 2013. 1
[44] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show
and tell: A neural image caption generator. arXiv preprint
arXiv:1411.4555, 2014. 1
[45] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu. Action recognition by dense trajectories. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages
3169–3176. IEEE, 2011. 1
[46] Y. Yang, C. Fermüller, Y. Aloimonos, and A. Guha. A cognitive system for understanding human manipulation actions.
Advances in Cognitive Systems, 3:67–86, 2014. 1
[47] Y. Yang, C. L. Teo, H. Daumé, III, and Y. Aloimonos.
Corpus-guided sentence generation of natural images. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 444–454,
Stroudsburg, PA, USA, 2011. Association for Computational
Linguistics. 2
[48] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S. C. Zhu. I2t:
Image parsing to text description. Proceedings of the IEEE,
98(8):1485–1508, 2010. 2
[49] X. Yu and Y. Aloimonos. Attribute-based transfer learning for object categorization with zero/one training example.
In Computer Vision–ECCV 2010, pages 127–140. Springer
Berlin Heidelberg, 2010. 1
[50] X. Yu, C. Fermuller, C. L. Teo, Y. Yang, and Y. Aloimonos.
Active scene recognition with vision and language. In Computer Vision (ICCV), 2011 IEEE International Conference
on, pages 810–817. IEEE, 2011. 1
[51] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.
Learning deep features for scene recognition using places
database. NIPS, 2014. 3
[52] C. L. Zitnick and D. Parikh. Bringing semantics into focus
using visual abstraction. In CVPR, pages 3009–3016. IEEE,
2013. 2

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❆PP❊◆❉■❳ ♦❢ ✏❋r♦♠ ■♠❛❣❡s t♦ ❙❡♥t❡♥❝❡s t❤r♦✉❣❤ ❙❝❡♥❡
❉❡s❝r✐♣t✐♦♥ ●r❛♣❤s ✉s✐♥❣ ❈♦♠♠♦♥s❡♥s❡ ❘❡❛s♦♥✐♥❣ ❛♥❞
❑♥♦✇❧❡❞❣❡✑
❈♦♥t❡♥ts
✶

◗✉❡st✐♦♥✲❆♥s✇❡r✐♥❣ ❜❛s❡❞ ♦♥ ❙❉●s

✶

✷

■♠❛❣❡s✱ ❈♦rr❡s♣♦♥❞✐♥❣ ❙❉●s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

✸

✸

❙♦♠❡ ▼♦r❡ ■♠❛❣❡s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

✺

✹

❆▼❚ ■♥str✉❝t✐♦♥s

✽

✶

◗✉❡st✐♦♥✲❆♥s✇❡r✐♥❣ ❜❛s❡❞ ♦♥ ❙❉●s

❋✐❣✉r❡ ✶✿ ❆♥ ❊①❛♠♣❧❡ ■♠❛❣❡ ❢r♦♠ ❋❧✐❝❦r ✽❦✿ ❚❤❡ ❣r❡❡♥ ❜♦① s❤♦✇s ❛ ♣❡rs♦♥ ✐s ❞❡t❡❝t❡❞ ✇✐t❤ s❝♦r❡

1.36056✳

■♥ t❤✐s ✐♠❛❣❡✱ t❤✐s ✐s t❤❡ ♦♥❧② ♦♥❡ ✐♥st❛♥❝❡ ♦❢ ❝❧❛ss ✏♣❡rs♦♥✑ ❞❡t❡❝t❡❞ ✇✐t❤ ❤✐❣❤ ❝♦♥✜❞❡♥❝❡✳

❋♦r t❤❡ ✐♠❛❣❡ ✐♥ ❋✐❣✉r❡ ✶✱ t❤❡ ❣❡♥❡r❛t❡❞ ❢❛❝ts ✐♥ ❙❉● ❛r❡✿

has(scene,component,water).
has(scene,component,water_droplets).
has(scene,component,exterior_of_building).
has(person1,semantic_role,drinker).
has(water,semantic_role,liquid).
has(person1,semantic_role,creator).
has(drink,recipient,water).
has(drink,agent,person1).
has(drink,origin,fountain).
has(drink,next_event,make).
✶

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❖♥❡ ♦❢ t❤❡ ❛❞✈❛♥t❛❣❡ ♦❢ ❙❉● ✐s✱ ✇❡ ❝❛♥ ❞✐r❡❝t❧② ✉s❡ t❤❡ ❛❜♦✈❡ tr✐♣❧❡s ❛s ❢❛❝ts ❛♥❞ ❛s ❛♥ ✐♥♣✉t t♦ ❛♥
❆♥s✇❡r ❙❡t Pr♦❣r❛♠✳ ◆♦✇✱ ✐❢ ✇❡ ♣♦s❡ t❤❡ q✉❡st✐♦♥ t❤❛t ✏■s s♦♠❡♦♥❡ ❞r✐♥❦✐♥❣ ❢r♦♠ t❤❡ ❢♦✉♥t❛✐♥❄✑
✐♥ ❆❙P✱ ✇❡ ❝❛♥ ❛♥s✇❡r ❜❛s❡❞ ♦♥ t❤❡ ❛❜♦✈❡ ❢❛❝ts ✉s✐♥❣ t❤❡ ❢♦❧❧♦✇✐♥❣ ♣r♦❣r❛♠ ✐♥ ❈❧✐♥❣♦✲✸✿

entity(person;dog;water;shorts;frisbee).
animate(person;dog).
inanimate(A) :- not animate(A), entity(A).
drink_yes :- animate(A), has(drink,agent,A), has(drink,recipient,water).
yes_fountain(A) :- drink_yes, has(drink,agent,A), has(drink,origin, fountain).
#hide.
#show yes_fountain/1.
❲❡ ❡①❡❝✉t❡ t❤❡ ❛❜♦✈❡ ♣r♦❣r❛♠ ✐♥ ❈❧✐♥❣♦✲✸✱ ✇❡ ❣❡t t❤❡ ❛♥s✇❡r ❛s

✷

②❡s❴❢♦✉♥t❛✐♥✭♣❡rs♦♥✶✮✳

❆♣♣❡♥❞✐①

✷

◆♦✈❡♠❜❡r ✷✵✶✺

■♠❛❣❡s✱ ❈♦rr❡s♣♦♥❞✐♥❣ ❙❉●s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

❋✐❣✉r❡ ✷✿ ❈❛s❡ ❙t✉❞②✲■

❋✐❣✉r❡ ✸✿ ❈❛s❡ ❙t✉❞②✲■■

✸

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✹✿ ❈❛s❡ ❙t✉❞②✲■■■

❋✐❣✉r❡ ✺✿ ❈❛s❡ ❙t✉❞②✲■❱

✹

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✻✿ ❈❛s❡ ❙t✉❞②✲❱

✸ ❙♦♠❡ ▼♦r❡ ■♠❛❣❡s ❛♥❞ ●❡♥❡r❛t❡❞ ❙❡♥t❡♥❝❡s

❋✐❣✉r❡ ✼✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✺

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✽✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

❋✐❣✉r❡ ✾✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✻

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

❋✐❣✉r❡ ✶✵✿ ❙❡♥t❡♥❝❡ ❊①❛♠♣❧❡s

✼

❆♣♣❡♥❞✐①

◆♦✈❡♠❜❡r ✷✵✶✺

✹ ❆▼❚ ■♥str✉❝t✐♦♥s

❋✐❣✉r❡ ✶✶✿ ❘❡❧❡✈❛♥❝❡ ❊✈❛❧✉❛t✐♦♥ ❚❛s❦ ■♥str✉❝t✐♦♥s ❛s ♣r♦✈✐❞❡❞ t♦ t❤❡ ❚✉r❦❡rs

❋✐❣✉r❡ ✶✷✿ ❚❤♦r♦✉❣❤♥❡ss ❊✈❛❧✉❛t✐♦♥ ❚❛s❦ ■♥str✉❝t✐♦♥s ❛s ♣r♦✈✐❞❡❞ t♦ t❤❡ ❚✉r❦❡rs

✽

2013 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)
November 3-7, 2013. Tokyo, Japan

Minimalist Plans for Interpreting Manipulation Actions
Anupam Guha1 , Yezhou Yang1 , Cornelia Fermüller2 , and Yiannis Aloimonos1

Abstract— Humans attribute meaning to actions, and can
recognize, imitate, predict, compose from parts, and analyse
complex actions performed by other humans. We have built a
model of action representation and understanding which takes
as input perceptual data of humans performing manipulatory
actions and finds a semantic interpretation of it. It achieves
this by representing actions as minimal plans based on a
few primitives. The motivation for our approach is to have
a description, that abstracts away the variations in the way
humans perform actions. The model can be used to represent
complex activities on the basis of simple actions. The primitives
of these minimal plans are embodied in the physicality of the
system doing the analysis. The model understands an action
under observation by recognising which plan is occurring.
Using primitives thus rooted in its own physical structure, the
model has a semanticist and causal understanding of what
it observes. Using plans, the model considers actions as well
as complex activities in terms of causality, compositions, and
goal achievement, enabling it to perform complex tasks like
prediction of primitives, separation of interleaved actions and
filtering of perceptual input. We use our model over an action
dataset involving humans using hand tools on objects in a
constrained universe to understand an activity it has not seen
before in terms of actions whose plans it knows of. The model
thus illustrates a novel approach of understanding human
actions by a robot.

I. INTRODUCTION
Humans have an ability to understand meaning in the
actions of other humans by observing them. Not only can
humans learn complex actions, they can do more than mere
imitation of previously observed actions because humans,
unlike most AI systems operate in their semantic space.
They have the ability to reason and predict future actions
of others in real time. To achieve a goal, a human can
construct complex plans of actions by having observed only
parts of them, having never seen the complete sequence of
the right action in the first place. Actions are interesting
because infants are sensorimotor agents, and develop abstract
concepts later. Certain linguists are of the opinion [1] that
even abstract non physical concepts are rooted in physical
primitives.
Humans have a robust mechanism to filter and simplify
observed behaviour of other humans, as well as a mechanism
to compose complex plans out of those observations. We
would like to endow robots with a capability of this kind
so that they can understand human behaviour and coexist
with humans. There are two basic questions one needs to
address in order to achieve this goal. The filtering of sensory
1 A. Guha, Y. Yang, and Y. Aloimonos, are from the Department of Computer Science, and 2 C. Fermüller is from UMIACS, University of Maryland,
College Park, MD 20742, USA {aguha, yzyang, yiannis} at

cs.umd.edu and fer at umiacs.umd.edu

978-1-4673-6358-7/13/$31.00 ©2013 IEEE

data, is a necessity in understanding the way humans deal
with complex actions. The amount of information a human
perceives is staggering, and to use it effectively humans
can filter out large parts of its perception. How does a
human decide what to filter out and what to focus on while
observing a complex action in real time, simultaneously
understanding it? It has been argued that infants drastically
filter out information [2] in order to build schemas or plans
with perceptual data. Understanding the representation of
actions is fundamental to have any kind of mechanism of action understanding. What kind of information is important in
Action Representation and how is the information structured?
Human activity recognition is an active area of research due
to its many potential applications, and currently a lot of
different approaches are being followed. Most approaches
focus on full body motions and involve observing spatio
temporal positions of entities or humans [3], [4], [5] in
the universe and learning patterns from them. Some involve
trying to model the symbolic representation of sensory data.
There have been suggestions [6] that a minimalist generative
grammar, similar if not the same as the one which exists for
languages, also exists for action understanding, and attempts
to understand the grammar of this action-language with the
grammar primitives as the trajectories [7] or the objects [8]
have been made. However, these grammar models, while
addressing how the filtering might be done, do not provide
much insight into why the filtering is done in that manner.
Also, a Context Free Grammar doesn’t provide a lot of
explanation as to when an action ends since it does not
concern itself much with the causality of the sub-actions.
We suggest a model in which we provide a basis for
a semanticist understanding of actions. Firstly, this model
attempts to represent complex activities composed of plans
of relatively simple action descriptions. Secondly, this model
represents these simple actions humans do with their two
hands in terms of plans making use of just two primitives,
namely ‘MOVE’ and ‘GRASP’. These are what we call
“minimal plans”. While MOVE and GRASP can be resolved
to finer ontologies, this level of abstraction provides a
good balance between robustness and adequate information
and solves several issues raised above. A plan acts as a
description in addition to being a representation. It imparts
an understanding of causality. Actions, by their very nature,
are goal oriented. There is evidence of action planning in the
human brain. Infants have been shown to have the faculty to
identify and recreate actions in a hierarchy [9]. We suggest
that the grammar of human actions is more than a grammar,
and must fundamentally involve planning with a hierarchy of
plans arising out of some kind of minimalist plans. Minimal

5908

plans also explain ‘compositionality’, or the ability to understand larger activities based on smaller learned actions, an
ability which infants have been observed to have [9]. Most
importantly, using minimal plans explains the filtering out of
a lot of perceptual data because the goal of a small plan helps
the human to focus on particular sub-actions, by observing
the changes in the universe due to the visible consequences
of earlier actions. This explains the phenomena of ‘attention’.
This kind of minimal action representation can be used to
segment and parse videos robustly due to the very small
number of primitives involved. Important also, it gives us a
natural way of segmenting the motion sequence in time: a
new action starts whenever an object comes in contact with
a hand (i.e. the object is grasped)
Our model gives a justification of the two primitives used
on a semantic basis which has meaning in terms of the robot.
Thus, we attempt to move away from the conventionally
functionalist approaches towards semantic mechanisms. A
shortcoming most of existing approaches have is that while
attempting to understand “meaning” of complex actions, they
keep a functionalist framework in which the robot doesn’t
have any understanding of meaning in the symbols it uses.
Thus our approach attempts to provide a robot with a very
basic form of “intentionality”
II. RELATED WORK
The area of human activity recognition and understanding is one attracting a lot of interest. Generally it can
be divided into two types, the visual recognition methods,
which comprise of recognition techniques, and the non-visual
description methods, which are traditionally functionalist
approaches forming representations of actions. A few good
surveys of the former can be found in [10], [11], and [12].
There is a lot of motivation for these kind of models as
the possibilities of application are immense, especially in
areas like HCI, recognition and retrieval, biometrics, in the
video domain, and various kinds of image segmentations
and classifications. The general method used by most of the
visual recognition methods is to learn from a lot of spatiotemporal points what an action looks like and a few works of
interest can be found in [3], [4], [5], and [13].Till now most
of the focus is in recognizing singular human actions like
walking, jumping, running or gait etc. like the work done
in [14] and [15]. More complex actions require the usage
of HMMs [16] or other parametric approaches for learning
the representation as done in works like [17], [18], and [19].
This is the primary thrust of research in computer vision for
such recognition techniques.
However, to properly reason about complex actions, semantics are required, and if independent semantics cannot be
achieved, at least meaningful syntax is required . Some methods use the concept of regenerative minimalist grammars
in analogy with languages. In these methods to relate the
objects to actions, HMMs are used in [20] and [21]. Closely
related to our work is the work in [22] where an attempt is
made to understand complex actions compositionally using
a minimalist grammar tree using object detection. This work

on action trees based on objects is further developed in [23].
They address the question of actions being compositionally
interleaved as well as make an attempt to have action
prediction. They use an action grammar structure which is
powerful enough to analyse complex action and also resolve
interleaved actions. However, the grammar oriented method
of most of these works do not address the theme of any
independent semantic understanding of actions. That is their
systems do not have any actual understanding of the complex
actions in terms of what is meaningful to the robot instead
of the programmer, or a non-arbitrary theory of where the
bootstrapping should be. While our approach to what we
think is rudimentary semantic capability may not be a correct
analogue of any biological agent, we think that a semanticist
justification is necessary for an action understanding model
and we attempt to provide one in this paper.
III. DESCRIPTION
In this section we will describe the entire process of
understanding actions using minimalist plans by first describing our methodology in collecting data in sub-section IIIA, then the concept of minimalist plans and the primitives
used in them in sub-section III-B. In subsequent sub-sections
III-C, III-D, and III-E we describe how action recognition
is done by recognizing which plan is occurring on the
basis of primitives and the action alterations. Lastly, in subsection III-F we talk about automated planning and how it
complements this approach.
A. UMD Minimalist action dataset
In order to build our action primitives we require the
robot to observe a series of hand-actions performed by
a human actor using several different kinds of tools. In
addition, we have compiled a new dataset, consisting of
6 actions: A={SLICE, JOIN, MASH, TRANSFER, POUR,
STIR} performed by 2 different human actors using 6
common tools: T ={knife, ladle, pitcher, ladle, mug, bowl}
and 4 other objects1 : O={tomato, cucumber, bread, cheese}.
For the purpose of an accurate tracking of both hands, at
the beginning of the clip, a calibration phase is included. In
total, there are 10 video clips, which serves as a testbed for
the analysis of minimalist action plans.
B. Minimal Plans
To begin with, as mentioned before in the introduction, our
model is based on minimal plans. All plans have primitive
actions, i.e, actions that cannot be subdivided into anything
simpler on some consistent basis, on which the plan gets
based. What is the justification for an action to be primitive
from the terms of the robot even if those are bootstrapped?
We would like that the basis of selecting primitive actions
has to be both consistent and non-arbitrary. To address nonarbitrariness first let us ask a question, how does a human get
rudimentary knowledge of an action and its consequences
on objects? A human does that by imitation [24], that is,
1 For the rest of the paper, unless specified, the term “object” will be used
to indicate both tools and objects. This does gets sorted in the planning stage

5909

it observes as an infant [25] what other humans are doing
to same objects under altered conditions and tries to mirror
their limb movements to what may be an internal motor [26]
. Thus, the most elementary actions seem to be based on the
smallest limb movements an actor can produce. We posit
that herein lies the basis of rooted meaning. To elaborate we
think that ultimately the meaning of an action lies in what it
means to the agent’s body in terms of the smallest and most
fundamental of its physical characteristics. After all the ways
in which an agent can modify the universe with its own limbs
should be a major part of any comprehension of its universe.
Firstly, we only consider actions that can be accomplished
by the two hands of a human. As our data shows, we are
dealing with a finite domain of actions. Secondly, we abstract
all “fine motoric actions” by the fingers due to the limitations
of our perceptual apparatus to robustly view those actions.
If we allow for these two assumptions, we reach a curious
conclusion that to describe all simple actions that can be
done by the two hands of a human, we need only two action
primitives, namely, that of the action MOVE and the action
GRASP where these actions are defined as
•

•

MOVE: Transport the object under observation from
one location to the other. The environment or object(s)
may or may not alter due to MOVE.
GRASP: To use the hand, or a suitable robotic analogue,
to hold the object under consideration. The opposite of
GRASP is UNGRASP. GRASP may or may not alter
the environment or objects

Using these two action primitives in our dataset we evaluate six human activities, namely SLICE, JOIN, MASH,
TRANSFER, POUR, and STIR. Since these plan scripts
have overlapping structures, a condensed unified script is
given in Fig. 1. We are considering GRASP and UNGRASP

13 groups (and thus corresponding ways to UNGRASP) a
human can perform. Similarly, as we will demonstrate later,
there are various kinds of MOVE, and currently we are
able to detect three kinds of them. Having said that, the
basic commonality between all MOVEs and all GRASPs
persuades us to use these two as the primitives. Secondly,
one may envision other primitives (aside from fine motoric
actions which we will not consider), like ‘Reach’ wherein
one reaches towards an object, ‘Engage’ in which an object is
brought to interact with another object etc. and we hold that
all these primitives are essentially cases of MOVE. A plan
requires a consistent definition of the world, and a finite set
of precondition and postconditions in addition to primitive
actions. In our minimalist plans, objects are provided with
several properties, like “Graspability”, “Slicability”, “Being
a Tool”, “Being a Container” etc. Preconditions consist of
a combination of existence of objects, their presence in
specific locations, and their properties. Postconditions consist
of all of these three but can also have what we call “Action
Alterations”, namely perceivable consequences or changes in
the properties of the objects or the environment due to the
primitive actions. Each of the six actions mentioned before
has an expanded script. In all of these minimal plans there
are essentially two scripts occurring simultaneously, one for
each of the hands of the human. These are Partial Order Plans
in which at times the order in which both the hands do these
tasks may be indeterminate. An important point to be stressed
is that to infer which plan the robot is observing, it does
not need to know all the parameters of the plan to compare
it to the scripts it has in its memory. Actually, as we will
demonstrate in the next section, if there are a finite number
of scripts in consideration, then a few parameters of the plan
will be enough to infer which action is being performed.
This is a very useful property because one a script of a plan
is known, it allows the inference of affordances. Thus, once
we know that we are operating in the SLICE plan space,
the object the plan is operating upon is a “Slicable” object
and the object doing the slicing is a “Slicing Tool”. Thus
these minimal plans allow an easy way to engage in semantic
reasoning. In our simplified action universe we are dealing
with only six actions but these actions can be composed into
more complex actions. Since these are plans, all our model
needs to do is maintain correct causality. Aside from these
actions we have worked with, potentially a lot more can be
analysed in terms of just our two primitives. After obtaining
the scripts of the plans it becomes very easy to determine in
an automated manner which script the agent is operating in.
However, to learn the new scripts from just observation of
human actions with no such bootstrapping in a completely
automated manner is beyond the scope of our current work.

Fig. 1. The six actions of our dataset, SLICE, JOIN, MASH, TRANSFER,
POUR, and STIR

C. Plan Detection

as a tuple in the same primitive. Here we clarify a few
points. Firstly, we know of course that both MOVE and
GRASP have large variety in themselves. [27] considers
there are 33 kinds of GRASPs divided hierarchically into

As mentioned before, an action is recognized by detecting
which plan the agent is currently operating in. For example,
the previous Fig. 1 demonstrates that if one hand does only
a GRASP followed by an UNGRASP, then it has to be one
of the five actions which are not JOIN, as JOIN requires

5910

a movement in both hands. However, if there is a GRASP
followed by a MOVE followed by an UNGRASP in both
hands, then it cannot be a SLICE or MASH. If one adds
in the information of the movements in addition to that
of grasping, it is possible to whittle down the possibilities
further. Add in the information of locations of objects
varying with time and the search space decreases even more.
Lastly, if there is still confusion about which action is being
done, we have the powerful tool of Action Alteration which
determines what consequence the action has done on the
environment and/or the object. Knowing all these effectively
recognizes the action. As soon as the action is recognised
the complete plan is confirmed which in turn allows us to
predict affordances, or properties, of the objects involved.
To detect the primitive GRASP, we take recourse in the
inherent property of moving and grasping, namely that before
a GRASP the hand will be in motion while the object will
be stationary, and after the UNGRASP it will be the same.
However, in the former case the hand will move towards the
object whereas in the latter case the hand will move away
from the object. Also, a GRASP and the UNGRASP may
have none, one, or more MOVEs between them. A property
of a hand moving an object is that while in motion both
the hand and the object will have relatively similar velocity
in magnitude and direction. This will suddenly change as
soon as the movement stops and UNGRASP happens. Thus,
by making a graph of the relative distances of hands and
the objects from each other, and another graph with the
relative velocities of the hands and the objects, it is possible
to parse out instances of GRASPs, MOVEs, and UNGRASPs
by looking for changes in the relative velocities. The method
is illustrated in the Fig. 2.

dimensions which can be parsed to see if the linear motion
changes for example to a circular one as shown in Fig. 3 . At
this stage in recognition the system has information about the
action primitives. But to correctly recognize the action it may
need some additional information regarding the consequence
of the action. For that there is our procedure of monitoring
alterations in objects after actions are performed.

Fig. 3.

Using object location to get different MOVEs

D. Hand Tracking and Grasp Type Recognition
We pre-process the dataset using the FORTH hand tracker
available2 [28] which tracks the 3D position, orientation and
full articulation of a human hand from markerless visual
observations with Kinect input. Currently, we are trying
to find if a finer classification of “GRASP” primitive can
be obtained using the full articulation and orientation of
both hands. While we maintain “GRASP” as semantically
atomic, it is useful for future work where we can consider
more elaborate means of recognizing which minimal plan
we are in. We collect 10 different grasp types following
[27] as training data, and extract longitudinal and oblique
arches of each finger as features. We further reduce the
dimensionality by PCA and then apply k-means clustering
to discover the underlying four general types of hand status,
1)REST, 2)FIRM GRASP, 3)DELICATE GRASP (PINCH)
and 4)EXTENSION GRASP. (See Fig. 4). To every trail, a
naive-bayes classification is used to classify grasp types for
each frame.
E. Object Monitoring and Alterations

Fig. 2.

Parsing out the action primitives using relative velocity

However, a GRASP and an UNGRASP may have more
than one sort of MOVE between them. In our minimalist
plans we differentiate moves when they undergo a sharp
change in property, like a relatively rectilinear motion changing into a periodic up and down motion will count as two
MOVEs. We see examples of this happening in four of our
actions. To detect these sudden change in movement properties, we plot a highly smoothed graph of the coordinates
of the centre of observation of the two objects in the three

An object monitoring process is needed to deduce the
action primitives. We use the joint segmentation and tracking
method presented in [29]. This method combines stochastic
tracking [30] with a fixation based active segmentation [31].
The tracking module provides a number of tracked points.
The locations of these points are used to define an area
of interest and a fixation point for the segmentation, and
the color in their immediate surroundings are used in the
data term of the segmentation module. The segmentation
module segments the object, and based on the segmentation,

5911

2 http://cvrlcode.ics.forth.gr/handtracking/

Fig. 4. Four types of grasp and their territories on low dimensional space.

updates the appearance model for the tracker. Fig 5 illustrates
the method over time, which is a dynamically closed-loop
process. Another crucial pre-condition and post-condition of

Fig. 5. Flow chart of the proposed active segmentation and tracking method
for object monitoring.

human actions is action alterations, aka, the consequence
of every action. In the context of our work where we
are concerned with how actions change the universe it is
necessary to ascertain how they alter the objects they operate
on. From their very nature, action alterations can be defined
into six primitive categories, 1)DIVIDE, 2)ASSEMBLE,
3)TRANSFER, 4)DEFORM, 5)CREATE and 6)CONSUME.
For further details please refer to [29].
Thus knowing the positions of the objects and the hands,
parsing the MOVEs and GRASPs from their relative velocities, evaluating what kind of MOVE occurs by the location,
and evaluating the action consequence, the complete plan is
built and the action recognized.
F. Integration with Automated Planning
Representing actions as minimal plans has another very
important advantage, namely that they can be seamlessly
integrated with an automated planner. An automated planner
can generate a sequence of minimal plan representations for
an unknown activity, which can be used to either verify or
predict the observed sequence. To a potential robotic platform, using both minimal and complex plans in a hierarchy
makes it possible for it to incrementally reason about actions.
We converted our primitives in the constrained world to a
suitable PDDL 3 representation, and we investigated planning
3 http://ipc.informatik.uni-freiburg.de/PddlResources

techniques to find a planning algorithm suited to generate
sequences of these representations. It should be mentioned
here that planning is a PSPACE-complete problem even for
simple problems and there is no universal efficient solution
for all methods.
The first algorithm which we investigated is FF [32] planning. Its base system architecture uses Enforced Hill Climbing, a forward search engine, [32] as the search algorithm
which uses a certain heuristic called relaxed GRAPHPLAN
on every state. Enforced Hill Climbing uses a forward searching technique to search through the state space. At every
state relaxed GRAPHPLAN estimates the distance from the
goal heuristically, and also computes the promising successor
states. In addition to this, FF has techniques to avoid wasting
time getting to goals needed later which happens where
goal orderings are present. This creates problems in using
FF in our particular problem. The reason being the action
sequences observed had a significant number of primitive
actions with indeterminate orders where FF simply fails
instead of trying out random combinations of such orders.
Thus, while FF is fast where indeterminate orders do not
exist it tends to fail gracelessly in the planning problem
we are facing. This problem will remain in all state space
planners.
The second class of algorithms we investigated were
partial order causal link (POCL) planners which search in
partial plan space. Partial Order refers to the fact that the plan
generated may have primitive actions in indeterminate order.
A POCL planner searches for the solution in partial plan
space, i.e. it makes a set of partial plans, each with flaws,
and then tries to select these partial plans while resolving
the flaws. This continues till either a plan without flaws
is discovered, in which case it is a success or the partial
plan space is exhausted in which case it is a failure. Thus,
the problem of ordering or indeterminate primitives never
arises. The planner among these best suited to our needs is
VHPOP [33] which works where FF fails. VHPOP uses
the A∗ algorithm to search through plan space and also uses
multiple flaw selection strategies concurrently which gives it
much more accurate and faster performance than comparative
planners. Also, VHPOP has capability for durative planning,
something we may incorporate in future work when we
incorporate temporal elements into the plans. Tying in our
minimalist plans with an automated planner is important for
any future application on a robot platform as it gives the
robot the option of verification or prediction in addition
to directly planning its own actions. The entire semantic
framework of the robot operates with knowledge of causality
and we approach the ideal of “intentionality”.
IV. EVALUATION
As mentioned earlier, our dataset has 10 videos of the
six actions which our system is able to recognize. To
make certain this recognition is robust, we evaluate it by
deliberately inducing artificial noise in the input parses and
noting for a particular level of incorrect inputs what will
be the accuracy in recognizing the action. By doing this we

5912

are able to derive theoretical bounds of accuracy in plan
recognition. Also, having built our system which recognises
actions by identifying which minimalist plan is occurring we
evaluate the robustness of our system by making it observe
an activity whose script it does not know of previously, and
letting it reason about it in terms of the scripts it knows of.
A. Observing Unknown Activity
Our system observes a complex human activity which is
essentially, making a sandwich from a few components like
a loaf of bread, a block of cheese and a tomato. This task,
aside from being unknown, also has the added complexity
of a large amount of objects being simultaneously moved
around in the environment making it confusing for the
system. The system does not have the sequence of minimal
plans representing this entire activity. On observing this
activity our system manages to interpret it as a series of
interleaved actions of SLICEs and JOINs as it correctly
identifies the order of MOVEs and GRASPs and is able to
associate them with the consequences for the two actions
which are correctly observed. In Fig. 6 the part of the
sandwich making process which is one of the JOIN actions
is being correctly identified. The two GRASPs and the two
MOVEs along with the consequence ASSEMBLE lead to the
correct conclusion that this part of the activity is a JOIN.
Interestingly, despite the profusion of objects, due to correct
tracking of the hand and the relative velocities with objects, it
does not make mistakes with deducing the order of activities
in making of a sandwich. Also, the entire sequence of actions
observed matches nicely with the automated planner’s output
as expected. The only weak part observed in this experiment
was difficulty in tracking the knife in the repeated SLICE
actions because of its reflective surface but since we were
doing velocity comparisons and since the GRASPs on the
knife had been accurately found the knife movements were
nevertheless correctly identified.

Fig. 7.

in detecting the velocities leading to errors in the detection of
the primitives MOVE and GRASP and then we computed the
percentages of incorrect plan identification. We did this by
finding which plan is the closest to the one being observed.
Here, closest is defined as the plan with the least “edit
distance” from the six known minimal plans where one edit
has the same weight for consequence, primitive and location.
We then counted the number of plans correctly identified for
each level of error. Then we did the same thing by inducing
errors in the location measurement thus making what kind of
MOVE we are observing as inaccurate randomly. Thirdly, we
induced errors in all of the perceptual inputs simultaneously,
the primitives, the locations as well as the consequences,
and computed the closest possible plans being identified,
and counted how many plans were correctly recognized for
what level of induced error. The results of this are in the
Fig. 7. Till 40% of all the three categories of errors the
accuracy of detecting the correct plan lies in the 80-90%
range. The graph shows high resilience even when subjected
to >60% induced errors in all categories. In the worst
case scenarios, inaccuracies in the location doesn’t change
much of the results because most of the plan is determined
by the primitives. Actual performance is much better than
these figures because we chose the errors out of uniform
error distribution. In reality some aspects of perception, like
the consequences and velocity measurement, are generally
quite robust so the errors are not evenly distributed. Even
with such unfavourable conditions the minimalist planning
method exhibits the robustness needed to deal with noise.
V. CONCLUSIONS AND FUTURE WORK
Representing actions by minimalist plans seems to be a
useful, versatile, and robust method to do action understanding. Our future work would involve refining the definition
of the primitives MOVE and GRASP giving them their own
ontologies. This will be done keeping in mind that the robustness of this system depends on the very small amount of
primitives. The perceptual apparatus will be made robust to
do feature processing in order to better track the objects and
be able to observe fine motoric actions by finger mapping.
The plans will be used in a feedback loop to develop a
prediction and attention mechanism which uses a plan to
direct its attention to objects which confirm that the plan
is correct thus improving object recognition in the process.
Finally, we will work on a method to automatically learn
plans in a generative manner by observing human actions on
a basis of lesser bootstrapped information. That will allow
faithful emulation by a robotic platform of observed human
activities.

Accuracy scores for random corruption.

ACKNOWLEDGMENT

B. Inducing Artificial Noise
In the real world no object detection and segmentation
methodology can claim complete accuracy. Occlusion by
hand sometimes leads to incorrect segmentation of the objects. Thus our model needs to be robust over imperfect
perception. We tested this by first randomly inducing errors

The support of the European Union under the grant
Poeticon++ in the Cognitive Systems program, and the support of the National Science foundation under an INSPIRE
grant in the Science of Learning Activities program and a
grant in the Cyberphysical Systems program are gratefully
acknowledged.

5913

Fig. 6. Illustration of recognizing one of the JOIN actions while making a sandwich. The first row has the results of the hand tracking and object
segmentation, the second row has a graph depicting distance of object from hands followed by a row of graphs depicting relative velocities of the objects
and hands which detect the GRASPs, UNGRASPs and MOVEs, the last row depicts where the ASSEMBLE alteration is detected.

R EFERENCES
[1] G. Lakoff, “Women, fire, and dangerous things,” What categories
reveal, 1987.
[2] J. M. Mandler, “How to build a baby: On the development of an
accessible representational system,” Cognitive Development, vol. 3,
no. 2, pp. 113–136, 1988.
[3] I. Laptev, “On space-time interest points,” International Journal of
Computer Vision, vol. 64, no. 2, pp. 107–123, 2005.
[4] P. Dollár, V. Rabaud, G. Cottrell, and S. Belongie, “Behavior recognition via sparse spatio-temporal features,” in Visual Surveillance and
Performance Evaluation of Tracking and Surveillance, 2005. 2nd Joint
IEEE International Workshop on. IEEE, 2005, pp. 65–72.
[5] L. Wang and D. Suter, “Learning and matching of dynamic shape
manifolds for human action recognition,” Image Processing, IEEE
Transactions on, vol. 16, no. 6, pp. 1646–1661, 2007.
[6] N. Chomsky, Lectures on government and binding: The Pisa lectures.
Walter de Gruyter, 1993, vol. 9.
[7] Y. A. Ivanov and A. F. Bobick, “Recognition of visual activities
and interactions by stochastic parsing,” Pattern Analysis and Machine
Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 852–872, 2000.
[8] D. Moore and I. Essa, “Recognizing multitasked activities from video
using stochastic context-free grammar,” in Proceedings of the National
Conference on Artificial Intelligence. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999, 2002, pp. 770–776.
[9] A. Whiten, E. Flynn, K. Brown, and T. Lee, “Imitation of hierarchical
action structure by young children,” Developmental science, vol. 9,
no. 6, pp. 574–582, 2006.
[10] T. Moeslund, A. Hilton, and V. Krüger, “A survey of advances in
vision-based human motion capture and analysis,” Computer vision
and image understanding, vol. 104, no. 2, pp. 90–126, 2006.
[11] P. Turaga, R. Chellappa, V. Subrahmanian, and O. Udrea, “Machine
recognition of human activities: A survey,” Circuits and Systems for
Video Technology, IEEE Transactions on, vol. 18, no. 11, pp. 1473–
1488, 2008.
[12] D. M. Gavrila, “The visual analysis of human movement: A survey,”
Computer vision and image understanding, vol. 73, no. 1, pp. 82–98,
1999.
[13] G. Willems, T. Tuytelaars, and L. Van Gool, “An efficient dense
and scale-invariant spatio-temporal interest point detector,” ECCV, pp.
650–663, 2008.
[14] J. Ben-Arie, Z. Wang, P. Pandit, and S. Rajaram, “Human activity
recognition using multidimensional indexing,” Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 24, no. 8, pp. 1091–
1104, 2002.
[15] A. Yilmaz and M. Shah, “Actions sketch: A novel action representation,” in CVPR, vol. 1, 2005, pp. 984–989.
[16] A. Kale, A. Sundaresan, A. Rajagopalan, N. Cuntoor, A. RoyChowdhury, V. Kruger, and R. Chellappa, “Identification of humans
using gait,” Image Processing, IEEE Transactions on, vol. 13, no. 9,
pp. 1163–1173, 2004.

[17] C. Hu, Q. Yu, Y. Li, and S. Ma, “Extraction of parametric human
model for posture recognition using genetic algorithm,” in Automatic
Face and Gesture Recognition, 2000. Proceedings. Fourth IEEE International Conference on. IEEE, 2000, pp. 518–523.
[18] P. Saisan, G. Doretto, Y. Wu, and S. Soatto, “Dynamic texture
recognition,” in CVPR, vol. 2, 2001, pp. II–58.
[19] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal, “Histograms of
oriented optical flow and binet-cauchy kernels on nonlinear dynamical
systems for the recognition of human actions,” in CVPR, 2009, pp.
1932–1939.
[20] S. Hongeng and R. Nevatia, “Large-scale event detection using semihidden markov models,” in Computer Vision, 2003. Proceedings. Ninth
IEEE International Conference on. IEEE, 2003, pp. 1455–1462.
[21] N. Oliver, E. Horvitz, and A. Garg, “Layered representations for
human activity recognition,” in Multimodal Interfaces, 2002. Proceedings. Fourth IEEE International Conference on. IEEE, 2002, pp. 3–8.
[22] K. Pastra and Y. Aloimonos, “The minimalist grammar of action,”
Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 367, no. 1585, pp. 103–117, 2012.
[23] D. Summers-Stay, C. Teo, Y. Yang, C. Fermuller, and Y. Aloimonos,
“Using a minimal action grammar for activity understanding in the
real world,” in Intelligent Robots and Systems, IEEE International
Conference on, 2013.
[24] S. Schaal, “Is imitation learning the route to humanoid robots?” Trends
in cognitive sciences, vol. 3, no. 6, pp. 233–242, 1999.
[25] M. Bornstein, “A descriptive taxonomy of psychological categories
used by infants,” Origins of cognitive skills, pp. 313–338, 1984.
[26] M. Iacoboni, R. P. Woods, M. Brass, H. Bekkering, J. C. Mazziotta,
and G. Rizzolatti, “Cortical mechanisms of human imitation,” Science,
vol. 286, no. 5449, pp. 2526–2528, 1999.
[27] T. Feix, R. Pawlik, H. Schmiedmayer, J. Romero, and D. Kragic, “A
comprehensive grasp taxonomy,” in Robotics, Science and Systems
Conference: Workshop on Understanding the Human Hand for Advancing Robotic Manipulation, 2009.
[28] I. Oikonomidis, N. Kyriazis, and A. Argyros, “Efficient model-based
3d tracking of hand articulations using kinect,” in BMVC 2011, 2011.
[29] Y. Yang, C. Fermuller, and Y. Aloimonos, “Detection of manipulation
action consequences (mac),” in CVPR, 2013.
[30] B. Han, Y. Zhu, D. Comaniciu, and L. Davis, “Visual tracking
by continuous density propagation in sequential bayesian filtering
framework,” PAMI, IEEE Transactions on, vol. 31, no. 5, pp. 919–
930, 2009.
[31] A. Mishra, C. Fermuller, and Y. Aloimonos, “Active segmentation with
fixation,” in IROS, 2009.
[32] J. Hoffmann and B. Nebel, “The ff planning system: Fast plan
generation through heuristic search,” arXiv preprint arXiv:1106.0675,
2011.
[33] H. L. S. Younes and R. G. Simmons, “Vhpop: Versatile heuristic partial
order planner,” J. Artif. Intell. Res. (JAIR), vol. 20, pp. 405–430, 2003.

5914

Learning the Semantics of Manipulation Action
Yezhou Yang† and Yiannis Aloimonos† and Cornelia Fermüller† and Eren Erdal Aksoy‡
†
UMIACS, University of Maryland, College Park, MD, USA
{yzyang, yiannis, fer}@umiacs.umd.edu
‡
Karlsruhe Institute of Technology, Karlsruhe, Germany
eren.aksoy@kit.edu

Abstract

robot collaboration, action planning and policy design, etc.
In this paper, we are concerned with manipulation actions, that is actions performed by agents
(humans or robots) on objects, resulting in some
physical change of the object. However most of
the current AI systems require manually defined
semantic rules. In this work, we propose a computational linguistics framework, which is based
on probabilistic semantic parsing with Combinatory Categorial Grammar (CCG), to learn manipulation action semantics (lexicon entries) from annotations. We later show that this learned lexicon
is able to make our system reason about manipulation action goals beyond just observation. Thus
the intelligent system can not only imitate human
movements, but also imitate action goals.
Understanding actions by observation and executing them are generally considered as dual problems for intelligent agents. The sensori-motor
bridge connecting the two tasks is essential, and
a great amount of attention in AI, Robotics as well
as Neurophysiology has been devoted to investigating it. Experiments conducted on primates have
discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti
et al., 2001; Gazzola et al., 2007). This suggests
that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able
to first build up a semantic structure from observations, and then the decomposition of that same
structure should occur when the intelligent agent
executes commands.
Additionally, studies in linguistics (Steedman,
2002) suggest that the language faculty develops
in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in
the world by composing affordances of tools and
consequences of actions. It is this more primitive

In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with
a robotic mechanism (e.g. a humanoid
robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs λ-calculus; (2) enable a probabilistic semantic parsing schema to learn the
lambda-calculus representation of manipulation action from an annotated action
corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their
meaning while it can reason beyond observations using propositional logic and
axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.

1

Introduction

Autonomous robots will need to learn the actions
that humans perform. They will need to recognize
these actions when they see them and they will
need to perform these actions themselves. This requires a formal system to represent the action semantics. This representation needs to store the semantic information about the actions, be encoded
in a machine readable language, and inherently be
in a programmable fashion in order to enable reasoning beyond observation. A formal representation of this kind has a variety of other applications such as intelligent manufacturing, human
676

Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 676–686,
c
Beijing, China, July 26-31, 2015. 
2015
Association for Computational Linguistics

The advantage of our approach is twofold: 1)
Learning semantic representations from annotations helps an intelligent agent to enrich automatically its own knowledge about actions; 2) The
formal logic representation of the action could be
used to infer the object-wise consequence after a
certain manipulation, and can also be used to plan
a set of actions to reach a certain action goal. We
further validate our approach on a large publicly
available manipulation action dataset (MANIAC)
from (Aksoy et al., 2014), achieving promising experimental results. Moreover, we believe that our
work, even though it only considers the domain of
manipulation actions, is also a promising example
of a more closely intertwined computer vision and
computational linguistics system. The diagram in
Fig.1 depicts the framework of the system.

apparatus that is our major interest in this paper.
Such an apparatus is composed of a “syntax part”
and a “semantic part”. In the syntax part, every linguistic element is categorized as either a function
or a basic type, and is associated with a syntactic
category which either identifies it as a function or a
basic type. In the semantic part, a semantic translation is attached following the syntactic category
explicitly.
Combinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can
be used to represent such structures with a small
set of combinators such as functional application
and type-raising. What do we gain though from
such a formal description of action? This is similar to asking what one gains from a formal description of language as a generative system. Chomskys contribution to language research was exactly
this: the formal description of language through
the formulation of the Generative and Transformational Grammar (Chomsky, 1957). It revolutionized language research opening up new roads for
the computational analysis of language, providing researchers with common, generative language
structures and syntactic operations, on which language analysis tools were built. A grammar for
action would contribute to providing a common
framework of the syntax and semantics of action,
so that basic tools for action understanding can be
built, tools that researchers can use when developing action interpretation systems, without having
to start development from scratch. The same tools
can be used by robots to execute actions.
In this paper, we propose an approach for learning the semantic meaning of manipulation action
through a probabilistic semantic parsing framework based on CCG theory. For example, we want
to learn from an annotated training action corpus
that the action “Cut” is a function which has two
arguments: a subject and a patient. Also, the action consequence of “Cut” is a separation of the
patient. Using formal logic representation, our
system will learn the semantic representations of
“Cut”:

Figure 1: A CCG based semantic parsing framework for manipulation actions.

2

Related Works

Reasoning beyond appearance: The very small
number of works in computer vision, which aim to
reason beyond appearance models, are also related
to this paper. (Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques,
we could possibly infer implicit information (such
as functional objects) from video, and they call
them “Dark Matter” and “Dark Energy”. (Yang
et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance. (Joo et al., 2014)
used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who
captured an image. More recently, (Pirsiavash et
al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in

Cut :=(AP \N P )/N P : λx.λy.cut(x, y) → divided(y)

Here cut(x, y) is a primitive function. We will further introduce the representation in Sec. 3. Since
our action representation is in a common calculus
form, it enables naturally further logical reasoning
beyond visual observation.
677

proved that grammar based approaches are practical in activity recognition systems, and shed
insight onto human manipulation action understanding. However, as mentioned, thinking about
manipulation actions solely from the viewpoint
of recognition has obvious limitations. In this
work we adopt principles from CFG based activity recognition systems, with extensions to a CCG
grammar that accommodates not only the hierarchical structure of human activity but also action
semantics representations. It enables the system
to serve as the core parsing engine for both manipulation action recognition and execution.

a large corpus using natural language processing
techniques. Different from these fairly general investigations about reasoning beyond appearance,
our paper seeks to learn manipulation actions semantics in logic forms through CCG, and further
infer hidden action consequences beyond appearance through reasoning.
Action Recognition and Understanding: Human activity recognition and understanding has
been studied heavily in Computer Vision recently,
and there is a large range of applications for this
work in areas like human-computer interactions,
biometrics, and video surveillance. Both visual
recognition methods, and the non-visual description methods using motion capture systems have
been used. A few good surveys of the former can
be found in (Moeslund et al., 2006) and (Turaga et
al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz
and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such
as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual
frames e.g. (Saisan et al., 2001; Chaudhry et al.,
2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain
(SEC) representation to model and learn the semantic segment-wise relationship transition from
spatial-temporal video segmentation.

Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a
minimalist generative grammar, similar to the one
of human language, also exists for action understanding and execution. The works closest related
to this paper are (Pastra and Aloimonos, 2012;
Summers-Stay et al., 2013; Guha et al., 2013).
(Pastra and Aloimonos, 2012) first discussed a
Chomskyan grammar for understanding complex
actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of
such a grammar using as perceptual input only
objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al.,
2015) applied it on unconstrained instructional
videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction.

There also have been many syntactic approaches to human activity recognition which used
the concept of context-free grammars, because
such grammars provide a sound theoretical basis
for modeling structured processes. Tracing back
to the middle 90’s, (Brand, 1996) used a grammar
to recognize disassembly tasks that contain hand
manipulations. (Ryoo and Aggarwal, 2006) used
the context-free grammar formalism to recognize
composite human activities and multi-person interactions. It is a two level hierarchical approach
where the lower-levels are composed of HMMs
and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with
errors from low-level processes such as tracking,
stochastic grammars such as stochastic CFGs were
also used (Ivanov and Bobick, 2000; Moore and
Essa, 2002). More recently, (Kuehne et al., 2014)
proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works

Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing
originally was used mainly to translate natural
language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins,
2007). (Mooney, 2008) presented a framework
of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and
actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek
et al., 2011), forklift operation (Tellex et al., 2014)
and of human-robot interaction (Matuszek et al.,
2014). In this work, instead of grounding natural
language sentences directly, we ground information obtained from visual perception into seman678

represents that if the cucumber is cut by x, then
the status of the cucumber is divided.
λ expressions: lambda expressions represent
functions with unknown arguments. For example,
λx.cut(knif e, x) is a function from entities to entities, which is of type NP after any entities of type
N that is cut by knife.

tically informed structures, specifically in the domain of manipulation actions.

3

A CCG Framework for Manipulation
Actions

Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete
background reading, we would like to refer readers to (Steedman, 2000). We will first give a brief
introduction to CCG and then introduce a fundamental combinator, i.e., functional application.
The introduction is followed by examples to show
how the combinator is applied to parse actions.
3.1

3.2

Combinatory Categorial Grammar

The semantic parsing formalism underlying our
framework for manipulation actions is that of
combinatory categorial grammar (CCG) (Steedman, 2000). A CCG specifies one or more logical forms for each element or combination of elements for manipulation actions. In our formalism,
an element of Action is associated with a syntactic “category” which identifies it as functions, and
specifies the type and directionality of their arguments and the type of their result. For example, action “Cut” is a function from patient object phrase
(NP) on the right into predicates, and into functions from subject object phrase (NP) on the left
into a sub action phrase (AP):

Manipulation Action Semantics

The semantic expression in our representation of
manipulation actions uses a typed λ-calculus language. The formal system has two basic types:
entities and functions. Entities in manipulation
actions are Objects or Hands, and functions are
the Actions. Our lambda-calculus expressions are
formed from the following items:
Constants: Constants can be either entities or
functions. For example, Knife is an entity (i.e., it
is of type N) and Cucumber is an entity too (i.e., it
is of type N). Cut is an action function that maps
entities to entities. When the event Knife Cut Cucumber happened, the expression cut(Knife, Cucumber) returns an entity of type AP, aka. Action
Phrase. Constants like divided are status functions
that map entities to truth values. The expression
divided(cucumber) returns a true value after the
event (Knife Cut Cucumber) happened.
Logical connectors: The λ-calculus expression
has logical connectors like conjunction (∧), disjunction (∨), negation(¬) and implication(→).
For example, the expression

Cut := (AP \N P )/N P.

(1)

As a matter of fact, the pure categorial grammar is a conext-free grammar presented in the accepting, rather than the producing direction. The
expression (1) is just an accepting form for Action “Cut” following the context-free grammar.
While it is now convenient to write derivations as
follows, they are equivalent to conventional tree
structure derivations in Figure. 3.2.
Knife
N
NP

Cut

Cucumber
N
NP

(AP \NP )/NP

AP \NP

>

<

AP
AP

AP

NP

connected(tomato, cucumber)∧
divided(tomato) ∧ divided(cucumber)

N

represents the joint status that the sliced tomato
merged with the sliced cucumber. It can be
regarded as a simplified goal status for “making a cucumber tomato salad”. The expression
¬connected(spoon, bowl) represents the status
after the spoon finished stirring the bowl.

A

Knife Cut

NP
N
Cucumber

Figure 2: Example of conventional tree structure.
The semantic type is encoded in these categories, and their translation can be made explicit

λx.cut(x, cucumber) → divided(cucumber)

679

in an expanded notation. Basically a λ-calculus
expression is attached with the syntactic category.
A colon operator is used to separate syntactical
and semantic expressions, and the right side of the
colon is assumed to have lower precedence than
the left side of the colon. Which is intuitive as any
explanation of manipulation actions should first
obey syntactical rules, then semantic rules. Now
the basic element, Action “Cut”, can be further
represented by:

Knife
N
NP
knife
knife

4

4.1

The functional application rules with semantics
can be expressed in the following form:

B : g A\B : f => A : f (g)

(3)

Learning Model and Semantic Parsing

Learning Approach

First we assume that complete syntactic parses of
the observed action are available, and in fact a manipulation action can have several different parses.
The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one
given by (Zettlemoyer and Collins, 2007). We assume a probabilistic categorial grammar (PCCG)
based on a log linear model. M denotes a manipulation task, L denotes the semantic representation
of the task, and T denotes its parse tree. The probability of a particular syntactic and semantic parse
is given as:

Functional application

(2)

>

After having defined the formalism and application rule, instead of manually writing down all the
possible CCG representations for each entity, we
would like to apply a learning technique to derive them from the paired training corpus. Here
we adopt the learning model of (Zettlemoyer and
Collins, 2005), and use it to assign weights to the
semantic representation of actions. Since an action may have multiple possible syntactic and semantic representations assigned to it, we use the
probabilistic model to assign weights to these representations.

(AP \N P )/N P denotes a phrase of type AP ,
which requires an element of type N P to specify
what object was cut, and requires another element
of type N P to further complement what effector
initiates the cut action. λx.λy.cut(x, y) is the λcalculus representation for this function. Since the
functions are closely related to the state update,
→ divided(y) further points out the status expression after the action was performed.
A CCG system has a set of combinatory rules
which describe how adjacent syntatic categories
in a string can be recursively combined. In the
setting of manipulation actions, we want to point
out that similar combinatory rules are also applicable. Especially the functional application rules
are essential in our system.

A/B : f B : g => A : f (g)

(AP \NP )/NP
λx .λy.cut(x , y)
→ divided (y)

Cucumber
N
NP
cucumber
cucumber

AP \NP
λx .cut(x , cucumber )
→ divided (cucumber )
<
AP
cut(knife, cucumber )
→ divided (cucumber )

Cut :=(AP \N P )/N P : λx.λy.cut(x, y) → divided(y).

3.3

Cut

Rule. (2) says that a string with type A/B can be
combined with a right-adjacent string of type B to
form a new string of type A. At the same time, it
also specifies how the semantics of the category A
can be compositionally built out of the semantics
for A/B and B. Rule. (3) is a symmetric form of
Rule. (2).
In the domain of manipulation actions, following derivation is an example CCG parse. This
parse shows how the system can parse an observation (“Knife Cut Cucumber”) into a semantic representation (cut(knif e, cucumber) →
divided(cucumber)) using the functional application rules.

ef (L,T,M )·Θ
f (L,T,M )·Θ
(L,T ) e

P (L, T |M ; Θ) = P

(4)

where f is a mapping of the triple (L, T, M ) to
feature vectors ∈ Rd , and the Θ ∈ Rd represents
the weights to be learned. Here we use only lexical features, where each feature counts the number
of times a lexical entry is used in T . Parsing a manipulation task under PCCG equates to finding L
such that P (L|M ; Θ) is maximized:
argmaxL P (L|M ; Θ)
X
= argmaxL
P (L, T |M ; Θ).
T

680

(5)

We use dynamic programming techniques to
calculate the most probable parse for the manipulation task. In this paper, the implementation from
(Baral et al., 2011) is adopted, where an inverse-λ
technique is used to generalize new semantic representations. The generalization of lexicon rules
are essential for our system to deal with unknown
actions presented during the testing phase.

5

in the format of observed triplets (subject action
patient) and a corresponding semantic representation of the action as well as its consequence. The
semantic representations in λ-calculus format are
given by human annotators after watching each action clip. A set of sample training pairs are given
in Table.1 (one from each action category in the
training set). Since every training clip contains
one single full execution of each manipulation action considered, the training corpus thus has a total
of 120 paired training samples.

Experiments

5.1

Manipulation Action (MANIAC) Dataset

Snapshot

(Aksoy et al., 2014) provides a manipulation action dataset with 8 different manipulation actions
(cutting, chopping, stirring, putting, taking, hiding, uncovering, and pushing), each of which consists of 15 different versions performed by 5 different human actors1 . There are in total 30 different objects manipulated in all demonstrations. All
manipulations were recorded with the Microsoft
Kinect sensor and serve as training data here.
The MANIAC data set contains another 20 long
and complex chained manipulation sequences
(e.g. “making a sandwich”) which consist of a total of 103 different versions of these 8 manipulation tasks performed in different orders with novel
objects under different circumstances. These serve
as testing data for our experiments.
(Aksoy et al., 2014; Aksoy and Wörgötter,
2015) developed a semantic event chain based
model free decomposition approach. It is an unsupervised probabilistic method that measures the
frequency of the changes in the spatial relations
embedded in event chains, in order to extract the
subject and patient visual segments. It also decomposes the long chained complex testing actions
into their primitive action components according
to the spatio-temporal relations of the manipulator. Since the visual recognition is not the core
of this work, we omit the details here and refer
the interested reader to (Aksoy et al., 2014; Aksoy
and Wörgötter, 2015). All these features make the
MANIAC dataset a great testing bed for both the
theoretical framework and the implemented system presented in this work.
5.2

triplet

semantic representation

cleaver chopping carrot

chopping(cleaver, carrot)
→ divided(carrot)

spatula cutting pepper

cutting(spatula, pepper)
→ divided(pepper)

spoon stirring bucket

stirring(spoon, bucket)

cup take down bucket

take down(cup, bucket)
→ ¬connected(cup, bucket)
∧moved(cup)

cup put on top bowl

put on top(cup, bowl)
→ on top(cup, bowl)
∧moved(cup)

bucket hiding ball

hiding(bucket, ball)
→ contained(bucket, ball)
∧moved(bucket)

hand pushing box

pushing(hand, box)
→ moved(box)

box uncover apple

uncover(box, apple)
→ appear(apple)
∧moved(box)

Table 1: Example annotations from training corpus, one per manipulation action category.
We also assume the system knows that every
“object” involved in the corpus is an entity of its
own type, for example:
Knif e := N : knif e
Bowl := N : bowl
......

Additionally, we assume the syntactic form of
each “action” has a main type (AP \N P )/N P
(see Sec. 3.2). These two sets of rules form the
initial seed lexicon for learning.
5.3

Training Corpus

Learned Lexicon

We applied the learning technique mentioned in
Sec. 4, and we used the NL2KR implementation from (Baral et al., 2011). The system learns
and generalizes a set of lexicon entries (syntactic
and semantic) for each action categories from the
training corpus accompanied with a set of weights.

We first created a training corpus by annotating
the 120 training clips from the MANIAC dataset,
1

Dataset available for download at https:
//fortknox.physik3.gwdg.de/cns/index.
php?page=maniac-dataset.

681

they serve as the testing corpus. However, since
many of the objects used in the testing data are not
present in the training set, an object model-free approach is adopted and thus “subject” and “patient”
fields are filled with segment IDs instead of a specific object name. Fig. 3 and 4 show several examples of the detected triplets accompanied with a
set of key frames from the testing sequences. Nevertheless, the method we used here can 1) generalize the unknown segments into the category of
object entities and 2) generalize the unknown actions (those that do not exist in the training corpus)
into the category of action function. This is done
by automatically generalizing the following two
types of lexicon entries using the inverse-λ technique from (Baral et al., 2011):

We list the one with the largest weight for each action here respectively:
Chopping :=(AP \N P )/N P : λx.λy.chopping(x, y)
→ divided(y)
Cutting :=(AP \N P )/N P : λx.λy.cutting(x, y)
→ divided(y)
Stirring :=(AP \N P )/N P : λx.λy.stirring(x, y)
T ake down :=(AP \N P )/N P : λx.λy.take down(x, y)
→ ¬connected(x, y) ∧ moved(x)
P ut on top :=(AP \N P )/N P : λx.λy.put on top(x, y)
→ on top(x, y) ∧ moved(x)
Hiding :=(AP \N P )/N P : λx.λy.hiding(x, y)
→ contained(x, y) ∧ moved(x)
P ushing :=(AP \N P )/N P : λx.λy.pushing(x, y)
→ moved(y)
U ncover :=(AP \N P )/N P : λx.λy.uncover(x, y)
→ appear(y) ∧ moved(x).

Object [ID] :=N : object [ID]
U nknown :=(AP \N P )/N P : λx.λy.unknown(x, y)

The set of seed lexicon and the learned lexicon
entries are further used to probabilistically parse
the detected triplet sequences from the 20 long
manipulation activities in the testing set.
5.4

Among the 90 detected triplets, using the
learned lexicon we are able to parse all of them
into semantic representations. Here we pick the
representation with the highest probability after
parsing as the individual action semantic representation. The “parsed semantics” rows of Fig. 3 and
4 show several example action semantics on testing sequences. Taking the fourth sub-action from
Fig. 4 as an example, the visually detected triplets
based on segmentation and spatial decomposition
is (Object 014, Chopping, Object 011). After semantic parsing, the system predicts that
divided(Object 011). The complete training corpus and parsed results of the testing set will be
made publicly available for future research.

Deducing Semantics

Using the decomposition technique from (Aksoy
et al., 2014; Aksoy and Wörgötter, 2015), the reported system is able to detect a sequence of action triplets in the form of (Subject Action Patient) from each of the testing sequence in MANIAC dataset. Briefly speaking, the event chain
representation (Aksoy et al., 2011) of the observed
long manipulation activity is first scanned to estimate the main manipulator, i.e. the hand, and manipulated objects, e.g. knife, in the scene without
employing any visual feature-based object recognition method. Solely based on the interactions
between the hand and manipulated objects in the
scene, the event chain is partitioned into chunks.
These chunks are further fragmented into subunits to detect parallel action streams. Each parsed
Semantic Event Chain (SEC) chunk is then compared with the model SECs in the library to decide
whether the current SEC sample belongs to one
of the known manipulation models or represents a
novel manipulation. SEC models, stored in the library, are learned in an on-line unsupervised fashion using the semantics of manipulations derived
from a given set of training data in order to create
a large vocabulary of single atomic manipulations.
For the different testing sequence, the number
of triplets detected ranges from two to seven. In total, we are able to collect 90 testing detections and

5.5

Reasoning Beyond Observations

As mentioned before, because of the use of λcalculus for representing action semantics, the obtained data can naturally be used to do logical reasoning beyond observations. This by itself is a
very interesting research topic and it is beyond this
paper’s scope. However by applying a couple of
common sense Axioms on the testing data, we can
provide some flavor of this idea.
Case study one: See the “final action consequence and reasoning” row of Fig. 3 for case one.
Using propositional logic and axiom schema, we
can represent the common sense statement (“if an
object x is contained in object y, and object z is
on top of object y, then object z is on top of object
x”) as follows:
682

Figure 3: System output on complex chained manipulation testing sequence one. The segmentation
output and detected triplets are from (Aksoy and Wörgötter, 2015)
.

Figure 4: System output on the 18th complex chained manipulation testing sequence. The segmentation
output and detected triplets are from (Aksoy and Wörgötter, 2015)
.
Axiom (1):
∃x, y, z, contained(y, x) ∧
on top(z, y) → on top(z, x).
Then it is trivial to deduce an additional final action consequence in this scenario that
(on top(object 007, object 009)). This matches
the fact: the yellow box which is put on top of the
red bucket is also on top of the black ball.
Case study two: See the “final action consequence and reasoning” row of Fig. 4 for a more
complicated case. Using propositional logic and
axiom schema, we can represent three common
sense statements:
1) “if an object y is contained in object x, and
object z is contained in object y, then object z is
contained in object x”;
2) “if an object x is contained in object y, and
object y is divided, then object x is divided”;
3) “if an object x is contained in object y, and
object y is on top of object z, then object x is on
top of object z” as follows:

Axiom (2):
∃x, y, z, contained(y, x) ∧
contained(z, y) → contained(z, x).
Axiom (3):
∃x, y, contained(y, x) ∧
divided(y) → divided(x).
Axiom (4):
∃x, y, z, contained(y, x) ∧
on top(y, z) → on top(x, z).
With these common sense Axioms, the system
is able to deduce several additional final action
consequences in this scenario:
divided(object 005) ∧ divided(object 010)
∧ on top(object 005, object 012)
∧ on top(object 010, object 012).

From Fig. 4, we can see that these additional
consequences indeed match the facts: 1) the bread
and cheese which are covered by ham are also divided, even though from observation the system
only detected the ham being cut; 2) the divided
bread and cheese are also on top of the plate, even
though from observation the system only detected
the ham being put on top of the plate.
683

We applied the four Axioms on the 20 testing
action sequences and deduced the “hidden” consequences from observation. To evaluate our system
performance quantitatively, we first annotated all
the final action consequences (both obvious and
“hidden” ones) from the 20 testing sequences as
ground-truth facts. In total there are 122 consequences annotated. Using perception only (Aksoy
and Wörgötter, 2015), due to the decomposition
errors (such as the red font ones in Fig. 4) the system can detect 91 consequences correctly, yielding
a 74% correct rate. After applying the four Axioms and reasoning, our system is able to detect
105 consequences correctly, yielding a 86% correct rate. Overall, this is a 15.4% of improvement.
Here we want to mention a caveat: there are definitely other common sense Axioms that we are
not able to address in the current implementation.
However, from the case studies presented, we can
see that using the presented formal framework, our
system is able to reason about manipulation action
goals instead of just observing what is happening
visually. This capability is essential for intelligent
agents to imitate action goals from observation.

scenarios, based on whether the action is transitive or intransitive, the main types of action can be
extended to include AP \N P .
Moreover, the logical expressions can also be
extended to include universal quantification ∀ and
existential quantification ∃. Thus, manipulation
action such as “knife cut every tomato” can be
parsed into a representation as ∀x.tomato(x) ∧
cut(knif e, x) → divided(x) (the parse is given
in the following chart). Here, the concept “every”
has a main type of N P \N P and semantic meaning of ∀x.f (x). The same framework can also
extended to have other combinatory rules such as
composition and type-raising (Steedman, 2002).
These are parts of the future work along the line of
the presented work.

6

The presented computational linguistic framework enables an intelligent agent to predict and
reason action goals from observation, and thus has
many potential applications such as human intention prediction, robot action policy planning, human robot collaboration etc. We believe that our
formalism of manipulation actions bridges computational linguistics, vision and robotics, and
opens further research in Artificial Intelligence
and Robotics. As the robotics industry is moving
towards robots that function safely, effectively and
autonomously to perform tasks in real-world unstructured environments, they will need to be able
to understand the meaning of actions and acquire
human-like common-sense reasoning capabilities.

Knife
N
NP
knife
knife

Cut

every

(AP \NP )/NP
λx .λy.cut(x , y)
→ divided (y)

N P \N P
∀x .f (x )
∀x .f (x )

Tomato
N
NP
tomato
tomato

NP
∀x .tomato(x )

>
>

AP \NP
∀y.λx .tomato(y) ∧ cut(x , y) → divided (y)
<
AP
∀y.tomato(y) ∧ cut(knife, y) → divided (y)

Conclusion and Future Work

In this paper we presented a formal computational framework for modeling manipulation actions based on a Combinatory Categorial Grammar. An empirical study on a large manipulation action dataset validates that 1) with the introduced formalism, a learning system can be devised
to deduce the semantic meaning of manipulation
actions in λ-schema; 2) with the learned schema
and several common sense Axioms, our system is
able to reason beyond just observation and deduce
“hidden” action consequences, yielding a decent
performance improvement.
Due to the limitation of current testing scenarios, we conducted experiments only considering a
relatively small set of seed lexicon rules and logical expressions. Nevertheless, we want to mention that the presented CCG framework can also
be extended to learn the formal logic representation of more complex manipulation action semantics. For example, the temporal order of manipulation actions can be modeled by considering a seed
rule such as AP \AP : λf.λg.bef ore(f (·), g(·)),
where bef ore(·, ·) is a temporal predicate. For
actions in this paper we consider seed main type
(AP \N P )/N P . For more general manipulation

7

Acknowledgements

This research was funded in part by the support of the European Union under the Cognitive Systems program (project POETICON++),
the National Science Foundation under INSPIRE
grant SMA 1248056, and by DARPA through
U.S. Army grant W911NF-14-1-0384 under the
Project: Shared Perception, Cognition and Reasoning for Autonomy.

684

References

Jungseock Joo, Weixin Li, Francis F Steen, and SongChun Zhu. 2014. Visual persuasion: Inferring communicative intents of images. In Computer Vision
and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 216–223. IEEE.

E E. Aksoy and F. Wörgötter. 2015. Semantic decomposition and recognition of long and complex manipulation action sequences. International Journal
of Computer Vision, page Under Review.

A. Kale, A. Sundaresan, AN Rajagopalan, N.P. Cuntoor, A.K. Roy-Chowdhury, V. Kruger, and R. Chellappa. 2004. Identification of humans using
gait. IEEE Transactions on Image Processing,
13(9):1163–1173.

E.E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen,
and F. Wörgötter. 2011. Learning the semantics of
object–action relations by observation. The International Journal of Robotics Research, 30(10):1229–
1249.
E E. Aksoy, M. Tamosiunaite, and F. Wörgötter. 2014.
Model-free incremental learning of the semantics
of manipulation actions. Robotics and Autonomous
Systems, pages 1–42.

Hilde Kuehne, Ali Arslan, and Thomas Serre. 2014.
The language of actions: Recovering the syntax
and semantics of goal-directed human activities. In
Computer Vision and Pattern Recognition (CVPR),
2014 IEEE Conference on, pages 780–787. IEEE.

Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Jiayu Zhou. 2011. Using inverse λ and generalization to translate english to formal languages. In
Proceedings of the Ninth International Conference
on Computational Semantics, pages 35–44. Association for Computational Linguistics.

Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2011. A joint
model of language and perception for grounded attribute learning. In International Conference on Machine learning (ICML).

Jezekiel Ben-Arie, Zhiqian Wang, Purvin Pandit, and
Shyamsundar Rajaram.
2002.
Human activity recognition using multidimensional indexing.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 24(8):1091–1104.

Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and
Dieter Fox. 2014. Learning from unscripted deictic
gesture and language for human-robot interactions.
In Twenty-Eighth AAAI Conference on Artificial Intelligence.

Matthew Brand. 1996. Understanding manipulation in
video. In Proceedings of the Second International
Conference on Automatic Face and Gesture Recognition, pages 94–99, Killington,VT. IEEE.

T.B. Moeslund, A. Hilton, and V. Krüger. 2006. A
survey of advances in vision-based human motion
capture and analysis. Computer vision and image
understanding, 104(2):90–126.

R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal.
2009. Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for
the recognition of human actions. In Proceedings
of the 2009 IEEE Intenational Conference on Computer Vision and Pattern Recognition, pages 1932–
1939, Miami,FL. IEEE.

Raymond J Mooney. 2008. Learning to connect language and perception. In AAAI, pages 1598–1601.
Darnell Moore and Irfan Essa. 2002. Recognizing
multitasked activities from video using stochastic
context-free grammar. In Proceedings of the National Conference on Artificial Intelligence, pages
770–776, Menlo Park, CA. AAAI.

N. Chomsky. 1957. Syntactic Structures. Mouton de
Gruyter.

K. Pastra and Y. Aloimonos. 2012. The minimalist grammar of action. Philosophical Transactions of the Royal Society B: Biological Sciences,
367(1585):103–117.

Noam Chomsky. 1993. Lectures on government and
binding: The Pisa lectures. Walter de Gruyter.

Hamed Pirsiavash, Carl Vondrick, and Antonio Torralba. 2014. Inferring the why in images. arXiv
preprint arXiv:1406.5472.

V Gazzola, G Rizzolatti, B Wicker, and C Keysers.
2007. The anthropomorphic brain: the mirror neuron system responds to human and robotic actions.
Neuroimage, 35(4):1674–1684.

Giacomo Rizzolatti, Leonardo Fogassi, and Vittorio
Gallese. 2001. Neurophysiological mechanisms underlying the understanding and imitation of action.
Nature Reviews Neuroscience, 2(9):661–670.

Anupam Guha, Yezhou Yang, Cornelia Fermüller, and
Yiannis Aloimonos. 2013. Minimalist plans for interpreting manipulation actions. Proceedings of the
2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5908–5914.

Michael S Ryoo and Jake K Aggarwal. 2006. Recognition of composite human activities through contextfree grammar based representation. In Proceedings
of the 2006 IEEE Conference on Computer Vision
and Pattern Recognition, volume 2, pages 1709–
1718, New York City, NY. IEEE.

Yuri A. Ivanov and Aaron F. Bobick. 2000. Recognition of visual activities and interactions by stochastic
parsing. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(8):852–872.

685

P. Saisan, G. Doretto, Y.N. Wu, and S. Soatto. 2001.
Dynamic texture recognition. In Proceedings of the
2001 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 2, pages
58–63, Kauai, HI. IEEE.
Mark Steedman. 2000. The syntactic process, volume 35. MIT Press.
Mark Steedman. 2002. Plans, affordances, and combinatory grammar. Linguistics and Philosophy, 25(56):723–753.
D. Summers-Stay, C.L. Teo, Y. Yang, C. Fermüller,
and Y. Aloimonos. 2013. Using a minimal action grammar for activity understanding in the real
world. In Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4104–4111, Vilamoura, Portugal. IEEE.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy.
2014.
Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning, 94(2):151–167.
P. Turaga, R. Chellappa, V.S. Subrahmanian, and
O. Udrea. 2008. Machine recognition of human activities: A survey. IEEE Transactions on Circuits
and Systems for Video Technology, 18(11):1473–
1488.
Dan Xie, Sinisa Todorovic, and Song-Chun Zhu. 2013.
Inferring “dark matter” and “dark energy” from
videos. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2224–2231. IEEE.
Yezhou Yang, Cornelia Fermüller, and Yiannis Aloimonos. 2013. Detection of manipulation action
consequences (MAC). In Proceedings of the 2013
IEEE Conference on Computer Vision and Pattern
Recognition, pages 2563–2570, Portland, OR. IEEE.
Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos.
2014. A cognitive system for understanding human manipulation actions. Advances in Cognitive
Sysytems, 3:67–86.
Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis
Aloimonos. 2015. Robot learning manipulation action plans by “watching” unconstrained videos from
the world wide web. In The Twenty-Ninth AAAI
Conference on Artificial Intelligence (AAAI-15).
A. Yilmaz and M. Shah. 2005. Actions sketch: A
novel action representation. In Proceedings of the
2005 IEEE Intenational Conference on Computer
Vision and Pattern Recognition, volume 1, pages
984–989, San Diego, CA. IEEE.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Structured classification with probabilistic categorial
grammars. In UAI.
Luke S Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678–687.

686

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

1

Action Attribute Detection from Sports
Videos with Contextual Constraints
Xiaodong Yu1

1

Comcast Corporation
Washington DC, USA

2

University of Maryland,
College Park, MD, USA

xiaodong_yu@cable.comcast.com

Ching Lik Teo2
cteo@cs.umd.edu

Yezhou Yang2
yzyang@cs.umd.edu

Cornelia Fermüller2
fer@cfar.umd.edu

Yiannis Aloimonos2
yiannis@cs.umd.edu

Abstract
In this paper, we are interested in detecting action attributes from sports videos for
event understanding and video analysis. Action attribute is a middle layer between low
level motion features and high level action classes, which includes various motion patterns of human limbs and bodies and the interaction between human and objects. Successfully detecting action attributes provides a richer video description that facilitates
many other important tasks, such action classification, video understanding, automatic
video transcript, etc.
A naive approach to deal with this challenging problem is to train a classifier for each
attribute and then use them to detect attributes in novel videos independently. However,
this independence assumption is often too strong, and as we show in our experiments,
produces a large number of false positives in practice. We propose a novel approach
that incorporates the contextual constraints for activity attribute detection. The temporal
contexts within an attribute and the co-occurrence contexts between different attributes
are modelled by a factorial conditional random field, which encourages agreement between different time points and attributes. The effectiveness of our methods are clearly
illustrated by the experimental evaluations.

1

Introduction

While traditional computer vision research focuses on categorizing objects, scenes and actions, more and more people are interested in going beyond naming these entities and devising approaches to infer attributes from them. The capacity to detect attributes allows us to
describe a visual entity in greater detail; compare two visual entities at multiple levels, and
helps us to categorize unseen visual classes [4, 6]. The attribute layer can be considered as an
intermediate layer between the low-level features and action class labels, and this property
can facilitate novel applications such as zero-shot learning and one-shot learning [4, 8, 16].
c 2013. The copyright of this document resides with its authors.

It may be distributed unchanged freely in print or electronic forms.

2

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

Figure 1: Overview of the proposed system including the key components (left) and example
inputs/outputs (right): row (a) shows example video frames along the timeline; row (b) shows
the detected STIP interest points; row (c) shows the probability output from the elementary
attribute detector (the brighter the color, the higher the probability that corresponding attribute presents in that frame); row (d) shows the final attribute label after applying context
constraints (bright color represents positive label and black represent negative). The frame
numbers are illustrated on the bottom of row (c) and (d).

In this paper, we study the problem of detecting action attributes from sport videos.
Action attributes include atomic components of action classes (such as the motion patterns
of human limbs and body), contextual components of action classes (such as the objects and
scenes involved in the action), and non-semantic attributes, a.k.a data-driven attributes [9].
A common property of action attributes is that they can be generalized into different action
classes. This is especially true in sports videos. For example, bend, as an action attribute that
describes the motion of human body, is present in the action tennis serve, bowling,
snatch, etc. That is why they can be learned even from training sets that contain only a
few examples for each action class (one-shot learning) or even no example for some action
classes (zero-shot learning). We focus on the action attributes related to motion patterns of
human body in this paper; however, our model can be easily extended to detect the other
types of action attributes as well.
The concept of action attribute was first introduced in [9]. However, the approach proposed in [9] has several severe limitations that restrict the applicability of action attributes.
One of the most noticeable problems is that action attributes are labelled at the level of an
action class, instead of being labelled at the level of a frame or at the level of a video. As a
result, all videos from the same action class are assumed to have the same set of action attributes, regardless of the exact content of a specific video, and the temporal structure of the
action attributes is totally discarded. But in reality, not every video belonging to the same
action class have the same set of action attributes; more often than not, real-world videos
would have some exceptions. For example, in a video of the snatch activity, the athlete
may not be able to completely lift the barbell above his head at the end so we cannot say this
video has an action attribute two arms raise pose. Furthermore, the temporal structure is a
unique property of action attributes and we lose lots of descriptive capacity if we ignore it.

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

3

For example, given a video of basketball layup, a description “the athlete starts with
a slow run and lasts for half a second, then jumps forward with single leg in the next second,
finally jumps up and throws the ball (into the basket), and maintains a slow run at the end of
the video” will be more useful than simply saying “there are slow running, jumping forward,
jumping up, throwing in this video”. The goal of this paper is thus to detect the key action
attributes at each frame from a given video so that we can generate video descriptions at a
much finer granularity than those from previous work. Figure 1 shows an example of the
action attributes proposed in this paper for a particular activity (see row (d)).
While having great advantages as discussed above, it is obviously a much more challenging task to locate the temporal occurrences of every action attributes in a given video. As a
high-level semantic concept, a particular action attribute may exhibit significant variability
due to viewpoint changes, photometric measurements, intra-class variability (e.g. attribute
two arms open can have different opening angles between two arms, jump up can have different height and velocity, etc). Naive detectors that rely entirely on local features will easily
be overwhelmed by a large number of false positives and/or false negatives. So we must take
into account the contextual constraints in both the temporal and semantic domains, thereby
reducing the noise in the local feature space to produce more reliable results. This is exactly
the theme of this paper.

2
2.1

Detecting Action Attributes using Contextual
Constraints
Systematic Overview

A systematic overview of our approach is shown in Figure 1. It is composed of three parts:
feature extractor, which extracts low-level features from a video; elementary attribute detector, which detects attributes in a video using local cues only; and context constraints, which
combine outputs from the elementary detectors of all attributes so as to determine a set of
globally optimized attribute labels. We use off-the-shelf algorithms in the first two parts and
a factorial conditional random field to incorporate the contextual constraints in the last part.
The details are presented in the rest of this section.

2.2

Low-level feature extraction

Since our goal is to detect human action attributes, we need a module to detect human and
extract motion features within the human bounding box. This problem has been thoroughly
studied and numerous algorithms have been proposed in the last few decades. Interested
readers can refer to [1, 2, 3, 5, 10] for a few exemplar implementations. Since this is beyond
the scope of this paper, we assume that this step has been done and simply use the annotated
bounding box as the one produced by any algorithm that does human detection and tracking. In this way, we can measure the upper bound on the performance of various attribute
detectors described in the next section.
The low-level features for representing motions in a video are HOG and HOF, which
are extracted at the detected interest points using the author’s latest implementation of STIP
[15] with a default noise threshold and video resolution of 320×240. All descriptors within
the human bounding box are quantized into one of the 400 visual words, which are computed using k-means from 40,000 randomly selected descriptors. At the end, each frame is

4

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

represented by a histogram of visual words, x ∈ Xd , which counts the number of quantized
descriptors within the bounding box of the human in a frame and its two closest neighbours.

2.3

Elementary attribute detectors

Our elementary attribute detector is an SVM with a χ 2 kernel, which takes the histogram of
visual words as inputs and predicts the probability of a particular action attribute occurring
in each frame of an unseen video. The kernel function K(hi , h j ) is given as in Section 3.4 of
[17]
During training, we have training videos with attributes labelled at each frame. For each
attribute, we select features from all positive frames as positive examples and randomly select an equal number of negative examples to train the SVM. For testing, we first extract
histogram of visual words from each frame t in an unseen video, denoted as xt . Then the
SVM for attribute a predicts the presence of this attribute in frame t, and the output probability value is denoted as Na (xt ). This value will be used as the node feature of the conditional
random field described in the next section. The SVM also predicts a binary value to indicate
the presence/absence of attribute a at frame t by applying a threshold of 0.5 over Na (xt ).
This will be used as a baseline in our evaluation. See Section 3.2 for more details.

2.4

Incorporating Contextual Constraints

2.4.1

Factorial Conditional Random Field Model

We take into account two types of contextual constraints in this paper: the temporal context
and the semantic context. To promote agreement between the different attribute labels at
different frames, we model the contextual constraints with a conditional random field (CRF)
as shown in Figure 2(b). The features extracted from T frames in a video are denoted as a
vector of T local observations, X = {x1 , x2 , ..., xT }; and at each local observation at frame t,
by xt , the histogram of visual words as discussed in the preceding section. At each frame
t, we wish to detect the presence of A action attributes yt = {yt1 , yt2 , ..., ytA }, which are the
states in the CRF model. In the literature, this is also known as a factorial CRF [14]. To
better understand this CRF model, we can compare it with a linear chain CRF [7] as shown
in Figure 2(a), which has been used in action class recognition from video streams [13]. In a
linear chain CRF, the state of each time point is dependent on its immediate neighbors only
(Markovian assumption) and we enforce agreement between states in adjacent time points
after accounting for the correlation between the neighboring temporal states. In a factorial
CRF, we have multiple states at each time point, yt = {yt1 , yt2 , ..., ytA }, and there are edges
between every pair of yti and ytj , (i, j) ∈ {1, 2, ..., A}1 . To avoid clutter, we only show two
attributes at each time point in Figure 2(b). In the experimental dataset used, there are 24
attributes at each time point. The between-chain edges are designed to promote agreement
between different attributes at the same time point. The intuition is that some attributes tend
to occur together, e.g.two arms oscillate and fast run while others don’t, e.g. slow run and
fast run. Thus the between-chain edges take into account the co-temporal correlation among
attributes.
1 For clarity, we call the edges between states of the same time points as between-chain edges and the edges
between states of adjacent time points as within-chain edges. In Figure 2(b), the former are colored in red and the
latter in black

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

5

The factorial CRF is defined as follows
1
P(Y|X) =
Z(X)

T

!

A

∑ ∑ ϒt (yt,a , X)

t=1 a=1

!

T −1 A

∑ ∑ Ψt (yt,a , yt+1,a , X)

t=1 a=1

!

T

∑

∑

Φt (yt,a , yt,b , X) ,

(1)

t=1 a,b∈{1,...,A},a6=b

where Z(X) is the partition function, {ϒt } the local (node) potential functions, {Ψt } the
within-chain potential functions, and {Φt } the between-chain potential functions. The potential functions are defined by a set of feature functions { fk } together with corresponding
weights {λk } as:
!
ϒt (yt,a , X) = exp

∑ λk fk (yt,a , X)
k

!
Ψt (yt,a , yt+1,a , X) = exp

∑ λk fk (yt,a , yt+1,a , X)
k

!
Φt (yt,a , yt,b , X) = exp

∑ λk fk (yt,a , yt,b , X)

.

(2)

k

The feature functions for the above potential functions are defined as follows:
fk (yt,a , X) = I[yt,a = m] log Na (xt− j )
fk (yt,a , yt+1,a , X) = I[yt,a = m ∧ yt+1,a = n]
fk (yt,a , yt,b , X) = I[yt,a = m ∧ yt,b = n]φ (a, b)

(3)

where j ∈ [−W,W ]2 , m, n ∈ {0, 1}, a, b ∈ {1, ..., A}, I[x = A] is an indicator function so that
I[x = A] = 1 if x = A, or 0 otherwise. Intuitively, the node feature functions encode correlations from the presence of attribute a and the observation confidence forward or backward
within a time window; the within-chain edge feature functions encode the transition probability between adjacent time point for attribute a; the between-chain edge feature functions
encode the compatibility of two attributes, a and b, measured by φ (a, b) and is obtained by
normalizing the co-occurrence frequency between these two attributes.
2.4.2

Learning Model Parameters

In general, given a training set consisting of N sequences D = {Xn , Yn }Ni=1 , we want to
find the optimal parameter Λ∗ = {λk∗ } that maximizes the following objective function, as
discussed in [7],
N

L(Λ) =

∑ log P(Yn |Xn , Λ),

n=1
2 where W

= 1 is the size of the sliding window around a video frame

(4)

6

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

(a)
(b)
Figure 2: (a) A linear chain CRF model and (b) a factorial CRF model. To avoid clutter, we
only show two attributes at each time points. In reality, we can have as many as attributes at
each time points and they are fully connected to each other.

where the right hand side is the conditional log-likelihood of the training data. The partial
derivative of the log-likelihood w.r.t λk associated with clique index c is [14]
∂L
n
n
= ∑ fk (Yt,c
, Xn ) − ∑ ∑ ∑ p(Yt,c |Xn ) fk (Yt,c
Xn )
∂ λk ∑
n t
n t Yt,c

(5)

n is the assignment to Y in Yn , and Y ranges over assignments to the clique c
where Yt,c
t,c
t,c
at time point t. The first term in the right hand side is easy to compute. The second term
computes marginal probabilities p(Yt,c |Xn ), which will be discussed in Section 2.4.3.
To reduce overfitting, we define a spherical Gaussian prior [14] to the parameter, which
has zero mean and covariance matrix Σ = σ 2 I, i.e.,

Λ ∼ N (0, σ 2 I),

(6)

and this is equivalent to optimizing L(Λ) using `2 regularization:
N

Lr (Λ) =

1

∑ log P(Yn |Xn , Λ) − 2σ 2 k Λ k2 ,

(7)

n=1

and the gradient becomes
∂ Lr (Λ)
∂L
λk
=
−
∂ λk
∂ λk σ 2

(8)

To speed up the training process, we use stochastic gradient ascent to search for the
optimal parameters [12]. At each iteration, we randomly pick a training sequence, evaluate
the gradient w.r.t λk on that training sequence, and update λk by taking a small step in the
direction of the negative gradient
λk ← λk + α



λk 
,

n
n
, Xn ) − ∑ ∑ p(Yt,c |Xn ) fk (Yt,c
, Xn ) − 2
∑ fk (Yt,c
σ
t

(9)

t Yt,c

where α is a learning rate parameter, which is set to a small value. The iteration continues
until we reach the maximum iteration or the change of objective function is below a threshold
for 10 iterations. In our experiments, the optimization procedure usually completes within
10N iterations.

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

2.4.3

7

Inference

Two types of inference tasks are addressed during training and testing. In training, we need
to compute the marginal probability of each clique p(Yt,c |Xn ). In testing, we need to perform Viterbi decoding, i.e. estimate the most probable attribute sequence Y∗ for an unseen
sequence X that maximizes the conditional probability
Y∗ = arg max P(Y|X, Λ∗ )

(10)

Y

where the parameter Λ∗ = {λk } are learned from training examples. Both tasks can be
achieved by Loopy Belief Propagation (LBP) [14]. In this section, we briefly discuss the LBP
procedure for computing the marginal probability. The Viterbi decoding can be performed
in a similar fashion by replacing the summation in Equation (11) with a maximization.
Belief propagation algorithms iteratively update a vector m = (mi (v j )), which are called
messages between pair of vertices vi and v j . The message mi (v j ) sent from vertex vi to its
neighbor v j is given by:
!
mi (v j ) ← ∑ ϒ(vi )Ω(vi , v j ) ∏ mk (vi ) ,
i

(11)

k6= j

where ϒ(vi ) is the local potential, Ω(vi , v j ) the edge potential between vi and v j , mk (vi ) is the
message sent to vi from its neighbors except v j . A random schedule is adopted and messages
propagate through the CRF until convergence or a maximum number of iterations is reached.
Then the marginal probability of nodes vi and v j are computed as:
p(vi , v j ) ∝ ϒ(vi )ϒ(v j )Ω(vi , v j ) ∏ mk (vi ) ∏ mk (v j ).
k6= j

(12)

k6=i

Note that we have omitted X in the above two equations for clarity, and we use Ω(vi , v j ) to
refer either Ψt (yt,a , yt+1,a , X) or Φt (yt,a , yt,b , X), which can be determined by the clique that
involves vi and v j .

3
3.1

Experiments
Dataset and action attributes

We tested our approach on the Olympic Sports Dataset [11]. This dataset includes 16 action
classes and 783 videos. The original purpose of this dataset is for recognizing action classes
and thus there is no attribute labels available. Liu et al [9] defined 39 attributes on this dataset
on the action class level, i.e., each action class has a list of fixed attributes across all videos of
this class. As we argued in Section 1, this level of action attribute labels is neither sufficient
to describe the dynamics in the action videos nor capture the unique characteristics within a
particular video. Thus we create a new dataset to evaluate the performance of action attribute
detection using the videos from the Olympic Sport Dataset. In particular, we defined 24
action attributes, which include 9 leg motion patterns, 6 arm motion patterns, 6 whole body
motion patterns, and 3 human-object interactions. For each action class, we randomly select
20 videos from the Olympic Sports Dataset to create the Action with Attribute dataset with
320 videos. In each video, we labelled the presence/absence of each action attribute as well
as the bounding box of the athlete in each frame.

8

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

To evaluate the performance of action attribute detector, we divide the Action with Attribute dataset into two disjoint subsets, which include a training set with 240 videos and a
testing set with 80 videos. Both sets include all 16 action classes.

3.2

Baseline algorithms

The first baseline algorithm is the elementary attribute detectors described in Section 2.3.
This algorithm treats each frame as an independent sample and does not take into account
any contextual constraints.
The second baseline algorithm is a linear CRF as illustrated in Figure 2(a). The node
feature functions and edge feature functions are defined in Equation (3). The learning algorithm is the same as the one presented in Section 2.4.2, except that there are no betweenchain feature functions involved. Since this is a chain structured CRF, exact inference can
be achieved by the forward-backward algorithm and Viterbi algorithm [14]. This algorithm
takes into account the temporal contextual constraints but treats each attribute independently:
the semantic context is still ignored.

3.3

Experimental Results

We measure the performance of attribute detection by precision, recall and F1-score at the
frame level. Every frame is treated as an individual sample for this purpose. A positive
sample that is correctly (incorrectly) detected as positive (negative) is counted as true positive, a.k.a., TP, (false negative, a.k.a., FN); A negative sample that is correctly (incorrectly)
detected as negative (positive) is counted as true negative, a.k.a., TN, (false positive, a.k.a.,
FP).
We compute the three measurements for each attribute and their mean and standard deviation for the baselines and the proposed factorial CRF (FCRF). Their overall detection
performances are summarized in Figure 3(a). This figure shows that the proposed FCRF significantly outperforms the baseline algorithms SVM and LCRF in precision and F1-score,
while the recall of all three algorithms are similar. To test the significance of these performance results, we did a t-test of the null hypothesis that the mean performances of FCRF are
the same as those of SVM and LCRF, against the alternative that the mean performances of
FCRF are higher than those of SVM and LCRF, with significance α = 0.05. The result of ttest failed to reject the null hypothesis for recall but rejected the null hypothesis for precision
and F1-score, with p-value < 0.0005.
Figure 3(b) compares the precision scores across each attribute. It shows that FCRF
improves the detection precision for every attribute. In particular, the baseline algorithms
achieve very low precision for a few attributes, e.g. walk forward, stand up, one arm open,
put down and throw. FCRF removes a large number false positives using contextual constraints which results in significantly better precision scores.
Figure 4 illustrates the outputs of the three algorithms on a video of the activity tennis
serve. It shows that the decision values of SVM are quite noisy and there are a large
number of false positives for the attribute bend and false negatives for the attribute one arm
swing. The LCRF, which only considers temporal constraints, is only able to smooth out
noisy detections that last for short periods but is unable to deal with persistent false positives. On the other hand, FCRF corrects many of such errors by using semantic contextual
constraints: stand still and bend are less likely to co-occur while bend and one arm swing
are more likely to co-occur.

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

(a)

9

(b)

Figure 3: (a) Overall performance of action attribute detection and (b) precision of action
attribute detection with three algorithms with three algorithms: SVM, linear CRF (LCRF)
and factorial CRF (FCRF)

4

Conclusion and Future Work

In this paper, we study the problem of detecting action attributes from sport videos. Although
the concept of action attributes has been discussed in previous work, to our knowledge, there
does not exist an action attribute detection method that report presence/absence of attributes
at the frame level. Our work not only answers the question “is there an action attribute
a in this video?”, but also addresses the question “when does this attribute occur?”. The
attribute annotation we propose at this granularity will benefit lots of interesting applications
in video understanding and event detection. In this work, we proposed an approach that
uses contextual constraints as post-processing to an off-the-shelf discriminative model for
attribute detection. We observed that semantic context, i.e. the co-occurrence of attributes is
an important constraint that can compensates for the ambiguity that arises from noisy video
features. As a result, our approach is able to produce attribute labels that maximizes the
agreement among labels at different time points and different attributes.
In our ongoing work, we are exploring more action attributes, such as the scene (e.g.
swimming pool, track) and objects (e.g. basketball/tennis ball, pole/javelin). We will also
incorporate more contextual constraints into our model. In particular, we are interested in
the absolute and relative temporal order of attributes. An example for the absolute temporal
order is in high jump: the athlete must first do a slow run, then jump forward with one
leg, followed by jumping up and moving up in the air, and finally move down in the air; an
example for the relative temporal order for high jump is that slow run must occur before
the other attributes, moving up in the air must occur before move down in the air, etc. We
believe these two types of temporal orders are both strong contextual cues that will facilitate
the detection of action attributes more accurately.

References
[1] Lubomir Bourdev, Subhransu Maji, Thomas Brox, and Jitendra Malik. Detecting People Using Mutually Consistent Poselet Activations. In European Conference on Computer Vision (ECCV), 2010.
[2] Navneet Dalal and Bill Triggs. Histograms of Oriented Gradients for Human Detection.

10

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

Figure 4: Detailed detection results of the activity tennis serve, where imposing contextual constraints in the FCRF improves the attribute detection precision compared to the
other two baseline approaches: SVM and LCRF.
In Cordelia Schmid, Stefano Soatto, and Carlo Tomasi, editors, CVPR, volume 2, pages
886–893, June 2005.
[3] Liam Ellis, Nicholas Dowson, Jiri Matas, and Richard Bowden. Linear Regression and
Adaptive Appearance Models for Fast Simultaneous Modelling and Tracking. IJCV,
95:154–179, 2011.
[4] Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing Objects by Their
Attributes. In CVPR, 2009.
[5] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with
discriminatively trained part based models. PAMI, 32, 2010.
[6] V. Ferrari and A. Zisserman. Learning Visual Attributes. In NIPS, December 2007.
[7] John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional Random
Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML,
pages 282–289, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
[8] H. Nickisch Lampert, C. H. and S. Harmeling. Learning To Detect Unseen Object
Classes by Between-Class Attribute Transfer. In CVPR, 2009.
[9] Jingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing Human Actions by
Attributes. In CVPR, 2011.
[10] S. M. Shahed Nejhum, Jeffrey Ho, , and Ming-Hsuan Yang. Online visual tracking
with histograms and articulating blocks. Computer Vision and Image Understanding
(CVIU), 114:901–914, 2010.
[11] Juan Carlos Niebles, Chih-Wei Chen, and Li Fei-Fei. Modeling Temporal Structure of
Decomposable Motion Segments for Activity Classification. In ECCV, 2010.
[12] Vishwanathan Schraudolph and Schmidt Murphy. Accelerated Training of Conditional
Random Fields with Stochastic Gradient Methods. In ICML, 2006.
[13] Cristian Sminchisescu, Atul Kanaujia, and Dimitris Metaxas. Conditional Models for
Contextual Human Motion Recognition. CVIU, 104:210–220, 2006.

YU, ET AL: ACTION ATTRIBUTE DETECTION FROM SPORTS VIDEOS

11

[14] Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Research, 8:693–723, 2007.
[15] Heng Wang, Muhammad Muneeb Ullah, Alexander Kläser, Ivan Laptev, and Cordelia
Schmid. Evaluation of local spatio-temporal features for action recognition. In BMVC,
2009.
[16] Xiaodong Yu and Yiannis Aloimonos. Attribute-based Transfer Learning for Object
Categorization with Zero or One Training Example. In ECCV, 2010.
[17] Jianguo Zhang, Marcin Marszalek, Svetlana Lazebnik, and Cordelia Schmid. Local
Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study. IJCV, 73(2):213–238, 2007.

2013 IEEE Conference on Computer Vision and Pattern Recognition

Detection of Manipulation Action Consequences (MAC)
Yezhou Yang

Cornelia Fermüller

Yiannis Aloimonos

yzyang@cs.umd.edu

fer@umiacs.umd.edu

yiannis@cs.umd.edu

Computer Vision Lab, University of Maryland, College Park, MD 20742, USA
Abstract

the movement of the body (especially the hands) is not a
very good characteristic feature. There is great variability in
the way humans carry out such actions. It has been realized
that such actions are better described by involving a number
of quantities. Besides the motion trajectories, the objects involved, the hand pose, and the spatial relations between the
body and the objects under inﬂuence, provide information
about the action. In this work we want to bring attention
to another concept, the action consequence. It describes the
transformation of the object during the manipulation. For
example during a CUT or a SPLIT action an object is divided into segments, during a GLUE or a MERGE action
two objects are combined into one, etc.
The recognition and understanding of human manipulation actions recently has attracted the attention of Computer
Vision and Robotics researchers because of their critical
role in human behavior analysis. Moreover, they naturally
relate to both, the movement involved in the action and the
objects. However, so far researchers have not considered
that the most crucial cue in describing manipulation actions
is actually not the movement nor the speciﬁc object under
inﬂuence, but the object centric action consequence. We can
come up with examples, where two actions involve the same
tool and same object under inﬂuence, and the motions of the
hands are similar, for example in “cutting a piece of meat”
vs. “poking a hole into the meat”. Their consequences are
different. In such cases, the action consequence is the key
in differentiating the actions. Thus, to fully understand manipulation actions, the intelligent system should be able to
determine the object centric consequences.
Few researchers have addressed the problem of action
consequences due to the difﬁculties involved. The main
challenge comes from the monitoring process, which calls
for the ability to continuously check the topological and appearance changes of the object-under-manipulation. Previous studies of visual tracking have considered challenging situations, such as non-rigid objects [8], adaptive appearance model [12], and tracking of multiple objects with
occlusions [24], but none can deal with the difﬁculties involved in detecting the possible changes on objects during
manipulation. In this paper, for the ﬁrst time, a system

The problem of action recognition and human activity
has been an active research area in Computer Vision and
Robotics. While full-body motions can be characterized
by movement and change of posture, no characterization,
that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the
consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classiﬁcation of manipulation actions. In
this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a
novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in
a visual semantic graph (VSG) based procedure applied
to the time sequence of the monitored object to recognize
the action consequence. We provide a new dataset, called
Manipulation Action Consequences (MAC 1.0), which can
serve as testbed for other studies on this topic. Several experiments on this dataset demonstrates that our method can
robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the
effectiveness and efﬁciency of the method.

1. Introduction
Visual recognition is the process through which intelligent agents associate a visual observation to a concept from
their memory. In most cases, the concept either corresponds
to a term in natural language, or an explicit deﬁnition in natural language. Most research in Computer Vision has focused on two concepts: objects and actions; humans, faces
and scenes can be regarded as special cases of objects. Object and action recognition are indeed crucial since they are
the fundamental building blocks for an intelligent agent to
semantically understand its observations.
When it comes to understanding actions of manipulation,
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.331

2561
2563

sponded more during the observation of goal-directed actions than similar movements not directed at goals. These
evidences support the idea of goal matching, as well as the
crucial role of action consequence in the understanding of
manipulation actions.
Taking an object-centric point of view, manipulation actions can be classiﬁed into six categories according how the
object is transformed during the manipulation, or in other
words what consequence the action has on the object. These
categories are: DIVIDE, ASSEMBLE, CREATE, CONSUME, TRANSFER, and DEFORM. Each of these categories is denoted by a term that has a clear semantic meaning in natural language given as follows:
• DIVIDE: one object breaks into two objects, or two attached objects break the attachment;
• ASSEMBLE: two objects merge into one object, or two
objects build an attachment between them;
• CREATE: an object is brought to, or emerges in the visual
space;
• CONSUME: an object disappears from the visual space;
• TRANSFER: an object is moved from one location to
another location;
• DEFORM: an object has an appearance change.
To describe these action categories we need a formalism.
We use the visual semantic graph (VSG) inspired from the
work of Aksoy et. al [1]. This formalism takes as input
computed object segments, their spatial relationship, and
temporal relationship over consecutive frames. To provide
the symbols for the VSG, an active monitoring process (discussed in sec. 4) is required for the purpose of (1) tracking
the object to obtain temporal correspondence, and (2) segmenting the object to obtain its topological structure and
appearance model. This active monitoring (consisting of
segmentation and tracking) is related to studies on active
segmentation [20], and stochastic tracking ([11] etc.).

is implemented to conquer these difﬁculties and eventually
achieve robust action consequence detection.

2. Why Consequences and Fundamental Types
Recognizing human actions has been an active research
area in Computer Vision [10]. Several excellent surveys
on the topic of visual recognition are available ([21], [29]).
Most work on visual action analysis has been devoted to the
study of movement and change of posture, such as walking, running etc. The dominant approaches to the recognition of single actions compute as descriptors statistics of
spatio-temporal interest points ([16], [31]) and ﬂow in video
volumes, or represent short actions by stacks of silhouettes
([4], [34]). Approaches to more complex, longer actions
employ parametric approaches, such as Hidden Markov
Models [13], Linear Dynamical Systems [26] or Non-linear
Dynamical Systems [7], which are deﬁned on extracted features. There are a few recent studies on human manipulation
actions ([30], [14], [27]), but they do not consider action
consequences for the interpretation of manipulation actions.
Works like [33] emphasize the role of object perception in
action or pose recognition, but they focus on object labels,
not object-centric consequences.
How do humans understand, recognize, and even replicate manipulation actions? Psychological studies on human
manners ([18] etc.) have pointed out the importance of manipulation action consequences for both understanding human cognition and intelligent system research. Actions, by
their very nature, are goal oriented. When we perform an
action, we always have a goal in mind, and the goal affects
the action. Similarly, when we try to recognize an action,
we also keep a goal in mind. The close relation between
the movement during the action and goal is reﬂected also
in language. For example, the word “CUT” denotes both
the action in which hands move up and down or in and out
with sharp bladed tools, and the consequence of the action,
namely that the object is separated. Very often, we can recognize an action purely by the goal satisfaction, and even
neglect the motion or the tools used. For example, we may
observe a human carry out movement with a knife, that is
”up and down”, but if the object remains as one whole, we
won’t draw the conclusion that a “CUT” action has been
performed. Only when the goal of the recognition process,
here “DIVIDE”, is detected, the goal satisfaction is reached
and a “CUT” action is conﬁrmed. An intelligent system
should have the ability to detect the consequences of manipulation actions, in order to check the goal of actions.
In addition, experiments conducted in neuronscience
[25] show that a monkey’s mirror neuron system ﬁres when
a hand/object interaction is observed, and it will not ﬁre
when a similar movement is observed without hand/object
interaction. Recent experiments [9] further showed that the
mirror neuron regions responding to the sight of actions re-

3. Visual Semantic Graph (VSG)
To deﬁne object-centric action consequences, a graph
representation is used. Every frame in the video is described by a Visual Semantic Graph (VSG), which is represented by an undirected graph G(V, E, P ). The vertex set
|V | represents the set of semantically meaningful segments,
the edge set |E| represents the spatial relations between any
of the two segments. Two segments are connected when
they share parts of their borders, or when one of the segments is contained in the other. If two nodes v1 , v2 ∈ V are
connected, E(v1 , v2 ) = 1, otherwise, E(v1 , v2 ) = 0. In addition, every node v ∈ V is associated with a set of properties P (v), that describes the attributes of the segment. This
set of properties provides additional information to discriminate the different categories, and in principle many properties are possible. Here we use location, shape, and color.
We need to compute the changes of the object over time.

2564
2562

In our formulation this is expressed as the change in the
VSG. At any time instance t, we consider two consecutive
VSGs, the VSG at time t − 1, denoted as Ga (Va , Ea , Pa )
and the VSG at time t, denoted as Gz (Vz , Ez , Pz ). We then
deﬁne the following four consequences, where → is used to
denote the temporal correspondence between two vertices,
 is used to denote no correspondence:
• DIVIDE: {∃v1 ∈ Va ; v2 , v3 ∈ Vz |v1 → v2 , v1 →
v3 )} or {∃v1 , v2 ∈ Va ; v3 , v4 ∈ Vz |Ea (v1 , v2 ) =
1, Ez (v3 , v4 ) = 0, v1 → v3 , v2 → v4 } Condition (1)
• ASSEMBLE: {∃v1 , v2 ∈ Va ; v3 ∈ Vz |v1 → v3 , v2 →
v3 } or {∃v1 , v2 ∈ Va ; v3 , v4 ∈ Vz |Ea (v1 , v2 ) =
0, Ez (v3 , v4 ) = 1, v1 → v3 , v2 → v4 } Condition (2)
• CREATE:{∀v ∈ Va ; ∃v1 ∈ Vz |v  v1 } Condition (3)
• CONSUME:{∀v ∈ Vz ; ∃v1 ∈ Va |v  v1 } Condition(4)
While the above actions can be deﬁned purely on the basis of topological changes, there are no such changes for
TRANSFER and DEFORM. Therefore, we have to deﬁne
them through changes in property. In the following deﬁnitions, P L represents properties of location, and P S represents properties of appearance (shape, color, etc.).
• TRANSFER:{∃v1 ∈ Va ; v2 ∈ Vz |PaL (v1 ) = PzL (v2 )}
Condition (5)
• DEFORM: {∃v1 ∈ Va ; v2 ∈ Vz |PaS (v1 ) = PzS (v2 )}
Condition (6)

Figure 1: Graphical illustration of the changes for Condition (1-6).
A graphical illustration for Condition (1-6) is shown in
Fig. 1. Sec. 4 describes how we ﬁnd the primitives used in
the graph. A new active segmentation and tracking method
is introduced to 1) ﬁnd correspondences (→) between Va
and Vz ; 2) monitor location property P L and appearance
property P S in the VSG.
The procedure for computing action consequences, ﬁrst
decides on whether there is a topological change between
Ga and Gz . If yes, the system checks whether Condition
(1) to Condition (4) are fulﬁlled and returns the corresponding consequence. If no, the system then checks whether
Condition (5) or Condition (6) is fulﬁlled. If both of them
are not met, no consequence is detected.

4. Active Segmentation and Tracking
Previously, researchers have treated segmentation and
tracking as two different problems. Here we propose a new
method combining the two tasks to obtain the information
necessary to monitor the objects under inﬂuence. Our methods combines stochastic tracking [11] with a ﬁxation based

active segmentation [20]. The tracking module provides a
number of tracked points. The locations of these points are
used to deﬁne an area of interest and a ﬁxation point for the
segmentation, and the color in their immediate surroundings
are used in the data term of the segmentation module. The
segmentation module segments the object, and based on the
segmentation, updates the appearance model for the tracker.
Fig 2 illustrates the method over time, which is a dynamically closed-loop process. We next describe the attention
based segmentation (sec. 4.1 - 4.4), and then the segmentation guided tracking (sec. 4.5).

Figure 2: Flow chart of the proposed active segmentation
and tracking method for object monitoring.
The proposed method meets two challenging requirements, necessary to detect action consequences: 1) the system is able to track and segment objects when the shape
or color (appearance) changes; 2) the system is also able to
track and segment objects when they are divided into pieces.
Experiments in sec. 5.1 show that our method can handle
these requirements, while systems implementing independently tracking and segmentation cannot.

4.1. The Attention Field
The idea underlying our approach is, that ﬁrst a process
of visual attention selects an area of interest. Segmentation then is considered the process that separates the area
selected by visual attention from background by ﬁnding
closed contours that best separate the regions. The minimization uses a color model for the data term and edges
in the regularization term. To achieve a minimization that
is very robust to the length of the boundary, edges are
weighted with their distance from the ﬁxation center.
Visual attention, the process of driving an agent’s attention to a certain area, is based on both bottom-up processes
deﬁned on low level visual features, and top-down processes inﬂuenced by the agent’s previous experience [28].
Inspired by the work of Yang et al. [32], instead of using a
single ﬁxation point in the active segmentation [20], here we
use a weighted sample set S = {(s(n) , π (n) )|n = 1 . . . N }

2565
2563

to represent the attention ﬁeld around the ﬁxation point
(N = 500 in practice). Each sample consists of an element s from the set of tracked
N points and a corresponding
discrete weight π where n=1 π (n) = 1.
Generally, any appearance model can be used to represent the local visual information around each point. We
choose to use a color histogram with a dynamic sampling
area deﬁned by an ellipse. To compute the color distribution, every point is represented by an ellipse, s =
{x, y, ẋ, ẏ, Hx , Hy , H˙x , Ḣy , } where x and y denote the location, ẋ and ẏ the motion, Hx , Hy the length of the half
axes, and H˙x , Ḣy the changes in the axes.

4.2. Color Distribution Model
To make the color model invariant to various textures
or patterns, a color distribution model is used. A function
h(xi ) is deﬁned to create a color histogram, which assigns
one of the m-bins to a giving color at location xi . To make
the algorithm less sensitive to lighting conditions, the HSV
color space is used with less sensitivity in the V channel
(8 × 8 × 4 bins). The color distribution for each ﬁxation
point s(n) is computed as:
p(s(n) )(u) = γ

I


k(||y − xi ||)δ[h(xi ) − u],

(1)

i=1

where u = 1 . . . m, δ(.) is the Kronecker delta function,
1
and γ is the normalization term γ = I k(||y−x
. k(.)
i ||)
i=1
is a weighting function designed from the intuition that not
all pixels in the sampling region are equally important for
describing the color model. Speciﬁcally, pixels that are
farther away
the point are assigned smaller weights,
 from
1 − r2 if r < a
, where the parameter a is
k(r) =
0
otherwise
used to adapt the size of the region, and r is the distance
from the ﬁxation point. By applying the weighting function, we increase the robustness of the color distribution by
weakening the inﬂuence from boundary pixels, which possibly belong to the background or are occluded.

4.3. Weights of the Tracked Point Set
In the following weighted graph cut approach, every
sample is weighted by comparing its color distribution with
the one of the ﬁxation point. Initially a ﬁxation point is selected, later the ﬁxation point is computed as the center of
the tracked point set. Let’s call the distribution at the ﬁxation point q, and the histogram of the nth tracked point,
p(s(n) ). In assigning weights π (n) to the tracked points we
want to favor points whose color distribution is similar to
the ﬁxation point.
We use the Bhattacharyya coefﬁcient
m  (u) (u)
ρ[p, q] = u=1 p q
with m the number of bins to
weigh points by a Gaussian with variance σ (σ = 0.2 in

practice) and deﬁne π (n) as:
π (n) = √

1−ρ[p(s(n) ),q]
d2
1
1
2σ 2
.
e− 2σ2 = √
e−
2πσ
2πσ

(2)

4.4. Weighted Graph Cut
The segmentation is formulated as a minimization that
is solved using graph cut. The unary terms are deﬁned on
the tracked points on the basis of their color, and the binary
terms are deﬁned on all points on the basis of edge information. To obtain the edge information, in each frame, we
compute a probabilistic edge map IE using the Canny edge
detector. Consider every pixel x ∈ X in this edge map as
a node in a graph. Denoting the set of all the edges connecting neighboring nodes in the graph as Ω, and using the
label set l = 0, 1 to indicate whether a pixel x is “inside”
(lx = 0) or “outside” (lx = 1), we need to ﬁnd a labeling
f (X) −→ l , that minimizes the energy function:
Q(f ) =


x∈X

Ux (lx ) + λ



Vx,y δ(lx , ly ).

(3)

(x,y)∈Ω

Vx,y is the cost of assigning different labels to neighboring pixels x and y,which we deﬁnes as Vx,y = e−ηIE,xy +k,
1 if lx = ly
with δ(lx , ly ) =
, λ = 1, η = 1000, k =
0 otherwise
−16
10 , IE,xy = (IE (x)/Rx + IE (y)/Ry )/2, Rx , Ry are
the euclidean distances between the x, y and the center of
the tracked point set St . We use them as weights to make
the segmentation robust to the length of the contours.
Ux (lx ) is the cost of assigning label lx to pixel x. In
our system, we have a set of points St , and for each sample
s(n) , there is a weight π (n) . The weight itself indicates the
likelihood that the area around that ﬁxation point belongs
to the “inside” of the object. It becomes straightforward
to assign weights π (n) to the 
pixel s(n) , which are tracked
N π (n) if lx = 1
points as follows: Ux (lx ) =
. We
0
otherwise
assume that pixels on the boundary of a frame are “outside”
10
of the object,
 and assign to them a large weight W = 10 :
W if lx = 0
Ux (lx ) =
. Using this formulation, we
0
otherwise
run a graph cut algorithm [5] on each frame. Fig. 3a illustrates the procedure on a texture-rich natural image from the
Berkeley segmentation dataset [19].
Two critical limitations of the previous active segmentation method [20] in practice are: 1) the segmentation performance largely varies under different initial ﬁxation points;
2) the segmentation performance also is strongly affected
by texture edges, which often leads to a segmentation of
object parts. Fig. 3b shows that our proposed segmentation
method is robust to the choice of initial ﬁxation point, and
only weakly affected by texture edges.

2566
2564

4.6. Incorporating Depth and Optical Flow
It is easy to extend our algorithm to incorporate depth
(for example from Kinect) or image motion ﬂow information. Depth information can be used in a straightforward
way during two crucial steps. 1) As described in sec. 4.2,
we can add in depth information as another channel in the
distribution model. In preliminary experiments we used 8
bins for the depth, to obtain in RGBD space a model with
8 × 8 × 4 × 8 bins. 2) Depth can be used to achieve cleaner
edge maps, IE , in the segmentation step 4.4.
Optical ﬂow can be incorporated to provide cues for the
system to predict the movement of edges to be used for the
segmentation step in the next iteration, and the movement of
the points in the tracking step. We performed some experiments using the optical ﬂow estimation method proposed
by Brox [6] and the improved implementation by Liu [17].
Optical ﬂow was used in the segmentation by ﬁrst predicting the contour of the object in the next frame, and then
fusing it with the next frame’s edge map. Fig. 4a shows an
example of an edge map improved by optical ﬂow. Optical
ﬂow was incorporated into tracking by replacing the ﬁrst order velocity components for each tracked point in matrix A
(eq. 4) by its ﬂow component. Fig. 4b shows that the optical ﬂow drives the tracked points to move along the ﬂow
vectors into the next frame.

Figure 3: Upper: (1) Sampling of tracked points sampling
and ﬁltering; (2) Weighted graph cut. Lower: Segmentation
with different initial ﬁxations. Green Cross: initial ﬁxation.

4.5. Active Tracking
At the very beginning of the monitoring process, a Gaussian sampling with mean at the initial ﬁxation point and
variances σx , σy is used to generate the initial point set S0 .
When a new frame comes in, the point set is propagated
through a stochastic tracking paradigm:
st = Ast−1 + wt−1 ,

(4)

where A denotes the deterministic, and wt−1 the stochastic
component. In our implementation, we have considered a
ﬁrst order model for A, which assumes that the object is
moving with constant velocity. The reader is referred to [23]
for details. The complete algorithm is given in Algorithm 1

Algorithm 1 Active tracking and segmentation
Require: Given the tracked point set St−1 and the target
model qt−1 , perform the following steps:
1. SELECT N samples from the set St−1 with proba(n)
bility πt−1 . Fixation points with a high weight may be
chosen several times, leading to identical copies, while
others with relatively low weights may not be chosen at

all. Denote the resulting set as St−1
;

by a linear
2. PROPAGATE each sample from St−1
stochastic differential eq. 4. Denote the new set as St
3. OBSERVE the color distributions for each sample of
St using eq. 1. Weigh each sample using eq. 2.
4. SEGMENTATION using the weighted sample set.
Apply the weighted graph cut algorithm described in
sec. 4.4. and get the segmented object area M .
5. UPDATE the target distribution qt−1 by the area M to
achieve the new target distribution qt .

Figure 4: (a): Incorporating optical ﬂow into segmentation.
(b): Incorporating optical ﬂow into tracking.

5. Experiments
5.1. Deformation and Division
To show that our method can segment challenging cases,
we ﬁrst demonstrate its performance for the case of de-

2567
2565

forming and dividing objects. Fig. 5a shows results for a
sequence with the main object deforming, and Fig. 5b for
a synthetic sequence with the main object dividing. The
ability to handle deformations comes from the updating of
the target model using the segmentation of previous frames.
The ability to handle division comes from the tracked point
set that is used to represent the attention ﬁeld (sec. 4.1),
which guides the weighted graph cut algorithm (sec. 4.4).

Figure 5: (a): Deformation Invariance: upper: state-of-theart appearance based tracking [2]; middle: tracking without
updating target model; lower: updating target model. (b):
Division Invariance: synthetic cell division sequence.

5.2. The MAC 1.0 Dataset
To quantitatively test our method, we collected a dataset
of several RGB+Depth image sequences of humans performing different manipulation actions. In addition, several
sequences from other publicly available datasets ([1], [22]
and [15]) were included to increase the variability and make
it more challenging. Since the two action consequences
CREATE and CONSUME (sec.2) relate to the existence of
the object and would require a higher level attention mechanism, which is out of this paper’s scope, we did not consider
them. For the other four consequences, six sequences were
collected each to make the ﬁrst Manipulation Action Consequence (MAC 1.0) dataset.1 .

5.3. Consequence Detection on MAC 1.0
We ﬁrst evaluated the method’s ability in detecting the
various consequences. Consequences happen in an event
based manner. In our description, a consequence is detected
1 The

dataset is available at www.umiacs.umd.edu/˜yzyang.

using the VSG graph at a point in time, if between two consecutive image frames one of the conditions listed in sec. 3
is met. For example, a consequence is detected for the case
of DIVIDE, when one segment becomes two segments in
the next frame (Fig. 6), or for the case of DEFORM, when
one appearance model changes to another (Fig. 8). We obtained ground truth by asking people not familiar with the
purpose to label the sequences in MAC 1.0.
Fig. 6, 7, 8 show typical example active segmentation
and tracking, the VSG graph, and the corresponding measures used to identify the different action consequences, as
well as the ﬁnal detection result along the time-line are illustrated. Speciﬁcally, for DIVIDE we monitor the change
in the number of segments, for ASSEMBLE we monitor the
minimum Euclidean distance between the contours of segments, for DEFORM we monitor the change of appearance
(color histogram and shape context [3]) of the segment, and
for TRANSFER we monitor the velocity of the object. Each
of the measurements is normalized to the range of [0, 1] for
the ROC analysis. The detection is evaluated, by counting
the correct detections over the sequence. For example, for
the case of DIVIDE, at any point in time we have either the
detection, “not divided” or “divided”. For the case of ASSEMBLE , we have either the detection “two parts assembled” or “nothing assembled”, and for DEFORM, we have
either “deformed” or “nor deformed”. The ROC curves
obtained are shown in Fig. 9. The graphs indicate that
our method is able to correctly detect most of the consequences. Several failures point out the limitations of our
method as well. For example, for the PAPER-JHU sequence
the method has errors in detecting DIVIDE, because the part
that was cut out, connects visually with the rest of the paper. For the CLOSE-BOTTLE sequence our method fails
for ASSEMBLE because the small bottle cap is occluded
by the hand. However, our method detects that an ASSEMBLE event happened after the hand move away.

5.4. Video Classiﬁcation on MAC 1.0
We also evaluated our method on the problem of classiﬁcation, although the problem of consequence detection
is quite different from the problem of video classiﬁcation.
We compared our method with the state-of-the-art STIP +
Bag of Words + classiﬁcation (SVM or Naive Bayes). The
STIP features for each video in the MAC 1.0 dataset were
computed using the method described in [16]. For classiﬁcation we used a bag of words + SVM and a Naive Bayes
method. The dictionary codebook size was 1000, and a
polynomial kernel SVM with a leave-one-out cross validation setting was used. Fig. 10 shows that our method dramatically outperforms the typical STIP + Bag of Words +
SVM and the Naive Bayes learning methods. However, this
does not come as a surprise. The different video sequences
in an action consequence class contain different objects and

2568
2566

Figure 9: ROC curve of each sequence by categories: (a) TRANSFER, (b) DEFORM, (c) DIVIDE, and (d) ASSEMBLE.

Figure 8: “Deformation” detection on “close book 1” sequence; 1st row: Original sequence with segmentation and
tracking; 2nd row: VSG representation; 3rd row: appearance description (here color histogram) of each segment;
4th row: measurement of appearance change; 5th row: Deformation consequence detection.

Figure 6: “Division” detection on “cut cucumber” sequence. Upper row: Original sequence with segmentation
and tracking; Middle and lower right: VSG representations;
Lower left: Division consequences detection.

therefore not well suited for standard classiﬁcation. On the
other hand, our method has been speciﬁcally designed for
the detection of manipulation action consequences all the
way from low-level signal processing through the mid-level
semantic representation to high-level reasoning. Moreover,
different from a learning based method, it does not rely on
training data. After all, the method stems from the insight
of manipulation action consequences.

Figure 7: “Assemble” detection on “make sandwich 1” sequence; 1st row: Original sequence with segmentation and
tracking; 2nd row: VSG representation; 3rd row: Distance
between each two segments (red line: bread and cheese,
magenta line: bread and meat, blue line: cheese and meat;
4th row: Assemble consequence detection.
Figure 10: Video classiﬁcation performance comparison.
different actions and thus different visual features, and are

2569
2567

6. Discussion and Future Works

[8] D. Comaniciu, V. Ramesh, and P. Meer. Real-time tracking of non-rigid objects
using mean shift. In CVPR, volume 2, pages 142–149, 2000. 1

A system for detecting action consequences and classifying videos of manipulation action according to action consequences has been proposed. A dataset has been provided,
which includes both data that we collected and eligible manipulation action video sequences from other publicly available datasets. Experimental results were performed that validate our method, and at the same time point out several
weaknesses for future improvement.
For example, to avoid the inﬂuence from the manipulating hands, especially occlusions caused by hands, a hand
detection and segmentation algorithm can be applied. Then
we can design a hallucination process to complete the contour of the occluded object under manipulation. Preliminary results are shown in Fig. 11. However, resolving the
ambiguity between occlusion and deformation from visual
analysis is a difﬁcult task that requires further attention.

[9] V. Gazzola, G. Rizzolatti, B. Wicker, and C. Keysers. The anthropomorphic
brain: the mirror neuron system responds to human and robotic actions. Neuroimage, 35(4):1674–1684, 2007. 2
[10] G. Guerra-Filho, C. Fermuller, and Y. Aloimonos. Discovering a language for
human activity. In Proceedings of the AAAI 2005 fall symposium on anticipatory cognitive embodied systems, Washington, DC, 2005. 2
[11] B. Han, Y. Zhu, D. Comaniciu, and L. Davis. Visual tracking by continuous
density propagation in sequential bayesian ﬁltering framework. PAMI, IEEE
Transactions on, 31(5):919–930, 2009. 2, 3
[12] A. Jepson, D. Fleet, and T. El-Maraghi. Robust online appearance models for
visual tracking. Pattern Analysis and Machine Intelligence, IEEE Transactions
on, 25(10):1296–1311, 2003. 1
[13] A. Kale, A. Sundaresan, A. Rajagopalan, N. Cuntoor, A. Roy-Chowdhury,
V. Kruger, and R. Chellappa. Identiﬁcation of humans using gait. Image Processing, IEEE Transactions on, 13(9):1163–1173, 2004. 2
[14] H. Kjellström, J. Romero, D. Martı́nez, and D. Kragić. Simultaneous visual
recognition of manipulation actions and manipulated objects. ECCV, pages
336–349, 2008. 2
[15] K. Lai, L. Bo, X. Ren, and D. Fox. A large-scale hierarchical multi-view rgb-d
object dataset. In ICRA, pages 1817–1824. IEEE, 2011. 6
[16] I. Laptev. On space-time interest points. International Journal of Computer
Vision, 64(2):107–123, 2005. 2, 6
[17] C. Liu et al. Beyond pixels: exploring new representations and applications for
motion analysis. PhD thesis, MIT, 2009. 5
[18] E. Locke and G. Latham. A theory of goal setting & task performance. PrenticeHall, Inc, 1990. 2
[19] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented
natural images and its application to evaluating segmentation algorithms and
measuring ecological statistics. In ICCV, volume 2, pages 416–423, 2001. 4

Figure 11: A hallucination process of contour completion
(paint stone sequence in MAC 1.0). Left: original segments;
Middle: contour hallucination with second order polynomials ﬁtting (green lines); Right: ﬁnal hallucinated contour.

[20] A. Mishra, Y. Aloimonos, and C. Fermuller. Active segmentation for robotics.
In Intelligent Robots and Systems, 2009. IROS 2009. IEEE/RSJ International
Conference on, pages 3133–3139. IEEE, 2009. 2, 3, 4
[21] T. Moeslund, A. Hilton, and V. Krüger. A survey of advances in vision-based
human motion capture and analysis. Computer vision and image understanding,
104(2):90–126, 2006. 2
[22] J. Neumann and etc. Localizing objects and actions in videos with the help of
accompanying text. Final Report, JHU summer workshop, 2010. 6

7. Acknowledgements
The support of the European Union under the Cognitive
Systems program (project POETICON++) and the National
Science Foundation under the Cyberphysical Systems Program is gratefully acknowledged. Yezhou Yang has been
supported in part by the Qualcomm Innovation Fellowship.

References
[1] E. Aksoy, A. Abramov, J. Dörr, K. Ning, B. Dellen, and F. Wörgötter. Learning the semantics of object–action relations by observation. The International
Journal of Robotics Research, 30(10):1229–1249, 2011. 2, 6
[2] C. Bao, Y. Wu, H. Ling, and H. Ji. Real time robust l1 tracker using accelerated proximal gradient approach. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 1830–1837. IEEE, 2012. 6
[3] S. Belongie, J. Malik, and J. Puzicha. Shape context: A new descriptor for
shape matching and object recognition. Advances in neural information processing systems, pages 831–837, 2001. 6

[23] K. Nummiaro, E. Koller-Meier, L. Van Gool, et al. A color-based particle ﬁlter.
In First International Workshop on Generative-Model-Based Vision, volume
2002, page 01, 2002. 5
[24] V. Papadourakis and A. Argyros. Multiple objects tracking in the presence of
long-term occlusions. Computer Vision and Image Understanding, 114(7):835–
846, 2010. 1
[25] G. Rizzolatti, L. Fogassi, and V. Gallese. Neurophysiological mechanisms underlying the understanding and imitation of action. Nature Reviews Neuroscience, 2(9):661–670, 2001. 2
[26] P. Saisan, G. Doretto, Y. Wu, and S. Soatto. Dynamic texture recognition. In
CVPR, volume 2, pages II–58, 2001. 2
[27] M. Sridhar, A. Cohn, and D. Hogg. Learning functional object-categories from
a relational spatio-temporal representation. In ECAI, pages 606–610, 2008. 2
[28] J. Tsotsos. Analyzing vision at the complexity level. Behavioral and brain
sciences, 13(3):423–469, 1990. 3
[29] P. Turaga, R. Chellappa, V. Subrahmanian, and O. Udrea. Machine recognition
of human activities: A survey. Circuits and Systems for Video Technology, IEEE
Transactions on, 18(11):1473–1488, 2008. 2
[30] I. Vicente, V. Kyrki, D. Kragic, and M. Larsson. Action recognition and understanding through motor primitives. Advanced Robotics, 21(15):1687–1707,
2007. 2

[4] M. Blank, L. Gorelick, E. Shechtman, M. Irani, and R. Basri. Actions as spacetime shapes. In ICCV, volume 2, pages 1395–1402, 2005. 2

[31] G. Willems, T. Tuytelaars, and L. Van Gool. An efﬁcient dense and scaleinvariant spatio-temporal interest point detector. ECCV, pages 650–663, 2008.
2

[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/maxﬂow algorithms for energy minimization in vision. PAMI, IEEE Transactions
on, 26(9):1124–1137, 2004. 4

[32] Y. Yang, M. Song, N. Li, J. Bu, and C. Chen. Visual attention analysis by
pseudo gravitational ﬁeld. In ACM MM, pages 553–556. ACM, 2009. 3

[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical ﬂow
estimation based on a theory for warping. ECCV, pages 25–36, 2004. 5
[7] R. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal. Histograms of oriented
optical ﬂow and binet-cauchy kernels on nonlinear dynamical systems for the
recognition of human actions. In CVPR, pages 1932–1939, 2009. 2

[33] B. Yao and L. Fei-Fei. Modeling mutual context of object and human pose in
human-object interaction activities. In CVPR, pages 17–24, 2010. 2
[34] A. Yilmaz and M. Shah. Actions sketch: A novel action representation. In
CVPR, volume 1, pages 984–989, 2005. 2

2570
2568

Corpus-Guided Sentence Generation of Natural Images
Yezhou Yang † and Ching Lik Teo † and Hal Daumé III and Yiannis Aloimonos
University of Maryland Institute for Advanced Computer Studies
College Park, Maryland 20742, USA
{yzyang, cteo, hal, yiannis}@umiacs.umd.edu

Abstract
We propose a sentence generation strategy
that describes images by predicting the most
likely nouns, verbs, scenes and prepositions
that make up the core sentence structure. The
input are initial noisy estimates of the objects
and scenes detected in the image using state of
the art trained detectors. As predicting actions
from still images directly is unreliable, we use
a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns,
scenes and prepositions. We use these estimates as parameters on a HMM that models
the sentence generation process, with hidden
nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.

1

Figure 1: The processes involved for describing a scene.

Introduction

What happens when you see a picture? The most
natural thing would be to describe it using words:
using speech or text. This description of an image is
the output of an extremely complex process that involves: 1) perception in the Visual space, 2) grounding to World Knowledge in the Language Space and
3) speech/text production (see Fig. 1). Each of these
components are challenging in their own right and
are still considered open problems in the vision and
linguistics fields. In this paper, we introduce a computational framework that attempts to integrate these
†

indicates equal contribution.

components together. Our hypothesis is based on
the assumption that natural images accurately reflect
common everyday scenarios which are captured in
language. For example, knowing that boats usually
occur over water will enable us to constrain the
possible scenes a boat can occur and exclude highly
unlikely ones – street, highway. It also enables us to predict likely actions (Verbs) given the
current object detections in the image: detecting a
dog with a person will likely induce walk rather
than swim, jump, fly. Key to our approach is
the use of a large generic corpus such as the English
Gigaword [Graff, 2003] as the semantic grounding
to predict and correct the initial and often noisy visual detections of an image to produce a reasonable
sentence that succinctly describes the image.
In order to get an idea of the difficulty of this
task, it is important to first define what makes up

444
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454,
c
Edinburgh, Scotland, UK, July 27–31, 2011. 
2011
Association for Computational Linguistics

Figure 2: Illustration of various perceptual challenges for
sentence generation for images. (a) Different images with
semantically the same content. (b) Pose relates ambiguously to actions in real images. See text for details.

a description of an image. Based on our observations of annotated image data (see Fig. 4), a descriptive sentence for an image must contain at minimum: 1) the important objects (Nouns) that participate in the image, 2) Some description of the actions (Verbs) associated with these objects, 3) the
scene where this image was taken and 4) the preposition that relates the objects to the scene. That is, a
quadruplet of T = {n, v, s, p} (Noun-Verb-ScenePreposition) that represents the core sentence structure. Generating a sentence from this quadruplet is
obviously a simplification from state of the art generation work, but as we will show in the experimental results (sec. 4), it is sufficient to describe images. The key challenge is that detecting objects, actions and scenes directly from images is often noisy
and unreliable. We illustrate this using example images from the Pascal-Visual Object Classes (VOC)
2008 challenge [Everingham et al., 2008]. First,
Fig. 2(a) shows the variability of images in their raw
image representations: pixels, edges and local features. This makes it difficult for state of the art object detectors [Felzenszwalb et al., 2010; Schwartz
et al., 2009] to reliably detect important objects in
the scene: boat, humans and water – average precision scores reported in [Felzenszwalb et al., 2010]
manages around 42% for humans and only 11% for
boat over a dataset of almost 5000 images in 20 object categories. Yet, these images are semantically
similar in terms of their high level description. Second, cognitive studies [Urgesi et al., 2006; Kourtzi,
2004] have proposed that inferring the action from
static images (known as an “implied action”) is of445

ten achieved by detecting the pose of humans in the
image: the position of the limbs with respect to one
another, under the assumption that a unique pose occurs for a unique action. Clearly, this assumption
is weak as 1) similar actions may be represented by
different poses due to the inherent dynamic nature of
the action itself: e.g. walking a dog and 2) different
actions may have the same pose: e.g. walking a dog
versus running (Fig. 2(b)). The missing component
here is whether the key object (dog) under interaction is considered. Recent works [Yao and Fei-Fei,
2010; Yang et al., 2010] that used poses for recognition of actions achieved 70% and 61% accuracy
respectively under extremely limited testing conditions with only 5-6 action classes each. Finally, state
of the art scene detectors [Oliva and Torralba, 2001;
Torralba et al., 2003] need to have enough representative training examples of scenes from pre-defined
scene classes for a classification to be successful –
with a reported average precision of 83.7% tested
over a dataset of 2600 images.
Addressing all these visual challenges is clearly
a formidable task which is beyond the scope of this
paper. Our focus instead is to show that with the
addition of language to ground the noisy initial visual detections, we are able to improve the quality of the generated sentence as a faithful description of the image. In particular, we show that it
is possible to avoid predicting actions directly from
images – which is still unreliable – and to use the
corpus instead to guide our predictions. Our proposed strategy is also generic, that is, we make no
prior assumptions on the image domain considered.
While other works (sec. 2) depend on strong annotations between images and text to ground their predictions (and to remove wrong sentences), we show
that a large generic corpus is also able to provide
the same grounding over larger domains of images.
It represents a relatively new style of learning: distant supervision [Liang et al., 2009; Mann and Mccallum, 2007]. Here, we do not require “labeled”
data containing images and captions but only separate data from each side. Another contribution is
a computationally feasible way via dynamic programming to determine the most likely quadruplet
T ∗ = {n∗ , v ∗ , s∗ , p∗ } that describes the image for
generating possible sentences.

2

3

Related Work

Recently, several works from the Computer Vision
domain have attempted to use language to aid image scene understanding. [Kojima et al., 2000] used
predefined production rules to describe actions in
videos. [Berg et al., 2004] processed news captions
to discover names associated with faces in the images, and [Jie et al., 2009] extended this work to associate poses detected from images with the verbs
in the captions. Both approaches use annotated examples from a limited news caption corpus to learn
a joint image-text model so that one can annotate
new unknown images with textual information easily. Neither of these works have been tested on complex everyday images where the large variations of
objects and poses makes it nearly impossible to learn
a more general model. In addition, no attempt was
made to generate a descriptive sentence from the
learned model. The work of [Farhadi et al., 2010] attempts to “generate” sentences by first learning from
a set of human annotated examples, and producing the same sentence if both images and sentence
share common properties in terms of their triplets:
(Nouns-Verbs-Scenes). No attempt was made to
generate novel sentences from images beyond what
has been annotated by humans. [Yao et al., 2010]
has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids.
Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al.,
2003] are based on three steps: selection, planning
and realization. A common challenge in generation
problems is the question of: what is the input? Recently, approaches for generation have focused on
formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization.
We address a tangential problem that has not received much attention in the generation literature:
how to deal with noisy inputs. In our case, the inputs
themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty
into account.
446

Our Approach

Our approach is summarized in Fig. 3. The input is a
test image where we detect objects and scenes using
trained detection algorithms [Felzenszwalb et al.,
2010; Torralba et al., 2003]. To keep the framework
computationally tractable, we limit the elements of
the quadruplet (Nouns-Verbs-Scenes-Prepositions)
to come from a finite set of objects N , actions V,
scenes S and prepositions P classes that are commonly encountered. They are summarized in Table. 1. In addition, the sentence that is generated
for each image is limited to at most two objects occurring in a unique scene.

Figure 3: Overview of our approach. (a) Detect objects
and scenes from input image. (b) Estimate optimal sentence structure quadruplet T ∗ . (c) Generating a sentence
from T ∗ .

Denoting the current test image as I, the initial
visual processing first detects objects n ∈ N and
scenes s ∈ S using these detectors to compute
Pr (n|I) and Pr (s|I), the probabilities that object
n and scene s exist under I. From the observation
that an action can often be predicted by its key objects, Nk = {n1 , n2 , · · · , ni }, ni ∈ N that participate in the action, we use a trained Language model
Lm to estimate Pr (v|Nk ). Lm is also used to compute Pr (s|n, v), the predicted scene using the corpus given the object and verb; and Pr (p|s), the predicted preposition given the scene. This process is
repeated over all n, v, s, p where we used a modified HMM inference scheme to determine the most
likely quadruplet: T ∗ = {n∗ , v ∗ , s∗ , p∗ } that makes
up the core sentence structure. Using the contents
and structure of T ∗ , an appropriate sentence is then
generated that describes the image. In the following
sections, we first introduce the image dataset used
for testing followed by details of how these components are derived.

Objects n ∈ N
’aeroplane’ ’bicycle’ ’bird’
’boat’ ’bottle’ ’bus’ ’car’
’cat’ ’chair’ ’cow’ ’table’
’dog’ ’horse’, ’motorbike’
’person’ ’pottedplant’
’sheep’ ’sofa’ ’train’
’tvmonitor’

Actions v ∈ V
’sit’ ’stand’ ’park’
’ride’ ’hold’ ’wear’
’pose’ ’fly’ ’lie’ ’lay’
’smile’ ’live’ ’walk’
’graze’ ’drive’ ’play’
’eat’ ’cover’ ’train’
’close’ ...

Scenes s ∈ S
’airport’
’field’
’highway’
’lake’ ’room’
’sky’ ’street’
’track’

Preps p ∈ P
’in’ ’at’ ’above’
’around’ ’behind’
’below’ ’beside’
’between’
’before’ ’to’
’under’ ’on’

Table 1: The set of objects, actions (first 20), scenes and preposition classes considered

(a)

Figure 4: Samples of images with corresponding annotations from the UIUC scene description dataset.

3.1

Object and Scene Detections from Images

We use the Pascal-VOC 2008 trained object detectors [Felzenszwalb et al., 2008] of 20 common everyday object classes that are defined in N . Each of
the detectors are essentially SVM classifiers trained
on a large number of the objects’ image representations from a large variety of sources. Although
20 classes may seem small, their existence in many
1

Figure 5: (a) [Top] The part based object detector from
[Felzenszwalb et al., 2010]. [Bottom] The graphical
model representation of an object, for e.g. a bike. (b)
Examples of GIST gradients: (left) an outdoor scene vs
(right) an indoor scene [Torralba et al., 2003].

Image Dataset

We use the UIUC Pascal Sentence dataset, first introduced in [Farhadi et al., 2010] and available online1 . It contains 1000 images taken from a subset of the Pascal-VOC 2008 challenge image dataset
and are hand annotated with sentences that describe
the image by paid human annotators using Amazon Mechanical Turk. Fig. 4 shows some sample
images with their annotations. There are 5 annotations per image, and each annotation is usually
short – around 10 words long. We randomly selected
900 images (4500 sentences) as the learning corpus
to construct the verb and scene sets, {V, S} as described in sec. 3.3, and kept the remaining 100 images for testing and evaluation.
3.2

(b)

http://vision.cs.uiuc.edu/pascal-sentences/

447

natural images (e.g. humans, cars and plants) makes
them particularly important for our task, since humans tend to describe these common objects as well.
As object representations, the part-based descriptor
of [Felzenszwalb et al., 2010] is used. This representation decomposes any object, e.g. a cow, into
its constituent parts: head, torso, legs, which are
shared by other objects in a hierarchical manner.
At each level, image gradient orientations are computed. The relationship between each parts is modeled probabilistically using graphical models where
parts are the nodes and the edges are the conditional
probabilities that relate their spatial compatibility
(Fig. 5(a)). For example, in a cow, the probability
of finding the torso near the head is higher than finding the legs near the head. This model’s intuition lies
in the assumption that objects can be deformed but
the relative position of each constituent parts should
remain the same. We convert the object detection scores to probabilities using Platt’s method [Lin
et al., 2007] which is numerically more stable to obtain Pr (n|I). The parameters of Platt’s method are
obtained by estimating the number of positives and
negatives from the UIUC annotated dataset, from

which we determine the appropriate probabilistic
threshold, which gives us approximately 50% recall
and precision.
For detecting scenes defined in S, we use the
GIST-based scene descriptor of [Torralba et al.,
2003]. GIST computes the windowed 2D Gabor filter responses of an input image. The responses of
Gabor filters (4 scales and 6 orientations) encode the
texture gradients that describe the local properties
of the image. Averaging out these responses over
larger spatial regions gives us a set of global image properties. These high dimensional responses
are then reprojected to a low dimensional space via
PCA, where the number of principal components are
obtained empirically from training scenes. This representation forms the GIST descriptor of an image
(Fig. 5(b)) which is used to train a set of SVM classifiers for each scene class in S. Again, Pr (s|I) is
computed from the SVM scores using [Lin et al.,
2007]. The set of common scenes defined in S is
learned from the UIUC annotated data (sec. 3.3).
3.3

Corpus-Guided Predictions

(sec. 3.1) as a learning corpus that allows us to determine the appropriate target verb set that is amenable
to our problem. We first apply the CLEAR parser
[Choi and Palmer, 2010] to obtain a dependency
parse of these annotations, which also performs
stemming of all the verbs and nouns in the sentence.
Next, we process all the parses to select verbs which
are marked as ROOT and check the existence of a
subject (DEP) and direct object (PMOD, OBJ) that
are linked to the ROOT verb (see Fig. 6(a)). Finally,
after removing common “stop” verbs such as {is,
are, be} we rank these verbs in terms of their occurrences and select the top 50 verbs which accounts
for 87.5% of the sentences in the UIUC dataset to be
in V.
Object class n ∈ N
bus

chair

bicycle

Synonyms, hni
autobus charabanc
double-decker jitney
motorbus motorcoach omnibus
passenger-vehicle schoolbus
trolleybus streetcar ...
highchair chaise daybed
throne rocker armchair
wheelchair seat ladder-back
lawn-chair fauteuil ...
bike wheel cycle velocipede
tandem mountain-bike ...

Table 2: Samples of synonyms for 3 object classes.

Figure 6: (a) Selecting the ROOT verb from the dependency parse ride reveals its subject woman and direct
object bicycle. (b) Selecting the head noun (PMOD)
as the scene street reveals ADV as the preposition on

Predicting Verbs: The key component of our approach is the trained language model Lm that predicts the most likely verb v, associated with the objects Nk detected in the image. Since it is possible that different verbs may be associated with varying number of object arguments, we limit ourselves
to verbs that take on at most two objects (or more
specifically two noun phrase arguments) as a simplifying assumption: Nk = {n1 , n2 } where n2 can
be NULL. That is, n1 and n2 are the subject and
direct objects associated with v ∈ V. Using this assumption, we can construct the set of verbs, V. To
do this, we use human labeled descriptions of the
training images from the UIUC Pascal-VOC dataset
448

Next, we need to explain how n1 and n2 are
selected from the 20 object classes defined previously in N . Just as the 20 object classes are defined visually over several different kinds of specific objects, we expand n1 and n2 in their textual descriptions using synonyms. For example,
the object class n1 =aeroplane should include
the synonyms {plane, jet, fighter jet,
aircraft}, denoted as hn1 i. To do this, we expand each object class using their corresponding
WordNet synsets up to at most three hyponymns levels. Example synonyms for some of the classes are
summarized in Table 2.
We can now compute from the Gigaword corpus [Graff, 2003] the probability that a verb exists given the detected nouns, Pr (v|n1 , n2 ). We do
this by computing the log-likelihood ratio [Dunning,
1993] , λnvn , of trigrams (hn1 i , v, hn2 i), computed
from each sentence in the English Gigaword corpus
[Graff, 2003]. This is done by extracting only the
words in the corpus that are defined in N and V (in-

cluding their synonyms). This forms a reduced corpus sequence from which we obtain our target trigrams. For example, the sentence:

ity that a scene co-occurs with the object and action,
Pr (s|n, v) by:
Pr (n, v|s)Pr (s)
Pr (n, v)
Pr (n|s)Pr (v|s)Pr (s)
=
Pr (n)Pr (v)
∝ Pr (s|n) × Pr (s|v)

Pr (s|n, v) =

the large brown dog chases a small young cat
around the messy room, forcing the cat to run
away towards its owner.

will be reduced to the stemmed sequence dog
chase cat cat run owner2 from which we obtain the target trigram relationships: {dog chase
cat}, {cat run owner} as these trigrams respect the (n1 , v, n2 ) ordering. The log-likelihood ratios, λnvn , computed for all possible (hn1 i , v, hn2 i)
are then normalized to obtain Pr (v|n1 , n2 ). An example of ranked λnvn in Fig. 7(a) shows that λnvn
predicts v that makes sense: with the most likely
predictions near the top of the list.
Predicting Scenes: Just as an action is strongly
related to the objects that participate in it, a
scene can be predicted from the objects and verbs
that occur in the image. For example, detecting Nk ={boat, person} with v={row} would
have predicted the scene s={coast}, since boats
usually occur in water regions. To learn this relationship from the corpus, we use the UIUC dataset
to discover what are the common scenes that should
be included in S. We applied the CLEAR dependency parse [Choi and Palmer, 2010] on the UIUC
data and extracted all the head nouns (PMOD) in
the PP phrases for this purpose and excluded those
nouns with prepositions (marked as ADV) such as
{with, of} which do not co-occur with scenes in
general (see Fig. 6(b)). We then ranked the remaining scenes in terms of their frequency to select the
top 8 scenes used in S.
To improve recall and generalization, we expand
each of the 8 scene classes using their WordNet
synsets hsi (up to a max of three hyponymns levels).
Similar to the procedure of predicting the verbs described above, we compute the log-likelihood ratio
of ordered bigrams, {n, hsi} and {v, hsi}: λns and
λvs , by reducing the corpus sentence to the target
nouns, verbs and scenes defined in N , V and S. The
probabilities Pr (s|n) and Pr (v|n) are then obtained
by normalizing λns and λvs . Under the assumption
that the priors Pr (n) and Pr (v) are independent and
applying Bayes rule, we can compute the probabil2

stemming is done using [Choi and Palmer, 2010]

449

(1)

where the constant of proportionality is justified under the assumption that Pr (s) is equiprobable for all
s. (1) is computed for all nouns in Nk . As shown
in Fig. 7(b), we are able to predict scenes that colocate with reasonable correctness given the nouns
and verbs.
Predicting Prepositions: It is straightforward to
predict the appropriate prepositions associated with
a given scene. When we construct S from the UIUC
annotated data, we simply collect and rank all the associated prepositions (ADV) in the PP phrase of the
dependency parses. We then select the top 12 prepositions used to define P. Using P, we then compute
the log-likelihood ratio of ordered bigrams, {p, hsi}
for prepositions that co-locate with the scene synonyms over the corpus. Normalizing λps yields
Pr (p|s), the probability that a preposition co-locates
with a scene. Examples of ranked λps are shown in
Fig. 7(c). Again, we see that reasonable predictions
of p can be found.

Figure 7: Example of how ranked log-likelihood values
(in descending order) suggest a possible T : (a) λnvn for
n1 = person, n2 = bus predicts v = ride. (b) λns
and λvs for n = bus, v = ride then jointly predicts
s = street and finally (c) λps with s = street predicts p = on.

3.4

Determining T ∗ using HMM inference

Given the computed conditional probabilities:
Pr (n|I) and Pr (s|I) which are observations
from an input test image with the parameters of the trained language model, Lm :

Pr (v|n1 , n2 ), Pr (s|n, v), Pr (p|s), we seek
find the most likely sentence structure T ∗ by:

to

T ∗ = arg max Pr (T |n, v, s, p)
n,v,s,p

= arg max{Pr (n1 |I)Pr (n2 |I)Pr (s|I)×
n,v,s,p

Pr (v|n1 , n2 )Pr (s|n, v)Pr (p|s)} (2)
where the last equality holds by assuming independence between the visual detections and corpus predictions. Obviously a brute force approach to try all
possible combinations to maximize eq. (2) will not
be feasible due to the large number of possible combinations: (20 ∗ 21 ∗ 8) ∗ (50 ∗ 20 ∗ 20) ∗ (8 ∗ 20 ∗ 50) ∗
(12 ∗ 8) ≈ 5 × 1013 . A better solution is needed.

Figure 8: The HMM used for optimizing T . The relevant
transition and emission probabilities are also shown. See
text for more details.

Our proposed strategy is to pose the optimization of T as a dynamic programming problem, akin
to a Hidden Markov Model (HMM) where the hidden states are related to the (simplified) sentence
structure we seek: T = {n1 , n2 , s, v, p}, and the
emissions are related to the observed detections:
{n1 , n2 , s} in the image if they exist. To simplify our notations, as we are concerned with object pairs we will write NN as the hidden states for
all n1 , n2 pairs and nn as the corresponding emissions (detections); and all object+verb pairs as hidden states NV. The hidden states are therefore denoted as: {NN, NV, S, P} with values taken from
their respective word classes from Table 1. The
450

emission states are {nn, s} with binary values: 1
if the detections occur or 0 otherwise. The full
HMM is summarized in Fig. 8. The rationale for
using a HMM is that we can reuse all previous computation of the probabilities at each level to compute the required probabilities at the current level.
From START, we assume all object pair detections
1
are equiprobable: Pr (NN|START) = |N |∗(|N
|+1)
where we have added an additional NULL value for
objects (at most 1). At each NN, the HMM emits
a detection from the image and by independence
we have: Pr (nn|NN) = Pr (n1 |I)Pr (n2 |I). After NN, the HMM transits to the corresponding verb
at state NV with Pr (NV|NN) = Pr (v|n1 , n2 ) obtained from the corpus statistic3 . As no action detections are performed on the image, NV has no emissions. The HMM then transits from NV to S with
Pr (S|NV) = Pr (s|n, v) computed from the corpus
which emits the scene detection score from the image: Pr (s|S) = Pr (s|I). From S, the HMM transits
to P with Pr (P|S) = Pr (p|s) before reaching the
END state.
Comparing the HMM with eq. (2), one can see
that all the corpus and detection probabilities are
accounted for in the transition and emission probabilities respectively. Optimizing T is then equivalent to finding the best (most likely) path through
the HMM given the image observations using the
Viterbi algorithm which can be done in O(105 ) time
which is significantly faster than the naive approach.
We show in Fig. 9 (right-upper) examples of the top
viterbi paths that produce T ∗ for four test images.
Note that the proposed HMM is suitable for generating sentences that contain the core components
defined in T which produces a sentence of the form
NP-VP-PP, which we will show in sec. 4 is sufficient for the task of generating sentences for describing images. For more complex sentences with
more components: such as adjectives or adverbs, the
HMM can be easily extended with similar computations derived from the corpus.
3.5

Sentence Generation

Given the selected sentence structure T
=
{n1 , n2 , v, s, p}, we generate sentences using the
3

each verb, v, in NV will have 2 entries with the same value,
one for each noun.

entire UIUC testing dataset which are scored lower
in our evaluation metrics (sec. 4.1) since they do not
fully describe the image content in terms of the objects and actions.
Some examples of sentences generated using this
strategy are shown in Fig. 9(right-lower).

4 Experiments

Figure 9: Four test images (left) and results. (Rightupper): Sentence structure T ∗ predicted using Viterbi
and (Right-lower): Generated sentences. Words marked
in red are considered to be incorrect predictions. Complete results are available at http://www.umiacs.umd.
edu/˜yzyang/sentence_generateOut.html.

following strategy for each component:
1) We add in appropriate determiners and cardinals: the, an, a, CARD, based on the content
of n1 ,n2 and s. For e.g., if n1 = n2 , we will use
CARD=two, and modify the nouns to be in the plural form. When several possible choices are available, a random choice is made that depends on the
object detection scores: the is preferred when we
are confident of the detections while an, a is preferred otherwise.
2) We predict the most likely preposition inserted
between the verbs and nouns learned from the Gigaword corpus via Pr (p|v, n) during sentence generation. For example, our method will pick the preposition at between verb sit and noun table.
3) The verb v is converted to a form that agrees
with in number with the nouns detected. The
present gerund form is preferred such as eating,
drinking, walking as it conveys that an action is being performed in the image.
4) The sentence structure is therefore of the form:
NP-VP-PP with variations when only one object
or multiple detections of the same objects are detected. A special case is when no objects are detected (below the predefined threshold). No verbs
can be predicted as well. In this case, we simply generate a sentence that describes the scene
only: for e.g. This is a coast, This is
a field. Such sentences account for 20% of the
451

We performed several experiments to evaluate our
proposed approach. The different metrics used for
evaluation and comparison are also presented, followed by a discussion of the experimental results.
4.1

Sentence Generation Results

Three experiments are performed to evaluate the effectiveness of our approach. As a baseline, we simply generated T ∗ directly from images without using
the corpus. There are two variants of this baseline
where we seek to determine if listing all objects in
the image is crucial for scene description. Tb1 is a
baseline that uses all possible objects and scene detected: Tb1 = {n1 , n2 , · · · , nm , s} and our sentence
will be of the form: {Object 1, object 2 and
object 3 are IN the scene.} and we simply
selected IN as the only admissible preposition. For
the second baseline, Tb2 , we limit the number of objects to just any two: Tb2 = {n1 , n2 , s} and the
sentence generated will be of the form {Object
1 and object 2 are IN the scene}. In the
second experiment, we applied the HMM strategy
described above but made all transition probabilities
equiprobable, removing the effects of the corpus,
and producing a sentence structure which we denote
as Teq∗ . The third experiment produces the full T ∗
with transition probabilities learned from the corpus.
All experiments were performed on the 100 unseen
testing images from the UIUC dataset and we used
only the most likely (top) sentence generated for all
evaluation.
We use two evaluation metrics as a measure of the
accuracy of the generated sentences: 1) ROUGE-1
[Lin and Hovy, 2003] precision scores and 2) Relevance and Readability of the generated sentences.
ROUGE-1 is a recall based metric that is commonly
used to measure the effectiveness of text summarization. In this work, the short descriptive sentence of
an image can be viewed as summarizing the image

content and ROUGE-1 is able to capture how well
this sentence can describe the image by comparing it
with the human annotated ground truth of the UIUC
dataset. Due to the short sentences generated, we
did not consider other ROUGE metrics (ROUGE-2,
ROUGE-SU4) which captures fluency and is not an
issue here.
Experiment
∗
Baseline 1, Tb1
∗
Baseline 2, Tb2
HMM no cor∗
pus, Teq
Full HMM, T ∗
Human Annotation

R1 ,(length)
0.35,(8.2)
0.39,(6.8)
0.42,(6.5)

Relevance
2.84 ± 1.40
2.14 ± 1.13
2.44 ± 1.25

Readability
3.64 ± 1.20
3.94 ± 0.91
3.88 ± 1.18

0.44,(6.9)
0.68,(10.1)

2.51 ± 1.30
4.91 ± 0.29

4.10 ± 1.03
4.77 ± 0.42

Table 3: Sentence generation evaluation results with human gold standard. Human R1 scores are averaged over
the 5 sentences using a leave one out procedure. Values
in bold are the top scores.

A main shortcoming of using ROUGE-1 is that
the generated sentences are compared only to a finite set of human labeled ground truth which obviously does not capture all possible sentences that
one can generate. In other words, ROUGE-1 does
not take into account the fact that sentence generation is innately a creative process, and a better recall metric will be to ask humans to judge these
sentences. The second evaluation metric: Relevance and Readability is therefore proposed as an
empirical measure of how much the sentence: 1)
conveys the image content (relevance) in terms of
the objects, actions and scene predicted and 2) is
grammatically correct (readability). We engaged the
services of Amazon Mechanical Turks (AMT) to
judge the generated sentences based on a discrete
scale ranging from 1–5 (low relevance/readability
to high relevance/readability). The averaged results
of ROUGE-1, R1 and mean length of the sentences
with the Relevance+Readability scores for all experiments are summarized in Table 3. For comparison,
we also asked the AMTs to judge the ground truth
sentences as well.
4.2

Discussion

The results reported in Table 3 reveals both the
strengths and some shortcomings of the approach
which we will briefly discuss here. Firstly, the R1
452

scores indicate that based on a purely summarization (unigram-overlap) point of view, the proposed
approach of using the HMM to predict T ∗ achieves
the best results compared to all other approaches
with R1 = 0.44. This means that our sentences are
the closest in agreement with the human annotated
ground truth, correctly predicting the sentence structure components. In addition sentences generated by
T ∗ are also succinct: with an average length of 6.9
words per sentence. However, we are still some way
off the human gold standard since we do not predict
other parts-of-speech such as adjectives and adverbs.
Given this fact, our proposed approach performance
is comparable to other state of the art summarization
work in the literature [Bonnie and Dorr, 2004].
Next, we consider the Relevance+Readability
metrics based on human judges. Interestingly, the
first baseline, Tb1∗ is considered the most relevant description of the image and the least readable at the
same time. This is most likely due to the fact that
this recall oriented strategy will almost certainly describe some objects but the lack of any verb description; and longer sentences that average 8.2 words per
sentence, makes it less readable. It is also possible
that humans tend to penalize less irrelevant objects
compared to missing objects, and further evaluations
are necessary to confirm this. Since Tb2∗ is limited
to two objects just like the proposed HMM, it is a
more suitable baseline for comparison. Clearly, the
results show that adding the HMM to predict the optimal sentence structure increases the relevance of
the produced sentence. Finally, in terms of readability, T ∗ generates the most readable sentences,
and this is achieved by leveraging on the corpus to
guide our predictions of the most reasonable nouns,
verbs, scenes and prepositions that agree with the
detections in the image.

5

Future Work

In this work, we have introduced a computationally
feasible framework that integrates visual perception
together with semantic grounding obtained from a
large textual corpus for the purpose of generating a
descriptive sentence of an image. Experimental results show that our approach produces sentences that
are both relevant and readable. There are, however,
instances where our strategy fails to predict the ap-

propriate verbs or nouns (see Fig. 9). This is due
to the fact that object/scene detections can be wrong
and noise from the corpus itself remains a problem.
Compared to human gold standards, therefore, much
work still remains in terms of detecting these objects
and scenes with high precision. Currently, at most
two object classes are used to generate simple sentences which was shown in the results to have penalized the relevance score of our approach. This can
be addressed by designing more complex HMMs to
handle larger numbers of object and verb classes.
Another interesting direction of future work would
be to detect salient objects, learned from training
image+corpus or eye-movement data, and to verify
if these objects aid in improving the descriptive sentences we generate. Another potential application

Figure 10: Images retrieved from 3 verbal search terms:
ride,sit,fly.

of representing images using T ∗ is that we can easily sort and retrieve images that are similar in terms
of their semantic content. This would enable us to
retrieve, for example, more relevant images given a
verbal search query such as {ride,sit,fly}, returning images where these verbs are found in T ∗ .
Some results of retrieved images based on their verbal components are shown in Fig. 10: many images
with dissimilar visual content are correctly classified
based on their semantic meaning.

453

6 Acknowledgement
This material is based upon work supported by
the National Science Foundation under Grant No.
1035542. In addition, the support of the European Union under the Cognitive Systems program (project POETICON) and the National Science Foundation under the Cyberphysical Systems
Program, is gratefully acknowledged.

References
Berg, T. L., Berg, A. C., Edwards, J., and Forsyth, D. A.
(2004). Who’s in the picture? In NIPS.
Bonnie, D. Z. and Dorr, B. (2004). Bbn/umd at duc-2004:
Topiary. In In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL
2004, pages 112–119.
Choi, J. D. and Palmer, M. (2010). Robust constituentto-dependency conversion for english. In Proceedings
of the 9th International Workshop on Treebanks and
Linguistic Theories, pages 55–66, Tartu, Estonia.
Dunning, T. (1993). Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61–74.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn,
J., and Zisserman, A. (2008). The PASCAL Visual
Object Classes Challenge 2008 (VOC2008) Results.
Farhadi, A., Hejrati, S. M. M., Sadeghi, M. A., Young, P.,
Rashtchian, C., Hockenmaier, J., and Forsyth, D. A.
(2010). Every picture tells a story: Generating sentences from images. In Daniilidis, K., Maragos, P.,
and Paragios, N., editors, ECCV (4), volume 6314
of Lecture Notes in Computer Science, pages 15–29.
Springer.
Felzenszwalb, P. F., Girshick, R. B., and McAllester, D.
(2008). Discriminatively trained deformable part models, release 4. http://people.cs.uchicago.edu/ pff/latentrelease4/.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D. A.,
and Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627–1645.
Golland, D., Liang, P., and Klein, D. (2010). A gametheoretic approach to generating spatial descriptions.
In Proceedings of EMNLP.
Graff, D. (2003). English gigaword. In Linguistic Data
Consortium, Philadelphia, PA.
Jie, L., Caputo, B., and Ferrari, V. (2009). Who’s doing what: Joint modeling of names and verbs for simultaneous face and pose annotation. In NIPS, editor,
Advances in Neural Information Processing Systems,
NIPS. NIPS.
Kojima, A., Izumi, M., Tamura, T., and Fukunaga, K.
(2000). Generating natural language description of human behavior from video images. In Pattern Recognition, 2000. Proceedings. 15th International Conference on, volume 4, pages 728 –731 vol.4.
Kourtzi, Z. (2004). But still, it moves. Trends in Cognitive Sciences, 8(2):47 – 49.

454

Liang, P., Jordan, M. I., and Klein, D. (2009). Learning
from measurements in exponential families. In International Conference on Machine Learning (ICML).
Lin, C. and Hovy, E. (2003). Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
NAACLHLT.
Lin, H.-T., Lin, C.-J., and Weng, R. C. (2007). A note
on platt’s probabilistic outputs for support vector machines. Mach. Learn., 68:267–276.
Mann, G. S. and Mccallum, A. (2007). Simple, robust,
scalable semi-supervised learning via expectation regularization. In The 24th International Conference on
Machine Learning.
McKeown, K. (2009). Query-focused summarization using text-to-text generation: When information comes
from multilingual sources. In Proceedings of the 2009
Workshop on Language Generation and Summarisation (UCNLG+Sum 2009), page 3, Suntec, Singapore.
Association for Computational Linguistics.
Oliva, A. and Torralba, A. (2001). Modeling the shape
of the scene: A holistic representation of the spatial
envelope. International Journal of Computer Vision,
42(3):145–175.
Schwartz, W., Kembhavi, A., Harwood, D., and Davis,
L. (2009). Human detection using partial least squares
analysis. In International Conference on Computer Vision.
Torralba, A., Murphy, K. P., Freeman, W. T., and Rubin,
M. A. (2003). Context-based vision system for place
and object recognition. In ICCV, pages 273–280. IEEE
Computer Society.
Traum, D., Fleischman, M., and Hovy, E. (2003). Nl generation for virtual humans in a complex social environment. In In Proceedings of he AAAI Spring Symposium
on Natural Language Generation in Spoken and Written Dialogue, pages 151–158.
Urgesi, C., Moro, V., Candidi, M., and Aglioti, S. M.
(2006). Mapping implied body actions in the human
motor system. J Neurosci, 26(30):7942–9.
Yang, W., Wang, Y., and Mori, G. (2010). Recognizing
human actions from still images with latent poses. In
CVPR.
Yao, B. and Fei-Fei, L. (2010). Grouplet: a structured
image representation for recognizing human and object interactions. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, San
Francisco, CA.
Yao, B., Yang, X., Lin, L., Lee, M. W., and Zhu, S.-C.
(2010). I2t: Image parsing to text description. Proceedings of the IEEE, 98(8):1485 –1508.

Neural Self Talk: Image Understanding via
Continuous Questioning and Answering

arXiv:1512.03460v1 [cs.CV] 10 Dec 2015

Yezhou Yang1 , Yi Li2 , Cornelia Fermuller1 , and Yiannis Aloimonos1
1
University of Maryland, College Park, MD 20742
2
Toyota Research Institute of North America, Ann Arbor, MI 48105

Abstract
In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG)
module and a Visual Question Answering module,
in which Recurrent Neural Networks (RNN) and
Convolutional Neural Network (CNN) are used.
Given a dataset that contains images, questions and
their answers, both modules are trained at the same
time, with the difference being VQG uses the images as input and the corresponding questions as
output, while VQA uses images and questions as
input and the corresponding answers as output. We
evaluate the self talk process subjectively using
Amazon Mechanical Turk, which show effectiveness of the proposed method.

1

Introduction

Acclaimed as “one of the last cognitive tasks to be performed
well by computers” [Stork, 1998], exploring and analyzing
novel visual scenes is a journey of continuous discovery,
which requires not just passively detecting objects and segmenting the images, but arguably more importantly, actively
asking the right questions and subsequently closing the semantic loop by answering the questions being asked.
This paper proposes a framework that can continuously
discover novel questions on an image, and then provide legitimate answers. This “self talk” approach for image understanding goes beyond visual classification by introducing a
theoretically infinite interaction between a natural language
question generation module and a visual question answering
module. Under this architecture, the “thought process” for
image understanding can be revealed by a sequence of consecutive question and answer pairs (Fig. 1).
Our “self talk” framework has two “executives” that takes
their roles iteratively: 1) question generation, which is responsible for asking the right questions, and 2) question answering, which accepts the questions and generate potential
answers. With the rapid development in computer vision and
machine learning [Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2014; Karpathy and Li, 2014; Vinyals et al., 2014;

Figure 1: One example self talk by the presented system,
while the affirmative or questionable answer is decided by
confidence score from visual answering executive.
Chen and Zitnick, 2014] there are a few tools developed for
this seemingly intuitive philosophy in Artificial Intelligence,
but self-talk is certainly beyond the aggregation of tools, because it is fundamentally a challenging chicken egg problem.
1) Questions from a single image can be as diversified as
possible. Researchers have attempted a few approaches that
mostly centered on asking limited questions such as “what”
(e.g., object and action recognition) and “where” (e.g., place
recognition). Unfortunately, questions can be anything related or unrelated to the given picture. This puzzling issue
of unconstrained questions can be traced back to the original
Turing test1 , and the solution is still elusive.
Luckily, researchers have advanced the viewpoint that if
we are able to develop a semantic understanding of a visual
scene, we should be able to produce natural language descriptions of such semantics. This “image captioning” perspective
are indeed exciting achievements, but it is only limited to generate descriptive captions, thus we propose to consider the
question “Can we generate questions, based on images?”.
2) Evaluating the correctness of automatic questions
answering is in the realm of Turing test. The “Visual
1

“Would the questions have to be sums, or could I ask it what
it had had for breakfast?” Turing “Oh yes, anything.” [Rapaport,
2005]

Question Answering” [Antol et al., 2015] problem recently
becomes an important area in computer vision and machine
learning, and sometimes it is referred as Visual Turing challenge [Malinowski and Fritz, 2014]. A few approaches [Malinowski et al., 2015; Ren et al., 2015] have shown that deep
neural nets again can be trained to answer a related question
for an arbitrary scene with promising success.
3) The semantic loop between the above two “executives” must be closed. While the above two “executives” are
very interesting entities, they cannot achieve the “self talking” by their own. On one hand, the image captioning task
neglects the importance of the thought process behind the appearance. Also, the amount of information covered by a finite
language description is limited. These limitations have been
pointed out by several recent works [Johnson et al., 2015;
Schuster et al., 2015; Aditya et al., 2015] and have been addressed partially by introducing middle layer knowledge representations. On the other hand, the setting of the visual question answering task requires as input a related question given
by human beings. These questions themselves inevitably contain information about the image, which are recognized by
human beings and only available through human intervention. Several recent results on the image VQA benchmarks
indicate that language only information seem to contribute to
the most of the good performance and how important the role
of the visual recognition is still unclear.
In our formalism, the input of the final deep trained system
is solely an image. Both questions and answers are generated
from the trained models. Also, we want to argue that the capability of raising relevant and reasonable questions actively
is the key to intelligent machinery. Thus, the main contributions of this paper are twofold: 1) we propose to automatically generate “self talk” for arbitrary image understanding, a
conceptually intuitive yet AI-challenging task; 2) we propose
an image question generation module based on deep learning
method.
Figure. 2 illustrates the flow chart of our approach (Sec.3).
In Sec.4, we report experiments on two publicly available datasets (DAQUAR for indoor domain [Malinowski and
Fritz, 2014] and COCO for arbitrary domain [Antol et al.,
2015]). Specificaly, we 1) evaluate the quality of the generated questions using standard language based metrics similar to image captioning and 2) use Amazon Mechanical Turk
(AMT)-based evaluations of the generated question-answer
pairs. We further discuss the insights from experimental results and challenges beyond them.

2

Related Work

Our work is related mainly to three lines of research of natural image understanding: 1) question generation, 2) image
captioning and 3) visual question answering.
Question Generation is one of the key challenges in natural languages. Previous approaches of question generation
from natural language sentences are mainly through template matching in a conservative manner [Brown et al., 2005;
Heilman and Smith, 2010; Ali et al., 2010]. [Ren et al.,
2015] proposed to use parsing based approach to synthetically create question and answer pairs from image annota-

Figure 2: The flow chart or our approach.
tions. In this paper, we propose a visual question generation
module through a technique directly adapted from image captioning system [Karpathy and Li, 2014], which is data driven
and the potential output questions space is significantly larger
than previous parsing or template based approaches, and the
trained module only takes in image as input.
In Image Captioning, in addition to the deep neural nets
based approaches mentioned in Sec. 1 we also share our
roots with the works of generating textual descriptions. This
includes the works that retrieves and ranks sentences from
training sets given an image such as [Hodosh et al., 2013],
[Farhadi et al., 2010], [Ordonez et al., 2011], [Socher et al.,
2014]. [Elliott and Keller, 2013], [Kulkarni et al., 2011],
[Kuznetsova et al., 2012], [Yang et al., 2011], [Yao et al.,
2010] are some of the works that have generated descriptions
by stitching together annotations or applying templates on detected image content.
In the filed of Visual Question Answering, very recently
researchers spent a significant amount of efforts on both creating datasets and proposing new models [Antol et al., 2015;
Malinowski et al., 2015; Gao et al., 2015; Ma et al., 2015].
Interestingly both [Antol et al., 2015] and [Gao et al., 2015]
adapted MS-COCO [Lin et al., 2014] images and created an
open domain dataset with human generated questions and
answers. The creation of these visual question answering
testbed costs more than 20 people year of effort using Amazon Turk platform, and some questions are very challenging
which actually require logical reasoning in order to answer
correctly. Both [Malinowski et al., 2015] and [Gao et al.,
2015] use recurrent networks [] to encode the sentence and
output the answer. Specifically, [Malinowski et al., 2015] applies a single network to handle both encoding and decoding,
while [Gao et al., 2015] divides the task into an encoder network and a decoder one. More recently, the work from [Ren
et al., 2015] reported state-of-the-art VQA performance using
multiple benchmarks. The progress is mainly due to formulating the task as a classification problem and focusing on the
domain of questions that can be answered with one word. The
visual question answering module adopts this approach.

3
3.1

Self talk: Theory and Practice
Theory and Motivation

The phenomenon of “self talk” has been studied in the field
of psychology for hundreds of years. The term is defined

as a special form of intrapersonal communication: a communicator’s internal use of language or thought. Using the
terms of computer science and engineering, it could be useful to envision intrapersonal communication occurring in the
mind of the individual in a model which contains a sender,
receiver, and a potential feedback loop. This process happens
consciously or sub-consciously in our mind. The capability
of self-raising questions and answer them is also crucial for
learning. Question raising and answering facilitate the learning process. For example, in the field of education, reciprocal questioning has been studied as a strategy, where students
take on the role of the teacher by formulating their own list of
questions about a reading material. In this paper, we regard
this as another challenge for computers, and we believe that
one key to intelligence is raising the right questions.
The benefits of modeling scene understanding task as a revealing of the “self talk” of the intelligent agents are mainly
twofold: 1) the understanding of the scene can be revealed
step by step and the failure cases could be tracked to specific
question answer pairs. In other words, the process is more
transparent; 2) theoretically the number of questions could
be infinite and the question and answer loop could be never
ending. This is especially crucial for active agent, such as
movable robots, while their view of the scene keeps changing
by moving around space, and the “self talk” in this scenario
is never-ending. For a specific task, such as scene category
recognition, this formulation has been proven to be efficient
[Yu et al., 2011].
From a practical point of view, the revealing of “self talk”
makes computers more human like, and the presented system
has application potential in creating robotic companions [Yu
et al., 2011]. Note that as human being, we make mistakes,
and some of them are “cute” mistakes. In Sec. 4, we show that
our system makes many “cute” mistakes too, which actually
makes it more human-like.

3.2

Our Approach

We have two hypotheses to validate in this work: 1) with the
current progress in image captioning, a system can be trained
to generate reasonable and relevant questions, and 2) by incorporating it with a visual question answering system, a system could be trained to generate human like “self talk” with
promising success.
In this section, we introduce a frustratingly straightforward policy to generate a sequence of questions for the purpose of “self talk”. We repeat this sampling process q =
QuestionSampling(I) N times (five times typically in our
experiments). For each question qi generated and the accompanied original image I, we pass it through the VQA module
a = V isualAnswer(q, I) to achieve an answer ai . In such a
manner we achieve the “self talk” question and answers pairs
{(q1 , a1 ), ..., (qN , aN )}. The “self talk” is further evaluated
by Amazon Mechanical Turk based human evaluation.
Question Generation
In this section, we assume an input set of images and their
questions raised by human annotators. In our scenario, these
are full images and their questions set. We adopted the
method from [Karpathy and Li, 2014], where a simple but

Algorithm 1 A Primitive “Self Talk” Generation Algorithm
1:
2:
3:
4:
5:
6:

procedure S ELF TALK G ENERATION((I))
i←1
while i ≤ N do
qi = QuestionSampling(I)
ai = V isualAnswer(qi , I)
i=i+1
return {(q1 , a1 ), ..., (qN , aN )}

effective extension is introduced from previously developed
Recurrent Neural Networks (RNNs) [] based language models to train image captioning model effectively. For the purpose of a self-contained work, we briefly go over the method
here.
Specifically, during the training of our image question generation module, the multimodal RNN takes the image pixels
I and a sequence of input vectors (x1 , ..., xT ). It then computes a sequence of hidden states (h1 , ..., hT ) and a sequence
of outputs (y1 , ..., yT ) by iterating the following recurrence
relation from t = 1 to t = T .
bv = Whi [CN Nθc (I)]
ht = f (Whx xt + Whh ht−1 + bh + 1(t = 1)  bv )
yt = sof tmax(Who ht + bo ),

(1)
(2)
(3)

In the equations above, Whi , Whx , Whh , Woh , xi and bh ,bo
are learnable parameters, and CN Nθc (I) is the last layer of
a pre-trained Convolutional Neural Network (CNN) []. The
output vector yt holds the (unnormalized) log probabilities
of words in the dictionary and one additional dimension for a
special END token. In the approach, the image context vector
bv to the RNN is only given at the first iteration. A typical size
of the hidden layer of the RNN is 512 neurons.
The RNN is trained to combine a word (xt ), the previous
context (ht1 ) to predict the next word (yt ) in the generated
question. The RNNs predictions on the image information bv
via bias interactions on the first step. The training proceeds as
follows (refer to Figure.3a)): First set h0 = 0, x1 to a special
START vector, and the desired label y1 as the first word in the
training question. Then set x2 to the word vector of the first
word and expect the network to predict the second word, etc.
Finally, on the last step when xT represents the last word, the
target label is set to a special END token. The cost function is
to maximize the log probability assigned to the target labels
(here, a Softmax classifier).
During testing time, to generate one question, we first compute the image representation bv , and then set h0 = 0, x1
to the START vector and compute the distribution over the
first word y1 . We sample each word in the question from
the distribution, set its embedding vector as x2 , and repeat
this process until the END token is generated. In the rest
of the paper, we denote this question generation process as
q = QuestionSampling(I).
Question Answering
In this section, we assume an input set of images and their
annotated question answer pairs from human labelers. We
adopted the approach from [Ren et al., 2015], which introduced a model builds directly on top of the long short-term
memory (LSTM) [Hochreiter and Schmidhuber, 1997] sen-

In the experiments on these two datasets, we first report the
question generation performance using standard image captioning language based evaluation metric. Then, in order to
evaluate the performance of the “self talk” we report the AMT
results and provide further discussion.

4.1

Figure 3: The presented architecture of question generation
module (part A) and question answering module (part B), and
how they are connected.
tence model and is called the VIS+LSTM model. It treats the
image as one word of the question as shown in Figure.3b).
The model uses the last hidden layer of the 19-layer Oxford VGG Conv Net [Simonyan and Zisserman, 2014] trained
on ImageNet 2014 Challenge as the visual embeddings. The
CNN part of the model is kept frozen during training. The
model also uses word embedding model from general purpose skip-gram embedding [Pennington et al., 2014]. In our
experiments, the word embedding is kept dynamic (trained
with the rest of the model). Please refer to [Ren et al., 2015]
for the details. In the rest of the paper, we denote this trained
VQA module as a = V isualAnswer(q, I).
Amazon Mechanical Turk based Evaluation
For the generated question answer (“self talk”) pairs, since
there are no groundtruth annotations that could be used for
automatic evaluation, we designed a Amazon Mechanical
Turk (AMT) based human evaluation metric to report.
We ask the Turkers to imagine they have a companion robot
whose name is “self talker”. Once they bring the robot to a
place shown in the image give, the robot started to generate
questions and then self-answer the questions as if he is talking to himself. Specifically we ask the Turkers to evaluate
three metrics: 1) Readability: how readable the “self talker”
’s “self talk”. Scores range from 1: not readable at all, to
5: no grammatical errors. Grammatically sound “self-talk”
have better readability ratings; 2) Correctness: how correct
the “self talk” is. “self-talk” content that correctly describes
the image content with higher precision have better correctness ratings (range from 1 to 5); 3) Human likeness: how
human-like does the robot perform (range from 1 to 5).

4

Experiments

We test the presented approach on two visual question answering (VQA) datasets, namely, DARQUAR [Malinowski
and Fritz, 2014] and MSCOCO-VQA [Antol et al., 2015].

Datasets

We first briefly describe the two testing-beds we are using for
the experiments.
DAQUAR: Indoor Scenes: DAQUAR [Malinowski and
Fritz, 2014] vqa dataset contains 12,468 human question answer pairs on 1,449 images of indoor scene. The training set
contains 795 images and 6,793 question answer pairs, and
the testing set contains 654 images and 5,675 question answer pairs. We run experiments for the full dataset with all
classes, instead of their reduced set where the output space is
restricted to only 37 object categories and 25 test images in
total. This is because the full dataset is much more challenging and the results are more meaningful in statistics.
COCO: General Domain: MSCOCO-VQA [Antol et
al., 2015] is the latest VQA dataset that contains openended questions about arbitrary images collect from the Internet. This dataset contains 369,861 questions and 3,698,610
ground truth answers based on 123,287 MSCOCO images.
These questions and answers are sentence-based and openended. The training and testing split follows MSCOCO-VQA
official split. Specifically, we use 82,783 images for training
and 40,504 validation images for testing. The variation of
the images in this dataset is large and till now it is considered as the largest general domian VQA dataset. The effort
of collecting this dataset cost over 20 people year working
time using Amazon Mechanical Turk interface.

4.2

Question Generation Evaluation

We now evaluate the ability of our RNN model to raise questions about a given image. We first trained our Multimodal
RNN to generate questions on full images with the goal of
verifying that the model is rich enough to support the mapping from image data to sequences of words. We report
the BLEU [Papineni et al., 2002], METEOR [Lavie, 2014],
ROUGE [Lin, 2004] and CIDEr [Vedantam et al., 2014]
scores computed with the coco-caption code [Chen et al.,
2015]. Each method evaluates a candidate generated question
by measuring how well it matches a set of several reference
questions (averagely eight questions for DAQUAR dataset,
and three questions for MSCOCO-VQA) written by humans.
To further validate the performance of question generation,
we further list the performance metrics reported in the stateof-the-art image captioning work [Karpathy and Li, 2014].
From Table. 1, except CIDEr score, the question generation
performance is comparable with the state-of-the-art image
captioning performance. Note that for CIDEr score is a consensus based metric. The facts that, 1) coco-VQA has three
reference ground-truth questions while coco-Caption has five
and 2) human annotated questions by its nature varies more
than captions, makes it hard to achieve high CIDEr score for
question generation task.

Figure 4: Example “self talk” generated on DAQUAR testing set.

Figure 5: Example “self talk” generated on MSCOCO-VQA testing set.
DAQUAR question MAX
DAQUAR question SAMPLE
coco-VQA question MAX
coco-VQA question SAMPLE
coco-Caption [Karpathy and Li, 2014]

CIDEr
.512
.143
.331
.133
.66

METEOR
.361
.256
.178
.127
.195

ROUGE L
.761
.631
.493
.342
–

Bleu-1
.81
.685
.594
.388
.625

Bleu-2
.735
.561
.422
.220
.45

Bleu-3
.635
.428
.291
.117
.321

Bleu-4
.361
.337
.193
.064
.23

Table 1: Evaluation of question generation on DAQUAR and coco-VQA datasets. MAX: the generated questions have max
probability from trained model. SAMPLE: the generated questions are randomly drawn from the trained probabilistic model.

4.3

“Self talk” Evaluation

In Table. 2 we report the average score as well as its standard
deviation for each metric. We randomly drawn 100 and 1000
testing samples from DAQUAR and MSCOCO-VQA testing
sets for the human evaluation reported here. From the human evaluation, we can see that the questions generated have
achieved close to human readability. The correctness of the
generated “self talk” averagely has some relevance to the image and according to Turkers, the imagined companion robot
acts averagely beyond “a bit like human being” but below the
“half human, half machine” category.

We also asked the Turkers to choose from five immediate
feelings after their companion robot’s performance. Fig. 7
and Fig. 8 depicts the feedback we got from the users. Given
the fact that the performance of the “self talker” robot is still
far from human performance, most of the Turkers thought
they like such a robot or feel its amusing. And only very few
of the users felt scared, which indicates that our image understanding performance is far from being trapped into the
so-called “uncanny valley” [Mori et al., 2012] of machine intelligence. At the end of our evaluation, we also asked Turkers to comment about what the robot’s performance. Some

Figure 6: Example Turkers’ comments about the “self talk” robot.
example comments could be found in Fig. 6.
DAQUAR
coco-VQA

Readability

Correctness

3.35 ± 0.92
3.39 ± 1.18

2.5 ± 1.03
2.7 ± 1.29

Human
likeness
2.49 ± 1.02
2.40 ± 1.33

Table 2: “Self talk” AMT human evaluation.

5

Figure 7: User feedback on DAQUAR set.

Figure 8: User feedback on MSCOCO-VQA set.

Conclusion and Future Work

In this paper, we consider the image understanding problem
as a self-questioning and answer process and we present a
primitive “self talk” generation method based on two deep
neural network modules. From the experimental evaluation
on both the performance of question generation and final “self
talk” pairs, we show that the presented method achieved a decent amount of success. There are still several potential pathways to improve the performance of intelligent “self talk”.
The role of common-sense knowledge. Common-sense
knowledge has a crucial role in question raising and answering process for human beings [Aditya et al., 2015]. The experimental result shows that our system by learning the model
from large annotated question answer pairs, it implicitly encodes a certain level of common-sense. The real challenge is
to deal with situations that the visual input conflicts with the
common-sense learned from context data. In our experiment,
it seems that the model is biased towards to trust his common
sense more than the visual input. How to incorporate either
logical or numerical forms of common-sense into end-to-end
based image understanding system is still an open problem.
Creating a story-line. When human beings perform intrapersonal communication, we tend to follow a logic flow
or so-called story-line. This requires a question generation
modules that takes in consideration the answers from previous questions for consideration. This indicates a more sophisticated dialogue generation process (such as a cognitive
dialogue [Aloimonos and Fermüller, 2015]), and it can also
potentially prevent self-contradictions happened in this paper’s generated results (see last comment in Fig. 6).
As indicated from AMT feedback, human users felt it is
cute and fondness to have a robot companion that moves
around and talkative. Another open avenue is to integrate the
current trained model onto a robot platform and through interaction with users to continuously refine its trained model.

References
[Aditya et al., 2015] Somak Aditya, Yezhou Yang, Chitta
Baral, Cornelia Fermuller, and Yiannis Aloimonos. From
images to sentences through scene description graphs
using commonsense reasoning and knowledge. arXiv
preprint arXiv:1511.03292, 2015.
[Ali et al., 2010] Husam Ali, Yllias Chali, and Sadid A
Hasan. Automation of question generation from sentences.
In Proceedings of QG2010: The Third Workshop on Question Generation, pages 58–67, 2010.
[Aloimonos and Fermüller, 2015] Yiannis Aloimonos and
Cornelia Fermüller. The cognitive dialogue: A new model
for vision implementing common sense reasoning. Image
and Vision Computing, 34:42–44, 2015.
[Antol et al., 2015] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering.
In International Conference on Computer Vision (ICCV),
2015.
[Brown et al., 2005] Jonathan C Brown, Gwen A Frishkoff,
and Maxine Eskenazi. Automatic question generation for
vocabulary assessment. In Proceedings of the conference
on Human Language Technology and Empirical Methods
in Natural Language Processing, pages 819–826. Association for Computational Linguistics, 2005.
[Chen and Zitnick, 2014] Xinlei Chen and C Lawrence Zitnick. Learning a recurrent visual representation for image
caption generation. arXiv preprint arXiv:1411.5654, 2014.
[Chen et al., 2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin,
Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar,
and C Lawrence Zitnick.
Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015.
[Donahue et al., 2014] Jeff Donahue, Lisa Anne Hendricks,
Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and
description. arXiv preprint arXiv:1411.4389, 2014.
[Elliott and Keller, 2013] Desmond Elliott and Frank Keller.
Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle,
Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1292–1302, 2013.
[Farhadi et al., 2010] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. Every picture tells
a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg,
2010. Springer-Verlag.
[Gao et al., 2015] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking

to a machine? dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612,
2015.
[Heilman and Smith, 2010] Michael Heilman and Noah A
Smith. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 609–617.
Association for Computational Linguistics, 2010.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and
Jürgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.
[Hodosh et al., 2013] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking
task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853–899, 2013.
[Johnson et al., 2015] Justin Johnson, Ranjay Krishna,
Michael Stark, Jia Li, Michael Bernstein, and Li Fei-Fei.
Image retrieval using scene graphs. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
June 2015.
[Karpathy and Li, 2014] Andrej Karpathy and Fei-Fei Li.
Deep visual-semantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306, 2014.
[Kiros et al., 2014] Ryan Kiros, Ruslan Salakhutdinov, and
Richard S Zemel. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv preprint
arXiv:1411.2539, 2014.
[Kulkarni et al., 2011] Girish Kulkarni, Visruth Premraj,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg,
and Tamara L Berg. Baby talk: Understanding and generating image descriptions. In Proceedings of the 24th
CVPR, 2011.
[Kuznetsova et al., 2012] Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin
Choi. Collective generation of natural image descriptions.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume
1, ACL ’12, pages 359–368, Stroudsburg, PA, USA, 2012.
Association for Computational Linguistics.
[Lavie, 2014] Michael Denkowski Alon Lavie. Meteor universal: Language specific translation evaluation for any
target language. ACL 2014, page 376, 2014.
[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014,
pages 740–755. Springer, 2014.
[Lin, 2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out: Proceedings of the ACL-04 workshop, volume 8, 2004.
[Ma et al., 2015] Lin Ma, Zhengdong Lu, and Hang Li.
Learning to answer questions from image using convolu-

tional neural network. arXiv preprint arXiv:1506.00333,
2015.
[Malinowski and Fritz, 2014] Mateusz Malinowski and
Mario Fritz. Towards a visual turing challenge. arXiv
preprint arXiv:1410.8027, 2014.
[Malinowski et al., 2015] Mateusz Malinowski, Marcus
Rohrbach, and Mario Fritz. Ask your neurons: A neuralbased approach to answering questions about images.
arXiv preprint arXiv:1505.01121, 2015.
[Mao et al., 2014] Junhua Mao, Wei Xu, Yi Yang, Jiang
Wang, and Alan L Yuille. Explain images with multimodal recurrent neural networks.
arXiv preprint
arXiv:1410.1090, 2014.
[Mori et al., 2012] Masahiro Mori, Karl F MacDorman, and
Norri Kageki. The uncanny valley [from the field].
Robotics & Automation Magazine, IEEE, 19(2):98–100,
2012.
[Ordonez et al., 2011] Vicente Ordonez, Girish Kulkarni,
and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In John ShaweTaylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N.
Pereira, and Kilian Q. Weinberger, editors, NIPS, pages
1143–1151, 2011.
[Papineni et al., 2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for computational
linguistics, pages 311–318. Association for Computational
Linguistics, 2002.
[Pennington et al., 2014] Jeffrey Pennington,
Richard
Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014), pages 1532–1543, 2014.
[Rapaport, 2005] William J. Rapaport. The turing test: Verbal behavior as the hallmark of intelligence edited by stuart shieber. Computational Linguistics, 31(3):407–412,
September 2005.
[Ren et al., 2015] Mengye Ren, Ryan Kiros, and Richard
Zemel. Exploring models and data for image question answering. arXiv preprint arXiv:1505.02074, 2015.
[Schuster et al., 2015] Sebastian Schuster, Ranjay Krishna,
Angel Chang, Li Fei-Fei, and Christopher D. Manning.
Generating semantically precise scene graphs from textual
descriptions for improved image retrieval. In Proceedings
of the Fourth Workshop on Vision and Language, pages
70–80, Lisbon, Portugal, September 2015. Association for
Computational Linguistics.
[Simonyan and Zisserman, 2014] Karen Simonyan and Andrew Zisserman.
Very deep convolutional networks
for large-scale image recognition.
arXiv preprint
arXiv:1409.1556, 2014.
[Socher et al., 2014] Richard Socher, Andrej Karpathy,
Quoc V. Le, Christopher D. Manning, and Andrew Y.

Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2:207–218, 2014.
[Stork, 1998] David G Stork. HAL’s Legacy: 2001’s Computer as Dream and Reality. MIT Press, 1998.
[Vedantam et al., 2014] Ramakrishna
Vedantam,
C Lawrence Zitnick, and Devi Parikh. Cider: Consensusbased image description evaluation.
arXiv preprint
arXiv:1411.5726, 2014.
[Vinyals et al., 2014] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. Show and tell: A neural
image caption generator. arXiv preprint arXiv:1411.4555,
2014.
[Yang et al., 2011] Yezhou Yang, Ching Lik Teo, Hal
Daumé, III, and Yiannis Aloimonos. Corpus-guided sentence generation of natural images. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 444–454, Stroudsburg,
PA, USA, 2011. Association for Computational Linguistics.
[Yao et al., 2010] Benjamin Z. Yao, Xiong Yang, Liang Lin,
Mun Wai Lee, and Song Chun Zhu. I2t: Image parsing
to text description. Proceedings of the IEEE, 98(8):1485–
1508, 2010.
[Yu et al., 2011] Xiaodong Yu,
Cornelia Fermuller,
Ching Lik Teo, Yezhou Yang, and Yiannis Aloimonos.
Active scene recognition with vision and language.
In Computer Vision (ICCV), 2011 IEEE International
Conference on, pages 810–817. IEEE, 2011.

Visual Attention Analysis by Pseudo Gravitational Field
Yezhou Yang, Mingli Song, Na Li, Jiajun Bu, Chun Chen
Zhejiang University, Hangzhou 310027, China

{ecoyang,brooksong,nli,bjj,chenc}@zju.edu.cn

ABSTRACT

1. INTRODUCTION

As a crucial step of the visual cognition and perception,
visual attention analysis shows great importance in many
research or application areas. In this paper, we treat this
problem from a new angle, inspired by the classic gravitational field theory. By defining “mass” of each pixel, we
compute “force” between them to obtain a so-called pseudo
gravitational field over an image. Then, we propose an iteration algorithm to simulate the movement of the fixation
points1 affected by this field. Finally, stable visual attention points or areas are obtained when those fixation points
finally aggregate around some special pixels or areas. The
main contributions are threefold: (1) by introducing classic gravitational field theory into visual attention analysis,
a new point of view is proposed; (2) a competition scheme
is constructed and the meaning of attraction can be applied
into visual attention analysis intuitively; (3) by using pixels
into computation through down sampling, a faster analysis method is achieved. The experimental result shows that
our method is effective and consistent with the generally
accepted definition of visual attention.

Visual attention is one of the most important features of human visual system. As a concept within the area of psychology and biology, it is hard to define visual attention merely
by mathematical or computational methods. Tracing back
to 1890, according to James’[12] suggestion, visual attention
serves as a mediating mechanism involving competition between different aspects of the visual scene and selecting the
most relevant areas to the detriment of others.
As the nexus between cognition and perception, the usefulness of visual attention is obvious, ranging from effective
information retrieval to an adaptive image display strategy.
As the mediation mechanism of human is not completely
understood yet, using computational methods to analyze
images becomes the main trend to determine the important and representative regions in whole image nowadays.
A number of computational attention models have been developed, such as the models proposed in [11, 3]. The basic principles behind these efforts are greatly influenced by
psychophysical research. Niebur [2] proposed a computational attention model which indicated that the so-called
”focus of attention” scans the scene both in the form of
a rapid, bottom-up, saliency-driven and task-independent
manner and in a slower, top-down, volition-controlled and
task dependent manner. Based on the work in [1, 9, 10], Itti
proposed a saliency-based visual attention model for scene
analysis in [4]. Based on Itti’s model, Zhang et al [7] proposed a method from the fuzzy theory to extract attention
areas and points from saliency map. Though Itti’s framework builds up an elegant mapping from biological theories
to computational implementation, it’s computational complexity is still very high. Zhang’s method employs the contrast feature of image but ignores visual attention competition scheme. Recently, several related work have been published, such as region-based attention analysis [5], but they
all base on Itti’s model and correct semantic segmentation
is still an open problem now.
In this paper, we propose a new scheme to do the visual
attention analysis in a bottom-up manner. Similar to the
fact that objects with relevant small mass will be attracted
to those objects with huge mass, e.g., “black hole” in the
astrophysics theory, the so-called human fixation points are
driven to a small area or a specific point with huge “mass” in
the image. In order to simulate this effect, we build a gravitational field over an image by defining the “mass” of each
pixel, the mutual “force” between two pixels. As the definitions of these concepts are not strictly ruled by the classic
gravitational field, we regard it as a pseudo one. Based on

Categories and Subject Descriptors
I.4.10 [Image Processing and Computer Vision]: Image Representation-Statistical, Hierarchical

General Terms
Algorithms, Experimentation, Theory

Keywords
Attention analysis, Gravitational field, Simulation algorithm
1

Fixation points is a concept from Meur’s eye tracking experiments, which originally means the regular time sampling
of eye gaze on the monitor. In this paper, we treat these
points as basic simulation units.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MM’09, October 19–24, 2009, Beijing, China.
Copyright 2009 ACM 978-1-60558-608-3/09/10 ...$10.00.

553

features Y, Cb, Cr of a pixel are the natural qualities, it is
reasonable to use Y, Cb, Cr to compute the “mass” of each
pixel in an image in our approach. Moreover, to measure
the “mass” quantitatively, zero level is defined based on the
universal average of each pixel’s “mass”. So the “mass” MaY
of a pixel located at index a can be computed as follows:
MaY = Y − Y

where
is the pixel a’s “mass” of feature Y. And Y is the
universal average of all the pixels’ Y value in the image. This
definition is similar to feature Cb and Cr. We notice that
our definition will lead to a negative value of “mass”. This
is why we call our model pseudo gravitational field. And
such definition helps us to simulate the competition scheme
better than using a classical one.

Figure 1: eye tracking experiments by Oliver (a)
The original picture, (b) the spatial distribution of
human fixations for 14s of viewing time, (c) fixation density map obtained by convolved the spatial
distribution with a 2D Gaussian filter, and (d) highlighted human RoI (Regions of Interest) obtained
by redrawing the original picture by leaving in the
darkness in the nonfixated areas.[8]

3.2 The definition of the mutual force
Given the mass of each pixel, we can easily compute the
magnitude of the mutual attracting “force” from pixel a to
b as follows:
(
Y
WY ∗Ma
∗MbY
if MaY ∗ MbY < 0
~
Y
R2
(3)
fa,b =
0
Otherwise

these definitions, we initially locate the fixation points uniformly. Assuming these fixation points generate forces based
on gravitational law according to their “masses”, and are
driven by these forces among them. After loops to simulate
this process, the fixation points tend to aggregate towards
some specific visual attention areas or points.

2.

where R is the distance between two pixels and W is the
weight of each feature (W = (0.6, 0.2, 0.2) in our approach
corresponding to Y, Cb and Cr respectively, because human
is more sensitive to luminance feature rather than chrominance ones. And the direction of the force is from b to a,
Y
Thus the force is a vector quantity represented as f~a,b
Note that f~Y is only the force concerned with feature Y.

MOTIVATION

Though previous approaches built an elegant mapping from
psychology and biology to computer science, they overlooked
two crucial properties of visual attention: a scheme of competition and the effect of attraction, which defined by James
[12]. Inspired by the famous word by Liyd and Ng [6]: “In
keeping with the spirit of the age, researchers can think of
the laws of physics as computer programs and the universe
as a computer.”, we find that the theory of gravitational field
by Newton can be used to construct a competition scheme
to simulate the visual attention converging procedure.
Based on the result of Meur’s eye tracking experiments [8]
in Figure 1, the fixation points of eyes should be initially
distributed identically. And the natural process of visual
attention should be a series steps of computation, towards
a convergent state. This motivates us that, there seems to
be a gravitational field over the image and drive the fixation points moving into their position. The so-called attention areas or points are the winners of the competition to
the detriment of others. Thus, in this paper, we propose a
scheme by which try to simulate a field to take effect on the
fixation points in visual attention analysis by computer.

3.

a,b

And this definition is similar to feature Cb and Cr. In this
paper, the total “force” of point b to point a is computed as:
~ +f~Cr
Y
Cb
F~a,b = f~a,b
+ fa,b
a,b

4. FIXATION POINTS MOVEMENT SIMULATION
4.1 Initialization of the simulation
The eye tracking experiment Figure 1 for visual attention
by Meur [8] show that, the fixation points of human initially
distribute sparsely over an image and tend to aggregate after
several millisecond. Based on this fact, we initially distribute
dozens of fixation points uniformly and number them from
1 to N , where N is the total number of the fixation points.

Gravitation is a natural phenomenon by which objects attract each other. Let us suppose we have two objects A
and B, the distance between them is |rAB |, then each object
exerts a gravitational force on the other as below:
(MA ∗ MB )
|rAB |2

4.2 Specific details of each step
As we scatter the fixation points, every fixation point locates
at a specific pixel at each step. Suppose number i fixation
point is at pixel a and number j fixation point is at pixel
b, the “force” between two fixation points is equal to the
“force” between two pixels defined in Section 3.2. That is:
F~j,i = F~a,b . The universal “force” to the jth fixation point
located at pixel a is computed as:

(1)

where G is the universal gravitational constant. Similarly,
we define the “mass” and “force” in our approach as follows.

3.1

(4)

Finally, with the definition of “mass” and mutual “force”, a
pseudo gravitational field is constructed over the image.

PSEUDO GRAVITATIONAL FIELD

F =G∗

(2)

MaY

The definition of the “mass” at a pixel

Mass is the basic element of a gravitational field definition,
and it is the natural quality of an object. As the three basic

F~j =

i=N
X
i=1

554

F~j,i j²[1, N ]butj 6= i

(5)

3) Color quantization. Human vision perception usually is
more sensitive to the changes in smooth areas than in texture areas. So color quantization is performed to make color
smooth in texture areas.
Step 2: Simulation
In this step, given an image, we simulate the visual fixation points’ movements based on our pseudo gravitational
field. A proper number of fixation points is important for a
practical application, because the computational complexity
will increase in a quadratic manner with the increase of the
simulation points and a simulation with too little points is
unacceptable. We initially put a fixation point at the center
of each 8*8 block. The simulation step S also should be chosen carefully because a short S will lead to a longer execution
time towards convergence while a long S will neglect some
local and small attention areas. In this approach, S=50
demonstrates the best average performance on the testing
data. The fixation points of visual attention aggregate step
by step with the looping process.
Step 3: Visual Attention Points/Areas Extraction
In order to get the visual attention points, we regard the
pixel aggregated by the most fixation points as the most
attractive points. Normally, there are several visual attention points, we can just regard the top 5 or 10 pixels as the
visual attention points. For the visual attention area extraction, we use K-means clustering or other classic classifiers to
group the fixation points based on location and density.
Computing a saliency map of an image is a crucial step
in many computational models of visual attention analysis, such as fuzzy growing or segmentation based analysis [7,
5]. In this paper, we also try to invert our fixation points
distribution into a “saliency map”. The simplest way is to
compute a density value of each pixel using the average number of fixation points of all the pixels near it, and we use an
8*8 block as the perception unit. After obtaining “saliency
map”, we can do the extraction similar to those existing
methods.

: Pixels
: Fixation points
F F

F:

F

F

"force" between two
fixation points

F
F

F

Figure 2: Pseudo gravitational field over an image
Table 1: Fixation Points Movement Simulation
Initial:
Uniformly sample N points of a Image and Set a step
constant S
Do:
Compute the universal ”force” of each fixation point;
Compute the distance and direction traveled by each
point
then put that point into a new location;
Until:
Loop a specific times or the first norm of the distance
vector is within a small bound
~ is a vector, which means the sum of the
And note that the F
~ is also a vector, as shown in Figure 2. To all the fixation
F
~ =
points, we can get a vector to represent the force: F
[F~1 , F~2 , F~3 ...F~N ] When an object is acted upon by a force,
it will move towards the direction of the force. We assume
that after a round of loop in the simulation computation,
the distance traveled by a fixation point located at pixel a
is reversal ratio with the square of the mass defined by Eq.
(2):
~a =
D

F~a
(Ma2 ) ∗ S

(6)

6. EVALUATION AND DISCUSSION

The distance is along the direction of the “force”, where S is
the step length serving as a parameter of the algorithm. To
all the fixation points, we can get a vector to represent the
~ =
distance traveled, note that each element is a vector:D
~1 , D
~2 , D
~3 ...D~N ]
[D

4.3

As shown in Figure 3, we use the test images from Meur’s
eye-tracking experiment in Figure 1 to run our algorithm on
them. It is interesting to compare these results with correspondent ones in the eye-tracking experiment. The final
distribution of the fixation points from our experiment is basically coincide with the distribution of the fixation points
from the eye-tracking experiment, which can prove that our
method do agree with the eye-track experiment. Moreover,
the saliency maps computed from our method are also similar with those from eye-tracking experiment.
Due to the subjectivity of human attention perception, a
standardized objective function is still unavailable for the
evaluation of image attention analysis. In our experiment,
we randomly choose over 100 pictures from internet and test
our methods on them. As shown in Figure 5, the fixation
points actually aggregate towards the most relevant areas to
the detriment of others, such as the black bird in the bird
picture, the red shirt boy in the sand picture, the red leaf in
the leaves picture and so on, which is attractive to people
and is agree with our expectation. Moreover, in Figure 4,
the movements of the fixation points during the aggregating
process reflect the competition mechanism among the areas
of the image. And the final fixation points’ distribution
demonstrates the winner areas and points.

The Stopping Criterion

The most intuitive one is that when the first norm of the
distance vector is within a small bound, we can regard the
simulation is converged and stop.
We conclude the main simulation algorithm in Table 1

5.

IMPLEMENTATION

There are three steps in our implementation:
Step 1: Pre-processing
The pre-processing step includes:
1) Image resizing. The original image is resized to a uniform
image with its aspect ratio unchanged through down sampling. The advantages of resizing are twofold: all images are
considered in the same scale and the computational complexity is effectively reduced. In this paper, we rescale each
image into 120*160 pixels.
2) Color space transformation. As YCbCr space is consistent
with human color perception system well, the resized image
is transformed from RGB space to YCbCr space.

555

We also test the efficiency of the proposed approach. To
compute the result of a 120*160 image, under the circumstance of Matlab 2008a and a 2.8G Intel core CPU, it only
takes less than 2 minutes to converge, while the implementation of Zhang’s method[7] under similar environment takes
more than 20 minutes to get a saliency map from a specific
input image.

20

5
20

20

4
40
40

40

3

60

2

60

60

1
80

80

80

0
150
100

100

200

100

150

120
20

40

60

80

100

120

140

160

50

120
20

40

60

80

100

120

140

0

160

20

120

0

20

20

40

40

60

60

80

80

80

60

80

100

120

140

160

20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

160

40
40

10

60

5

40

60

80

60

80

80

0
150
100

100

200

100

150

120
40

60

80

100

120

140

160

100

100

50
50

120
20

40

60

80

100

120

140

0

160

120

0

10
20

20

60

40

20

20

40

20

15
20

20

20

100

100

50

8
40
40

40

6

60

4

60

60

2
80

80

100

100

20

100

120
40

60

80

100

120

140

150

40

60

80

100

120

140

160

100

100
50

120
20

40

60

80

100

120

140

0

160

120

0

100

120
20

200

100
50

120

100

80

0
150

160

20

40

60

80

100

120

140

20

120

160

20

40

60

80

100

120

140

8

160

20

20

6

40
40

40

4
60

20

60

20

20

40

40

80

80

0
150

40
100

60

60

2
80

100

200

100

150

60

60

120
20

80

80

80

100

100

100

40

60

80

100

120

140

160

100

100

50
50

120
20

40

60

80

100

120

140

0

160

20

120

0

5
20

20

4
40

120

40

120
20

40

60

80

100

120

140

160

20

40

60

80

100

120

140

20

40

60

80

100

120

140

160

60

2

60

(a)

(b)

(c)

40

3

120

160

60

1
80

80

80

0
150
100

100

200

100

150

120
20

20

20

20

40

40

40

40

60

60

60

80

80

80

80

100

100

100

100

120

120

120

120

20

40

60

80

100

120

140

160

20

40

(a)

60

80

100

120

140

160

20

40

20

20

40

40

60

60

80

80

100

100

60

80

100

120

140

160

20

40

(c)

60

80

100

120

140

20

4
40

3
60

2
1

80

0
150
200

100

120

50

120
20

40

60

80

100

(e)

120

140

160

20

40

60

80

100

120

140

160

(f)

100

150
100

50
0

0

(g)

120
20

40

60

80

100

120

140

160

(h)

Figure 4: (a) Initially distributed fixation points;
(b) After 5 simulation loops (c) After 10 loops (d)
After 15 loops(e) After 20 loops (f ) After 25 loops
(g) Final distribution (h) Final saliency map

7.

CONCLUSION

In this paper, a new approach on visual attention analysis
is proposed inspired by the classical gravitational field theory, called pseudo gravitational field. The fixation points
are initially placed uniformly over the whole image. Then,
being driven by the forces from the pseudo gravitational
field, these fixation points aggregate to some specific pixels
or areas in the image. Finally, the visual attention points
or regions can be extracted based on the aggregated points.
The experimental result shows that this method is not only
faster, but also explicitly brings out the meaning of visual attention itself and agrees with the eye-tracking experiments.
In the future, we will improve its performances by including
more combinations of the early visual features, such as texture and shape. Moreover, an adaptive parameters scheme
could be applied to adapt the simulation step or the number
of fixation points dynamically.

8.

ACKNOWLEDGMENTS

This paper is supported by National Science Foundation
of China (Grant No. 60873124).

9.

100

120

140

160

50

120
20

40

60

80

100

(b)

120

140

160

0

0

(c)

120

(d)

[2] E.Niebur and C.Koch. Computational architectures for
attention. R. Parasuraman, (Ed.), The attentive brain,
Cambridge, MA: MIT Press.
[3] J.M.Wolfe and K. Cave. Deploying visual attention:
The guided search model. John Wiley & Sons Ltd.,
1990.
[4] L.Itti, C.Koch, and E.Niebur. A model of
saliency-based visual attention for rapid scene
analysis. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 20(11):1254–1259, 1998.
[5] H. Liu, S. Jiang, Q. Huang, and et al. Region-based
visual attention analysis with its application in image
browsing on small displays. In Proceedings of the 15th
international conference on Multimedia, 2007.
[6] S. Lloyd and Y. Ng. Black hole computers. Scientific
American, 291:52–61, 2004.
[7] Y. Ma and H. Zhang. Contrast-based image attention
analysis by using fuzzy growing. ACM Multimedia,
34:374–381, 2003.
[8] O. L. Meur and P. L. Callet. A coherent
computational approach to model bottom-up visual
attention. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 28(5):802–817, 2006.
[9] R.Milanese, S.Gil, and T.Pun. attentive mechanism
for dynamic and static scene analysis. Optical
Engineering, 34:2428–2434, 1995.
[10] S.Baluja and D.A.Pomerleau. Expectation-based
selective attention dor visual monitoring and control
of a robot vehicle. Robotics and Autonomous System,
22(3-4):329–344, Dec 1997.
[11] J. K. Tsotsos, S. M. Culhane, W. Wai, and et al.
Modeling visual attention via selective tuning.
Artificial Intelligence, 78:507–545, 1995.
[12] W.James. The Principles of Psychology. Holt, New
York, 1890.

160

(d)

5

80

Figure 5: (a) Initial distribution of fixation points;
(b)Final distribution in 2-D (c) Final distribution in
3-D (d) Final saliency map

60

(b)

60

(a)

Figure 3: (a)Initially distributed fixation points;
(b)Final distribution (c) Final saliency map
20

40

100

100

50

REFERENCES

[1] C.Koch and S.Ullman. Shifts in selective visual
attention: towards the underlying neural circuitry.
Human Neurobiology, 4:219–227, 1985.

556

Co-active Learning to Adapt Humanoid Movement for Manipulation

arXiv:1609.03628v1 [cs.RO] 12 Sep 2016

Ren Mao1 , John S. Baras1 , Yezhou Yang2 , and Cornelia Fermüller2

Abstract— In this paper we address the problem of robot
movement adaptation under various environmental constraints
interactively. Motion primitives are generally adopted to generate target motion from demonstrations. However, their generalization capability is weak while facing novel environments.
Additionally, traditional motion generation methods do not
consider the versatile constraints from various users, tasks,
and environments. In this work, we propose a co-active learning
framework for learning to adapt robot end-effector’s movement
for manipulation tasks. It is designed to adapt the original
imitation trajectories, which are learned from demonstrations,
to novel situations with various constraints. The framework
also considers user’s feedback towards the adapted trajectories,
and it learns to adapt movement through human-in-the-loop
interactions. The implemented system generalizes trained motion primitives to various situations with different constraints
considering user preferences. Experiments on a humanoid
platform validate the effectiveness of our approach.

I. Introduction
Trajectories learning from human demonstrations has been
studied in the field of Robotics for decades due to its
wide range of applications, in both industrial and domestic
scenarios. Among the various approaches, Motion Primitives
(MPs) aims to parameterize observed human motion and
then reproduce it, given different initial and target states.
However, it is widely known that general MPs methods,
such as Dynamic Movement Primitives (DMPs) [1], have
limited capability to generalize towards novel environments
involving other constraints. Moreover, standard MPs learning
method ignores user preferences of the tasks and the environments. For real world humanoid applications, a practical
robot movement learning framework needs to take user
preferences and environment constraints into consideration.
Let’s start from a common example, that a human user
teaches a humanoid how to transfer a bottle from different
start and end states. Using off-the-shelf approach, the robot
is able to learn the motion by acquiring MPs from the
demonstrated trajectories and apply them to generate new
trajectories given different initial and target states. However,
solely following the generated trajectories may fail if the
environment has slight alteration, such as having a bowl
blocking the trajectory as illustrated in Fig. 2(a). Here we
assume that the robot can only be able to receive these
constraints during task execution phase (testing phase), and
these constrains are not presented during the training phase.
In this work, we propose an optimization based framework to
1 R. Mao and J. Baras are with the Department of Electrical and Computer
Engineering and the ISR, 2 Y. Yang and C. Fermüller are with the Department
of Computer Science and the UMIACS, University of Maryland, College
Park, Maryland 20742, USA. {neroam, baras} at umd.edu, yzyang at
cs.umd.edu and fer at umiacs.umd.edu.

Fig. 1. System for learning movement adaptation for manipulation tasks.
Dashed lines indicate feedback.

adapt trained movements for novel environments. The first
goal of our system is to generate adapted trajectories, as
shown in Fig. 2(b), that can: 1) follow the demonstrated
trajectories for the purpose of preserving movement patterns,
and 2) fulfill novel constraints perceived from the environment during testing phase.
Moreover, novel environment constraints perceived during
testing phase could be more complicated than just obstacles.
Following the example mentioned before, this time let’s
consider a situation where the target bottle is leaking. Ideally
an intelligent robot that understands the situation should
avoid moving the bottle over the bowl, but follows the
movement path around it. Even though we could adjust
the objective function during optimization for movement
adaptation, what if in another scenario the robot is asked
to transfer a knife while avoiding obstacles above them to
prevent potential scratches? Such constraints are not only
associated with the task context, i.e, leaking bottle or knife
as the manipulated object, but also associated with user’s
preference, i.e, avoiding the bowl with a certain manner.
Therefore, a human-in-the-loop on-line adaptation system is
necessary to generate manipulation trajectories for different
preferences. In the optimization framework presented in this
paper for movement adaptation, we first treat the reward
weights as the adjustable parameters to alter the quality of
the trajectory. Then based on user feedback, the framework
learns the preferred behavior, that fulfills constraints, by
updating the reward weights. Therefore, the learned behavior
can be generalized towards different situations where similar
constraints are encountered. As illustrated in Fig. 2(c), after a

(a)

(b)

(c)

Fig. 2. Baxter Transferring Leaking Bottle: (a) Movement imitation, failed to avoid the bowl; (b) Movement adaptation with initial weights, successfully
avoided the bowl by path above it but spilled water in the bowl; (c) Movement adaptation with learned weights for new situation where obstacle locates
differently, successfully avoided the bowl by path around it and avoided spilling water in the bowl.

few iterations of on-line learning, the robot is able to generate
an adapted trajectory according to the learned preferences.
This paper proposes an approach for interactively learning
movement adaptation for manipulation tasks. Fig. 1 illustrates the proposed system. The main contributions of this
work are: 1) A system for robot to generalize movement
learned from demonstrations to fulfill constraints perceived
from novel environment. It is able to adapt trajectories
for various situations according to user preferences; 2) An
approach for robot learning to adapt trajectories by updating
reward weights based on users’ feedback. The user thus
can co-actively train the robot in-the-loop by demonstrating
desired trajectories; 3) An implementation of the optimization schema to adapt transferring skill considering obstacles
and different manners. We validate the implementation on
a humanoid platform (Baxter) and the experimental results
support our claims.
II. Related Work
Various approaches have been proposed to enable robot
learning manipulation movements. Among them, imitation
learning [2] focuses on mimicking human demonstrations,
while learning from demonstration (LfD) techniques [3] are
applicable. However, with these approaches, the robot could
only reproduce learned movement in a similar environment.
To deal with novel environments, extended approaches [4]
augmented the trajectory generation with additional cost
terms or different objective function as a criterion of trajectories’ quality. The criterion is based on human experts’
prior knowledge about the task or environment before execution phase. Then, the motion is generalized with these
predefined constraints in similar situations. These approaches
do not consider various user preferences. Here, we present
another layer of exploration and learning to adapt the trained
movement considering novel environment constraints, such
as observed obstacles and task preferences.
Approaches [5] for encoding trajectory as motion primitives have been proposed for various forms of generalization and modulation, such as Gaussian mixture regression
and Gaussian mixture models [6], [7]. In [8], a mixture
model was used to estimate the entire movement skill from
several sample trajectories. Another school of approaches
derive from Hidden Markov models [9]. One popular representation to encode motion from demonstrated trajectories

is Dynamic Movement Primitives (DMPs), as introduced
in [1]. It consists of differential equations with well-defined
attractor properties and a non-linear learnable component
that allows modeling of almost arbitrarily complex motion.
Recently, Probabilistic Movement Primitives (ProMPs) [10]
was proposed as an alternative representation in probabilistic
formulation. It learns a trajectory distribution from multiple
demonstrations and modulates the movement by conditioning
on desired target states. Incorporating the variance of demonstrations, ProMPs approach handles noise from different
demonstrations and provides increased flexibility for reproducing movement. However, all these approaches hardly
deal with novel environments such as involving different
obstacles. In our work, we first train our robot using ProMPs
and then generalize these trained motion primitives to newly
introduced environment constraints.
In order to enable MPs to adapt to novel environments
with obstacles [11], [12], Kober et al. [4] proposed an
augmented version of DMPs which incorporates perceptual
coupling to an external variable. They firstly learned the
initial dynamic models by standard imitation learning and
subsequently used a reinforcement learning method for selfimprovement. Ghalamzan et al. [13] proposed a three-tiered
approach for robot learning from demonstrations that can
generalize noisy task demonstrations to a new target state and
to an environment with obstacles. They encoded the nominal
path generated from a Gaussian Mixture Model with DMPs
and generated trajectory for a new target state. Then they
adapted the DMP-generated trajectory to avoid obstacles by
formulating an optimal control problem regarding the reward
function learned from demonstrations by inverse optimal
control. This approach allows an non-expert user to teach
a robot the desired response to different objects but requires
offline training in the environment involving those obstacles
for learning the reward function. However, in real world
scenarios, the human users often have different preferences
for trajectories generation according to various environments
and tasks, while it is extremely challenging for them to
provide the optimal trajectories in every situation. Instead,
in our approach, the human users can interactively provide
sub-optimal suggestions on how to improve the trajectory and
the robot learns the preference for different constraints, and
also incorporate it in generating more applicable trajectories.
User preferences over robot’s trajectories have been stud-

ied in the field of human robot interaction (HRI). Sisbot
et al. [14] proposed to model user specified preferences as
constraints on the distance of the robot from the user, the
visibility of the robot and the users arm comfort. Then a
path planner fulfilling such user preferences is provided.
Ashesh Jain et al. [15] proposed a co-active learning method
to learn user preferences over generated trajectories for
manipulation tasks by iteratively taking user sub-optimal
feedback, thereafter the optimal trajectory was selected based
on the learned reward function. In our work, we adopt the
co-active learning paradigm and further propose a reward
formulation to model user preferences over constraints for
movement generation. Then we integrate it with movement
adaptation through optimization based planning.

objects or obstacles existing during these demonstrations
and is hard to update the learned reward function online
when the robot is facing situations that involve new objects.
To learn the reward function which controls how the robot
adapts trajectories under new contexts, we applied a co-active
learning technique [15] in which the user only corrects the
robot by providing an improved trajectory ȳ and then the
robot updates the parameter w of f (·; w) based on user’s
feedback. It is worth to note that this feedback only indicates
f ∗ (ȳ, xc , yD ) > f ∗ (y, xc , yD ), and ȳ may be non-optimal
trajectories. With iterations of improvement, the robot could
learn a function that approximates the oracle f ∗ (·) tightly.

III. Co-active Learning for Movement Generalization

Overall, after the robot has offline learned the movement
skill from demonstrations, when facing a different task
context xc in a novel environment, the testing phase includes
three stages: 1) Movement Imitation, which computes an imitation trajectory yD by generalizing demonstrated movement
to new initial and target states; 2) Movement Adaptation,
which generates an adapted trajectory y under new task
and environment contexts by maximizing the given reward
function; 3) Rewards Learning, which updates the parameters
of estimated reward function according to user’s feedback
through co-active learning. Fig. 1 demonstrates our proposed
framework. In the following sections, we entail and formulate
each stage.

For the problem of robot learning from demonstrations [3],
a common practice is to offline learn the skills by encoding
the trajectories with movement patterns such as DMPs [16].
They can be then, during the testing phase, used to generalize
the movement to novel situations with slight alterations,
such as different initial and target states. Nevertheless, this
generalization capability does not apply to novel environments with different obstacles or to a new task contexts
with a variety of manipulated objects. In this paper, we
propose a complementary framework for generalizing movement skills, which are offline learned from demonstrations, to
novel situations, and in addition incorporate on-line learning
preferences of how to generalize from human’s feedback coactively.
While facing a novel situation, the robot is given a
manipulation task context xc that describes the environment,
the objects and any other task-related information. It could
compute an imitation movement trajectory yD by generalizing offline learned skills to new initial and target states.
Such a trajectory can be executed if the new environment
does not have obstacles and there is no other constraints
inherited from the task.
To further generalize learned movement skills to more
challenging situations, the robot has to generate an adapted
trajectory y based on the task contexts xc and the computed
imitation trajectory yD . Here we use a reward function
f ∗ (y, xc , yD ) to reflect how much reward the adapted trajectory y can achieve for different contexts. Therefore, we
can adapt the movement by solving an optimal control
problem which outputs an adapted trajectory by maximizing
the reward function f ∗ . The reward function consists of
a Imitation Reward fD describing the tendency to follow
the imitation trajectory yD , a Control Reward fC describing
the smoothness of executing the adapted trajectory y and
a Response Reward fE describing the expected response
given the environment. Although this reward function can be
recovered from demonstrations by Inverse Optimal Control,
as [13] suggests, it assumes that demonstrations are from
experts, which bears an oracle reward function. In fact,
it is common for non-expert users to provide non-optimal
trajectories in practice. Also, [13] requires the manipulated

IV. Our System

A. Movement Imitation
At the beginning, our system offline learns movement skill
in an environment without obstacle or other constraints. In
this work, we adopt the Probabilistic Movement Primitives
(ProMPs) [10] for offline learning and movement imitation. It
obtains a distribution over trajectories from multiple demonstrations, which captures the variations, and can be easily
generalized to new initial and target states while imitating
the movement.
To be specific, we consider that a robot’s end-effector has
d degrees of freedom (DOF) along with its arm, with its
state denoted as y(t) = [y1 (t), . . . , yd (t)]T . The trajectory of
the robot’s end effector is represented as a sequence T =
{y(t)}t=0,...,T . We model each dimension i of y(t) using linear
regression with n Gaussian time-dependent basis functions
ψ and a n-dimensional weight vectors wi as
yi (t) = ψ(t)T wi + y ,

(1)

where y ∼ N(0, σ2y ) denotes zero-mean i.i.d. Gaussian noise.
With the underlying weight vectors w = [wT1 , . . . , wTd ]T , the
probability of observing a trajectory T can be given by
Y
Y
N(y(t)|Ψ(t)T w, Σy ) (2)
p(T |w) =
p(y(t)|w) =
t

t
d

z }| {
where Ψ(t) = diag(ψ(t), . . . , ψ(t)) and Σy = σ2y Id×d .

1) Learning from Demonstrations: For each demonstration, the trajectory can be easily represented by a weight
vector w which has fewer dimensions than the number of
time steps. To capture trajectory variations from multiple
demonstrations of the movement, a Gaussian distribution
p(w; θ) = N(w|µw , Σw ) over the weights w is estimated.
Therefore, the distribution of the trajectory p(T |w) can be
represented as
Z
p(T ; θ) =
p(T |w)p(w; θ)dw
(3)
Y
=
N(y(t)|Ψ(t)T µw , Ψ(t)T Σw Ψ(t)T + Σy ) (4)
t

We can then estimate the parameters θ = {µw , Σw } by using
maximum likelihood estimation as suggested in [10].
2) Trajectory Generation: In novel situations, the trajectory could be modulated by conditioning with different
observed states. By adding an observation vector of Y ∗ =
[y0∗T , yT∗T ]T indicating desired initial state y0∗ and target state
yT∗ with the accuracy Σ∗y , we could apply Bayes theorem
and represent conditional distribution for w as


p(w|Y ∗ ) = N(w|µ0w , Σ0w ) ∝ N Y ∗ |Ψ∗T w, Σ∗Y p(w)
−1 


Y ∗ − Ψ∗T µw
µ0w = µw + Σw Ψ∗ Σ∗Y + Ψ∗T Σw Ψ∗
−1

Σ0w = Σw − Σw Ψ∗ Σ∗Y + Ψ∗T Σw Ψ∗ Ψ∗T Σw
(5)
where Ψ∗ = [Ψ(0), Ψ(T )] and Σ∗Y = diag(Σ∗y , Σ∗y ) are
augmented for observation vector Y ∗ .
With a conditional distribution of w, we could generate
conditional trajectory distribution and easily evaluate the
mean yD and the variance ΣD of the trajectory T for any
time point t according to Eq.( 2) and Eq.( 3). Therefore, the
mean trajectory yD (t) can be used as the imitation trajectory
in movement adaptation and the variance ΣD (t) can be used
to indicate which parts or dimensions of the trajectory are
more flexible to adapt. The larger variance reflects higher
variations in demonstrations. It means more flexibility to
modify the corresponding part of the trajectory.
It is worth to mention that, although we adopt ProMPs for
movement imitation in this work, the proposed Movement
Adaptation framework can be integrated with any other
movement imitation learning techniques.
B. Movement Adaptation
As mentioned before, if the environment of a new situation
is exactly the same as the one during demonstration when
ProMPs are learned, e.g, no obstacle, safety constraints or
other new considerations, the robot can perform movement
optimally by directly following the imitation trajectory yD ∈
d in discrete time generated by learned ProMPs.
In this work, we want to have a system that can adapt
to an environment with novel constraints. Thus, we model
the movement adaptation as an optimal control problem
with fixed time horizon T in discrete time. The output
of the adaptation system is a new trajectory y ∈ d in
discrete time. The input consists of the task context xc that

describes the environment, the objects and any other taskrelated information which are obtained from the perception
module, the imitation trajectory yD which is generated from
learned ProMPs, and the reward function f (y, xc , yD ) which
represents the reward of the adapted trajectory y corresponding to the new situation.
1) Optimization with Constraints: Let’s consider that the
perception module detects Nob j objects in the environment,
which may be obstacles during the manipulation. Each object
is abstracted as a sphere in the space represented by its center
location and semi-diameter {Ok , dk }, k = 1, . . . , Nob j . Assuming the reward function can be modeled as accumulated sum
of rewards from each state y(t) at time step t:
f (y, xc , yD ) =

T
X

ft (y(t), xc , yD ).

(6)

t=0

Because we are only modulating the trajectory, we can model
the adaptation system as linear dynamics with the control
signal a ∈ m , as it does not involve real physical dynamics.
According to the embodiment of robotic end-effector based
on its design, we could compute the end-effector’s position in
spatial space E(y) following the kinematics modeling [17].
Then, considering obstacles avoidance in spatial space, the
target optimal policy π∗ = {a(t)∗ }t=0,...,T −1 could be defined
from Eq. (7) with constraints.
P
π∗ = arg max Tt=0 ft (y(t), xc , yD )
(7)
π

subj. to

∀t = 0, · · · , T − 1
z(t + 1) = Az(t) + Ba(t)

(8)
(9)

y(t) = Cz(t)

(10)

U ≥ y(t) ≥ L

(11)

kE(y(t)) − Ok k2 ≥ dk2 ,

∀k = 1, · · · , Nob j (12)

y(T ) = yD (T ),

(13)

where A, B, C are system matrices, Eq.( 13) constrains the
final position of the adapted trajectory, Eq.( 11) constrains
the trajectory within feasible limits, and Eq. 12 ensures the
adapted trajectory can avoid obstacles safely by keeping a
minimum distance dk between the robot’s end-effector and
any object.
2) Model Predictive Control: In order to find an optimal
solution of such a system with continuous state and action
spaces, we adopt Model Predictive Control which computes
the optimal actions in a finite prediction horizon. Therefore,
by considering a prediction time horizon T p , the optimal
action a(i)∗ , at time step i = 0, . . . , T − 1, can be solved by:
Pi+T p
max
t=i+1 ft (y(t), xc , yD )
(a(i),··· ,a(i+T p −1))

subj. to

∀t = i, · · · , i + T p − 1
z(t + 1) = Az(t) + Ba(t)
y(t) = Cz(t)
U ≥ y(t) ≥ L
kE(y(t)) − Ok k2 ≥ dk2 , ∀k = 1, · · · , Nob j
y(T ) = yD (T ).
(14)

At each step i, the optimal actions {a(i)∗ , · · · , a(i + T p − 1)∗ }
for T p decision steps in future are computed but only the
action for current step a(i)∗ is performed. Therefore, it can
deal with changing environments as these changes could be
considered in the next decision steps.
3) Reward Function: In order to adapt robot movements
to perform well in novel situations, considering only hard
constraints such as obstacle avoidance, Eq.( 12), does not
suffice. Thus, our framework further models a reward function f (y, xc , yD ) that reflects the amount of rewards that an
adapted trajectory y can gain within the context xc and yD .
As the reward function f (y) is assumed temporally discrete
in Eq.( 6), we model the reward function ft (y(t)) at t by
three parts:
ft (y(t); w) = fD,t (y(t); wD ) + fC,t (y(t); wC ) + fE,t (y(t); wE ),
(15)
where the Imitation Reward fD models the tendency to follow
the imitation trajectory yD , the Control Reward fC models
the smoothness of executing the adapted trajectory y and
the Response Reward fE characterize the expected response
to the environment. Meanwhile, w = [wDT , wCT , wET ]T are parameters that affect the behavior of the movement adaptation.
We describe each reward function in detail as follows.
a) Imitation Reward: Imitation Reward characterizes
how well the adapted trajectory can imitate the demonstrations through the distance between points on y and yD .
Recall that we have the variance ΣD (t) of the imitation
trajectory yD by Movement Imitation IV-A.2, which indicates how flexible we could adapt the trajectory. Considering
ΣD (t) = diag(σ21 (t), . . . , σ2d (t)) to be diagonal for the sake of
simplicity, we model the Imitation Reward by the weighted
distance:
fD,t (y(t); wD ) = −(y(t) − yD (t))T V (t)(y(t) − yD (t))

(16)

−σ21 (t)

(17)

V (t) = diag(wD )diag(e

−σ2d (t)

,...,e

),

where V (t) is a weight matrix consisting of parameters wD
2
and {e−σi (t) } in which the variances learned from demonstrations ΣD (t) are modeled to affect adaptation rewards.
b) Control Reward: Control Reward fC characterizes
the smoothness of executing the adapted trajectory y through
the following formulation:
fC,t (y(t); wC ) = −wC k(y(t) − y(t − 1))k2 ,

(18)

where wC is the parameter to weigh this reward.
c) Response Reward: Response reward fE describes
the expected response to the environment such as safety
considerations for obstacles and objects under manipulation.
Here we give intuitive examples for Response Reward. Although we can ensure minimum distance to avoid obstacles
using Eq.( 14), as human users we still expect the robot
to transfer a cup full of water around a laptop instead of
above it, in case of spilling. Another example is that the user
would prefer the robot manipulating sharp objects, such as a
knife, to keep a relatively larger distance from the human for
safety consideration. These examples indicate that we would
have preferences towards how the robot avoids obstacles.

Moreover, for safety consideration, we also prefer the robot
to transfer a fragile object closer to the table top to maintain
a safety margin. All the above preferences are specific to
objects under manipulation and the exact environment. Thus,
we set the Response Reward to ensure that the better the
adapted trajectory fulfills these preferences, the higher the
reward is.
To formally represent the Response Reward, let us consider a scenario with Nob j obstacles on the table. The leftmost
and rightmost locations of the table are B1 , B2 and the table
surface is S, we then can formulate Response Rewards as
follows:

 Nob j

X
T

(19)
fE,t (y(t); wE ) = −  wO,k φO,k + wB φB + wS φS 
k=1
h
i
φTO,k = −kE(y(t)) − Ok k, (E(yD (t)) − E(y(t)))T


2
kk
· exp − kE(y(t))−O
dk
(20)
φB =

2
X
i=1

!
kE(y(t)) − Bi k2
exp −
dmin

φS = kE(y(t)) − S k2 ,

(21)
(22)

where φO,k represents the feature vector for preferences in
avoiding obstacle Ok , of which the first element denotes
avoiding distance and the second element denotes the deviation vector as shown in Fig. 3. The preferred deviation vector
is given as reward weights and the inner product between two
vectors indicates the rewards of deviation considering the
given preference. The exponential decay function is applied
so that the features are only effective when the robot’s endeffector is close to the obstacles. φB and φS are features
related to safety by considering boarders and surface of
T
T
the table. wE = [wO,1
, . . . , wO,N
, wB , wS ]T are weights
ob j
corresponding to the features respectively.
Given a set of parameters w = [wDT , wCT , wET ]T , the
MPC module generates an adapted trajectory by maximizing
f (·; w). The robot could follow the adapted trajectory and
execute the task facing the novel situation. However, the generated trajectory may not be satisfying enough from user’s
perspective, since the given or initialized parameters may not
be accurate for modeling the rewards. To accommodate the
issue, after the movement execution, our system allows the
user to provide a better trajectory as feedback to update the
parameters during the following Rewards Learning section.

Fig. 3. Illustration of deviation vector feature: vector from original imitation
trajectory to an adapted one.

Algorithm 1 Rewards Learning for Movement Adaptation

C. Rewards Learning
In this section, we describe how our system learns the
reward function. Assuming there is an oracle reward function f ∗ (y, xc , yD ) that reflects exactly how much reward
the adapted trajectory y can gain for each context. The
goal of this module is to estimate such a reward function
f (y, xc , yD ; w), where w are the parameters to be learned,
that approximate the oracle reward f ∗ (·) tightly.
By rewriting Eq.( 6) and Eq.( 15) for the entire trajectory,
we can have the reward function in a linear form represented
by features and weights:
f (y, xc , yD ; w) = wDT φD + wCT φC + wET φE
(23)
T
X



2
φD = φD,1 , . . . , φD,d T , φD,i = −
yi (t) − yD,i (t) 2 e−σi (t)
t=0

(24)
φC = −
φE = −

T
X

k(y(t) − y(t − 1))k2

(25)

t=1
T h
X

iT
φTO,1 (y(t)), . . . , φTO,Nob j (y(t)), φB (y(t)), φS (y(t))

t=0

Initialize w(0) = [wD(0)T , wC(0)T , wE(0)T ]T
for Iteration i = 0 to T l do
Task Context and Environment Perception: x(i)
c
Movement Imitation:
(i)
yD(i) , Σ(i)
D ← p(T |xc )
Movement Adaptation:
(i)
(i)
π ∗(i) = arg maxπ f (y, x(i)
c , yD ; w )
(i)
∗(i)
y ←π
Movement Execution: y (i)
if User Provides Feedback: ȳ (i) then
√
α(i) = 1/ i
wD(i+1) = wD(i) + α(i) (φD (ȳ (i) , yD(i) ) − φD (y (i) , yD(i) ))
wC(i+1) = wC(i) + α(i) (φC (ȳ (i) ) − φC (y (i) ))
(i)
(i)
wE(i+1) = wE(i) + α(i) (φE (ȳ (i) , x(i)
c ) − φE (y , xc ))
Weights Projection:
w̄(i+1) = [wD(i+1)T , wC(i+1)T , wE(i+1)T ]T
w(i+1) = arg minw∈C kw − w̄(i+1) k2
else w(i+1) = w(i)
end if
end for

(26)
where φD , φC , φE represent features of the entire trajectory
corresponding to Imitation, Control and Response Rewards.
Since the user only provides a feedback trajectory ȳ and
the system can not directly observe the reward function, we
apply the co-active learning technique [15] in which the robot
iteratively updates the parameter w of f (·; w) based on user’s
feedback. Note that this feedback only needs to indicate
f ∗ (ȳ, xc , yD ) > f ∗ (y, xc , yD ) and ȳ could be non-optimal
trajectories. Algorithm 1 gives our learning algorithm for
movement adaptation.
Note that α is a learning rate, which decays along iterations and C in the weights projection part is a bounded
set to ensure updated parameters w are in feasible space.
After iterations of improvements, the robot can learn an estimated reward function f (·; w∗ ) that approximates the oracle
reward function f ∗ (·) as proved in [18]. By maximizing
the estimated reward function f (y, xc , yD ; w∗ ), the robot can
generate an adapted trajectory y that maximizes the rewards
facing situation xc based on imitation trajectory yD .
V. Experiments
To validate the system described above, we design and
conduct the following experiments on a Baxter humanoid
platform. The Baxter robot is asked to do manipulation
tasks such as cleaning on a table top, with the surface as
S = (0, 0, −0.1), the leftmost location as B1 = (0, 0.8, 0) and
the rightmost location as B2 = (0, −0.8, 0) in robot spatial
space in meter. It needs to learn transferring the manipulated
object between different locations while avoiding obstacles
with desired manners.
During an off-line learning phase, the robot learns the
movement skill from multiple kinethestic demonstrations
with no obstacles on the table. During the online learning

(a)

(b)

Fig. 4.
Movement Imitation with ProMPs for Transferring Task: (a)
Imitation trajectory predicted based on prior movement and task contexts
in spatial space; (b) Imitation trajectory for joint s0 in joint space, shaded
area indicating the predicted variance.

stage, a variety of obstacles are located randomly on the
table and we assume the robot can obtain their locations from
perception modules. The system learns iteratively to adapt
the movement skill in novel situations such as with different
manners avoiding obstacles, at the same time follows the
similar movement pattern from off-line demonstrations.
A. Movement Imitation
In the first stage of the experiments, we have our robot
learn off-line the movement skill from demonstrations. All
trajectories are sampled discretely and normalized to T =
200 steps for transferring movement in joint space, and the
left arm of the Baxter has d = 7 degree of freedom. The
training trajectories are encoded by ProMPs with n = 10
Gaussian basis functions so that the movement skill can be
generalized to different initial and target states.

(a)

(b)

(c)

Fig. 5. Learning to Adapt Movement for Transferring a Leaking Bottle: (a) Movement Imitation, failed to avoid the obstacle; (b) Movement adaptation with
initial weights, successfully avoided the obstacle by path above it but has a potential danger of spilling water, feedback trajectory is provided afterwards; (c)
Movement adaptation for a different situation with new task contexts and obstacle locations, with updated weights after learning from feedback trajectory,
successfully avoids the obstacle through a path around. Corresponding execution on the Baxter platform is given by Fig. 2.

Fig. 4(a) shows an example of our generated imitation trajectory in spatial space for new task contexts using ProMPs.
Fig. 4(b) shows the corresponding imitation trajectory of
joint s0 in joint space. The blue crosses here are desired new
initial and target states, and the shaded area is the estimated
variance for imitation trajectory, which reflects the variations
of demonstrations. True trajectory here means a trajectory
recorded from user demonstration in the testing scenario for
comparison. It is not hard to see that the predicted mean
of the imitation trajectory is well generalized to new initial
and target states and follows the same movement pattern
as the prior mean trajectory learned from demonstrations.
Therefore, the robot can perform the task well by following
this imitation trajectory if there is no obstacles or other safety
constraints under new situations.
B. Learning Adaptation
While facing a task of transferring a leaking bottle, the
robot may find a bowl with food inside as an obstacle on
the table where its center location O1 and minimum safety
distance d1 are assumed to be obtained through perception.
For movement adaptation, we set the prediction horizon
T p = 11 in model predictive control and select system
matrices A = 0.9 · I, B = C = I to make the system stable
in the prediction window as suggested in [13]. The limits of
joints could be found from the Baxter hardware specification.
The minimum safety distance with table boarder is set
as dmin = 0.1. And the weights for reward function are
initialized to be wD = 30 · 1, wC = 10, wE = 0. And then we
apply the native Matlab Gradient-based optimization method
fmincon to solve the optimization at each time step.
Fig. 5(a) shows the output from movement imitation for
transferring the leaking bottle, which failed to avoid the
obstacle even though the trajectory generalizes to a novel
initial and target states. Fig. 5(b) shows the movement adaptation with initial weights. There is no preference specified in
reward function about how to avoid obstacles or take safety
considerations about boarders. Therefore, even though the

adapted trajectory could avoid the obstacle successfully, it
may be not an ideal trajectory.

(a)

(b)

Fig. 6. Rewards Learning from User Feedback for Transferring Leaking
Bottle: (a) User feedback via kinethestic demonstration; (b) Learning curve
for adaptation under the same feedback.

To learn the user preference, we then provide feedback
via kinethestic demonstration illustrated in Fig. 6(a) and
the feedback trajectory is shown in Fig. 5(b) as dash line
to indicate user preferences. Following Algo. 1, the robot
iteratively updates the rewards weights based on the user
feedback. Weights are limited via projection in the feasible
set C where wD ∈ [1, 100]7 , wC ∈ [1, 100], wE ∈ [0, 100]
except that last two parameters in wO,k indicating preferred
deviation direction could be [−100, 100]. To quantitatively
validate the performance of our method in movement adaptation, we consider the metric of cumulative error between
the adapted
trajectory and the feedback trajectory e(i) =

2
1 PT
(i)
ȳ
(t)
− y (i) (t) as the learning error at iteration i.
t=0
T
Since the metric is affected by different situations such as
obstacles’ locations, we consider the feedback trajectory as
fixed and let the robot iteratively learn several times to see
how it performs and record the “learning curve” under the
same feedback. From Fig. 6(b), we can see that the error
decreases and converges after several iterations, and it only
requires a few of iterations to achieve an adapted trajectory
as desired preference according to the feedback.
After learning, the robot uses the updated weights for
movement adaptation in a different situation with novel

initial/target states and the obstacles’ locations. Fig. 5(c)
shows the adapted trajectory based on the updated weights
after one iteration, where it successfully avoids the obstacle
via the desired direction.
In a second scenario where a robot is transferring a knife
around some fragile obstacle, the user may prefer robot
to avoid the obstacle above it instead of around it. With
the same methods here, we could also generate adapted
trajectories as shown in Fig. 7(a) and Fig. 7(c) for initial
weights. With the user provided feedback trajectory, the
robot successfully learns the user specified preferences for
movement adaptation and generates the improved adapted
trajectories for different situations as shown in Fig. 7(b) and
Fig. 7(d).

(a)

(c)

(b)

(d)

Fig. 7.
Baxter Learning to Adapt Movement for Transferring Knife:
(a) (c) Movement adaptation with initial weights, successfully avoided
duck doll around it but may risk scratches, afterwards feedback trajectory
is provided for adaptation preferences; (b) (d) Movement adaptation for
different situation, with updated weights after learning from feedback
trajectory, successfully avoided the duck doll above it as desired.

VI. Conclusion and Future Work
We present a framework for learning to adapt robot end
effector movement for manipulation tasks. The proposed
method generalizes offline learned movement skills to novel
situations considering obstacle avoidance and other taskdependent constraints. It adapts the imitation trajectory generated from demonstrations, while maintaining the learned
movement pattern and considering the variations, to avoid
obstacles with desired directions and distances and keep a
safety margin within a workspace. Also it provides a way
to learn how to adapt the movement by on-line interactions
from user’s feedback.
Besides learning how to adapt movement from user’s
feedback, the visual information of the objects and the
environment could also indicate the preferences of movement
adaptation. For instance, the deviation direction for avoiding

a knife could be inferred directly from the location of
its blade from visual space. We are further investigating
the possibility of directly learning the preferences to adapt
movement from visual perception of the task context.
References
[1] A. J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,
“Dynamical movement primitives: learning attractor models for motor
behaviors,” Neural computation, vol. 25, no. 2, pp. 328–373, 2013.
[2] T. Asfour, P. Azad, F. Gyarfas, and R. Dillmann, “Imitation learning
of dual-arm manipulation tasks in humanoid robots,” International
Journal of Humanoid Robotics, vol. 5, no. 02, pp. 183–202, 2008.
[3] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, “Learning and generalization of motor skills by learning from demonstration,” in Robotics
and Automation, 2009. ICRA’09. IEEE International Conference on.
IEEE, 2009, pp. 763–768.
[4] J. Kober, B. Mohler, and J. Peters, “Learning perceptual coupling for
motor primitives,” in Intelligent Robots and Systems, 2008. IROS 2008.
IEEE/RSJ International Conference on. IEEE, 2008, pp. 834–839.
[5] A. Gams, A. J. Ijspeert, S. Schaal, and J. Lenarčič, “On-line learning
and modulation of periodic movements with nonlinear dynamical
systems,” Autonomous robots, vol. 27, no. 1, pp. 3–23, 2009.
[6] F. Guenter, M. Hersch, S. Calinon, and A. Billard, “Reinforcement
learning for imitating constrained reaching movements,” Advanced
Robotics, vol. 21, no. 13, pp. 1521–1544, 2007.
[7] S. Calinon, F. D’halluin, E. L. Sauser, D. G. Caldwell, and A. G.
Billard, “Learning and reproduction of gestures by imitation,” Robotics
& Automation Magazine, IEEE, vol. 17, no. 2, pp. 44–54, 2010.
[8] S. M. Khansari-Zadeh and A. Billard, “Imitation learning of globally
stable non-linear point-to-point robot motions using nonlinear programming,” in Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ
International Conference on. IEEE, 2010, pp. 2676–2683.
[9] T. Inamura, I. Toshima, H. Tanie, and Y. Nakamura, “Embodied symbol emergence based on mimesis theory,” The International Journal
of Robotics Research, vol. 23, no. 4-5, pp. 363–377, 2004.
[10] A. Paraschos, C. Daniel, J. R. Peters, and G. Neumann, “Probabilistic
movement primitives,” in Advances in neural information processing
systems, 2013, pp. 2616–2624.
[11] D.-H. Park, P. Pastor, S. Schaal et al., “Movement reproduction and
obstacle avoidance with dynamic movement primitives and potential
fields,” in Humanoid Robots, 2008. Humanoids 2008. 8th IEEE-RAS
International Conference on. IEEE, 2008, pp. 91–98.
[12] H. Hoffmann, P. Pastor, D.-H. Park, and S. Schaal, “Biologicallyinspired dynamical systems for movement generation: automatic realtime goal adaptation and obstacle avoidance,” in Robotics and Automation, 2009. ICRA’09. IEEE International Conference on. IEEE,
2009, pp. 2587–2592.
[13] A. M. Ghalamzan E., C. Paxton, G. D. Hager, and L. Bascetta,
“An incremental approach to learning generalizable robot tasks from
human demonstration,” in Robotics and Automation (ICRA), 2015
IEEE International Conference on. IEEE, 2015, pp. 5616–5621.
[14] E. A. Sisbot, L. F. Marin, and R. Alami, “Spatial reasoning for human
robot interaction,” in Intelligent Robots and Systems, 2007. IROS 2007.
IEEE/RSJ International Conference on. IEEE, 2007, pp. 2281–2287.
[15] A. Jain, S. Sharma, T. Joachims, and A. Saxena, “Learning preferences
for manipulation tasks from online coactive feedback,” The International Journal of Robotics Research, p. 0278364915581193, 2015.
[16] R. Mao, Y. Yang, C. Fermuller, Y. Aloimonos, and J. S. Baras,
“Learning hand movements from markerless demonstrations for humanoid tasks,” in Humanoid Robots (Humanoids), 2014 14th IEEERAS International Conference on. IEEE, 2014, pp. 938–943.
[17] Z. Ju, C. Yang, and H. Ma, “Kinematics modeling and experimental
verification of baxter robot,” in Control Conference (CCC), 2014 33rd
Chinese. IEEE, 2014, pp. 8518–8523.
[18] G. Ciná and U. Endriss, “Proving classical theorems of social choice
theory in modal logic,” Autonomous Agents and Multi-Agent Systems,
pp. 1–27, 2016.

