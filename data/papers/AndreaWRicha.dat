Self-Stabilizing Leader Election for
Single-Hop Wireless Networks despite Jamming
Andrea Richa1 , Christian Scheideler2 , Stefan Schmid3 , Jin Zhang1
1

Computer Science and Engineering, SCIDSE, Arizona State University, Tempe, AZ 85287, USA; {aricha,jzhang82}@asu.edu
2 Department of Computer Science, University of Paderborn, D-33102 Paderborn, Germany; scheideler@upb.de
3 Deutsche Telekom Laboratories & TU Berlin, D-10587 Berlin, Germany; stefan@net.t-labs.tu-berlin.de

ABSTRACT
Electing a leader is a fundamental task in distributed computations.
Many coordination problems, such as the access to a shared resource, and the resulting inefficiencies, can be avoided by relying
on a leader. This paper presents S ELECT, a leader election protocol for wireless networks where nodes communicate over a shared
medium. S ELECT is very robust in two respects. First, the protocol is self-stabilizing in the sense that it converges to a correct
solution from any possible initial network state (e.g., where no or
multiple nodes consider themselves a leader). This is an appealing
property, especially for dynamic networks. Second, the described
protocol is resilient against a powerful reactive jammer that blocks
a significant fraction of all communication rounds. The reactive
model is general and of interest beyond jamming (e.g., in the context of co-existing networks). The paper also reports on experimental results obtained from our simulation framework which allows us
to study convergence behavior under different types of adversarial
jammers.

Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and WideArea Networks—Access schemes; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms and Problems—Sequencing and scheduling

General Terms
Algorithms, Reliability, Theory

Keywords
Wireless Ad-hoc Networks, MAC Protocols, Jamming

1.

INTRODUCTION

Leader election is a classical theme in the field of distributed algorithms. Once a leader is determined, many coordination tasks
are simplified. In this paper, we consider the problem of electing a leader in a wireless network in order to coordinate access to a

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Mobihoc’11, May 16-19, 2011 Paris, France
Copyright 2011 ACM 978-1-4503-0722-2 ...$10.00.

shared communication medium. This can greatly improve the overall throughput of the network and reduce the energy consumption
at the nodes.
We focus on a harsh environment where the wireless nodes contend for a single wireless channel that is jammed by a powerful
reactive adversary blocking an arbitrary constant fraction of all
time slots. In addition, we require that the election protocol works
correctly if started from any initial network state—i.e., it is selfstabilizing. This implies that arbitrary join and leave behavior can
be tolerated. For example, when a leader node leaves the network,
a substitute leader is elected.
Disruptions of the shared communication medium—either due to
interference of concurrent transmissions or adversarial jamming—
is one of the foremost challenges in wireless computing. Jamming
attacks are a very cumbersome problem as they are typically easy to
implement and the attacker does not need any special hardware. For
example, it has been pointed out that the widely used IEEE 802.11
medium access control (MAC) protocol already fails to handle simple, oblivious jammers [6].
Model. We consider the problem of designing a self-stabilizing
distributed protocol to elect a leader among a set V of n simple
wireless nodes (e.g., nodes of a sensor network) that are within each
other’s transmission range and communicate over a single channel. For our formal analysis, we assume that the time proceeds in
synchronous rounds (or steps).1 In each round, a node may either
transmit a message or sense the channel, but it cannot do both, and
there is no immediate feedback mechanism telling a node whether
its transmission was successful.2 A node which is sensing the channel may either (i) sense an idle channel (in case no transmission
takes place at that round), (ii) sense a busy channel (in case two or
more nodes transmit at the current round), or (iii) receive a packet
(in case exactly one node transmits at that round). Henceforth, we
will sometimes say that a message is successfully sent if there is
exactly one transmission at this round. Thus, if a message is successfully sent, all nodes will successfully receive it in this round,
except for the sender itself (the sender does not know whether the
transmission was successful).
In addition to these nodes there is an adversary. We allow the
adversary to know the protocol and its entire history and to use
this knowledge in order to jam the wireless channel at will at any
round. Such an adversary is called adaptive. If in addition to that
the adversary also knows (through physical carrier sensing) the cur1
A round may represent the time needed to send a message, e.g., a
multiple of the 50µs unit in 802.11, depending on the message size.
2
We believe that such a feedback mechanism is problematic in the
broadcast setting as it increases the communication load, and its
benefits are not clear either. Moreover, this assumption strengthens
our result.

rent channel state, we call it reactive. That is, a reactive adversary can distinguish between the channel being currently idle (no
node transmits) or busy (either because of a successful transmission, a collision of transmissions, or too much background noise)
and can instantly make a jamming decision based on that information. Whenever the adversary jams the channel, all nodes will
notice a busy channel. The nodes cannot distinguish between the
adversarial jamming and a collision of two or more messages that
are sent at the same time.
In order to study the degree of jamming activity needed by the
adversary to prevent successful message transmissions, we use the
notion of a (T, 1 − )-bounded adversary. An adversary is called
(T, 1 − )-bounded for some T ∈ N and 0 <  < 1 if for any time
window of size w ≥ T the adversary can jam at most (1 − )w
of the time steps in that window. Moreover we assume that the
n nodes use an encryption mechanism that prevents the adversary
from inspecting their messages.
As mentioned earlier, our goal is to design a leader election protocol that is self-stabilizing despite adversarial jamming. Following the usual notation in the self-stabilization literature, the system
state is determined by the state of all variables in the system. That
is, the protocol and any constants used by the protocol are assumed
to be immutable and not part of the system state. A system is called
self-stabilizing if and only if (1) when starting from any state, it is
guaranteed to eventually reach a legal state (convergence) and (2)
given that the system is in a legal state, it is guaranteed to stay in
a legal state (closure), provided that there are no faults or membership changes in the system. In our case, roughly speaking, the
legal state is the state in which we have exactly one leader. We will
define the set of legal states more formally when we introduce our
protocol. While our protocol is randomized and the leader election has to be performed under adversarial jamming, our protocol
is still guaranteed to eventually elect exactly one leader from any
initial state.
Related Work. Leader election is an evergreen in distributed algorithms research due to its numerous applications, and there exist
many theoretical and practical results [4, 12, 18, 20, 22, 24, 28,
29]. Please refer to the following two books for a good introduction: Chapter 3 in [3] and Chapter 8 in [15]. A leader election algorithm should be as flexible as possible in the sense that a correct
solution is computed independently of the initial network state. For
instance, the algorithm should be able to react to a leader departure,
or be able to cope with situations where for some reasons, multiple
nodes consider themselves leaders. So-called self-stabilizing algorithms [10] with good convergence times are an active research
topic (see e.g., the works on time-adaptive self-stabilization such
as [19]), and several self-stabilizing leader election protocols are
known already, e.g., [2, 8, 16] (see also the fault-contained solutions such as [13]). None of these approaches allows us to elect a
leader in a wireless network that is exposed to harsh interference or
even adaptive jamming. However, interruptions of communication
is often unavoidable in wireless systems, and we believe that electing a leader can be particularly useful in such harsh environments.
Interference (either due to collisions or jamming) on the shared
medium renders the task of electing a leader challenging; on the
other hand, once a leader is determined, throughput may be improved significantly due to the coordinated medium access. It is
well-known that jamming attacks are often simple and cheap to
implement, and there exists a large body of literature on the subject [11, 14, 17, 25]. Only the closest results to ours can be discussed here, and for a broader overview on the field, we refer the
reader to the literature reviews provided in the corresponding papers.

Classic defense mechanisms operate on the physical layer [21,
23] and there exist approaches both to avoid as well as to detect
jamming. Spread spectrum and frequency hopping technologies
have been shown to be very effective to avoid jamming with widely
spread signals. These physical layer solutions are orthogonal to our
work, and can improve the robustness of the protocol presented
here further. However, the ISM frequency band used by IEEE
802.11 variants is too narrow to effectively apply spread spectrum
techniques [7].
Recent work has also studied MAC layer strategies against jamming, including coding strategies (e.g., [9]), channel surfing and
spatial retreat (e.g., [1, 31]), or mechanisms to hide messages from
a jammer, evade its search, and reduce the impact of corrupted messages (e.g., [30]). Unfortunately, these methods do not help against
an adaptive jammer with full information about the history of the
protocol, like the one considered in our work.
The works closest to ours are the resilient MAC protocols studied in [5, 26, 27]. In particular, our adversarial model was introduced by Awerbuch et al. [5] who present a jamming-resistant
MAC protocol that guarantees a constant throughput against an
adaptive adversary in a single-hop environment. The MAC protocol has been adapted for multi-hop wireless networks [26] as well
as for reactive jammers [27]; interestingly, a competitive throughput can be obtained even in these more general scenarios, although
the nodes’ cumulative sending probability may vary within a larger
range compared to the single-hop, non-reactive scenario. Our work
complements this line of research on MAC protocols [5, 26, 27]
and focuses on a fundamental application in wireless networks:
the problem of electing a leader. To achieve this, our leader election protocol builds upon the techniques described in [5] to adjust
medium access probabilities in a multiplicative manner to resolve
contention efficiently and recover from jammed time periods, and
introduces a scheme on top of the MAC protocol that allows the
nodes to obtain a consistent view on the application’s state and the
presence of potential leaders. (Note that we slightly adapted the
medium access protocol itself and tailored it for the leader election
problem: for instance, leaders increase their medium access probabilities faster than followers in order to increase the likelihood of a
successful leader message transmission.)
A leader election protocol has already been sketched briefly
in [5]. However, this algorithm is rather preliminary and not robust. For instance, if a leader leaves the network, the other nodes
cannot realize its absence in order to, e.g., elect a substitute. In
contrast, in this paper, we propose a self-stabilizing leader election
protocol that converges to the desired state from any initial configuration. We believe that this is a vital property in real networks
where membership is dynamic. Moreover, in contrast to [5], we
focus on a reactive jammer model, because (1) many nodes support
carrier sensing today; (2) the reactive model is more general and
hence more difficult than the adaptive model; and finally, note that
(3) a reactive model can also make sense in scenarios without jammers, e.g., in co-existing networks: many MAC protocols based on
carrier sensing activate nodes during idle time periods. It turns out
that protocols that perform well under adaptive jammers may have
a high convergence time under reactive jammers, and hence additional techniques are required. For example, by selective jamming,
a reactive adversary can find out certain (e.g., pseudo-random and
secret) communication patterns and become even more powerful in
the future. Moreover, in contrast to the protocol in [5], we synchronize of the nodes’ sending probabilities (which also improves
fairness).
Our Contribution. This paper presents S ELECT (“SElfstabilizing Leader EleCTion”), a protocol that solves the leader

election problem in harsh environments—namely in wireless
networks under adversarial reactive jamming—and in a selfstabilizing manner, independently of the initial network state. We
believe that self-stabilization is a crucial feature in real networks
where membership is often dynamic. Although our algorithm is
randomized, we will present a formal proof that its correctness
holds deterministically. Moreover, while our analysis is rather involved, the S ELECT protocol itself is simple and hence easy to implement.
Concretely, in this paper we will derive the following theorem.
T HEOREM 1.1. Given an arbitrary initial configuration and in
the absence of state faults, our leader election protocol reaches a
state where there is exactly one leader and n − 1 followers, despite
a reactive (T, 1 − )-bounded jammer, for any T and any constant
 > 0.
In S ELECT, the nodes do not have to know anything about the
system for the protocol to work. The only assumption that we need
is that some fixed common parameter γ used by the nodes satisfies
γ = O(1/(log T + log log n)). As log T and log log n are small
for all reasonable values of T and n, this is scalable and not a critical constraint, as it leaves room for a super-polynomial change in
n and a polynomial change in T over time.3 Thus, in practice we
expect that choosing γ to be a sufficiently small constant yields a
good performance for any practical network, which is confirmed by
our simulations.

2.

THE SELECT PROTOCOL

S ELECT is based on the following idea. Each node v maintains
a parameter pv which describes v’s probability of accessing the
medium at a given moment of time. That is, in each round, each
node v decides to transmit a message with probability pv (e.g., in
an attempt to become a leader). (This is similar to classic random
backoff mechanisms where the next transmission time t is chosen
uniformly at random from an interval of size 1/pv .) The nodes
adapt and synchronize their pv values over time in a multiplicative
increase multiplicative decrease manner, i.e., the value is lowered
in times of high interference or increased during times where the
channel is idling. However, pv will never exceed p̂, for some constant 0 < p̂ < 1.
In addition, each node maintains two variables, a threshold variable Tv and a counter variable cv . Tv is used to estimate the adversary’s time window T : a good estimation of T can help the nodes
recover from a situation where they experience high interference in
the network. In times of high interference, Tv will be increased and
the sending probability pv will be decreased.
Initially, every node v sets cv := 1 and pv := p̂. Note however
that while we provide some initial values for the variables in our
description, our protocol is self-stabilizing and works for any initial
variable values, as we will show in our proofs.
S ELECT distinguishes between two node roles: follower and
leader. We use sv to indicate the role of the node: sv = 1 means
that node v is a leader, whereas sv = 0 means v is a follower. The
basic idea of our protocol is to divide time into intervals of a small
number of rounds specified by the constant parameter b > 5 (we
use the variable mc as a modulo counter); in the following, we will
refer to a sequence of rounds between two consecutive mc = 0
events as a b-interval. (Of course, it can happen that all b slots of
an interval are jammed.)
3
On the other hand, note that the assumption that the nodes know
constant factor approximations of n or T directly would render the
problem trivial. Moreover, such an assumption is unrealistic and
non-scalable.

Our protocol is based on the concept of so-called leader slots,
special rounds—in each b-interval through which S ELECT cycles—
in which leaders are obliged to send an alive message (a so-called
leader message) and in which followers keep silent. The idea is
that the followers learn that the leader has left in case of an idling
medium during a leader slot (of course, the leader slots may be
jammed!) and a new election is triggered automatically.
S ELECT uses four leader slots:4 ls1 , ls2 , ls3 and ls4 . Of course,
in the beginning, all nodes may have different ls values and may
disagree on which slots during the b-interval are leader slots. However, over time, the nodes synchronize their states and a consistent
view emerges. For the synchronization, five temporary variables
ls00 , ls01 , ls02 , ls03 , and ls04 are used, which store future ls values.
Depending on whether the node is of type follower or leader, the
leader slots are updated differently: At the beginning of a new binterval, a leader copies its ls0i values to the lsi values. A follower
on the other hand copies the ls0 values “diagonally” in the sense
that ls0i is copied to ls0i+1 for i ∈ {0, 1, 2, 3}. As we will see, this
mechanism ensures that an elected leader covers the leader slot ls3
of each follower. (S ELECT guarantees that the reactive adversary
has no knowledge about the ls3 slots at all until it is already too late
to prevent a successful election.) Another special slot besides ls3
is ls00 which is a random seed to mix the execution for increased
robustness.
In Figure 1 we give the detailed formal description of the follower and the leader protocol, respectively. Recall that our algorithms can tolerate any initial values of mc, pv , Tv , cv , sv , s0v , ls1 ,
ls2 , ls3 , ls4 , ls00 , ls01 , ls02 , ls03 , ls04 . For instance, in the beginning,
all nodes v may be leaders and for all v, sv = 1. However, the fixed
parameters used by the algorithms, namely p̂, γ, or b, are assumed
to be immutable.
Both the follower and the leader algorithm consist of three main
parts. The b-interval wise update (Lines 2 − 4) makes sure that ls
values are refreshed frequently. Lines 6 − 33 (in case of a follower)
and Lines 5−24 (in case of a leader) are used for medium access in
order to synchronize the nodes’ states (by a message that includes
cv , Tv , and pv values) and give nodes the chance to become or
remain leader (by a ‘LEADER’ message). The last sections of the
algorithms are used to react to high interference (by reducing pv )
and to reset leader slots. The reason for checking whether ls3 is
undefined in Line 6 of the follower protocol is to keep the leader
slots hidden from the reactive adversary until it is already too late
to prevent a successful leader election.5
Both the follower and the leader protocol depend on the following crucial C ONDITION.
D EFINITION 2.1 (C ONDITION ). We define C ONDITION
(Line 37 for followers, and Line 28 for leaders) as the event that at
least one ‘LEADER’ message was received during the past b · Tv
steps.
The idea is that if C ONDITION is fulfilled, we know that the protocol is already in a good state. Moreover, we will see that the
adversary cannot prevent C ONDITION to become true for a long
time as the Tv values would continue to increase.
Finally, also note that leaders increase pv faster (i.e., by larger
multiplicative factors) during idle rounds than followers. With this
mechanism, S ELECT improves the likelihood that a ‘LEADER’
message gets through and hence that a unique leader is elected.
4
It is an open question whether a protocol with less leader slots can
be devised.
5
This check would not be necessary against a non-reactive adversary.

Algorithm 1 Leader Election: Follower
1: mc := cv mod b
2: if mc = 0 then
Algorithm 1 Leader Election: Follower
0
0
0
0
3:
ls1 := ls1:
2 :=
3 :=bls2 , ls4 := ls3
0 , ls
mc
:=ls
cv1 , ls
mod
Algorithm 2 Leader Election: Leader
4:
sv := s0v2: if mc = 0 then
1: mc := cv mod b
0
0
0
5: end if
3:
ls1 := ls0 , ls2 := ls1 , ls3 := ls2 , ls4 :=2:lsif03 mc = 0 thenAlgorithm 2 Leader Election: Leader
6: if (ls3 = undefined)
or
(mc
=
6
0 ls1 and mc 6= ls2 and
4:
sv := sv
3:
ls1 := ls01 ,1:ls2mc
:=:=
ls02c, ls3mod
:= lsb03 , ls4 := ls04
v
mc 6= ls3 and5:mc
6= if
ls4 ) then
end
4: end if
2: if mc = 0 then
7:
v decides6:with
pv to=send
a follower
if (ls
undefined)
or message
(mc 6= ls1 and mc 6=
and
5: ls
if 2mc
= ls1 or3:mc = ls
ls2 :=
or ls
mc
3
0 = ls3 or 0 mc = ls4 0
0
8:
if v sends a mc
follower
1
1 , ls2 := ls2 , ls3 := ls3 , ls4 := ls4
6= ls3message
and mcthen
6= ls4 ) then
then
4: end if
9:
the message contains:
7:
v decides with pv to send a follower message
6:
v sends the leader message ‘LEADER’
5: if mc = ls1 or mc = ls2 or mc = ls3 or mc = ls4
10:
cc1 := ls00 , cc2 := ls01 , cc3 := ls02 , cc4 := ls03 ,
8:
if v sends a follower message then
7: else
then
cnew := cv , Tnew := Tv , pnew := pv
9:
the message contains:
8:
v decides with pv to send ‘LEADER’
11:
end if
6:
v sends the leader message ‘LEADER’
9: 4 := ls
if03v, does not send ‘LEADER’ then
10:
cc1 := ls00 , cc2 := ls01 , cc3 := ls02 , cc
12: end if
7: else
v senses the channel
cnew := cv , Tnew := Tv , pnew :=10:
pv
13: if v does not send a follower message then
8:
v decides with pv to send ‘LEADER’
11:
if channel is idle then
11:
end if
14:
v senses the channel
if v{(1
does
not
2 send ‘LEADER’ then
12:
p9:v := min
+ γ)
pv , p̂}
12: end if
15:
if channel is idle then
senses the
13:
else 10:
if v receives a vmessage
thenchannel
13:
if
v
does
not
send
a
follower
message
then
16:
if mc = ls3 then
11:
if
channel
is idle then
−1
14:
p
:=
(1
+
γ)
p
v
v
14:s0 := 1v senses the channel
2
17:
12:
pv := min
v
15:
if
message
is
‘LEADER’
then{(1 + γ) pv , p̂}
15:p := p̂if channel is idle then
18:
v
13: s := 0, else
if 0v receives a message then
0
16:
s
:=
v
16:
if mc = ls3 then
v
−1
19:
else
14: ls
pv :=ined,
(1 + γ)
0
17:
:= undef
ls02 pv:=
3
1
17:p := min {(1 + sγ)p
v :=
20:
v
v , p̂}
15: undef ined if message is ‘LEADER’ then
18:if
pv := p̂
21:
end
16:
sv := 0,message,
s0v := 0
18:
else if message is a follower
else then
0
22:
else if v19:
receives ‘LEADER’
17:
:=4 , cnew
undef
3 3 , cc
i.e., a tuple of {cc1 , cc2ls
, cc
, ined, ls2 :=
0 20:
p
:=
min
{(1
+
γ)p
,
p̂}
v
v
23:
sv := 0
undef ined
T
,
p
}
then
new
new
end if
24:
ls321:
:= undef ined
18: cv := cnew , Tvelse
message is a follower message,
19:
:= if
Tnew
else
if v receives ‘LEADER’ then
25:
ls0222:
:= undef
ined
0
a tuple 0of {cc1 3, ,cc2 , cc3 , cc4 , cnew ,
20:
ls01 := cc1 , lsi.e.,
2 := cc2 , ls3 := cc
s0vof:=
26:
else if v23:
receives a tuple
{cc01 , cc2 , cc3 , cc4 , cnew ,
ls04 := cc4 Tnew , pnew } then
ls3 := undef ined
Tnew , p24:
new } then
19:
cv := cnew , Tv := Tnew
21:
end if
ls02 := undef ined
27:
Tv25:
:= Tnew
ls01 := cc1 , ls02 := cc2 , ls03 := cc3 ,
end 20:
if
−1if v receives a tuple of {cc , cc , cc 22:
else
,
cc
,
c
,
28:
pv 26:
:= (1 + γ)
pnew
1
2
3
4 new
ls04 := cc4
23:
end if
29:
cv := cnew Tnew , pnew } then
24: end if
21:
end if
0
30:
ls027:
:= random(0,
− 1)
Tvb :=
Tnew
25: cv := cv + 1 22:
end if
0
31:
ls0128:
:= cc1 , ls02 :=pvcc:=
:=γ)cc−1
(103 +
2 , ls
3 ,pls
new
4 := cc4
26: if cv ≥ b · Tv then
23:
end if
32:
end if 29:
cv := cnew
27:
cv := 0 24: end if
33: end if
30:
ls00 := random(0, b − 1)
28: 0
if (not C ONDITION
25: cv := )cvthen
+1
34: cv := cv + 131:
ls01 := cc1 , ls02 := cc2 , ls03 := cc3 ,29:
ls4 := cc4 p := (1 +
−1
:= Tv + 1
v 26: if cγ)
· ,TTvvthen
v ≥ bpv
35: if cv ≥ b · Tv32:
then end if
0
30:
ls00 27:
:= undef
cv ined,
:= 0 ls1 := undef ined,
36:
cv := 033: end if
ls02 28:
:= undef
ined,C ONDITION
ls03 := undef
if (not
) then ined,
37:
if (not C
ONDITION
)
then
34: cv := cv + 1
ls04 :=
−1
29:undef inedpv := (1 + γ)−1 pv , Tv := Tv + 1
38:
pv 35:
:= (1
+
γ)
p
,
T
:=
T
+
1
v
v
if cv ≥ b · Tvv then
31:
else
30:
ls00 := undef ined, ls01 := undef ined,
39:
ls0036::= undef
ls01 := undef ined,
cv :=ined,
0
0
0
32:
Tv := max{Tv − 1,
0
0
ls4}
2 := undef ined, ls3 := undef ined,
ls237::= undef
ined,
ls3 := ) undef
if (not
C ONDITION
then ined,
33:
end if
ls04 := undef ined
:= undef ined
ls0438:
pv := (1 + γ)−1 pv , Tv := Tv + 134: end if
31:
else
40:
else 39:
ls00 := undef ined, ls01 := undef ined,
32:
Tv := max{Tv − 1, 4}
41:
Tv := max{Tv −ls1,0 4}:= undef ined, ls0 := undef ined,
2
3
33:
end if
42:
end if
ls04 := undef ined
34: end if
43: end if
40:
else
41:
Tv := max{Tv − 1, 4}
42:
end if
Figure 1: Algorithm for followers (left) and leaders (right).
43: end if
2

3.

ANALYSIS

This section shows that the randomized S ELECT protocol is
guaranteed to eventually reach a situation where there is exactly
one leader and n − 1 followers. We make use of the following
definitions. First, we define the system state.
D EFINITION 3.1 (S TATE AND S YSTEM S TATE ). The state of
node v is determined by the state of the variables pv , Tv , cv , sv ,
s0v , mc, ls00 , ls1 , ls01 , ls2 , ls02 , ls3 , ls03 , ls4 and ls04 . The state of the
system is the set of the states of all nodes.
We use the following LSL set to describe the union of all possible leader slot values present in the system.

D EFINITION 3.3 (F OLLOWER S TATE ). A state S is called
a follower state, denoted by S ∈ FOLLOWER, if all the
2
following conditions hold. (i) All nodes are followers (∀v ∈
V : sv = 0); (ii) for every node v: ls1 (v), ls2 (v), ls3 (v),
ls4 (v) ∈ [b] ∪{undef ined}, ls01 (v), ls02 (v), ls03 (v), ls04 (v) ∈
[b] ∪ {undef ined}, ls00 (v) ∈ [b]; (iii) the follower nodes can be
partitioned into two sets {v} and V \ {v}, according to their ls0
values (v is the node that successfully sent the last follower message); for each w ∈ V \ {v}: ls01 (w) = ls00 (v), ls02 (w) = ls01 (v),
ls03 (w) = ls02 (v), ls04 (w) = ls03 (v), and ls2 (w) = ls1 (v),
ls3 (w) = ls2 (v), and ls4 (w) = ls3 (v); (iv) for any pair of follower nodes v, w ∈ V with ls02 (v) ∈ [b] and ls3 (v) ∈ [b], cv = cw
and Tv = Tw .

D EFINITION 3.2 (T HE LSL S TATE S ET ). For any given system state, let LSL = {ls1 (v), ls2 (v), ls3 (v), ls4 (v) |v is leader}
\{undef ined}.

We use the concept of so-called pre-leader states, i.e., states that
result from follower states before some nodes become leaders.

The system can be in several special states which are formalized
next: follower states, pre-leader states, and leader states. Let [b] =
{0, . . . , b − 1}.

D EFINITION 3.4 (P RE - LEADER S TATE ). A state S is called
a pre- leader state, denoted by S ∈ PRE − LEADER, if it is a
follower state, and at least one follower node v has s0v = 1.

While in the beginning, the leader sets may be large as each node
regards different slots during the b-interval as the “leader slots”,
over time the values synchronize and the LS sets become smaller.
This facilitates a fast leader (re-) election.
D EFINITION 3.5 (L EADER S TATE ). A state S is called a
leader state, denoted by S ∈ LEADER, if all the following conditions are satisfied:
(i) There is at least one leader, i.e., |{v|v ∈ V : sv =
1}| ≥ 1; (ii) for every node v, ls1 (v), ls2 (v), ls3 (v), ls4 (v) ∈
[b] ∪ {undef ined}, ls01 (v), ls02 (v), ls03 (v), ls04 (v) ∈ [b] ∪
{undef ined}, ls00 (v) ∈ [b]; (iii) let v be any follower and let w be any follower or leader, then ls3 (v) ∈
{ls1 (w), ls2 (w), ls3 (w), ls4 (w)} ∪ {undef ined}, ls02 (v) ∈
{ls00 (w), ls01 (w), ls02 (w), ls03 (w)} ∪ {undef ined}; (iv) |LSL | ≤
5; (v) for every follower w with ls3 (w) ∈ [b] or ls02 (w) ∈ [b],
cw = cv and Tw = Tv for any leader v.
So in a leader state, it holds that any follower’s ls3 and ls02 slots are
covered by either another follower’s ls and ls0 slots, or a leader’s
ls and ls slots (cf Condition (iii)).
Finally, it is useful to define safe and legal states.
D EFINITION 3.6 (S AFE AND L EGAL S TATE ). A system state
S is called safe (denoted by S ∈ SAFE) if S ∈ FOLLOWER or
S ∈ LEADER, and legal (denoted by S ∈ LEGAL) if S is safe
and there is exactly one node v with sv = 1.
Thus, according to our definitions, any legal state is also a safe
state. In the following, let S be the set of all possible system states,
SAFE ⊂ S be the set of all safe system states and LEGAL ⊂
SAFE be the set of all legal system states.
The proof of Theorem 1.1 unfolds in a number of lemmas. An
interesting property of our randomized algorithm is that it is guaranteed to be correct, in the sense that deterministically exactly one
leader is elected; only the runtime is probabilistic (i.e., depends on
the random choices made by S ELECT).
First, we study leader messages.
L EMMA 3.7. For any network state it holds that if a leader successfully transmits a ‘LEADER’ message, the system will immediately enter a legal state.
P ROOF. When a node (either follower or leader) receives a
‘LEADER’ message, it sets ls3 and ls02 to undef ined (Lines
22 − 25 in Figure 1 left; after Lines 15 − 17 of Figure 1 right),
and considers itself a follower. Thus, in the new state, there is exactly one leader (the sender of the ‘LEADER’ message) and n − 1
followers. The state is also a safe state, namely a leader state: Conditions (i) and (ii) are fulfilled trivially. Condition (iv) also holds
as there is only one leader that has four slots. Condition (iii) is
fulfilled because nodes receiving a ‘LEADER’ message reset their
slots ls3 and ls02 ; since ls3 and ls02 are undefined for a follower,
also Condition (v) holds.
We next consider what happens if nodes hear a message sent by
a follower.
L EMMA 3.8. For any network state it holds that when a follower successfully transmits a message, the system is guaranteed
to enter a safe state at the beginning of the next b-interval.
P ROOF. First note that if a leader message gets through before
the next b-interval, the claim holds trivially due to Lemma 3.7.
Otherwise we distinguish two cases: (A) For every node v, s0v =
0 (not pre-leader) and sv = 0 (not leader) by the end of current

b-interval. (B) There is at least one node v with either s0v = 1
(pre-leader) or sv = 1 (leader) by the end of current b-interval.
In Case (A), after the follower message has been successfully
sent, there are still n followers and no leaders or pre-leaders. We
will show that the system enters the follower state at the beginning of the next b-interval. Let us refer to the follower node that
sent the message by v and to any remaining node by w. When
w receives the message from v (Lines 26 − 32 in Figure 1 left),
it sets ls01 (w) := ls00 (v), ls02 (w) := ls01 (v), ls03 (w) := ls02 (v),
and ls04 (w) := ls03 (v). The c values become the same (cw = cv ),
and Tw := Tv . The new state therefore fulfills the follower state
conditions: Clearly, Conditions (i), (ii), and (iv) are fulfilled immediately, and Condition (iii) holds as well, as for all followers
w that did not send a message and follower v which sent a message, at the beginning of the next b-interval: ls3 (w) = ls02 (w) =
ls01 (v) = ls2 (v), ls3 (v) = ls02 (v) = ls03 (w) = ls4 (w), and
ls1 (v) = ls2 (w) = ls00 (v) = ls01 (w).
For Case (B), observe that during the remainder of the b-interval
the number of pre-leader nodes with s0v = 1 cannot decrease, and
hence there will be at least one leader at the beginning of the next
b-interval. We now show that the new state will indeed be a leader
state as nodes “synchronize” with the follower node that sent the
message. Without loss of generality, assume that node u is the last
follower that successfully sent a follower message in the current
b-interval. Let us refer to the other follower nodes by v1 and to
the leader nodes or the pre-leader nodes (i.e., the followers v with
s0v = 1) by v2 . Again, Conditions (i) and (ii) are fulfilled trivially.
As for Condition (iii), we need to consider two sub-cases:
(Case 1) No node experienced an idle channel in its ls3 slot
after the message has been successfully sent. If this is the case
and follower u is not a pre-leader, it holds that for follower
v1 : ls02 (v1 ) = ls02 (v2 ) = ls01 (u) in the current b-interval, and
ls3 (v1 ) = ls2 (v2 ) = ls2 (u) at the beginning of the next binterval; on the other hand, if follower u is a pre-leader, then in
the current b-interval it holds that for follower v1 : ls02 (v1 ) =
ls02 (v2 ) = ls01 (u), and ls3 (v1 ) = ls2 (v2 ) = ls1 (u) at the beginning of the next b-interval. Hence, Condition (iii) holds. Regarding the cardinality of the leader set LSL , observe that at the
beginning of the next b-interval, if u is not a pre-leader, all leaders will have ls1 = ls00 (u), ls2 = ls01 (u), ls3 = ls02 (u), ls4 =
ls03 (u), and hence LSL = {ls00 (u), ls01 (u), ls02 (u), ls03 (u)}, therefore |LSL | ≤ 5; otherwise, if u is a pre-leader, then LSL =
{ls00 (u), ls01 (u), ls02 (u), ls03 (u), ls04 (u)}, therefore |LSL | ≤ 5.
(Case 2) One or more nodes experienced an idle channel in their
ls3 slots after the message has been successfully sent. In the following, we prove this case correct assuming that u is a follower and
not a pre-leader. If u is a pre-leader, the proof is analogous.
1. If v1 experienced the idle channel at its ls3 time slot, and became a pre-leader:
Note that a node v1 may experience an idle channel after receiving the message from u and hence become a preleader, however Condition (iii) is still satisfied, as it holds
that for follower u: ls02 (u) = ls03 (v2 ) = ls03 (v1 ) in the
current b-interval and ls3 (u) = ls3 (v2 ) = ls3 (v1 ) at the
beginning of the next b-interval. As for the cardinality of
the leader set LSL , observe that at the beginning of the
next b-interval, all leaders will have ls1 = ls00 (u), ls2 =
ls01 (u), ls3 = ls02 (u), ls4 = ls03 (u), and hence LSL =
{ls00 (u), ls01 (u), ls02 (u), ls03 (u)}, therefore |LSL | ≤ 5.
2. If u experienced the idle channel at its ls3 time slot, and became a pre-leader:
If node u experienced an idle channel after successfully

sending the message, u became a pre-leader, and we have
for a follower v1 , ls02 (v1 ) = ls02 (v2 ) = ls01 (u) in the current
b-interval and ls3 (v1 ) = ls2 (v2 ) = ls1 (u) at the beginning
of the next b-interval. Hence, Condition (iii) is satisfied.
As for |LSL |, observe that at the beginning of the next binterval, for a leader v2 , ls1 = ls00 (u), ls2 = ls01 (u), ls3 =
ls02 (u), ls4 = ls03 (u), while for the remaining leader u, it
holds that ls1 = ls01 (u), ls2 = ls02 (u), ls3 = ls03 (u), ls4 =
ls04 (u). Hence, also in this case, we have that |LSL | ≤ 5.
Finally, Condition (v) is true for both of the sub-cases, because
the cv and Tv values are “synchronized” when the follower message is received (Lines 27 and 29 in Figure 1 left; Line 19 in Figure 1 right).
An important property of S ELECT is that once it is in a safe state,
it will remain so in future (given that there are no external changes).
Similar properties can be derived for other states, as we will see.
L EMMA 3.9. Once the system is in a safe state, it will remain
in a safe state in the future.
P ROOF. We study what can happen in one round, and show that
in each case, the safety properties are maintained. In a round, (A)
either a ‘LEADER’ message is successfully sent, (B) a follower
message is successfully sent, (C) there are collisions or the channel
is jammed, or (D) there is an idle channel.
In Case (A), the claim directly follows from Lemma 3.7 and from
the fact that safe states are a super set of the legal states (SAFE ⊃
LEGAL). In Case (B), the claim follows from Lemma 3.8 and by
the fact that the system is in the safe state already.
In Case (C), if the channel is blocked, follower nodes (even those
which sent a message in this round) do not change their state except
for the synchronized rounds in Lines 35 − 43, and similarly for
the leaders in Lines 26 − 34. Our protocols guarantee that the
leaders have the same cv and Tv values as the followers when ls3
and ls02 are valid, and since the leaders experience the same number
of successful transmissions and idle time steps as the followers do
(single-hop network), the claim follows.
If there is an idle channel (Case (D)), all nodes v for which
ls3 (v) = mc will set s0v = 1 in the current b-interval, while
other values remain the same. It is clear that from this point on
until the end of the current b-interval, the claim holds. Moreover, as we show next, the claim is still true at the beginning
of next b-interval. If ls3 (v) is undefined, then the claim holds
trivially, as no states will change in this case. If ls3 (v) = mc
for any node v and the nodes experience an idle channel, there
is no leader since, if there was a leader, according to Condition (iii) of the leader state definition (Definition 3.5), a follower’s ls3 slot would always be covered by a leader slot of a
leader, which yields the contradiction. Hence, the current safe
state must be a pre-leader state. Let v denote the followers that
have s0v = 0 (i.e., they are not pre-leaders); let u denote the followers with s0u = 1 (pre-leaders). In the current b-interval, we
have ls02 (v) ∈ {ls00 (u), ls01 (u), ls02 (u), ls03 (u)} ∪ {undef ined},
which is true according to Condition (iii) of the follower state definition (Definition 3.3). Then, at the beginning of next b-interval,
u will become a leader, and hence we have ls3 (v) = ls02 (v),
ls1 (u) = ls01 (u), ls2 (u) = ls02 (u), and ls3 (u) = ls03 (u). This
implies that ls3 (v) ∈ {ls00 (u), ls1 (u), ls2 (u), ls3 (u)}, which satisfies Condition (iii) of the leader state Definition 3.5. Conditions (i) and (ii) are clearly satisfied. Condition (iv) holds simply
because we have shown (in Lemma 3.8, Case (B)), when there is
an idle time step, |LSL | ≤ 5. Condition (v) is true because we
always synchronize the cv and Tv values.

L EMMA 3.10. Once a system is in a leader state, it will remain
in a leader state in the future.
P ROOF. Lemma 3.9 tells us that the system will never leave a
safe state. Therefore, it remains to prove that there will always be
at least one node v with sv = 1. This clearly holds as the only way
a leader can become a follower again is by receiving a ‘LEADER’
message (see Lines 15 − 17), which of course implies that another
leader is still active and remains to be a leader. Also, since we
are in a leader state, Condition (v) holds and it further implies that
leaders will never invalidate their ls slots before the followers. This
guarantees that the protocol will never get out of a leader state.
L EMMA 3.11. Once a system is in a legal state, it will remain
in a legal state in the future.
P ROOF. By Lemma 3.9, we know that our system will never
leave a safe state again, and hence, we only need to prove that
there will always be exactly one node v with sv = 1. This
is true because in the safe state, a follower node w can never
become a leader, as its ls3 (w) slot is covered by the leader
v: ls3 (w) ∈ {ls1 (v), ls2 (v), ls3 (v), ls4 (v)} and ls02 (w) ∈
{ls00 (v), ls01 (v), ls02 (v), ls03 (v)} (Condition (iii) of leader state).
Since a follower will never send a ‘LEADER’ message, v will remain a leader forever, which proves the claim.
Regarding convergence, note that the system quickly enters a
safe state, deterministically.
L EMMA 3.12. For any initial system state with T̂ = maxv Tv ,
it takes at most b · T̂ rounds until the system is in a safe state.
P ROOF. We distinguish three cases: if a leader message gets
through sometimes in these rounds, then the claim holds by
Lemma 3.7; if a follower message gets through, then the claim
holds by Lemma 3.8. If within maxv Tv b rounds neither a follower
message nor a leader message gets through, all nodes will have to
reset their ls slots (since C ONDITION in Line 37 (Figure 1 left)
resp. Line 28 (Figure 1 right) is not met). This however constitutes
the safe state (all conditions fulfilled trivially), which is maintained
according to Lemma 3.9.
Armed with these results, we can prove convergence.
L EMMA 3.13. For any safe state, S ELECT will eventually reach
a legal state.
P ROOF. We divide the proof in two phases: the phase where the
protocol transitions to the leader state from the follower state, and
the phase where it transitions to the legal state from the leader state.
1. Follower state to leader state
If C ONDITION is fulfilled, we know that a ‘LEADER’ message got through and the system is in a legal state (and
hence also in a leader state). As long as C ONDITION is
not fulfilled, Tv is increasing for each node v. So eventually, T̂ = maxv Tv ≥ 2T /b. We can also provide a
lower bound on the cumulative probability p. W.l.o.g. suppose that T ≥ (3/) log1+γ n (a smaller T will only make
the jammer less flexible and weaker). Suppose that p is at
most /4 throughout some T -interval I. Then it follows
from the standard Chernoff bounds that there are at most
T /3 busy steps in I with high probability.6 If this is true,
then no matter how the adversary jams during I, at least
6
“With high probability”, or short “w.h.p.”, means a probability of
at least 1 − 1/nc for any constant c > 0.

(1 − /3)T − (1 − )T = 2T /3 non-jammed steps will
be idle, which implies that the cumulative probability at the
end of I will be by a factor of at least (1 + γ)T /3 ≥ n3
higher than at the beginning of I. Using this insight, it follows that eventually a T -interval is reached with p > /4.
Once such a T -interval has been reached, it is easy to show
that p will not get below 1/n2 any more w.h.p. so that for every T -interval afterwards there is a time point t with p > /4
w.h.p. So infinitely often the following event can take place
with some lower-bounded, positive probability:
Consider two consecutive T -intervals I1 and I2 starting at
a time when cv = 0 for every node v. Suppose that I1
just consists of busy steps and I2 just consists of idle time
steps. Then the adversary has to leave T busy time steps
in I1 non-jammed and T idle time steps in I2 non-jammed.
For I1 , there is a positive probability in this case that exactly 3 messages from different nodes are successfully sent
in 3 different b-intervals. In this case, all but one follower
respect the leader slots (as their ls3 -value is defined) while
the follower that sent the last successful message may still
send out messages at all time steps (as its ls3 -value is still
undefined, see Line 6 of the follower protocol). Thus, it is
indeed possible that all time steps in I1 are busy. Up to that
point, the adversary has not learned anything about the leader
slots. In I2 , there is also a positive probability that none of
the followers transmits a message throughout I2 so that all
time steps are idle. As the adversary does not know which of
them is a leader slot and has to leave T non-jammed, there
is a positive probability that ls3 is non-jammed, and some of
the followers become pre-leaders and then leaders.
Thus, the expected time to get from a follower to a leader
state is finite.
2. Leader state to legal state
If there is only one leader in the leader state, the system is
already in a legal state by definition. If there is more than
one leader, then we distinguish between the following cases.
If C ONDITION is fulfilled, we know that a ‘LEADER’ message got through and the system is in a legal state. Otherwise, the leaders will invalidate all of their ls slots once their
cv values are reset to 0. At this point there is a positive probability that for the next T steps a ‘LEADER’ message is successfully sent. As the adversary has to leave T time steps
non-jammed, at least one ‘LEADER’ message will be successfully transmitted within these T steps so that the system
reaches a legal state.
Analogous to the followers in the previous case, one can
lower bound the cumulative probability of the leaders (in
fact, the leaders will eventually reach a time point with a cumulative probability of Ω() as they increase their probabilities in case of an idle channel more aggressively than the followers) so that the chance above of successfully transmitting
a ‘LEADER’ message repeats itself infinitely often with a
lower-bounded positive probability. Thus, the expected time
to get from a leader to a legal state is finite as well.
From these cases, the lemma follows.

4.

EXPERIMENTS

We conducted several simulations to study the behavior of S E LECT under different types of jammers and interference.

4.1

Performance under Jamming

For our formal analysis, we introduced the notion of a (T, 1−)bounded adversary for some T ∈ N and 0 <  < 1 which denotes
that for any time window of size w ≥ T the adversary can jam at
most (1 − )w of the time steps in that window. While our protocol
is provably robust to any adversary meeting these constraints, for
our simulations, we will need to focus on specific instantiations.
For example, we will consider an adversary that reactively jams all
non-idle time periods only (as long as the budget is not used up), in
order not to waste energy jamming idling periods.
We consider jammers of different powers, one that can block
the channel 90% of the entire time, one that blocks 70% of the
time, and a “weak” one that blocks 50% of the time (i.e.,  ∈
{0.1, 0.3, 0.5}, resp.). We set T = 100 and consider a b-interval
(see Figure 1) with parameter b = 15 (smaller b values are possible as well). Experiments are repeated 50 times for each individual
setting, and average values are recorded correspondingly. We run
each experiment until one and only one leader is elected.
We conducted experiments with different types of reactive jammers: jammers ADVrand which interrupt transmissions at random,
jammers ADVbusy which only jam busy periods where one or more
nodes transmit, and jammers ADVidle which jam the channel whenever it is idle. Concretely, for ADVbusy and ADVidle we assume
that the adversary will jam each busy resp. idle time period until
the “jamming budget” is used up for this T -period. For ADVrand
we set the jamming probability per round equal to (1 − ). ADVidle
may appear less challenging to the deal with. However, note that
an adversary may be able to lead a protocol to suboptimal states by
jamming idle time periods. Moreover, this scenario also describes
interference from co-existing networks where nodes are activated
in quiet times. Hence, this adversary constitutes an interesting case
that should not be neglected in the analysis.
Recall from Lemma 3.12 that from any initial state, the safe state
is reached quickly, and hence, we are mainly interested in the convergence time from the safe state to the legal state. Figure 2 (left)
plots the corresponding convergence times. At first sight the runtime may appear to be rather high. For example, under an adversary ADVrand that jams 90% of the entire time, it takes a few thousand time steps. However, note that this result implies that during
the merely a few hundred non-jammed time steps, the five hundred
nodes are able to successfully coordinate the medium access among
themselves—without being able to distinguish between time periods with collisions and time periods that are jammed!— and use
the computed access probabilities to elect a leader. We believe
that when taking this into account, and although we do not have
any lower bounds, the convergence time is very good and probably
cannot be improved much with alternative schemes.
Figure 2 (middle and right) presents the corresponding convergence times for the reactive jammers ADVbusy and ADVidle . As
expected, jamming the busy channel yields higher convergence
times, also when comparing these results to our experiments with
ADVrand . In contrast, interestingly, for ADVidle , the runtime is
fairly independent of the adversarial power: a reactive jammer
blocking idle channels gives similar results as ADVrand . Clearly,
among the scenarios we investigated, the most effective strategy
for the adversary is to reactively jam the busy time periods as long
as the total number of jammed time steps does not exceed (1−)·T .
Figure 3 complements Figure 2 by studying the execution times
in smaller networks.
Our protocol aims to quickly reach a cumulative sending probability around a small constant, such that on expectation, roughly
one node will try to transmit a message in a non-jammed step.
Thus, given the constant probability of having a successful trans-

Figure 2: Left: Convergence time from safe state to legal state, where the adversary ADVrand jams the channel. We ran our protocol
until exactly one leader is elected. Middle: Convergence time from safe state to legal state, where a reactive adversary ADVbusy jams
the channel when one or more nodes are transmitting. We ran S ELECT until only one leader is elected. Right: Convergence time
from safe state to legal state under the reactive ADVidle adversary.

Figure 3: Like Figure 2, but for smaller networks and using
 = 0.5 and p̂ = 1/24.

mission, a follower messages will get through soon, the nodes synchronize, and the ls slots are defined as well. Since the leaders’
sending probabilities reach higher values more quickly than the
sending probabilities of the followers (according to Line 12 of Figure 1 right), a leader message gets through soon, yielding a legal
state. We consider two initial states, a “well-initialized one” where
all nodes have the maximum access probability p̂ (in simulation we
set p̂ = 1/24), where there is no leader in the network, and where
the ls and ls0 slots are all invalidated (according to Definition 3.3,
this implies that we are in the follower state); and one with “arbitrary initialization” where the roles and variables are chosen at
random (each node is either follower or leader, pv is chosen uniformly at random between 0 and 1, and the ls values uniformly at
random between 0 and b − 1). Our experiments show that both scenarios yield similar results, which indicates that the convergence
time of self-stabilization if fairly independent of the initial state.
Figure 4 (left) shows a typical trace of the cumulative probabilities over time when the protocol is well initialized, i.e., the protocol
starts from the follower state. Initially, all nodes are followers, and
we will denote the cumulative sending probability of the followers
by pF , and the cumulative sending probability of the leaders by pL .
At beginning, pF > 500 · p̂ > 10 while pL = 0. As time goes
on, pF decreases quickly until it falls in an interval of small con-

stant range (i.e., pF < 10), and multiple successful transmissions
happen which synchronize the nodes’ ls and ls0 values. Next, multiple leaders are elected because many followers sense an idle time
step in their ls3 slot. That is why pL emerges at the same point
in time as pF decreases dramatically to a value between 0 and 1.
Then, the nodes continue to adjust their transmission probabilities
depending on the channel state, until the first leader message gets
through and all the other leaders become followers; this yields the
quick decrease of pL and increase of pF accordingly. One and only
one leader is elected after this point. Subsequently, both pF and pL
remain within a small constant range. Figure 4 (right) shows the cumulative probabilities when the protocol starts from an “arbitrary”
state (pv , Tv , leaders and follower roles, etc. chosen at random). In
the beginning, there are both followers and leaders in the network.
It can be seen that S ELECT converges fast, similarly to the wellinitialized case. After the legal state is reached, both pF and pL
also remain in a small constant range.

4.2

Co-existing Networks

Our leader election protocol is robust to arbitrary (but bounded
with respect to time) interruptions of the availability of the medium,
and it is convenient to regard these interruptions as caused by a
malicious adversary. However, there are many other forms of interference to which our protocol is resilient and under which the
few available time slots can be exploited effectively. In the following, we briefly report on one more source of interference, namely
co-existing protocol instances. Concretely, we remove the jammer
from the network and we compare the performance of our leader
election protocol when run alone to situations where additional networks (of the same size) are concurrently trying to elect a leader
and interfere with the other protocol instances accordingly.
Figure 5 (left) plots the averaged runtime until successful leader
election for one, two, three and four co-existing networks, as a
function of the corresponding sub-network sizes (i.e., four coexisting networks imply a four times larger total number of nodes).
Our results indicate that each additional interfering network increases the runtime by a factor corresponding to the additional
nodes. The convergence time among the co-existing networks exhibits a high fairness, as can be seen in Figure 5 (right): in all
networks, a leader is elected almost at the same time.

5.

CONCLUSION

This paper has introduced the first self-stabilizing leader election protocol, S ELECT, for wireless networks operating in harsh

Figure 4: Left: Fast convergence of pF and pL ( = 0.5, T = 100, network with 500 nodes) under ADVbusy when the protocol starts
from a safe state. Right: Corresponding convergence of pF and pL when the protocol starts from an arbitrary state.

Figure 5: Left: Convergence time of co-existing networks (as a function of their individual sizes) performing the leader election
algorithm. Right: Fair convergence time among co-existing networks.

environments, e.g., environments with hard-to-predict interference
from co-existing networks or environments subject to (both adaptive and reactive) adversarial jamming. Although the nodes are not
able to distinguish between collisions due to external interference
or jamming and concurrent transmissions of other nodes in the network, they are able to coordinate access to the medium in the few
and arbitrary time periods without external interference, and subsequently elect a leader in a robust manner. Although our protocol
is randomized, it yields deterministic guarantees. There are several
important open directions for future research. For example, the
formal study of convergence times under different adversaries is an
open problem. Another open problem is the generalization of our
algorithms to multi-hop networks where leaders need to be elected
in different regions (e.g., in order to construct a sparse backbone).

Acknowledgments
This work was supported in part by NSF awards CCF-0830791
and CCF-0830704, and by the DFG-Project SCHE 1592/1-1. We
would like to thank our shepherd Vijay Subramanian from Northwestern University for his help during the preparation of the final
manuscript.

6.

REFERENCES

[1] G. Alnifie and R. Simon. A multi-channel defense against jamming
attacks in wireless sensor networks. In Proc. of Q2SWinet ’07, pages
95–104, 2007.
[2] Gheorghe Antonoiu, Gheorghe Antonoiu, Pradip K Srimani, and
Pradip K Srimani. A self-stabilizing leader election algorithm for tree
graphs. Journal of Parallel and Distributed Computing, 34:227–232,
1996.
[3] Hagit Attiya and Jennifer Welch. Distributed Computing:
Fundamentals, Simulations and Advanced Topics (Chapter 3). John
Wiley & Sons, 2004.
[4] B. Awerbuch. Optimal distributed algorithms for minimum weight
spanning tree, counting, leader election, and related problems. In
Proc. STOC, 1987.
[5] Baruch Awerbuch, Andrea Richa, and Christian Scheideler. A
jamming-resistant mac protocol for single-hop wireless networks. In
Proc. of PODC ’08, 2008.
[6] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and
B. Thapa. On the performance of IEEE 802.11 under jamming. In
Proc. of IEEE Infocom ’08, 2008.
[7] T. Brown, J. James, and A. Sethi. Jamming and sensing of encrypted
wireless ad hoc networks. In Proc. of MobiHoc ’06, pages 120–130,
2006.
[8] Shukai Cai, Taisuke Izumi, and Koichi Wada. Space complexity of
self-stabilizing leader election in passively-mobile anonymous
agents. In Proc. 16th International Colloquium on Structural
Information and Communication Complexity (SIROCCO), 2009.

[9] J.T. Chiang and Y.-C. Hu. Cross-layer jamming detection and
mitigation in wireless broadcast networks. In Proc. of MobiCom ’07,
pages 346–349, 2007.
[10] Edsger W. Dijkstra. Self-stabilization in spite of distributed control.
Communications of the ACM, 17(11):643–644, 1974.
[11] Shlomi Dolev, Seth Gilbert, Rachid Guerraoui, Dariusz R. Kowalski,
Calvin Newport, Fabian Kuhn, and Nancy Lynch. Reliable
distributed computing on unreliable radio channels. In Proc. 2009
MobiHoc S3 Workshop, 2009.
[12] R. G. Gallager, P. A. Humblet, and P. M. Spira. A distributed
algorithm for minimum-weight spanning trees. ACM Trans.
Program. Lang. Syst., 5(1):66–77, 1983.
[13] Sukumar Ghosh and Arobinda Gupta. An exercise in
fault-containment: self-stabilizing leader election. Inf. Process. Lett.,
59(5):281–288, 1996.
[14] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and
suspicious sensors: On the efficiency of malicious interference in
wireless networks. In Proc. of OPODIS ’06, 2006.
[15] J. Hromkovic, R. Klasing, A. Pelc, P. Ruzicka, and W. Unger.
Dissemination of Information in Communication Networks:
Broadcasting, Gossiping, Leader Election, and Fault-Tolerance
(Chapter 8). Springer-Verlag New York, Inc., Secaucus, NJ, USA,
2005.
[16] Gene Itkis, Chengdian Lin, and Janos Simon. Deterministic, constant
space, self-stabilizing leader election on uniform rings. In Proc. 9th
International Workshop on Distributed Algorithms (WDAG), pages
288–302, 1995.
[17] C.Y. Koo, V. Bhandari, J. Katz, and N.H. Vaidya. Reliable broadcast
in radio networks: The bounded collision case. In Proc. of PODC
’06, 2006.
[18] E. Korach, S. Kutten, and S. Moran. A modular technique for the
design of efficient distributed leader finding algorithms. ACM Trans.
Program. Lang. Syst., 12(1):84–101, 1990.
[19] Shay Kutten and Boaz Patt-Shamir. Time-adaptive self stabilization.
In Proc. PODC, 1997.
[20] Seungjoon Lee, Dave Levin, Vijay Gopalakrishnan, and Bobby
Bhattacharjee. Backbone construction in selfish wireless networks. In
Proc. ACM International Conference on Measurement and Modeling
of Computer Systems (SIGMETRICS), 2007.

[21] Xin Liu, Guevara Noubir, Ravi Sundaram, and San Tan. Spread:
Foiling smart jammers using multi-layer agility. In Proc. of Infocom
’07, pages 2536–2540, 2007.
[22] Koji Nakano and Stephan Olariu. Randomized leader election
protocols in radio networks with no collision detection. In ISAAC
’00: Proceedings of the 11th International Conference on Algorithms
and Computation, pages 362–373, London, UK, 2000.
Springer-Verlag.
[23] Vishnu Navda, Aniruddha Bohra, Samrat Ganguly, and Dan
Rubenstein. Using channel hopping to increase 802.11 resilience to
jamming attacks. In Proc. of Infocom ’07, pages 2526–2530, 2007.
[24] Koji Nikano and Stephan Olariu. Uniform leader election protocols
for radio networks. IEEE Trans. Parallel Distrib. Syst.,
13(5):516–526, 2002.
[25] A. Pelc and D. Peleg. Feasibility and complexity of broadcasting
with random transmission failures. In Proc. of PODC ’05, 2005.
[26] Andrea Richa, Christian Scheideler, Stefan Schmid, and Jin Zhang.
A jamming-resistant mac protocol for multi-hop wireless networks.
In Proc. DISC, 2010.
[27] Andrea Richa, Christian Scheideler, Stefan Schmid, and Jin Zhang.
Competitive and fair medium access despite reactive jamming. In
Proc. 31st IEEE International Conference on Distributed Computing
Systems (ICDCS), 2011.
[28] Georgios Smaragdakis, Ibrahim Matta, and Azer Bestavros. Sep: A
stable election protocol for clustered heterogeneous wireless sensor
networks. In Proc. 2nd International Workshop on Sensor and Actor
Network Protocols and Applications (SANPA), 2004.
[29] Dan E Willard. Log-logarithmic selection resolution protocols in a
multiple access channel. SIAM J. Comput., 15(2):468–477, 1986.
[30] A.D. Wood, J.A. Stankovic, and G. Zhou. DEEJAM: Defeating
energy-efficient jamming in IEEE 802.15.4-based wireless networks.
In Proc. of SECON ’07, 2007.
[31] W. Xu, T. Wood, and Y. Zhang. Channel surfing and spatial retreats:
defenses against wireless denial of service. In Proc. of Workshop on
Wireless Security, 2004.


C

Mobile Networks and Applications 11, 119–120, 2006
2006 Springer Science + Business Media, LLC. Manufactured in The Netherlands.
DOI: 10.1007/s11036-006-4465-9

MONET Special Issue on Foundations of Mobile Computing
ANDRÉA W. RICHA
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-8809, USA

JENNIFER L. WELCH
Department of Computer Science, Texas A&M University, College Station, TX 77843-3112, USA
Published online: 31 March 2006

This special issue of MONET is devoted to algorithmic foundations of mobile computing. Mobile computing and communication devices will have an enormous impact on our lifestyle
over the next several decades. The mobility of distributed
computing components raises a number of interesting, and
difficult, algorithmic issues. Some of these questions were
considered at the 2003 DIALM-POMC workshop, which was
the genesis of this special issue.
The theme of the workshop was the application of discrete algorithms and methods in the context of mobile and
wireless computing and communications. This workshop was
the merging of two workshops: (1) DIAL-M (Discrete Algorithms and Methods for Mobile Computing and Communications), which was held annually in 1997–2002 as a workshop in conjunction with ACM MOBICOM; and (2) POMC
(Principles of Mobile Computing), which was held in 2001
in conjunction with the ACM Symposium on Principles of
Distributed Computing (PODC), and in 2002 in conjunction
with the International Symposium on Distributed Computing (DISC). DIALM-POMC 2003 was sponsored by ACM
SIGMOBILE in cooperation with ACM SIGACT, with financial support from the National Science Foundation and the
Consortium for Embedded and Internetworking Technologies
(CEINT).
There were 27 papers submitted to the workshop and eight
were accepted. Three of the papers in this special issue were
invited submissions from that workshop. We also posted an
open call for submissions to the special issue. In response,
34 additional papers were submitted, from which four were
chosen to appear in the special issue, making a total of seven
papers. All the papers appearing in this special issue underwent a rigorous reviewing process.
A popular theme of the papers appearing in this special
issue is topology control. The general idea is for nodes to
control their transmission power in order to ensure that the
resulting communication topology has some desired properties while also trying to conserve energy. Four of the papers
are on this topic, each with their own approach.
In “Range Assignment for Biconnectivity and k-Edge
Connectivity in Wireless Ad Hoc Networks” by Gruia Ca-

linescu and Peng-Jun Wan, the authors consider the problems
of finding an assignment of power to nodes that minimizes
the total power yet still produces a k-vertex or k-edge connected graph. It is shown that the 2-edge connected problem
is NP-complete. Approximation algorithms are also studied
and a new approximation algorithm that is simpler and can
more easily be made distributed, while still ensuring a good
approximation ratio, is presented.
Errol L. Lloyd, Rui Liu, and S.S. Ravi in their paper
“Approximating the Minimum Number of Maximum Power
Users in Ad hoc Networks” attack the problem of assigning
power values to nodes in such a way that the number of nodes
that have to use the maximum power is minimized, while still
keeping the graph connected. This goal is important as it helps
to decrease interference at the MAC layer. Since the problem
is NP-complete, some practical approximation algorithms are
presented and analyzed; the analysis is complemented with
experimental results.
In “Equilibria in Topology Control Games for Ad Hoc
Networks”, Stephan Eidenbenz, V.S. Anil Kumar, and Sibylle
Zust are concerned with the topology control problem when
nodes are owned by different entities and they have their
own self-interest to promote. Game theory is used to study
the problems of nodes’ choosing power levels to ensure several kinds of connectivity, both with and without directional
antennas. The focus is on proving whether or not a Nash
equilibrium (a stable operating point when players do not
cooperate) exists for the corresponding games and how their
quality compares to the optimal choice.
Related to topology control is the paper by Yu Wang and
Xiang-Yang Li, “Localized Construction of Bounded Degree
and Planar Spanner for Wireless Ad Hoc Networks”. The authors describe an algorithm to find a subgraph of the network
that has bounded degree and is planar; the planarity is useful
for some routing algorithms. This algorithm is the first one
with these properties that can be constructed efficiently and
distributedly.
Rounding out this special issue are three papers that consider other important problems in mobile networks. In “Competitive Algorithms for Maintaining a Mobile Center” by

120

Sergey Bereg, Bhinay Bhattacharya, David Kirkpatrick, and
Michael Segal, the authors consider the problem of locating a
mobile facility at or near the center of a set of nodes. An entity
at the center would be able to reach all the nodes with minimal energy. Stated as an optimization problem, the goal is
to (perhaps approximately) minimize the maximum distance
between a node and the center. The paper includes a negative
result that the center can move arbitrarily fast even when the
nodes have bounded velocity. In response to this result, the
authors show how to trade lower accuracy of approximation
for lower velocity of the center.
Qing Fang, Jie Gao, and Leonidas J. Guibas in their paper
“Locating and Bypassing Holes in Sensor Networks” consider the reality that sensor networks usually have holes, areas
in which the distribution of working sensors is not sufficiently
dense. Such areas pose problems for a number of greedy algorithms. The paper describes two distributed algorithms to
identify and build routes around holes; these algorithms are
applicable in a number of important applications, including
geographic routing.

RICHA AND WELCH

In “Verifiable Distributed Oblivious Transfer and Mobile
Agent Security”, Sheng Zhong and Yang Richard Yang focus
on the mobile agent paradigm and the need to protect the privacy of agents as well as that of the hosts. This paper defines a
new cryptographic primitive, in which the well-known building block called oblivious transfer is accomplished by a set
of servers, some of which may experience malicious faults.
An algorithm for the primitive is described, an implementation is proposed, and performance evaluation results are
presented.
The work presented in this special issue demonstrates the
value of applying discrete mathematical methods to problems
arising in mobile computing. We hope that it inspires more
work in the same vein.
We would like to thank a number of people, without whom
this special issue could not have appeared. First, we thank all
the authors who submitted papers. We thank the reviewers
for their diligent work. And we thank Beth Johnson of the
Department of Computer Science at Texas A&M University
for her expert administrative assistance.

Optimal-Stretch Name-Independent Compact Routing in
Doubling Metrics
Goran Konjevod ∗

Andréa W. Richa †

Donglin Xia ∗

CSE Department
Arizona State University
Tempe, AZ 85287-8809, USA

CSE Department
Arizona State University
Tempe, AZ 85287-8809, USA

CSE Department
Arizona State University
Tempe, AZ 85287-8809, USA

goran@asu.edu

aricha@asu.edu

dxia@asu.edu

ABSTRACT

General Terms

We consider the problem of name-independent routing in
doubling metrics. A doubling metric is a metric space whose
doubling dimension is a constant, where the doubling dimension of a metric space is the least value α such that any ball
of radius r can be covered by at most 2α balls of radius r/2.
Given any δ > 0 and a weighted undirected network G
whose shortest path metric d is a doubling metric with doubling dimension α, we present a name-independent routing
scheme for G with (9+δ)-stretch, (2+ 1δ )O(α) (log Δ)2 (log n)bit routing information at each node, and packet headers of
size O(log n), where Δ is the ratio of the largest to the smallest shortest path distance in G.
In addition, we prove that for any  ∈ (0, 8), there is
a doubling metric network G with n nodes, doubling dimension α ≤ 6 − log , and Δ = O(21/ n) such that any
name-independent routing scheme on G with routing in2
formation at each node of size o(n(/60) )-bits has stretch
larger than 9 − . Therefore assuming that Δ is bounded
by a polynomial on n, our algorithm basically achieves optimal stretch for name-independent routing in doubling metrics with packet header size and routing information at each
node both bounded by a polylogarithmic function of n.

Algorithms, Performance, Theory

Keywords
Name-Independent Routing, Compact Routing, Doubling
Metrics

1. INTRODUCTION
A routing scheme is a distributed algorithm that allows
any node (source node) to route packets to any other node
(destination node). Each node in the network maintains
a local routing table and runs a routing daemon. When
the node receives a packet, the daemon decides whether the
packet has reached the right destination, and if not, how to
deliver it to the next hop based on the local routing table
and the packet header.
The stretch of a routing scheme is the maximum ratio
of the length of the routing path, on which a packet is delivered, to the length of the shortest path from the source
to the destination node, over all source destination pairs.
One of the fundamental trade-oﬀs for routing schemes is between the space required to store the routing table (which
contains any information required by the routing scheme) at
each node, and the stretch of the resulting routing scheme.
In general, one would like to keep the space requirement at
the nodes and the packet header size both polylogarithmic
on the number of nodes, while optimizing on stretch. Such
a scheme is usually called a compact routing scheme.
There are two variants of routing scheme design: (i) namedependent (or labeled) routing, where the designer is allowed
to rename the nodes so that the names (labels) can contain
additional routing information, e.g. topological information;
and (ii)name-independent routing, which works on top of the
arbitrary original node names in the network, i.e. the node
names are independent of the routing scheme. Although
name-dependent routing has the advantage of embedding
useful routing information on the node labels to facilitate
routing, it has severe limitations in practice. For instance,
it requires additional mechanisms to publish the labels and
to retrieve the label of a given node. Moreover, there are
applications where node names cannot be modiﬁed such as
distributed hash tables [7] requiring that node names be
randomly distributed in a segment, or a mobile network requiring nodes names independent of their location.
This paper focuses on the design of name-independent
routing schemes for undirected weighted graphs whose short-

Categories and Subject Descriptors
C.2.2 [Computer Communication Networks]: Network
Protocols - Routing protocols; E.1 [Data Structures]: Distributed data structures; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms
and Problems - Computations on discrete structures, Routing and Layout; G.2.2 [Discrete Mathematics]: Graph
Theory - Graph algorithms, Graph labeling, Network problems
∗
†

Supported in part by NSF grant CCR-0209138.
Supported in part by NSF CAREER grant CCR-9985284.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’06, July 22-26, 2006, Denver, Colorado, USA.
Copyright 2006 ACM 1-59593-384-0/06/0007 ...$5.00.

198

est path metrics are doubling. A doubling metric is a metric space whose doubling dimension is a constant, where the
doubling dimension of a metric space is the least value α
such that any ball of radius r can be covered by at most 2α
balls of radius r/2. Many problems become easier in metrics
of bounded doubling dimension [10, 17, 15, 20, 19, 18, 9,
14, 13], including metric embeddings, the traveling salesman
problem, compact data structures, distance estimation and
ﬁnding nearest neighbors. In particular, doubling metrics
can be viewed as a generalization of growth-bounded metrics [16, 10, 6], which have been shown to provide a good
approximation of communication cost metrics on the Internet.

structures separately at the nodes, saving on the size of the
routing tables maintained. Also, having this global hierarchy allows us to “think outside the box” when building a
search tree for each relevant local area L(u, j) around node
u at level j, in the sense that we do not require that this
tree be fully contained within L(u, j).
Section 2.3 presents our name-independent routing algorithm. In Section 3, we provide the proof of our lower bound.
Some directions for future work and concluding remarks are
presented in Section 4.

1.2 Related Work
In addition to the distinction between name-dependent
and name-independent routings, the other feature that distinguishes diﬀerent routing problems is the generality of
the underlying network. In all cases, the trade-oﬀ between
stretch and routing table size plays an important role. We
summarize the existing results in the next few paragraphs.
In all of the work below, the packet header size is polylogarithmic in n and Δ, with the exception of the scale-free
schemes of [2], in which it depends only on n. For a table
containing a summary of the exact packet header sizes, as
well as stretch and node storage requirements, of the work
on compact routing schemes for doubling metrics, please refer to [2].
For general graphs, Awerbuch and Peleg [8] proposed a
name-independent routing schemes with stretch O(k2 ) and
routing tables of size O(kn1/k log n log Δ), where Δ is the
normalized diameter of the graph. Furthermore, the stretch
was improved to O(k) with Õ(n1/k log Δ)∗ bits of storage
by Abraham, Gavoille and Malkhi [3].
Several more restrictive models have been studied. For
graphs excluding a ﬁxed Kr,r minor, there is a constantstretch name-independent scheme with polylogarithmic routing tables [4]. For the labeled case, (1+)-stretch schemes do
exist for planar graphs and the points in Euclidean plane [21,
12, 5]. For k-path separable graphs, a (1 + )-stretch routing
scheme with polylogarithmic routing tables is presented in
[1]. For growth-bounded metrics, which are a subclass of
doubling metrics, a randomized name-independent (1 + )stretch routing scheme with polylogarithmic routing tables
is presented in [6].
For the labeled model, constant doubling dimension does
not seem to make the problem signiﬁcantly harder than for
growth-bounded metrics, and several (1 + )-stretch routing schemes are known [10, 20, 9, 19, 11, 18]. Abraham et
al. show in [2] that one cannot achieve exact (1-stretch) compact routing schemes on growth-bounded graphs
√ (and hence
on doubling metric graphs) without using Ω( n) memory
at some nodes.
The only variation of the problem in which a signiﬁcant
separation between the name-independent and labeled models is known is the restriction to networks of bounded doubling dimension. While in this case, (1 + )-stretch labeled
compact routing is possible, as mentioned above, for the
name-independent model there exist stronger lower bounds.
Abraham et al. [2] give two contrasting results. On one
hand, they show that any scheme with o(αn)-bit routing tables must have stretch at least 3 −  (this paper strengthens
the lower bound to 9 − , with somewhat stronger assump-

1.1 Our Contributions
In this paper we present the ﬁrst polylogarithmic space
name-independent routing scheme for doubling metrics with
asymptotically optimal stretch. By asymptotically optimal
stretch, we mean that the stretch of our algorithm is t+δ, for
any ﬁxed δ > 0, where t is a lower bound on the best possible
stretch for any constant doubling dimension α (in particular,
t → 9 as α increases in our problem). The matching lower
bound proof on the stretch of a name-independent scheme
on a doubling metric is also a major contribution of this
work. We ﬁrst deﬁne the aspect ratio of a graph.
Definition 1.1 (Aspect Ratio). The aspect ratio Δ
of a weighted undirected graph G is the ratio of the largest
to the smallest shortest path distance in G.
Our main results are stated in the theorems below:
Theorem 1.2. Given any δ > 0 and a weighted undirected graph G with n nodes and doubling dimension α > 0,
we present a name-independent routing scheme for G with
(9 + δ)-stretch and (2 + 1δ )O(α) (log Δ)2 (log n)-bit routing information at each node and O(log n) packet header size.
Theorem 1.3. For any  ∈ (0, 8), there is an undirected
weighted graph G with n nodes , doubling dimension α ≤
6 − log  and aspect ratio Δ = O(21/ n) such that any nameindependent routing scheme on G that uses routing tables of
2
size o(n(/60) ) bits at each node has stretch at least 9 − .
Note that in Theorem 1.2 for δ ≥ 1/2, the storage requirement at the nodes basically does not depend on δ itself and
becomes proportional to 2O(α) (log Δ)2 (log n), while for δ <
1/2 the storage requirement becomes ( 1δ )O(α) (log Δ)2 (log n)
bits.
Therefore, assuming that Δ is bounded by a polynomial
in n (which is true for any network with polynomial edge
weights), the stretch achieved by our algorithm is asymptotically optimal for any name-independent routing in doubling
metrics with no more than a polylogarithmic in n number of
bits in the packet header size and the routing table at each
node.
The techniques presented in this paper are also of interest in their own right. We use a global hierarchy of r-nets
(the formal deﬁnition of r-nets appears in Section 2.3) which
is employed throughout the algorithm for moving through
the diﬀerent search levels, and also for performing searches
in the “local areas” at each level. Since we use the same
hierarchy for all the search structures employed by our algorithm, we avoid the need of keeping each of these search

∗
The Õ() notation denotes complexity similar to O() up to
poly-logarithmic factors.

199

Lemma 2.2 ([10]). Let (X, d) be a metric with doubling
dimension α, and Y be an r-net of X. For any x ∈ X and
 α
any radius r  ≥ r, we have |Bx (r  ) ∩ Y | ≤ 4rr
.

tions). On the other hand, for name-independent routing
in doubling metrics they provide the ﬁrst constant-stretch
scheme (namely, they achieve stretch 64).
Furthermore, Abraham et al. [2] also present a variation
of their name-independent scheme which is scale-free — that
is, their stretch, packet header size and routing table size do
not depend on the aspect ratio of the space — at the expense
of getting a prohibitively large constant for the stretch. All
previous results, as well as ours in the current paper, have
a factor with logarithmic or doubly-logarithmic dependence
on the aspect ratio in the size of the routing tables. In that
respect, while our stretch of 9 +  is better, and basically
matches the lower bound we give, there is still room for
improvement by elimination of the dependence on aspect
ratio of the metric space from the memory bounds.
Some of the techniques used in the name-independent
schemes developed in [2] and our paper, albeit having been
developed independently, may seem to share some similarity
at a high level. However, it is by a careful data structure
speciﬁcation and algorithm analysis that we manage to get
a tight bound on the stretch for doubling metrics, while still
keeping the storage requirement at the nodes and the packet
header size small.

2.

Our name-independent routing scheme will use the namedependent routing scheme of Abraham, Gavoille, Goldberg
and Malkhi [2, Theorem 4] as the eﬀective underlying labeled routing scheme. For reference, we list the main results
achieved by this labeled routing scheme:
Lemma 2.3 ([2]). Given any undirected weighted graph
with n nodes, doubling dimension α, and aspect ratio Δ, for
any δ  ≤ 1/2, there exists a (1 + δ  )-stretch labeled routing
scheme with 
log n-bit routing labels, ( δ1 )O(α) log n log Δbit routing information at each node and O(log n)-bit packet
header.
(It is important to note that we could use any labeled routing scheme with stretch 1+δ  , polylogarithmic routing information and polylogarithmic header size, to achieve the optimal stretch of our name-independent routing scheme; the
labeled routing scheme of [2] has the best currently known
storage and packet header size bounds.)
Thus, every time a node x wants to eﬀectively communicate with a node y, x needs to know the routing label of
node y and then uses the above labeled routing scheme in
order to ﬁnd a route to y. Hence the main merit of our
name-independent scheme is to present an eﬃcient method
for distributedly storing and retrieving the nodes routing labels so that when a node x wants to communicate with a
node y, x does not need to know the routing label assigned
to node y a priori: this routing label will be retrieved as
part of the routing process (and the cost of retrieving the
label will be accounted for when bounding the total cost of
routing in our scheme).

NAME-INDEPENDENT ROUTING FOR
DOUBLING METRICS

In this section, we propose a name-independent routing
scheme for doubling metrics with stretch (9 + δ), for any
ﬁxed δ > 0.
We divide our algorithm description into four sections: in
Section 2.1, we present a set of basic deﬁnitions and some
prior results which will be used as ”black boxes” in our algorithm and proofs; the basic data structures which will
be kept at the nodes in order to be able to implement our
name-independent algorithm are deﬁned in Section 2.2; we
combine the results in Section 2.1 and 2.2 to present the
complete routing algorithm in Section 2.3; and, ﬁnally, in
Section 2.4, we present the analysis of our algorithm.

2.2 Data Structure
In this section we present the data structures to be used
in our name-independent scheme. Let Yi be a 2i -net of
G, for i ∈ [M ], where M = 
log Δ and [x] denotes the
set {0, 1, . . . , x − 1}. Note that Y0 = V , since the minimum weight on edges is 1. The following deﬁnition selects speciﬁc “neighbors” of a node u in the diﬀerent nets
Y0 , Y1 , . . . , YM −1 .

2.1 Preliminaries
In this section we present the deﬁnition and some basic
properties of r-nets. We also state the main results regarding
the name-dependent scheme presented in [2, Theorem 4],
which we will use as a “black box” in our algorithm.
Let G = (V, E, w) be a weighted undirected network,
where |V | = n and w : E → R+ . Without loss of generality, assume the minimum weight on an edge is equal to
1. Suppose the metric space (V, d), where d is the shortest
path metric of G, has doubling dimension α. Let Bu (r)
be the closed ball of radius r around node u in G, i.e.
Bu (r) = {x ∈ V (G) | d(u, x) ≤ r}.

Definition 2.4 (Neighbor Sequence). For each node
u, recursively define n(u, i), the neighbor of node u in Yi , as
follows:
1. n(u, 0) = u, which is in V = Y0 ;
2. For 0 < i ≤ M , n(u, i) is a node in Yi that minimizes
d(n(u, i − 1), n(u, i)).

Definition 2.1. An r-separated set A in a metric space
(X, d) is a subset A ⊆ X such that d(x, x ) ≥ r for every
x = x in A. An r-net is a maximal r-separated set.

We call the sequence n(u, 0), . . . , n(u, M − 1) the neighbor
sequence of node u.

Let A(u, j) = ji=1 d(n(u, i − 1), n(u, i)) for any node u,
and any positive j ∈ [M ]. The claim below follows directly
from Deﬁnitions 2.1 and 2.4.

For a ﬁnite metric it is easy to see that such an r-net exists
and can be constructed greedily. Given an r-net Y of a
metric space (X, d), we have that for any x ∈ X there exists
a node y ∈ Y such that d(x, y) ≤ r, since Y is maximal.
Moreover for a space of bounded doubling dimension, the
following is a well-known result [10]:

Lemma 2.5. For any node u and any positive j ∈ [M ],

A(u, j) ≤ ji=1 2i < 2j+1 .

200

Lemma 2.10. For any i ∈ [M ] and any node v ∈ Yi ,

We now deﬁne the local area of a node u belonging to the
2j -net Yj . Suppose a packet is being routed from a source
node u to a destination node v in the network. Every time
our routing algorithm moves one level higher — say to level
j — in the hierarchy of r-nets, it will search the local area of
u in Yj for the routing label of node v, using the local tree
T (u, j) also deﬁned below, as we will see in Section 2.3.

1. the up-link set U L(v, i) contains all nodes z such that
z is the parent of node v in some tree T (u, j) that contains v as n(w, i) for some w ∈ L(u, j), where j > i
and u ∈ Yj ;
2. the down-link set DL(v, i) contains all nodes z such
that z is a child of v in some tree T (u, j) that contains
v as n(w, i) for some w ∈ L(u, j) for i < j or as the
root u for i = j, where j ≥ i and u ∈ Yj .

Definition 2.6 (Local Area). For any j ∈ [M ] and
any node u ∈ Yj , its local area, denoted by L(u, j), is the
ball Bu (2j f (δ)), where f (δ) = 80
+ 2.
δ
Definition 2.7 (Local Search Tree). For any j ∈
[M ] and any node u ∈ Yj , its local search tree, denoted by
T (u, j), is the tree that consists of (i) root u, (ii) the leaf set
L(u, j), and (iii) for each w ∈ L(u, j), the path P (w; u, j) =
w → n(w, 1) → n(w, 2) → · · · → n(w, j − 1) → u connecting
w to the root u. The weight of each (virtual) edge in the
tree is equal to the length of the shortest path between its
endpoints.

Proof.
1. If i < j −1, v’s parent node is n(w, i+1) ∈ Yi+1 by Definition 2.7. Thus, since Yi+1 is a 2i+1 -net, d(v, n(w, i+
1)) ≤ 2i+1 < 2i+1 (f (δ) + 1) by Deﬁnition 2.4. Hence
U L(v, i) contains v’s parent node.
If i = j − 1, v’s parent node is the root u. By Deﬁnitions 2.6 and 2.7, d(u, v) ≤ d(u, w) + A(w, j − 1) ≤
2j (f (δ) + 1). Hence U L(v, i) contains v’s parent node
u.

Note that P (w; u, j) (and hence T (u, j)) may not be entirely contained in Bu (2j f (δ)). Nevertheless the lemma below shows that P (w; u, j) does not go “too far away” from u,
since the total length of this path is bounded by 2j (f (δ)+2).
The length of a path P , denoted by |P |, is given by the sum
of the weights of the edges on P . Thus

2. If i < j, v’s children are all of the form n(w, i − 1)
for some w ∈ L(u, j) such that v = n(w, i). Thus by
Deﬁnition 2.4, d(v, n(w, i − 1)) ≤ 2i < 2i (f (δ) + 1).
Hence v’s children are contained in DL(v, i).

Lemma 2.8. For any j ∈ [M ], node u ∈ Yj and node
w ∈ L(u, j), |P (w; u, j)| ≤ 2j (f (δ) + 2).
Proof. By Lemma 2.5 and Deﬁnition 2.6,
|P (w; u, j)| =

j−1


If i = j, then v = u and its children are of the form
n(w, j − 1) for some w ∈ L(u, j). Since d(u, n(w, j −
1)) ≤ d(u, w) + A(w, j − 1) ≤ 2j (f (δ) + 1), v’s children
are contained in DL(v, i).

d(n(w, i − 1), n(w, i)) + d(n(w, j − 1), u)

i=1

≤ A(w, j − 1) + (d(n(w, j − 1), w) + d(w, u))


≤ A(w, j − 1) + A(w, j − 1) + 2j f (δ)

The corollary below follows from Lemma 2.2.
Corollary 2.11. For any i ∈ [M ] and any node v ∈
Yi , we have |U L(v, i)| ≤ (4(f (δ) + 1))α and |DL(v, i)| ≤
(8(f (δ) + 1))α .

≤ 2 (f (δ) + 2)
j

In the next lemma we bound the number of trees T (u, j)
that contain node v. This information, along with the bound
on the size of the up-link and down-link sets given by Corollary 2.11, will be used when bounding the total storage requirement at the nodes in Lemma 2.15.

The two endpoints of each edge (x, y) of T (u, j) need to
be able to eﬀectively communicate with each other in the
underlying labeled routing scheme. Hence we need node x to
keep the routing label of node y and vice-versa. Note that for
a node v, there may exist many nodes u ∈ Yj such that v ∈
T (u, j). Many of these local search trees T (u, j) containing
node v may share the same parent and children nodes for
v. Hence, instead of keeping the routing labels of v’s parent
and children nodes explicitly for each tree T (u, j), v keeps
a single set (its up-link set) that contains the routing labels
of all nodes w such that w is the parent of v in some tree
T (u, j) and a single down-link set that contains the routing
labels of all nodes z such that z is a child of v in some tree
T (u, j). We will see in Corollary 2.11 that the cardinality of
the up-link and down-link sets of a node v are both O(1).

Lemma 2.12. For any i ∈ [M ] and any node v ∈ Yi , the
number of trees T (u, j), for any j ≥ i and u ∈ Yj , that
contain v as n(w, i) for some w ∈ L(u, j) if j > i, or as the
root u if i = j, is no more than (4(f (δ) + 1))α · log Δ.
Proof. For each j > i such that v = n(w, i), we have
d(u, v) ≤ d(u, w) + A(w, i) ≤ 2j (f (δ) + 1). Since u ∈ Yj ,
by Lemma 2.2 the number of possible choices for u (and
hence for the tree T (u, j)) is no more than (4(f (δ) + 1))α .
For j = i, we have u = v and the number of such trees is
trivially equal to 1. Since j ∈ [M ], the total number of such
trees is no more than (4(f (δ) + 1))α · log Δ.

Definition 2.9. For any i ∈ [M ] and any node v ∈
Yi , define the up-link set U L(v, i) = Bv (2i+1 (f (δ) + 1)) ∩
Yi+1 (except for i = M ), and the down-link set DL(v, i) =
Bv (2i (f (δ) + 1)) ∩ Yi−1 (except for i = 0).

We now show how to evenly store the routing labels of
the nodes in L(u, j) among the nodes of L(u, j) itself, in
a way such that we can eﬃciently search for these labels
using T (u, j). For any j ∈ [M ] and any node u ∈ Yj , we
visit the tree T (u, j) using depth-ﬁrst search, and store the
routing labels of nodes in L(u, j) at the nodes in the leaf set
of T (u, j), i.e. L(u, j) itself, as follows:

The following lemma shows that the up-link and downlink sets of node v at level j as deﬁned above indeed contain
all the parent and children nodes of node v in any T (u, j)
containing v.

201

1. Sort the nodes in L(u, j) according to their global IDs
(i.e. their original names) into a list.

The following lemma proves that our algorithm correctly
ﬁnds node v within a ﬁnite number of steps, if v belongs to
G.

2. Visit the leaves of T (u, j) in depth-ﬁrst order, and for
each visited leaf of T (u, j), pick up a new node x from
the list formed in Step 1, and store its global ID and
routing label at the current leaf.

Lemma 2.13. From any source node u, given any destination node v, the algorithm finds v within a finite number
of steps.
Proof. Since 2M −1 f (δ) ≥ Δ, we have v ∈ V (G) ⊆
L(n(u, M − 1), M − 1) by Deﬁnition 2.6. Hence v’s routing
label is stored in the tree T (n(u, M −1), M −1). Thus, in the
worst case, v’s routing label is retrieved by Search(n(u, M −
1), M − 1, v) in Phase 1. Therefore the algorithm ﬁnds v
within a ﬁnite number of steps.

3. Each node v in T (u, j) stores the range of node IDs
that are stored in v’s descendants in tree T (u, j), in
the form of (v.minID, v.maxID; u, j). Furthermore,
in order to facilitate the local search tree, v stores the
range information of all its children.

2.4 Stretch and Storage Analysis

Note that each node of L(u, j) stores exactly one node’s
routing label (which may not be its own) in Step 2, since
the leaf set of T (u, j) is L(u, j) itself. In addition, since
we are visiting the nodes in depth-ﬁrst order, for each node
v ∈ T (u, j), if a node w ∈ L(u, j) is such that its ID is
between v.minID and v.maxID in the tree T (u, j), then
the routing label of w is stored in some descendant of v, and
vice-versa.
We now deﬁne the search procedure for the routing label
of a node w in T (u, j).

In this section, we analyze the ﬁnal stretch and total storage requirement at the nodes of our name-independent routing scheme. Fix the (1 + δ  ) stretch required of the underlying labeled scheme according to δ, by choosing δ  =
min{δ/20, 1/10} < 1/2 .
Lemma 2.14. For any source node u and any destination
node v in G, the total routing cost of our algorithm is no
more than (9 + δ)d(u, v).
Proof. Let j be the index of the level at which the procedure Search(n(u, j), j, v) ﬁnds the routing label of v in
Phase 1.
By Lemma 2.8, the routing cost of Search(n(u, i), i, v) is
2·2i (f(δ)+2)·(1+δ ). Then the total routing cost isno more

than A(u, j) + ji=0 2i+1 (f (δ) + 2) + d(n(u, j), v) (1+δ  ).
First we have

Search(u,j,w)
Begin at the root u and let v = u initially.
While there exists a child v  of v in T (u, j)
such that w’s ID is in the range
(v  .minID, v  .maxID)
do Go to node v  and let v = v  .
If v has w’s routing label,
then report w’s routing label,
else report that w’s routing label wasn’t found
in T (u, j), by following the path back to the
root u from v along T (u, j).

A(u, j) +

j


2i+1 (f (δ) + 2) + d(n(u, j), v)

i=0

≤ A(u, j) +

j


2i+1 (f (δ) + 2) + d(u, v) + A(u, j)

(1)

i=0

2.3 Routing Algorithm

≤ 2j+2 (f (δ) + 3) + d(u, v)

Now that we have built on the preliminaries of Section 2.1
and on the local search procedure for T (u, j) described in
Section 2.2, we are ready to describe our name-independent
routing scheme in its whole, and to prove its performance
bounds.
Assume that a source node u wants to send a message to a
destination node v. Basically our routing scheme proceeds
in two phases. In Phase 1, we ﬁnd the routing label of
node v, by repeatedly invoking the local search procedure
Search(n(u, i), i, v) for higher levels of the hierarchy of rnets (i.e., for increasing i). In Phase 2 of the algorithm, we
simply use the routing label of node v found in Phase 1 to
eﬀectively route to node v using the labeled routing scheme
of [2]. Let i = 0 initially.

Since v’s routing label is not found by Search(n(u, j −
1), j − 1, v), we have d(n(u, j − 1), v) > f (δ)2j−1 . Then by
triangle inequality, we have
d(u, v) ≥ d(n(u, j − 1), v) − d(n(u, j − 1), u)
≥ d(n(u, j − 1), v) − A(u, j − 1)
≥ f (δ)2j−1 − 2j

(2)

= 2j−1 (f (δ) − 2)
and thus,
8(f (δ) + 3)
d(u, v) + d(u, v)
f (δ) − 2


40
d(u, v).
= 9+
f (δ) − 2
(3)

2j+2 (f (δ) + 3) + d(u, v) ≤

• Phase 1 (Finding the routing label of the destination): Perform a local search at n(u, i) by calling the
procedure Search(n(u, i), i, v). If the routing label of
v is not found, go to n(u, i + 1), let i = i + 1, and
continue Phase 1; otherwise, go to Phase 2.

Since f (δ) = 80
+ 2, the total routing cost is no more than
δ


40
9+
d(u, v)(1 + δ  )
f (δ) − 2



δ
δ 1
(4)
= 9+
1 + min{ , } d(u, v)
2
20 10

• Phase 2 (Labeled routing to the destination): Once
the routing label of v is found during a local search
at level i, go to v from n(u, i) using the underlying
labeled routing scheme of [2].

≤ (9 + δ)d(u, v).

202

diﬀerent congruent namings, it is found in any branch of
the tree. Second, given one of these namings, a sequence
of branches is deﬁned according to the routing path from
the root to the node with the speciﬁc target name. We will
use this sequence to show that the stretch achieved by the
algorithm cannot be less than 9 − , for any ﬁxed  ∈ (0, 8).

Lemma 2.15. The routing information at each node is no
more than (2 + 1δ )O(α) (log Δ)2 (log n) bits.
Proof.
1. For the underlying labeled routing scheme,
it suﬃces to have ( δ1 )O(α) log n log Δ bits stored at
each node, by Lemma 2.3.

3.1 Congruent Namings

2. The storage required for the routing labels in each
node’s down-link and up-link sets can be bounded as
follows. Given that a node v can be in all the r-nets Yi
for i ∈ [M ], by Lemma 2.11 the number of routing labels stored at v is f (δ)O(α) log Δ. Since by Lemma 2.3
a routing label has size 
log n, the total storage for
the routing labels in a node v’s down-link and up-link
sets is (2 + 1δ )O(α) log n log Δ bits.

Given an integer c ≥ 2 and a graph G = (V, E) with n
nodes and a β-bit routing table at each node, where β =
o(n1/c ), consider any name-independent routing scheme on
G. First we give some deﬁnitions as follows:

3. We now bound the storage for the range information
for all the local search trees that contain the node.
Consider any node v ∈ Yi for any i ∈ [M ]. The number of v’s child nodes in any tree that contains v is
no more than 8(f (δ) + 1)α by Lemma 2.11. In addition, by Lemma 2.12, the number of local search trees
that contain v is no more than 4(f (δ) + 1)α · log Δ.
Since v can be in all the r-nets Yi for all i ∈ [M ],
the storage for the range information for all the local search trees that contain the node is no more than
(2 + 1δ )O(α) (log Δ)2 (log n) bits stored at each node.
Thus there are at most (2 + 1δ )O(α) (log Δ)2 (log n) bits of
routing information stored at each node.

Note that given a naming on V , the name-independent
routing scheme conﬁgures the β-bit routing table at each
node. Therefore it naturally deﬁnes a routing conﬁguration
function as follows.

Definition 3.1 (Naming). A naming l on nodes in V
is a bijective function l : V → [n].
Let L denote the family of all namings.

Definition 3.2 (Routing Configuration Function).
A routing configuration function is a function:
f : L × V → [2β ].
Definition 3.3 (Set of Congruent Namings).
Given a routing configuration function f : L × V → [2β ], a
mapping g : V → [2β ] and a subset of nodes V  ⊆ V , the set
of namings congruent with respect to V  and g is the set of
namings L = {l ∈ L : f (l , v) = g(v), ∀v ∈ V  }.

Proof of Theorem 1.2: The bounds on stretch and storage requirement at the nodes follow from Lemma 2.14 and
2.15, respectively. The packet header size of our algorithm
is given by the size of the routing labels used by the underlying labeled routing scheme and the size of the destination
node ID, i.e. O(log n) bits, by Lemma 2.3.

3.

Let {Vi : i = 0, 1, · · · , c} be a partition of V such that
i/c
(i−1)/c
for 1 ≤ i ≤ c. Note that
|V
0c| = 1 and |Vi | = n − n
|V
|
=
n.
Then
we
have
i
i=0
Lemma 3.4. Given any routing configuration function f ,
there exists a mapping g : V → [2β ] such that |Li | ≥ βnn!i/c ,
2
where Li is the set of namings congruent with respect to
∪ij=0 Vj and g, for 0 ≤ i ≤ c. Moreover by definition, L0 ⊇
L1 ⊇ · · · ⊇ L c .

LOWER BOUND

In this section, we present the proof of our lower bound
as stated in Theorem 1.3.
A name-independent routing scheme consists of two parts.
First, given a graph and a naming of its nodes (See Deﬁnition 3.1 for ’naming’), the routing table at each node is conﬁgured. Second, given a source node s and a target name
t, a message is delivered from the source node s to the corresponding destination node named t, based on t and the
routing tables of nodes that the routing algorithm visits.
In Section 3.1, by taking advantage of the small number
of diﬀerent conﬁgurations of routing tables compared to the
number of diﬀerent namings, we show that there exist many
namings such that the routing conﬁguration for a large number of nodes is identical according to these namings. These
identical namings will be called congruent (See Deﬁnition 3.3
for a formal deﬁnition). We show that, given a ﬁxed source
node and destination name, the routing algorithm must follow the same initial steps for any two congruent namings,
provided that the nodes visited by the routing algorithm
during these initial steps have the same routing conﬁguration for both namings.
In Section 3.2, we build the counterexample, a tree, to
be used in the lower bound proof. First, from Section 3.1,
it follows there exists a speciﬁc target name such that, for

Proof. We recursively deﬁne g and apply the pigeonhole
principle.
1. Deﬁne g on the node set V0 so that |L0 | ≥ 2n!β . Such
an assignment exists since |L| = n! and there are 2β
possible values for the routing table at the single node
of V0 .
2. For 1 ≤ i ≤ c, recursively deﬁne g on the node set Vi
so that |Li | ≥ βnn!i/c . Such an assignment exists since
2

i/c

(i−1)/c

)
n!
and there are 2β(n −n
pos|Li−1 | ≥ βn(i−1)/c
2
sible values for routing tables at all nodes of Vi .

Given any name-independent routing scheme on G and
its routing conﬁguration function f , let Li be deﬁned as in
Lemma 3.4, for 0 ≤ i ≤ c. Then
Lemma 3.5. There exists a name t ∈ [n] such that for
any 0 < i < c there exist two distinct namings l1 , l2 ∈ Li−1
with t = l1 (v) for some node v ∈ Vi , and t = l2 (v) for every
node v ∈ Vi .

203

Since |Li−1 | ≥

Proof. For 0 < i < c, let Yi be set of names used only
for nodes in Vi for all namings in Li−1 , i.e. Yi = {x ∈ [n] :
∀l ∈ Li−1 , ∃v ∈ Vi , x = l (v)}, and let Ni be set of names
never used for any node in Vi for any naming in Li−1 , i.e.
Ni = {x ∈ [n] : ∀l ∈ Li−1 , ∀v ∈ Vi , x = l (v)}. Note that the
number of namings l such that Yi ⊆ {l (v) : ∀v ∈ Vi } and
Ni ∩ {l (v) : ∀v ∈ Vi } = ∅ is

	
n − |Yi | − |Ni |
(5)
|Vi |! · (n − |Vi |)!
|Vi | − |Yi |

(i−1)/c

2βn


|Li−1 | ≤

Since |Li−1 | ≥
(i−1)/c

2βn

n!

2βn

	
2|Vi |
|Vi |! · (n − |Vi |)!
|Vi |

(i−1)/c

≥ 
2|V |
i

|Vi |

n!
|Vi |! · (n − |Vi |)!

βn
|Yi | + |Ni |
≤ 2 ni/c −n(i−1)/c
n
= 1 + o(1),

(12)

where the last equation follows from ex = 1 + O(x)
and β = o(n1/c )
Therefore |Yi | + |Ni | = o(n)
c−1
Let X = [n] − ∪c−1
i=1 Yi − ∪i=1 Ni . Then |X| = n − o(n) =
Ω(n). Let t ∈ X. Since t ∈
/ Yi ∪ Ni for 0 < i < c, by
the deﬁnition of Yi and Ni , it follows that there exist two
distinct namings l1 , l2 ∈ Li−1 with t = l1 (v) for some node
v ∈ Vi , and t = l2 (v) for every node v ∈ Vi .
By Deﬁnition 3.3, for any naming l ∈ Li−1 , the conﬁguration of the routing table of every node v in ∪i−1
j=0 Vj is the
same, i.e. f (l, v) = g(v). Thus by Lemma 3.5, we have

(7)

Corollary 3.6. For 0 < i < c, given any naming l ∈
Li−1 and a target name t ∈ [n] that satisfies the conditions
of Lemma 3.5, the routing tables of nodes in ∪i−1
j=0 Vj cannot
uniquely determine whether the node named t belongs to Vi
or not.

(8)

Hence, no routing algorithm can be certain of making
the right choice whether the node named t belongs to Vi
or not without seeing some information from nodes outside
of ∪i−1
j=0 Vj .

n!
|Vi |! · (n − |Vi |)!

3.2 Lower Bound Proof

(9)

In this section, we start by building a graph G; later with
the help of results in Section 3.1, we will show that, for any
2
name-independent routing scheme with o(n(/60) )-bit routing table at each node, the stretch on G cannot be smaller
than 9 − .
The graph G will be a tree with root node u connecting
the subtrees Ti,j , which are paths as deﬁned below. Given
 ∈ (0, 8), let p = 
72/ + 6 and q = 
48/ − 4. For any

Thus
(i−1)/c

βn
n
≤ 2 ni/c −n(i−1)/c
(i−1)/c
−n
)


(10)
βn(i−1)/c
=1+O
ni/c − n(i−1)/c

iq+j+1

iq+j

i ∈ [p] and j ∈ [q], let Ti,j be a path on n pq − n pq
nodes with edges of weight 1/n. Note that the length of
each path is at most 1. For any integer i and j ∈ [q], let
wi,j = (1 − qj ) · 2i q + qj · 2i+1 q = 2i (q + j). Since wi+1,0 =
2i (q + q), we also write wi,q = wi+1,0 and Ti,q = Ti+1,0 , for
any 0 ≤ i < p − 1.
As shown in Figure 1, the graph G is a tree with root u
and an edge of length wi,j connecting u to the median node
of the path Ti,j , for each i ∈ [p] and j ∈ [q].
For i ∈ [p] and j ∈ [q], let

= 1 + o(1),
where the last two equations follow from ex = 1+O(x)
and β = o(n1/c ) respectively.
Contradiction for i < c. Omit this case.
2. If |Vi | ≤ (n − |Yi | − |Ni |)/2, then

	
n − |Yi | − |Ni |
|Li−1 | ≤
|Vi |! · (n − |Vi |)!
|Vi |



(i−1)/c

1+

=

2(ni/c

i |−|Ni |
|Vi |

Thus

by Lemma 3.4, we have

n(n − 1) · · · (n − |Vi | + 1)
2|Vi |(2|Vi | − 1) · · · (|Vi | + 1)

|Vi |
n
≥
2|Vi |

ni/c −n(i−1)/c
n
=
2(ni/c − n(i−1)/c )

≥ 
n−|Y

n(n − 1) · · · (n − |Vi | + 1)
(n − |Yi | − |Ni |) · · · (n − |Yi | − |Ni | − |Vi | + 1)

|Vi |
n
≥
n − |Yi | − |Ni |
|Yi | + |Ni | ni/c −n(i−1)/c
≥(1 +
)
n

Consider two cases depending on whether |Vi | ≤ (n−|Yi |−
|Ni |)/2:

Thus

by Lemma 3.4, we have

(i−1)/c

=

The above formula follows from two observations: (1)
The number
of diﬀerent
sets of names that l may use for



i |−|Ni |
,
since
the names in Yi are preselected,
Vi is n−|Y
|Vi |−|Yi |
and those in Ni are not allowed. (2) Once the set of names
for Vi is selected, the number of such diﬀerent namings is
|Vi |! · (n − |Vi |)!.
Then for 0 < i < c, we have

	
n − |Yi | − |Ni |
|Li−1 | ≤
(6)
|Vi |! · (n − |Vi |)!
|Vi | − |Yi |

1. If |Vi | > (n − |Yi | − |Ni |)/2, then

	 
	
n − |Yi | − |Ni |
2|Vi |
≤
|Vi | − |Yi |
|Vi |

n!

2βn

Si,j = {u} +

(11)

i−1
 q−1

x=0 y=0

204

Tx,y +

j

y=0

Ti,y

(13)

T0,0

w0,0

u

wp−1,q−1

w0,1
w0,q−1
w1,0 w
T0,q−1
1,q−1
T0,1

T1,0

wp−1,0
wi,0

T1,q−1

wi,q−1

Ti,0

message from the root u to the node v named t by visiting the subtrees (Tik ,jk : k = 0, · · · , m̃ − 1) in order.
Let σ = (b0 , b1 , · · · , bm−1 ) be a maximal subsequence of
σ̃ = (wik ,jk : k = 0, · · · , m̃ − 1) such that (i) b0 = wi0 ,j0 ,
and (ii) for 0 < i < m, bi is the ﬁrst element of σ̃ that comes
after bi−1 in σ̃ and that is greater than bi−1 . Note that σ is
strictly increasing
and bm−1 equals the largest element of σ̃.

Let Ai = ij=0 bj . The following three technical claims
respectively bound Ai and Ai+1 in terms of bi , provide a
bound on the length of σ, and relate Ak+1 to bk .

Tp−1,q−1
Tp−1,0

Ti,q−1

Claim 3.8. For any i ∈ [m], suppose wx,y = bi . Then

Figure 1: Example graph for the lower bound proof
Since |Ti,j | = n

(p−1)q+(q−1)+1
pq

iq+j+1
pq

−n

iq+j
pq

1. If i ≤ m − 3, Ai ≤ (4 − /3)bi ;

, we have |Sp−1,q−1 | =

2. If bi+1 > wx,y+1 , then Ai+1 ≤ (4 − /3)bi .

= n, i.e. the number of nodes in G is n.
The following lemma shows that G induces a doubling
metric.

n

Proof.
1. Since i ≤ m−3 and (b0 , b1 , · · · , bm−2 , bm−1 )
is a strictly increasing sequence, then bi ≤ wp−1,q−3 .
Thus xq + y + 1 ≤ (p − 1)q + (q − 3) + 1 = c − 2.
Hence l ∈ Lc−2 ⊆ Lxq+y+1 by Lemma 3.4. Note that
Lxq+y+1 is the set of congruent namings with respect
to ∪xq+y+1
Vj = Sx,y . After ﬁrst visiting the subtree
j=0
Tx,y , the routing algorithm has routing information
only from the routing tables of nodes in Sx,y . Thus
by Corollary 3.6, the routing algorithm is not able to
decide on the location of the node named t so far.

Lemma 3.7. The shortest path metric of G is a doubling
metric with dimension α ≤ 6 − log , and aspect ratio Δ =
O(21/ n).
Proof. Let B be a ball of radius r centered at a node v
in G.
If u ∈ B, then B is contained in the path Ti,j that contains
the center node v since r < d(u, v). Thus B can be covered
by at most 2 balls of radius r/2.
If u ∈ B and d(u, v) ≥ r/2, then B can be covered by the
ball centered at u of radius r/2 and the ball centered at v
of radius r/2.
If u ∈ B and d(u, v) < r/2, then B can be covered by the
ball centered at u of radius r/2 and the paths
{Ti−1,j , Ti−1,j+1 , · · · , Ti,j }, where wi,j ≤ r < wi,j+1 . The
the nodes that belong to the ball B in each of these paths
can be covered by the ball of radius r/2 centered at the
median node of the path since the length of any these paths
is less than 1, thereby less than r/2. Thus B can be covered
with no more than q + 2 balls of radius r/2.
Therefore the shortest path metric of G is a doubling
metric with dimension at most log(q + 2) ≤ 6 − log  for
q = 
48/ − 4.
2w
Moreover, the aspect ratio Δ ≤ p−1,q−1
= O(21/ n).
1/n

On the other hand, by Lemma 3.5, there is a naming l1 ∈ Lxq+y+1 such that t = l1 (v  ) for some node
v  ∈ Tx,y+1 . Since l ∈ Lc−2 ⊆ Lxq+y+1 , the conﬁguration of the routing table of each node in Sx,y for
naming l is the same as that for naming l1 . Therefore
if the naming were l1 instead of l , the routing algorithm
would have visited nodes in the exact same order until
it ﬁrst visits some node not in Sx,y . Hence in order
to guarantee our assumption on the stretch bound for

i +d(u,v )
the naming l1 , we must have 2Ad(u,v
≤ 9 − . Thus
)
Ai ≤ (4 − /2)d(u, v  ). Since d(u, v  ) ≤ wx,y+1 + 1 =
wx,y+1 +1
bi ≤ (1 + 2/q)bi and q ≥ 48/ − 6, then
wx,y
Ai ≤ (4 − /2)(1 + 2/q)bi ≤ (4 − /3)bi .
2. Suppose wx ,y  = bi+1 . Then the ﬁrst node visited
by the routing algorithm that is not in Sx,y must be
in Tx ,y  , since by the deﬁnition of σ, wx ,y  is the
ﬁrst element of σ̃ greater than bi . Hence if bi+1 >
wx,y+1 , using a similar argument as above, we have
2Ai+1 +d(u,v  )
≤ 9 − . Similarly, we have Ai+1 ≤
d(u,v  )
(4 − /3)bi .

We now present the proof of our lower bound.
Proof of Theorem 1.3:
For our counterexample graph G, by contradiction, assume there is a name-independent routing scheme with βbit routing table at each node and stretch less than 9 − ,
2
where β = o(n(/60) ).
Let c = pq. For a simple calculation, we have c = (
72/+
6)(
48/ − 4) < (60/)2 for  ∈ (0, 8). Thus β ∈ o(n1/c ).
Note that {{u}, Ti,j : i ∈ [p], j ∈ [q]} is a partition {Vi : i =
0, 1, · · · , c} of V such that |V0 | = |{u}| = 1 and |Viq+j+1 | =
|Ti,j | = n(iq+j+1)/c − n(iq+j)/c for i ∈ [p], j ∈ [q]. Hence
the results from Lemma 3.4 and 3.5 can be applied to this
partition. For any name-independent routing scheme on G
with its routing conﬁguration function f , let Li be deﬁned
as in Lemma 3.4, for 0 ≤ i ≤ c. Let a target name t ∈ [n] be
selected by Lemma 3.5 so that for any 0 < i < c there exist
two distinct namings l1 , l2 ∈ Li−1 with t = l1 (v) for some
node v ∈ Vi , and t = l2 (v) for every node v ∈ Vi .
Given a naming l ∈ Lc−2 for which ∃v ∈ Vc−1 such
that t = l(v), suppose the routing algorithm delivers the

Claim 3.9. The length of σ is no less than p/2, i.e. m ≥
p/2.
Proof. First we show that bi ≤ w2i+2,0 , for any i ∈ [m],
by a simple inductive argument:
(1) The base case is to show b0 ≤ w2,0 . If b0 = w0,0 ≤
w2,0 , we are done. Otherwise, b0 = wi0 ,j0 > w0,0 . Since
Ti0 ,j0 is the ﬁrst subtree visited by the algorithm, then consider a naming l1 ∈ L0 such that t = l1 (v  ) for some node
v  ∈ T0,0 . Since l ∈ Lc−2 ⊆ L0 , the conﬁguration of the
routing table at the root u for naming l is the same as that
for naming l1 . Therefore if the naming were l1 instead of
l , the ﬁrst subtree visited would be Ti0 ,j0 as well. Hence

205



+d(u,v )
we have 2b0d(u,v
≤ 9 − . Thus b0 ≤ (4 − /2)d(u, v  ) ≤
)
(4 − /2)(w0,0 + 1) ≤ 4w0,0 = w2,0 , since q ≥ 8/ − 1.
bi
≤ 4. Other(2) For any i ≥ 1 and i ∈ [m], we have bi−1
wise, suppose bi−1 = wx ,y  , and thus bi > 4bi−1 > wx ,y  +1 .
bi
i
< bA
≤ 4, a
By Claim 3.8(2), Ai ≤ 4bi−1 . Thus bi−1
i−1

in n (e.g., any network whose edge lengths are bounded by
a polynomial in n), the space requirement of our scheme is
polylogarithmic in the number of nodes. We also present a
matching lower bound which shows that our scheme basically achieves optimal stretch for doubling metrics.
Our main goal in this paper was to focus on ﬁnding optimal stretch compact routing schemes for networks that do
not exhibit anomalies with respect to edge weights (e.g.,
edges of exponential length) and hence have Δ bounded by
a polynomial in n.
The main question we leave open is that of achieving
optimal stretch for a scale-free name-independent routing
scheme on doubling dimension networks. It is by no means
clear that (9+)-stretch is achievable by a scale-free scheme.
Even if we were to apply some of the techniques used in [2]
to develop scale-free schemes, that would still not directly
imply storage polylogarithmic in n and optimal stretch.
Another important vein for future work would be to design
dynamic compact routing schemes that can adapt eﬃciently
(maybe with polylogarithmic work) to changes in the network topology. Such schemes would be of particular interest
in mobile ad-hoc networks. It would also be interesting to
devise compact routing schemes with polylogarithmic storage and packet header size for more general (graph) metrics.

contradiction. Hence bi ≤ 4i · w2,0 = w2i+2,0 .
Since v ∈ Vc−1 = Tp−1,q−2 , then Tp−1,q−2 must be visited,
i.e. wp−1,q−2 is in σ̃. Hence bm−1 ≥ wp−1,q−2 , since bm−1
equal to the largest element in σ̃. Since bm−1 ≤ w2(m−1)+2,0
and q − 2 > 0, we have w2(m−1)+2,0 ≥ bm−1 ≥ wp,0 , i.e.
m ≥ p/2.
Claim 3.10. There exists k ≤ m − 4 such that
(4 − /4).

Ak+1
bk

>

b

for i ∈ [M ] and
Proof. Let ri = Abii and r̃i = i+1
bi
i < m − 1. Since Ai+1 = Ai + bi+1 = (ri + r̃i )bi and
Ai+1 = ri+1 bi+1 , we have ri + r̃i = ri+1 r̃i .
Then
m−4

i=0

ri+1 r̃i =

m−4


(ri + r̃i )

i=0

= r0 +

m−4


(ri+1 + r̃i ) − rm−4

i=0

≥2

m−4




4.1 Acknowledgments

(14)

We thank Cyril Gavoille and the PODC reviewers for helpful comments and suggestions.

ri+1 r̃i + r0 − rm−4

i=0

≥2

m−4




5. REFERENCES

ri+1 r̃i − 3

i=0

[1] I. Abraham and C. Gavoille. Object location using
path separators. Technical Report RR-1394-06,
LaBRI, University of Bordeaux, March 2006.
[2] I. Abraham, C. Gavoille, A. V. Goldberg, and
D. Malkhi. Routing in networks with low doubling
dimension. In 26th International Conference on
Distributed Computing Systems (ICDCS). IEEE
Computer Society Press, July 2006. To appear.
[3] I. Abraham, C. Gavoille, and D. Malkhi. Routing with
improved communication-space trade-oﬀ. In
Proceedings of the 18th International Conference on
Distributed Computing, volume 3274 of Lecture Notes
in Computer Science, pages 305–319, 2004.
[4] I. Abraham, C. Gavoille, and D. Malkhi. Compact
routing for graphs excluding a ﬁxed minor. In
Proceedings of the 19th International Conference on
Distributed Computing, volume 3724 of Lecture Notes
in Computer Science, pages 442–456, 2005.
[5] I. Abraham and D. Malkhi. Compact routing on
Euclidean metrics. In Proceedings of the 23rd Annual
ACM Symposium on Principles of Distributed
Computing, pages 141–149, 2004.
[6] I. Abraham and D. Malkhi. Name independent
routing for growth bounded networks. In Proceedings
of the 17th Annual ACM Symposium on Parallel
Algorithms and Architecture, pages 49–55, 2005.
[7] I. Abraham, D. Malkhi, and O. Dobzinski. Land:
stretch (1 + ) locality-aware networks for DHTs. In
Proceedings of the 15th Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 550–559,
2004.
[8] B. Awerbuch and D. Peleg. Routing with polynomial



where the
inequality follows from the inequality i (xi +
ﬁrst
√
yi ) ≥ 2 i xi yi for all sequences of nonnegative numbers
xi , yi , and the second inequality follows because r0 = 1, and
rm−4 ≤ 4 by Claim 3.8(1).
√Now by averaging ∃k ∈ [0, m − 4] such that rk+1 r̃k ≥
2 rk+1 r̃k − 3/(m − 3). By solving 
the quadratic equation
√
√
r̃
>
1
+
1 − 3/(m − 3). Then
in rk+1 r̃k , we get rk+1

k
rk+1 r̃k > 2−3/(m−3)+2 1 − 3/(m − 3) > 4−9/(m−3) ≥
A
4 − /4, since m ≥ p/2 ≥ 36
+ 3. Note that rk+1 r̃k = bk+1
.

k
Thus the claim follows.
We are now ready to conclude the proof of Theorem 1.3.
A
Let k be the index as deﬁned in Claim 3.10 such that bk+1 >
k
(4−/4). Suppose wx,y = bk . There are two cases depending
on whether bk+1 = wx,y+1 .
A
A
wx,y
>
(1) If bk+1 = wx,y+1 , then we have b k+1 = bk+1 wx,y+1
x

k+1

k

(q+y)
q
≥ (4−/4) q+1
≥ 4−/3, since q ≥ 48/−
(4−/4) 2x2(q+y+1)
4. On the other hand, by Claim 3.8(1) Ak+1 ≤ (4−/3)bk+1
for k + 1 ≤ m − 3, leading to a contradiction.
(2) If bk+1 = wx,y+1 , then bk+1 > wx,y+1 . Thus by
Claim 3.8(2) Ak+1 ≤ (4 − /3)bk < (4 − /4)bk , a contradiction.
Therefore, the theorem follows.

4.

CONCLUSIONS AND FUTURE WORK

In this paper, we present a (9 + δ)-stretch name independent routing scheme for networks with doubling dimension
α which requires (2+ 1δ )O(α) (log Δ)2 (log n)-bit routing information at each node. In networks where Δ is a polynomial

206

[9]

[10]

[11]

[12]

[13]

[14]

[15]

communication-space trade-oﬀ. SIAM J. Discret.
Math., 5(2):151–162, 1992.
H. T.-H. Chan, A. Gupta, B. Maggs, and S. Zhou. On
hierarchical routing in doubling metrics. In
Proceedings of the 16th Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 762–771,
2005.
A. Gupta, R. Krauthgamer, and J.R.Lee. Bounded
geometries, fractals and low-distortion embeddings. In
Proceedings of the 44th Annual IEEE Symposium on
Foundations of Computer Science, pages 534–543,
2003.
S. Har-Peled and M. Mendel. Fast construction of nets
in low dimensional metrics and their applications. In
Proceedings of the 21st Annual ACM Symposium on
Computational Geometry, pages 150–158, 2005.
Y. Hassin and D. Peleg. Sparse communication
networks and eﬃcient routing in the plane. Distributed
computing, 14:205–215, 2001.
D. Karger and M. Ruhl. Finding nearest neighbors in
growth-restricted metrics. In Proceedings of the 34th
Annual ACM Symposium on Theory of Computing,
pages 63–66, 2002.
J. Kleinberg, A. Slivkins, and T. Wexler.
Triangulation and embedding using small sets of
beacons. In Proceedings of the 45th Annual IEEE
Symposium on Foundations of Computer Science,
pages 444–453, 2004.
R. Krauthgamer and J. R. Lee. Navigating nets:
simple algorithms for proximity search. In Proceedings
of the 15th Annual ACM-SIAM Symposium on
Discrete Algorithms, pages 798–807, 2004.

[16] T. S. E. Ng and H. Zhang. Predicting Internet
network distance with coordinates-based approaches.
In Proceedings of the 21th Annual Joint Conference of
the IEEE Computer and Communications Society,
pages 170–179, 2002.
[17] R.Krauthgamer, J. R. Lee, M. Mendel, and A. Naor.
Measured descent: A new embedding method for
ﬁnite metrics. In Proceedings of the 45th Annual IEEE
Symposium on Foundations of Computer Science,
pages 434–443, 2004.
[18] A. Slivkins. Distance estimation and object location
via rings of neighbors. In Proceedings of the 24th
Annual ACM Symposium on Principles of Distributed
Computing, pages 41–50, 2005.
[19] A. Slivkins. Distributed approaches to triangulation
and embedding. In Proceedings of the 16th Annual
ACM-SIAM Symposium on Discrete Algorithms, pages
640–649, 2005.
[20] K. Talwar. Bypassing the embedding: algorithms for
low dimensional metrics. In Proceedings of the 36th
Annual ACM Symposium on Theory of Computing,
pages 281–290, 2004.
[21] M. Thorup. Compact oracles for reachability and
approximate distances in planar digraphs. In
Proceedings of the 42nd Annual IEEE Symposium on
Foundations of Computer Science, pages 242–251,
2001.

207

Brief Announcement: Parameterized Maximum and
Average Degree Approximation in Topic-based
Publish-Subscribe Overlay Network Design
∗

Melih Onus

Andréa W. Richa

Department of Computer Science and
Engineering
Arizona State University
Tempe, AZ 85281

Department of Computer Science and
Engineering
Arizona State University
Tempe, AZ 85281

melih@asu.edu

aricha@asu.edu

ABSTRACT

t, the subgraph induced by the nodes interested in t will be
connected.
The node degrees and number of edges required by a
topic-connected overlay network will be low if the node subscriptions are well-correlated. In this case, by connecting
two nodes with many coincident topics, one can satisfy connectivity of many topics for those two nodes with just one
edge. Several recent empirical studies suggest that correlated workloads are indeed common in practice.
In this work, we consider the problem of devising topicbased pub/sub overlay networks with low node degrees. One
could argue for keeping the maximum degree of a node low
or for keeping the overall average node degree low, since both
are important and relevant measures of the complexity and
scalability of a system. Unfortunately, previous attempts
at minimizing either one of these degree measures alone [1,
3] resulted in a linear explosion for the other measure (see
Table 1).
Hence it is only natural to consider the following problem:

Designing an overlay network for publish/subscribe communication in a system where nodes may subscribe to many
different topics of interest is of fundamental importance. For
scalability and efficiency, it is important to keep the degree
of the nodes in the publish/subscribe system low. It is only
natural then to formalize the following problem: Given a
collection of nodes and their topic subscriptions connect the
nodes into a graph which has low average and maximum
degree and in such a way that for each topic t, the graph induced by the nodes interested in t is connected. We present
the first polynomial time parameterized sublinear approximation algorithm for this problem.

Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and Design—Network Topology; G.2.2 [Mathematics of Computing]: Graph Theory—Network Problems

Low Degree Topic-Connected Overlay (Low-TCO) Problem:
Given a collection of nodes V , a set of topics T , and the
node interest assignment I, connect the nodes in V into a
topic-connected overlay network G which has low average
and maximum degree.
We present a parameterized sublinear approximation algorithm (Low-ODA) for this problem which approximates both
the average and the maximum degree well (See Table 1). To
the best of our knowledge, this is the first overlay network
design algorithm that achieves sublinear approximations
√ on
both the average and maximum degrees (e.g., for k = n).
The Low-ODA algorithm is a greedy algorithm which relies
on repeatedly evaluating the trade-off of greedily adding an
edge that would not increase the maximum degree versus
greedily adding an edge that would lead to a small number
of total edges in the final overlay network. The main contribution of this work is therefore to show that such a greedy
approach can work and indeed leads to non-trivial sublinear
approximation on both the average and maximum degree.
We expect that the greedy parameterized template introduced by our algorithm will lead to applications in other
network design domains where scalability is a key issue.
Chockler et al. [1] introduced the MinAv-TCO problem,
which aims at minimizing the average degree alone of a
topic-connected overlay network. They present an algorithm, called GM, which achieves a logarithmic approxima-

General Terms
Algorithms, Theory, Experimentation

Keywords
pub/sub, overlay networks, peer-to-peer, multicast, optimization

1.

INTRODUCTION

In the publish/subscribe (pub/sub) communication para
digm, publishers and subscribers interact in a decoupled
fashion. Publishers publish their messages through logical
channels and subscribers receive the messages they are interested in by subscribing to the appropriate services, which
deliver messages through these channels.
In this paper, we will design a (peer-to-peer) overlay network for each pub/sub topic, in the sense that for each topic
∗
This work was supported in part by NSF awards CCF0830791 and CCF-0830704.

Copyright is held by the author/owner(s).
SPAA’09, August 11–13, 2009, Calgary, Alberta, Canada.
ACM 978-1-60558-606-9/09/08.

39

Chockler et. al. [1]
Onus and Richa [3]
This Paper
Lower Bound

Avg Degree
O(log(n ∗ t))
θ(n)
O(k ∗ log(n ∗ t))
Ω(log n)

Max Degree
θ(n)
O(log(n ∗ t))
O((n/k) ∗ log(n ∗ t))
Ω(log n)

Table 1: Summary of known results on overlay network construction for publish/subscribe communication (n: number of nodes, t: number of topics, k
is any parameter between 1 and n)

tion on the minimum average degree of the overlay network.
While minimizing the average degree is a step forward
towards improving the scalability and practicality of the
pub/sub system, their algorithm may still produce overlay
networks of very uneven node degrees where the maximum
degree may be unnecessarily high. In [3], it is shown that the
GM algorithm may produce a network with maximum degree |V | while a topic-connected overlay network of constant
degree exists for the same configuration of I (See Table 1).
In [3], the problem of minimizing the maximum degree of a
topic-connected overlay network (MinMax-TCO) is considered, and a logarithmic approximation algorithm on the minimum maximum degree of the overlay network (MinMaxODA) is presented.
The MinMax-ODA algorithm may produce overlay networks of very high average degree: As we will show in Section 2, this algorithm may produce a network with average
degree |V | − 2 while a topic-connected overlay network of
constant average degree exists for the same configuration of
I (See Table 1).

2.

Algorithm 1 Low Degree Overlay Design Algorithm (LowODA)
1: OverlayEdges ← ∅
2: V ← Set of all nodes
3: G0 (V, E 0 ) ← Complete graph on V
4: for {u, v} ∈ E 0 do
5:
w{u, v} ← Number of topics that nodes u and v have
in common
6: end for
7: while G(V,OverlayEdges) is not topic-connected do
8:
Let e1 be a maximum weight edge in G0 (V, E 0 , w)
among the ones which increase the maximum degree
of G(V,OverlayEdges) minimally.
9:
Let e2 be a maximum weight edge in G0 (V, E 0 , w)
10:
if w(e1 ) ≥ w(e2 )/k then
11:
e = e1
12:
else
13:
e = e2
14:
end if
S
15:
OverlayEdges = OverlayEdges e
0
0
16:
E ←E −e
17:
for {u, v} ∈ E 0 do
18:
w{u, v}
←
NC(V
OverlayEdges)
S ,
NC(V ,OverlayEdges {u, v} )
19:
end for
20: end while
maximum degree by 1 or not increase it at all. The crux
in the analysis of this algorithm is to show that each of the
edges will reduce the number of connected components by a
“large” amount without increasing the maximum degree by
too much.

3.

LOW DEGREE OVERLAY DESIGN ALGORITHM (LOW-ODA)

APPROXIMATION RATIO

Let k be a parameter with value in [1, n] (k does notp
necessarily need to be a constant, e.g., k may be equal to (n)).

In this section we present our overlay design algorithm
(Low-ODA) for the Low-TCO problem. The weight of an
edge (u, v) is given by the reduction on the number of topicconnected components which would result from the addition
of (u, v) to the current overlay network. Let 1 ≤ k ≤ n.
Low-ODA starts with the overlay network G(V, ∅). At each
iteration of Low-ODA, the algorithm considers two edges:

Theorem 1. The overlay network output by Low-ODA
P
has average node degree within a factor of O(k∗log( v∈V |{t ∈
T |I(v, t) = 1}|)) from the minimum possible average node
degree for any topic-connected overlay network on V . We
prove that the following both hold for Low-ODA.
Theorem 2. The overlay network output by Low-ODA
P
has maximum node degree within a factor of O((n/k)∗log( v∈V
|{t ∈ T |I(v, t) = 1}|)) from the minimum possible maximum
node degree for any topic-connected overlay network on V .

e1 : a maximum weight edge among the ones which minimally increases maximum degree of the current graph

Please refer to the full version of this paper [2] for the
proofs.

e2 : a maximum weight edge
If the weight of edge e1 is greater than the weight of e2
divided by k, edge e1 is added to edge set of the overlay
network; otherwise edge e2 is added.
Let N C(V, E) denote total number of topic connected
components in the overlay network given by (V, E).
At each iteration of the while loop, two edges are considered: an edge (e1 ) with maximum weight among the edges in
E 0 that increase the maximum degree of the current graph
minimally and an edge (e2 ) with maximum weight in all of
E 0 . If weight of the first one (e1 ) is greater than or equal
to weight of the second one (e2 ) over k, e1 is added to the
set of overlay edges; otherwise e2 is added. Note that the
addition of an edge to OverlayEdges can either increase the

4.

REFERENCES

[1] G. Chockler, R. Melamed, Y. Tock, and R. Vitenberg.
Constructing scalable overlays for pub-sub with many
topics. In PODC, pages 109–118, 2007.
[2] M. Onus and A. W. Richa. Parameterized maximum
and average degree approximation in topic-based
publish-subscribe overlay network design. In Technical
Report, Arizona State University, Department of
Computer Science and Engineering, TR-09-006, 2009.
[3] M. Onus and A. W. Richa. Minimum maximum degree
publish-subscribe overlay network design. In
INFOCOM, 2009.

40

Principles of Robust Medium Access and an Application
to Leader Election
BARUCH AWERBUCH, The Johns Hopkins University, Baltimore, MD
ANDREA RICHA, Arizona State University, Tempe, AZ
CHRISTIAN SCHEIDELER, University of Paderborn, Paderborn Germany
STEFAN SCHMID, Telekom Innovation Laboratories (T-Labs) & TU, Berlin, Germany
JIN ZHANG, Arizona State University, Tempe, AZ

This article studies the design of medium access control (MAC) protocols for wireless networks that are
provably robust against arbitrary and unpredictable disruptions (e.g., due to unintentional external interference from co-existing networks or due to jamming). We consider a wireless network consisting of a set
of n honest and reliable nodes within transmission (and interference) range of each other, and we model
the external disruptions with a powerful adaptive adversary. This adversary may know the protocol and its
entire history and can use this knowledge to jam the wireless channel at will at any time. It is allowed to jam
a (1 − )-fraction of the timesteps, for an arbitrary constant  > 0 unknown to the nodes. The nodes cannot
distinguish between the adversarial jamming or a collision of two or more messages that are sent at the
same time. We demonstrate, for the first time, that there is a local-control MAC protocol requiring only very
limited knowledge about the adversary and the network that achieves a constant (asymptotically optimal)
throughput for the nonjammed time periods under any of the aforementioned adversarial strategies. The
derived principles are also useful to build robust applications on top of the MAC layer, and we present an
exemplary study for leader election, one of the most fundamental tasks in distributed computing.
Categories and Subject Descriptors: C.2.5 [Computer-Communication Networks]: Local and Wide-Area
Networks—Access schemes; F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical
Algorithms and Problems—Sequencing and scheduling
General Terms: Algorithms, Reliability, Theory
Additional Key Words and Phrases: Wireless ad-hoc networks, MAC protocols, jamming
ACM Reference Format:
Baruch Awerbuch, Andrea Richa, Christian Scheideler, Stefan Schmid, and Jin Zhang. 2014. Principles of
robust medium access and an application to leader election. ACM Trans. Algor. 10, 4, Article 24 (July 2014),
26 pages.
DOI: http://dx.doi.org/10.1145/2635818

This work is supported in part by NSF awards CCF-0830791 and CCF-0830704, and by DFG projects SCHE
1592/2-1 and SFB 901.
Authors’ addresses: B. Awerbuch, Department Computer Science, The Johns Hopkins University,
Baltimore, MD 21218; A. Richa, Computer Science and Engineering, SCIDSE, Arizona State University,
Tempe, AZ 85287; C. Scheideler, Department of Computer Science, University of Paderborn, Germany; S.
Schmid, Telekom Innovation Laboratories (T-Labs) & TU, Berlin, Germany; J. Zhang, Computer Science and
Engineering, SCIDSE, Arizona State University, Tempe, AZ 85287.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2014 ACM 1549-6325/2014/07-ART24 $15.00

DOI: http://dx.doi.org/10.1145/2635818

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24

24:2

B. Awerbuch et al.

1. INTRODUCTION

The efficient use of a shared medium is arguably not only one of the most relevant
but also one of the most complex problems in distributed computing. First, a wireless network requires distributed access coordination mechanisms that minimize the
internal interference due to simultaneous transmissions from wireless devices in the
same network. In addition, the availability of the wireless medium can vary significantly over time due to the external interference, for example, due to disturbances
from other sources such as microwaves, due to transmissions of coexisting (potentially
mobile) networks, or due to intentional or even adversarial interruptions. Adversarial
attacks constitute a major threat, especially because they often do not require any
special hardware and may be implemented by simply listening to the open medium
and broadcasting in the same frequency band as the network.
This article studies the design of distributed medium access schemes that are robust
even against a powerful adversary who can block the medium at arbitrary and unpredictable times and in an adaptive manner (i.e., depending on the protocol history). This
adversarial model is used to capture a wide range of interference scenarios. Despite
the adversary’s power, we show that provably robust medium access solutions exist
in the sense that in the time periods where the medium is available, there are many
successful transmissions.
1.1. Our Model

We attend to a wireless network consisting of n reliable and honest nodes within
each other’s transmission (and interference) range. All of the nodes are continuously
contending for sending a packet on the wireless channel. We assume that time proceeds
in synchronous timesteps and in each timestep any node may decide to transmit a
packet. A node may either transmit a message or sense the channel at a timestep,
but it cannot do both, and there is no immediate feedback mechanism telling a node
whether its transmission was successful. A node that is sensing the channel may either
(1) sense an idle channel (in case no other node is transmitting at that time), (2) sense
a busy channel (in case two or more nodes transmit at the timestep), or (3) receive a
packet (in case exactly one node transmits at the timestep).
In addition to these nodes, there is an adversary. We allow the adversary to know the
protocol and its entire history and to use this knowledge in order to jam the wireless
channel at will at any time (i.e, the adversary is adaptive). Whenever it jams the
channel, all nodes will notice a busy channel. However, the nodes cannot distinguish
between the adversarial jamming or a collision of two or more messages that are sent
at the same time. We assume that the adversary is only allowed to jam a (1−)-fraction
of the time steps, for an arbitrary constant  > 0 unknown to the honest nodes.
We allow the adversary to perform bursty jamming. More formally, an adversary is
called (T , 1 − )-bounded for some T ∈ N and 0 <  < 1 if for any time window of size
w ≥ T the adversary can jam at most (1 − )w of the timesteps in that window. A MAC
protocol is called c-competitive against some (T , 1 − )-bounded adversary (with high
probability1 or on expectation) if, for any sufficiently large number of timesteps, the
nodes manage to perform successful message transmissions in at least a c-fraction of
the timesteps not jammed by the adversary (with high probability or on expectation).
Our goal is to design a symmetric local-control MAC protocol that is constant competitive against any (T , 1 − )-bounded adversary, that is, there is no central authority
controlling the nodes, and the nodes have symmetric roles at any point in time. The
nodes do not know , but we do allow them to have a very rough upper bound of their
1 With

high probability or w.h.p. means a probability of at least 1 − 1/nc for any constant c > 0.

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:3

number n and T . More specifically, we will assume that the nodes have a common
parameter γ = O(1/(log T + log log n)). Such an estimate leaves room for a superpolynomial change in n and a polynomial change in T over time, so it does not make the
problem trivial (as it would be the case if the nodes knew constant factor approximations of n or T ).
1.2. Our Contributions

This article introduces techniques for the design of robust medium access protocols. In
particular, it presents the first MAC protocol that is constant competitive w.h.p., under
any (T , 1 − )-bounded adversary, given that the protocol is executed for a sufficiently
long time. The protocol does not need to know , and  can be an arbitrarily small
constant. The developed principles can also be used to build robust applications on
top of the MAC layer. In this respect, we present a new solution to the leader election
problem—an evergreen in the distributed computing. Our solution is not only robust
to interference, but it is also self-stabilizing in the sense that it converges to a correct
state from any initial state. This is particularly interesting in dynamic environments.
We are not aware of any similarly robust solution to the leader election problem.
1.3. Related Work

Wireless network jamming has been extensively studied in the applied networking
domain [Alnifie and Simon 2007; Brown et al. 2006; Chiang and Hu 2007; Law et al.
2005; Li et al. 2007; Liu et al. 2007; Navda et al. 2007; Negi and Perrig 2003; Thuente
and Acharya 2006; Wood et al. 2007; Xu et al. 2005, 2006]. Mechanisms for launching
jamming attacks [Chiang and Hu 2007; Law et al. 2005; Li et al. 2007; Xu et al. 2005]
as well as defense mechanisms against these attacks [Alnifie and Simon 2007; Chiang
and Hu 2007; Wood et al. 2007; Li et al. 2007; Liu et al. 2007; Navda et al. 2007; Brown
et al. 2006; Xu et al. 2005] have been proposed and validated through simulations and
experiments.
Traditional defenses against jamming primarily focus on the design of physical layer
technologies, such as spread spectrum [Liu et al. 2007; Navda et al. 2007; Simon et al.
2001]. Although widely spread frequencies could potentially help in guarding against
physical layer jamming, spread spectrum techniques cannot be used effectively in the
relatively narrow frequency bands used by the 802.11 standard.
More recent work has also focused on various MAC layer strategies in order to
handle jamming, including coding strategies [Chiang and Hu 2007], channel surfing
and spatial retreat [Xu et al. 2004; Alnifie and Simon 2007], or mechanisms to hide
messages from a jammer, evade its search, and reduce the impact of corrupted messages
[Wood et al. 2007]. Most of these strategies have only been evaluated experimentally
and would not help against the jammers considered in this article.
The study by Bayraktaroglu et al. [2008] shows both theoretically and experimentally
that an adaptive jammer, such as the one proposed here, can dramatically reduce the
throughput of the standard random backoff MAC protocol of the IEEE 802.11 standard
with only limited energy cost on the adversary side (please also refer to Bayraktaroglu
et al. [2008] for other references on jamming in 802.11).
Adversarial jamming has also been studied theoretically. There are two basic approaches in the literature. The first assumes that messages may be corrupted at random (e.g., Pelc and Peleg [2005]), and the second bounds the number of messages that
the adversary can transmit or disrupt due to, for example, a limited energy budget
(e.g., Gilbert et al. [2006] and Koo et al. [2006]). In a single hop wireless network (like
ours), messages will not be corrupted independently at random (every time the jammer
transmits, all messages in that timestep will be corrupted); moreover, an adaptive adversary seems more powerful than one that jams uniformly at random [Bayraktaroglu
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:4

B. Awerbuch et al.

et al. 2008]. Hence, we focus on the second line of theoretical work because it is more
relevant to the results in this article.
The results in Gilbert et al. [2006] and Koo et al. [2006] address adversarial jamming
at both the MAC and network layers, where the adversary may not only be jamming the
channel but also introducing malicious (fake) messages (possibly with address spoofing). The results in Gilbert et al. [2006] only consider the scenario that the nodes have
one message to transmit (e.g., a broadcast operation). When translated to our continuous datastream scenario, the protocol presented in Gilbert et al. [2006] would not be
able to sustain a constant-competitive ratio if the adversary is allowed to jam more than
half of the timesteps (i.e., if  < 1/2), given the fact that their single message broadcast
algorithm takes at least twice as many steps as the number of timesteps utilized by
the jammer. Moreover, Gilbert et al. [2006] assumes that the nodes have knowledge of
n and of the fact that the adversary has a bounded number of messages it can transmit
(in contrast, we only need the nodes to have an estimate on log log n and log T ).
Koo et al. [2006] consider a wireless network in which node positions form a grid
where multiple (at most t) adversarial nodes are allowed in the direct neighborhood of
a node. If t is at most a suitably small constant, then they give a protocol for reliable
broadcast of a single message given that there is a fixed bound on the number of
timesteps the adversary is disrupting communication (if t is large, no broadcast protocol
is guaranteed to terminate). The authors only show that eventually the broadcast
operation will be completed, but they give no bounds on how long that will take.
Moreover, their algorithms will clearly deplete the energy of the nonfaulty nodes at a
higher rate than that of the faulty nodes.
Most of the theoretical work on the design of efficient MAC protocols has focused
on random backoff protocols (e.g., Bender et al. [2005], Chlebus et al. [2006], Goldberg
et al. [2000], Hastad et al. [1996], Kwak et al. [2005], and Raghavan and Upfal [1999])
that do not take jamming activity into account and, therefore, are not robust against
it. MAC protocols have also been designed in the context of broadcasting (e.g., Czumaj
and Rytter [2006]) and clustering (e.g., Kuhn et al. [2004]). Most of them use random
backoff or tournaments to handle interference and thereby achieve a fast runtime.
In general terms, in a random backoff protocol, each node periodically attempts to
transmit a message starting with a certain probability p. In case the message transmission is unsuccessful (due to interference), the node will retry sending the message
in the next timesteps with monotonically decreasing probabilities (e.g., p2 , p4 , p8 , . . .)
until the message is successfully transmitted or the minimum allowable probability is
reached. In a dense network (as in our single-hop scenario), an adversary with knowledge of the MAC protocol would simply wait until the nodes have reached transmission
probabilities that are inversely proportional to the number of close-by nodes to start
jamming the channel, forcing the nodes to lower their transmission probabilities by so
much that a constant throughput is not achievable.
The multichannel medium access problem introduced in the theory community by
Dolev et al. [2007b] and also studied in Dolev et al. [2007a, 2007b, 2008, 2009], Gilbert
et al. [2009a, 2009b], and Meier et al. [2009], a node can only access one channel at a
time, which results in protocols with a fairly large runtime (which can be exponential for
deterministic protocols [Dolev et al. 2007a; Gilbert et al. 2009a] and at least quadratic
in the number of jammed channels for randomized protocols [Dolev et al. 2008; Meier
et al. 2009] if the adversary can jam almost all channels at a time). Recent work
[Dolev et al. 2009] also focuses on the wireless synchronization problem which requires
devices to be activated at different times on a congested single-hop radio network to
synchronize their round numbering while an adversary can disrupt a certain number
of frequencies per round. Gilbert et al. [2009b] study robust information exchange in
single-hop networks.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:5

Our model was first introduced in Awerbuch et al. [2008], where the competitive
throughput result is derived. Subsequently, the approach was successfully extended
to reactive jamming environments [Richa et al. 2011], multihop Unit Disk networks
[Richa et al. 2010] and coexisting networks [Richa et al. 2012].
There is also a large body on the leader election application considered in this article.
Leader election is an evergreen in distributed algorithms research and there exist many
theoretical and practical results [Awerbuch 1987; Gallager et al. 1983; Korach et al.
1990; Lee et al. 2007; Nakano and Olariu 2000; Nikano and Olariu 2002; Smaragdakis
et al. 2004; Willard 1986]. The following two book chapters provide a good introduction:
Chapter 3 in Attiya and Welch [2004] and Chapter 8 in Hromkovic et al. [2005]. A leader
election algorithm should be as flexible as possible in the sense that a correct solution is
computed independently of the initial network state. For instance, the algorithm should
be able to react to a leader departure or be able to cope with situations where, for some
reasons, multiple nodes consider themselves leaders. Self-stabilization [Dijkstra 1974]
is an attractive concept to describe such self-repairing properties of an algorithm, and it
has been intensively studied already, not only in terms of eventual stabilization but also
in terms of guaranteed convergence times (see, e.g., the works on time-adaptive selfstabilization such as Kutten and Patt-Shamir [1997]). Several self-stabilizing leader
election protocols have been devised [Antonoiu et al. 1996; Cai et al. 2009; Itkis et al.
1995] (see also the fault-contained solutions such as Ghosh and Gupta [1996]). However,
none of these approaches allows us to elect a leader in a wireless network that is
exposed to harsh interference or even adaptive jamming. But such interruptions of
communication are often unavoidable in wireless systems, and we believe that electing
a leader can be particularly useful in such harsh environments.
1.4. Organization

The remainder of this article is organized as follows. Section 2 introduces the main
principles of our approach and presents the robust medium access protocol (Section 2.1).
We prove competitive throughput in Section 2.2 and also show that the number of
useless message transmission attempts in times of high external interference is small
(i.e., the protocol does not waste transmission energy). Section 3 then attends to the
specific application of leader election and presents a protocol (Section 3.1) together
with a proof of the robustness properties (Section 3.2). Section 4 concludes the article.
2. ROBUST MEDIUM ACCESS

In this section, we present and analyze our MAC protocol. We start with a description
of our basic ideas behind the protocol and then provide the formal listing of the protocol
and analyze its competitiveness.
Our MAC protocol is based on a simple idea. Suppose that each node v decides to
send a message at the current
timestep with probability pv with pv ≤ p̂ for some small
constant 0 < p̂ < 1. Let p = v pv , q0 be the probability that the channel is idle and
q1 be the probability that exactly one node is sending a message. Then, the following
claim holds.
CLAIM 2.1. q0 · p ≤ q1 ≤

q0
1− p̂

PROOF. It holds that q0 =
q1 ≤


v

pv



· p.
v (1

− pv ) and q1 =


v

pv



w=v (1

− pw ). Hence,

 
1 
q0 · p
and q1 ≥
(1 − pw ) =
pv
(1 − pw ) = q0 · p.
1 − p̂ w
1 − p̂
v
w

Hence, if the nodes observe that the number of timesteps in which the channel is
idle is essentially equal to the number of timesteps in which exactly one message is
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:6

B. Awerbuch et al.


sent, then p = v pv is likely to be around 1. Otherwise, they know that they need
to adapt their probabilities. Therefore, if we had sufficiently many cases in which an
idle channel or exactly one message transmission is observed (which is the case if the
adversary does not heavily jam the channel and p is not too large), then one can adapt
the probabilities pv just based on these two events and ignore all cases in which the
wireless channel is blocked (either because the adversary is jamming it or at least two
messages interfere with each other). Essentially, the following strategy could be used
at every node for some small enough γ > 0:
In each timestep, every node v is sending a message with probability pv . If it decides
not to send a message, it checks the following two cases:
—If the wireless channel is idle, then pv := (1 + γ ) pv .
—If exactly one message is sent, then pv := (1 + γ )−1 pv .
The beauty of the algorithm is that it ignores blocked timesteps, which makes it more
robust against adversarial jamming: the access probabilities are maintained. However,
there is a catch to this strategy because it only works well as long as p does not get
too high. If p is initially very high or by chance gets very high, it will be extremely
unlikely for the nodes to observe one of the two cases mentioned previously. Hence,
further ideas are necessary.
Our idea is to use a threshold Tv for each node v that cuts its time into time intervals.
If v does not observe a successful message transmission for Tv many steps, then pv is
decreased. In this way, eventually p will become small. However, because the algorithm
is not aware of T , the time window of the adversary, p may be decreased too quickly
or too slowly in this way. Hence, we need proper rules for adapting Tv over time. It
turns out that the following rules work: whenever v senses a successful transmission,
Tv is decreased by 1, and whenever v does not sense a successful transmission for
Tv timesteps, Tv is increased by 1 for the next time interval considered by v. One
may ask why Tv should not be decreased as well if an idle channel is sensed, but
interestingly this is not a good rule, as will come out in the analysis. Next, we give a
formal description of our MAC protocol.
2.1. Description of the MAC Protocol

In our MAC protocol, each node v maintains a probability value pv , a threshold Tv and
a counter cv . The parameter γ is the same for every node and is set to some sufficiently
small value in O(1/(log T + log log n)). Thus, we assume that the nodes have some
polynomial estimate of T and even rougher estimate of n. Let p̂ be any constant so
that 0 < p̂ ≤ 1/24. Initially, every node v sets Tv := 1, cv := 1 and pv := p̂. (These
initial values are not required for a correct execution of our protocol, as we will prove.)
Afterward, the protocol works in synchronized timesteps. We assume synchronized
timesteps for the analysis, but a nonsynchronized execution of the protocol would also
work as long as all nodes operate at roughly the same speed.
In each step, each node v does the following: v decides with probability pv to send a
message. If it decides not to send a message, it checks the following two conditions:
(1) If v senses an idle channel, then pv := min{(1 + γ ) pv , p̂}.
(2) If v successfully receives a message, then pv := (1+γ )−1 pv and Tv := max{1, Tv −1}.
Afterward, v sets cv := cv + 1. If cv > Tv then it does the following: v sets cv := 1,
and if there was no step among the past Tv timesteps in which v sensed a successful
message transmission, then pv := (1 + γ )−1 pv and Tv := Tv + 1.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:7

2.2. Robustness

Let N = max{T , n}. In this section, we will prove the following theorem.
THEOREM 2.2. For n ≥ 2 the MAC protocol is constant competitive w.h.p.
under any (T , 1 − )-bounded adversary if the protocol is executed for at least
( 1 log N max{T , γ1 2 log3 N}) many timesteps.
In fact, as we will prove in Theorem 2.16, our protocol can quickly recover from
any setting of the (Tv , cv , pv )-values: the cumulative probability pt at time t will quickly
converge to a relatively high value that yields a good throughput, and the Tv values cannot grow infinitely. However, we will first analyze the MAC protocol for well-initialized
settings (Tv := 1, cv := 1 and pv := p̂).
Notice that for n = 1 a node will never experience a timestep with a successful
transmission. Hence, it would just keep reducing its access probability in our protocol,
thereby reaching a dormant state, which is the best it can do in this case, as there is
no one else to communicate with. Thus, it only makes sense to consider the case n ≥ 2.
The proof of Theorem 2.2 will frequently use the following general form of the wellknown Chernoff bounds, which may be of independent interest. They are derived from
Chernoff bounds presented in Schmidt et al. [1995].
LEMMA 2.3. Consider any set of binary
X1 ,. . . , Xn. Suppose that
random variables

there are values p1 , . . .
, pn ∈ [0, 1] with 
E[ i∈S Xi ] ≤ i∈S pi for every set S ⊆ {1, . . . , n}.
n
n
Xi and μ = i=1
pi and any δ > 0 that
Then it holds for X = i=1

μ
δ2 μ
eδ
− 2(1+δ/3)
≤
e
.
Pr[X ≥ (1 + δ)μ] ≤
(1 + δ)1+δ


If, on the other hand, it holds that E[ i∈S Xi ] ≥ i∈S pi for every set S ⊆ {1, . . . , n}, then
it holds for any 0 < δ < 1 that

μ
e−δ
2
Pr[X ≤ (1 − δ)μ] ≤
≤ e−δ μ/2 .
1−δ
(1 − δ)
Let V be the set of all nodes. For the proof of the theorem, we will consider all possible
decompositions of V into a single node v0 and U = V \{v0 }. Let pt (v) be node
v’s access
probability pv at the beginning of the t-th timestep. Furthermore, let pt = v∈U pt (v)
(i.e., without node v0 ) and L = ( 1 log N max{T , γ1 2 log3 N}) be the number of timesteps
for which we study the competitiveness of the protocol. If L ≥ N, we will redefine
N to N = max{T , n, L} in order to cover long runtimes. If we can prove a constant
competitiveness for any such L, Theorem 2.2 follows.
We prove the theorem by induction over sufficiently large timeframes. Let I be a
2
timeframe consisting of α log N subframes I  of size f = max{T , αβ
log3 N}, where α
γ 2
and β are sufficiently large constants. Let F = α log N · f denote the size of I. In
order to simplify the calculations, we assume that f and F are integer multiples of
T and that T = (1/) is above a sufficiently
large constant.
Moreover, we assume
√
√
2
2 f
that at the beginning of I, pt ≥ 1/( f (1 + γ )
) and Tv ≤ F/2 for every node v. Our
goal is to show that in this case the MAC protocol is constant competitive for√ I with
2
2 f
respect
) and
√ to every subset U = V \{v0 } and at the end of I, pt ≥ 1/( f (1 + γ )
c
Tv ≤ F/2 for every node v with probability at least 1 − 1/N for any constant c > 0
(which we will also call with high probability or w.h.p. in the following). Since initially
Tv = 1 and pv = p̂ for every v, this implies that the MAC protocol achieves a constant
competitiveness in the first timeframe, w.h.p., and due to the properties on Tv and pv ,
this also holds for polynomially many timeframes, w.h.p.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:8

B. Awerbuch et al.

The proof for timeframe I proceeds as follows. Consider some fixed subset U =
V \{v0 }. A timestep t or subframe I  of I with starting time t is called good if pt ≤ 9.
Otherwise, it is called
bad. First, we show that for any
subframe I  in which initially
√
√
2
2 f
2
2 f
), also afterward pt ≥ 1/( f√(1+γ )
), w.h.p. (Lemma 2.4). Then we
pt ≥ 1/( f (1+γ )
show that for any subframe I  with Tv ≤ (3/4) F for every node v ∈ U at the beginning
of I  , the subsequent subframe is good with probability at least 1−1/ f c for any constant
c > 0 (which we will call with moderate probability or w.m.p.) (Lemma 2.7). Based on
the insights gained in the proof, we show that in a good subframe I  , all nonjammed
timesteps in I  are good w.m.p. (Corollary 2.11). After that, we prove that a constant
fraction of the timesteps in such a subframe also have probabilities lower bounded
by a constant (Lemma 2.12), w.h.p., which implies that the MAC protocol is√
constant
competitive for I  w.m.p. (Lemma 2.13). If at the beginning of frame I, √
Tv ≤ F/2 for
every node v ∈ U , then during the first eighth of I, called J, Tv ≤ (3/4) F, no matter
what happens to the nodes in J. This allows us to show that a constant fraction of the
subframes of J are constant competitive w.h.p., which implies that the MAC protocol is
constant competitive for J √
w.h.p. (Lemma 2.14). With that insight we can show that if
at the beginning of J, Tv ≤ F/2 for every node v ∈ U , then this also holds at the end of
J w.h.p. (Lemma 2.15). Hence, all eighths of I have a constant competitiveness, w.h.p.,
√
which implies that I has a constant competitiveness and at the end of I, Tv ≤ F/2
for every node v, w.h.p. Applying these results inductively over all timeframes I yields
Theorem 2.2.
At the end of this subsection, we also study the recovery properties of our MAC
protocol (Theorem 2.16). It turns out that the MAC protocol can get quickly out of any
set of ( pv , cv , Tv )-values, which implies that it also works well if the nodes enter the
network at arbitrary times and with arbitrary values instead of starting the protocol
at the same time and with the same values, which is not realistic in practice.
initially pt0 ≥ 1/( f 2 (1 + γ )2
LEMMA 2.4. For any subframe I  in which
√
timestep t of I  satisfies pt ≥ 1/( f 2 (1 + γ )2 f ), w.h.p.

√

f

), the last

PROOF. We start with the following claim about the maximum number of times nodes
decrease their probabilities in I  due to cv > Tv .
CLAIM 2.5. If in subframe I  the number of successful
message transmissions is at

most k, then every node v increases Tv at most k + 2 f many times.
PROOF. Only successful message transmissions reduce Tv . If there is no successful
message transmission within Tv many steps, Tv is increased. Suppose that k = 0. Then
the number of times a node v increases Tv is upper bounded by the largest possible

Tv0 +	
0
0
	 so that i=T
2 f,
0 i ≤ f , where Tv is the initial size of Tv . For any Tv ≥ 1, 	 ≤
v
so the claim is true for k = 0. At best, each additional successful transmission allows
us to reduce all thresholds for v by 1, so we are searching for the maximum 	 so that

Tv0 −k+	
max{i, 1} ≤ f . This 	 is upper bounded by k+ 2 f , which proves our claim.
i=T 0 −k
v

This claim allows us to show the following claim.


CLAIM 2.6. For any interval I  with
√  T ≤ |I | ≤ f and |I | being an integer multiple
2
|I
|
), 1/ f 2 ] for the first timestep t0 in I  , there is a
of T in which pt0 ∈ [1/( f 2 (1 + γ )
timestep t in I  with pt ≥ 1/ f 2 , w.h.p.

PROOF. Suppose that there are g nonjammed timesteps in I  . Let k0 be the number of
these steps with an idle channel and k1 be the number of these steps with a successful
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:9

message transmission. Furthermore, let k2 be the maximum number of times a node v
increases Tv in I  . If all timesteps t in I  satisfy pt < 1/ f 2 , then it must hold that
	


k0 − log1+γ (1/ f 2 )/ pt0 ≤ k1 + k2
This is because no v has reached a point with pt (v) = p̂ in this case, which implies
that for each timestep t with an idle channel, pt +1 = (1 + γ ) pt . Furthermore, at most
log1+γ (1/ pt0 ) increases of pt due to an idle channel would be needed to get pt to 1/ f 2 ,
and then there would have to be a balance between further increases and
 decreases of
pt to avoid the case pt ≥ 1/ f 2 . We know from Claim 2.5 that k2 ≤ k1 + 2|I  |. Hence,


k0 ≤ 2 |I  | + 2k1 + 2|I  |
Since
T =

 (1/) is assumed to be above a sufficiently large constant, it holds that
2 |I  | + 2|I  | ≤ |I  |/2. Because g ≥ |I  | due to our adversarial model, it follows
that we must satisfy k0 ≤ 2k1 + g/2.
For any timestep t with pt ≤ 1/ f 2 ,

pv (t) = pt + p̂
Pr[≥ 1 message transmitted at t] ≤
v

≤ 1/ f 2 + p̂
where p̂ is due to node v0 not considered in pt . Hence, E[k0 ] ≥ (1 − 1/ f 2 − p̂)g and
E[k1 ] ≤ (1/ f 2 + p̂)g. To prove bounds on k0 and k1 that hold w.h.p., we can use the
general Chernoff bounds stated earlier. For any step t, let the binary random variable
Xt be 1 if and only if the channel is idle at step t or pt > 1/ f 2 . Then,
Pr[Xt = 1] = Pr[channel idle and pt ≤ 1/ f 2 ] + Pr[ pt > 1/ f 2 ]
= Pr[ pt ≤ 1/ f 2 ] · Pr[channel idle | pt ≤ 1/ f 2 ] + Pr[ pt > 1/ f 2 ]
≥ Pr[ pt ≤ 1/ f 2 ](1 − 1/ f 2 − p̂) + Pr[ pt > 1/ f 2 ]
≥ 1 − 1/ f 2 − p̂,
and because this probability bound holds irrespective of prior steps and is independent
of the adversarial jamming decision at time t, it follows for any set S of timesteps prior
to some timestep t that



Pr Xt = 1 |
Xs = 1 ≥ 1 − 1/ f 2 − p̂.
s∈S


Thus, for any set of timesteps S, it holds that E[ s∈S Xs ] ≥ (1 − 1/ f 2 − p̂)|S| . Together
with the fact that g ≥  f ≥ α log N, the Chernoff bounds imply that, w.h.p., either
k0 > 3g/4 (given that p̂ ≤ 1/24) or we have a timestep t with pt ≥ 1/ f 2 .
On the other hand, let the binary random variable Yt be 1 if and only if exactly one
message is sent at time t and pt ≤ 1/ f 2 . Then,
Pr[Yt = 1] = Pr[ pt ≤ 1/ f 2 ] · Pr[one msg sent | pt ≤ 1/ f 2 ]
≤ 1/ f 2 + p̂,
and it holds for any set S of timesteps prior to some timestep t that



Ys = 1 ≤ 1/ f 2 + p̂.
Pr Yt = 1 |
s∈S

Thus, the Chernoff bounds imply that k1 < g/8, w.h.p. (given that p̂ ≤ 1/24). That,
however, would violate the condition that k0 ≤ 2k1 + g/2.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:10

B. Awerbuch et al.

Note that the choice of g is not oblivious as the adversary may adaptively decide
to set g based on the history of events. Hence, we need to sum up the probabilities
over all adversarial strategies of selecting g to show that none of them succeeds, but
because there are only f many, and for each the claimed property holds w.h.p., the
claim follows.
From Claim 2.6, it follows that under the conditions of Lemma 2.4 there is a timestep
t in subframe I  with pt ≥ 1/ f 2 , w.h.p. Let t be the last of these timesteps. If t belongs
to one of the last β log N nonjammed steps in I  , then it follows for the probability pt
at the end of I  that
pt ≥

√
1
1
√ ,
· (1 + γ )−2β log N+ 2 f ≥ 2
2
f
f (1 + γ )2 f

given that  = (1/ log3 N) as at most β log Ndecreases of pt can happen due to a
successful transmission and at most β log N + 2 f decreases of pt can happen due to
exceeding Tv .
Otherwise, suppose that t belongs to one of the last T steps in I  . Because there must
be at least β log N nonjammed steps in the time interval I  from t till the end of I 
(otherwise we are in case 1), we conclude similarly to the proof of Claim 2.6 that for
the nonjammed timesteps in I  , k0 ≥ 2k1 w.h.p., unless there is a timestep
t in I  with
√
2
pt ≥ 1/ f , which contradicts the definition of t. Moreover, k2 ≤ k1 + 2T for I  due to
Claim 2.5 and the fact that |I  | ≤ T . Hence, it follows for the probability pt at the end
of I  that
√
1
1
√ ,
pt ≥ 2 · (1 + γ )− 2T ≥ 2
f
f (1 + γ )2 f
w.h.p., unless there is a timestep t in I  with pt ≥ 1/ f 2 , which contradicts the definition
of t.
Otherwise, t belongs to a timestep before the last T steps in I  . Let I  be the largest
possible time interval starting with t with |I  | being an integer multiple of T that ends
no later than I  . Then it follows from Claim 2.6 that there must be a timestep t in I 
with pt ≥ 1/ f 2 , w.h.p., contradicting the definition of t. Hence, w.h.p., only one of the
first two cases will apply, which finishes the proof of Lemma 2.4.
√
LEMMA 2.7. For any subframe I  with Tv ≤ (3/4) F for all nodes v at the beginning
of I  , the last timestep t of I  satisfies pt ≤ 9 w.m.p.
PROOF. We first show that there is a timestep t in I  with pt ≤ 6, w.h.p. Let the
timesteps in which the adversary does not jam the channel and at most one message
is sent by the nodes be called useful. Suppose that there are g useful timesteps in I  .
Let k0 be the number of these steps with an idle channel and k1 be the number of these
steps with a successful message transmission. To establish a relationship between k0
and k1 , we need the following claims.
CLAIM 2.8. If all timesteps t ∈ I  satisfy pt > 6, then it holds for any g ≥ δ log N for a
sufficiently large constant δ that k1 ≥ k0 w.h.p.
PROOF. Let q0 (t) be the probability of an idle channel and q1 (t) be the probability of
a successful message transmission at a useful step t. If pt > 6, then it follows from
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:11

Claim 2.1 that the probability of an idle channel (given a useful round) is at most
q0 (t)
q0 (t)
≤
q0 (t) + q1 (t)
q0 (t) + pt · q0 (t)
1
1
= ,
≤
1+6
7
irrespective of what happened at previous timesteps. Hence, E[k0 ] ≤ g/7 under the
assumption that all useful timesteps t satisfy pt > 6. Thus, our Chernoff bounds
yield k0 ≤ g/2 w.h.p. (given that δ is a sufficiently large constant), which implies that
k1 ≥ k0 .
Pr[channel idle] =

Now we are ready for the following claim.
CLAIM 2.9. If all timesteps in I  satisfy pt > 6, then it must hold w.h.p. that
k1 − 2 log1+γ N ≤ (5/4)k0
PROOF. If exactly one message is sent at a step t, then pt+1 ≥ (1 + γ )−1 pt and
pt+1 ≤ (1 + γ )−1 ( pt − p̂) + p̂ ≤ (1 + γ )−1 pt + γ (1 + γ )−1 p̂
because only the sending node does not decrease its probability, and for this node the
maximum probability is p̂. For pt > 6, it follows that pt+1 ∈ [(1 + γ )−1 pt , (1 + γ )−4/5 pt ].
From Claim 2.8, we now that after the first δ log N useful steps, there must have been
more steps with a successful transmission than with an idle channel for any one of
the remaining useful steps w.h.p, which implies that for each of them, pv ≤ p̂/γ for
all nodes v. Thus, whenever there is an idle channel for these steps, pt+1 = (1 + γ ) pt .
Hence, if we start with pt = 6 after the first δ log N useful steps, then in order to avoid
a step t with pt ≤ 6 in I  , we must have that k1 ≤ (5/4)k0 . Because pt might be as
high as p̂n initially, we can allow at most (5/4) log1+γ N further events of a successful
message transmission without having a step t with pt ≤ 6.
Since log1+γ N = ω(log N), it holds that
δ log N + (5/4) log1+γ N ≤ 2 log1+γ N
for a sufficiently large N, which implies the claim.
Also, k0 + k1 = g. Suppose that g ≥ δ log1+γ N for a sufficiently large constant δ. It
holds that
(g − k0 ) − 2g/δ ≤ (5/4)k0 ⇔ k0 ≥ (4/9)(1 − 2/δ)g.
We know from the proof of Claim 2.8 that for any useful step t with pt > 6,
Pr[channel idle] ≤ 17 . Hence, E[k0 ] ≤ g/7. Since random decisions are made independently in each step, our Chernoff bounds imply that k0 < (4/9)(1 − 2/δ)g w.h.p. if δ
is sufficiently large.
Thus, if I  contains at least δ log1+γ N useful steps, we are done. Otherwise, notice
that for every node v, it follows
from the MAC protocol√and the choice of f and F
√
that if initially Tv ≤ (3/4)
F,
then
Tv can be at most F during I  . Let us cut I 
√
into m intervals of size 2 F each. It is easy to check that if β in the definition of f is
sufficiently large compared to δ, then m ≥ 3δ log1+γ N. If there are less than δ log1+γ N
useful steps, then at least 2δ log1+γ N of these intervals do not contain any useful step,
which implies that pv is reduced by at least (1+γ )−1 by each v in each of these intervals.
Hence, altogether, every pv gets reduced by a factor of at least (1 + γ )−2δ log1+γ N during

I . The useful timesteps can only raise that by (1 + γ )δ log1+γ N , so altogether we must
have pt ≤ 6 at some time point during I  w.h.p.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:12

B. Awerbuch et al.

In the following, let t0 denote any time in I  with pt0 ≤ 6. We finally prove the following
claim.
CLAIM 2.10. For any useful timestep t after a step t0 in I  with pt0 ≤ φ for some φ ≥ 6
and any constant δ > 0, it holds that
Pr[ pt ≥ (1 + δ)φ] ≤ 8 · (1 + δ)−1/(6γ ) .
PROOF. Suppose that t0 is the last useful timestep before step t in I  with pt0 ≤ φ. Let
g be the number of useful timesteps from t0 to t. Then g ≥ ln(1 + δ)/ ln(1 + γ ) because
otherwise it is not possible that pt ≥ (1 + δ)φ. Recall that for any useful step r with
pr ≥ 6, Pr[ pr+1 = (1 + γ ) pr ] ≤ 1/7. If exactly one message is sent at a useful step, then
pr+1 ∈ [(1 + γ )−1 pr , (1 + γ )−4/5 pr ]. Let k0 be the number of useful steps with an idle
channel and k1 be the number of useful steps with a successful message transmission.
It must hold that k0 ≥ (4/5)k1 + ln(1 + δ)/ ln(1 + γ ) so that pt ≥ (1 + δ)φ. Also, k0 + k1 = g.
Hence, k0 ≥ (4/9)g + (5/9) ln(1 + δ)/ ln(1 + γ ) ≥ max{(4/9)g, ln(1 + δ)/ ln(1 + γ )}. It holds
that E[k0 ] ≤ g/7, so the Chernoff bounds imply that
Pr[k0 ≥ (4/9)g] ≤ Pr[k0 ≥ (1 + 2)g/7]
≤ e−[2
Hence,

2

/(2(1+2/3))](g/7)

= e−g/6 .



Pr[ pt ≥ (1 + δ)φ] ≤
Pr[k0 ≥ (4/9)g] ≤
e−g/6
ln(1+δ)
g≥ ln(1+γ
)

≤ 8(1 + δ)

ln(1+δ)
g≥ ln(1+γ
)
1
− 6 ln(1+γ
)

≤ 8(1 + δ)−1/(6γ ) .

Because we assume that γ = O(1/ log f ), it follows that w.m.p. pt ≤ (1 + δ)6 for any
particular timestep t after t0 , resulting in the lemma with δ = 1/2.
Claim 2.10 with φ = 9 and δ = 1/3 implies the following result.
COROLLARY 2.11. For any good subframe I  , all nonjammed timesteps t of I  satisfy
pt ≤ 12 w.m.p.
We also need to show that for a constant fraction of the nonjammed timesteps in a
good subframe, pt is also lower bounded by a constant. Recall that p̂ ≤ 1/24.
LEMMA 2.12. For any subframe I  in which initially pt ≥ 1/( f 2 (1 + γ )2
1/8 of the nonjammed steps t satisfy pt ≥ p̂ w.h.p.

√

f

), at least

PROOF. Let G be the set of all nonjammed timesteps in I  and S be the set of all steps
t in G with pt < p̂. Let g = |G| and s = |S|. If s ≤ 7g/8, we are done. Hence, consider
the case that s ≥ 7g/8.
Suppose that pt must be increased k0 many times to get from its initial value up to
a value of p̂ and that pt is decreased k1 many times in S due a successful message
transmission. Furthermore, let k2 be the maximum number of times a node v decreases
pv due to cv > Tv in the MAC protocol. For S to be feasible (i.e., probabilities can be
assigned to each t ∈ S so that pt < p̂), it must hold for the number 	 of times in S in
which the channel is idle that
	 ≤ k0 + k1 + k2 .
For the special case that k0 = k2 = 0, this follows from the fact that whenever there
is a successful message transmission, pt is reduced to pt+1 ≥ (1 + γ )−1 pt . On the other
hand, whenever there is an idle channel, it holds that pt+1 = (1+γ ) pt because of pt < p̂.
Thus, if 	 > k1 , then one of the steps in S would have to have a probability of at least p̂,
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:13

violating the definition of S. k0 comes into the formula due to the startup cost of getting
to a value of p̂, and k2 comes into the formula because the reductions of the pt (v) values
due to cv > Tv in the MAC protocol allow up to k2 additional increases of pt for S to
stay feasible.
First, we bound 	. If pt < p̂, then Pr[idle channel at step t] ≥ 1 − p̂ − p̂ (where the
second p̂ is due to node v0 ), irrespective of prior timesteps, Hence, E[	] ≥ (1 − 2 p̂)s. For
p̂ ≤ 1/24, our Chernoff bounds imply (because
of s ≥ 7g/8 ≥ (7/8) f ) that 	 ≥ s/2 w.h.p.
√
√
At the beginning of I  , if pt ≥ 1/( f 2 (1 + γ )2 f ), then k0 ≤ 2 log1+γ f + 2 f . Moreover,

√
k2 ≤ g/8 + k1 + 2 f because of Claim 2.5. Hence, k0 + k1 + k2 ≤ 2 log1+γ f + 2 f +

2k1 + g/8 + 2 f , which must be at least s/2 so that 	 ≤ k0 + k1 + k2 (given that 	 ≥ s/2).
√
Suppose that 2 log1+γ f + 4 f ≤  f/16 (which is true if f = (1/ 2 ) is large enough).
Then for this to be true, it must hold that
2k1 + g/8 + g/16 ≥ (7g/8)/2

⇔

k1 ≥ g/8.

If k1 ≥ g/8, then also k1 ≥ s/8, so our goal will be to show that k1 < s/8 w.h.p.
If pt < p̂, then Pr[successful message transmission at step t] ≤ 2 p̂, irrespective of
prior timesteps. Hence, E[k1 ] ≤ 2 p̂s. Furthermore, for p̂ ≤ 1/24 our Chernoff bounds
imply because of s ≥ 7g/8 ≥ (7/8) f that k1 < s/8 w.h.p. Since there are at most f 2
ways (for the adversary) of choosing g and s, this holds for any combination of g and s,
which yields the lemma.
Combining these results, we get:
LEMMA 2.13. For any good subframe I  , the MAC protocol is constant competitive in
I  w.m.p.
PROOF. From Corollary 2.11 and Lemma 2.12, we know that in a good subframe at
least 1/8 of the nonjammed timesteps t have a constant probability value pt w.m.p. For
these steps, there is a constant probability that a message is successfully sent. Using
the Chernoff bounds results in the lemma.
Consider now the first eighth of frame I, called J.

√
√
LEMMA 2.14. If at the beginning of J, p ≥√1/( f 2 (1 + γ )2 f ) and Tv ≤ F/2 for all
nodes v, then we also have p ≥ 1/( f 2 (1 + γ )2 f ) at the end of J and the MAC protocol
is constant competitive for J, w.h.p.

PROOF. The bound for p at the end
√ of J directly follows from Lemma 2.4. Suppose, as
a worst case, that initially Tv = F/2 for some v. Clearly, Tv assumes the maximum
possible value at
√ the end√of J if Tv is never decreased in J. Since Tv can be increased
at most (F/8)/(
F/2) = F/4 many times in J, Tv can reach a maximum value of at
√
most (3/4) F inside of J, so we can apply Lemma 2.7.
α
Recall that J consists of k = 8
log N many subframes, numbered I1 , . . . , Ik. For each
Ii , let the binary random variable Xi be 1 if and only if Ii is good. From Lemma 2.7, it
follows that for any i ≥ 1 and any set S ⊆ {1, . . . , i − 1},
⎡
⎤

Pr ⎣ Xi = 1 |
X j = 1⎦ ≥ 1 − 1/ f c
j∈S

forsome constant c that can be made arbitrarily large. Hence, for any set S ⊆ {1, . . . , k},
E[ i∈S Xi ] ≥ (1 − 1/ f c )|S| . Our Chernoff bounds, therefore, imply that at most
(α/24) log N of the subframes in J are bad, w.h.p, if α is sufficiently large. According to Lemma 2.13, each of the good subframes is constant competitive w.m.p., where
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:14

B. Awerbuch et al.

the probability bounds are only based on events in the subframes themselves and,
therefore, hold irrespective of the other subframes (given that each of them is good). So
the Chernoff bounds imply that at most (α/24) log N of them do not result in a constant
competitiveness of the MAC protocol, w.h.p. The remaining (α/24) log N subframes in
J achieve constant competitiveness, which implies that the MAC protocol is constant
competitive on J, w.h.p.
We finally need the following lemma that bounds Tv . The proof of this lemma requires
considering all possible decompositions of V into a node v0 and U = V \{v0 } so that every
node experiences many successful transmissions.
√
LEMMA
√ 2.15. If at the beginning of J, Tv ≤ F/2 for all v, then it holds that also
Tv ≤ F/2 at the end of J, w.h.p.
PROOF. We know from Lemma 2.14 that for any node v our protocol is constant
competitive for V \{v} w.h.p. Hence, every node v notices (|J|) successful message
transmissions in J w.h.p. Tv is maximized at the end of J if all of these successful
transmissions happen at the beginning of J, which would get
tTv down to 1. Afterward,
Tv can raise to 
a value of at most t for the maximum t with
i ≤ |J|. Since such a t
i=1 √

can be at most 2|J|, it follows that Tv can be at most 2F/8 = F/2 at the end of J,
w.h.p.
Inductively using Lemmas 2.13 and 2.15 on the eighths of frame I implies that our
√
2
2 f
)
MAC protocol
√ is constant competitive on I and at the end of I, pv ≥ 1/( f (1 + γ )
and Tv ≤ F/2 for all v w.h.p. Hence, our MAC protocol is constant competitive for
L many timesteps, w.h.p., for any L = ( 1 log N max{T , γ1 2 log3 N}), which implies
Theorem 2.2.
Finally, we show that our protocol can quickly recover from any setting of the (Tv ,
cv , pv )-values.
THEOREM 2.16. For any pt0 and T̂ = maxv Tv it takes at most O( 1 log1+γ (1/ pt0 ) + T̂ 2 )
√
many timesteps,
w.h.p., until the MAC protocol satisfies again pt ≥ 1/( f 2 (1 + γ )2 f ) and
√
maxv Tv ≤ F/2 for the original definitions of F and f described earlier.
√

PROOF. Suppose that pt0 < 1/( f 2 (1 + γ )2 f ) for some timepoint t0 . Then it follows
from the constraints of the adversary and the Chernoff bounds that it takes at most
δ/ log1+γ (1/ pt0 ) steps for some sufficiently large constant δ to get the system from pt0 up
1/2
to pt0 , w.h.p. (in fact, with a probability of at least 1− ptc0 for any constant c, irrespective
1/2
1/4
of T̂ ). Another 2δ log1+γ (1/ pt0 ) steps will then get the system from pt0 to pt0 , w.h.p. (in
1/2
fact, with probability at least 1−( pt0 )c for any constant c). Continuing these arguments
1/2i

1/2i+1

it follows that altogether at most 2δ
log1+γ (1/ pt0 ) steps
in order to get from pt0 to pt0

1 √
are needed to get the system from pt0 to a probability pt ≥ f 2 (1+γ
, w.h.p. (or more
)2 f
c
precisely, with probability at least 1 − 1/N ).
√
√ It remains to bound the time to get Tv down to F/2 for every v. It holds that T̂ ≤
F/2 if and only if F ≥ 4T̂ 2 . Hence, consider a timeframe I of size F  = max{F, 4T̂ 2 }
for the old definition of F described earlier, where I√starts at the point at which the
probabilities pv have recovered to pt ≥ 1/( f 2 (1 + γ )2 f ). Then all the aforementioned
proofs go through and imply that I is constant competitive. Moreover, when cutting I
into pieces of size |I|/32 instead of |I|/8,
√ the proof of Lemma 2.15 implies that at the
end of the first 1/32-piece J of I, Tv ≤ F  /4, w.h.p. Hence, the timeframes of the nodes
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:15

shrank by a factor of√
at least 2 in J. Inductively using this bound, it follows that also
at the end of I, Tv ≤ F  /4 for all v, w.h.p. This allows
us to reduce F  by a factor of 2
√


for the next frame I. Also for this F , we get Tv ≤ F /4 for all v, w.h.p., so we can keep
shrinking I by a factor of 2 until |I| = F for the original
F considered in our previously
√
described proofs. Altogether, the recovery to T̂ ≤ F/2 for all v takes at most O(T̂ 2 )
time.
Combining the two upper bounds for the recovery time yields the theorem.
Interestingly, we can show that our MAC protocol is also efficient under adversarial
attacks in terms of transmitted messages. The first lemma follows directly from our
earlier insights.
LEMMA 2.17. For any timeframe I of size F as defined earlier, the total number of
transmitted messages by all the nodes is bounded by O(F) w.h.p.
If the adversary performs permanent jamming, the number of message transmissions
converges, that is, our MAC protocol reaches a dormant stage.

LEMMA 2.18. Consider any timestep t0 with v pv ≤ p and maxv Tv ≤ T̂ for some
values p > 0 and T̂ ≥ 1/γ . Then for any continuous jamming attack starting at t0 , the
total number of message transmissions during the entire attack is at most O( p · T̂ /γ +
log N) w.h.p.
PROOF. First, we determine the expected number of transmissions of a single node
v. Let pv (t) be the probability that v transmits a message in round t0 + t. Due to our
MAC protocol, pv (t) decreases by (1 + γ )−1 at latest for t = T̂ , then another time after
T̂ + 1 further steps, another time after T̂ + 2 further steps, and so on. Hence, the total
expected number of transmissions of v for any continuous jamming attack is at most

Tv · pv (t0 )(1 + γ )Tv −T̂
Tv ≥T̂

= pv (t0 )



(T̂ + i)(1 + γ )−i

i≥0

≤

1+γ
· T̂ · pv (t0 ) +
γ



1+γ
γ

2
· pv (t0 )

= O( pv (t0 )T̂ /γ ).
Summing up over all nodes, we obtain a total of O( p · T̂ /γ ) transmissions. Because all
transmission decisions are done independently at random, the Chernoff bounds imply
a total of at most O( p · T̂ /γ + log N) w.h.p.
In our MAC protocol, beyond f steps after any initial choice of the access probabilities,
p = O(log N) w.h.p. This is due to the proof of Lemma 2.7 and the fact that for p ≥
c log N, the probability that an idle channel is experienced is at most 1/N c , so further
increasing p has a polynomially small probability. Furthermore, T̂ = O(log2 N/γ )
w.h.p. for any constant  given that all nodes v start with Tv = 1. Hence, the total
number of transmissions of our MAC protocol under a permanent attack that starts
after f steps would be bounded by O(log3 N/γ 2 ) w.h.p.
3. AN APPLICATION TO LEADER ELECTION

Robust medium access techniques can constitute an important building block for many
robust applications. In this section, we provide an exemplary application to the classic
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:16

B. Awerbuch et al.

leader election problem where n nodes need to agree on a single leader among them.
Concretely, our goal is to design a leader election protocol that is self-stabilizing despite
adversarial jamming.
Following the usual notation in the self-stabilization literature, the system state is
determined by the state of all variables in the system. That is, the protocol and any
constants used by the protocol are assumed to be immutable and not part of the system
state. A system is called self-stabilizing if and only if (1) when starting from any state,
it is guaranteed to eventually reach a legal state (convergence) and (2) given that the
system is in a legal state, it is guaranteed to stay in a legal state (closure), provided
that there are no faults or membership changes in the system. In our case, roughly
speaking, the legal state is the state in which we have exactly one leader.
We will define the set of legal states more formally when we introduce our protocol.
Although our protocol is randomized and the leader election has to be performed under
adversarial jamming, our protocol is still guaranteed to eventually elect exactly one
leader from any initial state.
3.1. The SELECT Protocol

Our leader election algorithm (called SELECT for SElf-stabilizing Leader EleCTion) is
based on the ideas introduced for the medium access protocol. Again, each node v
maintains a parameter pv that describes v’s probability of accessing the medium at a
given moment of time. The nodes adapt and synchronize their pv values over time in
a multiplicative increase multiplicative decrease manner, that is, the value is lowered
in times of high interference or increased during times where the channel is idling.
However, pv will never exceed p̂, for some constant 0 < p̂ < 1.
In addition, each node maintains two variables, a threshold variable Tv and a counter
variable cv . Again, Tv is used to estimate the adversary’s time window T : a good
estimation of T can help the nodes recover from a situation where they experience
high interference in the network. In times of high interference, Tv will be increased
and the sending probability pv will be decreased.
Initially, every node v sets cv := 1 and pv := p̂. Note, however, that while we provide
some initial values for the variables in our description, our protocol is self-stabilizing
and works for any initial variable values, as we will show in our proofs.
SELECT distinguishes between two node roles: follower and leader. We use sv to indicate the role of the node: sv = 1 means that node v is a leader, whereas sv = 0 means
v is a follower. The basic idea of our protocol is to divide time into intervals of a small
number of rounds specified by the constant parameter b > 5 (we use the variable mc
as a modulo counter); in the following, we will refer to a sequence of rounds between
two consecutive mc = 0 events as a b-interval. (Of course, it can happen that all b slots
of an interval are jammed.)
Our protocol is based on the concept of so-called leader slots, special rounds—in each
b-interval through which SELECT cycles—in which leaders are obliged to send an alive
message (a so-called leader message) and in which followers keep silent. The idea is
that the followers learn that the leader has left in case of an idling medium during a
leader slot (of course, the leader slots may be jammed!) and a new election is triggered
automatically.
SELECT uses four leader slots:2 ls1 , ls2 , ls3 , and ls4 . Of course, in the beginning, all
nodes may have different ls values and may disagree on which slots during the binterval are leader slots. However, over time, the nodes synchronize their states and a
consistent view emerges. For the synchronization, five temporary variables ls0 , ls1 , ls2 ,
ls3 , and ls4 are used, which store future ls values.
2 It

is an open question whether a protocol with less leader slots can be devised.

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:17

Fig. 1. Algorithm for followers (left) and leaders (right).

Depending on whether the node is of type follower or leader, the leader slots are
updated differently: At the beginning of a new b-interval, a leader copies its lsi values to the lsi values. A follower, on the other hand, copies the ls values “diagonally”

in the sense that lsi is copied to lsi+1
for i ∈ {0, 1, 2, 3}. As we will see, this mechanism ensures that an elected leader covers the leader slot ls3 of each follower. (SELECT guarantees that the adaptive adversary has no knowledge about the ls3 slots
at all until it is already too late to prevent a successful election.) Another special
slot besides ls3 is ls0 , which is a random seed to mix the execution for increased
robustness.
In Figure 1, we give the detailed formal description of the follower and the leader
protocol, respectively. Recall that our algorithms can tolerate any initial values of mc,
pv , Tv , cv , sv , sv , ls1 , ls2 , ls3 , ls4 , ls0 , ls1 , ls2 , ls3 , and ls4 . For instance, in the beginning,
all nodes v may be leaders and for all v, sv = 1. However, the fixed parameters used by
the algorithms, namely p̂, γ , or b, are assumed to be immutable.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:18

B. Awerbuch et al.

Each node v executes either the follower or the leader algorithm, depending on the
value sv . Both the follower and the leader algorithm consist of three main parts. The
b-interval wise update (Lines 2–4) makes sure that ls values are refreshed frequently.
(In the case of the follower algorithm (Figure 1, left), if a node v sets sv to 1 in Line 4, the
former “follower” v will immediately start running the leader algorithm in Figure 1.)
Lines 6–33 (in case of a follower) and Lines 5–24 (in case of a leader) are used for
medium access to synchronize the nodes’ states (by a message that includes cv , Tv , and
pv values). This gives nodes the chance to become or remain leader (by a “LEADER”
message).
The last sections of the algorithms are used to react to high interference (by reducing
pv ) and to reset leader slots. The reason for checking whether ls3 is undefined in Line 6
of the follower protocol is to keep the leader slots hidden from the adaptive adversary
until it is already too late to prevent a successful leader election.3
Both the follower and the leader protocol depend on the following crucial CONDITION.
Definition 3.1 (CONDITION). We define CONDITION (Line 37 for followers, and Line 28
for leaders) as the event that at least one LEADER message was received during the
past b · Tv steps.
The idea is that if CONDITION is fulfilled, we know that the protocol is already in a good
state. Moreover, we will see that the adversary cannot prevent CONDITION to become
true for a long time, as the Tv values would continue to increase.
Finally, also note that leaders increase pv faster (i.e., by larger multiplicative factors)
during idle rounds than followers. With this mechanism, SELECT improves the likelihood
that a LEADER message gets through and hence that a unique leader is elected.
3.2. Analysis

This section shows that the randomized SELECT protocol is guaranteed to eventually
reach a situation where there is exactly one leader and n − 1 followers. Concretely, we
will derive the following theorem.
THEOREM 3.2. Given an arbitrary initial configuration and in the absence of state
faults, our leader election protocol reaches a state where there is exactly one leader and
n − 1 followers, despite an adaptive (T , 1 − )-bounded jammer, for any T and any
constant  > 0.
We make use of the following definitions. First, we define the system state.
Definition 3.3 (State and System State). The state of node v is determined by the
state of the variables pv , Tv , cv , sv , sv , mc, ls0 , ls1 , ls1 , ls2 , ls2 , ls3 , ls3 , ls4 , and ls4 . The
state of the system is the set of the states of all nodes.
We use the following LSL set to describe the union of all possible leader slot values
present in the system.
Definition 3.4 (The LSL State Set). For any given system state, let LSL = {ls1 (v),
ls2 (v), ls3 (v), ls4 (v) |v is leader}\{undefined}.
The system can be in several special states which are formalized next: follower states,
preleader states, and leader states. Let [b] = {0, . . . , b − 1}.
Definition 3.5 (Follower State). A state S is called a follower state, denoted by
S ∈ FOLLOWER, if all the following conditions hold. (i) All nodes are followers (∀v ∈
V : sv = 0); (ii) for every node v: ls1 (v), ls2 (v), ls3 (v), ls4 (v) ∈ [b] ∪{undefined}, ls1 (v),
3 This

check allows the adversary to be even reactive.

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:19

ls2 (v), ls3 (v), ls4 (v) ∈ [b] ∪ {undefined}, ls0 (v) ∈ [b] (but not undefined); (iii) the follower
nodes can be partitioned into two sets {v} and V \{v}, according to their ls values
(v is the node that successfully sent the last follower message); for each w ∈ V \{v}:
ls1 (w) = ls0 (v), ls2 (w) = ls1 (v), ls3 (w) = ls2 (v), ls4 (w) = ls3 (v), and ls2 (w) = ls1 (v),
ls3 (w) = ls2 (v), and ls4 (w) = ls3 (v); (iv) for any pair of follower nodes v, w ∈ V with
ls2 (v) ∈ [b] and ls3 (v) ∈ [b], cv = cw and Tv = Tw .
We use the concept of so-called preleader states, that is, states that result from
follower states before some nodes become leaders.
Definition 3.6 (Preleader State). A state S is called a preleader state, denoted by S ∈
PRELEADER, if it is a follower state, and at least one follower node v has sv = 1.
While in the beginning, the leader sets may be large as each node regards different
slots during the b-interval as the “leader slots” over time the values synchronize and
the LS sets become smaller. This facilitates a fast leader (re-)election.
Definition 3.7 (Leader State). A state S is called a leader state, denoted by S ∈
LEADER, if all the following conditions are satisfied:
(i) There is at least one leader, that is, |{v|v ∈ V : sv = 1}| ≥ 1; (ii) for every node v,
ls1 (v), ls2 (v), ls3 (v), ls4 (v) ∈ [b]∪{undefined}, ls1 (v), ls2 (v), ls3 (v), ls4 (v) ∈ [b]∪{undefined},
ls0 (v) ∈ [b]; (iii) let v be any follower and let w be any follower or leader, then
ls3 (v) ∈ {ls1 (w), ls2 (w), ls3 (w), ls4 (w)}∪{undefined}, ls2 (v) ∈ {ls0 (w), ls1 (w), ls2 (w), ls3 (w)}∪
{undefined}; (iv) |LSL| ≤ 5; (v) for every follower w with ls3 (w) ∈ [b] or ls2 (w) ∈ [b],
cw = cv and Tw = Tv for any leader v.
So in a leader state, it holds that any follower’s ls3 and ls2 slots are covered by either
another follower’s ls and ls slots, or a leader’s ls and ls slots (cf. Condition (iii)).
Finally, it is useful to define safe and legal states.
Definition 3.8 (Safe and Legal State). A system state S is called safe (denoted by
S ∈ SAFE) if S ∈ FOLLOWER or S ∈ LEADER, and legal (denoted by S ∈ LEGAL) if
S is safe and there is exactly one node v with sv = 1.
Thus, according to our definitions, any legal state is also a safe state. In the following,
let S be the set of all possible system states, SAFE ⊂ S be the set of all safe system
states and LEGAL ⊂ SAFE be the set of all legal system states.
The proof of Theorem 3.2 unfolds in a number of lemmas. An interesting property
of our randomized algorithm is that it is guaranteed to be correct, in the sense that
deterministically exactly one leader is elected; only the runtime is probabilistic (i.e.,
depends on the random choices made by SELECT).
First, we study leader messages.
LEMMA 3.9. For any network state, it holds that if a leader successfully transmits a
“LEADER” message, the system will immediately enter a legal state.
PROOF. When a node (either follower or leader) receives a LEADER message, it sets
ls3 and ls2 to undefined (Lines 22–25 in Figure 1, left; after Lines 15–17 of Figure 1,
right), and considers itself a follower. Thus, in the new state, there is exactly one leader
(the sender of the LEADER message) and n− 1 followers. The state is also a safe state,
namely a leader state: Conditions (i) and (ii) are fulfilled trivially. Condition (iv) also
holds, as there is only one leader that has four slots. Condition (iii) is fulfilled because
nodes receiving a LEADER message reset their slots ls3 and ls2 ; since ls3 and ls2 are
undefined for a follower, Condition (v) also holds.
We next consider what happens if nodes hear a message sent by a follower.
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:20

B. Awerbuch et al.

LEMMA 3.10. For any network state, it holds that when a follower successfully transmits a message, the system is guaranteed to enter a safe state at the beginning of the
next b-interval.
PROOF. First, note that if a leader message gets through before the next b-interval,
the claim holds trivially due to Lemma 3.9.
Otherwise, we distinguish two cases: (A) For every node v, sv = 0 (not preleader) and
sv = 0 (not leader) by the end of current b-interval. (B) There is at least one node v with
either sv = 1 (preleader) or sv = 1 (leader) by the end of current b-interval.
In Case (A), after the follower message has been successfully sent, there are still n
followers and no leaders or preleaders. We will show that the system enters the follower
state at the beginning of the next b-interval. Let us refer to the follower node that sent
the message by v and to any remaining node by w. When w receives the message from
v (Lines 26–32 in Figure 1, left), it sets ls1 (w) := ls0 (v), ls2 (w) := ls1 (v), ls3 (w) := ls2 (v),
and ls4 (w) := ls3 (v). The c values become the same (cw = cv ), and Tw := Tv . The new
state, therefore, fulfills the follower state conditions: Clearly, Conditions (i), (ii), and
(iv) are fulfilled immediately, and Condition (iii) holds as well, as for all followers w
that did not send a message and follower v that sent a message, at the beginning of
the next b-interval: ls3 (w) = ls2 (w) = ls1 (v) = ls2 (v), ls3 (v) = ls2 (v) = ls3 (w) = ls4 (w), and
ls1 (v) = ls2 (w) = ls0 (v) = ls1 (w).
For Case (B), observe that during the remainder of the b-interval the number of
preleader nodes with sv = 1 cannot decrease, and hence there will be at least one leader
at the beginning of the next b-interval. We now show that the new state will indeed
be a leader state as nodes “synchronize” with the follower node that sent the message.
Without loss of generality, assume that node u is the last follower that successfully sent
a follower message in the current b-interval. Let us refer to the other follower nodes
by v1 and to the leader nodes or the preleader nodes (i.e., the followers v with sv = 1)
by v2 . Again, Conditions (i) and (ii) are fulfilled trivially. As for Condition (iii), we need
to consider two subcases:
Case 1. No node experienced an idle channel in its ls3 slot after the message
has been successfully sent. If this is the case and follower u is not a preleader, it
holds that for follower v1 : ls2 (v1 ) = ls2 (v2 ) = ls1 (u) in the current b-interval, and
ls3 (v1 ) = ls2 (v2 ) = ls2 (u) at the beginning of the next b-interval; on the other hand,
if follower u is a preleader, then in the current b-interval it holds that for follower
v1 : ls2 (v1 ) = ls2 (v2 ) = ls1 (u), and ls3 (v1 ) = ls2 (v2 ) = ls1 (u) at the beginning of the next
b-interval. Hence, Condition (iii) holds. Regarding the cardinality of the leader set
LSL, observe that at the beginning of the next b-interval, if u is not a preleader,
all leaders will have ls1 = ls0 (u), ls2 = ls1 (u), ls3 = ls2 (u), ls4 = ls3 (u), and hence
LSL = {ls0 (u), ls1 (u), ls2 (u), ls3 (u)}; therefore, |LSL| ≤ 5. Otherwise, if u is a preleader,
then LSL = {ls0 (u), ls1 (u), ls2 (u), ls3 (u), ls4 (u)}; therefore, |LSL| ≤ 5.
Case 2. One or more nodes experienced an idle channel in their ls3 slots after the message has been successfully sent. In the following, we prove this case correct assuming
that u is a follower and not a preleader. If u is a preleader, the proof is analogous.
(1) If v1 experienced the idle channel at its ls3 timeslot, and became a preleader:
Note that a node v1 may experience an idle channel after receiving the message from u and hence become a preleader; however, Condition (iii) is still
satisfied, as it holds that for follower u: ls2 (u) = ls3 (v2 ) = ls3 (v1 ) in the current
b-interval and ls3 (u) = ls3 (v2 ) = ls3 (v1 ) at the beginning of the next b-interval. As
for the cardinality of the leader set LSL, observe that at the beginning of the next
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:21

b-interval, all leaders will have ls1 = ls0 (u), ls2 = ls1 (u), ls3 = ls2 (u), ls4 = ls3 (u), and
hence LSL = {ls0 (u), ls1 (u), ls2 (u), ls3 (u)}; therefore, |LSL| ≤ 5.
(2) If u experienced the idle channel at its ls3 timeslot and became a preleader: If node
u experienced an idle channel after successfully sending the message, u became a
preleader, and we have for a follower v1 , ls2 (v1 ) = ls2 (v2 ) = ls1 (u) in the current binterval and ls3 (v1 ) = ls2 (v2 ) = ls1 (u) at the beginning of the next b-interval. Hence,
Condition (iii) is satisfied. As for |LSL|, observe that at the beginning of the next
b-interval, for a leader v2 , ls1 = ls0 (u), ls2 = ls1 (u), ls3 = ls2 (u), ls4 = ls3 (u), while for
the remaining leader u, it holds that ls1 = ls1 (u), ls2 = ls2 (u), ls3 = ls3 (u), ls4 = ls4 (u).
Hence, also in this case, we have that |LSL| ≤ 5.
Finally, Condition (v) is true for both of the subcases, because the cv and Tv values
are “synchronized” when the follower message is received (Lines 27 and 29 in Figure 1,
left; Line 19 in Figure 1, right).
An important property of SELECT is that once it is in a safe state, it will remain so
in future (given that there are no external changes). Similar properties can be derived
for other states, as we will see.
LEMMA 3.11. Once the system is in a safe state, it will remain in a safe state in the
future.
PROOF. We study what can happen in one round, and show that in each case, the
safety properties are maintained. In a round, (A) either a LEADER message is successfully sent, (B) a follower message is successfully sent, (C) there are collisions or the
channel is jammed, or (D) there is an idle channel.
In Case (A), the claim directly follows from Lemma 3.9 and from the fact that safe
states are a superset of the legal states (SAFE ⊃ LEGAL). In Case (B), the claim
follows from Lemma 3.10 and by the fact that the system is in the safe state already.
In Case (C), if the channel is blocked, follower nodes (even those which sent a message in this round) do not change their state except for the synchronized rounds in
Lines 35–43, and similarly for the leaders in Lines 26–34. Our protocols guarantee
that the leaders have the same cv and Tv values as the followers when ls3 and ls2 are
valid, and since the leaders experience the same number of successful transmissions
and idle timesteps as the followers do (single-hop network), the claim follows.
If there is an idle channel (Case (D)), all nodes v for which ls3 (v) = mc will set
sv = 1 in the current b-interval, while other values remain the same. It is clear that
from this point on, until the end of the current b-interval, the claim holds. Moreover,
as we show next, the claim is still true at the beginning of next b-interval. If ls3 (v)
is undefined, then the claim holds trivially, as no states will change in this case. If
ls3 (v) = mc for any node v and the nodes experience an idle channel, there is no leader
because, if there was a leader, according to Condition (iii) of the leader state definition
(Definition 3.7), a follower’s ls3 slot would always be covered by a leader slot of a leader,
which yields the contradiction. Hence, the current safe state must be a preleader
state. Let v denote the followers that have sv = 0 (i.e., they are not preleaders); let
u denote the followers with su = 1 (preleaders). In the current b-interval, we have
ls2 (v) ∈ {ls0 (u), ls1 (u), ls2 (u), ls3 (u)} ∪ {undefined}, which is true according to Condition
(iii) of the follower state definition (Definition 3.5). Then, at the beginning of next
b-interval, u will become a leader, and hence we have ls3 (v) = ls2 (v), ls1 (u) = ls1 (u),
ls2 (u) = ls2 (u), and ls3 (u) = ls3 (u). This implies that ls3 (v) ∈ {ls0 (u), ls1 (u), ls2 (u), ls3 (u)},
which satisfies Condition (iii) of the leader state Definition 3.7. Conditions (i) and (ii) are
clearly satisfied. Condition (iv) holds simply because we have shown (in Lemma 3.10,
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:22

B. Awerbuch et al.

Case (B)), when there is an idle timestep, |LSL| ≤ 5. Condition (v) is true because we
always synchronize the cv and Tv values.
LEMMA 3.12. Once a system is in a leader state, it will remain in a leader state in the
future.
PROOF. Lemma 3.11 tells us that the system will never leave a safe state. Therefore,
it remains to prove that there will always be at least one node v with sv = 1. This
clearly holds as the only way a leader can become a follower again is by receiving a
LEADER message (see Lines 15–17), which of course implies that another leader is still
active and remains to be a leader. Also, because we are in a leader state, Condition (v)
holds and it further implies that leaders will never invalidate their ls slots before the
followers. This guarantees that the protocol will never get out of a leader state.
LEMMA 3.13. Once a system is in a legal state, it will remain in a legal state in the
future.
PROOF. By Lemma 3.11, we know that our system will never leave a safe state again,
and hence, we only need to prove that there will always be exactly one node v with
sv = 1. This is true because in the safe state, a follower node w can never become a
leader, as its ls3 (w) slot is covered by the leader v: ls3 (w) ∈ {ls1 (v), ls2 (v), ls3 (v), ls4 (v)} and
ls2 (w) ∈ {ls0 (v), ls1 (v), ls2 (v), ls3 (v)} (Condition (iii) of leader state). Because a follower
will never send a LEADER message, v will remain a leader forever, which proves the
claim.
Regarding convergence, note that the system quickly enters a safe state,
deterministically.
LEMMA 3.14. For any initial system state with T̂ = maxv Tv , it takes at most b · T̂
rounds until the system is in a safe state.
PROOF. We distinguish three cases: If a leader message gets through sometimes in
these rounds, then the claim holds by Lemma 3.9; if a follower message gets through,
then the claim holds by Lemma 3.10. If within maxv Tv b rounds neither a follower
message nor a leader message gets through, all nodes will have to reset their ls slots
(because CONDITION in Line 37 (Figure 1, left), respectively, Line 28 (Figure 1, right)
is not met). This, however, constitutes the safe state (all conditions fulfilled trivially),
which is maintained according to Lemma 3.11.
Armed with these results, we can prove convergence.
LEMMA 3.15. For any safe state, SELECT will eventually reach a legal state.
PROOF. We divide the proof in two phases: the phase where the protocol transitions
to the leader state from the follower state and the phase where it transitions to the
legal state from the leader state.
(1) Follower state to leader state
If CONDITION is fulfilled, we know that a LEADER message got through and the
system is in a legal state (and hence also in a leader state). As long as CONDITION is
not fulfilled, Tv is increasing for each node v. So eventually, T̂ = maxv Tv ≥ 2T /b.
We can also provide a lower bound on the cumulative probability p. Without loss
of generality, suppose that T ≥ (3/) log1+γ n (a smaller T will only make the
jammer less flexible and weaker). Suppose that p is at most /4 throughout some
T -interval I. Then it follows from the standard Chernoff bounds that there are at
most T /3 busy steps in I w.h.p. If this is true, then no matter how the adversary
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:23

jams during I, at least (1 − /3)T − (1 − )T = 2T /3 nonjammed steps will be idle,
which implies that the cumulative probability at the end of I will be by a factor of at
least (1 + γ )T /3 ≥ n3 higher than at the beginning of I. Using this insight, it follows
that eventually a T -interval is reached with p > /4. Once such a T -interval has
been reached, it is easy to show that p will not get below 1/n2 any more w.h.p.
so that for every T -interval afterward, there is a time point t with p > /4 w.h.p.
So infinitely often the following event can take place with some lower-bounded,
positive probability:
Consider two consecutive T -intervals I1 and I2 starting at a time when cv = 0 for
every node v. Suppose that I1 just consists of busy steps and I2 just consists of idle
timesteps. Then the adversary has to leave T busy timesteps in I1 nonjammed
and T idle timesteps in I2 nonjammed. For I1 , there is a positive probability in this
case that exactly three messages from different nodes are successfully sent in three
different b-intervals. In this case, all but one follower respect the leader slots (as
their ls3 -value is defined), whereas the follower that sent the last successful message may still send out messages at all timesteps (as its ls3 -value is still undefined,
see Line 6 of the follower protocol). Thus, it is indeed possible that all timesteps
in I1 are busy. Up to that point, the adversary has not learned anything about the
leader slots. In I2 , there is also a positive probability that none of the followers
transmits a message throughout I2 so that all timesteps are idle. As the adversary
does not know which of them is a leader slot and has to leave T nonjammed, there
is a positive probability that ls3 is nonjammed, and some of the followers become
preleaders and then leaders.
Thus, the expected time to get from a follower to a leader state is finite.
(2) Leader state to legal state
If there is only one leader in the leader state, the system is already in a legal
state by definition. If there is more than one leader, then we distinguish between
the following cases. If CONDITION is fulfilled, we know that a LEADER message got
through and the system is in a legal state. Otherwise, the leaders will invalidate
all of their ls slots once their cv values are reset to 0. At this point, there is a
positive probability that for the next T steps a LEADER message is successfully
sent. As the adversary has to leave T timesteps nonjammed, at least one LEADER
message will be successfully transmitted within these T steps so that the system
reaches a legal state.
Analogous to the followers in the previous case, one can lower bound the cumulative probability of the leaders (in fact, the leaders will eventually reach a timepoint
with a cumulative probability of () as they increase their probabilities in case
of an idle channel more aggressively than the followers) so that the chance of successfully transmitting a LEADER message repeats itself infinitely often with a
lower-bounded positive probability. Thus, the expected time to get from a leader to
a legal state is finite as well.
From these cases, the lemma follows.
4. CONCLUSION

This article presented the first medium access scheme robust to a wide range of interference types which even include adaptive jamming, together with a rigorous analysis
proving an asymptotically optimal, constant competitive throughput. We regard this
result as an important step toward a better understanding of more complex protocol
or physical models for signal propagation. Moreover, as we have shown for the case of
leader election, such a protocol can also serve as a basis for robust applications. Another important direction for future research regards the study of dynamical aspects
ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:24

B. Awerbuch et al.

(e.g., How can a MAC algorithm adapt to join and leave behavior or mobility of the
nodes, and which rate is sustainable without losing a constant competitiveness?)
REFERENCES
G. Alnifie and R. Simon. 2007. A multi-channel defense against jamming attacks in wireless sensor networks.
In Proceedings of Q2SWinet. 95–104.
G. Antonoiu, G. Antonoiu, P. K. Srimani, and P. K. Srimani. 1996. A self-stabilizing leader election algorithm
for tree graphs. Journal on Parallel and Distributed Computing 34 (1996), 227–232.
H. Attiya and J. Welch. 2004. Distributed Computing: Fundamentals, Simulations and Advanced Topics.
John Wiley & Sons, Chapter 3.
B. Awerbuch. 1987. Optimal distributed algorithms for minimum weight spanning tree, counting, leader
election, and related problems. In Proceedings of the Annual ACM Symposium on Theory of Computing
(STOC’87).
B. Awerbuch, A. Richa, and C. Scheideler. 2008. A jamming-resistant MAC protocol for single-hop wireless
networks. In Proceedings of the ACM Symposium on Principles of Distributed Computing (PODC’08).
E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and B. Thapa. 2008. On the performance
of IEEE 802.11 under jamming. In Proceedings of the IEEE International Conference on Computer
Communications (INFOCOM’08). 1265–1273.
M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E. Leiserson. 2005. Adversarial contention
resolution for simple channels. In Proceedings of the Annual ACM Symposium on Parallel Algorithms
and Architectures (SPAA’05).
T. Brown, J. James, and A. Sethi. 2006. Jamming and sensing of encrypted wireless ad hoc networks.
In Proceedings of the ACM International Symposium on Mobile Ad Hoc Networking and Computing
(MOBIHOC’06). 120–130.
S. Cai, T. Izumi, and K. Wada. 2009. Space complexity of self-stabilizing leader election in passively-mobile
anonymous agents. In Proceedings of the 16th International Colloquium on Structural Information and
Communication Complexity (SIROCCO’09).
J. T. Chiang and Y.-C. Hu. 2007. Cross-layer jamming detection and mitigation in wireless broadcast networks. In Proceedings of the Annual International Conference on Mobile Computing and Networking
(MOBICOM’07). 346–349.
B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki. 2006. Adversarial queuing on the multiple-access channel.
In Proceedings of the Annual ACM Symposium on Principles of Distributed Computing (PODC’06).
A. Czumaj and W. Rytter. 2006. Broadcasting algorithms in radio networks with unknown topology. Journal
of Algorithms 60, 2 (2006), 115–143.
E. W. Dijkstra. 1974. Self-Stabilization in spite of distributed control. Communications of ACM 17, 11 (1974),
643–644.
S. Dolev, S. Gilbert, R. Guerraoui, F. Kuhn, and C. C. Newport. 2009. The wireless synchronization problem.
In Proceedings of the 28th Annual ACM Symposium on Principles of Distributed Computing (PODC’09).
190–199.
S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. 2007a. Gossiping in a multi-channel radio network: An
oblivious approach to coping with malicious interference. In Proceedings of the Symposium on Distributed
Computing (DISC’07).
S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. 2008. Secure communication over radio channels. In
Proceedings of the 27th ACM Symposium on Principles of Distributed Computing (PODC’08). 105–114.
S. Dolev, S. Gilbert, R. Guerraoui, and C. C. Newport. 2007b. Gossiping in a multi-channel radio network. In
Proceedings of the 21st International Symposium on Distributed Computing (DISC’07). 208–222.
R. G. Gallager, P. A. Humblet, and P. M. Spira. 1983. A distributed algorithm for minimum-weight spanning
trees. ACM Transactions on Programming and Language Systems 5, 1 (1983), 66–77.
S. Ghosh and A. Gupta. 1996. An exercise in fault-containment: self-stabilizing leader election. Information
Processing Letters 59, 5 (1996), 281–288. DOI:http://dx.doi.org/10.1016/0020-0190(96)00121-4
S. Gilbert, R. Guerraoui, D. Kowalski, and C. Newport. 2009a. Interference-resilient information exchange.
In Proceedings of the IEEE International Conference on Computer Communications (INFOCOM’09).
S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport. 2009b. Interference-Resilient information
exchange. In Proceedings of the 28th IEEE International Conference on Computer Communications
(INFOCOM’09). 2249–2257.
S. Gilbert, R. Guerraoui, and C. Newport. 2006. Of malicious motes and suspicious sensors: On the efficiency
of malicious interference in wireless networks. In Proceedings of OPODIS.

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Robust MAC and Leader Election

24:25

L. A. Goldberg, P. D. Mackenzie, M. Paterson, and A. Srinivasan. 2000. Contention resolution with constant
expected delay. Journal of the ACM 47, 6 (2000).
J. Hastad, T. Leighton, and B. Rogoff. 1996. Analysis of backoff protocols for mulitiple access channels. SIAM
Journal on Computing 25, 4 (1996).
J. Hromkovic, R. Klasing, A. Pelc, P. Ruzicka, and W. Unger. 2005. Dissemination of Information in Communication Networks: Broadcasting, Gossiping, Leader Election, and Fault-Tolerance (Chapter 8). SpringerVerlag, New York, Chapter 8.
G. Itkis, C. Lin, and J. Simon. 1995. Deterministic, constant space, self-stabilizing leader election on uniform
rings. In Proceedings of the 9th International Workshop on Distributed Algorithms (WDAG’95). 288–
302.
C. Y. Koo, V. Bhandari, J. Katz, and N. H. Vaidya. 2006. Reliable broadcast in radio networks: The bounded
collision case. In Proceedings of the Annual ACM Symposium on Principles of Distributed Computing
(PODC’06).
E. Korach, S. Kutten, and S. Moran. 1990. A modular technique for the design of efficient distributed leader
finding algorithms. ACM Transactions on Programming Languages and Systems 12, 1 (1990), 84–101.
F. Kuhn, T. Moscibroda, and R. Wattenhofer. 2004. Radio network clustering from scratch. In Proceedings of
ESA.
S. Kutten and B. Patt-Shamir. 1997. Time-adaptive self stabilization. In Proceedings of the Annual ACM
Symposium on Principles of Distributed Computing (PODC’97).
B.-J. Kwak, N.-O. Song, and L. E. Miller. 2005. Performance analysis of exponential backoff. IEEE/ACM
Transactions on Networking 13, 2 (2005), 343–355.
Y. W. Law, L. van Hoesel, J. Doumen, P. Hartel, and P. Havinga. 2005. Energy-efficient link-layer jamming
attacks against wireless sensor network MAC protocols. In Proceedings of SASN. 76–88.
S. Lee, D. Levin, V. Gopalakrishnan, and B. Bhattacharjee. 2007. Backbone construction in selfish wireless networks. In Proceedings of the ACM International Conference on Measurement and Modeling of
Computer Systems (SIGMETRICS’07).
M. Li, I. Koutsopoulos, and R. Poovendran. 2007. Optimal jamming attacks and network defense policies in
wireless sensor networks. In Proceedings of the IEEE International Conference on Computer Communications (INFOCOM’07). 1307–1315.
X. Liu, G. Noubir, R. Sundaram, and S. Tan. 2007. SPREAD: Foiling smart jammers using multi-layer agility.
In Proceedings of the IEEE International Conference on Computer Communications (INFOCOM’07).
2536–2540.
D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer. 2009. Speed dating despite jammers. In Proceedings
of DCOSS.
K. Nakano and S. Olariu. 2000. Randomized leader election protocols in radio networks with no collision detection. In Proceedings of the 11th International Conference on Algorithms and Computation (ISAAC’00).
Springer-Verlag, 362–373.
V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein. 2007. Using channel hopping to increase 802.11 resilience
to jamming attacks. In Proceedings of the IEEE International Conference on Computer Communications
(INFOCOM’07). 2526–2530.
R. Negi and A. Perrig. 2003. Jamming Analysis of MAC Protocols. Technical Report. Carnegie Mellon
University.
K. Nikano and S. Olariu. 2002. Uniform leader election protocols for radio networks. IEEE Transactions on Parallel and Distributed Systems 13, 5 (2002), 516–526. DOI:http://dx.doi.org/10.1109/TPDS.
2002.1003864
A. Pelc and D. Peleg. 2005. Feasibility and complexity of broadcasting with random transmission failures. In
Proceedings of the Annual ACM Symposium on Principles of Distributed Computing (PODC’05).
P. Raghavan and Eli Upfal. 1999. Stochastic contention resolution with short delays. SIAM Journal on
Computing 28, 2 (1999), 709–719.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang. 2010. A jamming-resistant MAC protocol for multi-hop wireless networks. In Proceedings of the 24th International Symposium on Distributed Computing (DISC’10).
179–193.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang. 2011. Competitive and fair medium access despite reactive jamming. In Proceedings of the 31st International Conference on Distributed Computing Systems
(ICDCS).
A. Richa, C. Scheideler, S. Schmid, and J. Zhang. 2012. Competitive and fair throughput for co-existing networks under adversarial interference. In Proceedings of the 31st Annual ACM Symposium on Principles
of Distributed Computing (PODC’12).

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

24:26

B. Awerbuch et al.

J. Schmidt, A. Siegel, and A. Srinivasan. 1995. Chernoff-Hoeffding bounds for applications with limited
independence. SIAM Journal on Discrete Mathematics 8, 2 (1995), 223–250.
M. K. Simon, J. K. Omura, R. A. Schultz, and B. K. Levin. 2001. Spread Spectrum Communications Handbook.
McGraw-Hill.
G. Smaragdakis, I. Matta, and A. Bestavros. 2004. SEP: A stable election protocol for clustered heterogeneous
wireless sensor networks. In Proceedings of the 2nd International Workshop on Sensor and Actor Network
Protocols and Applications (SANPA’04).
D. Thuente and M. Acharya. 2006. Intelligent jamming in wireless networks with applications to 802.11b
and other networks. In Proceedings of MILCOM.
D. E. Willard. 1986. Log-logarithmic selection resolution protocols in a multiple access channel. SIAM
Journal on Computing 15, 2 (1986), 468–477. DOI:http://dx.doi.org/10.1137/0215032
A. D. Wood, J. A. Stankovic, and G. Zhou. 2007. DEEJAM: Defeating energy-efficient jamming in IEEE
802.15.4-based wireless networks. In Proceedings of SECON.
W. Xu, K. Ma, W. Trappe, and Y. Zhang. 2006. Jamming sensor networks: Attack and defense strategies.
IEEE Network 20, 3 (2006), 41–47.
W. Xu, W. Trappe, Y. Zhang, and T. Wood. 2005. The feasibility of launching and detecting jamming attacks in
wireless networks. In Proceedings of the ACM International Symposium on Mobile Ad Hoc Networking
and Computing (MOBIHOC’05). 46–57.
W. Xu, T. Wood, and Y. Zhang. 2004. Channel surfing and spatial retreats: Defenses against wireless denial
of service. In Proceedings of the Workshop on Wireless Security.
Received November 2011; revised April 2013; accepted May 2013

ACM Transactions on Algorithms, Vol. 10, No. 4, Article 24, Publication date: July 2014.

Brief Announcement: Randomized Compact Routing
in Decomposable Metrics
Goran Konjevod

Andréa W. Richa

∗

Lawrence Livermore National
Laboratory, CA, USA

Arizona State University
Tempe, AZ, USA

konjevod1@llnl.gov

aricha@asu.edu
Ling Zhou

Donglin Xia
Microsoft
Redmond, WA, USA

doxia@microsoft.com

Microsoft
Redmond, WA, USA

lizho@microsoft.com
ABSTRACT

metric (X, d) is α-decomposable, if for all r > 0 there exists
an r-bounded 1/α-padded decomposition.
A routing scheme is a distributed algorithm that deliver
packets from any source to any destination node. Making
the routing storage requirment scalable with the network
size while minimizing the routing stretch is one of the fundamental trade-oﬀs for routing scheme design, where the
stretch is the maximum ratio of the routing path length to
the shortest path length over all source destination pairs. We
say a routing scheme is compact if its storage requirement
at each node and for each packet header is polylogarithmic
on the number of nodes. There are two variants of routing
scheme design: (i) name-dependent (or labeled) routing and
(ii) name-independent routing. Labeled routing allows the
scheme designer to label the nodes with additional routing
information. In name-independent routing, the scheme must
use solely the (arbitrary) original naming.
In this paper, we present both labeled (name-dependent)
and name-independent compact routing with polylogarithmic storage and constant stretch for networks with bounded
decomposable metrics. Compact routing has been studied
in various networks, such as growth-bounded networks [3],
power-law networks [4], and doubling metrics [1, 9, 10]. It
is known [7, 11] that a bound on the doubling dimension
implies a bound on the padded decomposability. In [8, 12,
5], it is shown that every metric space X induced by an
edge-weighted graph excluding Kr,r as a minor is O(r2 )decomposable. To the best of our knowledge, our work is
the to provide randomized schemes for compact routing with
constant stretch in networks as general as bounded decomposable metrics.

We study the compact routing problem in networks whose
shortest path metrics are decomposable. Decomposable metrics are more general than doubling metrics, growth-bounded
metrics, and metrics induced by graphs excluding Kr,r as a
minor. In this work, we present both name-dependent and
name-independent constant stretch compact routing schemes
for bounded decomposable metrics with polylogarithmic storage requirements at each node and polylogarithmic packet
headers. Our work is the ﬁrst to design compact routing
schemes with constant stretch for networks as general as
decomposable metrics.

Categories and Subject Descriptors
C.2.2 [Computer Communication Networks]: Network
Protocols - Routing protocols; E.1 [Data Structures]: Distributed data structures; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms
and Problems - Computations on discrete structures, Routing and Layout; G.2.2 [Discrete Mathematics]: Graph
Theory - Graph algorithms, Graph labeling, Network problems

General Terms
Algorithms, Design

1. INTRODUCTION
Decomposable metrics have been used as a fundamental
tool in the design of randomized algorithms on metric spaces
in recent years. An r-bounded δ-padded decomposition of a
metric (X, d) is a distribution over X’s partitions such that
(i) for each random-generated partition, all of its clusters
have radius no more than r; and (ii) the probability that
the distance from any node x ∈ X to the “boundary” of the
partition is no less than δr is no less than 1/2. We say a

2. RANDOMIZED COMPACT ROUTING
SCHEMES
Given an edge-weighted n-node Δ-diameter graph G =
(V, E) whose shortest path metric is α-decomposable, we
present (i) a 4α-stretch labeled routing scheme w.h.p.1 with
O(log Δ log3 n/ log log n)-bit routing labels, O(log2 n/ log log n)bit packet headers, and O(log Δ log3 n/ log log n)-bit routing
tables of size; and (ii) a 22α-stretch name-independent rout-

∗This work was supported in part by NSF awards CCF0830791 and CCF-0830704.
Copyright is held by the author/owner(s).
PODC’11, June 6–8, 2011, San Jose, California, USA.
ACM 978-1-4503-0719-2/11/06.

1
With high probability, i.e., with probability at least 1 −
1/nc , where c > 0 is a constant.

351

ing scheme w.h.p. with O(log Δ log5 n/(loglog n)2 )-bit routing labels, and o(log2 n))-bit packet headers.
Since G is α-decomposable, there are a bundle of 2i -bounded
1/α-padded decompositions, ∀i ∈ [log(αΔ)] 2 . For each
i ∈ [log αΔ], we randomly pick up log n partitions from
the 2i -bounded 1/α-padded decomposition. The key observation is that, for any source-destination pair (u, v) with
d(u, v) ≤ 2i /α, the probability that u and v are contained
in the same cluster for one of the log n random 2i -bounded
partitions is no less than 1 − (1/2)log n = 1 − 1/n. Thus with
the help of local compact routing schemes at each cluster at
each level, searching for the destination from lower to higher
levels i ∈ [log αΔ] could result in constant stretch (depending on α) with high probability. In the following, we build
upon this idea in our labeled and name-independent routing
schemes.

2.1 Labeled Routing Scheme
Each cluster of all the log n random 2i -bounded partitions,
for each i ∈ [log αΔ], maintains a labeled routing scheme
as in the following lemma on the shortest path tree of the
cluster with its center as the root.
Lemma 2.1 ([6]) For every weighted tree T on n nodes,
there exists a labeled routing scheme that, given any destination label, routes optimally on T from any source to the
destination. The storage per node, the label size, and header
size are O(log2 n/ log log n) bits.

Stretch Analysis.
Note that for each iteration i, the cost of an error report
is d(u, c) + 4d(T ) + d(c, u) ≤ 6 · 2i , where c is the center of
the cluster with u and T is the shortest path tree rooted at
c and spanning the cluster. For the last iteration where the
destination is reached, the cost is d(u, c)+2d(c, v)+2d(T ) ≤
5 · 2i . W.l.o.g., we assume that 2i−1 /α < d(u, v) ≤ 2i /α.
Thus with probability (1 − 1/n), the routing cost is at most
i−1
j
i
i
i−1
/α < d(u, v),
j=0 6 · 2 + 5 · 2 < 11 · 2 . Therefore, 2
and thus the routing stretch is at most 22α w.h.p.

REFERENCES

[1] I. Abraham, C. Gavoille, A. V. Goldberg, and
D. Malkhi. Routing in networks with low doubling
dimension. In Proc. 26th ICDCS, page 75, 2006.
[2] I. Abraham, C. Gavoille, and D. Malkhi. Routing with
improved communication-space trade-oﬀ. In Proc.
18th DISC, pages 305–319, 2004.
[3] I. Abraham and D. Malkhi. Name independent
routing for growth bounded networks. In Proc. 17th
SPAA, pages 49–55, 2005.
[4] W. Chen, C. Sommer, S.-H. Teng, and Y. Wang.
Compact routing in power-law graphs. In Proc. 23rd
DISC, pages 379–391, 2009.
[5] J. Fakcheroenphol and K. Talwar. An improved
decomposition theorem for graphs excluding a ﬁxed
minor. In APPROX, pages 36–46, 2003.
[6] P. Fraigniaud and C. Gavoille. Routing in trees. In
Proc. 28th ICALP, pages 757–772, 2001.
[7] A. Gupta, R. Krauthgamer, and J. R. Lee. Bounded
geometries, fractals and low-distortion embeddings. In
Proc. 44th FOCS, pages 534–543, 2003.
[8] P. Klein, S. A. Plotkin, and S. Rao. Excluded minors,
network decomposition, and multicommodity ﬂow. In
Proc. 25th STOC, pages 682–690, 1993.
[9] G. Konjevod, A. W. Richa, and D. Xia.
Optimal-stretch name-independent compact routing in
doubling metrics. In Proc. 25th PODC, pages 198–207,
2006.
[10] G. Konjevod, A. W. Richa, and D. Xia. Optimal
scale-free compact routing schemes in networks of low
doubling dimension. In Proc. 18th SODA, pages
939–948, 2007.
[11] R. Krauthgamer, J. R. Lee, M. Mendel, and A. Naor.
Measured descent: A new embedding method for ﬁnite
metrics. In Proc. 45th FOCS, pages 434–443, 2004.
[12] S. Rao. Small distortion and volume preserving
embeddings for planar and euclidean metrics. In Proc.
15th SCG, pages 300–306, 1999.

Stretch Analysis.

W.l.o.g.3 , assume that 2i−1 /α < d(u, v) ≤ 2i /α. Thus
with probability (1 − 1/n), nodes u and v are in the same
cluster of one of the log n partition, i.e. node u could route
the message to v with cost 2i+1 . Therefore, by 2i−1 /α <
d(u, v), we have that the routing stretch is at most 4α w.h.p..

2.2 Name-independent Routing Scheme
Each cluster of all the log n random 2i -bounded partitions,
for each i ∈ [log αΔ], maintains a name-independent tree
routing scheme as in the following lemma on the shortest
path tree of the cluster with its center as the root.
Lemma 2.2 ([2]) Every weighted rooted tree with n nodes
has a single-source name-independent routing scheme (in the
designer port model) such that the distance traveled between
the root r and a destination v is at most d(r, v) + 2d(T ),
where d(T ) is the height of the tree, and the error-report to
the root costs at most 4d(T ) if the destination is not in the
3

Since we pick up log n partitions, ∀i ∈ [log(αΔ)], the routing
table at each node has size O(log Δ log5 n/(loglog n)2 ) bits.
We now present the name-independent routing algorithm
from any source node u to any destination v. For i from 0
to log(αΔ), node u repeatedly searches for the destination v
using the single source name-indepement routing algorithm
at the cluster containing u of one of the log n partitions at
level i such that u is 2i /α-padded in the partition, where
the probability that such partition exists is 1 − 1/n.

3.

The label of a node consists of its local routing labels given
by Lemma 2.1 for all clusters containing the node over all
partitions and all levels. Thus it has O(log Δ log3 n/ log log n)
bits. Similarly, the total size of the routing table at each
node is also O(log Δ log3 n/ log log n) bits.
The routing algorithm from u to v, ∀u, v ∈ V , is described
as follows. Node u identiﬁes the minimal i ∈ [log(αΔ)] such
that u and v have the local labels from the same cluster of
one of the log n random partitions P at level i. Then u uses
the local tree labeled routing scheme on P to route to v by
the local label of v.

2

tree. Moreover, only O(log4 n/(loglog n)2 ) bits are needed
per node and headers have size o(log2 n).

For any integer x > 0, let [x] denote the set {0, 1, · · · , x−1}.
Without loss of generality.

352

Mobile Networks and Applications 9, 151–161, 2004
 2004 Kluwer Academic Publishers. Manufactured in The Netherlands.

Approximation Algorithms for the Mobile Piercing Set Problem
with Applications to Clustering in Ad-Hoc Networks
HAI HUANG ∗ and ANDRÉA W. RICHA ∗,∗∗
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-5406, USA

MICHAEL SEGAL
Communication Systems Engineering Department, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel

Abstract. The main contributions of this paper are two-fold. First, we present a simple, general framework for obtaining efficient constantfactor approximation algorithms for the mobile piercing set (MPS) problem on unit-disks for standard metrics in fixed dimension vector
spaces. More specifically, we provide low constant approximations for L1 and L∞ norms on a d-dimensional space, for any fixed d > 0, and
for the L2 norm on two- and three-dimensional spaces. Our framework provides a family of fully-distributed and decentralized algorithms,
which adapt (asymptotically) optimally to the mobility of disks, at the expense of a low degradation on the best known approximation factors
of the respective centralized algorithms: Our algorithms take O(1) time to update the piercing set maintained, per movement of a disk. We
also present a family of fully-distributed algorithms for the MPS problem which either match or improve the best known approximation
bounds of centralized algorithms for the respective norms and space dimensions.
Second, we show how the proposed algorithms can be directly applied to provide theoretical performance analyses for two popular 1-hop
clustering algorithms in ad-hoc networks: the lowest-id algorithm and the Least Cluster Change (LCC) algorithm. More specifically, we
formally prove that the LCC algorithm adapts in constant time to the mobility of the network nodes, and minimizes (up to low constant
factors) the number of 1-hop clusters maintained. While there is a vast literature on simulation results for the LCC and the lowest-id
algorithms, these had not been formally analyzed prior to this work.
We also present an O(log n)-approximation algorithm for the mobile piercing set problem for nonuniform disks (i.e., disks that may have
different radii), with constant update time.
Keywords: distributed algorithms, wireless networks

1. Introduction
The mobile piercing set (MPS) problem is a variation of the
(classical) piercing set problem that arises in dynamic distributed scenarios. The MPS problem has many applications
outside its main computational geometry domain, as for example in mobile ad-hoc communication networks, as we will
see later.
We start by formalizing some basic definitions. A disk D
of radius r with center q in d with respect to Lp norm1 is
given by the set of points D = {z ∈ d : z − qp  r}. Let
q(D) denote the center of a disk D. A piercing set of a given
collection of disks D is a set of points P such that for every
disk D ∈ D, there exists a point p ∈ P such that p ∈ D –
i.e., P pierces every disk D ∈ D. The (classical) k-piercing
set problem seeks to find whether a piercing set P of cardinality k of D exists, and if so, produces it. If the value of k
is minimal over all possible cardinalities of piercing sets of
D then the set P is called a minimum piercing set of D. The
minimum piercing set problem asks for the minimum piercing
∗ This work was supported in part by NSF CAREER Award CCR-9985284.
∗∗ Corresponding author.
1 The L norm, for any fixed p, of a vector z = (z , z , . . . , z ) in d
p
d
1 2
is given by zp = (|z1 |p + |z2 |p + · · · + |zd |p )1/p ; if p = ∞, then
z∞ = max(|z1 |, |z2 |, . . . , |zd |).

set of a given collection D.
We consider a dynamic variation of the classical piercing
set problem, which arises in mobile and distributed scenarios,
where disks are moving in space. In the mobile piercing set
(MPS) problem, we would like to maintain a dynamic piercing set P of a collection of mobile disks D such that, at any
time t, P is a minimum piercing set of the current configuration of the disks. In other words, P must adapt to the mobility
of the disks. Moreover, we would like to be able to devise a
distributed algorithm to solve this problem, where the individual disks can decide in a distributed fashion (with no centralized control) where to place the piercing points. In this
scenario, we assume that the disks are able to detect whether
they intersect other disks. We can think about a disk as being
the communication range of a given mobile device (node),
which resides at the center of the disk: A disk can communicate with all of its adjacent nodes by a broadcast operation
within O(1) time. Below, we will present applications of the
mobile piercing set problem in mobile networks.
In this paper, we focus on the case when the disks are all
of the same radius r – or equivalently, of same diameter 2r.
Hence, without loss of generality, in the remainder of this
paper, unless stated otherwise, we assume that 2r = 1, and
therefore that we have a collection of unit-diameter disks, or
unit-disks for short. In section 5, we address an extension of

152

our algorithms to the nonuniform case, where the disks may
not all have the same radius.
In recent years, the technological advances in wireless
communications have led to the realization of ad-hoc mobile
wireless networks, which are self-organizing and which do
not rely on any sort of stationary backbone structure. These
networks are expected to significantly grow in size and usage
in the next few years. For scalability, specially in order to be
able to handle updates due to the constant changes in network
topology, clustering becomes mandatory.
As hinted above, mobile unit-disks can be used to model
an ad-hoc network where all mobile wireless nodes have the
same range of communication. Each mobile node’s communication range is represented by a disk in 2 (or 3 ) centered at the node with radius equal to 1; a mobile node A can
communicate with mobile node B if and only if B is within
A’s communication range. The ad-hoc network scenario is
a direct application scenario for the unit-disk MPS problem,
since an ad-hoc network is fully decentralized and any algorithm running on such a network must adapt to mobility in an
efficient way.
If all disks are of the same size, then the k-piercing set
problem is equivalent to the decision version of a well-known
problem: the geometric k-center problem [2]. The geometric k-center problem under Lp metric is defined as follows:
Given a set S of n demand points in d , find a set P of k
supply points so that the maximum Lp distance between a
demand point and its nearest supply point in P is minimized.
The corresponding decision problem is to determine, for a
given radius r, whether S can be covered by the union of k
Lp -disks of radius r, or in other words, to determine whether
there exists a set of k points that pierces the set of n Lp -disks
of radius r centered at the points of S. In some applications,
P is required to be a subset of S, in which case the problem is
referred to as the discrete k-center problem. When we choose
the L2 metric, the problem is called the Euclidean k-center
problem, while for L∞ metric the problem is called the rectilinear k-center problem. Since the Euclidean and rectilinear
k-center problems in 2 are NP-complete (see, e.g., [26,29])
when k is part of the input, the planar unit-disk k-piercing
set problem in 2 under L1 , L2 or L∞ norms is also NPcomplete. Unfortunately, an approximation algorithm for the
k-center problem does not translate directly into an approximation algorithm for the unit-disk piercing set problem (and
vice-versa), since an algorithm for the former problem will
give an approximation on the radius of the covering disks,
while for the latter problem we need an approximation on the
number of piercing points. Still, the two approximation factors are highly related [2].
The remainder of this paper is organized as follows. In
section 1.1, we state our main contributions in this work. In
section 2, we discuss more related work in the literature. Section 3 proves some geometric properties of the piercing set
problem. We use the results in section 3 to develop the approximation algorithms presented in sections 4 and 5: The algorithm introduced in section 4 leads to lower approximation
factors, for the norms and dimensions considered, while the

H. HUANG ET AL.

one in section 5 adapts optimally to the movement of disks.
In section 6, we relate the algorithms presented for the MPS
problem to clustering in ad-hoc networks. Finally, we present
some future work directions in section 7.
1.1. Our results
In this paper we propose fully distributed (decentralized) approximation algorithms for the unit-disk MPS problem for
some fixed norms and dimensions. All of the approximation
factors presented in this paper are with respect to the number
of points in a minimum piercing set.
For each algorithm, we are interested in computing the
cost associated with building an initial approximate piercing set for the given initial configuration of the collection of
disks – which we call the setup cost of the algorithm – and
the cost associated with updating the current piercing set due
to the movement of a disk – which we call the update cost
of the algorithm. Actually we charge the update costs per
event, as we explain below. We assume that all the costs that
do not involve communication between disks are negligible
when compared to the cost of a disk communicating with its
neighbors (through a broadcast operation). Therefore we will
only consider communication costs when evaluating the algorithms.
In order to maintain an optimal or approximate piercing
set of the disks, there are two situations which mandate an
update on the current piercing set. The first situation is when
the movement of a disk D results in having at least one disk
D 
 of D unpierced (note that D 
 may be D itself). The second
situation is when some piercing points in the set maintained
become “redundant”, and we may need to remove them from
the set. Thus, we say that an (update) event is triggered (or
happened) whenever one of the two situations just described
occurs.
The main contributions of this paper are two-fold. First,
we present a family of constant-factor approximation algorithms – represented by the M-algorithm – for the unit-disk
MPS problem with (asymptotically) optimal setup and update costs, for all the norms and space dimensions considered.
Moreover, we achieve this without a significant increase in
the approximation factor of the corresponding best known approximation algorithms for the classical piercing set problem.
Let P ∗ be a minimum piercing set. More specifically, in d dimensions, we devise a 2d -approximation algorithm under L1
or L∞ . For L2 norm, we devise a 7-approximation algorithm
in 2 , and a 21-approximation algorithm in 3 . All these
algorithms have O(|P ∗ |) setup cost and O(1) update cost.
Note that any dynamic algorithm that approximates the minimum piercing set of a collection of mobile disks has setup
cost (|P ∗ |), and update cost (1). These algorithms are
the first constant-approximation algorithms for the unit-disk
MPS problem, with asymptotically optimal setup and update
costs. We summarize these results in table 1.2
2 All the results are for unit-disks; L is equivalent to L for any p in one
p
∞

dimension.

APPROXIMATION ALGORITHMS FOR THE MOBILE PIERCING SET PROBLEM

Table 1
Main results for 1D, 2D, and 3D with optimal update costs.
Space/Norm

Approximation factor, Setup/Update cost

1D
2D/L1 , L∞
2D/L2
3D/L1 , L∞
3D/L2

2-approximation, O(|P ∗ |)/O(1)
4-approximation, O(|P ∗ |)/O(1)
7-approximation, O(|P ∗ |)/O(1)
8-approximation, O(|P ∗ |)/O(1)
21-approximation, O(|P ∗ |)/O(1)

153

2 (or 3 ), and that the lowest-id algorithm also maintains
the same approximation factor as the M-algorithm, while incurring higher update costs.
Another contribution of our work addresses the MPS problem on nonuniform radius disks. If the ratio between the
maximum and minimum disk radii is bounded by a polynomial on n = |D|, we present a fully-distributed O(log n)approximation algorithm for this problem, with constant update cost.

Table 2
Main results for 2D and 3D with better approximation factors.
Space/Norm

Approximation factor, Setup/Update cost

2D/L1 , L∞
2D/L2
3D/L1 , L∞
3D/L2

2-approximation, O(|P ∗ |)/O(|P ∗ |)
4-approximation, O(|P ∗ |)/O(|P ∗ |)
4-approximation, O(|P ∗ |)/O(|P ∗ |)
11-approximation, O(|P ∗ |)/O(|P ∗ |)

We also present a second family of fully distributed algorithms – represented by the A-algorithm – for L1 or L∞
norms in any space d , and for L2 norm in 2 and 3 . These
algorithms achieve the same, or better, constant approximation factors as the best known centralized algorithms with
comparable running times for the corresponding norm and
space dimension, but have a poorer update cost of O(|P ∗ |).
These algorithms are, to the best of our knowledge, the first
fully distributed (decentralized) approximation algorithms
which achieve the same approximation factors as their centralized counterparts. We summarize these results in table 2
(see also footnote 2).
The simple framework presented for the M-algorithm,
which can handle mobility efficiently in a dynamic scenario,
is an important contribution of this work on its own. It avoids
the use of involved data structures, which in general cannot
avoid the use of some sort of “centralization” (even if implicitly). In order to be able to apply the given framework to
a particular norm and dimension, one needs only to be able
to compute a set of piercing points which are guaranteed to
pierce the immediate neighborhood of any disk D: The number of such points will be used in bounding the approximation
factor of the algorithms proposed.
The second main contribution of this work is the application of the algorithms developed for the MPS problem to
the problem of finding an efficient self-organizing 1-hop underlying clustering structure for (wireless and mobile) ad-hoc
networks, as seen in section 6. In fact, one can use the algorithms developed for the MPS problem to derive the first
theoretical performance analyses of the popular Least Cluster
Change (LCC) algorithm proposed by Chiang et al. [7], and of
the lowest-id algorithm (discussed by Gerla and Tsai in [15]),
both in terms of the number of 1-hop clusters maintained and
in terms of update and setup costs, thus providing a deeper
understanding of these two algorithms and validating the existing simulation results for the same. No previous formal
analysis of either algorithm exists in the literature. Namely,
we show that the LCC algorithm has the same approximation
factor, setup and update costs as the M-algorithm for L2 in

2. Related work
The k-center and k-piercing problems have been extensively
studied. In d dimensions, a brute-force approach leads to an
exact algorithm for the k-center problem with running time
O(ndk+2 ). For the planar case of the
Euclidean k-center prob√
lem, Hwang et al. [24] gave an nO( k) time algorithm improving Drezner [9] solution which runs in time O(n2k+1 ). An algorithm with the same running time was presented in Hwang
et al. [23] for the planar discrete Euclidean k-center problem.
Recently, Agarwal and Procopiuc [1] extended and simplified
1−1/d )
the technique by Hwang et al. [24] to obtain an nO(k
time
algorithm for computing the Euclidean k-center problem in d
dimensions.
Sharir and Welzl [32] explain a reduction from the rectilinear k-center problem to the k-piercing set problem (under L∞
metric), using a sorted matrix searching technique developed
by Frederickson and Johnson [13]. Ko et al. [26] proved the
hardness of the planar version of the rectilinear k-center and
presented an O(n log n) time 2-approximation (on the covering radius) algorithm. (In fact, Ko et al. [26] proved that,
unless P = NP, the best approximation factor that can be
achieved in polynomial time for the rectilinear k-center problem is 2.) Several approximation results (on the radii of the
disks) have been obtained in [11,17,20,21]. For more results
on the k-center problem, please refer to [2].
Regarding the k-piercing set problem in d , Fowler et
al. [12] proved the NP-completeness of finding the minimum
value of k for a given set of n disks. Hochbaum and Maas [19]
d
gave an O(l d n2l +1 ) polynomial time algorithm for the minimum piercing set problem under L1 or L∞ norm with approximation factor (1 + 1/ l)d for any fixed integer l  1. Thus,
for l = 1, their algorithm yields an O(n3 ) time (sequential)
algorithm with performance ratio 2d . Hochbaum and Maas
also present a (1 + 1/ l)d -approximation algorithm, for any
fixed integer l >√0, for Ld norm in Rd , whose running time
d
depends on nd(l d) . For the one-dimensional case, Katz et
al. [25] presented an algorithm that maintains the exact piercing set of points for a collection of n intervals in O(|P ∗ | log n)
time, where P ∗ is a minimum piercing set. Their solution
can be adapted to obtain an algorithm with distributed running time O(|P ∗ |) for computing a minimum piercing set of
n intervals. Nielsen [30] proposed a 2d−1-approximation algorithm that works in d-dimensional space under L∞ metric
in O(dn + n log c) time, where c is the size of the piercing

154

set found. This algorithm is based on the divide-and-conquer
paradigm.
Although not stated explicitly, the approximation on the radius of the k-center problem in [1] implies a 4-approximation
algorithm for the minimum piercing set problem for 2
and L2 . Efrat et al. [10] introduced a dynamic data structure
based on segment trees which can be used for the piercing set
problem. They presented a sequential algorithm which gives
a constant factor approximation for the minimum piercing set
problem for “fat” objects with polynomial setup and update
time. See [10] for the definition of “fatness” and more details.
A large number of clustering algorithms have been proposed and evaluated through simulations in the ad-hoc network domain, as for example in [3,4,15,27,28,31]. Gerla
and Tsai in [15] considered two distributed clustering algorithms, the lowest-id algorithm and the highest-degree algorithm, which select respectively the lowest-id mobile or the
highest-degree mobile in a one-hop neighborhood as the clusterhead. A weight oriented clustering algorithm, more suitable to “quasi-static” networks, was introduced by Basagni
[4], where one-hop clusters are formed according to a weightbased criterion that chooses the nodes that coordinate the
clustering process based on node mobility-related parameters.
In [27], Lin and Gerla described a non-overlapping clustering
algorithm where clusters are able to be dynamically reconfigured.
The LCC algorithm proposed by Chiang et al. [7] aims to
maintain a one-hop clustering of a mobile network with least
number of changes in the clustering structure, where clusters
will be broken and re-clustered only when necessary. In fact,
our algorithm for the MPS problem, when translated to a clustering algorithm in the ad-hoc scenario, is essentially the LCC
algorithm, as discussed in section 6.
Recently, researchers have investigated using geometric
centers as clusterheads in order to minimize the maximum
communication cost between a clusterhead and the cluster
members. Bepamyatnikh et al. [6] discussed how to compute
and maintain the one-center and the one-median for a given
set of n moving points on the plane (the one-median is a point
that minimizes the sum of all distances to the input points).
Their algorithm can be used to select clusterheads if mobiles
are already partitioned into clusters.
Gao et al. [14] proposed a randomized algorithm for maintaining a set of clusters based on geometric centers, for a fixed
radius, among moving points on the plane. Their algorithms
have expected approximation factor on the optimal number
of centers (or,√equivalently, of clusters) of c1 log n for intervals and of c2 n for squares3, for some constants c1 and c2 .
The probability that there are more than c ln n times the op2
timal number of centers is 1/n(c ) for the case of intervals;
√
for squares, the probability that there are more than c n ln n
2
times the optimal number of centers is 1/n(c ) ln n , for constant c. An extension of this basic algorithm led to a hierar3 Disks in 1D correspond to intervals on the line; in 2D, disks under L or
∞

L1 are called squares.

H. HUANG ET AL.

chical algorithm, also presented in [14], based on kinetic data
structures [5]. The hierarchical algorithm admits an expected
constant approximation factor on the number of discrete centers, where the approximation factor also depends linearly on
the constants c1 and c2 . The dependency of the approximation
factor and the probability that the algorithm chooses more
than a constant times the optimal number of centers is similar to that of the non-hierarchical algorithm for the squares
case. The constants c1 and c2 , which have not been explicitly
determined in [14], can be shown to be very large (certainly
more than an order of magnitude larger than the corresponding approximation constant presented in this paper), even if
we allow the probability of deviating from the expected constant approximation on the number of centers (which depends
linearly on c1 and c2 ) not to be close to one. Their algorithm
has an expected update time of O(log3.6 n) (while the update
cost is constant in our algorithm), the number of levels used
in the hierarchy is O(log log n), with O(n log n log log n) total space.
Har-Peled [18] found a scheme for determining centers
in advance, if the degree of motion of the points is known:
More specifically, if in the optimal solution the number of
centers is k and r is the optimal radius for the points moving
with degree of motion , then his scheme guarantees a 2+1 approximation (of the radius) with k +1 centers chosen from
the set of input points before the points start to move.

3. Geometry for the piercing set problem
In this section, we prove some geometric properties of the
minimum piercing set problem. More specifically, we solve
the minimum piercing set problem on the neighborhood of a
disk, which will provide the basic building block for our approximation algorithms presented in the following sections.
The main step of the approximation algorithms is to select
an unpierced unit-disk and pierce all of its neighbors. By
repeating this procedure, we will eventually pierce all unitdisks and form a piercing set. The approximation factors are
determined by the number of piercing points chosen for each
selected unpierced unit-disk.
If two disks D and D 
 intersect, we say that D is a neighbor of D 
 . The neighborhood of a disk D, denoted by
N (D), is defined as the collection of all disks that intersect D,
N (D) = {D 
 : D ∩ D 
 = ∅, D 
 ∈ D}. Note that D ∈ N (D).
We are interested on the minimum number of points that
pierce all disks in the neighborhood of a given disk. However,
this number may vary, depending on the distribution of the
disks in the particular neighborhood in consideration. Thus,
we compute the minimum number (along with the fixed positions) of points needed to pierce any possible neighborhood
of a disk. This number is called the neighborhood piercing
number. The neighborhood piercing number is tight in the
sense that for any set of points with smaller cardinality, we
can find some configuration of the neighborhood of a disk
which has an unpierced disk. The corresponding piercing
points are called the neighborhood piercing points. Clearly,

APPROXIMATION ALGORITHMS FOR THE MOBILE PIERCING SET PROBLEM

155

(a)

(b)
Figure 1. Piercing points for neighborhood of (a) an arbitrary disk, (b) a top disk.

the piercing number is a function of both dimension d and
norm index p. Hence, we denote the neighborhood piercing
number for dimension d and norm index p as N(d, p), and
we use PN(D, d, p) to denote a corresponding set of neighborhood piercing points of a unit-disk D. 4 We prove in this
section that N(d, 1) = N(d, ∞) = 2d for all d  1, and that
N(2, 2) = 7. We also place an upper bound of 21 on N(3, 2).
For each of the norms and dimensions considered, we give a
corresponding set of neighborhood piercing points.
First we reduce the minimum piercing set problem into an
equivalent disk covering problem. Let D be a collection of
unit-disks and P be a set of points. Let P 
 be the set of centers
of all disks in D, and D
 be a collection of unit-disks centered
at points in P . Then P pierces D if and only if D
 covers P 
 .
Moreover, P is a minimum piercing set for D if and only if
D
 is a minimum set of disks (with respect to cardinality) that
covers P 
 . We define the unit-disk covering problem to be
the problem of finding the minimum k such that there are k
unit-disks whose union covers a given point set.
We now reduce the problem of finding the neighborhood
piercing number to a unit-disk covering problem as follows.
For a unit-disk D, all the centers of unit-disks in N (D) are
located in the region G = G(D) = {z: z − q(D)p  1},
where q(D) is the center of D. Conversely, a disk centered at
any point in G must intersect D. Therefore, we seek for the
minimum number of unit-disks that cover region G. The centers of those disks serve as the set of neighborhood piercing
points PN(D). The tightness of N can be seen from the fact
that in the disk covering problem, we cannot cover the entire
region G with less than N disks, as proven in the following
lemma. Note that the region G is a disk of radius 1, and that
all of the disks that we use to cover G are unit-disks (i.e., of
radius 12 ).
Lemma 1. The neighborhood piercing number is equal to 2d
for a d-dimensional space under L1 or L∞ norm. The neighborhood piercing number for two dimensions and L2 is equal
to 7.
4 In general, we omit the parameters p, d, or D, whenever clear from the

context.

Proof. For any Lp norm, in a d-dimensional space, the ratio
of the area of G to the area of a unit-disk is 2d . Thus, we need
at least 2d disks to cover G – i.e., N  2d for any dimension
d  1 and any norm Lp . The lower bound of 2d is in fact tight
for p = 1 or p = ∞, since in any dimension d, the unit-disk
D has 2d “corners” under these norms, and the set of unitdisks centered at those “corners” cover the entire region G.
The case p = 2 is more involved since we cannot pack
“spheres” as tightly as “hypercubes” without leaving uncovered points in the region G, if no intersection of disks is allowed. Without loss of generality, assume that we are covering the neighborhood of a disk D centered at the origin.
Any given point p ∈ G can be represented by (, α), where
0    1 and 0  α  2π are the radius and angle on
the polar axis coordinates of p, respectively. A set √
PN(D)
is given by the points with polar coordinates (0, 0), ( 23 , π6 ),
√

√

√

√

√

3 7π
3 3π
3 11π
( 23 , π2 ), ( 23 , 5π
6 ), ( 2 , 6 ), ( 2 , 2 ), ( 2 , 6 ), as shown.
(If we assume that point q represents the origin in figure 1(a),
then PN(D) is given by the points q, r, s, t, u, v and w.) Consider the sector 0  α  π3 . For the other sectors, analogous
arguments apply after a rotation. Let p = (, α), 0    1,
be a point in G such that 0  α  π3 . If   1/2, then p
is√covered by D. The boundary of the unit-disk centered at
( 23 , π6 ) intersects the boundary of region G at points (1, 0)
and (1, π3 ). It also intersects the boundary of D at points
( 12 , 0) and ( 12 , π3 ). Clearly, p is located in the unit-disk cen√

tered at ( 23 , π6 ), if 1/2 <   1.
The perimeter of the boundary of region G is 2π, and one
unit-disk can cover at most π3 of this perimeter. Thus we need
at least six unit-disks to cover the boundary of G – i.e., seven
is the minimum number of unit-disks covering the entire region G. Hence N(2, 2) = 7.

Figure 1(a) shows an optimal seven-disk covering with
disks centered at q, r, s, t, u, v and w, for the region G under L2 norm in 2 . If q = (x, y) is the center of the unitdisk D, the Cartesian
coordinates
of the six other points are
√
√
3
3
3
(x ± 4 , y ± 4 ), (x, y ± 2 ).
For L2 norm in 3 , we were only able to place an upper
bound on the number of unit-disks needed to cover a disk of

156

H. HUANG ET AL.

Table 3
Neighborhood piercing number and that on halfspace in 1D and 2D.
1D

2D/L1

2D/L∞

N

2

4

4

PN
N

2 endpoints
1

4 “corners”
2

4 “corners”
2

PN
n

left endpoint
−1

left & top “corners”
(1, −1)

2 bottom “corners”
(0, −1)

Figure 2. Piercing points for neighborhood of a rightmost interval and an
arbitrary interval.

diameter 2, hence placing an upper bound on N(3, 2). A simple argument [22] suffices to verify that 20 unit-disks centered
at some evenly spaced points on the surface of G plus a unitdisk D centered at the origin cover a disk G of diameter two
also centered at the origin. Hence we have N(3, 2)  21.
It remains an open problem to compute the exact value of
N(3, 2). The neighborhood piercing number for L2 is closely
related to the sphere packing and sphere covering problems
described in [8].
When compared to the results in the literature, the approximation factors based on the neighborhood piercing points
are not the best known. For example, we have shown that
N(1, ·) = 2, which leads to a two-approximation algorithm for piercing unit-intervals on the line (see section 5).
In [16, p. 193] (see also [25]), an exact solution (i.e., 1approximation) for piercing unit-intervals is proposed. The
idea there, shown in figure 2, is to start from the rightmost interval D, where only one endpoint of D – the left endpoint l
– is enough for piercing all neighbors of D (since D has no
neighbor to its right). In order to be able to extend and generalize this idea to other norms and higher dimensions, we
need to define, the halfspace of a disk D with orientation n ,
 · n  0}. For
denoted by HD (
n): HD (
n) = {z: (z − q(D))
the one-dimensional case, all of the centers of the neighboring
disks of the rightmost interval D are located in the halfspace
HD (−1) (to the “left” of D), and only half of the neighborhood piercing points (i.e., only N(1)/2 points) are enough for
piercing N (D). More generally, in any d-dimensional space,
there exists an orientation n , such that we need roughly half of
the neighborhood piercing points to pierce all the neighbors
of disk D located in HD (
n). The minimum number of piercing points needed for the halfspace HD (
n), over all possible
orientations n , is called the halfspace neighborhood piercing number, and is denoted by N . The set of corresponding
piercing points are called the halfspace neighborhood piercing points of D and are denoted by PN(D).
If PN(D) is symmetric with respect to the center of the
unit-disk D, then N = N/2 if the center of D does not belong to PN, or N = (N + 1)/2 otherwise. Note that this
is the case for PN(d, 1), PN(d, ∞) and PN(2, 2). The set of
piercing points which correspond to the upper bound of 21

2D/L2
7√

√

{(0, 0), (± 43 , ± 43 ), (0, ± 23 )}
4√
√
{(0, 0), (± 43 , − 43 ), (0, 23 )}
(0, −1)

for N(3, 2) is not symmetric, but we can still find an orientation such that 11 points are enough to pierce the halfspace
neighborhood of a disk with respect to the orientation. Figure 1(b) illustrates halfspace neighborhood piercing points –
points q, r, s and t – for 2 under L2 norm. The orientation
considered is n = (0, −1). Table 3 summarizes some values
of neighborhood piercing number and that on halfspace for
lower dimensions and norms L1 , L∞ and L2 , where we den) as N and corresponding PN(
n) as
note the minimum of N(
PN. It follows from the upper bound on N(3, 2) that N  11
for L2 and 3 . The corresponding halfspace neighborhood
piercing points are also a subset of the points used for establishing the upper bound on N(3, 2). It also remains an open
question to determine the exact value of N for L2 and 3 .
For an orientation n , if we order all unit-disks D in D according to the values q (D) · n , then a unit-disk D bearing the
smallest q (D) · n value satisfies the property that all its neighn). Thus, by carefully
bors are located in the halfspace HD (
choosing the order in which we consider the neighborhoods
of disks to be pierced, we can use the halfspace neighborhood
piercing points as the basis of the fully-distributed algorithms
for the MPS problem presented in section 4, which match or
improve the best known approximation factors of the respective centralized algorithms.
The problem of computing N for other Lp metrics is
more involved and may not have many practical applications.
A method to estimate an upper bound on N and compute the
corresponding set of neighborhood piercing points for arbitrary Lp metrics is discussed in [22] for completeness.
4. Better approximation factors
In this section we present a family of constant-factor fullydistributed (decentralized) approximation algorithms for the
piercing set problem, which at least match the best known
approximation factors of centralized algorithms with comparable running times for the respective norms and dimensions. (Note that Hochbaum and Maas [19] present
centralized PTAS’s for the minimum piercing set problem
with approximation factor of (1 + 1/ l)d , for any fixed integer l > 0 and dimension d; however, the running time of their
d
algorithms depends on n2l , which makes them of rather limited practical application.) This algorithm introduces some
basic concepts which will be useful when developing the algorithms in section 5. The algorithms in this section all
follow a general algorithmic framework, which we call the
A-algorithm (for having better approximation factors) in con-

APPROXIMATION ALGORITHMS FOR THE MOBILE PIERCING SET PROBLEM

trast with the slightly looser approximation factors of the
other family of algorithms presented in section 5 (represented
by the M-algorithm) which can better handle mobility.
Consider a set of unit-disks in a d-dimensional space under
norm Lp . As shown in section 3, we need at most N piercing
points to pierce the neighborhood of a unit-disk D bearing the
smallest q (D) · n among the (unpierced) disks in its neighborhood, where n is an orientation that gives N . We call such a
disk D a top disk. Thus, at each step of the algorithm, each top
unpierced disk D elects itself as a piercing disk and selects the
points in PN(D) as piercing points. Since all the unpierced
disks in N (D) are now pierced by PN(D), we mark all the
unpierced disks in N (D) as pierced, and repeat the procedure
above. After repeating this step for at most |P ∗ | times, all the
unit-disks in D are pierced and a piercing set with cardinality at most N times |P ∗ | is produced, as shown in theorem 1.
Provided that broadcasting has O(1) cost, the running time
of the distributed A-algorithm is O(|P ∗ |). Theorem 1 states
the main properties of the A-algorithm. This theorem actually
extends the results in [25] and in [30] – for L1 and L∞ norms
in d-dimensional spaces – to a more general distributed scenario, and also to the L2 norm in two- and three-dimensional
spaces.
We re-invoke the A-algorithm to maintain the piercing set
every time an event (as defined in section 1.1) happens. In
a distributed scenario, this can be done by flooding a reset
message to unpierce all disks. Thus the update cost of the
A-algorithm is also O(|P ∗ |).
Theorem 1. The approximation factor of the distributed
A-algorithm is N , and its setup and update costs are both
O(|P ∗ |).
Proof. For each piercing unit-disk D, we need at least one
point in the minimum piercing set P ∗ to pierce D. For any
two distinct piercing unit-disks D and E, the point in P ∗ that
pierces D cannot pierce E since no two (distinct) piercing
disks intersect. Thus we have at most |P ∗ | piercing unitdisks. For each piercing unit-disk, we select N piercing
points. Hence the approximation factor follows. It takes constant time to pierce the neighborhood of each piercing unitdisk using a broadcast operation. Hence the running time for

both setup and update operations is O(|P ∗ |).
5. Better handling of mobility
We now present the M-algorithm, a fully distributed constant
approximation algorithm for the mobile piercing set problem
that adapts optimally to the mobility of disks: The update
cost of the M-algorithm is O(1). We break the M-algorithm
into two parts: the M-Setup algorithm, which builds an initial
piercing set, and the M-Update algorithm, which is in charge
of adapting the piercing set maintained in response to the mobility of disks (we will see later that the M-Update algorithm
may initiate a local call to M-Setup as a subroutine at some
of the disks). The M-algorithm is more suitable for highly
dynamic ad-hoc mobile network scenarios.

157

Figure 3. The movement of the rightmost interval changes all piercing points.

The key idea behind the M-algorithm is to break the
sequential running fashion of the A-algorithm. In the Aalgorithm, an ordering of the unit-disks is mandatory (even
if implicitly). As shown in figure 3, in the worst-case, the
movement of one disk (the rightmost one in the figure) could
lead to a global update of all selected piercing disks, while the
cardinality of the minimum piercing set does not change. In
order to maintain a relatively stable piercing set, the desired
algorithm needs to be able to sever this “cascading effect” –
i.e., the algorithm needs to be able to keep the updates local.
Lemma 2 shows that the cardinality of an optimal piercing
set cannot change by much, due to the movement of a single
disk. This property suggests that an update can be kept local.
The proof of this lemma, while trivial, is presented here for
completeness.
Lemma 2. If at one time only one unit-disk moves, then
||P ∗ |−|P ∗∗ ||  1, where P ∗ denotes a minimum piercing set
before the movement, and P ∗∗ denotes a minimum piercing
set after the movement.
Proof. If the cardinality of the minimum piercing set
changes, then it can either increase or decrease. Since the reverse of a movement that increases the cardinality of the minimum piercing set is a movement that decreases it, we only
need to show that the cardinality of the minimum piercing set
cannot be increased by more than 1. Let D be the moving
disk. Since only D moves, D is the only disk which may become unpierced. Let P = P ∗ ∪ {q(D)}. Then P is a piercing
set after the movement. Let P ∗∗ be a minimum piercing set
after the movement of D, |P ∗∗ |  |P | = |P ∗ | + 1. Hence

||P ∗ | − |P ∗∗ ||  1.
In the M-Setup algorithm, instead of choosing a disk with
respect to the ordering given by a direction n, we select arbitrary unpierced disks as piercing disks in each step, then
pierce the neighborhood of each selected disk D using the
points in PN(D). By repeating this procedure O(|P ∗ |) times,
we will generate a piercing set for D: Since now we use N
points to pierce the neighborhood of each selected piercing
disk, the approximation factor is roughly doubled compared
to that of the A-algorithm. However, this small degradation in
the approximation factor pays for an optimal update strategy,
as will be shown later.
In order to implement the above idea in a distributed fashion, we repeat the following procedure. Each disk D first

158

H. HUANG ET AL.

M-Setup
For each unmarked unit-disk D:
1. Repeat
2.
If there is piercing unit-disk in N (D) then
3.
IsMarked = True
4.
Elseif D bears the lowest label among all its neighbors which
attempt to become a piercing disk then
5.
For each unmarked neighbor D 
 of D
6.
D’.IsMarked = True
7.
End
8.
End
9. Until the disk becomes marked
M-Update
When a unit-disk D moves
1. If D is a piercing unit-disk, then
2.
If D’s boundary meets that of another piercing unit-disk E, then
3.
remove the neighborhood piercing points of D from P
4.
Unmark D and all normal unit-disks that were marked by D
5.
End
6.
If D’s boundary separates from that of a normal unit-disk D 
 , then
7.
Unmark D 
8.
End
9. Else (D is a normal unit-disk)
10.
If D’s boundary separates from that of D’s piercing unit-disk, then
11.
Unmark D
12.
End
13. Call M-Setup
Figure 4. The M-Setup and M-Update algorithm.

checks if there are any piercing disks in its neighborhood. If
so, then D marks itself as pierced. Otherwise, each unpierced
disk tries to become a piercing disk itself. In order to guarantee that only one disk becomes a piercing disk in an unpierced
disk’s neighborhood – this is a key property for proving the
approximation factor of this algorithm – a mechanism such as
“lowest labeled neighbor wins” (assuming that each disk has
a unique identification label) is required. Note that, unlike the
A-algorithm, in the M-Setup algorithm disks do not need to
know their coordinates (since no comparisons of the q (D) · n
values are required), which may be desirable in an ad-hoc network scenario. The proof of theorem 2 is analogous to that of
theorem 1, and is therefore omitted.
Theorem 2. The M-Setup generates a piercing set of cardinality within a factor of N of |P ∗ | in O(|P ∗ |) time.
As disks start moving in space, each disk needs to be able
to trigger an update procedure whenever an update is necessary. To facilitate the following discussion, we call a disk that
is not a piercing disk a normal disk. When a disk moves,
the following events may make the current piercing set invalid and trigger an update: (i) the boundaries of two piercing
disks D and E meet (thus D may become a redundant piercing disk); (ii) the boundaries of one piercing disk D and some
normal disk D 
 pierced by D separate (thus at least one of the

disks becomes unpierced). An M-Update procedure is initiated at disk D in events of type (i), or at disks D and D 
 for
events of type (ii). The M-Update procedure can be divided
into two phases: In the first phase, we will unmark some of
the disks as to be now unpierced; in the second phase, we
select piercing disks for those unpierced disks. The second
phase is executed by a local call to M-Setup initiated at each
unpierced disk.
The details of the M-Update procedure are as follows. If
we have an event of type (i), the M-Update will degrade disk
D to a normal disk and unpierce all disks that were currently
pierced by D (including D itself). Otherwise, if case (ii) applies, the M-Update will simply unpierce disk D 
 . Each node
that is marked unpierced by the M-Update procedure will invoke M-Setup locally. The M-Setup procedure invoked at an
unpierced disk F will first check if any of its neighbors is a
piercing disk. If so, it marks itself pierced. Otherwise, if F
has the lowest label among its unpierced neighbors, it elects
itself as a piercing disk and marks all its unpierced neighbors
as pierced. The M-Setup and M-Update algorithms are shown
in figure 4.
As proven in theorem 3, all unit-disks will be pierced at
the end of the calls to M-Setup, and the approximation factor
on the size of the piercing set maintained is still guaranteed to
be N.

APPROXIMATION ALGORITHMS FOR THE MOBILE PIERCING SET PROBLEM

159

Theorem 3. The M-Update procedure maintains an N-approximation of the MPS, with update cost of O(1) for each
event.

N-approximation factor on the cardinality of the set of piercing points produced.
Case 3. A piercing unit-disk D moves and after the movement, D is pierced by some other piercing unit-disk E ∈ E.
Let D = Di , where Di ∈ E. The M-Update will degrade D to
a normal disk and unpierce all unit-disks previously pierced
by D. The M-Update procedure then invokes local calls to
M-Setup at all unpierced disks. For each unit-disk D 
 previously pierced by D, M-Setup will first check if there is
another piercing disk that pierces D 
 . If so, D 
 will be
marked pierced. Otherwise, if there are neighbors of D
which still remain unpierced, then the M-Setup algorithm
will upgrade some normal disks to piercing disks. Let E 

 =
{Dm+1 , . . . , Dm+k } be the collection of those upgraded piercing disks. Then we have E 
 = (E − {Di }) ∪ E 

 as the
new set of piercing disks. As in case 2, if label(D1 ) <
· · · < label(Di−1 ) < label(Di+1 ) < · · · < label(Dm+k ) <
label(E), for all disks E not in E 
 , the M-Setup algorithm
when applied to the current configuration of the disks in D,
assuming all disks are unpierced at start, will produce E 
 as
the resulting set of piercing disks. Thus the N-approximation
factor follows.
Case 4. A piercing unit-disk D moves and after the movement, D is not pierced by any other piercing unit-disk E ∈ E.
Essentially the same as case 3, but for the fact that we do not
degrade D to a normal disk.


Proof. First we show that the running time of M-Update is
constant per event. Assume that at one time only one event
occurs. All the disks possibly affected by the event are located in the neighborhood of a disk D. Thus the operation
of marking disks as unpierced (in the first phase) takes constant time. Since all nodes that invoked a call to M-Setup
were neighbors of a former piercing disk D, it follows that
the calls to M-Setup will have at most a constant number, N,
of rounds of “lowest labeled neighbor wins” until a valid set
of piercing disks is restored. Therefore the total time taken by
each of the invoked M-Setup calls also takes constant time. If
several events occur at the same time, then the final effect is
the same as if a sequence of events occurs in a row, and the
update cost per event remains the same.
Now we show that the approximation factor maintained is
equal to N. Clearly the resulting piercing set is determined
by the collection of selected piercing unit-disks. We will
show that the updated collection of piercing disks produced
by the M-Update procedure could have been the initial collection of piercing disks produced by the M-Setup algorithm
(for a given ordering of the labels of the disks), thus proving
the claimed approximation factor. Assume that the collection
E = {D1 , . . . , Dm } of selected piercing unit-disks before the
call to M-Update is invoked is an N-approximation on the
MPS. Let E 
 be the collection of selected piercing unit-disks
after the call to M-Update is completed at all nodes (which
may involve calling the M-Setup algorithm locally). One of
the following four cases may occur:
Case 1. A normal unit-disk D 
 moves and after the movement, it is still pierced by some piercing unit-disk in E. In
this case, the M-Update procedure never invokes M-Setup at
a node, and E 
 = E. Since we still need at least one piercing point to pierce each of the selected piercing disks (no two
piercing disks overlap) and since E was an N-approximation
of the MPS, the approximation factor still holds.
Case 2. A normal unit-disk D 
 moves, and after the
movement, D 
 is no longer pierced by a piercing unit-disk
in E. In this case, the M-Setup procedure invoked by the
call to M-Update will upgrade D 
 to a piercing disk. Thus
E 
 = E ∪ {D 
 }.
We prove the bound on the size of the piercing set maintained by showing that E 
 could have been obtained by a general call to the M-Setup algorithm to the current configuration
(placement in space) of the disks if all disks were currently
unpierced, for a given assignment of labels to the disks. Suppose that the labels of the disks in E 
 are smaller than the
labels of all other disks in D, and that label(D1 ) < · · · <
label(Dm ) < label(D 
 ). Thus, on or before step i  m, disk
Di will be selected by M-Setup to become a piercing disk
(since all Di ’s were unpierced disks in E initially, and no two
piercing disks intersect). After all disks D1 , . . . , Dm are selected, only disk D 
 is not pierced. Thus M-Setup must select
D 
 to be a piercing disk. Hence E 
 is obtained, proving the

A simple extension of the M-algorithm provides a logarithmic approximation algorithm for the nonuniform case. If the
collection contains disks of various radii, then we can guarantee an N-approximation if at each step we find the unpierced
disk of smallest radii in the collection and pierce all of its
neighborhood. However, we cannot guarantee having O(1)
update cost in this case. Without loss of generality, assume
that the minimum radius of a disk is equal to 1. If the largest
disk radius is bounded by a polynomial on n = |D|, then we
have the following corollary.
Corollary 1. By grouping the disks into O(log n) classes
such that each class contains disks of radii in [2i−1 , 2i ), we
have an O(log n) approximation for the MPS problem on
nonuniform disks with distributed update cost of O(1).
Proof. In each class, as we show below, N 2 points are
enough to pierce an arbitrary neighborhood. Since we have
O(log n) classes, and the piercing set for each class is an
N 2 -approximation of the overall minimum piercing set, the
approximation factor is bounded by O(log n). Once a disk
moves, it only affects the piercing set selected for one class,
thus the update cost is still constant. We now show that N 2
points are in fact enough for covering a disk of diameter 2i+2 ,
using disks of diameter in [2i , 2i+1 ). In the worst case, we
need to cover a region of diameter 2i+2 with disks of diameter 2i . We can do this in two phases. First we cover the
region using N disks of diameter 2i+1 . Then for each disk D
of diameter 2i+1 , we cover D using N disks of diameter 2i . 

160

6. Applications to clustering in mobile networks
For the ad-hoc network scenario described in the introduction, where all nodes have equal range of communication, the
algorithms proposed for the mobile piercing set problem can
be directly applied in order to obtain a one-hop clustering of
the network. A clustering of a network G is a partition of the
nodes of G into subsets (clusters) Ci , where for each Ci , we
elect a node v ∈ Ci as its clusterhead. A 1-hop clustering
of G is a clustering of G such that every node in the network
can communicate in one-hop with the clusterhead of the cluster it belongs to. We can view the network G as a collection
of unit-disks in 2 (respectively 3 ) under L2 (as discussed
in the introduction).
The algorithm in section 5 can be used to obtain an almost optimal (with respect to number of clusters) one-hop
clustering of a wireless network where all nodes have equal
communication range. We have that |P ∗ |/N(2, 2) = |P ∗ |/7
(respectively |P ∗ |/N(3, 2)  |P ∗ |/21) is a lower bound on
the minimum number of 1-hop clusters (and therefore on the
number of selected clusterheads) needed to cover the entire
network, since we need at least one piercing point for each
of the neighborhoods of a piercing disk, and since we use at
most seven (respectively 21) piercing points for each of these
neighborhoods in a MPS in 2 (respectively 3 ). The number
of piercing disks selected by the algorithm in section 5 is at
most |P ∗ |. Since each of these piercing disks D corresponds
uniquely to a 1-hop cluster C in the network (given by all the
disks pierced by D), and since the union of all these clusters
covers the entire network, we have that the number of clusters is at most |P ∗ |, which is a 7-approximation (respectively
21-approximation) on the minimum number of 1-hop clusters
needed in 2 (respectively 3 ). This algorithm is also suitable for maintaining such an optimal structure as nodes start
moving in space, with optimal update costs. The algorithm
tends to keep the number of changes in the set of selected
clusterheads low.
In fact, the algorithm presented in section 5, when translated to a clustering algorithm on ad-hoc networks, is essentially the same as the Least Cluster Change (LCC) algorithm
presented by Chiang et al. [7]. Therefore, in this paper we
provide a theoretical analysis of the performance of this popular clustering algorithm, validating the simulation results that
showed that the clusters maintained by this algorithm are relatively stable. More specifically, we have proved that this
algorithm sustains a 7-approximation on the number of 1-hop
clusters maintained, while incurring optimal setup and update
costs.
A closer look at the lowest-id algorithm, investigated by
Gerla and Tsai in [15], shows that this algorithm corresponds
to several applications of the M-Setup procedure of section 5.
Every time a disk becomes unpierced, or two piercing disks
intersect, the lowest-id algorithm starts updating the clustering cover maintained in a fashion that may correspond to an
application of the M-Setup algorithm on the current configuration of the disks if all disks were unpierced – in the worstcase, the lowest-id algorithm may generate a “cascading ef-

H. HUANG ET AL.

fect” which correspond to an application of the M-Setup algorithm on a collection of all unpierced disks, if the disk labels
are given by the node ids. Thus the setup and the worst-case
update costs of the lowest-id algorithm are both O(|P ∗ |), and
the approximation on the number of clusters maintained is
equal to 7 and 21, for 2 and 3 , respectively.
7. Future work
There are many natural extensions of the work in this paper. We would like to extend the one-hop clustering structure to a full network clustering hierarchy. One idea would be
to apply the same algorithm presented to construct O(log n)
clustering covers of the network: Clustering i would be obtained by assuming that all disks have radius equal to 2i , for
i = 0, . . . , log n, where n = |D|. One problem with this
strategy is that by artificially increasing the communication
ranges on the nodes in the network (radii of the disks), a resulting cluster in the hierarchy may not even be connected.
Other directions for future work are (i) to develop constant
approximation algorithms for piercing a collection of disks of
different radii; (ii) to extend any results on nonuniform radius disks to ad-hoc network clustering – note that if we have
nonuniform radius disks, we can no longer guarantee symmetric communication between nodes in the network; and
(iii) to determine the exact neighborhood piercing number for
L2 norm in 3- (or higher) dimensional spaces.
Acknowledgement
We would like to express our thanks to Martin Ziegler for
valuable discussions on estimating N(3, 2).
References
[1] P.K. Agarwal and C.M. Procopiuc, Exact and approximation algorithms
for clustering, in: Proc. of 9th ACM–SIAM Sympos. on Discrete Algorithms (1998) pp. 658–667.
[2] P.K. Agarwal and M. Sharir, Efficient algorithms for geometric optimization, ACM Comput. Surv. 30 (1998) 412–458.
[3] S. Basagni, Distributed and mobility-adaptive clustering for multimedia
support in multi-hop wireless networks, in: Proc. of IEEE Vehicular
Tech. Conf. (1999) pp. 19–22.
[4] S. Basagni, Distributed clustering for ad-hoc networks, in: Proc. of
1999 Int. Sympos. on Parallel Architectures (1999) pp. 310–315.
[5] J. Basch, L.J. Guibas and J. Hershberger, Data structures for mobile
data, in: Proc. of 8th ACM–SIAM Sympos. on Discrete Algorithms
(1997) pp. 747–756.
[6] S. Bepamyatnikh, B. Bhattacharya, D. Kirkpatrick and M. Segal, Mobile facility location, in: Proc. of ACM Int. Workshop on Discrete
Algorithms and Methods for Mobile Computing and Communications
(2000) pp. 46–53.
[7] C.-C. Chiang, H.-K. Wu, W. Liu and M. Gerla, Routing in clustered
multihop, mobile wireless networks with fading channel, in: Proc. of
IEEE Singapore Int. Conf. on Networks (1997) pp. 197–211.
[8] J.H. Conway and N.J.A. Sloane, Sphere Packings, Lattices, and Groups
(Springer, 1999).
[9] Z. Drezner, The p-center problem: heuristic and optimal algorithms,
J. Oper. Res. Soc. 35 (1984) 741–748.

APPROXIMATION ALGORITHMS FOR THE MOBILE PIERCING SET PROBLEM

161

[10] A. Efrat, M.J. Katz, F. Nielsen and M. Sharir, Dynamic data structures
for fat objects and their applications, in: Proc. of Workshop on Algorithms and Data Structures (1997) pp. 297–306.
[11] T. Feder and D.H. Greene, Optimal algorithms for approximate clustering, in: Proc. of 20th Annu. ACM Sympos. on Theory of Computing
(1988) pp. 434–444.
[12] R.J. Fowler, M.S. Paterson and S.L. Tanimoto, Optimal packing and
covering in the plane are NP-complete, Inform. Process. Lett. 12(3)
(1981) 133–137.
[13] G.N. Frederickson and D.B. Johnson, Generalized selection and ranking: sorted matrices, SIAM J. Comput. 13 (1984) 14–30.
[14] J. Gao, L.J. Guibas, J. Hershburger, L. Zhang and A. Zhu, Discrete mobile centers, in: Proc. of 17th ACM Sympos. on Computational Geometry (2001) pp. 188–196.
[15] M. Gerla and J.T.C. Tsai, Multicluster mobile multimedia radio networks, ACM–Baltzer J. Wireless Networks 1(3) (1995) 255–256.
[16] M. Golumbic, Algorithmic Graph Theory (Academic Press, New York,
1980).
[17] T. Gonzalez, Covering a set of points in multidimensional space, Inform. Process. Lett. 40 (1991) 181–188.
[18] S. Har-Peled, Clustering motion, in: Proc. of 42nd Annu. IEEE Sympos.
on Foundations of Computer Science (2001) pp. 84–93.
[19] D.S. Hochbaum and W. Maas, Approximation schemes for covering
and packing problems in image processing and vlsi, J. of the ACM 32
(1985) 130–136.
[20] D.S. Hochbaum and D. Shmoys, A best possible heuristic for the
k-center problem, Math. Oper. Res. 10 (1985) 180–184.
[21] D.S. Hochbaum and D. Shmoys, A unified approach to approximation
algorithms for bottleneck problems, J. of the ACM 33 (1986) 533–550.
[22] H. Huang, A.W. Richa and M. Segal, Approximation algorithms for the
mobile piercing set problem with applications to clustering, Technical
Report TR-01-007, Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ (2001).
[23] R.Z. Hwang, R.C. Chang and R.C.T. Lee, The generalized searching
over separators strategy to solve some NP-hard problems in subexponential time, Algorithmica 9 (1993) 398–423.
[24] R.Z. Hwang, R.C.T. Lee and R.C. Chang, The slab dividing approach
to solve the Euclidean p-center problem, Algorithmica 9 (1993) 1–22.
[25] M.J. Katz, F. Nielsen and M. Segal, Maintenance of a piercing set for
intervals with applications, in: Proc. of 11th Int. Symp. on Algorithms
and Computation (2000) pp. 552–563.
[26] M.T. Ko, R.C.T. Lee and J.S. Chang, An optimal approximation algorithm for the rectilinear m-center problem, Algorithmica 5 (1990)
341–352.
[27] C.R. Lin and M. Gerla, Adaptive clustering for mobile wireless networks, IEEE J. Selected Areas Commun. 15(7) (1997) 1265–1275.
[28] A.B. McDonald and T. Znati, A mobility-based framework for adaptive
clustering in wireless ad-hoc networks, IEEE J. Selected Areas Commun. 17(8) (1999).
[29] N. Megiddo and K.J. Supowit, On the complexity of some common
geometric location problems, SIAM J. Comput. 13(1) (1984) 182–196.
[30] F. Nielsen, Fast stabbing of boxes in high dimensions, in: Proc. of 8th
Canad. Conf. in Computational Geometry (1996) pp. 87–92.
[31] R. Ramanathan and M. Steenstrup, Hierarchically-organized, multihop
mobile wireless for quality-of-service support, Mobile Networks and
Applications 3 (1998) 101–119.

[32] M. Sharir and E. Welzl, Rectilinear and polygonal p-piercing and
p-center problems, in: Proc. of 12th Annu. ACM Sympos. on Computational Geometry (1996) pp. 122–132.

Hai Huang is an M.S. candidate in the Department
of Computer Science and Engineering at Arizona
State University, under the supervision of Prof. Andrea W. Richa. He received a B.A. and a M.A.
in the Mathematics Department at Tsinghua University, P.R. China, in 1996 and 1999, respectively. His
current research work focus on clustering and routing problems in mobile ad-hoc networks, and on the
chordal graph completion problem with applications
to scientific computing.
E-mail: hai@asu.edu
Andrea W. Richa joined the Department of Computer Science and Engineering at Arizona State University as an Assistant Professor in August 1998. She
received her M.S. and Ph.D. degrees from the School
of Computer Science at Carnegie Mellon University,
in 1995 and 1998, respectively. She also earned an
M.S. degree in computer systems from the Graduate
School in Engineering (COPPE), and a B.S. degree
in computer science, both at the Federal University
of Rio de Janeiro, Brazil, in 1992 and 1990, respectively. Prof. Richa’s main area of research is in network algorithms. Some
of the topics Dr. Richa has worked on include packet scheduling, distributed
load balancing, packet routing, mobile network clustering and routing protocols, and distributed data tracking. Prof. Richa’s data tracking (or lookup)
algorithm has been widely recognized as the first benchmark algorithm for
the development of distributed databases in peer-to-peer networking, having
received over 114 academic journal or conference publications, and being
implemented as part of two of the current leading pojects in peer-to-peer networking. Dr. Richa’s was the recipient of an NSF CAREER Award in 1999.
For a selected list of her publications, CV, and current research projects,
please visit http://www.public.asu.edu/ aricha.
E-mail: aricha@asu.edu
Michael Segal was born at October 12, 1972 in
USSR. In 1991 he immigrated to Israel and started
to study Computer Science in Ben-Gurion University
of the Negev. He finished his B.Sc, M.Sc and Ph.D.
degrees in 1994, 1997, and 1999, respectively. During a period of 1999–2000 Dr. Michael Segal held a
MITACS National Centre of Excellence Postdoctoral
Fellow position in University of British Columbia,
Canada. Dr. Segal joined the Department of Communication Systems Engineering, Ben-Gurion University, Israel in 2000 where he holds now a position of Senior Lecturer. His
primary research is algorithms (sequential and distributed), data structures
with applications to optimization problems, mobile wireless networks, communications and security.
E-mail: segal@cse.bgu.ac.il

Optimal Scale-free Compact Routing Schemes in Networks
of Low Doubling Dimension ∗
Goran Konjevod

Andréa W. Richa

Abstract
We present optimal-stretch scale-free compact routing schemes for networks of low doubling dimension, in both
the name-independent and name-dependent models. Our
name-independent algorithm is the first scale-free nameindependent compact routing scheme to achieve asymptotically optimal stretch, closing the gaps left by the
work of Abraham et al. (ICDCS’06) and Konjevod et al.
(PODC’06). Our name-dependent algorithm is the first
scale-free optimal-stretch name-dependent compact routing
scheme that uses optimal ⌈log n⌉-bit routing labels, in spite
of the limited routing label information. We define a simple
hierarchical decomposition technique based on ball-packings.
Our algorithms rely on a novel combination of ball-packings
and hierarchical r-nets, which we see as a contribution in its
own right.

1 Introduction
A routing scheme is a distributed algorithm that allows any source node to route packets to any destination node in a distributed network. A routing scheme
consists of two steps: the pre-processing, and the routing
algorithm. In the pre-processing step, given a network,
the scheme renames each node if applicable, and configures the local routing table at each node. In the routing
algorithm, given a destination’s name, the source node
sets up a packet header and sends the packet to one of
its neighbors, based on the destination’s name and the
local routing table. A relay node, upon reception of a
packet, decides whether the packet has reached its destination and if not, where to forward it, based on the
packet header and the local routing table. The stretch
of a routing scheme is the maximum ratio of the length
of the path by which a packet is delivered, to the length
of the shortest source-destination path, over all sourcedestination pairs. One of the fundamental trade-offs for
routing schemes is between the space used to store the
routing tables at each node (which contain all the information required for routing), and the stretch of the
routing scheme. In general, one would like to keep both
∗ CSE Dept., Arizona State University, Tempe, AZ 85287-8809,
USA. {goran,aricha,dxia} at asu.edu. Supported in part by NSF
grant CCR-0209138 and NSF CAREER grant CCR-9985284.

Donglin Xia

the storage per node and the packet header size polylogarithmic in the number of nodes, while optimizing
stretch. Such a scheme is usually called a compact routing scheme.
The aspect ratio ∆ of a graph is the ratio of
the largest to the smallest shortest path distance in
the graph. For many routing schemes [8, 24, 10, 21,
18], the routing table size at each node, the packet
header size, or the routing label directly depend on a
polylogarithmic function on ∆. While those schemes
are compact for networks where ∆ is polynomial in n,
they do not scale well if ∆ grows exponentially with
n. Hence, one would prefer a compact routing scheme
that does not directly depend on ∆. We call a routing
scheme scale-free if the memory requirements for its
routing tables, packet headers, and routing labels are
independent of the aspect ratio.
In this paper, we give efficient scale-free compact
routing schemes for networks whose shortest path metric has low doubling dimension, where the doubling dimension of a metric space is the minimum α such that
any ball of radius r can be covered by at most 2α balls
of radius r/2.
We consider two variants of the routing problem:
the name-dependent (or labeled) model, and the nameindependent model. The former allows the designer of
the routing scheme to rename (or label) the nodes so
that they contain additional routing (e.g. topological)
information. In the latter case, the routing scheme
must use the (arbitrary) original naming. A labeled
scheme has the advantage of embedding information
in the node labels to facilitate routing, but it requires
the source node to know the designer-given label of the
destination node, which is not always feasible. Thus,
given a name-dependent scheme, it remains an issue to
determine how (and where) the source will find the label
of the destination. Therefore name-independent routing
schemes are preferable, especially in applications with
intrinsic requirements on node names (e.g. distributed
hash tables [7]), that require randomly distributed
node names (e.g. Chord [23]), or that perform network
operations such as locating nearby copies of replicated
objects and tracking of mobile objects [7, 8].

939

1.1 Our Contributions. We present optimalstretch scale-free compact routing schemes for networks
of low doubling dimension in both the name-independent and name-dependent models. More precisely,
our main results are Theorems 1.1 and 1.2.

independent scheme for networks of low doubling dimension. In particular, we believe that our ball-packing
decomposition, used in both the name-independent and
name-dependent schemes, will have an impact on other
problems that also rely on a hierarchical structure of
r-nets (See Definition 2.1). In a nutshell, both types of
Theorem 1.1. Given any ǫ ∈ (0, 1) and a weighted schemes rely on a global hierarchy of r-nets. In order
undirected graph G with n nodes and doubling dimension to avoid a dependence on ∆, we cannot store informaα, we present a scale-free name-independent routing tion for all O(log ∆) layers of r-nets: We only maintain
2
scheme for G with (9 + ¡ǫ)-stretch, O(log
¢ n/ log log n)- information about O(log n) layers at each node, while
3
1 O(α)
bit packet headers, and ( ǫ )
log n -bit routing in- packing balls are used to account for the layers for which
formation at each node.
no information exists at a node.
Theorem 1.2. Given any ǫ ∈ (0, 1) and a weighted
undirected graph G with n nodes and doubling dimension
α, we present a (1+ǫ)-stretch labeled routing scheme for
2
G with ⌈log n⌉-bit routing
labels, O(log
¡ 1 O(α)
¢ n/ log log n)-bit
3
packet headers, and ( ǫ )
log n -bit routing information at each node.
We define a network to have low doubling dimension if α = O(loglog n). Hence our algorithm is the first
name-independent scale-free compact routing scheme for
networks of low doubling dimension with (asymptotically) optimal stretch, closing the gaps left by the results in [18] (where an optimal-stretch, but not scalefree, name-independent scheme is presented) and in [2]
(where a scale-free, but not optimal-stretch, nameindependent scheme is presented). By (asymptotically)
optimal stretch, we mean that the stretch of our algorithm is t + ǫ, for any fixed ǫ > 0, where t is a lower
bound on the best possible stretch for any constant doubling dimension α (in particular, t → 9 as α increases).
See [18] for the proof of the lower bound.
Our contributions for the name-dependent model
are twofold. First, our algorithm is the first (asymptotically) optimal-stretch name-dependent scale-free compact routing scheme for networks of low doubling dimension that uses optimal ⌈log n⌉-bit routing labels
(and hence embeds the minimal required amount of
network-dependent routing information into the routing
labels). Second, our techniques are significantly simpler than the ones used by Abraham et al. in [2], who
also present an asymptotically optimal-stretch scale-free
name-dependent scheme (they use 2O(α) log3 n-bit routing labels though). Our routing scheme relies on a
simple and unifying hierarchical network decomposition
technique using a ball-packing, rather than the complex
sparse-dense decomposition of [2].
We believe that some of the techniques introduced
in this paper are a major contribution on their own. The
new techniques and data structures presented in this
paper enable us to go beyond the results in both [18]
and [2] and obtain an optimal-stretch scale-free name-

1.2 Related Work. Not surprisingly, there has been
a vast amount of research on efficient network routing
schemes. General overviews are available in Peleg’s
book [20] and the surveys by Gavoille [14] and Gavoille
and Peleg [16]. For a table summarizing the exact
packet header sizes, stretch and storage requirements
of most compact routing schemes for networks of low
doubling dimension, please refer to [2].
An extreme solution is to keep a complete routing
table at each node, providing the next hop information
for shortest paths to any other node (note that routing
along a shortest path results in stretch 1). However,
this requires Θ(n log D) bits at each node of degree D.
In fact, Gavoille and Hanusse [15] showed that stretch1 name-independent routing schemes require Ω(n log k)
bits memory per node for the complete bipartite graph
Kk,n−k .
The trade-off between space and stretch has been
further explored in other work. Awerbuch and Peleg [9] pioneered compact routing schemes, designing
a name-independent scheme with stretch O(k 2 ) and
Õ(n1/k log ∆) bits of storage per node, where ∆ is aspect ratio of the graph (the Õ() notation denotes complexity similar to O() up to polylogarithmic factors.).
The stretch was improved to O(k) with the same space
requirement in [3]. In addition, Abraham, Gavoille
and Malkhi [5] presented a scale-free name-independent
routing scheme with O(k) stretch, and Õ(n1/k ) routing
tables, asymptotically optimal for general graphs [4].
Constant-stretch name-independent compact routing schemes do exist for restricted classes of graphs.
For growth-bounded networks (a subclass of networks
of constant doubling dimension), a randomized (1 + ǫ)stretch compact routing scheme is known [6]. For unweighted graphs excluding fixed Kr,r minors (including trees and planar graphs), a (1 + ǫ)-stretch compact
scheme is presented in [1].
For networks with doubling dimension α, Konjevod,
Richa and Xia [18] presented an asymptotically optimal
(9 + ǫ)-stretch name-independent routing scheme with

940

( 1ǫ )O(α) log2 ∆ log n bits of storage per node, for any
ǫ > 0. They also gave a matching lower bound of (9 − ǫ)
on the stretch of any routing scheme for networks of low
2
doubling dimension that uses o(n(ǫ/60) ) bits of routing
information per node, for any ǫ > 0. Abraham et al. [2]
achieved a scale-free name-independent routing scheme
with (large) constant stretch and ( 1ǫ )O(α) log4 n bits of
storage per node.
In the name-dependent (or labeled) model, Eilam
et al. [12] achieved stretch 5 with Õ(n1/2 ) memory and O(log n)-bit labels, while Cowen [11] proposed a stretch 3 labeled routing scheme with Õ(n2/3 )
memory and O(log n)-bit labels. Furthermore, Thorup and Zwick [26] achieve stretch 2k − 1 using
Õ(n1/k ) routing tables and O(k log2 n)-bit labels. For
trees, optimal stretch 1 labeled routing schemes with
O(log2 n/ log log n) bits of label, packet header and
memory were presented in [13, 26]. There are (1 +
ǫ)-stretch labeled routing schemes with polylogarithmic space for planar graphs [25], graphs excluding a
fixed minor [1], and networks of low doubling dimension [22, 2].
Recently, a scale-free constant-stretch nameindependent routing scheme with 2O(α) log4 n memory and 2O(α) log3 n packet headers was proposed by
Abraham et al. [2] for networks of low doubling dimension; their stretch factor, albeit being constant, is
very large and of limited pratical interest. They also
present a scale-free (1 + ǫ)-stretch labeled scheme with
( 1ǫ )O(α) log4 n-bit memory, and ( 1ǫ )O(α) log3 n-bit label
and header. Both their schemes rely on a sparse-dense
decomposition technique, which differentiates dense and
sparse regions of the network. Intuitively, they maintain
two sets of routing schemes, one for dense and one for
sparse regions, that are applied alternatively depending on the density of the region of the network considered. The sparse-dense decomposition technique is
rather involved and does not yield good stretch factors
for the name-independent case, nor does it yield optimal stretch if we use optimal-size routing labels in the
name-dependent case. In this paper, we define a new
unifying and simple O(log n)-level hierarchical decomposition technique based on ball packing, which “efficiently” covers dense and sparse regions of the network
alike. We carefully design data structures to combine
the two hierarchical network decompositions used by
our algorithms, namely r-nets and ball packings, in order to eliminate the storage dependence on ∆. The
combined hierarchies also eliminate the need for differentiated treatment of sparse and dense regions, allowing
for much simpler routing algorithms, and thus optimal
stretch in the name-independent case, and optimal routing label size in the name-dependent case.

2

Preliminaries

Let G = (V, E) be a connected, edge-weighted, undirected graph with n nodes, shortest-path metric d, aspect ratio ∆, and doubling dimension α. Let Bu (r)
be the closed ball of radius r around node u in G, i.e.
Bu (r) = {x ∈ V | d(u, x) ≤ r}, for any u ∈ V and r > 0.
Let r(u, j) be the radius such that |Bu (r(u, j))| = 2j ,
for any node u ∈ V and j ∈ [log n], where [x] denotes
the set {0, · · · , x − 1}.
Definition 2.1. (r-net) An r-net of a metric space
(V, d) is a subset Y ⊆ V such that any point in V is at
distance at most r from Y , and any two points in Y are
within distance at least r.
For any finite metric it is easy to show that such
an r-net exists and can be constructed greedily. The
following is a well-known result about r-nets:
Lemma 2.1. ([17]) Let Y be an r-net of (V,³d). ´For
α
′
any u ∈ V and r′ ≥ r, we have |Bu (r′ ) ∩ Y | ≤ 4rr
.
Let M = log ∆ + 1, and for each i ∈ [M ], let Yi
be a 2i -net of G. W.l.o.g. assume minu6=v d(u, v) = 1,
thereby ∆ = max d(u, v). Thus we have Y0 = V and
|Ylog ∆ | = 1. By connecting each node u ∈ Yi to its
nearest node in Yi+1 for 0 ≤ i < log ∆, we naturally
define a tree, called the netting tree of r-nets {Yi } and
denoted by T ({Yi }) (Note that some nodes of V may
appear multiple times in the netting tree). For each
u ∈ V , let {z(u, i) ∈ Yi : i ∈ [M ]} be a sequence of
nodes from the leaf u = z(u, 0) to the root z(u, log ∆)
in the netting tree T ({Yi }); we call this sequence the
zooming sequence of u.
Let N = log n, and for each j ∈ [N ], let Bj be a
ball packing as defined in Lemma 2.2.
Lemma 2.2. (Packing Lemma) For any j ∈ [N ],
there exists a ball packing Bj of G, i.e. a (maximal)
set of non-intersecting balls, such that
1. For any ball B ∈ Bj , |B| = 2j .
2. For any node u, there exists a ball B ∈ Bj centered
at c such that the radius of B is at most r(u, j) —
i.e. r(c, j) ≤ r(u, j) — and d(u, c) ≤ 2r(u, j).
Proof: Consider the set of balls {Bu (r(u, j)) | u ∈
V }. Greedily select balls from this set in the order of
shortest radius to longest to form a maximal set of nonintersecting balls Bj .
First, such a ball packing Bj has Property (1), since
|Bu (r(u, j))| = 2j for any node u ∈ V . Second, for
any node u ∈ V , if Bu (r(u, j)) ∈ Bj , Property (2)
is trivially satisfied. Otherwise, Bu (r(u, j)) intersects

941

some B ∈ Bj with center c. The radius of B is at
most r(u, j), i.e. r(c, j) ≤ r(u, j), since the balls are
selected from Bj by increasing radius. Moreover, since
B and Bu (r(u, j)) intersect and r(c, j) ≤ r(u, j), we
have d(u, c) ≤ 2r(u, j). Thus Property (2) follows.
3 Overview
In this section, we give a brief overview of our routing schemes. Our scale-free name-independent routing scheme uses our scale-free name-dependent routing
scheme of Theorem 1.2. Thus every time a node u wants
to communicate with a node v, u first uses v’s name
to retrieve the label of v and then routes to v using
the labeled routing scheme. Hence the main merit of
our scale-free name-independent routing scheme is an
efficient method for distributed storage and retrieval of
node labels from their names.
3.1 Ball packings for label storage. First, for
each j ≤ log n we maintain a packing Bj of balls of size
2j , as described in Section 2. For each ball B ∈ Bj , we
maintain a search tree on B to store the labels of nodes
in Bc (r(c, j + 2)) (where c is the center of B) according
to their names. Second, for each i ≤ log ∆ + 1, we
maintain a 2i -net Yi , and a single netting tree (defined
in Section 2) that stores all the necessary information to
route along the nets when possible. Finally, to link the
ball packings with the netting tree, each node u ∈ Yi
keeps the labels of nodes in the ball Bu (2i f ), for a
constant f depending on ǫ and α. If there is a packing
ball B ∈ Bj with center c for some j, such that B is
roughly contained in Bu (2i f ) and the search tree on
B has already stored the labels of nodes in Bu (2i f ),
then u just makes use of the search tree on B to index
the labels of nodes in Bu (2i f ) by maintaining a link
to the center c, instead of maintaining a search tree on
ball Bu (2i f ). Otherwise, we maintain a search tree on
Bu (2i f ) to store the labels of nodes in Bu (2i f ). In this
way, we avoid storing too many search trees, and achieve
scale-free storage. Thus for both cases, we can define a
local search procedure at u based on the search tree to
find labels of nodes in Bu (2i f ). Finally, the routing
algorithm finds the label of the destination node just
by recursively calling a local search procedure along the
zooming sequence of the source node.

from source u to destination v given the label l(v). If u
keeps the range information of nodes in Bu (2i+f ) ∩ Yi
(the ith ring of u), and there exists x ∈ Bu (2i+f ) ∩ Yi
with l(v) ∈ Range(x, i) and d(u, x) + d(x, v) is roughly
equal to d(u, v), then for the minimal such index i, we
deliver the packet to the next hop on the shortest path
from u to x, and recurse. However, in order to achieve
scale-free storage, we only maintain range information
for O(log n) rings at each node. On the other hand,
for each j ≤ log n and each packing ball B ∈ Bj with
center c, we maintain a labeled tree-routing scheme (see
Lemma 5.1) for the shortest-path spanning tree on the
Voronoi region of c in the Voronoi diagram of centers of
balls in Bj . In addition, we use the search tree technique to store the local tree-routing labels and retrieve
them according to the global labels. Thus whenever we
cannot route to the destination because of lack of some
ring information at the current node, we can retrieve
the local tree label of v and use the local labeled treerouting scheme to deliver the packet to the destination.
4

A Scale-Free Name-Independent Routing
Scheme

4.1 Data Structures. Our scale-free name-independent routing scheme will use our scale-free name-dependent routing scheme given by Theorem 1.2 and described in Section 5 as the effective underlying labeled
routing scheme. For any u ∈ V , let n(u) denote the arbitrary original name, and let l(u) denote the label given
by the underlying labeled routing scheme. Thus every
time u wants to communicate with a node v given by
its name n(v), u uses n(v) to retrieve the label l(v) and
then routes to v using the underlying labeled scheme.
In Section 4.1.1, we define a search tree for a ball,
and provide procedures to store and retrieve (key, data)
pairs, where for our name-independent scheme we take
the original node name as the key and the node label
as the data. In Section 4.1.2, we define the hierarchical
data structures to maintain search trees.
4.1.1

Search Tree

Definition 4.1. (Search Tree) For any ǫ′ ∈ (0, 1)
and any ball Bc (r), let U0 = {c}, and for 1 ≤
′
i ≤ ⌊log(ǫ′ r)⌋ let Ui be a 2⌊log(ǫ r)⌋−i -net of Bc (r) \
S
0≤j<i Uj . Then the search tree on Bc (r), denoted by
T (c, r), is formed by connecting each node v ∈ Ui to its
closest node in Ui−1 for 0 < i ≤ ⌊log(ǫ′ r)⌋, and defining
the weight on each edge (u, v) equal to d(u, v) in G.

3.2 Labeled routing. For our scale-free labeled
routing scheme, we define the labels through an enumeration of the leaf set of the netting tree in a depthfirst-search traversal, producing labels of ⌈log n⌉ bits.
Moreover, for each node x ∈ Yi of the netting tree, we
have a label range Range(x, i) of nodes in the leaf set
From the definition of r-net, we can derive the
of the subtree rooted at x. Assume we want to route following bound on the height of the search tree T (c, r):

942

(4.1)

r+

′
r)
⌊log(ǫ
X ⌋

′
2⌊log(ǫ r)⌋−i ≤ (1 + ǫ′ )r.

i=1

Let the two endpoints of each virtual edge in the
search tree keep each other’s routing label, so that they
can communicate using the underlying labeled scheme.
Note that by Lemma
the maximum
³ 2.1 the root
´α has
¡ 1 ¢O(α)
4r
degree in the tree,
= ǫ′
. Hence
′
2⌊log(ǫ r)⌋−1
¡ 1 ¢O(α)
labels for the search tree.
each node keeps ǫ′
Next, given a search tree T (c, r) with m nodes, we
show how to store k (key, data) pairs in the search tree:

the tree with u ∈ Yi , v ∈ Yi+1 , and u 6= v, let u store the
label l(v). Since u ∈
/ Yi+1 (otherwise u = v), we have
u∈
/ Yj for any j > i and u ∈ Yj for any j ≤ i. Thus each
node u ∈ V stores at most one label of its parent in the
netting tree. Now each node u ∈ V can route packets
along its zooming sequence {z(u, i)} to the root.
Let Bj be a ball packing defined as in Lemma 2.2,
for each j ∈ [N ], and let f (ǫ) = 80
ǫ + 2. We maintain
search trees for two types of balls:

Algorithm 1 Store data in the search tree T (c, r)
1: sort all pairs according to their keys into a list
2: for each visited node during a depth-first traversal
of T (c, r) do
3:
pick k/m new pairs from the list of Step 1, and
store them at the current node
4: end for
5: Each node v in T (c, r) stores the range of keys of
the pairs stored in v and its descendants in T (c, r);
furthermore, v stores the range information of all its
children.

1. B ∈ Bi with center c, for i ∈ [N ]. The search tree
for B stores the pair (n(v), l(v)) for each node v in
the ball Bc (r(c, i + 2)).
2. Any ball Bu (2i f (ǫ)), for i ∈ [M ] and any u ∈
Yi , except those balls such that Bu (2i (f (ǫ) + 1))
contains a ball B ∈ Bj with center c for some
j ∈ [N ] and Bu (2i f (ǫ)) is contained in the ball
Bc (r(c, j+2)). The search tree for Bu (2i f (ǫ)) stores
the pair (n(v), l(v)) for each node v in the same ball.

We denote by
SNB the collection of balls of the first
type, i.e. B = j=0 Bj , and by B ′ the collection of
balls of the second type. Let S(u) be the set of indexes
i such that Bu (2i f (ǫ)) ∈
/ B ′ , i.e. S(u) = {i ∈ [M ] :
i
′
Bu (2 f (ǫ)) ∈
/ B }.
Finally, for i ∈ S(u), i.e. Bu (2i f (ǫ)) ∈
/ B ′ , there
exists a ball B ∈ Bj with center c for some j ∈ [N ]
i
i
Finally, we define a procedure that, given a key, such that B ⊆ Bu (2 (f (ǫ) + 1)) and Bu (2 f (ǫ)) ⊆
retrieves the corresponding data, or reports an error if Bc (r(c, j + 2)). Without loss of generality, assume such
j and then d(u, c) are both minimal, and let H(u, i)
such a pair (key, data) is not stored in T (c, r).
denote the ball B. Let u maintain a link to c by storing
its routing label l(c).
Algorithm 2 SearchT ree(key, T (c, r))
Therefore for any i ∈ [M ] and any u ∈ Yi , we can
1: u ← c
i
2: while there exists a child u′ of u in T (c, r) such index the routing labels of nodes in Bu (2 f (ǫ)) either by
i
i
the search tree for Bu (2 f (ǫ)) if Bu (2 f (ǫ)) ∈ B ′ , or by
that the range of u′ contains key do
the search tree for the ball H(u, i) if Bu (2i f (ǫ)) ∈
/ B′,
3:
go to node u′ and u ← u′
i.e. i ∈ S(u). Thus we define a search procedure for any
4: end while
5: if u stores the data corresponding to the given key i ∈ [M ] and any u ∈ Yi as follows:
then
Algorithm 3 Search(name, u, i)
6:
report the data
7: else
1: if Bu (2i f (ǫ)) ∈ B ′ then
8:
report error: there is no pair with key equal to
2:
call SearchT ree(name, T (u, 2i f (ǫ)));
the given key in T (c, r)
3: else
9: end if
4:
let c be the center of H(u, i), and r be the radius;
10: go back from u to c along the tree T (c, r)
5:
go to c from u by the labeled scheme;
6:
call SearchT ree(name, T (c, r));
7:
go back from c to u.
8: end if
4.1.2 Hierarchical Search Structures Let YM −1
be an arbitrary 2M −1 -net of G, that is, a singleton.
1
Recursively construct a 2i -net Yi by greedily expanding
Note that setting ǫ′ = f (ǫ)+1
from Equation 4.1 we
i
Yi+1 with nodes to obtain a 2 -net, for i from M − 2 to have that the cost of Search(name, u, i) is at most
0. Thus, we have YM −1 ⊆ YM −2 ⊆ · · · ⊆ Y0 . Consider
the netting tree T ({Yi }). For each virtual edge (u, v) of (4.2)
2 · (1 + ǫ′ ) · 2i (f (ǫ) + 1) = 2i+1 (f (ǫ) + 2),

943

d(v, c) ≤ 2r(v, j) ≤ 2i−1 . Since d(u, v) ≤ 2i f (ǫ), we
have d(u, c) + r(c, j) ≤ d(u, v) + d(v, c) + r(c, j) ≤
2i (f (ǫ) + 1). Thus B ⊆ Bu (2i (f (ǫ) + 1)). Since
Bu (2i f (ǫ)) ∈ B ′ , we have Bu (2i f (ǫ)) * Bc (r(c, j + 2)
(otherwise we use the search tree on B, instead of mainLemma 4.1. For any u ∈ V , the number of balls H(u, i) taining a search tree for Bu (2i f (ǫ))). Hence r(c, j +
i
for all i ∈ S(u) is at most 4 log n.
2) ≤ d(u,
d(v,
¡ c) + 2 f (ǫ). Thus
¢ c) + r(c, j + 2) ≤
i−1
2
+ d(u, v) + d(v, c) + 2i f (ǫ) ≤ 2i+b , since b =
Proof: We show that for any j ∈ [N ], the number of
⌈log(2f (ǫ) + 1)⌉. Therefore Bc (r(c, j + 2)) ⊆ Bv (2i+b ).
different balls H(u, i) ∈ Bj with i ∈ S(u) is at most 4.
Hence |Bv (2i+b )| ≥ 2j+2 > 2|Bv (2i−2 )|. The claim folBy contradiction, assume there is an index j ∈ [N ]
lows.
such that the number of different balls H(u, i) ∈ Bj
with i ∈ S(u) is at least 5. Let i0 < i1 < i2 < i3 < i4
Lemma 4.3. The routing information at each node has
be five of these indices. By definition of H(u, ik ), we
( 1ǫ )O(α) log3 n bits.
have H(u, ik ) ⊆ Bu (2ik (f (ǫ) + 1)). Since ik < i4 for
k < 4, we have Bu (2ik (f (ǫ) + 1)) ⊂ Bu (2i4 f (ǫ)). Thus
Proof:
By Theorem 1.2, the underlying labeled
Bu (2i4 f (ǫ)) contains balls H(u, ik ) for k = 0, . . . , 3.
routing scheme requires ( 1ǫ )O(α) log3 n bits at each node.
Since all balls H(u, ik ) in Bj are non-intersecting,
Each node maintains at most one label of its parent
we have |Bu (2i4 f (ǫ))| > 2j+2 . Thus Bu (2i4 f (ǫ)) *
node in the netting tree.
Bc (r(c, j + 2)), where c is the center of H(u, i4 ). This
By Lemma 4.1, the total storage for maintaining
contradicts the definition of H(u, i4 ).
links to the centers of balls H(u, i), for all i ∈ S(u), is
2
Lemma 4.2. For any v ∈ V , the number of search trees O(log n) bits.
The storage required to maintain search trees and
that contain v is at most ( 1ǫ )O(α) log n.
store routing labels in them can be bounded as folProof: First, for each j ∈ [N ], the packing balls in lows. Since the maximum degree in any search tree is
Bj are disjoint, and so at most one of them contains ( ǫ1′ )O(α) = ( 1ǫ )O(α) , and the size of each range and routv. Thus the number of search trees for balls in B that ing label is O(log n), each node in a search tree maintains ( 1ǫ )O(α) log n bits of range information and link
contain v is no more than log n.
′
Second, consider the search trees for balls in B . information. Since each node in a search tree stores
Let b = ⌈log(2f (ǫ) + 1)⌉. For every v ∈ V , define a at most 4 (name, label) pairs, it stores O(log n) bits of
sequence of indexes R(v) = {i ∈ [M ] : |Bv (2i+b )| ≥ data. Since the number of search trees containing any
2|Bv (2i−2 )|}. The following claim, whose proof we defer node is at most ( 1ǫ )O(α) log n by Lemma 4.2, the total
to the full version of the paper [19], bounds the size of storage for maintaining search trees and routing labels
at each node is no more than ( 1ǫ )O(α) log2 n.
R(v).
Thus each node stores ( 1ǫ )O(α) log3 n bits of routing
Claim 4.1. |R(v)| ≤ (2 + b) log n.
information.
The next claim relates R(v) to the search trees for balls
in B ′ that contain v, thereby bounding the number of 4.2 Routing Algorithm Now we are ready to describe the scale-free name-independent routing scheme,
such trees.
and prove its performance bounds.
Claim 4.2. If there is a ball Bu (2i f (ǫ)) ∈ B ′ with
The routing procedure is described in Algorithm 4.
u ∈ Yi that contains v, then i ∈ R(v).
Assume that a source node u wants to send a message
For each i ∈ R(v), the number of u ∈ Y such that to a destination node v, given the name n(v) of v.
where we ignore the cost due to the stretch of the
underlying labeled scheme–this will be considered later.
The following two lemmas show the storage at each
node is scale-free.

i

v ∈ Bu (2i f (ǫ)) is at most (4f (ǫ))α by Lemma 2.1. Now
by Claim 4.1 and 4.2, the number of search trees for balls
in B ′ that contain v is at most (2 + b) log n · (4f (ǫ))α =
( 1ǫ )O(α) log n. The lemma follows.
Proof of Claim 4.2: Let j be the index such
that 2j ≤ |Bv (2i−2 )| < 2j+1 . Thus r(v, j) ≤ 2i−2 . We
will show that |Bv (2i+b )| ≥ 2j+2 > 2|Bv (2i−2 )|, and
therefore i ∈ R(v).
By Lemma 2.2, there exists a ball B ∈ Bj with
center c such that r(c, j) ≤ r(v, j) ≤ 2i−2 and

944

v
Bz(u,j) (2j f ())
z(u,j-1)

u

z(u, j)

Figure 1: Name-Independent Routing Scheme

Algorithm 4 Name-Independent Routing
Since f (ǫ) = 80
ǫ + 2, the total routing cost together
with the (1 + ǫ/20) of the underlying labeled scheme is
1: i ← 0
no more than
2: repeat
µ
¶
3:
go to z(u, i) using the underlying routing scheme
40
9+
d(u, v)(1 + ǫ/20)
4:
perform a local search at z(u, i) by calling the
f (ǫ) − 2
procedure Search(n(v), z(u, i), i)
´
³
ǫ
(1 + ǫ/20) d(u, v)
= 9+
5:
i←i+1
2
6: until the routing label l(v) of v is found
≤ (9 + ǫ)d(u, v).
7: go to v from z(u, i) using the underlying labeled
routing scheme.
Proof of Theorem 1.1: The bounds on stretch and
storage requirement at the nodes follow from Lemma 4.4
Figure 1 illustrates an execution of Algorithm 4. and 4.3, respectively. The packet header size of our
The procedure Search(n(v), z(u, i), i) searches for the name-independent routing scheme is dominated by the
label of v in the ball Bz(u,i) (2i f (ǫ)) repeatedly, until packet header size of the underlying labeled scheme, i.e.
at level j it finds the routing label of v in the ball O(log2 n/ log log n) bits, by Theorem 1.2.
Bz(u,j) (2j f (ǫ)). Then we call the underlying labeled
routing scheme to route to v from z(u, j). Fix the 5 A Scale-Free Labeled Routing Scheme
(1 + ǫ/20)-stretch required of the underlying labeled
5.1 Data Structures In this section, we define the
scheme. Then we have the following stretch lemma.
data structures needed for our scale-free labeled routing
Lemma 4.4. (Stretch) For any source node u and scheme.
any destination node v in G, the total routing cost of
First, we have two hierarchical data structures as
our algorithm is no more than (9 + ǫ)d(u, v).
defined in Section 2: r-nets {Yi : i ∈ [M ]} and ball
packings {Bj : j ∈ [N ]}.
Proof:
For simplicity, we factor in the (1 + ǫ/20)
Second, let l : V → [n] be the enumeration of
stretch of the underlying labeled scheme at the end
the leaves in a depth-first traversal of the netting tree
of the proof. As illustrated in Figure 1, let j be
T ({Yi }). Since the leaf set of T ({Yi }) is Y0 = V , we take
the index of the level at which v’s routing label is
l as the label function for our scale-free labeled routing
found. Thus the routing cost from u to v consists of
P
scheme. For each i ∈ [M ], and each node u ∈ Yi ,
j
i−1), z(u, i)) for the cost along the zooming
i=1 d(z(u,
let Range(u, i) ∈ [n] × [n] be the range of labels of
Pj
sequence, i=0 2i+1 (f (ǫ)+2) for the search procedures, nodes v such that z(v, i) = u, i.e. each v is a leaf
and d(z(u, j), v) for the cost from z(u, j) to v. Since of the subtree rooted at u in T ({Y }). Observe that
Pj
i
j+1
by the definition Range(u, i) requires 2 ⌈log n⌉ bits, and by the properties
i=1 d(z(u, i − 1), z(u, i)) ≤ 2
Pj
of r-nets and d(z(u, j), v) ≤ d(u, v) + i=1 d(z(u, i − of depth-first-search, we have l(v) ∈ Range(u, i) iff
1), z(u, i)) ≤ d(u, v)+2j+1 by triangle inequality, we find u = z(v, i).
Pj
that the total cost is at most i=1 d(z(u, i−1), z(u, i))+
Third, let the ith ring of u be the node set Xi (u) =
Pj
1
i+f
i+1
j+2
(f (ǫ) + 2) + d(z(u, j), v) ≤ 2 (f (ǫ) + 3) + Bu (2 ) ∩ Yi , for f = 8 + log ǫ , and R(u) = {i ∈
i=0 2
−a
[M ] : ∃j ∈ [N ], r(u, j) · 2
≤ 2i ≤ r(u, j)}, for
d(u, v).
1
Since v’s routing label is not found by a = 12 + log ǫ . Then each node u stores the range
Search(n(v), z(u, j−1), j−1), we have d(z(u, j−1), v) > information Range(x, i) for nodes x ∈ Xi (u) and i ∈
R(u), and the log n-bit information to identify which
f (ǫ)2j−1 . Then by triangle inequality, we have
neighbor of u is on the shortest path from u to x. Note
d(u, v) ≥ d(z(u, j − 1), v) − d(z(u, j − 1), u)
that |R(u)| = O( logǫ n ), and by Lemma 2.1, we have
j−1
X
|Xi (u)| = (1/ǫ)O(α) . Thus the range information stored
≥ d(z(u, j − 1), v) −
d(z(u, i − 1), z(u, i))
at each node is (1/ǫ)O(α) log2 n bits.
i=1
Finally, for each j ∈ [N ] and each B ∈ Bj with cen≥ 2j−1 (f (ǫ) − 2)
ter c, let V (c, j) be the Voronoi region of c in the Voronoi
diagram of centers of balls in Bj , i.e. V (c, j) = {u ∈ V :
and thus,
d(u, c) ≤ d(u, c′ ), for the center c′ of any ball in Bj },
8(f
(ǫ)
+
3)
d(u, v) + d(u, v) and let Tc (j) be a shortest path tree rooted at c and
2j+2 (f (ǫ) + 3) + d(u, v) ≤
f (ǫ) − 2
spanning V (c, j). For each tree Tc (j), we maintain a la¶
µ
beled routing scheme as follows, and let l(v; c, j) denote
40
d(u, v).
= 9+
the local routing label of v ∈ Tc (j):
f (ǫ) − 2

945

Lemma 5.1. ([13, 26]) For every weighted tree T on n
nodes, there exists a labeled routing scheme that, given
any destination label, routes optimally on T from any
source to the destination. The storage per node, the
label size, and header size are O(log2 n/ log log n) bits.
For each j ∈ [N ], each node u ∈ V stores the local
routing label l(c; c, j) of the center c such that c is the
center of a ball B ∈ Bj and u ∈ V (c, j). Note that
by Voronoi diagram properties, for each fixed j, the
trees Tc (j) are disjoint. Thus the local routing label
information at each node is O(log3 n/ log log n) bits.
In addition, we build a search tree T ′ (c, r(c, j)) as
in Definition 5.1 to store the (key, data) pairs of nodes
v ∈ Tc (j) ∩ Bc (r(c, j + 1)), where the key is the global
routing label l(v), and data is the local routing label
l(v; c, j) of v in the tree Tc (j). Thus given a key, i.e.
l(v), the SearchT ree(l(v), T ′ (c, r(c, j))) procedure, as
defined in Section 4.1.1, retrieves the label l(v; c, j) of v
along the shortest path of the search tree.
Definition 5.1. (Search Tree II) For any ball
Bc (r), the search tree II, denoted by T ′ (c, r), is modified from the search tree T (c, r) given by Definition 4.1,
where ǫ′ = 1/2:
(i)
of building the tree by iterating¥ i from
¦
¦
¥ Instead
1 to log 2r , we iterate only to min(⌈2 log n⌉ , log 2r ),
and define the weight on each of these edges (u, v) equal
to d(u, v) in G.
¥
¦
(ii) If ⌈2 log n⌉ < log 2r , for any u ∈ U⌈2 log n⌉ , let
V (u) be the Voronoi region of u in the Voronoi diagram
of a set of sites U⌈2 log n⌉ in Bc (r), i.e. V (u) = {x ∈
′
′
Bc (r) : d(x, u) ≤ d(x,
S u ), for any u ∈ U⌈2 log n⌉ }. Link
the nodes in V (u)\ 0≤j≤⌈2 log n⌉ Uj into a path, connect
it to u and define the weight on these edges equal to nr2 ,
for each u ∈ U⌈2 log n⌉ .
Note that the height of the search tree II T ′ (c, r) is
at most (1 + 12 )r + nr2 · n < 2r. The following lemma
shows how to link the endpoints of each virtual edge.

the number of nodes v ∈ Ui whose closest node
in
¥
¦ Ui−1
is u is 8α . Since 0 < i ≤ min(⌈2 log n⌉ , log 2r ) and
each next hop information has size no more than log n,
each node in Bc (r) requires at most (1/2)O(α) log2 n bits
of the next hop information.
Second,Slet a virtual edge link u and each node
in V (u) \ 0≤j≤⌈2 log n⌉ Uj for each u ∈ U⌈2 log n⌉ .
Let T (u) be a shortest path tree rooted at u and
spanning V (u), for each u ∈ U⌈2 log n⌉ . We maintain
a local labeled routing scheme given by Lemma 5.1
for the tree T (u). Let the two endpoints of each
of these virtual edges keep each other’s local label.
⌈log(r/2)⌉−⌈2 log n⌉
Since d(u,
≤ 2nr 2 for any v ∈
S v) ≤ 2
V (u) \ 0≤j≤⌈2 log n⌉ Uj , the routing cost along each
of these virtual edges is at most nr2 . By the Voronoi
diagram properties, the trees T (u) for all nodes u ∈
⌈2 log n⌉ are disjoint. Thus by Lemma 5.1, each node
in Bc (r) maintains O(log2 n/ log log n) bits of routing
information for the local labeled routing.
Lemma 5.3. (Storage) The routing information at
each node is at most ( 1ǫ )O(α) log3 n bits.
Proof: Each node maintains ( 1ǫ )O(α) log2 n-bit range
information, O(log3 n/ log log n)-bit local routing label
information, and ( 12 )O(α) log3 n-bit data structures and
O(log3 n/ log log n)-bit data storage for search trees.
Hence the routing information at each node is at most
( 1ǫ )O(α) log3 n bits.
5.2 Routing Algorithm Assume that a source node
u wants to send a packet to a destination node v given
its label l(v). The routing procedure is defined in
Algorithm 5.
x0
x1
u0 = u

u1

uk

uk+1

xk
ut

xt

1

v
c

Figure 2: Labeled Routing Scheme

Lemma 5.2. We can deliver packets along each virtual
Figure 2 illustrates the routing path from u to v,
edge of any search tree II T ′ (c, r), with cost at most the
which
consists of the path u0 → u1 → · · · → ut ,
2
weight of the edge, by maintaining (1/2)O(α) log n bits
then the routing path from ut to c, the search trail of
of data per node.
SearchT ree() in the ball Bc (r(c, j)), and the routing
path from c to v. Then the following two lemmas
Proof:
First, for any virtual edge (u, v),¥ where
¦
r
u ∈ Ui−1 , v ∈ Ui and 0 < i ≤ min(⌈2 log n⌉ , log 2 ), guarantee that the SearchT ree() procedure in the ball
let each node x on the shortest path from u to v Bc (r(c, j)) retrieves the local label l(v; c, j) sucessfully.
maintain the next hop information. Then u and v can Lemma 5.4. Let t ∈ [M ] and j ∈ [N ] be defined as in
communicate along the shortest path. Now we bound line 10. Then 64 r(u , j) < d(u , v) < r(ut ,j+1) .
t
t
ǫ
8
the memory requirement. Since u is the closest node in
Ui−1 to v and x is on the shortest path between u and v,
Proof: Omitted due to page limit. See the full
u is also the closest node in Ui−1 to x. By Lemma 2.1, version of the paper [19].

946

Algorithm 5 Labeled Routing Scheme
1: u0 ← u, and i−1 ← +∞
2: for k = 0 to +∞ do
3:
ik ← the minimal index in R(uk ) such that there
exists xk ∈ Xik (uk ) with l(v) ∈ Range(xk , ik ),
i.e. xk = z(v, ik )
4:
if ik ≤ ik−1 and d(uk , xk ) ≥ 2ik +f −1 − 2ik then
5:
uk+1 ← the next hop along the shortest path
from uk to xk , and go to uk+1
6:
else
7:
break
8:
end if
9: end for
10: t ← k. j ← the index in [N ] such that r(ut , j) ≤
2it < r(ut , j + 1), and c ← the center of a ball
B ∈ Bj such that ut ∈ V (c, j)
11: route to c using the labeled tree routing on Tc (j)
[the label l(c; c, j) is stored at ut ]
12: SearchT ree(l(v), T ′ (c, r(c, j))). [By Lemma 5.4 and
Lemma 5.5 this retrieves l(v; c, j).]
13: route to v using the labeled tree routing on Tc (j)

d(c, v) ≤ d(c, u) + d(u, v) < r(u, j + 1) − d(u, c) ≤
r(c, j + 1), i.e. v ∈ Bc (r(c, j + 1)). Therefore the
tree T ′ (c, r(c, j)) stores the local label l(v; c, j) of v, and
SearchT ree(l(v), T ′ (c, r(c, j))) retrieves it.
Lemma 5.6. (Stretch) For any source node u and
any destination node v in G, the total routing cost of our
labeled routing scheme is no more than (1 + ǫ)d(u, v).
Proof: First we bound the stretch of routing from
ut to v by 1 + ǫ/4. By Lemma 5.1, from ut to c and
from c to v we route along the shortest path. Since the
cost for SearchT ree(l(v), T ′ (c, r(c, j))) is bounded by
4r(c, j), the routing cost from ut to v is no more than
d(ut , c) + 4r(c, j) + d(c, v). Since d(ut , c) ≤ 2r(ut , j) and
r(c, j) ≤ r(ut , j) by Lemma 2.2 and 64
ǫ r(ut , j) < d(ut , v)
by Lemma 5.4, we have
d(ut , c) + 4r(c, j) + d(c, v)
d(ut , v)
2d(ut , c) + 4r(c, j) + d(ut , v)
≤
d(ut , v)
8r(ut , j)
≤ 1 + 64
≤ 1 + ǫ/4.
ǫ r(ut , j)

Lemma 5.5. For any node u, v and any j ∈
[N ], if 64
< d(u, v) < r(u, j + 1)/8,
Now consider the total stretch. If t = 0, the stretch
ǫ r(u, j)
SearchT ree(l(v), T ′ (c, r(c, j))) retrieves the local label is at most 1 + ǫ/4 as above.
l(v; c, j) of v successfully, where c is the center of a ball
If t > 0, then d(u, x0 ) > 2i0 +f −1 − 2i0 . By the
B ∈ Bj and u ∈ V (c, j).
routing
and Lemma 5.5, the routing cost is at
Palgorithm
t−1
most k=0 d(uk , uk+1 ) + (1 + 4ǫ )d(ut , v). As illustrated
Proof: First we show v ∈ V (c, j), i.e. v ∈ Tc (j) in Figure 2, by triangle inequality, we have
and then v ∈ Bc (r(c, j + 1)) so that the local label
t−1
l(v; c, j) is stored in the search tree T ′ (c, r(c, j)).
X
d(uk , uk+1 ) + d(ut , v)
For a contradiction, assume v ∈ V (c′ , j) where
k=0
c′ 6= c is a center of a ball B ′ ∈ Bj . Thus d(v, c′ ) ≤
t−2
X
d(v, c) ≤ d(v, u) + d(u, c). Let B = Bc (r(c, j)) ∈ Bj .
′
′
′
≤ d(u, x0 ) + (
d(xk , xk+1 ) + d(xt−1 , v))
Since B ∩ B = ∅, r(c, j) + r(c , j) < d(c, c ). Since
d(c, c′ ) ≤ d(c, v) + d(v, c′ ) ≤ 2d(v, c), we have

k=0

d(u, c′ ) + r(c′ , j) ≤(d(u, v) + d(v, c′ )) + (d(c, c′ ) − r(c, j))
≤d(u, v) + 3d(v, c)
≤4d(u, v) + 6r(u, j) <

2
r(u, j + 1),
3

≤ d(u, x0 ) + 2i0 +1 .
The last inequality follows since xk =¡z(v, ik ). Thus the
¢
routing cost is no more than (1 + 4ǫ ) d(u, x0 ) + 2i0 +1 .
Since d(u, v) ≥ d(u, x0 ) − d(x0 , v) ≥ d(u, x0 ) − 2i0 +1 and
f = 8 + log 1ǫ , then

where the last inequality follows from 64
d(u, x0 ) + 2i0 +1
d(u, x0 ) + 2i0 +1
ǫ r(u, j) <
≤
2
′
d(u, v) < r(u,j+1)
.
Thus
B
⊂
B
(
r(u,
j
+
1)).
u 3
d(u, v)
d(u, x0 ) − 2i0 +1
8
Since d(u, c) + r(c, j) ≤ 3r(u, j) < 32 r(u, j + 1),
2i0 +2
≤ 1 + i0 +f −2 ≤ 1 + ǫ/4.
B ⊂ Bu ( 32 r(u, j + 1)). Thus the ball Bu ( 32 r(u, j + 1))
2
with size less than 2j+1 contains two disjoint balls B
′
j
and B , both of size 2 , contradicting the assumption
Therefore the routing cost is no more than (1 +
and implying v ∈ Tc (j).
ǫ/4)2 d(u, v) < (1 + ǫ)d(u, v).
Now we show that v ∈ Bc (r(c, j + 1)). Since balls
Bc (r(c, j + 1)) and Bu (r(u, j + 1)) have the same size, Proof of Theorem 1.2:
The bounds on stretch
we have d(u, c) + r(c, j + 1) ≥ r(u, j + 1). Hence and storage at the nodes follow from Lemmas 5.6 and

947

of the 16th Annual ACM-SIAM Symposium on Discrete
5.3, respectively. The packet header size of our labeled
Algorithms, pages 762–771, 2005.
routing scheme is dominated by the packet header
size of the underlying labeled tree-routing scheme, i.e. [11] L. Cowen. Compact routing with minimum stretch.
Journal of Algorithms, 38:170–183, 2001.
O(log2 n/ log log n) bits, by Lemma 5.1.

6

Future work

Since 9 is the asymptotically optimal stretch for nameindependent compact routing schemes on networks with
low doubling dimension [18], and (2k+1)-stretch routing
schemes for general graphs require Ω((n log n)1/k )-bit
memory per node [4], we see as the main open problem
a relaxed version of the routing problem, or routing with
slack : can an even better stretch be achieved if we allow
a small constant fraction of nodes to have large memory,
or a small constant fraction of source-destination pairs
to incur large routing stretch?
References
[1] I. Abraham and C. Gavoille. Object location using path
separators. In Proceedings of the 25th Annual ACM
Symposium on Principles of Distributed Computing.
ACM Press, July 2006.
[2] I. Abraham, C. Gavoille, A. V. Goldberg, and
D. Malkhi. Routing in networks with low doubling
dimension. In 26th ICDCS. IEEE Computer Society
Press, July 2006. To appear.
[3] I. Abraham, C. Gavoille, and D. Malkhi. Routing with
improved communication-space trade-off. In Proceedings of the 18th International Conference on Distributed
Computing, volume 3274 of Lecture Notes in Computer
Science, pages 305–319, 2004.
[4] I. Abraham, C. Gavoille, and D. Malkhi. On spacestretch trade-offs: Lower bounds. In Proceedings of the
1ith Annual ACM Symposium on Parallel Algorithms
and Architecture. ACM Press, July 2006.
[5] I. Abraham, C. Gavoille, and D. Malkhi. On spacestretch trade-offs: Upper bounds. In Proceedings of the
18th Annual ACM Symposium on Parallel Algorithms
and Architecture. ACM Press, July 2006.
[6] I. Abraham and D. Malkhi. Name independent routing
for growth bounded networks. In Proceedings of the
17th Annual ACM Symposium on Parallel Algorithms
and Architecture, pages 49–55, 2005.
[7] I. Abraham, D. Malkhi, and O. Dobzinski. Land:
stretch (1 + ǫ) locality-aware networks for DHTs. In
Proceedings of the 15th Annual ACM-SIAM Symposium
on Discrete Algorithms, pages 550–559, 2004.
[8] B. Awerbuch and D. Peleg. Sparse partitions. In
Proceedings of the 31st Annual IEEE Symposium on
Foundations of Computer Science, pages 503–513, 1990.
[9] B. Awerbuch and D. Peleg. Routing with polynomial communication-space trade-off. SIAM J. Discret.
Math., 5(2):151–162, 1992.
[10] H. T.-H. Chan, A. Gupta, B. Maggs, and S. Zhou. On
hierarchical routing in doubling metrics. In Proceedings

[12] T. Eilam, C. Gavoille, and D. Peleg. Compact routing
schemes with low stretch factor. Journal of Algorithms,
46:97–114, 2003.
[13] P. Fraigniaud and C. Gavoille. Routing in trees. In 28th
ICALP, volume 2076 of LNCS, pages 757–772. Springer,
2001.
[14] C. Gavoille. Routing in distributed networks: Overview
and open problems. ACM SIGACT News - Distributed
Computing Column, 32(1):36–52, 2001.
[15] C. Gavoille and N. Hanusse. Compact routing tables
for graphs of bounded genus. In 26th ICALP, volume
1644 of LNCS, pages 351–360. Springer, July 1999.
[16] C. Gavoille and D. Peleg. Compact and localized
distributed data structures. Journal of Distributed
Computing, 16:111–120, 2003.
[17] A. Gupta, R. Krauthgamer, and J.R.Lee. Bounded
geometries, fractals and low-distortion embeddings. In
Proceedings of the 44th Annual IEEE Symposium on
Foundations of Computer Science, pages 534–543, 2003.
[18] G. Konjevod, A. Richa, and D. Xia. Optimal-stretch
name-independent compact routing in doubling metrics.
In Proceedings of the 25th Annual ACM Symposium on
Principles of Distributed Computing, 2006.
[19] G. Konjevod, A. Richa, and D. Xia.
Compact routing in networks of low doubling dimension. Manuscript available at http://thrackle.eas
.asu.edu/papers/compact.ps.gz, 2007.
[20] D. Peleg. Distributed Computing: A Locality-Sensitive
Approach. Monographs on Discrete Mathematics and
Applications. SIAM, Philadelphia, 2000.
[21] A. Slivkins. Distance estimation and object location
via rings of neighbors. In Proceedings of the 24th
Annual ACM Symposium on Principles of Distributed
Computing, pages 41–50, 2005.
[22] A. Slivkins. Distributed approaches to triangulation
and embedding. In Proceedings of the 16th Annual
ACM-SIAM Symposium on Discrete Algorithms, pages
640–649, 2005.
[23] I. Stoica, R. Morris, D. Karger, F. Kaashoek, and
H. Balakrishnan.
Chord: A scalable peer-to-peer
lookup service for internet applications. In In Proceeding of the 2001 ACM SIGCOMM, 2001.
[24] K. Talwar. Bypassing the embedding: algorithms for
low dimensional metrics. In Proceedings of the 36th
Annual ACM Symposium on Theory of Computing,
pages 281–290, 2004.
[25] M. Thorup. Compact oracles for reachability and
approximate distances in planar digraphs. Journal of
the ACM, 51(6):993–1024, 2004.
[26] M. Thorup and U. Zwick. Compact routing schemes.
In Proceedings of the 13th Annual ACM Symposium on
Parallel Algorithms and Architecture, pages 1–10, 2001.

948

Compact Routing with Slack in Low Doubling Dimension
Goran Konjevod

∗

Andréa W. Richa

†

Donglin Xia

‡

CSE Department
Arizona State University
Tempe, AZ 85287, USA

CSE Department
Arizona State University
Tempe, AZ 85287, USA

CSE Department
Arizona State University
Tempe, AZ 85287, USA

goran@asu.edu

aricha@asu.edu
Hai Yu

dxia@asu.edu

CS Department
Duke University
Durham, NC 27708, USA

fishhai@cs.duke.edu

ABSTRACT

Categories and Subject Descriptors

We consider the problem of compact routing with slack in
networks of low doubling dimension. Namely, we seek nameindependent routing schemes with (1 + ) stretch and polylogarithmic storage at each node: since existing lower bound
precludes such a scheme, we relax our guarantees to allow for
(i) a small fraction of nodes to have large storage, say size of
O(n log n) bits, or (ii) a small fraction of source-destination
pairs to have larger, but still constant, stretch.
In this paper, given any constant  ∈ (0, 1), any δ ∈
Θ(1/ polylog n) and any connected edge-weighted undirected
graph G with doubling dimension α ∈ O(log log n) and arbitrary node names, we present

C.2.2 [Computer Communication Networks]: Network
Protocols - Routing protocols; E.1 [Data Structures]: Distributed data structures; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms
and Problems - Computations on discrete structures, Routing and Layout; G.2.2 [Discrete Mathematics]: Graph
Theory - Graph algorithms, Graph labeling, Network problems

General Terms
Algorithms, Performance, Theory

1. a (1 + )-stretch name-independent routing scheme for
G with polylogarithmic packet header size, and with
(1 − δ)n nodes storing polylogarithmic size routing tables each and the remaining δn nodes storing O(n log n)bit routing tables each.

Keywords
Compact Routing, Doubling Dimension, Name-independent

1.

INTRODUCTION

A routing scheme is a distributed algorithm that allows
any source node to deliver packets to any destination node
in a network. In this paper, we model a network as an
undirected graph with arbitrary given node names and edge
weights. A name-independent routing scheme uses only the
given node names to route, while a name-dependent (or labeled) routing scheme allows the designer to label (or rename) the nodes to embed additional information that will
aid in routing, such as topological information.
One of the fundamental trade-oﬀs in routing is between
stretch and space. The stretch of a routing scheme is the
maximum ratio of the length of a routing path between a
pair of nodes s and t according to the scheme, to the length
of a shortest s-t path, over all pairs s, t. The space requirement of a scheme refers to the size of routing tables maintained at each node and the size of the packet headers used
by the scheme. A routing scheme is compact if the routing
table at each node and every packet header have size at most
a polylogarithmic function of the number of nodes.
Compact routing schemes are desirable for scalability. For
general graphs, however, any (name-independent or namedependent) routing scheme with Õ(n1/k )1 -bit routing tables requires Ω(k) stretch [16, 19, 5], and hence no com-

2. a name-independent routing scheme for G with polylogarithmic storage and packet header size, and with
stretch (1 + ) for (1 − δ)n source nodes and (9 + ) for
the remaining δn source nodes.
These results are to be contrasted with our lower bound from
PODC 2006, where we showed that stretch 9 is asymptotically optimal for name-independent compact routing schemes
in networks of constant doubling dimension.
∗Supported in part by NSF grant CCR-0209138.
†Supported in part by NSF CAREER grant CCR-9985284.

‡Supported in part by NSF grant CCR-0209138, NSF CAREER grant CCR-9985284, and ASU GPSA research grant.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’07, August 12–15, 2007, Portland, Oregon, USA.
Copyright 2007 ACM 978-1-59593-616-5/07/0008 ...$5.00.

1

71

Õ() denotes complexity similar to O() up to polylog factors.

pact routing scheme of constant stretch exists for general
graphs. Moreover, every name-independent routing scheme
with less than (n/(9k))1/k bits of space requirement per
node has average stretch at least Θ(k) [5]. On the other
hand, optimal (9 + )-stretch compact name-independent
routing schemes exist for networks of low doubling dimension [14]. In fact, we showed that stretch 9 is asymptotically
optimal for name-independent compact routing in networks
with constant doubling dimension [13]. The doubling dimension of a network is the least value α such that any ball of
radius r can be covered by at most 2α balls of radius r/2
(the ball of radius r centered at c is the set of nodes within
distance at most r of c).
In this paper, we consider the problem of achieving (1+)stretch name-independent compact routing schemes for networks of low doubling dimension if we are allowed slightly
relaxed guarantees either in memory or in stretch—i.e., we either allow a 1/ polylog n fraction of the nodes to have larger
than polylogarithmic storage size, or allow the stretch between a 1/ polylog n fraction of all pairs of nodes to be larger
than (1 + ) (albeit still constant). If we compare our results
with the lower bound of 9 for stretch in networks of constant doubling dimension, we have managed to improve the
stretch of our scheme considerably by allowing a small relaxation of the guarantees. Relaxed guarantees have also been
recently considered in the context of metric embeddings [12,
1, 2, 3]. Our main results are described in the following two
theorems:

by the previous two papers and presented a compact routing scheme with (9 + )-stretch and polylogarithmic storage
that is scale-free (independent of the normalized network
diameter).
There are several lower bound results on compact routing schemes [16, 19, 5, 13]. The strongest lower bound result for name-independent compact routing schemes in networks of low doubling dimension [13] shows that any name2
independent scheme with o(n(/60) )-bit storage has stretch
at least 9 −  for a tree with doubling dimension no more
than 6 − log  and normalized diameter O(21/ n), for any
 ∈ (0, 8).

2.

OVERVIEW

In this section, we give a brief overview of our compact
name-independent routing schemes with slack.
First, our compact name-independent routing schemes use
a (1 + )-stretch compact name-dependent routing scheme
as their underlying labeled routing scheme. Thus given the
name of the destination, a source node ﬁrst retrieves the label of the destination according to its name, and then routes
to the destination using the labeled routing scheme. Hence
the main merit of our name-independent routing schemes is
an eﬃcient method for distributed storage and retrieval of
node labels from their names.
Second, we maintain a hierarchy of r-nets (see Section 3
for the formal deﬁnition), which has been widely used in
context of compact routing schemes [7, 17, 4, 13]. For our
schemes, in order to achieve 1 +  stretch, a natural idea is
that, for any node u in the 2i -net, a search tree is maintained
on the ball Bu (2i f ) to store routing information of nodes
in the ball Bu (2i f /), where f can be any ﬁxed constant
and Bu (r) denotes the set of nodes within r distance to
the center u. By using a typical search tree technique, we
provide a search procedure on ball Bu (2i f ) with cost 2i+1 f
to either retrieve the routing label of a given node if the
node is in the ball Bu (2i f /), or report error if it is not
in the ball Bu (2i f /). Thus the routing algorithm starts
at the source node and repeatedly applies search procedure
from level i = 0 until the routing label of the destination is
founded. Then it uses the underlying labeled routing scheme
to route the message to the destination. The overall stretch
is 1 + O(), since the cost for the retrieval of the routing
label is at most O() times the source-destination distance.
However, the ball Bu (2i f /) might contain so many nodes
that the search tree on Bu (2i f ) with polylog-size storage
per node cannot store the routing information of all nodes
in Bu (2i f /). We present a novel counting lemma (See
Lemma 4.2) to deal with the case of too many nodes in
Bu (2i f /). The counting lemma guarantees the number of
2i -net nodes u with |Bu (2i f /2 ) \ Bc (2i+2 )|/|Bu (2i f )| =
Ω(polylog n) is limited, where c is a center node of the
nearest packing ball to u (see Section 3 for a formal deﬁnition of ball packings); in some sense, Bc (2i+2 ) is a local
dense area within 2i f / distance to u , and |Bu (2i f /2 ) \
Bc (2i+2 )|/|Bu (2i f )| = Ω(polylog n) implies that there is
more than one dense area in the ball Bu (2i f /2 ). For those
r-net nodes u with more than one dense area in Bu (2i f /2 ),
we just relax guarantees either on storage or on stretch.
On the other hand,if |Bu (2i f /2 ) \ Bc (2i+2 )|/|Bu (2i f )| =
O(polylog n), let the search tree on Bu (2i f ) store the routing information of nodes in Bu (2i f /2 ) \ Bc (2i+2 ). Thus

Theorem 1.1 Given any δ ∈ Θ(1/ polylog n), any constant  ∈ (0, 1), and an edge-weighted undirected graph G
with n nodes and doubling dimension α ∈ O(loglog n), we
present a (1 + )-stretch name-independent routing scheme
for G with packet headers
of O(log2 n/ loglog n) bits, where


(1 − δ)n nodes store log2 n/O(α) (1/δ + log n) -bit routing
tables each, and the remaining δn nodes store O (n log n)-bit
routing tables each.

Theorem 1.2 Given any δ ∈ Θ(1/ polylog n), any constant
 ∈ (0, 1), and an edge-weighted undirected graph G with n
nodes and doubling dimension α ∈ O(loglog n), we present a
name-independent compact
 routing scheme for G with routing tables at each node of log4 n/(δO(α) ) bits, and packet
header size of O(log2 n/ loglog n) bits, and with stretch (1+)
for (1 − δ)n source nodes, and (9 + ) for the remaining δn
source nodes.

1.1 Other related work
In recent years, there has been a great amount of work on
eﬃcient network routing schemes. For a general review of
this topic, we refer to the book of Peleg [15] and the surveys
by Gavoille [9] and Gavoille and Peleg [10]. More recent
references can be found in [14].
In particular, there has been a lot of recent progress in
designing compact routing schemes for networks of low doubling dimension. Abraham et al. [4] achieved a compact
routing scheme with (large) constant stretch and storage
polylogarithmic in n, while Konjevod et al. [13] obtained an
asymptotically optimal (9 + )-stretch routing scheme with
storage of size polylogarithmic in n and the normalized network diameter. In [14], Konjevod et al. closed the gap left

72

if the search procedure on Bu (2i f ) cannot ﬁnd the destination’s label, the routing algorithm will go to node c to search
the routing information in the local dense area of Bc (2i+2 ).
In that case, the 1 + O() stretch is still guaranteed, since
the distance between u and c is basically at most 2i f /,
and since the destination is either in the local area of c, i.e.
Bc (2i+2 ), or outside of Bu (2i f /2 ).
Finally the counting lemma relies on the combination of
hierarchical r-nets and ball packings together with a novel
entropy analysis. We believe that this novel data structure
and technique may be of independent interest. In addition,
for the routing scheme with slack on stretch, we use the
techniques of our previous work in [14] to guarantee (i) 9 +
 stretch for all source-destination pairs and (ii) scale-free
storage (independent of the normalized network diameter).

Definition 3.2 (r-net) An r-net of a metric space (V, d) is
a subset Y ⊆ V such that any point in V is within distance at
most r from Y , and any two points in Y are within distance
at least r.
For example, V is a 1-net, and a single node v is a Δnet. Note that for any ﬁnite metric such an r-net exists
and can be constructed greedily. Let Bu (r) be the closed
ball of radius r around node u in G, i.e. Bu (r) = {x ∈
V | d(u, x) ≤ r}, for any u ∈ V and r > 0. Then the
following is a well-known result about the number of r-net
nodes in a ball:
Lemma 3.3 ([11]) Let Y be an r-net of(V, d).
 For any
 α
.
u ∈ V and r ≥ r, we have |Bu (r ) ∩ Y | ≤ 4rr

3. PRELIMINARIES

We now construct 2i -nets Yi for all i ∈ [log Δ]1 as follows:

Let G = (V, E) be a connected, edge-weighted, undirected graph with n nodes, shortest-path metric d, doubling dimension α ∈ O(loglog n), arbitrary log n-bit node
id (name), and arbitrary normalized diameter Δ, i.e. Δ =
max d(u, v)/ minu=v d(u, v). W.l.o.g., assume that the minimum weight of an edge is normalized to be one, and assume
that both n and Δ are some power of 2. Otherwise just let
n (and Δ) be the least power of 2 that is at least n (and Δ),
and the results of Theorem 1.1 and 1.2 still hold.

1. The Δ-net Ylog Δ is a singleton consisting of an arbitrary node in V .
2. The 2i -net Yi is constructed recursively by greedily
expanding Yi+1 with nodes to obtain a 2i -net, for i =
log Δ − 1, log Δ − 2, . . . , 0.
Following this construction, we have

3.1 Labeled routing

Ylog Δ ⊆ Ylog Δ−1 ⊆ · · · ⊆ Y0 = V.

Our name-independent routing schemes for G will use the
labeled routing scheme of [14] as the eﬀective underlying labeled routing scheme. For reference, we list the main results
achieved by this labeled routing scheme:

(1)

For any node u ∈ V , we deﬁne its zooming sequence as
follows: (i) Let u(0) = u; (ii) recursively deﬁne u(i) be the
nearest node to u(i−1) in Yi for i from 1 to log Δ (if there are
several such nearest nodes, use some arbitrary tie-breaking
mechanism, e.g., the least node id). By the deﬁnition of
r-net, for any u ∈ V and any i ∈ [log Δ], we have

Lemma 3.1 (Labeled Routing [14]) Given any weighted
undirected graph with n nodes, and doubling dimension α,
for any  ∈ (0, 1), there exists a (1 + )-stretch
labeled rout


i


ing scheme with log n-bit routing labels, ( 1 )O(α) log3 n 2

d(u(k − 1), u(k)) ≤

k=1

bit storage at each node and O(log n/ log log n)-bit packet
header.

i


2k < 2i+1 .

(2)

k=1

We can now form a tree T of V by building a path along
the zooming sequence 
u(0), u(1), · · · , u(log Δ), for each
node u ∈ V . For each virtual edge (u(i), u(i + 1)) of the tree
T with u(i) = u(i + 1), let u(i) store the label of u(i + 1).
By Eqn. (1) and the fact that u(i) = u(i + 1), we have that
/ Yk for all k > i. Thus the
u(i) ∈ Yk for all k ≤ i and u(i) ∈
node u(i) stores only one zooming sequence label, namely
that of its parent u(i + 1) in the tree T . Each node u ∈ V
can now route packets along its zooming sequence by using
the underlying labeled routing scheme.
We next introduce the concept of ball packing. Let ru (s)
denote the radius of the ball centered at u with size s, i.e.
|Bu (ru (s))| = s. The following lemma is proved in [14]:

For any u ∈ V , let id(u) denote the arbitrary original node
id of u, and let l(u) denote the label assigned to u by the
underlying labeled routing scheme. When a node u wants
to send a message to another node v in the network, it will
initiate a route request for id(v). The routing algorithm uses
id(u) to retrieve the label l(v) and then routes to v using
l(v) via the underlying labeled scheme. Hence the main
merit of our name-independent routing scheme is to present
an eﬃcient distributed method for storing and retrieving
the label of a node given its id. Since (1 + )(1 + O()) =
1 + O() for  < 1, we will omit the (1 + ) stretch caused
by the underlying labeled routing when analyzing our nameindependent schemes. Moreover, for simplicity, we will prove
Theorem 1.1 and 1.2 with stretch in terms of big-O of , i.e.
with stretch 1 + O() or 9 + O().

Lemma 3.4 (Ball Packing [14]) For any positive integer
s ≤ n, there exists an s-size ball packing B of G, i.e. a
(maximal) set of non-intersecting balls, such that

3.2

r-nets and ball packing.
We now introduce two hierarchical data structures that
will be used in our routing schemes: r-nets and ball packings. R-nets have been widely used in the context of routing
schemes and metric embeddings [18, 7, 17, 4, 13, 14], while
ball packings were used in the context of routing schemes [14,
8] and metric embeddings [6].

1. For any ball B ∈ B, |B| = s.
2. For any node u, there exists a ball B ∈ B centered
at c such that the radius of B is at most ru (s) (i.e.,
rc (s) ≤ ru (s)) and d(u, c) ≤ 2ru (s).
1

73

For any integer x > 0, let [x] denote the set {0, 1, · · · , x}.

Let B j be a 2j -size ball packing, for all j ∈ [log n], and Cj
be the set of centers of all balls in B j . For each node u ∈ V
and j ∈ [log n], let B(u, j) denote the nearest ball in B j to
u, i.e. maxv∈B(u,j) d(u, v) is minimized; let c(u, j) denote
the center of ball B(u, j). Node u maintains a link to c(u, j)
by storing its routing label, for all j ∈ [log n]. In addition,
the following lemma states that there is a packing ball near
a node.

4.

COUNTING LEMMA

In this section we present a counting lemma that combines
the properties of both r-nets and ball packings and is used to
limit number of nodes with slack on either storage or stretch.
To prove the counting lemma, we need the following result
whose proof is in the Appendix.
Lemma 4.1 Let (V, d) be an n-point metric space with minimum distance 1, and G = (V, E) be an undirected graph on
V . If for every node x ∈ V and every integer i ≥ 0, the number of edges (x, y) ∈ E incident to x with 2i ≤ d(x, y) < 2i+1
is no more than c, then we have |E| ≤ 3cn log n.

Lemma 3.5 For any u ∈ V , any real numbers r > r >
|Bu (r  )|
0, and any positive integer j ≤ log (4r
 /r)α ,there exists a
packing ball B ∈ B j of radius at most r and such that B ⊆
Bu (r + 3r). Moreover B(u, j) ⊆ Bu (r + 3r).

Lemma 4.2 (Counting) For any j ∈ [log n] and any i ∈
[log Δ], let D i,j be the set of nodes u ∈ Yi with |Bu (2i f ) \
Bc (2i+2 )| ≥ (4f )α 2j , where f > 1 is a constant and c =
c(u, j). We have

Proof: Let Y be an r-net of Bu (r ). By the deﬁnition of
r-net, the union of balls Bx (r) for all x ∈ Y covers the ball
Bu (r ). By Lemma 3.3, we have |Bu (r ) ∩ Y | ≤ (4r /r)α .
Thus by the pigeonhole principle, there exists a ball Bx (r),
for some x ∈ Y , such that |Bx (r)| ≥ |Bu (r )|/(4r /r)α . Thus
|Bu (r  )|
by Lemma 3.4, for any j ≤ log (4r
 /r)α , there exists a ball
B ∈ B j with center c such that d(x, c) ≤ 2r and the radius
of B is no more than r. Since d(u, x) ≤ r , we have B ⊆
Bu (r + 3r). Moreover since B(u, j) is the nearest ball to u
in B j , we have B(u, j) ⊆ Bu (r + 3r).

log Δ



|D i,j | = (O(f ))3α · (n log n/2j ).

i=0

Proof: By Lemma 3.5 with r = 2i f , r = 2i and |Bu (2i f )\
Bc (2i+2 )| ≥ (4f )α 2j , there exists a packing ball B  ∈ B j
with center c such that B  ⊆ Bu (2i (f + 3)), the radius of
B  is no more than 2i , and B(u, j) ⊆ Bu (2i (f + 3)). Thus
we have

3.3 Search tree.
Finally we give the deﬁnition of a search tree on any ﬁxed
ball, used to store and retrieve information.

d(c, c ) ≤ d(u, c) + d(u, c ) ≤ 2i+1 (f + 3).
Definition 3.6 (Search Tree) For any  ∈ (0, 1) and any
and for 1 ≤ i ≤ log(r) let Xi be
ball Bc (r), let X0 = {c}, 
an (r/2i )-net of Bc (r) \ 0≤j<i Xj . Then the search tree
on Bc (r), denoted by T (c, r), is formed by connecting each
node v ∈ Xi to its closest node in Xi−1 for 0 < i ≤ log(r),
and defining the weight on each edge (u, v) to be d(u, v).

Moreover, since we applied Lemma 3.5 while excluding the
nodes of Bc (2i+2 ), by a more careful argument, we have
d(c , c) ≥ 2i+2 − 2 · 2i = 2i+1 .

log(r)



2log(r)−i ≤ (1 + )r.

(5)

Fix any such c ∈ Cj , and denote it by φ(u, j).
Let E  = {(c(u, j), φ(u, j)) | ∀u ∈ D i,j , ∀i ∈ [log Δ]},
and deﬁne a graph G = (Cj , E  ). We now show that G
satisﬁes the condition of Lemma 4.1, i.e. for any node
x ∈ Cj , the number of edges (x, y) ∈ E  incident on x
with 2k ≤ d(x, y) < 2k+1 , for every integer k ≥ 0, is a
constant O(log f )O(f )α . First for a ﬁxed k, the number
of indices i ∈ [log Δ] such that there exists u ∈ Yi with
2k ≤ d(c(u, j), φ(u, j)) < 2k+1 is a constant O(log f ), since
2i+1 ≤ d(c(u, j), φ(u, j)) ≤ 2i+1 (f + 3) by Eqn. (4) and (5).
Second, for a ﬁxed x ∈ Cj and a ﬁxed i ∈ [log Δ], the number
of u ∈ Yi with either c(u, j) = x or φ(u, j) = x is a constant
O(f )α , by Lemma 3.3 with Eqn. (4) and u ∈ Yi . Hence
the number of edges (x, y) ∈ E  incident on x with 2k ≤
d(x, y) < 2k+1 is a constant O(log f )(O(f ))α . Therefore, by
Lemma 4.1 we have |E  | = O(log f )(O(f ))α |Cj | log|Cj |.
Furthermore, by a similar argument, one can
show that
Δ
for a ﬁxed edge (x, y) ∈ E  , the number of u ∈ log
i=0 D i,j
with (c(u, j), φ(u, j)) = (x, y) is at most O(log f )(O(f ))α .
Thus we obtain

From the deﬁnition of r-net, the height of the search tree
T (c, r) is at most
r+

(4)

(3)

i=1

Let the two endpoints of each virtual edge in the search
tree keep each other’s routing label, so that they can communicate using the underlying labeled scheme. Note that
has
{Xi } is a partition of Bc (r), and byLemma 3.3
α the root
O(α)
4r
= 1
.
the maximum degree in the tree, 2log(r)−1
 1 O(α)
labels for the search tree.
Hence each node keeps 
With typical search tree techniques, we can use our search
tree to store a collection of (key, data) pairs evenly in the
nodes of the tree, and provide a search procedure that starts
from and reports back to the root to search the data by
the given key with cost 2(1 + )r. For simplicity, we use
SearchT ree(key, Bc (r)) denote the search procedure if there
is a search tree on Bc (r). For detailed description of the
store and search procedures in search trees, please refer to
our previous paper [14].
For our name-independent schemes we take the node id
as the key and the node label as the data.

log Δ



|D i,j | ≤ O(log f )(O(f ))α |E  | < (O(f ))3α |Cj | log|Cj |.

i=0

Finally, since B j is a 2j -size packing, we have |Cj | = |B j | =
n/2j . The lemma then follows.

74

5. ROUTING SCHEME WITH SLACK ON
STORAGE

Algorithm 1 Name-independent routing with relaxed guarantees on storage
1: set i ← 0
2: if u(i) has the routing label l(v) of v then
3:
go to v from u(i) using the underlying labeled routing
scheme, and terminate
4: end if
5: if i ≥ log(ru(i) ) then
6:
go to c(u(i)) from u(i) using the underlying labeled
routing scheme
7:
get the label l(v) at c(u(i)) according to the id id(v)
8:
go to v from c(u(i)) using the underlying labeled routing scheme, and terminate
9: end if
10: go to u(i + 1) from u(i) using the underlying labeled
routing scheme
11: increase i by one, and go back to Step 2

In this section, we present our name-independent routing
scheme with relaxed storage guarantees.

5.1 Data structure
First, as deﬁned in Section 3, we have Yi , a 2i -net, for
i ∈ [log Δ], and each node u can route packets along its
zooming sequence.
Second, let j0 = log(log n/(δc1 α )), for a constant c1 = 7,
and only maintain the j0 -th level of ball packings B j0 ; for
simplicity, denote B = B j0 , C = Cj0 , and c(u) = c(u, j0 )
for all u ∈ V . Let each node in C have large memory and
store the (id, label) pairs of all the nodes in V . Since B is a
(log n/(δc1 α ))-size ball packing, we have
|B| = |C| ≤ nδc1 α / log n.

(6)

Third, let each node u ∈ V store the (id, label) pairs of
nodes in the ball Bu (ru ), where ru is the radius of the ball
Bu (ru ) with size log n/(δc2 α ) for a constant c2 = 9, i.e. ru
is an abbreviation of ru (log n/(δc2 α )).
Finally, for any node u ∈ V and i = log(ru ), if u ∈
Yi , let u store the (id, label) pairs of nodes in Bu (2i /2 ) \
Bc(u) (2i+2 ). Let D i,j0 as deﬁned in Lemma 4.2 with f =
1/2 , i.e. D i,j0 is the set of nodes u ∈ Yi with |Bu (2i /2 ) \
 Δ
Bc(u) (2i+2 )| ≥ (4/2 )α log n/(δc1 α ). Let D = log
i=0 D i,j0 .
By Lemma 4.2, we have
|D| = (O(1/2 ))3α · n log n/2j0 ≤ δn/2,
where the last inequality follows for j0 = log(log n/(δ
and c1 = 7.

Lemma 5.2 (Stretch) For any source node u and any destination node v in G, the total routing cost of Algorithm 1
is no more than (1 + O())d(u, v).
Proof: Let t be the index of the level at which v’s routing
label l(v) is found either at u(t) in Step 2, or at c(u(t)) in
Step 7.
Since the label of v is not found at the (t − 1)-th iteration, we have t − 1 < log(ru(t−1) ); otherwise it would
have been found in Step 7 at Iteration t − 1. Hence by
v∈
/ Bu(t−1) (ru(t−1) ), we have d(u(t − 1), v) ≥ 2t−1 /. Thus
by Eqn. (2) and the triangle inequality, we have

(7)
c1 α

))

Lemma 5.1 (Storage) In the above scheme, there are δn
nodes with O (n log n)-bit storage
 each, and (1 − δ)n nodes
2
O(α)
(1/δ + log n) -bit storage each.
with log n/

d(u, v) ≥ d(u(t − 1), v) −

t−1


d(u(i − 1), u(i))

i=1
t−1

≥2

Proof: There are two kinds of nodes with large storage:
the nodes in C storing the (id, label) pairs of all nodes in the
network, and the nodes u ∈ D storing the (id, label) pairs of
more than log n/(δO(α) ) nodes. By Eqn. (6) and (7), the
number of nodes in C ∪ D is no more than δn.
All the remaining nodes, the nodes in V \ (C ∪ D), store
at most log n/(δO(α) ) pairs (id, label) each.
Since both the node id and label have size log n bits, we
have δn nodes with
 storage each, and (1 −
 O (n log n)-bit

(8)

(1/ − O(1)).

Now consider the routing cost. First, if l(v) is found at
u(t) in Step 2, by Eqn. (2) and the triangle inequality, the
routing cost is at most
t


d(u(i − 1), u(i)) + d(u(t), v)

i=1

≤ d(u, v) + 2

δ)n nodes with log2 n/(δO(α) ) -bit storage each. Thus
together with the storage requirement in Lemma 3.1, the
lemma follows.

t


d(u(i − 1), u(i))

(9)

i=1
t+2

≤ d(u, v) + 2

≤ (1 + O())d(u, v),

5.2 Routing algorithm
where the last inequality follows from Eqn. (8).
Second, consider the case that l(v) is found at c = c(u(t))
in Step 7. Since the if condition of Step 5 is satisﬁed, we
have t ≥ i0 , for i0 = log(ru(t) ). Thus |Bu(t) (2i0 /)|/(4/)α =
log n/(δc2 α )/(4/)α ≥ log n/(δc1 α ) = 2j0 for c2 − c1 = 2.
Hence by Lemma 3.5 with r = 2i0 / and r = 2i0 , we have

Now we present our routing algorithm in full. Assume
that a source node u wants to send a packet to a destination
node v, given the id id(v) of v.
The routing procedure is described in Algorithm 1. The
algorithm searches for the label l(v) according to the id id(v)
along the zooming sequence of u, until it ﬁnds the label l(v)
of v stored at the current node u(i) or i ≥ log(ru(i) ). In
the latter case, the algorithm goes to c(u(i)), which stores
the (id, label) pairs of all nodes in the network, and then it
goes to v from c(u(i)) using the underlying labeled routing
scheme, and terminates.

d(u(t), c) ≤ 2i0 (1/ + 3).

(10)

Since l(v) is not found at u(t) in Step 2, we have v ∈
/
Bu(t) (2i0 /2 )\Bc (2i0 +2 ). Hence if v ∈ Bc (2i0 +2 ), by Eqn. (2)

75

|Bu (2i /)| · g1 . A Search procedure at u based on the
SearchT ree procedure is described in Algorithm 2. Since

and t ≥ i0 , the routing cost is at most
t


d(u(i − 1), u(i)) + d(u(t), c) + d(c, v)

i=1

	

≤ d(u, v) + 2

t


Algorithm 2 Search(id,u,i)
1: if j(u, i) = null then
2:
go to c(u, j(u, i)); SearchT ree(id, B(u, j(u, i))); and
report back to u
3: else
4:
SearchT ree(id, Bu (2i /))
5: end if



d(u(i − 1), u(i)) + d(c, v)

(11)

i=1
t+2

≤ d(u, v) + 3 · 2
≤ (1 + O())d(u, v),
where the last inequality follows by Eqn. (8). If v is not in
the ball Bu(t) (2i0 /2 ), by Eqn. (2) and the triangle inequality, we have
d(u, v) ≥ d(u(t), v) −
i0

2

t


d(u(i − 1), u(i))

i=1
t+1

≥ 2 / − 2

B(u, j(u, i)) ⊆ Bu (2i (1/ + 3)) if j(u, i) = null, the cost of
the Search procedure is
max(2 · 2i (1/ + 3), 2 · 2i /) = 2i+1 (1/ + O(1)).

Finally, for each i ∈ [log Δ] and each j ∈ [log n], let D i,j
be as deﬁned in Lemma 4.2 with f = 1/3 , i.e. D i,j is the
set of nodes u ∈ Yi with |Bu (2i /3 ) \ Bc (2i+2 )| ≥ (4/3 )α 2j .
Now we deﬁne the set of source nodes that we allow larger
stretch, i.e. 9 + O(), by D = {u | ∃i ∈ [log Δ] s.t. u(i) ∈
D i,j  for j  = log(|Bu(i) (2i /)| · g2 )}.

(12)

.

Thus by Eqn. (2), (10) and the triangle inequality, the routing cost is at most
t


d(u(i − 1), u(i)) + d(u(t), c) + d(c, v)

i=1

	

≤ d(u, v) + 2

t


Lemma 6.1 The number of nodes in D is at most δn.
 Δ
3 3α
j
Proof: First we have log
i=0 |D i,j | ≤ (O(1/ )) ·n log n/2
by Lemma 4.2, for each j.
Second, for each j ∈ [log n], let D j = {u | ∃i s.t. u(i) ∈
D i,j for log(|Bu(i) (2i /)| · g2 ) = j}. Since d(u, u(i)) ≤ 2i+1 <
2i / by Eqn. (2), for any ﬁxed node x ∈ Yi , we have |{u | u(i) =
 Δ
x}| < |Bx (2i /)|. Hence |D j | ≤ 2j /g2 · log
i=0 |D i,j |. Thus



d(u(i − 1), u(i)) + d(u(t), c)

(14)

(13)

i=1
t+2

≤ d(u, v) + 2
+ 2i0 +1 (1/ + O(1))
≤ (1 + O())d(u, v),
where the last inequality follows from Eqn. (8) and (12).
In conclusion, by Eqn. (9), (11) and (13), the lemma follows.

|D j | ≤ 2j /g2 · (O(1/3 ))3α · n log n/2j
≤ δn/ log n,

(15)

Proof of Theorem 1.1: The bounds on stretch follow
from Lemma 5.2 and the bounds on storage follow from
Lemma 5.1 and 3.1.

where the last inequality follows from g2 = log2 n/(δc2 α )
and c2 = 10. 
n
Since D = log
j=0 D j , we have |D| ≤ δn.

6. ROUTING SCHEME WITH SLACK ON
STRETCH

Lemma 6.2 For each node v ∈ V , the number of search
trees that contain v is log n/O(α) .

In this section, we present our name-independent routing
scheme with relaxed guarantees on stretch.

Proof: First, for each j ∈ [log n], the packing balls in B j
are disjoint, so at most one of them contains v. Thus the
number of search trees for balls in B j , for all j, that contain
v is no more than O(log n).
Second, consider the search trees for the ball Bu (2i /)
that contain v, for u ∈ Yi and j(u, i) = null. Let j be
an integer such that 2j ≤ Bv (2i ) < 2j+1 . By Lemma 3.4,
there exists a ball B ∈ B j such that B ⊆ Bv (3 · 2i ). Since
v ∈ Bu (2i /), we have B ⊆ Bu (2i (1/ + 3)), and therefore
B(u, j) ⊆ Bu (2i (1/ + 3)). Since we maintain a search tree
for Bu (2i /), we have Bu (2i /2 )  Bc (rc (2j g1 )), where c =
c(u, j). Thus rc (2j g1 ) ≤ d(u, c) + 2i /2 = 2i (1/2 + O(1/));
otherwise, Bu (2i /2 ) ⊆ Bc (rc (2j g1 )). Hence

6.1 Data structure
Let g1 = log2 n/(δc1 α ) and g2 = log2 n/(δc2 α ) for constants c1 = 14 and c2 = 10.
First as deﬁned in Section 3, we have Yi , a 2i -net, for
i ∈ [log Δ], and each node u can route packets along its
zooming sequence.
Second for each j ∈ [log n], we maintain a search tree on
each ball B ∈ B j with center c, storing the (id, label) pairs
of nodes in Bc (rc (2j g1 )).
Third, for each u ∈ Yi , i ∈ [log Δ], if there is an index j ∈
[log n] such that B(u, j) ⊆ Bu (2i (1/ + 3)) and Bu (2i /2 ) ⊆
Bc (rc (2j g1 )) as illustrated in Fig. 1(a), where c = c(u, j) is
the center of B(u, j), we use the search tree on B(u, j) to
index the routing labels of nodes in Bu (2i /2 ), and w.l.o.g.
assume such an index j is minimized, denoted by j(u, i) = j.
Otherwise set j(u, i) = null, and as in Fig. 1(b) we maintain a search tree on Bu (2i /), storing the (id, label) pairs of
nodes in Bu (2i /) and nodes in Bu (r(u, i))\Bc (2i+2 ), where
c = c(u, j  ) for j  = log(|Bu (2i /)| · g2 ), and r(u, i) is the
radius of Bu (r(u, i)) such that |Bu (r(u, i)) \ Bc (2i+2 )| =

d(v, c) + rc (2j g1 ) ≤ 2i (1/2 + O(1/)),

(16)

which implies Bc (rc (2j g1 )) ⊆ Bv (2i (1/2 + O(1/))). Therefore we have
|Bc (rc (2j g1 ))|
|Bv (2i (1/2 + O(1/)))|
≥
i
|Bv (2 )|
|Bv (2i )|
≥ g1 /2
= log2 n/(δO(α) ).

76

(17)

Bu (r(u, i))

Bu (2i /2 )
B(u, j)

u

Bu (2i /) Bc (2

u

j

Bc (rc (2 g1 ))

c

2i (1/ + 3)

c



i+2 )

2i (1/2 + O(1/))

(a) j(u, i) = null

(b) j(u, i) = null

Figure 1. (a) Node u uses the search tree on B(u, j) to index the routing information of nodes in Bu (2i /2 ), since B(u, j) ⊆
Bu (2i (1/ + 3)) and Bu (2i /2 ) ⊆ Bc (rc (2j g1 )), where j = j(u, i) and c = c(u, j). (b) Node u uses the search tree on
Bu (2i /) to index the routing information of nodes in Bu (r(u, i)) \ Bc (2i+2 ), where j  = log(|Bu (2i /)|g2 ), c = c(u, j  ), and
|Bu (r(u, i)) \ Bc (2i+2 )| = |Bu (2i /)|g1 .
Algorithm 3 Name-independent routing with relaxed guarantees on stretch
1: set i ← 0 {Initial Step}
2: set  ← 1 {Set the step increment}
3: Search(id(v), u(i), i) {Search at u(i) with cost 2i (1/ +
O(1)) as in Fig. 2}
4: if the label l(v) is found then
5:
go to v from u(i); and terminate the algorithm
6: end if
7: if j(u(i), i) = null and u(i) ∈
/ D i,j  , for j  =
log(|Bu(i) (2i /)| · g2 ) then
8:
set c ← c(u(i), j  ), and i ← i − (log(1/) − 3); go to
c from u(i)
9:
go to c (i ) along the zooming sequence of c ;
Search(id(v), c (i ), i ); and report back to c {Search
in Bc (2i+2 ) with cost O(2i ) as in Fig. 2(b)}
10:
if the label l(v) is found then
11:
go to v from c ; and terminate the algorithm
12:
end if
13:
go back to u(i) from c ; and set  ← log(1/) + 1 {Set
the step increment to log(1/) + 1}
14: end if
15: go to u(i + ) from u(i) along the zooming sequence
16: set i ← i + ; and repeat Step 2

By a simple calculation, for a ﬁxed node v, the number of
indices i that satisfy the above inequality is at most (1/2 +
log n
< O(log n/2 ). Furthermore, for a ﬁxed i,
O(1/)) · log(g
1 /2)
the number of nodes u ∈ Yi with Bu (2i /) containing v is no
more than (1/)O(α) by Lemma 3.3. Therefore the number
of search trees on balls Bu (2i /) containing v, for all u ∈ Yi
and all i ∈ [log Δ], is no more than log n/O(α) .
Lemma
6.3 (Storage)
In the above scheme, each node has


4
O(α)
a log n/(δ
) -bit routing table.
Proof:
Note that each node in any search tree stores
g1 = log2 n/(δO(α) ) pairs (id, label). By Lemma 6.2, the
number of search trees containing a ﬁxed node is log n/O(α) .
Since both the
 node id and label have size log n bits, each
node stores log4 n/(δO(α) ) -bit routing information.

6.2 Routing algorithm
Now we present our routing algorithm in full. Assume
that a source node u wants to send a packet to a destination
node v, given the id id(v) of v.
The routing procedure is described in Algorithm 3 and
illustrated in Fig. 2. The algorithm searches for the label
l(v) according to the id id(v) along the zooming sequence of
u, until it ﬁnds the label of v either in Step 3 or in Step 9.
Then it routes the packet to v using the underlying labeled
scheme by l(v).

Moreover since (20) + (19) + (21) = 2i+1 (1/2 + O(1/)) =
2
(1/+O(1)), we can consider the cost for Step 8,
9 and 13 at Iteration i as the cost of Eqn. (18) at Iteration
i + log(1/), which we skip by setting the step increment
 = log(1/) + 1 in Step 13.
Now let t be the index of the level at which v’s label l(v)
is found. Since l(v) is not found before the t-th iteration,
we have the following claim, whose proof is provided latter.
(i+log(1/))+1

Lemma 6.4 (Stretch) Algorithm 3 guarantees (1 + O())
stretch for (1 − δn)n source nodes, and (9 + O()) stretch for
the remaining δn source nodes.
Proof: First we bound the cost of each step (the Claim is
proved latter):

Claim 6.6 If the label of v is not found before the t-th iteration, we have
 t−1
/D
2 (1/2 − O(1)) if u ∈
(22)
d(u, v) ≥
2t−1 (1/ − O(1)) for all u.

Claim 6.5 For each iteration i, we have
The cost of Step 3 =

2i+1 (1/ + O(1)),
i

(18)

The cost of Step 9 =

O(2 ),

(19)

The cost of Step 8 ≤

2i (1/2 + O(1/))

(20)

The cost of Step 13 ≤

2i (1/2 + O(1/)).

(21)

Note that the label of v can be found either at u(t) in
Step 3, or at c in Step 9, where c = c(u(t), j  ) and j  =
log(|Bu(t) (2t /)| · g2 ).

77

Bu(i) (r(u(i), i))

Bu(i) (2i /2 )

Bu(i) (2i /)

B(u(i), j)

u(i)

u(i)

Bc (rc (2j g1 ))

③ c

Bc (2i+2 )
c (i )
13 c ⑨
○

⑧

③

(a) j(u(i), i) = null

(b) j(u(i), i) = null

Figure 2. (a) If j(u(i), i) = null, Step ③ is executed with cost 2i+1 (1/ + O(1)), where j = j(u(i), i) and c = c(u(i), j). (b) If
/ D i,j  , execute Step ⑧
j(u(i), i) = null, Step ③ is executed with cost 2i+1 (1/ + O(1)). If the label l(v) is not found and u(i) ∈
13 is executed with
with cost at most 2i (1/2 + O(1/)) and Step ⑨ with cost O(2i ). If the label l(v) is still not found, Step ○
cost at most 2i (1/2 + O(1/)). Here j  = log(|Bu(i) (2i /)| · g2 ), c = c(u(i), j  ), and i = i − (log(1/) − 3).
Hence since d(c , v) ≤ d(u, v)+2t+1 +d(u(t), c ), by Eqn. (18),
(19), (20) and (21), the routing cost is at most

If l(v) is found in Step 3, by Eqn. (2), (18) and the triangle
inequality, the routing cost is at most
t


t


2i+1 (1/ + O(1)) + d(u(t), v)

i=0

≤

t


2i+1 (1/ + O(1)) + d(u(t), c ) + O(2t ) + d(c , v)

i=0

2i+1 (1/ + O(1)) + d(u, v) +

i=0

t


≤ d(u, v) + 2t+1 (1/2 + O(1/))
≤ (1 + O())d(u, v),

d(u(i), u(i − 1))

i=1

(26)

= d(u, v) + 2t+2 (1/ + O(1)),
(23)

where the last inequality follows from Eqn. (25).
Since |D| ≤ δn by Lemma 6.1, the lemma follows from
Eqn. (23), (24) and (26).
Proof of Claim 6.5: The cost of Step 3 follows directly
from Eqn. (14).
By Eqn. (2), (14) and i = i − (log(1/) − 3) in Step 8, the
cost of Step 9 is at most

which implies (1 + O()) stretch for source nodes u ∈
/ D
and (9 + O()) stretch for the remaining source nodes u by
Claim 6.6.
Consider the case that l(v) is found at c in Step 9. Since
j(u(t), t) = null and l(v) is not found in Step 3, we have v ∈
/
Bu(t) (r(u(t), t)) \ Bc (2t+2 ). If v ∈ Bc (2t+2 ), by Eqn. (2),
(18) and (19), the routing cost is at most
t


(1/ + O(1)) + d(u, v),
(24)

which implies (1 + O()) stretch for source nodes u ∈
/ D
and (9 + O()) stretch for the remaining source nodes u by
Claim 6.6. In addition, since i = t − (log(1/) − 3) as in

Step 8, we have d(c , c (i )) + 2t+2 ≤ 2i /, i.e. Bc (2t+2 ) ⊆

Bc (i ) (2i /). Thus if v ∈ Bc (2t+2 ), the Search(id(v), c (i ), i )
subprocedure in Step 9 returns the label l(v).
Now consider the case of v ∈
/ Bu(t) (r(u(t), t)). Note that
the if condition in Step (7) is satisﬁed in order to run Step 9.
Thus u(t) ∈
/ D t,j  , i.e. we have |Bu(t) (2t /3 ) \ Bc (2t+2 )| ≤
3 α j
(4/ ) 2 ≤ (4/3 )α |Bu(t) (2t /)| · g2 ≤ |Bu(t) (2t /)| · g1 for
c1 − c2 = 4, which implies r(u(t), t) ≥ 2t /3 . Thus by
Eqn. (2) and the triangle inequality, we have
d(u, v) ≥ d(u(t), v) − 2t+1
≥ r(u(t), t) − 2t+1
t

i




d(c (k − 1), c (k)) + 2i

+1

(1/ + O(1)) = O(2i ). (27)

k=1

2i+1 (1/ + O(1)) + d(u(t), c ) + O(2t ) + d(c , v)

i=0
t+2

≤2



2

(25)

Now consider the cost of Step 8 and 13, i.e. d(u(i), c ).
Since the if condition at Step 7 is satisﬁed, we have j(u(i), i) =
null, which implies that Bu(i) (2i /2 ) is large relative to
Bu(i) (2i /). Speciﬁcally, by the pigeonhole principle and
a similar argument as in Lemma 3.5, we have
|Bu(i) (2i (1/2 + O(1/)))| ≥


Thus 2j = |Bu(i) (2i /)| · g2 ≤

|Bu(i) (2i /)|
g1 .
(4/)α
|Bu(i) (2i /)|·g1

(28)

(4/)α ·(4(1/2 +O(1/)))α
|Bu(i) (2i (1/2 + O(1/)))|/(4(1/2 + O(1/)))α , for c1 − c2

i
2

≤

=
4. Hence by applying Lemma 3.5 with r = 2 (1/ +O(1/))
and r = 2i , we have B(u(i), j  ) ⊆ Bu(i) (2i (1/2 + O(1/))).
Thus it follows Eqn. (20) and (21).
Proof of Claim 6.6:
First, if the (t − 1)-th iteration
is skipped because of setting  = log(1/) + 1 at Step 13,
the last iteration run before the t-th iteration is Iteration
t − (log(1/) + 1). By Eqn. (25), we have
d(u, v) ≥ 2t−(log(1/)+1) (1/3 − 2) = 2t−1 (1/2 − O(1/)).
(29)

3

≥ 2 (1/ − 2).

78

Second, consider the case where the (t − 1)-th iteration
is executed and the label l(v) is not found at this iteration.
If j(u(t − 1), t − 1) = null, the Search procedure in Step 3
guarantees v ∈
/ Bu(t−1) (2t−1 /2 ) by running the Step 2 of
Algorithm 2, which implies

[6] H. T.-H. Chan, M. Dinitz, and A. Gupta. Spanners
with slack. In Proceedings of the 14th European
Symposium on Algorithms, volume 4168 of Lecture
Notes in Computer Science, pages 196–207, 2006.
[7] H. T.-H. Chan, A. Gupta, B. Maggs, and S. Zhou. On
hierarchical routing in doubling metrics. In
Proceedings of the 16th ACM-SIAM Symposium on
Discrete Algorithms, pages 762–771, 2005.
[8] M. Dinitz. Compact routing with slack. In Proceedings
of the 26th ACM Symposium on Principles of
Distributed Computing, 2007.
[9] C. Gavoille. Routing in distributed networks:
Overview and open problems. ACM SIGACT News Distributed Computing Column, 32(1):36–52, 2001.
[10] C. Gavoille and D. Peleg. Compact and localized
distributed data structures. Journal of Distributed
Computing, 16:111–120, 2003.
[11] A. Gupta, R. Krauthgamer, and J.R.Lee. Bounded
geometries, fractals and low-distortion embeddings. In
Proceedings of the 44th IEEE Symposium on
Foundations of Computer Science, pages 534–543,
2003.
[12] J. Kleinberg, A. Slivkins, and T. Wexler.
Triangulation and embedding using small sets of
beacons. In Proceedings of the 45th IEEE Symposium
on Foundations of Computer Science, pages 444–453,
2004.
[13] G. Konjevod, A. Richa, and D. Xia. Optimal-stretch
name-independent compact routing in doubling
metrics. In Proceedings of the 25th ACM Symposium
on Principles of Distributed Computing, pages
198–207, 2006.
[14] G. Konjevod, A. Richa, and D. Xia. Optimal scale-free
compact routing schemes in networks of low doubling
dimension. In Proceedings of the 18th ACM-SIAM
Symposium on Discrete Algorithms, pages 939–948,
2007.
[15] D. Peleg. Distributed Computing: A Locality-Sensitive
Approach. Monographs on Discrete Mathematics and
Applications. SIAM, Philadelphia, 2000.
[16] D. Peleg and E. Upfal. A trade-oﬀ between space and
eﬃciency for routing tables. Jour. of the ACM,
36(3):510–530, July 1989.
[17] A. Slivkins. Distance estimation and object location
via rings of neighbors. In Proceedings of the 24th ACM
Symposium on Principles of Distributed Computing,
pages 41–50, 2005.
[18] K. Talwar. Bypassing the embedding: algorithms for
low dimensional metrics. In Proceedings of the 36th
ACM Symposium on Theory of Computing, pages
281–290, 2004.
[19] M. Thorup and U. Zwick. Compact routing schemes.
In Proceedings of the 13th ACM Symposium on
Parallel Algorithms and Architecture, pages 1–10,
2001.

d(u, v) ≥ d(u(t − 1), v) − O(2t ) ≥ 2t−1 (1/2 − O(1)). (30)
If j(u(t − 1), t − 1) = null, the Search procedure in Step 3
guarantees v ∈
/ Bu(t−1) (2t−1 /) by running the Step 4 of
Algorithm 2, which implies
d(u, v) ≥ d(u(t − 1), v) − O(2t ) ≥ 2t−1 (1/ − O(1)). (31)
Note that the if condition in Step 7 is not satisﬁed; otherwise Step 13 would be executed and Iteration t would
be skipped. Thus if j(u(t − 1), t − 1) = null, we have
u(t − 1) ∈ D t−1,j  for j  = log(|Bu(t−1) (2t−1 /)| · g2 ), which
implies u ∈ D. Therefore the claim follows from Eqn. (29),
(30) and (31).
Proof of Theorem 1.2: The bounds on stretch follow
from Lemma 6.4 and the bounds on storage follow from
Lemma 6.3 and 3.1.

7. FUTURE WORK
Abraham et. al [5] show that any name-independent routing scheme for general graph with o((n/(9k))1/k )-bit storage at each node has average stretch k/4 + 7/8. Hence,
an interesting question is whether a constant-stretch nameindependent compact routing scheme for general graphs with
relaxed guarantees exists. Furthermore, in the labeled routing model, it may be interesting to investigate whether we
can achieve a (1 + )-stretch labeled routing scheme for general graphs with relaxed guarantees. The strongest lower
bound result for labeled routing in general graphs states
that a labeled scheme with stretch < 3 requires O(n2 )-bit
storage at some nodes [19].

8. REFERENCES
[1] I. Abraham, Y. Bartal, J. Kleinberg, T.-H. Chan,
O. Neiman, K. Dhamdhere, A. Slivkins, and A. Gupta.
Metric embeddings with relaxed guarantees. In
Proceedings of the 46th IEEE Symposium on
Foundations of Computer Science, pages 83–100, 2005.
[2] I. Abraham, Y. Bartal, and O. Neiman. Advances in
metric embedding theory. In Proceedings of the 38th
ACM Symposium on Theory of Computing, pages
271–286, 2006.
[3] I. Abraham, Y. Bartal, and O. Neiman. Embedding
metrics into ultrametrics and graphs into spanning
trees with constant average distortion. In Proceedings
of the 18th ACM-SIAM Symposium on Discrete
Algorithms, pages 502–511, 2007.
[4] I. Abraham, C. Gavoille, A. V. Goldberg, and
D. Malkhi. Routing in networks with low doubling
dimension. In Proceedings of the 26th IEEE
International Conference on Distributed Computing
Systems, page 75, 2006.
[5] I. Abraham, C. Gavoille, and D. Malkhi. On
space-stretch trade-oﬀs: Lower bounds. In Proceedings
of the 18th ACM Symposium on Parallel Algorithms
and Architecture, pages 207–216, 2006.

9.

APPENDIX

Proof of Lemma 4.1:
Partition E by deﬁning Ei =
i
i+1
≤
d(x,
y)
<
2
} for all i > 0.Now either
{(x,
y)
∈
E
|
2


|E
|
≥
|E|/3,
or
|E
| ≥ |E|/3, or
3i
3i+1
i
i
i |E3i+2 | ≥
|E|/3. W.l.o.g., assume the ﬁrst inequality is true.

79

 ⊆ Pi−1 denote the set of ancestors
every S ∈ Pi , let D(S)
 We deﬁne
of S.

 =
 · log(n/|S|).

Δ(S)
|S| · log(n/|S|) − |S|

A partition P
of V is a collection of nonempty disjoint subsets of V with S∈P S = V . We now construct a sequence
of partitions P−1 , P0 , P1 , · · · of V , so that the following is
true:


S∈D(S)

() For any Pi in the sequence, any S ∈ Pi , and any x, y ∈
S, we have d(x, y) < 23(i+1) .


To start with, let P−1 = {x} | x ∈ V . Given the partition
Pi−1 , we construct Pi as follows. Set P = Pi−1 . Let S ∈ P
be the largest set in P (if there are multiple such sets, choose
an arbitrary one from them as S). Let

It is easy to verify the following equality:


Δ(S).
E(Pi−1 ) − E(Pi ) =
Furthermore, we can write

N(S) = {K ∈ P | ∃ an edge (x, y) ∈ E3i s.t. x ∈ S and y ∈ K}.
We deﬁne S as the union of S and the sets in N(S), i.e.,



S = S ∪
K∈N(S) K , and put S into the set P i . We say

that S ∈ Pi is induced by S ∈ Pi−1 , and the sets in N(S)
 We repeat the above
together with S are the ancestors of S.
process in the leftover set P = P\{S}\N(S) until P becomes
empty. At the end of the process, we obtain the set Pi . See
Figure 3 for an illustration.

 =
Δ(S)


|S| · log(|S|/|S|)
+


≥




|K| · log(|S|/|K|)

K∈N(S)


|K| · log(|S|/|K|)

K∈N(S)



≥

|K|,

(33)

K∈N(S)

 ≥
where the last inequality follows from the fact that |S|
|S| + |K| ≥ 2|K| for each K ∈ N(S) as |S| ≥ |K| by the
choice of S.
 ⊆ E3i be the subset of edges in E3i that are
Let σi (S)

incident to points in K∈N(S) K = S \ S, where S ∈ Pi−1
 Because for any node in V , the
is the set that induces S.
number of edges in E3i that are incident to it is at most c,

 Combined with (33), we
we have c · K∈N(S) |K| ≥ |σi (S)|.
then obtain

S

 ≥ |σi (S)|/c.

Δ(S)
Se

(34)

On the other hand, we show that any edge in E3i must
be incident to a point in S \ S, for some S ∈ Pi induced by
some S ∈ Pi−1 . That is,

 = E3i .
σi (S)
(35)

Figure 3. Constructing Pi from Pi−1 . Solid circles represent sets in Pi−1 . Regions bounded by dotted curves represent sets in Pi . Line segments represent edges in E3i . The
sets in Pi−1 that induce sets in Pi are shaded.


S∈P
i

Otherwise, there exists an edge (x, y) ∈ E3i for which one
of the two cases has to happen:
(a) x, y ∈ S for an S ∈ Pi−1 that induces some S ∈ Pi ; or

Clearly, Pi is a valid partition of V . It remains to prove
that Pi satisﬁes (), assuming Pi−1 satisﬁes (). Let S ∈ Pi
 We need to show that d(x, y) < 23(i+1) .
and x, y ∈ S.

Assume S is induced by S ∈ Pi−1 . There are three simple
cases to consider:
 By () for Pi−1 ,
(a) x, y ∈ K for some ancestor K of S.
3i

(32)


S∈P
i

(b) x ∈ S1 for an S1 ∈ Pi−1 that induces some S1 ∈ Pi ,
and y ∈ S2 for an S2 ∈ Pi−1 that induces some S2 ∈ Pi .

The ﬁrst case cannot happen because otherwise by () we
have d(x, y) < 23i , contradicting with the fact (x, y) ∈ E3i .
The second case cannot happen either, because otherwise
we will have S1 ∈ N(S2 ) (in which case S1 cannot induce
S1 ) or S2 ∈ N(S1 ) (in which case S2 cannot induce S2 ).
It follows from (32), (34) and (35) that


σi (S)/c
≥ |E3i |/c.
E(Pi−1 ) − E(Pi ) ≥

3(i+1)

we have d(x, y) < 2 < 2
.
(b) x ∈ S and y ∈ K for some K ∈ N(S). Let (x0 , y0 )
be the edge in E3i with x0 ∈ S and y0 ∈ K. Then
d(x, y) ≤ d(x, x0 ) + d(x0 , y0 ) + d(y0 , y) < 23i + 23i+1 +
23i < 23(i+1) ,
(c) x ∈ K1 and y ∈ K2 for some K1 , K2 ∈ N(S). Similarly, let (x1 , y1 ), (x2 , y2 ) be the two edges in E3i with
x1 , x2 ∈ S, and y1 ∈ K1 and y2 ∈ K2 respectively.
We have d(x, y) ≤ d(x, y1 ) + d(y1 , x1 ) + d(x1 , x2 ) +
d(x2 , y2 ) + d(y2 , y) < 3 · 23i + 2 · 23i+1 < 23(i+1) .


S∈P
i

Because E(P−1 ) = n log n and E(P) ≥ 0 for any partition
P, summing up the above inequality for all i ≥ 0, we then
obtain

|E3i |/c ≤ E(P−1 ) = n log n.

Hence we can conclude that Pi satisﬁes ().
We deﬁne the entropy of a partition P as

|S| · log(n/|S|).
E(P) =

Recall that
as desired.

S∈P

Here all log’s are in base 2. Next we show that the entropy
decreases signiﬁcantly when we move from Pi−1 to Pi . For

80



i
i |E3i | ≥ |E|/3. This implies |E| ≤ 3cn log n,

Competitive and Fair Throughput for Co-Existing Networks
Under Adversarial Interference
Andrea Richa, Jin Zhang

Christian Scheideler

Stefan Schmid

Computer Science and
Engineering, SCIDSE
Arizona State University
Tempe, AZ 85287, USA

Department of Computer
Science
University of Paderborn
D-33102 Paderborn, Germany

Deutsche Telekom
Laboratories & TU Berlin
D-10587 Berlin, Germany

{aricha,jzhang82}@asu.edu

scheideler@upb.de

ABSTRACT

spectrum is often simultaneously used by many devices belonging
to different, so-called co-existing networks. It is expected that the
popularity of wireless mobile devices will further increase the resource sharing by such networks in the future.
Interestingly, not much is known today on how a given spectrum
can be shared efficiently and fairly among co-existing networks, especially in environments with uncontrollable external interference.
Existing distributed MAC protocols (typically based on random
backoff schemes) are either not resistent to the unpredictable unavailability of the medium at all, or are optimized towards a single
network only, in the sense that the nodes of a network collaboratively seek to coordinate the access among themselves [24]. However, the state-of-the-art protocols fail if multiple networks are collocated (as illustrated, for example, in our simulation study in Section 4).
This paper is the first to present (and rigorously prove the performance of) a robust MAC protocol suited for co-existing networks
exposed to a harsh environment with unpredictable or even adversarial interference.

This paper initiates the formal study of a fundamental problem:
How to efficiently allocate a shared communication medium among
a set of K co-existing networks in the presence of arbitrary external interference? While most literature on medium access focuses
on how to share a medium among nodes, these approaches are often either not directly applicable to co-existing networks as they
would violate the independence requirement, or they yield a low
throughput if applied to multiple networks. We present the randomized medium access (MAC) protocol C O MAC which guarantees that a given communication channel is shared fairly among
competing and independent networks, and that the available bandwidth is used efficiently. These performance guarantees hold in the
presence of arbitrary external interference or even under adversarial jamming. Concretely, we show that the co-existing networks
can use a Ω(ε2 min{ε, 1/poly(K)})-fraction of the non-jammed
time steps for successful message transmissions, where ε is the (arbitrarily distributed) fraction of time which is not jammed.

Categories and Subject Descriptors

1.1 Model

C.2.5 [Computer-Communication Networks]: Local and WideArea Networks—Access schemes; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms and Problems—Sequencing and scheduling

We attend to a simplified scenario where a set of n wireless
nodes V are located within transmission range of each other and
need to communicate over a single shared channel. The wireless
nodes belong to K co-existing networks Ni with node sets Vi , i.e.,
V = V1 ∪ V2 ∪ . . . ∪ VK , for some constant K (which is of unknown to the nodes). For simplicity we will assume that these networks are node disjoint. However, by emulating multiple instances,
a node may also participate in several networks simultaneously; the
performance guarantees derived in this paper would still hold.
We aim to design a distributed MAC protocol for these wireless
nodes. Although the protocol is used by all nodes v ∈ V , it should
not depend on any knowledge of how many nodes n there are in
total, on the number of co-existing networks K, or on the size of
the co-existing network v belongs to. Moreover, it should ensure
that the K networks are independent in the sense that no communication is required between different networks.
Co-existing wireless networks appear in many scenarios where
different wireless networks share the same wireless medium. For
example, consider a major conference, e.g., organized by the
United Nations, where participants from different countries use
their hand-held devices to communicate with the other representatives of their country. We assume that the different networks only
share the same medium access protocol, but are otherwise different and inter-network communication may not be desired or possible (except, e.g., for multi-national participants). Another scenario
where ensuring fairness among co-existing networks is crucial are
emergency response networks, where many emergency response

General Terms
Algorithms, Reliability, Theory

Keywords
Wireless Ad-hoc Networks, MAC Protocols, Jamming

1.

stefan@net.t-labs.tuberlin.de

INTRODUCTION

The decentralized allocation of a communication medium among
a set of wireless nodes does not only constitute one of the most
fundamental theoretical problems in distributed computing, but is
also of direct practical relevance. Today, a chunk of the wireless

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’12, July 16–18, 2012, Madeira, Portugal.
Copyright 2012 ACM 978-1-4503-1450-3/12/07 ...$10.00.

291

services, such as fire squads, police, and paramedics, all arrive simultaneously at some accident or disaster scene and have to share
the wireless medium in a fair and even manner in order to establish
their own separate communication networks.1
This paper presents a robust and fair medium access (MAC) protocol C O MAC that makes effective use of the few and arbitrarily distributed time periods where a wireless medium is available.
We model interference—due to simultaneous transmissions, coexisting networks, changes in the environment that affect the wireless medium, etc., and, when applicable, intentional jamming—
generally as an adversary, which we may sometimes simply refer
to as the jammer (even when a malicious jammer is not present in
the environment and interference may be caused by other factors).
Our adversary may behave in an adaptive manner: we assume that
the adversary has full knowledge of the protocol and its history, and
that it uses this knowledge to decide on whether to jam at a certain
moment in time.
Let us use the simplifying notation N (v) to denote the network
node v ∈ V belongs to. We assume that a node v can distinguish
among the following events at some time t: (1) idle channel (no
node in V transmits and there is no outside interference, including
jamming activity, at time t); (2) successful transmission of a packet
in network N (v) (which occurs every time a single node in N (v)
transmits, and no other node in V nor the adversary transmits); and
(3) medium busy (due to a transmission by a node in some coexisting network different from network N (v), or to simultaneous
transmissions by two or more nodes in N (v), or to external interference or jamming).
How to design such a distributed medium access protocol which
shares the bandwidth fairly among the K networks, without sacrificing performance? At first sight this may seem impossible: as
the total number of co-existing networks and the number of devices is not known, a node cannot guess its fair share of the channel time. This paper shows that this is indeed possible, even in
the presence of a powerful adaptive adversarial jammer, referred to
as a (T, 1 − ε)-bounded (adaptive) adversary, which can jam the
medium an arbitrary (1 − ε) fraction of the time for an arbitrarily
small constant ε > 0 and which hence models a wide range of external interference scenarios or jammers. For the ease of presentation, we assume a synchronous environment where time proceeds
in rounds (also called steps). Formally, the (T, 1 − ε)-bounded
adversary is defined as follows: for some T ∈ N and a constant
0 < ε < 1, the adversary may jam at most (1 − ε)w of the time
steps, for any time window of size w ≥ T . In the following, we
will use the notation N = max{T, n} to denote the maximum over
the adversarial window size and n.
Assuming backlogged traffic at the wireless devices, we require
that our MAC protocol fulfill the following properties: (1) ccompetitiveness: Given a time interval I, we define g(I) as the
number of time steps in I that are non-jammed, and s(I) as the
total number of time steps in I in which a successful transmission
happens in any network. A MAC protocol is called c-competitive
against some (T, 1 − ε)-bounded adversary if, for any sufficiently
large time interval I, s(I) ≥ c · g(I). (2) Fairness: The probabilities of having a successful transmission in any two networks Ni
and Nj , where i, j ∈ [1, K], do not differ by much; moreover, the
nodes inside a network share the bandwidth fairly as well.
Note that the nodes have no knowledge of how many nodes

are there in the same network as itself, nor do the nodes know
how many other networks are co-existing and how many nodes
are there in each of these co-existing networks, respectively. However, we assume that the nodes have a common parameter γ ∈
O(1/(log T + log log n)). The assumption that nodes know γ is
not critical for the scalability of our protocol, as it requires only a
polynomial estimate of T and an even rougher estimate of n.
Although the presented C O MAC protocol converges fast and is
therefore expected to work well under continuously entering and
leaving nodes, in this paper we will just focus on a synchronous
setting where nodes do not join or leave.

1.2 Related Work
The classic approach to design efficient MAC protocols is to use
random backoff schemes (e.g., [5, 6, 12, 13, 18, 22]). However,
these works do not take into account adversarial interference and
are hence not robust against it. Generally, in a random backoff
protocol, each node periodically attempts to transmit a message
starting with a certain probability p. If the message transmission
fails (due to interference), the node may retry sending the message
in the next time steps with polynomially or exponentially decreasing probabilities (for example, p2 , p4 , p8 , . . .) until the message is
successfully transmitted or the minimum allowable probability is
reached. Thus, in a dense network (as in our single-hop scenario),
an adversary with knowledge of the MAC protocol could simply
wait until the nodes have reached transmission probabilities that
are inversely proportional to the number of nodes and then start
jamming the medium, forcing the nodes to lower their transmission probabilities to a point where a competitive throughput is not
achievable.
There also exist several interesting results on protocols that
are robust to more complex or even adversarial interference (see,
e.g., [7] or [29] for a nice overview). There are two basic approaches in the literature. The first assumes randomly corrupted
messages (e.g., [21]), which is much easier to handle than adaptive
adversarial jamming [4]. The second line of work either bounds
the number of messages that the adversary can transmit or disrupt
with a limited energy budget (e.g. [1, 11, 16, 17]), or bounds the
number of channels the adversary can jam (e.g. [8, 9, 10, 19]). The
protocols in, e.g., [17] can tackle adversarial jamming at both the
MAC and network layers, where the adversary may not only jam
the channel but also introduce malicious (fake) messages (possibly
with address spoofing). However, these solutions depend on the
fact that the adversarial jamming budget is finite, so it is not clear
whether the protocols would work under heavy continuous jamming. (The result in [11] seems to imply that a jamming rate of 0.5
is the limit whereas the handshaking mechanisms in [17] seem to
require an even lower jamming rate.)
Our work is motivated by the jamming-resistant single network
MAC protocols studied in [3, 23, 24]. In particular, our adversarial
model was introduced by Awerbuch et al. [3] who present a singlehop MAC protocol that guarantees a constant throughput against an
adaptive adversary that can block the medium a constant fraction of
the time. The MAC protocol and the throughput guarantees were
subsequently generalized to multi-hop networks [23, 26], and also
the adversary was strengthened further such that it can even jam
the medium reactively, i.e., it has a binary feedback whether the
medium will be idle or busy in the current round [24], before it has
to make a decision whether to jam the current round. It has also
been shown that the MAC protocol can serve as a basis to design
robust applications such as leader election [25].
However, the performance achieved by the MAC protocols described in [3, 23, 24] drops sharply if multiple networks are collo-

1
Whereas in some scenarios it may be desirable that messages are
broadcast across all emergency unit networks, for better immediate response action to a disaster/accident, in the longer run, it is
still important to be able to differentiate among the different adhoc networks established.

292

2.1 Intuition

cated. This is due to the fact that in these protocols, each individual
co-existing network will strive to achieve a constant competitive
throughput in the non-jammed time periods, which requires a constant cumulative access probability per co-existing network. As we
will explain in the next section in more detail, this necessarily leads
to a throughput which is exponentially small in the number of coexisting networks.
It turns out that in a co-existing scenario, the nodes must strike a
good balance between a less aggressive (more cooperative) medium
access strategy while remaining robust against external interference. We will show that this can be achieved by monitoring the
availability of the wireless medium over time and adjusting the
sending probabilities or backoffs according to the fraction of observed idle time periods. (A similar approach is used in the IdleSense [14] Distributed Coordination Function to synchronize the
nodes’ contention windows.) Implicitly synchronizing access via
idle time periods is also the key to enable fairness between coexisting networks. The performance analysis of such an algorithm
however is involved, as the distributed and randomized decisions
exhibit many non-trivial dependencies. Nevertheless, we are able
to rigorously prove good competitive throughput and fairness properties, which is also confirmed by our simulation study.
Interestingly, although co-existing networks are ubiquitous and
many different aspects are discussed intensively (e.g., the packet
inter-arrival time and fairness in co-existing 802.11a/g and 802.11n
networks [2], interference cancelation phenomena [27], transmission capacities in multi-antenna adhoc networks [15], or
even explicit inter-network communication for frequency cooperation [30]) in different contexts (e.g., in the current debate on white
space liberalization [20] where primary TV and microphone users
announcing their reservations in a central database are given strict
priority), we are not aware of any work on the design of MAC protocols for independent co-existing networks with rigorous formal
competitive throughput and fairness guarantees.

In the C O MAC protocol, each node v maintains a medium access probability pv which determines the probability that v transmits a message in a communication round. The nodes adapt and
synchronize (inside a co-existing network) their pv values over time
(which as a side-effect also guarantees fairness within the network)
in a multiplicative-increase multiplicative-decrease manner in order to ensure a throughput that is as good as possible. More precisely, the sending probabilities are changed by a factor of (1 + γ).
Moreover, we impose an upper bound of p̂ on pv , for some constant
0 < p̂ < 1. As we will see, unlike in most classic backoff protocols, our adaptation rules for pv ensure that the adversary cannot
influence pv much by adaptive jamming.
In addition, each node maintains two variables, a threshold variable Tv and a counter variable cv . Tv is used to estimate the adversary’s time window T . A good estimation of T can help the nodes
recover from a situation where they experience high interference in
the network. In times of high interference, Tv will be increased and
the sending probability pv will be decreased.
While these concepts have already been used in our other protocols in [3, 23, 24], they are not sufficient to ensure a jammingresistant protocol that also works well in case of co-existing networks. The basic problem lies in the fact that all of these protocols aim at reaching a constant cumulative probability, irrespective
of the adversarial jamming, so that a good throughput can be obtained in those steps that are not jammed. In co-existing networks,
however, this is not a good idea: Suppose that we have K coexisting networks such that each has a constant cumulative probability. Then the overall cumulative probability would be Θ(K)
and therefore, the probability of having a successful transmission
in any network would be as low as Θ(K)e−Θ(K) , which is exponentially low in K.
Hence, a less aggressive approach than the one pursued in [3, 23,
24] is needed. Ideally, this approach should also make sure that the
available bandwidth is shared in a fair way among the networks.
Surprisingly, a relatively simple change in the protocol in [24] can
achieve jamming-resistance, a good throughput in co-existing networks, and also fairness. The basic idea behind this change is to
remember the latest idle time step, and whenever there is a new
idle time step, then with a probability qv that is inversely proportional to the time difference to the previous idle time step, pv and Tv
are adapted. (The protocol in [24] would always adapt pv and Tv
in case of an idle channel.) Since this probabilistic rule turned out
to be very hard to analyze, we transformed it into a deterministic
rule that shows the same performance in the experiments.

1.3 Our Contributions
To the best of our knowledge, this is the first paper to present
a robust medium access protocol which provably performs well in
an environment with co-existing networks. The C O MAC protocol
features a guaranteed competitive throughput in the presence of coexisting networks as well as a wide range of external interference
patterns that can be subsumed and modeled as a (T, 1−ε)-bounded
adaptive adversary blocking the medium a (1 − ε) fraction of all
time. Moreover, it features fairness among co-existing networks
and within an individual network. Finally, the protocol is attractive
for its simple design. Our main theoretical result is summarized in
the following theorem.

2.2 Algorithm
Now we are ready to provide the detailed and formal description
of the C O MAC algorithm. Initially, each node v sets pv = p̂ (p̂ ≤
1/24), cv = Tv = 1, and qv = 0. In the following, Lv ≥ 1 is the
time that went by from v’s viewpoint since the last idle time step.
(If there has not yet been an idle time step, Lv = ∞.)

T HEOREM 1.1. The C O MAC medium access protocol
guarantees that in a backlogged scenario, if executed for
Ω( 1ε log N max{T, εγ12 log3 N }) many time steps, C O MAC
achieves a competitive throughput of Ω(ε2 min{ε, 1/poly(K)})
w.h.p., for any (T, 1 − ε)-bounded adaptive adversary that arbitrarily jams the medium up to a (1 − ε) fraction of the time, and
which has complete knowledge of the protocol history. Moreover,
the cumulative probabilities among different networks, as well
as the access probabilities of individual nodes within the same
network, differ only by a small factor.

In each step, each node v does the following: v decides with
probability pv to send a message along with the tuple (cv , Tv , pv ).
If it decides not to send a message, it checks the following two
conditions:

Simulations complement our theoretical asymptotic bounds.

2.

1. If v senses an idle channel, then qv := qv + 1/Lv . If qv ≥ 1
then

MAC FOR CO-EXISTING NETWORKS

• pv := min{(1 + γ)pv , p̂}, Tv := max{1, Tv − 1}, and

Before presenting the formal MAC algorithm, we explain its
variables and provide some intuition.

293

L EMMA 3.4. For any non-jammed time step,

• qv := qv − 1.

P

e− 1−p̂ ≤ P[channel is idle] ≤ e−P

2. If v successfully receives a message from node u with the
tuple (cu , Tu , pu ) then

Pi ·e

• pv := (1 + γ)−1 pu , cv = cu , and Tv = Tu .
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no idle step among the
past Tv time steps, then pv := (1 + γ)−1 pv and Tv := Tv + 2.

3.

P
− 1−
p̂

and

≤ P[successful msg transmission in Ni ] ≤

Pi
·e−P
1 − p̂

3.2 Cumulative Probability
In the following, we will derive the first fundamental property of
our
Kprotocol: we show that the overall cumulative probability P =
i=1 Pi converges to some range of values so that the contention
on the wireless medium is moderate. This is a necessary condition
for a good performance. Our proof framework basically follows
the framework of [3] but the proof arguments significantly differ in
various places when it comes to analyzing the specifics of our new
protocol. We refer to Section 2 of [3] for a comparison.
The proof works by induction over sufficiently large time frames.
Let I be a time frame consisting of αε log N subframes I  of size

ANALYSIS

For the analysis of our protocol we will use the following notation. We are given K ≥ 2 co-existing networks denoted by
N1 , . . . , NK . Each network Ni consists of a node set Vi where
ni = |Vi | ≥ 2 (otherwise, the network would be irrelevant).
The cumulative probability due to nodes in Ni is given by Pi =

v∈Vi pv , and the cumulative probability over all co-existing netK
works is given by P =
i=1 Pi . Whenever we consider some
specific time step t, Pi (t) is the value of Pi at time t and P (t) is
the value of P at time t.

2

f = max{T, αβ
log3 N } rounds, where α and β are sufficiently
εγ 2
large constants and N = max{T, n}. Let F = αε log N · f denote
the size of I.
First, we show that for any subframe I  in which initially
the
√
overall cumulative probability is at least 1/(f 2 (1 + γ)2 f ), √also
afterwards this cumulative probability is at least 1/(f 2 (1+γ)2 f ),
w.h.p.

3.1 Basic Observations
Given that we have a single-hop network, any idle time period
is observed by all nodes in all co-existing networks. Hence, the qv
and Lv values of all nodes are identical if all start at the same time
(otherwise, two idle time steps suffice to synchronize the Lv values so that the increase of the qv ’s is synchronized from that point
on, which would also be sufficient for our analysis to go through).
Henceforth, we will drop the subscript v from qv and Lv . Since
after the first successful transmission in Ni , the Tv and cv values
are synchronized among the nodes in Ni , we arrive at the following
fact, which establishes fairness within a network.

L EMMA 3.5.√ For any subframe I  = [t0 , t1 ) in√which P (t0 ) ≥
1/(f 2 (1 + γ)2 f ), also P (t1 ) ≥ 1/(f 2 (1 + γ)2 f ) w.h.p.
P ROOF. We start with the following claim about the maximum
number of times nodes decrease their probabilities in I  due to cv >
Tv .
C LAIM 3.6. If in subframe I  , Tv is decreased
√ at most k times,
then node v increases Tv by 2 at most k/2 + f many times.

FACT 3.1. After the first successful transmission in network Ni ,
the access probabilities pv of the nodes v ∈ Vi differ by a factor of
at most (1 + γ).

P ROOF. Only an idle time step can potentially reduce Tv by 1.
If there is no idle time step during the last Tv many steps, Tv is
increased by 2. Suppose that k = 0. Then the number of times a
node v increases
Tv by 2 is upper bounded by the largest possible

 so that i=0 Tv0 + √
2i ≤ f , where Tv0 is the initial value of Tv .
0
For any Tv ≥ 1,  ≤ f , so the claim is true for k = 0. For each
decrease of Tv , the current Tv as well as all subsequent values of Tv
(until a Tv is reached with Tv = 1) get reduced by one. Hence, for
an arbitrary
value of k ≥ 0 we are searching for the maximum √
 so

that i=0 max{Tv0 + 2i − k, 1} ≤ f . This  is at most k/2 + f ,
which proves our claim.

Throughout our analysis, we will make use of generalized Chernoff bounds that are derived from [28].
L EMMA 3.2. Consider any set of random variables X1 ,
. . . , Xn with
values in [0,
1]. If there exist values p1 , . . . , pn ∈
[0, 1] with E[ i∈S Xi
] ≤ i∈S pi for every
nset S ⊆ {1, . . . , n},
then it holds for X = n
i=1 Xi and μ =
i=1 pi and any δ > 0
that
μ

δ2 μ
eδ
− 2(1+δ/3)
≤
e
P[X ≥ (1 + δ)μ] ≤
(1 + δ)1+δ


If, on the other hand, it holds that E[ i∈S Xi ] ≥
i∈S pi for
every set S ⊆ {1, . . . , n}, then it holds for any 0 < δ < 1 that
μ

2
e−δ
≤ e−δ μ/2
P[X ≤ (1 − δ)μ] ≤
1−δ
(1 − δ)

This claim allows us to prove that the overall cumulative probability P will exceed a certain threshold in a subframe w.h.p.
C LAIM 3.7. Suppose that in I  = [t0 , t1 ), P (t0 ) ∈ [1/(f 2 (1+
γ) 2f ), 1/f 2 ]. Then there is a time step t in I  with P (t) ≥ 1/f 2 ,
w.h.p.
√

P ROOF. Suppose that there are g non-jammed time steps in I  .
Let k0 be the number of these steps with an idle channel and k1 be
the number of these steps with a successful message transmission
in any of the co-existing networks. Let the binary random variable
Xi be 1 if and only if the nodes increase their
 0access probabilities
in the i-th idle time step in I  , and let X = ki=1
Xi . Furthermore,
let k2 be the maximum number of times a node v increases Tv by
2 in I  .

The following lemma follows immediately from the Taylor series
of the exponential function.
L EMMA 3.3. For all 0 < x < 1 it holds that e−x/(1−x) ≤
1 − x ≤ e−x .
This implies the following lemma.

294

Suppose for the moment that P (t0 ) = 1/f 2 . If all time steps t
in I  satisfy P (t) ≤ 1/f 2 , then it must hold that the total decrease
of P (t) in I  (due to successful transmissions and cases in which
access probabilities are decreased when cv > Tv ), which is at most
(1 + γ)k1 +k2 , has to be at least as large as the total increase of
P (t) (due to idle time steps), which is equal to (1 + γ)X . Hence,
we must have that X ≤ k1 + k2 . For an arbitrary initial probability
P (t0 ) ≤ 1/f 2 , we must therefore have
2

X − log 1+γ ((1/f )/P (t0 )) ≤ k1 + k2

Moreover, it certainly holds for any set S of time steps prior to
some time step t that

Ys = 1] ≤ 1/f 2 .
P[Yt = 1 |
s∈S

Therefore, we have
P[

(1)



Ys = 1]

s∈S

= P[Y1 = 1] · P[Y2 = 1|Y1 = 1] · P[Y3 = 1|

to avoid a time step t in I  with P (t) > 1/f 2 . Our goal is to show
that this inequality is violated w.h.p., which implies that I  has a
time step t with P (t) > 1/f 2 w.h.p.
Next, we focus on k2 . Consider some fixed k0 ≥ 2 (as we will
see later, k0 ≥ 2 w.h.p.). Let Li be the L-value of the nodes at the
i-th idle time step (note that they are all the same) and let qi = 1/Li
denote the increase of the 
q-values of the nodes in the i-th idle time
0
step. Also, let q̄ = k01−1 ki=2
qi . Certainly, the number of times
any node v decreases Tv in I  is bounded
by the number of times
 0
qi 
 ≤ 	1 + (k0 − 1)q̄
.
q is at least 1, which is at most 	 ki=1
Hence, it follows from Claim 3.6 that

k2 ≤ 	q̄(k0 − 1) + 1
/2 + f
(2)

·

Ys = 1] · . . .

s=1,2



P[Y|S| = 1|



Ys = 1]

s=1,2,...,|S|−1

≤ (1/f 2 )|S|
and
E[



Ys = 1] = P[

s∈S



Ys = 1] ≤ (1/f 2 )|S| .

s∈S

Thus, the
 Chernoff bounds and our choice of f imply that w.h.p.
either t∈I  Yt < ε2 f /8 and P (t) ≤ 1/f 2 throughout I  , or
there must be a time step t in I  with P (t) > 1/f 2 , which would
finish the proof. Therefore, unless P (t) > 1/f 2 at some point in
I  , k1 < ε2 f /8 w.h.p.
Next we prove a lower bound on k0 that holds w.h.p. For any
time step t with P (t) ≤ 1/f 2 it holds that

On the other hand, the
 0number of times any node v increases
qi  = (k0 − 1)q̄ (because due to
pv in I  is at least  ki=2
Fact 3.1 it follows from P (t) ≤ 1/f 2 that pv (t) < p̂ for all
v). Plugging this together
with (2) into (1) and using the fact that
√
P (t0 ) ≥ 1/(f 2 (1 + γ) 2f ), we obtain


2f + k1 + f
(k0 − 1)q̄ − 	(k0 − 1)q̄ + 1
/2 ≤

⇒ (k0 − 1)q̄/2 ≤ k1 + 4 f
(3)

P[channel is idle] ≥ e−P (t)/(1−p̂) ≥ 1 −

P (t)
≥ 1 − 1/f
1 − p̂

Hence, E[k0 ] ≥ g · (1 − 1/f ) ≥ εf (1 − 1/f ). Using similar
arguments as for k1 , it follows that k0 > (7/8)εf w.h.p. unless
P (t) > 1/f 2 at some point in I  . When combining the bounds for
q̄ and k0 , we obtain

given that f is large enough. It remains to lower bound q̄ and k0
and to upper bound k1 in order to arrive
 0
 0 at a contradiction.
Li . Since ki=2
Li < f ,
We start with q̄. Let L̄ = k01−1 ki=2
it holds that L̄ < k0f−1 . Moreover, we make use of the following
well-known fact.

(k0 − 1)q̄/2 ≥
>

(k0 − 1)2
≥ (7/8)2 ε2 f /2
2f


ε2 f /8 + 4 f > k1 + 4 f

x1 , . . . , xn it
FACT 3.8. For any sequence of positive numbers
n
x
and its harholds for its arithmetic mean
A
=
(1/n)
i
i=1

−1
monic mean H = ((1/n) n
that A ≥ H.
i=1 1/xi )

w.h.p., if f is large enough, which violates Inequality (3) and therefore completes the proof of Claim 3.7.

 0
1/Li ) and therefore,
Hence, it follows that L̄ ≥ 1/( k01−1 ki=2
k 0
1
1/L
≥
1/
L̄.
This
in
turn
implies
that
i
i=2
k0 −1

Similarly, we can also prove that once the cumulative probability
exceeds a certain threshold, it cannot become too small again.

q̄ ≥ 1/L̄ ≥

C LAIM 3.9. Suppose that for the first time step t0 in I  ,
P (t0 ) ≥ 1/f 2 . Then there is no time step t in I  with P (t) <
1 √
, w.h.p.
f 2 (1+γ) 2f

k0 − 1
f

Next we provide an upper bound for k1 that holds w.h.p. Certainly,
for any time step t with P (t) ≤ 1/f 2 ,
P[≥ 1 message transmitted at step t]

P ROOF. Consider some fixed subinterval I  = [t1 , t2 ) in I 
with the property that P (t1 ) ≥ 1/f 2 and P (t) ≤ 1/f 2 for all
other t in I  (i.e., we will use conditional probabilities based on
P (t) ≤ 1/f 2 like in the bound for k1 in the proof of Claim 3.7).
Suppose that there are g non-jammed time steps in I  . If g ≤
β log N for a (sufficiently large) constant β, then it follows for the
probability P (t2 ) at the end of I  that

≤ 1/f 2 .

Hence, E[k1 ] ≤ g · (1/f 2 ) ≤ 1/f . In order to prove an upper
bound on k1 that holds w.h.p., we can use the general Chernoff
bounds stated in Lemma 3.2. For any step t let the binary random
variable Yt be 1 if and only if at least one message is transmitted
successfully at time t and P (t) ≤ 1/f 2 . Then
P[Yt = 1]

P (t2 ) ≥

= P[P (t) ≤ 1/f 2 ] ·

√
1
1
√
· (1 + γ)−((3/2)β log N+ f ) ≥ 2
2
f
f (1 + γ) 2f

given that f is large enough (i.e., ε = Ω(1/ log3 N )). This is because in the worst case for the decrease of P (t) all non-jammed
time steps are successful. In this case, P (t) is decreased at most

P[successful msg transmission | P (t) ≤ 1/f 2 ]
≤ 1/f 2 .

295

β log N times due to these steps. Moreover, from Claim 3.6 it √
follows that P (t) can be decreased another at most β log N/2 + f
times due to cv > Tv .
So suppose that g > β log N . Let X be the number of time steps
in I  in which P (t) increases and k1 be the maximum number
of time steps in I  (over all networks) with a successful message
transmission. Furthermore, let k2 be the maximum number of times
1 √
then it must
a node v increases Tv in I  . If P (t2 ) < f 2 (1+γ)
2f

Pi (t + 1)

w∈Vi \{v}

=
≤

hold that the total increase in P (t) (which is equal to (1 + γ)X ) is
at most the total decrease in P (t) (which is at most (1 + γ)k1 +k2 ),
or in other words,

=

X ≤ k1 + k 2 .

≤

From the previous claim we know that this is not true w.h.p. given
that P (t) ≤ 1/f 2 for all t > t1 in I  and the constant β is sufficiently large to achieve polynomially small probability bounds.
Since there are at most f 2 possible values for t1 and t2 , there is no
1 √
time step t2 in I  with P (t2 ) < f 2 (1+γ)
w.h.p., which com2f
pletes the proof.

γ
· pv,t
1+γ
Pi (t)
γ
· Pi (t) +
·
1+γ
ni


γ
1+
Pi (t)
ni
· Pi (t) +

(1 + γ)1/ni Pi (t) ≤ √

1
Pi (t)
1+γ

If the same node v successfully transmits again at time t, then
Pi (t + 1) = Pi (t), which only happens with probability at most
(1 + γ)/ni because in this case the transmitting node has an access
probability that is by a (1 + γ) factor larger than the other access
probabilities in Ni . Hence, on expectation, at least 1/3 of the time
steps with successful transmission, Pi (t) is reduced by at least (1+
γ)1/2 , which implies that E[k1 ] ≥ k1 /6.
Based on this insight, the next claim shows that under certain
conditions, Inequality (4) is not true w.h.p. Let gi be the number of
useful time steps for Ni , which are time steps that are either idle or
successful for Ni in I  .

Next we show an upper bound for P (t). In the following, K  =
O(K) is a sufficiently large constant ≥ K.
3.10. For any subframe I  = [t0 , t1 ) with Tv ≤
L EMMA
√
(3/4) F for all nodes v at the beginning of I  , P (t1 ) ≤ 12 ln K 
w.m.p.

C LAIM 3.12. If all time steps t ∈ I  satisfy P (t) ≥ 4 ln K 
and gi ≥ δ log1+γ N for a sufficiently large constant δ, then X +
log1+γ ni < k1 w.h.p.

P ROOF. First, we will show that if P (t) ≥ 4 ln K  throughout I  , then for each Ni , there must be a step t with Pi (t ) ≤
(2 ln K  )/K  w.h.p., and once such a step is reached, we show
that Pi (t ) < (4 ln K  )/K  w.m.p. for all time steps t following t . Hence, there must be a time step t in I  with Pi (t ) <
(4 ln K  )/K  for all i, w.m.p., contradicting the assumption that
P (t) ≥ 4 ln K  throughout I  . Once we have that, we will show
that at the end of I  , P (t1 ) ≤ 12 ln K  w.m.p.
Consider some fixed network i. Let k0 be the number of idle
steps in I  and k1 be the number of successful time steps for network i. Moreover, let X be the total number of times Pi (t) is
increased by (1 + γ) due to an idle channel in I  . For Ni to avoid
a time step t in I  with Pi (t ) ≤ (2 ln K  )/K  , we must have
that the total increase of Pi (t) (which is equal to (1 + γ)X ) is
at least the total decrease of Pi (t) once we have reached a point
t with Pi (t) = (2 ln K  )/K  , which is the case after at most
log 1+γ (ni · p̂) reductions of Pi (t). Hence, we must have
≥ k1 − log1+γ (ni · p̂)

1
1+γ
1
1+γ
1
1+γ
1
1+γ

1
· pv,t
1+γ

given that ni ≥ 2.

Combining Claims 3.7 and 3.9 completes the proof of
Lemma 3.5.

X



= pv,t +

P ROOF. It is easy to see that for any useful time step t,
P[t successful for Ni ] ≥ Pi (t) · P[t idle]

(5)

2 ln K 
K

and therefore E[k1 ] ≥
E[k0 ] unless there is a time step t
with Pi (t) < (2 ln K  )/K  . For a given number of useful time
steps gi , since k0 + k1 = gi and therefore also E[k0 ] + E[k1 ] = gi ,

K
E[k1 ] ≥ 2 ln
(gi − E[k1 ]), which implies that E[k1 ] ≥ lnKK · gi
K


if K = O(K) is a sufficiently large constant. Since E[k1 ] ≥
k1 /6, gi = Ω(log 1+γ N ), and for each useful time step there is an
independent probability whether this time step is idle or successful,

it follows from the Chernoff bounds that k1 ≥ ln8KK gi w.h.p.
Next we bound X. Let the binary random variable Xj denote
the increase of Pi (t) by (1 + γ)Xj in the j-th idle time step. Then
k 0
X =
j=1 Xj . Moreover, let Lj be the number of time steps
between the (j − 1)-th and j-th idle time steps. It holds that

(4)

P[t idle] ≤ e−P (t) ≤ 1/(K  )4

k1

is the total decrease (in the exponent) of Pi (t) due to
where
successful transmissions to avoid a time step t in I  with Pi (t ) ≤
(2 ln K  )/K  . Notice that k1 is not equal to k1 because if, for
example, a node successfully transmits twice in a row, Pi (t) does
not get decreased the second time.
In order to contradict this bound, we first need to have a closer
look at what happens when there is a successful transmission in Ni .

for every t ∈ I  given that P (t) ≥ 4 ln K  . Hence,
−1

 1 
1
1
E[Xj ] =
1
−
P[Lj = ] · 1/ ≤
·
(K  )4
(K  )4

≥1

≥1

≤

 −/(K  )4
1
1
e
/ ≤
· 2 ln(K  )4
(K  )4 − 1
(K  )4 − 1

=

4 ln K 
(K  )4 − 1

≥1

C LAIM 3.11. If the node v successfully transmitting a message in Ni at time t is different from the node that previously
successfully transmitted a message in Ni , then Pi (t + 1) ∈
1
1
Pi (t), √1+γ
Pi (t)] for any ni ≥ 2.
[ 1+γ





4 ln K
4 ln K
and therefore, E[X] ≤ (K
 )4 −1 · k0 ≤ (K  )4 −1 · gi . Since the
upper bound on E[Xj ] holds independently for each j, it follows
ln K 
from the Chernoff bounds that X ≤ 6(K
 )4 · gi w.h.p.

P ROOF. The lower bound is obvious. Moreover, it follows from
the protocol that

296

Since gi = Ω(log 1+γ N ), X + log1+γ ni < k1 w.h.p. if
K  = O(K) is sufficiently large, which completes the proof of
the claim.

g ≤ log1+γ 2, then certainly P (t) ≤ 12 ln K  for all t in I  .
So suppose that g > log1+γ 2. Consider some fixed network
Ni . Let X be the number of time steps in I  in which Pi (t)
increases and k1 be the number of time steps in I  with a successful message transmission in Ni . Furthermore, let k2 be the
maximum number of times a node v ∈ Vi increases Tv in I  .
If P (t2 ) > 12 ln K  then there must be a network Ni with
Pi (t2 ) > max{(8 ln K  )/K  , 2Pi (t1 )}. To see this, let I1 be the
set of all i with Pi (t1 ) < (4 ln K  )/K  and I2 be the set of all other


i. As long as for all i, P
i (t2 ) ≤ max{(8 ln K )/K
 , 2Pi (t1 )}, it


must hold that P (t2 ) ≤ i∈I1 (8 ln K )/K + i∈I2 2Pi (t1 ) ≤
(8 ln K  )/K  · K + 2P (t1 ) ≤ 12 ln K  if K  = O(K) is sufficiently large.
First, consider the case that for some i with Pi (t1 ) ≥
(4lnK  )/K  , Pi (t2 ) > 2Pi (t1 ). Then the total increase of Pi (t)
in I  (which is equal to (1 + γ)X is at least the total decrease in
Pi (t) plus log1+γ 2. Hence,

Otherwise, suppose that gi < δ log1+γ N . For every node v it
follows from the C O MAC
√ protocol and the choice of f√and F that
if initially Tv ≤ (3/4) F , then Tv can be
√ at most F during
I  . Let us cut I  into m intervals of size 2 F each. It is easy to
check that if β in the definition of f is sufficiently large compared
to δ, then m ≥ 3δ log1+γ N . Since there are less than δ log1+γ N
useful steps in Ni in I  , at least 2δ log 1+γ N of these intervals do
not contain any useful step, which implies that pv is reduced by
(1 + γ) by each v ∈ Vi in each of these intervals.
Hence, altogether, every pv gets reduced by a factor of at least
(1+γ)−2δ log1+γ N during I  in Ni . The useful time steps can only
raise that by at most (1 + γ)δ log1+γ N , so altogether we must have
Pi (t ) ≤ (2 ln K  )/K  at some time point t in I  , w.h.p.
Next we prove the following claim, which implies that for all
t > t in I  , Pi (t ) < (4 ln K  )/K  w.m.p.

X ≥ k1 + log1+γ 2

where k1 is the total decrease (in the exponent) of P (t) due to
successful transmissions in Ni . From Inequality (5) we know that
K
K
· E[k0 ] and therefore E[k1 ] ≥ 2 ln
· g if K  =
E[k1 ] ≥ 4 ln
K
K

O(K) is large enough. Since E[k1 ] ≥ k1 /6 and g = Ω(ln f ) it

follows from the Chernoff bounds that k1 ≥ ln4KK ·g w.m.p. On the
6 ln K 
other hand, we also know that X ≤ (K  )4 ·g w.m.p., which implies
that Inequality (7) is violated w.m.p. Hence, Pi (t2 ) ≤ 2Pi (t1 )
w.m.p.
For the case that Pi (t1 ) < (4 ln K  )/K  let t1 be the first step
in I  with Pi (t1 ) ≥ (4 ln K  )/K  . If t1 does not exist, we are
done, and otherwise we prove in the same way as above that w.m.p.
Pi (t2 ) ≤ (12 ln K  )/K  .
Since there are at most f 2 ways of choosing t1 and t2 , there is
no time step t in I  with P (t) ≤ 12 ln K  w.m.p., which completes
the proof.

C LAIM 3.13. If all time steps t ∈ I  satisfy P (t) ≥ 4 ln K  and
initially Pi (t) ≤ (2 ln K  )/K  , then for all steps t ∈ I  , Pi (t) ≤
(4 ln K  )/K  w.m.p.
P ROOF. Consider some fixed subinterval I  = [t1 , t2 ) in I 
with the property that Pi (t1 ) ≤ (2 ln K  )/K  and Pi (t) ≥
(2 ln K  )/K  for all other t in I  . Suppose that there are gi useful
time steps in I  . If gi ≤ ln1+γ 2, then it follows for the probability
K
· (1 + γ)ln1+γ 2 ≤
Pi (t2 ) at the end of I  that Pi (t2 ) ≤ 2 ln
K
4 ln K 
. Otherwise, suppose that gi > ln1+γ 2, which is at least
K
1/(2γ) = Ω(ln f ). Let X be the number of time steps in I  in
which Pi (t) increases and k1 be the number of time steps in I 
with a successful transmission in Ni . Furthermore, let k2 be the
maximum number of times a node v ∈ Vi increases Tv in I  . If
P (t2 ) > (4 ln K  )/K  then it must hold that the total increase in
Pi (t) (which is equal to (1 + γ)X ) is at least the total decrease in
P (t) (which is at most (1 + γ)k1 +k2 ) plus ln1+γ 2, or formally,
X≥

k1

+ ln1+γ 2

(7)

All claims combined imply Lemma 3.10.

(6)

A proof similar to Lemma 3.10 also implies the following result.

k1

is the total decrease (in the exponent) of Pi (t) due to
where
successful transmissions. We know that E[k1 ] ≥ k1 /6. Also, from

the proof of the previous claim it follows that E[k1 ] ≥ lnKK gi if

K = O(K) is a sufficiently large constant, unless there is a time
step t in I  with Pi (t) < (2 ln K  )/K  . Since gi = Ω(ln f ), it

follows from the Chernoff bounds that k1 ≥ ln8KK gi w.m.p. On
the other hand, it follows from the proof of the previous claim that
ln K 
X ≤ 6(K
 )4 · gi w.m.p. Hence, inequality (6) is violated w.m.p.,

C OROLLARY 3.15. For any subframe I  that satisfies P (t) ≤
12 ln K  at the beginning of I  , all time steps t of I  satisfy P (t) ≤
36 ln K  w.m.p.
We also need to show that for a constant fraction of the nonjammed time steps in a subframe where initially P (t) ≤ 12 ln K  ,
P (t) is also lower bounded by a constant for a sufficiently large
fraction of time steps t.



K
w.m.p. Since there are at most
which implies that Pi (t2 ) ≤ 4 ln
K
2
f different values of t1 and t2 , there is no time step t2 in I  with
K
w.m.p., which completes the proof.
Pi (t2 ) > 4 ln
K

For any subframe I  in which initially P (t0 ) ≥
L EMMA 3.16.
√
2
2 f
1/(f (1 + γ)
), at least ε/8 of the non-jammed steps t satisfy
P (t) ≥ εp̂/4, w.h.p.

Combining the insights above, it follows that there must be a
time step t in I  with P (t) < 4 ln K  w.m.p. To finish the proof,
we need the following claim.

P ROOF. Let G be the set of all non-jammed time steps in I  and
S be the set of all steps t in G with P (t) < εp̂/4. Let g = |G| and
s = |S|. If s ≤ (1 − ε/8)g, we are done. Hence, consider the case
that s ≥ (1 − ε/8)g.
Suppose that P (t) must be increased  many times to get from
its initial value up to a value of εp̂/4. (If P (t0 ) ≥ εp̂/4 then
 = 0.) Let k0 be the number of time steps in S with an idle
channel and k1 be the number of time steps in S with a successful
message transmission in any of the co-existing networks. Let the
binary random variable Xi be 1 if and only if the nodes increase
their access probabilities in the i-th idle time step in S, and let

C LAIM 3.14. If for the first time step t0 in I  , P (t0 ) ≤ 4 ln K  ,
then P (t) ≤ 12 ln K  for all time steps t in I  w.m.p.
P ROOF. Consider some subinterval I  = [t1 , t2 ) in I  with
the property that P (t1 ) ≤ 4 ln K  and P (t) ≥ 4 ln K  for
all t > t1 in I  . Suppose that there are g useful time steps
in I  , where a time step is useful if there was either a successful transmission in some network or the channel is idle. If

297


X = i=1 Xi . Furthermore, let k2 be the maximum number of
times a node v decreases pv due to cv > Tv in I  . For S to be
feasible (i.e., probabilities can be assigned to each t ∈ S so that
P (t) < εp̂/4), we must have
X

≤

 + k1 + k2

3.3 Throughput
Summarizing the results above, we obtain the following result
for the throughput.
T HEOREM 3.20. For any polynomial sequence of time steps of
length at least F , C O MAC achieves a competitive throughput of
Ω(ε2 min{ε, 1/poly(K)}) for any constants ε and K.

(8)

For the special case that  = k2 = 0 this follows from the fact
that whenever there is a successful message transmission, P (t) is
reduced by (1 + γ)−1 , at most. On the other hand, whenever the
nodes decide to increase P (t) for some t ∈ S, P (t) can indeed
increase because of P (t) < εp̂/4 and therefore pv < p̂ for all
v. Thus, if X > k1 , then one of the steps in S would have to
have a probability of at least εp̂/4, violating the definition of S. 
comes into the formula due to the startup cost of getting to a value
of εp̂/4, and k2 comes into the formula since the reductions of the
pv (t) values due to cv > Tv allow up to k2 additional decreases of
P (t) for S to stay feasible.
√
Certainly,  ≤ 2 log1+γ f + 2 f . Moreover, for k1 it holds that
E[k1 ] ≤ εp̂/4 · s and therefore,
√ k1 ≤ εp̂/2 · s w.h.p. For k2 it holds
that k2 ≤ (X + εg/8)/2 + f . Hence, Inequality (8) implies that
⇒

X

≤

X

≤



2 log1+γ f + 2 f + εp̂s/2 + (X + εg/8)/2 + f

(p̂ + 1/16)εg + 8 f

3.4 Fairness
Finally, we show that C O MAC also ensures a limited degree
of fairness. Note that by Lemma 3.4, we can directly bound the
probabilities of having a successful transmission within networks
Ni and Nj by their respective cumulative probabilities, which we
bound on the following theorem.
T HEOREM 3.21. If all nodes v initially start with access probability p̂, then it takes at most F time steps until a time step is
reached in which the difference between minimum and maximum
cumulative probability of a network is at most O(K 2 ).
P ROOF. Consider the potential function summing up the differences of the networks’ cumulative
probabilities compared to the

minimum probability Φ = i |xi − xmin | where xi = log 1+γ Pi
and xmin = mini xi . We focus on the events with a successful transmission, since only successful transmissions can change
the difference among individual network probabilities. Assume
that a successful tranmission occured in Ni , if xi > xmin , then
the change in Φ, denoted by ΔΦ, satisfies ΔΦ = −1. If
xi = xmin , then ΔΦ ≤ K. Hence, E[ΔΦ] ≤ −P[xi >
xmin successful] + KP[xi = xmin successful]. Suppose that
xmax ≥ xmin + log1+γ (2K 2 ). Then, P[xi > xmin successful] ≥
2K · P[xi = xmin successful] as there can be up to K − 1 many
Ni with xi = xmin . Certainly, P[xi > xmin successful] + P[xi =
xmin successful] = 1 given that there is a successful transmission.
2K
, which imHence in this case, P[xi > xmin successful] ≥ 2K+1
2K
K
K
plies that E[ΔΦ] ≤ − 2K+1 + 2K+1 = − 2K+1 ≤ −1/3, whenever there is a successful transmission.
Now, let us define the random variable Xt as follows for the
t-th successful transmission: Xt = 1 if either xmax < xmin +
log1+γ (2K 2 ) (i.e., we reached our goal) or the successful transmission is from a network Ni with xi > xmin ; and Xt = −K
otherwise.
Suppose that thereare s successful transmissions across all nets
works. Let X =
t=1 Xt . Then it holds that E[X] ≥ s/3.
In order to apply Chernoff
bounds, let us define Yt = (Xt +
s
K)/(K + 1) and Y =
t=1 Yt . Then Yt is a binary random variable with E[Yt ] ≥ (K + 1/3)/(K + 1) and therefore
E[Y ] ≥ s(K + 1/3)/(K + 1). Since the upper bound on E[Yt ]
holds irrespective of previous Yj ’s, it follows from the Chernoff
2
bounds that P[Y ≤ (1 − δ)s(K + 1/3)/(K + 1)] ≤ e−δ s/3 ,
for any 0 < δ < 1. Since Y = (X + s · K)/(K + 1),
2
we get P[X ≤ (1 − δ)s/3 − δsK] ≤ e−δ s/3 . If we choose
δ = 1/(6(K + 1/3)) then P[Y ≤ (1 − δ)s(K + 1/3)/(K + 1)] =
2
P[X ≤ s/6] and hence, P[X ≤ s/6] ≤ e−δ s/3 . Now, from The2
orem 3.20 we know that s = Ω(ε min{ε, 1/poly(K)}F ) w.h.p.,
so s = ω(K log N ). This implies that when running the protocol
for F time steps, X > K log N w.h.p. Thus, if the initial value of
the potential Φ0 is at most K log N , we must have reached a point
where xmax < xmin + log1+γ (2K 2 ) as otherwise we would end
up with a negative potential. It remains to bound Φ0 .
Given that all nodes start with the same access probability p̂,
the maximum initial difference between Pi and Pj for any i and
j is N and therefore, xmax < xmin + log1+γ N . Hence, Φ0 ≤
K log1+γ N , which implies the theorem.

(9)

if f is sufficiently large. It remains to compute a lower bound for
X.
Let X  be the total number of times P (t) is increased over all
time steps in G, k0 be the number of idle time steps in G, and q̄
be the average increase of the qv -values in I  . From the proof of
Claim 3.7 we know that q̄ ≥ (k0 −1)/f and that X  ≥ (k0 −1)q̄.
Moreover, X ≥ X  − εg/8. Hence, X ≥ (k0 − 1)2 /f  − εg/8.
We know that E[k0 ] ≥ (1 − εp̂/4)s and therefore, k0 ≥ 3g/4
w.h.p. Hence, X ≥ g 2 /(4f ) − εg/8 ≥ εg/8 w.h.p. Since this
violates Inequality (9), the lemma follows.
In the following, let us call a subframe I  good if its initial step
t0 satisfies P (t0 ) ≤ 12 ln K  . Combining the results above, we
get:
L EMMA 3.17. For any good subframe I  , there are at least
ε f /8 non-jammed time steps t in I  with P (t) ∈ [εp̂/4, 36 ln K  ]
w.m.p.
2

Consider now the first eighth of frame I, called J. The following
lemma follows directly from Lemma 2.14 in [3].
√

2
2 f
)
L EMMA 3.18.
√ If at the beginning of J, pv ≥ 1/(f (1+γ)
and Tv ≤
F
/2
for
all
nodes
v,
then
we
also
have
p
≥
v
√
1/(f 2 (1 + γ)2 f ) at the end of J for every v and the number
of non-jammed time steps t in I  with P (t) ∈ [εp̂/4, 36 ln K  ] is at
least ε2 f /16 w.h.p.

We finally need the following lemma, which follows from
Lemma 2.15 in [3].
√
of J, Tv ≤ F /2 for all v,
L EMMA 3.19. If at the beginning
√
then it holds that also Tv ≤ F /2 at the end of J w.h.p.
Inductively using Lemmas 3.18 and 3.19 on the eighths of frame
I implies that C O MAC satisfies the property of Lemmas √3.18 for
2
2 f
) and
the entire
√ I and at the end of I, pv ≥ 1/(f (1 + γ)
Tv ≤ F /2 for all v w.h.p. Since our results hold with high
probability, we can also extend them to any polynomial number of
frames.

298

Throughput

0.5
0.4
0.3
0.2
0.1
0
1

2

3

4

5

6

7

Number of Networks

8

9

10

MIN/MAX Throughput Ratio

AntiJam, ε=0.5
CoMac, ε=0.5
AntiJam, ε=0.3
CoMac, ε=0.3

1

0.75

0.5

0.25

0

1

2

3

4

5

6

7

8

9

Number of Networks

10

AntiJam, ε=0.5
CoMac, ε=0.5
AntiJam, ε=0.3
CoMac, ε=0.3

0.5

Throughput

MIN/MAX Throughput Ratio

Figure 1: Left: Throughput of C O MAC and A NTI JAM [24] as a function of the number of co-existing networks and for two different
adversaries (ε = {0.5, 0.3}). The total number of nodes for each K = 1, . . . , 10 is 500, and each co-existing network has the same
size (up to an additive node due to rounding). The protocol is executed for 7000 rounds, and the result is averaged over 10 runs. The
adversary is modeled in a simplified manner and simply jams each round with independent probability 1 − ε. Right: Fairness as the
min/max competitive throughput ratio for ε = 0.3.

0.4
0.3
0.2
0.1
0
1

2

3

4

5

6

7

Number of Networks

8

9

10

1

0.75

0.5

0.25

0

1

2

3

4

5

6

7

8

9

Number of Networks

10

Figure 2: Left: Throughput and fairness of C O MAC and A NTI JAM [24] for a setting like in Figure 1 but where the size of the
co-existing networks is heterogenous, i.e., the i-th largest network is roughly 1.5 times the size of (i + 1)-largest network. Right:
Fairness as the min/max competitive throughput ratio for ε = 0.3.

Fact 3.1 ensures that the access probabilities of the nodes within
a network differs by at most a (1 + γ) factor, ensuring fairness
within each network Ni .

4.

Scenario 1: The size of individual networks are the same,
namely |Vi | ∈ {500/K, 	500/K
}. In Figure 1 (left) we study
the competitive throughput, i.e., the fraction of non-jammed time
steps that are used for successful transmissions among all K networks. We observe that for a single network (K = 1) the competitive throughput of C O MAC is relatively worse compared to
A NTI JAM as pv is raised more strictly when the channel is idle.
However, C O MAC is always better than A NTI JAM when there is
more than one network (K > 1) as the additional interference introduced by co-existing networks is bounded. For example, when
K = 10, the competitive throughput of C O MAC is still above 20%
even when adversary can jam 70% of all time steps, while the competitive throughput of A NTI JAM is below 10%. Note that there is
a trend towards smaller competitiveness for larger K, as expected
from our formal worst-case analysis. Figure 1 (right) studies the
fairness of C O MAC in terms of min/max competitive throughput
ratio, where the minimum and maximum competitive throughput
are selected from the K co-existing networks. The closer this ratio
is to 1, the fairer the protocol. Obviously C O MAC is fair in a sense
that even when K = 10, the min/max competitive throughput ratio
is above 0.78.
Scenario 2: The size of i-th largest network is roughly 1.5 times
the size of (i+1)-th largest network. Figure 2 shows that even when

SIMULATION

Although the focus of this paper is on the formal, asymptotic and
worst-case performance guarantees achieved by C O MAC, we also
briefly report on some of our quantitative insights from a simulation
study. We are interested in: (i) how the competitive throughput of
all the networks changes when the number of networks varies;2 (ii)
the fairness of C O MAC, i.e., whether the successful transmissions
are evenly distributed among all the networks. Also, we compare
C O MAC to the state-of-the-art jamming resistent MAC protocol
A NTI JAM in [24], and find that C O MAC indeed better suits coexisting networks.
There is a total of 500 nodes among all the co-existing networks,
and the number of networks K ranges from 1 to 10. All the results
are averaged over 10 runs, and the confidence intervals are provided
as well. More specifically, we conduct competitive throughput and
fairness experiments in two different scenarios.
2

The competitive throughput of all the networks is defined as the
fraction of non-jammed time steps that are used for successful
transmissions among all K networks.

299

the size of individual networks vary a lot, C O MAC still achieves
a better competitive throughput (above 20% when K = 10) compared to A NTI JAM (below 10% when K = 10), and more importantly, C O MAC is still fair in a sense that the min/max competitive
throughput ratio when K = 10 is still above 0.73.

5.

[11]
[12]

CONCLUSION

[13]

Motivated by our observation that MAC algorithms optimized
for a single network often yield a poor performance in scenarios
with multiple co-existing networks due to too high sending probabilities, this paper presented the first protocol for provably robust,
efficient and fair medium allocation among a set of co-existing
networks (e.g., of a multi-nation conference or of an emergency
network). Interestingly, with simple adaptions, our protocol could
even be used in scenarios where the throughput is required to be
distributed according to some specific proportions (i.e., not necessarily fairly) among the co-existing networks. For instance, a
spectrum owner may require the co-existing networks to use only a
share of the medium that corresponds to the negotiated or auctioned
share. We believe that our work raises a series of interesting questions for future research. For example, we have assumed a rather
naive interference model and it would be interesting to generalize
our results for the SINR physical interference model.

[14]

[15]

[16]

[17]
[18]

Acknowledgments

[19]

We would like to thank Çiğdem Şengül and Ruben Merz from
Telekom Innovation Laboratories for interesting discussions. This
work was supported in part by NSF awards CCF-0830791 and
CCF-0830704, and by DFG projects SCHE 1592/2-1 and SFB 901.

6.

[20]

REFERENCES

[21]

[1] L. Anantharamu, B. S. Chlebus, D. R. Kowalski, and M. A.
Rokicki. Medium access control for adversarial channels
with jamming. In Proc. 18th International Conference on
Structural Information and Communication Complexity
(SIROCCO), pages 89–100, 2011.
[2] H. Asai, K. Fukuda, and H. Esaki. Towards characterization
of wireless traffic in coexisting 802.11a/g and 802.11n
network. In Proc. ACM CoNEXT Student Workshop, 2010.
[3] B. Awerbuch, A. Richa, and C. Scheideler. A
jamming-resistant mac protocol for single-hop wireless
networks. In Proc. PODC, 2008.
[4] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman,
and B. Thapa. On the performance of IEEE 802.11 under
jamming. In Proc. INFOCOM, 2008.
[5] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul,
and C. E. Leiserson. Adversarial contention resolution for
simple channels. In Proc. SPAA, pages 325–332, 2005.
[6] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki.
Adversarial queuing on the multiple-access channel. In Proc.
PODC, pages 92–101, 2006.
[7] S. Dolev, S. Gilbert, R. Guerraoui, D. R. Kowalski,
C. Newport, F. Kuhn, and N. Lynch. Reliable distributed
computing on unreliable radio channels. In Proc. 2009
MobiHoc S3 Workshop, 2009.
[8] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport.
Gossiping in a Multi-Channel Radio Network (An Oblivious
Approach to Coping With Malicious Interference). In Proc.
21st International Symposium on Distributed Computing
(DISC), pages 208–222, 2007.
[9] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Secure
communication over radio channels. In Proc. 27th ACM
Symposium on Principles of Distributed Computing
(PODC), pages 105–114, 2008.
[10] S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport.
Interference-resilient information exchange. In Proc. 28th

[22]
[23]
[24]
[25]

[26]
[27]

[28]
[29]

[30]

300

IEEE International Conference on Computer
Communications (INFOCOM), pages 2249–2257, 2009.
S. Gilbert, R. Guerraoui, and C. Newport. Of malicious
motes and suspicious sensors: On the efficiency of malicious
interference in wireless networks. In Proc. OPODIS, 2006.
L. A. Goldberg, P. D. Mackenzie, M. Paterson, and
A. Srinivasan. Contention resolution with constant expected
delay. Journal of the ACM, 47(6):1048–1096, 2000.
J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff
protocols for mulitiple access channels. SIAM Journal on
Computing, 25(4):740–774, 1996.
M. Heusse, F. Rousseau, R. Guillier, and A. Duda. Idle
sense: an optimal access method for high throughput and
fairness in rate diverse wireless LANs. SIGCOMM Comput.
Commun. Rev., 35(4):121–132, 2005.
J. Ji and W. Chen. Transmission capacity of two co-existing
wireless ad hoc networks with multiple antennas. In Proc.
IEEE International Conference on Communications (ICC),
pages 1–6, 2011.
V. King, J. Saia, and M. Young. Conflict on a communication
channel. In Proc. 30th Annual ACM Symposium on
Principles of Distributed Computing (PODC), pages
277–286, 2011.
C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable
broadcast in radio networks: The bounded collision case. In
Proc. PODC, 2006.
B.-J. Kwak, N.-O. Song, and L. E. Miller. Performance
analysis of exponential backoff. IEEE/ACM Transactions on
Networking, 13(2):343–355, 2005.
D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer.
Speed dating despite jammers. In Proc. DCOSS, June 2009.
G. Nychis, R. Chandra, T. Moscibroda, I. Tashev, and
P. Steenkiste. Reclaiming the white spaces: spectrum
efficient coexistence with primary users. In Proc. 7th
Conference on Emerging Networking Experiments and
Technologies (CoNEXT), 2011.
A. Pelc and D. Peleg. Feasibility and complexity of
broadcasting with random transmission failures. In Proc.
PODC, 2005.
P. Raghavan and E. Upfal. Stochastic contention resolution
with short delays. SIAM Journal on Computing,
28(2):709–719, 1999.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A
jamming-resistant mac protocol for multi-hop wireless
networks. In Proc. DISC, 2010.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Competitive and fair medium access despite reactive
jamming. In Proc. 31st IEEE ICDCS, 2011.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Self-stabilizing leader election for single-hop wireless
networks despite jamming. In Proc. 12th ACM MobiHoc,
2011.
A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Towards
jamming-resistant and competitive medium access in the
SINR model. In Proc. 3rd Annual ACM S3 Workshop, 2011.
A. Santoso, Y. Tang, B. Vucetic, A. Jamalipour, and Y. Li.
Interference cancellation in coexisting wireless local area
networks. In Proc. 10th IEEE Singapore International
Conference on Communication Systems, pages 1–7, 2006.
J. Schmidt, A. Siegel, and A. Srinivasan. Chernoff-Hoeffding
bounds for applications with limited independence. SIAM
Journal on Discrete Mathematics, 8(2):223–250, 1995.
M. Young and R. Boutaba. Overcoming adversaries in sensor
networks: A survey of theoretical models and algorithmic
approaches for tolerating malicious interference. IEEE
Communications Surveys and Tutorials, 13(4):617–641,
2011.
G. Zhou, J. A. Stankovic, and S. H. Son. Crowded spectrum
in wireless sensor networks. In Proc. 3rd Workshop on
Embedded Networked Sensors (EmNets), 2006.

Distrib. Comput. (2013) 26:159–171
DOI 10.1007/s00446-012-0180-x

Competitive throughput in multi-hop wireless networks despite
adaptive jamming
Andrea Richa · Christian Scheideler ·
Stefan Schmid · Jin Zhang

Received: 20 December 2010 / Accepted: 6 June 2012 / Published online: 11 September 2012
© Springer-Verlag 2012

Abstract This article presents a simple local medium
access control protocol, called Jade, for multi-hop wireless
networks with a single channel that is provably robust against
adaptive adversarial jamming. The wireless network is
modeled as a unit disk graph on a set of nodes distributed
arbitrarily in the plane. In addition to these nodes, there are
adversarial jammers that know the protocol and its entire history and that are allowed to jam the wireless channel at any
node for an arbitrary (1 − )-fraction of the time steps, where
0 <  < 1 is an arbitrary constant. We assume that nodes
can perform collision detection (unless they are transmitting themselves), but that they cannot distinguish between
jammed transmissions and collisions of regular messages.
Nevertheless, we show that Jade achieves an asymptotically optimal throughput by efficiently exploiting the unpredictable time periods in which the medium is available.

A preliminary version of this article appeared at the 24th International
Symposium on Distributed Computing (DISC), 2010. Research is
partly supported by NSF grants CCF-1116368 and CCF-0830704, as
well as by DFG grants DFG SFB 901 and DFG SCHE 1592/2-1.
A. Richa · J. Zhang
Computer Science and Engineering, SCIDSE, Arizona State
University, Tempe, AZ 85287, USA
e-mail: aricha@asu.edu
J. Zhang
e-mail: jzhang82@asu.edu
C. Scheideler (B)
Universität Paderborn, 33102 Paderborn, Germany
e-mail: scheideler@upb.de
S. Schmid
Telekom Innovation Laboratories (T-Labs), Technische Universität
Berlin, 10587 Berlin, Germany
e-mail: stefan@net.t-labs.tu-berlin.de

Keywords Medium access · Jamming · Distributed
algorithms · Randomization

1 Introduction
The problem of coordinating the access to a shared medium is
a central challenge in wireless networks. In order to solve this
problem, a proper medium access control (MAC) protocol is
needed. Ideally, such a protocol should not only be able to use
the wireless medium as effectively as possible, but it should
also be robust against attacks. Unfortunately, most of the
MAC protocols today can be easily attacked. A particularly
critical class of attacks are jamming attacks (i.e., denial-ofservice attacks on the broadcast medium). Jamming attacks
are typically easy to implement as the attacker does not need
any special hardware. Attacks of this kind usually aim at the
physical layer and are realized by means of a high transmission power signal that corrupts a communication link or an
area, but they may also occur at the MAC layer, where an
adversary may either corrupt control packets or reserve the
channel for the maximum allowable number of slots so that
other nodes experience low throughput by not being able
to access the channel. In this article we focus on jamming
attacks at the physical layer, that is, the interference caused
by the jammer will not allow the nodes to receive messages.
The fundamental question that we are investigating is: Is there
a MAC protocol such that for any physical-layer jamming
strategy, the protocol will still be able to achieve an asymptotically optimal throughput for the non-jammed time steps?
Such a protocol would force the jammer to jam all the time
in order to prevent any successful message transmissions.
Finding such a MAC protocol is not a trivial problem. In
fact, the widely used IEEE 802.11 MAC protocol already
fails to deliver any messages for very simple oblivious

123

160

jammers that jam only a small fraction of the time steps [3].
On the positive side, Awerbuch et al. [2] have demonstrated
that there are MAC protocols which are provably robust
against even massive adaptive jamming, but their results only
hold for single-hop wireless networks with a single jammer,
that is, all nodes experience the same jamming sequence.
In this article, we significantly extend the results in [2].
We present a MAC protocol called Jade (a short form of
“jamming defense”) that can achieve a constant fraction of
the best possible throughput for a large class of jamming
strategies in a large class of multi-hop networks where transmissions and interference can be modeled using unit-disk
graphs. These jamming strategies include jamming patterns
that can be completely different from node to node. It turns
out that while Jade differs only slightly from the MAC protocol of [2], the proof techniques needed for the multi-hop
setting significantly differ from the techniques in [2].
1.1 Model
We consider the problem of designing a robust MAC protocol
for multi-hop wireless networks with a single wireless channel. The wireless network is modeled as a unit disk graph
(UDG) G = (V, E) where V represents a set of n = |V |
honest and reliable nodes and two nodes u, v ∈ V are within
each other’s transmission range, i.e., {u, v} ∈ E, if and only
if their (normalized) distance is at most 1. We assume that
time proceeds in synchronous time steps called rounds. In
each round, a node may either transmit a message or sense
the channel, but it cannot do both. Moreover, we assume
that a (receiving) node can detect collisions. Concretely, a
node which is sensing the channel may either (1) sense an
idle channel (if no other node in its transmission range is
transmitting at that round and its channel is not jammed), (2)
sense a busy channel (if two or more nodes in its transmission
range transmit at that round or its channel is jammed), or (3)
receive a packet (if exactly one node in its transmission range
transmits at that round and its channel is not jammed).
In addition to these nodes there is an adversary (controlling
any number of jamming devices). We allow the adversary
to know the protocol and its entire history and to use this
knowledge in order to jam the wireless channel at will at any
round (i.e, the adversary is adaptive). However, like in [2], the
adversary has to make a jamming decision before it knows the
actions of the nodes at the current round. The adversary can
jam the nodes individually at will, as long as for every node
v, at most a (1 − )-fraction of its rounds is jammed ( > 0
can be an arbitrarily small constant independent of n), among
which at least an arbitrary constant fraction are open: We say
a round t is open for a node v if v and at least one other node
in its neighborhood are non-jammed (which implies that v’s
neighborhood is non-empty). More formally, an adversary is
(T, 1−)-bounded for some T ∈ N and 0 <  < 1, if for any

123

A. Richa et al.

time window of size w ≥ T and at any node v, the adversary
can jam at most (1 − )w of the w rounds at v, and at least
an arbitrary constant fraction of the non-jammed rounds at
v are open in every time interval of size w. We, later in this
paper, will also consider a stronger adversary that does not
have the limitation of providing open rounds. While if not
stated otherwise and by default, we will always refer to the
adversary defined here, we will sometime explicitly use the
adjective weak to distinguish this adversary from the stronger
variant.
Given a node v and a time interval I of size |I | (in terms
of rounds), we define f v (I ) as the number of time steps
in I that are non-jammed at v and sv (I ) as the number
of time steps in I in which v successfully receives a message. A MAC protocol is called c-competitive against some
(T, 1 − )-bounded adversary if, for any time interval I with
|I | ≥ K 
for a sufficiently 
large K (that may depend on
T and n), v∈V sv (I ) ≥ c · v∈V f v (I ). In other words, a
c-competitive MAC protocol can achieve at least a c-fraction
of the best possible throughput.
Our goal is to design a symmetric local-control MAC
protocol (i.e., there is no central authority controlling the
nodes, and all the nodes are executing the same protocol) that
has a constant-competitive throughput (i.e., a c-competitive
throughput where c does not depend on n) against any
(T, 1 − )-bounded adversary in any multi-hop network that
can be modeled as a UDG. Not only the nodes are distributed
in space in our model, but also the adversary. Concretely, we
introduce the concept of a k-uniform adversary, an adversary
that can jam different nodes at different times. An adversary
is k-uniform if the node set V can be partitioned into k subsets
so that the jamming sequence is the same within each subset.
In other words, we require that at all times, the nodes in a
subset are either all jammed or all non-jammed. Thus, a 1uniform jammer jams either everybody or nobody in a round
whereas an n-uniform jammer can jam the nodes individually
at will. Note that the adversary must hence not necessarily
be geometrically constrained.
As already mentioned, this article also discusses a stronger
adversary: we say that a strong adversary is (T, 1 − )bounded, if for any time window of size w ≥ T and at any
node v, the adversary can jam at most (1 − )w of the w
rounds at v, where T ∈ N and 0 <  < 1. Note that this
adversary is stronger as we only guarantee that an -fraction
of the rounds at v are non-jammed, but not that during these
rounds there exists at least one neighbor free to receive a
message from v. While the nodes do not know , we do
allow them to have a very rough upper bound of the values n
and T .
Finally, let us emphasize that our notion of throughput is
constrained to Layer 2 (the MAC layer), and measures the
number of successful transmissions over “links”, i.e., pairs
of nodes. That is, assuming a backlogged situation where

Competitive throughput

packets are constantly submitted to the medium access layer
from higher layers, we can schedule transmissions over Layer
2 links efficiently. In contrast to other throughput models in
literature (e.g., [40]), we explicitly consider the receiver-side
which we believe is much more meaningful: in a broadcast
medium and in a distributed setting, the throughput computed by focusing on the sender only can be misleading as
simply sending a packet out does not imply that it is also
received (and by how many nodes). However, also note that a
(MAC layer) link-based throughput does not imply any minimal end-to-end throughput between remote nodes on higher
layers, e.g., on the transport layer (especially when using
TCP with its flow and congestion control mechanisms), or
throughput of flows. Moreover, note that we do not model any
retransmissions that would happen on higher layers. Indeed,
our MAC protocol has the nice property that it does not rely
on any acknowledgements on the MAC layer to guarantee the
throughput, and assumes that retransmission mechanisms are
in place on higher layers.
In this article, we will say that a claim holds with high
probability (w.h.p.) iff it holds with probability at least
1 − 1/n c for any constant c ≥ 1; it holds with moderate probability (w.m.p.) iff it holds with probability at least
1 − 1/(log n)c for any constant c ≥ 1.
1.2 Related work
Due to the topic’s importance, wireless network jamming
has been extensively studied in the applied research fields
[1,5,6,22,26–28,30,31,39,41–43], both from the attacker’s
perspective [6,26,27,43] as well as from the defender’s perspective [1,5,6,27,28,30,41,43]—also in multi-hop settings
(e.g. [21,32,45–47]).
Traditionally, jamming defense mechanisms operate on
the physical layer [28,30,38]. Mechanisms have been
designed to avoid jamming as well as detect jamming. Spread
spectrum technology has been shown to be very effective
to avoid jamming as with widely spread signals, it becomes
harder to detect the start of a packet quickly enough in order to
jam it. Unfortunately, protocols such as IEEE 802.11b use relatively narrow spreading [20], and some other IEEE 802.11
variants spread signals by even smaller factors [5]. Therefore, a jammer that simultaneously blocks a small number
of frequencies renders spread spectrum techniques useless
in this case. As jamming strategies can come in many different flavors, detecting jamming activities by simple methods
based on signal strength, carrier sensing, or packet delivery
ratios has turned out to be quite difficult [27].
Recent work has also studied MAC layer strategies against
jamming, including coding strategies [6], channel surfing and
spatial retreat [1,44], or mechanisms to hide messages from
a jammer, evade its search, and reduce the impact of corrupted messages [41]. Unfortunately, these methods do not

161

help against an adaptive jammer with full information about
the history of the protocol, like the one considered in our
work.
In the theory community, work on MAC protocols has
mostly focused on efficiency. Many of these protocols are
random backoff or tournament-based protocols [4,7,17,18,
25,34] that do not take jamming activity into account and,
in fact, are not robust against it (see [2] for more details).
The same also holds for many MAC protocols that have
been designed in the context of broadcasting [8] and clustering [24]. Also some work on jamming is known (e.g.,
[9] for a short overview). There are two basic approaches
in the literature. The first assumes randomly corrupted messages (e.g. [33]), which is much easier to handle than adaptive adversarial jamming [3]. The second line of work either
bounds the number of messages that the adversary can transmit or disrupt with a limited energy budget (e.g. [16,23]) or
bounds the number of channels the adversary can jam (e.g.
[10–15,29]).
The protocols in [16,23] can tackle adversarial jamming
at both the MAC and network layers, where the adversary
may not only be jamming the channel but also introducing
malicious (fake) messages (possibly with address spoofing).
However, they depend on the fact that the adversarial jamming budget is finite, so it is not clear whether the protocols
would work under heavy continuous jamming. (The result in
[16] seems to imply that a jamming rate of 1/2 is the limit
whereas the handshaking mechanisms in [23] seem to require
an even lower jamming rate.)
In the multi-channel version of the problem introduced in
the theory community by Dolev [13] and also studied in [10–
15,29], a node can only access one channel at a time, which
results in protocols with a fairly large runtime (which can be
exponential for deterministic protocols [11,14] and at least
quadratic in the number of jammed channels for randomized
protocols [12,29] if the adversary can jam almost all channels
at a time). Recent work [10] also focuses on the wireless synchronization problem which requires devices to be activated
at different times on a congested single-hop radio network to
synchronize their round numbering while an adversary can
disrupt a certain number of frequencies per round. Gilbert
et al. [15] study robust information exchange in single-hop
networks.
Our work is motivated by the work in [3] and [2]. In
[3] it is shown that an adaptive jammer can dramatically
reduce the throughput of the standard MAC protocol used in
IEEE 802.11 with only limited energy cost on the adversary
side. Awerbuch et al. [2] initiated the study of throughputcompetitive MAC protocols under continuously running,
adaptive jammers, but they only consider single-hop wireless networks. (Their approach has later been extended to
reactive jamming environments [35] and applications such
as leader election [36].)

123

162

We go one step further by considering multi-hop networks
where different nodes can have different channel states at a
time, e.g., a transmission may be received only by a fraction
of the nodes. It turns out that while the MAC protocol of [2]
can be adopted to the multi-hop setting with a small modification, the proof techniques cannot. We are not aware of any
other theoretical work on MAC protocols for multi-hop networks with provable performance against adaptive jamming.
1.3 Our contributions
In this article, we present a robust MAC protocol called Jade.
Jade is a fairly simple protocol: it is based on a small set of
rules and assumptions (e.g., collision detection at receivers),
and has a minimal storage overhead. We can prove the following main theorem:
Theorem 1 When running Jade for Ω([T+(log3 n)/(γ 2 )]·
(log n)/) rounds it holds w.h.p. that Jade achieves a constant competitive throughput (i.e., independent of n) for any
(T, 1 − )-bounded (weak) adversary, where n is the total
number of nodes and γ ∈ O(1/(log T + log log n) is a parameter.
Since log T and log log n are small the assumption on γ
is not too restrictive: A conservative estimate on log T and
log log n would leave room for a superpolynomial change in
n and a polynomial change in T over time. Also note that
the (unrealistic and non-scalable) assumption that the nodes
know constant factor approximations of n or T directly would
render the problem trivial. (Whether a competitive MAC protocol exists without any assumptions on the magnitude of
these parameters is an open question. We conjecture no such
algorithm exists.)
Regarding the strong adversary, we can show constant
throughput only if one of the conditions in Theorem 2 is
satisfied.
Theorem 2 When running Jade for Ω((T log n)/ +
(log n)4 /(γ )2 ) rounds, Jade has a constant competitive
throughput against any strong adversary that is (T, 1 − )bounded and in any UDG w.h.p., as long as (a) the adversary
is 1-uniform and the UDG is connected, or (b) there are at
least 2/ nodes within the transmission range of every node.
In Sect. 3.4, we show that Theorem 2 captures all the
scenarios for which Jade can have a constant competitive
throughput under a strong adversary.
Concretely, we will show the following limitations under
a strong adversary.
Theorem 3 In general, Jade is not strongly c-competitive
for a constant c > 0 (independent of n) if the strong adversary is allowed to be 2-uniform and  ≤ 1/3. Moreover, Jade
is also not c-competitive for a constant c if there are nodes

123

A. Richa et al.

u with |D(u)| = o(1/) and the strong adversary is allowed
to be 2-uniform.
Here, strongly c-competitive refers to a stronger throughput model where we require that for any sufficiently large
time interval and any node v, the number of rounds in which
v successfully receives a message is at least a c-fraction of
the total number of non-jammed rounds at v.
1.4 Article organization
The remainder of this article is organized as follows. Section 2 presents our MAC protocol, and the formal analysis is
given in Sect. 3. Section 4 reports on the simulation results.
The article is concluded in Sect. 5.

2 Description of Jade
This section first gives a short motivation for our algorithmic
approach and then presents the Jade protocol in detail.
2.1 Intuition
The intuition behind our MAC protocol is simple: Each node
u maintains a parameter pu which describes u’s probability
of accessing the channel at a given moment of time. That is, in
each round, each node u decides to broadcast a packet with
probability pu . (This is similar to classic random backoff
mechanisms where the next transmission time t is chosen
uniformly at random from an interval of size 1/ pv .) The
nodes adapt and synchronize their pu values over time in a
multiplicative increase multiplicative decrease manner, i.e.,
the value is lowered in times of high interference or increased
during times where the channel is idling. However, pu will
never exceed p̂, for some constant 0 < p̂ < 1.
Consider the unit disk D(u) around node u consisting of
u’s neighboring nodes as wellas u.1 Moreover, let N (u) =
D(u) \ {u} and p = p(u) = v∈N (u) pv ; henceforth, when
u is clear from the context, we will often simply write p
instead of p(u). Suppose that u is sensing the channel. Let
q0 be the probability that the channel is idle at u and let q1
be the probability that exactly one node in N (u) is sending

amessage. Itholds that q0 = v∈N (u) (1 − pv ) and q1 =
v∈N (u) pv
w∈N (u)\{v} (1 − pw ). Hence,
q1 ≤



pv

v∈N (u)

q1 ≥



v∈N (u)
1

pv

1
1 − p̂




(1 − pw ) =

w∈N (u)

q0 · p
1 − p̂

(1 − pw ) = q0 · p.

w∈N (u)

In this article, disks (and later sectors) will refer both to 2-dimensional
areas in the plane as well as to the set of nodes in the respective areas.
The exact meaning will become clear in the specific context.

Competitive throughput

163

Thus we have the following lemma, which has also been
derived in [2] for the single-hop case.
Lemma 1 q0 · p ≤ q1 ≤

q0
1− p̂

· p.

By Lemma 1, if a node v observes that the number of
rounds in which the channel is idle is equal to the number
of
 rounds in which exactly one message is sent, then p =
v∈N (v) pv is likely to be around 1 (if p̂ is a sufficiently small
constant), which would be ideal. Otherwise, the nodes know
that they need to adapt their probabilities. Thus, if we had
sufficiently many cases in which an idle channel or exactly
one message transmission is observed (which is the case if
the adversary does not heavily jam the channel and p is not
too large), then one can adapt the probabilities pv just based
on these two events and ignore all cases in which the wireless
channel is blocked, either because the adversary is jamming
it or because at least two messages interfere with each other
(see also [19] for a similar conclusion). Unfortunately, p can
be very high for some reason (e.g., due to high initial sending
probabilities), which requires a more sophisticated strategy
for adjusting the access probabilities.
2.2 Protocol description
In Jade, each node v maintains, in addition to the probability
value pv , a threshold Tv and a counter cv for Tv . Tv is used to
estimate the adversary’s time window T : a good estimation
of T can help the nodes recover from a situation where they
experience high interference in the network. In times of high
interference, Tv will be increased and the sending probability
pv will be decreased.
Initially, every node v sets cv := 1 and pv := p̂. Note
however that while we provide some initial values for the
variables in our description, our protocol is self-stabilizing
and works for any initial variable values, as we will show in
our proofs.
Initially, every node v sets Tv := 1, cv := 1 and pv := p̂.
Afterwards, the Jade protocol works in synchronized rounds.
In every round, each node v decides with probability pv to
send a message. If it decides not to send a message, it checks
the following two conditions:
– If v senses an idle channel, then pv := min{(1+γ ) pv , p̂}.
– If v successfully receives a message, then pv := (1 +
γ )−1 pv and Tv := max{Tv − 1, 1}.
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no round among
the past Tv rounds in which v sensed a successful message
transmission or an idle channel, then pv := (1 + γ )−1 pv
and Tv := min{Tv + 1, 21/(4γ ) }.
As we will see in the upcoming section, the concept
of using a multiplicative-increase-multiplicative-decrease

mechanism for pv and an additive-increase-additive-decrease
mechanism for Tv , as well as the slight modifications of the
protocol in [2], marked in italic above, are crucial for Jade
to work. If in the Afterwards part of the algorithm we did
not include the “idle” condition, in a distributed setting, it
could happen that a center node u with high pu value will
never see any successful transmissions: This can happen if
the cumulative probability of u’s neighbors is low while the
cumulative probability of u’s neighbors’ neighbors is high,
which makes u’s neighbors always stay at low probabilities.
In this situation, Tu may increase arbitrarily. Such high Tu
values however are harmful to the fast recovery properties of
the protocol.
Our simulation study showed that this protocol change
is crucial also in the average case, and not just in artificial
scenarios.

3 Analysis of Jade
In contrast the description of Jade, its stochastic analysis
is rather involved as it requires to shed light onto the complex interplay of the nodes all following their randomized
protocol in a dependent manner. We first prove Theorem 1
in Sects. 3.1 and 3.2, and then derive Theorem 2 in Sect. 3.3.
The limitations of Jade under the strong adversary are discussed in Sect. 3.4. In order to show the theorems, we will
frequently use the following variant of the Chernoff bounds
[2,37].
Lemma 2 Consider any set of binary random variables
that there
X 1 , . . . , X n . Suppose
 are values p1 , . . . , pn ∈

for every set S ⊆
[0, 1] with E[ i∈S X i ] ≤
i∈S pi 
n
{1, . . . , n}. Then it holds for X =
i=1 X i and μ =
n
i=1 pi and any δ > 0 that
μ

δ2 μ
eδ
− 2(1+δ/3)
≤
e
.
P[X ≥ (1 + δ)μ] ≤
(1 + δ)1+δ


If, on the other hand, it holds that E[ i∈S X i ] ≥ i∈S pi
for every set S ⊆ {1, . . . , n}, then it holds for any 0 < δ < 1
that
μ

e−δ
2
P[X ≤ (1 − δ)μ] ≤
≤ e−δ μ/2 .
(1 − δ)1−δ
Throughout the section we assume that γ = O(1/(log T +
log log n)) is sufficiently small.
3.1 Proof of Theorem 1
First, we focus on a time frame F consisting of (α log n)/
subframes of size f = α[T + (log3 n)/(γ 2 )] each, where
f is a multiple of T and α is a sufficiently large constant.
The proof needs the following three lemmas. The first one is

123

164

identical to Claim 2.5 in [2]. It is true because only successful
message transmissions reduce Tu .
Lemma 3 If in a time interval I the number of rounds in
which a node u successfully receives a message is at most r ,
√
then u increases Tu in at most r + 2|I | rounds in I .
The following lemma even holds for a strong adversary
and will be shown in Sect. 3.2.

Lemma 4 For every node u, v∈D(u) pv = O(1) for at
least a (1−β)-fraction of the rounds in time frame F, w.h.p.,
where the constant β > 0 can be made arbitrarily small.
The following lemma follows from simple geometric
arguments.
Lemma 5 A disk of radius 2 can be cut into at most 20
regions so that the distance between any two points in a
region is at most 1.
Consider some fixed node u. Let J ⊆ F be the set of all
non-jammed open rounds at u in time frame F (which are a
constant fraction of the non-jammed rounds at u). Let p be a

constant satisfying Lemma 4 (i.e., w∈D(v) pw ≤ p). Define
D D(u) to be the disk of radius 2 around u (i.e., it has twice
the radius of D(u)). Cut D D(u) into 20 regions R1 , . . . , R20
satisfying Lemma 5, and let vi be any node in region Ri (if
such a node exists), where vi = u if u ∈ Ri . According to
Lemma 4 it holds for each i that at least a (1 − β  /20)
fraction of the rounds in F satisfy w∈D(vi ) pw ≤ p for any

constant β  > 0, w.h.p. Thus,
 at least a (1 − β )-fraction
of the rounds in F satisfy w∈D(vi ) pw ≤ p for every i
for any constant β  > 0, w.h.p. As D(v) ⊆ D D(u) for all
v ∈ D(u) and u has at least |F| non-jammed rounds in F,
we get the following lemma, which also holds for arbitrary
(T, 1 − )-bounded adversaries.
Lemma 6 At least a (1 − β)-fraction of the rounds in J


satisfy v∈D(u) pv ≤ p and w∈D(v) pw = O( p) for all
nodes v ∈ D(u) for any constant β > 0, w.h.p.
Let us call these rounds good. Since the probability that
u senses the channel is at least 1 − p̂ and the probability

that the channel at u is idle for w∈D(u) pw ≤ p is equal


to v∈N (u) (1 − pv ) ≥ v∈N (u) e−2 pv ≥ e−2 p , u senses an
idle channel for at least (1 − p̂)(1 − β)|J |e−2 p ≥ 2β|J |
many rounds in J on expectation if β is sufficiently small.
This also holds w.h.p. when using the Chernoff bounds under
the condition that at least (1 − β)|J | rounds in F are good
(which also holds w.h.p.). Let k be the number of times u
receives a message in F. We distinguish between two cases.
Case 1 k ≥ β|J |/6. Then Jade is constant competitive for
u and we are done.

123

A. Richa et al.

Case 2 k < β|J |/6. Then we know from Lemma 3 that pu is
√
decreased at most β|J |/6+ 2|F| times in F due to cu > Tu .
In addition to this, pu is decreased at most β|J |/6 times
in F due to a received message. On the other hand, pu is
increased at least 2β|J | times in J (if possible) due to an
idle channel w.h.p. Also, we know from the Jade protocol
that at the beginning of F, pu = p̂. Hence, there must be at
√
least β(2 − 1/6 − 1/6)|J | − 2|F| ≥ (3/2)β|J | rounds in J
w.h.p. at which pu = p̂. As there are at least (1 − β)|J | good
rounds in J (w.h.p.), there are at least β|J |/2 good rounds
in J w.h.p. in which pu = p̂. For these good rounds, u has
a constant probability to transmit a message and every node
v ∈ D(u) has a constant probability of receiving it, so u
successfully transmits (|J |) messages to at least one of its
non-jammed neighbors in F (on expectation and also w.h.p.).
If we charge 1/2 of each successfully transmitted message to the sender and 1/2 to the receiver, then a constant
competitive throughput can be identified for every node in
both cases above, so Jade is constant competitive in F.
It remains to show that Theorem 1 also holds for larger
time intervals than |F|. First, note that all the proofs are
valid as long as γ ≤ 1/[c(log T + log log n)] for a constant
c ≥ 2, so we can increase T and thereby also |F| as long as
this inequality holds. So w.l.o.g. we may assume that γ =
√
1/[2(log T + log log n)]. In this case, 21/(4γ ) ≤ |F|, so our
√
rule of increasing Tv in Jade implies that Tv ≤ |F| at any
time. This allows us to extend the competitive throughput
result to any sequence of time frames. Let J ⊂ l · F be the
set of all non-jammed open rounds at u overall time frames,
where l is the number of frames considered here. Hence,
Case 1 holds directly; as for Case 2, we have β(2 − 1/6 −
√
1/6)|J | − 2l|F| ≥ (3/2)β|J | rounds in J w.h.p. at which
pu = p̂. Hence, the rest of the proof follows directly, which
completes the proof of Theorem 1.
3.2 Proof of Lemma 4
This section is dedicated to the proof of Lemma 4 which
is rather involved. Consider any fixed node u. We partition
u’s unit disk D(u) into six sectors of equal angles from
u, S1 , . . . , S6 . Note that all nodes within a sector Si have
distances of at most 1 from each other, so they can directly
communicate with one another (in D(u), distances can be up
to 2). We will first explore properties of an arbitrary node in
one sector, then consider the implications for a whole sector,
and finally bound the cumulative sending probability in the
entire unit disk.
Recall the definition of a time frame, a subframe and f in
the proof of Theorem 1. Fix a sector S in D(u) and consider
some fixed time frame F. Let us refer to the sum of the
probabilities of the neighboring nodes of a given node v ∈ S

by p̄v := w∈S\{v} pw . The following lemma shows that pv

Competitive throughput

165

will decrease dramatically if p̄v is high throughout a certain
time interval.
Lemma 7 Consider a node v in a unit disk D(u). If p̄v >
5 − p̂ during all rounds of a subframe I of F, then pv will
be at most 1/n 2 at the end of I , w.h.p.
Proof We say that a round is useful for node v if from v’s
perspective there is an idle channel or a successful transmission at that round (when ignoring the action of v); otherwise the round is called non-useful. Note that in a non-useful
round, according to our protocol, pv will either decrease (if
the threshold Tv is exceeded) or remain the same. On the
other hand, in a useful round, pv will increase (if v senses
an idle channel), decrease (if v senses a successful transmission) or remain the same (if v sends a message). Hence, pv
can only increase during useful rounds of I . Let U be the set
of useful rounds in I for our node v. We distinguish between
two cases, depending on the cardinality |U|. In the following, let pv (0) denote the probability of v at the beginning of
I (which is at most p̂). Suppose that f ≥ 2[(3c ln n)/γ ]2 for
a sufficiently large constant c. (This lower bound coincides
with our definition of f in the proof of Theorem 1.)
Case 1: Suppose that |U| < (c ln n)/γ , that is, many
rounds are blocked and pv can increase only rarely. As there
are at least (3c ln n)/γ occasions in I in which cv > Tv and
|U| < (c ln n)/γ , in at least (2c ln n)/γ of these occasions
v only saw blocked channels for Tv consecutive rounds and
therefore decides to increase Tv and decrease pv . Hence, at
the end of I ,
pv ≤ (1 + γ )|U |−2c ln n/γ pv (0)
≤ (1 + γ )−c ln n/γ pv (0)

≤ e−c ln n = 1/n c .

Case 2: Next, suppose that |U| ≥ (c ln n)/γ . We will show
that many of these useful rounds will be successful such that
pv decreases. Since pv ≤ p̂ ≤ 1/24 throughout I , it follows
from the Chernoff bounds that w.h.p. v will sense the channel
for at least a fraction of 2/3 of the useful rounds w.h.p. Let
this set of useful rounds be called U  . Consider any round
t ∈ U  . Let q0 be the probability that there is an idle channel
at round t and q1 be the probability that there is a successful
transmission at t. It holds that q0 + q1 = 1. From Lemma 1
we also know that q1 ≥ q0 · p̄v . Since p̄v > 5 − p̂ for all
rounds in I , it follows that q1 ≥ 4/5 for every round in U  .
Thus, it follows from the Chernoff bounds that for at least
2/3 of the rounds in U  , v will sense a successful transmission
w.h.p. Hence, at the end of I it holds w.h.p. that


pv ≤ (1 + γ )−(1/3)·|U | pv (0)

≤ (1 + γ )−(1/3)·(2c/3) ln n/γ pv (0)

Given this property of the individual probabilities, we can
derive a bound for the cumulative probability of an entire

sector S. In order to compute p S = v∈S pv , we introduce
three thresholds, a low one, ρgr een = 5, one in the middle,
ρ yellow = 5e, and a high one, ρr ed = 5e2 . The following
three lemmas provide some important insights about these
probabilities.
Lemma 8 For any subframe I in F and any initial value of
p S in I there is at least one round in I with p S ≤ ρgr een
w.h.p.
Proof We prove the lemma by contradiction. Suppose that
throughout the entire interval I, p S > ρgr een . Then it holds
for every node v ∈ S that p̄v > ρgr een − p̂ throughout I .
In this case, however, we know from Lemma 7, that pv will
decrease to at most 1/n 2 at the end of I w.h.p. Hence, all
nodes v ∈ S would decrease pv to at most 1/n 2 at the end
of I w.h.p., which results in p S ≤ 1/n. This contradicts our
assumption, so w.h.p. there must be a round t in I at which


	
p S ≤ ρgr een .
Lemma 9 For any time interval I in F of size f and any
sector S it holds that if p S ≤ ρgr een at the beginning of I , then
p S ≤ ρ yellow throughout I , w.m.p. Similarly, if p S ≤ ρ yellow
at the beginning of I , then p S ≤ ρr ed throughout I , w.m.p.
Proof It suffices to prove the lemma for the case that initially
p S ≤ ρgr een as the other case is analogous. Consider some
fixed round t in I . Let p S be the cumulative probability at the
beginning of t and p S be the cumulative probability at the
end of t. Moreover, let p (0)
S denote the cumulative probability
of the nodes w ∈ S with no transmitting node in D(w) \ S in
(1)
round t. Similarly, let p S denote the cumulative probability
of the nodes w ∈ S with a single transmitting node in D(w)\
(2)
S, and let p S be the cumulative probability of the nodes
w ∈ S that experience a blocked round either because they
are jammed or at least two nodes in D(w)\ S are transmitting
(0)
(1)
(2)
at t. Certainly, p S = p S + p S + p S . Our goal is to

determine p S in this case. Let q0 (S) be the probability that
all nodes in S stay silent, q1 (S) be the probability that exactly
one node in S is transmitting, and q2 (S) = 1−q0 (S)−q1 (S)
be the probability that at least two nodes in S are transmitting.
When ignoring the case that cv > Tv for a node v ∈ S at
round t, it holds:
	

(0)
(1)
(2)
E[ p S ] = q0 (S) · (1 + γ ) p S + (1 + γ )−1 p S + p S
	

(1)
(2)
+ q1 (S) · (1 + γ )−1 p (0)
S + pS + pS
	

(0)
(1)
(2)
+ q2 (S) · p S + p S + p S

≤ e−(2c/9) ln n = 1/n 2c/9 .

Combining the two cases with c ≥ 9 results in the lemma.



	

This is certainly also an upper bound for E[ p S ] if cv > Tv
for a node v ∈ S because pv will never be increased (but

123

166

A. Richa et al.

possibly decreased) in this case. Now, consider the event
E 2 that at least two nodes in S transmit a message. If E 2
holds, then E[ p S ] = p S = p S , so there is no change in the
system. On the other hand, assume that E 2 does not hold. Let
q0 (S) = q0 (S)/(1 − q2 (S)) and q1 (S) = q1 (S)/(1 − q2 (S))
be the probabilities q0 (S) and q1 (S) under the condition of
¬E 2 . Then we distinguish between three cases.
Case 1: p (0)
S = p S . Then

or




((1/a)x0 +(1/b)x1 +(1/c)x2 )2 a 2 x0 +b2 x1 + c2 x2 ≤ 1
(2)

To prove this, we need two claims whose proofs are tedious
but follow from standard math.

q0 (S) · (1 + γ ) p S + q1 (S) · (1 + γ )−1 p S
((1 + γ )q0 (S) + (1 + γ )−1 q1 (S)) p S .

Claim For any a, b, c > 0 and any x0 , x1 , x2 > 0 with
x0 + x1 + x2 = 1,



(ax0 + bx1 + cx2 )2 ≤ a 2 x0 + b2 x1 + c2 x2

From Lemma 1 we know that q0 (S) ≤ q1 (S)/ p S , so q0 (S) ≤
q1 (S)/ p S . If p S ≥ ρgr een , then q0 (S) ≤ q1 (S)/5. Hence,

Claim For any a, b, c > 0 and any x0 , x1 , x2 > 0 with
x0 + x1 + x2 = 1,

E[ p S ] ≤ ((1 + γ )/6 + (1 + γ )−1 5/6) p S ≤ (1 + γ )−1/2 p S

((1/a)x0 + (1/b)x1 + (1/c)x2 ) (ax0 + bx1 + cx2 ) ≤ 1

E[ p S ]

≤
=

since γ = o(1). On the other hand, p S ≤ (1 + γ ) p S in any
case.
(1)
Case 2: p S = p S . Then
E[ p S ] ≤ q0 (S) · (1 + γ )−1 p S + q1 (S) p S

= (q0 (S)/(1 + γ ) + (1 − q0 (S))) p S
= (1 − q0 (S)γ /(1 + γ )) p S .

Now, it holds that 1 − xγ /(1 + γ ) ≤ (1 + γ )−x/2 for all
x ∈ [0, 1] because from the Taylor series of e x and ln(1 + x)
it follows that
(1 + γ )−x/2 ≥ 1 − (x ln(1 + γ ))/2 ≥ 1 − (x(1 − γ /2)γ )/2
and
1 − xγ /(1 + γ ) ≤ 1 − (x(1 − γ /2)γ )/2
for all x, γ ∈ [0, 1] as is easy to check. Therefore, when
defining ϕ = q0 (S), we get E[ p S ] ≤ (1 + γ )−ϕ/2 p S . On the
other hand, p S ≤ p S ≤ (1 + γ )ϕ p S .
(2)
Case 3: p S = p S . Then for ϕ = 0, E[ p S ] ≤ p S =
−ϕ/2
p S and p S ≤ p S = (1 + γ )ϕ p S .
(1 + γ )
Combining the three cases and taking into account that
(0)
(1)
(2)
p S + p S + p S = p S , we obtain the following result.
(0)

(1)

Lemma 10 There is a φ ∈ [0, 1] (depending on p S , p S
(2)
and p S ) so that
E[ p S ] ≤ (1 + γ )−φ p S and p S ≤ (1 + γ )2φ p S .

(1)

Proof Let a = (1 + γ )1/2 , b = (1 + γ )ϕ/2 for the ϕ defined
(0)
in Case 2, and c = 1. Furthermore, let x0 = p S / p S , x1 =
(1)
(2)
p S / p S and x2 = p S / p S . Define φ = − log1+γ ((1/a)x0 +
(1/b)x1 + (1/c)x2 ). Then we have
(0)

(1)

(2)

E[ p S ] ≤ (1 + γ )−1/2 p S + (1 + γ )−ϕ/2 p S + p S
= (1 + γ )−φ p S .

We need to show that for this φ, also p S ≤ (1 + γ )2φ p S . As
(0)
(1)
(2)
p S ≤ (1 + γ ) p S + (1 + γ )ϕ p S + p S , this is true if
a 2 x 0 + b2 x 1 + c2 x 2 ≤

123

1
((1/a)x0 + (1/b)x1 + (1/c)x2 )2

Combining the claims, Eq. (2) follows, which completes
the proof.


	
Hence, for any outcome of E 2 , E[ p S ] ≤ (1 + γ )−ϕ p S
and p S ≤ (1 + γ )2ϕ p S for some ϕ ∈ [0, 1]. If we define
q S = log1+γ p S , then it holds that E[q S ] ≤ q S − ϕ. For
any time t in I , let qt be equal to q S at time t and ϕt be
defined as ϕ at time t. Our calculations above imply that
as long as p S ∈ [ρgr een , ρ yellow ], E[qt+1 ] ≤ qt − ϕt and
qt+1 ≤ qt + 2ϕt .
Now, suppose that within subframe I we reach a point t
when p S > ρ yellow . Since we start with p S ≤ ρgr een , there
must be a time interval I  ⊆ I so that right before I  , p S ≤
ρgr een , during I  we always have ρgr een < p S ≤ ρ yellow ,
and at the end of I  , p S > ρ yellow . We want to bound the
probability for this to happen.
Consider some fixed interval I  with the properties above,
i.e., with p S ≤ ρgr een right before I  and p S ≥ ρgr een at the
first round of I  , so initially, p S ∈ [ρgr een , (1 + γ )ρgr een ].
We use martingale theory to bound the probability that in this
case, the properties defined above for I  hold. Consider the
rounds in I  to be numbered from 1 to |I  |, let qt and ϕt be
t−1
ϕi . It holds that
defined as above, and let qt = qt + i=1


t


ϕi
E[qt+1 ] = E qt+1 +
i=1
t
t


 
= E qt+1 +
ϕi ≤ qt − ϕt +
ϕi
i=1

= qt +

t−1


i=1

ϕi

i=1

= qt .
Moreover, it follows from Inequality (1) that for any round
t, p S ≤ (1 + γ )2ϕt p S . Therefore, qt+1 ≤ qt + 2ϕt , which

≤ qt + ϕt . Hence, we can define a martinimplies that qt+1
gale (X t )t∈I  with E[X t+1 ] = X t and X t+1 ≤ X t + ϕt that
stochastically dominates qt . Recall that a random variable Yt

Competitive throughput

167

stochastically dominates a random variable Z t if for any z,
P[Yt ≥ z] ≥ P[Z t ≥ z]. In that case, it is also straight

forward to show that i Yi stochastically dominates i Z i ,
which we will need in the following. Let T = |I  |. We will
make use of Azuma’s inequality to bound X T .
Fact 4 (Azuma Inequality) Let X 0 , X 1 , . . . be a martingale
satisfying the property that X i ≤ X i−1 + ci for all i ≥ 1.
Then for any δ ≥ 0,
P [X T > X 0 + δ] ≤ e

T
−δ 2 /(2 i=1
ci2 )

Thus, for δ = 1/γ +
P [X T > X 0 + δ] ≤ e

T

i=1 ϕi

.

it holds in our case that

T
−δ 2 /(2 i=1
ϕi2 )

.

This implies that
T


2
2
P qT > q0 + δ ≤ e−δ /(2 i=1 ϕi ) ,
for several reasons. First of all, stochastic dominance holds as
long as p S ∈ [ρgr een , ρ yellow ], and whenever this is violated,
we can stop the process as the requirements on I  would
be violated, so we would not have to count that probability
towards I  . Therefore,
P[qT > q0 + 1/γ ] ≤ e−δ

2 /(2 T ϕ 2 )
i=1 i

.

Notice that qT > q0 + 1/γ is required so that p S > ρ yellow
at the end of I  , so the probability bound above is exactly
T
ϕi . Since ϕi ≤ 1 for all i, ϕ ≥
what we need. Let ϕ = i=1
T
2 . Hence,
ϕ
i=1 i


1
δ2
(1/γ + ϕ)2
ϕ
≥
.
≥
+
T
2ϕ
2ϕγ 2
2
2 i=1
ϕi2
This is minimized for 1/(2ϕγ 2 ) = ϕ/2 or equivalently, ϕ =
1/γ . Thus,
P[qT > q0 + 1/γ ] ≤ e−1/γ

 
Since there are at most 2f ways of selecting I  ⊆ I , the
probability that there exists an interval I  with the properties
above is at most
 
f −1/γ
1
e
≤ f 2 e−1/γ ≤
logc n
2
for any constant c if γ = O(1/(log T + log log n)) is small
enough.
Lemma 11 For any subframe I in F it holds that if there
has been at least one round during the past subframe where
p S ≤ ρgr een , then throughout I, p S ≤ ρr ed w.m.p.
Proof Suppose that there has been at least one round during
the past subframe where p S ≤ ρgr een . Then we know from
Lemma 9 that w.m.p. p S ≤ ρ yellow at the beginning of I . But
if p S ≤ ρ yellow at the beginning of I , we also know from
Lemma 9 that w.m.p. p S ≤ ρr ed throughout I , which proves
the lemma.


	

Now, define a subframe I to be good if p S ≤ ρr ed throughout I , and otherwise I is called bad. With the help of Lemmas 8 and 11 we can prove the following lemma.
Lemma 12 For any sector S, at most β/6 of the subframes
I in F are bad w.h.p., where the constant β > 0 can be made
arbitrarily small depending on the constant α in f .
Proof From Lemma 8 it follows that for every subframe I in
F there is a time point t ∈ I at which p S ≤ ρgr een w.h.p.
Consider now some fixed subframe I in F that is not the first
one and suppose that the previous subframe in F had at least
one round with p S ≤ ρgr een . Then it follows from Lemma 11
that for all rounds in I, p S ≤ ρr ed w.m.p. (where the probability only depends on I and its preceding subframe), i.e., I
is good. Hence, it follows from the Chernoff bounds that at
most β/7 of the odd-numbered as well as the even-numbered
subframes after the first subframe in F are bad w.h.p. (if the
constant α is sufficiently large). This implies that overall at
most β/6 of the subframes in F are bad w.h.p.


	
From Lemma 12 it follows that apart from an β-fraction

of the subframes, all subframes I in F satisfy v∈D(u) pv ∈
O(1) throughout I , which completes the proof of Lemma 4.
3.3 Proof of Theorem 2
Now, let us consider the two cases of Theorem 2 under the
strong adversary.
Case 1: the adversary is 1-uniform and the UDG is
connected.
In this case, every node has a non-empty neighborhood and
therefore all non-jammed rounds of the nodes are open.
Hence, the conditions on a (T, 1 − )-bounded adversary are
satisfied. So Theorem 1 applies, which completes the proof
of Theorem 2 a).
Case 2: |D(v)| ≥ 2/ for all v ∈ V.
Consider some fixed time interval I with |I | being a multiple
of T . For every node v ∈ D(u) let f v be the number of nonjammed rounds at v in I and ov be the number of open rounds
at v in I . Let J be the set of rounds in I with at most one
non-jammed node. Suppose that |J | > (1 − /2)|I |. Then
every node in D(u) must have more than (/2)|I | of its nonjammed rounds in J . As these non-jammed rounds must be
serialized in J to satisfy our requirement on J , it holds that

|J | > v∈D(u) (/2)|I | ≥ (2/) · (/2)|I | = |I |. Since this
is impossible,
that |J | ≤ (1 − /2)|I |. 
 it must hold
Thus, v∈D(u) ov ≥ ( v∈D(u) f v )−|J | ≥ (1/2) v∈D(u)

f v because v∈D(u) f v ≥ (2/) · |I | = 2|I |. Let D  (u) be
the set of nodes v ∈ D(u) with ov ≥ f v /4. That is, for each

123

168

of these nodes, a constant fraction of the non-jammed time


steps is open. Then v∈D(u)\D  (u) ov < (1/4) v∈D(u) f v ,



so v∈D  (u) ov ≥ (1/2) v∈D(u) ov ≥ (1/4) v∈D(u) f v .
Consider now a set U ⊆ V of nodes so that u∈U D(u) =
V and for every v ∈ V there are at most 6 nodes u ∈ U
with v ∈ D(u) (U is easy to construct in a greedy fashion for arbitrary UDGs and also known
 as a dominating

set of constant density). Let V  =
u∈U D (u). Since


(1/4) v∈D(u) f v for
v∈D  (u) ov ≥ 
everynode u ∈ U ,
o
≥
(1/6)
it follows
that

v
 v∈V
u∈U v∈D  (u) ov ≥

(1/24) u∈U v∈D(u) f v ≥ (1/24) v∈V f v . Using that
together with Theorem 1, which implies that Jade is constant competitive w.r.t. the nodes in V  , completes the proof
of Theorem 2 b).
3.4 Limitations under the strong adversary
One may ask whether a stronger throughput result than Theorem 2 can be shown for the strong adversary. Ideally, we
would like to use the following model. A MAC protocol
is called strongly c-competitive against some (T, 1 − )bounded adversary if, for any sufficiently large time interval and any node v, the number of rounds in which v successfully receives a message is at least a c-fraction of the
total number of non-jammed rounds at v. In other words, a
strongly c-competitive MAC protocol can achieve at least a
c-fraction of the best possible throughput for every individual node. Unfortunately, such a protocol seems to be difficult
to design. In fact, Jade is not strongly c-competitive for any
constant c > 0, even if the node density is sufficiently high.
We can prove the following lemmas which imply Theorem 3.
Lemma 13 In general, Jade is not strongly c-competitive
for a constant c > 0 if the strong adversary is allowed to be
2-uniform and  ≤ 1/3.
Proof Suppose that (at some corner of the UDG) we have a
set U of at least 1/ p̂ nodes located closely to each other that
are all within the transmission range of a node v. Initially, we

assume that u∈U pu ≥ 1, pv = p̂ and Tx = 1 for all nodes
x ∈ U ∪{v}. The time is partitioned into time intervals of size
T . In each such time interval, called T-interval, the (T, 1−)bounded adversary jams all but the first T rounds at U and
all but the last T rounds at v. It follows directly from Section
2.3 of [2] that if T = Ω((log3 n)/(γ 2 )), then for every node
√
u ∈ U, Tu ≤ α T log n/ w.h.p. for some sufficiently large
constant α. Thus, Tu ≤ γ T /(β log n) w.h.p. for any constant
β > 0 if T is sufficiently large. Hence, between the last nonjammed round at U and the first non-jammed round at v in
a T -interval, the values Tu are increased (and the values pu
are decreased) at least β(log n)/(6γ ) times. Thus, at the first
non-jammed round at v, it holds for every u ∈ U that
pu ≤ p̂ · (1 + γ )−β(log n)/(6γ ) ≤ p̂ · e−(β/6) log n ≤ 1/n β/6

123

A. Richa et al.


and, therefore, u∈U pu = O(1/n 2 ) if β ≥ 18. This cumulative probability will stay that low during all of v’s nonjammed rounds as during these rounds the nodes in U are
jammed. Hence, the probability that v receives any message
during its non-jammed rounds of a T -interval is O(1/n 2 ), so
Jade is not c-competitive for v for any constant c > 0. 	


Also, in our original model, Jade is not constant competitive if the node density is too low.
Lemma 14 In general, Jade is not c-competitive for a constant c independent of  if there are nodes u with |D(u)| =
o(1/) and the strong adversary is allowed to be 2-uniform.
Proof Suppose that we have a set U of k = o(1/) nodes
located closely to each other that are all within the transmission range of a node v. Let T = Ω((log3 n)/(γ 2 )). In each
T -interval, the adversary never jams v but jams all but the
first T rounds at U . Then Section 2.3 of [2] implies that for
every node u ∈ U, Tu ≤ γ T /(β log n) w.h.p. for any constant β > 0 if T is sufficiently large. The nodes in U continuously increase their Tu -values and thereby reduce their pu
values during their jammed time steps. Hence, the nodes in
U ∪{v} will receive at most T ·|U |+(T + O(T / log n)) =
T · o(1/) + ( + o(1))T = ( + o(1))T messages in each
T -interval on expectation whereas the sum of non-jammed
rounds over all nodes is more than T .


	
Hence, Theorem 2 is the best one can show for Jade
(within our notation). More generally, of course, no MAC
protocol can guarantee a constant competitive throughput
if the UDG is not connected. However, it is still an open
question whether there are simple MAC protocols that are
constant competitive under non-uniform jamming strategies
even if there are o(1/) nodes within the transmission range
of a node.
4 Simulations
In order to complement our theoretical insights, we report
on our simulation results. First, we present our throughput
results for a sufficiently large time interval, and then we discuss the convergence behavior. For our simulations, as in our
formal analysis, we assume that initially all nodes v ∈ V have
a high sending probability of pv = p̂ = 1/24. The nodes
are distributed at random over a square plane of 4 × 4 units,
and are connected in a unit disk graph manner (multi-hop).
We simulate the jamming activity in the following way: for
each round, a node is jammed independently with probability (1 − ). Note that in the terminology we introduced, this
adversary is strong (as rounds do not need to be open) and
n-uniform (as nodes are jammed independently). The reason
for studying this rather simplistic randomized “adversary” is
twofold. First, although our formal results hold for arbitrary

Competitive throughput

169
1

Uniform distribution
Gaussian distribution

Throughput

0.8
0.6
0.4
0.2
0

200

400

600

800 1000 1200 1400 1600 1800 2000

Network Size
15

3
10

v

2.5
2

T

Cumulative Probability

4
3.5

1.5

5

1
0.5
0

200

400

600

800 1000 1200 1400 1600 1800 2000

0

200

400

Number of Rounds

600

800 1000 1200 1400 1600 1800 2000

Number of Rounds

Fig. 1 Top Throughput as a function of network size, where n ∈
[100, 2, 000],  = 0.1, T = 200, and γ = 1/(log T + log log n).
The result is averaged over 10 runs. Bottom left Convergence behavior for multi-hop networks (uniform distribution). As a demonstration,
we used n = 500,  = 0.1, T = 200, and γ = 1/(log T + log log N ).

Note that the start-up phase where the sending probabilities are high
is short (no more than 50 rounds). Bottom right Convergence of Tv for
multi-hop networks (uniform distribution). For demonstration, we used
n = 500,  = 0.1, T = 200, and γ = 1/(log T + log log N )

adversaries, it is not clear how to constructively compute such
a worst adversarial strategy; second, a random adversary also
complements our formal results better as it may capture the
“average case” behavior.
We run the simulation for a sufficiently large number
of time steps indicated by the Theorem 1, i.e., for ([T +
(log3 n)/(γ 2 )] · (log n)/ rounds, where  = 0.1, T =
200, and γ = 1/(log T + log log N ). Simulations with
different combinations of  ∈ {0.5, 0.3, 0.1} and T =
{50, 100, 150, 200} showed that  = 0.1 and T = 200
yields the lowest throughput (and the strongest adversary),
and hence, in the following, we will focus on this most challenging case. (The parameter γ is set to a value satisfy its
definition, i.e., γ = O(1/(log T + log log N )).)
Figure 1(top) shows the throughput competitiveness of
Jade for a scenario where different numbers of nodes (i.e.,
n ∈ [100, 2, 000]) are distributed uniformly at random over
the plane and a scenario where the nodes are distributed
according to a normal/Gaussian distribution N (0, 1). In both
cases, the throughput is larger when the density is higher. This
corresponds to our formal insight that a constant competitive throughput is possible only if the node density exceeds
a certain threshold. For example, in a scenario with 100
nodes in the 4 × 4 plane (density of 6.25), there are at least
6.25π ≈ 20 ≥ 2/ = 20 uniformly distributed nodes in

one unit disk. As can be seen in the figure, when the number of nodes is larger than 600, the throughput falls between
20% and 40% for both uniform distribution and Gaussian
distribution.
Convergence time is the second most important evaluation
criterion. We found that already after a short time, a constant
throughput is achieved; in particular, the total sending probability per unit disk approaches a constant value quickly. This
is due to the nodes’ ability to adapt their sending probabilities fast, see Fig. 1(bottom left). The figure also illustrates
the high correlation between success ratio and aggregated
sending probability.
Finally, we have also studied the average of the Tv values
over time. The average quickly stabilizes to a value around
10, as shown in Fig. 1(bottom right).
5 Conclusion
This article has presented the first jamming-resistant MAC
protocol with provably good performance in multi-hop networks exposed to an adaptive adversary. While we have
focused on unit disk graphs, we believe that our stochastic analysis is also useful for more realistic wireless network
models. Moreover, although our analysis is involved, our protocol is rather simple.

123

170

There are several questions that remain open. For instance,
we assumed a common parameter γ which is known by all
nodes and which depends on n and T . Although the estimations on these parameters we need are very rough and
scalable, it remains an open question whether this limitation
can be relaxed,and e.g., a local value γv = 1/ log Tv would
also work.

References
1. Alnifie, G., Simon, R.: A multi-channel defense against jamming
attacks in wireless sensor networks. In: Proceedings of Q2SWinet
’07, pp. 95–104 (2007)
2. Awerbuch, B., Richa, A., Scheideler, C.: A jamming-resistant
MAC protocol for single-hop wireless networks. In: Proceedings
of PODC ’08 (2008)
3. Bayraktaroglu, E., King, C., Liu, X., Noubir, G., Rajaraman, R.,
Thapa, B.: On the performance of IEEE 802.11 under jamming. In:
Proceedings of IEEE Infocom ’08, pp. 1265–1273 (2008)
4. Bender, M.A., Farach-Colton, M., He, S., Kuszmaul, B.C., Leiserson, C.E.: Adversarial contention resolution for simple channels.
In: Proceedings of SPAA ’05 (2005)
5. Brown, T., James, J., Sethi, A.: Jamming and sensing of encrypted
wireless ad hoc networks. In: Proceedings of MobiHoc ’06,
pp. 120–130 (2006)
6. Chiang, J., Hu, Y.-C.: Cross-layer jamming detection and mitigation in wireless broadcast networks. In: Proceedings of MobiCom
’07, pp. 346–349 (2007)
7. Chlebus, B.S., Kowalski, D.R., Rokicki, M.A.: Adversarial queuing on the multiple-access channel. In: Proceedings of PODC ’06
(2006)
8. Czumaj, A., Rytter, W.: Broadcasting algorithms in radio networks
with unknown topology. J. Algorithms 60(2), 115–143 (2006)
9. Dolev, S., Gilbert, S., Guerraoui, R., Kowalski, D., Newport, C.,
Kuhn, F., Lynch, N.: Reliable distributed computing on unreliable
radio channels. In: Proceedings of the 2009 MobiHoc S3 Workshop
(2009)
10. Dolev, S., Gilbert, S., Guerraoui, R., Kuhn, F., Newport, C.C.:
The wireless synchronization problem. In: Proceedings of the 28th
Annual ACM Symposium on Principles of Distributed Computing
(PODC), pp. 190–199 (2009)
11. Dolev, S., Gilbert, S., Guerraoui, R., Newport, C.: Gossiping in
a multi-channel radio network: An oblivious approach to coping
with malicious interference. In: Proceedings of the Symposium on
Distributed Computing (DISC) (2007)
12. Dolev, S., Gilbert, S., Guerraoui, R., Newport, C.: Secure communication over radio channels. In: Proceedings of the 27th ACM
Symposium on Principles of Distributed Computing (PODC),
pp. 105–114 (2008)
13. Dolev, S., Gilbert, S., Guerraoui, R., Newport, C.C.: Gossiping in
a multi-channel radio network. In: Proceedings of the 21st International Symposium on Distributed Computing (DISC), pp. 208–222
(2007)
14. Gilbert, S., Guerraoui, R., Kowalski, D., Newport, C.: Interferenceresilient information exchange. In: Proceedings of the 28th Conference on Computer Communications. IEEE Infocom 2009 (2009)
15. Gilbert, S., Guerraoui, R., Kowalski, D.R., Newport, C.C.:
Interference-resilient information exchange. In: Proceedings of the
28th IEEE International Conference on Computer Communications (INFOCOM), pp. 2249–2257 (2009)

123

A. Richa et al.
16. Gilbert, S., Guerraoui, R., Newport, C.: Of malicious motes and
suspicious sensors: On the efficiency of malicious interference in
wireless networks. In: Proceedings of OPODIS ’06 (2006)
17. Goldberg, L.A., Mackenzie, P.D., Paterson, M., Srinivasan, A.:
Contention resolution with constant expected delay. J. ACM 47(6)
1048–1096 (2000). doi:10.1145/355541.355567
18. Håstad, J., Leighton, T., Rogoff, B.: Analysis of backoff protocols
for mulitiple access channels. SIAM J. Comput. 25(4) 710–774
(1996). http://dx.doi.org/10.1137/S0097539792233828
19. Heusse, M., Rousseau, F., Guillier, R., Duda, A.: Idle sense: An
optimal access method for high throughput and fairness in rate
diverse wireless lans. In: Proceedings of SIGCOMM (2005)
20. IEEE: Medium access control (MAC) and physical specifications.
In: IEEE P802.11/D10 (1999)
21. Jain, K., Padhye, J., Padmanabhan, V.N., Qiu, L.: Impact of interference on multi-hop wireless network performance. In: Proceedings
of the 9th Annual International Conference on Mobile Computing
and Networking (MobiCom), pp. 66–80 (2003)
22. Jiang, S., Xue, Y.: Providing survivability against jamming attack
via joint dynamic routing and channel assigment. In: Proceedings of
the 7th Workshop on Design of Reliable Communication Networks
(DRCN) (2009)
23. Koo, C., Bhandari, V., Katz, J., Vaidya, N.: Reliable broadcast in
radio networks: The bounded collision case. In: Proceedings of
PODC ’06 (2006)
24. Kuhn, F., Moscibroda, T., Wattenhofer, R. Radio network clustering
from scratch. In: Proceedings. of ESA ’04 (2004)
25. Kwak, B.-J., Song, N.-O., Miller, L.E.: Performance analysis of
exponential backoff. IEEE/ACM Trans. Netw. 13(2), 343–355
(2005)
26. Law, Y., van Hoesel, L., Doumen, J., Hartel, P., Havinga, P.: Energyefficient link-layer jamming attacks against wireless sensor network mac protocols. In: Proceedings of SASN ’05, pp. 76–88
(2005)
27. Li, M., Koutsopoulos, I., Poovendran, R.: Optimal jamming attacks
and network defense policies in wireless sensor networks. In: Proceedings of Infocom ’07, pp. 1307–1315 (2007)
28. Liu, X., Noubir, G., Sundaram, R., Tan, S.: Spread: Foiling smart
jammers using multi-layer agility. In: Proceedings of Infocom ’07,
pp. 2536–2540 (2007)
29. Meier, D., Pignolet Y.A., Schmid S., Wattenhofer, R.: Speed dating
despite jammers. In: Proceedings of DCOSS ’09 June (2009)
30. Navda, V., Bohra, A., Ganguly, S., Rubenstein, D.: Using channel hopping to increase 802.11 resilience to jamming attacks. In:
Proceedings of Infocom’07, pp. 2526–2530 (2007)
31. Negi, R., Perrig, A.: Jamming Analysis of MAC Protocols. Technical Report. Carnegie Mellon University (2003)
32. Noubir, G.: On connectivity in ad hoc networks under jamming
using directional antennas and mobility. In: Proceedings of the 2nd
International Conference on Wired/Wireless Internet Communications (WWIC), pp. 186–200 (2004)
33. Pelc A., Peleg, D.: Feasibility and complexity of broadcasting
with random transmission failures. In: Proceedings of PODC ’05
(2005)
34. Raghavan, P., Upfal, E.: Stochastic contention resolution with short
delays. SIAM J. Comput. 28(2), 709–719 (1999)
35. Richa, A., Scheideler, C., Schmid, S., Zhang, J.: Competitive and
fair medium access despite reactive jamming. In: Proceedings of the
31st International Conference on Distributed Computing Systems
(ICDCS) (2011)
36. Richa, A., Scheideler, C., Schmid, S., Zhang, J.: Self-stabilizing
leader election for single-hop wireless networks despite jamming. In: Proceedings of the 12th ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc)
(2011)

Competitive throughput
37. Schmidt, J., Siegel, A., Srinivasan, A.: Chernoff-Hoeffding bounds
for applications with limited independence. SIAM J. Discrete
Math. 8(2), 223–250 (1995)
38. Simon, M.K., Omura, J.K., Schultz, R.A., Levin, B.K.: Spread
Spectrum Communications Handbook. McGraw-Hill, New York
(2001)
39. Thuente, D., Acharya, M. Intelligent jamming in wireless networks
with applications to 802.11b and other networks. In: Proceedings
of MILCOM ’06 (2006)
40. van de Ven, P.M., Janssen, A.J.E.M., van Leeuwaarden, J.S.H.,
Borst, S.C.: Achieving target throughputs in random-access networks. Perform. Eval. 68, 1103–1117 (2011)
41. Wood, A., Stankovic, J., Zhou, G. DEEJAM: Defeating energyefficient jamming in IEEE 802.15.4-based wireless networks. In:
Proceedings of SECON ’07 (2007)
42. Xu, W., Ma, K., Trappe, W., Zhang, Y.: Jamming sensor networks:
attack and defense strategies. IEEE Netw. 20(3), 41–47 (2006)

171
43. Xu, W., Trappe, W., Zhang, Y., Wood, T.: The feasibility of launching and detecting jamming attacks in wireless networks. In: Proceedings of MobiHoc ’05, pp. 46–57 (2005)
44. Xu, W., Wood, T., Zhang, Y.: Channel surfing and spatial retreats:
defenses against wireless denial of service. In: Proceedings of
Workshop on Wireless Security (2004)
45. Ye, S., Wang, Y., Tseng, Y. A jamming-based MAC protocol for
wireless multihop ad hoc networks. In: Proceedings of the IEEE
58th Vehicular Technology Conference (2003)
46. Ye, S.-R., Wang, Y.-C., Tseng, Y.-C.: A jamming-based MAC protocol to improve the performance of wireless multihop ad-hoc networks. Wirel. Commun. Mob. Comput. 4(1), 75–84 (2004)
47. Zander, J.: Jamming in slotted ALOHA multihop packed radio
networks. IEEE Trans. Netw. 39(10), 1525–1531 (1991)

123

Ameba-inspired Self-organizing Particle Systems
(Extended Abstract)
Shlomi Dolev1 , Robert Gmyr2 , Andréa W. Richa3 , Christian Scheideler2

arXiv:1307.4259v1 [cs.DC] 16 Jul 2013

1

Department of Computer Science,
Ben-Gurion University of the Negev, Israel,
dolev@cs.bgu.ac.il
2

Department of Computer Science,
University of Paderborn, Germany,
gmyr@mail.upb.de, scheideler@mail.upb.de
3

Department of Computer Science and Engineering,
Arizona State University, USA,
aricha@asu.edu

Abstract
Particle systems are physical systems of simple computational particles that can bond to neighboring particles and
use these bonds to move from one spot to another (non-occupied) spot. These particle systems are supposed to be able
to self-organize in order to adapt to a desired shape without any central control. Self-organizing particle systems have
many interesting applications like coating objects for monitoring and repair purposes and the formation of nano-scale
devices for surgery and molecular-scale electronic structures. While there has been quite a lot of systems work in this
area, especially in the context of modular self-reconfigurable robotic systems, only very little theoretical work has been
done in this area so far. We attempt to bridge this gap by proposing a model inspired by the behavior of ameba that
allows rigorous algorithmic research on self-organizing particle systems.

Keywords: Particle systems; self-organization; nano-structures; nano-computing.

1

Introduction

Over the next few decades, two emerging technologies—microfabrication and cellular engineering—will make it possible
to assemble systems incorporating myriads of simple information processing units at almost no cost. Microelectronic
mechanical components have become so inexpensive to manufacture that one can anticipate integrating logic circuits,
microsensors, actuators, and communication devices on the same chip to produce particles that could be mixed with bulk
materials, such as paints, gels, and concrete [1]. Imagine coating bridges and buildings with smart paint that senses
and reports on traffic and wind loads and monitors structural integrity. A smart-paint coating on a wall could sense
vibrations, monitor the premises for intruders, and cancel noise. There has also been amazing progress in understanding
the biochemical mechanisms in individual cells such as the mechanisms behind cell signaling [33] and cell movement
[3]. Recent results have also demonstrated that, in principle, biological cells can be turned into finite automata [7] or even
pushdown automata [23], so one can imagine that some day one can tailor-make biological cells to function as sensors
and actuators, as programmable delivery devices, and as chemical factories for the assembly of nano-scale structures.
Particularly interesting applications for this technology would be the construction of molecular-scale electronic structures
and of nano-scale devices for surgery as well as cancer treatment [30].
Yet fabrication is only part of the story. One can envision producing vast quantities of individual computing elements—
whether microfabricated particles or engineered cells—but research on how to use them effectively is still in its infancy.
The opportunity to exploit these new technologies poses a broad conceptual challenge that was coined by Abelson, Knight,
Sussman, et al. as amorphous computing [1]. In amorphous computing one usually assumes that there is a very large number of locally interacting computing elements, called computational particles, that may form an arbitrary initial structure.
The particles are possibly faulty, sensitive to the environment, and may produce various types of actions that range from
changing their internal state to communicating with other particles, moving to a different location, or even replicating
(to mimic biological cell replication). Each particle has modest computational power and a modest amount of memory.
The particles are not synchronized, although it is usually assumed that they compute at similar speeds (as long as they
are non-faulty), since they are all fabricated by the same process. The particles are all programmed identically, but they
usually have means for storing local state and for generating random numbers, which allows them to differentiate over
time. In general, the particles do not have any a priori knowledge of their positions or orientations.
We propose a new ameba-inspired model for computational particles representing finite automata that form a connected structure with the help of local bonds. The particles cannot move through other particles or in open space, and
while they move they cannot drag other particles with them due to limitations on their energy and strength. So if they
wish to form a particular pattern, then the only way to achieve that is through individual movements of particles along the
surface of a particle structure (which could be done by releasing and engaging bonds to static particles in a similar way
biological cells are moving [3]) until they have reached the desired location. This is continued until the desired pattern has
been reached. Although it is clear that this can have many interesting applications like the repair of structural damages,
theoretical research on our type of particle systems is basically non-existent, so there is a dire need to provide a solid
theoretical base.

2

Related Work

Self-organizing systems have been studied in many contexts. One can distinguish here between active and passive systems.
In active systems, there are computational particles that can control the way they act and move, while in passive systems
the particles either do not have any intelligence at all, or they may have limited computational capabilities but they cannot
control their movements. Examples of algorithmic research on passive systems are DNA computing [2, 9, 12, 15, 26, 37],
population protocols [4], and slime molds [10, 24, 31]. We will not describe these models in detail as they are only of
little relevance for our approach. Examples of active systems are self-organizing networks as well as swarm robotics and
modular robotic systems.
Self-organizing networks have been studied in many different contexts. Networks that evolve out of local, selforganizing behavior have been heavily studied in the context of complex networks [35] such as small-world networks
[6, 22, 32]. However, whereas a common approach for the complex networks field is to study the global effect of given
local interaction rules, we are interested in developing local interaction rules in order to obtain a desired global effect.
Also so-called self-stabilizing overlay networks have been studied, see, for example, [16] for an in-depth treatment. While
the proof techniques used in this area promise to be useful also for self-organizing particle systems, the constraints on
1

particle systems are much more severe than on nodes in overlay networks, which can freely change their interconnections
and which have no limitations on their computational power.
In swarm robotics it is usually assumed that there is a collection of autonomous robots that have a limited sensing and
communication range and that can freely move in a given area. Surveys of recent results in this area can be found, e.g.,
in [21, 25]. While the analytical techniques developed in this area are of some relevance for this work, the underlying
model differs significantly as we do not allow free movement of particles. While swarm robotics focuses on inter-robotic
aspects in order to perform certain tasks, the field of modular self-reconfigurable robotic systems focuses on intra-robotic
aspects such as the design, fabrication, motion planning, and control of autonomous kinematic machines with variable
morphology. Since the field started with the development of CEBOT by Toshio Fukuda [19], a growing number of research
groups have become actively involved in modular robotics research [18, 38]. Metamorphic robots form a subclass of selfreconfigurable robots that shares the characteristics of our model that all particles are identical and that they fill space
without gaps [13]. The hardware development in the field of self-reconfigurable robotics has been complemented by
a number of algorithmic advances (e.g., [11, 14, 27, 29]), but so far mechanisms that scale to hundreds or thousands
of individual units are still under investigation, and no rigorous theoretical foundation is available yet. So although the
advances in this area are relevant for the feasibility of our model, the area does not provide algorithmic and analytical
techniques that are directly applicable to our self-organizing particle systems.

3

The Artificial Ameba

In the following, we present our ameba-inspired model for particle systems in two dimensions. In our model, particles
are of hexagonal shape such that they can fill space without gaps. The number of particles in the system is not known by
any individual particle and particles are anonymous. Furthermore, particles do not know their position or orientation (nocompass model, e.g., [17]). However, the particles are assumed to be able to distinguish clock-wise from counter-clockwise. Particles form bonds with their immediate neighbors and the particles in a particle system have to form a connected
structure at all times. Computationally, particles act like probabilistic finite automata. Note, that this decision does not
restrict the number of particles in the system as particles have no global knowledge. The particles act in synchronous
rounds. A particle uses the configurations of neighboring particles together with its own configuration to compute its next
action. Locomotion of particles is realized by sequences of expansion and contraction. Both, the ability to form bonds
and the locomotion of particles, are inspired by a behavior called cell crawling that can be found in ameba [20] and cells
like macrophages [3]. Our decision for hexagonal particles is a consequence of the way particles connect and move since
other regular polygons that tile two-dimensional space, namely the triangle and the square, introduce certain technical
problems in this context. As an example, square particles might require diagonal connections for one particle to move
around another particle; however, diagonal connections might be hard to implement since diagonal particles meet only in
one point. Finally, particles are allowed to replicate and dissolve which is inspired by cell division and cell death.
Formally, a particle p is a tuple (Q, δ, q0 ) where Q is a set of states, δ is a probabilistic transition function, and q0 ∈ Q
is the start state. A particle system P is an ordered set of particles. The particles are positioned on an infinite hexagonal
grid. We assign coordinates from Z2 to this grid as depicted in Figure 1. This coordinate system is adopted from [28] and
is a modification of the coordinate system presented in [13]. An element of the grid is called a cell c ∈ Z2 . A direction is
an element d ∈ Z6 where Z6 are the integers modulo 6. We define the neighbor nd (c) of a cell c in direction d as follows.
(
c + (0, 1)
, if d = 0
c + (1, 0) , if d = 1
c + (1, −1) , if d = 2
nd (c) =
c + (0, −1) , if d = 3
c + (−1, 0) , if d = 4
c + (−1, 1) , if d = 5
Accordingly, the neighborhood of a cell c is defined as N (c) = {nd (c) | d ∈ Z6 }. Every particle assumes a shape
s ∈ S = {s1 , s2 }, see Figure 2. We denote the set of cells occupied by a particle p as C(p); accordingly we have |C(p)| ∈
{1, 2}. We call a cell occupied if there is a particle occupying it, otherwise we call it free. Every
cell can be occupied
S
by at most one particle at all times. We define the neighborhood of a particle p as N (p) = ( c∈C(p) N (c)) − C(p). In
Figure 2, the cells in the neighborhood of the particles are shown in gray. Depending on the shape of a particle p we have
|N (p)| ∈ {6, 8}. The position of a particle is uniquely defined by its head cell h ∈ Z2 . The rotation of a particle is defined
by a direction r ∈ Z6 that we call its orientation. In Figure 2, the orientation is depicted as an arrow inside the particle
and the head cell is the cell that contains this arrow. The cells in the neighborhood of a particle are numbered according
2

Figure 2: The two shapes of particles. The neighboring
cells (gray) are numbered according to the orientation of
the particle (arrow); the cell labeled with number i is the
neighbor ni (p) of the respective particle p.

Figure 1: Coordinates in the hexagonal grid. The cells that
constitute the coordinate axes are shown in black.

(q1 , s2 , (n0 , . . . , n7 ))
n0
n2
n4
n6

= (q3 , s2 , {6})
= (q0 , s1 , {0, 1})
=ǫ
= (q1 , s2 , {5, 6})

n1
n3
n5
n7

=ǫ
=ǫ
= (q1 , s2 , {5, 6})
= (q3 , s3 , {6})

Figure 3: Example of an element of the domain of the transition function. The transition function is to be evaluated for the gray
particle in the left half of this figure. The labels in the head cells of the particles represent their respective state. The right half shows
the formalization of the depicted situation.

to the orientation of the particle as shown in Figure 2. The numbering starts with 0 at the cell nr (h) and is increased in
clock-wise direction around the particle. We denote the neighboring cell of a particle with number i as ni (p). Finally, a
configuration of a particle is a tuple (q, s, h, r) and consists of its state q ∈ Q, its shape s ∈ S, its head cell h ∈ Z2 , and
its orientation r ∈ Z6 . Visually, any valid configuration of a particle can be achieved by taking one of the configurations
shown in Figure 2, rotating it by a multiple of 60◦ , and translating it to the desired position. The configuration of a particle
system is the ordered set of configurations of all its particles. We define two particles u and v to be connected according
to their configurations if N (u) ∩ C(v) 6= ∅ or, equivalently, N (v) ∩ C(u) 6= ∅. The configuration of a particle system P
then induces an undirected graph GP = (P, E) where {u, v} ∈ E if and only if particles u and v are connected. We call
GP the connectivity graph of the particle system P according to its configuration. A configuration is called connected if
the according connectivity graph is connected.
Initially, the particle system is assumed to be in some connected configuration in which all particles are of shape s1
and in the start state q0 . The configuration of the particle system has to stay connected at all times. The particles act in
synchronous rounds according to their respective probabilistic transition function δ that has the following signature.
δ : Q × S × N 8 → P(Q × A)
The transition function maps the state and shape of a particle p combined with some neighborhood information to tuples
of a new state and an action. The neighborhood information represents the local view of p on its neighboring cells N (p)
and particles occupying these cells. For a single cell it is of the following form.
N = (Q × S × P({0, . . . , 7})) ∪ {ǫ}
Considering a specific cell c = ni (p), the (i + 1)-th element of the tuple N 8 is ǫ if i ≥ |N (p)| or the cell c is free.
Otherwise, i.e., if the cell is occupied by a particle p′ , it consists of the state and shape of p′ as well as the set {j | nj (p′ ) ∈
C(p)}. An example of an element of the domain of the transition function is given in Figure 3. The set of actions is
defined as A = {N, T, E, C, D, K} and the definitions of these actions are given in Table 1. An example of particles
executing these actions is depicted in Figure 4. Which actions a particle can execute depends on the shape of the particle;
3

Action
N
T

Description
Null Action
Turn

Shape
{s1 , s2 }
{s1 , s2 }

E
C
D

Expand
Contract
Divide

{s1 }
{s2 }
{s2 }

K

Kill

{s1 }

Configuration Changes
None.
If s = s1 set r ← r + 1.
Otherwise set h ← nr+3 (h) and then r ← r + 3.
Set h ← nr (h) and s ← s2 .
Set s ← s1 .
Set s ← s1 and then add a copy of the particle to the system
with h′ ← nr+3 (h) and q ′ being the state after δ was applied.
Remove the particle from the particle system.

Table 1: Definitions of the actions a particle of admissible shape can execute.

Figure 4: An example of a set of particles executing various actions over five rounds. The particles are labeled according to the
actions they execute.

the admissible shapes for each action are given in Table 1. If a transition function assigns an action to a particle in a
shape that is inadmissible for the action, the action fails and the transition function is not applied. If a particle p attempts
to expand into a cell that is already occupied by a particle p′ in round t and remains occupied by p′ in round t + 1, the
expansion of p fails and, again, the transition function is not applied. If a particle p attempts to expand into a cell that is
occupied by a particle p′ in round t but is freed in round t + 1 due to a contraction of particle p′ , p can expand into the
cell. In general, if multiple particles attempt to expand into the same cell, one arbitrary particle succeeds in expanding
while the others fail and their transition function is not applied.

4

Research Challenges

The proposed model allows to investigate various problems. For example, the class of smart paint problems might be
considered. Here, the surface of an object is to be covered as uniformly as possible by a set of particles. A second
example is the class of shape formation problems in which particles have to arrange to form specific shapes. In bridging
and covering problems particles have to cover or bridge gaps in given structures. For the above problem classes, particles
are not allowed to execute the divide or kill action. Another example is what we call the macrophage problem. This
problem is inspired by biological cells called macrophages [36] that can be found, for example, in the human body. Their
task is to engulf and digest pathogens. To locate their target, macrophages use chemotaxis [34], i.e., they move along
a gradient of chemicals in their environment. Our model can be extended to include chemotaxis, e.g., as in [5]. The
macrophage problem models the behavior of macrophages in the following way. The macrophage and the pathogen are
represented by two distinct particle systems; accordingly the macrophage and the pathogen can move independently of
each other. The challenge now is to find an algorithm such that the macrophage hunts and surrounds the pathogen to
immobilize it. Several variants of this problem emerge when deciding whether particles should be allowed to execute
the divide and kill action and whether macrophage and pathogen move at the same or different speeds. For an additional
variant, a new action could be added that allows the macrophage to kill particles that belong to the pathogen.
Also, variants of our model might be of interest. Firstly, in a physical realization particles may become faulty and a
particle system may not start in a well-initialized configuration. For a particle system to handle such occurrences it may
be necessary to allow for particles to detect other faulty particles and to cope with them, for example, by allowing particles
to carry other particles. Such a model would require self-stabilizing algorithms. Secondly, the model may be extended
from two to three dimensions using, for example, the rhombic dodecahedron as the shape for the particles [8, 39]. Lastly,
the model could be modified so that particles act asynchronously.
4

References
[1] H. Abelson, D. Allen, D. Coore, C. Hanson, G. Homsy, T. F. Knight, R. Nagpal, E. Rauch, G. J. Sussman, and
R. Weiss. Amorphous computing. Communications of the ACM, 43(5):74–82, 2000.
[2] L. M. Adleman. Molecular computation of solutions to combinatorial problems. Science, 266(11):1021–1024, 1994.
[3] R. Ananthakrishnan and A. Ehrlicher. The forces behind cell movement. International Journal of Biological Sciences, 3(5):303–317, 2007.
[4] D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in networks of passively mobile
finite-state sensors. Distributed Computing, 18(4):235–253, 2006.
[5] L. Bai, M. Eyiyurekli, and D. E. Breen. An emergent system for self-aligning and self-organizing shape primitives.
In Proceedings of SASO ’08, pages 445–454, oct. 2008.
[6] A.-L. Barabási and R. Albert. Emergence of scaling in random networks. Science, 286(5439):509–512, 1999.
[7] Y. Benenson, T. Paz-Elizur, R. Adar, E. Keinan, Z. Livneh, and E. Shapiro. Programmable and autonomous computing machine made of biomolecules. Nature, 414(6862):430–434, 2001.
[8] H. Bojinov, A. Casal, and T. Hogg. Emergent structures in modular self-reconfigurable robots. In Proceedings of
ICRA ’00, volume 2, pages 1734–1741, 2000.
[9] D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall. On the computational power of dna. Discrete Applied Mathematics, 71:79–94, 1996.
[10] V. Bonifaci, K. Mehlhorn, and G. Varma. Physarum can compute shortest paths. In Proceedings of SODA ’12, pages
233–240, 2012.
[11] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita. Generic decentralized control for lattice-based self-reconfigurable
robots. International Journal of Robotics Research, 23(9):919–937, 2004.
[12] K. C. Cheung, E. D. Demaine, J. R. Bachrach, and S. Griffith. Programmable assembly with universally foldable
strings (moteins). IEEE Transactions on Robotics, 27(4):718–729, 2011.
[13] G. Chirikjian. Kinematics of a metamorphic robotic system. In Proceedings of ICRA ’94, volume 1, pages 449–455,
1994.
[14] G. Chirikjian and A. Pamecha. Bounds for self-reconfiguration of metamorphic robots. In Proceedings of ICRA ’96,
volume 2, pages 1452–1457, 1996.
[15] E. D. Demaine, M. J. Patitz, R. T. Schweller, and S. M. Summers. Self-assembly of arbitrary shapes using rnase
enzymes: Meeting the kolmogorov bound with small scale factor (extended abstract). In Proceedings of STACS ’11,
pages 201–212, 2011.
[16] S. Dolev. Self-Stabilization. MIT Press, 2000.
[17] A. Efrima and D. Peleg. Distributed models and algorithms for mobile robot systems. In Proceedings of SOFSEM
’07, pages 70–87, 2007.
[18] R. Fitch and D. Rus. Self-reconfiguring robots in the usa. Japanese Robotic Society Journal, 21(8):4–10, 2003.
[19] T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss. Self organizing robots based on cell structures - cebot. In
Proceedings of IROS ’88, pages 145–150, 1988.
[20] Y. Fukui. Mechanics of amoeboid locomotion: Signal to forces. Cell Biology International, 26(11):933–944, 2002.
5

[21] S. Kernbach, editor. Handbook of Collective Robotics – Fundamentals and Challanges. Pan Stanford Publishing,
2012.
[22] J. Kleinberg. The small-world phenomenon: an algorithmic perspective. In Proceedings of STOC ’00, pages 163–
170, 2000.
[23] T. Krasinski, S. Sakowski, and T. Poplawski. Autonomous push-down automaton built on dna. Informatica, 36:263–
276, 2012.
[24] K. Li, K. Thomas, C. Torres, L. Rossi, and C.-C. Shen. Slime mold inspired path formation protocol for wireless
sensor networks. In Proceedings of ANTS ’10, pages 299–311, 2010.
[25] J. McLurkin. Analysis and Implementation of Distributed Algorithms for Multi-Robot Systems. PhD thesis, Massachusetts Institute of Technology, 2008.
[26] R. Nagpal, A. Kondacs, and C. Chang. Programming methodology for biologically-inspired self-assembling systems. Technical report, AAAI Spring Symposium on Computational Synthesis, 2003.
[27] E. Ostergaard. Distributed control of the ATRON selfreconfigurable robot. PhD thesis, University of Southern
Denmark, 2004.
[28] J. E. Walter, J. L. Welch, and N. M. Amato. Distributed reconfiguration of metamorphic robot chains. In Proceedings
of PODC ’00, pages 171–180, 2000.
[29] J. E. Walter, J. L. Welch, and N. M. Amato. Distributed reconfiguration of metamorphic robot chains. Distributed
Computing, 17(2):171–189, 2004.
[30] Y. Wang, P. Brown, and Y. Xia. Nanomedicine: Swarming towards the target. Nature Materials, 10(7):482–483,
2011.
[31] S. Watanabe, A. Tero, A. Takamatsu, and T. Nakagaki. Traffic optimization in railroad networks using an algorithm
mimicking an amoeba-like organism, physarum plasmodium. Biosystems, 105(3):225–232, 2011.
[32] D. J. Watts and S. H. Strogatz. Collective dynamics of ’small-world’ networks. Nature, 393(6684):440–442, 1998.
[33] Wikipdedia. Cell signaling. http://en.wikipedia.org/wiki/Intercellular_communication,
2013.
[34] Wikipdedia. Chemotaxis. http://en.wikipedia.org/wiki/Chemotaxis, 2013.
[35] Wikipdedia. Complex networks. http://en.wikipedia.org/wiki/Complex_network, 2013.
[36] Wikipdedia. Macrophages. http://en.wikipedia.org/wiki/Macrophages, 2013.
[37] E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman. Design and self-assembly of two-dimensional dna crystals.
Nature, 394(6693):539–544, 1998.
[38] M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll, H. Lipson, E. Klavins, and G. S. Chirikjian. Modular selfreconfigurable robot systems. IEEE Robotics Automation Magazine, 14(1):43–52, 2007.
[39] M. Yim, Y. Zhang, J. Lamping, and E. Mao. Distributed control for 3d metamorphosis. Autonomous Robots,
10(1):41–56, 2001.

6

Globecom 2012 - Wireless Networking Symposium

Optimal Transmission Power Control in the
Presence of a Smart Jammer
Dejun Yang

Jin Zhang

Xi Fang

Andrea Richa

Guoliang Xue

Abstract— Jamming defense is an important yet challenging problem. In this paper, we study the jamming
defense problem in the presence of a smart jammer, who
can quickly learn the transmission power of the user and
adaptively adjust its transmission power to maximize the
damaging effect. By modeling the problem as a Stackelberg
game, we compute the optimal transmission power for the
user to maximize its utility, in spite of the existence of the
smart jammer. We prove that the smart jammer is not
more damaging than a jammer without the intelligence,
provided that the user plays its strategy corresponding to
a Stackelberg equilibrium. This nice property is due to the
user’s ability to predict the jammer’s behavior.
Fig. 1.

I. I NTRODUCTION
Jamming defense is an important yet challenging problem in wireless networks, since jamming attacks are easy
to launch, as the jammer does not need any special
hardware. Attacks of this kind usually aim at the physical
layer and are realized by means of a high transmission
power signal that corrupts a communication channel. We
are interested in defending against smart jammers, who
can quickly learn the transmission pattern of the users
and adjust their jamming strategies so as to achieve
maximum damage. As a first step along this line, we
study the battle between a single user (a transmissionreceiver pair) and a single smart jammer. Using a game
theoretic approach, we derive optimal power control for
the user in the presence of a smart jammer.
Game theory is a natural tool to solve this problem.
Jamming defense can be considered as a game, where
both the user and the jammer are players. A Nash
Equilibrium (NE) is the status where no player has an
incentive to change its strategy unilaterally so as to
increase its own utility. Previous work [1–3] has been
done on this topic by proving the existence of an NE
and computing the NE. However, Nash Equilibrium is
not the best solution to the problem studied in this
paper, because the rationality of Nash Equilibrium is
based on the assumption that all players take actions
simultaneously. Whereas, in our model, the jammer is
Yang, Zhang, Fang, Richa and Xue are all affiliated with Arizona
State University, Tempe, AZ 85287. Email:{dejun.yang, zhang.jin,
xi.fang, andrea.richa, xue}@asu.edu. This research was supported
in part by ARO grant W911NF-09-1-0467 and NSF grants CCF0830704, CCF-1116368, CNS-1115129, and ECCS-0901451. The information reported here does not reflect the position or the policy of
the federal government.

978-1-4673-0921-9/12/$31.00 ©2012 IEEE

Jamming

intelligent in the sense that it can quickly learn the user’s
transmission power and adjust its transmission power
accordingly.
To the best of our knowledge, this paper is the first
to study the power control problem in the presence
of a smart jammer. As an initial step, we consider a
single user and a single jammer (the more challenging
scenario with multi users/jammers is a subject of future
research). The strategies of the user and the jammer are
their transmission power levels. We consider the utilities
of both the user and the jammer as functions of the
SINR value. We model the power control problem with a
smart jammer as a Stackelberg game [13], called Power
Control with Smart Jammer (PCSJ) game. In this game,
the user is the leader and the jammer is the follower.
The user is aware of the jammer’s existence and has the
knowledge of jammer’s intelligence, based on which the
user chooses a certain strategy so as to maximize its own
utility, while the jammer plays its optimal strategy given
the user’s strategy. In this way, the user behaves like
the so called ’foresighted’ player, and the Stackelberg
Equilibrium can be reached.
The rest of this paper is organized as follows: Section II briefly describes related work. The system model
and the Stackelberg game formulation are introduced in
Section III. In Section IV, we study the optimal strategies
of the user and the jammer, leading to the optimal
power control of the user. In Section V, we compare the
Stackelberg Equilibrium with the Nash Equilibrium. In
Section VI, we present numerical results. We conclude
this paper in Section VII.

5506

2

II. R ELATED W ORK
Due to the importance of jamming defense, wireless
network jamming has been extensively studied in the
past few years. A lot of jamming defense mechanisms
on the physical layer have been proposed [7–9, 14, 15].
Mechanisms have been designed to detect jamming as
well as avoid it. Frequency hopping and spread spectrum technology have been shown to be very effective
to avoid jamming. With enough bandwidth or widely
spread signals, it becomes harder to detect the start of a
packet quickly enough in order to jam it. Some jammingdefense MAC protocols, proposed by Richa et al. ([10],
[11]) also deal with the jamming problem: [10] proposed
a protocol that works in multi-hop wireless networks
modeled as a unit disk graph despite an adaptive adversary; [11] proposed a protocol that works in single-hop
wireless networks despite a reactive adversary. Note that
being reactive allows the adversary to make a jamming
decision based on the actions of the nodes at the current
step, which makes it more powerful.
Since jamming activities can be considered as a
player (the jammer) playing against another player (the
user), game theory is an appropriate method to deal
with this kind of problem. Many previous works have
studied jamming defense with game theory formulations [1, 3, 4, 6, 12]. In [6], Lai and El Gamal analyzed
the resource allocation problem in fading multi-access
channels using a Stackelberg game, where the base
station is the leader and the users are followers. The
base station first announces its strategy defined as the
decoding order of the different users as a function of
the channel state. Then the users compete based on
this particular decoding strategy. In [1], Altman et al.
studied the jamming game in wireless networks with
transmission cost. In this game, both user and jammer
take the power allocation on channels as their strategies. The utility function of the user is the weighted
capacity minus transmission cost. The utility function
of the jammer is the negative of the user’s weighted
capacity minus transmission cost. They proved the existence and uniqueness of Nash Equilibrium. In addition,
they provided analytical expressions for the equilibrium
strategies. In [3], the same group of authors extended
the jamming problem to the case with several jammers.
The difference from [1] is that they did not consider
transmission cost and they considered SINR and -SINR
as the utility functions for user and jammer, respectively.
They showed that the jammers equalize the quality of the
best subcarriers for transmitter on as low level as their
power constraint allows, mean while the user distributes
his power among these jamming sub-carriers. In [12],
Sagduyu et al. considered the power-controlled MAC
game, which includes two types of players, selfish and
malicious transmitters. Each type of user has different

utility function depending throughput reward and energy
cost. They also considered the case where the transmitters have incomplete information regarding other transmitter’s types, modeled as probabilistic beliefs. They
derived the Bayesian Nash Equilibrium strategies for
different degrees of uncertainty, and characterized the
resulting equilibrium throughput of selfish nodes.
In all the previous works on jamming defense, the
authors assume that the users and the jammers take
actions simultaneously. In this paper, we study the power
control problem in the presence of a smart jammer,
which has more power compared to the jammer model
studied before. To the best of our knowledge, we are the
first to address this problem.
III. S YSTEM M ODEL AND P ROBLEM F ORMULATION
In this section, we present the system model and formulate the problem to be studied.
A. System Model
Our system consists of a user (i.e., a transmitter-receiver
pair), and a jammer (i.e., a transmitter), as illustrated in
Figure 1. The user (jammer, respectively) has control
over its own transmission power.
Let P denote the transmission power of the user
and J denote the transmission power of the jammer.
In addition, we assume that the user and the jammer
transmit with cost E and C per unit power. As in [3, 12],
we adopt SINR as the reward function of the user in our
model. Hence, the utility function of the user is
u(P, J) =

αP
− EP,
N + βJ

(1)

and the utility function of the jammer is
v(P, J) = −

αP
− CJ,
N + βJ

(2)

where N is the background noise level on the channel,
and α > 0 and β > 0 are fading channel gains of the
user and the jammer, respectively.
In this paper, we deal with a smart jammer, which can
quickly learn the user’s transmission power and adjust
its transmission power accordingly to maximize (2). The
user’s transmission power can be accurately estimated
using physical carrier sensing and location knowledge.
We are interested in determining the transmission power
of the user such that the utility function (1) is maximized,
in the presence of a smart jammer.
B. Stackelberg Game Formulation
We model the power control problem as a strategic game.
In this game, both the user and the jammer are players.
The strategy of each player is its transmission power. The
objective of each player is to maximize its own utility
function.

5507

3

In our model, the jammer is smart. Based on this fact,
we model the power control problem despite the smart
jamming activity as a Stackelberg game, called Power
Control with Smart Jammer (PCSJ) game. The Stackelberg game is an appropriate tool to model the scenario,
where hierarchy of actions exists between players [5].
One of the players is the leader, and the others are
followers. The leader chooses a strategy that can maximize its own utility, while each of the followers chooses
a strategy that is its optimal strategy to the leader’s
chosen strategy. Moreover, the leader can foresee the
followers’ reaction, and hence adjust its strategy based
on this knowledge. In our game, the user is the leader
and the jammer is the follower. We assume that both of
them have the knowledge of the channel information. In
a Stackelberg game, the Stackelberg Equilibrium (SE)
is the set of strategies of the players, such that the
follower maximizes its utility given the leader’s strategy,
and the leader maximizes its utility given that it knows
the follower would do so.
We illustrate these concepts using a simple example
given in Table I. Assume that Player A is the leader
and Player B is the follower. If A plays strategy U , B
would play strategy R, as it gives player B a utility of
5 (as opposed to a utility of 2 should B play strategy
L). This leads to a utility of 6 for player A. If A plays
strategy D, B would play strategy L, as it gives player
B a utility of 3 (as opposed to a utility of 2 should B
play strategy R). This leads to a utility of 4 for player
A. Therefore A would play strategy U , leading to the
Stackelberg equilibrium (U , R).

αP
− CJ.
N + βJ
Thus we have the following lemma.
max v(P, J) = −

Lemma 1. Let P be a given strategy of the user. Then
the corresponding optimal strategy of the jammer is
{
2
0,
P ≤ CN
αβ ,
J(P ) = √ αβP −N
(4)
CN 2
C
,
P
>
,
β
αβ
whereas the corresponding utility value of the jammer is
{
2
− αP
P ≤ CN
N ,√
αβ ,
v(P, J(P )) = CN −2 αβCP
(5)
2
, P > CN
β
αβ .
Proof: To find the maximum value of v(P, J),
we differentiate v(P, J) with respect to J and set the
resulting derivative equal to 0,
0=

Player A

U
D

R

3,2
4,3

6,5
8,2

∂v(P, J)
αβP
=
− C.
∂J
(N + βJ)2

(6)

Considering the constraint J ≥ 0, we have the optimal
strategy of the jammer in (4). Plugging (4) into (2), we
obtain the optimal value of jammer’s utility function in
(5).
B. User’s Optimal Strategy
The user is aware of the existence of the jammer and
knows that the jammer will play its optimal strategy to
maximize its own utility. Therefore, the user can derive
the jammer’s strategy based on Lemma 1. To compute
the user’s optimal strategy, we solve the following optimization problem.

Player B
L

(3)

J≥0

max u(P, J(P )) =
P ≥0

αP
− EP,
N + βJ(P )

(7)

where J(P ) is given in (4).
The optimal strategy and the optimal utility value are
given in the following Lemma.

TABLE I
A SIMPLE GAME : THE FIRST NUMBER IN EACH CELL IS THE
UTILITY OF P LAYER A, WHILE THE SECOND NUMBER IS THE
UTILITY OF P LAYER B.

Lemma 2. The optimal strategy of the user is

αC
α

 4βE22 , E ≤ 2N ,
α
α
P SE = CN
αβ ,
2N < E ≤ N ,


α
0,
E>N
,

IV. O PTIMAL T RANSMISSION P OWER C ONTROL
In this section, we solve the power control problem by
studying the PCSJ game. First, we compute the optimal
strategy of the jammer, for a given strategy of the user.
Then, we compute the optimal strategy of the user, based
on the knowledge of the optimal strategy of the jammer.
A. Jammer’s Optimal Strategy
Assume that the user’s strategy P is given. Then the
jammer’s optimal strategy can be computed by solving
the following optimization problem.

and the optimal utility value of the user is

αC
α
E ≤ 2N
,

 4βE ,
(α−EN )CN
SE
SE
α
u(P , J(P )) =
, 2N < E ≤
αβ


α
0,
E>N
.

(8)

α
N,

(9)

Proof: Plugging (4) into the objective function (7),
we have

2
( α − E)P,
P ≤ CN
N
αβ ,
√
u(P, J(P )) =
(10)
CN 2
 αCP
β − EP, P > αβ .

5508

4

Hence u(P, J(P )) is a linear function in P for 0 ≤
2
P ≤ CN
αβ , and is a strictly concave function in P for
2
P > CN
αβ . Note that the derivative of u(P, J(P )) with
2
respect to P in the range P > CN
αβ is given by
√
∂u(P, J(P ))
1 αC
=
− E.
(11)
∂P
2 βP

u ( P , J ( P ))
aC
4bE

(a - EN ) CN

ab

CN 2
(a) E ≤

αC
Setting equation (11) to 0, we obtain P = 4βE
2.
To compute the maximum value of (10), we consider
three disjoint cases.
α
Case-1: E ≤ 2N
. In this case, we can verify that
2
CN
αC
αβ ≤ 4βE 2 . As illustrated in Figure 2(a), u(P, J(P ))
αC
αC
achieves its maximum value of 4βE
when P = 4βE
2.
α
α
Case-2: 2N < E ≤ N . In this case, we can verify that
CN 2
αC
αβ > 4βE 2 . As illustrated in Figure 2(b), u(P, J(P ))
)CN
when P =
achieves its maximum value of (α−EN
αβ
2
CN
αβ .
2
α
Case-3: E > N
. In this case, we also have CN
αβ >
αC
4βE 2 . As illustrated in Figure 2(c), u(P, J(P )) achieves
its maximum value of 0 when P = 0.
This proves the lemma.
Lemmas 1 and 2 lead to the following theorem.

{
J

SE

=

α
2E −N

β

0,

, E≤
E>

α
2N ,
α
2N .

The utility of the user is

αC
α
E ≤ 2N
,

 4βE ,
(α−EN )CN
SE
SE
α
u(P , J ) =
, 2N < E ≤
αβ


α
0,
E>N
.
and the utility of the jammer is
 (
)
C
α
α

 β N − E , E ≤ 2N ,
α
v(P SE , J SE ) = − CN
β ,
2N < E ≤


α
.
0,
E>N

α
N,

α
N,

V. C OMPARISON WITH NASH E QUILIBRIUM
In this section, we study the impact of the jammer’s
intelligence on the utilities of both players.
In the formulation of the PCSJ game, we have assumed that the jammer has the intelligence to quickly
learn the transmission power of the user and adjust its
own transmission power. If the jammer dose not have

P

α
2N

u ( P , J ( P ))
(a - EN ) CN

ab

CN 2

P

ab
(b)

α
2N

<E≤

α
N

u ( P , J ( P ))
CN 2

P

ab
(a - EN ) CN

Theorem 1. The strategy pair (P SE , J SE ) is the Stackelberg Equilibrium of the PCSJ game, where

α
αC

 4βE22 , E ≤ 2N ,
α
α
P SE = CN
αβ ,
2N < E ≤ N ,


α
0,
E>N
,
and

aC
4bE 2

ab

ab

(c) E >
Fig. 2.

α
N

User’s utility function

this intelligence, it would be natural to model the game
in which both players play the game simultaneously in
a non-cooperative manner. We refer the power control
game, where the jammer is lack of the intelligence,
as Power Control with Regular Jammer (PCRJ) game.
In the PCRJ game, a Nash Equilibrium (NE) strategy
corresponds to a desirable strategy of the player, since
by definition, it is a point where none of the players can
improve its utility by unilaterally changing its strategy.
We show that the user’s utility at the SE of the PCSJ
game is at least as high as the user’s utility at the NE
of the PCRJ game, and the jammer’s utility at the SE
of the PCSJ game is at least as high as the jammer’s
utility at the NE of the PCRJ game.
Before getting to the proofs, we illustrate the concepts
defined in this section using the example given in Table I.
This time, we assume that players A and B play their
strategies simultaneously. The strategy profile (U, R) is
not an NE, as player A can increase its utility from 6
to 8 by unilaterally changing its strategy from U to D.
(D, L) is an NE for this example, because neither A

5509

5

Lemma 3. There exists a unique NE (P N E , J N E ) in
the PCRJ game when the jammer does not have the
intelligence to learn the user’s strategy. In addition,
{
α
αC α/E−N
), E ≤ N
,
( βE
2,
NE
NE
β
(P
,J ) =
(12)
α
(0, 0),
E > N,
u(P

NE

and

{

v(P

NE

,J

NE

)=

,J

C
β

0,

NE

(

) = 0,

N−

2α
E

)

(13)

, E≤
E>

α
N,
α
N.

(14)

Proof: We consider two disjoint cases:
α
Case-1: E ≤ N
. If J = α/E−N
, the value of (1) is
β
0, for any P . However, in order to have J = α/E−N
,
β
αC
we must have P = βE
2 according to (6). Thus the NE
αC α/E−N
).
is (P N E , J N E ) = ( βE
2,
β
We now prove the uniqueness of NE in this case.
Assume to the contrary that there exists another NE
(P ′ , J ′ ). We first use contradiction to prove that J ′ =
J N E . If J ′ > J N E , P ′ = 0 according to (1). However,
if P ′ = 0, we must have J ′ = 0 according to (2),
contradicting to the assumption that J ′ > J N E > 0. If
J ′ < J N E , (1) becomes a strictly increasing function of
P . Hence the user can increase its utility by unilaterally
increase its transmission power, contradicting the NE assumption (P ′ , J ′ ). Thus we have proved that J ′ = J N E .
Since J ′ is a function of P ′ according to (6), we have
P ′ = P NE.
α
Case-2: E > N
. The derivative of (1) with respect to
P is
α
− E < 0,
N + βJ

α
α
< E ≤ N
.
v(P SE , J SE ) ≥ v(P N E , J N E ) when 2N
According to (14), we have
(
)
C
2α
C
v(P N E , J N E ) =
N−
≤ (N − 2N )
β
E
β
CN
= v(P SE , J SE )
= −
β

The theorem is proved.
VI. S IMULATIONS
In order to validate our theoretical insights, we ran
simulations and computed the SE of the PCSJ game,
as well as utilities of the user and the jammer at the SE.
For comparison, we also computed the NE of the PCRJ
game, we well as the utilities at the NE. The simulation
results show that SE leads to higher utilities for both
players than NE.
Five variables determine the players’ strategies and
their utilities, which are N , α, β, C, and E. Among these
five variables, only α and β, i.e., fading channel gains
of the user and the jammer, may vary significantly due
to the change of players’ physical locations. Hence, we
explore the relations of user and jammer’s utilities with
respect to different values of α and β. We set α and β to
be in the range of [0.1, 0.9]. Moreover, let C = E = 0.1
(as in [1]), and N = 2.
Figures 3(a) and 3(b) show the impact of α on the
players’ utilities, with β = 0.5. We observe that SE
leads to higher utilities for both players than NE does.
Recall that α is the fading channel gain of the user.
Therefore the larger α is, the closer the transmitter is
from the receiver. Hence, as α increases, user’s SE utility
increases while jammer’s SE utility decreases.
1
User’s Utility

nor B can increase its utility by unilaterally changing
its strategy.

for any J ≥ 0. Hence P = 0 is the unique optimal
strategy for the user. Since v(0, J) = −CJ, J = 0 is
the unique optimal strategy for the jammer. The NE is
then (P N E , J N E ) = (0, 0).
Combining (12), (1) and (2), we get (13) and (14).
Lemma 3 leads to the following important theorem,
as stated in the 3rd paragraph of this section.

≥ u(P N E , J N E ),
≥ v(P N E , J N E ).

0
0.4

0.6

0.8

α

(a) User’s utility
Jammer’s Utility

1

SE
NE

0
−1
−2
−3
−4

0.2

0.4

0.6

0.8

α

(b) Jammer’s utility

(15)
(16)

Proof: It is clear that u(P SE , J SE )
≥
u(P
, J N E ). For the jammer, it suffices to prove that
NE

0.5

0.2

Theorem 2. Let (P SE , J SE ) be the SE of the PCSJ
game, and (P N E , J N E ) be the NE of the PCRJ game.
Then we have
u(P SE , J SE )
v(P SE , J SE )

SE
NE

Fig. 3.

Impact of α on players’ utilities

Figures 4(a) and 4(b) show the impact of β on the
players’ utilities, with α = 0.5. Again, SE leads to higher

5510

6

utilities for both players than NE does. Note that the
jammer’s SE utility increases while the user’s SE utility
decreases due to the fact that the jammer’s influence on
the receiver gets stronger as β increases.
User’s Utility

1.5

SE
NE

1
0.5
0

−0.5

0.2

0.4

β

0.6

0.8

Jammer’s Utility

(a) User’s utility
4
2
0
−2
−4
−6
−8

SE
NE

0.2

0.4 β 0.6

0.8

(b) Jammer’s utility
Fig. 4.

Impact of β on players’ utilities

We can make many other observations. For example,
we observe from Figure 3(a) and Figure 4(a) that the
utility of the user is 0 in the NE. This validates our claim
in Lemma 3. We can also observe from Figure 3(a) that
the user’s SE utility is 0 for α ≤ 0.2, and positive for
α > 0.2. This confirms our analysis in Theorem 1 and
Lemma 2, which says the user’s SE utility is 0 when
)CN
α < EN , and (α−EN
when EN ≤ α < 2EN .
αβ
Note that we have EN = 0.1 × 2 = 0.2 for Figure 3(a).
VII. C ONCLUSIONS
In this paper, we have studied the problem of optimal
power control in the presence of a smart jammer, who
can quickly learn the transmission power of the user and
adjust its transmission power to maximize the damaging
effect. By modeling the problem as a Stackelberg game,
we have derived the optimal power control strategy for
the user to maximize its utility, despite the existence
of the smart jammer. We have proved that, due to the
user’s ability to predict the jammer’s behavior, the smart
jammer is not more damaging than a jammer without
the intelligence.
This paper studied the single-user single-jammer
single-channel scenario. As future research directions,
it would be interesting to consider the scenarios, where
there are multiple users or multiple channels.

[2] E. Altman, K. Avrachenkov, and A. Garnaev,
“Closed form solutions for symmetric water filling
games,” in Proc. of INFOCOM, 2008, pp. 673–681.
[3] E. Altman, K. Avrachenkov, and A. Garnaev, “Jamming in wireless networks: The case of several
jammers,” in Proc. of GameNets, 2009, pp. 585–
592.
[4] M. Bennis, M. Le Treust, S. Lasaulce, M. Debbah,
and J. Lilleberg, “Spectrum sharing games on the
interference channel,” in Proc. of GameNets, 2009,
pp. 515–522.
[5] D. Fudenberg and J. Tirole, Game Theory. Cambridge: The MIT Press, 1991.
[6] L. Lai and H. El Gamal, “The water-filling game
in fading multiple-access channels,” IEEE Transactions on Information Theory, vol. 54, no. 5, pp.
2110–2122, May 2008.
[7] A. Liu, P. Ning, H. Dai, and Y. Liu, “USD-FH:
Jamming-resistant wireless communication using
frequency hopping with uncoordinated seed disclosure,” in Proc. of MASS, 2010.
[8] X. Liu, G. Noubir, R. Sundaram, and S. Tan,
“SPREAD: Foiling smart jammers using multilayer agility,” in Proc. of INFOCOM, 2007, pp.
2536–2540.
[9] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein, “Using channel hopping to increase 802.11
resilience to jamming attacks,” in Proc. of INFOCOM, 2007, pp. 2526–2530.
[10] A. Richa, C. Scheideler, S. Schmid, and J. Zhang,
“A jamming-resistant mac protocol for multi-hop
wireless networks,” in Proc. of DISC, 2010.
[11] A. Richa, C. Scheideler, S. Schmid, and J. Zhang,
“Competitive and fair medium access despite reactive jamming,” in Proc. of ICDCS, 2011.
[12] Y. Sagduyu, R. Berry, and A. Ephremides, “Mac
games for distributed wireless network security
with incomplete information of selfish and malicious user types,” in Proc. of GameNets, 2009, pp.
130 –139.
[13] V. Stackelberg, Marketform und Gleichgewicht.
Oxford University Press, 1934.
[14] W. Xu, W. Trappe, Y. Zhang, and T. Wood, “The
feasibility of launching and detecting jamming attacks in wireless networks,” in Proc. of MobiHoc,
2005, pp. 46–57.
[15] W. Xu, T. Wood, and Y. Zhang, “Channel surfing
and spatial retreats: defenses against wireless denial
of service,” in Proc. of Workshop on Wireless
Security, 2004.

R EFERENCES
[1] E. Altman, K. Avrachenkov, and A. Garnaev, “A
jamming game in wireless networks with transmission cost,” in Proc. of NET-COOP, 2007, pp. 1–12.

5511

A Markov Chain Algorithm for Compression in Self-Organizing
Particle Systems
Sarah Cannon1∗

Joshua J. Daymude2†

Dana Randall1‡

Andrea Richa2§

arXiv:1603.07991v3 [cs.DC] 21 Sep 2016

1

2

College of Computing, Georgia Institute of Technology, Atlanta, GA 30332-0765
Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85281

Abstract
We consider programmable matter as a collection of simple computational elements (or
particles) with limited (constant-size) memory that self-organize to solve system-wide problems
of movement, configuration, and coordination. Here, we focus on the compression problem, in
which the particle system gathers as tightly together as possible, as in a sphere or its equivalent
in the presence of some underlying geometry. More specifically, we seek fully distributed, local,
and asynchronous algorithms that lead the system to converge to a configuration with small
perimeter. We present a Markov chain based algorithm that solves the compression problem
under the geometric amoebot model, for particle systems that begin in a connected configuration
with no holes. The algorithm takes as input a bias parameter λ, where λ > 1 corresponds to
particles√favoring inducing more lattice triangles within the particle system. We show that for all
λ > 2 + 2, there is a constant α > 1 such that at stationarity with all but exponentially small
probability the particles are α-compressed, meaning the perimeter of the system configuration
is at most α · pmin , where pmin is the minimum possible perimeter of the particle system. We
additionally prove that the same algorithm can be used for expansion for small values of λ; in
particular, for all 0 < λ < 2.17, there is a constant β < 1 such that at stationarity, with all but
an exponentially small probability, the perimeter will be at least β · pmax , where pmax is the
maximum possible perimeter.

1

Introduction

Many programmable matter systems have recently been proposed and realized—modular and
swarm robotics, synthetic biology, DNA tiling, and smart materials form an incomplete list—
and each is often tailored toward a specific task or physical setting. In our work on self-organizing
particle systems, we abstract away from specific settings and instead describe programmable matter as a collection of simple computational elements (to be referred to as particles) with limited
computational power that each perform fully distributed, local, asynchronous algorithms to solve
system-wide problems such as movement, configuration, and coordination. Here we present an
algorithm for compression, in which the particle system gathers as tightly together as possible, as
in a sphere or its equivalent in the presence of some underlying geometry. This phenomenon is
∗

sarah.cannon@gatech.edu. Supported in part by NSF DGE-1148903 and a grant from the Simons Foundation
(#361047 to Sarah Cannon).
†
jdaymude@asu.edu. Supported in part by an NSF undergraduate research award REU-026935.
‡
randall@cc.gatech.edu. Supported in part by NSF CCF-1526900.
§
aricha@asu.edu. Supported in part by the NSF under Awards CCF-1353089 and CCF-1422603.

1

often found in natural systems: fire ants form floating rafts by gathering in such a manner, and
honey bees communicate foraging patterns within their hives. While each individual ant or bee
cannot view the group as a whole when soliciting information, it can take cues from its immediate
neighbors to achieve cooperation. It is with this motivation that we present a distributed algorithm
for compression in the amoebot model derived from a Markov chain process.
In the (geometric) amoebot model, more formally defined in Section 2.1, particles with limited
computational power move among the vertices of the triangular lattice Γ (Figure 1(a)) by traveling
along the edges of Γ. The compression problem seeks to reorganize the configuration of a particle
system (via movements of particles) such that the system converges to a configuration with small
perimeter, where we measure the perimeter of a configuration by the length of the walk along its
boundary. We say a particle system is α-compressed, for α > 1, if the perimeter of the particle
configuration is at most α times the minimum possible perimeter for those particles.

1.1

Our results and techniques

We present a Markov chain M for particle compression under the geometric amoebot model that
can be directly translated into a fully distributed, local, asynchronous compression algorithm A.
Both A and M take as input a bias parameter λ (where λ > 1 makes induced lattice triangles more
favorable) and start from an arbitrary initial configuration for the particles that is connected and
has no holes.
Markov chain M is carefully designed according to the distributed and local nature of the
system, so that the particles always stay connected and no holes form. Furthermore, we prove M
is reversible and ergodic, meaning many of the standard tools of Markov chain analysis can be
applied. While most of these proofs rely only on first principles, we emphasize they are far from
trivial; working in a distributed setting necessitated carefully defined protocols for local moves that
made proofs challenging.
When the particles execute the local moves of M (by running A) for long enough, the configuration of the particles converges to the stationary distribution of M. We prove for all large
enough λ there is a constant α = α(λ) > 1 such that at stationarity, with all but exponentially
small probability, the particles are α-compressed, meaning the perimeter of the particle configura√
tion is at most α times the minimum perimeter (which is Θ( n) for systems of n particles). We
additionally show the counterintuitive result that λ > 1 is not enough to guarantee compression.
In fact, for all 0 < λ < 2.17, there is a constant β < 1 such that at stationarity with all but
exponentially small probability the perimeter is at least a β fraction of the maximum perimeter,
which is Θ(n) for systems of n particles. We call such a configuration β-expanded. This implies
that for any 0 < λ < 2.17, the probability that the particles are α-compressed is exponentially
small for any constant α.
The motivation underlying the design of this Markov chain is from statistical physics, where
ensembles of particles reminiscent of our amoebot model are used to study physical systems. Like
a spring relaxing, systems tend to favor configurations that minimize energy. The energy function
is determined by a Hamiltonian H(σ); each configuration
σ has weight w(σ) = e−B·H(σ) /Z, where
P −B·H(τ
) is the normalizing constant known as
where B = 1/T is inverse temperature and Z = τ e
the partition function.
In our amoebot model, we assign each configuration σ a Hamiltonian H(σ) = −t(σ), where t(σ)
is the number of triangles in σ, i.e., the number of faces of the triangle lattice Γ with all three vertices
occupied by particles. Setting λ = eB , we get w(σ) = λt(σ). As λ gets larger (by increasing B,
effectively lowering temperature), we favor configurations with a large number of occupied triangles,
causing increasingly compressed configurations. Favoring edges with both endpoints occupied is
2

an alternative metric we could consider, but we prove for connected configurations on Γ without
holes the two measures are equivalent. Likewise, favoring shorter perimeter is a third equivalent
representation.
The key tool used to establish compression is a careful Peierls argument, used in statistical
physics to study non-uniqueness of limiting Gibbs measures and to determine the presence of phase
transitions (see, e.g., [10]), and in computer science to establish slow mixing of Markov chains (see,
e.g., [3]). For standard Peierls arguments, configurations typically are not required to be connected
and can have holes. Here, we focus on the connected and hole-free model to show compression
can be achieved, even in an asynchronous distributed system where particles have constant-size
memory and use only local information.

1.2

Related Work

Our algorithm is derived from a carefully designed Markov chain, enabling us to provide provable
guarantees of its behavior. Random particle processes on the grid with hard constraints (e.g., simple exclusion processes where no two particles can occupy the same location) have been studied in
statistical physics, but we want results under the further constraints of distributed computing. Our
work exploits the memoryless, stochastic nature of Markov chains to accomplish particle compression in the amoebot model, just one of many distributed physical systems where compression-type
problems have been studied.
When considering physical systems and models, one can differentiate between active and passive
systems. Particles in passive systems have no explicit control over their movements, and in some
cases do not have any computational power. One example is the DNA self-assembly work described
in [22], where strands of DNA gather together to form larger structures with certain patterns.
In active systems, particles have control over their behavior and—depending on the model—
can achieve some directed locomotion. Swarm robotics is one example; different variations of
shape formation and collection problems have been studied (e.g. [13, 20]), but always with more
computational power or global knowledge of the system. Similarly, pattern formation and creation
of convex structures has been studied in the cellular automata domain (e.g. [5, 9]), but differs
from our model by assuming more powerful computational capabilities. The nubot model [23]
addresses a framework for biomolecular-inspired models which—although allowing some non-local
movements—provides additional ways to create two dimensional shapes in polylogarithmic time.
Nature offers a variety of examples in which gathering and cooperative behavior is apparent.
For example, social insects often exhibit compression-like characteristics in their collective behavior:
fire ants form floating rafts [18], cockroach larvae perform self-organizing aggregation [15, 19], and
honey bees choose hive locations based on a decentralized process of swarming and recruitment [4].
The rendezvous (or gathering) problem seeks to gather mobile agents together on some node of
a graph (see, e.g., [1] and the references within). In comparison, our particles follow the exclusion
principle, and hence would not be able to gather at a single node, and are computationally simpler
than the mobile agents considered.
Lastly, in [6, 7], algorithms for hexagon shape formation in the amoebot model were presented.
Although a hexagon satisfies a compressed configuration as we define here, the Markov chainbased algorithm we present takes a fully decentralized and local approach. This is naturally selfstabilizing, forgoing the need for a seed particle that may coordinate or initiate some underlying
organization of the set of particles, as required in [6] and even more critically in [7].

3

(a)

(b)

Figure 1: (a) A section of the triangular lattice Γ; (b) expanded particles (each denoted by its two
occupied adjacent locations in Γ and a thick line in between) and contracted particles (occupying
one location).

2

Background and Model

We begin with the geometric amoebot model for programmable matter. We then define some
properties of particle systems and discuss what it means for a particle system to be compressed. We
conclude with an overview of Markov chains, which form the basis for our algorithm for compression
to be presented in Section 3.

2.1

The Amoebot Model

In the amoebot model [8], programmable matter consists of particles whose properties we now detail.
An infinite undirected graph G = (V, E) represents the set of relative locations that the particles
can occupy (given by V ) and the set of all possible atomic transitions between locations in V
(given by E) [8]. We further assume the geometric variant of the amoebot model, which imposes
an underlying geometric structure G = Γ, where Γ is the triangular lattice shown in Figure 1(a)
(also called the infinite regular triangular grid graph, and denoted by Geqt in earlier work).
Each particle occupies either a single location (i.e., it is contracted) or a pair of two adjacent
locations on the graph (i.e., it is expanded); Figure 1(b) illustrates expanded and contracted particles
on Γ. Each location can be occupied by at most one particle. Particles achieve movement via a
series of expansions and contractions: a contracted particle may expand into an adjacent unoccupied
location to become expanded, and completes its movement by contracting to once again occupy
only one location.
Two particles occupying adjacent nodes are said to be connected by a bond (and hence induce
an edge in the particle system), and we refer to them as neighbors. Particles are anonymous, but
can uniquely identify each one of their (six) possible bonds and check which bonds lead to nodes
occupied by neighboring particles. Additionally, the particles do not share any global orientation
or coordinate system.
Every particle has a constant-size, shared, local memory which both it and its neighbors can
read and write to for communication. Because of the limitation on memory size, particles can know
neither the total size of the system nor an estimate of it. Particles execute a sequence of atomic
actions, in each of which they do some local computation (in our case, this may involve checking
which of its adjacent locations are occupied with particles) and an expansion or contraction. We
assume a fully asynchronous system, where particles perform atomic actions concurrently and at
different, possibly variable speeds; conflicts, which in our context arise when two particles attempt
to expand into the same location, are resolved in an arbitrary manner. In order to analyze such
systems, we use the standard asynchronous model from distributed computing, allowing us to
evaluate the progress of the system through a sequential series of individual particle activations,
where every time a particle is activated, it performs an atomic action.
4

2.2

Terminology for Particle Systems

First, we introduce notation and terminology that will be used throughout this paper. We call the
collection of locations in Γ that are occupied by particles an arrangement; note two arrangements
are the same even if particles occupy different locations within the arrangement. We can define an
equivalence relation on arrangements, where two arrangements are equivalent if one is a translation
of the other. We define a configuration to be an equivalence class of arrangements. If configuration σ
is a rotation of configuration τ , we still consider σ and τ to be distinct configurations. That is,
for the purpose of monitoring the particle system we maintain a global orientation of the particles,
even though each individual particle has no sense of global orientation.
We will let capital letters refer to particles and lower case letters refer to locations on the
triangular lattice Γ, e.g., “particle P at location `.” For a particle P (respectively, location `),
we use N (P ) (respectively, N (`)) to denote the set of particles adjacent to P (respectively, to `),
where by adjacent we mean connected by a lattice edge. Similarly, for a particle P (respectively,
location `), we will use n(P ) (respectively, n(`)) to denote the six locations in the neighborhood
of P (respectively, of `). For locations ` and `0 , by N (` ∪ `0 ) we mean (N (`) ∪ N (`0 )) \ {`, `0 }; the
same holds for n(` ∪ `0 ).
By an edge or triangle of a configuration σ we mean, respectively, an edge or face of Γ such
that all (respectively, two or three) incident vertices are occupied by particles. The number of
edges of σ is e(σ) and the number of triangles is t(σ). Throughout, by a path or a cycle we mean
a path or cycle in the underlying graph Γ where all vertices are occupied by particles, and in the
case of a cycle, at least one location inside the cycle is unoccupied. Two particles are connected
if there exists a path between them, and a configuration is connected if all pairs of particles are.
A hole in a configuration is a maximal finite component of adjacent unoccupied locations. We
specifically consider connected configurations with no holes, and our algorithm, if starting at such
a configuration, will maintain these properties.

2.3

Compression of Particle Systems

Our objective is to find a solution to the particle compression problem. There are many ways to
formalize what it means for a particle system to be compressed. For example, one could try to
minimize the diameter of the system, maximize the number of edges, or maximize the number of
triangles. We choose to define compression in terms of minimizing the perimeter. We prove for
connected configurations with no holes, minimizing perimeter, maximizing the number of edges,
and maximizing the number of triangles are all equivalent and are stronger notions of compression
than minimizing the diameter.
For a connected configuration σ of n particles with no holes, the perimeter of σ, denoted p(σ),
is the length of the walk around the (single external) boundary of the particles. In an abuse of
notation, we use the term perimeter to refer both to the length p(σ) of this walk and the walk
itself. We assume any walk W along the perimeter of a configuration is in the clockwise direction.
→
−
For a walk W in Γ with some notion of direction, we say location ` is left of edge e = st ∈ W
traversed from s to t if `, s and t are the vertices of a triangular face of Γ with ` the next
vertex counterclockwise around this face from t. With this terminology, every edge traversed in a
clockwise walk W along the perimeter of σ has an unoccupied location to its left. Specifically, for
any consecutive particles A, B, C in W, the locations in the clockwise span of n(B) from A to C are
always unoccupied and there is at least one of them. Note an edge may appear twice in a perimeter
walk W; in this case, its length is counted twice in p(σ). For a connected configuration of n particles
without holes, the perimeter ranges from a maximum value of 2n − 2 when the particles are in their

5

√
least compressed state (a tree with no induced triangles) to some minimum value pmin (n) = Θ( n)
√
when the particles are in their most compressed state. It is easy to see pmin (n) ≤ 4 n, and we
√
now prove any configuration σ of n particles has p(σ) ≥ n; this bound is not tight but suffices
for our proofs.
√
Lemma 2.1. A connected configuration with n ≥ 2 particles and no holes has perimeter at least n.
Proof.
√ We argue by induction on n. A particle system with two particles necessarily has perimeter
2 ≥ 2, as claimed. Let σ be any particle configuration with n particles where n > 2, and suppose
the lemma holds for all configurations with less than n particles.
First, suppose there is a particle Q ∈ σ not incident to any triangles of σ. This implies Q has
one, two, or three neighbors, none of which are adjacent. If Q has one neighbor, removing
Q from σ
√
0
yields a configuration σ with n − 1 particles and, by induction, perimeter at least n − 1. Thus
√
√
p(σ) = p(σ 0 ) + 2 ≥ n − 1 + 2 ≥ n.
If Q has two neighbors, removing Q from σ produces two connected particle configurations σ1
and σ2 , where σ1 has n1 particles, σ2 has n2 particles, and n1 + n2 = n − 1. Thus
√
√
√
√
p(σ) ≥ n1 + n2 + 4 > n − 1 + 4 > n.
Similarly, if Q has three neighbors its removal produces three particle configurations with n1 , n2 ,
and n3 particles, where n1 + n2 + n3 = n − 1, and we conclude
√
√
√
√
p(σ) ≥ n1 + n2 + n3 + 6 > n.
Now, suppose every particle in σ is incident to some triangle of σ,√implying there are at least
n/3 triangles in σ. An equilateral
√triangle with side length 1 has area 3/4, so the perimeter of σ
encloses an area of at least A = 3n/12. By the isoperimetric inequality, the minimum perimeter
way of enclosing this area, without regard to lattice constraints, is with a circle of radius r and
perimeter p, where
s √
r
r
√
A
n 3
πn
r=
=
, p = 2πr = √ > n.
π
12π
3
√
√
As the perimeter of σ also encloses an area of at least 3n/12, it is of length at least n.
When n is clear from context we omit it and refer to pmin = pmin (n) and pmax = pmax (n). We now
define what it means for a particle system to be compressed.
Definition 2.2. For any α > 1, a connected configuration σ with no holes is α-compressed if
p(σ) ≤ α · pmin .
We prove in Section 5 that our algorithm, when executed for a sufficiently long time, achieves
α-compression with all but exponentially small probability for any constant α > 1, provided n is
√
sufficiently large. We note α-compression implies the diameter of the particle system is also O( n),
so our definition of α-compression is stronger than defining compression in terms of diameter.
In order to minimize perimeter using only simple local moves, we exploit the following relationship.
Lemma 2.3. For a connected particle configuration σ with no holes, t(σ) = 2n − p(σ) − 2.

6

Proof. We count particle-triangle incidences, of which there are 3t(σ). Counting another way, every
particle has six incident triangles, except for those on the perimeter. Consider any traversal W
of the perimeter; at each particle, the exterior angle is 120, 180, 240, 300, or 360 degrees. These
correspond to the particle “missing” 2, 3, 4, 5, or 6 of its possible six incident triangles, or degree/60
missing triangles. If W visits the same particle multiple times, count the appropriate exterior angle
at each visit. The sum of exterior angles along W is 180p(σ) + 360, so in total particles on the
perimeter are missing 3p(σ) + 6 triangles. This implies there are 6n − 3p(σ) − 6 particle-triangle
incidences, so 3t(σ) = 6n − 3p(σ) − 6.
Minimizing perimeter is also equivalent to maximizing edges.
Lemma 2.4. For a connected particle configuration σ with no holes, e(σ) = 3n − p(σ) − 3.
Proof. The proof is nearly identical to that of Lemma 2.3, counting particle-edge incidences instead,
of which there are 2e(σ). Counting another way, every particle has six incident edges, except for
those on the perimeter. Consider any traversal W of the perimeter; at each particle, the exterior
angle is 120, 180, 240, 300, or 360 degrees. These correspond to the particle “missing” 1, 2, 3, 4,
or 5 of its possible six incident edges, or degree/60 − 1 missing edges. If W visits the same particle
multiple times, count the appropriate exterior angle at each visit. The sum of exterior angles along
W is 180p(σ) + 360, so in total particles on the perimeter are missing 3p(σ) + 6 − p(σ) edges. This
implies there are 6n − 2p(σ) − 6 particle-edges incidences, so 2e(σ) = 6n − 2p(σ) − 6.
Corollary 2.5. For a connected particle configuration σ with no holes, t(σ) = e(σ) − (n − 1).
Corollary 2.6. A connected particle configuration σ with no holes and minimum perimeter is also
a configuration with the maximum number of triangles (and edges).

2.4

Markov chains

The distributed protocol for particle compression we present is based on a Markov chain, i.e., a
memoryless stochastic process defined on a finite set of states Ω. The transition matrix P on
Ω × Ω → [0, 1] is defined so that P (x, y) is the probability of moving from state x to state y in one
step, for any pair x, y ∈ Ω. The t-step transition probability P t (x, y) is the probability of moving
from x to y in exactly t steps.
A Markov chain is ergodic if it is irreducible, i.e., for all x, y ∈ Ω, there is a t such that
t
P (x, y) > 0, and aperiodic, i.e., for all x, y ∈ Ω, g.c.d.{t : P t (x, y) > 0} = 1. Any finite, ergodic
Markov chain converges to a unique distribution π, i.e., for all x, y ∈ Ω, limt→∞ P t (x, y) = π(y). In
fact,P
for any distribution π 0 such that π 0 (x)P (x, y) = π 0 (y)P (y, x) (the detailed balance condition)
and x∈Ω π 0 (x) = 1, π 0 is the unique stationary distribution of M (see, e.g., [12]).
Given a desired stationary distribution π on Ω, the celebrated Metropolis-Hastings algorithm [14]
defines appropriate transition probabilities. Starting at state x, pick a neighbor y in Ω uniformly
with probability 1/(2∆), where ∆ is the maximum number of neighbors of any state, and move
to y with probability min (1, π(y)/π(x)); with the remaining probability stay at x and repeat. Using detailed balance, one can verify if the state space is connected then π must be the stationary
distribution. While calculating π(x)/π(y) seems to require global knowledge, this ratio can often
be calculated using only local information when
 many terms cancel out. In our case, the Metropolis probabilities are simply min 1, λ|t(y)−t(x)| ; this difference is just the change in the number of
triangles incident to the moving particle, which can be calculated with only local information.

7

3

Algorithms for Compression

Our objective is to demonstrate how stochastic algorithms can provably achieve compression, focusing on self-organizing particle systems on the triangular lattice Γ. Our algorithm is based on
Markov chain principles that will enable us to prove rigorous results about the algorithm’s behavior.
Remarkably, it does not even require the particles to communicate more than one bit of information to each other, even though the amoebot model allows for such exchanges; at any activation,
a particle only needs to know which of its neighboring locations are occupied and if any of those
neighbors are expanded.
Our algorithm carefully maintains several critical properties throughout its execution. First,
the particle system stays connected and no holes form, even while particles decide where to move
based only on local information. Additionally, any moves made are reversible: if a particle moves
to a new location, there is a nonzero probability that during the next step, it moves back to its
previous location. Finally, the moves allowed by the algorithm (and respective Markov chain)
suffice to transform any configuration of particles into any other configuration. These conditions
are essential so certain tools from Markov chain analysis can be applied.
In addition to the precise conditions needed to ensure connectivity and reversibility, our algorithm achieves compression by making particles more likely to move into a position where they
form more triangles with their neighbors. Specifically, a bias parameter λ controls how strongly
the particles favor being incident to triangles; λ > 1 corresponds to favoring triangles, while λ < 1
corresponds to disfavoring triangles. As Lemma 2.3 shows, locally favoring more triangles is equivalent to globally favoring a shorter perimeter; this is the relationship we exploit to obtain particle
compression.

3.1

The Local Algorithm A

We start with two key properties that enable a particle to move from location ` to location `0 . If `
and `0 are neighboring locations on Γ, let S = N (`) ∩ N (`0 ) be the set of particles adjacent to both `
and `0 (i.e., |S| = 0, 1, or 2).
Property 1. |S| = 1 or 2 and every particle in N (` ∪ `0 ) is connected to a particle in S by a path
through N (` ∪ `0 ).
Property 2. |S| = 0; ` and `0 each have at least one neighbor; all particles in N (`) \ {`0 } are
connected by paths within this set; and all particles in N (`0 ) \ {`} are connected by paths within this
set.
These properties capture precisely the structure required to maintain particle connectivity and
prevent holes from forming. Additionally, both are symmetric for ` and `0 , necessary for reversibility.
However, they are not so restrictive as to limit the movement of particles and prevent compression
from occurring; we will see moves satisfying these properties suffice to transform any configuration
into any other.
We now present the local asynchronous algorithm that each particle runs. Later we show how
to view this algorithm through the lens of our Markov chain M. Both A and M take as an input
a bias parameter λ > 1, and begin in an arbitrary starting configuration σ0 of contracted particles
that is connected and has no holes.
Each particle P continuously runs Algorithm A, executing Steps 1–7 if P is contracted, and
Steps 8–11 if P is expanded. Note a constant number of random bits suffice to generate q, as only
a constant precision is required (given that t0 − t is an integer in [−6, 6] and λ is a constant). In
Step 9, Condition (1) ensures no holes form; Condition (2) ensures the particles stay connected
8

Algorithm A: Algorithm for compression run by particle P
If P is contracted:
1: Let ` denote P ’s current location.
2: Particle P chooses neighboring location `0 uniformly at random from the six possible choices,
and generates a random number q ∈ (0, 1).
3: if `0 is unoccupied and P has no expanded neighboring particle at ` then
4:
P expands to simultaneously occupy ` and `0 .
5:
if there are no expanded particles adj. to ` or `0 then
6:
P sets f lag = T RU E in its local memory
7:
else P sets f lag = F ALSE.
If P is expanded:
Let t be the number of triangles formed by P when it was contracted at its original location
`, and let t0 be the number of triangles formed by P if it contracts at the location it expanded
into, `0 .
9: if (1) location ` does not have five neighboring particles, (2) locations ` and `0 satisfy Property 1
0
or Property 2, (3) q < λt −t , and (4) f lag = T RU E then
10:
P contracts to `0 .
11: else P contracts back to `.
8:

and the corresponding Markov chain M is reversible; Condition (3) ensures the moves happen
with probabilities such that M converges to the desired distribution; and Condition (4) ensures P
is the only particle in its neighborhood potentially moving to a new position, `0 , since P last
expanded. The claims regarding Conditions (1)–(3) pertain to the underlying Markov process and
will be formalized in the next subsections. To understand why Condition (4) suffices to ensure P
is the only particle (potentially) moving to a new location in its immediate neighborhood, note
that once P expands, any other particle P 0 expanding into a position adjacent to P will contract
back to its original position during its next execution, since P 0 will set its own flag to FALSE in
Step 7. We assume any conflicts arising from two particles attempting to concurrently move into
the same location are resolved arbitrarily. Hence, any concurrent movements will cover pairwise
disjoint neighborhoods and the respective actions will be mutually independent.
Following the classical asynchronous model [17], for any starting configuration σ0 , we have that
for any concurrent execution of A that reaches a configuration σ, there is a sequential execution of
the same atomic actions that also reaches σ, for all σ.

3.2

The Markov Chain M

We now make explicit the Markov chain M corresponding to Algorithm A. The state space of
M is the set of all connected configurations of n contracted particles with no holes. Transitions
between states happen subject to the rules and probabilities we now present.
Beginning at any connected, hole-free configuration σ0 of contracted particles, repeat:
1. Select particle P uniformly at random from among all particles; let ` be its location. Choose,
uniformly at random, `0 ∈ n(`) and q ∈ (0, 1).
2. If `0 is unoccupied then P expands to simultaneously occupy ` and `0 ; Else return to Step 1.
3. Let t be the number of triangles formed by P in position `, and let t0 be the number of
triangles formed by P in position `0 .
9

4. If (1) location ` does not have five neighboring particles, (2) locations ` and `0 satisfy Property
0
1 or Property 2, and (3) q < λt −t then contract P to `0 .
5. Else contract P back to `.
In M, we have paired up the consecutive actions that a particle will take according to Algorithm
A when contracted and then expanded into one state transition, so that a transition corresponds to
the complete movement of exactly one (contracted) particle into a new (adjacent) location. Let P
be a particle that eventually moves from location ` to `0 at some time t0 according to an execution
of A. Condition (4) in Step 9 and the condition in Step 3 of A ensure that if P was activated
and expanded at time t before being activated once again and contracting to occupy `0 at time
t0 > t, even if other particles are activated between t and t0 no movement to a new location will
occur in the neighborhood of P in that time interval. Hence all activations of other particles in
the neighborhood of P in the interval (t, t0 ) can be ignored, justifying the pairing of actions in the
Markov chain M. Note we assume a particle can tell the difference between an expanded particle’s
tail and head, that is, the location the particle previously occupied and the location that particle
has recently expanded into. When deciding which location to contract to, a particle whose flag is
set to true can ignore the heads of any adjacent expanded particles, knowing that those particles
necessarily have their flags set to false and will soon contract back to their original locations.
For every sequential execution of atomic actions that leads to configuration σ 0 in A, there exists
a sequence of transitions in M that reaches a configuration σ such that σ can be obtained from σ 0
by preserving the locations of all contracted particles in σ 0 and by letting every expanded particle
in σ 0 contract according to the rules of A, for all corresponding pairs (σ 0 , σ). Conversely, every
sequence of transitions in M that reaches a configuration σ directly corresponds to a sequence of
atomic actions in A also leading to σ 0 = σ. The perimeter of the respective σ 0 and σ differs by at
most a constant factor, and hence proving α-compression for σ also implies α0 -compression for σ 0 ,
and vice-versa, for constants α and α0 . Hence, we can use M, and respective Markov chain tools
and techniques, in order to analyze the correctness of algorithm A.
We note that, under the assumptions of the asynchronous model of distributed computing, one
cannot typically assume the next particle to be activated is equally likely to be any particle, as
we assume in Step 1 of the description of M. We make this assumption in order to be able to
explicitly calculate the stationary distribution of M so that we can provide rigorous guarantees
about its structure, but do not expect the behavior of the system would be substantially different
if this requirement was relaxed.
To justify this random activation assumption, we note that random sequences of particle activations can be approximated using Poisson clocks with mean 1.1 That is, each particle activates, and
executes Algorithm A, at a random real time drawn from the exponential distribution e−t . After
each action, the particle then computes another random time drawn from the same distribution
e−t and executes again after that amount of time has elapsed. The exponential distribution is
unique in that, if particle P has just activated, it is equally likely that any particle will be the next
particle to activate, including particle P (see, e.g., [12]). Moreover, the particles update without
requiring knowledge of any of the other particles’ clocks. Similar Poisson clocks are commonly used
to describe physical systems that perform updates in parallel in continuous time.
1

The analysis can be modified to accommodate each clock having its own constant mean; however, for the sake of
ease of presentation, we assume here that they are all i.i.d.

10

3.3

Invariants for Markov chain M

We begin by showing M maintains certain invariants. We prove Conditions (1) and (2) in Step 4 of
Markov chain M ensure the particles remain in a connected configuration with no holes, provided
they start in such a configuration.
Lemma 3.1. If the particles are initially connected, during the execution of Markov chain M they
remain connected.
Proof. Consider one iteration of M where a particle P moves from location ` to location `0 . Let σ
be the configuration before this move, and σ 0 the configuration after. We show if σ is connected,
then so is σ 0 .
A move of particle P from ` to `0 occurs only if ` and `0 are adjacent and satisfy Property 1
or Property 2. First, suppose they satisfy Property 1. Let P1 , P2 6= P be particles, and let Q be
a path connecting them in σ. If P ∈
/ Q, then P1 and P2 remain connected by Q in σ 0 . If P ∈ Q,
let N1 and N2 be the two particles on path Q before and after P , respectively. By Property 1, there
exist paths in N (` ∪ `0 ) from N1 to a particle S1 ∈ S and from N2 to a particle S2 ∈ S, possibly
with S1 = S2 . A (not necessarily simple) path from P1 to P2 in σ 0 is the union of Q from P1 to N1 ;
the path from N1 to S1 in N (` ∪ `0 ); if S1 6= S2 , the path from S1 to P at location `0 to S2 ; the
path from S2 to N2 in N (` ∪ `0 ); and Q from N2 to P2 . As P is connected to S1 ∈ S and by the
above argument S1 is connected to all other particles, we conclude P is connected to every other
particle and thus σ 0 is connected.
Next, assume locations ` and `0 satisfy Property 2. Let P1 , P2 6= P be particles; we show they
are connected by a path not containing P . Path Q connecting P1 and P2 in σ exists, and suppose it
contains P . Let N1 and N2 be the vertices on Q before and after P , respectively. Both N1 and N2
are neighbors of `, and by Property 2 all neighbors of ` are connected by a path in N (`). Thus Q
can be augmented to form a (not necessarily simple) walk Q0 by replacing P with a path from N1
to N2 in N (`). As P ∈
/ Q0 , this walk also connects P1 and P2 in σ 0 . Additionally, because `0 has
at least one neighbor by Property 2, P remains connected to all other particles in σ 0 and thus σ 0 is
connected.
Lemma 3.2. If the particles begin in a connected configuration with no holes, during the execution
of Markov chain M they will never form a configuration with a hole.
Proof. Recall we assume a cycle in σ encircles at least one unoccupied location; note a configuration
has a hole if and only if it has a cycle encircling that hole. Let σ be a particle configuration with
particle P at location `, and σ 0 the same configuration with P at neighboring location `0 . We
assume σ has no cycles (i.e., no holes) and prove σ 0 has no cycles (i.e., no holes).
We first show if a cycle is introduced in σ 0 , then P must be on that cycle. Suppose this is not
the case and σ 0 has a cycle C with P ∈
/ C. If P is removed from location `0 , C still exists. If P
is then placed at `, yielding σ, then C still exists unless it had enclosed exactly one unoccupied
location, `. However, this is not possible as any cycle in σ 0 \ P encircling ` would also necessarily
encircle neighboring unoccupied location `0 . This implies C is present in σ, a contradiction, so we
conclude any cycle in σ 0 must contain P .
By the conditions in Step 4 of Markov chain M that must be met before a move occurs,
particle P necessarily has fewer than five neighbors in σ and locations ` and `0 satisfy Property 1
or Property 2. First, suppose they satisfy Property 2. While P might momentarily create a cycle
when it expands to occupy both locations ` and `0 , it will then contract to location `0 . Suppose P
is part of some cycle C in σ 0 . Before and after P on C are some neighbors N1 and N2 of P . By
Property 2, N1 and N2 are connected by a path in N (`0 ), which doesn’t contain P . Replacing path
11

Figure 2: A particle configuration for which all valid moves of Markov chain M satisfy Property 2;
no particle has a valid move satisfying Property 1. This demonstrates the subtlety of the Markov
chain rules we have defined.
N1 − P − N2 in cycle C by this path in N (`0 ) yields a (not necessarily simple) cycle C 0 in σ 0 not
containing P , a contradiction.
Suppose ` and `0 satisfy Property 1; recall location ` has less than five neighbors in σ. Suppose
there exists a cycle C in σ 0 , which by definition encircles at least one unoccupied location. If ` is
unoccupied inside C, then so is at least one of its neighbors; we conclude C encircles some unoccupied
location `00 6= `. Let N1 and N2 be the particles on cycle C before and after P . If there exists a
path between N1 and N2 in N (`0 ), the argument in the previous paragraph applies, so we suppose
this is not the case. It must be, without loss of generality, that |S| = 2 and there exist paths in
N (` ∪ `0 ) from N1 to S1 ∈ S and from N2 to S2 ∈ S, with S1 6= S2 . There then exists cycle C 0
in σ, obtained from C by replacing path N1 − P − N2 , where P is in location `0 , with the path
N1 − ... − S1 − P − S2 − ... − N2 , where P is in location `. This is a valid (not necessarily simple)
cycle in σ, as it still encircles unoccupied location `00 6= `. This is also a contradiction, so in all
cases we find σ 0 has no cycles.

3.4

Ergodicity of Markov chain M

We next show the carefully-defined moves of M suffice to move from any configuration σ to any other
configuration τ , necessary for showing M is ergodic and thus has a unique stationary distribution.
We emphasize the details of this proof are far from trivial, and occupy the next ten pages. Figure 2
illustrates one difficulty. It depicts a particle configuration for which there exist no valid moves
satisfying Property 1; the only valid moves satisfy Property 2. Thus if moves satisfying Property 2
are not included, the state space of M is not connected.
Our approach relies critically on moves satisfying Property 2. At a high level, we prove for any
configuration σ there exists a sequence of valid particle moves transforming σ into a straight line,
and then prove M is reversible, implying for any other configuration τ there exists a sequence of
valid particle moves transforming that straight line into τ .
Given a particle configuration σ with lowest leftmost particle S, we find a sequence of valid
particle moves transforming σ into a line of particles stretching down and left from S; after one
potential move in a initialization step, particle S never moves. We traverse the perimeter of σ,
starting at S, finding particles which can be eliminated, or moved to this line stretching away
from S. We begin with some crucial definitions and subsequently examine the neighborhood of S,
where our iterative process will begin.

12

Q

S

l

T

S

(a)

T

(b)

Figure 3: Particle positions from the proofs of (a) Lemma 3.5 and (b) Lemma 3.6, where S is
the lowest leftmost particle of σ and T is its first neighbor. A black circle indicates a particle is
present, and an unfilled dashed circle indicates a particle is not present. Lines between particles
demonstrate adjacencies and do not denote expanded particles.
Definition 3.3. An unoccupied location ` in the triangle lattice is a gap of configuration σ if
placing an additional particle P at ` results in a hole in configuration σ ∪ P .
Definition 3.4. Let S be the lowest leftmost particle of σ. Its first neighbor T is the first particle
encountered in a clockwise traversal of n(S) beginning at the location directly above S.
Specifically, S’s first neighbor T is the particle directly above S, if such a particle is present;
otherwise, T is the particle above and right of S, if such a particle is present; if not, there must
be a particle below and right of S, and we call this particle T . Particle T will serve as the second
vertex of our walk W around the perimeter of σ as we look for particles to eliminate; given S and
T , W will be unique. We begin with an initialization lemma that will set up the base case of this
iterative process.
Lemma 3.5. Let S be the lowest leftmost particle of σ. If its first neighbor T is below and right of
S, there exists a valid move for S after which T is above and right of S.
Proof. As S is the lowest leftmost particle of σ, there are no particles in the column left of S or in
the same column as S below S. This also implies T is the only particle in N (S); see Figure 3(a).
It follows that particle S moving down one unit is a valid move satisfying Property 1, regardless of
whether there is a particle below T or not. After this move, T is still S’s first neighbor, and now
T is above and right of S.
Lemma 3.6. Let σ be a connected configuration with no holes and lowest leftmost particle S which
has first neighbor T above and right of S. Let ` be the location above S, and suppose ` is a gap.
There exists a sequence of particle moves, where S and T do not move, after which ` is not a gap.
Proof. Location ` is only a gap if the location two units above S is occupied by some particle Q
and the location above T is unoccupied; see Figure 3(a). Particle Q has at most two neighbors, and
those neighbors must be connected. Particle Q moving to ` is valid because Property 2 is satisfied.
There then exists two more valid moves for particle Q yielding configuration σ 0 with Q below and
left of S, where ` is not a gap in σ 0 .
Thus, we may assume S has its first neighbor T above S or above and right of S, and there are
no gaps in the clockwise span of n(S) from the location below and left of S to T . This will be an
important base case in the induction to follow.

13

We now begin to define the walk W around the perimeter of σ that will be the focus of our
inductive arguments. Let S be the lowest leftmost particle of σ. After a potential initialization
step described in Lemma 3.5, particle S will not move, so we refer to it as the anchor of particle
configuration σ and also as the anchor of all intermediate particle configurations. That is, if we
say ‘configuration σ with anchor S,’ either σ is the configuration we have started with and S is its
lowest leftmost particle or σ is an intermediate configuration and S was the lowest leftmost particle
in the initial configuration. We maintain that the only particles below and left of S stretch down
and left from S in a straight line.
A clockwise walk W around the perimeter of σ starting at S satisfies the property that for
any particle C (‘current’) that is after particle P (‘previous’) and before particle N (‘next’) in W,
there are no particles in the locations traversed in a clockwise span of n(C) from P to N . We
assume all walks start at S and have its first neighbor T as their second particle; given this, W
is unique. However, the ‘previous’ or ‘next’ particle of W is not well-defined at the start or end
of W, respectively. We will still refer to ‘the particle P before S in the walk’, but we will mean
the location below and left of S, whether or not there is a particle there. If the walk ends at some
particle U before traversing the whole perimeter, by ‘the particle N after U in the walk’, we mean
the next particle along the perimeter of σ. If the walk traverses the entire perimeter of σ and ends
again at particle S, by ‘the particle N after S in the walk’, we mean the location below and left
of S, whether or not there is a particle there. This will ensure we can use uniform language when
discussing the unoccupied locations in the clockwise span of N (C) from a previous particle P to a
next particle N , even when C is at the start or end of W.
Our plan is to iteratively move all particles along the perimeter of σ to a location adjacent
to particle S, and subsequently into a straight line stretching down and left from S. We say a
particle P is eliminated once it has moved to join this line. However, there may be obstacles to
immediately eliminating a particle P . We say a gap g is adjacent to a walk W if there exists
consecutive particles P , C, and N in W such that g is in the clockwise traversal from P to N of
n(C). Specifically, we consider a gap to be adjacent to a walk W if it is immediately left of an edge
in W. It is possible to eliminate a particle P by moving it along the perimeter from its current
position to S precisely when there are no gaps adjacent to, i.e. on the left side of, the perimeter
of σ from S to P . We now formalize this.
Lemma 3.7. Let σ be a connected configuration with no holes. Let V be any particle in σ with at
most four neighbors that are connected within n(V ). Let U be the farthest clockwise of V ’s connected
neighbors. Let `0 be the (necessarily unoccupied) location in n(V ) one unit clockwise from U , and
suppose `0 is not a gap. Then particle V moving to location `0 is a valid move satisfying Property 1.
Proof. As we assume V has fewer than five neighbors, it only remains to show V ’s location ` and
location `0 satisfy Property 1. First, we note S = N (`) ∩ N (`0 ) = {U }. This is because particle U
is adjacent to both ` and `0 , and as P ’s neighbors are connected and number at most four, it is not
possible for the other location `1 in n(`) ∩ n(`0 ) to also be occupied.
Every particle in N (`) is connected to U by assumption. It only remains to show every particle
in N (`0 ) \ V is connected to U by a path in this set. Suppose this is not the case and there exists
F ∈ N (`0 ) \ V not connected to U by a path in this set; see Figure 4(a) for an example. There must
be some unoccupied location `2 in the clockwise span of n(`0 ) from U to F . As σ is connected,
there exists some path P connecting F to U . Placing an additional particle Q at location `0 then
creates a cycle in σ, going from U to Q to F and then following P back to U , that encircles either
unoccupied location `1 or unoccupied location `2 . We conclude `0 is a gap, a contradiction. Thus
` and `0 must satisfy Property 1, so V moving from ` to `0 is valid as claimed.

14

l2 F
U

l’
V

l’
l1
S=U

V

V
U

S

U’

U’’

(a)

(b)

(c)

.

Figure 4: Images from the proofs of (a) Lemma 3.7 and (b,c) Lemma 3.8. A black circle indicates
a particle is present, and an unfilled dashed circle indicates a particle is not present. Lines between
particles demonstrate adjacencies and do not denote expanded particles.
Lemma 3.8. Let σ be a connected particle configuration with no holes and anchor S. Consider
the clockwise walk W along the perimeter from S to some particle U , and suppose there aren’t any
gaps adjacent to W. Let V be the next particle after U on the perimeter of σ, and suppose V ’s
neighbors are connected and number at most four. Then there exists a valid sequence of moves for
particle V that eliminates it, moving it below and left of S.
Proof. We use induction on the length of walk W. First, suppose |W| = 0, i.e., U = S, and there
are no gaps adjacent to this length 0 walk in the clockwise span of n(S) from S’s lower left neighbor
to V ; see Figure 4(b) for an example. As N (V ) is connected, by Lemma 3.7 particle V moving
from its current location one unit counterclockwise around S is a valid move. After this move,
N (V ) remains locally connected, and in fact N (V ) = S so |N (V )| = 1 < 5. We repeat the same
argument until V is above and left of S, at which point there is a sequence of valid moves for V
that eliminates it.
Now, let U be at distance k > 0 from S, and suppose the statement holds for all walks of length
less than k. Let `0 be the location in n(V ) clockwise from U . Because `0 is adjacent to U , it is
not a gap; see Figure 4(c). Because N (V ) is connected and of size at most four, by Lemma 3.7
particle V moving to location `0 is valid. If V at location `0 is still not adjacent to the particle
U 0 that is before U in W, as is the case in Figure 4(c), repeat: necessarily, V at location `0 has
exactly one neighbor, U , and as none of U ’s neighbors are gaps we apply Lemma 3.7 until V is
adjacent to U 0 . At this point, the walk W 0 from S along the perimeter of σ to particle U 00 before V
on the perimeter (possibly U 00 = U 0 ) is strictly shorter than W and still not adjacent to any gaps.
Particle V must have at most four neighbors in its new location, otherwise that new location was
a hole before V occupied it, impossible by Lemma 3.2. By the induction hypothesis, there exists a
sequence of valid moves for V that eliminates it.
Ideally, there would be no gaps adjacent to the perimeter of σ; then, all particles could be
eliminated. As gaps are problematic, our procedure will iteratively walk around the perimeter
of σ, eliminating particles to remove gaps when it encounters them. In order to be able to remove
gaps when we reach them, additionally we ensure W, the shortest perimeter walk beginning at S
adjacent to a gap, will not visit any particle more than once. If a particle P is revisited by W, it
is possible to eliminate a particle between the two occurrences of P on W, as we illustrate in the
next lemma. This can then be repeated to eliminate all particles between the two occurrences of P
on W, resulting in P only appearing once on W.
Lemma 3.9. Let σ be a connected particle configuration with no holes and anchor S. Let W be
the shortest perimeter walk starting at S to which a gap is adjacent, and suppose W visits some
15

particle P twice. Then there exists a particle in W between the two occurrences of P that can be
eliminated.
Proof. First, suppose there is no particle that appears in W two or more times between the two
occurrences of P . Let F be any particle on W between the two occurrences of P with at most four
neighbors; such a particle must exist as all particles in W have at most five neighbors, and it is not
realizable for every particle on W between the two occurrences of P to have exactly five neighbors.
Suppose F ’s neighbors are not connected. There then must be unoccupied locations on both sides
of walk W as it passes through F , which it does exactly once. However, this then implies the cycle
formed by the particles of W from the first occurrence of P in W to the second occurrence of P in
W encircles an unoccupied location, a contradiction as we assumed σ had no holes. We conclude
F ’s neighbors are connected. Because F ’s neighbors number at most four and are connected, we
apply Lemma 3.8 to eliminate F as claimed.
Suppose there is a particle that appears two or more times on W between the two occurrences
of P in W. By recursion, we can find some such particle P 0 , appearing twice on W between the two
occurrences of P , where no other particle appears twice between both occurrences of P 0 in W. We
then repeat the above argument for P 0 ; the particle between the first two occurrences of P 0 that
can be eliminated is also between the first two occurrences of P in W, satisfying the conclusion of
the lemma.
We now prove an additional lemma about moving particles while not introducing gaps. In
particular, we focus on W, the shortest perimeter walk starting at S to which a gap is adjacent.
The only gap(s) adjacent to W, i.e. to the left of W, are necessarily adjacent to its last particle.
Lemma 3.10. Let σ be a connected configuration with no holes and anchor S. Let W be the
shortest perimeter walk starting at S to which a gap is adjacent. Let Q be a particle with exactly
two neighbors N1 and N2 both adjacent to some location ` ∈ n(Q), where W ends on the same side
of the line through N1 and N2 as Q (possibly Q ∈ W, possibly Q ∈
/ W). There exists a sequence of
valid moves after which Q is at ` or some other particle is at `, where in either case no additional
gaps are created adjacent to W.
Proof. We assume without loss of generality Q’s neighborhood consists of, consecutively in clockwise
order, N2 , `, N1 , and three unoccupied locations. Let ζ be the line spanning N1 and N2 . We will
induct on the number of particles on the opposite side of ζ from particle Q.
First, suppose there are no particles on the opposite side of ζ from Q. Then particle Q moving
to location ` is a valid move satisfying Property 1, as all neighbors of n(Q) ∪ ` are in the set
S = N (Q) ∩ N (`) = {N1 , N2 }. Furthermore, no additional gaps have been created and thus there
are no new gaps adjacent to W.
We now suppose there are k ≥ 1 particles on the opposite side of ζ from Q, and assume the
conclusions of the lemma hold for all k 0 < k. We split the analysis into two cases, depending on
whether location ` is a gap or not.
Case 1: Location ` is a gap: The location on the opposite side of ` from Q must be occupied
by some particle Q0 , and the two locations in n(`) ∩ n(Q0 ) must be unoccupied; see Figure 5(a).
The clockwise perimeter walk from N2 to Q to N1 , which has ` on its left, cannot be in W as it is
adjacent to the gap at location `. Similarly, the clockwise perimeter walk through Q0 with ` to its
left is not contained in W.
If N (Q0 ) is not connected, we apply the induction hypothesis. Particle Q0 necessarily has two
neighbors Q1 and Q2 both adjacent to a common location `0 ; see Figure 5(b). The line ζ 0 spanning
Q1 and Q2 is parallel to line ζ, so W ends on the same side of ζ 0 as Q0 and there are strictly
16

ζ

ζ

N1

Q’

l

ζ’

Q

Q1

N2

(a)

Q’
l’

N1
l

ζ
Q
N2

Q’

N1
l

Q
N2

Q2

(b)

(c)

Figure 5: Particle positions from the proof of Lemma 3.10, Case 1. A black circle indicates a
particle is present, and an unfilled dashed circle indicates a particle is not present. Lines between
particles demonstrate adjacencies and do not denote expanded particles.
fewer than k particles on the opposite side of ζ 0 from Q0 . We conclude by induction there exists a
sequence of moves such that either Q0 moves to location `0 or another particle moves to location `0 .
In both cases, no gaps are introduced adjacent to W. In the first case, ` is no longer a gap and
we proceed to Case 2, below. In the second case, N (Q0 ) has become connected, the situation we
consider next.
If N (Q0 ) is connected, Q0 moving to location ` is a valid move satisfying Property 2; see
Figure 5(c). This move creates a new gap at the location Q0 used to occupy. However, this gap
is only adjacent to and left of perimeter walks not in W. We conclude a particle has moved to
location ` without creating any additional gaps adjacent to W, as desired.
Case 2: Location ` is not a gap: As location ` is not a gap, particle Q moving to location `
is a valid move that we assume occurs. If this move did not create an additional gap adjacent
to W we are done, so suppose such a gap is created. See Figure 6(a); we label the three other
neighbors of location ` as `1 , `2 , and `3 in counterclockwise order, where `1 is adjacent to N1 and `3
is adjacent to N2 . If Q created a gap adjacent to W when it moved to location `, this gap must be
at `1 , `2 , or `3 . A gap at one of these locations necessitates another particle on the other side of
the gap from location `. There are five such potential locations: a, adjacent to `1 ; b, adjacent to `1
and `2 ; c, adjacent to `2 ; d, adjacent to `2 and `3 ; and e, adjacent to `3 .
First, suppose a gap was created at `1 ; this means there must be a particle present at location a
or b. But then because there was already a particle at location N1 , `1 must have already been a
gap even before Q moved to `. This contradicts the assumption that moving Q to ` created an
additional gap adjacent to W. We conclude Q moving to ` does not create a gap at location `1 ,
and by symmetry, does not create a gap at location `3 . Thus, this move must have created a
gap at location `2 , where Q or the particle on the opposite side of this gap is in W. There is
necessarily a particle B at location b, a particle C at location c, or a particle D at location d, cases
we consider next.
Note this implies location `2 must be unoccupied. Furthermore, locations `1 and `3 must also
be unoccupied; if `1 or `3 is occupied by a particle P , any gap created at location `2 by Q moving
to ` would necessarily already be a gap because of P , contradicting that a new gap is created.
Gap created between Q and B: Suppose a gap is created at `2 between Q and a particle B at
location b, where W passes though Q or B with `2 to its left (there may or may not be particles
at c or d).
Suppose, for the sake of contradiction, `1 was a gap before Q moved to `. Then W cannot pass
through B or N1 such that `1 is on the left of W, as we assumed W is not adjacent to any gaps

17

a
b
c

N1
Q

l1
l2
d

A

l
l3
e
(a)

N2

N1

B

Q

l1
c

l2
d

l
l3
e

(b)

N2

N1

a
b

l1

c

l2
D

Q
l
l3

N2

E
(c)

Figure 6: Particle configurations from the analysis of Case 2 of the proof of Lemma 3.10. A
black circle indicates a particle is present, and an unfilled dashed circle indicates a particle is not
present. Locations with neither could be either occupied or unoccupied. Lines between particles
demonstrate adjacencies and do not denote expanded particles.
except for at its last particle, which is on the far side of ζ from B and N1 . As W does not pass
through N1 such that `1 is on the left of W, the only way for Q to be in W such that `1 is on the
left of the perimeter walk through Q would be if Q was the last particle in W. However, this would
imply there was already a gap in the clockwise span from N2 to N1 of n(Q) before Q moved to `,
impossible as we assumed ` was not a gap. We conclude W does not pass through Q or B such
that `1 is left of W, a contradiction.
Thus, if a gap is created at `2 between Q and a particle B at location b, `1 was not already a
gap before Q moved to `. In this case, the only way for `2 to become a gap between Q and B is
depicted in Figure 6(b): all particles in the clockwise span of n(`1 ) from B to N1 must be occupied.
While Q moving to ` creates a gap at `2 , subsequently N1 moving to `1 eliminates this gap. This
is a valid move because, after Q moves to `, N1 has two or three neighbors, and all are either in S
or adjacent to a particle in S so Property 1 is satisfied. Thus N1 moves to `1 and `2 is no longer a
gap.
Gap created between Q and D: Next, suppose a gap is created adjacent to W at location `2
between Q and a particle D at location d, where W passes though Q or D with `2 to its left (there
may or may not be particles at b or c). We note this is not quite symmetric to the argument in the
previous paragraphs for a particle B.
Suppose, for the sake of contradiction, that `3 was a gap before Q moved to `. Then W cannot
pass through D or N2 such that `3 is left of W, as we assumed W is not adjacent to any gaps except
at its last particle, which is on the far side of ζ from D and N2 . As W does not pass through N2
such that `3 is its left, and Q follows N2 on any perimeter walk, W does not pass through Q (at `)
such that `3 is on its left. We conclude W does not pass through Q or D such that `3 is left of W,
contradicting the creation of a new gap at `3 adjacent to W.
Thus, if a gap is created at location `2 between Q and a particle D at location d, it must
be that `3 was not already a gap before particle Q moved to location `. In this case, the only
way for `2 to become a gap after Q moves to ` is depicted in Figure 6(c): all particles in the
counterclockwise span of n(`3 ) from D to N2 must be occupied. While moving Q to ` creates a
gap at `2 , subsequently moving N2 to `3 is a valid move satisfying Property 2. Thus this move is
made and `2 is no longer a gap.
Gap created between Q and C: It remains to consider the case where a gap is created at `2
between Q and a particle C at location c, where W passes though Q or C with `2 to its left.
As we have already considered the cases where locations b or d are occupied, we assume they are
18

N1

a
b
C

Q

l1

l

l2
d

l3
e
(a)

N2

N1

a
N1’

b
C

l’

N2’

Q

l1

l

l2
d

l3
e
(b)

N2

N1

a
b
C

Q

l1

l

l2
d

l3

N2

e

(c)

Figure 7: Particle configurations from the analysis of Case 2 of the proof of Lemma 3.10. A
black circle indicates a particle is present, and an unfilled dashed circle indicates a particle is not
present. Locations with neither could be either occupied or unoccupied. Lines between particles
demonstrate adjacencies and do not denote expanded particles.
unoccupied; see Figure 7(a).
If C’s neighborhood is not connected, C has exactly two neighbors N10 and N20 , both incident to
a common unoccupied location l0 ∈ n(C); see Figure 7(b). The line through N10 and N20 is parallel
to line ζ, so W ends on the same side of this line as C. There are strictly fewer than k particles on
the opposite side of this line from C. We conclude by induction there exists a sequence of moves
such that either C moves to location `0 or another particle moves to location `0 . In both cases, no
gaps are introduced adjacent to W. In the first case, `2 is no longer a gap when particle Q occupies
location ` and we are done. In the second case, N (C) is now connected, the next case we consider.
Suppose N (C), which is at most size 3, is connected; see Figure 7(c) for an example. As moving
particle Q to location ` creates a gap at `2 along W, walk W passes through C or Q such that
location `2 is to its left. First, if C is in W, because C’s neighbors are connected and fewer than 5,
by Lemma 3.8 C can be eliminated. If C is not in W, then Q must be. Particle C moving to
position `2 is a valid move, satisfying Property 2, and C can then be eliminated using Lemma 3.8
as C has exactly one neighbor, Q. After this, `2 is no longer a gap as particle C has been eliminated.
We conclude Q has moved to location ` as part of a sequence of moves that does not introduce a
gap adjacent to W, as desired.
Lemma 3.11. Let σ be a connected configuration with no holes and anchor S such that at least
one particle is above or right of S. Suppose there is at least one gap adjacent to the perimeter of
σ, and let W be the shortest perimeter walk starting at S to which a gap is adjacent. There exists
a sequence of valid particle moves eliminating a particle or increasing the length of W.
Proof. First, suppose there are no gaps adjacent to the perimeter of σ. Perimeter walk W from its
start at S until it returns to S the first time satisfies the hypotheses of Lemma 3.9. We conclude
there exists a particle on W between the first two occurrences of S on W that can be eliminated.
If this is not the case, W as described in the statement of the lemma is well defined, and it has a
gap adjacent to its last particle V and no gaps are adjacent to any other particles in W.
If |W| = 0, there must be a gap adjacent to S in the clockwise span of n(S) from the location
below and left of S to S’s first neighbor T . If T is above S there cannot be a gap in this span, and
by Lemma 3.5 we may assume T is not below and right of S, so we conclude T is above and right
of S. This implies there is exactly one gap in the clockwise span of n(S) from the location below
and left of S to T , and it is the location ` directly above S. This is exactly the case described in
Lemma 3.6, so we conclude there exists a sequence of moves for a particle that is not S or T after
19

a
b
c

N1
l1
l2
d
(a)

l
l3
e

N3
V
N2

N1

A
B

l1
c

l

l2
d

l3

N3

Q

V
N2

P

g
V

e
(b)

(c)

Figure 8: Particle configurations appearing in the analysis of cases 2 and 3 of the proof of
Lemma 3.11. A black circle indicates a particle is present, and an unfilled dashed circle indicates a particle is not present. Locations with neither could be either occupied or unoccupied.
Lines between particles demonstrate adjacencies and do not denote expanded particles.
which ` is no longer a gap. After these moves there are no gaps adjacent to S in the clockwise span
of n(S) from the location below and left of S to T , so the length of W, the shortest perimeter walk
to which a gap is adjacent, has increased by at least one, as desired.
Now, suppose |W| ≥ 1. Note the perimeter of σ does not make a left turn at V . If it did, V
would have a single unoccupied neighbor in the clockwise span of n(V ) from the particle before V
to the particle after V on the perimeter of σ, and this location would necessarily be the gap V is
adjacent to. But then the particle before V on W is also adjacent to this gap, contradicting our
definition of W as the minimum length perimeter walk to which a gap is adjacent. In particular,
this implies |N (V )| < 5.
If N (V ) is connected, V satisfies the hypotheses of Lemma 3.8 and there exists a sequence of
moves eliminating it. If a particle B appears on W more than once, by Lemma 3.9, there exists a
particle of W somewhere between the first two occurrences of B that can be eliminated. We now
suppose N (V ) is not connected, and note this implies 2 ≤ |N (V )| ≤ 3 because the perimeter of σ
does not make a left turn at V . Furthermore, we assume particle V does not appear on walk W
except as its last vertex. There are three remaining cases to consider.
Case 1. Suppose |N (v)| = 2 and it is possible to add exactly one particle to some location ` to
make N (V ) connected. In this case we can’t eliminate V immediately because doing so could disconnect σ, so we instead consider V moving to location `. We are in exactly the case of Lemma 3.10,
as W ends at V , which is on the necessary side of the line spanning its two neighbors. We conclude there exists a sequence of moves that brings V to location ` or brings some other particle to
location ` such that no gaps were introduced adjacent to W. In the first case, V ∈ W is no longer
adjacent to a gap: the only possible gap left of W that V could be adjacent to is the location it
previously occupied, which cannot be a gap. In this case W can be extended beyond V and its
length increased, as claimed. In the second case, N (V ) has become connected, and by Lemma 3.8
particle V can be eliminated.
Case 2. Suppose |N (V )| = 3; we must be more careful in applying Lemma 3.10. At least two
of V ’s neighbors must be adjacent, and there is a unique location ` where we can add a particle to
make N (V ) connected. We label the neighbors of V adjacent to location ` as N1 and N2 , where N1
is clockwise from ` and N2 is counterclockwise from `. We label V ’s third neighbor as N3 , where N3
is adjacent to N1 or N2 , but not adjacent to `.
First, we suppose V ’s third neighbor N3 is adjacent to N1 ; see Figure 8(a) for this configuration

20

and a labeling of nearby locations, identical to that used in the Proof of Lemma 3.10. The proof of
Lemma 3.10 almost applies; the only argument that fails due to the existence of particle N3 is the
last paragraph of the analysis of the case titled Gap created between Q and B, with particle V now
playing the role of particle Q. This situation arises if location b is occupied by a particle B and
location `1 was not a gap before V moved to `. The only way for `2 to be a gap and `1 to not be a
gap is depicted in Figure 8(b). Recall we removed the gap created at `2 by subsequently moving N1
to `1 ; however, this is no longer valid because of N3 . We now argue this case cannot possibly occur.
By assumption, the clockwise perimeter walk N2 − V − N1 is not in W because V appears at most
once on this walk, as its last particle. This implies W does not pass through B such that `2 is to its
left. We conclude moving V to location ` could not have created a gap between B and V adjacent
to W.
A similar argument shows if N3 is adjacent to N2 , the identical problematic configuration where
a gap is created between V and particle D at location d also could not occur. In this case we crucially
use the fact that W ends at V such that ` is on its right side, and in particular W does not end on
the clockwise perimeter traversal from D to N2 to V .
The only cases for which the proof of Lemma 3.10 does not provide a valid sequence of moves
to accomplish the desired result do not occur. Thus the conclusions of Lemma 3.10 can be applied
to V . There exists a sequence of valid particle moves such that V moves to location ` or some
other particle moves to location `, and after this sequence of moves there are no additional gaps
adjacent to W. In the first case, there is no longer a gap adjacent to walk W at particle V so
|W| has increased; the only possibility for such a gap is the location V previously occupied, which
cannot be a gap. In the second case, V now satisfies the hypotheses of Lemma 3.8 because N (V )
is connected, so V can be eliminated.
Case 3. Suppose V has exactly two neighbors, and they are on opposite sides of V . In this
case, we have no hope of eliminating a gap from n(V ) by moving V , so we instead turn to the
particle on the opposite side of the gap adjacent to V . Because V is adjacent to some gap g, and
particle P before V on W is not adjacent to g or to any gap, the particles locally near V must look
as in Figure 8(c). We let Q be the particle on the opposite side of gap g from V . Because g is a
gap and neither P nor any other particle before P in W is adjacent to a gap, Q can have at most
three neighbors.
If N (Q) is not connected, Q satisfies the conditions of Lemma 3.11, and there exists a sequence
of moves such that Q moves away from gap g or some other particle moves adjacent to Q. In either
case, no gaps adjacent to W are introduced. In the first case, there is no longer a gap adjacent to V
and W can be extended. In the second case, N (Q) has become connected. If N (Q) is connected,
then Q moving from its current location to gap location g is a valid move, satisfying Property 2.
Particle Q can then be eliminated by Lemma 3.8.
Lemma 3.12. There exists a sequence of moves from any configuration σ to a straight line.
Proof. We apply Lemma 3.11 fewer than 2n2 times to eliminate all particles: a particle must be
eliminated at least once every 2n steps, as the length of W can increase at most p(σ) < 2n times
in a row. After this, all particles have been eliminated and they form a line stretching down and
left from S.
We present one more lemma before proving M is irreducible. The following lemma ensures if there
exists a valid sequence of moves transforming σ into a line, then there exists a valid sequences of
moves transforming the line back into σ. Recall P (σ, τ ) is the probability of moving from state σ
to state τ in one step of M.

21

Lemma 3.13. For any two configurations σ and τ in Ω, if P (σ, τ ) > 0 then P (τ, σ) > 0.
Proof. Let σ, τ ∈ Ω be any two configurations such that P (σ, τ ) > 0. This means σ and τ differ by
one particle P at location ` in σ and at adjacent location `0 in τ .
Note in τ , particle P at location `0 has at most four neighbors. This is because ` ∈ n(`0 ) is
unoccupied as particle P is instead at `0 , and at least one other location in n(`0 ) is unoccupied
as otherwise `0 would have been a hole in σ, impossible by Lemma 3.2. Because P (σ, τ ) > 0,
Property 1 or Property 2 must hold for ` and `0 . Both properties are symmetric with regard to
the role played by ` and `0 . If Markov chain M, in state τ , selects in Step 1 particle P , location
` ∈ n(P ), and a sufficiently small probability q, then because Conditions 1, 2, and 3 are necessarily
satisfied, particle P moves to location `. This proves P (τ, σ) > 0.
Lemma 3.14. Markov chain M connects the state space of all connected configurations without
holes.
Proof. Let σ and τ be any two connected configurations of n particles with no √
holes. By Lemma 3.12,
there exists a sequence of moves transforming σ into a line with slope 1/ 3. By Lemmas 3.12
and 3.13, there exists a sequence of valid moves transforming this line into τ .
Corollary 3.15. M is ergodic.
Proof. By Lemma 3.14, the state space is connected. M is aperiodic as at each iteration there is
a probability of at least 1/6 that no move is made. Therefore M is ergodic.

3.5

The stationary distribution π of M

We now know the Markov chain M is ergodic and finite, so its stationary distribution is unique.
Lemma 3.16. The stationary distribution π of M is
π(σ) = λt(σ) /Z,
where Z =

P

σ

λt(σ) is the normalizing constant, also called the partition function.

Proof. We confirm π is the stationary distribution by detailed balance. Let σ and τ be configurations in Ω with σ 6= τ such that P (σ, τ ) > 0. By Lemma 3.13, also P (τ, σ) > 0. Suppose particle P
moves from location ` in σ to neighboring location `0 in τ . Let t be the number of triangles on
which P is incident when it is in location `, and let t0 be that number when P is in location `0 .
This implies t(σ) − t(τ ) = t − t0 . Without loss of generality, let t0 < t. We see
P (σ, τ ) =

1 1 t0 −t
1 1
· ·λ
and P (τ, σ) = · · 1.
n 6
n 6

We now show σ and τ satisfy the detailed balance condition:
0

π(σ)P (σ, τ ) =

λt(σ) λt −t
λt(τ )
=
= π(τ )P (τ, σ).
Z 6n
Z · 6n

We conclude π is the stationary distribution of M.
While it is natural to assume maximizing the number of triangles in a particle configuration
results in more compression, here we formalize this. We prove π can also be expressed in terms of
perimeter. This implies M converges to a distribution weighted by the perimeter of configurations,
a global characteristic, even though the probability of any particle move is determined only by local
information.
22

Corollary 3.17. The stationary distribution π of M is also given by
π(σ) = λ−p(σ) /Z,
where Z =

P

σ

λ−p(σ) is the normalizing constant, also called the partition function.

Proof. We use Lemma 2.3 and Lemma 3.16:
λ2n−p(σ)−2
λ2n−2
λ−p(σ)
λ−p(σ)
λt(σ)
π(σ) = P t(σ) = P 2n−p(σ)−2 = 2n−2 · P −p(σ) = P −p(σ) .
λ
σλ
σλ
σλ
σλ

The stationary distribution of M can also be expressed in terms of edges.
Corollary 3.18. The stationary distribution π of M is also given by
π(σ) = λe(σ) /Z,
where Z =

P

σ

λe(σ) is the normalizing constant, also called the partition function.

Proof. This follows from Lemma 2.4 and Corollary 3.17:
λ−p(σ)
λe(σ)
λ−(3n−e(σ)−3)
λ−3n+3
λe(σ)
π(σ) = P −p(σ) = P −(3n−e(σ)−3) = −3n+3 · P e(σ) = P e(σ) .
λ
σλ
σλ
σλ
σλ

Convergence Time of Markov Chain M
√
We prove in Section 5 that when λ > 2 + 2, if Markov chain M has converged to its stationary
distribution, then with all but exponentially small probability the particle system will be compressed. However, we do not give explicit bounds on the time required for this to occur; we give
experimental evidence of convergence times in Section 7, but believe proving rigorous bounds will
be challenging.
A common measure of convergence time of a Markov chain is the mixing time, the number of
iterations until the distribution is within total variation distance ε of the stationary distribution,
starting from the worst initial configuration. Bounding the mixing time of a Markov chain achieving
compression is likely to be challenging because of the similarity to physical systems such as the Ising
and Potts models, common models for ferromagnetism. Algorithms that perform local updates are
known to have exponential mixing time for many of these models precisely because of a type of
compression of the systems [21]. However, mixing time most likley is not the correct measure of
our algorithm’s convergence. Even if it takes exponential time for M to converge to its stationary
distribution, which is certainly plausible, it may be true that the particles achieve α-compression
after only a polynomial number of steps. In fact, based on simulations, it appears compression
occurs in polynomial time; doubling the number of particles consistently results in a ten-fold increase
in iterations until compression starting from a straight line of n particles, so we conjecture the
number of iterations until compression occurs is close to O(n3.3 ).
3.6

23

(a)

(b)

(c)

Figure 9: (a) The hexagonal lattice. (b) A self-avoiding walk in the hexagonal lattice. (c) A walk
that is not self-avoiding.

4

Counting Particle Configurations by Perimeter

Before we prove our main results about particle compression, we first present some crucial lemmas
focusing on counting the number of particle configurations with a given perimeter. Having good
bounds on these numbers will strengthen the results about compression and expansion in the
stationary distribution π of algorithm M in the next section. In particular, if Sα is the set of
all particle
P configurations with perimeter at least α · pmin , we wish to have an upper bound on
π(Sα ) = σ∈Sα π(σ)/Z in order to argue that the stationary probability of a configuration having
large perimeter is small. Letting mk denote the number of configurations with perimeter k, using
Corollary 3.17 we can write π(Sα ) as
P2n−2
−k
k=dα·pmin e mk λ
π(Sα ) =
Z
In order to give an upper bound on π(Sα ), we must first establish an upper bound on mk , which
we do in Section 4.1.
proving particle compression we use a trivial lower bound on the normalizing constant Z =
P In−p(σ)
λ
in the equation above, noting Z ≥ λ−pmin , the weight of the configuration with minimum
σ
perimeter. However, in proving expansion properties for small λ we require a much better bound
on the normalizing constant Z. In particular, if S α is the set of all configurations
with perimeter
P
at most α · pmax for some constant α < 1, we wish to show π(S α ) = σ∈S α λ−p(σ) /Z is small, and
we do so by showing Z is large. We give a lower bound on Z in Section 4.2 that is valid for all
λ > 0 and a lower bound in Section 4.3 that is valid for all λ ≥ 1. As above, doing so requires a
bound on the number of configurations with certain perimeters.

4.1

An upper bound on the number of configurations with perimeter k

We turn to lattice duality and self-avoiding walks; for a more thorough treatment of self-avoiding
walks, see, e.g., [2].
Definition 4.1. A self-avoiding walk (SAW) in a planar lattice is a walk beginning at the origin
that never visits the same vertex twice.
We will focus on SAWs in the hexagonal lattice, also called the honeycomb lattice (Figure 9(a));
examples of self-avoiding walks and non-self avoiding walks in this lattice are shown in Figure 9
(b) and (c), respectively. The hexagonal lattice is of interest because it is dual to the triangular
lattice that particles occupy in our model. That is, if you create a new vertex in every face of the
24

(a)

(b)

Figure 10: (a) The duality between the triangular lattice and the hexagonal lattice. (b) An example
of a particle configuration σ, the polygon Aσ (shaded), and the self-avoiding walk in the hexagon
lattice that bounds Aσ (bold).
triangular lattice and connect two of these new vertices if their corresponding faces have a common
edge, you obtain the hexagonal lattice; see Figure 10(a).
Theorem 4.2 ([11]). The lth root of the number of self-avoiding walks of length ` in
pthe hexagonal
√
lattice from a given starting vertex x converges to µhex as l → ∞, where µhex = 2 + 2 is the
connective constant of the hexagonal lattice.
Lemma 4.3.
number of particle configurations with n particles and perimeter k is at most
√ The
k
f (k) · (2 + 2) for some subexponential function f .
Proof. To prove this theorem, we will consider the dual to the triangular lattice Γ, which is the
hexagonal lattice Γ0 . For any connected particle configuration σ with n particles and no holes,
consider the union Aσ of all the faces of Γ0 corresponding to vertices of Γ that are occupied in σ.
Whenever two particles are adjacent in Γ, their corresponding faces in Γ0 share an edge. This union
Aσ is a connected simple polygon because σ is connected and has no holes. Thus, the perimeter
of Aσ is a self-avoiding polygon in the hexagonal lattice, where a self-avoiding polygon is simply a
self-avoiding walk that starts and ends at the same vertex.
We next claim that if σ has perimeter k, then the perimeter of Aσ has 2k +6 edges (equivalently,
2k + 6 vertices). Consider an appearance of a particle Q on the perimeter of σ with exterior angle
θQ , where θQ = 120, 180, 240, 300, or 360 degrees. The corresponding hexagon HQ in the hexagonal
lattice then has (θQ /60) − 1 of its corresponding edges in the perimeter of Aσ . Additionally, any
hexagon with edges on the boundary of Aσ necessarily corresponds to a particle Q appearing on
the perimeter of σ, and vice versa. For particle configurations σ with perimeter k, we conclude the
number of edges on the perimeter of Aσ is



X  θQ
X
1 
1
p(Aσ ) =
−1 =
θQ  − k =
(180k + 360) − k = 2k + 6.
60
60
60
Q∈p(σ)

Q∈p(σ)

The number of self-avoiding polygons of perimeter 2k +6 in Γ0 is an upper bound on the number
of particle configurations with perimeter k. This value is itself less than the number of self-avoiding
p
√
walks of length 2k+5 in Γ0 . As the connective constant for the hexagonal lattice is√
µhex = 2 + 2,
5
k
the number of self avoiding walks of this length converges to µ2k+5
hex = µhex (2 + 2) as k goes to
25

infinity. In particular, there is some subexponential function f (k) such that the number of self√ k
avoiding walks of length 2k + 5 is at most f (k) · 2 + 2 .

4.2

A lower bound on normalizing constant Z

As discussed above, to strengthen our expansion results we first find a lower bound for the normalizing constant Z; this exploration will again proceed via counting the number of configurations
with n particles and a given perimeter. To begin, we recall pmax = 2n − 2 and note
X
X
λ−p(σ) ≥ m2n−2 λ−(2n−2) ,
Z=
λ−p(σ) ≥
σ

σ:p(σ)=pmax

where m2n−2 is the number of configurations with perimeter exactly 2n−2. Note if a configuration σ
with n particles has perimeter 2n−2, then it must be that σ has exactly n−1 edges and no triangles;
that is, σ is an induced tree in Γ. We present a method for enumerating a subset of these trees,
giving a lower bound on m2n−2 .
√
Lemma 4.4. For any λ, Z ≥ ( 2/λ)pmax .
Proof. We enumerate n-vertex paths in Γ where every step is either down-right or up-right; this is
a subset of the trees contributing to m2n−2 . Starting from the first particle, there are 2n−1 ways
to place rest of the particles to form such a path, where each one is either up-right or down-right
√ 2n−2 √ pmax
from the previous one. This gives m2n−2 ≥ 2n−1 = 2
= 2
. From this, it follows that
X
X
√
√ pmax −p
λ max = ( 2/λ)pmax .
Z=
λ−p(σ) ≥
λ−pmax ≥ 2
σ

σ:p(σ)=pmax

As we will see
√ in Section 5, this directly implies the particles do not compress, even in the limit,
for any λ < 2. This bound can be improved significantly with a better lower bound for m2n−2 ,
though this is eclipsed by the lower bound for Z when λ ≥ 1 given in the next subsection.

4.3

An improved lower bound on normalizing constant Z for λ ≥ 1

In this section we provide an improved bound on Z when λ ≥ 1. The key observation is that for
λ > 1, for any value k < 2n − 2, λ−k ≥ λ−(2n−2) . Thus as pmax = 2n − 2, it follows that
X
X
Z=
λ−p(σ) ≥
λ−(2n−2)
σ

σ

Thus it suffices to find a lower bound on the total number of connected, hole-free configurations
with n particles and any perimeter, instead of only counting the number of configurations with
maximum perimeter; exploiting this, we will be able to get a better lower bound on Z than in the
case above where λ was unrestricted.
Lemma 4.5. For λ ≥ 1, Z ≥ 0.12 · (1.67/λ)pmax .
Proof. We first give a lower bound on the number of connected, hole-free configurations on n
particles by iteratively constructing a subset of them. Note there are 11 connected hole-free configurations with exactly 3 particles. Construct a particle configuration beginning with one particle P ,
and attach one of the 11 configurations with 3 particles to the right of P , either by placing its
highest leftmost particle H below and right of P , or by placing its lowest leftmost particle L above
26

P

H
P

+

H

=

L

OR

P

L

(a)

P
Q

H

+

=

Q

L

σ

OR
H

P

L

(b)
Figure 11: The iterative process of Lemma 4.5. (a) One of the 11 connected hole-free configurations
on 3 vertices, and the two ways it can attach to the single particle with which the iterative process
begins. (b) Another of the 11 connected hole-free configurations on 3 vertices, and the two ways it
can attach to a partially constructed configuration.
and right of P ; see Figure 11(a) for an example. In the first case, all locations directly below P and
all locations directly above H are unoccupied; this ensures no cycles and thus no holes have been
created. Similarly in the second case, all locations above P or below L are unoccupied, ensuring no
holes form. Thus, using this process, there are 22 possible ways to attach 3 particles to P to yield
a hole-free configuration with 4 particles: each of the 11 hole-free configurations on 3 particles can
be attached to P in two ways. We note this does not construct all hole-free configurations of 4
particles, but constructs a subset via a process that will iterate nicely, as we now see.
For some hole-free configuration σ with 1 + 3j particles, j ≥ 1, let P be the highest rightmost
particle of σ and let Q be the lowest rightmost particle of σ; possibly P = Q. Choose any of
the 11 hole-free configurations with 3 particles, and let L be its lowest leftmost particle and H be
its highest leftmost particle; possibly H = L. Attach this configuration to σ either by placing H
below and right of Q or by placing L above and right of P ; see Figure 11(b) for an example. Note
even if Q = P and H = L, this still results in two distinct attachments. In the first case, the
locations directly below Q and the locations directly above H are necessarily all unoccupied. This
implies the only adjacency between σ and the newly added configuration is between Q and H; thus
a cycle is not created and the resulting configuration is hole free. The same conclusion follows in
the second case by examining the locations above P and the locations below L.
This process, applied j times, will yield a unique hole-free configuration on 1 + 3j particles. If
n 6≡ 1 mod 3, up to two additional particles can be added to the right of σ in some deterministic
way to form a configuration on n particles. At each step, there are 2 · 11 = 22 ways to attach the
n−1
next three particles; this process thus enumerates (22)j = 22b 3 c configurations on n particles.
This implies the total number of configurations with n particles is at least
22b

n−1
c
3

≥ 22

n−1
3

· 22−2/3 = 22−2/3 (221/6 )2n−2 > 0.12 · 1.672n−2 .
27

Using this bound, it follows that
Z=

X

λ

−p(σ)

≥

X

λ

−(2n−2)

2n−2

> 0.12 · 1.67

·λ

−(2n−2)


= 0.12 ·

σ

σ

1.67
λ

pmax
.

This bound can be improved even farther, using a result of Jensen [16]. In that paper, the author
presents a parallel algorithm efficient enough to enumerate the number of benzenoid hydrocarbons
containing h hexagonal cells up to h = 50. A benzenoid hydrocarbon containing h hexagonal cells
is exactly equivalent to a connected particle configuration with no holes and h particles, implying
the next lemma.
Lemma 4.6 ([16]). The number of connected particle configurations with no holes and 50 particles
is 2,430,068,453,031,180,290,203,185,942,420,933.
Lemma 4.7. For λ > 1, Z ≥ 0.13 · (2.17/λ)pmax .
Proof. We use the same approach as in Lemma 4.5, noting that 2.17 ∼ (2N50 )1/100 . Write n
as n = 1 + 50i + j, where i, j ∈ Z≥0 and j < 50; note subject to these requirements, i and j
are unique. Iteratively construct a particle configuration σ by beginning with one particle and
repeatedly attaching one of the N50 configurations with 50 particles to the right as in the proof of
Lemma 4.5: place its highest leftmost particle H below and right of σ’s lowest rightmost particle
Q, or place its lowest leftmost particle L above and right of σ’s highest rightmost particle P . For
the same reasons as in the previous lemma, this process, applied i times, will yield a hole-free
configuration on n − j particles. Attach the remaining j particles as one of the Nj configurations
on j particles, again attaching it in one of the two ways described above. This yields a connected
hole-free configuration on n particles. Using this method, we enumerate (2N50 )i · 2Nj unique holefree configurations on n particles. It follows that the number of connected hole-free configurations
on n particles is at least
(2N50 )i · 2Nj = (2N50 )

n−1−j
50

· 2Nj = (2N50 )

n−1
50

j

· (2N50 )− 50 · 2Nj .

Calculations show that for all 0 ≤ j < 50, (2N50 )−j/50 · 2Nj ≥ 0.13. It follows that the number of
connected hole-free configurations with n particles is at least

2n−2

pmax
0.13 · (2N50 )1/100
= 0.13 · (2N50 )1/100
.
Noting that (2N50 )1/100 > 2.17, it follows that
X
Z≥
λ−(2n−2) ≥ 0.13 · (2.17)pmax λ−pmax = 0.13 · (2.17/λ)pmax .
σ

As we will see in Section 6, this directly implies that the particle system will not exhibit
compression for any λ < 2.17. We expect this bound will improve given accurate counts of the
number of particle configurations for even larger n. However, computationally this seems infeasible,
and a careful analysis of the work done in [16] suggests the best bound achievable by this method
would be expansion for all λ√< 2.27, only a mild improvement and still far from the known lower
bound for compression, 2 + 2.

28

5

Achieving Compression

If M executes long enough, it will converge to its stationary distribution π; we will use the expression
of π given in Corollary 3.17. To simplify notation, we define the weight of a configuration σ to be
w(σ) = λ−p(σ) . For a set S ⊆ Ω, we define w(S) as the sum of the weights of all configurations in S.
We now show that, provided λ and n are large enough, with all but exponentially small probability
if M is at stationarity then the particles are in an α-compressed configuration. Constant α > 1
can be as close to 1 as desired, though smaller α requires larger λ. We now prove our main result.
√  α
Theorem 5.1. For any α > 1, there exists λ∗ = 2 + 2 α−1 , n∗ ≥ 0, and γ < 1 such that for
all λ > λ∗ and n > n∗ , the probability that a random sample σ drawn according to the stationary
distribution π of M is not α-compressed is exponentially small:
√

P (p(σ) ≥ α · pmin ) < γ

n

.

Proof. Let Sα be the set of configurations of perimeter at least α · pmin . Let σmin be a configuration
of n particles achieving the minimum perimeter pmin . We show
π(Sα ) =

√
w(Sα )
w(Sα )
<
≤ γ n.
Z
w(σmin )

The first equality is the definition of π; the next inequality follows√from the definitions of Z and w.
We focus on the last inequality. For simplicity, we let ν := 2 + 2 ∼ 3.42. Applying Lemma 4.3
and noting the weight of any σ with p(σ) = k is λ−p(σ) = λ−k , we sum over Sα :
P2n−2
k −k
2n−2
X
w(Sα )
k=dα·pmin e f (k)ν λ
≤
=
f (k)ν (1−logν λ)k+(logν λ)pmin ,
w(σmin )
λ−pmin
k=dα·pmin e

where f is the subexponential function from Lemma 4.3. Using the inequality pmin ≤ k/α, it
follows that
w(Sα )
≤
w(σmin )

2n−2
X

f (k)ν

(1−logν λ)k+(logν λ)(k/α)

2n−2
X

=

k=dα·pmin e

f (k)ν (1−(1−1/α) logν λ)k .

k=dα·pmin e

α

As λ > λ∗ = ν α−1 , then 0 > 1 − (1 − 1/α) logν λ =: −c1 . Also k ≥ α · pmin , and by Lemma 2.1,
√
pmin > n, so
w(Sα )
≤
w(σmin )
where f 0 (n) =
all n ≥ n∗ ,

Pn

√
k= n f (k)

2n−2
X

f (k)ν −c1 ·2α

√

n

≤ f 0 (n) ν −2αc1

 √n

,

k=dα·pmin e

is a subexponential function. There exists γ < 1 and n∗ such that for

P (p(σ) ≥ α · pmin ) = π(Sα ) ≤

√
 √n
w(Sα )
≤ f 0 (n) ν −2αc1
< γ n.
w(σmin )

√
Corollary 5.2. For any λ > 2 + 2 =: ν, there exists a constant α = logν λ/(logν λ − 1), n∗ ≥ 0,
and γ < 1 such that for all n > n∗ , a random sample σ drawn according to the stationary distribution
π of M satisfies
√

P (p(σ) ≥ α · pmin ) < γ
29

n

.

6

Using M for Expansion

A nice feature of our algorithm is that it also provably achieves particle expansion for different values
of bias parameter λ. We say a configuration σ is β-expanded for some β < 1 if p(σ) > β · pmax ,
√
where pmax = 2n − 2. We note as pmax = Θ(n) and pmin = Θ( n) for a system of n particles,
β-expansion and α-compression for any constants β and α are mutually exclusive for sufficiently
large n. We show that, provided n is large enough, for all 0 < λ < 2.17 there is a constant β
such that with all but exponentially small probability, if M is at stationarity then the particles are
β-expanded. This is notable because it implies, counterintuitively, that λ > 1 is not sufficient to
guarantee particle compression as one might first guess.
√
Theorem 6.1. For all 0 < β < 1, there exists λ∗ = λ∗ (β) < 2, n∗ ≥ 0, and γ < 1 such that for
all λ < λ∗ and n > n∗ , the particles achieve β-expansion with all but exponentially small probability:
for a configuration σ drawn at random according to stationary distribution π,
√

P (p(σ) < β · pmax ) ≤ γ
√
Proof. Again, for simplicity let ν := 2 + 2. Let
∗

λ<λ =ν

logν 2
β−
2
β−1

<ν

n

logν 2
0−
2
0−1

.

=

√
2.

Let Sβ be the set of configurations σ with p(σ) ≤ β · pmax . We show
π(Sβ ) =

√
w(Sβ )
≤ γ n.
Z

Applying Lemma 4.4 and then Lemma 4.3, we see
Pbβ·pmax c


bβ·pmax c
k −k
X
w(Sβ )
w(Sβ )
λ pmax
k=pmin f (k)ν λ
k −k
 √ pmax
√
,
≤  √ pmax ≤
=
f (k)ν λ
2
2
Z
2
λ

k=pmin

λ

where f (k) is the subexponential function from Lemma 4.3. Recalling that k < β · pmax , we see
w(Sβ )
≤
Z

bβ·pmax c

X

k −k

f (k)ν λ

k=pmin



λ
√
2

bβ·pmax c

k

β

X

=

f (k)ν



log λ
log 2
k 1−logν λ+ βν − 2βν

.

k=pmin

Let c3 be such that

logν λ logν 2
−
.
β
2β
We want −c3 < 0, so we solve for λ and see −c3 < 0 precisely when λ < λ∗ , a condition we know
to hold. Recalling Lemma 2.1, it follows that
−c3 = 1 − logν λ +

w(Sβ )
≤
Z

bβ·pmax c

X
k=pmin

bβ·pmax c

f (k)ν −c3 k ≤

X

f (k)ν −c3

√

n

≤ f 0 (n)ν −c3

√

n

,

k=pmin

Pn

where f 0 (n) = k=√n f (k) is a subexponential function. Hence there exists n∗ ≥ 0 and γ < 1 such
that for all n ≥ n∗ ,
P (p(σ) ≤ β · pmax ) = π(Sβ ) =

√
√
w(Sβ )
≤ f 0 (n)ν −c3 · n < γ n .
Z

30

√
Corollary 6.2. For all 0 < λ < 2, there exists a constant 0 < β < 1 such that with all but
exponentially small probability a sample drawn according to stationary distribution π of M is βexpanded.
√
Proof. This follows immediately from the previous theorem; for λ < 2, there exists β such that
λ < λ∗ (β).
When we know λ ≥ 1, the improved bounds in Lemma 4.7 can be used to show β-expansion
occurs for some β for an even greater range of values for λ. We note larger values of λ necessitate
smaller, but still constant, values of β.
Theorem 6.3. For all 1 ≤ λ < (2 · N50 )1/100 ∼ 2.17, there exists a constant 0 < β < 1 such that
with all but exponentially small probability a sample drawn according to stationary distribution π
of M is β-expanded.
Proof. For simplicity we prove the above result for all λ < 2.17, though the result (and the proof)
√
actually hold for all λ < (2 · N50 )1/100 ∼ 2.172033329... . As
above,
let
ν
:=
2
+
2. Let

2.17
log
ν
λ
β < β∗ =
.
1 − logν λ
Let Sβ be the set of configurations σ with p(σ) ≤ β · pmax . We show there is a constant γ < 1 such
that
π(Sβ ) =

√
w(Sβ )
≤ γ n.
Z

Applying Lemma 4.7 and then Lemma 4.3 and noting 0.13−1 < 8, we see
Pbβ·pmax c


bβ·pmax c
k −k
X
w(Sβ )
w(Sβ )
λ pmax
k=pmin f (k)ν λ
k −k
pmax < 8 ·

≤
8f (k)ν λ
,
=
2.17 pmax
Z
2.17
0.13 · 2.17
λ
λ
k=p
min

where f (k) is the subexponential function from Lemma 4.3. Recalling that k < β · pmax , we see
w(Sβ )
≤
Z

bβ·pmax c

X

k −k

8f (k)ν λ

k=pmin



λ
2.17

bβ·pmax c

k

β

X

=

8f (k)ν



log (λ/2.17)
k 1−logν λ+ ν β

.

k=pmin

Let c3 be such that
−c3 = 1 − logν λ +

logν

λ
2.17



.
β
We want −c3 < 0, so we solve for β and see −c3 < 0 precisely when β < β ∗ , a condition we know
to hold. Recalling Lemma 2.1,
w(Sβ )
≤
Z

bβ·pmax c

X
k=pmin

bβ·pmax c

8f (k)ν −c3 k ≤

X

8f (k)ν −c3

√

n

≤ f 0 (n)ν −c3

√

n

,

k=pmin

Pn

where f 0 (n) = k=√n 8f (k) is a subexponential function. Hence there exists n∗ ≥ 0 and γ < 1
such that for all n ≥ n∗ ,
P (p(σ) ≤ β · pmax ) = π(Sβ ) =

√
√
w(Sβ )
≤ f 0 (n)ν −c3 n < γ n .
Z

31

(a)

(c)

(b)

(d)

(e)

Figure 12: 100 particles in a line with occupied edges drawn, after (a) 1 million, (b) 2 million, (c)
3 million, (d) 4 million, and (e) 5 million iterations of M with bias λ = 4.
Corollary 6.4. For all 0 ≤ λ < 2.17, there exists a constant 0 < β < 1 such that with all
but exponentially small probability a sample drawn according to stationary distribution π of M is
β-expanded.
Proof. Corollary 6.2 and Theorem 6.3.

7

Simulations

In practice, Markov chain M yields good compression. We simulated M for λ = 4 on 100 particles
that began in a line; the configurations after 1, 2, 3, 4, and 5 million steps of M are shown in
Figure 12.
In contrast, λ = 2, while still favoring particles forming triangles, does not appear to yield
compression; see Figure 13, where even after 20 million simulated steps of M, the particles have
not compressed. We conjecture there is a phase transition in λ, i.e., a critical value λc such that
for all λ > λc the particles compress and for all λ < λc they do not compress. Such phase
transitions exist for similar
statistical physics models (e.g., [3]). Our proofs indicate if λc exists,
√
then 2.17 ≤ λc ≤ 2 + 2.

References
[1] E. Bampas, J. Czyzowicz, L. Ga̧sieniec, D. Ilcinkas, and A. Labourel. Almost optimal asynchronous rendezvous in infinite multidimensional grids. In Distributed Comp.: 24th Int. Symp.,
32

(a)

(b)

Figure 13: 100 particles in a line with occupied edges drawn, after (a) 10 million and (b) 20 million
iterations of M with bias λ = 2.
DISC 2010, pages 297–311, 2010.
[2] R. Bauerschmidt, H. Duminil-Copin, J. Goodman, and G. Slade. Lectures on self-avoiding
walks. Clay Mathematics Proceedings, 15, 2012.
[3] C. Borgs, J.T. Chayes, A. Frieze, J.H. Kim, P. Tetali, E. Vigoda, and V.H. Vu. Torpid mixing
of some MCMC algorithms in statistical physics. In 40th IEEE Symp. on Found. of Comp.
Sci., FOCS 1999, pages 218–229, 1999.
[4] S. Camazine, K.P. Visscher, J. Finley, and S.R. Vetter. House-hunting by honey bee swarms:
Collective decisions and individual behaviors. Insectes Sociaux, 46:348–360.
[5] A. Chavoya and Y. Duthen. Using a genetic algorithm to evolve cellular automata for 2D/3D
computational develop- ment. In Genetic and Evolut. Comp. Conf., GECCO 2006.
[6] Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler, and T. Strothmann. An algorithmic
framework for shape formation problems in self-organizing particle systems. In Proc. of the
2nd Ann. Int. Conf. on Nanoscale Computing and Comm., NANOCOM’ 15, pages 21:1–21:2,
2015.
[7] Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler, and T. Strothmann. Universal shape
formation for pro- grammable matter. In To appear, 28th ACM Symp. on Parallelism in Alg.
and Arch., SPAA ’16, 2016. To appear.
[8] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R.A. Bazzi, A.W. Richa, and C. Scheideler.
Leader election and shape formation with self-organizing programmable matter. In 21st DNA
Comp. and Molec. Prog., DNA 21, pages 117–132, 2015.
[9] A. Deutsch and S. Dormann. Cellular Automaton Modeling of Biological Pattern Formation:
Characterization, Applications, and Analysis. Modeling and Simulation in Science, Eng. and
Technology. Birkhäuser Boston, 2007.
[10] R.L. Dobrushin. The problem of uniqueness of a gibbsian random field and the problem of
phase transitions. Functional Analysis and Its Applications, 2:302–312, 1968.
33

[11] p
H. Duminil-Copin and S. Smirnov. The connective constant of the honeycomb lattice equals
√
2 + 2. Annals of Mathematics, 175:1653–1665, 2012.
[12] W. Feller. An Introduction to Probability Theory and Its Applications, volume 1. Wiley, 1968.
[13] P. Flocchini, G. Prencipe, N. Santoro, and P. Widmayer. Arbitrary pattern formation by
asynchronous, anonymous, oblivious robots. Theoretical Computer Science, 407:412–447, 2008.
[14] W. K. Hastings. Monte carlo sampling methods using markov chains and their applications.
Biometrika, 57:97–109, 1970.
[15] R. Jeanson, C. Rivault, J.L. Deneubourg, S. Blanco, R. Fournier, C. Jost, and G. Theraulaz.
Self-organized aggregation in cockroaches. Animal Behaviour, 69:169 – 180, 2005.
[16] Iwan Jensen. A parallel algorithm for the enumeration of benzenoid hydrocarbons. Journal of
Statistical Mechanics: Theory and Experiment, 2009(02):P02065, 2009.
[17] N. Lynch. Distributed Algorithms. Morgan Kauffman, 1996.
[18] N.J. Mlot, C.A. Tovey, and D.L. Hu. Fire ants self- assemble into waterproof rafts to survive
floods. Proc. of the National Academy of Sci., 108:7669–7673, 2011.
[19] C. Rivault and A. Cloarec. Cockroach aggregation: Discrimination between strain odours in
Blattella germanica. Animal Behaviour, 55:177–184, 1998.
[20] M. Rubenstein, A. Cornejo, and R. Nagpal. Programmable self-assembly in a thousand-robot
swarm. Science, 345:795–799, 2014.
[21] L.E. Thomas. Bounds on the mass gap for finite volume stochastic Ising models at low temperature. Comm. Math. Phys., 126:1–11, 1989.
[22] E. Winfree, F. Liu, L.A. Wenzler, and N.C. Seeman. Design and self-assembly of twodimensional DNA crystals. Nature, 394(6693):539–544, 1998.
[23] D. Woods, H.L. Chen, S. Goodfriend, N. Dabby, E. Winfree, and P. Yin. Active self-assembly
of algorithmic shapes and patterns in polylogarithmic time. In Proc. of the 4th Conf. on
Innovations in Theoretical Computer Science, pages 353–354, 2013.

34

A Data Tracking Scheme for General Networks
Rajmohan Rajaraman 1 Andrea W. Richa 2 Berthold Vocking 3 Gay athri Vuppuluri 4

ABSTRACT

1. INTRODUCTION

Consider an arbitrary distributed network in which large
numbers of objects are continuously being created, replicated, and destroyed. A basic problem arising in such an
environment is that of organizing a data tracking scheme
for locating object copies. In this paper, we present a new
tracking sc heme for locating nearby copies of replicated objects in arbitrary distributed environments.

Replication is a pow erful tool in the design of scalable highperformance distributed systems. F or example, the scalability problem that arises when a large number of clients simultaneously access a single object (the \hot spot" problem)
can be addressed by creating several copies of the object and
then distributing the load among these copies. As another
example, the latency associated with accessing an object at
some distant node of a large netw ork can often be reduced
by caching. Indeed, large-scale replication and cooperative
caching are central themes that underlie the emergence of
the paradigms of content deliv ery net works and peer-to-peer
netw orking.A basic problem arising in such replicated data
environments is that of organizing a data tracking scheme for
locating object copies.In this paper, w e present a new tracking sc heme for locating nearby copies of replicated objects
in arbitrary distributed environments. Our tracking scheme
describes the control structures that need to be stored at the
netw ork nodes, denes protocols for locating nearby copies
of data objects, and protocols for updating our control structures in case of insertions and deletions of copies. We also
consider the adaptability of our sc heme as nodes join and
leave the system.

Our tracking sc heme supports ecient accesses to data objects while keeping the local memory overhead low. In particular, our tracking sc heme achiev es an expected polylog(n)appro ximationin the cost of an y access operation, for an
arbitrary network. The memory overhead incurred by our
scheme is O(polylog(n)) times the maximum number of objects stored at any node, with high probability. We also show
that our tracking sc heme adapts well to dynamic changes in
the net w ork.

We represent the netw ork b y a collectionV of n nodes with
a single communication cost function that takes into accoun tthe combined eect of netw ork parameters such as
congestion, edge delays, edge capacities, distance and buer
space. The cost of c ommunication is dened by a function
c : V 2 ! R+ . F or any two nodes u and v in V , c(u; v) is
the cost of transmitting a unit size message from node u to
node v. We assume that c is a metric | that is, it is symmetric and satises the triangle inequality. In the remainder
of this paper, we will address the proximity of two nodes u
and v | e.g., u is close to v, the distanc e betw een u and
v, or u is near to v | always with respect to the cost of
communication c(u; v) between these nodes.

College of Computer Science, Northeastern University,
Boston, MA 02115, rraj@ccs.neu.edu. Supported by NSF
CAREER
aw ard NSF CCR{9983901.
2
Department of Computer Science and Engineering, Arizona
State University, Tempe, AZ 85287{5406, aricha@asu.edu.
Supported in part by NSF CAREER Award CCR{9985284
and NSF Grant CCR{9900304.
3
Max-Planck-Institut fur Informatik, Saarbrucken, Germany, voecking@mpi-sb.mpg.de. Supported in part by
the IST Programme of the EU under contract number
IST-1999-14186 (ALCOM-FT)
4
Compaq Corporation, Cupertino, CA 95014,
Gayathri.Vuppuluri@compaq.com.
This w ork w as done
while the author was a graduate student at ArizonaState
University, supported in part by NSF CAREER Award
CCR{9985284.
1

An important measure of the eciency of a data trac king
scheme is the stretch factor for a giv en operation, which
compares the cost of the operation incurred by the tracking
scheme to the optimal communication cost. We consider
three operations: access, whereby a node fetches a copy of a
particular object, insert, whereby a new copy of an object is
added to the system, and delete, whereby an existing copy of
an object is removed from the system.
The stretc h factor of
an access, insert or delete operation at a node u for an object
A is the ratio of the actual communication cost incurred

Permission to make digital or hard copies of part or all of this work or
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers, or to redistribute to lists, requires prior
specific permission and/or a fee.
SPAA ’01 Crete, Greece
© 2001 ACM ISBN 1-58113-409-6/01/07…$5.00

247

searching within a cluster that keeps the memory overhead
low and adapts quickly to changes in the network. For this
purpose, we embed a de Bruijn graph [18] of appropriate
size into each cluster. We use the embedding to guide access
requests within the cluster via hashed pointers to relevant
object copies. Since the diameter of a de Bruijn graph is logarithmic in the size of the graph, the search process within
any cluster is ecient.

by the data tracking system in performing the operation to
c(u; v), where v 6= u is the nearest node to u that holds
a copy of A; if there is no such v, then we set the stretch
factor to be the ratio of the cost incurred to diam(G), where
diam(G) = maxu;v c(u; v) is the diameter of the network.
Note that we have assumed that the optimal cost of an access
operation at node u is the communication cost to the nearest
node holding a copy of the desired object. In the event that
the node u fetches and modies the object, other copies may
have to be invalidated or updated. In this work, we do not
address the cost of maintaining consistency among copies
of the same object; this cost is a function of the particular
process of replication and is separate from our main concern.
Our work primarily applies to scenarios where consistency
either is not required (e.g., static web pages) or is enforced
by an independent mechanism.

Another important feature of our tracking scheme is that it
needs to store only a small number of signposts (pointers)
for every data object. Typically, these signposts are tuples
of object names (object IDs) and node addresses. We assume that both the object IDs and the node addresses have
a unique integer representation that can be stored in a constant number of words. Furthermore, since the in-degree
and out-degree of each vertex in a de Bruijn graph is 2, the
embedding in each cluster only requires a constant amount
of words to be stored at each node. We now summarize the
main properties of our data tracking scheme.

A challenge in the design of ecient tracking schemes is to
achieve a low stretch factor while maintaining small control
structures at the network nodes. Consider a naive approach
that optimizes the stretch factor for the read operation by
storing at each node the location of the closest copy of each
object in the network. The control memory required at each
node is prohibitive since it is proportional to the total number of objects in the system. Furthermore, when any object
copy is inserted or deleted, a large number of nodes (possibly, all) need to be informed. Clearly, the memory overhead
required at each node is also an important performance metric of a data tracking scheme. Let S denote the maximum
number of objects that may be stored on any individual
node. We relate the amount of memory required by our
data tracking scheme to S . Formally, we measure the memory overhead at each node by the fraction of local memory
that is used for control structures.

 The stretch factor for any read operation is O(log n).
3

 The expected stretch factor for any insert or delete
operation is O(log3 n). (The worst case cost for these
operations is O(diam(G) log2 n).)

 The local memory requirement is
O (S log(minfdiam(G); ng)(log n + log S )
+ log(minfdiam(G); ng) log n
words whp . Assuming S is polynomially bounded in
n, our upper bound on the local memory requirement
simplies to O(S log n + log n).
2

1

A nal performance metric that we consider is how the
scheme adapts to dynamic changes in the network under
the assumption that the cost metric does not change. We
adopt the following model for this study. We assume that
the set V of nodes and the cost function are xed; however,
individual nodes may join or leave the data management
system. When a node leaves the system, the functionality
provided by the node needs to be taken over by the rest of
the network. Similarly, in order to achieve scalability, when
new nodes join the system, a portion of the data structure
needs to be distributed among the new nodes. We evaluate
the adaptability of a tracking scheme by the number of nodes
that are updated when a node joins or leaves the system.

2

3

 The amortized adaptability of our tracking scheme is
O(log2 n).

The logarithmic factors in the stretch are partly inherited
from the properties of a network decomposition of Bartal [9],
as we will see in Sections 3 and 4. Utilizing improved clustering techniques may result in better stretch factors. For
example, for planar graphs one can divide the bounds for
the stretch by a log log n factor by employing a clustering
algorithm of [16]. Furthermore, increasing the bound on
the memory requirement by only an additive O(n ) term,
for constant  > 0, reduces the bounds on the stretch by
a factor of (log n= log log n). Finally, if one only wants a
guarantee for the expected stretch for access operations, then
one can drop another log n factor in all bounds for stretch
and memory requirement (a simplied such scheme, which
only provides guarantees on the expected cost of a read operation, is presented in Section 3).

1.1 Our contributions

Our main contribution in this work is the development of
the rst data tracking scheme for arbitrary networks, that
simultaneously achieves polylogarithmic approximations in
stretch factors for access, insert and delete operations, as
well as for local memory overhead per node. Our data tracking scheme is based on a randomized hierarchical decomposition technique of Bartal [8], that partitions the network into
disjoint clusters at various degrees of locality. The protocol
for accessing an object in our tracking scheme is to search
for the object level by level, from the smallest clusters to
the largest, until an object copy is found (if it exists).

We use the abbreviation \whp" throughout the paper to
mean \with high probability" or, more precisely, \with probability 1 ; n;c , where n is the number of nodes in the network and c is a constant that can be set arbitrarily large by
appropriately adjusting other constants dened within the
relevant context."
1

A challenge is then to provide an ecient mechanism for

248

an edge in T (H ) connecting a parent cluster C with a child
cluster C 0 is dened by diam(C ), where diam(C ) denotes
the (weak) diameter of C .

1.2 Related Work

The clustering and decomposition techniques of Bartal [8,
9, 16] build on the seminal work of Awerbuch and Peleg [6]
(see also [4]), who provide the rst low-diameter hierarchical decomposition for arbitrary networks. These clustering techniques have found several applications in distributed
networks such as maintaining routing tables [5], distributed
data management [2, 10, 3], tracking of mobile users [7],
network design [23], and locating Internet servers [14].

We will use randomly generated cluster constructions: The
cluster hierarchy H is chosen at random from a distribution
H = H(G) of hierarchical clusterings on G. The performance of our data tracking scheme depends on the quality
of the hierarchical clusterings obtained by the randomized
construction scheme. The most important quality measure
is the stretch factor of H , denoted by s(H), which is equal to
the maximum stretch factor su;v (H) over all pairs of nodes
u; v 2 V , where

distT (H ) (u; v) 
su;v (H) = E dist (u; v)
(1)
G
with distG (u; v) and distT (H ) (u; v) denoting the cost of communication between nodes u and v with respect to G (given
by the cost function c) and as dened by the clustering decomposition tree T (H ), respectively (Observe that the random variable distT (H )(u;v) is dened with respect to the random choice of H from H.). Another quality measure is the
depth d(H) of the clustering scheme, which we dene to be
the maximum height of the clustering tree T (H ) over all H
in H.

Closely related to our work is the study of distributed paging [3, 24], which addresses a more general online adversarial version of the problem that we consider in this paper. The distributed algorithm of [3] achieves polylog(n)competitiveness in terms of access cost. Their study does
not address the overhead due to control information, however, and direct extension of their results to our problem
may require local memory proportional to the number of
objects.
The randomized tracking scheme of Plaxton, Rajaraman,
and Richa [21] addresses many of the same concerns that
we consider in this paper. Their protocol, which has also
been implemented as part of a large-scale persistent object repository named Oceanstore [17], achieves a constantfactor approximation in expected access cost for a restricted
class of communication cost functions motivated by hierarchical networks. Ours, in contrast, achieves a polylog(n)approximation for arbitrary networks.

In Sections 3 and 4, we will prove the following theorem,
which relates the performance of our data tracking scheme
to the quality of the randomized clustering scheme. (Recall that n denotes the number of nodes and S denotes the
maximum number of objects that may be held by a single
node.)

The idea of partitioning the network into clusters and allocating pointers (and copies) inside the clusters using hash
functions was introduced by Maggs et al. in [19]. The data
management schemes presented in [19], however, assume
unbounded memory capacities and are restricted to structured networks like meshes or hierarchical networks. In [20],
these schemes are generalized to broader classes of networks
and, furthermore, local memory constraints are incorporated. These memory constraints, however, only concern the
stored copies and not the pointers to these copies, which in
turn is our main concern in this paper. Other work on data
management for restricted network models can be found,
e.g., in [1] and [15].
2.

Theorem 1. Given a graph G with clustering scheme H =
H(G), there exists a randomized data tracking algorithm
with (deterministic) stretch factor O(s(H) logk n log n) for
access requests, expected stretch factor O(s(H) logk n log n)

for insert and delete requests and memory overhead of
O (S d(H)(log n + log S ) + d(H)(k + log n) log n) ;
words at each node whp, for every 2  k  n.

TECHNICAL OVERVIEW

It remains to describe how the results presented in Section 1
can be derived from Theorem 1. In [8], Bartal presents
a probabilistic approximation of metric spaces by so-called
hierarchical well separated trees (HSTs). In fact, the construction of these trees is based on a hierarchical partitioning
scheme with small stretch and depth. Meanwhile the original results of Bartal have been improved. The currently
best known bounds are as follows.

Our scheme is based on a hierarchical clustering H = H (G)
of a network G = (V; E ). We break the network into disjoint clusters (i.e., subsets of nodes) with smaller diameter2 .
These clusters are partitioned recursively until we reach single nodes. In particular, we demand that the diameter of
a child cluster is at most half the diameter of the parent
cluster.
The clustering denes a decomposition tree T (H ) whose
nodes represent the clusters of H . In particular, the root
of the tree represents the cluster V , and the children of a
cluster C represent the clusters into which C is partitioned
according to H . Also, every leaf of the decomposition tree
corresponds to a cluster containing a single node, and therefore represents a distinct node of the graph G. The length of
2
We consider weak diameters. The (weak) diameter of a
cluster C is the maximum cost of communication in G between any pair of nodes in C .

 For general graphs, s(H) equals log n log log n and d(H)
equals minfdiam(G); log ng [9].
 For planar graphs, s(H) equals log n and d(H) equals
minfdiam(G); log ng [16].
Combining these bounds with Theorem 1 (for k = log n)
yields the results stated in Section 1.1.

249

The above path is called the search path of u for A. We
denote its length by `(u; A). Clearly, if u issues an access
request to A, then one only has to perform a search to a
copy and return the absolute address of node v. In case of
an insert of a copy at u, in each cluster C one follows the
search path of u for A and, at every cluster C on this path,
one follows a path to L(C ), adding a signpost for A at L(C ).
Similarly, in case of a delete of a copy on u, one follows the
search path of u for A and, for every cluster C on this path,
one follows a path to L(C ), removing the signpost for A at
L(C ). Thus, the cost for access, insert and delete operations
are bounded above by O(`(u; A)).

In Section 3, we will show how to obtain an upper bound
on the expected stretch factor for all operations and on the
memory overhead. Then, in Section 4, we will build on the
results proved in Section 3, showing how to achieve a deterministic stretch for the access operation if we use log n
copies of the data structure dened in Section 3. Finally, in
Section 5 we will investigate the adaptability of our scheme.
We conclude with some directions for future work in Section 6.
3.

MINIMIZING EXPECTED STRETCH

In this section, we will show a slightly weaker version of
Theorem 1. The following lemma bounds only the expected
stretch for access operations.

Next we give an upper bound on `(u; A). For notational
convenience, we let distG (u; A) (resp., distT (H ) (u; A)) denote
the distance in G (resp., T (H )) between u and the closest
copy of A on another node.

Main Lemma 2. For every graph G with randomized clustering scheme H = H(G) there exists a data tracking algorithm with expected stretch O(s(H) logk n) and local memory
requirement

 
S  + log n + k) ;
O d(H) S 1 + log
log n
words whp, for every 2  k  n.

Lemma 4.

E [`(u; A)]  2s(H)  distG (u; A).

Proof. Let vG and vT (H ) denote the closest node with
respect to G and T (H ), resp., holding a copy of A. Observe
that possibly, v 6= vT (H ) 6= vG . Invariant 3, however, yields
that we always nd a copy in the smallest cluster containing
u and a copy of A. Since the length of edges in T (H ) along a
path from a root to a leaf decrease geometrically by a factor
of at least two, we obtain
`(u; A) = distT (H ) (u; v)
 2distT (H ) (u; vT (H ) ):
We now calculate the expected value of distT (H ) (u; vT (H ) ).
E [distT (H ) (u; vT (H ) )]  s(H)  distG (u; vT (H ) )
 s(H)  distG (u; vG )
= s(H)  distG (u; A);
which yields the lemma.

In Section 3.1, we will rst present a simple data tracking
scheme that achieves small expected stretch but stores all
information regarding the state of all copies in a cluster on a
single node, which we refer to as a cluster leader. In Sections
3.2 and 3.3, we will show how to reduce the local memory
requirement by embedding de Bruijn graphs into the clusters.
3.1 The cluster leaders

Let us assume that each cluster has a special node that holds
all relevant information about copies in the cluster. For a
cluster C , this node is the cluster leader L(C ). We maintain
the following invariant.

A naive implementation of Invariant 3 requires that a cluster
leader of a cluster with  children has to store up to 
signposts per cluster. We conclude this section by showing
that one can redistribute the signposts in such a way that
the memory requirement per cluster leader is independent
from .

Invariant 3. Suppose C 0 is the child cluster of a cluster

C . For every data
object A, L(C ) holds a signpost for A
pointing to L(C 0 ) i C 0 holds a copy of A.
Using this invariant, one can always nd a nearby copy by
following the shortest path in the decomposition tree T (H ).
Recall that a node u 2 V corresponds to a leave in T (H ). If
u searches for an object A then it simply can send a message
upward in the decomposition tree until it reaches a signpost
for A, that is,

Lemma 5. Using cluster leaders, every access, insert, or
delete operation of a node u with respect to an object A can
be performed at cost O(`(u; A)) storing only O(1) signposts
for A on the leader of those clusters that contain a copy of
A.

 the message is forwarded along the chain of cluster

leaders representing the clusters on the path upward
in the decomposition tree,
 the forwarding along the upward chain is stopped as
soon as the message reaches a cluster leader L(C ) of a
cluster C holding a signpost for A, and then
 the message follows the chain of signposts downwards
until it reaches a copy of A on a node v.

Proof. Let C1 ; : : : ; C denote the child clusters of C =
C0 that contain a copy of A. We connect these clusters in
a doubly linked list so that L(Ci ) holds pointers to L(Ci+1 )
and L(Ci;1 ), for 0  i  . In this way, one can eciently
search for a copy in C by following the pointer to C1 . If a
rst copy of A is inserted in a dierent child cluster, then
we add the leader of this cluster to the head of the linked
list. If the last copy of A is deleted from a child cluster,

250

Now, for each cluster C , we number the nodes in C from 0
to jC j ; 1. The signposts of an object A are stored in the
node with label homeC (A) = key (A) mod jC j.

then this cluster is removed from the linked list. If this
leaves an empty linked list, then C has no copies of A, and
L(C ) removes itself from the linked list at the next level
of the hierarchy, if any. Thus, insertions and deletions of
copies are implemented using standard operations for doubly
lists requiring only to change two pointers in the list. Each
change of a pointer costs O(diam(C )). Thus, the asymptotic
cost for insertions and deletions of copies does not change.

Embedding de Bruijn graphs.

In order to avoid large tables that translate the virtual node
labels within the clusters into physical addresses, we embed
a dlog jC je-dimensional de Bruijn graph into each cluster
C . For convenience, we let d = dlog jC je in the following
discussion. The d-dimensional de Bruijn graph consists of
2d vertices whose labels are d-ary binary strings that can be
identied with integers from 0 : : : d ; 1. The nodes in the
cluster C are assigned labels from [jC j]. Let C (u) denote
the label of u in C . Any de Bruijn vertex with integer label
` 2 [jC j] is hosted by the cluster node u with label C (u) = `.
Any de Bruijn vertex with integer label `  jC j is hosted by
the cluster node u whose label C (u) is identical to ` in
binary representation, except that the most signicant bit
in C (u) is 0 instead of 1. Observe that each cluster node
hosts either one or two de Bruijn vertices.

3.2 Distributing the cluster leadership

An obvious drawback of the cluster leader concept is that
the leader node needs to store signposts to all copies in a
cluster (with the leader of the top-level cluster storing signposts to all copies in the whole network). To overcome this
problem one might dene dierent leader nodes for dierent
objects using a hash function that distributes the signposts
evenly among the nodes in a cluster. A naive implementation of this concept, however, assumes that every node is
known to every other node in the cluster. For example, a
typical implementation of a hash function requires that the
nodes in a cluster are numbered in a consecutive fashion.
However, even if the nodes in the network are numbered
from 0 to n ; 1, the locality conditions of the clusters can
produce arbitrary subsets of these numbers within a given
cluster. Therefore, we number the nodes in dierent clusters independently. We can then compute a hash function
that maps signposts to nodes. However, we do not want to
store gigantic tables translating the labels for all nodes in
all clusters into physical addresses. Instead we locate the
pseudo-randomly distributed signposts by following shortest paths in an embedded de Bruijn graph. In this way, we
ensure that every node has to store only its own label and
the labels of a few other nodes in each of the d(H) clusters in
which it is contained. We now describe the hashing scheme
and the embedding in more detail.

In the de Bruijn graph, there is a directed edge from each
vertex with label u1 u2 : : : ud (in binary representation) to
the vertices with labels u2 : : : ud 0 and u2 : : : ud 1. The diameter of this directed graph is log C and there is a unique
shortest path between every pair of nodes that can be computed easily. (For more details about de Bruijn graphs see
[18].) If a node v in cluster C needs to send a message to
node homeC (A), for some object A, then this message follows the edges on the shortest path from v to homeC (A) in
the de Bruijn network. In this way, each node on this path
only needs to know the physical address of the next node on
the path. This information can be obtained easily if every
node stores the physical addresses of the nodes incident on
its outgoing edges. We refer to this table as the neighbor
table. Since the out-degree of each vertex in the de Bruijn
graph is 2 and at most 2 vertices are emulated by any cluster
node, the neighbor table at each node is constant size.
The price for routing messages along shortest paths in the
de Bruijn network is that the search within a cluster has
cost O(diam(C ) log jC j) rather than O(diam(C )) because
the path visits up to dlog jC je;1 intermediate nodes. Clearly,
one can save cost by storing a larger neighborhood in the
neighbor table. This yields the following tradeo. If a node
memorizes a neighborhood of size k  2 for each of its at
most two de Bruijn vertices of cluster C , then a search
in cluster C has cost O(diam(C ) logk jC j). Consequently,
adapting the bound on the cost of access, insert, and delete
operations in Lemma 5 to the embedding yields the following
result.

Hash function.

We assign keys to the objects. For an object A 2 A, let
key (A) denote the key of object A. Keys are not unique, we
choose them from the set [P ] using a hash function3 , where
P  jAj is a prime number. The hash function is chosen
as follows. Following Carter and Wegmann [12], we draw a
polynomial f from a class of integer polynomials F of degree
q = O(log(S n)). (Observe that the representation of f requires only q words.) We dene key (A) = f (int(A)), where
int(A) is a unique integer representation of A in ZP . This
polynomial hashing scheme guarantees q-wise independence.
In particular, we can conclude the following lemma.
Lemma 6

(Carter and Wegmann [12]). Let M

Lemma 7. For every

P

and m 2 [M ] be two integers. For every collection of q
distinct objects A1 ; : : : ; Aq ,
 q
Pr [(key (A ) mod M ) = m for 1  i  q]  2 :
i

k  2,

 every access, insert, or delete operation of a node u
with respect to an object A can be performed at cost
O(`(u; A) logk n)

M

 for every cluster C , each node v 2 C needs to memorize O(k) words to store the de Bruijn neighborhood,
and

For any nonnegative integer x, [x] denotes the set
f0; 1; : : : ; x ; 1g.
3

251

 for every cluster C , each node v 2 C needs to hold

yields

O(1) signposts for every object A if homeC (A) = v
and cluster C contains copies of A.

3.3 Analysis of memory requirement

It remains to count all labels, addresses and signposts over
all clusters that need to be stored in the local memory modules of the nodes.
Lemma 8. Let S denote the maximum number of objects
that can be stored in the main memory of any node. Let k
denote the number of memorized de Bruijn neighbors. Then
the local memory requirement is

 
S  + log n + k) ;
O d(H) S 1 + log
log n

words whp.



Combining the bounds in Lemma 4, Lemma 7, and Lemma 8
yields Main Lemma 2 stated at the beginning of this section.

Proof. A node is contained in d(H) clusters. We show
for every cluster C that each node v 2 C holds at most
O((S + log n)(log n);1 log(nS )) signposts whp. Lemma 7
shows that the additional number of words that need to be
stored for the de Bruijn neighborhood is O(k) per cluster.
Furthermore, we will need an q = O(log(nS ))-wise independent hash function in order to show the upper bound on
the number of signposts. For the representation of this hash
function we require O(log(nS )) words of memory in every
node. Putting altogether, we obtain an upper bound on the
local memory requirement of


!

q
Pr[ri (v)  q]  2 logqnjC j jC2 j

q
4
e
log
n

q
;
1 ;c
 S n ;
provided q = c1 log(nS ) with c1 > 0 denoting a suitable
constant. Now let r(v)Sdenote
the number of signposts on v
for all objects in R = i=1 R(i). Then
Pr[r(v)  q]  S ;1n;c  n;c :
Finally observe that




log n log(nS ) :
q = logS n c1 log(nS ) = O S +
log n
This completes the proof of Lemma 8.





4. DETERMINISTIC STRETCH

The probability for the bound on the expected stretch in the
Main Lemma in Section 3 is with respect to the randomized
construction of the hierarchical clustering of Bartal [8, 9].
This means that there are possibly some allocations of copies
to nodes in which accesses issued by particular nodes are
always very expensive. This may be acceptable for insert or
delete requests for which one typically aims to minimize the
overall work load, but it is not acceptable for access requests
which typically occur much more frequently, and for which
one aims to minimize the latency for any particular request.
The following lemma addresses this problem.



nS )
O d(H) (S + log n) log(
log n + k + log(nS ) ;

Lemma 9. Using O(log n) copies of the data tracking scheme
presented in Section 3, one can ensure a deterministic stretch
factor of O(s(H) logk n log n) for an access operation, and an
expected stretch factor of O(s(H) logk n log n) for the insert
and delete operations.

which after simplication corresponds to the bound in the
lemma.
We now place an upper bound on the number of signposts
stored at a node. Fix a cluster C . We show that each
node v in C holds at most O((S + log n)(log n);1 log(nS ))
signposts, with probability 1 ; n;c , for any constant c > 0.
Recall that only the node homeC (A) = key (A) mod jC j
may hold a signpost directed to a copy of A where key (A)
is dened by a q-wise independent hash function.

Proof. Consider a randomized clustering scheme H generating a hierarchical clustering H (G) with stretch s(H).
Since E[distT (H ) (u; v)]  s(H), Markov's inequality yields
Pr[distT (H ) (u; v)  2s(H)]  12 ;
for every pair of nodes u; v 2 V . Now suppose we use H for
generating r = 2 log n independent hierarchical clusterings
H1 ; : : : ; Hr . Then
Pr[9u; v 2 V; 8i 2 f1; : : : ; rg : distT (Hi ) (u; v)  2s(H)]

Let R denote the set of objects with at least one copy in cluster C . We partition R into  = dS = log ne groups R1 ; : : : ; R
each of which having size at most
2jRj logS n  2 log njC j:




(The last inequality holds because jRj  SjC j as each node
in C can store at most S objects.) For a node v 2 C , let
ri (v) (1  i  ) denote the number of signposts for objects
from Ri that are stored on v.

;

n(n 1)
2

; 1 r
2

(2)
Initially, our data tracking scheme repeatedly generates rtuples of hierarchical partitions terminating with the rst
tuple H  = (H1 ; : : : ; Hr) satisfying
max min distHi (u; v)  2s(H):
u;v 2V 1ir

Our hash function aims to distribute the signposts evenly
among the nodes in the cluster. In fact, applying Lemma 6

252

1
2

of its labels and merges the signpost lists associated with
the two labels. Thus, all of the jC j nodes are updated. We
nally consider the case in which C (u) is less than jC j ; 1.
In this case, we set the label of the node with current label
jC j ; 1 to C (u), and then repeat the updates associated
with the removal of the node with label jC j ; 1. Again, the
number of nodes updated is O(1). Since the adaptability
is O(1) whenever jC j ; 1 is not a power of 2, and O(jC j)
otherwise, a simple amortization argument shows that the
amortized adaptability in any sequence of node departures
is O(1).

Equation 2 shows that this process terminates after generating only O(log n) tuples whp.
We implement r versions of our data tracking scheme, the
ith version is based on partitioning Hi. Insert and delete
operations are always executed in all versions. The expected
cost for these operations increases by at most a factor of
2r = O(log n) since equation 2 implies
X
E[distHi (u; v)]  2rs(H)
1

ir

because the randomized process generating the partitions
chooses eectively an s-tuple from a probability distribution (H )s in which each s-tuple has at most twice the
probability as in Hs . Thus, the expected stretch for insert and delete operations increases from O(s(H) logk n) to
O(s(H) logk n log n).

When a node u joins a cluster C , then it is assigned a new
label jC j. There are two cases. If jC j + 1 is not a power
of 2, then the node u0 that was previously emulating the
label jC j and the nodes that have de Bruijn edges to u0 need
to be updated. Thus, the number of updates is O(1). If
jC j + 1 is a power of 2, then prior to the addition of u,
each node had exactly 1 label. But after the addition of u,
each node other than u needs to emulate two labels. The
dimension of the embedded de Bruijn graph also increases by
1. Consequently, the number of nodes updated is jC j. Since
this cost can be amortized against earlier node additions,
we obtain in a sequence of node additions, the amortized
adaptability within a cluster is O(1).

In order to perform a search procedure for an object A initiated at a node u, we alternate in round robin fashion among
the r versions of the data tracking scheme. In the rst round,
for every version, we search for copies of A in the smallest
clusters containing u. If we do not nd a copy of A in
one of these clusters then, in the next iteration, we inspect
clusters of the next higher level of all the clustering hierarchies. (A more practical implementation may inspect all
clusters of the same level of all versions in parallel.) This
way, an access operation always locates a copy of A at distance 2distG (u; A)s(H). Thus, the cost of performing an
access operation is 2rdistG (u; A)s(H) logk n and, hence, the
stretch factor for an access operation is O(s(H) logk n log n).
This completes the proof of Lemma 9.

The above adaptation mechanism does not achieve amortized O(1) adaptability within a cluster for a sequence that
contains nodes both joining and leaving the system. We
achieve this bound by allowing the size of the de Bruijn
graph to be up to four times the size of the cluster. This
increases the number of labels emulated by a node by at
most a factor of 2. When a node leaves, the relabeling of
the nodes is done only when the ratio of the number of labels to the size of the cluster is less than 4. Using standard
amortized analysis [11], we show that the amortized number of nodes updated within a cluster for any sequence of
network changes that aect the cluster is O(1). If we have
O(log n) copies of the tracking
scheme of Section 3, then each
node belongs to O(log2 n) clusters. Therefore, the amortized
adaptability is O(log2 n).

The modied data tracking scheme in the proof of Lemma 9
incurs an extra O(log n) factor in the memory overhead at
each node: This fact combined with Lemma 9 concludes the
proof of Theorem 1.
5.

ADAPTABILITY

An important aspect of our tracking scheme is its adaptability to changes in network conditions. We consider a scenario
in which the nodes of the network may leave or join the
system over time. We dene the adaptability of a tracking
scheme to be the number of nodes that have to be updated
when a node joins or leaves. We show that the amortized
adaptability of our tracking scheme is O(log2 n).

6. FUTURE WORK

It would be interesting to devise low-stretch clustering hierarchies that adapt well to a highly dynamic network environment. Such a hierarchy, when combined with our data
tracking scheme, would provide a tracking scheme that could
be used, for example, in mobile network scenarios or in networks where the data trac pattern tends to change often,
thus also potentially changing the cost of communication between nodes. Closely related is the issue of fault-tolerance
which we have not explicitly considered in this paper.

We rst address the case when a node u leaves the system.
Consider a cluster C to which u belongs. If C (u) is jC j ;
1, then there are two cases. If jC j ; 1 is not a power of
2, then the label C (u) is emulated by the node u0 that
has a label identical to that of u except in bit position 0.
The neighbor table at u0 (listing the neighbors according
to the de Bruijn embedding) is updated to account for the
new neighbors associated with the label. The neighbor table
at the nodes that have de Bruijn edges to u update their
neighbor table as well. Furthermore, the list of signposts at
u0 is also updated to include the signposts at u. Thus, the
total number of nodes that are updated is O(1). If jC j; 1 is
a power of 2, then each node other than u has two labels. On
the removal of u, the dimension of the embedded de Bruijn
graph decreases by 1. Each node now maintains exactly one

Another direction for future research is the design of data
tracking schemes for objects that appear in dierent representation formats (e.g., image and video les is a distributed
multimedia network). A node may recognize only a few representation formats among the many formats available for
an object. Some nodes in the network may be able to perform a format conversion, at some specied cost. Therefore
the cost of an access operation now depends not only on the
communication costs between pair of nodes, but also on the
conversion costs at the nodes.

253

7.

[15] D. Karger, E. Lehman, F. T. Leighton, M. Levine,
D. Lewin, and R. Panigrahy. Consistent hashing and
random trees: Distributed caching protocols for
relieving hot spots on the World Wide Web. In
Proceedings of the 29th Annual ACM Symposium on
Theory of Computing, pages 654{663, May 1997.
[16] G. Konjevod, R. Ravi, and F. S. Salman. On
approximating planar metrics by tree metrics.
Submitted for publication in J. Algorithms, July 1997.
[17] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski,
P. Eaton, D. Geels, R. Gummadi, S. Rhea,
H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao.
Oceanstore: An architecture for global-scale persistent
storage. In Proceedings of the Ninth International
Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS), 2000.
[18] F. T. Leighton. Introduction to Parallel Algorithms
and Architectures: Arrays  Trees  Hypercubes.
Morgan Kaufmann, San Mateo, CA, 1992.
[19] B. M. Maggs, F. Meyer auf der Heide, B. Vocking,
and M. Westermann. Exploiting locality for data
management in systems of limited bandwidth. In
Proceedings of the Thirty-Eighth Annual Symposium
on Foundations of Computer Science, pages 284{293,
October 1997.
[20] F. Meyer auf der Heide, B. Vocking, and
M. Westermann. Caching in networks. In Proceedings
of the Eleventh Symposium on Discrete Algorithms,
pages 430{439, 2000.
[21] C. G. Plaxton, R. Rajaraman, and A. W. Richa.
Accessing nearby copies of replicated objects in a
distributed environment. Theory of Computing
Systems, 32:241{180, 1999. A preliminary version of
this paper appeared in Proceedings of the 9th Annual
ACM Symposium on Parallel Algorithms and
Architectures (SPAA), pages 311{320, June 1997.
[22] M. van Steen, F. J. Hauck, and A. S. Tanenbaum. A
model for worldwide tracking of distributed objects. In
Proceedings of the 1996 Conference on
Telecommunications Information Networking
Architecture (TINA 96), pages 203{212, September
1996.
[23] B. Wu, G. Lancia, V. Bafna, K. Chao, R. Ravi, and
C. Tang", A polynomial time approximation scheme
for Minimum Routing Cost Spanning Trees In
Proceedings of the Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 21{32,
January 1998.
[24] O. Wolfson, S. Jajodia, and Y. Huang An adaptive
data replication algorithm ACM Transactions on
Database Systems, 22:255{314, 1997.

REFERENCES

[1] T. E. Anderson, M. D. Dahlin, J. N. Neefe, D. A.
Patterson, D. S. Rosselli, and R. Y. Wang. Serverless
network le systems. In Proceedings of the 15th
Symposium on Operating Systems Principles, pages
109{126, 1995.
[2] B. Awerbuch, Y. Bartal, and A. Fiat. Competitive
distributed le allocation. In Proceedings of the 25th
Annual ACM Symposium on Theory of Computing,
pages 164{173, May 1993.
[3] B. Awerbuch, Y. Bartal, and A. Fiat. Distributed
paging for general networks. In Proceedings of the
Seventh Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 574{583, January 1996.
[4] B. Awerbuch, B. Berger, L. Cowen, and D. Peleg.
Near-linear cost sequential and distributed
constructions. In Proceedings of the Thirty-Fourth
Annual Symposium on Foundations of Computer
Science, pages 638{647, 1993.
[5] B. Awerbuch and D. Peleg. Routing with polynomial
communication space tradeo. SIAM Journal on
Discrete Mathematics, 5:151{162, 1990.
[6] B. Awerbuch and D. Peleg. Sparse partitions. In
Proceedings of the Thirty-First Annual IEEE
Symposium on Foundations of Computer Science,
pages 503{513, October 1990.
[7] B. Awerbuch and D. Peleg. Online tracking of mobile
users. Journal of the ACM, 42(5):1021{1058,
September 1995.
[8] Y. Bartal. Probabilistic approximation of metric
spaces and its algorithmic applications. In Proceedings
of the Thirty-Seventh Annual IEEE Symposium on
Foundations of Computer Science, pages 184{193,
October 1996.
[9] Y. Bartal. On approximating arbitrary metrics by tree
metrics. In Proceedings of the Thirtieth Annual ACM
Symposium on Theory of Computing, pages 161{168,
1998.
[10] Y. Bartal, A. Fiat, and Y. Rabani. Competitive
algorithms for distributed data management. Journal
of Computer and System Sciences, 51:341{358, 1995.
[11] T. Cormen, C. Leiserson, and R. Rivest. Introduction
to Algorithms, chapter 18. MIT Press/McGraw-Hill,
Cambridge, MA, 1990.
[12] J. L. Carter and M. N. Wegman. Universal Classes of
Hash Functions. Journal of Computer and System
Sciences, 18:143{154, 1979.
[13] J. D. Guyton and M. F. Schwartz. Locating nearby
copies of replicated Internet servers. In Proceedings of
ACM SIGCOMM, pages 288{298, 1995.
[14] S. Jamin, C. Jin, Y. Jin, D. Raz, Y. Shavitt, and
L.Zhang, The placement of Internet instrumentation,
In Proceedings of IEEE INFOCOMM, 2000.

254

2011 31st International Conference on Distributed Computing Systems

Competitive and Fair Medium Access despite Reactive Jamming
Andrea Richa
Christian Scheideler
Stefan Schmid
Jin Zhang
Arizona State University University of Paderborn
T-Labs/TU Berlin
Arizona State University
Tempe, Arizona, USA
D-33102 Paderborn
D-10587 Berlin
Tempe, Arizona, USA
aricha@asu.edu
scheideler@upb.de
stefan@net.t-labs.tu-berlin.de
jzhang82@asu.edu

we argue that adversaries may behave in an adaptive
and reactive manner: adaptive in the sense that their
decisions on whether to jam at a certain moment in
time can depend on the protocol history; and reactive
in the sense that the adversary can perform physical
carrier sensing (which is part, e.g., of the 802.11
standard) to learn whether the channel is currently
idle or not, and jam the medium depending on these
measurements.
This paper presents the medium access (MAC)
protocol A NTI JAM that makes effective use of the
few and arbitrarily distributed nonjammed time periods, to achieve fairness and a provable throughput
despite the presence of such a strong reactive jammer. As we will see, the throughput is competitive,
i.e., a constant fraction of the nonjammed time period is used for successful transmissions. Besides this
interesting theoretical result, our protocol is simple
to implement and performs well also in the average
case. Also, worth to note is that our approach is at
the MAC level and may be used in conjunction with
some of the anti-jamming techniques developed at
the physical layer (e.g., frequency hopping, spread
spectrum).

Abstract—Intentional interference constitutes a major threat for communication networks operating over
a shared medium where availability is imperative. Jamming attacks are often simple and cheap to implement.
Today’s jammers can perform physical carrier sensing
in order to disrupt communication more efficiently,
especially in a network of simple wireless devices such
as sensor nodes, which usually operate over a single
frequency (or a limited frequency band) and which
cannot benefit from the use of spread spectrum or
other more advanced technologies. This paper proposes
the medium access (MAC) protocol A NTI JAM which is
provably robust against a powerful reactive adversary
who can jam a (1 − ε)-portion of the time steps,
where ε is an arbitrary constant. The adversary uses
carrier sensing to make informed decisions on when it
is most harmful to disrupt communications. Moreover,
we allow the adversary to be adaptive and to have
complete knowledge of the entire protocol history.
Our MAC protocol is able to make efficient use of
the nonjammed time periods and achieves a Θ(1)competitive throughput in this harsh scenario, if ε
is constant. In addition, A NTI JAM features a low
convergence time and has excellent fairness properties
in the sense that channel access probabilities among
nodes do not differ by more than a small constant
factor.

I. I NTRODUCTION

A. Related Work

Disruptions of the communication over a shared
medium—either because of interference of concurrent transmissions or intentionally—are a central
challenge in wireless computing. It is well-known
that already simple jamming attacks—without any
special hardware—constitute a threat for the widely
used IEEE 802.11 MAC protocol. Due to the problem’s relevance, there has been a significant effort
to cope with such disruption problems both from the
industry and the academia side, and much progress
has been made over the last years on how to deal
with different jammer types.
While simple oblivious jammers are wellunderstood today and many countermeasures exist,
this paper goes an important step further and studies
MAC protocols against smart jammers. In particular,
1063-6927/11 $26.00 © 2011 IEEE
DOI 10.1109/ICDCS.2011.8

Researchers have studied the problem of unintentional and malicious interference in wireless networks for several years now, e.g., [6], [14], [16],
[17], [18], [20], [21], [27], [29]. Classic defense
mechanisms operate on the physical layer [18], [20]
and there exist approaches both to avoid as well as
to detect jamming. Spread spectrum and frequency
hopping technologies have been shown to be very effective to avoid jamming with widely spread signals.
However, the ISM frequency band used by IEEE
802.11 variants is too narrow to effectively apply
spread spectrum techniques [5]. Also, as jamming
strategies can come in many different flavors, detecting jamming activities by simple methods based
on signal strength, carrier sensing, or packet delivery
507

ratios has turned out to be quite difficult [17].
Recent work has also studied MAC layer strategies against jamming, including coding strategies (e.g., [6]), channel surfing and spatial retreat
(e.g., [1], [30]), or mechanisms to hide messages
from a jammer, evade its search, and reduce the
impact of corrupted messages (e.g., [28]). However,
these methods do not help against an adaptive jammer with full information about the history of the
protocol, like the one considered in our work.
In the theory community, work on MAC protocols has mostly focused on efficiency. Many of
these protocols are random backoff protocols (e.g.,
[4], [7], [8], [12], [23]) that do not take jamming
activity into account and, in fact, are not robust
against it (see [2] for more details). But also some
theoretical work on jamming is known (e.g., [9] for
a short overview). There are two basic approaches
in the literature. The first assumes randomly corrupted messages (e.g. [22]), which is much easier to
handle than adaptive adversarial jamming [3]. The
second line of work either bounds the number of
messages that the adversary can transmit or disrupt
with a limited energy budget (e.g. [11], [15]), or
bounds the number of channels the adversary can
jam (e.g. [10], [19]). The protocols in, e.g., [15]
can tackle adversarial jamming at both the MAC
and network layers, where the adversary may not
only be jamming the channel but also introducing
malicious (fake) messages (possibly with address
spoofing). However, they depend on the fact that
the adversarial jamming budget is finite, so it is not
clear whether the protocols would work under heavy
continuous jamming. (The result in [11] seems to
imply that a jamming rate of 0.5 is the limit whereas
the handshaking mechanisms in [15] seem to require
an even lower jamming rate.)
Our work is motivated by the results in [3] and
[2]. In [3] it is shown that an adaptive jammer can
dramatically reduce the throughput of the standard
MAC protocol used in IEEE 802.11 with only limited energy cost on the adversary side. Awerbuch et
al. [2] initiated the study of throughput-competitive
MAC protocols under continuously running, adaptive
jammers, and presented a protocol that achieves a
high performance under adaptive jamming.
In this paper, we extend the model and result
from [2] in two crucial ways. (1) We allow the
jammer to be reactive, i.e., to listen to the current
channel state in order to make smarter jamming
decisions. Note that a reactive model is not only
meaningful in the context of jamming: for example,

in many MAC protocols based on carrier sensing,
nodes become active during idle time periods and
hence, a MAC protocol in the reactive model also
performs well in scenarios with co-existent networks.
(2) We design a fair protocol in the sense that channel access probabilities among nodes do not differ
by more than a small constant factor. The protocol
in [2] was inherently unfair, as confirmed by our
theoretical and simulation results. We believe that the
reactive jammer model is much more realistic and
hence that our study is of practical importance. For
example, by sensing the channel, the adversary may
avoid wasting energy by not jamming idle rounds.
Note however that depending on the protocol, it
may still make sense for the adversary to jam idle
rounds, e.g., to influence the protocol execution.
Indeed, due to the large number of possible strategies a jammer can pursue, the problem becomes
significantly more challenging than the nonreactive
version: Not only is the analysis more involved, but
also key modifications to the protocol in [2] were
needed. While we still build upon the algorithmic
ideas presented in [2], our A NTI JAM protocol seeks
to synchronize the nodes’ sending probabilities. This
has the desirable effect of achieving fairness, where
all nodes are basically granted the same channel
access probabilities, greatly improving the unfair
protocol of [2]. While our formal analysis confirms
our expectations that the overall throughput under
reactive jammers is lower than the throughput obtainable against nonreactive jammers, we are still
able to prove a constant-competitive performance
(for constant ε), which is also confirmed by our
simulation study. Finally, our first insights indicate
that A NTI JAM-like strategies can also be used in
multi-hop settings (see also the recent extension
of [2] to unit disk graphs [25]) and to devise robust
applications such as leader election protocols [26].
B. Model
We study a wireless network that consists of n
honest and reliable simple wireless devices (e.g.,
sensor nodes) that are within the transmission range
of each other and that communicate over a single
frequency (or a limited, narrow frequency band).
We assume a back-logged scenario where the nodes
continuously contend for sending a packet on the
wireless channel. A node may either transmit a
message or sense the channel at a time step, but it
cannot do both, and there is no immediate feedback
mechanism telling a node whether its transmission
was successful. A node sensing the channel may

508

either (i) sense an idle channel (in case no other
node is transmitting at that time), (ii) sense a busy
channel (in case two or more nodes transmit at the
time step), or (iii) receive a packet (in case exactly
one node transmits at the time step).
In addition to these nodes there is an adversary.
We allow the adversary to know the protocol and its
entire history and to use this knowledge in order
to jam the wireless channel at will at any time
(i.e, the adversary is adaptive). Whenever it jams
the channel, all nodes will notice a busy channel.
However, the nodes cannot distinguish between the
adversarial jamming or a collision of two or more
messages that are sent at the same time. We assume
that the adversary is only allowed to jam a (1 − ε)fraction of the time steps, for an arbitrary constant
0 < ε ≤ 1.
Moreover, we allow the jammer to be reactive: it is
allowed to make a jamming decision based on the actions of the nodes at the current step. In other words,
reactive jammers can determine (through physical
carrier sensing) whether the channel is currently idle
or busy (either because of a successful transmission,
a collision of transmissions, or too much background
noise) and can instantly make a jamming decision
based on that information. Those jammers arise in
scenarios where, for example, encryption is being
used for communication and where the jammer cannot distinguish between an encrypted package and
noise in the channel. Note that robustness in the
reactive model is relevant beyond jamming, e.g., in
situations with co-existent networks, as many MAC
protocols based on carrier sensing activate nodes
during idle time periods.
In addition, we allow the adversary to perform
bursty jamming. More formally, an adversary is
called (T, 1 − ε)-bounded for some T ∈ N and
0 < ε < 1 if for any time window of size w ≥ T
the adversary can jam at most (1 − ε)w of the time
steps in that window.
The network scenario described above arises, for
example, in sensor networks, which consist of simple
wireless nodes usually running on a single frequency
and which cannot benefit from more advanced antijamming techniques such as frequency hopping or
spread spectrum. In such scenarios, a jammer will
also most probably run on power-constrained devices
(e.g., solar-powered batteries), and hence will not
have enough power to continuously jam over time.
(The time window threshold T can be chosen large
enough to accommodate the respective jamming pattern.)

This paper studies competitive MAC protocols.
A MAC protocol is called c-competitive against
some (T, 1 − ε)-bounded adversary (with high probability or on expectation) if, for any sufficiently
large number of time steps, the nodes manage to
perform successful message transmissions in at least
a c-fraction of the time steps not jammed by the
adversary (with high probability or on expectation).
Our goal is to design a symmetric local-control
MAC protocol (i.e., there is no central authority
controlling the nodes, and the nodes have symmetric roles at any point in time) that is fair and
O(1)-competitive against any (T, 1 − ε)-bounded
adversary. The nodes do not know ε, but we do
allow them to have a very rough upper bound of
the number n and T . More specifically, we will
assume that the nodes have a common parameter
γ = O(1/(log T + log log n)). This is still scalable,
since such an estimate leaves room for a superpolynomial change in n and a polynomial change
in T over time, so it does not make the problem
trivial (as it would be the case if the nodes knew
constant factor approximations for n or T ).
C. Our Contributions
This paper introduces and analyzes the medium
access protocol A NTI JAM. A NTI JAM is robust to
a strong adaptive and reactive jammer, who can
block the medium a constant fraction of the time
and thus models a large range of (intentional and
unintentional) interference models. Nevertheless, we
can show that the A NTI JAM MAC protocol achieves
a high throughput performance by exploiting any
nonblocked time intervals effectively. The main theoretical contribution is the derivation of the following
theorem that shows that A NTI JAM is competitive in
the sense that a constant fraction of the nonjammed
execution time is used for successful transmissions,
i.e., A NTI JAM is able to benefit from the rare
and hard-to-predict time intervals where the shared
medium is available. Moreover, A NTI JAM converges
fast and allocates the shared medium fairly to the
nodes.
Theorem I.1. Let N = max{T, n}. The A NTI JAM
2
protocol is constant-competitive, namely e−Θ(1/ε ) competitive w.h.p., under any (T, 1 − ε)-bounded
reactive adversary if the protocol is executed for at
2
least Θ( 1ε log N max{T, (eδ/ε /εγ 2 ) log3 N }) many
time steps, where ε ∈ (0, 1) is a constant,
γ = O(1/(log T + log log n)), and where δ is
a sufficiently large constant. Moreover, A NTI JAM

509

time P
step with probability pv with pv ≤ p̂. Let
p = v pv , q0 be the probability that the channel is
idle and q1 be the probability that exactly one node
is sending a message. The following claim appeared
originally in [2]. It states that if q0 = Θ(q1 ), then the
cumulative sending probability p is constant, which
in turn implies that at any nonjammed time step
we have constant probability of having a successful
transmission. Hence our protocol aims at adjusting
the sending probabilities pv of the nodes such that
q0 = Θ(q1 ), in spite of the reactive adversarial
jamming activity. This will be achieved by using a
multiplicative increase/decrease mechanism for the
probabilities pv and by synchronizing all the nodes,
both in terms of sending probabilities and their
own estimates Tv on the time window T , at every
successful transmission.

achieves high fairness: the channel access probabilities among nodes do not differ by more than a
factor of (1 + γ) after the first message was sent
successfully.
We believe that A NTI JAM is interesting also from
a practical point of view, as—in contrast to the
analysis—the basic protocol is very simple.
D. Paper Organization
The remainder of this paper is organized as follows. Section II introduces the A NTI JAM MAC protocol. Subsequently, we present a formal analysis of
the throughput performance under reactive jamming
(Section III). Section IV reports on the insights
gained from our simulation experiments. The paper
is concluded in Section V. Due to space constraints,
not all proofs and simulation results are included in
this article, and the interested reader is referred to
the ArXiv Technical Report 1007.4389 [24].

Claim II.1. q0 · p ≤ q1 ≤

q0
1−p̂

· p.

Now we present our A NTI JAM protocol:

II. T HE A NTI JAM MAC P ROTOCOL

In each step, each node v does the following. v
decides with probability pv to send a message along
with a tuple: (pv , cv , Tv ). If it decides not to send a
message, it checks the following two conditions:
1) If v senses an idle channel, then pv :=
min{(1 + γ)pv , p̂} and Tv := Tv − 1.
2) If v successfully receives a message along with
the tuple of (pnew , cnew , Tnew ), then pv :=
(1 + γ)−1 pnew , cv := cnew , and Tv := Tnew .
Afterwards, v sets cv := cv + 1. If cv > Tv then
it does the following: v sets cv := 1, and if there
was no idle step among the past Tv time steps, then
pv := (1 + γ)−1 pv and Tv := Tv + 2.

The basic ideas of the A NTI JAM MAC protocol
are inspired by slotted ALOHA schemes where
nodes change their access probabilities over time,
in particular the protocols described in [13] (which
also uses access probabilities depending on the ratio between idling and successful time slots) and
particularly [2]. However, the algorithm in [2] does
not achieve a provable performance under reactive
jammers, which is due to the asymmetric access
probabilities. Therefore, in our protocol, we explicitly try to equalize access probabilities, which also
improves fairness among the nodes.
Each node v maintains a time window threshold
estimate Tv and a counter cv . The parameter γ is the
same for every node and is set to some sufficiently
small value in O(1/(log T + log log n)). Note that
this assumption is not critical as it allows for a high
scalability: the nodes only need some polynomial
estimate of T and even rougher estimate of n.1 Let
p̂ be any constant so that 0 < p̂ ≤ 1/24. Initially,
every node v sets Tv := 1, cv := 1 and pv := p̂.
Afterwards, the protocol works in synchronized time
steps. We assume synchronized time steps for the
analysis, but a nonsynchronized execution of the
protocol would also work as long as all nodes operate
at roughly the same speed.
The basic protocol idea is simple. Suppose that
each node v decides to send a message at the current

III. A NALYSIS
Let V be the set of all nodes. Let pt (v) be node v’s
access probability pv at the beginning
of the t-th time
P
step. Furthermore, let pt = v∈V pt (v). Let I be a
time frame consisting of αε log N subframes I 0 of
2
δ/ε2
size f = max{T, αβ
log3 N }, where α, β and
εγ 2 e
δ are sufficiently large constants. Let F = αε log N ·f
denote the size of I.
First, we will derive some simple facts on the
behavior of A NTI JAM. We then show that given a
certain sufficiently large initial cumulative probability pt in a subframe, the cumulative probability
cannot be smaller at the end of the subframe. We
proceed to show that A NTI JAM performs well in
time periods in which pt is upper bounded by δ/ε2
for some constant δ. Finally, we show that for

1 The assumption of having a constant factor approximation of
T and n would render the problem simple.

510

pv = (1 + γ)−1 p̂ for all nodes v 6= u, then pt+1 =
(1 + γ − O(1/n))pt (because all nodes except for u
increase their sending probability by a factor (1+γ)
from p̂/(1 + γ)); or (iii) if pv < p̂ for all nodes v,
then pt+1 = (1 + γ)pt .
2. If there is a successful transmission at time t, and
if cv ≤ Tv or there was an idle time step in the previous Tv rounds, then (i) if the sender is the same as
the last successful sender, then pt+1 = pt (because
for the sender u, pu (t + 1) = pu (t), and the other
nodes remain at pu (t+1)/(1+γ) = pu (t)/(1+γ)); if
(ii) the sender w is different from the last successful
sender u and pv = p̂ for all nodes v (including u and
w), then pt+1 = (1 + γ − O(1/n))−1 pt (all nodes
except w reduce their sending probability); or (iii)
if the sender w is different from the last successful
sender u and pv < p̂ for at least one node v (including u and w), then pt+1 = (1 + γ)−1 pt (because at
time t, for all nodes v 6= u: pv (t) = pu (t)/(1 + γ);
subsequently, pw (t + 1) = pw (t) and for all nodes
v 6= w: pv (t + 1) = pw (t + 1)/(1 + γ)).
3. If the channel is busy at time t, then pt+1 = pt
when ignoring the case that cv > Tv .
Whenever cv > Tv and there has not been an
idle time step during the past Tv steps, then pt+1 is,
in addition to the actions specified in the two cases
above, reduced by a factor of (1 + γ).

any jamming strategy, A NTI JAM has a cumulative
probability of pt ≤ δ/ε2 for most of the time, which
yields our main theorem.
In our analysis, we will use the following relation.
Lemma III.1. For all 0 < x < 1 it holds that
e−x/(1−x) ≤ 1 − x ≤ e−x
Moreover, we make extensive use of the following
Chernoff bounds.
Lemma III.2. Consider any set of binary random
variables X1 , . . . , Xn . Suppose
Q that there
Qare values
p1 , . . . , pn ∈ [0, 1] with E[ i∈S Xi ] ≤ i∈S pi for
every
. . . , n}. Then it holds for X =
Pn set S ⊆ {1, P
n
X
and
µ
=
i
i=1
i=1 pi and any δ > 0 that

µ
δ2 µ
eδ
P[X ≥ (1 + δ)µ] ≤
≤ e− 2(1+δ/3) .
1+δ
(1 + δ)
Q
If,
Q on the other hand, it holds that E[ i∈S Xi ] ≥
i∈S pi for every set S ⊆ {1, . . . , n}, then it holds
for any 0 < δ < 1 that

µ
2
e−δ
P[X ≤ (1 − δ)µ] ≤
≤ e−δ µ/2 .
1−δ
(1 − δ)
We start with some simple facts. Fact III.3 shows
that the protocol synchronizes the sending probabilities of the nodes (up to a factor of (1 + γ)), and
that all values cv and Tv are also synchronized.

The next two lemmas bound how many times Tv
can be increased in a subframe and the minimum cumulative probability we should expect w.h.p., given
some sufficiently large initial cumulative probability
at the start of a subframe.

Fact III.3. Right after a successful transmission
of the tuple (p0 , c0 , T 0 ), (pv , cv , Tv ) = ((1 +
γ)−1 p0 , c0 , T 0 ) for all receiving nodes v and
(pu , cu , Tu ) = (p0 , c0 , T 0 ) for the sending node u.
In particular, for any time step t after a successful
transmission by node u, (cv , Tv ) = (cw , Tw ) for all
nodes v, w ∈ V .

Lemma III.6. If in subframe I 0 the number of idle
time steps is at most k, √
then every node v increases
Tv by 2 at most k/2 + f many times in I 0 .

Fact III.3 also implies the following corollary.

0
Lemma III.7. For any
√ subframe I in which initially
2
2f
pt0 ≥ 1/(f (1 + γ)
), the last√time step t of I 0
again satisfies pt ≥ 1/(f 2 (1 + γ) 2f ), w.h.p.

Corollary III.4. After a successful transmission, the
access probabilities pv of the nodes v ∈ V will never
differ by more than a factor (1 + γ) in the future.
The next fact follows from the protocol and
Fact III.3, and they help one understand how the cumulative probabilities vary over time with successful
transmissions, idle time steps, etc.

The proofs of both lemmas follow from the arguments in the proofs of similar lemmas in [2].
Lemma III.8 shows that for times of low cumulative probabilities, A NTI JAM yields a good performance.

Fact III.5. For any time step t after a successful
transmission or a well-initialized state of the protocol (in which (pv , cv , Tv ) = (p̂, 1, 1) for all nodes v)
it holds:
1. If the channel is idle at time t then (i) if pv =
p̂ for all v, then pt+1 = pt ; (ii) if pu = p̂ and

Lemma III.8. Consider any subframe I 0 , and let
δ > 1 be a sufficiently large constant. Suppose√that
at the beginning
of I 0 , pt0 ≥ 1/(f 2 (1 + γ) 2f )
√
and Tv ≤ F /2 for every node v. If pt ≤ δ/ε2
for at least half of the nonjammed time steps in
2
I 0 , then A NTI JAM is at least 8(1−δp̂)ε2 e−δ/(1−p̂)ε -

511

competitive in I 0 .

Claim III.10. Let g be the number of nonjammed
time steps t in I 0 with pt ≤ δ/ε2 . If g ≥ εf /2 then

Proof: A time step t in I is called useful
if we either have an idle channel or a successful
transmission at time t (i.e., the time step is not
jammed and there are no collisions) and pt ≤ δ/ε2 .
Let k be the number of useful time steps in I 0 .
Furthermore, let k0 be the number of useful time
steps in I 0 with an idle channel, k1 be the number of
useful time steps in I 0 with a successful transmission
and k2 be the maximum number of times a node v
reduces pv in I 0 because of cv > Tv . Recall that
k = k0 + k1 . Moreover, the following claim holds:

k≥

2
δ
e−δ/(1−p̂)ε · g
2(1 − p̂)ε2

w.h.p.
Proof: Consider any (T, 1−ε)-bounded jammer
for I 0 . Suppose that of the nonjammed time steps t
with pt ≤ δ/ε2 , s0 have an idle channel and s1 have
a busy channel. It holds that s0 + s1 = g ≥ εf /2.
For any one of the nonjammed time steps with an
idle channel, the probability that it is useful is one,
and for any one of the nonjammed time steps with
a busy channel, the probability that it is useful (in
this case, that it has a successful transmission) is at
least

Claim III.9. If n ≥ (1 + γ)δ/(ε2 p̂), then
k0 − log1+γ (δ/(ε2 · pt0 )) ≤ k10 + k2
where k10 is the number of useful time steps with
a successful transmission in which the sender is
different from the previously successful sender.

X

pv

v

Y

(1 − pw )

≥

w6=v

≥

Proof: According to Fact III.5, pv ∈ [(1 +
γ)−1 p, p] for some access probability p for all time
steps in I 0 . Hence, if pt ≤ δ/ε2 and n ≥ (1 +
γ)δ/(ε2 p̂), then pv (t) ≤ p̂/(1+γ) for all v. This implies that whenever there is a useful time step t ∈ I
with an idle channel, then pt+1 = (1 + γ)pt . Thus,
it takes at most log1+γ (δ/(ε2 · pt0 )) many useful
time steps with an idle channel to get from pt0 to a
cumulative probability of at least δ/ε2 . On the other
hand, each of the k10 successful transmissions reduces
the cumulative probability by (1 + γ). Therefore,
once the cumulative probability is at δ/ε2 , we must
have k0 ≤ k10 + k2 since otherwise there must be
at least one useful time step where the cumulative
probability is more than δ/ε2 , which contradicts the
definition of a useful time step.
√
Since pt0 ≥ 1/(f 2 (1 + γ) 2f ) it holds that
p
log1+γ (δ/(ε2 · pt0 )) ≤ log1+γ (δf 2 /ε2 ) + 2f

=
=

1 X Y
pv
(1 − pw )
1 − p̂ v
w
1 X Y −pw /(1−p̂)
pv
e
1 − p̂ v
w
1 X
pv e−p/(1−p̂)
1 − p̂ v
p
e−p/(1−p̂)
1 − p̂

where p is the cumulative probability at the step.
Since pt ≤ δ/ε2 , it follows that the probability of a
busy time step to be useful is at least
2
δ
e−δ/(1−p̂)ε
2
(1 − p̂)ε

Thus,
E[k] ≥ s0 +
≥

2
δ
e−δ/(1−p̂)ε s1
2
(1 − p̂)ε

2
δ
e−δ/(1−p̂)ε · g
(1 − p̂)ε2

since k is minimized for s0 = 0 and s1 = g.
Since our lower bound for the probability of
a busy step to be useful holds independently for
all nonjammed busy steps t with pt ≤ δ/ε2 and
E[k] ≥ α log N for our choice of g, it follows from
the Chernoff bounds that k ≥ E[k]/2 w.h.p.
From Claim III.10 it follows that
p
2
δ
k1 ≥ (
e−δ/(1−p̂)ε · g)/3 − 2 f
2
2(1 − p̂)ε

From
Lemma III.6 we also know that k2 ≤ k0 /2 +
√
f . Hence,
p
p
k0 ≤ 2k10 + 2 · log1+γ (δf 2 /ε2 ) + 2 · ( f + 2f )
p
≤ 2k10 + 6 f
if f is sufficiently large. Also, k0√= k − k1 and k10 ≤
k1 . Therefore, k − k1 ≤ 2k1 + 6 f or equivalently,
p
k1 ≥ k/3 − 2 f

w.h.p., which completes the proof of Lemma III.8.
It remains to consider the case that for less than
half of the nonjammed time steps t in I 0 , pt ≤ δ/ε2 .
Fortunately, this does not happen w.h.p.

It remains to find a lower bound for k.

512

0
Lemma
√ III.11. Suppose that at the beginning of I ,
Tv ≤ F /2 for every node v. Then at most half of
the nonjammed time steps t can have the property
that pt > δ/ε2 w.h.p.

and we did not optimize the parameters to obtain the
best constants.
Note that the formal guarantees derived in the
previous section hold for any type of reactive jammer
that falls within our (T, 1 − ε)-bounded adversary
model. However, for the simulations, specific instantiations need to be considered. Concretely, we
use three different jamming strategies, for different
ε values and where T = 100: (1) one that jams busy
channels with probability (1 − ε); (2) one that jams
busy channels deterministically (as long the jamming
budget is not used up); (3) one that jams idle
channels deterministically (as long as the jamming
budges is not used up). (At first sight, the scenario
where only idle time periods are jammed seems
to be of less relevance. However, observe that this
attack may be effective against certain protocols to
steer the execution into inefficient states, e.g., against
protocols that only increase sending probabilities in
idle rounds.)
We define throughput as the number of successful
transmissions over the number of nonjammed time
steps.

The full proof is quite complex and appears in the
ArXiv Technical Report 1007.4389 [24].
Notice√that by the choice of f and F , Tv never
exceeds F /2 for any v when initially Tv = 1 for
all v. Hence, the prerequisites of the lemmas are
satisfied. We can also show the following lemma,
which shows that Tv remains bounded over time.
Lemma III.12.√ For any time frame I in √
which
initially Tv ≤ F /2 for all v, also Tv ≤ F /2
for all v at the end of I w.h.p.
Proof: We already know that in each subframe
I 0 in I, at least εf /2 of the nonjammed time steps t
in I 0 satisfy pt ≤ δ/ε2 w.h.p. Hence, for all (T, 1 −
ε)-bounded jamming strategies, there are at least
2

(δ/ε2 ) · e−δ/ε · εf /2
useful time steps in I√0 w.h.p. Due to the lower bound
of pt ≥ 1/(f 2 (1+γ) f ) for all time steps in I w.h.p.
we can also conclude that
√

k0 ≥ k10 + k2 − log1+γ ((δ/ε2 ) · f 2 (1 + γ)

f

A. Throughput
In a first set of experiments we study the throughput as a function of the network size and ε. We
evaluate the throughput performance for each type of
adversary introduced above. We find that for all three
strategies, the throughput is basically constant (i.e.,
independent of the network size) and, depending
on the scenario, between 20 and 40 %. (Please
refer to the ArXiv Technical Report 1007.4389 [24]
for the corresponding figures and a more detailed
discussion.) This is in accordance with our theoretical insight of Theorem I.1. We observed that
given our conditions on ε and T , the strategy that
jams busy channels deterministically results in the
lowest throughput. Hence, in the remaining experiments described in this section, we will focus on
this particular strategy. As expected, jamming idle
channels does not affect the protocol behavior much.
In our simulations, A NTI JAM makes effective use
of the nonjammed time periods, yielding 20%-40%
successful transmissions even without optimizing the
protocol parameters. In additional experiments we
also studied the throughput as a function of γ,
see Figure 1. As expected, the throughput declines
slightly for large γ, but this effect is small. (Note
that for very small γ, the convergence time becomes
large and the experiments need run for a long time
in order not to underestimate the real throughput.)

).

From the technical report [24], it follows that
k0 ≥ k1 /3
2

w.h.p. Since k0 + k1 = k and k ≥ (δ/ε2 ) · e−δ/ε ·
εf /2 it follows that k0 = Ω(f ). Therefore, there
must be at least one time point in I 0 with T√
v = 1
for all v ∈ V . This in turn ensures that Tv ≤ F /2
for all v at the end of I w.h.p.
With Lemma III.12, we show that Lemma III.11
is true for a polynomial number of subframes. Then,
Lemma III.11 and Lemma III.12 together imply
that Lemma III.8 holds for a polynomial number of
subframes. Together with the fairness result (Corollary III.4), our main Theorem I.1 follows. Finally,
note that along the same line as in [2], we can show
that A NTI JAM is self-stabilizing, so the throughput
result can be extended to an arbitrary sequence of
time frames.
IV. S IMULATION
We have implemented a simulator to study additional properties of our protocol. This section reports
on some of our results. The focus here is on the
qualitative nature of the performance of A NTI JAM,

513

Figure 1.

Throughput as a function of γ under three different jamming strategies. Left: ε = 0.2, Right: ε = 0.5).

B. Convergence Time
Besides a high throughput, fast convergence is
the most important performance criterion of a MAC
protocol. The traces in Figure 2 show the evolution
of the cumulative probability over time. It can be
seen that the protocol converges quickly to constant
access probabilities. (Note the logarithmic scale.) If
the initial probability for each node is high, the
protocol needs more time to bring down the lowconstant cumulative probability. Moreover, the ratio
of the time period the cumulative probability is in the
1 2
range of [ 2ε
, ε ] to the time period the protocol being
executed is 92.98% when p̂ = 1/24, and 89.52%
when p̂ = 1/2. This implies that for a sufficiently
large time period, the cumulative probability is well
bounded most of the time, which corresponds to our
theoretical insights. Figure 3 studies the convergence
time for different network sizes. We ran the protocol
50 times, and assume that the execution has converged when the cumulative probability p satisfies
p ∈ [0.1, 10], for at least 5 consecutive rounds.
The simulation result also confirms our theoretical
analysis in Theorem I.1, as the number of rounds
needed to converge the execution is bounded by
Θ( 1ε log N max{T, εγ12 log3 N }).

Figure 3. A NTI JAM runtime as a function of network size for
p̂ = 1/24, and ε = 0.5.

values p̂ and Tv , the throughput rises quickly (up
above 20%) and stays there afterwards.

Figure 4.
0.5.

Convergence in a network of 1000 nodes where ε =

C. Fairness
As A NTI JAM synchronizes cv , Tv , and pv values
upon message reception, the nodes are expected to
transmit roughly the same amount of messages; in
other words, our protocol is fair. Figure 5 presents a
histogram showing how the successful transmissions
are distributed among the nodes. More specifically,
we partition the number of successful transmissions
into intervals of size 4. Then, all the transmissions
are grouped according to those intervals in the histogram.
We also compare minimum throughputs achieved
by individual nodes, when using the MAC protocol

Figure 2. Evolution of cumulative probability over time (network
size is 1000 nodes, and ε = 0.5). Note that the plot has
logarithmic scale.

Figure 4 indicates that independently of the initial

514

D. Comparison to 802.11
Finally, to put A NTI JAM into perspective, as a
comparison, we implemented a simplified version of
the widely used 802.11 MAC protocol (with a focus
on 802.11a).
The configurations for the simulation are the following: (1) the jammer is reactive and (T, 1 − ε)bounded; (2) the unit slot time for 802.11 is set
to 50µs; for simplicity, we define one time step
for A NTI JAM to be 50µs also; (3) we run A N TI JAM and 802.11 for 4 min, which is equal to
4.8 · 106 time steps in our simulation; (4) the backoff
timer of the 802.11 MAC protocol implemented here
uses units of 50µs; (5) we omit SIFS, DIFS, and
RTS/CTS/ACK since they are not relevant to this
implementation.
A comparison is summarized in Figure 8. The
throughput achieved by A NTI JAM is significantly
higher than the one by the 802.11 MAC protocol,
specially for lower values of ε, when the 802.11
MAC protocol basically fails to deliver any successful message.

Figure 5. Fairness in a network of 1000 nodes, where ε = 0.5,
and p̂ = 1/24 (averaged over 10 runs).

in [2] and A NTI JAM, so as to demonstrate the fairness property of the latter protocol. Figure 6 clearly
shows that A NTI JAM achieves much higher fairness
than the MAC protocol in [2]: since the minimum
throughput produced by A NTI JAM is significantly
higher than that of [2]. Moreover, according to
M IN
Figure 7, the M
AX throughput ratio of A NTI JAM
is also significantly higher than that of [2]. This
indicates the difference between the minimum and
maximum throughput achieved by A NTI JAM is much
smaller, and hence A NTI JAM is fair while [2] is not.

Figure 8.
Throughput as a function of ε ∈ [0.05, 0.95],
compared to 802.11, averaged over 10 runs, where p̂ = 1/24.
Figure 6. Minimum throughput as a function of ε ∈ [0.05, 0.95],
compared to the MAC protocol in [2], averaged over 10 runs,
where p̂ = 1/24.

V. C ONCLUSION
This paper presented a simple, fair and selfstabilizing distributed MAC protocol called A NTI JAM that is able to make efficient use of a shared
communication medium whose availability is changing quickly and in a hard to predict manner over
time. In particular, we proved that our protocol
achieves a constant competitive throughput if ε is
constant.
It is an open question whether there is a protocol
where the throughput is independent of ε, as it exists
for adaptive, nonreactive jammers [2]. At least for
protocols that are based on cumulative probabilities,
it seems that this is impossible, for the following
reason: Note that information about the current state
allows a (1 − ε)-bounded reactive adversary to jam

Figure 7. MIN/MAX throughput ratio as a function of ε ∈
[0.05, 0.95], compared to the MAC protocol in [2], averaged over
10 runs, where p̂ = 1/24.

515

all time steps with transmissions, given that there is
at least an ε-fraction of idle time steps. This indicates
that a reactive jammer is much stronger than a
jammer which is only adaptive to the past: only when
the probability of seeing an idle channel drops below
ε, the nodes have a chance to successfully transmit
messages. This however implies that the cumulative
sending probability must be at least log 1/ε, which
means that if the access probabilities of all nodes
are equal, the probability of successfully transmitting
a message is at most ε · log 1/ε. Hence, at best
one might be able to achieve a competitiveness that
drops linearly with ε while we can only prove a
competitiveness that drops exponentially with ε, so
further research is needed to close that gap.

[13] M. Heusse, F. Rousseau, R. Guillier, and A. Duda.
Idle sense: An optimal access method for high
throughput and fairness in rate diverse wireless lans.
In Proc. Conference on Applications, Technologies,
Architectures, and Protocols for Computer Communications (SIGCOMM), pages 121–132, 2005.
[14] S. Jiang and Y. Xue. Providing survivability against
jamming attack via joint dynamic routing and channel assigment. In Proc. 7th Workshop on Design of
Reliable Communication Networks (DRCN), 2009.
[15] C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable
broadcast in radio networks: The bounded collision
case. In Proc. of PODC ’06, 2006.
[16] Y. Law, L. van Hoesel, J. Doumen, P. Hartel, and
P. Havinga. Energy-efficient link-layer jamming
attacks against wireless sensor network MAC protocols. In Proc. of SASN ’05, pages 76–88, 2005.
[17] M. Li, I. Koutsopoulos, and R. Poovendran. Optimal
jamming attacks and network defense policies in
wireless sensor networks. In Proc. of Infocom ’07,
pages 1307–1315, 2007.
[18] X. Liu, G. Noubir, R. Sundaram, and S. Tan. Spread:
Foiling smart jammers using multi-layer agility. In
Proc. of Infocom ’07, pages 2536–2540, 2007.
[19] D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer. Speed dating despite jammers. In Proc.
DCOSS ’09, June 2009.
[20] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein.
Using channel hopping to increase 802.11 resilience
to jamming attacks. In Proc. of Infocom ’07, 2007.
[21] R. Negi and A. Perrig. Jamming analysis of MAC
protocols. Technical report, Carnegie Mellon University, 2003.
[22] A. Pelc and D. Peleg. Feasibility and complexity of
broadcasting with random transmission failures. In
Proc. of PODC ’05, 2005.
[23] P. Raghavan and E. Upfal. Stochastic contention
resolution with short delays. SIAM Journal on
Computing, 28(2):709–719, 1999.
[24] A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Antijam: Efficient medium access despite adaptive
and reactive jamming. In ArXiv Report 1007.4389,
2010.
[25] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A
jamming-resistant mac protocol for multi-hop wireless networks. In Proc. 24th International Symposium on Distributed Computing (DISC), 2010.
[26] A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Self-stabilizing leader election for single-hop wireless networks despite jamming. In Proc. 12th ACM
International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc), 2011.
[27] D. Thuente and M. Acharya. Intelligent jamming in
wireless networks with applications to 802.11b and
other networks. In Proc. of MILCOM ’06, 2006.
[28] A. Wood, J. Stankovic, and G. Zhou. DEEJAM: Defeating energy-efficient jamming in IEEE 802.15.4based wireless networks. In Proc. of SECON ’07,
2007.
[29] W. Xu, W. Trappe, Y. Zhang, and T. Wood. The feasibility of launching and detecting jamming attacks
in wireless networks. In Proc. of MobiHoc ’05, pages
46–57, 2005.
[30] W. Xu, T. Wood, and Y. Zhang. Channel surfing and
spatial retreats: defenses against wireless denial of
service. In Proc. of Workshop on Wireless Security,
2004.

ACKNOWLEDGMENTS
This work was supported in part by NSF awards
CCF-0830791 and CCF-0830704, and by the DFG
project SCHE 1592/1-1.
R EFERENCES
[1] G. Alnifie and R. Simon. A multi-channel defense
against jamming attacks in wireless sensor networks.
In Proc. of Q2SWinet ’07, pages 95–104, 2007.
[2] B. Awerbuch, A. Richa, and C. Scheideler. A
jamming-resistant MAC protocol for single-hop
wireless networks. In Proc. of PODC ’08, 2008.
[3] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and B. Thapa. On the performance of IEEE
802.11 under jamming. In Proc. of IEEE Infocom
’08, pages 1265–1273, 2008.
[4] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E. Leiserson. Adversarial contention
resolution for simple channels. In Proc. of SPAA
’05, pages 325–332, 2005.
[5] T. Brown, J. James, and A. Sethi. Jamming and
sensing of encrypted wireless ad hoc networks. In
Proc. of MobiHoc ’06, pages 120–130, 2006.
[6] J. Chiang and Y.-C. Hu. Cross-layer jamming detection and mitigation in wireless broadcast networks.
In Proc. of MobiCom ’07, pages 346–349, 2007.
[7] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki.
Adversarial queuing on the multiple-access channel.
In Proc. of PODC ’06, pages 92–101, 2006.
[8] A. Czumaj and W. Rytter. Broadcasting algorithms
in radio networks with unknown topology. Journal
of Algorithms, 60(2):115 – 143, 2006.
[9] S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski,
C. Newport, F. Kuhn, and N. Lynch. Reliable
distributed computing on unreliable radio channels.
In Proc. 2009 MobiHoc S3 Workshop, 2009.
[10] S. Gilbert, R. Guerraoui, D. R. Kowalski, and
C. C. Newport. Interference-resilient information exchange. In Proc. 28th IEEE International Conference
on Computer Communications (INFOCOM), 2009.
[11] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and suspicious sensors: On the efficiency
of malicious interference in wireless networks. In
Proc. of OPODIS ’06, 2006.
[12] J. Hastad, T. Leighton, and B. Rogoff. Analysis of
backoff protocols for mulitiple accesschannels. SIAM
Journal on Computing, 25(4):740–774, 1996.

516

2010 International Conference on Distributed Computing Systems

Parameterized Maximum and Average Degree
Approximation in Topic-based Publish-Subscribe
Overlay Network Design
Melih Onus

Andréa W. Richa

Department of Computer Engineering
Computer Science and Engineering
TOBB University of Economics and Technology School of Computing, Informatics, and Decision Systems Engineering
Ankara, Turkey
Arizona State University
Email: monus@etu.edu.tr
Tempe, AZ 85287
Email: aricha@asu.edu

Given their simplicity and wide applicability, we have seen
many implementations of those systems in recent years (see
e.g., [1]–[4], [6]–[10], [16], [19], [24]–[28]), as well as
many applications built on top of them, such as stock-market
monitoring engines, RSS feeds [20], [27], on-line gaming and
several others. For a survey on pub/sub systems, see [15].
We will implement a topic-based pub/sub system by designing a connected (peer-to-peer) overlay network for each
pub/sub topic: more specifically, for each topic t, we will
enforce that the subgraph induced by the nodes interested in
t will be connected. This translates into a fully decentralized
topic-based pub/sub system since any given topic-based overlay network will be connected and thus nodes subscribed to
a given topic do not need to rely on other nodes (agents) for
forwarding their messages. Such an overlay network is said to
be topic-connected.
Low node degrees are desirable in practice for scalability
and also due to bandwidth constraints. Nodes with a high
number of adjacent links will have to manage all these links
(e.g., monitor the availability of its neighbors, incurring in
heartbeats and keep-alive state costs, and connection state
costs in TCP) and the traffic going through each of the links,
without being able to take great advantage of aggregating the
traffic (which would also reduce the number of packet headers,
responsible for a significant portion of the traffic for small
messages). See [11], [21] for further motivation.
The node degrees and number of edges required by a topicconnected overlay network will be low if the node subscriptions are well-correlated. In this case, by connecting two nodes
with many coincident topics, one can satisfy connectivity of
many topics for those two nodes with just one edge. Several
recent empirical studies suggest that correlated workloads are
indeed common in practice [20], [27].
In this paper, we focus on building overlay networks with
low (average and maximum) node degrees. The importance
of minimizing both the maximum and average degree has
been recognized in some network domains, such as that
of survivable network design [18] and that of establishing
connectivity in wireless networks [13]. To the best of our

Abstract—Publish/subscribe communication systems where
nodes subscribe to many different topics of interest are becoming
increasingly more common. Designing overlay networks that
connect the nodes subscribed to each distinct topic is hence
a fundamental problem in these systems. For scalability and
efficiency, it is important to keep the degree of the nodes in
the publish/subscribe system low. Ideally one would like to be
able not only to keep the average degree of the nodes low, but
also to ensure that all nodes have equally the same degree, giving
rise to the following problem: Given a collection of nodes and
their topic subscriptions, connect the nodes into a graph with
low average and maximum degree such that for each topic t,
the graph induced by the nodes interested in t is connected.
We present the first polynomial time parameterized sublinear
approximation algorithm for this problem.
We also propose two heuristics for constructing topicconnected networks with low average degree and constant diameter and validate our results through simulations. In fact, the
results in this section are a refinement of the preliminary results
by Onus and Richa in INFOCOM’09.

I. I NTRODUCTION
In publish/subscribe (pub/sub) systems, publishers and subscribers interact in a decoupled fashion. They use logical
channels for delivering messages according to the nodes
subscription to the services of interest. Publishers publish their
messages through logical channels that deliver the messages
to the nodes that subscribed to the respective services.
Pub/sub systems can be either topic-based or content-based.
In a topic-based pub/sub system, messages are published to
“topics”, where each topic is uniquely associated with a logical
channel. Subscribers in a topic-based system will receive all
messages published to the topics to which they subscribe. The
publisher is responsible for defining the classes of messages
to which subscribers can subscribe. In a content-based system,
messages are only delivered to a subscriber if the attributes of
those messages match constraints defined by the subscriber;
each logical channel is characterized by a subset of these
attributes. The subscriber is responsible for classifying the
messages.
This work was supported in part by NSF awards CCF-0830791 and CCF0830704.

1063-6927/10 $26.00 © 2010 IEEE
DOI 10.1109/ICDCS.2010.54

644

knowledge, minimizing both the maximum and the average
degree in topic-connected pub/sub overlay network design had
not been directly addressed prior to this work.
As in many other systems, a space-time trade-off exists
for pub-sub systems: On one hand, one would like the total
time taken by a topic-based broadcast (which directly depends
on the diameter of each topic-connected subnetwork) to be
as small as possible; on the other hand, for memory and
node bandwidth considerations, one would like to keep the
total degree of a node small. Those two measures are often
conflicting.
Most of the current solutions adopted in practice actually
fail at maintaining both the diameter and the node degrees low.
Take the naive, albeit popular, solution to topic connectedoverlay network design to construct a cycle connecting all
nodes interested in a topic independently for each given
topic [28]: This construction results both in very large diameter
for each topic-connected network (proportional to the total
number of nodes subscribed to a topic) and in node degrees proportional to the nodes’ subscription sizes, whereas a
more careful construction, taking into account the correlations
among the node subscription sets might result in much smaller
node degrees (and total number of edges) and topic-based
diameters. Even the more recent advances on approximating
the average or maximum degree alone have been made [11],
[21] still fail at approximating the diameter well.
Whereas in the main contribution of our work (see Section III), we completely neglect the diameter of the networks
constructed, in Section VI, we propose some heuristics for
constructing topic-connected networks with low average degree and constant diameter. In fact, the results in Section VI
are a refinement of the preliminary results presented in [21].

Chockler et. al. [11]
Onus and Richa [21]
This Paper
Lower Bound

Avg Degree
O(log(n · t))
θ(n)
O(k · log(n · t))
Ω(log n)

Max Degree
θ(n)
O(log(n · t))
O((n/k) · log(n · t))
Ω(log n)

TABLE I
S UMMARY OF KNOWN RESULTS ON APPROXIMATION RATIOS OF
MAXIMUM / AVERAGE DEGREE IN OVERLAY NETWORK CONSTRUCTION FOR
PUBLISH / SUBSCRIBE COMMUNICATION (n: NUMBER OF NODES , t:
NUMBER OF TOPICS , k IS ANY PARAMETER BETWEEN 1 AND n)

maximum degree well. More specifically, the Low-ODA algorithm achieves an average degree approximation of O(min{k ·
log(n · t), n}) and a maximum degree approximation of
O(min{(n/k) · log(n · t), n}), where n = |V | is the number
of nodes in the network, t = |T | is the number of topics and
k is any parameter chosen from [1, n] (See also Table I). To
the best of our knowledge, this is the first overlay network
design algorithm that achieves sublinear approximations
√ on
both the average and maximum degrees (e.g., for k = n).
The Low-ODA algorithm is a greedy algorithm which relies on
repeatedly evaluating the trade-off of greedily adding an edge
that would not increase the maximum degree versus greedily
adding an edge that would lead to a small number of total
edges in the final overlay network. The main contribution of
this work is therefore to show that such a greedy approach can
work and indeed leads to non-trivial sublinear approximation
on both the average and maximum degree. We expect that the
greedy parameterized template introduced by our algorithm
will lead to applications in other network design domains
where scalability is a key issue (see Section VII).
In addition, we present two algorithms (heuristics), Constant
Diameter Overlay Design Algorithm I (CD-ODA-I) and Constant Diameter Overlay Design Algorithm II (CD-ODA-II),
for building topic-based pub/sub networks where each topicconnected component is guaranteed to be of constant diameter
— more specifically of diameter 2 —and where we aim at
keeping the average degree low. Our experimental results show
that our algorithms improve on the previous heuristic presented
in [21] by a reduction of 20% on the average degree. As we
mentioned earlier, keeping the node degrees and the network
diameter low are key to the design of scalable topic-based
pub/sub systems. We provide some preliminary results along
these lines.

A. Our contributions
In this work, we consider the problem of devising topicbased pub/sub overlay networks with low node degrees. One
could argue for keeping the maximum degree of a node low
or for keeping the overall average node degree low, since
both are important and relevant measures of the complexity
and scalability of a system [13], [18]. Unfortunately, previous
attempts at minimizing either one of these degree measures
alone [11], [21] resulted in a linear explosion for the other
measure (see Table I).
In this work, we present the first algorithm that aims at
keeping both the average and the maximum degree low. More
specifically, we consider the following problem:

B. Related Work
Chockler et al. [11] introduced the Minimum Topic Connected Overlay (MinAv-TCO) problem [In the original paper,
this problem was called Min-TCO.], which aims at minimizing
the average degree alone of a topic-connected overlay network.
They present an algorithm, called Greedy Merge GM, which
achieves a logarithmic approximation on the minimum average
degree of the overlay network. The GM algorithm follows the
greedy approach described below:
The Greedy-Merge (GM) Algorithm [11]: The GM algorithm
greedily adds the edge which maximally reduces the total

Low Degree Topic-Connected Overlay (Low-TCO) Problem:
Given a collection of nodes V , a set of topics T , and the node
interest assignment I, connect the nodes in V into a topicconnected overlay network G which has both low average and
low maximum degree.
We present a parameterized sublinear approximation algorithm, Low Degree Overlay Design Algorithm(Low-ODA), for
this problem which approximates both the average and the

645

number of topic-connected components at each step of the
algorithm (initially we have the set of nodes V and no edges
between the nodes).

C. Structure of the paper
In Section II, we present some definitions and restate the
formal problem definition. In Section II-A, we present an
outline of the related problem of minimizing the maximum
node degree, namely the MinMax-TCO problem, and the
corresponding logarithmic approximation algorithm MinMaxODA proposed by Onus and Richa [21], since some of the
ideas presented will be useful for the Low-TCO problem. Section III presents our topic-connected overlay design algorithm
Low-ODA, whose approximation ratio is proved in Section IV.
Section V presents our simulation results which validate the
performance of the Low-ODA algorithm. Section VI presents
our two new heuristics for the the problem of minimizing
the average node degree while enforcing a 2-diameter overlay
network. We conclude the paper, also presenting some future
work, in Section VII.

While minimizing the average degree is a step forward towards improving the scalability and practicality of the pub/sub
system, their algorithm may still produce overlay networks of
very uneven node degrees where the maximum degree may
be unnecessarily high. In [21], it is shown that GM algorithm
may produce a network with maximum degree |V | while a
topic-connected overlay network of constant degree exists for
the same configuration of I (See Table I).
In [21], the problem of minimizing the maximum degree of
a topic-connected overlay network, Minimum Maximum Degree Topic Connected Overlay (MinMax-TCO), is considered,
and a logarithmic approximation algorithm on the minimum
maximum degree of the overlay network, Minimum Maximum Degree Overlay Design Algorithm (MinMax-ODA),
is presented. The MinMax-ODA algorithm is also a greedy
algorithm, as described below:

II. P RELIMINARIES
Let V be the set of nodes, and T be the set of topics. Let
n = |V |. The interest function I is defined as I : V × T →
{0, 1}. For a node v ∈ V and topic t ∈ T , I(v, t) = 1 if
and only if node v is subscribed to topic t, and I(v, t) = 0
otherwise.
For a set of nodes V , an overlay network G(V, E) is an
undirected graph on the node set V with edge set E ⊆ V × V .
For a topic t ∈ T , let Vt = {v ∈ V |I(v, t) = 1}. Given a topic
t ∈ T and an overlay network G(V, E), the number of topicconnected components of G for topic t is equal to the number
of connected components of the subgraph of G induced by
Vt . An overlay network G is topic-connected if and only if it
has one topic-connected component for each topic t ∈ T . The
diameter of a graph is the length of the longest shortest path
in the graph. The degree of a node v in an overlay network
G(V, E) is equal to the total number of edges adjacent to v
in G.

Min-Max Overlay Design Algorithm (MinMax-ODA) [21]:
Initially there are no edges between the set of nodes V . At each
step of the algorithm, add the edge which maximally reduces
the total number of topic-connected components among the
edges which increases maximum degree of the current graph
minimally.
The MinMax-ODA algorithm may produce overlay networks of very high average degree: As we will show in Section
II-A, this algorithm may produce a network with average
degree |V | − 2 while a topic-connected overlay network of
constant average degree exists for the same configuration of I
(See Table I).
In this work, we are, to the best of our knowledge, the
first to formally address the problem of minimizing both
the maximum and the average degree in topic-connected
pub/sub overlay network design. As we mentioned earlier,
minimizing both maximum and average degree is important
in many network domains, such as that of survivable network
design [18] and that of establishing connectivity in wireless
networks [13]. The overlay networks resulting from [2], [5],
[10] are not required to be topic-connected. In [4], [9], [12],
[28], topic-connected overlay networks are constructed, but
they make no attempt to minimize the average or maximum
node degree. The first papers to directly consider node degrees
when building topic-connected pub/sub systems were [11]
and [21], as we mentioned above. Minimizing the diameter
in topic-connected pub/sub overlay network design first addressed in [21].

A. Minimizing the maximum degree only
The MinMax-ODA algorithm (see Section I-B), proposed
by Onus and Richa [21], addressed the MinMax-TCO problem, in which they aim at minimizing the maximum node
degree. Unfortunately, while the MinMax-ODA algorithm
produces a logarithmic approximation on the maximum node
degree, it fails to approximate well the average node degree of
a topic-connected overlay network: The approximation ratio on
the average degree obtained by the MinMax-ODA algorithm
may be as bad as Θ(n), as we show in the lemma below.
Lemma 1. The MinMax-ODA algorithm can only guarantee
an approximation ratio of Θ(n) on the average node degree,
where n is number of nodes in the pub/sub system.
Proof: Consider the example where we have n nodes
v1 , v2 , ..., vn , and n2 topics T = {ti,j |1 ≤ i, j ≤ n}. Node
v1 is interested in all topics in T and each vi is interested
in ti,j and tj,i , 2 ≤ i ≤ n, 1 ≤ j ≤ n. W.l.o.g., assume
that n is even. The MinMax-ODA algorithm will produce
an overlay network with n(n − 2)/2 edges, by repeatedly

Some of the high level ideas and proof techniques of [11]
and [21] have their roots in techniques used for the classical
Set-Cover problem. We benefit from some of the ideas in [11],
[21] and also build upon their constructions for Set-Cover,
extending and modifying them to be able to handle the
maximum degree and the average degree altogether.

646

connecting a maximal matching of the nodes v1 , . . . , vn , n−2
times. The optimal overlay network with minimum number of
edges is E = {(v1 , vi )|1 < i ≤ n}, – the number of edges
of this overlay network is n − 1. Hence the approximation
ratio of the MinMax-ODA algorithm can be as large as
(n(n − 2)/2)/(n − 1) = Θ(n).

equal to the amount of decrease in the number of topicconnected components resulting from the addition of the edge
{u, v} to the current overlay network (represented by the edges
in OverlayEdges). Initially, this amount will be equal to the
number of topics that nodes u and v have in common.
At each iteration of the while loop, two edges are considered: an edge (e1 ) with maximum weight among the edges
in E ′ that increase the maximum degree of the current graph
minimally and an edge (e2 ) with maximum weight in all of
E ′ . If weight of the first one (e1 ) is greater than or equal to
weight of the second one (e2 ) over k, e1 is added to the set of
overlay edges; otherwise e2 is added. Note that the addition
of an edge to OverlayEdges can either increase the maximum
degree by 1 or not increase it at all. The crux in the analysis of
this algorithm is to show that each of the edges will reduce the
number of connected components by a “large” amount without
increasing the maximum degree by too much.
While at a first glance the Low-ODA algorithm may seem
like a trivial combination of the GM and MinMax-ODA
algorithms, the analysis show that such a combination is far
from trivial: Once we allow the algorithm (Low-ODA) to
select some edges based solely on their weight (edges of type
e2 ), the “perfect matching” behavior of the edges selected by
MinMax-ODA (basically one could show that the first n/2
edges selected by MinMax-ODA formed a perfect matching in
G′ , as did the second set of n/2 edges, etc.) is no longer valid
and the approximation ratio analysis used in MinMax-ODA
(which heavily relied on this perfect matching decomposition
of the edges selected) can no longer be directly used here.
Before we proceed in proving the approximation ratio on the
maximum degree and the approximation ratio on the average
degree guaranteed by Low-ODA, we prove that the algorithm
terminates in O(|V |4 |T |) time.

III. L OW D EGREE OVERLAY D ESIGN A LGORITHM
(L OW-ODA)
In this section we present our overlay design algorithm
(Low-ODA) for the Low-TCO problem. The weight of an
edge {u, v} is given by the reduction on the number of topicconnected components which would result from the addition
of {u, v} to the current overlay network. Let 1 ≤ k ≤ n.
Low-ODA starts with the overlay network G(V, ∅). In each
iteration of Low-ODA, the algorithm considers two edges:
e1 : a maximum weight edge among the ones which
minimally increases maximum degree of the current graph
e2 : a maximum weight edge
If the weight of edge e1 is greater than the weight of e2
divided by k, edge e1 is added to edge set of the overlay
network; otherwise edge e2 is added.
Let N C(V, E) denote total number of topic connected
components in the overlay network given by (V, E).
Algorithm 1 Low Degree Overlay Design Algorithm (LowODA)
1: OverlayEdges ← ∅
2: V ← Set of all nodes
3: G′ (V, E ′ ) ← Complete graph on V
4: for {u, v} ∈ E ′ do
5:
w{u, v} ← Number of topics that both of nodes u and
v have
6: end for
7: while G(V,OverlayEdges) is not topic-connected do
8:
Let e1 be a maximum weight edge in G′ (V, E ′ , w)
among the ones which increase the maximum degree
of G(V,OverlayEdges) minimally.
9:
Let e2 be a maximum weight edge in G′ (V, E ′ , w)
10:
if w(e1 ) ≥ w(e2 )/k then
11:
e = e1
12:
else
13:
e = e2
14:
end if
∪
15:
OverlayEdges = OverlayEdges {e}
16:
E ′ ← E ′ − {e}
17:
for {u, v} ∈ E ′ do
18:
w{u, v}
←
NC(V ,
OverlayEdges)
∪
NC(V ,OverlayEdges {u, v} )
19:
end for
20: end while

Lemma 2. The Low-ODA algorithm terminates within
O(|V |2 ) iterations of the while loop.
Proof: At each iteration of the while loop, exactly one
edge is added to the current overlay network. Hence the
algorithm will terminate in at most O(|V |2 ) iterations.
Lemma 3. The running time of Low-ODA is O(|V |4 |T |).
Proof: The weight initialization takes O(|V |2 |T |) time.
Updating the weight of each of the remaining edges takes O(1)
time( [11], Lemma 6.4). Finding the edge with max weight
will take at most O(|V |2 ) time. Since the total weight of the
edges is O(|V |2 |T |) at the beginning and greater than 0 at
the end, Low-ODA takes O(|V |2 |T |) · O(|V |2 ) = O(|V |4 |T |)
time.
IV. A PPROXIMATION R ATIO
In this section, we will prove that our overlay design
algorithm (Low-ODA)
approximates the average degree by a
∑
factor O(k·log(∑ v∈V sv )) and the maximum degree by factor
O((n/k) · log( v∈V sv )), where sv = |{t ∈ T |I(v, t) = 1}|.
As we mentioned in the previous section, the main challenge
in the analysis is to overcome the fact that we can no longer

Steps 1-6 of Low-ODA build an initial weighted graph
G′ (V, E ′ , w) on V , where E ′ = V × V and w({u, v}) is

647

think of the algorithm as selecting a sequence of perfect
matchings of the nodes in V when bounding the approximation
ratio on the maximum degree (the analysis of MinMax-ODA
algorithm heavily relied on this “perfect matching behavior”).
Now we present some definitions which will be useful for
the proofs of both Theorems 1 and 2. Recall that we use
sv to denote |{t ∈ T |I(v, t) = 1}|. At the beginning of
the algorithm,
∑ the total number of connected components is
Cstart =
v∈V sv and at the end Cend = |{t|t ∈ T and
∃v ∈ V such that I(v, t) = 1}|. Note that since we count
the connected components for each topic separately, once we
get down to Cend components, there must exist exactly one
component for each active topic t (i.e., each t such that there
exists some v with I(v, t) = 1) — i.e., the overlay network is
topic-connected.

ratio for the classic set cover problem.
However, before we can apply the set cover framework, we
first need to carefully relate the sequence of edges selected by
the Low-ODA algorithm to a sequence of optimal matchings
which reduce the current number of connected components
maximally. Note that we no longer can break the sequence
of edges selected by our algorithm into a sequence of perfect
matchings, as in the MinMax-ODA algorithm.
Assume we have an instance of the Low-TCO problem
and that G(V, Eopt ) is a solution with minimum possible
maximum degree for this instance. Let this maximum degree
be dopt . We will use the following well-known result in graph
theory for the proof.
Lemma 4 ((Lemma 4 in [21])). Given a graph G(V, E) with
maximum degree d, we can partition the edge set E into d + 1
matchings Mi , 1 ≤ i ≤ (d + 1).

Theorem 1. The overlay network output by Low-ODA
has
∑
average node degree within a factor of O(k · log( v∈V sv ))
from the minimum possible average node degree for any topicconnected overlay network on V .

Using the lemma above, we can partition the edge set Eopt
of the optimum solution into dopt + 1 matchings Mi , 1 ≤ i ≤
(dopt + 1).
At the start, all nodes have degree zero. At each iteration of
the while loop, a maximum weight edge among the ones that
increase the maximum degree of the current graph minimally
or a maximum weight edge is added to the set of overlay
edges.
After a number of iterations, the weight of a maximum
weight edge among the ones that increase the maximum degree
of the current graph minimally will be less than the weight
of a maximum weight edge over k and we will add this
maximum weight edge to the graph — this edge will increase
the maximum degree of the graph by 1.
Let Si be the edge set containing all edges added by LowODA from the time an edge e′ increased the maximum degree
of G′ from i − 1 to i until the last time an edge is added to G′
without increasing its maximum degree further (i.e., without
increasing the maximum
+ 1).∪Let h = n/(2k) + 1.
∪ degree to i∪
Let Ri = Sh·(i−1)+1 Sh·(i−1)+2 ... Sh·(i−1)+h — i.e.,
Ri denotes the set of all edges added by Low-ODA while the
resulting maximum
∪
∪ degree
∪ was between h(i−1)+1 and hi. Let
RAi = R1 R2 ... Ri−1 be the union of all edges added
before the algorithm starts adding the set Ri . Let ni be the
total number of connected components before the algorithm
adds Ri , so n1 = Cstart .
The following lemma proves that each set Ri chosen by
our algorithm decreases the current total number of connected
components by at least 1/3 of the number of current components connected by any optimal matching, where an optimal
matching is one that reduces the current number of connected
components maximally among all possible maximal matchings
in G′ . Note that a matching increases the maximum degree of
G′ by at most 1.

Proof: The proof follows the general lines as the proof
of the logarithmic approximation ratio for the classic set cover
problem (which was also the basis for the approximation ratio
proof of the GM algorithm for the MinAv-TCO problem [11]).
Assume we have an instance of the Low-TCO problem and
that G(V, Eopt ) is a solution for this instance with minimum
number of edges. Let |Eopt | = m.
Let ei be the ith edge added to the set by the algorithm
Low-ODA. Let ni be total number of connected components
before we add the ith edge, so n1 = Cstart . Let Si =
{e1 , e2 , ..., ei−1 } be the set of all edges found before the
algorithm starts adding the i-th edge. Before Low-ODA starts
adding the ith edge, we have ni components and we know
that if we add all the edges Eopt − Si , to the current solution,
the total number of connected components will be reduced
to Cend . Since |Eopt − Si | ≤ m, there exists an edge which
decreases the total number of connected components by at
least (ni − Cend )/m. Since our algorithm always adds at least
a (1/k)-optimal edge, the edge ei that our algorithm adds
must decrease the total number of connected components at
that time by at least (1/k) of this amount. Therefore,
ni − ni+1 ≥ (ni − Cend )/(m · k)
⇒ ni+1 − Cend ≤ (1 − 1/(m · k))(ni − Cend ).
Hence, the number of iterations of our algorithm Low-ODA
is less than or equal to the smallest z which satisfies
1 > (n1 − Cend )(1 − 1/(m · k))z
⇒ z ≤ m · k ln(Cstart − Cend )
⇒ z ≤ m · k ln(Cstart ).
Theorem 2. The overlay network output by Low-ODA
has ∑
maximum node degree within a factor of O((n/k) ·
log( v∈V sv )) from the minimum possible maximum node
degree for any topic-connected overlay network on V .

Lemma 5. Joining the set Ri to the edges of G(V, RAi ) reduces the total number of connected components of G(V, RAi )
by at least 1/3 of the number of current components connected
by any optimal matching.

Proof: As with the proof of Theorem 1, the proof follows
the general lines of the proof of the logarithmic approximation

648

Consider the case where el does not increase the maximum
degree of current graph or l = 1 or el increases the maximum
degree of current graph and there is no possible edge which
does not increase maximum degree of the current graph, 1 ≤
l ≤ j. If Xl has two edges, then our algorithm did not choose
one of these two edges at that step and choose el instead,
0 ≤ l ≤ j. Since our algorithm greedily chooses the edges, el
reduces the total number of connected components of Gl−1 by
at least as much as each of the edges in Xl . Hence, yl ≥ xl /2.
Similarly, if Xl has one or zero edges, then yl ≥ xl .
Now consider the case where el increases the maximum
degree of current graph and there are some edges which
does not increase maximum degree of the current graph,
2 ≤ l ≤ j. Xl has at most k edges. Our algorithm did
not choose one of these k edges at that step and choose
el instead, 0 ≤ l ≤ j. Since our algorithm greedily
chooses the edges, el reduces the total number of connected
components of Gl−1 by at least as much as k times any
of the edges in Xl . Since Xl has at most k edges, yl ≥ xl . So,

Proof: Let P be the edge set of the matching that
reduces the total number of connected components of the
G(V, RAi ) by the maximum amount, which we denote by
c. Let Q = {e1 , e2 , . . . , ej } be the edge set of Ri . Let
el = ul vl for 1 ≤ l ≤ j. For ea and eb , if a < b, then ea
is found before eb by our algorithm. Let Q reduce the total
number of connected components ∪of G(V, RAi ) by c′ . Let
G0 = G(V, RAi ) and Gl = Gl−1 {el }, for 1 ≤ l ≤ j. Let
el reduce the total number of connected components of Gl−1
by yl . Then,
c′ =

∑
1≤l≤j

yl

(1)

Consider the case where el does not increase the maximum
degree of the current graph, or l = 1, or el increases the
maximum degree of current graph and there is no possible
edge which does not increase the maximum degree of the
current graph, 1 ≤ l ≤ j. In this scenario, we let Xl be the
set of edges in P which are incident to ul or vl , 1 ≤ l ≤ j,
and not in Xl′ , 1 ≤ l′ ≤ l − 1. Thus, Xl will have zero, one
or two edges for 1 ≤ l ≤ j.
Now consider the case where el increases the maximum
degree of the current graph and there are some edges that
do not increase the maximum degree of the current graph,
2 ≤ l ≤ j. In this case, we define Xl to be the set of the
first k maximum weight edges in P which are not in Xl′ ,
1 ≤ l′ ≤ l − 1. If there are less than k elements in P which
are not in Xl′ , 1 ≤ l′ ≤ l − 1, Xl will only have these edges.
If there are edges which are incident to ul or vl and not in
Xl′ , 1 ≤ l′ ≤ l, then replace any edges from Xl with those
edges (note that there may be at most two edges of this kind).
Let P0 = P and Pl = Pl−1 − Xl for 1 ≤ l ≤ j. Let Xl
reduce the total number of connected components of Gl−1 by
xl for 1 ≤ l ≤ j. Let Pl reduce the total number of connected
components of Gl by cl for 0 ≤ l ≤ j.
If there is an edge el that increases the maximum degree
of current graph and there is no possible edge which does not
increase maximum degree of the current graph, 2 ≤ l ≤ j,
then for each vertex of the graph, there is at least one edge
el′ incident to this vertex, 1 ≤ l′ ≤ l − 1. So, union of
sets Xl′ , 1 ≤ l′ ≤ l − 1, contains all the edges in P . Thus,
Pj = ∅. Now, consider the case when there is no edge which
satisfies these properties (so, when algorithm chooses an
edge el which increases the maximum degree, there is always
an edge which does not increase maximum degree of the
current graph). Since Ri contains h sets of Si′ , there are
h − 1 = n/(2k) edges that increase the maximum degree of
the current graph. Thus there are at least n/(2k) of the sets
′
Xl with k edges each — we call them X1′ , . . . , Xn/(2k)
— (if
Xl has less than k edges than all edges of set P are already
in one of sets Xl′ , 1 ≤ l′ ≤ l − 1, and hence Pj = ∅). The
union of sets Xl′ has at least (n/2k) · k = n/2 edges. On
the other hand, since P is a matching, this union can have at
most n/2 edges. Hence, Pj = ∅. Hence,
c0 = c, cj = 0

yl ≥
⇒

xl
1≤
2 ,∑

l≤j
yl ≥

1≤l≤j

1
2

∑
1≤l≤j

xl

(3)

∪
Since Pl+1 = Pl − Xl+1 and Gl+1 = Gl {el+1 },
0 ≤ l ≤ j − 1, the amount that Pl reduces the total number
of connected components of Gl is smaller than sum of the
amount that Pl+1 reduces the total number of connected
components of Gl+1 and the amount that el+1 reduces the
total number of connected components of Gl and the amount
Xl+1 reduces the total number of connected components of
Gl . Hence,
cl+1 ≥ cl − (xl+1 + yl+1 ) for 0 ≤ l ≤ j − 1

(4)

If we add all the inequalities (2) and (4), we will have
∑
1≤l≤j

xl +

∑
1≤l≤j

yl ≥ c

(5)

From the inequalities (3) and (5), we will have
3

∑
1≤l≤j

yl ≥ c

(6)

From the inequalities (1) and (6), we will have
c′ ≥ c/3
Before Low-ODA starts adding the set Ri , we have ni
components and we know that if we add all the (dopt + 1)
matchings Mj − RAi , 1 ≤ j ≤ (dopt + 1), to the current
solution, the total number of connected components will be
reduced to Cend . Therefore, there exists a matching Mj −RAi
which decreases the total number of connected components
by at least (ni − Cend )/(dopt + 1). Our algorithm always
finds the set Ri that reduces the total number of connected
components of G(V, RAi ) by at least 1/3 of any optimal

(2)

649

matching which reduces the current number of connected
components by the maximum amount (Lemma 5). Hence, the
set Ri that our algorithm uses must decrease the total number
of connected components at that time by at least (1/3) of the
optimal amount. Therefore,
ni − ni+1 ≥ (ni − Cend )/(3(dopt + 1))
⇒
ni+1 − Cend ≤ (1 − 1/(3(dopt + 1)))(ni − Cend ).
Hence, the number of iterations for our algorithm Low-ODA
is less than or equal to the smallest m which satisfies
1 > (n1 − Cend )(1 − 1/(3(dopt + 1)))m
⇒ m ≤ 3(dopt + 1) ln (Cstart − Cend )
⇒ m ≤ 3(dopt + 1) ln(Cstart )

Fig. 1.

Maximum node degree for GM, MinMax-ODA and Low-ODA

Since h = n/(2k)+1, the maximum degree of resulting graph
is less than or equal to (n/(2k) + 1) · 3(dopt + 1) · ln (Cstart ).
V. E XPERIMENTAL R ESULTS
The GM algorithm [12], the MinMax-ODA algorithm [21]
and the Low-ODA algorithm are implemented in Java. These
three algorithms are compared according to maximum degree
and average degree in the resulting overlay graphs. Our
experimental results show that the Low-ODA algorithm has
better maximum degree than the GM algorithm and has
better average degree than the MinMax-ODA algorithm, at
the expense of a small degradation of the other corresponding
degree parameter (recall that the GM algorithm has proven
approximation bounds on the average degree only and the
MinMax-ODA on the maximum degree only).

Fig. 2.

Average node degree for GM, MinMax-ODA and Low-ODA

C. Different Parameters
The experimental setting is similar to previous subsections.
The number of nodes is 100. The parameter k of the Low-ODA
algorithm varies between 1 and 8. When k = 1, the LowODA algorithm behaves basically in the same way as the GM
algorithm, and when k = 8, the Low-ODA algorithm behaves
basically in the same way as the MinMax-ODA algorithm. As
k increases, the maximum degree decreases and the average
degree increases (Figure 3 and Figure 4).

A. Maximum Node Degree
For these experiments, the number of nodes varies between
100 and 1000, and the number of topics is fixed at 100.
Each experiment is done 1000 times. We also fix number of
subscriptions to s = 10. For the Low-ODA algorithm, the
parameter k is chosen
to be equal to 3 (k is chosen as a
√
number close to 8, since the Low-ODA algorithm behaves
pretty much like the MinMax-ODA algorithm when k = 8 –
see results in Section V-C). Each node subscribes to each topic
ti with probability pi . The value of pi is distributed according
to a Zipf distribution (α = 0.5). This experimental setting is
similar to previous studies [12].
Figure 1 presents a comparison of the GM, MinMaxODA and Low-ODA algorithms according to the maximum
degree. When we compare the results of the algorithms, LowODA takes values in between the GM and the MinMax-ODA
algorithms.

VI. C ONSTANT D IAMETER OVERLAYS FOR
P UBLISH -S UBSCRIBE
In this section, we study the following optimization
problem, intially proposed in [21]:
Constant Diameter Topic-Connected Overlay (CD-TCO)
Problem: [21] Given a collection of nodes V , a set of topics
T , and a node–interest assignment I, connect the nodes in
V into a topic-connected overlay network G which has least
possible average degree and constant diameter.

B. Average Node Degree
The experimental setting is the same as in the previous
subsection. Figure 2 is a comparison of the GM, MinMaxODA and Low-ODA algorithms according to the average
degree. When we compare the results of the algorithms, LowODA takes values in between the GM and the MinMax-ODA
algorithms’.

We present two new overlay network construction heuristics
that guarantee constant diameter and topic-connectivity, which
are most important factors for efficient routing. Our heuristic
also aims at keeping the average node degree low.
In [21], a heuristic (CD-ODA) is presented for this problem.
CD-ODA starts with the overlay network G(V, ∅). At each

650

B. Constant Diameter Overlay Design Algorithm II(CD-ODAII)
The second heuristic presented, Constant Diameter Overlay
Design Algorithm II (CD-ODA-II), also starts with the overlay
network G(V, ∅). At each iteration of CD-ODA-II, a node u
which has maximum connection density, du , is chosen. The
connection density of a node u, du , is given by
∑
|{v∈V |Int(v,t)=Int(u,t)=1}|
t∈T
du = |{v∈V
|∃t∈T,Int(v,t)=Int(u,t)=1}| .

Fig. 3.

u
Note that du = w
nu . We add an edge between a node u with
maximum density and each of its neighbors and then remove
the topics in this node’s interest assignment from the set of
topics.

Maximum node degree for different parameters

C. Analysis of Algorithms
Lemma 6. Both CD-ODA-I and CD-ODA-II algorithms terminate within O(|V |2 · |T |) time.
Lemma 7. Both CD-ODA-I and CD-ODA-II algorithms generate a 2-diameter overlay for each topic.
Proof: Since the algorithms generate a star for each topic,
each topic overlay network will have diameter 2.
D. Experimental Results

Fig. 4.

The GM algorithm [11], the CD-ODA algorithm [21] and
the two heuristics presented above are implemented in Java.
These algorithms are compared according to the average
degree in the resulting graph. Experiments are done 1000
times. The diameter is always 2 for our algorithms and for
the CD-ODA algorithm of [21], and it may be θ(n) for the
GM algorithm. When we compare the results of GM, CDODA, CD-ODA-I, and CD-ODA-II according to the average
degree, CD-ODA require at most 2.3 times more edges than
GM, and CD-ODA-II requires at most 1.8 times more edges
than the GM algorithm. The CD-ODA-II algorithm improves
CD-ODA [21] by a factor of 20%.
1) Average Node Degree with Varying Subscription Size:
The number of nodes and the number of topics are fixed to
100. The subscription size varies between 10 and 50. Each
node is interested in each topic uniformly at random. This
experimental setting is similar to previous studies [11], [24].
Figure 5 is a comparison of GM, CD-ODA and our algorithms according to the average degree. The average degree
of the overlay network decreases for both GM, CD-ODA and
our algorithms as the subscription size increases, since the
algorithms can find edges with higher correlations. When we
compare the results of GM and our algorithm CD-ODA-II,
our algorithm requires at most 1.8 times more edges than GM
(Figure 5). CD-ODA-II improves CD-ODA by factor 20% on
average and CD-ODA-I by factor 1% on average.

Average node degree for different parameters

iteration of CD-ODA, a node which has maximum number
neighbors with non-empty interest intersection is chosen.
Number of neighbors is equal to nu = |{v ∈ V |∃t ∈
T, Int(v, t) = Int(u, t) = 1}|. After that, an edge between
this node and each of its neighbors is added and the topics
in this node’s interest assignment are removed from the set of
topics.
We present two new heuristics for this problem when the
diameter is restricted to be equal to 2, validating our heuristics
via experimental results. Our experimental results show that
our heuristics improves CD-ODA [21] by a factor of 20%.

A. Constant Diameter Overlay Design Algorithm I(CD-ODAI)
The first heuristic presented, the Constant Diameter Overlay Design Algorithm I (CD-ODA-I), starts with the overlay
network G(V, ∅). At each iteration of CD-ODA-I, a node u
which has maximum weight neighbors is chosen. The neighbor
weight of a node z is equal to
∑
wz = t∈T |{v ∈ V |Int(v, t) = Int(z, t) = 1}|.

VII. C ONCLUSIONS
In this paper, we study a new optimization problem (LowTCO) that constructs a practical and scalable overlay network
for publish/subscribe communication with many topics. We
present a topic-connected overlay network design algorithm

We add an edge between u and each of its neighbors and
then remove the topics in this node’s interest assignment from
the set of topics.

651

[8] A. Carzaniga, M. J. Rutherford, and A. L. Wolf, A routing scheme for
content-based networking., IEEE INFOCOM 2004, Hon Kong, China,
Mar. 2004.
[9] M. Castro, P. Druschel, A. M. Kermarrec, and A. Rowstron, SCRIBE: a
large-scale and decentralized application-level multicast infrastructure,
IEEE J. Selected Areas in Comm. (JSAC), 20(8):14891499, 2002.
[10] R. Chand and P. Felber, Semantic peer-to-peer overlays for publish/subscribe networks, In Euro-Par 2005 Parallel Processing, Lecture
Notes in Computer Science, volume 3648, pages 11941204. Springer
Verlag, 2005.
[11] G. Chockler, R. Melamed, Y. Tock and R. Vitenberg, Constructing
scalable overlays for pub-sub with many topics, Proc. of the 26th ACM
Symp. on Principles of Distributed Computing (PODC), 2007, pp. 109–
118.
[12] G. Chockler, R. Melamed, Y. Tock, and R. Vitenberg, SpiderCast: A
Scalable Interest-Aware Overlay for Topic-Based Pub/Sub Communication, 1st International Conference on Distributed Event-Based Systems
(DEBS). ACM, 6 2007.
[13] E. De Santis, F. Grandoni, and A. Panconesi. Fast low degree connectivity of ad-hoc networks via percolation. In Proceedings of the European
Symposium on Algorithms (ESA), pages 206-217, 2007.
[14] R. Diestel, Graph Theory, Springer-Verlag, 2nd edition, New York,
2000.
[15] P. T. Eugster, P. A. Felber, R. Guerraoui, and A. M. Kermarrec. The many
faces of publish/subscribe. ACM Computing Surveys, 35(2):114131,
2003.
[16] R. Guerraoui, S. Handurukande, and A. M. Kermarrec, Gossip: a gossipbased structured overlay network for efficient content-based filtering,
Technical Report IC/2004/95, EPFL, Lausanne, 2004.
[17] B. Korte, J. Vygen, Combinatorial Optimization Theory and Algorithms,
Springer-Verlag, 2nd edition, 2000.
[18] L.C. Lau, J. Naor,M.R. Salavatipour, and M. Singh. Survivable Network Design with Degree or Order Constraints. In Procedings of ACM
STOC’07.
[19] R. Levis, Advanced Massaging Applications with MSMQ and MQSeries.
QUE, 1999.
[20] H. Liu, V. Ramasubramanian, and E. G. Sirer. Client behavior and feed
characteristics of rss, a publish-subscribe system for web micronews. In
Internet Measurement Conference (IMC), Berkeley, California, October
2005.
[21] M. Onus, A. W. Richa, Minimum Maximum Degree Publish-Subscribe
Overlay Network Design, 28th Annual IEEE Conference on Computer
Communications (INFOCOM), Rio de Janeiro, Brazil, April 2009.
[22] M. Onus, A. W. Richa, Publish-Subscribe Overlay Network Design, Technical Report, Arizona State University, Department of
Computer Science and Engineering, TR-09-005, 2009, available at
http://sci.asu.edu/news/technical/index.php.
[23] M. Onus, A. W. Richa, Brief Announcement: Parameterized Maximum
and Average Degree Approximation in Topic-based Publish-Subscribe
Overlay Network Design, 21st ACM Symposium on Parallelism in
Algorithms and Architectures (SPAA), Calgary, Canada, August 2009.
[24] V. Ramasubramanian, R. Peterson, and E. G. Sirer. Corona: A High
Performance publish-subscribe system for the world wide web., In NDSI,
2006.
[25] D. Sandler, A. Mislove, A. Post and P. Druschel. FeedTree: Sharing
web micronews with peer-to-peer event notification., In International
Workshop on Peer-to-Peer Systems(IPTPS), 2005.
[26] D. Tam, R. Azimi, and H.-A Jacobsen. Building content-based publish/subscribe systems with distributed hash tables., In 1st International Workshop on Databases, Information Systems, and P2P Computing(DBISP2P), Berlin, Germany, 2003.
[27] Y. Tock, N. Naaman, A. Harpaz and G. Gershindky. Hierarchical clustering of message flows in a multicast data dissemination system., In 17th
IASTED International Conferance Parallel and Distributed Computing
and Systems, pages 320-327, 2005.
[28] S. Voulgaris, E. Riviere, A. M. Kermarrec, and M. van Steen, Sub-2sub: Self-organizing content-based publish subscribe for dynamic large
scale collaborative networks, In IPTPS, 2006.

Fig. 5. Average node degree for GM, CD-ODA, CD-ODA-I and CD-ODA-II

(Low-ODA) which approximates both average and maximum
degree well. We anticipate that the parameterized algorithmic
framework proposed by Low-ODA will be applicable in other
network design domains where, for scalability, it is important
to keep both the maximum degree as well as the average degree of an overlay network low. Examples of such application
domains are in the design of survivable networks [18] and in
wireless networks [13].
As future work, we would like to build upon our CD-ODA-I
and CD-ODA-II algorithms, by formally and experimentally
evaluating the hardness of obtaining a topic-connected overlay
design algorithm which achieves a “good” trade-off between
low diameter and low node degree. This basically amounts
to a bicriteria optimization problem and we have to be able
to “quantify” the relative importance of optimizing over these
two parameters (e.g., in the CD-ODA-I algorithm and the CDODA-II algorithm we restrict our attention to networks of
diameter 2, while aiming at maintaining the average degree
low). Two other important lines for future work would be
to design efficient distributed algorithms for the Low-TCO
problem, and to look at this problem under the line of a
dynamic configuration of the node set V and the interest
assignment I.
R EFERENCES
[1]
[2]

[3]
[4]

[5]

[6]

[7]

Oracle9i Application Developers Guide Advanced Queuing, Oracle,
Redwood Shores, CA.
E. Anceaume, M. Gradinariu, A. K. Datta, G. Simon, and A. Virgillito,
A semantic overlay for self- peer-to-peer publish/subscribe, In ICDCS,
2006.
S. Baehni, P. T. Eugster, and E. Guerraoui, Data-aware multicast. In
DSN, 2004.
R. Baldoni, R. Beraldi, V. Quema, L. Querzoni, and S. T. Piergiovanni,
TERA: Topic-based Event Routing for Peer-to-Peer Architectures, 1st
International Conference on Distributed Event-Based Systems (DEBS).
ACM, 6 2007.
R. Baldoni, R. Beraldi, L. Querzoni, and A. Virgillito, Efficient publish/subscribe through a self-organizing broker overlay and its application
to SIENA, The Computer Journal, 2007.
S. Banerjee, B. Bhattacharjee, and C. Kommareddy, Scalable application
layer multicast, SIGCOMM Comput. Commun. Rev, 32(4):205-217,
2002.
S. Bhola, R. Strom, S. Bagchi, Y. Zhao, and J. Auerbach, Exactly-once
delivery in a content-based publish-subscribe system. In DSN, 2002.

652

1

AntiJam: Efficient Medium Access despite
Adaptive and Reactive Jamming
Andrea Richa1 , Christian Scheideler2 , Stefan Schmid3 , Jin Zhang1
1

arXiv:1007.4389v4 [cs.DS] 3 Mar 2011

2

Computer Science and Engineering, SCIDSE, Arizona State University
Tempe, Arizona, USA; {aricha,jzhang82}@asu.edu
Department of Computer Science, University of Paderborn, D-33102 Paderborn, Germany
scheideler@upb.de
3
Deutsche Telekom Laboratories, TU Berlin, D-10587 Berlin, Germany
stefan@net.t-labs.tu-berlin.de

Abstract—Intentional interference constitutes a major threat for communication networks operating over
a shared medium where availability is imperative.
Jamming attacks are often simple and cheap to implement. In particular, today’s jammers can perform
physical carrier sensing in order to disrupt communication more efficiently, specially in a network of simple
wireless devices such as sensor nodes, which usually
operate over a single frequency (or a limited frequency
band) and which cannot benefit from the use of spread
spectrum or other more advanced technologies. This
article proposes the medium access (MAC) protocol
A NTI JAM that is provably robust against a powerful
reactive adversary who can jam a (1 − ε)-portion
of the time steps, where ε is an arbitrary constant.
The adversary uses carrier sensing to make informed
decisions on when it is most harmful to disrupt
communications; moreover, we allow the adversary to
be adaptive and to have complete knowledge of the
entire protocol history. Our MAC protocol is able to
make efficient use of the non-jammed time periods and
achieves an asymptotically optimal, Θ(1)-competitive
throughput in this harsh scenario. In addition, A N TI JAM features a low convergence time and has good
fairness properties. Our simulation results validate our
theoretical results and also show that our algorithm
manages to guarantee constant throughput where the
802.11 MAC protocol basically fails to deliver any
packets.

I. I NTRODUCTION
Disruptions of the communications over a shared
medium—either because of interference of concurrent transmissions or intentionally—are a central
challenge in wireless computing. It is well-known
that already simple jamming attacks—without any
special hardware—constitute a threat for the widely
used IEEE 802.11 MAC protocol. Due to the problem’s relevance, there has been a significant effort
to cope with such disruption problems both from the

industry and the academia side and much progress
has been made over the last years on how to deal
with different jammer types.
While simple oblivious jammers are wellunderstood today and many countermeasures exist, this article goes an important step further and
studies MAC protocols against “smart” jammers. In
particular, we argue that adversaries may behave in
an adaptive and reactive manner: adaptive in the
sense that their decisions on whether to jam at a
certain moment in time can depend on the protocol
history; and reactive in the sense that the adversary
can perform physical carrier sensing (which is part,
e.g., of the 802.11 standard) to learn whether the
channel is currently idle or not, and jam the medium
depending on these measurements.
This article presents the first medium access
(MAC) protocol called A NTI JAM that makes effective use of the few and arbitrarily distributed
non-jammed time periods, and achieves a provable
throughput despite the presence of such a strong
reactive jammer. As we will see, the throughput is
asymptotically optimal, i.e., a constant fraction of
the non-jammed time period is used for successful transmissions. Besides this interesting theoretic
result, our protocol is simple to implement and
performs well also in the average case. Also, worth
to note is that our approach is at the MAC level
and may be used in conjunction with some of the
anti-jamming techniques developed at the physical
layer (e.g., frequency hopping, spread spectrum).
A. Related Work
Researchers have studied the problem of unintentional and malicious interference in wireless

2

networks for several years now, e.g., [6], [15], [17],
[18], [19], [21], [22], [26], [28]. Classic defense
mechanisms operate on the physical layer [19], [21]
and there exist approaches both to avoid as well as
to detect jamming. Spread spectrum and frequency
hopping technologies have been shown to be very
effective to avoid jamming with widely spread signals. However, IEEE 802.11 variants spread signals
with smaller factors [5] (IEEE 802.11b uses a
narrow spreading factor of 11 [14]). As jamming
strategies can come in many different flavors, detecting jamming activities by simple methods based
on signal strength, carrier sensing, or packet delivery
ratios has turned out to be quite difficult [18].
Recent work has also studied MAC layer strategies against jamming, including coding strategies
(e.g., [6]), channel surfing and spatial retreat (e.g.,
[1], [29]), or mechanisms to hide messages from
a jammer, evade its search, and reduce the impact
of corrupted messages (e.g., [27]). Unfortunately,
these methods do not help against an adaptive
jammer with full information about the history of
the protocol, like the one considered in our work.
In the theory community, work on MAC protocols
has mostly focused on efficiency. Many of these
protocols are random backoff protocols (e.g., [4],
[7], [8], [12], [24]) that do not take jamming activity
into account and, in fact, are not robust against it
(see [2] for more details). Also some theoretical
work on jamming is known (e.g., [9] for a short
overview). There are two basic approaches in the
literature. The first assumes randomly corrupted
messages (e.g. [23]), which is much easier to handle
than adaptive adversarial jamming [3]. The second
line of work either bounds the number of messages
that the adversary can transmit or disrupt with a
limited energy budget (e.g. [11], [16]), or bounds the
number of channels the adversary can jam (e.g. [10],
[20]).
The protocols in, e.g., [16] can tackle adversarial
jamming at both the MAC and network layers,
where the adversary may not only be jamming
the channel but also introducing malicious (fake)
messages (possibly with address spoofing). However, they depend on the fact that the adversarial
jamming budget is finite, so it is not clear whether
the protocols would work under heavy continuous
jamming. (The result in [11] seems to imply that
a jamming rate of 0.5 is the limit whereas the
handshaking mechanisms in [16] seem to require
an even lower jamming rate.)

Our work is motivated by the results in [3] and
[2]. In [3] it is shown that an adaptive jammer can
dramatically reduce the throughput of the standard
MAC protocol used in IEEE 802.11 with only limited energy cost on the adversary side. Awerbuch et
al. [2] initiated the study of throughput-competitive
MAC protocols under continuously running, adaptive jammers, and presented a protocol that achieves
a high performance under adaptive jamming.
In this article, we extend the model and result
from [2] in a crucial way: we allow the jammer
to be reactive, i.e., to listen to the current channel
state in order to make smarter jamming decisions.
We believe that the reactive jammer model is much
more realistic and hence that our study is of practical
importance. For example, by sensing the channel,
the adversary may avoid wasting energy by not
jamming idle rounds. Note however that depending
on the protocol, it may still make sense for the
adversary to jam idle rounds, e.g., to influence the
protocol execution. Indeed, due to the large number
of possible strategies a jammer can pursue, the problem becomes significantly more challenging than the
non-reactive version: Not only is the analysis more
involved, but also key modifications to the protocol
in [2] were needed. While we still build upon the
algorithmic ideas presented in [2], in order to avoid
asymmetries, our A NTI JAM protocol seeks to synchronize the nodes’ sending probabilities. (This has
the desirable side effect of an improved fairness.)
While our formal analysis confirms our expectations
that the overall throughput under reactive jammers
is lower than the throughput obtainable against
non-reactive jammers, we are still able to prove
a good, constant-competitive performance, which
is also confirmed by our simulation study. As a
final remark, although this article focuses on singlehop environments, our first insights indicate that
A NTI JAM-like strategies can also be used in multihop settings (see also the recent extension of [2] to
unit disk graphs [25]).
B. Model
We study a wireless network that consists of n
honest and reliable simple wireless devices (e.g.,
sensor nodes) that are within the transmission range
of each other and which communicate over a single
frequency (or a limited, narrow frequency band).
We assume a back-logged scenario where the nodes
continuously contend for sending a packet on the

3

wireless channel. A node may either transmit a
message or sense the channel at a time step, but it
cannot do both, and there is no immediate feedback
mechanism telling a node whether its transmission
was successful. A node sensing the channel may
either (i) sense an idle channel (in case no other
node is transmitting at that time), (ii) sense a busy
channel (in case two or more nodes transmit at the
time step), or (iii) receive a packet (in case exactly
one node transmits at the time step).
In addition to these nodes there is an adversary.
We allow the adversary to know the protocol and
its entire history and to use this knowledge in order
to jam the wireless channel at will at any time
(i.e, the adversary is adaptive). Whenever it jams
the channel, all nodes will notice a busy channel.
However, the nodes cannot distinguish between the
adversarial jamming or a collision of two or more
messages that are sent at the same time. We assume
that the adversary is only allowed to jam a (1 − ε)fraction of the time steps, for an arbitrary constant
0 < ε ≤ 1.
Moreover, we allow the jammer to be reactive:
it is allowed to make a jamming decision after
it knows the actions of the nodes at the current
step. In other words, reactive jammers can determine (through physical carrier sensing) whether the
channel is currently idle or busy (either because of a
successful transmission, a collision of transmissions,
or too much background noise) and can instantly
make a jamming decision based on that information.
Those jammers arise in scenarios where, for example, encryption is being used for communication and
where the jammer cannot distinguish between an
encrypted package and noise in the channel.
In addition, we allow the adversary to perform
bursty jamming. More formally, an adversary is
called (T, 1 − ε)-bounded for some T ∈ N and
0 < ε < 1 if for any time window of size w ≥ T
the adversary can jam at most (1 − ε)w of the time
steps in that window.
The network scenario described above arises,
for example, in sensor networks, which consist
of simple wireless nodes usually running on a
single frequency and which cannot benefit from
more advanced anti-jamming techniques such as
frequency hopping or spread spectrum. In such
scenarios, a jammer will also most probably run
on power-constrained devices (e.g., solar-powered
batteries), and hence will not have enough power
to continuously jam over time (note that the time

window threshold T can be chosen large enough to
accommodate the respective jamming pattern).
This article studies competitive MAC protocols.
A MAC protocol is called c-competitive against
some (T, 1−ε)-bounded adversary (with high probability or on expectation) if, for any sufficiently
large number of time steps, the nodes manage to
perform successful message transmissions in at least
a c-fraction of the time steps not jammed by the
adversary (with high probability or on expectation).
Our goal is to design a symmetric local-control
MAC protocol (i.e., there is no central authority
controlling the nodes, and the nodes have symmetric
roles at any point in time) that is O(1)-competitive
against any (T, 1−ε)-bounded adversary. The nodes
do not know ε, but we do allow them to have a very
rough upper bound of the number n and T . More
specifically, we will assume that the nodes have a
common parameter γ = O(1/(log T + log log n)).
This is still scalable, since such an estimate leaves
room for a super-polynomial change in n and a
polynomial change in T over time, so it does not
make the problem trivial (as would be the case if
the nodes knew constant factor approximations of n
or T ).
C. Our Contributions
This article introduces and analyzes the medium
access protocol A NTI JAM. A NTI JAM is robust to a
strong adaptive and reactive jammer who can block
a constant fraction of the time and thus models a
large range of (intentional and unintentional) interference models. Nevertheless, we can show that the
A NTI JAM MAC protocol achieves a high throughput
performance by exploiting any non-blocked time intervals effectively. The main theoretical contribution
is the derivation of the following theorem that shows
that A NTI JAM is asymptotically optimal in the sense
that a constant fraction of the non-jammed execution
time is used for successful transmissions:
Theorem I.1.
The MAC protocol is Θ(1)competitive w.h.p.1 under any (T, 1 − ε)-bounded
adversary for some constant ε if the protocol is executed for at least Θ( 1ε log N max{T, εγ12 log3 N })
many time steps.
We believe that A NTI JAM is interesting also from
a practical point of view, as the basic protocol
1 With high probability, i.e., with probability at least 1 − 1/nc ,
where c is a constant. As n grows to infinity, the probability tends
to 1.

4

is very simple. We also report on our simulation
results. It turns out that A NTI JAM is able to benefit
from the rare and hard-to-predict time intervals
where the shared medium is available. Moreover,
A NTI JAM converges fast and allocates the shared
medium fairly to the nodes.
D. Article Organization
The remainder of this article is organized as
follows. Section II introduces the A NTI JAM MAC
protocol. Subsequently, we present a formal analysis
of the throughput performance under reactive jamming (Section III). Section IV reports on the insights
gained from our simulation experiments. The article
is concluded in Section V.
II. T HE A NTI JAM MAC P ROTOCOL
The basic ideas of the A NTI JAM MAC protocol
are inspired by the protocols described in [13]
(which also uses access probabilities depending
on the ratio between idling and successful time
slots) and particularly [2]. However, the algorithm
in [2] does not achieve a good performance under
reactive jammers, which is due to the asymmetric
access probabilities. Therefore, in our protocol, we
explicitly try to equalize access probabilities, which
also improves fairness among the nodes.
Each node v maintains a time window threshold
estimate Tv and a counter cv . The parameter γ is the
same for every node and is set to some sufficiently
small value in O(1/(log T + log log n)). Thus, we
assume that the nodes have some polynomial estimate of T and even rougher estimate of n. Let p̂
be any constant so that 0 < p̂ ≤ 1/24. Initially,
every node v sets Tv := 1, cv := 1 and pv := p̂.
Afterwards, the protocol works in synchronized time
steps. We assume synchronized time steps for the
analysis, but a non-synchronized execution of the
protocol would also work as long as all nodes
operate at roughly the same speed.
The basic protocol idea is simple. Suppose that
each node v decides to send a message at the current
time P
step with probability pv with pv ≤ p̂. Let
p = v pv , q0 be the probability that the channel is
idle and q1 be the probability that exactly one node
is sending a message. The following claim appeared
originally in [2]. It states that if q0 = Θ(q1 ), then the
cumulative sending probability p is constant, which
in turn implies that at any non-jammed time step
we have constant probability of having a successful

transmission. Hence our protocol aims at adjusting
the sending probabilities pv of the nodes such that
q0 = Θ(q1 ), in spite of the reactive adversarial
jamming activity. This will be achieved by using a
multiplicative increase/decrease game for the probabilities pv and by synchronizing all the nodes,
both in terms of sending probabilities and their own
estimates on the time window threshold estimate
Tv ’s, at every successful transmission.
Claim II.1. q0 · p ≤ q1 ≤

q0
1−p̂

· p.

Now we present our A NTI JAM protocol:
In each step, each node v does the following. v
decides with probability pv to send a message along
with a tuple: (pv , cv , Tv ). If it decides not to send
a message, it checks the following two conditions:
1) If v senses an idle channel, then pv :=
min{(1 + γ)pv , p̂} and Tv := Tv − 1.
2) If v successfully receives a message along
with the tuple of (pnew , cnew , Tnew ), then
pv := (1 + γ)−1 pnew , cv := cnew , and
Tv := Tnew .
Afterwards, v sets cv := cv + 1. If cv > Tv then
it does the following: v sets cv := 1, and if there
was no idle step among the past Tv time steps, then
pv := (1 + γ)−1 pv and Tv := Tv + 2.

III. A NALYSIS
Now we restate Theorem I.1 more precisely. We
will prove this more technical version of Theorem I.1. Let N = max{T, n}.
Theorem III.1. The A NTI JAM protocol is
2
e−Θ(1/ε ) -competitive w.h.p. under any (T, 1 − ε)bounded adversary if the protocol is executed for
2
at least Θ( 1ε log N max{T, (eδ/ε /εγ 2 ) log3 N })
many time steps, where δ is a sufficiently large
constant.
In our analysis, we will make use of the following
well-known relations.
Lemma III.2. For all 0 < x < 1 it holds that
e−x/(1−x) ≤ 1 − x ≤ e−x
Lemma III.3. Consider any set of binary random
variables X1 , . . . , Xn . Suppose
are valQ that there Q
ues p1 , . . . , pn ∈ [0, 1] with E[ i∈S Xi ] ≤ i∈S pi
for every set S ⊆ {1, . . . , n}. Then it holds for

5

X =
that

Pn

i=1

Xi and µ =


P[X ≥ (1 + δ)µ] ≤

Pn

i=1

pi and any δ > 0

eδ
(1 + δ)1+δ

µ

δ2 µ

≤ e− 2(1+δ/3) .

Q
If,
Q on the other hand, it holds that E[ i∈S Xi ] ≥
i∈S pi for every set S ⊆ {1, . . . , n}, then it holds
for any 0 < δ < 1 that

µ
2
e−δ
P[X ≤ (1 − δ)µ] ≤
≤ e−δ µ/2 .
(1 − δ)1−δ
Let V be the set of all nodes. Let pt (v) be node
v’s access probability pv at the beginning
of the t-th
P
time step. Furthermore, let pt = v∈V pt (v). Let
I be a time frame consisting of αε log N subframes
2
δ/ε2
I 0 of size f = max{T, αβ
log3 N }, where α,
εγ 2 e
β and δ are sufficiently large constants. Let F =
α
ε log N · f denote the size of I.
First, we will derive some simple facts on the
behavior of A NTI JAM. We then show that given a
certain minimal initial cumulative probability pt in
a subframe, the cumulative probability cannot be
smaller at the end of the subframe. We proceed to
show that A NTI JAM performs well in time periods
in which pt is bounded by δ/ε2 for some constant
δ. Finally, we show that for any jamming strategy,
A NTI JAM has a cumulative probability of pt ≤ δ/ε2
for most of the time, which yields our main theorem.
We start with some simple facts. Fact III.4 shows
that the protocol synchronizes the sending probabilities of the nodes (up to a factor of (1 + γ)), and
that all values cv and Tv are also synchronized.
Fact III.4. Right after a successful transmission
of the tuple (p0 , c0 , T 0 ), (pv , cv , Tv ) = ((1 +
γ)−1 p0 , c0 , T 0 ) for all receiving nodes v and
(pu , cu , Tu ) = (p0 , c0 , T 0 ) for the sending node u.
In particular, for any time step t after a successful
transmission by node u, (cv , Tv ) = (cw , Tw ) for all
nodes v, w ∈ V .
The next fact follows from the protocol and
Fact III.4, and they help one understand how the
cumulative probabilities vary over time with successful transmissions, idle time steps, etc.
Fact III.5. For any time step t after a successful
transmission or a well-initialized state of the protocol (in which (pv , cv , Tv ) = (p̂, 1, 1) for all nodes
v) it holds:
1. If the channel is idle at time t then (i) if pv =
p̂ for all v, then pt+1 = pt ; (ii) if pu = p̂ and

pv = (1 + γ)−1 p̂ for all nodes v 6= u, then pt+1 =
(1 + γ − O(1/n))pt (because all nodes except for u
increase their sending probability by a factor (1+γ)
from p̂/(1 + γ).); or (iii) if pv < p̂ for all nodes v,
then pt+1 = (1 + γ)pt .
2. If there is a successful transmission at time t,
and if cv ≤ Tv or there was an idle time step in
the previous Tv rounds, then (i) if the sender is the
same as the last successful sender, then pt+1 = pt
(because for the sender u, pu (t + 1) = pu (t), and
the other nodes remain at pu (t + 1)/(1 + γ) =
pu (t)/(1 + γ).); if (ii) the sender w is different
from the last successful sender u and pv = p̂ for
all nodes v (including u and w), then pt+1 =
(1 + γ − O(1/n))−1 pt (all nodes except w reduce
their sending probability.); or (iii) if the sender w
is different from the last successful sender u and
pv < p̂ for at least one node v (including u and
w), then pt+1 = (1 + γ)−1 pt (because at time
t, for all nodes v 6= u: pv (t) = pu (t)/(1 + γ);
subsequently, pw (t + 1) = pw (t) and for all nodes
v 6= w: pv (t + 1) = pw (t + 1)/(1 + γ).)
3. If the channel is busy at time t, then pt+1 = pt
when ignoring the case that cv > Tv .
Whenever cv > Tv and there has not been an idle
time step during the past Tv steps, then pt+1 is, in
addition to the actions specified in the two cases
above, reduced by a factor of (1 + γ).
We can now prove the following crucial lemma.
Lemma III.6. For any subframe
I 0 in which ini√
2f
2
), the last√time step t
tially pt0 ≥ 1/(f (1 + γ)
of I 0 again satisfies pt ≥ 1/(f 2 (1 + γ) 2f ), w.h.p.
Proof: We start with the following claim about
the maximum number of times the nodes decrease
their probabilities in I 0 due to cv > Tv .
Claim III.7. If in subframe I 0 the number of idle
time steps is at most k, √
then every node v increases
Tv by 2 at most k/2 + f many times.
Proof: Only idle time steps reduce Tv . If there
is no idle time step during the last Tv many steps,
Tv is increased by 2. Suppose that k = 0. Then the
number of times a node v increases Tv byP2 is upper
`
bounded by the largest possible ` so that i=0 Tv0 +
0
2i ≤ f , where√Tv is the initial size of Tv . For any
Tv0 ≥ 1, ` ≤ f , so the claim is true for k = 0.
At best, each additional idle time step allows us to
reduce all thresholds for v byP
1, so we are searching
`
for the maximum ` so that i=0 max{Tv0 + 2i −

6

√
k, 1} ≤ f . This ` is upper bounded by k/2 + f ,
which proves our claim.
This claim allows us to show the following claim.
Claim III.8. Suppose that √
for the first time step t0
in I 0 , pt0 ∈ [1/(f 2 (1 + γ) 2f ), 1/f 2 ]. Then there
is a time step t in I 0 with pt ≥ 1/f 2 , w.h.p.
Proof: Suppose that there are g non-jammed
time steps in I 0 . Let k0 be the number of these
steps with an idle channel and k1 be the number of
these steps with a successful message transmission.
Furthermore, let k2 be the maximum number of
times a node v increases Tv by 2 in I 0 . If all time
steps t in I 0 satisfy pt < 1/f 2 , then it must hold
that

pt ≤ 1/f 2 . Then
P[Xt = 1]

⇒

k0

≤

k0

≤

p
p
f + k1 + k0 /2 + f
p
4 log1+γ f + 2k1 + 4 f

2 log1+γ f +

√
Suppose that 4 log1+γ f + 4 f ≤ εf /4, which
is true if f = Ω(1/ε2 ) is sufficiently large (which
is true for ε = Ω(1/ log3 N )). Since g ≥ εf due
to our adversarial model, it follows that we must
satisfy k0 ≤ 2k1 + g/4.
Certainly, for any time step t with pt ≤ 1/f 2 ,
P[≥ 1 message transmitted at t] ≤ 1/f 2
Suppose for the moment that no time step is jammed
in I 0 . Then E[k1 ] ≤ (1/f 2 )f = 1/f . In order to
prove a bound on k1 that holds w.h.p., we can use
the general Chernoff bounds stated above. For any
step t, let the binary random variable Xt be 1 if and
only if at least one message is sent at time t and

P[pt ≤ 1/f 2 ] · P[≥ 1 msg sent | pt ≤ 1/f 2 ]

≤

1/f 2

and it particularly holds that for any set S of time
steps prior to some time step t that
Y
P[Xt = 1 |
Xs = 1] ≤ 1/f 2
s∈S

Then, we have
P[

Y

Xs = 1]

=

P[X1 = 1] · P[X2 = 1|X1 = 1]

·

P[X3 = 1|

s∈S

Y

Xs = 1]

s=1,2

·...·
·

P[X|S| = 1|

Y

Xs = 1]

s=1,2,...,|S|−1

k0 − log1+γ (1/pt0 ) ≤ k1 + k2 .
This is because no v has reached a point with
pt (v) = p̂ in this case, so Fact III.5 implies that
for each time step t with an idle channel, pt+1 =
(1 + γ)pt . Thus, at most log1+γ (1/pt0 ) time steps
with an idle channel would be needed to get pt to
1/f 2 , and then there would have to be a balance
between further increases (that are guaranteed to be
caused by an idle channel) and decreases (that might
be caused by a successful transmission or the case
cv > Tv ) of pt in order to avoid the case pt ≥ 1/f 2 .
The number of times we can allow an idle channel is
maximized if all successful transmissions and cases
where cv > Tv cause a reduction of pt . So we need
k0 − log1+γ (1/pt0 ) ≤ k1 + k2 to hold to avoid the
case pt ≥ 1/f 2 somewhere in I 0 .
√
We know from Claim III.7 that k2 ≤ k0 /2 + f .
Hence,

=

≤

(1/f 2 )|S|

and
E[

Y

s∈S

Xs = 1] = P[

Y

Xs = 1] ≤ (1/f 2 )|S|

s∈S

Thus, the Chernoff
P bounds and our choice of f
imply that either t∈I 0 Xt < εf /4 and pt ≤ 1/f 2
throughout I 0 w.h.p., or there must be a time step t
in I 0 with pt > 1/f 2 which would finish the proof.
Therefore, unless pt > 1/f 2 at some point in I 0 ,
k1 < εf /4 and k0 > (1 − ε/4)f w.h.p. As the
reactive adversary can now reduce k0 by at most
f − g when leaving g non-jammed steps, it follows
that for any adversary, k0 > (1 − ε/4)f − (f −
g) = g − (ε/4)f . That, however, would violate our
condition above that k0 ≤ 2k1 + g/4 as that can
only hold given the bounds on g and k1 if k0 ≤
g − (ε/4)f .
Note that the choice of g is not oblivious as
the adversary may adaptively decide to set g based
on the history of events. Hence, we need to sum
up the probabilities over all adversarial strategies
of selecting g in order to show that none of them
succeeds, but since there are only f many, and for
each the claimed property holds w.h.p., the claim
follows.
Similar to this claim, we can also prove the
following claim.
Claim III.9. Suppose that for the first time step t0
in I 0 , pt0 ≥ 1/f 2 . Then there is no time step t in
1 √
I 0 with pt < f 2 (1+γ)
2f , w.h.p.
Proof: Consider some fixed time step t in I 0
and let I 00 = (t0 , t]. Suppose that there are g nonjammed time steps in I 00 . If g ≤ β log N for a

7

(sufficiently large) constant β, then it follows for the
probability pt at the end of I 00 due to Claim III.7
that
√
1
1
√
pt ≥ 2 · (1 + γ)−(2β log N + f ) ≥ 2
f
f (1 + γ) 2f

I 0 because of cv > Tv . Recall that k = k0 + k1 .
Moreover, the following claim holds:

given that ε = Ω(1/ log3 N ), because at most
β log N decreases of pt can happen due to a suc√
cessful transmission and at most β log N/2 + f
further decreases of pt can happen due to exceeding
Tv .
So suppose that g > β log N . Let k0 be the
number of these steps with an idle channel and k1 be
the number of these steps with a successful message
transmission. Furthermore, let k2 be the maximum
number of times a node v increases Tv in I 00 . If
1 √
pt < f 2 (1+γ)
2f then it must hold that

where k10 is the number of useful time steps with
a successful transmission in which the sender is
different from the previously successful sender.

k0 ≤ k1 + k2
√
Since k2√ ≤ k0 /2 + f , this implies that k0 ≤
2k1 + 2 f ≤ 2k1 + g/4. Thus, we are back to
the case in the proof of Claim III.8, which shows
that k0 ≤ 2k1 + g/4 does not hold w.h.p., given that
g > β log N and we never have the case in I 00 that
pt > 1/f 2 .
If there is a step t0 in I 00 with pt0 > 1/f 2 , we
prune I 00 to the interval (t0 , t] and repeat the case
distinction above. As there are at most f time steps
in I 00 , the claim follows.
Combining Claims III.8 and III.9 completes the
proof of Lemma III.6.
Lemma III.10 shows that for times of low cumulative probabilities, A NTI JAM yields a good performance.
0

Lemma III.10. Consider any subframe I , and let
δ > 1 be a sufficiently large constant. Suppose√that
at the beginning
of I 0 , pt0 ≥ 1/(f 2 (1 + γ) 2f )
√
and Tv ≤ F /2 for every node v. If pt ≤ δ/ε2
for at least half of the non-jammed time steps in
2
I 0 , then A NTI JAM is at least 8(1−δp̂)ε2 e−δ/(1−p̂)ε competitive in I 0 .
Proof: A time step t in I is called useful if we
either have an idle channel or a successful transmission at time t (i.e., the time step is not jammed and
there are no collisions) and pt ≤ δ/ε2 . Let k be the
number of useful time steps in I 0 . Furthermore, let
k0 be the number of useful time steps in I 0 with an
idle channel, k1 be the number of useful time steps
in I 0 with a successful transmission and k2 be the
maximum number of times a node v reduces pv in

Claim III.11. If n ≥ (1 + γ)δ/(ε2 p̂), then
k0 − log1+γ (δ/(ε2 · pt0 )) ≤ k10 + k2

Proof: According to Fact III.5, pv ∈ [(1 +
γ)−1 p, p] for some access probability p for all time
steps in I 0 . Hence, if pt ≤ δ/ε2 and n ≥ (1 +
γ)δ/(ε2 p̂), then pv (t) ≤ p̂/(1 + γ). This implies
that whenever there is a useful time step t ∈ I with
an idle channel, then pt+1 = (1+γ)pt . Thus, it takes
at most log1+γ (δ/(ε2 · pt0 )) many useful time steps
with an idle channel to get from pt0 to a cumulative
probability of at least δ/ε2 . On the other hand,
each of the k10 successful transmissions reduces the
cumulative probability by (1 + γ). Therefore, once
the cumulative probability is at δ/ε2 , we must have
k0 ≤ k10 + k2 since otherwise there must be at
least one useful time step where the cumulative
probability is more than δ/ε2 , which contradicts the
definition of a useful time step.
√
Since pt0 ≥ 1/(f 2 (1 + γ) 2f ) it holds that
p
log1+γ (δ/(ε2 · pt0 )) ≤ log1+γ (δf 2 /ε2 ) + 2f
From
Lemma III.7 we also know that k2 ≤ k0 /2 +
√
f . Hence,
k0

≤
≤

p
p
2k10 + 2 · log1+γ (δf 2 /ε2 ) + 2 · ( f + 2f )
p
2k10 + 6 f

if f is sufficiently large. Also, k0√= k−k1 and k10 ≤
k1 . Therefore, k − k1 ≤ 2k1 + 6 f or equivalently,
p
k1 ≥ k/3 − 2 f
It remains to find a lower bound for k.
Claim III.12. Let g be the number of non-jammed
time steps t in I 0 with pt ≤ δ/ε2 . If g ≥ εf /2 then
k≥

2
δ
e−δ/(1−p̂)ε · g
2(1 − p̂)ε2

w.h.p.
Proof: Consider any (T, 1−ε)-bounded jammer
for I 0 . Suppose that of the non-jammed time steps t
with pt ≤ δ/ε2 , s0 have an idle channel and s1 have
a busy channel. It holds that s0 + s1 = g ≥ εf /2.
For any one of the non-jammed time steps with an
idle channel, the probability that it is useful is one,
and for any one of the non-jammed time steps with

8

a busy channel, the probability that it is useful (in
this case, that it has a successful transmission) is at
least
X

pv

v

Y

(1 − pw )

≥

w6=v

≥
=
=

1 X Y
pv
(1 − pw )
1 − p̂ v
w
1 X Y −pw /(1−p̂)
pv
e
1 − p̂ v
w
1 X
pv e−p/(1−p̂)
1 − p̂ v
p
e−p/(1−p̂)
1 − p̂

where p is the cumulative probability at the step.
Since pt ≤ δ/ε2 , it follows that the probability of a
busy time step to be useful is at least
2
δ
e−δ/(1−p̂)ε
(1 − p̂)ε2

Thus,
E[k] ≥ s0 +
≥

2
δ
e−δ/(1−p̂)ε s1
(1 − p̂)ε2

2
δ
e−δ/(1−p̂)ε · g
2
(1 − p̂)ε

since k is minimized for s0 = 0 and s1 = g.
Since our lower bound for the probability of
a busy step to be useful holds independently for
all non-jammed busy steps t with pt ≤ δ/ε2 and
E[k] ≥ α log N for our choice of g, it follows from
the Chernoff bounds that k ≥ E[k]/2 w.h.p.
From Claim III.12 it follows that
p
2
δ
k1 ≥ (
e−δ/(1−p̂)ε · g)/3 − 2 f
2
2(1 − p̂)ε
w.h.p., which completes the proof of Lemma III.10.
It remains to consider the case that for less than
half of the non-jammed time steps t in I 0 , pt ≤
δ/ε2 . Fortunately, this does not happen w.h.p.
0
Lemma
√ III.13. Suppose that at the beginning of I ,
Tv ≤ F /2 for every node v. Then at most half of
the non-jammed time steps t can have the property
that pt > δ/ε2 w.h.p.

Proof: Recall from Fact III.5 that as long as the
access probabilities of the nodes do not hit p̂, the
cumulative probability only changes by a (1 + γ)factor in both directions. Suppose that δ is selected
so that δ/ε2 represents one of these values. Let H
be the set of time steps t ∈ I 0 with the property
that either pt = δ/ε2 and the channel is idle or
pt ≥ (1 + γ)δ/ε2 . Now, we define a step t to be
useful if t ∈ H and there is either an idle channel or

a successful transmission at t. Let k be the number
of useful time steps in H. Furthermore, let k0 be the
number of useful time steps with an idle channel, k1
be the number of useful time steps with a successful
transmission and k2 be the maximum number of
times a node v reduces pv in H because of cv > Tv .
It holds that k = k0 + k1 .
Let us cut the time steps in H into passes where
each pass (t, p, S) consists of a time step t with
pt = p in which there is an idle channel (or t is the
beginning of I 0 if there is no such idle channel in
I 0 ) and S is the sequence of all non-idle time steps
t0 > t with pt0 = (1 + γ)p following t until a time
step t00 is reached in which pt00 < p (or the end of
I 0 is reached if there is no such step). t00 is either
due to cv > Tv or a successful transmission. More
precisely, we require that for any pair of passes
(t, p, S) and (t0 , p0 , S 0 ) with p0 = p and final time
step t00 in S, (t0 ∪ S 0 ) ∩ [t, t00 ] = ∅, but passes with
p 6= p0 are allowed to violate this (by one being
nested into the other). It is not difficult to see that for
any distribution of cumulative probabilities over the
time steps of I 0 one can organize the time steps in H
into passes as demanded above. Based on that, the
following claim can be easily shown, where k10 ≤ k1
is the number of useful time steps with a successful
transmission by a node different from the previously
successful node.
Claim III.14.
k0 ≥ k10 − log1+γ max{p0 /(δ/ε2 ), 1}
where p0 is the initial cumulative probability in I 0 .
This is because there can be at most
log1+γ max{p0 /(δ/ε2 ), 1} many passes not
starting with an idle step but the initial step of
I 0 , and every pass has at most one step counting
towards k10 . This also implies the following claim.
Claim III.15. For any collection P of passes,
k0 ≥ k10 − ∆
where k0 and k10 are defined w.r.t. these passes and
∆ is the number of different p-values in P .
Also, the following claim holds.
Claim III.16.
√
|H| ≤ (k + log1+γ max{p0 /(δ/ε2 ), 1}) F
where p0 is the initial cumulative probability in I 0 .

9

√
0
Proof: If at the beginning
√ of I , Tv ≤ F /2
for every node v, then Tv ≤ F for every node
√v
at any time during I 0 . Hence, after at most 2 F
non-useful steps we run into the situation that cv >
Tv for every node v, which reduces the cumulative
probability by a factor of (1 + γ). Given that we
only have k useful steps and we may initially start
with a probability p0 > δ/ε2 , there
√ can be at most
(k + log1+γ max{p0 /(δ/ε2 ), 1}) F time steps in
H, which proves the claim.
For the calculations below recall the definition of
f with the constants α and β that are assumed to
be sufficiently large. If k ≤ α log N , then it follows
from Claim III.16 that
√
|H| ≤ (α log N + log1+γ N ) F ≤ εf /β
Thus, the number of non-jammed time steps in H
is also at most εf /β, and since β can be arbitrarily
large, Lemma III.13 follows.
It remains to consider the case that k > α log N .
Let us assume that H contains at least εf /2 nonjammed time steps. Our goal is to contradict that
statement in order to show that the lemma is true.
For this we will show that Claim III.15 is violated
w.h.p.
Let Tp be the number of all time steps cov0 0
0
0
ered
P by passes (t , p , S ) with p = p. Certainly,
T
=
|H|.
Let
a
pass
(t,
p, S) be called
p≥δ/ε2 p
bad if the jamming rate in S is more than (1−ε/8).
A cumulative probability p is called bad if the
number of time steps covered by bad passes in p
is more than (1 − ε/8)Tp . A bad p contains at least
(1 − ε/8)2 Tp jammed time steps. Since the number
of jammed time steps in H is at most |H| − εf /2
it holds that
X
(1 − ε/8)2 Tp ≤ |H| − εf /2
p bad

For a cumulative probability p ≥ Φ, P[idle | p] ≤
e−Φ = (log N )/f and P[success | p] ≤ Φe−Φ ≤
ln(f / log N ) · (log N )/f . Hence, k ≤ ln f · log N
on expectation, and from the Chernoff bounds it
follows that k ≤ 2 ln f ·log N w.h.p., so Claim III.16
implies that the number of time steps in I 0 with
cumulative probability p ≥ Φ is at most
√
(2 ln f · log N + log1+γ N ) F ≤ εf /β
If we sum up over all non-helpful probabilities p
with φ ≤ p < Φ, they cover at most
log1+γ Φ

X
i=0

many time steps, so
X

Tp

if β is large enough. If φ ≤ pt < Φ and Φ ≤ 1/γ
(which is true if γ = O(1/(log T + log log n)) is
small enough), then it holds for any time step t0
with pt0 ≤ (1 + γ)pt that
P[successful transmission at t0 ]
X
Y
=
pv (t0 )
(1 − pw (t0 ))
v

X

≥

X

≥

|H| − (1 − ε/8)−2 (|H| − εf /2)

≥

f − (1 − ε/8)−2 (f − εf /2) ≥ εf /4

p good

≥
=

In the following, let φ = δ/ε2 and Φ =
ln(f / log N ). For each p ≥ φ let bp be the number
non-idle time steps among the Tp time steps associated with p-passes and k0,p be the number of idle
time steps associated with p-passes. A good probability p is called helpful if bp ≥ k0,p /P[idle | p] and
p < Φ.

Y

(1 − (1 + γ)pw (t))

w6=v

(1 + γ)pt −(1+γ)pt −2p2 /n
t
e
1 − p̂
(1 + γ)pt −pt −2
e
1 − p̂
(1 + γ)δ
P[idle channel at t]
(1 − p̂)e2 ε

Tp

p bad

(1 + γ)pv (t)

X (1 + γ)pv (t) Y
(1 − (1 + γ)pw (t))
1 − p̂
w
v
P
X (1 + γ)pv (t)
e− w (1+γ)pw (t)/(1−(1+γ)pw (t))
1
−
p̂
v
X (1 + γ)pv (t)
e−(1+γ)pt /(1−(1+γ)pt /n)
1 − p̂
v

≥

≥

|H| −

w6=v

v

=

=

Tp ≥ εf /6

p helpful

Hence, it holds for the good probabilities that
X

i

1/e−(1+γ) ≤ 2 · f / log N = o(f )

≥

Let k1,p be the number of successful time steps associated with p-passes. For each helpful probability
p it holds that E[k1,p ] is at least
ε/8
· P[success | (1 + γ)p]
P[idle | p]
ε/8
(1 + γ)δ
ε/8 · k0,p ·
·
· P[idle | p]
P[idle | p] (1 − p̂)e2 ε2
2

ε/8 · k0,p ·
≥
≥

10

large constant. Hence,
Pif δ is a sufficiently
P
k
≥
2
p helpful 1,p
p helpful k0,p and since
X
Tp · P[success | (1 + γ)p]

in I 0 satisfy pt ≤ δ/ε2 w.h.p. Hence, for all (T, 1 −
ε)-bounded jamming strategies, there are at least

p helpful

useful time steps in I 0 w.h.p.
Due to the lower bound
√
of pt ≥ 1/(f 2 (1 + γ) f ) for all time steps in I
w.h.p. we can also conclude that

−Φ/(1−Φ/n)

≥

(εf /6) · Φ · e

≥

(εf /6) · Φ · e−Φ−1

=

(εf /6) · ln(f / log N ) · (e log N )/f ≥ c log N

for
bounds imply that
P any constant c, the Chernoff
P
k
≥
(3/2)
p helpful 1,p
p helpful k0,p w.h.p. In
order to proceed, we need the following claim.
Claim III.17. For any collection P of passes it
holds that
E[k10 ] ≥ (1 − (1 + γ)/n)k1
where k1 and k10 are defined w.r.t. P .
Proof: Because of Fact III.5, the probability
that a successful transmission is done by a node
different from the node of the last successful transmission is equal to
1−

1+γ
(1 + γ)p
≥1−
.
(n + γ)p
n

To see this, observe that among the cumulative
probability p, if the last sender u has a share
pu (t) = x, all other nodes v have a share x/(1+γ),
and
P

x
pu (t)
1+γ
=
=
x
p
(t)
(n
−
1)
·
n
+γ
+
x
v∈V v
1+γ

Hence, E[k10 ] ≥ (1 − (1 + γ)/n)k1 .
The claim implies that
X
p helpful

0
k1,p
>

X

k0,p + log1+γ max{Φ/(δ/ε2 ), 1}

p helpful

w.h.p., which violates Claim III.15. This completes
the proof of Lemma III.13.
Notice√that by the choice of f and F , Tv never
exceeds F /2 for any v when initially Tv = 1 for
all v. Hence, the prerequisites of the lemmas are
satisfied. We can also show the following lemma,
which shows that Tv remains bounded over time.
Lemma III.18.√ For any time frame I in √
which
initially Tv ≤ F /2 for all v, also Tv ≤ F /2
for all v at the end of I w.h.p.
Proof: We already know that in each subframe
I 0 in I, at least εf /2 of the non-jammed time steps t

2

(δ/ε2 ) · e−δ/ε · εf /2

√

k0 ≥ k10 + k2 − log1+γ ((δ/ε2 ) · f 2 (1 + γ)

f

)

Because of Claims III.7 and III.17 it follows that
k0 ≥ k1 /3
2

w.h.p. Since k0 + k1 = k and k ≥ (δ/ε2 ) · e−δ/ε ·
εf /2 it follows that k0 = Ω(f ). Therefore, there
must be at least one time point in I 0 with T√
v = 1
for all v ∈ V . This in turn ensures that Tv ≤ F /2
for all v at the end of I w.h.p.
With Lemma III.18, we show that Lemma III.13
is true for a polynomial number of subframes. Then,
Lemma III.13 and Lemma III.18 together imply that
Lemma III.10 holds for a polynomial number of
subframes. Hence, our main Theorem III.1 follows.
Along the same line as in [2], we can show that
A NTI JAM is self-stabilizing, so the throughput result
can be extended to an arbitrary sequence of time
frames.
IV. S IMULATION
We have implemented a simulator to study additional properties of our protocol. This section reports on some of our results. Our focus here is on the
qualitative nature of the performance of A NTI JAM,
and we did not optimize the parameters to obtain the
best constants. We consider three different jamming
strategies for a reactive jammer that is (T, 1 − ε)bounded, for different ε values and where T = 100:
(1) one that jams busy channels with probability
(1 − ε); (2) one that jams busy channels deterministically (as long the jamming budget is not used up);
(3) one that jams idle channels deterministically (as
long as the jamming budges is not used up).
We define throughput as the number of successful
transmissions over the number of non-jammed time
steps.
A. Throughput
In a first set of experiments we study the throughput as a function of the network size and ε. We
evaluate the throughput performance for each type
of adversary introduced above, see Figure 1. For all

11

Fig. 1. Throughput under three different jamming strategies as a function of the network size and ε, where p̂ = 1/24 (left: ε = 0.5,
right: ε = 0.3).

Fig. 2.

Throughput as a function of γ under three different jamming strategies. Left: ε = 0.2, Right: ε = 0.5).

three strategies, the throughput is basically constant,
independently of the network size; this is in accordance with our theoretical insight of Theorem III.1.
We can see that given our conditions on ε and T , the
strategy that jams busy channels deterministically
results in the lowest throughput. Hence, in the
remaining experiments described in this section, we
will focus on this particular strategy. As expected,
jamming idle channels does not affect the protocol
behavior much.
In our simulations, A NTI JAM makes effective use
of the non-jammed time periods, yielding 20%-40%
successful transmissions even without optimizing
the protocol parameters. In additional experiments
we also studied the throughput as a function of γ,
see Figure 2. As expected, the throughput declines
slightly for large γ, but this effect is small. (Note
that for very small γ, the convergence time becomes
large and the experiments need run for a long time
in order not to underestimate the real throughput.)

of the cumulative probability over time. It can be
seen that the protocol converges quickly to constant
access probabilities. (Note the logarithmic scale.)
If the initial probability for each node is high, the
protocol needs more time to bring down the lowconstant cumulative probability. Moreover, the ratio
of the time period the cumulative probability is in
1 2
, ε ] to the time period the protocol
the range of [ 2ε
being executed is 92.98% when p̂ = 1/24, and
89.52% when p̂ = 1/2. This implies that for a sufficiently large time period, the cumulative probability
is well bounded most of the time, which corresponds
to our theoretical insights. Figure 4 studies the
convergence time for different network sizes. We ran
the protocol 50 times, and assume that the execution
has converged when the cumulative probability p
satisfies p ∈ [0.1, 10], for at least 5 consecutive
rounds. The simulation result also confirms our theoretical analysis in Theorem III.1, as the number of
rounds needed to converge the execution is bounded
by Θ( 1ε log N max{T, εγ12 log3 N }).

B. Convergence Time
Besides a high throughput, fast convergence is
the most important performance criterion of a MAC
protocol. The traces in Figure 3 show the evolution

Figure 5 indicates that independently of the initial
values p̂ and Tv , the throughput rises quickly (up
above 20%) and stays there afterwards.

12

Fig. 5. Convergence in a network of 1000 nodes where ε = 0.5.
Fig. 3. Evolution of cumulative probability over time (network
size is 1000 nodes, and ε = 0.5). Note that the plot has
logarithmic scale.

Fig. 6. Fairness in a network of 1000 nodes, where ε = 0.5,
and p̂ = 1/24 (averaged over 10 runs).
Fig. 4. A NTI JAM runtime as a function of network size for
p̂ = 1/24, and ε = 0.5.

C. Fairness
As A NTI JAM synchronizes cv , Tv , and pv values
upon message reception, the nodes are expected to
transmit roughly the same amount of messages; in
other words, our protocol is fair. Figure 6 presents a
histogram showing how the successful transmissions
are distributed among the nodes. More specifically,
we partition the number of successful transmissions
into intervals of size 4. Then, all the transmissions
are grouped according to those intervals in the
histogram.
D. Comparison to 802.11
Finally, to put A NTI JAM into perspective, as a
comparison, we implemented a simplified version
of the widely used 802.11 MAC protocol (with a
focus on 802.11a).
The configurations for the simulation are the
following: (1) the jammer is reactive and (T, 1 − ε)bounded; (2) the unit slot time for 802.11 is set to
50µs; for simplicity, we define one time step for

A NTI JAM to be 50µs also; (3) we run A NTI JAM
and 802.11 for 4 min, which is equal to 4.8 · 106
time steps in our simulation; (4) the backoff timer
of the 802.11 MAC protocol implemented here
uses units of 50µs; (5) we omit SIFS, DIFS, and
RTS/CTS/ACK.
A comparison is summarized in Figure 7. The
throughput achieved by A NTI JAM is significantly
higher than the one by the 802.11 MAC protocol,
specially for lower values of ε, when the 802.11
MAC protocol basically fails to deliver any successful message.
V. C ONCLUSION
This article presented a simple distributed MAC
protocol called A NTI JAM that is able to make efficient use of a shared communication medium whose
availability is changing quickly and in a hard to
predict manner over time. In particular, this article
has shown that the MAC protocol is able to achieve
a good (asymptotically optimal) throughput even
against an adaptive and reactive jammer that uses
carrier sensing for an informed decision on when to
jam, and whose strategy can depend on the entire

13

Fig. 7. Throughput as a function of ε ∈ [0.05, 0.95], compared
to 802.11, averaged over 10 runs, where p̂ = 1/24.

protocol history. Our simulation results indicate that
the nodes’ access probabilities converge quickly to
a good cumulative value and yields a fair allocation
of the shared medium among the nodes.
R EFERENCES
[1] G. Alnifie and R. Simon. A multi-channel defense against
jamming attacks in wireless sensor networks. In Proc. of
Q2SWinet ’07, pages 95–104, 2007.
[2] B. Awerbuch, A. Richa, and C. Scheideler. A jammingresistant MAC protocol for single-hop wireless networks.
In Proc. of PODC ’08, 2008.
[3] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and B. Thapa. On the performance of IEEE 802.11
under jamming. In Proc. of IEEE Infocom ’08, pages 1265–
1273, 2008.
[4] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul,
and C. E. Leiserson. Adversarial contention resolution for
simple channels. In Proc. of SPAA ’05, pages 325–332,
2005.
[5] T. Brown, J. James, and A. Sethi. Jamming and sensing of
encrypted wireless ad hoc networks. In Proc. of MobiHoc
’06, pages 120–130, 2006.
[6] J. Chiang and Y.-C. Hu. Cross-layer jamming detection
and mitigation in wireless broadcast networks. In Proc. of
MobiCom ’07, pages 346–349, 2007.
[7] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki. Adversarial queuing on the multiple-access channel. In Proc.
of PODC ’06, pages 92–101, 2006.
[8] A. Czumaj and W. Rytter. Broadcasting algorithms in radio
networks with unknown topology. Journal of Algorithms,
60(2):115 – 143, 2006.
[9] S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski, C. Newport, F. Kuhn, and N. Lynch. Reliable distributed computing
on unreliable radio channels. In Proc. 2009 MobiHoc S3
Workshop, 2009.
[10] S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C.
Newport. Interference-resilient information exchange. In
Proc. 28th IEEE International Conference on Computer
Communications (INFOCOM), 2009.
[11] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious
motes and suspicious sensors: On the efficiency of malicious interference in wireless networks. In Proc. of
OPODIS ’06, 2006.
[12] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff
protocols for mulitiple accesschannels. SIAM Journal on
Computing, 25(4):740–774, 1996.

[13] M. Heusse, F. Rousseau, R. Guillier, and A. Duda. Idle
sense: An optimal access method for high throughput and
fairness in rate diverse wireless lans. In Proc. Conference
on Applications, Technologies, Architectures, and Protocols
for Computer Communications (SIGCOMM), pages 121–
132, 2005.
[14] IEEE. Medium access control (MAC) and physical specifications. In IEEE P802.11/D10, 1999.
[15] S. Jiang and Y. Xue. Providing survivability against
jamming attack via joint dynamic routing and channel
assigment. In Proc. 7th Workshop on Design of Reliable
Communication Networks (DRCN), 2009.
[16] C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable
broadcast in radio networks: The bounded collision case.
In Proc. of PODC ’06, 2006.
[17] Y. Law, L. van Hoesel, J. Doumen, P. Hartel, and
P. Havinga. Energy-efficient link-layer jamming attacks
against wireless sensor network MAC protocols. In Proc.
of SASN ’05, pages 76–88, 2005.
[18] M. Li, I. Koutsopoulos, and R. Poovendran. Optimal
jamming attacks and network defense policies in wireless
sensor networks. In Proc. of Infocom ’07, pages 1307–
1315, 2007.
[19] X. Liu, G. Noubir, R. Sundaram, and S. Tan. Spread:
Foiling smart jammers using multi-layer agility. In Proc.
of Infocom ’07, pages 2536–2540, 2007.
[20] D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer.
Speed dating despite jammers. In Proc. DCOSS ’09, June
2009.
[21] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein. Using
channel hopping to increase 802.11 resilience to jamming
attacks. In Proc. of Infocom ’07, 2007.
[22] R. Negi and A. Perrig. Jamming analysis of MAC protocols. Technical report, Carnegie Mellon University, 2003.
[23] A. Pelc and D. Peleg. Feasibility and complexity of
broadcasting with random transmission failures. In Proc.
of PODC ’05, 2005.
[24] P. Raghavan and E. Upfal. Stochastic contention resolution
with short delays. SIAM Journal on Computing, 28(2):709–
719, 1999.
[25] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A
jamming-resistant mac protocol for multi-hop wireless networks. In Proc. 24th International Symposium on Distributed Computing (DISC), 2010.
[26] D. Thuente and M. Acharya. Intelligent jamming in
wireless networks with applications to 802.11b and other
networks. In Proc. of MILCOM ’06, 2006.
[27] A. Wood, J. Stankovic, and G. Zhou. DEEJAM: Defeating
energy-efficient jamming in IEEE 802.15.4-based wireless
networks. In Proc. of SECON ’07, 2007.
[28] W. Xu, W. Trappe, Y. Zhang, and T. Wood. The feasibility
of launching and detecting jamming attacks in wireless
networks. In Proc. of MobiHoc ’05, pages 46–57, 2005.
[29] W. Xu, T. Wood, and Y. Zhang. Channel surfing and spatial
retreats: defenses against wireless denial of service. In
Proc. of Workshop on Wireless Security, 2004.

1

Editorial to the Special Issue on SODA’12

This issue of Transactions on Algorithms contains five articles from the 23rd Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA’12), which was held January
17–19 in Kyoto, Japan. The articles were selected by members of the program committee, who judged them, based on the review, discussion, and presentation of their
scientific contribution, to be among the best articles in this conference. The authors,
upon invitation, submitted a full version of their article. This version was subjected
to the standard review process of the journal and accepted, as usual, based on the
reviewer’s recommendations and comments. We thank all the people who took part
in this endeavor: the authors for their indispensable contributions, the reviewers for
their thorough and useful reading of the submissions, and the journal staff for their
abundant support in the process.
Yuval Rabani
Andrea Richa
Jared Saia
David P. Woodruff
Guest Editors

c 2016 ACM 1549-6325/2016/02-ART1 $15.00

DOI: http://dx.doi.org/10.1145/2846001

ACM Transactions on Algorithms, Vol. 12, No. 1, Article 1, Publication date: February 2016.

200

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

A Modular Algorithm-Theoretic Framework for
the Fair and Efficient Collaborative Prefetching of
Continuous Media
Soohyun Oh, Yo Huh, Beshan Kulapala, Goran Konjevod, Andrea W. Richa, and Martin Reisslein

Abstract—Bursty continuous media streams with periodic
playout deadlines (e.g., VBR-encoded video) are expected to
account for a large portion of the traffic in the future Internet.
By prefetching parts of ongoing streams into client buffers
these bursty streams can be more efficiently accommodated in
packet-switched networks. In this paper we develop a modular
algorithm-theoretic framework for the fair and efficient transmission of continuous media over a bottleneck link. We divide
the problem into the two subproblems of (i) assuring fairness,
and (ii) efficiently utilizing the available link capacity. We develop and analyze algorithm modules for these two subproblems.
Specifically, we devise a bin packing algorithm for subproblem
(i), and a “layered prefetching” algorithm for subproblem (ii).
Our simulation results indicate that the combination of these two
algorithm modules compares favorably with existing monolithic
solutions. This demonstrates the competitiveness of the decoupled
modular algorithm framework, which provides a foundation
for the development of refined algorithms for fair and efficient
prefetching.
Index Terms—Client buffer, continuous media, fairness, playback starvation, prefetching, prerecorded media, video streaming.

I. INTRODUCTION

C

ONTINUOUS media are expected to account for a large
portion of the traffic in the Internet of the future and
next generation wireless systems. These media have a number
of characteristics that make their transport over networks
very challenging, especially when the media are streamed in
real-time. An alternative to real-time streaming is the download
of the entire media file before the commencement of playback.
This download significantly simplifies the network transport,
but results in long response times to user requests, which
are unattractive for many usage scenarios. We focus in this
paper on the real-time streaming with minimal response times
(start-up delays). Continuous media are typically characterized
by periodic playout deadlines. For instance, a new video frame
has to be delivered and displayed every 33 msec in NTSC
video and every 40 msec in PAL video to ensure continuous
playback. A frame that is not delivered in time is useless for
the media playback and results in interruptions of the playback.
Manuscript received November 12, 2003; revised December 22, 2004. This
work was supported in part by the National Science Foundation under Grant
Career ANI-0133252 and Grant ANI-0136774 and the State of Arizona through
the IT301 initiative.
The authors are with Arizona State University, Goldwater Center, MC
5706, Tempe, AZ 85287 USA (e-mail: soohyun@asu.edu; yohuh@asu.edu;
beshan@asu.edu; goran@asu.edu; aricha@asu.edu, reisslein@asu.edu).
Digital Object Identifier 10.1109/TBC.2005.847643

For the network transport the continuous media are typically
compressed (encoded) to reduce their bit rates. The efficient encoders, especially for video, produce typically highly variable
frame sizes, with ratios of the largest frame size to the average
frame size for a given video stream in the range between 8
and 15 and coefficients of variation (defined as the ratio of
standard deviation to mean) in the range from 0.8 to 1.3 [1], [2].
This highly variable traffic makes efficient real-time network
transport very challenging since allocating network resources
based on the largest frame size of a stream would result in low
network utilization for most of the time. Allocating resources
based on the average bit rates, on the other hand, could result
in frequent playout deadline misses as the larger frames could
not be delivered in time. An additional characteristic of a large
portion of the continuous media delivered over networks is that
it is prerecorded, e.g., stored video clips, such as news or music
video clips, or full length videos, such as movies or lectures are
streamed, as opposed to live media streams, e.g., the feed from
a conference or sporting event.
An important characteristic of many of the user devices
(clients) used for the media playback is that they have storage
space. This storage space—in conjunction with the fact that a
large portion of the media are prerecorded—can be exploited
to prefetch parts of an ongoing media stream. This prefetching,
which is often also referred to as work-ahead, can be used
to smooth out some of the variabilities in the media stream
and to relax the real-time constraints. The prefetching builds
up prefetched reserves in the clients which help in ensuring
uninterrupted playback. The prefetching (smoothing) schemes
studied in the literature fall into two main categories: noncollaborative prefetching schemes and collaborative prefetching
schemes.
Non-collaborative prefetching schemes, see for instance
[3]–[16], smooth an individual stream by pre-computing
(off-line) a transmission schedule that achieves a certain optimality criterion (e.g., minimize peak rate or rate variability
subject to client buffer capacity). The streams are then transmitted according to the individually pre-computed transmission
schedules. Collaborative prefetching schemes [17]–[20], on the
other hand, determine the transmission schedule of a stream
on-line as a function of all the other ongoing streams. For
a single bottleneck link, this on-line collaboration has been
demonstrated to be more efficient, i.e., achieves smaller playback starvation probabilities for a given streaming load, than the
statistical multiplexing of streams that are optimally smoothed
using a noncollaborative prefetching scheme [18]. We also note

0018-9316/$20.00 © 2005 IEEE

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

that there are transmission schemes which collaborate only at
the commencement of a video stream, e.g., the schemes that
align the streams such that the large intracoded frames of the
MPEG encoded videos do not collude [21].
As discussed in more detail in the review of related work in
Section I-A, most studies on collaborative prefetching in the
literature consider the Join-the-Shortest-Queue (JSQ) scheme.
The JSQ scheme is designed to achieve efficiency by always
transmitting the next video frame for the client that has currently
the smallest number of prefetched frames in its buffer. While
efficiency, i.e., achieving a high utilization of the network resources and supporting a large number of simultaneous media
streams with small playback starvation probabilities, is important for media streaming, so is the fair sharing of these resources
among the supported streams. Without fairness, the supported
streams may suffer significantly different playback starvation
probabilities. Fairness in collaborative prefetching has received
relatively little interest so far. The only study in this direction
that we are aware of is the work by Antoniou and Stavrakakis
[20], who introduced the deadline credit (DC) prefetch scheme.
In the DC scheme the next frame is always transmitted to the
client that has currently the smallest priority index, which counts
the current number of prefetched frames in a client’s buffer
minus the number of playback starvations suffered by the client
in the past. By considering the “history” of playback starvations at the individual clients, the DC scheme can ensure fairness
among the ongoing streams.
In this paper we re-examine the problem of fair and efficient
collaborative prefetching of continuous media over a single
bottleneck link. The single bottleneck link scenario is a fundamental problem in multimedia networking that arises in many
settings, e.g., in the on-demand streaming of video over a cable
plant [18], [22] and the periodic broadcasting of video in a
near video on demand system [19], [23]. In addition, a solid
understanding of the prefetching over a single bottleneck link
is valuable when considering multihop prefetching. Also, the
policy for prefetching over a wired bottleneck link is typically
a module of the protocols for streaming video in a wireless
networks, e.g., from a base station to wireless clients [24],
[25].
In this paper we develop and analyze a modular algorithmic
framework for collaborative prefetching. In contrast to the DC
scheme, where both fairness and efficiency are addressed by a
single scheduling algorithm which considers a single priority
index, we break the problem into the two subproblems of (i)
ensuring fairness by avoiding continuous starvation of a client,
and (ii) maximizing the bandwidth utilization. This decoupled,
modular algorithm framework—which our complexity analysis
and numerical results demonstrate to be competitive as it compares favorably with the existing monolithic approaches—has
the advantage that different algorithms can be used for the two
subproblems. Thus, our modular structure facilitates the future
development of advanced collaborative prefetching schemes by
allowing for the independent development and optimization of
algorithm families for the two subproblems of achieving fairness and efficiency. Such future algorithm developments may,
for instance, introduce different service classes and thus generalize the notion of fairness, where all clients receive the same

201

grade of service. On the efficiency side, future algorithm developments may, for instance, take the relevance of the video data
for the perceived video quality into consideration and strive to
achieve high efficiency in terms of the perceived video quality.
This paper is organized as follows. In the following subsection
we review the related work on collaborative prefetching. In
Section II, we describe the problem set-up and introduce the
notations used in the modeling of the collaborative prefetching.
In Section III, we address the subproblem (i) of ensuring fairness.
We develop and analyze a BIN-PACKING-ROUND algorithm
which computes the minimum number of slots needed to
schedule at least one frame for each stream with the minimum
number of transmitted frames so far. Next, in Section IV, we
develop and analyze the LAYERED-PREFETCHING-ROUND
algorithm which maximizes the number of additional frames
to be transmitted (prefetched) in the residual bandwidths of
the minimum number of time slots found in Section III. In
Section V, we conduct simulations to evaluate the developed
modular solution to the problem of fair and efficient continuous
media prefetching. Our simulation results indicate that the
combination of our algorithm modules compares favorably
with the JSQ and DC schemes. Our approach reduces the
playout starvation probability approximately by a factor of
two compared to the JSQ scheme. Also, the combination of
our algorithm modules achieves about the same (and in some
scenarios a slightly smaller) starvation probability and the same
fairness as the DC scheme which has been enhanced in this
paper with some minor refinements. In Section VI, we outline
an LP rounding approach to subproblem (ii). This approach
accommodates different optimization goals, taking for instance
the frame sizes into consideration when defining the frame
transmission priority, through a profit function. In Section VII,
we summarize our findings.
A. Related Work
In this section we give an overview of the existing literature
on the collaborative prefetching of continuous media. The
problem was first addressed in the patent filing by Adams
and Williamson [22] and in the conference paper [17] by
Reisslein and Ross. Both works independently proposed the
Join-the-Shortest-Queue (JSQ) scheme for the problem. The
JSQ scheme is a heuristic which is based on the earliest deadline
first scheduling policy. The JSQ scheme proceeds in rounds,
whereby the length of a given round is equal to the frame period
of the videos. In each round, the JSQ scheduler continuously
looks for the client which has currently the smallest reserve of
prefetched frames and schedules one frame for this client. (Note
that the scheduled frame is the frame with the earliest playout
deadline among all the frames that are yet to be transmitted to
all the clients.) If a client does not permit further transmissions
in the round, because the next frame to be transmitted for the
client does not fit into the remaining link capacity of the round
or the client’s prefetch buffer, then this client is removed from
consideration. This scheduling process continues until all of
the clients have been removed from consideration. This JSQ
scheme has been evaluated through simulations with traces
of bursty MPEG encoded video in [18]. It was demonstrated
that collaborative prefetching employing the JSQ scheme gives

202

smaller playback starvation probabilities for a given load of
video streams than the statistical multiplexing of the individually optimally smoothed streams. Also, it was demonstrated
that for a given tolerable playback starvation probability, the
JSQ scheme supports more streams.
In the following years the JSQ scheduling principle for continuous media has been employed in video-on-demand (VOD)
system designs, see for instance [26], [27]. Lin et al. [26] employ a least-laxity-first policy in their design. The laxity is defined as the deadline of a given chunk of video data minus
the current time minus the time needed to transmit the chunk.
Scheduling the chunk with the smallest laxity is thus roughly
equivalent to the JSQ principle. Lin et al. design a comprehensive VOD system that on the protocol side incorporates the
least-laxity-first policy and a variety of other mechanisms in
their overall design.
There have also been efforts to adapt the JSQ scheme, which
was originally designed for a centralized VoD system with one
server to a more general architecture with multiple distributed
servers sharing the same bottleneck link. The protocol design
by Reisslein et al. [28] for the distributed prefetching problem
employs quotas limiting the transmissions by the individual
servers. The protocol design by Bakiras and Li [29] smoothes
the videos over individual MPEG Groups of Pictures (GoPs)
to achieve a constant bit rate for a small time duration. These
constant bit rates for a given GoP are then exploited to conduct
centralized scheduling according to the JSQ scheme.
The JSQ scheme has also been employed in periodic broadcasting schemes, which are employed in Near-Video-onDemand (NVOD) systems. Saparilla et al. [23] partition a
given video into segments using a fixed broadcast series (which
specifies the relative lengths of the segments). Li and Nikolaidis
[30] adaptively segment the video according to the bit rates of
the various parts of a given VBR video. In both designs the
transmissions of all the segments of all the offered videos share
a common bottleneck link and the JSQ scheme is employed for
the scheduling of the transmissions on the bottleneck link.
Fitzek and Reisslein [24] as well as Zhu and Cao [25] have
employed the JSQ scheme as a component in their protocols
designs for the streaming of continuous media over the shared
downlink transmission capacity from a base station to wireless
and possibly mobile clients. In these designs the JSQ scheme is
combined with additional protocol components that account for
the timevarying transmission conditions on the wireless links to
the individual clients.
Recently, Antoniou and Stavrakakis [20] developed a deadline credit (DC) scheme which is designed to achieve efficient
resource utilizations (similar to the JSQ scheme) and at the same
time ensure that the resources are shared in a fair manner among
the supported clients. As we describe in more detail, after having
introduced our notation in Section II, the DC scheme differs
from the JSQ scheme in that it uses a differently slotted time
structure and transmits the next frame for the stream with the
smallest number of on-time delivered frames.
More recently, Bakiras and Li [19] developed an admission
control mechanism for their JSQ based prefetching scheme first
presented in [29]. This admission control mechanism aggregates

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

Fig. 1. J prerecorded video streams are multiplexed over a bottleneck link of
=
, and prefetched into client buffers of capacity B j bits,
capacity R
j
;
; J.

= 1 .. .

bits 1

()

the individual client buffers into one virtual buffer and then employs effective bandwidth techniques to evaluate the probability
for overflow of the virtual buffer, which corresponds to starvation of client buffers.
We note in passing that there have been extensive analyzes of
employing the join-the-shortest-queue policy in queueing systems consisting of multiple parallel queues, each being serviced
by one or multiple servers, see for instance [31], [32] and references therein. The problem considered in these studies differs
fundamentally from the problem considered here in that there
are multiple parallel servers in the queueing models, whereas
we have only one server in our problem setting. In addition,
there are multiple differences due to the periodic playout deadlines of variable size video frames in our problem setting and
the Poisson arrivals of jobs with exponentially distributed service times considered in the queueing models.
II. SYSTEM SET-UP AND NOTATIONS
Fig. 1 illustrates our system set-up for the streaming of prerecorded continuous media. The multimedia server contains a
large number of continuous media streams in mass storage. To
fix ideas we focus on video streams. Let denote the number of
video streams in progress. The video streams are encoded using
some encoding scheme (such as MPEG, H.263, etc.). For our
initial algorithm framework development and analysis we assume that the streams are of infinite length, i.e., have an infinite
number of video frames. (In Section IV-C we discuss how to accommodate finite length streams in our algorithms.) Let
denote the size of the th frame of video stream . Note that for a
’s are
constant-bit-rate (CBR) encoded video stream the
identical, whereas for a variable-bit-rate (VBR) encoded video
’s are variable. Because the video streams are
stream the
prerecorded the sequences of integers
are fully known when the streaming commences. We denote
for the average frame size of video and let
denote the largest frame of video stream , i.e.,
. We denote
for the ratio of the largest (peak)
,
to the average frame size of video , i.e.,
and let denote the largest peak-to-mean ratio of the ongoing
streams, i.e.,
. Throughout this study our focus
is on VBR video, which allows for more efficient encoding compared to CBR video [33]. Let denote the basic frame period of
the videos in seconds. We assume that all videos have the same
basic frame period . (Our algorithms extend to videos where
the frame periods are integer multiples of the basic frame period,

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

as it typical for variable frame period video, in a straightforward
fashion by inserting zeros for the sizes of the missing frames.)
We denote for the transmission capacity of the bottleneck
seconds) and
link in bits per basic frame period (of length
assume throughout that the switch and the links connecting
the switch to the clients are not a bottleneck. We also assume
that the transmission capacity of the link is large enough to
accommodate the largest video frame in one frame period, i.e.,
, which is reasonable as in practical scenarios the link supports a moderate to large number of streams
, whereby each individual stream contributes a moderate
fraction of the total load, even when this stream peaks in its
,
, for the capacity of the
bitrate. We denote
prefetch buffer (in bits) in client , which we assume initially
are accommodated in Section IV-C).
to be infinite (finite
For our model we initially assume that all streams start at
time zero; all with an empty prefetch buffer. (In Section IV-C
we discuss how to accommodate a practical streaming scenario
where ongoing streams terminate and new streams start up.)
The video frame scheduling and transmission proceeds in slots
(rounds) of length . The transmission schedule for a given slot
is computed before the slot commences and the video frames
are transmitted according to the precomputed schedule during
the slot. The video frames arriving at a client are placed in the
client’s prefetching buffer. For our model we assume that the
first video frame is removed from the buffer, decoded, and dis). (In fuplayed at the end of the first slot (denoted by
ture work we plan to extend our model to accommodate start-up
latencies.) Each client displays the first frame of video stream
) during the second slot (denoted by
),
(denoted by
then removes the second frame from its prefetch buffer at the
end of this second slot, decodes it, and displays it during the
third slot, and so on. If at any one of these epochs there is no
complete video frame in the prefetch buffer, the client suffers
playback starvation and loses (a part or all of) the current frame.
(The client may try to conceal the missing encoding information
by employing error concealment techniques [34].) At the subsequent epoch the client will attempt to display the next frame
of the video. Throughout, a video frame is not scheduled if it
would arrive after its playout deadline, i.e., frame of a stream
is only scheduled up to (and including in) slot . If frame can
not be scheduled before (or in) slot , then it is dropped at the
server (i.e., not transmitted) and the client will suffer a frame
loss (play back starvation) in slot .
,
, denote the number
More formally, we let
of bits in the prefetch buffer of client at the beginning of slot
,
(and note that
for
).
,
, denote the number of bits that are
Let
scheduled for transmission to client during slot . With these
definitions
(1)
where

constraint
.

. Note
must be
, for all slots ,
must

that the buffer constraint
satisfied for all clients ,
. Also, note that the link
be satisfied for all slots ,

203

,
, denote the length (run time) of the
Let
prefetched video segment (in terms of basic frame periods) in
the prefetch buffer of client at the beginning of slot . (If all
frame periods of a stream are equal to the basic frame period,
gives the number of prefetched video frames.) Let
then
,
, denote the length of the video segment (in
basic frame periods) that is scheduled for transmission to client
during slot . Thus,
(2)
Let
,
, denote the number of video frames
that have been transmitted to client up to (and including in) slot
(and note the initial condition
for
).
denote the minimum number of frames that have been
Let
transmitted to any of the clients up to (and including in) slot ,
.
i.e.,
,
, denote the lowest indexed frame
Let
for stream that is still on the server and has not been dropped
is the frame
at the beginning of slot . In other words,
with the earliest playout deadline that can still be transmitted
to meet its deadline. (In Section III we discuss in detail how
to maintain these variables.) Let denote the earliest deadline
frame among the ongoing streams on the server at the beginning
.
of slot , i.e.,
,
, denote the number of video frames
Let
of stream that have missed their playout deadline up to (and
is incremented by one
including in) slot . The counter
whenever client wants to retrieve a video frame from its buffer,
but does not find a complete frame in its buffer. We define the
frame loss (starvation) probability of client as

We define the average frame loss probability
.
A. Outlines of JSQ and DC Schemes
Before we proceed with the development of our modular
algorithm-theoretic framework for collaborative prefetchting,
we briefly outline the existing schemes for collaborative
prefetching—the JSQ scheme [17] and the DC scheme [20]—in
terms of our notation. These outlines are intended to facilitate
the comparisons with our analytical framework throughout this
paper; for details on the JSQ and DC schemes we refer to the
respective references.
The JSQ scheme proceeds in rounds, with the length of a
round equal to the basic frame period of the video (in seconds). For each round, the JSQ scheme precomputes the transmission schedule by considering to transmit a frame for the
. If
client with the smallest number of prefetched frames
the frame will meet its playout deadline and fits into the remaining link capacity for the round and buffer capacity of the
client, the considered frame is scheduled and the JSQ scheme
(which may be
looks again for the client with the smallest
the same or a different client). If the frame will not meet its deadline, it is dropped and the next frame of the client is considered.

204

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

If the considered frame does not fit into the remaining link bandwidth or the buffer space, the client is removed from consideration for this round and the client with the next smallest
is considered. This process continues until all clients have been
removed from consideration. The computational complexity of
the JSQ scheme with the originally proposed linked list data
. We have developed a novel data
structure [17], [18] is
structure based on a group of linked lists, whereby each list
. This novel data struckeeps the streams with the same
ture, for which we refer the interested reader to [35] for details due to space constraints, reduces the complexity of the JSQ
.
scheme to
The DC scheme proceeds in slots, with the length of a slot significantly shorter than the frame period of the video. When
considering the DC scheme we express the frame period in
units of slots (not seconds, as done for JSQ). A slot length of
slots is considered
1/100th of a frame period, i.e.,
in [20], but we found that shorter slot lengths give better reslots and
sults for the DC scheme and thus consider
2000 slots in our numerical work, see Section V. At the beginning of each slot the DC scheme looks for the stream with the
smallest priority index, which is defined as the current number
minus the number of dropped frames
of prefetched frames
. We note that in our notation,
.
. Since
Hence the priority index of the DC scheme is
is the same for all clients, the DC scheme essentially considers
the client with the smallest number of on-time transmitted frame
. The DC scheme transmits the considered frame if it will
meet its playout deadline and fit into the remaining client buffer
space. If the frame is transmitted, the DC algorithm completes
the transmission of the frame and decides on the next frame to
transmit at the beginning of the next slot. If a considered frame
is not transmitted, the DC scheme considers the client with the
next smallest priority index. The complexity of one execution
, which is due to the sorting
of the DC algorithm is
of the priority counters. The number of times that the algorithm
is executed in a frame period depends on the slot length and the
frame size distribution. In the worst case the algorithm is executed times in a frame period. Thus, the computational effort
in a frame period is
.
III. AVOIDING STARVATION WITH BIN PACKING
The key objectives of our algorithm-theoretic framework for
prefetching are to minimize starvation and to treat the clients
fairly, i.e., the number of instances of playback starvation should
be minimized and equally distributed among the clients. In
other words, the starvation probabilities of the clients should
be roughly equal. (In ongoing work we are extending this notion of fairness to service differentiation with different classes
of service, whereby the clients in each class experience about the
same level of playback starvation.) The basic idea of our algorithm module for achieving fairness is to schedule exactly one
frame per client for the clients which have so far received the
minimum number of frames. More formally, we establish a correlation between the classical bin packing problem and the minimum number of slots needed to increase the minimum number
of transmitted frames to a client by one. Let
be the set of

streams with the minimum number of transmitted frames, i.e.,
.
In the classical bin packing problem, objects with different
sizes and bins with a fixed capacity are given, and the problem
is to find the minimum number of bins required to pack all objects into the bins. Any object must be placed as a whole into
one of the bins. To pack an object into a bin, the residual bin
capacity must be larger than or equal to the size of the object. In
our video frame scheduling problem, we can think of the bottleneck link in each slot as a bin, the transmission capacity of the
bottleneck link in a slot (i.e., ) is the bin capacity, and the video
are the objects.
frames to be transmitted to the clients in
A. Specification of BIN-PACKING-ROUND Algorithm
The BIN-PACKING-ROUND algorithm proceeds in loops.
Each iteration of the loop completes a bin-packing round. More
specifically, suppose a bin-packing round starts with the bedenotes the minimum
ginning of slot and recall that
number of video frames transmitted to any of the clients up to
. During the bin-packing round
(and including in) slot
one video frame is scheduled for each of the clients in , i.e.,
the bin-packing round ends when the number of frames scheduled for each of the clients in
has been incremented by one.
This may take one or more slots, and we refer to the slots in a
given bin-packing round as scheduling steps. Note that in the
“avoiding starvation” subproblem addressed in this section we
do not prefetch additional frames, i.e., once each client in
has been scheduled a frame we move on the next bin-packing
round, even though there may be residual capacity in the bins
(slots on bottleneck link) making up the bin-packing round. In
Section IV we efficiently fill this residual capacity with additional frames.
The schedule for a given bin-packing round starting at the
beginning of slot
is computed with the BIN-PACKINGROUND
algorithm, which is summarized in Fig. 2. At the
end of the BIN-PACKING-ROUND, we have pre-computed
the schedule of frames to be transmitted for each client for each
of the scheduling steps in this round.
The basic operation of the algorithm is as follows. The values
in step 1.1 are inherited from the end of the previous bin-packing
round and
denotes the first time slot considered in the
new bin-packing round. The algorithm schedules one frame per
client in
as long as the size of the frame fits into the residual
, where is the corresponding time
bandwidth
slot found in step 3.1, and the frame playout deadline is met.
If necessary the frame is scheduled in a new slot
. If no
such time slot meeting the frame’s playout deadline exists, then
the frame is dropped and the next frame in the same stream is
considered (step 3.3).
After a bin-packing round has been precomputed, the actual
transmission of the video frames is launched. Note that for each
client in
(from the beginning of the bin-packing round) the
number of received frames is increased by one at the end of the
actual transmission of the frames scheduled for the bin-packing
round. That is for each client in
the number of transmitted
frames is increased from
, at the start of the bin-packing
at the end of one bin-packing round.
round to
If a given bin-packing round is longer than one time slot (or
one scheduling step), then every client not scheduled in a slot

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

205

Fig. 2. BIN-PACKING-ROUND algorithm.

inside the bin-packing round experiences a frame loss in slot .
Note that this does not hold when future frames are prefetched,
see Section IV.
B. Analysis of BIN-PACKING-ROUND Algorithm
Recall that we initially assume that all streams start at the
beginning of slot 0 with an empty prefetch buffer, i.e.,
for all clients . The number of scheduling steps comprising
the first bin-packing round is equal to the minimum number of
slots needed to transmit exactly one frame for each client. The
first bin-packing round ends with
for all clients . The
for all clients ,
second bin-packing round ends with
and so on. Hence, at any slot ,
or
, for all clients
and for some integer .
During one bin-packing round consisting of scheduling
steps, each client will experience exactly
frame losses
(provided no future frames are prefetched, see Section IV) and
the number of frame losses during this round is the same for all
clients, which is summarized in the following lemma.
Lemma 1: Suppose that all streams start at the beginning of
slot 0, then each client has the same number of frame losses in
one bin-packing round. Moreover, if we minimize the number
of scheduling steps in a bin-packing round, then we can also
minimize the number of frame losses by a client in this round.
The classical bin packing problem is well known to be
NP-hard. Hence, according to Lemma 1, it can be also
shown that achieving fairness while attempting to minimize
frame losses is NP-hard. The following lemma shows that
the BIN-PACKING-ROUND is a 1.7 approximation factor
algorithm using the analogy between our algorithm and a
well-known algorithm for the classical bin packing problem.
Let be the set of frames that will end up being transmitted in
this bin-packing round.
Lemma 2: The minimum number of slots to increase
by one when using the BIN-PACKING-ROUND algorithm is
asymptotically no more than
, where is the minimum
number of slots to increase
by one when an optimal algorithm is used on the frames in .
Proof: We are essentially running the FIRST FIT (FF)
algorithm that solves the classical bin packing problem on the
frames in . The analogy between our algorithm and the FF
algorithm is as follows: The frames in are our set of objects

considered for the bin packing problem and the order in which
we consider them in the bin packing problem is exactly the order
in which they are considered by the BIN-PACKING-ROUND
algorithm, ignoring all the frames dropped in-between. Hence,
the number of slots in one bin-packing round calculated using the
BIN-PACKING-ROUND algorithm is the same as the number
of bins calculated using the FF algorithm for the classical bin
packing problem. The approximation ratio on the minimum
number of bins for the FF algorithm has been proven to be
1.7 asymptotically [36].
For the classical bin packing problem, we can achieve better
performance by running the FF algorithm after sorting frames
by nonincreasing order of sizes, which gives us an approximation factor of roughly 1.2 [36]. This algorithm is called the First
Fit decreasing algorithm. However, the FF decreasing algorithm
is not applicable to our problem. The reason is that we cannot
guarantee that the frames will always be considered in nonincreasing order since frames may be dropped, being replaced by
larger frames within a given bin-packing round. As a conclusion, we introduce the following theorem, which follows immediately from Lemma 1.
Theorem 1: We obtain a 1.7-approximation on the maximum
number of frame losses per client using the BIN-PACKINGROUND algorithm, if we consider only the set of frames transmitted in this round.
Before we close this section, we consider the complexity of
the BIN-PACKING-ROUND algorithm.
Theorem 2: The BIN-PACKING-ROUND algorithm computes a bin-packing round in
, where is the number of
clients.
Proof: The worst case scenario is as follows. The number
of streams in
is . For the th iteration of the for loop (Step
3), the corresponding stream (say, stream ) has to drop the first
frames, i.e., frames
,
at Step
3.3, and then schedules frame
into the next empty
slot (i.e., it increases the number of scheduling steps in this bin
packing round). Hence the number of comparisons needed to
schedule a frame in the th iteration is
. Since
and there are at most streams in
, the overall time complexity is
.
For essentially all streaming scenarios of practical interest
we can assume that the sum of the average frame sizes of
all simultaneously supported streams is less than or equal to

206

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

the link capacity, i.e.,
. This condition is also
commonly referred to as stability condition and means that
the long run streaming utilization of the link bandwidth in
terms of the ratio of the sum of the long run average bit rates
of the supported streams to the link capacity is less than or
equal to 100%. Recalling from Section II that we know the
for the prerecorded
largest peak-to-mean frame size ratio
videos, which is typically in the range from 8 to 15 [1], we can
significantly tighten the worst case computational complexity
as shown in the following corollary.
Corollary 1: Given the largest peak-to-mean ratio of the
frame sizes and the stability condition
, the
BIN-PACKING-ROUND algorithm computes a bin packing
round in
.
Proof: Note that
(3)

(4)
(5)
where (3) follows from the definition of
, (4) follows from
the definition of , and (5) follows from the stability condition.
The FF heuristic always uses a number of bins which is at most
twice the sum of the sizes of the objects to be packed, divided by
the capacity of a bin [36]. Hence, for any set of frames involved
in a bin-packing round, the number of time slots in the binpacking round will be at most
bins

(6)

Re-tracing the steps in the proof of Theorem 2, we note that
the number of comparisons needed to schedule a frame for any
given stream in a given bin packing round is bounded by
. Hence the overall time complexity of the
BIN-PACKING-ROUND algorithm is
.
IV. MAXIMIZING BANDWIDTH UTILIZATION
WITH LAYERED PREFETCHING
While the bin-packing round algorithm module of the preceding section focuses on fairness among the clients, we now
turn to the subproblem of maximizing the bandwidth utilization (efficiency) by prefetching future video frames. In this section and in Section VI we introduce two algorithm modules to
maximize the bandwidth utilization after the schedule for the
bin-packing round has been computed. In this section we define a layered prefetching round as a series of computations that
schedules video frames after a bin-packing round is calculated
in order to better utilize the bandwidth. The basic idea of our
prefetching strategy is as follows: After the bin-packing round
schedule has been computed, the prefetching round computation starts as long as there is sufficient residual bandwidth in the
scheduling steps of the corresponding bin-packing round. It is
natural to schedule a frame with an earlier playout deadline before scheduling a frame with later playout deadline. Therefore,

a frame is considered for scheduling only if no frame with earlier playout deadline can fit into the residual bandwidth.
A. Specification of LAYERED-PREFETCHING-ROUND
Algorithm
be the set of scheduling steps within
Let
a given bin-packing round, each with residual bandwidth ,
. (If there is no residual bandwidth for scheduling
step , then
.) Suppose that this bin-packing round starts
(i.e., scheduling step
at slot , and ends at slot
corresponds to time slot
). We group the unscheduled
frames in the server queues according to their playout deadlines.
denotes the earliest playout deadline of the
Recall that
unscheduled (and not yet dropped) frames on the server at the
.
end of the bin-packing round ending at the end of slot
We define
to be the group of unscheduled frames
. Hence the frames in
whose playout deadline is
have the earliest playout deadline.
The goal of a prefetching round is to maximize the number
of frames scheduled for each group
while enforcing the
earliest deadline first policy. We first consider the frames in
and schedule as many frames as possible into the residual bandfit into any of the residual
widths. When no more frames in
, and schedule them
bandwidths, we consider the frames in
until no more frames fit into the residual bandwidths. This
until no frames
process is repeated for each ,
fit into the residual bandwidths or until there are no frames
left to be considered. At the end of this process, the scheduled
frames form the layers in each scheduling step; the frames from
form layer 0, the frames from
form layer 1, and so on.
We consider the scheduling for each group
as a variant of
the multiple knapsack problem: The set of knapsacks is defined
, where
; the
by the set
, where
capacity of knapsack is defined by
is the sum of the sizes of the frames in
,
,
assigned to slot ; the objects to be packed are defined by all
frames from the group , where the profit of placing any frame
in a knapsack is always equal to 1. Our objective here is to assign objects (frames in ) to the knapsacks (slots that meet the
of the frames in
on bottleneck link) in order to
deadline
optimize the total profit. We assume here that every video frame
has the same importance, i.e., the profit of packing (scheduling)
is the same for every frame. Thus, our goal is to maximize the
number of objects packed into the knapsacks. (The LP rounding
module introduced in Section VI is more general and assigns
different video frames different priorities.)
The LAYER algorithm provided in Fig. 3:(i) sorts the frame
according to their sizes in nondecreasing order, and (ii)
in
schedules each ordered frame in the first scheduling step that
accommodates the frame size and meets its playout deadline.
We use again to denote the first time slot (scheduling step)
of the bin-packing round, and to denote the last time slot of
this bin-packing round. To optimize the bandwidth utilization,
our LAYERED-PREFETCHING-ROUND algorithm allows for
a frame with a later playout deadline to be transmitted prior to
,
,
one with an earlier deadline. Recall that
denotes the number of frames that have been scheduled for client

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

207

ROUND algorithm accordingly. The modified version of the algorithm is presented in Fig. 6.
A prefetching round is computed by calling the LAYER
algorithm repeatedly for each layer , as specified in Fig. 5. In
practice we limit the number of times the LAYER algorithm
is called without significantly affecting the overall performance
for the
of the prefetching round, see Section V. We denote
lookup window size that determines the number of times the
LAYER algorithm is called.
B. Analysis of LAYERED-PREFETCHING-ROUND Algorithm

Fig. 3.

LAYER algorithm for layer `.

Fig. 4.

UPDATE algorithm.

so far. We define
,
, to be the number of
frames that are currently scheduled out of order for client . In
is increased by one whenever a frame for
other words,
client is scheduled but some of its preceding frames (frames
with earlier playout deadlines) are still unscheduled in the server
,
, i.e.,
is the
queue. Let
number of frames that have been scheduled for client in order
(without gaps). Note that with the scheduling of future frames
with gaps, the (1) and (2) need to be modified to account for
frame deadlines.
and
in addition to
is as
The reason we have
follows: Suppose that a stream has a large frame followed by
some small frames. Due to the restricted capacity of the residual
bandwidths, frame may not be scheduled, while the small
frames with later playout deadlines are scheduled during the layered prefetching rounds. If we consider only the total number of
for each client, then stream may not
transmitted frames
have a chance to schedule any of its frames including frame
during the next bin-packing round due to the frames prefetched
in the previous rounds. As a result, frame may miss its deadline and a frame loss occurs. Such a frame loss is unnecessary
is used instead of
and unfair to stream . Hence,
in determining the streams with the smallest number of successfully transmitted frames. We modify the BIN-PACKING-

Dawande et al. [37] give 1/2-approximation algorithms for
the multiknapsack problem with assignment restrictions. The
following lemma shows that our LAYER algorithm is a 1/2approximation factor algorithm.
is a 1/2-approxiLemma 3: The algorithm LAYER
mation algorithm on the maximum number of scheduled
frames from layer , given the residual bandwidths after
LAYER
LAYER
LAYER
have been executed in this order.
be the residual bandwidth of step
at the
Proof: Let
for
. Let
be the number of
start of LAYER
scheduled frames by the LAYER algorithm and let
be the
maximum possible number of scheduled frames in , given the
of steps
. The size of
residual bandwidths
is larger than
any unscheduled frame at the end of LAYER
the maximum residual bandwidth of the respective scheduling
steps after LAYER is executed. Thus we can schedule at most
more frames, where
is the residual bandat the end of the LAYER
width of each scheduling step
algorithm. Then the maximum number of scheduled frames

where is the number of scheduling steps in the bin-packing
round. Hence

If
, it is obvious that the algorithm is a 1/2-approximation.
. We claim that the optimal solution cannot
Suppose
frames. Since
, there exist at
schedule more than
least
scheduling steps that do not have any frames from
layer after the LAYER algorithm ends. Let denote the set
of scheduling steps that contain at least one frame from layer ,
. We observe that the scheduling steps in
and let
do not have enough residual bandwidth to transmit any of
the frames left at the server queue at the end of LAYER . The
may have enough residual bandwidth to
scheduling steps in
accommodate some or all of frames which have been scheduled
during the LAYER
algorithm. Suppose we move all
in
these frames to the scheduling steps in , and try to schedule
more frames into scheduling steps in . Since the sizes of the
new frames are larger than the frames originally scheduled in
, we can schedule at most new frames into the scheduling
steps in . Therefore, the claim holds. Hence, The algorithm
LAYER is a 1/2-approximation algorithm on the maximum
number of scheduled frames.

208

Fig. 5.

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

PREFETCHING-ROUND algorithm with lookup window size

W.

Fig. 6. BIN-PACKING-ROUND2 algorithm: the lines starting with symbol 3 are added or modified from the original BIN-PACKING-ROUND.

Lemma 4: The time complexity of the LAYER algorithm
.
Proof: Since the number of elements in each group
is
at most , sorting the frames in
in nondecreasing order of
their sizes takes
. The number of searches to schedule
each frame in
is at most equal to the number of slots in
the bin-packing round, which is no larger than ; since there
are at most frames in each
, the total amount time spent
in scheduling the frames in
is
. Updating
and
takes only constant time by keeping a pointer to the first
frame in the server for each stream . Therefore, the overall time
complexity of the algorithm is
.
The following theorem summarizes the above results:
Theorem 3: For given residual bandwidths, the LAYER
algorithm gives an 1/2-approximation factor on maximizing the
number of scheduled frames from group
and its run time is
. The overall time complexity of the layered prefetching
round is
, where
is the lookup window size for
prefetching frames.
For practical streaming scenarios that satisfy the stability condition and for which is known, the LAYER
algorithm is
significantly less complex. Noting that in these practical scenarios the number of slots in a bin packing round is bounded by
, as shown in the proof of Corollary 1, we obtain the
following theorem:
is

Theorem 4: Given the largest peak-to-mean ratio of the
and the stability condition
frame sizes
, the time complexity of the LAYER
algorithm is
. The overall running time of a layered
prefetching round is
.
Proof: Since the maximum number of steps considered in
, the time for scheduling
a layered prefetching round is
is
. Since we may also need to sort ,
the frames in
in the worst-case, the complexity of the LAYER algorithm is
.
groups
considered in a layered prefetching
Most of the
round have already been considered (and therefore have already
been sorted) in previous rounds: There are only at most
“new” groups
which need to be fully sorted in the current
new time slots in
round (since we only see at most
this round, our lookup window shifts by at most this much, exnew groups which need to be sorted
posing at most
time to sort
from scratch in this round). It takes
the respective groups. We still need to schedule frames for
groups
in a layered prefetching round. Thus the overall com.
plexity of a layered prefetching round is
C. Accommodating Finite Stream Durations and Finite Buffers
at time we initialize
For a newly joining stream
,
, and
as follows.

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

1.
2.

Find
.
If the minimum gap between the current time and the
is
earliest deadline among the unscheduled frames
larger then zero, i.e.,
then
and
,
If the same existing stream attains both
.
then we set
and
Else we set the new video stream’s
to the
of the client attaining
minus the
minimum of the gap.
3. Else we set
.
4. Set
This approach is based on the following reasoning. Since the
new video stream has the end of the current slot as playback
deadline of the first frame, it has high urgency for transmission.
If the on-going streams have some frames in their buffer, they
may not need to transmit a frame urgently in the current slot. So
until the deadline of a frame of the new video stream is equivalent to the earliest deadline among all unscheduled frames from
the on-going streams, the new video stream should be given priority over the other video streams. However, if there is at least
one already on-going stream that has the end of the current slot
as a deadline for an unscheduled frame, the new video stream
has at least the minimum of .
To accommodate finite client buffer capacities the server
keeps track of the prefetch buffer contents through (1) (modified
for the layered prefetching) and skips a frame that is supposed
to be scheduled but would not fit into the client buffer.
V. NUMERICAL RESULTS
A. Evaluation Set-Up
In this section we evaluate the algorithms developed and analyzed in the preceding sections through simulations using traces
of MPEG-4 encoded video. The employed frame size traces give
the frame size in bit in each video frame [1]. We use the traces of
QCIF video encoded without rate control and fixed quantization
scales of 30 for each frame type (I, P, and B). These traces correspond to video with a low, but roughly constant visual quality.
The choice of these traces of low-quality video is motivated by
the fact that the traffic burstiness of this low-quality video lies
between the less bursty high-quality video and the somewhat
more bursty medium-quality video. The average bit rate of the
low-quality encoded videos is in the range from 52 kbps to 94
kbps. To achieve constant average utilizations in our simulations
we scaled the frame sizes of the individual to an average bit rate
of 64 kbps. The peak to mean ratios, standard deviations, and coefficients of variation (standard deviation normalized by mean
frame size) of the frame sizes of the scaled traces are given in
Table I.
All used traces correspond to videos with a frame rate of
25 frames per second, i.e., the frame period is
for all videos. For ease of discussion of the numerical results we
normalize the capacity of the bottleneck link by the 64 kbit/sec
average bit rate of the streams and denote this normalized ca, where is in
pacity by . Note that
and is the frame period in seconds.
units of
We conduct two types of simulations, start-up simulations
and steady state simulations. In the start-up simulations all

209

TABLE I
VIDEO TRAFFIC STATISTICS: PEAKTO MEAN RATIOAND STANDARD
DEVIATION OF FRAME SIZE. AVERAGE BIT RATE IS
64 kbps FOR ALL STREAMS

streams start with an empty prefetch buffer at time zero, similar
to the scenario initially considered in our algorithm development. Whereas the streams had an infinite number of video
frames in our initial model, we fix the number of video frames
frames,
(stream duration) for the simulations at
i.e., 10 minutes. We run many independent trials of this start-up
simulation. For each independent trial we randomly pick a
video trace for each ongoing stream and a random starting
phase into each selected trace. For each trial the loss probability
for each individual stream is recorded. These independent loss
probability observations are then used to find the mean loss
probability for each client and the 90% confidence interval of
the loss probability.
With the steady state simulations all streams start again at
time zero with an empty prefetch buffer and a random trace and
random starting phase are selected. In addition, each stream has
a random lifetime drawn from an exponential distribution with a
mean of frames. When a stream terminates (i.e., the last frame
of the stream has been displayed at the client), the corresponding
client immediately starts a new stream (with an empty prefetch
buffer) at the beginning of the next slot. For the new stream we
draw again a random trace, starting phase, and lifetime. With the
steady state simulation there are always streams in progress.
We estimate the loss probabilities (and their 90% confidence
intervals) of the individual clients after a warm-up period of
60 000 frames (i.e., 40 minutes) by using the method of batch
means.
All simulations are run until the 90% confidence intervals are
less than 10% of the corresponding sample means.
B. Comparison of JSQ and Modular BP Approach
In Table II we first examine the effect of the window size
on the performance of the combination of the BIN-PACKINGROUND and LAYERED-PREFETCHING-ROUND algorithm
modules, which we refer to as “bin packing” in the discussion
and as “BP” for brevity in the plots. We observe that the loss

210

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

TABLE II
FRAME LOSS PROBABILITY WITH BIN PACKING AS A FUNCTION OF WINDOW
SIZE ; R = 32, J = 30, B = 64 Kbytes

W

Fig. 8. Frame loss probabilities for individual clients with (a, b) mix of
a 64 kbps streams and b 128 kbps streams; R = 32, B = 64 Kbytes, and
W = 150, fixed.

Fig. 7.

Average frame loss probability as a function of buffer capacity B for
= 15 streams for bin packing and JSQ for a link capacity of

J = 14 and J
R = 16.

probability decreases as the window size increases. That is,
as the layered prefetching algorithm considers more frames
for the scheduling, there is a better chance to fill even a very
small remaining transmission capacity. With the resulting higher
utilization of the transmission capacity and increased prefetched
reserves (provided the buffer capacity is sufficiently large),
the probability of playback starvation is reduced. However,
we also observe that the loss probability slightly increases as
to
. In
the window size increases from
brief, the reason for this is that with a very large window size
the bin packing algorithm tends to transmit frames that have
deadlines far into the future. These prefetched future frames
take up buffer space in the client and tend to prevent the
prefetching of (large) frames with closer playout deadlines.
Thus making the client slightly more vulnerable to playback
starvation. Overall the results indicate that a larger window
size generally reduces the loss probability; however, extremely
large window sizes may degrade the performance slightly.
Next, we evaluate the bin packing approach using the steady
state simulations and compare its performance with the JSQ apas
proach. Fig. 7 gives the average frame loss probability
a function of the prefetch buffer capacity . We set the transand consider
and 15 simission capacity to
multaneous streams. The window sizes for the bin packing alfor
,
for
gorithm are set to
,
for
, and
for
. We observe from Fig. 7 that the frame

loss probabilities with bin packing are roughly half of the corand a buffer
responding loss probabilities for JSQ. For
, the JSQ scheme gives a frame loss probaof
while bin packing gives a loss probability of
bility of
. For the smaller load of
, the gap widens to loss
for JSQ and
for bin packing
probabilities of
The explanation for this gap in performance is as follows. The
JSQ scheme stops scheduling frames for a frame period when
the first unscheduled frame from none of the streams fits into
the remaining bandwidth. The bin packing scheme, on the other
hand, continues scheduling frames in this situation, by skipping
the frame(s) that are too large to fit and looks for smaller future
frames to fit into the remaining bandwidth.
Note that so far we have considered only the average (aggregate) performance of the prefetch algorithm. We now examine
the fairness aspects. To test the fair allocation of transmission resources we consider heterogeneous streaming scenarios, where
the ongoing streams differ in their average bandwidth, traffic
variability, stream lifetime, and client buffer capacity. First, we
consider heterogeneous average bandwidths. We set the link castreams with an average bit rate of 64 kbps.
pacity to
We stream either 30 streams with an average bit rate of 64 kbps,
a mix of 14 streams with an average bit rate of 64 kbps and
8 streams with an average bitrate of 128 kbps, or 15 streams
with an average bit rate of 128 kbps. Note that the average
system load is 30/32 in all three scenarios. We observe from
Fig. 8 that with JSQ the higher average bit rate streams experience larger frame loss probabilities than the lower average bit
rate streams. Considering the fairness criterion of distributing
the frame losses equally among the clients the higher bandwidth
clients are treated unfairly with JSQ. With BP, on the other hand,
the frame losses are fairly distributed.
Next, we examine the effect of mixing streams with different
variabilities. For this experiment we consider constant bit rate
(CBR) streams with a bit rate of 64 kbps and higher variability
streams (generated by increasing the variability of the traces
in Table I while maintaining the 64 kbps average bit rate.) In
Fig. 9 we plot the individual loss probabilities for a mix of CBR

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

TABLE III
FRAME LOSS PROBABILITY COMPARISON BETWEEN DC, JSQ,
APPROACHES (R = 16, B = 64 Kbyte)

Fig. 9. Frame loss probabilities of individual clients for (a, b) mix of a CBR
and b higher variability streams; R = 32, J = 30, B = 64 KByte, fixed.

and higher variability streams. We observe that with JSQ the
clients with the higher variability streams experience smaller
frame loss probabilities than the clients with CBR streams. The
explanation for this result is as follows. The higher variability
streams have a small portion of very large video frames, but also
have a very large portion of small frames. As a result, higher
variability streams can transmit more (small) frames over any
remaining bandwidth. As a consequence the higher variability
streams have typically a larger number of frames prefetched and
thus experience fewer instances of play back starvation. Comparing the frame loss probabilities for JSQ and bin packing, we
observe that bin packing gives again smaller and roughly equal
loss probabilities for the individual clients.
In additional experiments, which we can not include here due
to space constraints, we have found that the average stream life
time and client buffer size have a relatively small impact on the
fairness, see [35].
C. Comparison of DC Scheme With Modular BP Approach
In this section we compare the DC scheme, which we
briefly outlined in Section II-A, with our modular bin packing
approach. We use the start-up simulation set-up for this comparison as the DC scheme is formulated for this scenario in [20].
We present results for our experiments with
and
2000 slots per frame period. (We found that these shorter slot
lengths give better results; a slot length of 1/100th of the frame
period is considered in [20].) In the DC scheme the client buffer
capacity is expressed in terms of a maximum deadline credit
counter in units of number of video frames (which are of variable size for VBR-encoded video resulting in varying capacity
in terms of the deadline credit counter). For the comparison
with our scheme where the buffer capacity is a fixed number
of bytes, we considered two adaptations of the DC scheme.
In the “DC avg.” adaptation we convert the buffer capacity
in bytes to a maximum deadline credit counter (in number of
video frames) using the average bit rate of the video. In the
“DC ref.” adaptation we convert the buffer capacity in bytes to
the maximum deadline credit counter using the actual sizes of
the frames in the buffer and considered for transmission.

211

AND

BP

TABLE IV
AND COMPUTING TIME T COMPARISON
FRAME LOSS PROBABILITY P
BETWEEN DC AND BP APPROACHES (R = 32, B = 64 Kbyte)

TABLE V
AND COMPUTING TIME T COMPARISON
FRAME LOSS PROBABILITY P
BETWEEN DC AND BP APPROACHES (R = 64, B = 64 Kbyte)

In Table III we compare the DC avg., DC ref., JSQ, and BP
approaches in terms of the frame loss probability for a system
. We observe from the table that
with a link capacity of
considering the 90% confidence intervals of 10% around the reported sample means, bin packing gives smaller loss probabili. For
, the DC scheme
ties than the DC scheme for
with the refined adaptation gives approximately the same performance as the other schemes.
To gain further insight into the relative performance comparison of the DC and BP approaches we compare in Taslots
bles IVthrough VI the DC ref. scheme with
with the BP approach in terms of the frame loss probability
and the computation time . The computation time
measures the time needed to compute the scheduling decisions
for a frame period on a contemporary PC with Pentium IV
processor running at 3.2 GHz. We observe from the tables that
the computing times for the DC and BP schemes are roughly
the same; there is a slight tendency for the BP approach to be
faster, but the differences are rather small. We note that all
measured computation times are well below the duration of a
frame period, which is 40 msec with PAL video and 33 msec
with NTSC video.
in
Turning to the results for the frame loss probability
Tables IV–VI we observe that the difference in
widens
increases and a correspondingly larger
as the link capacity
numbers of streams are transmitted. Whereas for the
scenario the BP gives roughly half the
of the DC scheme,

212

TABLE VI
FRAME LOSS PROBABILITY P
AND COMPUTING TIME T COMPARISON
BETWEEN DC AND BP APPROACHES (R = 128, B = 64 Kbyte,
J = 122 AND 123 GIVE WITH BP EXCEEDINGLY SMALL P
VALUES
WHICH ARE OMITTED)

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

TABLE VII
MAXIMUM FRAME LOSS PROBABILITY AMONG J = 10 streams, R = 10:5

can be scheduled during any time slot than the bandwidth allows. The last constraint-set (10) says that each frame can only
be counted once toward the number of scheduled frames.
subject to

(7)
(8)

the gap widens to over one order of magnitude for the scenario
. This appears to indicate that the BP scheme is
with
better able to exploit the increased statistical multiplexing effect
that comes with an increased number of streams.
When interpreting the results in Tables IV–VI from the
perspective of the number of supported streams subject to a
fixed maximum permissible frame loss probability, we observe
for
that the BP approach gives
streams in all considered scenarios. (When the number of
, and
streams is increased to the stability limit, i.e.,
beyond the loss probability generally increases significantly
is therefore generally not of interest.)
and the range
The DC approach, on the other hand, supports fewer streams
criterion; for the
scenario
with the
streams.
up to
To gain insight into the frame loss patterns we have examined
the runs of consecutively lost frames and the runs of consecutive frames without any losses. We found that the frame losses
are not bursty; rather the runs of lost frames consist typically of
and
,
only one frame. For the scenario with
for instance, and with the BP approach the lengths of the runs
of consecutively lost frames have a mean of 1.016 frames and a
standard deviation of 0.019 frames, while the lengths of the runs
of frames without any loss have a mean of 1480.1 frames and a
standard deviation of 28.41 frames. With the DC approach, on
the other hand, the lengths of the runs of lost frames were one
frame in all simulations, and the lengths of the runs of frames
without any loss had a mean of 331.3 frames and a standard
deviation of 2123.6 frames. In more extensive simulations (see
[35]) we have also observed that the DC scheme is approximately as fair as the modular bin packing approach.

D. Comparison of Modular BP Approach With LP Solution
To further assess the performance of the modular bin packing
approach we compare it with the following linear relaxation of
denote the total number of conthe prefetching problem. Let
sidered frames in stream . Recall from Section II that the end of
slot is the deadline of frame of stream . We use the variable
to denote the fraction of frame of stream that is transmitted during slot . The first constraint-set (8) says that
is at least the (fractional) number of dropped frames for every
stream . The second constraint-set (9) says that no more bits

(9)
(10)
An optimal (minimum
) solution to this LP is a lower
bound on the maximum frame loss probability of a client.
Solving the LP becomes computationally prohibitive even for
moderate numbers of considered frames . We were able to
run 20 iterations of a start-up simulation with stream durations
frames. In Table VII we report the maximum (fracof
tional) frame loss probability (with 90% confidence interval)
and the corresponding
corresponding to the LP solution
maximum frame loss probability obtained with the modular
bin packing approach for the same 20 experiments. Although
the confidence intervals are quite loose, due to the enormous
computational effort, the results do indicate that the solutions
are generally of the same order of magnitude. One has to keep
in mind here that the LP does not (and can not) enforce the
delivery of complete video frames whereas the bin packing
approach delivers only complete video frames as they are
required for successful decoding.
VI. MAXIMIZING UTILIZATION WITH LP ROUNDING
In this section, we consider again the scenario presented
in Section II, where all streams start at time zero with empty
prefetch buffers. We outline a more general algorithm module
to solve the subproblem of maximizing the bandwidth utilization. This more general module is more flexible than the layered
prefetching module developed in Section IV. Whereas in the
layered prefetching module every prefetched frame increases
the profit by one, the LP rounding module developed in this
section accommodates more general profit functions. With this
more general profit function module we can accommodate
different optimization objectives, such as minimize the long
run fraction of encoding information (bits) that misses its
playout deadline (note that the layered approach was limited
to minimizing the frame loss probability). Also, we can assign
the frames different priorities, e.g., higher priority for large
Intracoded (I) frames.
Our more general solution approach is based on solving a
linear relaxation of the original problem and then rounding the
fractional solution to obtain an integer solution for the original

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

problem. We reduce our problem to a maximization version of
the generalized assignment problem (Max GAP). The Max GAP
items and a set of
is defined as follows: There is a set of
knapsacks. Each item has to be assigned to exactly one of the
, and there is a
knapsacks; each knapsack has a capacity
and a size
associated with each knapsack
profit
and each item
. The optimization
criterion is to maximize the total profit. This problem has been
shown to be APX-hard [38], even for the case when all frames
have equal profit [39]. In our problem, the set of knapsacks is
defined by the set , the capacity of knapsack is defined by
,
, and the objects to be packed are defined by
the frames to be transmitted to the clients. The profit of placing
a frame in a knapsack can be defined as follows. Let be the
maximum residual bandwidth after a bin-packing round, i.e.,
. Then the profit of frame of stream can
be defined as
(11)
if frame

is scheduled into a time slot
, otherwise,
. Note that the frames in the same group
have
the same profit. Our objective here is to assign objects (frames)
into the knapsacks (scheduling steps) in order to maximize the
total profit. As we will see later, for the given profit function, the
objective of the prefetching round is to maximize the number of
are
frames scheduled for each group , where the groups
considered in increasing order of .
Claim 1: For the profit function defined above, a frame is
scheduled only after no frame with earlier deadline can be
scheduled into the residual bandwidth.
Proof: To prove this claim, we show that scheduling one
always produces higher profit than schedframe for group
,
, where is the maximum residual
uling frames for
bandwidth. Since

for
, the claim holds.
The above claim implies that for the profit function we defined in (11), the Max GAP is equivalent to the multiknapsack
problem presented in Section IV. The approximation bounds obtained by both approaches are the same, as we will see shortly.
The Max GAP approach has the advantage that it allows
for different profit functions. These profit functions translate
into different optimization criteria, which we explore in Section VI-A in greater detail. On the downside, the solution
techniques for the Max GAP are more involved than the ones
presented in Section IV.
The algorithm we use for computing the prefetching round is
based on solving the linear relaxation and rounding technique
given in [40]. Due to space constraints we give here only a brief
sketch of our proposed algorithm module. We first convert the
problem into a minimization problem, by changing profits into
suitable costs. We then solve a linear relaxation of a parametric
version of the minimization problem (where the sizes of the bins
are scaled up by a factor of 2). We build a bipartite graph with
edge costs based on the solution to this linear relaxation, and
find a minimum cost matching on this graph. We schedule the

213

frames according to this matching. A key property to be used
during the scheduling is that if we exceed the link capacity at any
scheduling step, the results in [40] guarantee that there exists a
frame to be removed which contributes to less than half of the
profit in that step. A full description of this algorithm appears
in [35], which gives also the proofs of the following lemmas on
the approximation guarantees of the algorithm:
Lemma 5: The prefetching algorithm outlined in this section
is a 1/2-approximation algorithm on the total profit.
Lemma 6: If we optimize the total profit, then we also maximize the number of frames scheduled for each group , given
the frames that have already been scheduled for
.
A. Profit Functions
The LP rounding approach allows us to achieve different objectives by simply changing the profit functions for each frame.
According to the profit function (11), the profits of the frames
with the same playout deadlines are equal. Hence, in order to
optimize the total profit according to (11), it is necessary to
maximize the number of frames to be scheduled in each playout
deadline group. If instead we want to optimize a different objective, such as the number of bits to be scheduled for each group,
the profit function can be changed accordingly. In the case of
maximizing the number of bits delivered in time, a new profit
function for frame (which is to be removed from the prefetch
buffer at the end of slot , see Section II) can be defined by
(12)
if frame is scheduled into time slot
and
if
. We observe that the profit function (12) has the following
properties.
• Among the frames in the same group, the larger the frame
size, the higher the profit.
• If frames , , and are in the same group and
, then
.
• The profit of a frame in group
is always larger than
.
that of a frame in group
For many video encoding schemes, e.g., MPEG with predictive encoding, the large intracoded frames are more important
than the smaller predictive encoded frames. In that case, it may
for group
inbe beneficial to schedule a large frame
for group . Let the profit function
stead of a small frame
then be
(13)
is scheduled into time slot
where
if
. Consider an example where
and
. Suppose that there are two frames and
According to the profit function (13), the profit of frame
is equal to or larger than that of frame if the size of frame
is at least five times larger than the size of frame .
if frame

;
.

VII. CONCLUSION
We have developed a modular algorithm-theoretic framework
for the prefetching of continuous media over a bottleneck link.
We have divided the problem into the two separate subproblems

214

IEEE TRANSACTIONS ON BROADCASTING, VOL. 51, NO. 2, JUNE 2005

of (i) ensuring fairness, and (ii) efficient bandwidth utilization.
We have developed algorithm modules for both subproblems.
We have investigated the theoretical performance bounds and
complexities of the individual algorithm modules and compared
the playout starvation probabilities achieved by the combination
of the modules with the JSQ and DC prefetching schemes. Our
simulation results indicate that the combined modules compare
favorably with the existing JSQ and DC schemes, thus demonstrating the competitiveness of our modular approach.
There are several interesting and important avenues for future work. One avenue is to develop new algorithm modules for
the first—the ensuring fairness—component in our algorithm
framework. In particular, those new algorithm modules could
be designed to generalize the notion of fairness from providing
all clients with the same service quality to providing different
classes of service quality.
Another avenue for future work is a detailed study of the profit
function space outlined in Section VI-A. The goal of such a
study could be to prioritize the prefetching of the video frames
according to their contribution to the perceived decoded video
quality to maximize the efficiency of the streaming not only in
terms of the number of simultaneously supported streams, but
also in terms of the visual quality provided by these streams.
We believe that our modular algorithm framework provides a
solid foundation for these future explorations.
REFERENCES
[1] F. H. P. Fitzek and M. Reisslein, “MPEG-4 and H.263 video traces for
network performance evaluation,” IEEE Network, vol. 15, no. 6, pp.
40–54, November/December 2001.
[2] P. Seeling, M. Reisslein, and B. Kulapala, “Network performance evaluation using frame size and quality traces of single-layer and two-layer
video: a tutorial,” IEEE Communications Surveys and Tutorials, vol. 6,
no. 3, pp. 58–78, 3rd Quarter 2004.
[3] C. Bewick, R. Pereira, and M. Merabti, “Network constrained
smoothing: enhanced multiplexing of MPEG-4 video,” in Proc. IEEE
Int. Symp. Computers and Communications, Taormina, Italy, Jul. 2002,
pp. 114–119.
[4] H.-C. Chao, C. L. Hung, and T. G. Tsuei, “ECVBA traffic-smoothing
scheme for VBR media streams,” Int. J. Netw. Manage., vol. 12, pp.
179–185, 2002.
[5] W.-C. Feng and J. Rexford, “Performance evaluation of smoothing algorithms for transmitting prerecorded variable-bit-rate video,” IEEE Trans.
Multimedia, vol. 1, no. 3, pp. 302–313, Sept. 1999.
[6] M. Grossglauser, S. Keshav, and D. Tse, “RCBR: a simple and efficient
service for multiple time-scale traffic,” IEEE/ACM Trans. Netw., vol. 5,
no. 6, pp. 741–755, Dec. 1997.
[7] Z. Gu and K. G. Shin, “Algorithms for effective variable bit rate traffic
smoothing,” in Proc. IEEE Int. Performance, Computing, and Communications Conf., Phoenix, AZ, Apr. 2003, pp. 387–394.
[8] M. Krunz, W. Zhao, and I. Matta, “Scheduling and bandwidth allocation
for distribution of archived video in VoD systems,” J. Telecommun. Syst.,
Special Issue on Multimedia, vol. 9, no. 3/4, pp. 335–355, Sep. 1998.
[9] M. Krunz, “Bandwidth allocation strategies for transporting variablebit-rate video traffic,” IEEE Commun. Mag., vol. 37, no. 1, pp. 40–46,
Jan. 1999.
[10] S. S. Lam, S. Chow, and D. K. Y. Yau, “A lossless smoothing algorithm for compressed video,” IEEE/ACM Trans. Netw., vol. 4, no. 5, pp.
697–708, Oct. 1996.
[11] R. Sabat and C. Williamson, “Cluster-based smoothing for MPEG-based
video-on-demand systems,” in Proc. IEEE Int. Conf. Performance, Computing, and Communications, Phoenix, AZ, Apr. 2001, pp. 339–346.
[12] J. Salehi, Z.-L. Zhang, J. Kurose, and D. Towsley, “Supporting stored
video: reducing rate variability and end-to-end resource requirements
through optimal smoothing,” IEEE/ACM Trans. Netw., vol. 6, no. 4, pp.
397–410, Aug. 1998.
[13] A. Solleti and K. J. Christensen, “Efficient transmission of stored
video for improved management of network bandwidth,” Int. J. Netw.
Manage., vol. 10, pp. 277–288, 2000.

[14] B. Vandalore, W.-C. Feng, R. Jain, and S. Fahmy, “A survey of application layer techniques for adaptive streaming of multimedia,” Real-Time
Imaging Journal, vol. 7, no. 3, pp. 221–235, 2001.
[15] D. Ye, Z. Xiong, H.-R. Shao, Q. Wu, and W. Zhu, “Wavelet-based
smoothing and multiplexing of VBR video traffic,” in Proc. IEEE
Globecom, San Antonio, TX, Nov. 2001, pp. 2060–2064.
[16] Z.-L. Zhang, J. Kurose, J. Salehi, and D. Towsley, “Smoothing, statistical
multiplexing and call admission control for stored video,” IEEE J. Select.
Areas Commun., vol. 13, no. 6, pp. 1148–1166, Aug. 1997.
[17] M. Reisslein and K. W. Ross, “A join-the-shortest-queue prefetching
protocol for VBR video on demand,” in Proc. IEEE Int. Conf. Network
Protocols (ICNP), Atlanta, GA, Oct. 1997, pp. 63–72.
[18]
, “High-performance prefetching protocols for VBR prerecorded
video,” IEEE Network, vol. 12, no. 6, pp. 46–55, Nov/Dec 1998.
[19] S. Bakiras and V. O. K. Li, “Maximizing the number of users in an interactive video-on-demand system,” IEEE Trans. Broadcast., vol. 48, no.
4, pp. 281–292, Dec. 2002.
[20] Z. Antoniou and I. Stavrakakis, “An efficient deadline-credit-based
transport scheme for prerecorded semisoft continuous media applications,” IEEE/ACM Trans. Netw., vol. 10, no. 5, pp. 630–643, Oct. 2002.
[21] M. Krunz and S. K. Tripathy, “Exploiting the temporal structure of
MPEG video for the reduction of bandwidth requirements,” in Proc.
IEEE Infocom, Kobe, Japan, Apr. 1997, pp. 67–74.
[22] M. B. Adams and L. D. Williamson, “Optimum Bandwidth Utilization in
a Shared Cable System Data Channel,” U.S. Patent 6 124 878, September
2000.
[23] D. Saparilla, K. W. Ross, and M. Reisslein, “Periodic broadcasting with
VBR-encoded video,” in Proc. IEEE Infocom, New York, Mar. 1999, pp.
464–471.
[24] F. H. P. Fitzek and M. Reisslein, “A prefetching protocol for continuous media streaming in wireless environments,” IEEE J. Select. Areas
Commun., vol. 19, no. 10, pp. 2015–2028, Oct. 2001.
[25] H. Zhu and G. Cao, “A power-aware and QoS-aware service model on
wireless networks,” in Proc. IEEE Infocom, Hong Kong, Hong Kong,
Mar. 2004.
[26] C.-S. Lin, M.-Y. Wu, and W. Shu, “Transmitting variable-bit-rate videos
on clustered VOD systems,” in Proc. IEEE Int. Conf. Multimedia and
Expo (ICME), New York, Jul. 2000.
[27] Y.-W. Leung and T. K. C. Chan, “Design of an interactive video-ondemand system,” IEEE Trans. Multimedia, vol. 5, no. 1, pp. 130–140,
Mar. 2003.
[28] M. Reisslein, K. W. Ross, and V. Verillotte, “A decentralized prefetching
protocol for VBR video on demand,” in Multimedia Applications, Services and Techniques—ECMAST ’98 (Lecture Notes in Computer Science, Vol. 1425), D. Hutchison and R. Schäfer, Eds. Berlin, Germany,
May 1998, pp. 388–401.
[29] S. Bakiras and V. O. K. Li, “Smoothing and prefetching video from distributed servers,” in Proc. IEEE Int. Conf. Networking Protocols (ICNP),
Toronto, Canada, Oct. 1999, pp. 311–318.
[30] F. Li and I. Nikolaidis, “Trace-adaptive fragmentation for periodic
broadcast of VBR video,” in Proc. 9th Int. Workshop on Network and
Operating Systems Support for Digital Audio and Video (NOSSDAV),
Basking Ridge, NJ, Jun. 1999, pp. 253–264.
[31] H.-C. Lin and C. S. Raghavendra, “An approximate analysis of the join
the shortest queue (JSQ) policy,” IEEE Trans. Parallel Distrib. Syst., vol.
7, no. 3, pp. 301–307, Mar. 1996.
[32] W. Zhu, “Analysis of JSQ policy on soft real-time scheduling in cluster,”
in Proc. IEEE Conf. High Performance Computing—Asia, Beijing,
China, 2000, pp. 277–281.
[33] T. V. Lakshman, A. Ortega, and A. R. Reibman, “VBR video: tradeoffs
and potentials,” Proc. IEEE, vol. 86, no. 5, pp. 952–973, May 1998.
[34] Y. Wang and Q. Zhu, “Error control and concealment for video communication: a review,” Proc. IEEE, vol. 86, no. 5, pp. 974–997, May 1998.
[35] S. Oh, Y. Huh, G. Konjevod, A. Richa, and M. Reislein, “A Modular Algorithm-Theoretic Framework for the Fair and Efficient Collaborative
Prefetching of Continuous Media (Extended Version),” Dept. of Electrical Eng, Arizona State University, Tech. Rep., Dec. 2004. [Online].
Available: http://www.fulton.asu.edu/~mre.
[36] E. G. Coffman Jr., M. R. Garey, and D. S. Johnson, “Approximation
algorithms for bin-packing—an updated survey,” in Algorithm Design
for Computer System Design, Springer Courses and Lecture Series no.
284: Springer-Verlag, 1984, pp. 49–106.
[37] M. Dawande, J. Kalagnanam, P. Keskinocak, F. S. Salman, and R. Ravi,
“Approximation algorithms for the multiple knapsack problem with assignment restrictions,” J. Combinatorial Optimization, vol. 4, no. 2, pp.
171–186, Jun. 2000.
[38] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, A. Marchetti-Spaccamela, and M. Protasi, Complexity and Approximation: Combinatorial
Optimization Problems and Their Approximability Properties: Springer,
1999.

OH et al.: MODULAR ALGORITHM-THEORETIC FRAMEWORK FOR PREFETCHING OF CONTINUOUS MEDIA

[39] C. Chekuri and S. Khanna, “A PTAS for the multiple knapsack
problem,” in Proc. ACM-SIAM Symp. Discrete Algorithms (SODA), San
Francisco, CA, Jan. 2000, pp. 213–222.
[40] D. B. Shmoys and E. Tardos, “Scheduling unrelated machines with
costs,” in Proc. ACM-SIAM Symp. Discrete Algorithms (SODA), Austin,
TX, Jan. 1993, pp. 448–454.

Soohyun Oh is a Ph.D. student in the Department of Computer Science and
Engineering at Arizona State University, Tempe. She received her M.S. degree
from Arizona State University in December 2001. Her research interests are in
the design and analysis of algorithms.

Yo Huh received the B. S. degree in Industrial Engineering from
SungKyunKwan University, Seoul, Korea, in 1995 and M.S. degree from
Texas A&M University, College Station, in 1998. He is currently a Ph.D.
student in the Industrial Engineering Department at Arizona State University,
Tempe. His research interests are in operations research, system modeling and
analysis, and simulation.

Beshan Kulapala received the B.S. degree in electrical engineering from the University of Kentucky,
Lexington, in 2001, and received the M.S. degree in
electrical engineering from Arizona State University,
Tempe, in 2003. Since 2003 he has been a Ph.D. student in the Department of Electrical Engineering at
Arizona State University. His research interests are in
the area of video transmission over wired and wireless networks. He is a student member of the IEEE.

Goran Konjevod, photograph and biography not available at time of publication.

215

Andrea W. Richa is an Associate Professor at the
Department of Computer Science and Engineering
at Arizona State University since August 2004. She
joined this department as an Assistant Professor
in August 1998. Prof. Richa received her M.S.
and Ph.D. degrees from the School of Computer
Science at Carnegie Mellon University, in 1995
and 1998, respectively. She also earned an M.S.
degree in Computer Systems from the Graduate
School in Engineering (COPPE), and a B.S. degree
in Computer Science, both at the Federal University
of Rio de Janeiro, Brazil, in 1992 and 1990, respectively. Prof. Richa’s main
area of research is in network algorithms. Some of the topics Dr. Richa has
worked on include packet scheduling, distributed load balancing, packet
routing, mobile network clustering and routing protocols, and distributed data
tracking. Prof. Richa’s data tracking (or name lookup) algorithm has been
widely recognized as the first benchmark algorithm for the development of
distributed databases in peer-to-peer networking, having being references
by over 130 academic journal or conference publications to date, and being
implemented as part of two of the current leading projects in peer-to-peer
networking. Dr. Richa’s was the recipient of an NSF CAREER Award in 1999.
For a selected list of her publications, CV, and current research projects, please
visit http://www.public.asu.edu/~aricha.

Martin Reisslein is an Assistant Professor in the
Department of Electrical Engineering at Arizona
State University, Tempe. He received the Dipl.-Ing.
(FH) degree from the Fachhochschule Dieburg,
Germany, in 1994, and the M.S.E. degree from the
University of Pennsylvania, Philadelphia, in 1996.
Both in electrical engineering. He received his
Ph.D. in systems engineering from the University
of Pennsylvania in 1998. During the academic year
1994–1995 he visited the University of Pennsylvania
as a Fulbright scholar. From July 1998 through
October 2000 he was a scientist with the German National Research Center
for Information Technology (GMD FOKUS), Berlin. While in Berlin he was
teaching courses on performance evaluation and computer networking at the
Technical University Berlin. He is Editor-in-Chief of the IEEE Communications
Surveys and Tutorials and has served on the Technical Program Committees
of IEEE Infocom, IEEE Globecom, and the IEEE International Symposium
on Computer and Communications. He has organized sessions at the IEEE
Computer Communications Workshop (CCW). He maintains an extensive
library of video traces for network performance evaluation, including frame
size traces of MPEG-4 and H.263 encoded video, at http://trace.eas.asu.edu.
He is co-recipient of the Best Paper Award of the SPIE Photonics East
2000—Terabit Optical Networking conference. His research interests are in
the areas of Internet Quality of Service, video traffic characterization, wireless
networking, and optical networking.

Brief Announcement: On the Feasibility of Leader Election
and Shape Formation with Self-Organizing Programmable
Matter
∗

Zahra Derakhshandeh†

Robert Gmyr‡

Thim Strothmann‡

Arizona State University, USA

University of Paderborn,
Germany

University of Paderborn,
Germany

Rida Bazzi

gmyr@mail.upb.de
Andréa W. Richa†

thim@mail.upb.de
Christian Scheideler‡

Arizona State University, USA

Arizona State University, USA

bazzi@asu.edu

aricha@asu.edu

University of Paderborn,
Germany

zderakhs@asu.edu

ABSTRACT

scheideler@.upb.de

We also consider a geometric variant of the amoebot model by
restricting the particle structures to form a connected subset on a
triangular grid. For these structures we can show that there are
local-control protocols for the leader election problem and the line
formation problem. The protocols can also be adapted to other regular geometric structures demonstrating that it is advisable to restrict particle structures to such structures.

Imagine that we had a piece of matter that can change its physical properties like shape, density, conductivity, or color in a programmable fashion based on either user input or autonomous sensing. This is the vision behind what is commonly known as programmable matter. Many proposals have already been made for
realizing programmable matter, ranging from DNA tiles, shapechanging molecules, and cells created via synthetic biology to reconfigurable modular robotics. We are particularly interested in
programmable matter consisting of simple elements called particles that can compute, bond, and move, and the feasibility of solving fundamental problems relevant for programmable matter with
these particles. As a model for that programmable matter, we will
use a general form of the amoebot model first proposed in SPAA
2014, and as examples of fundamental problems we will focus on
leader election and shape formation. For shape formation, we investigate the line formation problem, i.e. we are searching for a
local-control protocol so that for any connected structure of particles, the particles will eventually form a line.
Prior results on leader election imply that in the general amoebot
model there are instances in which leader election cannot be solved
by local-control protocols. Additionally, we can show that if there
is a local-control protocol that solves the line formation problem,
then there is also a protocol that solves the leader election problem, which implies that in the general amoebot model also the line
formation problem cannot be solved by a local-control protocol.

Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed Systems; F.2.0 [Analysis of Algorithms and Problem Complexity]:
General

1.

INTRODUCTION

There have been many conceptions of programmable matter, and
thus many avenues of research using that name, each pursuing solutions for specific application scenarios with their own, special
capabilities and constraints. However, most of the research in this
area has been done by scientists from the natural sciences and the
robotics area and not so much from theoretical computer science,
so not much is known yet about which assumptions or primitives
allow the development of distributed algorithms that can solve relevant problems for programmable matter in a self-organizing way.
Notable exceptions are DNA tile assembly and population protocols. However, most of these approaches are passive in one or
more aspects or require powerful entities while we are interested
in studying the feasibility of solving problems with simple entities
that can not just compute but also control their movements. For that
we use a general form of the amoebot model first proposed in [2].

∗
A full paper presenting the results outlined in this brief announcement will appear at the 21st International Conference on DNA
Computing and Molecular Programming (DNA21).
†
This work was supported in part by the NSF under Awards CCF1353089 and CCF-1422603.
‡
Supported in part by DFG grant SCHE 1592/3-1

1.1

The general amoebot model

In the general amoebot model, programmable matter consists of
a set of simple uniform computational units called particles that can
move and bond to other particles and use these bonds to exchange
information. The particles have very limited memory, act asynchronously, and they achieve locomotion by expanding and contracting, which resembles the behavior of amoeba.
As a base for our amoebot model, we assume that we have a
set of particles that aim at maintaining a connected structure at all
times. This is needed to prevent the particles from drifting apart in
an uncontrolled manner like in fluids and because in our case particles communicate only via bonds. The shape and positions of the

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
PODC’15, July 21–23, 2015, Donostia-San Sebastián, Spain.
c 2015 ACM 978-1-4503-3617-8 /15/07.
Copyright 
http://dx.doi.org/10.1145/2767386.2767451.

67

2.

bonds of the particles mandate that they can only assume discrete
positions in the particle structure. This justifies the use of a possibly infinite, undirected graph G = (V, E), where V represents all
possible positions of a particle (relative to the other particles in their
structure) and E equals all possible transitions between positions.
Each particle occupies either a single node or a pair of adjacent
nodes in G, and every node can be occupied by at most one particle. Two particles occupying adjacent nodes are connected, and we
refer to such particles as neighbors. Particles are anonymous but
the bonds of each particle have unique labels, which implies that
a particle can uniquely identify each of its outgoing edges. Each
particle has a local memory in which it can store some bounded
amount of information, and any pair of connected particles has a
bounded shared memory that can be read and written by both of
them and that can be accessed using the edge label associated with
that connection.
Particles move through expansions and contractions: If a particle occupies one node (i.e., it is contracted), it can expand to an
unoccupied adjacent node to occupy two nodes. If a particle occupies two nodes (i.e., it is expanded), it can contract to one of
these nodes to occupy only a single node. Performing movements
via expansions and contractions has various advantages. For example, it would easily allow a particle to abort a movement if its
movement is in conflict with other movements. A particle always
knows whether it is contracted or expanded — in the latter case,
it also knows along which edge it is expanded — and this information will be available to neighboring particles. In a handover,
two scenarios are possible: a) a contracted particle p can "push"
a neighboring expanded particle q and expand into the neighboring node previously occupied by q, forcing q to contract, or b) an
expanded particle p can "pull" a neighboring contracted particle q
to a cell occupied by it thereby expanding that particle to that cell,
which allows p to contract to its other cell. The ability to use a
handover allows the system to stay connected while particles move
(e.g., for particles moving in a worm-like fashion). Note that while
expansions and contractions may represent the way particles physically move in space (resembling loosely the movement of amoeba),
they can also be interpreted as a particle "looking ahead" and establishing new connections (by expanding) before it fully moves to a
new position and severs the old connections it had (by contracting).
In Figure 1, we illustrate a set of particles (some contracted, some
expanded) using the infinite regular triangular grid graph Geqt as
the underlying graph G .

LEADER ELECTION AND LINE FORMATION IN THE GENERAL AMOEBOT
MODEL

Here we focus on the feasibility of leader election and shape formation for particles with constant memory. For the shape formation, we just focus on forming a line of particles, that is, we are
searching for a local-control algorithm such that for any initial set
A ⊆ V of positions occupied by particles where G|A (i.e., the
subgraph of G induced by A) is connected, the particles will eventually rearrange themselves into a line without losing connectivity.
Given that we do not assume any underlying geometric information regarding G, by forming a line, we mean that for the final set
of occupied positions A0 , G|A0 forms an arbitrary simple path.
Suppose that there is a protocol for the line formation problem in
the general amoebot model. Then it is easy to extend it to a leader
election protocol: once the line has been formed, its two endpoints
contend for leadership using tokens with random bits sent back and
forth. On the other hand, one can deduce from [3] that in the general amoebot model there is no local-control protocol for leader
election. Hence, also line formation cannot be solved. So additional assumptions are needed to solve these.

3.

LEADER ELECTION IN THE GEOMETRIC AMOEBOT MODEL

In this section we assume that G is equal to Geqt (see Figure 1)
and that the edges are labeled in a consecutive way in clockwise
direction so that every particle has the same sense of clockwise
direction, but the labelings do not have to be consistent. So the
particles may not have a common sense of orientation. However,
our model allows a particle to maintain a fixed orientation as it
moves because in an expanded state it knows along which edge it
expands from both sides.
Let A ⊆ V be any initial distribution of contracted particles such
that Geqt |A is connected. Consider the graph Geqt |V \A induced by
the unoccupied nodes in Geqt . We call a connected component of
Geqt |V \A an empty region. Let N (R) be the neighborhood of an
empty region R in Geqt . Then all nodes in N (R) are occupied and
we call the graph Geqt |N (R) a border. Since Geqt |A is a connected
finite graph, exactly one empty region has infinite size while the
remaining empty regions have finite size. We define the border corresponding to the infinite empty region to be the unique outer border and refer to a border that corresponds to a finite empty region
as an inner border, see Figure 1. The common sense of clockwise
rotation can be used to give each border a unique orientation.
The leader election algorithm operates independently on each
border. Due to the nature of Geqt a particle can be adjacent to at
most three different empty regions for each region a particle simulates one agent. For simplicity and ease of presentation we assume
for now that agents have a global view of the border they are part
of and that agents act synchronously. At any given time, some subset of agents on a border will consider themselves candidates, i.e.
potential leaders of the system. Initially, every agent considers itself a candidate. Between any two candidates on a border there is
a (possibly empty) sequence of non-candidate agents. We call such
a sequence a segment and specifically refer to the segment coming
after (before) a candidate c in the direction of the border as the front
segment (back segment) of c. We denote the lengths of those segments as |f s(c)| and |bs(c)|. We use front candidate (f c(c)) and
back candidate (bc(c)) to denote the candidates at the end of these
segments. We drop the c in parentheses if it is clear from the context. We define the distance d(c1 , c2 ) between candidates c1 and

Figure 1: The left part shows an example of a particle structure
in Geqt . A contracted particle is depicted as a black dot, and an
expanded particle is depicted as two black dots connected by an
edge. The right part shows a particle structure with 3 borders.
The outer border is solid and the two inner borders are dashed.
We assume the standard asynchronous computation model, i.e.,
only one particle can be active at a time. Whenever a particle is
active, it can perform an arbitrary bounded amount of computation
(involving its local memory as well as the shared memories with
its neighboring particles) followed by no or a single movement. A
round is over once every particle has been activated at least once.

68

c2 as the number of agents between c1 and c2 when going from c1
to c2 in the direction of the border. We say a candidate c1 covers a
candidate c2 (or c2 is covered by c1 ) if |f s(c1 )| > d(c2 , c1 ). The
leader election progresses in phases. In each phase, each candidate
executes Algorithm 1. A phase consists of three synchronized subphases, i.e., agents can only progress to the next subphase once all
agents have finished the current subphase.

ticle simply joins the line. Whenever the line cannot be extended
any more because of an empty spot, particles are drawn to that spot
using the following idea:
All particles connected to the line that are not yet part of it, and
have an empty spot in the direction in which the line needs to grow,
form the root of a tree of particles so that all particles belong to
one of these trees. Every root tries to travel along the line in the
direction in which it is grown. If this is not possible any more
because the next spot is already occupied by an particle, it joins
the tree of that particle. Otherwise, it moves forward on top of
the line and drags the particles of its tree with it in a worm-like
fashion (which can be realized with the handover operation). This
guarantees that eventually all particles end up being part of the line.

Algorithm 1 Leader Election in the Geometric Amoebot Model
Subphase 1:
pos ← position of f c
if covered by any candidate or |f s| < |bs| then
return not leader

5.

Subphase 2:
if coin flip results in heads then
transfer candidacy to agent at pos
Subphase 3:
if only candidate on border then
if outside border then
return leader
else
return not leader
In the first subphase candidates are eliminated deterministically.
In the second subphase candidates that survived the first subphase
are eliminated in a randomized fashion by transferring candidacy
to another agent. This transferral of candidacy means that c withdraws its own candidacy but at the same time promotes the agent
at position pos (i.e., the front candidate of c in subphase 1) to be
a candidate. Subphase 1 and 2 make sure that eventually there is
only one leader on each border. Since we assume that each particle
has a global view, it can detect whether it is on the outer or an inner
border. Hence, the algorithm will elect a unique leader.
Algorithm 1 can be implemented entirely as a local-control protocol. This implementation heavily relies on token passing. Since a
detailed description of the protocol cannot be presented in this brief
announcement, we focus on the local-control protocol for one part
of the algorithm: the test whether the last remaining candidate of
a border is on the outer border or an inner border. We can achieve
this as follows. The candidate sends a token along its border that
keeps track of the sum of angles the path takes with respect to the
orientation of the candidate. Once the token manages to return to
its origin (the candidate itself), the sum of angles stored in the token is inspected. It turns out that it is always 360◦ for the outer
border and −360◦ for an inner border. Since we work on Geqt , we
just need to count integer multiples of 60◦ so that instead of the
actual angle we can simply store an integer k in the token such that
the angle is k · 60◦ . Note that the range of k during the traversal
of the path is unbounded. However, since we are only interested in
whether the value of k is 6 or −6 once the token returns to its origin, we can use a counter modulo 5 so that for the outer border we
get k = 1 and for an inner border k = 4. This way, the token only
needs 3 bits of memory independently of the shape of the border.

4.

SELF-STABILIZING ALGORITHMS

The leader election algorithm for Geqt can be extended to a selfstabilizing leader election algorithm with O(log∗ n) memory using
the results of [1, 4] (i.e., we use their self-stabilizing reset algorithm
on every border in order to recover from failure states). However,
it is not possible to design a self-stabilizing algorithm for the line
formation problem. The reason for this is that even a much simpler
problem we call the movement problem cannot be solved in a selfstabilizing manner.
In the movement problem we are given an initial distribution A
of particles that can be in a contracted as well as expanded state,
and the goal is to change the set of nodes occupied by the particles
without causing disconnectivity. For the ring of expanded particles
it holds that for any protocol P there is an initial state so that P
does not solve the movement problem. To show this we essentially
consider two cases: suppose that there is any state s for some particle in the ring that would cause that particle to contract. In this
case set two particles on opposite sides of the ring to that state, and
the ring will break due to their contractions. Otherwise, P would
not move any particle of the ring, so also in this case it would not
solve the movement problem. It is easy to see that if the movement
problem cannot be solved in a self-stabilizing manner, then also
the line formation problem cannot be solved in a self-stabilizing
manner. Hence, while self-stabilizing leader election is possible,
self-stabilizing line formation is not possible, which also implies
that it is not possible for the particles to decide in this setting when
a leader has been successfully chosen.

6.

REFERENCES

[1] Baruch Awerbuch and Rafail Ostrovsky. Memory-efficient and
self-stabilizing network RESET (extended abstract). In 13th
Annual ACM Symposium on Principles of Distributed
Computing, Los Angeles, California, USA, August 14-17,
1994, pages 254–263, 1994.
[2] Zahra Derakhshandeh, Shlomi Dolev, Robert Gmyr,
Andréa W. Richa, Christian Scheideler, and Thim Strothmann.
Brief announcement: amoebot - a new model for
programmable matter. In 26th ACM Symposium on
Parallelism in Algorithms and Architectures, Prague, Czech
Republic - June 23 - 25, 2014, pages 220–222, 2014.
[3] Alon Itai and Michael Rodeh. Symmetry breaking in
distributive networks. In 22nd Annual Symposium on
Foundations of Computer Science, Nashville, Tennessee, USA,
28-30 October 1981, pages 150–158, 1981.
[4] Gene Itkis and Leonid A. Levin. Fast and lean self-stabilizing
asynchronous protocols. In 35th Annual Symposium on
Foundations of Computer Science, Santa Fe, New Mexico,
USA, 20-22 November 1994, pages 226–239, 1994.

LINE FORMATION IN THE GEOMETRIC AMOEBOT MODEL

Our line formation algorithm is based on the leader election strategy above. Once a leader has been determined, the leader is used
as a seed to arrange the particles in a straight line starting from that
leader. Every time the line can be extended by a particle, that par-

69

A Jamming-Resistant MAC Protocol for Single-Hop
Wireless Networks
Baruch Awerbuch∗

Andrea Richa

Christian Scheideler†

Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA

Dept. of Computer Science
Arizona State University
Tempe, AZ, USA

Dept. of Computer Science
Technical University of Munich
85748 Garching, Germany

baruch@cs.jhu.edu

aricha@asu.edu

scheideler@in.tum.de

ABSTRACT

1.

In this paper we consider the problem of designing a medium access control (MAC) protocol for single-hop wireless networks that
is provably robust against adaptive adversarial jamming. The wireless network consists of a set of honest and reliable nodes that are
within the transmission range of each other. In addition to these
nodes there is an adversary. The adversary may know the protocol
and its entire history and use this knowledge to jam the wireless
channel at will at any time. It is allowed to jam a (1 − ²)-fraction
of the time steps, for an arbitrary constant ² > 0, but it has to make
a jamming decision before it knows the actions of the nodes at the
current step. The nodes cannot distinguish between the adversarial jamming or a collision of two or more messages that are sent
at the same time. We demonstrate, for the first time, that there is
a local-control MAC protocol requiring only very limited knowledge about the adversary and the network that achieves a constant
throughput for the non-jammed time steps under any adversarial
strategy above. We also show that our protocol is very energy efficient and that it can be extended to obtain a robust and efficient
protocol for leader election and the fair use of the wireless channel.

Jamming can disrupt wireless transmission and can occur either
unintentionally in the form of interference, noise or collision at the
receiver side or in the context of an attack. A jamming attack is
easy to perform since (i) no special hardware is needed for it to
be launched, (ii) it can be implemented by simply listening to the
open medium and broadcasting in the same frequency band as the
network, and (iii) if launched wisely, it can lead to significant disruptions with small incurred cost for the attacker. Jamming attacks
usually aim at the physical layer and are realized by means of a
high transmission power signal that corrupts a communication link
or an area, but they may also occur at the medium access control
(MAC) layer; an adversary may either corrupt control packets or
reserve the channel for the maximum allowable number of slots,
so that other nodes experience low throughput by not being able to
access the channel.
Traditional defenses against jamming focus on the design of physical layer technologies, such as spread spectrum (e.g., [24, 19,
18]). Spread spectrum techniques are useful because if signals are
widely spread, it becomes harder for the jammer to detect the start
of a packet quickly enough in order to jam it. Unfortunately, protocols such as 802.11b use relatively narrow spreading [11]. The
spreading factor for 1Mbps 802.11 is only a factor of 11. Other versions and rates in 802.11 spread signals by equal or smaller factors
[4]. Hence, a jammer that can simultaneously block a relatively
small number of frequencies would render spread spectrum techniques useless in these scenarios.
Besides defenses at the physical layer, it is also interesting to
study defenses at the MAC layer since in contrast to the physical
layer, the MAC layer is usually in software and can be changed,
so that even wireless devices that do not have a built-in protection
against jammers can be made robust against them. However, the
802.11 MAC protocol does not offer much protection here since
recent results show that the 802.11 MAC protocol cannot efficiently
handle even simple, oblivious jammers [2].

Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and WideArea Networks—Access schemes; F.2.2 [Analysis of Algorithms
and Problem Complexity]: Nonnumerical Algorithms and Problems—Sequencing and scheduling

General Terms
Algorithms, Reliability, Theory

Keywords
wireless ad-hoc networks, MAC protocols, jamming
∗
Supported by NSF CCF 0515080, ANIR-0240551, CCR0311795, and CNS-0617883
†
Supported by DFG grant SCHE 1592/1-1.

1.1

INTRODUCTION

Our model

In this paper we consider the problem of designing a MAC protocol for single-hop wireless networks that is provably robust against
adaptive adversarial jamming at the physical layer. The wireless
network consists of a set of n honest and reliable nodes that are
within the transmission range of each other. All of the nodes are
continuously contending for sending a packet on the wireless channel. We assume that time proceeds in synchronous time steps and
in each time step any node may decide to transmit a packet. A node
may either transmit a message or sense the channel at a time step,
but it cannot do both, and there is no immediate feedback mechanism telling a node whether its transmission was successful. A

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’08, August 18–21, 2008, Toronto, Ontario, Canada.
Copyright 2008 ACM 978-1-59593-989-0/08/08 ...$5.00.

45

1.3

node who is sensing the channel may either (i) sense an idle channel (in case no other node is transmitting at that time), (ii) sense a
busy channel (in case two or more nodes transmit at the time step),
or (iii) receive a packet (in case exactly one node transmits at the
time step). In addition to these nodes there is an adversary. We
allow the adversary to know the protocol and its entire history and
to use this knowledge in order to jam the wireless channel at will
at any time (i.e, the adversary is adaptive). Whenever it jams the
channel, all nodes will notice a busy channel. However, the nodes
cannot distinguish between the adversarial jamming or a collision
of two or more messages that are sent at the same time. We assume
that the adversary is only allowed to jam a (1 − ²)-fraction of the
time steps, for an arbitrary constant ² > 0, and it has to make a
jamming decision before it knows the actions of the nodes at the
current step.
We allow the adversary to perform bursty jamming. More formally, an adversary is called (T, λ)-bounded for some T ∈ N and
0 < λ < 1 if for any time window of size w ≥ T the adversary
can jam at most λw of the time steps in that window. A MAC
protocol is called c-competitive against some (T, λ)-bounded adversary (with high probability or on expectation) if, for any sufficiently large number of time steps, the nodes manage to perform
successful message transmissions in at least a c-fraction of the time
steps not jammed by the adversary (with high probability or on expectation).
Our goal is to design a symmetric local-control MAC protocol
that is constant competitive against any (T, 1 − ²)-bounded adversary, i.e., there is no central authority controlling the nodes, and the
nodes have symmetric roles at any point in time. The nodes do not
know ², but we do allow them to have a very rough upper bound of
their number n and T . More specifically, we will assume that the
nodes have a common parameter γ = O(1/(log T + log log n)).
Such an estimate leaves room for a superpolynomial change in n
and a polynomial change in T over time, so it does not make the
problem trivial (as would be the case if the nodes knew constant
factor approximations of n or T ). Next, we formally state our contributions before we go on discussing related work.

1.2

Related Work

Wireless network jamming has been extensively studied in the
applied networking domain (e.g., [28, 27, 17, 16, 5, 1, 26, 4, 19,
18, 20, 25]). Mechanisms for launching jamming attacks (e.g., [28,
17, 16, 5]) as well as defense mechanisms against these attacks
(e.g., [17, 28, 1, 26, 5, 4, 19, 18]) have been proposed and validated
through simulations and experiments.
There are many different forms of jammers, and detecting sophisticated jammers is not easy. Xu et al. [17], for example, observe that simple methods based on signal strength and carrier sensing are unable to conclusively detect the presence of a jammer.
Also the packet delivery ratio cannot be used to clearly distinguish
between link problems due to mobility, congestion or jamming.
Hence, enhanced detection schemes are necessary. To address this
need, the authors propose two enhanced detection protocols that
employ consistency checking. While being more effective than the
prior detection schemes, these protocols still leave room for ambiguities.
Traditional defenses against jamming primarily focus on the design of physical layer technologies, such as spread spectrum [24,
19, 18]. As argued in the introduction, while widely spread frequencies could potentially help in guarding against physical layer
jamming, spread spectrum techniques cannot be used effectively in
the relatively narrow frequency bands used by the 802.11 standard.
More recent work has also focused on various MAC layer strategies in order to handle jamming, including coding strategies [5],
channel surfing and spatial retreat [29, 1], or mechanisms to hide
messages from a jammer, evade its search, and reduce the impact of
corrupted messages [26]. Most of these strategies have only been
evaluated experimentally and would not help against the jammers
considered in this paper.
A recent study [2] shows both theoretically and experimentally
that an adaptive jammer, such as the one proposed here, can dramatically reduce the throughput of the standard random backoff MAC
protocol of the IEEE802.11 standard with only limited energy cost
on the adversary side (please also refer to [2] for other references
on jamming in 802.11).
Adversarial jamming has also been studied theoretically. There
are two basic approaches in the literature. The first assumes that
messages may be corrupted at random (e.g. [21]), and the second
bounds the number of messages that the adversary can transmit or
disrupt due to, for example, a limited energy budget (e.g. [12, 8]).
In a single hop wireless network (like ours), messages will not be
corrupted independently at random (every time the jammer transmits, all messages in that time step will be corrupted); moreover,
an adaptive adversary seems more powerful than one that jams uniformly at random [2]. Hence, we focus on the second line of theoretical work since it is more relevant to the results in this paper.
The latest results in [8, 12] address adversarial jamming at both
the MAC and network layers, where the adversary may not only
be jamming the channel but also introducing malicious (fake) messages (possibly with address spoofing). The results in [8] only consider the scenario that the nodes have one message to transmit (e.g.,
a broadcast operation). When translated to our continuous data
stream scenario, the protocol presented in [8] would not be able
to sustain a constant-competitive ratio if the adversary is allowed
to jam more than half of the time steps (i.e., if ² < 1/2), given
the fact that their single message broadcast algorithm takes at least
twice as many steps as the number of time steps utilized by the
jammer. Moreover, [8] assumes that the nodes have knowledge of
n and of the fact that the adversary has a bounded number of messages it can transmit (in contrast, we only need the nodes to have
an estimate on log log n and log T ).

Our contribution

Suppose that n ≥ 2, i.e., we have at least two honest nodes
in the system. Let N = max{T, n}. In this paper, we present
the first MAC protocol that is constant competitive w.h.p. under
any (T, 1 − ²)-bounded adversary if the protocol is executed for
Ω( 1² log N max{T, 1² (log3 N )(log T + log log n)2 }) many time
steps. It does not need to know ², so ² can be an arbitrarily small
constant (as long as ² = Ω(1/ log3 N )). The only information
it needs to be constant competitive is that the nodes have a common parameter γ = O(1/(log T + log log n)). In practice, log T
and log log n are reasonably small so that this is not a serious constraint. Also, as mentioned earlier, such an estimate leaves room
for a superpolynomial change in n and a polynomial change in
T over time. The MAC protocol is very simple and symmetric,
and it can recover quickly from any state. We also show that the
MAC protocol is very energy efficient. In fact, it converges to a
bounded amount of energy consumption under continuous adversarial jamming. In addition to this, we will show how to extend
the MAC protocol in order to obtain a robust and efficient protocol for leader election and the fair use of the wireless channel.
More specifically, our leader election protocol needs O( 1² log N
max{T, 1² (log3 N )(log T + log log n)2 }) steps until a leader is
selected and all nodes are aware of that, and our fair channel use
protocol essentially needs O(n/²) many steps until a fair channel
use is guaranteed. All runtime bounds hold with high probability.

46

Q
P ROOF
holds that q0 = v (1 − pv ) and
P . ItQ
q1 = v pv w6=v (1 − pw ). Hence,

In [12], the authors consider a wireless network in which node
positions form a grid where multiple (at most t) adversarial nodes
are allowed in the direct neighborhood of a node. If t is at most a
suitably small constant, then they give a protocol for reliable broadcast of a single message given that there is a fixed bound on the
number of time steps the adversary is disrupting communication (if
t is large, no broadcast protocol is guaranteed to terminate). The
authors only show that eventually the broadcast operation will be
completed, but give no bounds on how long that will take. Moreover, their algorithms will clearly deplete the energy of the nonfaulty nodes at a higher rate than that of the faulty nodes.
Most of the theoretical work on the design of efficient MAC protocols has focused on random backoff protocols (e.g., [3, 6, 10, 9,
15, 22]) that do not take jamming activity into account and therefore are not robust against it. MAC protocols have also been designed in the context of broadcasting (e.g., [7]) and clustering (e.g.,
[14]). Most of them use random backoff or tournaments in order to
handle interference and thereby achieve a fast runtime.
In general terms, in a random backoff protocol, each node periodically attempts to transmit a message starting with a certain
probability p. In case the message transmission is unsuccessful
(due to interference), the node will retry sending the message in
the next time steps with monotonically decreasing probabilities (for
example, p2 , p4 , p8 , . . .) until the message is successfully transmitted or the minimum allowable probability is reached. In a dense
network (as in our single-hop scenario), an adversary with knowledge of the MAC protocol would simply wait until the nodes have
reached transmission probabilities that are inversely proportional to
the number of close-by nodes to start jamming the channel, forcing the nodes to lower their transmission probabilities by so much
that a constant throughput is not achievable. In tournaments, local
leader election is used to determine the node that is allowed to use
the wireless medium for its message transmission. If the adversary
jams the channel whenever a local leader is about to be selected,
most protocols will fail and start all over, so that only rarely a message will get through. Also any work that relies on physical carrier
sensing in order to adjust the transmission probabilities of the nodes
(e.g., [13]) would fail in the presence of jamming as a blocked channel would be interpreted as a message collision. Hence, no solution
is currently available that can provably handle the jammers considered here.

1.4

and

w

In each time step, every node v is sending a message with probability pv . If it decides not to send a message, it checks the following
two cases:
• If the wireless channel is idle, then pv := (1 + γ)pv .
• If exactly one message is sent, then pv := (1 + γ)−1 pv .
The beauty of the algorithm is that it ignores blocked time steps,
which makes it more robust against adversarial jamming. However,
there is a catch to this strategy because it only works well as long as
p does not get too high. If p is initially very high or by chance gets
very high, it will be extremely unlikely for the nodes to observe one
of the two cases above. Hence, further ideas are necessary.
Our idea is to use a threshold Tv for each node v that cuts its
time into time intervals. If v does not observe a successful message
transmission for Tv many steps, then pv is decreased. In this way,
eventually p will become small. However, since the algorithm is not
aware of T , the time window of the adversary, p may be decreased
too quickly or too slowly in this way. Hence, we need proper rules
for adapting Tv over time. It turns out that the following rules work:
whenever v senses a successful transmission, Tv is decreased by
1, and whenever v does not sense a successful transmission for Tv
time steps, Tv is increased by 1 for the next time interval considered
by v. One may ask why Tv should not be decreased as well if an
idle channel is sensed, but interestingly this is not a good rule, as
will come out in the analysis. Next, we give a formal description of
our MAC protocol.

THE ROBUST MAC PROTOCOL

2.2

Description of the MAC protocol

In our MAC protocol, each node v maintains a probability value
pv , a threshold Tv and a counter cv . The parameter γ is the same for
every node and is set to some sufficiently small value in O(1/(log T
+ log log n)). Thus, we assume that the nodes have some polynomial estimate of T and even rougher estimate of n. Let p̂ be
any constant so that 0 < p̂ ≤ 1/24. Initially, every node v sets
Tv := 1, cv := 1 and pv := p̂. Afterwards, the protocol works
in synchronized time steps. We assume synchronized time steps
for the analysis, but a non-synchronized execution of the protocol
would also work as long as all nodes operate at roughly the same
speed.

Basic approach

Our MAC protocol is based on a simple idea. Suppose that each
node v decides to send a message at the current time step with
probability
Ppv with pv ≤ p̂ for some small constant 0 < p̂ < 1.
Let p = v pv , q0 be the probability that the channel is idle and
q1 be the probability that exactly one node is sending a message.
Then the following claim holds.
q0
1−p̂

≥

1 Y
q0 · p
(1 − pw ) =
1
−
p̂
1
− p̂
v
w
X Y
pv
(1 − pw ) = q0 · p
pv

Hence, if the nodes observe that the number of time steps in
which the channel is idle is essentially equal to the number
Pof time
steps in which exactly one message is sent, then p =
v pv is
likely to be around 1. Otherwise, they know that they need to adapt
their probabilities. Therefore, if we had sufficiently many cases in
which an idle channel or exactly one message transmission is observed (which is the case if the adversary does not heavily jam the
channel and p is not too large), then one can adapt the probabilities
pv just based on these two events and ignore all cases in which the
wireless channel is blocked (either because the adversary is jamming it or at least two messages interfere with each other). Essentially, the following strategy could be used at every node for some
small enough γ > 0:

Structure of the paper

C LAIM 2.1. q0 · p ≤ q1 ≤

q1

X

v

In this section we present and analyze our MAC protocol. We
start with a description of our basic ideas behind the protocol then
we formally describe the protocol and analyze its competitiveness.
At the end of the section, we also study its energy efficiency.

2.1

≤

which implies the claim.

In Section 2 we will present and analyze our MAC protocol, and
in Section 3 we will show how to extend it to robust leader election
and the fair use of the wireless channel.

2.

q1

· p.

47

√
and Tv ≤ F /2 for every node v with probability at least 1 −
c
1/N for any constant c > 0 (which we will also call with high
probability or w.h.p. in the following). Since initially Tv = 1 and
pv = p̂ for every v, this implies that the MAC protocol achieves a
constant competitiveness in the first time frame, w.h.p., and due to
the properties on Tv and pv , this also holds for polynomially many
time frames, w.h.p.
The proof for time frame I proceeds as follows. Consider some
fixed subset U = V \ {v0 }. A time step t or subframe I 0 of I
with starting time t is called good if pt ≤ 9. Otherwise, it is called
bad. First, we show√ that for any subframe I 0 in which initially
√
pt ≥ 1/(f 2 (1 + γ)2 f ), also afterwards pt ≥ 1/(f 2 (1 + γ)2 f ),
0
w.h.p. (Lemma
√ 2.4). Then we show that for any subframe I with
Tv ≤ (3/4) F for every node v ∈ U at the beginning of I 0 , the
subsequent subframe is good with probability at least 1 − 1/f c for
any constant c > 0 (which we will call with moderate probability
or w.m.p.) (Lemma 2.7). Based on the insights gained in the proof,
we show that in a good subframe I 0 , all non-jammed time steps
in I 0 are good w.m.p. (Corollary 2.11). After that, we prove that
a constant fraction of the time steps in such a subframe also have
probabilities lower bounded by a constant (Lemma 2.12), w.h.p.,
which implies that the MAC protocol is constant competitive√for I 0
w.m.p. (Lemma 2.13). If at the beginning of frame I, Tv ≤ F /2
for every node
√ v ∈ U , then during the first eighth of I, called J,
Tv ≤ (3/4) F , no matter what happens to the nodes in J. This
allows us to show that a constant fraction of the subframes of J
are constant competitive w.h.p., which implies that the MAC protocol is constant competitive for J w.h.p. (Lemma 2.14). With
√ that
insight we can show that if at the beginning of J, Tv ≤ F /2
for every node v ∈ U , then this also holds at the end of J w.h.p.
(Lemma 2.15). Hence, all eighths of I have a constant competitiveness, w.h.p., which implies
√ that I has a constant competitiveness
and at the end of I, Tv ≤ F /2 for every node v, w.h.p. Applying
these results inductively over all time frames I yields Theorem 2.2.
At the end of this subsection, we also study the recovery properties of our MAC protocol (Theorem 2.16). It turns out that the
MAC protocol can get quickly out of any set of (pv , cv , Tv )-values,
which implies that it also works well if the nodes enter the network
at arbitrary times and with arbitrary values instead of starting the
protocol at the same time and with the same values, which is not
realistic in practice.

In each step, each node v does the following. v decides with
probability pv to send a message. If it decides not to send a message, it checks the following two conditions:
1. If v senses an idle channel, then pv := max{(1 + γ)pv , p̂}.
2. If v successfully receives a message, then pv := (1+γ)−1 pv
and Tv := max{1, Tv − 1}.
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no step among the past
Tv time steps in which v sensed a successful message transmission,
then pv := (1 + γ)−1 pv and Tv := Tv + 1.

2.3

Robustness

Let N = max{T, n}. In this section, we will prove the following theorem.
T HEOREM 2.2. For n ≥ 2 the MAC protocol is constant competitive w.h.p. under any (T, 1−²)-bounded adversary if the protocol is executed for at least Θ( 1² log N max{T, ²γ12 log3 N }) many
time steps.
Notice that for n = 1 a node will never experience a time step
with a successful transmission. Hence, it would just keep reducing
its access probability in our protocol, thereby reaching a dormant
state, which is the best it can do in this case as there is no one else
to communicate with. Thus, it only makes sense to consider the
case n ≥ 2. More on energy efficiency will be discussed later.
The proof of the theorem will frequently use the following general form of the well-known Chernoff bounds, which may be of
independent interest. They are derived from Chernoff bounds presented in [23].
L EMMA 2.3. Consider any set of binary random variables X1 ,
. . .Q
, Xn . Suppose
Qthat there are values p1 , . . . , pn ∈ [0, 1] with
E[ i∈S Xi ] ≤P i∈S pi for everyPset S ⊆ {1, . . . , n}. Then it
n
holds for X = n
i=1 Xi and µ =
i=1 pi and any δ > 0 that
µ
¶µ
δ2 µ
eδ
−
P[X ≥ (1 + δ)µ] ≤
≤ e 2(1+δ/3)
1+δ
(1 + δ)
Q
Q
If, on the other hand, it holds that E[ i∈S Xi ] ≥ i∈S pi for
every set S ⊆ {1, . . . , n}, then it holds for any 0 < δ < 1 that
µ
¶µ
2
e−δ
P[X ≤ (1 − δ)µ] ≤
≤ e−δ µ/2
1−δ
(1 − δ)

L EMMA 2.4. For any subframe I 0 in which initially pt0 ≥ 1/
the last time step t of I 0 satisfies pt ≥ 1/(f 2 (1 +
γ)
), w.h.p.
P ROOF. We start with the following claim about the maximum
number of times nodes decrease their probabilities in I 0 due to cv >
Tv .
√
(f 2√
(1 + γ)2 f ),
2 f

Let V be the set of all nodes. For the proof of the theorem we will
consider all possible decompositions of V into a single node v0 and
U = V \ {v0 }. Let pt (v) be node v’s access probability
P pv at the
beginning of the t-th time step. Furthermore, let pt = v∈U pt (v)
(i.e., without node v0 ) and L = Ω( 1² log N max{T, ²γ12 log3 N })
be the number of time steps for which we study the competitiveness of the protocol. If L ≥ N , we will redefine N to N =
max{T, n, L} in order to cover long runtimes. If we can prove
a constant competitiveness for any such L, Theorem 2.2 follows.
We prove the theorem by induction over sufficiently large time
frames. Let I be a time frame consisting of α² log N subframes I 0

C LAIM 2.5. If in subframe I 0 the number of successful message
transmissions
is at most k, then every node v increases Tv at most
√
k + 2f many times.
P ROOF. Only successful message transmissions reduce Tv . If
there is no successful message transmission within Tv many steps,
Tv is increased. Suppose that k = 0. Then the number of times
a node v increases Tv is upper bounded by the largest possible `
PTv0 +`
0
so that i=T
0 i ≤ f , where Tv is the initial size of Tv . For any
v√
0
Tv ≥ 1, ` ≤ 2f , so the claim is true for k = 0. At best, each
additional successful transmission allows us to reduce all thresholds for v by 1, so we are searching for the maximum ` so that
√
PTv0 −k+`
max{i, 1} ≤ f . This ` is upper bounded by k + 2f ,
i=Tv0 −k
which proves our claim.

2

log3 N }, where α and β are sufficiently
of size f = max{T, αβ
²γ 2
large constants. Let F = α² log N · f denote the size of I. We
√
assume√that at the beginning of I, pt ≥ 1/(f 2 (1 + γ)2 f ) and
Tv ≤ F /2 for every node v. Our goal is to show that in this
case the MAC protocol is constant competitive for I w.r.t. every
√
subset U = V \ {v0 } and at the end of I, pt ≥ 1/(f 2 (1 + γ)2 f )

48

This claim allows us to show the following claim.

and it holds for any set S of time steps prior to some time step t
that
Y
P[Yt = 1 |
Ys = 1] ≤ 1/f 2 + p̂

C LAIM 2.6. √Suppose that for the first time step t0 in I 0 , pt0 ∈
[1/(f 2 (1 + γ)2 f ), 1/f 2 ]. Then there is a time step t in I 0 with
pt ≥ 1/f 2 , w.h.p.

s∈S

Thus, the Chernoff bounds imply that k1 < g/8, w.h.p. (given
that p̂ ≤ 1/24). That, however, would violate the condition that
k0 ≤ 2k1 + g/2.
Note that the choice of g is not oblivious as the adversary may
adaptively decide to set g based on the history of events. Hence,
we need to sum up the probabilities over all adversarial strategies
of selecting g in order to show that none of them succeeds, but
since there are only f many, and for each the claimed property
holds w.h.p., the claim follows.

P ROOF. Suppose that there are g non-jammed time steps in I 0 .
Let k0 be the number of these steps with an idle channel and k1 be
the number of these steps with a successful message transmission.
Furthermore, let k2 be the maximum number of times a node v
increases Tv in I 0 . If all time steps t in I 0 satisfy pt < 1/f 2 , then
it must hold that
k0 − log1+γ (1/pt0 ) ≤ k1 + k2

So suppose that there is a time step t in I 0 with pt ≥ 1/f 2 . If t
belongs to one of the last β log N non-jammed steps in I 0 , then it
follows for the probability pt0 at the end of I 0 that

This is because no v has reached a point with pt (v) = p̂ in this
case, which implies that for each time step t0 with an idle channel,
pt0 +1 = (1+γ)pt0 . Furthermore, at most log1+γ (1/pt0 ) increases
of pt due to an idle channel would be needed to get pt to 1/f 2 , and
then there would have to be a balance between further increases
2
and decreases of pt in order to avoid
√ the case pt ≥ 1/f . We know
from Claim 2.5 that k2 ≤ k1 + 2f . Hence,
p
p
k0 ≤ 2 log1+γ f + 2 f + 2k1 + 2f
√
Suppose that 2 log1+γ f + 4 f ≤ ²f /2, which is true if f =
2
Ω(1/² ) is sufficiently large (resp. ² = Ω(1/ log3 N )). Since
g ≥ ²f due to our adversarial model, it follows that we must satisfy
k0 ≤ 2k1 + g/2.
For any time step t with pt ≤ 1/f 2 ,
X
P[≥ 1 message transmitted at t] ≤
pv (t) = pt + p̂

pt0 ≥

given that ² = Ω(1/ log3 N ) as at most β log N decreases of pt
can happen due to a successful transmission and at most β log N +
√
2f decreases of pt can happen due to exceeding Tv .
Suppose, on the other hand, that there is no time step t among the
last β log N non-jammed steps in I 0 with pt ≥ 1/f 2 . In this case,
we assume that a specific step t in I 0 outside of these last steps is
the last time step with pt ≥ 1/f 2 . When defining k0 , k1 and k2 as
above but from that point on it follows that√ pt0 at the end of I 0 is
still bounded from below by 1/(f 2 (1+γ)2 f ) as long as k0 ≥ k1 .
Our analysis above implies that this is true w.h.p. (see Claim 2.8 for
similar arguments in the other direction), which finishes the proof
of Lemma 2.4.
√
L EMMA 2.7. For any subframe I 0 with Tv ≤ (3/4) F for all
0
nodes v at the beginning of I , the last time step t of I 0 satisfies
pt ≤ 9 w.m.p.

v

≤

1/f 2 + p̂

where p̂ is due to node v0 not considered in pt . Hence, E[k0 ] ≥
(1−1/f 2 − p̂)g and E[k1 ] ≤ (1/f 2 + p̂)g. In order to prove bounds
on k0 and k1 that hold w.h.p., we can use the general Chernoff
bounds stated above. For any step t, let the binary random variable
Xt be 1 if and only if the channel is idle at step t or pt ≥ 1/f 2 .
Then
P[Xt = 1]

=
=

P[channel idle and pt ≤ 1/f 2 ] + P[pt > 1/f 2 ]
P[pt ≤ 1/f 2 ] · P[channel idle | pt ≤ 1/f 2 ] +

≥

P[pt > 1/f 2 ]
P[pt ≤ 1/f 2 ](1 − 1/f 2 − p̂) + P[pt > 1/f 2 ]

≥

1 − 1/f 2 − p̂

P ROOF. We first show that there is a time step t in I 0 with pt ≤
6, w.h.p. Let the time steps in which the adversary does not jam
the channel and at most one message is sent by the nodes be called
useful. Suppose that there are g useful time steps in I 0 . Let k0
be the number of these steps with an idle channel and k1 be the
number of these steps with a successful message transmission. In
order to establish a relationship between k0 and k1 we need the
following claims.
C LAIM 2.8. If all time steps t ∈ I 0 satisfy pt > 6, then it holds
for any g ≥ δ log N for a sufficiently large constant δ that k1 ≥ k0
w.h.p.

and since this probability bound holds irrespective of prior steps
and is independent of the adversarial jamming decision at time t, it
follows for any set S of time steps prior to some time step t that
Y
P[Xt = 1 |
Xs = 1] ≥ 1 − 1/f 2 − p̂

P ROOF. Let q0 (t) be the probability of an idle channel and q1 (t)
be the probability of a successful message transmission at a useful
step t. If pt > 6, then it follows from Claim 2.1 that

s∈S

P[channel idle]

Q
Thus, for any set of time steps S it holds that E[ s∈S Xs ] ≥ (1 −
1/f 2 − p̂)|S| . Together with the fact that g ≥ ²f ≥ α log N , the
Chernoff bounds imply that, w.h.p., either k0 > 3g/4 (given that
p̂ ≤ 1/24) or we have a time step t with pt ≥ 1/f 2 .
On the other hand, let the binary random variable Yt be 1 if and
only if exactly one message is sent at time t and pt ≤ 1/f 2 . Then
P[Yt = 1]

=
≤

√
1
1
√
· (1 + γ)−2β log N + 2f ≥ 2
2
f
f (1 + γ)2 f

=
≤

q0 (t)
q0 (t)
≤
q0 (t) + q1 (t)
q0 (t) + pt · q0 (t)
1
1
=
1+6
7

irrespective of what happened at previous time steps. Hence, E[k0 ]
≤ g/7 under the assumption that all useful time steps t satisfy pt >
6. Thus, our Chernoff bounds yield k0 ≤ g/2 w.h.p. (given that δ
is a sufficiently large constant), which implies that k1 ≥ k0 .

P[pt ≤ 1/f 2 ] · P[one msg sent | pt ≤ 1/f 2 ]
1/f 2 + p̂

Now we are ready for the following claim.

49

C LAIM 2.9. If all time steps in I 0 satisfy pt > 6, then it must
hold w.h.p. that

the number of useful steps with a successful message transmission.
It must hold that k0 ≥ (4/5)k1 + ln(1 + δ)/ ln(1 + γ) so that
pt ≥ (1 + δ)φ. Also, k0 + k1 = g. Hence, k0 ≥ (4/9)g +
(5/9) ln(1 + δ)/ ln(1 + γ) ≥ max{(4/9)g, ln(1 + δ)/ ln(1 + γ)}.
It holds that E[k0 ] ≤ g/7, so the Chernoff bounds imply that

k1 − 2 log1+γ N ≤ (5/4)k0
P ROOF. If exactly one message is sent at a step t, then pt+1 ≥
(1 + γ)−1 pt and
−1

pt+1 ≤ (1 + γ)

−1

(pt − p̂) + p̂ ≤ (1 + γ)

−1

pt + γ(1 + γ)

P[k0 ≥ (4/9)g]

p̂

because only the sending node does not decrease its probability,
and for this node the maximum probability is p̂. For pt > 6 it follows that pt+1 ∈ [(1 + γ)−1 pt , (1 + γ)−4/5 pt ]. From Claim 2.8
we now that after the first δ log N useful steps, there must have
been more steps with a successful transmission than with an idle
channel for any one of the remaining useful steps, w.h.p, which implies that for each of them, pv < p̂ for all nodes v. Thus, whenever
there is an idle channel for these steps, pt+1 = (1 + γ)pt . Hence,
if we start with pt = 6 after the first δ log N useful steps, then
in order to avoid a step t0 with pt0 ≤ 6 in I 0 we must have that
k1 ≤ (5/4)k0 . Since pt might be as high as p̂n initially, we can allow at most (5/4) log1+γ N further events of a successful message
transmission without having a step t0 with pt0 ≤ 6.
Since log1+γ N = ω(log N ), it holds that

Hence,
P[pt ≥ (1 + δ)φ]

≤

≤

P[k0 ≥ (1 + 2)g/7]

≤

e−[2

X

2

/(2(1+2/3))](g/7)

P[k0 ≥ (4/9)g] ≤

ln(1+δ)
g≥ ln(1+γ)

≤

8(1 + δ)

= e−g/6
X

e−g/6

ln(1+δ)
g≥ ln(1+γ)
1
− 6 ln(1+γ)

≤ 8(1 + δ)−1/(6γ)

Since we assume that γ = O(1/ log f ), it follows that w.m.p.,
pt ≤ (1 + δ)6 for any particular time step t after t0 , resulting in
the lemma with δ = 1/2.
Claim 2.10 with φ = 9 and δ = 1/3 implies the following result.
C OROLLARY 2.11. For any good subframe I 0 , all non-jammed
time steps t of I 0 satisfy pt ≤ 12 w.m.p.

δ log N + (5/4) log1+γ N ≤ 2 log1+γ N
for a sufficiently large N , which implies the claim.

We also need to show that for a constant fraction of the nonjammed time steps in a good subframe, pt is also lower bounded
by a constant. Recall that p̂ ≤ 1/24.

Also, k0 + k1 = g. Suppose that g ≥ δ log1+γ N for a sufficiently large constant δ. It holds that

L EMMA 2.12.
For any subframe I 0 in which initially pt ≥ 1/
√
2 f
(f (1 + γ)
), at least 1/8 of the non-jammed steps t satisfy
pt ≥ p̂, w.h.p.

(g − k0 ) − 2g/δ ≤ (5/4)k0 ⇔ k0 ≥ (4/9)(1 − 2/δ)g

2

We know from the proof of Claim 2.8 that for any useful step t with
pt > 6, P[channel idle] ≤ 71 . Hence, E[k0 ] ≤ g/7. Since random decisions are made independently in each step, our Chernoff
bounds imply that k0 < (4/9)(1 − 2/δ)g w.h.p. if δ is sufficiently
large.
Thus, if I 0 contains at least δ log1+γ N useful steps, we are done.
Otherwise, notice that for every node v it follows from the MAC
√
protocol and the choice√of f and F that if initially Tv ≤ (3/4) F ,
0
0
then Tv can
√ be at most F during I . Let us cut I into m intervals
of size 2 F each. It is easy to check that if β in the definition of f
is sufficiently large compared to δ, then m ≥ 3δ log1+γ N . If there
are less than δ log1+γ N useful steps, then at least 2δ log1+γ N of
these intervals do not contain any useful step, which implies that pv
is reduced by at least (1+γ)−1 by each v in each of these intervals.
Hence, altogether, every pv gets reduced by a factor of at least
(1 + γ)−2δ log1+γ N during I 0 . The useful time steps can only raise
that by (1 + γ)δ log1+γ N , so altogether we must have pt ≤ 6 at
some time point during I 0 , w.h.p.
In the following, let t0 denote any time in I 0 with pt0 ≤ 6. We
finally prove the following claim.

P ROOF. Let G be the set of all non-jammed time steps in I 0 and
S be the set of all steps t in G with pt < p̂. Let g = |G| and
s = |S|. If s ≤ 7g/8, we are done. Hence, consider the case that
s ≥ 7g/8.
Suppose that pt must be increased k0 many times to get from its
initial value up to a value of p̂ and that pt is decreased k1 many
times in S due a successful message transmission. Furthermore,
let k2 be the maximum number of times a node v decreases pv
due to cv > Tv in the MAC protocol. For S to be feasible (i.e.,
probabilities can be assigned to each t ∈ S so that pt < p̂) it must
hold for the number ` of times in S in which the channel is idle that
` ≤ k0 + k1 + k2
For the special case that k0 = k2 = 0 this follows from the fact that
whenever there is a successful message transmission, pt is reduced
to pt+1 ≥ (1 + γ)−1 pt . On the other hand, whenever there is an
idle channel, it holds that pt+1 = (1 + γ)pt because of pt < p̂.
Thus, if ` > k1 , then one of the steps in S would have to have a
probability of at least p̂, violating the definition of S. k0 comes into
the formula due to the startup cost of getting to a value of p̂, and k2
comes into the formula since the reductions of the pt (v) values due
to cv > Tv in the MAC protocol allow up to k2 additional increases
of pt for S to stay feasible.
First, we bound `. If pt < p̂, then P[idle channel at step t] ≥
1−p̂−p̂ (where the second p̂ is due to node v0 ), irrespective of prior
time steps, Hence, E[`] ≥ (1 − 2p̂)s. For p̂ ≤ 1/24 our Chernoff
bounds imply because of s ≥ 7g/8 ≥ (7/8)²f that `√ ≥ s/2
w.h.p. If at the beginning
of I 0 , pt ≥ 1/(f 2 (1 + √
γ)2 f ) then
√
k0 ≤ 2 log1+γ f +2 f . Moreover, k2 ≤ g/8+k1 + 2f because
√
of Claim 2.5. Hence, k0 + k1 + k2 ≤ 2 log1+γ f + 2 f + 2k1 +

C LAIM 2.10. For any useful time step t after a step t0 in I 0 with
pt0 ≤ φ for some φ ≥ 6 and any constant δ > 0 it holds that
P[pt ≥ (1 + δ)φ] ≤ 8 · (1 + δ)−1/(6γ)
P ROOF. Suppose that t0 be the last useful time step before step
t in I 0 with pt0 ≤ φ. Let g be the number of useful time steps from
t0 to t. Then g ≥ ln(1 + δ)/ ln(1 + γ) because otherwise it is not
possible that pt ≥ (1 + δ)φ. Recall that for any useful step r with
pr ≥ 6, P[pr+1 = (1 + γ)pr ] ≤ 1/7. If exactly one message is
sent at a useful step, then pr+1 ∈ [(1 + γ)−1 pr , (1 + γ)−4/5 pr ].
Let k0 be the number of useful steps with an idle channel and k1 be

50

√
of J, Tv ≤ F /2 for all v,
L EMMA 2.15. If at the beginning
√
then it holds that also Tv ≤ F /2 at the end of J, w.h.p.
P ROOF. We know from Lemma 2.14 that for any node v our
protocol is constant competitive for V \ {v} w.h.p. Hence, every node v notices Ω(²|J|) successful message transmissions in J
w.h.p. Tv is maximized at the end of J if all of these successful
transmissions happen at the beginning of J, which would get Tv
down to 1. Afterwards,
P Tv can raise to a value of at most t for the
maximum t with ti=1 i ≤ |J|. Since such a t can be at most
p
p
√
2|J|, it follows that Tv can be at most 2F/8 = F /2 at the
end of J, w.h.p.

√
g/8 + 2f , which must be at least s/2 so that ` ≤ √
k0 + k1 + k2
(given that ` ≥ s/2). Suppose that 2 log1+γ f + 4 f ≤ ²f /16
(which is true if f = Ω(1/²2 ) is large enough). Then for this to be
true it must hold that
2k1 + g/8 + g/16 ≥ (7g/8)/2

⇔

k1 ≥ g/8

If k1 ≥ g/8 then also k1 ≥ s/8, so our goal will be to show that
k1 < s/8 w.h.p.
If pt < p̂, then P[successful message transmission at step t] ≤
2p̂, irrespective of prior time steps. Hence, E[k1 ] ≤ 2p̂s. Furthermore, for p̂ ≤ 1/24 our Chernoff bounds imply because of
s ≥ 7g/8 ≥ (7/8)²f that k1 < s/8 w.h.p. Since there are at most
f 2 ways (for the adversary) of choosing g and s, this holds for any
combination of g and s, which yields the lemma.

Inductively using Lemmas 2.13 and 2.15 on the eighths of frame
I implies that our MAC protocol is constant
competitive√on I and
√
at the end of I, pv ≥ 1/(f 2 (1 + γ)2 f ) and Tv ≤ F /2 for
all v w.h.p. Hence, our MAC protocol is constant competitive for
L many time steps, w.h.p., for any L = Ω( 1² log N max{T, ²γ12
log3 N }), which implies Theorem 2.2.
Finally, we show that our protocol can quickly recover from any
setting of the (Tv , cv , pv )-values.

Combining the results above, we get:
L EMMA 2.13. For any good subframe I 0 the MAC protocol is
constant competitive in I 0 w.m.p.
P ROOF. From Corollary 2.11 and Lemma 2.12 we know that in
a good subframe at least 1/8 of the non-jammed time steps t have
a constant probability value pt w.m.p. For these steps there is a
constant probability that a message is successfully sent. Using the
Chernoff bounds results in the lemma.

T HEOREM 2.16. For any pt0 and T̂ = maxv Tv it takes at
most O( 1² log1+γ (1/pt0 ) + T̂ 2 ) many time steps, w.h.p., until the
√
2
2 f
MAC
) and maxv Tv
√ protocol satisfies again pt ≥ 1/(f (1+γ)
≤ F /2 for the original definitions of F and f above.

Consider now the first eighth of frame I, called J.

√

√

P ROOF. Suppose that pt0 < 1/(f 2 (1 + γ)2 f ) for some time
point t0 . Then it follows from the constraints of the adversary and
the Chernoff bounds that it takes at most δ² log1+γ (1/pt0 ) steps
for some sufficiently large constant δ to get the system from pt0 up
1/2
to pt0 , w.h.p. (in fact, with a probability of at least 1 − pct0 for
δ
any constant c, irrespective of T̂ ). Another 2²
log1+γ (1/pt0 ) steps

L EMMA√2.14. If at the beginning of J, pv ≥ 1/(f 2 (1+γ)2 f )
and√Tv ≤ F /2 for all nodes v, then we also have pv ≥ 1/(f 2 (1+
γ)2 f ) at the end of J for every v and the MAC protocol is constant
competitive for J, w.h.p.
P ROOF. The bound for pv at the end of J directly follows from
Lemma 2.4.
√
Suppose, as a worst case, that initially Tv = F /2 for some
v. Clearly, Tv assumes the maximum possible value at the end
of J if Tv is never
√ decreased
√ in J. Since Tv can be increased at
most (F/8)/( F /2) = F /4 √
many times in J, Tv can reach a
maximum value of at most (3/4) F inside of J, so we can apply
Lemma 2.7.
α
Recall that J consists of k = 8²
log N many subframes, numbered I1 , . . . , Ik . For each Ii , let the binary random variable Xi be
1 if and only if Ii is good. From Lemma 2.7 it follows that for any
i ≥ 1 and any set S ⊆ {1, . . . , i − 1},
Y
P[Xi = 1 |
Xj = 1] ≥ 1 − 1/f c

1/2

1/4

will then get the system from pt0 to pt0 , w.h.p. (in fact, with
1/2
probability at least 1 − (pt0 )c for any constant c). Continuing
1/2i

1/2i+1

these arguments in order to get from pt0 to pt0
it follows
log
(1/p
)
steps
are
needed
to get
that altogether at most 2δ
t
0
1+γ
²
1 √
the system from pt0 to a probability pt ≥ f 2 (1+γ)
,
w.h.p.
(or
2 f

more precisely, with probability at least 1 − 1/N√c ).
It remains to bound
√ the time to get Tv down to F /2 for every v.
It holds that T̂ ≤ F /2 if and only if F ≥ 4T̂ 2 . Hence, consider a
time frame I of size F 0 = max{F, 4T̂ 2 } for the old definition of F
above, where I starts at the point √
at which the probabilities pv have
recovered to pt ≥ 1/(f 2 (1 + γ)2 f ). Then all the proofs above go
through and imply that I is constant competitive. Moreover, when
cutting I into pieces of size |I|/32 instead of |I|/8, the proof of
Lemma√2.15 implies that at the end of the first 1/32-piece J of I,
Tv ≤ F 0 /4, w.h.p. Hence, the time frames of the nodes shrank
by a factor of at least 2 in J. Inductively
using this bound, it follows
√
that also at the end of I, Tv ≤ F 0 /4 for all v, w.h.p. This allows
us to reduce F 0 by
√a factor of 2 for the next frame I. Also for this
F 0 , we get Tv ≤ F 0 /4 for all v, w.h.p., so we can keep shrinking
I by a factor of 2 until |I| = F for the original F√considered in
our proofs above. Altogether, the recovery to T̂ ≤ F /2 for all v
takes at most O(T̂ 2 ) time.
Combining the two upper bounds for the recovery time yields
the theorem.

j∈S

for some constant c that canQbe made arbitrarily large. Hence, for
any set S ⊆ {1, . . . , k}, E[ i∈S Xi ] ≥ (1 − 1/f c )|S| . Our Chernoff bounds therefore imply that at most (α/24²) log N of the subframes in J are bad, w.h.p, if α is sufficiently large. According
to Lemma 2.13, each of the good subframes is constant competitive w.m.p., where the probability bounds are only based on events
in the subframes themselves and therefore hold irrespective of the
other subframes (given that each of them is good). So the Chernoff
bounds imply that at most (α/24²) log N of them do not result in a
constant competitiveness of the MAC protocol, w.h.p. The remaining (α/24²) log N subframes in J achieve constant competitiveness, which implies that the MAC protocol is constant competitive
on J, w.h.p.

2.4

We finally need the following lemma that bounds Tv . The proof
of this lemma requires considering all possible decompositions of
V into a node v0 and U = V \ {v0 } so that every node experiences
many successful transmissions.

Energy efficiency

Next, we show that our MAC protocol is very energy-efficient
under adversarial attacks. The first lemma follows directly from
our insights gained in the previous subsection.

51

L EMMA 2.17. For any time frame I of size F as defined above,
the total energy spent by all the nodes together on sending out messages is bounded by O(F ) w.h.p.

1. If v senses an idle channel, then pv := max{(1 + γ)pv , p̂}.
2. If v successfully receives a message with some counter sw ,
then pv := (1 + γ)−1 pv and Tv := max{1, Tv − 1}. If
v is still in the state “unknown”, then v checks the following two cases: If sv ≤ sw then v becomes a “follower”,
otherwise v becomes a “leader”. In any case, v sets sv :=
max{sv , sw } + 1.

If the adversary performs permanent jamming, the energy spent
on message transmissions even converges, i.e., our MAC protocol
reaches a dormant stage.
P
L EMMA 2.18. Consider any time step t0 with v pv ≤ p and
maxv Tv ≤ T̂ for some values p > 0 and T̂ ≥ 1/γ. Then for any
continuous jamming attack starting at t0 the total energy consumption of the nodes during the entire attack is at most O(p · T̂ /γ +
log N ) w.h.p.

Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no step among the past
Tv time steps in which v sensed a successful message transmission,
then pv := (1 + γ)−1 pv and Tv := Tv + 1.
This protocol has the following performance.

P ROOF. First, we determine the expected energy consumption
of a single node v. Let pv (t) be the probability that v transmits
a message in round t0 + t. Due to our MAC protocol, pv (t) decreases by (1 + γ)−1 at latest for t = T̂ , then another time after
T̂ + 1 further steps, another time after T̂ + 2 further steps, and
so on. Hence, the total expected energy consumption of v for any
continuous jamming attack is at most
X
Tv · pv (t0 )(1 + γ)Tv −T̂
Tv ≥T̂

=

pv (t0 )

X

T HEOREM 3.1. Within O( 1² log N max{T, ²γ12 log3 N }) many
steps, the leader election protocol reaches a state in which there is
exactly one leader and the other nodes are followers, w.h.p.
P ROOF. At the beginning, all counters sv are set to 0. Once the
first node, say v, is able to successfully transmit a message, then all
nodes w 6= v will become a follower and set sw to 1. v may then
go on being successful for k more steps until the first node w 6= v
successfully transmits a message. When w transmits its message, it
also sends sw = k+1 which is greater than sv since sv is still set to
0. Hence, v will become a leader. According to the analysis of our
original MAC protocol, which is embedded in our leader election
protocol, it takes at most O( 1² log N max{T, ²γ12 log3 N }) many
steps until at least two nodes successfully transmit a message (as
constant competitiveness is ensured for any set U = V \ {v0 }),
w.h.p., which yields the theorem.

(T̂ + i)(1 + γ)−i

i≥0

≤

1+γ
· T̂ · pv (t0 ) +
γ

=

O(pv (t0 )T̂ /γ)

µ

1+γ
γ

¶2
· pv (t0 )

Summing up over all nodes, we obtain a total expected energy consumption of O(p · T̂ /γ). Since all transmission decisions are done
independently at random, the Chernoff bounds imply a total energy
consumption of at most O(p · T̂ /γ + log N ) w.h.p.

Once a node becomes a leader, it may then select a fixed access
probability of p̂ (which, as we know from our analysis, does not
cause problems for the competitiveness of the follower nodes) so
that an effective coordination of the follower nodes is possible.

In our MAC protocol, beyond f steps after any initial choice of
the access probabilities, p = O(log N ), w.h.p. This is due to the
proof of Lemma 2.7 and the fact that for p ≥ c log N , the probability that an idle channel is experienced is at most 1/N c , so further
increasing p has a polynomially small probability. Furthermore,
T̂ = O(log2 N/γ) w.h.p. for any constant ² given that all nodes
v start with Tv = 1. Hence, the total energy consumption of our
MAC protocol under a permanent attack that starts after f steps
would be bounded by O(log3 N/γ 2 ) w.h.p.

3.2

3.

APPLICATIONS OF THE MAC PROTOCOL

In this section we will demonstrate how our robust MAC protocol can be extended to perform robust leader election or to select
fair access probabilities for the nodes.

3.1

Establishing fairness

In our original MAC protocol, some probabilities may eventually
dominate the others. This is due to the fact that whenever there is a
successful message transmission, all nodes sensing the successful
transmission are lowering their access probabilities while the access probability of the sending node stays the same. Since nodes
with a larger access probability are more likely to transmit a message, there is a tendency towards preserving access probabilities of
those nodes that already have large access probabilities so that the
gap between large and small probabilities will increase over time.
This would result in an unfair use of the channel among the nodes.
In order to ensure fairness, we slightly modify our MAC protocol.
In the new protocol, each node v maintains a counter sv for successful transmissions and a counter mv of the different nodes it has
seen so far. It also maintains a state in {covered, uncovered} and
memorizes in olds the last counter it has seen so far. Initially, every node v sets Tv := 1, cv := 1 and pv := p̂. Also, sv and mv
are set to 0, olds is set to -1, and the state is set to “uncovered”.
Afterwards, every node v does the following in each step.
v decides with probability pv to send a message. If it does so, its
message is piggy-backed with sv and its state. If it decides not to
send a message, it checks the following two conditions:

Leader election

Consider the following adaptation of the MAC protocol. In addition to cv , Tv and pv , every node v maintains a counter sv for
successful transmissions. v also stores one of the states {unknown,
leader, follower}. Initially, every node v sets Tv := 1, cv := 1 and
pv := p̂. Also, v sets sv to 0 and its state to “unknown”. Afterwards, v does the following in each step.

1. If v senses an idle channel, and v is still uncovered then
pv := max{(1 + γ)pv , p̂}.

v decides with probability pv to send a message. If it does so,
its message is piggy-backed with sv . If it decides not to send a
message, it checks the following two conditions:

2. If v successfully receives a message with some counter sw ,
then v considers the following cases.

52

• If w is uncovered and sw 6= olds then mv := mv + 1.
If v is covered then it sets pv := p̂/mv .

our lemma as well. For all other nodes w, we consider the following cases. If u is uncovered, then each of these nodes increases
mw by 1 (because of su > olds), and otherwise, they leave mw
as before, which satisfies our claim. Putting all pieces together, the
lemma follows.

• If v is uncovered and sw > sv then v changes its state
to “covered”, sets mv := mv + 1 and pv := p̂/mv .
• If v is uncovered and sw ≤ sv then v sets pv := (1 +
γ)−1 pv and Tv := max{1, Tv − 1}.

Lemmas 3.4 and 3.5 and the way the covered nodes set their
access probabilities immediately yield the following result.

olds := sw and sv := max{sv , sw } + 1.

C OROLLARY 3.6. At any time, the set of covered nodes together
have an access probability in [(1 − 1/(m + 1))p̂, p̂], where m is
the number of nodes with successful transmissions so far, and this
probability is shared evenly among them.

Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no step among the past
Tv time steps in which v sensed a successful message transmission,
then pv := (1 + γ)−1 pv and Tv := Tv + 1.

Hence, once all nodes are covered, fairness is established among
all nodes. The following lemma bounds the time necessary to cover
all nodes.

We will prove the following result for this protocol:
T HEOREM 3.2. If T ≤ nδ for some constant δ < 1 and p̂ ≤
1/48, then it takes at most O(n/²) time steps until all nodes have
an access probability of Θ(1/n), w.h.p.

L EMMA 3.7. If T ≤ nδ for some constant δ < 1 and p̂ ≤
1/48, then it takes at most O(n/²) time steps until until all nodes
are covered, w.h.p.

We first state some properties of sv and mv .

P ROOF. First, we establish the following claim.

L EMMA 3.3. At any time, sv is equal to the number of successful transmissions performed so far, except for the most recent
transmissions of v without a transmission of a node w 6= v afterwards.

C LAIM 3.8. All nodes that have not been able to successfully
send a message so far have the same access probabilities.
P ROOF. Notice that all nodes that have not been able to successfully send a message so far have the property that whenever
there was an idle channel or a successful message transmission, all
of them noticed that. Since all of them start with cv := 1 and
Tv := 1, this implies that their time frames are in synchrony and
any changes in the access probabilities due to a channel condition
or the case cv > Tv are done in synchrony as well. As all nodes
initially start with pv = p̂, the claim follows.

P ROOF. We prove the lemma by induction over the number of
successful transmissions. Initially, the lemma is certainly true. So
consider the situation that it is still true after the first k successful
transmissions. Let v be the origin of the last message transmission.
Then sw = k for all w 6= v and sv = k − rv where rv is the
number of most recent transmissions of v without a transmission
of a node w 6= v afterwards.
If the next node successfully transmitting a message is v, then
all other nodes w receive a message with sv ≤ sw and therefore
increase sw by 1, which satisfies the lemma. If, on the other hand,
some node u 6= v transmits a message, then v receives a message
with su > sv , so it updates sv to su + 1 = k + 1. All nodes
w 6∈ {u, v} satisfy su ≤ sw , so they increase sw by 1. In both
cases, the lemma holds again, which completes the proof.

Notice that even if the nodes do not initialize pv , cv and Tv with
the same values, the analysis of our original MAC
√ protocol implies
that as long as all nodes v initially satisfy Tv ≤ F (for the parameter F in the previous section), it takes at most F steps until a point
is reached at which all Tv = 1 for all v, so the non-successful nodes
will operate in synchrony from that point on (though with different
probability offsets). For simplicity, however, we will consider the
case of Claim 3.8.
Now, it follows from the analysis of the original MAC protocol
that the time needed for the first node to be covered is polylogarithmic in N w.h.p. Once the first node has been covered, the
remaining nodes quickly become covered as well, as shown next.

L EMMA 3.4. A node is in the state “covered” if and only if it
has already successfully sent a message and received a message
from a node afterwards.
P ROOF. According to the protocol, a node v only becomes covered if sw > sv , so the lemma follows from Lemma 3.3.

C LAIM 3.9. Consider any consecutive sequence of log n nodes
that become covered during the algorithm after at least one node
has been covered. The number of successful transmissions they
need for that is O(log n) w.h.p.

L EMMA 3.5. mv counts the number of different nodes that have
successfully sent a message, except v itself if v has successfully sent
messages without receiving a message from another node so far.
P ROOF. We prove the lemma above by induction over the number of successful transmissions. Initially, the lemma is certainly
true. So suppose that it is true after the first k successful transmissions. Let v be the origin of the last message transmission. We
distinguish between several cases for the k + 1th message transmission.
Suppose that the next node successfully transmitting a message
is v. Then sv = olds and sv ≤ sw for every other node w
according to Lemma 3.3. Hence, no changes will happen to the
mw ’s. So suppose that the next node transmitting is u 6= v. Then
su > olds = sv according to Lemma 3.3. Thus, if v was still uncovered, then v changes to “covered” and increases mv by 1, which
satisfies the lemma. Otherwise, v does nothing, which also satisfies

P ROOF. Let Q
C be the set of covered nodes and m = |C|. Moreover, let p0 = v (1 − pv ) be the probability that the channel is
idle at a given time step. Since the covered nodes together have an
access probability of at least (1 − 1/(m + 1))p̂ at any time (Corollary 3.6) and the least recently successful but not yet covered node,
v, has an access probability of at most p̂, it holds that
X
Y
p0 · m
· p̂
P[node in C successful] =
pu
(1 − pw ) ≥
m
+1
u∈C
w6=v

and
P[node v successful] = pv

Y
w6=v

53

(1 − pw ) ≤

p0
· p̂
1 − p̂

Thus,
P[node in C successful] ≥

[5] J.T. Chiang and Y.-C. Hu. Cross-layer jamming detection and
mitigation in wireless broadcast networks. In Proc. of MobiCom ’07,
pages 346–349, 2007.
[6] Bogdan S. Chlebus, Dariusz R. Kowalski, and Mariusz A. Rokicki.
Adversarial queuing on the multiple-access channel. In Proc. of
PODC ’06, pages 92–101, 2006.
[7] A. Czumaj and W. Rytter. Broadcasting algorithms in radio networks
with unknown topology. Journal of Algorithms, 60(2):115 – 143,
2006.
[8] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and
suspicious sensors: On the efficiency of malicious interference in
wireless networks. In Proc. of OPODIS ’06, 2006.
[9] Leslie Ann Goldberg, Philip D. Mackenzie, Mike Paterson, and
Aravind Srinivasan. Contention resolution with constant expected
delay. Journal of the ACM, 47(6):1048–1096, 2000.
[10] Johan Hastad, Tom Leighton, and Brian Rogoff. Analysis of backoff
protocols for mulitiple accesschannels. SIAM Journal on Computing,
25(4):740–774, 1996.
[11] IEEE. Medium access control (MAC) and physical specifications. In
IEEE P802.11/D10, 1999.
[12] C.Y. Koo, V. Bhandari, J. Katz, and N.H. Vaidya. Reliable broadcast
in radio networks: The bounded collision case. In Proc. of PODC
’06, 2006.
[13] K. Kothapalli, C. Scheideler, M. Onus, and A. Richa. Constant
density spanners for wireless ad hoc networks. In Proc. of SPAA ’05,
pages 116–125, 2005.
[14] Fabian Kuhn, Thomas Moscibroda, and Roger Wattenhofer. Radio
Network Clustering from Scratch. In Proc. of ESA ’04, 2004.
[15] Byung-Jae Kwak, Nah-Oak Song, and Leonard E. Miller.
Performance analysis of exponential backoff. IEEE/ACM
Transactions on Networking, 13(2):343–355, 2005.
[16] Y.W. Law, L. van Hoesel, J. Doumen, P. Hartel, and P. Havinga.
Energy-efficient link-layer jamming attacks against wireless sensor
network mac protocols. In Proc. of SASN ’05, pages 76–88, 2005.
[17] M. Li, I. Koutsopoulos, and R. Poovendran. Optimal jamming attacks
and network defense policies in wireless sensor networks. In Proc. of
Infocom ’07, pages 1307–1315, 2007.
[18] Xin Liu, Guevara Noubir, Ravi Sundaram, and San Tan. Spread:
Foiling smart jammers using multi-layer agility. In Proc. of Infocom
’07, pages 2536–2540, 2007.
[19] Vishnu Navda, Aniruddha Bohra, Samrat Ganguly, and Dan
Rubenstein. Using channel hopping to increase 802.11 resilience to
jamming attacks. In Proc. of Infocom ’07, pages 2526–2530, 2007.
[20] R. Negi and A. Perrig. Jamming analysis of MAC protocols.
Technical report, Carnegie Mellon University, 2003.
[21] A. Pelc and D. Peleg. Feasibility and complexity of broadcasting
with random transmission failures. In Proc. of PODC ’05, 2005.
[22] Prabhakar Raghavan and Eli Upfal. Stochastic contention resolution
with short delays. SIAM Journal on Computing, 28(2):709–719,
1999.
[23] J. Schmidt, A. Siegel, and A. Srinivasan. Chernoff-Hoeffding bounds
for applications with limited independence. SIAM Journal on
Discrete Mathematics, 8(2):223–250, 1995.
[24] M. K. Simon, J. K. Omura, R. A. Schultz, and B. K. Levin. Spread
Spectrum Communications Handbook. McGraw-Hill, 2001.
[25] David Thuente and Mithun Acharya. Intelligent jamming in wireless
networks with applications to 802.11b and other networks. In Proc.
of MILCOM ’06, 2006.
[26] A.D. Wood, J.A. Stankovic, and G. Zhou. DEEJAM: Defeating
energy-efficient jamming in IEEE 802.15.4-based wireless networks.
In Proc. of SECON ’07, 2007.
[27] W. Xu, K. Ma, W. Trappe, and Y. Zhang. Jamming sensor networks:
attack and defense strategies. IEEE Network, 20(3):41–47, 2006.
[28] W. Xu, W. Trappe, Y. Zhang, and T. Wood. The feasibility of
launching and detecting jamming attacks in wireless networks. In
Proc. of MobiHoc ’05, pages 46–57, 2005.
[29] W. Xu, T. Wood, and Y. Zhang. Channel surfing and spatial retreats:
defenses against wireless denial of service. In Proc. of Workshop on
Wireless Security, page 80Ű89, 2004.

(1 − p̂)m
· P[node v successful]
m+1

which implies that the probability that k consecutive successful
transmissions are due to v is at most (1/(1 + c))k with c = (1 −
p̂)m/(m + 1). This is polynomially small if k = Ω(log n). Furthermore, when considering a consecutive sequence of O(log n)
nodes that become covered, it follows from the independence of
the transmission attempts of the nodes that altogether the number of
successful transmissions they need for that is O(log n) w.h.p.
It remains to bound the time until the uncovered nodes (at the
time of the transmission) have had Ω(n) successful transmissions.
Let v1 , v2 , . . . , vn be the order in which the nodes become covered, i.e., vi is the ith node with a successful transmission. Let
Ui = {v1 , . . . , vi } for all i ≥ 1. Once vi has had its first successful transmission,
P we consider the partition (Ui , V \ Ui ). For
Ui we know that u∈Ui pu ≤ 2p̂ ≤ 1/24 and at the time vi had
its P
first success, pvi is a 1/|V \ Ui−1 | = 1/(n − i + 1)-fraction
of v∈V \Ui pv . Hence, when switching from (Ui , V \ Ui ) to
(Ui+1 , V \ Ui+1 ), only a small fraction of the probability gets lost
in the uncovered nodes, and the probability in Ui stays bounded
by 1/24. In fact, as long as there are still at least f uncovered
nodes left, then the total
Q reduction in the access probability over a
subframe is at most 2f
g=f (1 − 1/g) ≥ 1/e. This is low enough
so that the analysis of the original MAC protocol still applies, i.e.
the protocol is constant competitive w.r.t. the still uncovered nodes
within time frames of size F , w.h.p. Once there are less than f
uncovered nodes, the analysis implies that at least one uncovered
node gets covered within a time frame of size F , w.h.p. Combining
that with Claim 3.9, it takes at most O(n/² + f · F ) steps, w.h.p.,
for all nodes to become covered. When assuming that T ≤ nδ for
some constant δ < 1, the lemma follows.

4.

CONCLUSIONS

In this paper we presented the first MAC protocol that is provably robust against adversarial jammers. In fact, our protocol can
even handle adaptive jammers. Many open questions remain. Can
the MAC protocol be extended to multi-hop networks? How can we
adapt to join and leave behavior or mobility of the nodes, and which
rate is sustainable without losing a constant competitiveness? Can
the MAC protocol be modified so that no knowledge about T and
n is required any more? We have tried several variants of our protocol that all had counterexamples. A constant γ appears to work
fine under stochastic jammers, but it does not seem to work under
adaptive jammers. What other applications than leader election and
a fair use of the wireless channel can be considered?

5.

REFERENCES

[1] G. Alnifie and R. Simon. A multi-channel defense against jamming
attacks in wireless sensor networks. In Proc. of Q2SWinet ’07, pages
95–104, 2007.
[2] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and
B. Thapa. On the performance of ieee 802.11 under jamming. In
Proc. of IEEE Infocom ’08, page 1265, 2008.
[3] Michael A. Bender, Martin Farach-Colton, Simai He, Bradley C.
Kuszmaul, and Charles E. Leiserson. Adversarial contention
resolution for simple channels. In Proc. of SPAA ’05, pages 325–332,
2005.
[4] T. Brown, J. James, and A. Sethi. Jamming and sensing of encrypted
wireless ad hoc networks. In Proc. of MobiHoc ’06, pages 120–130,
2006.

54

An Algorithmic Framework for Shape Formation Problems
in Self-Organizing Particle Systems
Zahra Derakhshandeh

Robert Gmyr

Andréa W. Richa

Computer Science and
Engineering, CIDSE,
Arizona State University, USA

Dept. of Computer Science,
University of Paderborn,
Germany

Computer Science and
Engineering, CIDSE,
Arizona State University, USA

arXiv:1504.00744v2 [cs.ET] 7 Apr 2015

zderakhs@asu.edu
gmyr@mail.upb.de
aricha@asu.edu
Christian Scheideler
Thim Strothmann
Dept. of Computer Science,
University of Paderborn,
Germany

Dept. of Computer Science,
University of Paderborn,
Germany

scheideler@upb.de

thim@mail.upb.de

ABSTRACT
Many proposals have already been made for realizing programmable matter, ranging from shape-changing molecules,
DNA tiles, and synthetic cells to reconfigurable modular
robotics. Envisioning systems of nano-sensors devices, we
are particularly interested in programmable matter consisting of systems of simple computational elements, called particles, that can establish and release bonds and can actively move in a self-organized way, and in shape formation problems relevant for programmable matter in those
self-organizing particle systems (SOPS). In this paper, we
present a general algorithmic framework for shape formation problems in SOPS, and show direct applications of this
framework to the problems of having the particle system
self-organize to form a hexagonal or triangular shape. Our
algorithms utilize only local control, require only constantsize memory particles, and are asymptotically optimal both
in terms of the total number of movements needed to reach
the desired shape configuration.

1.

INTRODUCTION

Imagine that we had a piece of matter that can change
its physical properties like shape, density, conductivity, or
color in a programmable fashion based on either user input
or autonomous sensing. This is the vision behind what is
commonly known as programmable matter. Programmable
matter has been the subject of many recent novel distributed
computing proposals, ranging from shape-changing molecules,
DNA tiles, and synthetic cells to reconfigurable modular
robotics. Each of these proposals pursued solutions for specific application scenarios with their own, special capabilities
and constraints.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
NANOCOM 2015 Boston, Massachusetts, USA
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.

We envision systems of nano-sensors devices that will have
very limited computational capabilities individually, but which
can collaborate to reach a lot more as a collective. Ideally,
those nano-sensor devices will be able to self-organize in order to achieve a desired collective goal without the need
of central control or external (in particular, human) intervention. For example, one could envision using a system
of self-organizing nano-sensor devices to identify and coat
(and possibly repair) leaks on a nuclear reactor without
the need for human intervention; self-organizing systems of
nano-sensor devices could also be used to monitor environmental and structural conditions in abandoned mines, on the
exterior of an airplane or spacecraft, bridges and other structures, possibly also self-repairing the structure— i.e., realizing what has been coined as ”smart paint”. The applications in the health arena are also endless, e.g., self-organizing
nano-sensor devices could be used within our bodies to detect and coat an area where internal bleeding occurs, eliminating the need of immediate surgery, or they could be used
to identify and isolate tumor/malignouos cells. In many applications, there may be a specific shape that one would like
the system to assume (e.g., a disc, or a line, or even any
compact shape).
Hence, from an algorithmic point-of-view, we are interested in programmable matter consisting of systems of simple computational elements, called particles, that can establish and release (communication or physical) bonds and can
actively move in a self-organized way, and in general shape
formation problems in those self-organizing particle systems
(SOPS).

1.1

Geometric Amoebot model

We will use the geometric amoebot model presented in [7,
6] as our basic model for SOPS.
In all of our shape formation algorithms, the set of particles will maintain a connected structure at all times. We
assume that we have a graph G(V, E) that represents the
relative positions that a connected set of particles may assume — i.e., V represents all possible positions of a particle (relative to the other particles in their structure) and
E represents all possible transitions between nodes. In the
geometric amoebot model we assume that G = Geqt , where

Geqt is the infinite regular triangular grid graph1 (see Part
(a) of Figure 1).
We briefly recall the main properties of the geometric
amoebot model. Each particle occupies either a single node
or a pair of adjacent nodes in Geqt , and every node can be
occupied by at most one particle. Two particles occupying
adjacent nodes are connected, and we refer to such particles
as neighbors.
Particles move through expansions and contractions: If a
particle occupies one node (i.e., it is contracted ), it can expand to an unoccupied adjacent node to occupy two nodes.
If a particle occupies two nodes (i.e., it is expanded ), it can
contract to one of these nodes to occupy only a single node.
Performing movements via expansions and contractions may
represent the way particles physically move, or may be seen
as a logical ”look-ahead and then move” logical operation. It
has several advantages, including allowing particles to abort
a movement if there is a conflict (see [7] for more details). A
particle always knows whether it is contracted or expanded
— in the latter, it also knows along which edge it expands
— and this information will be available to neighboring particles. A handover allows particles to stay connected as they
move; two scenarios are possible: a) a contracted particle p
can ”push” a neighboring expanded particle q and expand
into the neighboring node previously occupied by q, forcing q to contract, or b) an expanded particle p can ”pull”
a neighboring contracted particle q to a cell occupied by it
thereby expanding that particle to that cell, which allows p
to contract to its other cell. In part(b) of Figure 1, we illustrate a set of particles (some contracted, some expanded)
on the underlying graph Geqt .
Particles are anonymous but the bonds of each particle have unique labels, which implies that a particle can
uniquely identify each of its outgoing edges. Moreover, for
each particle the bonds are labeled in a consecutive way in
clockwise direction so that every particle has the same sense
of clockwise direction, but the particles may not have a common sense of orientation in a sense that they have different
offsets of the labelings (see Figure 1, Part (c)). Each particle
has a constant-size local memory in which it can store some
bounded amount of information, and any pair of connected
particles has a bounded shared memory that can be read
and written by both of them and that can be accessed using
the edge label associated with that connection. We assume
the standard asynchronous model from distributed computing, where the system of particles progresses by performing atomic actions, each of which affects the configuration
of one or two particles. Whenever a particle is activated
(i.e., performs an atomic action), it can perform an arbitrary bounded amount of computation (involving its local
memory as well as the shared memories with its neighboring particles) followed by no or a single movement. A round
is over once every particle has been activated at least once.

1.2

Our Contributions

In this paper, we present a general algorithmic framework
for shape formation problems in SOPS, which constitutes of
two basic algorithmic primitives: the spanning forest primitive and the snake formation primitive. We present concrete
applications of these two primitives to two specific shape
formation problems, namely to the problems of having the
1

The triangular grid graph Geqt is the dual graph of a regular
hexagonal tiling in 2D space.

system of particles self-organize to form a hexagonal shape
and to form a triangular shape. Both the hexagonal shape
and the triangular shape formation algorithms are optimal
with respect to work, which we measure by the total number of particle movements needed to reach the desired shape
configuration, as we prove in Theorems 1 and 2. Our algorithms rely only on local information (e.g., particles do not
have ids, nor do they know n, the total number of particles,
or have any sort of global coordinate/orientation system),
and require only constant-size memory particles.

1.3

Related Work

Many approaches related to programmable matter have
recently been proposed. One can distinguish between active
and passive systems. In passive systems (e.g., DNA computing [1, 2, 16], tile self-assembly systems [8, 13, 17]),) the
particles either do not have any intelligence at all (but just
move and bond based on their structural properties or due
to chemical interactions with the environment), or they have
limited computational capabilities but cannot control their
movements. We will not describe passive models in detail
as they are only of little relevance for our approach. On
the other hand in active systems, computational particles
can control the way they act and move in order to solve a
specific task. Robotic swarms, and modular robotic systems
are some examples of active programmable matter systems.
In the area of swarm robotics it is usually assumed that
there is a collection of autonomous robots that have limited sensing, and communication ranges, and that can freely
move in a given area. They follow a variety of goals, including for example shape formation problems (e.g., [9, 14]).
Surveys of recent results in swarm robotics can be found
in [11, 12]. While the analytical techniques developed in the
area of swarm robotics and natural swarms are of some relevance for this work, the individual units in those systems
have more powerful communication and processing capabilities than in the systems we consider.
The field of modular self-reconfigurable robotic systems
focuses on intra-robotic aspects such as the design, fabrication, motion planning, and control of autonomous kinematic machines with variable morphology (see e.g., [10, 19]).
Metamorphic robots form a subclass of self-reconfigurable
robots that share some of the characteristics of our geometric model [5]. The hardware development in the field
of self-reconfigurable robotics has been complemented by a
number of algorithmic advances (e.g., [3, 15, 14]), but so far
mechanisms that automatically scale from a few to hundreds
or thousands of individual units are still under investigation,
and no rigorous theoretical foundation is available yet.
The nubot model [18, 4] aims at providing the theoretical
framework that would allow for a more rigorous algorithmic study of biomolecular-inspired systems, more specifically of self-assembly systems with active molecular components. While bio-molecular inspired systems share many
similarities with our SOPS, there are many differences —
e.g., there is always an arbitrarily large supply of ”extra”
particles that can be added to the system as needed, and
the system allows for an additional (non-local) notion of
rigid-body movement.

2.

SHAPE FORMATION

In this paper we focus on solving shape formation problems
in the geometric amoebot model starting from any initial

Figure 1: Part (a) shows a section of Geqt ; nodes of Geqt are shown as black circles. Part (b) shows five particles on
Geqt : the underlying graph Geqt as a gray mesh; a particle occupying a single node is depicted as a black circle, and
a particle occupying two nodes is depicted as two black circles connected by an edge. Part (c) depicts two particles
occupying two non-adjacent positions on Geqt ; the particles have different offsets for their head bond labelings. Part
(d) shows an intermediate configuration of the HEX algorithm. The seed is depicted in green, retired particles are
black, and roots are red. Particle p is the last added particle to the retired structure. Hence, edge i connects p to the
retired particle p0 (edge i has the flag p0 .snakedir). The red arrow depicts the process of setting p.snakedir in clockwise
manner for p.
connected configuration of particles. We present a general
algorithmic framework for shape formation problems and
then specifically we investigate the Hexagonal Shape Formation (HEX) and the Triangular Shape Formation (TRI)
problems where the desired shape is a hexagon and a triangle respectively. We formally define a shape formation
problem as a tuple M = (I, G) where I and G are sets of
connected configurations. We say I is the set of possible
initial configurations and G is the set of goal configurations.
Accordingly, for the HEX problem, G would be all configurations where the positions of the set of particles induce
a hexagon on Geqt (note that depending on the number of
particles the constructed shape may not necessarily be a
perfect hexagon since the outer layer of the hexagon may
not be fully complete). Similarly, for the TRI problem, G
is equal to the set of all configurations that constitute a
triangle in Geqt (except for possibly the outer layer of the
triangle, which may be partially full). We say an algorithm
A solves a shape formation problem M if for any execution
of A on a system in an arbitrary configuration from I, A
terminates (i.e., the execution eventually reaches a configuration in which each particle does not move anymore) in
one of the valid configurations in G.
Before we proceed, we provide some preliminaries. For all
algorithms we assume that there is a specific particle we call
the seed particle, which provides the starting point for constructing the respective shape. If a seed is not available, one
can be chosen using the leader election algorithm proposed
in [7] . We define the set of states that a particle can be in
as inactive, follower, root, and retired. Initially, all particles
are inactive, except the seed particle, which is always in a
retired state. In addition to its state, each particle p may
maintain a constant number of flags in its shared memory.
For an expanded particle, we denote the node the particle
last expanded into as the head of the particle and call the
other occupied node its tail. In our algorithm, we assume
that every time a particle contracts, it contracts out of its
tail. Note that with this convention, the node occupied by
the head of a particle still is occupied by that particle after
a contraction. Part (c) of Figure 1 shows an example of the
labeling of the heads of two particles on Geqt .
Generally speaking, the shape formation algorithms we
propose for hexagonal and triangular shapes progress as follows. Particles organize themselves into a spanning set of

disjoint trees where the roots of the trees are non-retired
particles adjacent to the partially constructed shape structure (consisting of all retired particles). Root particles lead
the way by moving in a predefined direction around the current structure.
The remaining particles (i.e., the followers) follow behind
the leading root particles, hence the system flattens out towards the direction of movement. Once the leading particles
reach a valid position where the shape can be extended (following the rules for the snake formation for the particular
shape), they stop moving and change their state to retired
as well. This process continues until all particles become
retired. Note that the spanning forest component of this
general approach is the same for any shape formation algorithm: It is only in the rules that determine the next
valid position to be filled in the shape structure being built
that the respective algorithms differ. We determine the next
valid position to be filled sequentially following a snake (i.e.,
a line of consecutive positions in Geqt ), that fills in the space
of the respective shape structure and scales naturally with
the number of particles in the system.

2.1

Spanning Forest Algorithm

The Spanning Forest algorithm primitive, given in Algorithm 1, is a building block we use for all of our shape formation problems. This primitive was also used in [7], where we
present a preliminary self-organizing algorithm for forming
a straight line of particles. We present the algorithm here
for completeness. Each particle p continuously runs Algorithm 1 until it becomes retired. If particle p is a follower,
it stores a flag p.parent in its shared memory corresponding
to the edge adjacent to its parent p0 in the spanning forest
(any particle q can then easily check if p is a child of q).
Initially all system particles, except the seed, are inactive.
In a nutshell, the particles that are touching the seed or
other retired particle become roots; the root particles move
around the partially constructed shape structure in a clockwise manner until they find a valid position on the snake
and become retired; follower particles follow the movement
of the respective root until they become roots themselves.
As we will see later, the initial snapshots of Figures 2 and 3
illustrate the spanning forest formation for the respective
initial particle configurations.

2.2

Hexagonal Shape Formation

Algorithm 1 Spanning Forest Algorithm for Shape Formation
Depending on p’s current state, a particle p behaves as described below:
inactive: If p is connected to a retired particle, then p becomes a root particle. Otherwise, if an adjacent particle p0 is a
root or a follower, p sets the flag p.parent on the shared memory corresponding to the edge to p0 and becomes
a follower. If none of the above applies, it remains inactive.
follower: If p is contracted and connected a retired particle, then p becomes a root particle. Otherwise, it considers
the following three cases: (i) if p is contracted and p’s parent p0 is expanded, then p expands in the direction
given by p.parent in a handover with p0 , and may need to adjust p.parent to still point to particle p0 after the
handover; (ii) if p is expanded and has a contracted child particle p0 , then p executes a handover with p0 ; (iii)
if p is expanded, has no children, and p has no inactive neighbor, then p contracts.
root:
Particle p runs the corresponding snake formation algorithm (Algorithm 2 or 3, for HEX or TRI resp.), and
becomes retired accordingly. Otherwise, it considers the following three cases: (i) if p is contracted, it tries to
expand in the direction given by RootDirection (p); (ii) If p is expanded and has a child p0 , then p executes
a handover contraction with p0 ; (iii) if p is expanded and has no children, and no inactive neighbor, then p
contracts.
retired:
p performs no further action.
RootDirection (p):
Let i be the label of an edge connected to a retired particle.
while edge i points to a retired particle do
i ← label of next edge in clockwise direction
return i

We now investigate the Hexagonal Shape Formation (HEX)
problem where the system of particles has to assume the
shape of a hexagon (but for the outer layer, which may not
be completely full) in Geqt . The hexagon will be constructed
around the seed particle. Note that a hexagon in Geqt is actually a disk, since it can be defined by the set of all nodes
of Geqt within a certain distance r from a seed node.
We will organize the particles according to a spiral snake
structure which will incrementally add new layers to the
hexagon, scaling naturally with the number of particles in
the system. In order to characterize the snake formation for
a given shape formation problem, one only needs to specify
the direction in which the line of particles forming the snake
should continue to grow, for each new particle added to the
snake. Hence once a particle finds the next valid position on
the snake, it will become retired and set the snake direction
accordingly (by correctly setting the flag p.snakedir on the
respective edge). Different rules for snake formation will
realize different shapes. In particular, Algorithm 2 specifies
the rules for the spiral snake formation for HEX.
Initially, the seed particle p sets the flag p.snakedir in the
shared memory corresponding to one of its adjacent edges
(e.g., the edge with label 0). Any particle adjacent to a retired particle becomes a root following the spanning forest
algorithm. Each root p moves in a clockwise fashion around
the structure of retired particles until it finds the next position to extend the hexagonal snake (i.e., a position connected to a retired particle via an edge flagged p.snakedir)
and becomes retired, following Algorithm 2 (see Part (d) of
Figure 1).
Figure 2 depicts some snapshots of a run of HEX algorithm. See Appendix for the proof of the following theorem.
Theorem 1. Our algorithm solves the HEX problem in
worst-case optimal O(n2 ) work.

2.3

Triangular Shape Formation

Now we investigate the Triangular Shape Formation problem (TRI) where the system of particles has to assume a final
triangular shape on Geqt (but for possibly the outer layer).

Algorithm 2 Snake Formation for HEX
if p is a contracted root then
if p has an adjacent edge i to p0 with a flag p0 .snakedir,
where p0 is retired then
. retired condition
while edge i is connected to a retired particle do
i ← label of next edge in clockwise direction
p sets the flag p.snakedir for edge i
p becomes retired.

As we discussed for the HEX problem, in order to solve
the TRI problem in our Spanning Forest + Snake Formation algorithmic framework, one only needs to setup the correct rules for growing a ”triangular snake”, which will be
accomplished by Algorithm 3. The snake formation rules
for the TRI problem are complex than the ones we had
for the HEX problem, since we will need to explicitly take
into account the formation of different layers of particles as
we build the triangular structure (this was implicitly taken
care by the spiral formation in the HEX algorithm). The
TRI snake construction will start from the seed particle p,
which will occupy one of the triangle corners. The seed will
mark two of its adjacent edges as the direction along which
two of the borders of the triangle will be formed, by setting
p.border[lef t] and p.border[right] flags on the corresponding
edges (we arbitrarily pick the edges will labels 0 and 1 out of
p in our algorithm). These directions will be propagated by
the particles that end up on one of the two sides. The seed
starts the snake formation by setting the flag p.snakedir on
its 0-labeled edge. From there on, Algorithm 3 will build
the triangle snake layer by layer, alternating going ”to the
left” and ”to the right”. Every time the snake touches one
of the borders (Case 2 of Algorithm 3), it sets up the rules
for starting a new layer by setting the snake direction flags
accordingly, first on the last particle of a layer (the one that
just touched the border, in Case 2) and then on the first
particle of the newly formed layer (Case 3). If a new layer
is not needed, the snake proceeds to fill additional positions
on the current layer (Case 1). Figure 3 illustrates this ap-

proach through some snapshots of the execution of the TRI
algorithm. The proof of the following theorem appears in
the Appendix.
Algorithm 3 Snake Formation for TRI
if p is a contracted root then
if p has an adjacent edge i to p0 with a flag p0 .snakedir,
where p0 is retired then
. retired condition
bordertype = Border (p)
if bordertype = null then
. Case 1: continue on the same layer

p sets p.snakedir for edge opposite to i
(i.e., edge (i + 3) mod 6)
else
Let q be the border particle connected to p
Let j be the edge of p opposite to the edge
connecting p to q
p sets p.border[bordertype] on edge j
if p0 6= q then
. Case 2: start a new layer

p sets p.snakedir for edge j
else
. Case 3: snake direction from border

if bordertype = lef t then
p sets p.snakedir for edge (i + 5) mod 6
else
p sets p.snakedir for edge (i + 1) mod 6
p becomes retired
Border (p):
if p has an adjacent edge k to a particle q with a flag
q.border[bordertype], where bordertype ∈ {lef t, right}
then
return bordertype
else
return null
Theorem 2. Our algorithm correctly solves the TRI problem in worst-case optimal O(n2 ) work.

3.

CONCLUSION

We presented a general algorithmic framework for shape
formation problems in SOPS that combines our spanning
forest algorithmic primitive with a snake formation primitive. We have shown that by carefully determining how
to grow the appropriate snake structure, we were able to
solve the HEX and TRI problems. We can easily to extend
our snake primitive to build other shapes, such as square or
rectangular shapes, or diamonds. It would be interesting to
characterize all the general shapes that could be solved with
our approach on Geqt (and possibly also for other infinite
regular grid graphs, namely the square grid graph and the
hexagonal grid graph, if we considered those as the underlying graph G in the geometric amoebot model). Finally,
we would like to evaluate the performance of our algorithms
in terms of the worst-case number of asynchronous rounds
necessary for termination.

4.[1] L.REFERENCES
M. Adleman. Molecular computation of solutions to
combinatorial problems. Science, 266(11):1021–1024, 1994.
[2] D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall. On the
computational power of DNA. Discrete Applied
Mathematics, 71:79–94, 1996.

[3] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita. Generic
decentralized control for lattice-based self-reconfigurable
robots. International Journal of Robotics Research,
23(9):919–937, 2004.
[4] M. Chen, D. Xin, and D. Woods. Parallel computation
using active self-assembly. In DNA Computing and
Molecular Programming, pages 16–30. Springer, 2013.
[5] G. Chirikjian. Kinematics of a metamorphic robotic system.
In Proceedings of ICRA ’94, volume 1, pages 449–455, 1994.
[6] Z. Derakhshandeh, S. Dolev, R. Gmyr, A. W. Richa,
C. Scheideler, and T. Strothmann. Brief announcement:
Amoebot — a new model for programmable matter. In
SPAA, 2014.
[7] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. Bazzi,
A. W. Richa, and C. Scheideler. Leader election and shape
formation with self-organizing programmable matter.
arXiV, abs/1503.07991, 2015, Extended abstract submitted
to DNA21, 2015.
[8] D. Doty. Theory of algorithmic self-assembly.
Communications of the ACM, 55(12):78–88, 2012.
[9] P. Flocchini, G. Prencipe, N. Santoro, and P. Widmayer.
Arbitrary pattern formation by asynchronous, anonymous,
oblivious robots. Theoretical Computer Science,
407(1):412–447, 2008.
[10] T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss. Self
organizing robots based on cell structures - cebot. In
Proceedings of IROS ’88, pages 145–150, 1988.
[11] S. Kernbach, editor. Handbook of Collective Robotics –
Fundamentals and Challanges. Pan Stanford Publishing,
2012.
[12] J. McLurkin. Analysis and Implementation of Distributed
Algorithms for Multi-Robot Systems. PhD thesis,
Massachusetts Institute of Technology, 2008.
[13] M. J. Patitz. An introduction to tile-based self-assembly
and a survey of recent results. Natural Computing,
13(2):195–224, 2014.
[14] M. Rubenstein, A. Cornejo, and R. Nagpal. Programmable
self-assembly in a thousand-robot swarm. Science,
345(6198):795–799, 2014.
[15] J. E. Walter, J. L. Welch, and N. M. Amato. Distributed
reconfiguration of metamorphic robot chains. Distributed
Computing, 17(2):171–189, 2004.
[16] E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman.
Design and self-assembly of two-dimensional dna crystals.
Nature, 394(6693):539–544, 1998.
[17] D. Woods. Intrinsic universality and the computational
power of self-assembly. In T. Neary and M. Cook, editors,
Proceedings Machines, Computations and Universality
2013, MCU 2013, Zürich, Switzerland, September 9-11,
2013., volume 128 of EPTCS, pages 16–22, 2013.
[18] D. Woods, H.-L. Chen, S. Goodfriend, N. Dabby,
E. Winfree, and P. Yin. Active self-assembly of algorithmic
shapes and patterns in polylogarithmic time. In ITCS,
pages 353–354, 2013.
[19] M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll,
H. Lipson, E. Klavins, and G. S. Chirikjian. Modular
self-reconfigurable robot systems. IEEE Robotics
Automation Magazine, 14(1):43–52, 2007.

Figure 2: Snapshots of the HEX algorithm. The seed is green, retired particles are black, roots are red and followers
are blue. For a full simulation run of the algorithm see http://sops.cs.upb.de.

Figure 3: Snapshots of the TRI algorithm. The seed is green, retired particles are black, roots are red and followers
are blue. For a full simulation run of the algorithm see http://sops.cs.upb.de.

APPENDIX
A.
A.1

ANALYSIS
Analysis of the Spanning Forest Algorithm

We say followers and roots particles are active. As specified in Algorithm 1, only followers can set the flag p.parent.
The first three lemmas demonstrate some properties that
hold during the execution of the spanning forest procedure
and will be used later to analyze the proposed algorithms
for HEX and TRI problems.
Lemma 1. For a follower p, the node indicated by p.parent
is occupied by an active particle.
Proof. Consider a follower p in any configuration during
the execution of Algorithm 1. Note that p can only become
a follower from an inactive state, and once it leaves the follower state it will not switch to that state again. Consider
the first configuration c1 in which p is a follower. In the
configuration c0 immediately before c1 , p must be inactive
and it becomes a follower because of an active particle p0
occupying the position indicated by p.parent in c0 . The
particle p0 is still adjacent to the edge flagged by p.parent in
c1 . Now assume that p.parent points to an active particle
p0 in a configuration ci , and that p is still a follower in the
next configuration ci+1 that results from executing an action a. If a affects p and p0 , the action must be a handover
in which p updates its flag p.parent such that p.parent may
be moved to the edge that now connects p to p0 in ci+1 . If
a affects p but not p0 , it must be a contraction in which
p.parent does not change and still points to p0 . If a affects
p0 but not p, there are multiple possibilities. The particle
p0 might switch from follower to root state, or from root to
retired state, or it might expand, none of which violates the
lemma. Furthermore, p0 might contract. If p.parent points
to the head of p0 , p0 is still adjacent to the edge flagged by
p.parent in ci+1 . Otherwise, p is a child adjacent to the tail
of p0 in ci and therefore the contraction must be part of a
handover. As p is not involved in the action, the handover
must be between p0 and a third active particle p00 . It is easy
to see that after such a handover p.parent points to either
p0 or p00 . Finally, if a affects neither p nor p0 , p.parent will
still point to p0 in ci+1 .
Based on Lemma 1, we define a directed graph A(c) for
a configuration c as follows. A(c) contains the same nodes
as the nodes occupied in Geqt by the set of particles in c.
For every expanded particle p in c, A(c) contains a directed
edge from the tail to the head of p, and for every follower
p0 in c, A(c) contains a directed edge from the head of p0 to
p0 .parent.
Lemma 2. The graph A(c) is a forest, and if there is at
least one active particle, every connected component of inactive particles contains a particle that is connected to an
active particle.
Proof. In an initial configuration c0 , all particles are inactive and therefore the lemma holds trivially. Now assume
that the lemma holds for a configuration ci . We will show
that it also holds for the next configuration ci+1 that results
from executing an action a. If a affects an inactive particle
p, this particle either becomes a follower or a root. In the
former case p joins an existing tree, and in the latter case

p forms a new tree in A(ci+1 ). In either case, A(ci+1 ) is
a forest and the connected component of inactive particles
that p belongs to in ci is either non-existent or connected to
p in ci+1 . If a affects only a single particle p that is in state
follower, this particle can contract or become a root. In the
former case, p has no child p0 such that p0 .parent is the tail
of p and also p has no inactive neighbors. Therefore, the
contraction of p does not disconnect any follower or inactive
particle and, accordingly, does not violate the lemma. In
the latter case, p becomes a root of a tree which also does
not violates the lemma. If a involves only a single particle
p that is in state root, p can expand or contract or become
retired. An expansion and becoming retired trivially cannot
violate the lemma and the argument for the contraction is
the same as for the contraction of a follower above. Finally,
if a involves two active particles in ci , these particles perform
a handover. While such a handover can change the parent
relation among the nodes, it cannot violate the lemma.
The following lemma shows that the spanning forest always makes progress, by showing that as long as the roots
keep moving, the remaining particles will eventually follow.
Lemma 3. An expanded particle eventually contracts.
Proof. Consider an expanded particle p in a configuration c. Note that p must be active. If there is an enabled
action that includes the contraction of p, that action will
remain enabled until p eventually contracts when p is validated in the current round. So assume that there is no enabled action that includes the contraction of p. According to
Lemma 2 and the transition rule from inactive to active particles, at some point in time all particles in the system will
be active. If the contraction of p becomes part of an enabled
action before this happens, p will eventually contract. So assume that all particles are active but still p cannot contract.
If p has no children, the isolated contraction of p is an enabled action which contradicts our assumption. Therefore,
p must have children.
Furthermore, p must read at least one child p0 having its
p0 .parent flag pointing towards p over its tail and all children having their parent flags pointing towards p’s tail must
be expanded as otherwise p could again contract as part of
a handover. If p0 would contract, a handover between p0 and
p would become an enabled action. We can apply the complete argument presented in this proof so far to p0 and so on
backwards along a branch in a tree in A(c) until we reach
a particle that can contract. We will reach such a particle
by Lemma 2. Therefore, we found a sequence of expanded
particles that starts with p0 and ends with a particle that
eventually contracts. The contraction of that last particle
will allow the particle before it in the sequence to contract
and so on. Finally, the contraction of p will become part
of an enabled action and therefore p will eventually contract.

A.2

Hexagonal Shape Formation Analysis

Here, we show that the algorithmic primitives proposed
in Section 2.2 solve the HEX problem correctly.
Theorem 3. Our algorithm solves the HEX problem.
Proof. We need to show that the algorithm terminates
and that when it does, the system is in the shape of a
hexagon. According to Lemma 2, every particle p eventually activates. According to the spanning forest algorithm,

if p is adjacent to the retired structure (initially the structure only contains the seed particle), it becomes a root and
moves in a clockwise manner around the retired structure
until it eventually reaches the valid position that can extend
the hexagon and becomes retired. By contradiction, assume
p never becomes retired. Since the number of particles is
bounded (and therefore the size of the formed retired structure is bounded), there must be an infinite number of configurations ci where p had a root particle blocking its desired
clockwise movement around the hexagona retired structure.
Let p0 be the last root p sees as its clockwise neighbor over
the retired structure (since once a particle becomes a root,
it will stay connected to the hexagonal retired structure and
always attempt to move in a clockwise manner, p0 is welldefined). Applying the same argument inductively to p0 , we
will get an infinite sequence of roots on the retired structure
that never touch a valid spot pointed by q.snakedir flag of
an already retired particle q, a contradiction, since the current retired structure (and the number of retired particles)
is bounded. Therefore, every root eventually changes into a
retired state. From Algorithm 1, every follower in the neighborhood of a retired particle becomes a root. For every root
q with at least one follower child, let c be the first configuration when q becomes retired. If q still has any child in c
then all of its children p become roots. Applying this argument recursively we will reach to a configuration such that
there exists no root q having a follower child which proves
that eventually every follower becomes a root. Putting it
all together, eventually all particles become retired and the
algorithm terminates.
Note that it also follows from the argument above that
the set of retired particles at the end of the algorithm forms
a connected structure (since the particles start from a connected configuration and never get disconnected through the
process).
Now, we need to prove the correctness, i.e., that the resulting structure of retired particles is in a hexagonal form.
Initially the hexagon only contains the seed particle, therefore the claim holds trivially. By induction, let’s assume c
is the first configuration in which the current formed structure of the retired particles contains k retired particles and
by induction hypothesis, assume that those particles form a
valid hexagonal shape using k particles. According to Algorithm 2, the only way a root p can become the (k + 1)th
retired particle during or after c, is if it occupies the next
valid position pointed by the flag q.snakedir, where q was
the k-th particle to join the hexagonal shape. According
to induction hypothesis, the k first retired particles form
a hexagonal shape. By pointing to the next adjacent position in counter-clockwise direction around the outermost
retired particles in the current hexagonal structure, the flag
q.snakedir points to the next position (according to counterclockwise direction) on the last formed layer of the retired
structure, or to a starting position on the next layer once
the current layer is full, proving the correctness of the constructed shape.
We would like to measure the amount of the work of the
proposed algorithm.
Lemma 4. The worst-case work required by any algorithm
to solve the HEX problem is Ω(n2 ).
Proof. Consider a line of n particles on Geqt , where the
seed particle is located on one end of the line, as an initial

configuration of the particles. We label the particles connected to the seed starting with number 0 for the particle
adjacent to the seed. The particle labeled i > 1 requires
at least 2(i − 1 − d(i − 1)/Mi−1 e) ≥ 2(i − 1 − d(i − 1)/6e)
movements until it can lie contracted on the retired structure where Mj , Mj ≥ 6 and j ≥ 1, indicates the capacity
(i.e., the number of the retired particles) of the layer that the
retired particle with labelPj belongs to. Therefore, any algo2
rithm requires at least 2 n−1
i=2 (i − 1 − d(i − 1)/6e) = Ω(n )
work.
Theorem 4. The algorithm proposed for HEX terminates
in O(n2 ) work.
Proof. To prove the upper bound, we simply show that
every particle executes O(n) movements. The theorem then
follows. Consider a particle p. While p is in inactive or a
retired state, it does not move. Let c be the first configuration when p becomes a follower. Consider the directed path
in A(c) from the head of p to its root p0 . There always is
such a path since every follower belongs to a tree in A(c) by
Lemma 2 . Let P = (a0 , a1 , . . . , am ) be that path in A(c)
where a0 is the head of p and am is a child of p0 . According to Algorithm 1, p attempts to follow P by sequentially
expanding into the nodes a0 , a1 , . . . , am . The length of this
path is bounded by 2n and, therefore, the number of movements p executes while being a follower is O(n). Once p
becomes a root, it only performs expansions and contractions around the retired structure until it reaches one of the
valid positions on the hexagon. Since each root p and a
retired particle q never connect from the same edge more
than twice, and since the total number of retired particles is
at most n, therefore the number of movements is bound to
O(n) for p. Therefore, the number of movements a particle
p totally executes is O(n), which concludes the theorem.

A.3

Analysis for Triangular Shape Formation

Now we need to show that the algorithmic primitives presented in Section 2.3 solve the TRI problem correctly.
Theorem 5. Our algorithm solves the TRI problem.
Proof. Again, we need to show that the algorithm terminates and that when it does, the system is the shape of
a triangle. The termination part of the proof is identical
to that for the HEX problem presented in Theorem 3, and
hence it only remains to prove the correctness of the TRI algorithm. Assume we have three particles as the base case (to
build the smallest size perfect triangle on Geqt ). The seed
p∗ sets the p ∗ .snakedir flag and the p ∗ .border[lef t] flag
on its 0-labeled edge. A root particle q might have to move
around the seed p∗ until it connects to edge 0 of the seed
through an edges i. Since p sees both (border and snake)
flags coming from the same particle, p becomes retired while
it start constructing a new layer of the triangle and sets its
p.snakedir flag such that the next particle continues filling
this newly added layer (Case 3 of Algorithm 3). Particle p
also sets p.border[lef t] appropriately to propagate the inherited direction of the border from the seed to next layer.
The only position that the third particle can stop on Geqt
is the one pointed by p.snakedir and it is trivial to see that
the resulting retired structure of the three particles is in a
triangular shape. Let c be the first configuration in which
the current formed structure of the retired particles contains

k retired particles, and let q denote the (k)th particle to become retired. By induction hypothesis, assume that those
k particles form a triangle. According to Algorithm 3, the
only way a root p can become the (k + 1)th retired particle during or after c, is if it occupies the valid position
pointed by a flag q.snakedir. Depending on the location of
q in the triangle, three cases may arise. First, consider the
case when q is a left border particle (an analogous argument
works if q is a right border particle). Since q is the last particle added to the current valid triangular shape, we either
have a perfect triangle after the addition of q or we have a
perfect triangle plus particle q as the leftmost particle on
a newly created layer. In the former case, given the next
position pointed by q.snakedir, the root p follows Case 2
of algorithm, which means that p will retire on the leftmost
valid position on the next layer of the triangular structure,
pointed by q.border[lef t]. In the latter, p follows Case 3 and
will fill another position of the current layer next to q. In
both cases the resulting retired structure still forms a valid
triangular shape. Second, consider the situation where q is
not a border particle (Case 1). Therefore, q is located on
the last partially filled layer and q.snakedir is set to point to
the next unoccupied snake spot on that layer, which is then
filled by p, correctly extending the triangular structure, and
proving the claim.
Again, we would like to measure the work of the proposed
algorithm.
Lemma 5. The worst-case work required by any algorithm
to solve the TRI problem is Ω(n2 ).
Proof. With a very similar argument we had inP
Lemma 4
one can verify that it is required to have at least 2 n−1
i=1 (i −
1 − d(i − 1)/2e) = Ω(n2 ) work for the algorithm to terminate.
Theorem 6. The algorithm for TRI terminates in O(n2 )
work.
Proof. Same argument we have in Lemma 4 holds here
too. We just need to assume a triangular shape instead of a
hexagonal one.

On the Runtime of Universal Coating for
Programmable Matter
Joshua J. Daymude1? , Zahra Derakhshandeh1? , Robert Gmyr2?? , Alexandra
Porter1? , Andréa W. Richa1? , Christian Scheideler2?? , and Thim
Strothmann2??
1

arXiv:1606.03642v3 [cs.ET] 18 Jan 2017

2

Computer Science, CIDSE, Arizona State University, USA,
{jdaymude,zderakhs,amporte6,aricha}@asu.edu
Department of Computer Science, Paderborn University, Germany,
{gmyr,scheidel,thim}@mail.upb.de

Abstract. Imagine coating buildings and bridges with smart particles
(also coined smart paint) that monitor structural integrity and sense
and report on traffic and wind loads, leading to technology that could
do such inspection jobs faster and cheaper and increase safety at the
same time. In this paper, we study the problem of uniformly coating
objects of arbitrary shape in the context of self-organizing programmable
matter, i.e., the programmable matter consists of simple computational
elements called particles that can establish and release bonds and can
actively move in a self-organized way. Particles are anonymous, have
constant-size memory and utilize only local interactions in order to coat
an object. We continue the study of our Universal Coating algorithm by
focusing on its runtime analysis, showing that our algorithm terminates
within a linear number of rounds with high probability. We also present
a matching linear lower bound that holds with high probability. We use
this lower bound to show a linear lower bound on the competitive gap
between fully local coating algorithms and coating algorithms that rely
on global information, which implies that our algorithm is also optimal in
a competitive sense. Simulation results show that the competitive ratio
of our algorithm may be better than linear in practice.

1

Introduction

Inspection of bridges, tunnels, wind turbines, and other large civil engineering
structures for defects is a time-consuming, costly, and potentially dangerous task.
In the future, smart coating technology, or smart paint, could do the job more
efficiently and without putting people in danger. The idea behind smart coating
is to form a thin layer of a specific substance on an object which then makes it
possible to measure a condition of the surface (such as temperature or cracks) at
any location, without direct access to the location. The concept of smart coating
already occurs in nature, such as proteins closing wounds, antibodies surrounding bacteria, or ants surrounding food to transport it to their nest. These diverse
?
??

Supported in part by NSF grants CCF-1353089, CCF-1422603, and REU–026935.
Supported in part by DFG grant SCHE 1592/3-1.

2

Daymude, et al.

examples suggest a broad range of applications of smart coating technology in
the future, including repairing cracks or monitoring tension on bridges, repairing space craft, fixing leaks in a nuclear reactor, or stopping internal bleeding.
We continue the study of coating problems in the context of self-organizing programmable matter consisting of simple computational elements, called particles,
that can establish and release bonds and can actively move in a self-organized
way using the geometric version of the amoebot model presented in [1,2]. In doing so, we proceed to investigate the runtime analysis of our Universal Coating
algorithm, introduced in [3]. We first show that coating problems do not only
have a (trivial) linear lower bound on the runtime, but that there is also a linear
lower bound on the competitive gap between the runtime of fully local coating
algorithms and coating algorithms that rely on global information. We then investigate the worst-case time complexity of our Universal Coating algorithm and
show that it terminates within a linear number of rounds with high probability
(w.h.p.)3 , which implies that our algorithm is optimal in terms of worst-case
runtime and also in a competitive sense. Moreover, our simulation results show
that in practice the competitive ratio of our algorithm is often better than linear.
1.1

Amoebot model

In the geometric amoebot model, we consider the graph Geqt , where Geqt =
(Veqt , Eeqt ) is the infinite regular triangular grid graph. Each vertex in Veqt is a
position that can be occupied by at most one particle (see Figure 1(a)). Each
particle occupies either a single node or a pair of adjacent nodes in Geqt . Any
structure a particle system can form can be represented as a subgraph of Geqt .
Two particles occupying adjacent nodes are connected by a bond, and we refer
to such particles as neighbors. The bonds do not only ensure that the particles
form a connected structure but they are also used for exchanging information as
explained below.
Particles move by executing a series of expansions and contractions. A particle which occupies one node is contracted and can expand to an unoccupied
adjacent node to occupy two nodes. If it occupies two nodes it is expanded and
can contract to occupy a single node. In Figure 1(b), we illustrate a set of expanded and contracted particles on the underlying graph Geqt . For an expanded
particle, we denote the node the particle last expanded into as the head of the
particle and the other occupied node as its tail. For a contracted particle, the
single node occupied by the particle is both its head and its tail.
To stay connected as they move, neighboring particles coordinate their motion in a handover, which can occur in two ways. A contracted particle p can
initiate a handover by expanding into a node occupied by an expanded neighbor
q, “pushing” q and forcing it to contract. Alternatively, an expanded particle q
can initiate a handover by contracting “pulling” a contracted neighbor p to the
node it is vacating, thereby forcing p to expand. Figures 1(b) and 1(c) illustrate
two particles labeled p and q performing a handover. Particles are anonymous
3

By with high probability, we mean with probability at least 1 − 1/nc , where n is the
number of particles in the system and c > 0 is a constant.

On the Runtime of Universal Coating for Programmable Matter

3

Fig. 1. (a) shows a section of Geqt , where nodes of Geqt are shown as black circles. (b)
shows five particles on Geqt ; the underlying graph Geqt is depicted as a gray mesh; a
contracted particle is depicted as a single black circle and an expanded particle is depicted as two black circles connected by an edge. (c) depicts the resulting configuration
after a handover was performed by particles p and q in (b).
but each has a collection of uniquely labeled ports corresponding to the edges
incident to the nodes the particle occupies. Bonds between adjacent particles are
formed through ports that face each other. The particles are assumed to have
a common chirality, meaning they share the same notion of clockwise (CW) direction. This allows each particle p to label its ports counting in the clockwise
direction; without loss of generality, we assume each particle labels its head and
tail ports from 0 to 5. However, particles may have different offsets of the labelings, so they do not share a common sense of orientation. Each particle hosts a
local memory of constant size for which any neighboring particle has read and
write access. With this mechanism particles can communicate by writing into
each other’s memory. The configuration C of the system at the beginning of time
t consists of the nodes in Geqt occupied by the object and the set of particles,
and for each particle p, C contains the current state of p, including whether it is
expanded or contracted, its port labeling, and the contents of its local memory.
Following the standard asynchronous model of computation [4], we assume
that the system progresses through atomic activations of individual particles. At
each (atomic) activation a particle can perform at most one movement and an
arbitrary bounded amount of computation involving its local memory and the
shared memories of its neighbors. A classical result under this model is that for
any asynchronous execution of atomic particle activations, we can organize these
activations sequentially still producing the same end configuration [4]. We count
(asynchronous) time in terms of the number of activations. A round is over once
each particle has been activated at least once. We assume the activation sequence
to be fair, i.e., for each particle p and any point in time t, p will eventually be
activated at some time t0 ≥ t.
1.2

Universal Coating Problem

In the universal coating problem we consider an instance (P, O) where P represents the particle system and O represents the fixed object to be coated. Let
n = |P | be the number of particles in the system, V (P ) be the set of nodes
occupied by P , and V (O) be the set of nodes occupied by O (when clear from
the context, we may omit the V (·) notation). For any two nodes v, w ∈ Veqt ,
the distance d(v, w) between v and w is the length of the shortest path in Geqt

4

Daymude, et al.

from v to w. The distance d(v, U ) between a v ∈ Veqt and U ⊆ Veqt is defined as
minw∈U d(v, w). Define layer i to be the set of nodes that have a distance i to
the object, and let Bi be the number of nodes in layer i. An instance is valid if
the following properties hold:
1. The particles are all contracted and are initially in the idle state.
2. The subgraphs of Geqt induced by V (O) and V (P ) ∪ V (O), respectively, are
connected, i.e., there is a single object and the particle system is connected
to the object.
3. The subgraph of Geqt induced by Veqt \ V (O) is connected, i.e., the object
O has no holes.4
4. Veqt \ V (O) is 2(d Bn1 e + 1)-connected, i.e., O cannot form tunnels of width
less than 2(d Bn1 e + 1).
Note that a width of at least 2d Bn1 e is needed to guarantee that the object
can be evenly coated. The coating of narrow tunnels requires specific technical
mechanisms that complicate the protocol without contributing to the basic idea
of coating, so we ignore such cases in favor of simplicity.
A configuration C is legal if and only if all particles are contracted and
min

v∈Veqt \(V (P )∪V (O))

d(v, V (O)) ≥ max d(v, V (O)),
v∈V (P )

meaning that all particles are as close to the object as possible or coat O as
evenly as possible. A configuration C is said to be stable if no particle in C ever
performs a state change or movement. An algorithm solves the universal coating
problem if, starting from any valid instance, it reaches a stable legal configuration
in a finite number of rounds.
1.3

Related work

Many approaches have been proposed with potential applications in smart coating; these can be categorized as active and passive systems. In passive systems,
particles move based only on their structural properties and interactions with
their environment, or have only limited computational ability and lack control of
their motion. Examples include DNA self-assembly systems (see, e.g., the surveys
in [5,6,7]), population protocols [8], and slime molds [9,10]. Our focus is on active systems, in which computational particles control their actions and motions
to complete specific tasks. Coating has been extensively studied in the area of
swarm robotics, but not commonly treated as a stand-alone problem; it is instead
examined as part of collective transport (e.g., [11]) or collective perception (e.g.,
see respective section of [12]). Some research focuses on coating objects as an independent task under the name of target surrounding or boundary coverage. The
techniques used in this context include stochastic robot behaviors [13,14], rulebased control mechanisms [15] and potential field-based approaches [16]. While
the analytic techniques developed in swarm robotics are somewhat relevant to
4

If O does contain holes, we consider the subset of particles in each connected region
of Veqt \ V (O) separately.

On the Runtime of Universal Coating for Programmable Matter

5

this work, those systems have more computational power and movement capabilities than those studied in this work. Michail and Spirakis recently proposed
a model [17] for network construction inspired by population protocols [8]. The
population protocol model is related to self-organizing particle systems but is
different in that agents (corresponding to our particles) can move freely in space
and establish connections at any time. It would, however, be possible to adapt
their approach to study coating problems under the population protocol model.
In [3] we presented our Universal Coating algorithm and proved its correctness.
We also showed it to be worst-case work-optimal, where work is measured in
terms of number of particle movements.
1.4 Our Contributions
In this paper we continue the analysis of the Universal Coating algorithm introduced in [3]. As our main contribution in this paper, we investigate the runtime
of our algorithm and prove that our algorithm terminates within a linear number
of rounds with high probability. We also present a matching linear lower bound
for local-control coating algorithms that holds with high probability. We use this
lower bound to show a linear lower bound on the competitive gap between fully
local coating algorithms and coating algorithms that rely on global information,
which implies that our algorithm is also optimal in a competitive sense. We then
present some simulation results demonstrating that in practice the competitive
ratio of our algorithm is often much better than linear.
Overview In Section 2, we again present the algorithm introduced in [3]. We
present a comprehensive formal runtime analysis of our algorithm, by first presenting some lower bounds on the competitive ratio of any local-control algorithm
in Section 3, and then proving that our algorithm has a runtime of O(n) rounds
w.h.p. in Section 4, which matches our lower bounds.

2

Universal Coating Algorithm

In this section, we summarize the Universal Coating algorithm introduced in [3]
(see [3] for a detailed description). This algorithm is constructed by combining a
number of asynchronous primitives, which are integrated seamlessly without any
underlying synchronization. The spanning forest primitive organizes the particles into a spanning forest, which determines the movement of particles while
preserving system connectivity; the complaint-based coating primitive coats the
first layer by bringing any particles not yet touching the object into the first
layer while there is still room; the general layering primitive allows each layer i
to form only after layer i − 1 has been completed, for i ≥ 2; and the node-based
leader election primitive elects a node in layer 1 whose occupant becomes the
leader particle, which is used to trigger the general layering process for higher
layers.
2.1 Preliminaries
We define the set of states that a particle can be in as idle, follower, root,
and retired. In addition to its state, a particle maintains a constant number of

6

Daymude, et al.

other flags, which in our context are constant size pieces of information visible to
neighboring particles. A flag x owned by some particle p is denoted by p.x. Recall
that a layer is the set of nodes V ⊆ Veqt that are equidistant to the object O. A
particle keeps track of its current layer number in p.layer. In order to respect the
constant-size memory constraint of particles, we take all layer numbers modulo
4. Each root particle p has a flag p.down which stores a port label pointing to a
node of the object if p.layer = 1, and to an occupied node adjacent to its head
in layer p.layer − 1 if p.layer > 1. We now describe the coating primitives in
more detail.
2.2 Coating Primitives
The spanning forest primitive (Algorithm 1) organizes the particles into a
spanning forest F, which yields a straightforward mechanism for particles to
move while preserving connectivity (see [1,18] for details). Initially, all particles
are idle. A particle p touching the object changes its state to root. For any other
idle particle p, if p has a root or a follower in its neighborhood, it stores the
direction to one of them in p.parent, changes its state to follower, and generates
a complaint flag; otherwise, it remains idle. A follower particle p uses handovers
to follow its parent and updates the direction p.parent as it moves in order
to maintain the same parent in the tree (note that the particular particle at
p.parent may change due to p’s parent performing a handover with another of its
children). In this way, the trees formed by the parent relations stay connected,
occupy only the nodes they covered before, and do not mix with other trees.
A root particle p uses the flag p.dir to determine its movement direction. As p
moves, it updates p.dir so that it always points to the next position of a clockwise
movement around the object. For any particle p, we call the particle occupying
the position that p.parent resp. p.dir points to the predecessor of p. If a root
particle does not have a predecessor, we call it a super-root.
The complaint-based coating primitive is used for the coating of the first
layer. Each time a particle p holding at least one complaint flag is activated, it
forwards one to its predecessor as long as that predecessor holds less than two
complaint flags. We allow each particle to hold up to two complaint flags to
ensure that a constant size memory is sufficient for storing the complaint flags
and so the flags quickly move forward to the super-roots. A contracted superroot p only expands to p.dir if it holds at least one complaint flag, and when it
expands, it consumes one of these complaint flags. All other roots p move towards
p.dir whenever possible (i.e., no complaint flags are required) by performing a
handover with their predecessor (which must be another root) or a successor
(which is a root or follower of its tree), with preference given to a follower so
that additional particles enter layer 1. As we will see, these rules ensure that
whenever there are particles in the system that are not yet at layer 1, eventually
one of these particles will move to layer 1, unless layer 1 is already completely
filled with contracted particles.
The leader election primitive runs during the complaint-based coating
primitive to elect a node in layer 1 as the leader position. This primitive is
similar to the algorithm presented in [1] with the difference that leader candidates

On the Runtime of Universal Coating for Programmable Matter

7

Algorithm 1 Spanning Forest Primitive
A particle p acts depending on its state as described below:
idle:
If p is adjacent to the object O, it becomes a root particle, makes the current
node it occupies a leader candidate position, and starts running the leader
election algorithm. If p is adjacent to a retired particle, p also becomes a
root particle. If a neighbor p0 is a root or a follower, p sets the flag p.parent
to the label of the port to p0 , puts a complaint flag in its local memory, and
becomes a follower. If none of the above applies, p remains idle.
follower: If p is contracted and adjacent to a retired particle or to O, then p becomes a root particle. If p is contracted and has an expanded parent,
then p initiates Handover(p) (Algorithm 2); otherwise, if p is expanded,
it considers the following two cases: (i) if p has a contracted child particle q, then p initiates Handover(p); (ii) if p has no children and no idle
neighbor, then p contracts. Finally, if p is contracted, it runs the function
ForwardComplaint(p, p.parent) (Algorithm 3).
root:
If particle p is in layer 1, p participates in the leader election process.
If p is contracted, it first executes MarkerRetiredConditions(p) (Algorithm 6) and becomes retired, and possibly also a marker, accordingly.
If p does not become retired, then if it has an expanded root in p.dir,
it initiates Handover(p); otherwise, p calls LayerExtension(p) (Algorithm 4). If p is expanded, it considers the following two cases: (i) if p has
a contracted child, then p initiates Handover(p); (ii) if p has no children
and no idle neighbor, then p contracts. Finally, if p is contracted, it runs
ForwardComplaint(p, p.dir).
retired: p clears a potential complaint flag from its memory and performs no further
action.

Algorithm 2 Handover(p)
1: if p is expanded then
2:
if p.layer = 1 and p has a follower child q such that q.parent points to the tail
of p then
3:
if q is contracted then
4:
p initiates a handover with particle q
5:
else if p has a contracted (follower or root) child q such that q.parent points
to the tail of p then
6:
p initiates a handover with particle q
7: else if p has an expanded parent q or the position in p.dir is occupied by an
expanded root q then
8:
p initiates a handover with particle q

Algorithm 3 ForwardComplaint(p, i)
1: if p holds at least one complaint flag and the particle q adjacent to p in direction
i holds less than two complaint flags then
2:
p forwards one complaint flag to q

8

Daymude, et al.

are nodes instead of static particles (which is important because in our case
particles may still move during the leader election primitive). The primitive only
terminates once all positions in layer 1 are occupied. Once the leader position is
determined, all positions in layer 1 are filled by contracted particles and whatever
particle currently occupies that position becomes the leader. This leader becomes
a marker particle, marking a neighboring position in the next layer as a marked
position which determines a starting point for layer 2, and becomes retired. Once
a contracted root p has a retired particle in the direction p.dir, it retires as well,
which causes the particles in layer 1 to become retired in counter-clockwise
order. At this point, the general layering primitive becomes active, which builds
subsequent layers until there are no longer followers in the system. If the leader
election primitive does not terminate (which only happens if n < B1 and layer 1
is never completely filled), then the complaint flags ensure that the super-roots
eventually stop, which eventually results in a stable legal coating.

Algorithm 4 LayerExtension(p)
1:
2:
3:
4:
5:
6:
7:
8:
9:

Calculating p.layer, p.down and p.dir
The layer number of any node occupied by the object is equal to 0.
Let q be any neighbor of p with smallest layer number (modulo 4).
p.down ← p’s label for port leading to q
p.layer = (q.layer + 1) mod 4
Clockwise(p, p.down)
. Computes CW & CCW directions
if p.layer is odd then
p.dir ← p.CW
else
p.dir ← p.CCW

Extending layer p.layer
10: if the position at p.dir is unoccupied, and either p is not on the first layer or p
holds a complaint flag then
11:
p expands in direction p.dir
12:
p consumes a complaint flag, if it holds one

In the general layering primitive, whenever a follower is adjacent to a retired particle, it becomes a root. Root particles continue to move along positions
of their layer in a clockwise (if the layer number is odd) or counter-clockwise (if
the layer number is even) direction until they reach either the marked position of
that layer, a retired particle in that layer, or an empty position of the previous
layer (which causes them to change direction). Complaint flags are no longer
needed to expand into empty positions. Followers follow their parents as before.
A contracted root particle p may retire if: (i) it occupies the marked position
and the marker particle in the lower layer tells it that all particles in that layer
are retired (which it can determine locally), or (ii) it has a retired particle in
the direction p.dir. Once a particle at a marked position retires, it becomes a

On the Runtime of Universal Coating for Programmable Matter

9

marker particle and marks a neighboring position in the next layer as a marked
position.

Algorithm 5 Clockwise(p, i)
1: j ← i, k ← i
2: while edge j is incident to the object or to a retired particle with layer number
p.layer − 1 do
3:
j ← (j − 1) mod 6
4: p.CW ← j
5: while edge k is incident to the object or to a retired particle with layer number
p.layer − 1 do
6:
k ← (k + 1) mod 6
7: p.CCW ← k

Algorithm 6 MarkerRetiredConditions(p)
First Marker Condition:
1: if p is the leader then
2:
p becomes a retired particle
3:
p sets the flag p.marker to be the label of a port leading to a node guaranteed
not to be in layer 1 — e.g., by taking the average direction of p’s two neighbors
in layer 1 (by now complete)
Extending Layer Markers:
4: if p is connected to a marker q and the port q.marker points towards p then
5:
if both q.CW and q.CCW are retired then
6:
p becomes a retired particle
7:
p sets the flag p.marker to the label of the port opposite the port connecting
p to q
Retired Condition:
8: if the node in direction p.dir is occupied by a retired particle then
9:
p becomes a retired particle

3

Lower Bounds

Recall that a round is over once every particle in P has been activated at least
once. The runtime TA (P, O) of a coating algorithm A is defined as the worstcase number of rounds (over all sequences of particle activations) required for A
to solve the coating problem (P, O). Certainly, there are instances (P, O) where
every coating algorithm has a runtime of Ω(n) (see Lemma 1), though there
are also many other instances where the coating problem can be solved much

10

Daymude, et al.

faster. Since a worst-case runtime of Ω(n) is fairly large and therefore not very
helpful to distinguish between different coating algorithms, we intend to study
the runtime of coating algorithms relative to the best possible runtime.
Lemma 1. The worst-case runtime required by any local-control algorithm to
solve the universal coating problem is Ω(n).

Fig. 2. Worst-case configuration concerning number of rounds. There are n particles
(black dots) in a line connected to the surface via a single particle p1 .

Proof. Assume the particles p1 , . . . , pn form a single line of n particles connected
to the surface via p1 (Figure 2). Suppose B1 > n. Since d(pn , O) = n, it will
take Ω(n) rounds in the worst-case (requiring Θ(n) movements) until pn touches
the object’s surface. This worst-case can happen, for example, if pn performs no
more than one movement (either an expansion or a contraction) per round. t
u
Unfortunately, a large lower bound also holds for the competitiveness of any
local-control algorithm. A coating algorithm A is called c-competitive if for any
valid instance (P, O),
E[TA (P, O)] ≤ c · OPT(P, O) + K
where OPT(P, O) is the minimum runtime needed to solve the coating problem
(P, O) and K is a value independent of (P, O).
Theorem 1. Any local-control algorithm that solves the universal coating problem has a competitive ratio of Ω(n).
Proof. We construct an instance of the coating problem (P, O) which can be
solved by an optimal algorithm in O(1) rounds, but requires any local-control
algorithm Ω(n) times longer. Let O be a straight line of arbitrary (finite) length,
and let P be a set of particles which entirely occupy layer 1, with the exception
of one missing particle below O equidistant from its sides and one additional
particle above O in layer 2 equidistant from its sides (see Figure 3).
An optimal algorithm could move the particles to solve the coating problem
for the given example in O(1) rounds, as in Figure 4. Note that the optimal
algorithm always maintains the connectivity of the particle system, so its runtime
is valid even under the constraint that any connected component of particles
must stay connected. However, for our local-control algorithms we allow particles
to disconnect from the rest of the system.

On the Runtime of Universal Coating for Programmable Matter

11

	

Fig. 3. The object occupies a straight line in Geqt . The particles are all contracted and
occupy the positions around the object, with the exception that there is one unoccupied
node below the object and one extra particle above the object. Borders L and R are
shown as red lines.

Now consider an arbitrary local-control algorithm A for the coating problem.
Given a round r, we define the imbalance φL (r) at border L as the net number
of particles that have crossed L from the top of O to the bottom until round r;
similarly, the imbalance φR (r) at border R is defined to be the net number of
particles that have crossed R from the bottom of O to the top until round r.

Fig. 4. Each subfigure represents the configuration of the system at the beginning of
a round, and are ordered from left to right, top to bottom. After 5 rounds (i.e., at the
beginning of the sixth round) the object is coated. Note that the implied algorithm
can be adapted to any length of the object and always requires only 5 rounds to solve
the coating problem.

Certainly, there is an activation sequence in which information and particles
can only travel a distance of up to n/4 nodes towards L or R within the first
n/4 rounds. Hence, for any r ≤ n/4, the probability distributions of φL (r) and
φR (r) are independent of each other. Additionally, particles up to a distance of
n/4 from L and R cannot distinguish between which border they are closer to,
since the position of the gap is equidistant from the borders. This symmetry also
implies that Pr[φL (r) = k] = Pr[φR (r) = k] for any integer k. Let us focus on
round r = n/4. We distinguish between the following cases.
Case 1. φL (n/4) = φR (n/4). Then there are more particles than positions in
layer 1 above O, so the coating problem cannot be solved yet.
Case 2. φL (n/4) 6= φR (n/4). From our insights above we know that for any
two values k1 and k2 , Pr[φL (n/4) = k1 and φR (n/4) = k2 ] = Pr[φL (n/4) = k2

12

Daymude, et al.

and φR (n/4) = k1 ]. Hence, the cumulative probability of all outcomes where
φL (n/4) < φR (n/4) is equal to the cumulative probability of all outcomes where
φL (n/4) > φR (n/4). If φL (n/4) < φR (n/4), then there are again more particles
than positions in layer 1 above O, so the coating problem cannot be solved yet.
Thus, the probability that A has not solved the coating problem after n/4
rounds is at least 1/2, and therefore E[TA (P, O)] ≥ 1/2 · n/4 = n/8. Since, on
the other hand, OPT = O(1), we have established a linear competitive ratio. t
u
Therefore, even the competitive ratio can be very high in the worst case. We
will revisit the notion of competitiveness in Section 5.

4

Worst-Case Number of Rounds

In this section, we show that our algorithm solves the coating problem within
a linear number of rounds w.h.p. 5 . We start with some basic notation in Section 4.1. Section 4.2 presents a simpler synchronous parallel model for particle
activations that we can use to analyze the worst-case number of rounds. Section 4.3 presents the analysis of the number of rounds required to coat the first
layer. Finally, in Section 4.4, we analyze the number of rounds required to complete all other coating layers, once layer 1 has been completed.
4.1

Preliminaries

We start with some notation. Recall that Bi denotes the number of nodes of
Geqt at distance i from object O (i.e., the number of nodes in layer i). Let
N be the the layer number of the final layer for n particles (i.e., N satisfies
PN −1
PN
j=1 Bj < n ≤
j=1 Bj ). Layer i is said to be complete if every node in layer
i is occupied by a contracted retired particle (for i < N ), or if all particles have
reached their final position, are contracted, and never move again (for i = N ).
Given a configuration C, we define a directed graph A(C) over all nodes in
Geqt occupied by active (follower or root) particles in C. For every expanded
active particle p in C, A(C) contains a directed edge from the tail to the head of
p. For every follower p, A(C) has a directed edge from the head of p to p.parent.
For the purposes of constructing A(C), we also define parents for root particles:
a root particle p sets p.parent to be the active particle q occupying the node
in direction p.dir once p has performed its first handover expansion with q. For
every root particle p, A(C) has a directed edge from the head of p to p.parent, if
it exists. Certainly, since every node has at most one outgoing edge in A(C), the
nodes of A(C) form either a collection of disjoint trees or a ring of trees. A ring of
trees may occur in any layer, but only temporarily; the leader election primitive
ensures that a leader emerges and retires in layer 1 and marker particles emerge
and retire in higher layers, causing the ring in A(C) to break. The super-roots
defined in Section 2.2 correspond to the roots of the trees in A(C).
5

This version of the paper reflects what was submitted to the DNA22 Special Issue
of the journal Natural Computing, and updates the logical structure of this section
from its original publication in DNA22.

On the Runtime of Universal Coating for Programmable Matter

13

A movement executed by a particle p can be either a sole contraction in
which p contracts and leaves a node unoccupied, a sole expansion in which p
expands into an adjacent unoccupied node, a handover contraction with p0 in
which p contracts and forces its contracted neighbor p0 to expand into the node
it vacates, or a handover expansion with p0 in which p expands into a node
currently occupied by its expanded neighbor p0 , forcing p0 to contract.
4.2

From asynchronous to parallel schedules

In this section, we show that instead of analyzing our algorithm for asynchronous
activations of particles, it suffices to consider a much simpler model of parallel
activations of particles. Define a movement schedule to be a sequence of particle
system configurations (C0 , . . . , Ct ).
Definition 1. A movement schedule (C0 , . . . , Ct ) is called a parallel schedule if
each Ci is a valid configuration of a connected particle system (i.e., each particle
is either expanded or contracted, and every node of Geqt is occupied by at most
one particle) and for every i ≥ 0, Ci+1 is reached from Ci such that for every
particle p one of the following properties holds:
1.
2.
3.
4.

p
p
p
p

occupies the same node(s) in Ci and Ci+1 ,
expands into an adjacent node that was empty in Ci ,
contracts, leaving the node occupied by its tail empty in Ci+1 , or
is part of a handover with a neighboring particle p0 .

While these properties allow at most one contraction or expansion per particle
in moving from Ci to Ci+1 , multiple particles may move in this time.
Consider an arbitrary fair asynchronous activation sequence A for a particle
(A)
system, and let Ci , for 0 ≤ i ≤ t, be the particle system configuration at the
end of asynchronous round i in A if each particle moves according to Algorithm 1.
A forest schedule S = (A, (C0 , . . . , Ct )) is a parallel schedule (C0 , . . . , Ct ) with
the property that A(C0 ) is a forest of one or more trees, and each particle p
follows the unique path Pp which it would have followed according to A, starting
from its position in C0 . This implies that A(Ci ) remains a forest of trees for every
1 ≤ i ≤ t. A forest schedule is said to be greedy if all particles perform movements
according to Definition 1 in the direction of their unique paths whenever possible.
We begin our analysis with a result that is critical to both describing configurations of particles in greedy forest schedules and quantifying the amount of
progress greedy forest schedules make over time. Specifically, we show that if a
forest’s configuration is “well-behaved” at the start, then it remains so throughout its greedy forest schedule, guaranteeing that progress is made once every
two configurations.
Lemma 2. Given any fair asynchronous activation sequence A, consider any
greedy forest schedule (A, (C0 , . . . , Ct )). If every expanded parent in C0 has at
least one contracted child, then every expanded parent in Ci also has at least one
contracted child, for 1 ≤ i ≤ t.

14

Daymude, et al.

Proof. Suppose to the contrary that Ci is the first configuration that contains
an expanded parent p which has all expanded children. We consider all possible
expanded and contracted states of p and its children in Ci−1 and show that
none of them can result in p and its children all being expanded in Ci . First
suppose p is expanded in Ci−1 ; then by supposition, p has a contracted child
q. By Definition 1, q cannot perform any movements with its children (if they
exist), so p performs a handover contraction with q, yielding p contracted in Ci ,
a contradiction. So suppose p is contracted in Ci−1 . We know p will perform
either a handover with its parent or a sole expansion in direction p.dir since
it is expanded in Ci by supposition. Thus, any child of p in Ci−1 — say q —
does not execute a movement with p in moving from Ci−1 to Ci . Instead, if q
is contracted in Ci−1 then it remains contracted in Ci since it is only permitted
to perform a handover with its unique parent p; otherwise, if q is expanded, it
performs either a sole contraction if it has no children or a handover with one of
its contracted children, which it must have by supposition. In either case, p has
a contracted child in Ci , a contradiction.
As a final observation, two trees of the forest may “merge” when the superroot s of one tree performs a sole expansion into an unoccupied node adjacent
to a particle q of another tree. However, since s is a root and thus only defines
q as its parent after performing a handover expansion with it, the lemma holds
in this case as well.
t
u
For any particle p in a configuration C of a forest schedule, we define its
head distance dh (p, C) (resp., tail distance dt (p, C)) to be the number of edges
along Pp from the head (resp., tail) of p to the end of Pp . Depending on whether
p is contracted or expanded, we have dh (p, C) ∈ {dt (p, C), dt (p, C) − 1}. For
any two configurations C and C 0 and any particle p, we say that C dominates
C 0 w.r.t. p, denoted C(p)  C 0 (p), if and only if dh (p, C) ≤ dh (p, C 0 ) and
dt (p, C) ≤ dt (p, C 0 ). We say that C dominates C 0 , denoted C  C 0 , if and only
if C dominates C 0 with respect to every particle. Then it holds:
Lemma 3. Given any fair asynchronous activation sequence A which begins at
(A)
an initial configuration C0 in which every expanded parent has at least one
contracted child, there is a greedy forest schedule S = (A, (C0 , . . . , Ct )) with
(A)
(A)
C0 = C0 such that Ci  Ci for all 0 ≤ i ≤ t.
Proof. We first introduce some supporting notation. Let M (p) = p(1) , p(2) , . . .
be the sequence of movements p executes according to A. Let Mi (p) denote the
remaining sequence of movements in M (p) after the forest schedule reaches Ci ,
and let mi (p) denote the first movement in Mi (p).
Claim. A greedy forest schedule S = (A, (C0 , . . . , Ct )) can be constructed from
(A)
configuration C0 = C0 such that, for every 0 ≤ i ≤ t, configuration Ci is
obtained from Ci−1 by executing only the movements of a greedily selected,
mutually compatible subset of {mi−1 (p) : p ∈ P }.

On the Runtime of Universal Coating for Programmable Matter

15

Proof. Argue by induction on i, the current configuration number. C0 is trivially
obtained, as it is the initial configuration. Assume by induction that the claim
holds up to Ci−1 . W.l.o.g. let Mi−1 = {mi−1 (p1 ), . . . , mi−1 (pk )}, for k ≤ n, be
the greedily selected, mutually compatible subset of movements that S performs
in moving from Ci−1 to Ci . Suppose to the contrary that a movement m0 (p) 6∈
Mi−1 is executed by a particle p ∈ P . It is easily seen that m0 (p) cannot be
mi−1 (p); since mi−1 (p) was excluded when Mi−1 was greedily selected, it must
be incompatible with one or more of the selected movements and thus cannot
also be executed at this time. So m0 (p) 6= mi−1 (p), and we consider the following
cases:
Case 1. mi−1 (p) is a sole contraction. Then p is expanded and has no children
in Ci−1 , so we must have m0 (p) = mi−1 (p), since there are no other movements
p could execute, a contradiction.
Case 2. mi−1 (p) is a sole expansion. Then p is contracted and has no parent in
Ci−1 , so we must have m0 (p) = mi−1 (p), since there are no other movements p
could execute, a contradiction.
Case 3. mi−1 (p) is a handover contraction with q, one of its children. Then at
some time in S before reaching Ci−1 , q became a descendant of p; thus, q must
also be a descendant of p in Ci−1 . If q is not a child of p in Ci−1 , there exists a
particle z 6∈ {p, q} such that q is a descendant of z, which is in turn a descendant
of p. So in order for mi−1 (p) to be a handover contraction with q, M (z) must
include actions which allow z to “bypass” its ancestor p, which is impossible.
So q must be a child of p in Ci−1 , and must be contracted at the time mi−1 (p)
is performed. If q is also contracted in Ci−1 , then once again we must have
m0 (p) = mi−1 (p). Otherwise, q is expanded in Ci−1 , and must have become so
before Ci−1 was reached. But this yields a contradiction: since S is greedy, q
would have contracted prior to this point by executing either a sole contraction
if it has no children, or a handover contraction with a contracted child whose
existence is guaranteed by Lemma 2, since every expanded parent in C0 has a
contracted child.
Case 4. mi−1 (p) is a handover expansion with q, its unique parent. Then we
must have that mi−1 (q) is a handover contraction with p, and an argument
analogous to that of Case 3 follows.
t
u
We conclude by showing that each configuration of the greedy forest schedule
S constructed according to the claim is dominated by its asynchronous counter(A)
part. Argue by induction on i, the configuration number. Since C0 = C0 , we
(A)
have that C0  C0 . Assume by induction that for all rounds 0 ≤ r ≤ i − 1,
(A)
we have Cr  Cr . Consider any particle p. Since S is constructed using the
exact set of movements p executes according to A and each time p moves it
decreases either its head distance or tail distance by 1, it suffices to show that p
has performed at most as many movements in S up to Ci as it has according to
(A)
A up to Ci .

16

Daymude, et al.

If p does not perform a movement between Ci−1 and Ci , we trivially have
 Ci (p). Otherwise, p performs movement mi−1 (p) to obtain Ci from
(A)
Ci−1 . If p has already performed mi−1 (p) according to A before reaching Ci−1 ,
(A)
Ci (p)

(A)

then clearly Ci (p)  Ci (p). Otherwise, mi−1 (p) must be the next movement
p is to perform according to A, since p has performed the same sequence of
movements in the asynchronous execution as it has in S up to the respective
(A)
rounds i − 1, and thus has equal head and tail distances in Ci−1 and Ci−1 . It
(A)

(A)

remains to show that p can indeed perform mi−1 (p) between Ci−1 and Ci . If
mi−1 (p) is a sole expansion, then p is the super-root of its tree (in both Ci−1 and
(A)
(A)
Ci−1 ) and must also be able to expand in Ci−1 . Similarly, if mi−1 (p) is a sole
(A)

contraction, then p has no children (in both Ci−1 and Ci−1 ) and must be able
(A)

to contract in Ci−1 . If mi−1 (p) is a handover expansion with its parent q, then q
(A)

must be expanded in Ci−1 . Parent q must also be expanded in Ci−1 ; otherwise
(A)

dh (q, Ci−1 ) > dh (q, Ci−1 ), contradicting the induction hypothesis. An analogous
argument holds if mi−1 (p) is a handover contraction with one of its contracted
(A)
children. Therefore, in any case we have Ci (p)  Ci (p), and since the choice
(A)
of p was arbitrary, Ci  Ci .
t
u
We can show a similar dominance result when considering complaint flags.
Definition 2. A movement schedule (C0 , . . . , Ct ) is called a complaint-based
parallel schedule if each Ci is a valid configuration of a particle system in which
every particle holds at most one complaint flag (rather than two, as described
in Algorithm 3) and for every i ≥ 0, Ci+1 is reached from Ci such that for every
particle p one of the following properties holds:
1. p does not hold a complaint flag and property 1, 3, or 4 of Definition 1 holds,
2. p holds a complaint flag f and expands into an adjacent node that was empty
in Ci , consuming f ,
3. p forwards a complaint flag f to a neighboring particle p0 which either does
not hold a complaint flag in Ci or is also forwarding its complaint flag.
A complaint-based forest schedule S = (A, (C0 , . . . , Ct )) has the same properties as a forest schedule, with the exception that (C0 , . . . , Ct ) is a complaintbased parallel schedule as opposed to a parallel schedule. A complaint-based
forest schedule is said to be greedy if all particles perform movements according
to Definition 2 in the direction of their unique paths whenever possible.
We can now extend the dominance argument to hold with respect to complaint distance in addition to head and tail distances. For any particle p holding
a complaint flag f in configuration C, we define its complaint distance dc (f, C)
to be the number of edges along Pp from the node p occupies to the end of Pp .
For any two configurations C and C 0 and any complaint flag f , we say that C
dominates C 0 w.r.t. f , denoted C(f )  C 0 (f ), if and only if dc (f, C) ≤ dc (f, C 0 ).
Extending the previous notion of dominance, we say that C dominates C 0 , denoted C  C 0 , if and only if C dominates C 0 with respect to every particle and
with respect to every complaint flag.

On the Runtime of Universal Coating for Programmable Matter

17

It is also possible to construct a greedy complaint-based forest schedule whose
configurations are dominated by their asynchronous counterparts, as we did for
greedy forest schedules in Lemma 3. Many of the details are the same, so as to
avoid redundancy we highlight the differences here. The most obvious difference
is the inclusion of complaint flags. Definition 2 restricts particles to holding at
most one complaint flag at a time, where Algorithm 3 allows a capacity of two.
This allows the asynchronous execution to not “fall behind” the parallel schedule
in terms of forwarding complaint flags. Basically, Definition 2 allows a particle
p holding a complaint flag f in the parallel schedule to forward f to its parent
q even if q currently holds its own complaint flag, so long as q is also forwarding
its flag at this time. The asynchronous execution does not have this luxury of
synchronized actions, so the mechanism of buffering up to two complaint flags
at a time allows it to “mimic” the pipelining of forwarding complaint flags that
is possible within one round of a complaint-based parallel schedule.
Another slight difference is that a contracted particle cannot expand into an
empty adjacent node unless it holds a complaint flag to consume. However, this
restriction reflects Algorithm 4, so once again the greedy complaint-based forest
schedule can be constructed directly from the movements taken in the asynchronous execution. Moreover, since this restriction can only cause a contracted
particle to remain contracted, the conditions of Lemma 2 are still upheld. Thus,
we obtain the following lemma:
Lemma 4. Given any fair asynchronous activation sequence A which begins at
(A)
an initial configuration C0 in which every expanded parent has at least one contracted child, there is a greedy complaint-based forest schedule S = (A, (C0 , . . . , Ct ))
(A)
(A)
with C0 = C0 such that Ci  Ci for all 0 ≤ i ≤ t.
By Lemmas 3 and 4, once we have an upper bound for the time it takes
a greedy forest schedule to reach a final configuration, we also have an upper
bound for the number of rounds required by the asynchronous execution. Hence,
the remainder of our proofs will serve to upper bound the number of parallel
rounds any greedy forest schedule would require to solve the coating problem
for a given valid instance (P, O), where |P | = n. Let S ∗ = (A, (C0 , . . . , Cf )) be
such a greedy forest schedule, where C0 is the initial configuration of the particle
system P (of all contracted particles) and Cf is the final coating configuration.
In Sections 4.3 and 4.4, we will upper bound the number of parallel rounds
required by S ∗ in the worst case to coat the first layer and higher layers, respectively. More specifically, we will bound the worst-case time it takes to complete a layer i once layers 1, . . . , i − 1 have been completed. For convenience,
we will not differentiate between complaint-based and regular forest schedules
in the following sections, since the same dominance result holds whether or
not complaint flags are considered. To prove these bounds, we need one last
definition: a forest–path schedule S = (A, (C0 , . . . , Ct ), L) is a forest schedule
(A, (C0 , . . . , Ct )) with the property that all the trees of A(C0 ) are rooted at a
path L = v1 v2 · · · v` ⊆ Geqt , and each particle p must traverse L in the same
direction.

18

4.3

Daymude, et al.

First layer: complaint-based coating and leader election

Our algorithm must first organize the particles using the spanning forest primitive, whose runtime is easily bounded:
Lemma 5. Following the spanning forest primitive, the particles form a spanning forest within O(n) rounds.
Proof. Initially all particles are idle. In each round any idle particle adjacent to
the object, an active (follower or root) particle, or a retired particle becomes
active. It then sets its parent flag if it is a follower, or becomes the root of a
tree if it is adjacent to the object or a retired particle. In each round at least
one particle becomes active, so — given n particles in the system — it will take
O(n) rounds in the worst case until all particles join the spanning forest.
t
u
For ease of presentation, we assume that the particle system is of sufficient
size to fill the first layer (i.e., B1 ≤ n; the proofs can easily be extended to
handle the case when B1 > n); we also assume that the root of a tree also generates a complaint flag upon its activation (this assumption does not hurt our
argument since it only increases the number of the flags generated in the system). Let S1 = (A, (C0 , . . . , Ct1 ), L1 ) be the greedy forest–path schedule where
(A, (C0 , . . . , Ct1 )) is a truncated version of S ∗ , Ct1 — for t1 ≤ f — is the configuration in S ∗ in which layer 1 becomes complete, and L1 is the path of nodes
in layer 1. The following lemma shows that the algorithm makes steady progress
towards completing layer 1.
Lemma 6. Consider a round i of the greedy forest–path schedule S1 , where
0 ≤ i ≤ t1 − 2. Then within the next two parallel rounds of S1 , (i) at least
one complaint flag is consumed, (ii) at least one more complaint flag reaches a
particle in layer 1, (iii) all remaining complaint flags move one position closer
to a super-root along L1 , or (iv) layer 1 is completely filled (possibly with some
expanded particles).
Proof. If layer 1 is filled, (iv) is satisfied; otherwise, there exists at least one
super-root in A(Ci ). We consider several cases:
Case 1. There exists a super-root s in A(Ci ) which holds a complaint flag. If
s is contracted, then it can expand and consume its flag by the next round.
Otherwise, consider the case when s is expanded. If it has no children, then
within the next two rounds it can contract and expand again, consuming its
complaint flag; otherwise, by Lemma 2, s must have a contracted child with
which it can perform a handover to become contracted in Ci+1 and then expand
and consume its complaint flag by Ci+2 . In any case, (i) is satisfied.
Case 2. No super-root in A(Ci ) holds a complaint flag and not all complaint flags
have been moved from follower particles to particles in layer 1. Let p1 , p2 , . . . , pz
be a sequence of particles in layer 1 such that each particle holds a complaint
flag, no follower child of any particle except pz holds a complaint flag, and no

On the Runtime of Universal Coating for Programmable Matter

19

particles between the next super-root s and p1 hold complaint flags. Then, as
each pi forwards its flag to pi−1 according to Definition 2, the follower child of
pz holding a flag is able to forward its flag to pz , satisfying (ii).
Case 3. No super-root in A(Ci ) holds a complaint flag and all remaining complaint flags are held by particles in layer 1. By Definition 2, since no preference
needs to be given to flags entering layer 1, all remaining flags will move one
t
u
position closer to a super-root in each round, satisfying (iii).
We use Lemma 6 to show first that layer 1 will be filled with particles (some
possibly still expanded) in O(n) rounds. From that point on, in another O(n)
rounds, one can guarantee that expanded particles in layer 1 will each contract
in a handover with a follower particle, and hence all particles in layer 1 will be
contracted, as we see in the following lemma:
Lemma 7. After O(n) rounds, layer 1 must be filled with contracted particles.
Proof. We first prove the following claim:
Claim. After 8B1 + 2 rounds of S, layer 1 must be filled with particles.
Proof. Suppose to the contrary that after 8B1 + 2 rounds, layer 1 is not completely filled with particles. Then none of these rounds could have satisfied (iv)
of Lemma 6, so one of (i), (ii), or (iii) must be satisfied every two rounds. Case
(i) can be satisfied at most B1 times (accounting for at most 2B1 rounds), since a
super-root expands into an unoccupied position of layer 1 each time a complaint
flag is consumed. Case (iii) can also be satisfied at most B1 times (accounting for at most 2B1 rounds), since once all remaining complaint flags are in
layer 1, every flag must reach a super-root in B1 moves. Thus, the remaining
8B1 + 2 − 2B1 − 2B1 = 4B2 + 2 rounds must satisfy (ii) 2B1 + 1 times, implying that 2B1 + 1 flags reached particles in layer 1 from follower children. But
each particle can hold at most one complaint flag, so at least B1 + 1 flags must
have been consumed and the super-roots have collectively expanded into at least
B1 + 1 unoccupied positions, a contradiction.
t
u
By the claim, it will take at most 8B1 + 2 rounds until layer 1 is completely
filled with particles (some possibly expanded). In at most another B1 rounds,
every expanded particle in layer 1 will contract in a handover with a follower
particle (since B1 ≤ n), and hence all particles in layer 1 will be contracted after
O(B1 ) = O(n) rounds.
t
u
Once layer 1 is filled, the leader election primitive can proceed. The full
description of the Universal Coating algorithm in [3] uses a node-based version of
the leader election algorithm in [1] for this primitive. For consistency, we kept this
description of the primitive in this paper as well. However, in order to formally
prove with high probability guarantees on the runtime of our universal coating
algorithm, we use a Monte Carlo variant of the leader election algorithm in [1].
A description of this variant and its corresponding proofs appear in [19]. This
updated algorithm elects a leader with high probability and gives the following
runtime bound.

20

Daymude, et al.

Lemma 8. Within O(n) further rounds, a position of layer 1 has been elected
as the leader position, w.h.p.
Once a leader position has been elected and either no more followers exist
(if n ≤ B1 ) or all positions are completely filled by contracted particles (which
can be checked in an additional O(B1 ) rounds), the particle currently occupying
the leader position becomes the leader particle. Once a leader has emerged, the
particles on layer 1 retire, which takes O(B1 ) further rounds. Together, we get:
Corollary 1. The worst-case number of rounds for S ∗ to complete layer 1 is
O(n), w.h.p.

4.4

Higher layers

We again use the dominance results we proved in Section 4.2 to focus on parallel
schedules when proving an upper bound on the worst-case number of rounds
— denoted by Layer(i) — for building layer i once layer i − 1 is complete, for
2 ≤ i ≤ N . The following lemma provides a more general result which we can
use for this purpose.
Lemma 9. Consider any greedy forest–path schedule S = (A, (C0 , . . . , Ct ), L)
with L = v1 v2 · · · v` and any k such that 1 ≤ k ≤ `. If every expanded parent
in C0 has at least one contracted child, then in at most 2(` + k) configurations,
nodes v`−k+1 · · · v` will be occupied by contracted particles.
Proof. Let s be the super-root closest to v` , and suppose s initially occupies
node vi in C0 . Additionally, suppose there are at least k active particles in C0
(otherwise, we do not have sufficient particles to occupy k nodes of L). Argue
by induction on k, the number of nodes in L starting with v` which must be
occupied by contracted particles. First suppose that k = 1. By Lemma 2, every
expanded parent has at least one contracted child in any configuration Cj , so
s is always able to either expand forward into an unoccupied node of L if it
is contracted or contract as part of a handover with one of its children if it is
expanded. Thus, in at most 2(`+k) = 2`+2 configurations, s has moved forward
` positions, is contracted, and occupies its final position v`−k+1 = v` .
Now suppose that k > 1 and that each node v`−x+1 , for 1 ≤ x ≤ k − 1,
becomes occupied by a contracted particle in at most 2(` + k − 1) = 2(` +
k) − 2 configurations. It suffices to show that v`−k+1 also becomes occupied
by a contracted particle in at most two additional configurations. Let p be the
particle currently occupying v`−k+1 (such a particle must exist since we supposed
we had sufficient particles to occupy k nodes and S ensures the particles follow
this unique path). If p is contracted in C2(`+k)−2 , then it remains contracted
and occupying v`−k+1 , so we are done. Otherwise, if p is expanded, it has a
contracted child q by Lemma 2. Particles p and q thus perform a handover in
which p contracts to occupy only v`−k+1 at C2(`+k)−1 , proving the claim.
t
u

On the Runtime of Universal Coating for Programmable Matter

21

For convenience, we introduce some additional notation. Let ni denote the
number of particles
Pi−1 of the system that will not belong to layers 1 through i − 1,
i.e., ni = n − j=1 Bj , and let ti (resp., Cti ) be the round (resp., configuration)
in which layer i becomes complete.
When coating some layer i, each root particle either moves either (a) through
the nodes in layer i in the set direction dir (CW or CCW) for layer i, or (b)
through the nodes in layer i + 1 in the opposite direction over the already retired
particles in layer i until it finds an empty position in layer i. We bound the
worst-case scenario for these two movements independently in order to get a an
upper bound on Layer(i). Let Li = v1 , . . . , vBi be the path of nodes in layer
i listed in the order that they appear from the marker position v1 following
direction dir, and let Si = (A, (Cti−1 +1 , . . . , Cti ), Li ) be the greedy forest–path
schedule where (A, (Cti−1 +1 , . . . , Cti )) is a section of S ∗ . By Lemma 9, it would
take O(Bi ) rounds for all (a) movements to complete; an analogous argument
shows that all (b) movements complete in O(Bi+1 ) = O(Bi ) rounds. This implies
the following lemma:
Lemma 10. Starting from configuration Cti−1 +1 , the worst-case additional number of rounds for layer i to become complete is O(Bi ).
Putting it all together, for layers 2 through N :
Corollary 2. The worst-case number of rounds for S ∗ to coat layers 2 through
N is O(n).
Proof. Starting from configuration Ct1 +1 , it follows from Lemma 10 that the
worst-case number of rounds for S ∗ to reach a legal coating of the object is
upper bounded by
N
N
X
X
Layer(i) ≤ c
Bi = Θ(n),
i=2

where c > 0 is a constant.

i=2

t
u

Combining Corollaries 1 and 2, we get that S ∗ requires O(n) rounds w.h.p. to
coat any given valid object O starting from any valid initial configuration of the
set of particles P . By Lemmas 3 and 4, the worst-case behavior of S ∗ is an upper
bound for the runtime of our Universal Coating algorithm, so we conclude:
Theorem 2. The total number of asynchronous rounds required for the Universal Coating algorithm to reach a legal coating configuration, starting from an
arbitrary valid instance (P, O), is O(n) w.h.p., where n is the number of particles
in the system.

5

Simulation Results

In this section we present a brief simulation-based analysis of our algorithm
which shows that in practice our algorithm exhibits a better than linear average competitive ratio. Since OPT(P, O) (as defined in Section 3) is difficult

22

Daymude, et al.

to compute in general, we investigate the competitiveness with the help of an
appropriate lower bound for OPT(P, O). Recall the definitions of the distances
d(p, q) and d(p, U ) for p, q ∈ Veqt and U ⊆ Veqt . Consider any valid instance
(P, O). Let L be the set of all legal particle positions of (P, O); that is, L contains all sets U ⊆ Veqt such that the positions in U constitute a coating of the
object O by the particles in the system.
We compute a lower bound on OPT(P, O) as follows. Consider any U ∈ L,
and let G(P, U ) denote the complete bipartite graph on partitions P and U . For
each edge e = (p, u) ∈ P × U , set the cost of the edge to w(e) = d(p, u). Every
perfect matching in G(P, U ) corresponds to an assignment of the particles to
positions in the coating. The maximum edge weight in a matching corresponds
to the maximum distance a particle has to travel in order to take its place in the
coating. Let M (P, U ) be the set of all perfect matchings in G(P, U ). We define
the matching dilation of (P, O) as



MD(P, O) = min
min
max (w(e))
.
U ∈L

M ∈M (P,U )

e∈M

Since each particle has to move to some position in U for some U ∈ L to
solve the coating problem, we have OPT(P, O) ≥ MD(P, O). The search for
the matching that minimizes the maximum edge cost for a given U ∈ L can be
realized efficiently by reducing it to a flow problem using edges up to a maximum
cost of c and performing binary search on c to find the minimal c such that a
perfect matching exists. We note that our lower bound is not tight. This is due
to the fact that it only respects the distances that particles have to move but
ignores the congestion that may arise, i.e., in certain instances the distances
to the object might be very small, but all particles may have to traverse one
“chokepoint” and thus block each other.

Fig. 5. (a) shows the number of rounds varying the number of particles. (b) shows
the ratio of number of rounds to the lower bound in log scale. (c) shows the number
of rounds varying the static hexagon radius.

We implemented the Universal Coating algorithm in the amoebot simulator
(see [20] for videos). For simplicity, each simulation is initialized with the object

On the Runtime of Universal Coating for Programmable Matter

23

O as a regular hexagon of object particles; this is reasonable since the particles
need only know where their immediate neighbors in the object’s border are
relative to themselves, which can be determined independently of the shape
of the border. The particle system P is initialized as idle particles attached
randomly around the hexagon’s perimeter. The parameters that were varied
between instances are the radius of the hexagon and the number of (initially
idle) particles in P . Each experimental trial randomly generates a new initial
configuration of the system.
Figure 5(a) shows the number of rounds needed to complete the coating with
respect to the hexagon object radius and the number of particles in the system.
The number of rounds plotted are averages over 20 instances of a given |P | with
95% confidence intervals. These results show that, in practice, the number of
rounds required increases linearly with particle system size. This agrees with
our expectations, since leader election depends only on the length of the object’s
surface while layering depends on the total number of particles. Figure 5(b)
shows the ratio of the number of rounds to the matching dilation of the system.
These results indicate that, in experiment, the average competitive ratio of our
algorithm may exhibit closer to logarithmic behaviors. Figure 5(c) shows the
number of rounds needed to complete the coating as the radius of the hexagon
object is varied. The runtime of the algorithm appears to increase linearly with
both the number of active particles and the size of the object being coated, and
there is visibly increased runtime variability for systems with larger radii.

6

Conclusion

This paper continued the study of universal coating in self-organizing particle
systems. The runtime analysis shows that our Universal Coating algorithm, presented in [3], terminates in a linear number of rounds, so it is worst-case optimal.
This, along with the linear lower bound on the competitive gap between local
and global algorithms, further shows our algorithm to be competitively optimal.
Furthermore, the simulation results show the competitive ratio of our algorithm
is better than linear in practice. In the future, we would like to apply the algorithm and analysis to the case of bridging, in which particles create structures
across gaps between disconnected objects. We would also like to extend the algorithm to have self-stabilization capabilities, so that it could successfully complete
coating without human intervention after occasional particle failure or outside
interference.

References
1. J. Daymude, Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. A. Bazzi, A. W.
Richa, C. Scheideler, Leader election and shape formation with self-organizing
programmable matter, in: arXiv:1503.07991., 2016. A preliminary version of this
work appeared in DNA 21. 2015, pp. 117–132.
2. Z. Derakhshandeh, S. Dolev, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann,
Brief announcement: amoebot - a new model for programmable matter, in: ACM
SPAA, 2014, pp. 220–222.

24

Daymude, et al.

3. Z. Derakhshandeh, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann, Universal
coating for programmable matter, Theoretical Computer Sciencedoi:dx.doi.org/
10.1016/j.tcs.2016.02.039.
4. N. A. Lynch, Distributed algorithms, Morgan Kaufmann, 1996.
5. D. Doty, Theory of algorithmic self-assembly, Comm. of ACM 55 (12) (2012) 78–88.
6. M. J. Patitz, An introduction to tile-based self-assembly and a survey of recent
results, Natural Computing 13 (2) (2014) 195–224.
7. D. Woods, Intrinsic universality and the computational power of self-assembly, in:
Machines, Computations and Universality., 2013, pp. 16–22.
8. D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, R. Peralta, Computation in
networks of passively mobile finite-state sensors, Distributed Computing 18 (4)
(2006) 235–253.
9. V. Bonifaci, K. Mehlhorn, G. Varma, Physarum can compute shortest paths, in:
ACM SODA, 2012, pp. 233–240.
10. K. Li, K. Thomas, C. Torres, L. Rossi, C.-C. Shen, Slime mold inspired path
formation protocol for wireless sensor networks, in: ANTS, 2010, pp. 299–311.
11. S. Wilson, T. Pavlic, G. Kumar, A. Buffin, S. C. Pratt, S. Berman, Design of
ant-inspired stochastic control policies for collective transport by robotic swarms,
Swarm Intelligence 8 (4) (2014) 303–327.
12. M. Brambilla, E. Ferrante, M. Birattari, M. Dorigo, Swarm robotics: a review from
the swarm engineering perspective, Swarm Intelligence 7 (1) (2013) 1–41.
13. G. P. Kumar, S. Berman, Statistical analysis of stochastic multi-robot boundary
coverage, in: ICRA, 2014, pp. 74–81.
14. T. Pavlic, S. Wilson, G. Kumar, S. Berman, An enzyme-inspired approach to
stochastic allocation of robotic swarms around boundaries, in: ISRR, 2013, pp.
16–19.
15. L. Blázovics, K. Csorba, B. Forstner, H. Charaf, Target tracking and surrounding
with swarm robots, in: ECBS, 2012, pp. 135–141.
16. L. Blázovics, T. Lukovszki, B. Forstner, Target surrounding solution for swarm
robots, in: EUNICE, 2012, pp. 251–262.
17. O. Michail, P. G. Spirakis, Simple and efficient local codes for distributed stable
network construction, in: ACM PODC, 2014, pp. 76–85.
18. Z. Derakhshandeh, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann, An
algorithmic framework for shape formation problems in self-organizing particle
systems, in: NANOCOM, 2015, pp. 21:1–21:2.
19. J. J. Daymude, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann, Leader
election with high probability for self-organizing programmable matter, CoRR
abs/1701.03616.
20. Self-organizing particle systems, sops.engineering.asu.edu/simulations/.

Universal Shape Formation for Programmable Matter
Zahra Derakhshandeh

Robert Gmyr

Andréa W. Richa

Arizona State University

Paderborn University

Arizona State University

zderakhs@asu.edu
gmyr@mail.upb.de
aricha@asu.edu
Christian Scheideler
Thim Strothmann
Paderborn University

Paderborn University

scheideler@upb.de

thim@mail.upb.de

ABSTRACT

limited devices (which we refer to as particles) which can
move and bond and exchange information in order to solve
the given goal in a collective manner and without any outside
intervention. One can envision a number of applications for
these particle systems, most prominently shape formation
and coating problems. Various algorithms for the formation
of specific shapes have already been proposed within our
model, but they are all inherently sequential: starting with
a seed particle, the particles bond to each other in a chainlike fashion till the desired shape has been constructed. So
for n particles Ω(n) rounds are needed to finish the shape
formation. In this work we do not only show how to design a
universal shape formation algorithm but we also show that
for any shape composed of a constant number of equilat√
eral triangles of unit size, our approach just needs O( n)
rounds to arrive at the desired shape, when starting in a
well-initialized shape. We also show that this bound is best
possible. Our shape formation approach is based on the geometric amoebot model [11], which we briefly describe in the
next section.

We envision programmable matter consisting of systems of
computationally limited devices (which we call particles)
that are able to self-organize in order to achieve a desired
collective goal without the need for central control or external intervention. Central problems for these particle systems
are shape formation and coating problems. In this paper, we
present a universal shape formation algorithm which takes
an arbitrary shape composed of a constant number of equilateral triangles of unit size and lets the particles build that
shape at a scale depending on the number
of particles in the
√
system. Our algorithm runs in O( n) asynchronous execution rounds, where n is the number of particles in the system,
provided we start from a well-initialized configuration of the
particles. This is optimal in a sense that for any shape deviating from the initial
√ configuration, any movement strategy
would require Ω( n) rounds in the worst case (over all asynchronous activations of the particles). Our algorithm relies
only on local information (e.g., particles do not have ids,
nor do they know n, or have any sort of global coordinate
system), and requires only a constant-size memory per particle.

1.1

Keywords
Self-Organizing Systems; Programmable Matter; Distributed
Algorithms

1.

Amoebot model

We assume that any structure the particle system can
form can be represented as a subgraph of an infinite graph
G = (V, E) where V represents all possible positions the
particles can occupy relative to their structure, and E represents all possible atomic transitions a particle can perform
as well as all places where neighboring particles can bond to
each other. In the geometric amoebot model, we assume that
G = Geqt , where Geqt is the infinite regular triangular grid
graph. Figure 1(a) illustrates the standard planar embedding of Geqt .

INTRODUCTION

The vision behind programmable matter is to have a piece
of matter that can change its physical properties like shape,
density, conductivity, or color in a programmable fashion
based on either user input or autonomous sensing. Many
realizations of programmable matter have been proposed —
ranging from DNA tiles, shape-changing molecules, and synthetic cells, to reconfigurable modular robotics — each pursuing solutions for specific application scenarios with their
own, special capabilities and constraints. We envision programmable matter to consist of a system of computationally

(a)

(b)

(c)

Figure 1: (a) shows a section of Geqt . Nodes of Geqt
are shown as black circles. (b) shows five particles
on Geqt . The underlying graph Geqt is depicted as
a gray mesh. A particle occupying a single node is
depicted as a black circle, and a particle occupying
two nodes is depicted as two black circles connected
by an edge. (c) depicts two particles occupying two
non-adjacent positions on Geqt . The particles have
different offsets for their head port labelings.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SPAA ’16, July 11-13, 2016, Pacific Grove, CA, USA
c 2016 ACM. ISBN 978-1-4503-4210-0/16/07. . . $15.00

DOI: http://dx.doi.org/10.1145/2935764.2935784

289

We recall the main properties of the geometric amoebot
model from [11]. Each particle occupies either a single node
or a pair of adjacent nodes in Geqt , and every node can be
occupied by at most one particle. Two particles occupying
adjacent nodes are connected by a bond, and we refer to such
particles as neighbors. The bonds do not just ensure that
the particles form a connected structure but they are also
used for exchanging information as explained below.
Particles move through expansions and contractions: If a
particle occupies one node (i.e., it is contracted ), it can expand to an unoccupied adjacent node to occupy two nodes.
If a particle occupies two nodes (i.e., it is expanded ), it can
contract to one of these nodes to occupy only a single node.
Figure 1(b) illustrates a set of particles (some contracted,
some expanded) on the underlying graph Geqt . For an expanded particle, we denote the node the particle last expanded into as the head of the particle and call the other
occupied node its tail. For a contracted particle, the single
node occupied by the particle is both its head and its tail.
A handover allows particles to stay connected as they move.
Two scenarios are possible here: (1) a contracted particle
p can “push” a neighboring expanded particle q and expand
into the neighboring node previously occupied by q, forcing q
to contract, or (2) an expanded particle p can “pull” a neighboring contracted particle q to node v it occupies thereby
causing q to expand into v, which allows p to contract.
Particles are anonymous. Each particle has a collection
of ports, one for each edge incident to the nodes occupied
by it, that have unique labels from the local perspective of
that particle. Adjacent particles establish bonds through the
ports facing each other. We assume that the particles have
a common chirality, i.e., they all have the same notion of
clockwise direction, which allows a particle to order the port
labels of its head and tail in clockwise order. However, particles do not have a common sense of orientation since they
can have different offsets of the labelings, see Figure 1(c).
Without loss of generality, we assume that each particle labels its head and tail ports from 0 to 5 in clockwise order.
Whenever a particle p is connected through some port to a
particle q, we assume that p knows the label of q’s port that
lies opposite of the respective port of p. Futhermore, we
assume that p knows whether q’s port belongs to the head
or tail of q.
Each particle has a constant-size shared local memory that
can be read and written to by any neighboring particle. This
allows a particle to exchange information with a neighboring
particle by simply writing it into the other particle’s memory. A particle always knows whether it is contracted or
expanded, and in the latter case it also knows along which
head port label it is expanded. We assume that this information is also available to the neighboring particles (by
publishing that label in its local shared memory). Particles
do not know the total number of particles, nor do they have
any estimate on this number.
We assume the standard asynchronous model from distributed computing, where the particle system progresses
through a sequence of particle activations, i.e., only one particle is active at a time. Whenever a particle is activated, it
can perform an arbitrary bounded amount of computation
(involving its local memory as well as the shared memories
of its neighbors) and at most one movement. We define a
round to be over once each particle has been activated at
least once.

The configuration C of the system at the beginning of
time t consists of the nodes in Geqt occupied by the set
of particles; in addition, for every particle p, C contains the
current state of p, including whether the particle is expanded
or contracted, its port labeling, and the contents of its local
memory. For more details on the model, please refer to [11].

1.2

Related work

Many approaches have already been proposed that can potentially be used for shape formation. One can distinguish
between active and passive systems. In passive systems the
particles either do not have any intelligence at all (but just
move and bond based on their structural properties or due
to chemical interactions with the environment), or they have
limited computational capabilities but cannot control their
movements. Examples of research on shape formation in
passive systems appear in DNA self-assembly systems (see,
e.g., [9, 17, 20]), and also the surveys in [16, 23]). In particular, [17] presents an approach to fold long single-stranded
DNA molecules into arbitrary 2D shapes. In [15, 14], the authors study network topology and shape construction problems under a variation of the population protocols model [1],
focusing on specific simple structures, such as a spanning
line, a spanning star, or a spanning square.
In active systems, computational particles can control the
way they act and possibly move in order to solve a specific
task. Robotic swarms, modular robotic systems, and cellular automata are some examples of active programmable
matter systems. In swarm robotics, one usually has a collection of autonomous robots that have limited sensing, often
including vision, and communication ranges, and that can
freely move in a given area. They follow a variety of goals,
including shape formation (e.g., [18, 2, 12]). For example,
in [18], the authors demonstrate that programmable selfassembly of complex two-dimensional shapes with hundreds
or thousands of simple robots called kilobots is possible; however their approach differs considerably from ours, since the
algorithms rely on a global pre-processing phase for shape
formation that directly depends on the number of robots in
the system. In [2], the authors explore a method to build
and repair arbitrary two-dimensional shapes on a 2D coordinate system with a swarm of self-assembly robots. In [12],
the authors study the algorithmic limitations of building
a predefined exact shape by a set of autonomous mobile
robots. Samples of other representative work can be found
in e.g., [19, 7, 8]. While the analytic techniques developed
in the area of swarm robotics and natural swarms are of
some relevance for this work, the individual units in those
systems have more powerful communication and processing
capabilities than the systems we consider.
Beyond conventional actuation, sensing, and control typically found in fixed-morphology robots, self-reconfigurable
robots are also able to adapt to form desired shapes (see
e.g., [25, 26, 27]). Metamorphic robots form a subclass of
self-reconfigurable robots that share some characteristics of
our geometric model [6]. The hardware development in the
field of self-reconfigurable robotics has been complemented
by a number of algorithmic advances (e.g., [3, 22, 18]), but
so far mechanisms that scale from a few to hundreds or thousands of individual units are still under investigation. While
pattern formation has been extensively studied in the cellular automata model (e.g., in [21, 4, 13]), these studies differ
from our model since particles can replicate (or die) at will.

290

The nubot model [24, 5] aims at providing the theoretical
framework that would allow for a more rigorous algorithmic study of biomolecular-inspired systems, more specifically of self-assembly systems with active molecular components. One of the main results of [24] is to efficiently construct two-dimensional geometric shapes in polylogarithmic
time in the size of the shape. While bio-molecular inspired
systems share many similarities with self-organizing particle
systems, there are differences — e.g., the system allows for
an additional (non-local) notion of rigid-body movement.
Finally, in [10], we presented a preliminary algorithmic
framework in the geometric amoebot model that allows particles to to build simple shapes, such as a hexagon or a
triangle. The number of rounds required by the algorithms
presented in this work is at least linear on the number of
particles in the worst-case.

ometric shape S 0 defined by a translation, a rotation by a
multiple of 60◦ , and an isotropic scaling with the restriction
that the vertices of the triangles forming S 0 all coincide with
vertices in Geqt . Consider the set of nodes V (S 0 ) ⊆ V (Geqt )
that lie on a vertex of S 0 , on an edge of S 0 , or inside of S 0 .
We call V (S 0 ) a representation of the shape S. Figure 2
illustrates this definition by an example.

1.3

Figure 2: Two representations of a shape S consisting of 16 faces. For ease of presentation, we will often
represent sets of particles via geometric shapes. In
this example, each position inside a triangle, on a
side of a triangle, and on a vertex of a triangle is
occupied by a particle. The left part shows a representation in which the triangles coincide with the
faces of Geqt . Note that in this very small representation of S the hole suggested by the triangles is
actually closed and therefore the representation is
a solid quadrilateral. The right part shows a representation that is rotated counter-clockwise by 120◦
compared to the left representation and in which
each triangle consists of four faces in Geqt .

Our Contributions

In this paper, we present a universal shape formation algorithm for self-organizing particle systems which takes as
input an arbitrary shape composed of a constant number of
equilateral triangles and lets the particles build that shape
at a scale depending on the number of particles in the system. We limit the number of triangles to a constant because
in this case every particle can store the entire shape description, which can be seen as the base case for the study of
universal shape formation algorithms. More complex scenarios (i.e., a shape description is spread across multiple
particles, or the shape is only implicitly defined via some
algorithm)
are left for future research. Our algorithm runs
√
in O( n) asynchronous execution rounds, where n is the
number of particles in the system, provided we start from
a well-initialized configuration in which the particles form
a (not necessarily
complete) triangle. Starting in a shape
√
with O( n) diameter, like
√ a triangle, is important because
if the
initial
shape
had
ω(
n) diameter, a runtime bound of
√
O( n) can no longer be guaranteed, no matter which algorithm is used for the shape formation. We chose the triangle
as the initial structure since it turned out to be most convenient for the design of a universal shape formation algorithm.
Our algorithm relies only on local information (e.g., particles do not have ids, nor do they know n, the total number
of particles, or have any sort of global coordinate system),
and requires only a√
constant-size memory per particle.
Note that the O( n) runtime bound is much better than
the runtime bounds of O(n) that were previously shown for
specific shapes [10]. The reason for this is that the approach
in [10] is inherently sequential while we make full use of
the available parallelism in this paper.
In fact, any shape
√
formation algorithm would need Ω( n) rounds to form any
shape that is not just a single triangle, so our runtime bound
is best possible.

2.

For the shape formation problem, we assume that initially
all particles are contracted and they are arranged in a triangle. All particles are in the same idle state except for a
unique leader particle that occupies one of the vertices of
the initial triangle and that stores the desired shape S in
its local memory (note that a leader particle could also be
established by letting the particles occupying the vertices of
the initial triangle engange in a competition using random
coin flips that are transmitted along the sides of the triangle
in a pipelined fashion). If the number of particles in the system n is not a triangular number, the last row of the initial
triangle is not completely filled. For ease of presentation,
we assume for now that n is a triangular number and hence
the particles initially form a perfect triangle. We show how
to avoid this assumption at the end of Section 5. To solve
the shape formation problem, the system has to reconfigure so that the set of positions occupied by the particles is
some representation of S. We allow particles in the final
configuration to be expanded.

3.

PROBLEM STATEMENT

MOVEMENT PRIMITIVES

The shape formation algorithm reconfigures the system by
using primitives for moving connected sets of particles. The
simplest of these primitives moves a chain of particles along
a path: Consider a simple directed path P of length ` in
Geqt . Let the first m nodes of P be occupied by contracted
particles for some m ≤ `. The goal is to let the particles
traverse P , that is, the particles should move only through
the nodes in P , they have to stay connected at all times, and
in the end the particles should all be contracted and should
occupy the last m nodes of P . This is achieved by letting

In the shape formation problem, a particle system has to
reconfigure into a given shape. Formally, let S be a set of
faces in the standard planar embedding of Geqt . Hence, S is
a set of nodes in the dual graph of Geqt . We say two faces of
Geqt are connected if their corresponding nodes in the dual
graph of Geqt are connected. We say S is connected if the
induced subgraph of the dual graph of Geqt is connected.
We define a shape to be a connected set of faces that has
constant size. Consider any transformation of S into a ge-

291

the particles move as a connected chain using handovers.
Lemma 1 bounds the number of rounds required for the
particles to traverse P . Section 3.1 provides more detail
about particle chain movement.

chain and all particles move forward along the same, simple
path of nodes in Geqt (i.e., no node is traversed more than
once by a particle) in their given order, so at the end the
particles still form a particle chain of the same order as initially. Let P = (v1 , . . . , v` ) be the path used in a given line
schedule, where v1 is the node occupied by the tail of p1 (the
trailing particle in the particle chain) at the beginning of the
schedule and v` is the node occupied by the head of pm (the
leading particle) at the end of the schedule. For any configuration C of the particle chain and any particle pi in that
chain we define its head position h(C, i) to be the index of
the node in P occupied by the head of pi in C and its tail position t(C, i) to be the index of the node occupied by the tail
of pi in C. Certainly, t(C, i) ∈ {h(C, i), h(C, i) − 1}, depending on whether pi is contracted or expanded. For any two
chain configurations C1 and C2 and any particle pi we say
that C1 dominates C2 w.r.t. pi (or short, C1 (pi ) ≥ C2 (pi ))
if and only if h(C1 , i) ≥ h(C2 , i) and t(C1 , i) ≥ t(C2 , i). Altogether, we say that C1 dominates C2 (or short, C1 ≥ C2 )
if and only if C1 dominates C2 w.r.t. every particle.
We say that a given particle chain emulates a line schedule
S if it starts in the initial configuration of S and every particle greedily aims at reaching its final position in S. That is,
whenever pi has not yet reached its final position in S, it tries
to expand, contract, or use a handover whenever activated
in order to advance towards its final position. However, it
will only do so if this does not disconnect the particle chain.

Lemma 1. A chain of particles of size m ≤ ` can traverse
a path P of length ` in Geqt in O(`) rounds.
For the remaining primitives, consider a set of contracted
particles that forms a triangle in Geqt . Let one of the vertices
of the triangle be occupied by a distinguished particle that
we call the coordinator of the triangle. Furthermore, let
the particles on the boundary of the triangle span a path
oriented counter-clockwise around the triangle.
We define four primitive operations on triangles, namely
expansion, contraction, rotation, and shift. Expansion, contraction, and rotation are shown in Figure 3. A shift of a

Figure 3: A triangle (left) can expand to form an
expanded triangle (middle). An expanded triangle
can contract back to a simple triangle (right). Concatening an expansion and a contraction effectively
rotates a triangle.

Lemma 3. For any line schedule C0 , . . . , Ck that a particle chain wants to emulate and any activation sequence of
the particles it holds for the configuration Ci0 of the particles
after round i that Ci0 ≥ Ci .

triangle simply moves all particles of the triangle in a common direction by one position. A movement is initiated and
controlled by the coordinator of a triangle. We give a detailed description of a local-control protocol for the expansion of a triangle in Section 3.2. Based on the general ideas
presented in that section, it is easy to design protocols for
the remaining operations. We have the following lemma.

Proof. We will prove the lemma by induction. Initially,
C00 = C0 by assumption. Hence, suppose that for some i ≥ 0
it holds that Ci0 ≥ Ci . Let us consider some fixed particle
pj . If h(Ci0 , j) > h(Ci , j) or t(Ci0 , j) > t(Ci , j), then certainly
0
Ci+1
(pj ) ≥ Ci+1 (pj ). Thus, it remains to consider the case
that h(Ci0 , j) = h(Ci , j) and t(Ci0 , j) = t(Ci , j). Here, we
consider the following subcases:

Lemma 2. A triangle of side length ` can be expanded,
contracted, rotated, and shifted in O(`) rounds.
Note that a √
triangle consisting of m particles has a side
length of O( √m). Therefore, the primitives move such a
triangle in O( m) rounds.

3.1

0
• If Ci+1 (pj ) = Ci (pj ), it certainly holds that Ci+1
(pj ) ≥
Ci+1 (pj ). So for the remaining cases we assume that
Ci+1 (pj ) > Ci (pj ).

Particle Chain Movement

• If pj is contracted in Ci , it must be expanded in Ci+1 ,
which means that pj+1 is expanded in Ci (or the position in front of it is empty, i.e., j = m). Since pj is also
contracted in Ci0 , the particles must form a connected
chain, and Ci0 ≥ Ci , this means that pj+1 must also
be expanded in Ci0 . No matter which of pj and pj+1 is
activated first, a handover will happen between them,
0
which implies that Ci+1
(pj ) ≥ Ci+1 (pj ). pj cannot be
involved in a handover with pj+1 before that because
for that it would have to be expanded.

A movement schedule is a sequence of particle system configurations C0 , C1 , . . . , Ck . A movement schedule is called a
valid parallel schedule (or simply valid ) if every Ci represents a connected particle system and for every i ≥ 0, Ci+1
is reached from Ci in a way that for every particle p one of
the following properties holds:
1. p occupies the same positions in Ci and Ci+1 , or
2. p expands into a node that was empty in Ci , or
3. p contracts, leaving the node occupied by its tail empty
in Ci+1 , or

• If pj is expanded in Ci , it must be contracted in Ci+1 ,
which means that pj−1 is contracted in Ci (or the position behind it was empty, i.e., j = 1). Since pj is also
expanded in Ci0 , the particles must form a connected
chain, and Ci0 ≥ Ci , this means that pj−1 must also be
contracted in Ci0 . No matter which of pj and pj−1 is
activated first, a handover will happen between them,
0
which implies that Ci+1
(pj ) ≥ Ci+1 (pj ). pj cannot be
involved in a handover with pj−1 before that because
for that it would have to be contracted.

4. p is part of a handover with another particle p0 .
Note that this means that several particles are allowed to
move from Ci to Ci+1 .
A particle chain is a sequence of particles (p1 , p2 , . . . , pm )
forming an arbitrary connected path of contracted or expanded particles in Geqt . A line schedule is a valid movement schedule in which the particles initially form a particle

292

0
Hence, in any case, Ci+1
(pj ) ≥ Ci+1 (pj ) for all pj , which
proves the lemma.

it is sent back to p along L3 . Once p receives the validation
token, it knows that the expansion is completed.
Since the tokens in this scheme only traverse distances of
O(`), the corresponding parts of the algorithm can be easily
realised using O(`) rounds. Each row of the triangle acts
as a chain of particles and expands independently over a
distance of at most O(`). By arguments analogous to those
in the proof of Lemma 3, this movement takes O(`) rounds.
Therefore, Lemma 2 holds.

Consider a chain of contracted particles that should move
along a path P of length `. It is easy to see that there is
a line schedule of length O(`) that lets the particle chain
traverse P . Therefore, Lemma 3 implies that by greedily
moving forward the particle chain traverses P in O(`) rounds
independently of the activation order of the particles. Hence,
Lemma 1 holds.

3.2

4.

Triangle Expansion

INTERMEDIATE STRUCTURE

The goal of this section is to reorganize the particles from
the initially given triangle into an intermediate structure
that simplifies the actual shape formation process. The intermediate structure we aim for is shown in Figure 5. It con-

A triangle coordinator p can expand its triangle to form
an expanded triangle, i.e., two triangles of the same size of
the original triangle that share a common side as illustrated
in Figure 4. Let T be the set of nodes occupied by the
L3

Figure 5: The intermediate structure.
L1

L2

sists of ∆ many equilateral triangles of side length ` (shown
in dark gray) that are arranged in a straight line and a remainder of particles (light gray) that is too small to form an
additional triangle. All particles in this structure are contracted. Note that the algorithm we present in this section
might actually create a structure that is slightly worse in
the sense that it might leave a larger remainder of particles.
We will come back to this at the end of the section; for now
we assume that we can build the ideal structure.
Given a certain number of particles, the value of ∆ is
completely determined by the value of `. Let s = |S|. For
the intermediate structure to be useful, we want to choose
` such that 34 s + 1 ≤ ∆ ≤ s − 3. We will elaborate on these
bounds in the following section; for now our focus remains on
building the intermediate structure. Let L be the side
√ length
of the initial triangle. We choose ` = dL/bc · sce with
c = 80/81. Lemma 4 shows that this choice is appropriate
for sufficiently large s and L; we omit the proof.
√
Lemma 4. If s ≥ 123 and L ≥ 34b sc, we have that
3
s + 1 ≤ ∆ ≤ s − 3.
4

Figure 4: An example of a counter-clockwise triangle expansion. The coordinator p is highlighted by
a black circle. Every row of particles expands independently as a chain (i.e., without disconnecting)
around p. The resulting trajectories are shown as
dashed arrows in the right part of the figure. Once
every particle that did not occupy a position in L1
is expanded, the triangle expansion is complete.
particles before the expansion and let ET be the set of positions occupied after the expansion. Let L1 be the set of
positions on the side of the initial triangle that occurs first
when traversing the path on the boundary of the triangle
starting from the coordinator in counter-clockwise direction.
Let L2 be the set of positions on the side of the initial triangle that corresponds to rotating L1 counter-clockwise by 60◦
around p, and let L3 be the set of positions on the side of the
expanded triangle that corresponds to rotating L1 counterclockwise by 120◦ . Let a row of a triangle be a subset of T
that forms a straight line in Geqt that is parallel to the side
of the initial triangle opposite of p.
An expansion is coordinated using the following token
passing scheme. To initiate the expansion, p sends an activation token along L1 . Once a particle receives the activation
token, it forwards the token along L1 and sends a row token
along its row. A particle that receives a row token forwards
it along the row. Furthermore, it considers the particle it
forwards the token to as its parent and it will follow that particle using handovers. In each row there is a unique particle
that occupies L2 . Once this particle receives the expansion
token it starts moving as depicted in Figure 4. In this way,
all particles not on L1 eventually expand, hence forming the
expanded triangle. To determine when the expansion of the
triangle is complete, the coordinator p proceeds as follows.
After p sent out the activation token it sends a validation token along L3 . This token can only be forwarded to a specific
row when the expansion of the particles in that row is complete. After the validation token completely traversed L3 ,

Note that the bounds on s and L given in Lemma 4 could
be lowered by a better choice of c and a tighter analysis.
We now turn to the construction of the intermediate structure. First, the particles have to determine `. √
Note that in
general no single particle can store ` as ` = Θ( n) and the
particles only have constant memory. Hence, we have to
store ` in a distributed fashion over multiple particles. To
determine ` we use the following token passing scheme: The
leader particle, which occupies a vertex of the initial triangle,
sends a counter token along an adjacent edge of the initial
triangle. This token stores a counter that counts√the number of steps it takes along the edge modulo bc · sc. Since
c and s are constants,
√ the leader can use a hard-coded table
to determine bc · sc and the token only requires constant
memory. Initially, the value stored in the counter token is
0. When a particle holds the counter token while its value
is 0, that particle will create a marker token (note that also
the leader creates a marker token). Once the counter token

293

completely traversed the edge of the initial triangle, it is
simply consumed by the particle occupying the corresponding vertex of the initial triangle. The consuming particle
will then create a terminal token. Both the marker tokens
and the terminal token travel in the opposite direction of
the counter token (i.e., towards the leader). Each particle is
only allowed to store either one marker token or the terminal
token. The only exception to this rule is the particle that
creates the terminal token. This particle might also have
created a marker token. In this case, the particle can temporarily store both tokens and has to first pass the marker
token and then the terminal token. Since the tokens all move
towards the leader, they will eventually occupy a segment
of consecutive particles starting with the leader particle and
ending with the particle that holds the terminal token. To
detect when this state is reached, each marker token stores
a boolean value that is initially false and becomes true if
either the marker token occupies the leader particle or the
marker token has another marker token in front of it with
its value set to true. Once the terminal token has a marker
token with its value set to true in front of it, it knows that
all tokens are done moving and sends a final token towards
the leader to inform him that the token passing scheme has
terminated. It is easy to check that the length of the segment occupied by the tokens is exactly `. Furthermore, √
it is
easy to show that the token passing scheme requires O( n)
rounds and only uses constant memory.
Once ` has been established, the intermediate structure
can be built using a globally synchronized process. This
simple process is coordinated by the leader and only uses
triangle shifts and triangle rotations, as illustrated in Figure 6. For simplicity, we assume a global perspective to

triangle is rotated, shifted twice, and rotated again as described in the caption of Figure 6. Then the process recurs.
Note that the orientation of the rotations and the direction
of the shifts changes with every recurrence. The process
ends after moving the first triangle that has a side length
less or equal to `.
The final part of our algorithm divides the structure into
triangles, each having its own coordinator, by assigning structural information to the particles. From a global perspective,
the intermediate structure is simply divided into triangles
that are arranged as in Figure 5. This process ends once no
more triangle can be formed in this way. The process can be
easily implemented locally as a traversal of the intermediate
structure using token passing that is globally coordinated by
the leader. Note that in this traversal the token(s) only have
to move along the edges of the prospective triangles. We assume that during the process the leader counts the number
of triangles ∆0 that have been built, and that in the end
the leader token moves back to its initial position. Concerning the number of rounds required to build the intermediate
structure we have the following lemma.
√
Lemma 5. The algorithm terminates after O( n) rounds.
Proof Sketch. The recursive part of the algorithm terminates after dL/`e = O(1) recursions. Each recursion requires a constant number
√ of triangle rotations and shifts,
each of which takes O( n) rounds. The remainder of the
recursive part of the algorithm as well as the final part of the
algorithm that divides the intermediate structure into triangles√can be implemented via token passing schemes requiring
O( n) rounds.
As mentioned earlier, the given algorithm builds a structure that is slightly different from the ideal intermediate
structure shown in Figure 5. Roughly speaking, the end of
the structure is not necessarily perfectly suited for the purpose of dividing the structure into triangles of side length `.
Consequently, the number of triangles ∆0 in the intermediate
structure might be one less than the number of triangles ∆
in an ideal intermediate structure, as we show in Lemma 6.
Lemma 6. The algorithm builds an intermediate structure containing ∆0 triangles where ∆ − 1 ≤ ∆0 ≤ ∆. The
particles that are not included in any triangle are arranged
as shown in Figure 7.

Figure 6: Construction of the intermediate structure. The parts forming the intermediate structure
is shown in dark gray. The triangles that are shifted
and rotated are shown in light gray. For each dotted arrow, the circles at the origin and the tip of the
arrow marks the same particle before and after the
respective triangle is moved. As an example, the
left-most triangle is rotated clockwise by 60◦ around
the vertex at the origin of the corresponding arrow,
then it is shifted once to the right and once to the
bottom-right, and finally it is rotated clockwise by
120◦ around the vertex at the tip of the arrow.

Proof. We first need the following simple observation:
The end of the structure built by the algorithm can take on
one of the three different forms illustrated in the first three
parts of Figure 7. If the last triangle moved by the algorithm

(a)

(b)

(c)

(d)

Figure 7: The end of the intermediate structure.
has side length greater or equal than ` − 1, one of the forms
in (a) and (b) is created. Otherwise, the end of the structure
will look similar to part (c) of Figure 7.
For the rest of the proof consider part (d) of Figure 7.
Without loss of generality, let the last triangle formed by
the algorithm lie as shown in dark gray. The potential next

describe this process; a local-control protocol implementing
this process is easy to design. The process is executed recursively starting with the initial triangle. It splits the triangle
into a wedge of height ` and a smaller triangle. The wedge
becomes the first part of the intermediate structure. The

294

triangle, shown as a dashed outline, cannot be formed. Consequently, the vertex of that triangle marked by a circle cannot be occupied. This holds because if it were occupied all
positions of the dashed triangle would be occupied, no matter which of the forms the end of the structure assumes. By
the same argument, all positions on the bold line have to be
unoccupied and also every position to the right of that line
has to be unoccupied. Hence, all particles of the intermediate structure that are not part of a triangle have to lie in the
light gray area. This area contains `(` − 1) positions. Since
a triangle of side length ` contains `(` + 1)/2 positions, the
particles that are not part of any triangle could form at most
one additional triangle. Therefore, the lemma holds.

(a)

Figure 9: Part (a) shows a representation of a shape
with six faces. Part (b) shows the triangles forming
that representation which requires triangles of three
different sizes.

The following theorem follows immediately from the lemmas presented throughout this section.
√
Theorem 1. For s ≥ 123 and L ≥ 34b sc, the algorithm
builds an intermediate structure consisting of ∆0 triangles
where 43 s ≤ ∆0 ≤ s − 3. The particles that are not included
in any triangle are arranged as shown in Figure 7.

5.

as N D 6= ∅ the leader determines the next face in CS (in
backwards order), removes the corresponding face from N D,
and adds it to D, according to the following two rules.
1. Let GN D be the subgraph of GS induced by the nodes
of N D. Mark all nodes as prospects that can be removed from GN D without disconnecting the graph.
2. Instead of specifying the second rule as a graph property, it is easier and more useful to specify the condition geometrically. Interpret the nodes of GN D as
triangles in the Euclidean plane according to the standard embedding (see Figure 1(a)). Since GN D is connected, we can combine the triangles to a single geometric shape (e.g., see Figure 10). Pick a prospect
that has an edge on the outer boundary of said shape.

SHAPE FORMATION ALGORITHM

We now turn to the actual shape formation algorithm.
From this point onwards we will use the term face exclusively to refer to a triangular face of Geqt and the term
triangle exclusively to refer to a set of particles occupying
nodes in Geqt that form a triangle. The algorithm we present
can be interpreted as a sequential global algorithm that is
coordinated by the leader and that operates almost exclusively on triangles. Recall that for a given shape S we want
to build a representation R(S). We will use the shape in the
left part of Figure 8 as a running example to illustrate the intermediate steps of the shape formation algorithm. Observe

20

12
7

5
2

11
10

13

14

19
18

6
1

(b)

3

8

9

15

17
16

Figure 10: Two steps of the computation of CS. The
left part shows the current shape after the faces of
rank 20 and 19 have been chosen by the rules. The
dotted line indicates the outer boundary of the current shape. Note that even though the two faces in
the upper right corner are not connected, the hole
of the polygon is not part of the outer boundary because the faces share a vertex. The right part shows
the shape after the subsequent application of the
rules (i.e. face 18 is chosen). Now the former hole
is part of the boundary of the shape.

4

Figure 8: The running example of Section 5. The
left part shows the example shape. The right part
shows a corresponding construction schedule CS.
The number inside a face indicates its rank in CS.
that building R(S) generally requires triangles of different
side lengths, namely `, ` − 1, and ` − 2 (see Figure 9), since
nodes in Geqt can only be occupied by one particle.
Before building R(S), the leader computes a construction
schedule CS, i.e., a permutation of the faces that dictates
the chronological order in which the corresponding triangles
are placed. The schedule is computed backwards, i.e., the
leader starts with determining which triangles to place last.
Let GS be the subgraph of the dual graph of Geqt induced
by S. For the computation of CS, let D (resp. N D) be
the subset of S that contains the faces that already have
an assigned chronological ranking in CS (resp. have not
been assigned yet). Initially, D = ∅ and N D = S. As long

We call the chronological position of a face a ∈ S in CS
the rank of a in CS (i.e., the face chosen first by the rules has
rank s and the one chosen last rank 1). See the right part of
Figure 8 for a complete construction schedule for the running example. Informally speaking, the first rule guarantees
that the schedule respects the connectivity requirements of
a shape. The second rule ensures that we build R(S) from
the inside outwards, since outside faces are chosen early (i.e.,
they appear late in CS). The following lemma shows that
our algorithm always computes a correct schedule.

295

Lemma 7. The algorithm assigns a rank to each face.
Proof. It is sufficient to show that in each step there
exists at least one face of N D that fulfills both rules. We
will consider the two rules in reversed order. Let N D2 be
the set of faces in N D that have an edge that lies on the
outer boundary of the polygon as specified in the second
rule. If there is a node in N D2 with degree 1 in GN D2 (i.e.,
the subgraph of GS induced by the nodes of N D2 ), that
node trivially fulfills both rules. Therefore, assume that all
nodes of N D2 have a degree ≥ 2 in GN D2 . Consequently,
there has to exist at least one simple cycle among a subset
of nodes of N D2 . In each simple cycle there has to exist at
least one node with degree exactly two, since otherwise the
cycle would not be on the outer boundary of the polygon.
Naturally, we can remove this node from a cycle without
disconnecting GN D . Thus, in each step the two rules find a
node that can be removed from N D.

Figure 12: A conceptual sketch of how to incorporate the remainder of the intermediate structure
into R(S). As shown in Lemma 6 the remainder can
at most occupy the nodes of two adjacent triangles
of side length ` − 1 (here, we abstract from concrete
side lengths). The left part illustrates how the first
two triangles (light grey), one of side length ` the
other of side length ` − 1, are placed adjacent to the
remainder. The rows of those triangles that can expand into the white area do so (as depicted by the
arrows). In the right part the rows have expanded
as far as possible. The remainder and the two partially expanded triangles together form four faces of
S.

Moreover, we show that the C fulfills two properties that
we need in order to build R(S). The prefix of length i of a
construction schedule CS is a subschedule of CS containing
only the first i elements of CS.
Lemma 8. Consider CS as computed by our set of rules.
For any prefix of length i ∈ {1, . . . , n} it holds that: (i) the
subgraph of GS induced by the prefix is connected and (ii)
the face of rank i in CS has an edge that lies on the outer
boundary of the shape corresponding to the prefix.

right part of Figure 12). For this initial construction step
two expanded triangles are necessary.
However, it is possible that the first four faces in CS do
not form one of the subshapes shown in Figure 11 (a) and
(b), but the one shown in Figure 11 (c) (apart from 60◦ rotation of those shapes there are no other possibilities, since
faces have to be connected). Note that this subshape does
not have two adjacent faces such that each has another adjacent face. For case (c) consider all possibilities for the
fifth face in CS (i.e., faces with dashed boundary in Figure 11 (c)). No matter where it is placed, the resulting
shape contains one of the two desired shapes as a subshape.
Consequently, by switching the order of the first five faces
in CS, we get an adjusted schedule CS 0 that starts with
a desired subshape. In the running example, the first four
faces of CS form the subshape of figure Figure 11 (c) (see
right part of Figure 8); CS 0 is chosen accordingly (see left
part of Figure 13).

Proof. The first statement of the lemma follows immediately from the first rule for computing CS, and the second
statement follows immediately from the second rule.
Once CS is computed, the building of R(S) does not start
immediately. Section 4 pointed out that the intermediate
structure is not perfect (see Figure 7), but has a remainder,
i.e., particles that are not included in any triangle of the intermediate structure. Instead of ignoring the remainder, we
want to incorporate it into R(S) without moving it. Ultimately, we aim at building one of the two subshapes shown
in Figure 11 (a) and (b) first (possibly rotated by a multiple
of 60◦ ). These subshapes have the advantage that there are
two adjacent (inner) faces (shown in dark grey in Figure 11)
such that both face are also adjacent to another face (light
grey). The remainder will (partially) occupy the triangles

(a)

(b)

(c)

20

12
7

4

Figure 11: Three subshapes of S. We aim at building
a rotated version of subshape (a) or (b) first. The
subshape shown in (c) is the one we want to avoid
building first. The faces with a dashed boundary are
all possible locations to attach a fifth face.

2

20

11
10

13

14

6
1
5

3

8

9

12

19

6

18

5

15

17

4

16

2

1

11
10

13

14

19
18

3

8

9

15

17
16

7

Figure 13: The two adjusted construction schedules
of CS. The left part shows the adjusted construction
schedule CS 0 . The first four faces (dark gray) now
form a desired subshape. The right part illustrates
CS 00 . The root is the subshape shown in dark gray.

corresponding to the two inner faces. We then use two triangles from the intermediate structure to represent the outer
faces (see left part of Figure 12). Finally, the two triangles
will use the expansion primitive to occupy all nodes in the
inner triangles that are not occupied by the remainder (see

296

Once the remainder of the intermediate structure has been
incorporated as described, the algorithm performs one intermediate step before constructing the rest of R(S). As already mentioned, triangles of three possible sizes are needed.
Therefore, each triangle is pruned to its desired size while
it is still in the intermediate structure. We will elaborate
later how the leader decides which triangles to prune to
a certain size; for now we focus on the pruning itself and
waste particles arising from that pruning (i.e., particles that
are removed from triangles). The pruning of a triangle is
done by removing (i.e., moving away) the lower one or two
rows. The number of waste√ particles introduced through
this pruning procedure is O( n), since the number of triangles is constant and each triangle is pruned by at most two
rows. In order to incorporate the waste in R(S), we want to
construct an expanded triangle next (corresponding to faces
ranked 5 and 6 in CS 0 ). Note that the faces of rank 5 and 6
of CS 0 are not necessarily adjacent. Similar to the first four
faces we can adjust CS 0 to get a schedule that has the desired property. To be more precise, consider the subshapes
in Figure 11 (a) and (b) (i.e., the two possible subshapes
we fixed for the first four triangles). If we add seven more
faces to the subshape, the resulting shape will have at least
another pair of adjacent faces. Therefore, it is sufficient to
swap the order among the faces of rank 5 to 11 in CS 0 to
get an adjusted schedule CS 00 that has two adjacent faces
at rank 5 and 6 which are connected to the faces with ranks
1 through 4, while still having the properties of Lemma 8.
The schedule CS 00 for the running example is shown in the
right part of Figure 13. We call the substructure consisting
of the first six faces in CS 00 the root of the shape. During
the construction of the root, we use particle chain movement
to move the waste particles to some position that does not
interfere with the construction of the root. Once the the
root is constructed, we again use particle chain movement
to move the waste particles to the expanded triangle forming the faces with rank 5 and 6. This completely expanded
triangle then contracts as many particles as necessary to incorporate all of the waste particles. Note that the fact that
we need at least three expanded triangles for the construction of the root matches the upper bound ∆0 ≤ s − 3 from
to Section 4.
Note that CS 00 is oblivious to the fact that 34 s ≤ ∆0 ≤
s − 3. Consequently, it might be that more than the three
already mentioned expanded triangles are needed to build
R(S). Due to our lower bound on ∆0 , we certainly have
enough triangles. Nevertheless, the leader has to determine
which pairs of faces to construct using expanded triangles.
Let η = s − ∆0 , i.e., at most η many expanded triangles are
needed to built R(S).

2
3

2

2
2
2
2

2

2

1

Figure 14: A graphical representation of the two intermediate steps of the proof of Lemma 9. The left
part illustrates the spanning tree T (indicated by
the dotted lines). The right part shors the deconstruction of T . The numbers indicate the different
possibilities of the rules: (1) a single face connected
to the root, (2) a pair of faces that was found by the
first rule, and (3) three faces that the second rule
was applied to.
and count ρ, the number of pairs found. Since we already
know how to build the root, we do not apply the rules to
these six nodes, i.e., the process stops once T solely consist
of the six root nodes and constantly many leaves that the
rules cannot be applied to.
1. Remove a leaf and its predecessor from T if that removal does not disconnect the remaining tree. By definition such a pair can be represented by an expanded
triangle.
2. If the first rule is not applicable, each predecessor of
a leaf is a predecessor for two leafs. Remove all three
nodes from T , i.e., a pair and a singleton node. We
can represent these three faces by one expanded and
one contracted triangle.
The right part of Figure 14 shows the deconstruction of T
for our running example. The lemma holds if η ≤ ρ. Note
that ρ is minimized if we always have to apply the second
rule. Futhermore, note that at most 8 faces of S are adjacent
to the root. So overall, since the root consists of 6 faces, we
get that ρ ≥ 31 (s − 6 − 8) + 3. Additionally, it holds that
η ≤ 41 s due to our lower bound on ∆0 . Thus, η ≤ ρ if
s ≥ 20, which holds according to the lower bound on s (see
Theorem 1).
As a consequence, the leader exactly knows how many expanded triangles it needs and has a simple rule to determine
where to use an expanded triangle. The leader still has to
specify which triangles have to be pruned. However, with
CS 00 and the information gained from the deconstruction of
T , it can easily do so: It iterates over CS 00 and determines
for each face whether the face is represented by a triangle
of size ` − 2, ` − 1, or `. In case two adjacent faces are
represented by an expanded triangle, it is important that either both faces are assigned the same size or that the lower
ranked face gets the bigger size. Otherwise, the expansion
would fail, since a triangle of size ` − 1 has not enough particles to occupy all nodes of a triangle of size ` and and a
triangle of size ` − 1. Afterwards, the leader can prune the
triangles in the intermediate structure accordingly. Thus,
the building process of R(S) according to CS 00 is in general

Lemma 9. For any shape S the leader can identify at least
η many disjoint pairs of adjacent faces.
Proof. For a shape S consider the adjusted schedule
CS 00 . Let r(a) denote the rank of a face a ∈ S in CS 00 . Additionally, let the predecessor pred(a) of a be the face b ∈ S
such that a and b are connected, r(a) > r(b), and r(b) is minimal. Note that every face has a predecessor except for the
face with rank 1. Let T be the spanning tree of GS induced
by CS 00 , i.e., T = (V, E ∗ ) with E ∗ = {{a, b} | pred(a) = b}
(see left part of Figure 14).
In order to show that the leader can identify enough pairs
of faces, we give the following two rules to deconstruct T

297

straightforward. Once the root is built as described, the
leader sequentially builds R(S) according to CS 00 by either
moving a triangle with triangle rotations and triangle shifts
from the intermediate structure or by expanding an already
placed triangle.
Finally note that CS 00 is oblivious to the position of the
intermediate structure. If we keep the structure fixed in
its initial position, it might get in the way of the construction of the shape or it might get walled-in, i.e., it might get
completely surrounded by already placed triangles. We can
avoid both of these problems as follows: Before moving a
triangle in CS 00 , the leader can easily check whether the intermediate structure is in the way or would get walled in.
If this is the case, it simply moves the entire intermediate
structure, triangle by triangle, to a different position that
is still adjacent to the already placed triangles but where it
does not interfere with the construction. We now show that
CS 00 is a correct schedule, i.e., we can actually build R(S)
if we follow the order of the construction schedule.

or move √
a triangle to its desired position. The former case
takes O( n) rounds according to Lemma 2. For the latter
case, we have to move the triangle along all other
√ triangles
in the worst case. Doing so takes at most O( n) rounds,
due to Lemma 2 and Observation 1). Additionally, it might
be possible that we have to move the whole intermediate
structure every time a new triangle is moved. We move the
structure
triangle by triangle to its new position which takes
√
O( n) rounds.
The following theorem shows that our algorithm is actually optimal in terms of worst-case number of rounds.
Theorem 3. Let S be any shape except for a triangle.
Our algorithm is O(1)-competitive against any algorithm
that solves the shape formation problem with regards to number of rounds.
Proof. First note that an algorithm that solves the shape
formation problem cannot control the activation order of
particles, thus, every particle moves only a distance of at
most 1 per round in the worst case. If S is not a triangle
there√
exists at least one particle that has to move at distance
of Ω( n) in order to form the
√desired shape. Therefore, any
algorithm needs at least Ω( n) rounds to solve the shape
formation problem in the worst case.

Lemma 10. For any shape S our algorithm computes a
construction schedule CS 00 that allows the construction of
R(S).
Proof. First note that the intermediate structure does
not interfere with the construction process since it is moved
out of the way when necessary. We have already shown how
to build the root. It remains to show for i > 6 that if the
first i − 1 faces have been built, we can always build the
the face of rank i in CS 00 . There are two cases: Either an
already built triangle has to expand or a new triangle has to
be moved from the intermediate structure to the desired position. First consider the former case. There already exists
a triangle a that corresponds to a face of rank strictly less
than i that is adjacent to the face of rank i. Since the triangle corresponding to a has the same size or a larger size than
the triangle corrsponding to the face with rank i, the i-th
triangle can in fact be built using triangle expansion. Now
consider the second case, i.e., a triangle has to be moved.
According to property 1 of Lemma 8, the i-th face is connected to at least one of the first i − 1 faces. According to
property 2 of Lemma 8, there exists a path along which the
new triangle can be moved from the intermediate structure
to the desired position. Thus, we can always build R(S)
according to CS 00 .

Finally, we explain how to get rid of the assumption that
the number of particles is a triangular number (see Section 2). As a consequence, the initial triangle of particles
is not necessarily perfect anymore, i.e., the last row of the
initial triangle is not completely
filled with particles. Note
√
that there are at most O( n) particles in the last row and
the leader can determine by a token passing scheme whether
the last row is complete. While the intermediate structure is
constructed, the incomplete row is moved to a safe location
(by using the particle chain movement from Section 3) such
that it does not interfere with the process described in Section 4. During the construction of the representation of S
we move the incomplete row together with the intermediate
structure if necessary. We incorporate the row into R(S)
early on by moving it into the third expanded triangle we
construct, which is similar to the way the waste particles are
handled. There is enough space for the waste particles
√ and
the incomplete row since both amounts are in O( n) and
the expanded triangle can incorporate Ω(n) many particles.
During the entire algorithm,
the incomplete row only has to
√
move a distance of O( n), which is in line with the given
runtime bound.

We conclude our section with the following theorem.
Theorem 2. The shape formation
algorithm solves the
√
shape formation problem in O( n) rounds.

6.

Proof. By Lemma 10 we can build R(S) according to
CS 00 . Moreover, consider the following observation.

CONCLUSION

We presented a universal shape formation algorithm for
self-organizing particle systems which takes as input an arbitrary shape composed of a constant number of equilateral
triangles and lets the particles build that shape at a scale
depending on the number of particles in the system. For future work, it would be interesting to investigate more complex descriptions of the shape that has to be built: a shape
description of non-constant size could be spread across multiple particles, or the shape could be defined only implicitly
via some algorithm.
Furthermore, initial configurations of
√
diameter O( n) other than the triangle could be considered.

Observation 1. Consider two triangles of the same size
that are arranged such that two vertices of one triangle are
adjacent to a position of the other triangle (e.g., two triangles in the intermediate structure shown in Figure 5). We
can rotate one triangle completely around the other using
only a constant number of triangle rotations and shifts.
The computation of CS with its adjustments√is done locally by the leader. Building the root takes O( n) rounds.
Pruning
the triangles and moving the waste particles takes
√
O( n) rounds according to Lemma 1. For each face in the
schedule we either need to expand an already placed triangle

298

7.

ACKNOWLEDGMENTS

[14] O. Michail. Terminating distributed construction of
shapes and patterns in a fair solution of automata. In
Proceedings of PODC 2015, pages 37–46, 2015.
[15] O. Michail and P. G. Spirakis. Simple and efficient
local codes for distributed stable network construction.
In Proceedings of PODC 2014, pages 76–85, 2014.
[16] M. J. Patitz. An introduction to tile-based
self-assembly and a survey of recent results. Natural
Computing, 13(2):195–224, 2014.
[17] P. W. Rothemund. Folding DNA to create nanoscale
shapes and patterns. Nature, 440(7082):297–302, 2006.
[18] M. Rubenstein, A. Cornejo, and R. Nagpal.
Programmable self-assembly in a thousand-robot
swarm. Science, 345(6198):795–799, 2014.
[19] M. Rubenstein and W.-M. Shen. Automatic scalable
size selection for the shape of a distributed robotic
collective. In Proceedings of IROS 2010, pages
508–513. IEEE, 2010.
[20] N. Schiefer and E. Winfree. Universal computation
and optimal construction in the chemical reaction
network-controlled tile assembly model. In DNA
Computing and Molecular Programming (DNA 21),
pages 34–54, 2015.
[21] J. L. Schiff. Cellular automata: a discrete view of the
world, volume 45. John Wiley & Sons, 2011.
[22] J. E. Walter, J. L. Welch, and N. M. Amato.
Distributed reconfiguration of metamorphic robot
chains. Distributed Computing, 17(2):171–189, 2004.
[23] D. Woods. Intrinsic universality and the
computational power of self-assembly. In Proceedings
of MCU 2013, pages 16–22, 2013.
[24] D. Woods, H.-L. Chen, S. Goodfriend, N. Dabby,
E. Winfree, and P. Yin. Active self-assembly of
algorithmic shapes and patterns in polylogarithmic
time. In ITCS, pages 353–354, 2013.
[25] K. Yeom. Bio-inspired automatic shape formation for
swarms of self-reconfigurable modular robots. In
Proceedings of BIC-TA 2010, pages 469–476, 2010.
[26] K. Yeom and B.-J. You. Agent based morphological
approach for collaborative shape formation of
self-organizable unmanned aerial vehicles. In
Proceedings of HPCC EUC 2013, pages 85–92, 2013.
[27] M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll,
H. Lipson, E. Klavins, and G. S. Chirikjian. Modular
self-reconfigurable robot systems. IEEE Robotics
Automation Magazine, 14(1):43–52, 2007.

This work was partially supported by the NSF under the
Awards CCF-1353089 and CCF-1422603. Furthermore, it
was partially supported by DFG grant SCHE 1592/3-1.

8.

REFERENCES

[1] D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and
R. Peralta. Computation in networks of passively
mobile finite-state sensors. Distributed Computing,
18(4):235–253, 2006.
[2] D. Arbuckle and A. Requicha. Self-assembly and
self-repair of arbitrary shapes by a swarm of reactive
robots: algorithms and simulations. Autonomous
Robots, 28(2):197–211, 2010.
[3] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita.
Generic decentralized control for lattice-based
self-reconfigurable robots. International Journal of
Robotics Research, 23(9):919–937, 2004.
[4] A. Chavoya and Y. Duthen. Using a genetic algorithm
to evolve cellular automata for 2d/3d computational
development. In 8th annual conference on genetic and
evolutionary computation, pages 231–232, 2006.
[5] M. Chen, D. Xin, and D. Woods. Parallel computation
using active self-assembly. In DNA Computing and
Molecular Programming, pages 16–30. Springer, 2013.
[6] G. Chirikjian. Kinematics of a metamorphic robotic
system. In Proceedings of ICRA ’94, volume 1, pages
449–455, 1994.
[7] S. Das, P. Flocchini, N. Santoro, and M. Yamashita.
On the computational power of oblivious robots:
forming a series of geometric patterns. In Proceedings
of PODC 2010, pages 267–276, 2010.
[8] X. Defago and S. Souissi. Non-uniform circle
formation algorithm for oblivious mobile robots with
convergence toward uniformity. Theoretical Computer
Science, 396(1-3):97–112, 2008.
[9] E. D. Demaine, M. J. Patitz, R. T. Schweller, and
S. M. Summers. Self-assembly of arbitrary shapes
using RNAse enzymes: Meeting the kolmogorov
bound with small scale factor (extended abstract). In
Proceedings of STACS ’11, pages 201–212, 2011.
[10] Z. Derakhshandeh, R. Gmyr, A. W. Richa,
C. Scheideler, and T. Strothmann. An algorithmic
framework for shape formation problems in
self-organizing particle systems. In Proceedings of
NANOCOM 2015, 2015.
[11] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. A.
Bazzi, A. W. Richa, and C. Scheideler. Leader election
and shape formation with self-organizing
programmable matter. In DNA Computing and
Molecular Programming (DNA 21), pages 117–132,
2015.
[12] P. Flocchini, G. Prencipe, N. Santoro, and
P. Widmayer. Arbitrary pattern formation by
asynchronous, anonymous, oblivious robots.
Theoretical Computer Science, 407(1):412–447, 2008.
[13] K. Imai, Y. Kasai, Y. Sonoyama, C. Iwamoto, and
K. Morita. Self-reproduction and shape formation in
two and three dimensional cellular automata with
conservative constraints. In Proceedings of the Eighth
International Symposium on Artificial Life and
Robotics, pages 377–380, 2003.

299

Computer Networks 94 (2016) 307–317

Contents lists available at ScienceDirect

Computer Networks
journal homepage: www.elsevier.com/locate/comnet

Parameterized maximum and average degree approximation
in topic-based publish-subscribe overlay network design✩
Melih Onus a,∗, Andréa W. Richa b
a

Department of Computer Engineering, Ted University, Ankara, Turkey
Computer Science and Engineering, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe,
AZ 85287, Arizona, United States

b

a r t i c l e

i n f o

Article history:
Received 15 January 2015
Revised 16 October 2015
Accepted 26 October 2015
Available online 10 November 2015
Keywords:
peer to peer
multicast
publish/subscribe
optimization problems
overlay networks

a b s t r a c t
Publish/subscribe communication systems where nodes subscribe to many different topics
of interest are becoming increasingly more common. Designing overlay networks that connect the nodes subscribed to each distinct topic is hence a fundamental problem in these
systems. For scalability and eﬃciency, it is important to keep the degree of the nodes in the
publish/subscribe system low. Ideally one would like to be able not only to keep the average
degree of the nodes low, but also to ensure that all nodes have equally the same degree, giving
rise to the following problem: Given a collection of nodes and their topic subscriptions, connect the nodes into a graph with low average and maximum degree such that for each topic t,
the graph induced by the nodes interested in t is connected. We present the ﬁrst polynomial
time parameterized sublinear approximation algorithm for this problem.
We also propose a heuristic for constructing topic-connected networks with low average degree and diameter 2 and validate our results through simulations.
© 2015 Elsevier B.V. All rights reserved.

1. Introduction
In publish/subscribe (pub/sub) systems, publishers and
subscribers interact in a decoupled fashion. They use logical
channels for delivering messages according to the nodes subscription to the services of interest. Publishers publish their
messages through logical channels that deliver the messages
to the nodes that subscribed to the respective services.
Pub/sub systems can be either topic-based or contentbased. In a topic-based pub/sub system, messages are published to “topics”, where each topic is uniquely associated
with a logical channel. Subscribers in a topic-based system

✩
A preliminary version containing some of the results in this paper appeared at IEEE ICDCSÆ10. This research was supported in part by NSF grants
CCF-1116368 and CCF-0830704.
∗
Corresponding author. Tel.: +905302863985.
E-mail addresses: melih.onus@tedu.edu.tr (M. Onus), aricha@asu.edu
(A.W. Richa).

http://dx.doi.org/10.1016/j.comnet.2015.10.023
1389-1286/© 2015 Elsevier B.V. All rights reserved.

will receive all messages published to the topics to which
they subscribe. The publisher is responsible for deﬁning the
classes of messages to which subscribers can subscribe. In a
content-based system, messages are only delivered to a subscriber if the attributes of those messages match constraints
deﬁned by the subscriber; each logical channel is characterized by a subset of these attributes. The subscriber is responsible for classifying the messages.
Given their simplicity and wide applicability, we have
seen many implementations of those systems in recent years
(see e.g., [1–4,6–10,17,19,24,25,27–29]), as well as many applications built on top of them, such as stock-market monitoring engines, RSS [28] feeds [20], on-line gaming and several others. For a survey on pub/sub systems, see [16].
Publish/subscribe overlays with the topic connected overlay property are employed in a number of industrial solutions at Google, Yahoo, and Spotify [14,26]. PNUTS is a parallel and geographically distributed system for Yahoo!s web
applications. PNUTS uses asynchronous replication to ensure

308

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

low latency updates. PNUTS uses Yahoo! message broker, a
publish-subscribe system developed at Yahoo!, both as its
replacement for a redo log and as its replication mechanism [14]. Spotify is a worldwide popular peer-assisted music streaming service. Spotify uses publish/subscribe system
for matching, disseminating and persisting billions of publications everyday [26].
We will implement a topic-based pub/sub system by designing a connected (peer-to-peer) overlay network for each
pub/sub topic: more speciﬁcally, for each topic t, we will
enforce that the subgraph induced by the nodes interested
in t will be connected. This translates into a fully decentralized topic-based pub/sub system since any given topicbased overlay network will be connected and thus nodes subscribed to a given topic do not need to rely on other nodes
(agents) for forwarding their messages. Such an overlay network is said to be topic-connected.
Low node degrees are desirable in practice for scalability and also due to bandwidth constraints. Nodes with a high
number of adjacent links will have to manage all these links
(e.g., monitor the availability of its neighbors, incurring in
heartbeats and keep-alive state costs, and connection state
costs in TCP) and the traﬃc going through each of the links,
without being able to take great advantage of aggregating
the traﬃc (which would also reduce the number of packet
headers, responsible for a signiﬁcant portion of the traﬃc for
small messages). See [12,21–23] for further motivation.
The node degrees and number of edges required by a
topic-connected overlay network will be low if the node subscriptions are well-correlated. In this case, by connecting
two nodes with many coincident topics, one can satisfy connectivity of many topics for those two nodes with just one
edge. Several recent empirical studies suggest that correlated
workloads are indeed common in practice [20,28].
In this paper, we focus on building overlay networks with
low (average and maximum) node degrees. The importance
of minimizing both the maximum and average degree has
been recognized in some network domains, such as that of
survivable network design [18] and that of establishing connectivity in wireless networks [15]. To the best of our knowledge, minimizing both the maximum and the average degree
in topic-connected pub/sub overlay network design had not
been directly addressed prior to this work.
As in many other systems, a space-time trade-off exists
for pub-sub systems: On one hand, one would like the total time taken by a topic-based broadcast (which directly depends on the diameter of each topic-connected subnetwork)
to be as small as possible; on the other hand, for memory and
node bandwidth considerations, one would like to keep the
total degree of a node small. Those two measures are often
conﬂicting.
Most of the current solutions adopted in practice actually fail at maintaining both the diameter and the node degrees low. Take the naive, albeit popular, solution to topic
connected-overlay network design to construct a cycle connecting all nodes interested in a topic independently for each
given topic [29]: This construction results both in very large
diameter for each topic-connected network (proportional to
the total number of nodes subscribed to a topic) and in
node degrees proportional to the nodes’ subscription sizes,
whereas a more careful construction, taking into account the

Table 1
Summary of known results on overlay network construction for publish/subscribe communication (n: number of nodes, t: number of
topics, k is any parameter between 1 and n).

Chockler et. al. [12]
Onus and Richa [21,23]
This paper
Lower bound

Avg. degree

Max degree

O(log (n · t))
θ (n)
O(k · log (n · t))
(log n)

θ (n)

O(log (n · t))
O((n/k) · log (n · t))
(log n)

correlations among the node subscription sets might result in
much smaller node degrees (and total number of edges) and
topic-based diameters. Even the more recent advances on
approximating the average or maximum degree alone have
been made [12,21,23] still fail at approximating the diameter
well.
Whereas in the main contribution of our work (see
Section 3), we completely neglect the diameter of the networks constructed, in Section 6, we propose a new heuristic
for constructing topic-connected networks with low average
degree and diameter 2. In fact, the results in Section 6 are a
reﬁnement of the preliminary results presented in [21–23].
Section 6 constitutes the major extension in this work over
the results in [22].
1.1. Related work
The results in this paper are an improved and extended
version of our results in [22]. The major extensions in this
work are the following. We introduced an improved heuristic, 2-Diameter Overlay Design Algorithm(2D-ODA), and corresponding simulations(Section 6.3), which provide a much
more thorough approach to the problem and improves previous heuristics. We proved previously presented heuristics may produce overlay networks of high average degree.
The approximation ratio on the average degree obtained by
the previously presented algorithms may be as bad as (n)
(Section 6.2). We introduced a new case and our intuition for
the new heuristic and compared the heuristics using this case
(Section 6.2).
Chockler et al. [12] introduced the MinAv-TCO problem
[In the original paper, this problem was called Min-TCO.],
which aims at minimizing the average degree alone of a
topic-connected overlay network. They present an algorithm,
called GM, which achieves a logarithmic approximation on
the minimum average degree of the overlay network. The GM
algorithm follows the greedy approach described below:
The Greedy-Merge (GM) algorithm [12]: The GM algorithm
greedily adds the edge which maximally reduces the total
number of topic-connected components at each step of the
algorithm (initially we have the set of nodes V and no edges
between the nodes).
While minimizing the average degree is a step forward
towards improving the scalability and practicality of the
pub/sub system, their algorithm may still produce overlay
networks of very uneven node degrees where the maximum
degree may be unnecessarily high. In [21,23], it is shown that
GM algorithm may produce a network with maximum degree |V| while a topic-connected overlay network of constant
degree exists for the same conﬁguration of I (See Table 1).

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

In [21,23], the problem of minimizing the maximum degree of a topic-connected overlay network (MinMax-TCO)
is considered, and a logarithmic approximation algorithm
on the minimum maximum degree of the overlay network
(MinMax-ODA) is presented. The MinMax-ODA algorithm is
also a greedy algorithm, as described below:
Min-Max Overlay Design Algorithm (MinMax-ODA) [21, 23]:
Initially there are no edges between the set of nodes V. At
each step of the algorithm, add the edge which maximally
reduces the total number of topic-connected components
among the edges which increases maximum degree of the
current graph minimally.
The MinMax-ODA algorithm may produce overlay networks of very high average degree: As we will show in
Section 2.1, this algorithm may produce a network with average degree |V | − 2 while a topic-connected overlay network
of constant average degree exists for the same conﬁguration
of I (See Table 1).
In this work, we are, to the best of our knowledge, the
ﬁrst to formally address the problem of minimizing both
the maximum and the average degree in topic-connected
pub/sub overlay network design. As we mentioned earlier,
minimizing both maximum and average degree is important
in many network domains, such as that of survivable networks [18] and that of wireless networks [15]. The overlay
networks resulting from [2,5,10] are not required to be topicconnected. In [4,9,13,29], topic-connected overlay networks
are constructed, but they make no attempt to minimize the
average or maximum node degree. The ﬁrst papers to directly consider node degrees when building topic-connected
pub/sub systems were [12] and [21], as mentioned above.
Minimizing the diameter in topic-connected pub/sub overlay
network design ﬁrst addressed in [21,23].
Some of the high level ideas and proof techniques of [12]
and [21,23] have their roots in techniques used for the classical Set-Cover problem. We beneﬁt from some of the ideas
in [12,21,23] and also build upon their constructions for SetCover, extending and modifying them to be able to handle
the maximum degree and the average degree altogether.
1.2. Our contributions
In this work, we consider the problem of devising topicbased pub/sub overlay networks with low node degrees. One
could argue for keeping the maximum degree of a node low
or for keeping the overall average node degree low, since
both are important and relevant measures of the complexity and scalability of a system [15,18]. Unfortunately, previous
attempts at minimizing either one of these degree measures
alone [12,21] resulted in a linear explosion for the other measure (see Table 1).
In this work, we present the ﬁrst algorithm that aims
at keeping both the average and the maximum degree low.
More speciﬁcally, we consider the following problem:
Low-Degree Topic-Connected Overlay (Low-TCO) problem:
Given a collection of nodes V, a set of topics T, and the node
interest assignment I, connect the nodes in V into a topicconnected overlay network G which has both low average and
low maximum degree.
We present a parameterized sublinear approximation algorithm (Low-ODA) for this problem which approximates

309

both the average and the maximum degree well. More specifically, the Low-ODA algorithm achieves an average degree
approximation of O(min {k · log (n · t), n}) and a maximum
degree approximation of O(min {(n/k) · log (n · t), n}), where
n = |V | is the number of nodes in the network, t = |T | is the
number of topics and k is any parameter chosen from [1, n]
(See also Table 1). To the best of our knowledge, this is the
ﬁrst overlay network design algorithm that achieves sublinear approximations on both the average and maximum de√
grees (e.g., for k = n). The Low-ODA algorithm is a greedy
algorithm which relies on repeatedly evaluating the tradeoff of greedily adding an edge that would not increase the
maximum degree versus greedily adding an edge that would
lead to a small number of total edges in the ﬁnal overlay network. The main contribution of this work is therefore to show
that such a greedy approach can work and indeed leads to
non-trivial sublinear approximation on both the average and
maximum degree. We expect that the greedy parameterized
template introduced by our algorithm will lead to applications in other network design domains where scalability is a
key issue (see Section 7).
In addition, we present an algorithm (heuristic) for
building topic-based pub/sub networks where each topicconnected component is guaranteed to be of constant diameter – more speciﬁcally of diameter 2 – and where we aim
at keeping the average degree low. Our experimental results
show that our algorithm improves on the previous heuristic presented in [21–23] by factor 2 on the average degree.
We also showed that some previously presented algorithms
may produce overlay networks of high average degree: The
approximation ratio on the average degree obtained by the
previously presented algorithms may be as bad as (n), as
we show in Section 6.2. As we mentioned earlier, keeping the
node degrees and the network diameter low are key to the
design of scalable and eﬃcient topic-based pub/sub systems.
1.3. Structure of the paper
In Section 2, we present some deﬁnitions and restate the
formal problem deﬁnition. In Section 2.1, we present an outline of the related problem of minimizing the maximum
node degree, namely the MinMax-TCO problem, and the corresponding logarithmic approximation algorithm MinMaxODA proposed by Onus and Richa [21,23], since some of
the ideas presented will be useful for the Low-TCO problem. Section 3 presents our topic-connected overlay design
algorithm Low-ODA, whose approximation ratio is proved
in Section 4. Section 5 presents our simulation results
which validate the performance of the Low-ODA algorithm.
Section 6 presents our new algorithm for the problem of
minimizing the average node degree while enforcing a 2diameter overlay network. We conclude the paper, also presenting some future work, in Section 7.
2. Preliminaries
Let V be the set of nodes, and T be the set of topics. Let
n = |V |. The interest function I is deﬁned as I: V × T → {0, 1}.
For a node v ∈ V and topic t ∈ T, I(v, t ) = 1 if and only if node
v is subscribed to topic t, and I(v, t ) = 0 otherwise.

310

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

For a set of nodes V, an overlay network G(V, E) is an undirected graph on the node set V with edge set E⊆V × V. For a
topic t ∈ T, let Vt = {v ∈ V |I(v, t ) = 1}. Given a topic t ∈ T and
an overlay network G(V, E), the number of topic-connected
components of G for topic t is equal to the number of connected components of the subgraph of G induced by Vt . An
overlay network G is topic-connected if and only if it has one
topic-connected component for each topic t ∈ T. The diameter of a graph is the length of the longest shortest path in the
graph. The degree of a node v in an overlay network G(V, E) is
equal to the total number of edges adjacent to v in G.
2.1. Minimizing the maximum degree only
The MinMax-ODA algorithm (see Section 1.1), proposed
by Onus and Richa [21,23], addressed the MinMax-TCO problem, in which they aim at minimizing the maximum node degree. Unfortunately, while the MinMax-ODA algorithm produces a logarithmic approximation on the maximum node
degree, it fails to approximate well the average node degree
of a topic-connected overlay network: the approximation ratio on the average degree obtained by the MinMax-ODA algorithm may be as bad as (n), as we show in the lemma
below.
Lemma 1. The MinMax-ODA algorithm can only guarantee an
approximation ratio of (n) on the average node degree, where
n is number of nodes in the pub/sub system.
Proof. Consider the example where we have n nodes

v1 , v2 , . . . , vn , and n2 topics T = {ti, j |1 ≤ i, j ≤ n}. Node v1
is interested in all topics in T and each vi is interested in

ti, j and tj, i , 2 ≤ i ≤ n, 1 ≤ j ≤ n. W.l.o.g., assume that n is
even. The MinMax-ODA algorithm will produce an overlay
network with n(n − 2)/2 edges, by repeatedly connecting a
maximal matching of the nodes v1 , . . . , vn , n − 2 times. The
optimal overlay network with minimum number of edges is
E = {(v1 , vi )|1 < i ≤ n}, – the number of edges of this overlay network is n − 1. Hence the approximation ratio of the
MinMax-ODA algorithm can be as large as (n(n − 2)/2)/(n −
1) = (n). 
3. Low Degree Overlay Design Algorithm (Low-ODA)
In this section, we present our overlay design algorithm
(Low-ODA) for the Low-TCO problem. The weight of an edge
(u, v) is given by the reduction on the number of topicconnected components which would result from the addition
of (u, v) to the current overlay network. Let 1 ≤ k ≤ n. LowODA starts with the overlay network G(V, ∅). In each iteration
of Low-ODA, the algorithm considers two edges:
e1 : a maximum weight edge among the ones which minimally increases the maximum degree of the current graph
e2 : a maximum weight edge
If the weight of edge e1 is greater than the weight of e2
divided by k, edge e1 is added to edge set of the overlay network; otherwise edge e2 is added.
Let NC(V, E) denote total number of topic connected components in the overlay network given by (V, E).
Steps 1–6 of Low-ODA build an initial weighted graph
G	 (V, E 	 , w) on V, where E 	 = V × V and w({u, v}) is equal to
the amount of decrease in the number of topic-connected

components resulting from the addition of the edge (u, v)
to the current overlay network (represented by the edges
in OverlayEdges). Initially, this amount will be equal to the
number of topics that nodes u and v have in common.
At each iteration of the while loop, two edges are considered: an edge (e1 ) with maximum weight among the edges
in E	 that increase the maximum degree of the current graph
minimally and an edge (e2 ) with maximum weight in all of
E	 . If weight of the ﬁrst one (e1 ) is greater than or equal to
weight of the second one (e2 ) over k, e1 is added to the set of
overlay edges; otherwise e2 is added. Note that the addition
of an edge to OverlayEdges can either increase the maximum
degree by 1 or not increase it at all. The crux in the analysis of
this algorithm is to show that each of the edges will reduce
the number of connected components by a “large” amount
without increasing the maximum degree by too much.
While at a ﬁrst glance the Low-ODA algorithm may seem
like a trivial combination of the GM and MinMax-ODA algorithms, the analysis show that such a combination is far
from trivial: Once we allow the algorithm (Low-ODA) to select some edges based solely on their weight (edges of type
e2 ), the “perfect matching” behavior of the edges selected
by MinMax-ODA (basically one could show that the ﬁrst n/2
edges selected by MinMax-ODA formed a perfect matching
in G	 , as did the second set of n/2 edges, etc.) is no longer
valid and the approximation ratio analysis used in MinMaxODA (which heavily relied on this perfect matching decomposition of the edges selected) can no longer be directly used
here.
Lemma 2. The running time of Low-ODA is O(|V|2 |T|).
As shown in [11], by using a specialized indexing structure, the running time of the algorithm will be O(|V|2 |T|).
4. Approximation ratio
In this section, we will prove that our overlay design algorithm (Low-ODA) approximates the average degree by a

factor of O(k · log ( v∈V sv )) and the maximum degree by a

factor of O((n/k) · log ( v∈V sv )), where sv = |{t ∈ T |I(v, t ) =
1}|.
As we mentioned in the previous section, the main challenge in the analysis is to overcome the fact that we can no
longer think of the algorithm as selecting a sequence of perfect matchings of the nodes in V for bounding the approximation ratio on the maximum degree (the analysis of MinMaxODA algorithm heavily relied on this “perfect matching behavior”).
Now we present some deﬁnitions which will be useful for
the proofs of both Theorems 1 and 2. Recall that we use sv to
denote |{t ∈ T |I(v, t ) = 1}|. At the beginning of the algorithm,

the total number of connected components is Cstart = v∈V sv
and at the end Cend = |{t |t ∈ T and ∃v ∈ V such that I(v, t ) =
1}|. Note that since we count the connected components for
each topic separately, once we get down to Cend components,
there must exist exactly one component for each active topic t
(i.e., each t such that there exists some v with I(v, t ) = 1) and
hence the overlay network is topic-connected.
Theorem 1. The overlay network output by Low-ODA has aver
age node degree within a factor of O(k · log ( v∈V sv )) from the

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

minimum possible average node degree for any topic-connected
overlay network on V.
Proof. The proof follows the general lines of the proof of
the logarithmic approximation ratio for the classic set cover
problem (which was also the basis for the approximation ratio proof of the GM algorithm for the MinAv-TCO problem
[12]). Assume we have an instance of the Low-TCO problem
and that G(V, Eopt ) is a solution for this instance with minimum number of edges. Let |Eopt | = m.
Let ei be the ith edge added to the set by the algorithm Low-ODA. Let ni be total number of connected components before we add the ith edge, so n1 = Cstart . Let Si =
{e1 , e2 , . . . , ei−1 } be the set of all edges found before the algorithm starts adding the ith edge. Before Low-ODA starts
adding the ith edge, we have ni components and we know
that if we add all the edges Eopt − Si , to the current solution,
the total number of connected components will be reduced
to Cend . Since |Eopt − Si | ≤ m, there exists an edge which decreases the total number of connected components by at
least (ni − Cend )/m. Since our algorithm always adds at least a
(1/k)-optimal edge, the edge ei that our algorithm adds must
decrease the total number of connected components at that
time by at least (1/k) of this amount. Therefore,
ni − ni+1 ≥ (ni − Cend )/(m · k)
⇒ ni+1 − Cend ≤ (1 − 1/(m · k))(ni − Cend ).
Hence, the number of iterations of our algorithm LowODA is less than or equal to the largest z which satisﬁes
1 > (n1 − Cend )(1 − 1/(m · k))z
⇒ z ≤ m · k ln (Cstart − Cend )
⇒z ≤ m · kln (Cstart ).



Theorem 2. The overlay network output by Low-ODA has max
imum node degree within a factor of O((n/k) · log ( v∈V sv ))
from the minimum possible maximum node degree for any
topic-connected overlay network on V.
Proof. As with the proof of Theorem 1, the proof follows the
general lines of the proof of the logarithmic approximation
ratio for the classic set cover problem.
However, before we can apply the set cover framework,
we ﬁrst need to carefully relate the sequence of edges selected by the Low-ODA algorithm to a sequence of optimal
matchings which reduce the current number of connected
components maximally. Note that we no longer can break the
sequence of edges selected by our algorithm into a sequence
of perfect matchings, as in the MinMax-ODA algorithm.
Assume we have an instance of the Low-TCO problem and
that G(V, Eopt ) is a solution with minimum possible maximum degree for this instance. Let this maximum degree be
dopt . We will use the following well-known result in graph
theory for the proof. 
Lemma 3 (Lemma 4 in [21,23]). Given a graph G(V, E) with
maximum degree d, we can partition the edge set E into d + 1
matchings Mi , 1 ≤ i ≤ (d + 1).
Using the lemma above, we can partition the edge set Eopt
of the optimum solution into dopt + 1 matchings Mi , 1 ≤ i ≤
(dopt + 1).
At the start, all nodes have degree zero. At each iteration
of the while loop, a maximum weight edge among the ones

311

that increase the maximum degree of the current graph minimally or a maximum weight edge is added to the set of overlay edges.
After a number of iterations, the weight of a maximum
weight edge among the ones that increase the maximum degree of the current graph minimally will be less than the
weight of a maximum weight edge over k and we will add
this maximum weight edge to the graph – this edge will increase the maximum degree of the graph by 1.
Let Si be the edge set containing all edges added by LowODA from the time an edge e	 increased the maximum degree
of G	 from i − 1 to i until the last time an edge is added to G	
without increasing its maximum degree further (i.e., without
increasing the maximum degree to i + 1). Let h = n/(2k) + 1.

 
Let Ri = Sh·(i−1)+1 Sh·(i−1)+2 . . . Sh·(i−1)+h — i.e., Ri denotes the set of all edges added by Low-ODA while the resulting maximum degree was between h(i − 1) + 1 and hi.
  
Let RAi = R1 R2 . . . Ri−1 be the union of all edges added
before the algorithm starts adding the set Ri . Let ni be the total number of connected components before the algorithm
adds Ri , so n1 = Cstart .
The following lemma proves that each set Ri chosen by
our algorithm decreases the current total number of connected components by at least 1/3 of the number of current
components connected by any optimal matching, where an
optimal matching is one that reduces the current number of
connected components maximally among all possible maximal matchings in G	 . Note that a matching increases the maximum degree of G	 by at most 1.
Lemma 4. Joining the set Ri to the edges of G(V, RAi ) reduces
the total number of connected components of G(V, RAi ) by at
least 1/3 of the number of current components connected by any
optimal matching.
Proof. Let P be the edge set of the matching that reduces
the total number of connected components of the G(V, RAi )
by the maximum amount, which we denote by c. Let Q =
{e1 , e2 , . . . , e j } be the edge set of Ri . Let el = ul vl for 1 ≤ l ≤
j. For ea and eb , if a < b, then ea is found before eb by our algorithm. Let Q reduce the total number of connected compo
nents of G(V, RAi ) by c	 . Let G0 = G(V, RAi ) and Gl = Gl−1 el ,
for 1 ≤ l ≤ j. Let el reduce the total number of connected components of Gl−1 by yl . Then,

c	 =



yl

(1)

1≤l≤ j

Consider the case where el does not increase the maximum degree of the current graph, or l = 1, or el increases the
maximum degree of current graph and there is no possible
edge which does not increase the maximum degree of the
current graph, 1 ≤ l ≤ j. In this scenario, we let Xl be the set
of edges in P which are incident to ul or vl , 1 ≤ l ≤ j, and not
in Xl 	 , 1 ≤ l 	 ≤ l − 1. Thus, Xl will have zero, one or two edges
for 1 ≤ l ≤ j.
Now consider the case where el increases the maximum
degree of the current graph and there are some edges that
do not increase the maximum degree of the current graph,
2 ≤ l ≤ j. In this case, we deﬁne Xl to be the set of the ﬁrst
k maximum weight edges in P which are not in Xl 	 , 1 ≤ l 	 ≤
l − 1. If there are less than k elements in P which are not in
Xl 	 , 1 ≤ l 	 ≤ l − 1, Xl will only have these edges. If there are

312

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

edges which are incident to ul or vl and not in Xl 	 , 1 ≤ l	 ≤ l,
then replace any edges from Xl with those edges (note that
there may be at most two edges of this kind).
Let P0 = P and Pl = Pl−1 − Xl for 1 ≤ l ≤ j. Let Xl reduce the
total number of connected components of Gl−1 by xl for 1 ≤ l
≤ j. Let Pl reduce the total number of connected components
of Gl by cl for 0 ≤ l ≤ j.
If there is an edge el that increases the maximum degree of current graph and there is no possible edge which
does not increase maximum degree of the current graph, 2
≤ l ≤ j, then for each vertex of the graph, there is at least
one edge el 	 incident to this vertex, 1 ≤ l 	 ≤ l − 1. So, union
of sets Xl 	 , 1 ≤ l 	 ≤ l − 1, contains all the edges in P. Thus,
Pj = ∅. Now, consider the case when there is no edge which
satisﬁes these properties (so, when algorithm chooses an
edge el which increases the maximum degree, there is always an edge which does not increase maximum degree of
the current graph). Since Ri contains h sets of Si	 , there are
h − 1 = n/(2k) edges that increase the maximum degree of
the current graph. Thus there are at least n/(2k) of the sets Xl
	
with k edges each – we call them X1	 , . . . , Xn/
(2k) — (if Xl has

less than k edges than all edges of set P are already in one of
sets Xl 	 , 1 ≤ l 	 ≤ l − 1, and hence Pj = ∅). The union of sets Xl	
has at least (n/2k) · k = n/2 edges. On the other hand, since P
is a matching, this union can have at most n/2 edges. Hence,
Pj = ∅. Hence,

c0 = c, c j = 0

(2)

Consider the case where el does not increase the maximum degree of current graph or l = 1 or el increases the
maximum degree of current graph and there is no possible
edge which does not increase maximum degree of the current graph, 1 ≤ l ≤ j. If Xl has two edges, then our algorithm
did not choose one of these two edges at that step and choose
el instead, 0 ≤ l ≤ j. Since our algorithm greedily chooses the
edges, el reduces the total number of connected components
of Gl−1 by at least as much as each of the edges in Xl . Hence,
yl ≥ xl /2. Similarly, if Xl has one or zero edges, then yl ≥ xl .
Now consider the case where el increases the maximum
degree of current graph and there are some edges which does
not increase maximum degree of the current graph, 2 ≤ l ≤
j. Xl has at most k edges. Our algorithm did not choose one
of these k edges at that step and choose el instead, 0 ≤ l ≤
j. Since our algorithm greedily chooses the edges, el reduces
the total number of connected components of Gl−1 by at least
as much as k times any of the edges in Xl . Since Xl has at most
k edges, yl ≥ xl . So,

yl ≥


xl
1 
,1 ≤ l ≤ j ⇒
yl ≥
xl
2
2
1≤l≤ j

(3)

1≤l≤ j


Since Pl+1 = Pl − Xl+1 and Gl+1 = Gl el+1 , 0 ≤ l ≤ j − 1,
the amount that Pl reduces the total number of connected
components of Gl is smaller than sum of the amount that Pl+1
reduces the total number of connected components of Gl+1
and the amount that el+1 reduces the total number of connected components of Gl and the amount Xl+1 reduces the
total number of connected components of Gl . Hence,

cl+1 ≥ cl − (xl+1 + yl+1 ) for 0 ≤ l ≤ j − 1

(4)

If we add all the inequalities (2) and (4), we will have



xl +

1≤l≤ j



yl ≥ c

(5)

1≤l≤ j

From the inequalities (3) and (5), we will have

3



yl ≥ c

(6)

1≤l≤ j

From the inequalities (1) and (6), we will have
c	 ≥ c/3



Before Low-ODA starts adding the set Ri , we have ni components and we know that if we add all the (dopt + 1) matchings M j − RAi , 1 ≤ j ≤ (dopt + 1), to the current solution, the
total number of connected components will be reduced to
Cend . Therefore, there exists a matching M j − RAi which decreases the total number of connected components by at
least (ni − Cend )/(dopt + 1). Our algorithm always ﬁnds the
set Ri that reduces the total number of connected components of G(V, RAi ) by at least 1/3 of any optimal matching
which reduces the current number of connected components
by the maximum amount (Lemma 4). Hence, the set Ri that
our algorithm uses must decrease the total number of connected components at that time by at least (1/3) of the optimal amount. Therefore,
ni − ni+1 ≥ (ni − Cend )/(3(dopt + 1))
⇒ ni+1 − Cend ≤ (1 − 1/(3(dopt + 1)))(ni − Cend ).
Hence, the number of iterations for our algorithm LowODA is less than or equal to the largest m which satisﬁes
1 > (n1 − Cend )(1 − 1/(3(dopt + 1)))m
⇒ m ≤ 3(dopt + 1) ln (Cstart − Cend )
⇒ m ≤ 3(dopt + 1) ln (Cstart )
Since h = n/(2k) + 1, the maximum degree of resulting graph is less than or equal to (n/(2k) + 1) · 3(dopt + 1) ·
ln (Cstart ). 
5. Experimental results
The GM algorithm [13], the MinMax-ODA algorithm
[21,23] and the Low-ODA algorithm are implemented in Java.
These three algorithms are compared according to the maximum and average degrees in the resulting overlay graphs.
Our experimental results show that the Low-ODA algorithm
has better maximum degree than the GM algorithm and has
better average degree than the MinMax-ODA algorithm, at
the expense of a small degradation of the other quantity (recall that the GM algorithm has proven approximation bounds
on the average degree only and the MinMax-ODA on the
maximum degree only).
5.1. Maximum node degree
For these experiments, the number of nodes varies between 100 to 1000. The number of topics is 100. We ﬁxed
number of subscriptions to s = 10. For the Low-ODA algorithm, the parameter k is √
chosen to be equal to 3 (k is chosen as a number close to 8, since the Low-ODA algorithm
behaves pretty much like the MinMax-ODA algorithm when
k = 8 – see results in Section 5.3). Each node subscribes to

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

Fig. 1. Maximum node degree for GM, MinMax-ODA and Low-ODA.

Fig. 3. Maximum node degree for different parameters k.

Fig. 2. Average node degree for GM, MinMax-ODA and Low-ODA.

Fig. 4. Average node degree for different parameters k.

each topic ti with probability pi . The value of pi is distributed
according to a Zipf distribution (α = 0.5). This experimental
setting is similar to previous studies [13].
Fig. 1 presents a comparison of the GM, MinMax-ODA
and Low-ODA algorithms according to the maximum degree. When we compare the results of the algorithms, LowODA takes values in between the GM and the MinMax-ODA
algorithms.
5.2. Average node degree
The experimental setting is the same as in the previous
subsection. Fig. 2 is a comparison of the GM, MinMax-ODA
and Low-ODA algorithms according to the average degree:
Low-ODA also takes average degree values in between the
GM and the MinMax-ODA algorithms.
5.3. Different parameters
The experimental setting is similar to previous subsections. The number of nodes is 100. The parameter k of the
Low-ODA algorithm varies between 1 to 8. When k = 1,
the Low-ODA algorithm behaves basically in the same way as
the GM algorithm, and when k = 8, the Low-ODA algorithm
behaves basically in the same way as the MinMax-ODA algorithm. As k increases, the maximum degree decreases and
the average degree increases (Figs. 3 and 4).
6. Constant diameter overlays for publish-subscribe
In this section, we study the following optimization problem, intially proposed in [21,23]:
2-Diameter Topic-Connected Overlay (2D-TCO) Problem:
[21, 23] Given a collection of nodes V, a set of topics T, and
a node–interest assignment I, connect the nodes in V into a

313

topic-connected overlay network G which has diameter 2 and
has least possible average degree.
At the ﬁrst part of the paper, we consider both the average
degree and the maximum degree. In this section, we consider
both diameter and average degree. This is the ﬁrst step in this
direction. It is too complicated to consider both the average
and maximum node degree together with the diameter.
6.1. Previous algorithms
The ﬁrst result which has low average degree and diameter 2 is introduced in [21,23], where the algorithm Constant
Diameter Overlay Design Algorithm (CD-ODA) for building 2diameter overlay networks, is presented. CD-ODA follows the
greedy approach described below:
The Constant Diameter Overlay Design Algorithm (CD-ODA)
[21, 23]: CD-ODA starts with the overlay network G(V, ∅). At
each iteration of the CD-ODA, a node u with maximum number of neighbors (where a neighbor of u is a node v with nonempty interest intersection with u) is chosen. Let the number
of neighbors of u be equal to nu = |{v ∈ V |∃t ∈ T, Int (v, t ) =
Int (u, t ) = 1}|. After that, an edge between this node and
each of its neighbors is added and the topics in this node’s
interest assignment are removed from the set of topics.
In [21,23], two reﬁnements of CD-ODA, called Constant
Diameter Overlay Design Algorithm 1 (CD-ODA-I) and Constant Diameter Overlay Design Algorithm 2 (CD-ODA-II), are
presented. The CD-ODA-I and CD-ODA-II algorithms follow
the greedy approaches described below:
Constant Diameter Overlay Design Algorithm 1 (CD-ODA-I)
[21, 23]: CD-ODA-I starts with the overlay network G(V, ∅).
At each iteration of the CD-ODA-I, a node u which has maximum number of weighted neighbors is chosen. The number
of weighted neighbors of a node z is equal to

|{v ∈ V |Int (v, t ) = Int (z, t ) = 1}|.
wz =
t∈T

314

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

Algorithm 1 Low Degree Overlay Design Algorithm (LowODA).
1:
2:
3:
4:
5:
6:
7:
8:

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

OverlayEdges ← ∅
V ← Setof all nodes
G	 (V, E 	 ) ← Complete graph on V
for {u, v} ∈ E 	 do
w{u, v} ← Number of topics that both of nodesu and v
have
end for
while G(V,OverlayEdges) is not topic-connected do
Let e1 be a maximum weight edge in G	 (V, E 	 , w)
among theones which increase the maximum degree
of G(V,OverlayEdges)minimally.
Let e2 be a maximum weight edge in G	 (V, E 	 , w)
if w(e1 ) ≥ w(e2 )/k then
e = e1
else
e = e2
end if

OverlayEdges = OverlayEdges e
E	 ← E	 − e
for {u, v} ∈ E 	 do
w{u, v} ← NC(V , OverlayEdges) -NC(V ,OverlayEdges

{u, v} )
end for
end while

Algorithm 2 2-Diameter Overlay Design Algorithm(2DODA).
1:
2:
3:
4:
5:

6:
7:
8:

T ← Set of all topics
while T is not empty do
For each node u and topic t such that I(u, t ) = 1, calculate topic density. Denote this number by td(u, t ).
For each node u, ﬁnd topic tu which has maximum topic
density.
Examine all the nodes and maximum topic densities
and ﬁnd maximum of the maximum topic densities.Let
say it is for node v and topic tv .
Put an edge between v and all nodes w such that
I(v, tv ) = I(w, tv ) = 1.
Remove topic tv from T .
end while

We add an edge between u and each of its neighbors and
then remove the topics in this node’s interest assignment
from the set of topics.
Constant Diameter Overlay Design Algorithm 2 (CD-ODA-II)
[21, 23]: CD-ODA-II starts with the overlay network G(V, ∅).
At each iteration of the CD-ODA-II, a node u which has maximum connection density, du , is chosen. The connection density of a node u, du , is given by

t∈T |{v ∈ V |Int (v, t ) = Int (u, t ) = 1}|
.
du =
|{v ∈ V |∃t ∈ T, Int (v, t ) = Int (u, t ) = 1}|
u
Note that du = w
nu . We add an edge between a node u with
maximum density and each of its neighbors and then removing the topics in this node’s interest assignment from the set
of topics.

Fig. 5. Optimal solution for the example in Lemma 5.

Experimental results [22,23] show that CD-ODA-I and CDODA-II improve CD-ODA by a factor of 20%. CD-ODA-II is currently the best algorithm for the 2D-TCO problem.
6.2. Approximation ratios
The CD-ODA , CD-ODA-I and CD-ODA-II algorithms do not
work well: The approximation ratios on the average degree
obtained by the algorithms may be as large as (n), as we
show in the lemma below.
Lemma 5. The CD-ODA algorithm can only guarantee an approximation ratio of (n) on the average node degree, where n
is number of nodes in the pub/sub system.
Proof. Consider the example in Fig. 5 where we have n nodes
v1 , v2 , . . . , vn and n topics t1 , t2 , . . . , tn . Each node vi is interested in topics t1 , t2 , . . . , ti , 1 ≤ i ≤ n. The CD-ODA algorithm
can ﬁrst connect the node v1 with the nodes v2 , . . . , vn . Then,
it can connect the node v2 with the nodes v3 , . . . , vn , and so
on. Thus, the overlay network that CD-ODA constructs have
all the edges in the graph. The average degree of this overlay network is n − 1. The optimal overlay network with minimum number of edges is G(V, E	 ), where E 	 = {(vn , vi )|1 ≤
i < n}, which has average degree 2(n − 1)/n. Hence the approximation ratio of the CD-ODA algorithm can be as large as
(n − 1)/(2(n − 1)/n) = (n). 
Lemma 6. The CD-ODA-I and CD-ODA-II algorithms can only
guarantee an approximation ratio of (n) on the average node
degree, where n is number of nodes in the pub/sub system.
Proof. Consider the example in Fig. 6 where we have 2n
nodes v1 , v2 , . . . , vn and u1 , u2 , . . . , un . We have n topics
t1 , t2 , . . . , tn and we have n sets of topics S1 , S2 , . . . , Sn . Each
set Si have n3 (n − i + 1) different topics, 1 ≤ i ≤ n. Each
node vi is interested in topics t1 , t2 , . . . , ti and the topics in
the set Si , 1 ≤ i ≤ n. Each node ui is interested in topics in
the set Si , 1 ≤ i ≤ n. The CD-ODA-I and the CD-ODA-II algorithms ﬁrst connect the node v1 with the nodes u1 and
v2 , . . . , vn . Then, they connect the node v2 with the nodes u2
and v3 , . . . , vn , and so on. Thus, the overlay network that CDODA-I and CD-ODA-II construct have all the edges between
the nodes v1 , v2 , . . . , vn and have edges (ui , vi ), 1 ≤ i ≤ n.
The average degree of this overlay network is 2(n(n − 1)/2 +
n)/(2n) = (n + 1)/2. The optimal overlay network with minimum number of edges is G(V, E	 ), where E 	 = {(vn , vi )|1 ≤

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

315

Fig. 7. Optimal solution for Example VII.
Table 2
Number of edges in the overlay
networks constructed by the algorithms.
Case VII
CD-ODA [21,23]
CD-ODA-I [22,23]
CD-ODA-II [22,23]
2D-ODA(This Paper)
Optimum

Fig. 6. Optimal solution for the example in Lemma 6.


i < n} {(ui , vi )|1 ≤ i ≤ n}, which has average degree 2(2n −
1)/(2n) = (2n − 1)/n. Hence the approximation ratio of the
CD-ODA-I and CD-ODA-II algorithms can be as large as ((n +
1)/2)/((2n − 1)/n) = (n). 
6.3. 2-Diameter Overlay Design Algorithm (2D-ODA)
In this section, we present our overlay design algorithm (2D-ODA) for the 2D-TCO problem. 2D-ODA is a
greedy algorithm . Our algorithm generates a star for each
topic and hence each topic-connected component will have
diameter 2.
2D-ODA starts with the overlay network G(V, ∅) and topic
set T. At each iteration of 2D-ODA, ﬁrst for each node u a topic
tu ∈ T is chosen such that I(u, tu ) = 1 and for node u topic tu
has maximum topic density.
The topic density of a node u for a topic tu such that
I(u, tu ) = 1, td(u, tu ), for the overlay network G(V, E) is given
by

t∈T |{v ∈ V |I(v, t ) = I(u, t ) = 1}|
.
td(u, tu ) =
|{v ∈ V |I(v, tu ) = I(u, tu ) = 1 and (u, v) ∈/ E }|
Secondly each node is examined and maximum of the
maximum topic density is found. Let v and tv be the respective node and topic. A star for topic tv with center node v is
added to the network and topic tv is removed from the topic
set T.
While at a ﬁrst glance 2D-ODA may look similar to CDODA-II, it does not have to connect all the topics of the chosen node which improves the approximation ratio. This new
algorithm connects the topics one by one. This way, we do
not have to add edges with low density to the overlay network because of other edges incident to the node with high
density.
Lemma 7. 2D-ODA generates a 2-diameter overlay for each
topic.
Proof. Since the algorithm generates a star for each topic,
each topic overlay network will have diameter 2. 

2n-5
2n-5
2n-5
n
n

6.4. Cases
In [21–23] to give the intuitions of the presented algorithms, six special cases are presented (which we call cases
I-VI). None of the previous algorithms can ﬁnd an optimum
solution for all these cases. 2D-ODA ﬁnds optimum solutions
to all these cases and also to Case VII illustrated in Fig. 7. Note
that none of the previous algorithms ﬁnds the optimum solution to Case VII (See Table 2).
Case VII: In Fig. 7, we have n nodes v1 , v2 , w, u1 , u2 ,
… , un−3 . Node v1 is subscribed to the topics x1 , x2 , . . . , xn , z.
Node v2 is subscribed to the topics x1 , x2 , . . . , xn . Nodes u1 ,
u2 , … , un−3 are subscribed to topics x1 and y. Node w is subscribed to topic z. In the optimal solution, there are edges between node u1 and nodes v1 , v2 , u2 , u3 , … , un−3 , there is
an edge between nodes v1 and v2 , and there is an edge between nodes v1 and w. So, we have n edges in the optimal
solution. For this example, CD-ODA and CD-ODA-I will put
edges between the node v1 and all the other nodes. And they
put edges between node u1 and nodes u2 , u3 , … , un−3 . So, CDODA and CD-ODA-I use 2n − 5 edges. For this example, CDODA-II will put edges between the node v2 and nodes v1 , u1 ,
u2 , … , un−3 , between node u1 and nodes u2 , u3 , … , un−3 , and
between node v1 and node w. Thus, CD-ODA-II use 2n − 5
edges. 2D-ODA ﬁnds the optimal solution for this example.
This example shows CD-ODA, CD-ODA-I and CD-ODA-II may
work worse than 2D-ODA.
6.5. Experimental results
The GM algorithm [12], the CD-ODA algorithm [21,23], the
CD-ODA-I algorithm [23], the CD-ODA-II algorithm [23] and
the 2D-ODA algorithm are implemented in Java. These algorithms are compared according to the average degree in
the resulting graph. The diameter is always 2 for the CDODA, CD-ODA-I, CD-ODA-II and 2D-ODA algorithms and it
may be (n), where n is number of nodes, for the GM algorithm. When we compare the results the GM, CD-ODA, CD-

316

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

Fig. 8. Average node degree for GM, CD-ODA, CD-ODA-I, CD-ODA-II and 2DODA.

Fig. 10. Average node degree for GM, CD-ODA, CD-ODA-I, CD-ODA-II and
2D-ODA.

Fig. 9 gives the comparison results for the algorithms for
the average degree metric. The average degree of the graph
increases for the algorithms as the number of topics increases, since the edges will have lower correlations. When
we compare the results of the algorithms, 2D-ODA requires
at most 1.07 times more edges than the GM (Fig. 9). 2D-ODA
improves CD-ODA-II by a factor of 1.96 on average and CDODA and CD-ODA-I by a factor of 2 on average.

Fig. 9. Average node degree for GM, CD-ODA, CD-ODA-I, CD-ODA-II and 2DODA.

ODA-I, CD-ODA-II and 2D-ODA algorithms according to the
average degree, our algorithm 2D-ODA requires at most 1.1
times more edges than the GM algorithm, while improving
the diameter drastically. 2D-ODA improves the average degree of CD-ODA-II by a factor 2 on average and the average
degrees of CD-ODA-I and CD-ODA by a factor 2.1 on average.
6.5.1. Average node degree with varying number of nodes
For the ﬁrst experiment, the number of nodes varies between 100 to 1000. We ﬁxed the number of topics at 100, and
the number of subscriptions at s = 10. Each node is interested
in each topic uniformly at random. This experimental setting
is similar to previous studies [12,21–24].
Fig. 8 gives the comparison results for the algorithms according to the average degree. The average degree of the
graph decreases for the algorithms when the number of
nodes increases, since there are more edges with higher correlation. When we compare the results of GM and 2D-ODA,
our algorithm requires at most 1.09 times more edges than
GM (Fig. 8). 2D-ODA improves CD-ODA-II by a factor of 2.04
on average and CD-ODA and CD-ODA-I by a factor of 2.06 on
average.
6.5.2. Average node degree with varying number of topics
For the second experiment, the number of nodes is 100,
the number of topics varies between 100 and 500, and the
number of subscriptions is ﬁxed at s = 20. Each node is interested in each topic uniformly at random. This experimental
setting is also similar to previous studies [12,21–24].

6.5.3. Average node degree with varying subscription size
For the third experiment, the number of nodes and the
number of topics are ﬁxed at 100. The subscription size varies
between 10 and 50. Each node is interested in each topic uniformly at random. (The experimental setting is also similar to
previous studies [12,21–24]).
Fig. 10 gives the comparison results for the algorithms for
the average degree metric. The average degree of the overlay
network decreases for all the algorithms when the subscription size increases, since all algorithms can ﬁnd edges with
higher correlation. When we compare the results of GM and
2D-ODA, 2D-ODA requires at most 1.1 times more edges than
the GM (Fig. 10). 2D-ODA improves CD-ODA-II by a factor of
1.94 on average and CD-ODA and CD-ODA-I by a factor of 2.23
on average.
7. Conclusions
In this paper, we study a new optimization problem (LowTCO) that constructs a practical and scalable overlay network for publish/subscribe communication with many topics. We present a topic-connected overlay network design
algorithm (Low-ODA) which approximates both average degree and maximum degree well. We anticipate that the parameterized algorithmic framework proposed by Low-ODA
will be applicable in other network design domains where,
for scalability, it is important to keep both the maximum degree as well as the average degree of an overlay network low.
Examples of such application domains are in the design of
survivable networks [18] and in wireless networks [15].
As future work, we would like to build upon our 2DODA algorithm, by formally and experimentally evaluating
the hardness of obtaining a topic-connected overlay design
algorithm which achieves a “good” trade-off between low
diameter and low node degree. This basically amounts to a
bicriteria optimization problem and we have to be able to
“quantify” the relative importance of optimizing over these

M. Onus, A.W. Richa / Computer Networks 94 (2016) 307–317

two parameters (e.g., in the 2D-ODA algorithm we restrict
our attention to networks of diameter 2, while aiming at
maintaining the average degree low). Another open problem is overlay networks with logarithmic diameters as in
distributed systems such as DHT and small-world networks.
Two other important lines for future work would be to design
eﬃcient distributed algorithms for the Low-TCO problem, and
to look at this problem under the line of a dynamic conﬁguration of the node set V and the interest assignment I.
References
[1] Oracle9i Application Developers Guide Advanced Queuing, Oracle, Redwood Shores, CA.
[2] E. Anceaume, M. Gradinariu, A.K. Datta, G. Simon, A. Virgillito, A semantic overlay for self- peer-to-peer publish/subscribe, in: ICDCS, 2006.
[3] S. Baehni, P.T. Eugster, E. Guerraoui, Data-aware multicast, in: DSN,
2004.
[4] R. Baldoni, R. Beraldi, V. Quema, L. Querzoni, S.T. Piergiovanni, TERA:
topic-based event routing for peer-to-peer architectures, in: Proceedings of 1st International Conference on Distributed Event-Based Systems (DEBS), 6, ACM, 2007.
[5] R. Baldoni, R. Beraldi, L. Querzoni, A. Virgillito, Eﬃcient publish/subscribe through a self-organizing broker overlay and its application to SIENA, Comput. J. (2007).
[6] S. Banerjee, B. Bhattacharjee, C. Kommareddy, Scalable application
layer multicast, SIGCOMM Comput. Commun. Rev 32 (4) (2002) 205–
217.
[7] S. Bhola, R. Strom, S. Bagchi, Y. Zhao, J. Auerbach, Exactly-once delivery
in a content-based publish-subscribe system, in: DSN, 2002.
[8] A. Carzaniga, M.J. Rutherford, A.L. Wolf, A routing scheme for contentbased networking, in: Proceedings of IEEE INFOCOM, Hon Kong, China
2004, 2004.
[9] M. Castro, P. Druschel, A.M. Kermarrec, A. Rowstron, SCRIBE: a largescale and decentralized application-level multicast infrastructure, IEEE
J. Sel. Areas Comm. (JSAC) 20 (8) (2002) 1489–1499.
[10] R. Chand, P. Felber, Semantic peer-to-peer overlays for publish/subscribe networks, in: Euro-Par 2005 Parallel Processing, Lecture
Notes in Computer Science, 3648, Springer Verlag, 2005, pp. 1194–
1204.
[11] C. Chen, R. Vitenberg, H.A. Jacobsen, A generalized algorithm for publish/subscribe overlay design and its fast implementation, in: DISC,
2012, pp. 76–90.
[12] G. Chockler, R. Melamed, Y. Tock, R. Vitenberg, Constructing scalable
overlays for pub-sub with many topics, in: Proceedings of the 26th ACM
Symp. on Principles of Distributed Computing (PODC), 2007, pp. 109–
118.
[13] G. Chockler, R. Melamed, Y. Tock, R. Vitenberg, SpiderCast: a scalable
interest-aware overlay for topic-based pub/sub communication, in:
Proceedings of the 1st International Conference on Distributed EventBased Systems (DEBS), 6, ACM, 2007.
[14] B.F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon,
H.A. Jacobsen, N. Puz, D. Weaver, R. Yerneni, PNUTS: Yahoo!’s hosted
data serving platform, PVLDB 1 (2) (2008) 1277–1288.
[15] E. De Santis, F. Grandoni, A. Panconesi, Fast low degree connectivity
of ad-hoc networks via percolation, in: Proceedings of the European
Symposium on Algorithms (ESA), 2007, pp. 206–217.
[16] P.T. Eugster, P.A. Felber, R. Guerraoui, A.M. Kermarrec, The many faces
of publish/subscribe, ACM Comput. Surv. 35 (2) (2003) 114–131.
[17] R. Guerraoui, S. Handurukande, A.M. Kermarrec, Gossip: a gossip-based
structured overlay network for eﬃcient content-based ﬁltering, EPFL,
Lausanne, 2004 Technical report ic/2004/95.
[18] L.C. Lau, J. Naor, M.R. Salavatipour, M. Singh, Survivable network design
with degree or order constraints, in: Proceedings of ACM STOC’07.
[19] R. Levis, Advanced massaging applications with MSMQ and MQSeries,
QUE, 1999.
[20] H. Liu, V. Ramasubramanian, E.G. Sirer, Client behavior and feed characteristics of RSS, a publish-subscribe system for web micronews, in:
Proceedings of Internet Measurement Conference (IMC), Berkeley, California, 2005.

317

[21] M. Onus, A.W. Richa, Minimum maximum degree publish-subscribe
overlay network design, in: Proceedings of 28th Annual IEEE Conference on Computer Communications (INFOCOM), Rio de Janeiro, Brazil,
2009.
[22] M. Onus, A.W. Richa, Parameterized maximum and average degree approximation in topic-based publish-subscribe overlay network design,
in: Proceedings of 30th IEEE International Conference on Distributed
Computing Systems (ICDCS), Genoa, Italy, 2010.
[23] M. Onus, A.W. Richa, Minimum maximum degree publish-subscribe
overlay network design, IEEE/ACM Trans. Netw. 5 (19) (2011) 1331–
1343.
[24] V. Ramasubramanian, R. Peterson, E.G. Sirer, Corona: a high performance publish-subscribe system for the world wide web, in: NDSI,
2006.
[25] D. Sandler, A. Mislove, A. Post, P. Druschel, FeedTree: sharing web micronews with peer-to-peer event notiﬁcation, in: Proceedings of International Workshop on Peer-to-Peer Systems(IPTPS), 2005.
[26] V. Setty, G. Kreitz, R. Vitenberg, M. Steen, G. Urdaneta, S. Gimaker, The
hidden pub/sub of Spotify: (industry article), in: DEBS, 2013, pp. 231–
240.
[27] D. Tam, R. Azimi, H.-A. Jacobsen, Building content-based publish/subscribe systems with distributed hash tables, in: Proceedings
of the 1st International Workshop on Databases, Information Systems,
and P2P Computing(DBISP2P), Berlin, Germany, 2003.
[28] Y. Tock, N. Naaman, A. Harpaz, G. Gershindky, Hierarchical clustering
of message ﬂows in a multicast data dissemination system, in: Proceedings of the 17th IASTED International Conferance Parallel and Distributed Computing and Systems, 2005, pp. 320–327.
[29] S. Voulgaris, E. Riviere, A.M. Kermarrec, M. van Steen, Sub-2-sub: selforganizing content-based publish subscribe for dynamic large scale
collaborative networks, in: IPTPS, 2006.

Melih Onus received the BS degree in computer
engineering from Bilkent University, Turkey in
2003. He received Ph.D. degree in computer science from Arizona State University (ASU), Tempe,
in 2009. He is currently an instructor at TOBB University of Economics and Technology, Turkey. His
research interests are in the areas of distributed
computing, computer networks and algorithms.

Andrea W. Richa is an Associate Professor at
the Department of Computer Science and Engineering at Arizona State University, Tempe, since
August 2004. She joined this department as an
Assistant Professor in August 1998. Prof. Richa received her M.S. and Ph.D. degrees from the School
of Computer Science at Carnegie Mellon University, in 1995 and 1998, respectively. She also
earned an M.S. degree in Computer Systems from
the Graduate School in Engineering (COPPE), and
a B.S. degree in Computer Science, both at the
Federal University of Rio de Janeiro, Brazil, in
1992 and 1990, respectively. Prof. Richa’s main
area of research is in network algorithms. Some of the topics Dr. Richa has
worked on include packet scheduling, distributed load balancing, packet
routing, mobile network clustering and routing protocols, and distributed
data tracking. Prof. Richa’s data tracking (or name lookup) algorithm has
been widely recognized as the ﬁrst benchmark algorithm for the development of distributed databases in peer-topeer networking, having being references by over 130 academic journal or conference publications to date,
and being implemented as part of two of the current leading projects in
peer-to-peer networking. Dr. Richa’s was the recipient of an NSF CAREER
Award in 1999. For a selected list of her publications, CV, and current research projects, please visit http://www.public.asu.edu/aricha.

760

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

An Efficient and Fair MAC Protocol Robust to
Reactive Interference
Andréa Richa, Christian Scheideler, Stefan Schmid, and Jin Zhang

Abstract—Interference constitutes a major challenge to availability for communication networks operating over a shared
medium. This paper proposes the medium access (MAC) protocol
ANTIJAM, which achieves a high and fair throughput even in
harsh environments. Our protocol mitigates internal interference,
requiring no knowledge about the number of participants in the
network. It is also robust to intentional and unintentional external
interference, e.g., due to coexisting networks or jammers. We
model external interference using a powerful reactive adversary
that can jam a
-portion of the time-steps, where
is an arbitrary constant. The adversary uses carrier sensing to
make informed decisions on when it is most harmful to disrupt
communications. Moreover, we allow the adversary to be adaptive
and to have complete knowledge of the entire protocol history.
ANTIJAM makes efficient use of the nonjammed time periods and
achieves, if is constant, a
-competitive throughput. In addition, ANTIJAM features a low convergence time and has excellent
fairness properties, such that channel access probabilities do not
differ among nodes by more than a small constant factor.
Index Terms—Access protocols, electromagnetic interference,
interference, protocols, radio frequency interference, wireless
communication, wireless networks.

I. INTRODUCTION

D

ISRUPTIONS of communications over a shared medium
due to interference—intentional, or not—are a central
challenge to wireless computing. It is well known that simple
jamming attacks—requiring no special hardware—constitute
a threat for the widely used IEEE 802.11 MAC protocol. Due
to the problem’s relevance, there has been a significant effort
to cope with such disruption problems, both from industry and
academia. Accordingly, much progress has been made over the
last years on how to deal with different jammer scenarios.
This paper presents a very robust medium access (MAC)
protocol, ANTIJAM, that makes effective use of the few and
arbitrarily distributed time periods when the wireless medium
Manuscript received November 11, 2011; revised June 12, 2012 and July
01, 2012; accepted July 08, 2012; approved by IEEE/ACM TRANSACTIONS ON
NETWORKING Editor S. Kasera. Date of publication August 13, 2012; date of
current version June 12, 2013. This work was supported in part by the NSF
under Awards CCF-0830791 and CCF-0830704 and the DFG under Projects
SCHE 1592/2-1 and SFB 901.
A. Richa and J. Zhang are with the Department of Computer Science and Engineering, SCIDSE, Arizona State University, Tempe, AZ 85282 USA (e-mail:
aricha@asu.edu; jzhang82@asu.edu).
C. Scheideler is with the Department of Computer Science, University of
Paderborn, 33102 Paderborn, Germany (e-mail: scheideler@upb.de).
S. Schmid is with the Telekom Innovation Laboratories (T-Labs) and TU
Berlin, 10587 Berlin, Germany (e-mail: stefan@net.t-labs.tu-berlin.de).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TNET.2012.2210241

is available. We model the external interference (due to colocated networks, jamming, etc.) as an adversary, also called the
jammer.1
In contrast to related protocols that are robust to simple
oblivious adversaries, this paper studies MAC protocols
against “smart” adversaries, i.e., robust to more complex forms
of interference. In particular, our adversary may behave in an
adaptive and reactive manner: adaptive in the sense that the
decisions on whether to jam at a certain moment in time can
depend on the protocol history, and reactive in the sense that
the adversary can perform physical carrier sensing (which is
also part of the 802.11 standard) to learn whether the channel is
currently idle or not, and jam the medium depending on these
measurements.
Note that the study of reactive jamming is relevant beyond
purely adversarial contexts. Interactions between spatially colocated networks may appear as a reactive “jamming” when interference happens during “idle” time periods: The nodes in colocated networks are likely to transmit if there are no ongoing
transmissions in the other networks. Furthermore, transmissions
from colocated networks during idle time periods constitute a
nontrivial challenge to many MAC protocols whose operation
and states depend on the idle time periods.
A. Related Work
Many MAC layer strategies have been devised to resolve unintentional and malicious interference in the literature, including
coding strategies (e.g., [5]), channel surfing and spatial retreat
(e.g., [1], [21]), or mechanisms to hide messages from a jammer,
evade its search, and reduce the impact of corrupted messages
(e.g., [20]). These methods, however, do not help against an
adaptive jammer with full information about the history of the
protocol, which we consider in our work. An interesting related
approach is used in IdleSense [12], a variation of the 802.11 Distributed Coordination Function: In IdleSense, all nodes use similar values of the contention window to achieve good short-term
access fairness. The synchronization is achieved via monitoring
the number of idle slots between transmission attempts to dynamically control the contention. Although simulations indicate
a high throughput, no formal bounds are derived.
In the theory community, work on MAC protocols has mostly
focused on efficiency. Many of these protocols are random
backoff protocols (e.g., [4], [6], [7], [11], [16]) that do not take
jamming activity into account and are not robust against it
1Note, however, that our notion of adversary is intended (and limited to) describe arbitrary and worst-case patterns of external interference. In particular,
our adversary does not try to capture any kind of malicious or Byzantine behavior and cannot perform, e.g., insider attacks with additional information. The
study of such malicious adversaries is complementary work, beyond the scope
of this paper.

1063-6692/$31.00 © 2012 IEEE

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

(see [2] for more details). Some theoretical work on jamming
is known (e.g., [8] for a short overview), and there are two
basic approaches in the literature. The first assumes randomly
corrupted messages (e.g., [15]), which is much easier to handle
than adaptive adversarial jamming [3]. The second line of work
either bounds the number of messages that the adversary can
transmit or disrupt with a limited energy budget (e.g., [10]
and [13]), or bounds the number of channels the adversary can
jam (e.g., [9] and [14]). The protocols in, e.g., [13] can tackle
adversarial jamming at both the MAC and network layers,
where the adversary may not only jam the channel, but also
introduce malicious (fake) messages (possibly with address
spoofing). However, these solutions depend on the fact that the
adversarial jamming budget is finite, so it is not clear whether
the protocols would work under heavy continuous jamming.
(The result in [10, Theorem 1] upper-bounds the adversary’s
capability of disrupting communications with a budget of
messages, and then shows that the proposed protocol needs
at least
rounds to terminate, which implies a jamming rate
below 0.5. The handshaking mechanism in [13] requires an
even lower jamming rate because the faulty nodes can cause
collisions and rounds of address spoofing, while the number
of rounds needed to have a message successfully broadcast is
at least
.)
Our work is motivated by the results in [2] and [3]. In [3],
it is shown that an adaptive jammer can dramatically reduce
the throughput of the standard MAC protocol used in IEEE
802.11 with only limited energy cost on the adversary side.
Awerbuch et al. [2] initiated the study of throughput-competitive MAC protocols under continuously running, adaptive jammers and presented a protocol that achieves a high performance
under adaptive jamming.
In this paper, we extend the model and result from [2] in two
crucial ways.
1) We allow the jammer to be reactive, i.e., to briefly listen
to the current channel state in order to make smarter jamming decisions at the current time-step. Note that a reactive model is not only meaningful in the context of jamming: For example, in many MAC protocols based on carrier sensing, nodes become active during idle time periods,
and hence a MAC protocol in the reactive model also performs well in scenarios with colocated networks.
2) We design a fair protocol in the sense that channel access
probabilities among nodes do not differ by more than a
small constant factor. The protocol in [2] is inherently unfair, as confirmed by our theoretical and simulation results.
We believe that the reactive jammer model is much more
realistic and, hence, that our study is of practical importance. For example, by sensing the channel, the adversary
may avoid wasting energy by not jamming idle rounds.
Note, however, that depending on the protocol, it may still
make sense for the adversary to jam idle rounds, e.g., to
influence the protocol execution.
The problem becomes significantly more challenging than
the nonreactive version due to the large number of possible
strategies a jammer can pursue. First, the analysis is more involved as the nodes’ aggregate sending probability varies in a
larger range depending on the adversarial strategy. Technically,
the reactive jamming renders it impossible to apply Chernoff

761

bounds over the nonjammed time periods as their patterns are
no longer random; rather, we have to argue over all time periods. Second, modifications to the protocol in [2] are needed.
For instance, the ANTIJAM protocol seeks to synchronize the
nodes’ sending probabilities; this has the desirable side effect
of achieving fairness: All nodes are basically granted the same
channel access probabilities, which greatly improves the unfair
protocol of [2]. While our formal analysis confirms our expectations that the overall throughput under reactive jammers is lower
than the throughput obtainable against nonreactive jammers, we
are still able to prove a constant-competitive performance (for
constant ), which is also confirmed by our simulation study. Finally, our first insights indicate that ANTIJAM-like strategies can
also be used in multihop settings (see also the recent extension
of [2] to unit disk graphs [17]) and to devise robust applications
such as leader election protocols [18].
B. Model
We study a wireless network that consists of cooperative
and reliable simple wireless devices (e.g., sensor nodes) that are
within the transmission range of each other and that communicate over a single frequency (or a limited, narrow frequency
band).
In an adversarial context, this model can be motivated, e.g.,
in sensor networks. Sensor networks consist of simple wireless
nodes usually running on a single frequency and which cannot
benefit from more advanced anti-jamming techniques such as
spread spectrum. In such scenarios, a jammer will also most
probably run on power-constrained devices (e.g., solar-powered
batteries) and hence will not have enough power to continuously
jam over time.
We assume a back-logged scenario where the nodes continuously contend for sending a packet on the wireless channel.
A node may either transmit a message or sense the channel at
a time-step, but it cannot do both (half-duplex), and there is no
immediate feedback mechanism telling a node whether its transmission was successful. A node sensing the channel may either:
1) sense an idle channel (in case no other node is transmitting at
that time); 2) sense a busy channel (in case two or more nodes
transmit at the time-step); or 3) receive a packet (in case exactly
one node transmits at the time-step).
In addition to these nodes, there is arbitrary external interference, which we model as an adversary. (Note that our notion of
adversary is mainly meant as a model to describe external interference only. The adversary does not, e.g., read and modify
packet contents.)
We allow the adversary to know the protocol and its entire history (in terms of idle, busy, and successful transmission events)
and to use this knowledge in order to jam the wireless channel
at will at any time (i.e., the adversary is adaptive). Whenever it
jams the channel, all nodes will notice a busy channel. However,
the nodes cannot distinguish between the adversarial jamming
or a collision of two or more messages that are sent at the same
time.
Moreover, we allow the jammer to be reactive: It is allowed
to make a jamming decision based on the actions of the nodes
at the current step. In other words, reactive jammers can determine (through physical carrier sensing) whether the channel is
currently idle or nonidle (the channel is nonidle either because

762

of a successful transmission or because the channel is busy)
and can instantly make a jamming decision based on that information. Those jammers arise in scenarios where, for example,
encryption is used for communication and where the jammer
cannot distinguish between an encrypted package and noise in
the channel. Note that robustness in the reactive model is relevant beyond jamming, e.g., in situations with colocated networks, as many MAC protocols based on carrier sensing activate nodes during idle time periods.
We assume that the adversary is only allowed to jam a
-fraction of the time-steps, for an arbitrary constant
. In addition, we allow the adversary to perform bursty jamming. Formally, an adversary is called
-bounded for
some
and
if, for any time window of size
, the adversary can jam at most
of the time-steps
in that window.
This paper studies competitive MAC protocols.
Definition I.1 ( -Competitive): A MAC protocol is called
-competitive against some
-bounded adversary (with
high probability or on expectation) if, for any sufficiently large
number of time-steps, the nodes manage to perform successful
message transmissions in at least a -fraction of the time-steps
not jammed by the adversary.
In other words, in a -competitive protocol, there is a successful transmission in the network every th nonjammed round
on average. Note that a 1-competitive protocol would be optimal: A successful transmission occurs in every nonjammed
round.
Our goal is to design a symmetric local-control MAC protocol (i.e., there is no central authority controlling the nodes,
and the nodes have symmetric roles at any point in time) that is
-bounded reacfair and
-competitive against any
tive adversary. The nodes do not know , but we do allow them
to have a very rough upper bound of the number and . More
specifically, we will assume that the nodes have a common parameter
. As
and
are small for all reasonable values of and , this is scalable
and not a critical constraint, as it leaves room for a super-polynomial change in and a polynomial change in over time.2
Thus, all we need for our formal performance result to hold is a
very rough upper bound on . As we will see in our theorems,
there is a tradeoff between too low values (which causes the
protocol to react too slowly to changes) and too high values
(with which the aggregate probability may overshoot). In practice, we expect that choosing a constant, sufficiently small
yields a good performance for any practical network. Indeed, in
our simulations
results in a good throughput for a wide
range of networks.
C. Our Contributions
This paper presents a very simple and robust medium access protocol called ANTIJAM, together with a rigorous performance analysis. We are not aware of any other protocol with
2On the other hand, note that the assumption that the nodes know constant
factor approximations of or directly renders the problem simple: If the set
.
of nodes is static, nodes can simply access the medium with probability
This yields a high and fair throughput: If is known, a time period of length
without idle and successful periods implies that the aggregate probability is
too high. This information can be exploited by the algorithm. However, such
assumptions are unrealistic and do not scale.

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

provable competitive throughput guarantees in a similarly harsh
environment.
Concretely, ANTIJAM is provably robust against adaptive
and reactive external interference or, equivalently, an adversary
(an outsider) that can jam the medium a constant fraction of
the time. Despite this harsh environment, we can show that
the ANTIJAM MAC protocol achieves a high throughput by
exploiting any nonjammed time intervals effectively. The main
theoretical contribution is a formal and rigorous derivation of
the good throughput and fairness guarantees of our protocol.
We show that ANTIJAM is competitive in the sense that a
constant fraction of the nonjammed execution time is used for
successful transmissions, i.e., ANTIJAM is able to benefit from
the rare and hard-to-predict time intervals where the shared
medium is available.
Theorem I.2: Let
. The ANTIJAM
protocol is constant-competitive, namely
-competitive w.h.p.3 under any
-bounded reactive adversary if the protocol is executed for at least
many time-steps,
where
is a constant,
,
and where is a sufficiently large constant. Moreover, ANTIJAM
achieves a high fairness: The channel access probabilities
among nodes do not differ by more than a factor of
after the first message was sent successfully.
Our theoretical results are complemented by extensive
simulations.

II. ANTIJAM MAC PROTOCOL
The basic idea of the ANTIJAM MAC protocol is simple. In
an ANTIJAM network, each node maintains a medium access value
that describes the probability that transmits
a message in a communication round. The nodes adapt and
synchronize their
values over time (which as a side effect
also improves fairness) in a multiplicative-increase multiplicative-decrease manner in order to ensure maximum throughput.
The
values tend to be lowered in times of high interference
and increased during times where the channel is idling. More
precisely, the sending probabilities are changed by a factor of
. However, we impose an upper bound of on , for
some constant
.4 As we will see, unlike in most
classic backoff protocols, our adaption rules for
ensure that
the adversary cannot influence the values much by jamming.
In addition, each node maintains two variables: a threshold
variable
and a counter variable . The threshold variable
is used to estimate the adversary’s time window : A good estimation of can help the nodes recover from a situation where
they experience high interference in the network. In times of
high interference,
will be increased, and the sending probability
will be decreased.
Initially, every node may set
,
and
.
However, as we will see, ANTIJAM converges quickly and hence
3With

high probability, or w.h.p. for short, denotes a probability of a least
for some constant .

4While our formal result is valid for any choice of constant ,
should not
be chosen too low in small networks. See also our discussion in the simulation
section.

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

763

TABLE I
IMPORTANT VARIABLES

works for arbitrary initial variable values. Afterwards, the protocol works in synchronized time-steps. We assume synchronized time-steps for the analysis, but a nonsynchronized execution of the protocol would also work as long as all nodes operate
at roughly the same speed.
ANTIJAM is based on the following ideas and concepts. Suppose that each node decides to send a message at the current
time-step with probability
with
. Let
,
be the probability that the channel is idle, and be the probability that exactly one node is sending a message. The following
claim originally appeared in [2]. It states that if
,
then the aggregate sending probability , i.e., the sum of all
the nodes’ individual sending probabilities
(which can be
larger than one), is constant. This in turn implies that at any
nonjammed time-step, we have constant probability of having a
successful transmission. Hence, our protocol aims at adjusting
the sending probabilities of the nodes such that
,
in spite of the reactive adversarial jamming activity. This will
be achieved by using a multiplicative increase/decrease game
for the probabilities and by synchronizing all the nodes, both
in terms of sending probabilities and their own estimates on
the time window threshold estimate ’s, at every successful
transmission.
Claim II.1:
.
With these definitions and insights, we can now formally describe the ANTIJAM protocol. See Algorithm 1.
A summary of all our variables (including the ones from the
analysis) is provided in Table I.
III. ANALYSIS
Our analysis of Theorem I.2 unfolds in a number of lemmas.
We show that given a certain sufficiently large initial aggregate
probability in a subframe, the aggregate probability cannot be
very small at the end of the subframe (Lemma III.6). We proceed
to show that ANTIJAM performs well in time periods in which
is upper-bounded by
for some constant (Lemma III.10).

Finally, we show that for any jamming strategy, ANTIJAM has
an aggregate probability of
for most of the time
(Lemma III.13).
The analysis makes repeated use of the following well-known
relation and the Chernoff bounds derived from [19].
it holds that
Lemma III.1: For all

Lemma III.2 (Chernoff Bounds [19]): Consider any set of
. Suppose that there are
binary random variables
values
with
for
. Then, it holds for
every set
and
and any
that

If, on the other hand, it holds that
every set
, then it holds for any

for
that

Let
be the set of all nodes. Let
be node ’s acat the beginning of the th time-step.
cess probability
Furthermore, let
be the aggregate probability at the
beginning of time , i.e.,
. Let
be
subframes
of size
a time frame consisting of
, where , , and are suffidenote the size
ciently large constants. Let
of .
We start with some simple facts that also provide some intuition for ANTIJAM. Fact III.3 states that the protocol synchronizes the sending probabilities of the nodes (up to a factor of
) as well as the values and .
Fact III.3: Right after a successful transmission of the triple
,
for all receiving
nodes and
for the sending node .

764

In particular, for any time-step after a successful transmission
by node ,
for all nodes ,
.
Fact III.3 also implies the following corollary.
Corollary III.4: After the first successful transmission, the
of the nodes
will never differ by
access probabilities
in the future.
more than a factor
The following facts study how the aggregate sending probability varies over time depending on the different events.
Fact III.5: For any time-step after a successful transmission, or a well-initialized state of the protocol (in which
for all nodes ), the following holds.
1) If the channel is idle at time , then:
a) if
for all , then
;
b) if
and
for all nodes
,
(because all nodes
then
except for increase their sending probability by a
factor
from
);
for all nodes , then
.
c) if
2) If there is a successful transmission at time , and if
or there was an idle time-step in the previous rounds,
then:
a) if the sender is the same as the last successful sender,
(because for the sender ,
then
, and the other nodes remain at
);
b) if the sender is different from the last successful
for all nodes (including and
sender and
), then
(all nodes
except reduce their sending probability);
c) if the sender is different from the last successful
for at least one node (including
sender and
and ), then
(because at time
; subse, for all nodes
:
and
quently,
for all nodes
:
).
3) If the channel is busy at time , then
when
.
ignoring the case that
and there has not been an idle timeWhenever
steps, then
is, in addition to
step during the past
the actions specified in the two cases above, reduced by a
.
factor of
We can now prove the following crucial lemma lowerbounding the aggregate sending probability.
Lemma III.6: For any subframe in which initially
, the last time-step of again satisfies
, w.h.p.
Proof: We start with the following claim about the maximum number of times the nodes decrease their probabilities in
due to
.
Claim III.7: If in subframe the number of idle time-steps is
at most , then every node increases
by 2 at most
many times.
Proof: Only idle time-steps reduce . If there is no idle
time-step during the last
many steps,
is increased by 2.
Suppose that
. Then, the number of times a node
increases
by 2 is upper-bounded by the largest possible
so that
, where
is the initial size of .

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

,
, so the claim is true for
.
For any
At best, each additional idle time-step allows us to reduce all
thresholds for by 1, so we are searching for the maximum so
. This is upper-bounded
that
, which proves our claim.
by
This allows us to prove that exceeds a certain minimal
threshold in a subframe.
Claim III.8: Suppose that for the first time-step in ,
. Then, there is a time-step in with
, w.h.p.
Proof: Suppose that there are nonjammed time-steps in
. Let be the number of these steps with an idle channel, and
be the number of these steps with a successful message transbe the maximum number of times
mission. Furthermore, let
a node increases
by 2 in . If all time-steps in satisfy
, then it must hold that
.
in
This is because no has reached a point with
this case, so Fact III.5 implies that for each time-step with an
. Thus, at most
idle channel,
time-steps with an idle channel would be needed to get
to
, and then there would have to be a balance between further
increases (that are guaranteed to be caused by an idle channel)
and decreases (that might be caused by a successful transmis) of in order to avoid the case
sion or the case
. The number of times we can allow an idle channel is maximized if all successful transmissions and cases where
cause a reduction of . Thus, we need
to hold to avoid the case
somewhere in .
. Hence
We know from Claim III.7 that

Suppose that

, which is true
is sufficiently large (which is true for
). Since
due to our adversarial model,
it follows that we must satisfy
.
Certainly, for any time-step with

if

Suppose for the moment that no time-step is jammed in .
Then,
. In order to prove a bound on
that holds w.h.p., we can use the general Chernoff bounds
stated above. For any step , let the binary random variable
be 1 if and only if at least one message is transmitted at time
and
. Then

and it particularly holds that for any set of time-steps prior
to some time-step that if there are multiple message transmissions and since

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

Then, we have

and

Thus, the Chernoff bounds and our choice of imply that
either
and
throughout w.h.p.,
or there must be a time-step in with
that would
at some point in
finish the proof. Therefore, unless
,
and
w.h.p. As the reactive
adversary can now reduce
by at most
when leaving
nonjammed steps, it follows that for any adversary,
. That, however, would violate
as that can only hold
our condition above that
if
.
given the bounds on and
Note that the choice of is not oblivious as the adversary may
adaptively decide to set based on the history of events. Thus,
we cannot assume that is a fixed value, and the worst adaptive
adversarial path is hard to assess. Therefore, we apply a union
bound argument and sum up over all adversarial choices for ,
showing that our claim holds for all simultaneously. In order
to show that none of them succeeds, observe that there are only
many possible values for , and for each the claimed property holds w.h.p. (for all possible distributions of the events);
therefore, the claim holds simultaneously for the polynomially
many options of as well.
Similarly, we can also prove that once the aggregate probability exceeds a certain threshold, it cannot become too small
again.
Claim III.9: Suppose that for the first time-step in ,
. Then there is no time-step in with
,
w.h.p.
Proof: Consider some fixed time-step in and let
. Suppose that there are nonjammed time-steps in . If
for a (sufficiently large) constant , then it follows
for the probability at the end of
due to Claim III.7 that

given that
, because in order to compute a
pessimistic lower bound on , assume that all nonjammed
steps are successful so at most
decreases of
can
happen, or similarly, assume that all nonjammed steps are
idle, so at most
decreases of
can happen
due to exceeding ; the total number of decreases is smaller
.
than
. Let
be the number of
Hence, suppose that
these steps with an idle channel, and
be the number of these

765

steps with a successful message transmission. Furthermore, let
be the maximum number of times a node increases
in
. If
, then it must hold (deterministically)
that
because of our assumption that
(more idle rounds would yield higher values).
, this implies that
Since
. Thus, we are back to the case in the proof
of Claim III.8, which shows that
does not hold
and we never have the case in
w.h.p., given that
that
.
If there is a step in
with
, we prune
to the
and repeat the case distinction above. As there are
interval
at most time-steps in , the claim follows.
Combining Claims III.8 and III.9 completes the proof of
Lemma III.6.
Lemma III.10 establishes an important relationship between
aggregate sending probability and throughput.
Lemma III.10: Consider any subframe , and let
be a
sufficiently large constant. Suppose that at the beginning of ,
and
for every node . If
for at least half of the nonjammed time-steps in ,
-competitive in .
then ANTIJAM is at least
Proof: A time-step in is called useful if we either have
an idle channel or a successful transmission at time (i.e., the
time-step is not jammed and there are no collisions) and
. Let be the number of useful time-steps in . Furtherbe the number of useful time-steps in with an
more, let
idle channel,
be the number of useful time-steps in with
be the maximum number of
a successful transmission and
. Recall that
times a node reduces in because of
. Moreover, the following claim holds.
, then
Claim III.11: If

where
is the number of useful time-steps with a successful
transmission in which the sender is different from the previously
successful sender.
and
Proof: According to Corollary III.4, if
, then
. This implies that
with an idle channel,
whenever there is a useful time-step
. Thus, it takes at most
then
many useful time-steps with an idle channel to get from
to
. On the other hand,
an aggregate probability of at least
each of the
successful transmissions reduces the aggregate
. Therefore, once the aggreprobability by a factor of
, we must have
since
gate probability is at
otherwise there must be at least one useful time-step where the
aggregate probability is more than
, which contradicts the
definition of a useful time-step.
Since
, it holds that

From Lemma III.7, we also know that

. Hence

766

if is sufficiently large. Also,
and
Therefore,
or, equivalently

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

.

Thus, we have a lower bound for
that depends on , and it
remains to find a lower bound for .
Claim III.12: Let be the number of nonjammed time-steps
in with
. If
, then

w.h.p.
Proof: Consider any
-bounded jammer for .
,
Suppose that of the nonjammed time-steps with
have an idle channel and have a nonidle channel. It holds
that
. The probability that an idle channel is
useful is one, which is the maximum possible; for any one of the
nonjammed time-steps with a nonidle channel, the probability
that it is useful (in this case, that it has a successful transmission)
is at least

where is the aggregate probability at the step. Since we only
need to lower-bound the number of useful time-steps , and
, it follows that the probability of a nonidle time-step
to be useful (note that we are considering nonjammed time-steps
here) is at least

Thus

since is minimized for
and
.
Since our lower bound for the probability of a nonidle step to
be useful holds independently for all nonjammed nonidle steps
with
and
for our choice of , it
follows from the Chernoff bounds that
w.h.p.
From Claim III.12, it follows that

w.h.p., which completes the proof of Lemma III.10: if we divide the lower bound on by the number of nonjammed timesteps
(as
,
and as
is
negligible).
Finally, it remains to consider the case that for less than half
. Fortunately,
of the nonjammed time-steps in ,
this does not happen w.h.p.

Lemma III.13: Suppose that at the beginning of ,
for every node . Then, at most half of the nonjammed
w.h.p.
time-steps can have the property that
Proof: Recall from Fact III.5 that as long as the access
probabilities of the nodes do not hit , the aggregate probability
-factor in both directions. Suppose
only changes by a
that is selected so that
represents one of these values. Let
be the set of time-steps
with the property that either
and the channel is idle or
. Now,
we define a step to be useful if
and there is either an idle
channel or a successful transmission at . Let be the number
be the number
of useful time-steps in . Furthermore, let
of useful time-steps with an idle channel,
be the number of
useful time-steps with a successful transmission, and
be the
in
because
maximum number of times a node reduces
. It holds that
.
of
Let us cut the time-steps in
into passes where each pass
starting at time consists of a sequence of all (not neceswith
sarily consecutive) nonidle time-steps
following until a time-step is reached in which
, or
the end of is reached if there is no such step, where is either
or a successful transmission. The time-step is
due to
such that either
and there is an idle channel at , or is
the beginning of if there is no such idle channel to mark the
beginning of in . (Note that for two different passes
and
and
,
.)
Although passes defined like this could be nested, we
additionally require that for any pair of passes
and
with
and final time-step
in ,
, but passes with
are allowed to
violate this (by one being nested into the other). It is not difficult
to see that for any distribution of aggregate probabilities over
into
the time-steps of one can organize the time-steps in
passes as demanded above. Based on that, the following claim
is the number of useful
can be easily shown, where
time-steps with a successful transmission by a node different
from the previously successful node.
Let be any collection of passes in , and be the number
of distinct possible values of the aggregate probability in .
We have the following claim.
Claim III.14: For any collection of passes, w.h.p.,
where and are the number of idle time-steps
and the number of successful transmissions in .
. Recall that
is
Proof: We first show that
the number of successful transmissions in which the sender is
different from the previously successful sender. Moreover, we
as the number of times that the aggregate probability
define
; we define
as the number of times
decreased due to
(i.e., the pass started at
a pass started at the initial step of
a nonidle time-step). Clearly, we have
, and
.
Since
is any collection of passes in , it implies that the
throughout . Hence, we have
aggregate probability
the following inequality:

Together with the fact that

, and

, we have

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

Then, let
denote the event that the sender of the
th successful transmission is the same as the sender of the previous successful transmission. We show that the probability that
( is a constant) given
is extremely small. According to Corollary III.4, the nodes’ access probabilities do
-factor after the first successful
not differ by more than a
transmission. Hence, each node has almost the same probability
of transmitting a message at any given time-step, which implies
that

Since is polynomially smaller than ,
becomes very small even for small , which implies that
happens at most a constant number of times during
w.h.p.
Hence, the claim holds.
We have the following upper bound on the number of such
steps in .
Claim III.15:

where is the number of useful steps in .
Proof: If at the beginning of ,
for every
for every
node , then according to Claim III.7,
node
at any time during . Hence, after at most
nonuseful steps, we run into the situation that
for
every node , which reduces the aggregate probability by a
factor of
. Given that we only have useful steps and
, there can
we may initially start with a probability
time-steps in
be at most
; are the useful ones, and the nonuseful ones are the nonidle
and nonsuccessful steps in which the aggregate probability is
nonuseful steps give one reduction of ).
reduced: Every
This proves the claim.
For the calculations below, recall the definition of with the
constants and that are assumed to be sufficiently large. If
, then it follows from Claim III.15 that for large
enough

where
. Thus, the number of nonjammed timesteps in is also at most
, and since can be arbitrarily
large, Lemma III.13 follows, as the steps in fulfill this property (
yields half of the steps).
It remains to consider the case that
. Let us asnonjammed time-steps, othsume that contains at least
erwise the claim certainly holds. Our goal is to contradict that
statement in order to show that the lemma is true. For this, we
will show that Claim III.14 is violated w.h.p.
be the number of all time-steps covered by passes
Let
with
. Certainly,
. Let
, and
.
For an aggregate probability
,
and
. Hence, by multiplying these probabilities by the
steps, we get that
on expectation, and from the Chernoff bounds it follows that

767

w.h.p., so Claim III.15 implies that the
number of time-steps in with aggregate probability
is
at most
w.h.p.
Since can be arbitrarily large, we can only focus on the timesteps when
.
be the number of nonjammed time-steps in . We
Let
. Let
be the number
consider the case where
of successful time-steps associated with -passes (i.e., at aggre). Then,
gate probability
. If we sum up over all possible probabilities with
,
the number of nonjammed time-steps covered by all such that
is at most

many time-steps since the values always differ by factors
(recall that
is the corresponding probability of an
idle step).
. We
Hence, we can ignore all the passes where
denote the time-steps that are ignored by . Since we assumed
, we have that
, where
is a constant. Let
be the number of time-steps in
with
aggregate probability . Let
be a random variable, where
iff there is a successful transmission at time-step . This
, then
implies that

Applying Chernoff bounds, we have w.h.p.,
where
.
Similarly, let
be a random variable, where
channel is idle at . Then,

Applying Chernoff bounds, we have w.h.p.,
where
is a large enough constant.
It implies that w.h.p.

iff the

768

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

Note that
is a constant since both and are
constants. Moreover, the number of different values in
associated with a pass is at most
.
. This inequality holds w.h.p.
Hence,
when the constant
is small enough, and
is sufficiently
large.
This is a contradiction to Claim III.14 and hence completes
the proof of Lemma III.13.
In order to proceed, we need the following claim.
Claim III.16: For any collection of passes, it holds that

where
and
are defined w.r.t. .
Proof: Because of Fact III.5, the probability that a successful transmission is done by a node different from the node
of the last successful transmission is equal to

To see this, observe that among the aggregate probability , if
the last sender has a share
, all other nodes have
a share
, and

Hence,
.
Notice that by the choice of and , never exceeds
for any when initially
for all . Hence, the prerequisites of the lemmas are satisfied. We can also show the following
remains bounded over time.
lemma, which shows that
Lemma III.17: For any time frame in which initially
for all , also
for all at the end of w.h.p.
Proof: We already know that in each subframe in , at
of the nonjammed time-steps in satisfy
least
w.h.p. Hence, for all
-bounded jamming strategies,
there are at least

useful time-steps in w.h.p. Due to the lower bound of
for all time-steps in w.h.p. we can also
conclude that

Because of Claims III.7 and III.16, it follows that

w.h.p. Since
follows that

and
, it
. Therefore, there must be at least one

time point in with
for all
. This in turn ensures
that
for all at the end of w.h.p.
With Lemma III.17, we show that Lemma III.13 is true for
a polynomial number of subframes. Then, Lemmas III.13 and
III.17 together imply that Lemma III.10 holds for a polynomial
number of subframes. Hence, our main Theorem I.2 follows.
Along the same line as in [2], we can show that ANTIJAM is
self-stabilizing, so the throughput result can be extended to an
arbitrary sequence of time frames.
IV. SIMULATION
We have implemented a simulator to study additional properties of our protocol and to complement our worst-case bounds.
Our focus here is on the qualitative nature of the performance of
ANTIJAM, and we did not optimize the parameters to obtain the
best constants. We consider three different jamming strategies
for a reactive jammer that is
-bounded, for different
values and where
: 1) one that jams nonidle steps with
probability
; 2) one that jams nonidle steps deterministically (as long the jamming budget is not used up); 3) one that
jams idle steps deterministically (as long as the jamming budget
is not used up). Intuitively, it seems that jamming nonidle steps
is more harmful than jamming idle steps. However, note that
jamming idle steps may be an effective strategy to steer the protocol into bad states. Moreover, it may capture scenarios where
nodes in colocated networks start sending in quiet times.
We define throughput as the number of successful transmissions over the number of nonjammed time-steps. For networks
with more than 100 nodes, we choose
, whereas for
smaller networks we choose
. As a general guideline,
it is always better to choose larger values, as this avoids capping the throughput in small networks artificially. A smaller
can make sense for bootstrapping large networks, but due to the
fast convergence times of the protocol (see Section IV-B), this
is unproblematic.
A. Throughput
In a first set of experiments we study the throughput as a
function of the network size and . We evaluate the throughput
performance for each type of adversary introduced above; see
Fig. 1 (top). For all three strategies, the throughput is basically
constant, independently of the network size. This is in accordance with our theoretical insight of Theorem I.2. We can see
that given our conditions on and , the strategy that jams nonidle channels deterministically results in the lowest throughput.
Hence, in the remaining experiments described in this section,
we will focus on this particular strategy. As expected, jamming
idle channels does not affect the protocol behavior much. In our
simulations, ANTIJAM makes effective use of the nonjammed
time periods, yielding 20%–40% successful transmissions even
without optimizing the protocol parameters. Having shown
the protocol scales well for large network size, we also study
the throughput results when the network size is small; see
Fig. 1 (bottom). We observe that the results for small- and
large-scale networks are comparable, but the throughput in the
small-scale networks can be slightly lower under an adversary
that jams nonidle channels deterministically or with probability
.

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

769

Fig. 1. (top) Throughput under three different jamming strategies as a function of the network size (large) and , where
(averaged over 10 runs).
. (b)
. (bottom) Throughput under three different jamming strategies as a function of the network size (small) and of , where
(a)
(averaged over 10 runs). (c)
. (d)
.

Fig. 2. Throughput as a function of

under three different jamming strategies, when

In additional experiments, we also studied the throughput as a
function of (see Fig. 2). As expected, the throughput declines
slightly for large , but this effect is small. (Note that for very
small , the convergence time becomes too large for the simulation runs.)
B. Convergence Time
Besides a high throughput, fast convergence is the most
important performance criterion of a MAC protocol. The traces
in Fig. 3 (top) show the evolution of the aggregate probability
over time. It can be seen that the protocol converges quickly
to constant access probabilities. (Note the logarithmic scale.)
If the initial probability for each node is high, the protocol
needs more time to bring down the low-constant aggregate
probability. Moreover, the fraction of time in which the aggregate probability is in the range of
is 92.98% when
, and 89.52% when
. This implies that for
a sufficiently large time period, the aggregate probability is
well bounded most of the time, which corresponds to our theoretical insights (cf. Lemmas III.6 and III.13). Fig. 3 (bottom)
studies the convergence time for different network sizes. We

, and results are averaged over 10 runs. (a)

. (b)

.

performed 50 repetitions of each run and assumed that the
execution has converged when the aggregate probability
satisfies
, for at least five consecutive rounds. The
simulation result qualitatively confirms Theorem I.2, as the
number of rounds needed to converge the execution is bounded
by
. (Of course, the concrete
convergence time can depend on the scenario and may be faster
than expected in the general case.)
Fig. 4 (top) indicates that independently of the initial values
and , the throughput rises quickly (to more than 20%) and
stays there afterwards.
C. Fairness
As the nodes in the ANTIJAM network synchronize their ,
, and values upon message reception, they are expected to
transmit roughly the same amount of messages. In other words,
our protocol is fair. Fig. 5 presents a histogram showing how the
successful transmissions are distributed among the nodes. More
specifically, we partition the number of successful transmissions
into intervals of size 4. Then, all the transmissions are grouped
according to those intervals in the histogram.

770

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 21, NO. 3, JUNE 2013

Fig. 5. Throughput as a function of
, compared to the MAC
.
protocol in [2] and 802.11, averaged over 10 runs, where

Fig. 3. (top) Evolution of aggregate probability over time (network size is
). Note the logarithmic scale. (bottom) Box plot of
1000 nodes, and
, and
.
ANTIJAM runtime as a function of network size for

3) We run ANTIJAM, the MAC protocol in [2], and 802.11
for 4 min, which is equal to
time-steps in our
simulation.
4) The backoff timer of the 802.11 MAC protocol implemented here uses units of 50 s.
5) We omit SIFS, DIFS, and RTS/CTS/ACK.
A comparison is summarized in Fig. 4 (bottom). The
throughput achieved by ANTIJAM and the MAC protocol in [2]
are significantly higher than the one by the 802.11 MAC protocol, especially for lower values of , when the 802.11 MAC
protocol basically fails to deliver any successful message. Note
that the throughput results between ANTIJAM and the MAC
protocol in [2] are similar in the simulations, but ANTIJAM is
slightly better for the most .
V. CONCLUSION
This paper presents a simple, fair, and self-stabilizing distributed MAC protocol called ANTIJAM that is able to make
efficient use of a shared communication medium whose availability quickly and unpredictably changes over time. In particular, we prove that our protocol achieves a constant competitive
throughput if is constant.
We are not aware of any other protocol achieving a competitive throughput in similarly harsh environments. We regard our
work as an important step forward toward the design and formal
analysis of MAC protocols with provable performance guarantees in more general environments with arbitrary reactive (but
power-constrained) interference.
ACKNOWLEDGMENT

Fig. 4. (top) Convergence in a network of 1000 nodes where
, and
(bottom) Fairness in a network of 1000 nodes, where
(averaged over 10 runs).

.

D. Comparison
Finally, to put ANTIJAM into perspective, as a comparison,
we implemented the MAC protocol proposed in [2], as well as
a simplified version of the widely used 802.11 MAC protocol
(with a focus on 802.11a).
The configurations for the simulation are the following.
1) The jammer is reactive and
-bounded.
2) The unit slot time for 802.11 is set to 50 s; for simplicity,
we define one time-step for ANTIJAM to be 50 s also.

The authors thank D. Levin from Telekom Innovation Laboratories for proofreading this paper.
REFERENCES
[1] G. Alnifie and R. Simon, “A multi-channel defense against jamming
attacks in wireless sensor networks,” in Proc. Q2SWinet, 2007, pp.
95–104.
[2] B. Awerbuch, A. Richa, and C. Scheideler, “A jamming-resistant MAC
protocol for single-hop wireless networks,” in Proc. PODC, 2008, pp.
45–54.
[3] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and B.
Thapa, “On the performance of IEEE 802.11 under jamming,” in Proc.
IEEE INFOCOM, 2008, pp. 1265–1273.
[4] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E.
Leiserson, “Adversarial contention resolution for simple channels,” in
Proc. SPAA, 2005, pp. 325–332.
[5] J. Chiang and Y.-C. Hu, “Cross-layer jamming detection and mitigation in wireless broadcast networks,” in Proc. MobiCom, 2007, pp.
346–349.

RICHA et al.: AN EFFICIENT AND FAIR MAC PROTOCOL ROBUST TO REACTIVE INTERFERENCE

[6] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki, “Adversarial
queuing on the multiple-access channel,” in Proc. PODC, 2006, pp.
92–101.
[7] A. Czumaj and W. Rytter, “Broadcasting algorithms in radio networks
with unknown topology,” J. Algor., vol. 60, no. 2, pp. 115–143, 2006.
[8] S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski, C. Newport, F. Kuhn,
and N. Lynch, “Reliable distributed computing on unreliable radio
channels,” in Proc. MobiHoc S3 Workshop, 2009, pp. 1–4.
[9] S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport, “Interference-resilient information exchange,” in Proc. 28th IEEE INFOCOM,
2009, pp. 2249–2257.
[10] S. Gilbert, R. Guerraoui, and C. Newport, “Of malicious motes and suspicious sensors: On the efficiency of malicious interference in wireless
networks,” in Proc. OPODIS, 2006, pp. 215–229.
[11] J. Hastad, T. Leighton, and B. Rogoff, “Analysis of backoff protocols
for mulitiple accesschannels,” SIAM J. Comput., vol. 25, no. 4, pp.
740–774, 1996.
[12] M. Heusse, F. Rousseau, R. Guillier, and A. Duda, “Idle sense: An
optimal access method for high throughput and fairness in rate diverse
wireless LANs,” Comput. Commun. Rev., vol. 35, no. 4, pp. 121–132,
2005.
[13] C. Koo, V. Bhandari, J. Katz, and N. Vaidya, “Reliable broadcast in
radio networks: The bounded collision case,” in Proc. PODC, 2006,
pp. 258–264.
[14] D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer, “Speed dating
despite jammers,” in Proc. DCOSS, Jun. 2009, pp. 1–14.
[15] A. Pelc and D. Peleg, “Feasibility and complexity of broadcasting with
random transmission failures,” in Proc. PODC, 2005, pp. 334–341.
[16] P. Raghavan and E. Upfal, “Stochastic contention resolution with short
delays,” SIAM J. Comput., vol. 28, no. 2, pp. 709–719, 1999.
[17] A. Richa, C. Scheideler, S. Schmid, and J. Zhang, “A jamming-resistant
MAC protocol for multi-hop wireless networks,” in Proc. 24th DISC,
2010, pp. 179–193.
[18] A. Richa, C. Scheideler, S. Schmid, and J. Zhang, “Self-stabilizing
leader election for single-hop wireless networks despite jamming,” in
Proc. 12th ACM MobiHoc, 2011, Article no. 15.
[19] J. P. Schmidt, A. Siegel, and A. Srinivasan, “Chernoff-Hoeffding
bounds for applications with limited independence,” in Proc.
ACM-SIAM SODA, 1993, pp. 331–340.
[20] A. Wood, J. Stankovic, and G. Zhou, “DEEJAM: Defeating energyefficient jamming in IEEE 802.15.4-based wireless networks,” in Proc.
IEEE SECON, 2007, pp. 60–69.
[21] W. Xu, T. Wood, and Y. Zhang, “Channel surfing and spatial retreats:
Defenses against wireless denial of service,” in Proc. Workshop Wireless Security, 2004, pp. 80–89.
Andréa Richa received the M.S. and Ph.D. degrees
in computer science from Carnegie Mellon University, Pittsburgh, PA, in 1995 and 1998, respectively.
She is an Associate Professor of computer science
with the School of Computing, Informatics, and
Decision Systems Engineering, Arizona State University (ASU), Tempe. Her main area of research is
in network algorithms, including packet scheduling,
distributed load balancing, packet routing, wireless
network modeling and topology control, wireless
jamming, and distributed hash tables (DHTs). Her
work on DHTs has been cited over 1000 times by academic journal or conference publications to date.
Prof. Richa was the recipient of an NSF CAREER Award in 1999.

771

Christian Scheideler received the M.S. and Ph.D.
degrees in computer science from the University of
Paderborn, Paderborn, Germany, in 1993 and 1996,
respectively.
He is a Full Professor with the Department of
Computer Science, University of Paderborn. Before
that, he was a Postdoctoral Researcher with the
Weizmann Institute, Rehovot, Israel, for a year; an
Assistant Professor with Johns Hopkins University,
Baltimore, MD, for five years; and an Associate
Professor with the Technical University of Munich,
Munich, Germany, for three years. He is (co)author of more than 100 publications in refereed conferences and journals and has served on more than 50
conference committees. His main focus is on network theory (in particular,
peer-to-peer systems, mobile ad-hoc networks, and sensors networks), the
design and analysis of scalable distributed algorithms and data structures, and
the design of algorithms and architectures for robust and secure distributed
systems.

Stefan Schmid received the M.S. degree in computer science from the Swiss Federal Institute of
Technology (ETH Zurich), Zurich, Switzerland, in
2005.
He is a Senior Research Scientist with Deutsche
Telekom Laboratories (T-Labs) and TU Berlin,
Berlin, Germany. He works in the Internet Network
Architectures group headed by Prof. Dr. Anja
Feldmann, where he is developing the CloudNet
prototype architecture that connects cloud resources
with virtual networks. Before joining T-Labs, he was
a Postdoctoral Research with TU Munich, Munich, Germany, and the University of Paderborn, Paderborn, Germany (with Prof. Dr. Christian Scheideler),
and a Ph.D. student with Prof. Dr. Roger Wattenhofer’s group at ETH Zurich,
Zurich, Switzerland. He is interested in distributed systems and especially in
the design of robust and dynamic networks.

Jin Zhang received the B.E. degree in network engineering from the Beijing University of Posts and
Telecommunications, Beijing, China, in 2008, and is
currently pursuing the Ph.D. degree in computer science, under the supervision of Prof. Andrea Richa, at
Arizona State University, Tempe.
His research interest is in the area of designing
and analyzing efficient medium access protocols that
are robust against adversarial jamming in wireless
networks.

Resource Mapping and Scheduling for Heterogeneous
Network Processor Systems
Liang Yang

Tushar Gohad

Pavel Ghosh

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

MontaVista Software
Tempe, AZ 85282

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

tgohad@mvista.com

Liang.Yang@asu.edu

pavel.ghosh@asu.edu

Devesh Sinha

Arunabha Sen

Andrea Richa

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

Department of Computer
Science and Engineering
Arizona State University
Tempe, AZ 85287

devesh@asu.edu

asen@asu.edu

aricha@asu.edu

ABSTRACT

General Terms

Task to resource mapping problems are encountered during
(i) hardware-software co-design and (ii) performance optimization of Network Processor systems. The goal of the
first problem is to find the task to resource mapping that
minimizes the design cost subject to all design constraints.
The goal of the second problem is to find the mapping that
maximizes the performance, subject to all architectural constraints. To meet the design goals in performance, it may
be necessary to allow multiple packets to be inside the system at any given instance of time and this may give rise to
the resource contention between packets. In this paper, a
Randomized Rounding (RR) based solution is presented for
the task to resource mapping and scheduling problem. We
also proposed two techniques to detect and eliminate the
resource contention. We evaluate the efficacy of our RR approach through extensive simulation. The simulation results
demonstrate that this approach produces near optimal solutions in almost all instances of the problem in a fraction of
time needed to find the optimal solution. The quality of the
solution produced by this approach is also better than often
used list scheduling algorithm for task to resource mapping
problem. Finally, we demonstrate with a case study, the results of a Network Processor design and scheduling problem
using our techniques.

Design Performance

Keywords
Network Processor, HW-SW Partitioning, Randomized Rounding, Codesign

1.

INTRODUCTION

To meet twin goals of performance and flexibility, Network Processors (NP) were introduced by several vendors a
few years ago. Evolution in Network Processor family in the
last few years has seen increasing heterogeneity in the architectural design. This is evidenced by the introduction of
specialized co-processors for classification and security and
content addressable memory (CAM) for faster search. Such
heterogeneity contributes to added complexity for two problems, (i) hardware-software co-design of Network Processors
and (ii) mapping of application components to appropriate
resources for optimal performance. Network Processors are
required to support multiple applications, such as header
parsing, table lookup, encryption/decryption for virtual private networks (VPN), network address translation (NAT)
and voice processing. In the first problem, one needs to find
the optimal architecture that can support all the specified
applications. In the second problem, given the architecture,
one needs to find the optimal mapping of the application
components (task) to the available resources. In both the
cases, one needs to find a task-to-resource mapping. The
goal of the first problem is to find the task-to-resource mapping that minimizes the design cost subject to all design
constraints. The goal of the second problem is to find the
task-to-resource mapping that maximizes the performance
subject to all architectural constraints.
The first problem was studied extensively by Thiele et al.
in [11]. In their model of design space exploration problem,
each application (e.g, encryption/decryption, voice processing) is represented by a directed acyclic graph (DAG). The
nodes of this graph represents the application components

Categories and Subject Descriptors
B.8.2 [Hardware]: [Performance Analysis and Design Aids]
This research was supported in part by the Center for Embedded Systems
(CES) at Arizona State University.
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee.
ANCS’05, October 26–28, 2005, Princeton, New Jersey, USA.
Copyright 2005 ACM 1-59593-082-5/05/0010 ...$5.00.

19

(or tasks). A set of resources, such as general purpose
processors, co-processor for classification (classifier), co-processor
for security, micro-engines and others are available for execution of the tasks. The execution time of each task on
each resource is known. The goal of the task-to-resource
mapping problem is to find the optimal mapping that satisfies all the design and performance constraints. Figure 1
shows the DAGs corresponding to three common Network
 	 


  
   
 

%&'() *+(&,)- 
./0 /1 
23456( 

7'+8 .1 
4;)(8>0%)-?'&+
5+, ;)5,)- 6)+@A; 

7'+8 .1 
4;)(8 >0%)-?'&+
5+, ;)5,)- 6)+@A; 

465??'DC

"
2.0 7&&89:  4;)(8//7
#
4;)(8<9= 
B)(-)=)+A //7
4;)(8<9=


7'+8 /1 


.&9A) 7&&89:
!

465??'DC


23456(


B)A)-='+) 4&<
.&9A) 7&&89:

!

2.07&&89: $
7'+8 /1
3)5,)- *+(-C:A 

7'+8/1


Figure 3: DAG and Mapping Solutions

7&&89:E&<
4& ,
 6 9
!

have to be mapped to two different resources, a General
Purpose Processor(GPP) and a Co-Processor (CP). The CP
is a specialized unit designed to execute the task b and it
takes 2 units of time for the execution. Because the CP is
a specialized processor for the task b, it may not be able
to execute tasks a and c. This is captured in the bipartite
graph model by assigning a weight of infinity to the directed
edge from a to CP and c to CP. The execution times of three
tasks on the GPP are 6, 12 and 10 respectively (shown in
the Figure 3). The communication costs from GPP to CP
are 4 and 20 time units for task pairs (a, b) and (b, c),
respectively. Communication cost is assumed to be zero for
pair of tasks executing on the same processor. The table in
Figure 3 shows the mapping and start, finish times of the
tasks using the list scheduling algorithm [10], the optimal
solution and the randomized rounding (RR) technique based
solution. In case of the list scheduling algorithm, task b
will be mapped to the CP (since, communication cost(4) +
processing cost on CP(2)=6 < processing cost on GPP(12)).
Thus the greedy list scheduling technique will be trapped in
a local minimal point. In this case, this local optimal point
(finishing time = 42) is much worse than the global optimal
point (finishing time = 28) obtained from the RR algorithm.
Clearly this approach does not find the optimal solution
and in fact the difference between solution produced by this
approach and the optimal solution can be arbitrarily large.
For this reason, in this paper we present a Randomized
Rounding (RR) based approach for the solution of the taskto-resource mapping and scheduling problem. We evaluate
the efficacy of our RR approach through extensive simulation. The simulation results demonstrate that this approach
produces near optimal solutions in almost all instances of
the problem in a fraction of time needed to find the optimal
solution.
The mapping and scheduling problems in embedded systems have been studied extensively by the research community in recent years due to its importance in determining the
performance and the cost of the system. Most of the studies
in this arena have focused their attention on the optimal
design of systems that supports a single application [1, 6,
7]. However, a Network Processor System (NPS) may have
to support multiple applications of the type mentioned ear-

B)+C

Figure 1: Task graphs for different flows
Processor applications, Voice-over-IP, IPv4 Forwarding and
Best Effort Quality-of-Service. The bipartite graph of Figure 2 shows the execution time of a task Ti on a resource
Rj . The execution time of task Ti on a resource Rj is shown
on directed edge of the bipartite graph from Ti to Rj .

Figure 2: Task-Resource Mapping Graph
The second problem was studied by Ramaswamy et al. in
[10]. In the first part of their paper they conduct extensive
analysis of the application data to build the DAG that will
be used for mapping. The nodes of this DAG is a set of
instructions that may be viewed as the tasks in the first
problem. In the second part of the paper [10], the authors
provide an algorithm for mapping application DAGs to NP
Architectures. This is a greedy algorithm based on widely
known list scheduling scheme. Unfortunately, this greedy
scheme can produce solutions those are far from optimal.
Consider the DAG example shown in Figure 3. The DAG
is comprised of three nodes (tasks), a, b and c. The tasks

20

lier. Although a vast majority of the mapping and scheduling studies focus on single application systems and as such
they are not particularly useful in the Network Processor
domain. There have been only a few studies, where the issues of multi-application systems have been addressed [5,
8]. Since our focus is on the design of Network Processor systems, we will discuss efforts undertaken by others in
multi-application domain, but will refrain from discussion
of single-application systems. Mapping and scheduling of
multi-application systems may be defined in one of the following two ways [5]:

the resources on which the tasks can be executed and the
time needed for execution of the tasks on the resources is
shown in Figure 4(a). From the figure, the task T11 can be
executed on resources R1 , R2 and R3 and it takes 7, 9 and
15 nanoseconds respectively. With Kalavade’s assumption
of only one packet inside the system at any given time, there
will be no feasible solution. On increasing the deadline from
18ns to 36ns, all the tasks can be processed. However, increasing the deadline will be result in existence of a second
packet in the system before the first one has left. There
may be resource contention between the packets within the
system now. Kalavade’s Extended GCLP (Global Criticality Local Phase)approach [5] does not provide any safeguard
against this as shown in 4(c).If the packets belong to application 2 (specified by task graph T2), the first packet will
use resource R3 from time 0ns to 28ns (0 to 15 for executing T21 and 15 to 28 for T22 ) and R1 from 28ns to 36ns.
The second packet that enters the NP system at time 18ns,
will require the resource R3 between the times 18ns to 46ns,
(for processing T21 from 18 to 33 and T22 from 33 to 46).
As the first packet needs R3 from 0ns to 28ns and the second packet needs it from 18ns to 46ns, there is resource
contention between these two packets. In section 3.2, we
provide mechanisms for detection and elimination of such
resource contentions.
The contribution of this paper is twofold. First, we propose a methodology for finding the least cost mapping of
tasks to resources, satisfying all the constraints. As a first
step toward partitioning, an Integer Linear Programming
(ILP) formulation is developed from the system level specification. To reduce computational time of ILP, the integrality
constraints are relaxed and the corresponding LP is solved.
Randomized rounding technique is used to convert the fractional values assigned to the variables by the LP to integral
values. Second, the initial solution is evaluated for resource
contentions. If this initial solution is resource contention
free, we are done. If this solution is not free from resource
contention, then we change the deadline for completion of
tasks in a systematic way and repeat the entire process.
This process is discussed in detail in section 3. Finally, we
evaluate the efficacy of our methodology through extensive
simulation using the software package TGFF (Task Graph
For Free) [2].
It may be noted that we propose no new technique to
build the task graph (DAG). Accordingly, any method, including the one proposed in [10], can be utilized for this
purpose. The mapping problem discussed in this paper assumes that the task graph is provided as a part of the input
specification.

Definition 1: Given a set AP P = {AP P1 , AP P2 ,
..., AP Pk } of applications each specified by a
DAG, where each application AP Pj has a set
of constraints (e.g., timing constraint Dj , area
constraint Aj , etc.), find the mapping that minimizes the design cost (in monetary terms) while
satisfying all the design constraints.
Definition 2: Given a set AP P = {AP P1 , AP P2 ,
..., AP Pk } of applications each specified by a
DAG, where each application AP Pj has a set
of constraints (e.g., timing constraint Dj , area
constraint Aj , etc.), find the mapping that maximizes system performance while satisfying all the
design constraints.
The first definition can be used before the system is designed and the second definition can be used after the system
is designed. A profile-guided automated mapping compiler
was developed for runtime performance enhancement in [12].
In addition to [10], this may be viewed as an example of the
second definition.
In addition to supporting multiple applications, Network
Processor systems have to process a stream of packets continuously arriving at its input ports. To meet performance
goals, it may be necessary to allow more than one data unit
to be inside the embedded system at any given time. A data
unit is smallest chunk of data on which the system operates.
In a networking application, the data unit may be a packet.
Although the notion of a packet is associated with the networking domain, we will use it in a much broader sense in
this paper and will use the terms data unit and packet interchangeably. The possibility of multiple data units being
inside the embedded system at the same time adds to the
complexity of the design process, as it opens up the possibility of resource contention between successive data units.
We propose two techniques for detection and elimination of
such resource contention. To the best of our knowledge,
such resource contention issues were not studied in earlier
papers.
Kalavade et al.[5] were one of the earliest researchers to
study partitioning and scheduling problem for multi-function
(multi-application) systems. In their model they assumed
that only one data unit (or packet) will be inside the embedded system at any given time. However, this is a strong
assumption and it may be difficult to meet in practice. To
elaborate the issue, we provide the following example.
We assume a network processor has to process data at the
line rate of 20 Gbps and the packet size is 45 Bytes with no
inter-packet arrival gap. The NP has only 18 nanoseconds
to process each packet. This scenario is illustrated in Figure 4. The task graphs corresponding to two applications,

2.

MAPPING AND SCHEDULING APPLICATION DAGS TO NP SYSTEMS

This section describes randomized rounding approach followed by the methods handling resource contention among
packets.

2.1

Randomized Rounding Technique

As the first step of solving the mapping and scheduling
problem for application DAGs to NP systems, we set up the
Integer Liner Program (ILP) formulation of the problem.
The ILP formulation of the problem is provided in Appendix. As it is well known, the solution of ILP may take consid-

21

Figure 4: Resource Contention in GCLP Sol.
erable amount of time, specially if the application DAG has
large number of nodes. For this reason, we do not solve the
ILP. We relax the integrality constraints on the variables, i.e
allowing the solution to have fractional (non-integer) values
for the variables, and solve the corresponding Linear Program (LP). Solution of the LP can be obtained much faster
than the solution of the ILP as LP is solvable in polynomial
time. Relaxing the integrality constraint may give rise to
fractional (non-integral) solution. However, fractional values associated with task-to-resource mapping for variables
may not have any physical meaning. In this section, we describe the randomized rounding approach [9] to convert the
fractional values to integer values.

2.1.1

variable x and if x is rounded, it determines the values of y
and z as well. We chose to round variable d before variable
x.
The fractional value of variables is used as the probability to round them to the closest integer. For example, a
binary variable p has a fraction value 0.35 after some ILP
relaxation. In order to determine the value of p (0 or 1), we
generate a random number between 0 and 1. If the number
happens to be greater than or equal to 0.35, p is set to 1
and 0 otherwise.

2.1.2

Variable Fixing

The relaxed version of the ILP (i.e., the LP) may still produce some variables with integer (or binary) values. During
the next iteration of the LP, we assign integer values (obtained during the previous iteration) to these variables and
treat them as constants. We call this process variable fixing. By preceding variable fixing with variable rounding,
the number of variables to be rounded is reduced and thus
time can be saved later in the rounding process. However,
the variable-fixing process may not always lead to a situation where all variables end up having integer values. In
that case, we still need to use randomized rounding to turn
the left non-integer variables into integers. the same priority order is followed during variable fixing process as in the
variable rounding process.

Rounding Scheme

One of the key issues in a randomized rounding approach
is to maintain the feasibility of the solution as we round
fractional variables. The selection of an appropriate rounding scheme and the adoption of some randomization strategies helps reduce the probability of constraint violations and
speed up the randomization procedure. To maintain feasibility, one should carefully understand the inter-dependencies
amongst the variables. For a constraint of the form
a+b+c+d≤1
where a, b, c and d are variables in the range [0, 1], we know
that only one of them is to be rounded up to one, while the
remaining variables should be zero. Thus, we consider variables in groups, and define an ordering for rounding those
variables as explained in the following sections.
Among the five decision variables defined in appendix,
x, y, z and d are 0/1 variables and s is a positive integer
variable. The variables d and s are related (constraint (4))
and they determine the execution order and scheduling time
of each flow. Accordingly, they can be grouped and d is
rounded first. The variables x, y and z are responsible for
task-to-resource mapping and inter-task relationship. These
variables can be placed in another group. From constraints
(5) and (6), the variables y and z are determined by the

2.1.3

Rollback Point selection

Some of the constraints may not be satisfied even after
randomized rounding of some variables. In this case, we
need to roll back and undo the applied randomized rounding
steps. If during the rounding process, rollback has to be
done several times, it will significantly increase the execution
time. Thus selection of a good rollback point is critical to
the efficient performance of the algorithm. When we choose
rollback point, we need to consider the context in which the
constraint violation takes place. For example, as indicated
earlier, there are two randomized rounding stages involved in
solving the problem. The variables d’s are rounded first and

22

the variables x’s are done next. If any constraint violation is
detected during the first rounding stage, we remove all the
rounded d from the problem file and restart the randomized
rounding procedure. However, if any constraint violation is
detected in the second stage, the rounded d values from the
first stage can be preserved and we only require to roll back
to the beginning of the second stage.

2.1.4

Rounding Step Size

The rounding step size is defined as the number of variables being rounded in each iteration. As the computation
time is related to the number of iterations, lesser number of
iterations will imply lesser computation time. However, the
probability of some constraint being violated is also large
when several variables are rounded simultaneously. Obviously, such violations imply increased execution time. In
our experiments, we found that the computation time is essentially independent of the number of variables rounded
in each iteration. We choose only one constraint at random
and only one variable in that constraint is rounded at a time.

2.2 Resource Contention Problem

Figure 5: Range of Solutions

The total system cost in dollar value can be reduced further by relaxing the deadline for each packet and at the same
time allowing multiple packets inside the system to satisfy
the throughput constraint. Allowing multiple packets inside the system may introduce the problem of resource contention as already discussed in section 1, since more than one
packet may access the same resource at the same instance
of time. We have devised a technique to handle resource
contention amongst tasks belonging to same application.

2.2.1

finishes all the tasks scheduled for a packet on it. For each
resource r ∈ V − {s, t} the resource cycle time is defined as:
y(e)

(1)

x(e)=a

where a = mine∈δ− (r) (x(e)), and b = maxe∈δ− (r) (x(e)), and
δ − (r) is the set of incoming edges to node r.
Maximum Cycle Time: It is defined as the maximum of all
the resource cycle times, i.e.,

Exploration of Solution Space

The solution space of mapping and scheduling problem for
NPSs can be described on a one-dimensional range as shown
in Figure 5. If the deadline constraint is too strict, the ILP
model may not return a feasible solution. On the other
hand, making the deadline too relaxed will give a feasible
solution with a lower cost as the ILP tries to allocate more
than one task to a resource. This increases the chance of
resource contention amongst packets. The cost generally
increases on decreasing the deadline value, as the ILP has
to include faster (more expensive) resources. The goal is
to find a point in the solution space, which is feasible and
resource contention-free with lowest possible dollar cost.

2.2.2

X

x(e)=b

Tcr =

Tc =

max

{Tcr }.

r∈V −{s,t}

(2)

Packet flow graph construction from a given solution of ILP
and the calculation of maximum cycle time is shown in Figure 6. Figure 6(a) shows a particular task graph and 6(b)
has a corresponding solution given by the ILP. Figure 6(c)
shows the corresponding PFG. The calculation of maximum
cycle time is shown in 6(d).
In order to detect the resource contention in the allocation and scheduling corresponding to the ILP solution, the
packet-flow graph is built first. Since the maximum cycle
time in the PFG is a measure of the maximum timespan
a resource is busy processing a packet, there must exist at
least one resource contention amongst packets in the system if the maximum cycle time is greater than the packet
arrival rate. However, this technique may fail to detect an
existing resource contention if two consecutive packets follow different branches in the same task graph. In order to
detect the resource contention in this scenario, Gantt chart
method is used. Resource allocation time-lines are drawn for
each path. Any overlapping allocation of the same resource
for more than one packet is considered to be a resource contention. An iterative approach to resource contention detection and elimination is described next.

Resource Contention Detection

On account of the existence of multiple packets inside
the system, resource contention may be caused amongst the
packets following the same path or different paths in a task
graph. To describe the method for resource contention detection, the following terms are defined first:
Packet Flow Graph (PFG): It is defined as G(V, E), where
V is the subset of resources allocated by the ILP, with additional entry s and exit t nodes. An edge e = (u, v) ∈ E
indicates an allocation sequence between resource u and v.
There is an weight function w(e) associated with each edge.
For each w(e) = (x(e), y(e)); x(e) indicates the sequence
number of allocation of the following resource and y(e) indicates the corresponding execution time on the resource.
Resource Cycle Time: It indicates the maximum amount of
timespan for which a resource is busy in executing a set of
tasks for a packet, i.e. a resource is not available until it

2.2.3

Resource-Contention Elimination

To speed up the exploration of solution space, an iterative procedure is used. It finds the least cost feasible solution

23

we present a case study for design of a Network Processing
system that will support a designated set of applications.

3.1

Performance of Randomized Rounding

The quality of randomized mapping can be evaluated based
on the solution time and the objective value. To test the effectiveness of our technique, we use the task graphs shown
in figure 1 as the problem input which has 737 variables and
868 constraints in the ILP formulation. We vary the timing
deadline and conduct ten independent experiments for each
deadline. Figure 8 is the solution time comparison between
the randomized rounding (RR) approach and ILP technique.
Figure 9 reflects the deviation of heuristic solution from the
optimal objective value obtained by ILP. The figures show
that our approach produces a solution with near optimal
objective value in a fraction of the time needed to find an
optimal solution.
Figure 6: PFG Construction

ZQJQK[\HQ JWQ ]\O[QJQKI G^ OSS JWQ JOIP _KOVWI`
FWGGIQ JWQ [Oa\[L[ bcdG^ JWQ ]\O[QJQKI
efg h i jklmn opqnr s jt u q vwr
efg x i jh y kr z { i h
efg ¦f§¦¨©ªf« ¬ i jx ­ {r s ®
¯§°¦± §°f²e³´g± §°f µ§°g©g©³ª©ª¶
§ª¦ e·¸f¦¹¨©ª¶
¿ºµ ºµ»¼§ª¦³½©¾f¦ ¼³¹ª¦©ª¶
Figure 8: Solution Time Comparison

|H^QOI\}SQ
~




R\KIJ
\JQKOJ\GH
~


 ÉÅÊ
Ë ÅÁÌ ÁÅÃ
 N
N
 

FGHIJKLMJ NOMPQJ RSGT UKOVWI XNRUIY
FOSMLSOJQ Oa\[L[ FMSQ \[Q  OH] JGJOS
QaQMLJ\GH J\[Q  GQK OSS JWQ NRUI
ÀÁ
  h 

~
ÂÃ ÀÄÃÅÆ


 ¡ ¢£¤£¤¥£
efg x i 


Ç È
~



 Ã 
N

  efg { i


 
N

Ç È
~


Figure 9: Objective Value Deviation



3.2

Task-to-Resource Mapping Case Study

To demonstrate the validity of the model, we applied our
mapping method to design an IXP2400-like system. We
designed the system for 3 common and representative applications for NPSs defined by the Network Processor Forum
benchmarking implementation agreements [3].
The applications are shown in figure 10: Ingress and egress
processing for the Ethernet IPv4 unicast forwarder and Diffserv on the ingress path. IPv4 packet forwarding is based on
RFC1812 is the core in many NPS applications. Diffserv is
a method of facilitating end-to-end quality of service (QoS)
over an existing IP network.
In all three applications, the tasks to be used were decided
based on the Intel Application Building Blocks Design Guide

Figure 7: Iterative Improvement
which is free from resource contention. A flowchart representation of the iterative improvement process is shown in
figure 7.

3. EXPERIMENTAL RESULTS
We discuss the quality of solution provided by the randomized rounding approach compared to the ILP solution. Later

24

[4]. A resource set was chosen to be the set of resources
in IXP2400. This helped us have cycle-accurate simulation
model available for all the resources. The resource set with
associated design parameters is shown in table 1.

with its cycle-accurate simulator and transactor to get the
timing values. Communication delays were obtained based
on the bus speeds and the amount of data being transferred
between tasks. Communication delays are not shown here.
ILOG CPLEX 8.1 was used to solve the constraint system of the ILP formulation on an Intel XEON 1.5GHZ with
1GB RDRAM memory running RedHat Linux 8.0. The Intel IXP2400 workbench was configured for 600MHz microengine speed.
For the task graph in figure 10 with 22 tasks, 16 resources,
and 3 applications, the ILP generated 202 variables and 297
constraints. Packets were set to arrive every 205ns with no
inter-packet gap. The value 205ns corresponds to an OC48 configuration with packet size 64 bytes. The maximum
length of a path in the task graph specification was 14, thus
the initial relaxed deadline was set to (14 * 205 =) 2870ns.
The solution was found in 6 seconds after 7 iterations
of binary search for a feasible solution. The deadline was
relaxed from 51ns to 556ns. The total cost of hardware was
97 units. The mapping and schedule generated is shown in
table 2.

Figure 10: Task Graphs based on NPF Benchmark
Applications for NPSs

Resource
RISC Processors
Hash Unit
CRC Unit
CAM Accelerator
Route Lookup Engine
Checksum Unit
POS PHY
CSIX

Label
R1 ..R8
R9
R10
R11
R12
R13
R14
R15

Cost
7
11
9
17
13
12
9
7

Area
6
3
2
4
4
2
3
3

Task

Resource

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15
T16
T17
T18
T19
T20
T21

R14
R1
R7
R6
R7
R12
R7
R6
R13
R15
R15
R7
R9
R11
R6
R3
R6
R14
R7
R15
R6

IPv4
Ingress
0
15
93
146
146
189
238
240
242
434
468
123
–
–
–
–
–
–
240
–
–

Start times
Diffserv
IPv4
Ingress Egress
–
–
10
10
88
–
298
–
392
–
341
–
390
–
392
–
394
–
–
434
468
468
118
–
141
–
201
–
270
–
215
–
230
–
0
–
–
–
–
0
–
414

Table 2: Output Mapping

We compared our mapping against the task-resource mapping provided by Intel in the preconfigured applications of
[4]. Aggregate throughput, end-to-end packet latency and
resource utilization were the parameters used to compare
the models. We found our results to be within 7-10% of the
Intel mapping. Our method used less resources with better
resource utilization and achieved close to desired throughput
figures.
The important aspect of the mapping produced by our
approach is the cost per performance. Intel’s mapping costs
134 for the same performance achieved with our mapping
with cost 97. Moreover, our mapping method generated
the mapping in less than 7 seconds while Intel’s hand-tuned
mapping must have taken days to arrive at.
Figures 11 through 14 give the comparison of the performance of the two mappings.

Table 1: Set of Resources
Each possible implementation for a task was profiled in
the IXP2400 cycle accurate simulation environment. To obtain average execution cycles per task, the application was
tested with worst case input traffic. An instance of each
implementation of a task was run on the hardware and software resource options with the appropriate traffic. The line
rate was set to 2.5Gbps and packet size was set to minimum 64 bytes (size of mpacket for IXP2400) to simulate
the worst case. The Intel Developer Workbench was used

25

Diffserv Ingress Resource Utilization
90.00%

Throughput Results for IPv4 Ingress

80.00%
70.00%

4500

60.00%
Utilization

4000

Throughput (Mbps)

3500
3000

50.00%

Intel

40.00%

Our Approach

30.00%

2500

Intel

2000

Our Approach

20.00%

1500

10.00%

1000

0.00%
M1

500

M2

M3

M4

M5

M6

M7

M8

Microengine id

0
64

65

128

129

256

512

1518

Frame Size (bytes)

Figure 14: Diffserv Ingress Resource Utilizn

Figure 11: IPv4 Ingress Throughput

4.

IPv4 Ingress Resource Utilization
80.00%
70.00%

Utilization

60.00%
50.00%

5.

Intel

40.00%

Our Approach

10.00%
0.00%
M1

M2

M3

M4

M5

M6

M7

M8

Microengine id

Figure 12: IPv4 Ingress Resource Utilization

Throughput Results for Diffserv
4000
3500
3000
2500
Intel

2000

Our Approach

1500
1000
500
0
64

65

128

129

256

512

REFERENCES

[1] K. Chatha and R. Vemuri. Hardware-software
partitioning and pipelined scheduling of
transformative applications. IEEE Transactions on
Very Large Scale Integration (VLSI) Systems,
10:193–208, 2002.
[2] R. Dick, D. Rhodes, and W. Wolf. Tgff: task graphs
for free. International Workshop Hardware/Software
Codesign, pages 97–101, Mar 1998.
[3] N. P. Forum. Benchmark implementation agreements.
Network Processing Forum,
http://www.npforum.org/techinfo/IA.
[4] Intel. Intel application building blocks design guide.
http://www.intel.com/design/network/products/npfamily/sdk.htm.
[5] A. Kalavade and P. A. Subrahmanyam.
Hardware/software partitioning for multi-function
systems. IEEE Transactions on CAD of ICs and
Systems, 17(9):516–521, Sep 1998.
[6] R. Niemann and P. Marwedel. Hardware/software
partitioning using integer programming. Electronic
Design & Test Conference, pages 473–479, 1996.
[7] M. Palesi and T. Givargis. Multi-objective design
space exploration using genetic algorithms. Tenth
International Symposium on Hardware/Software
Codesign, pages 67–72, May 2002.
[8] A. Prasad, W. Qui, and R. Mahapatra. Hardware
software partitioning of multifunction systems. Design
Automation for Embedded Systems, Dec 2002.
[9] P. Raghavan and C. Thompson. Randomized
rounding: A technique for provably good algorithms
and algorithmic proof. Combinatorica, 7:365–374,
1987.

30.00%
20.00%

Throughput (Mbps)

CONCLUSION

In this paper we have presented a new methodology for
task to resource mapping problem. We show that our methodology produces near optimal solution in a fraction of time
needed to find the optimal solution. In addition, we have
identified the resource contention problem that can arise in
case multiple packets are allowed to be within the system
at the same time. We have provided two different solution
techniques for this resource contention problem. Finally, we
provided a case study of Network Processor system design
using our tool.

1518

Frame size (bytes)

Figure 13: Diffserv Ingress Throughput

26

[10] R. Ramaswamy, N. Neng, and T. Wolf. Application
analysis and resource mapping for heterogeneous
network processor architectures. Proc. of Third
Workshop on Network Processors and Applications
(NP-3) in conjunction with Tenth International
Symposium on High Performance Computer
Architecture (HPCA-10), pages 103–119, Feb 2004.
[11] L. Thiele, S. Chakraborty, M. Gries, and S. Kunzli.
Design space exploration of network processor
architectures. First Workshop on Network Processors
at the 8th International Symposium on
High-Performance Computer Architecture (HPCA8),
pages 30–41, 2002.
[12] H. Vin, J. Mudigonda, J. Jason, E. Johnson, R. Ju,
A. Kunze, and R. Lian. A programming environment
for packet-processing systems: Design considerations.
In the Workshop on Network Processors &
Applications - NP3 in conjunction with The 10th
International Symposium on High-Performance
Computer Architecture, Feb 2004.

Exclusive resource constraint: In each flow, if two tasks
occupy the same resource, they have to be scheduled sequentially, i.e.,

X
Gu

∀f, ∀k, ∀u&v, sfu +

X X
N

k
Tu,j
xk,f
u,j +

j=1

X

′

,f k,k′ ,f
zu,i

∀i,u→i k′ 6=k

X
Gv

Gu

−sfv ≤ (3 − dfu,v −

k,k
Cu,i

xk,f
u,j −

j=1

xk,f
v,j )T

j=1

(6)
Communication delay constraint: In each flow, a communication delay may be defined when two adjacent tasks use
different resources.

X

k,k
zu,v

′

,f

≥

xk,f
u,j +

j=1

X

X
Gv

Gu

∀f, ∀k&k′ , ∀u&v,

′

xkv,j,f − 1

j=1

Gu

k,k
zu,v

′

,f

≤

xk,f
u,j

(7)

j=1

X
Gv

k,k
zu,v

APPENDIX (ILP Formulation)

∀f, ∀u&v,

(3)

Timing constraint: In each flow, every task should finish
before the timing deadline, i.e.
Gi

(4)

k=1 j=1

Unique task constraint: In each flow, every task runs exactly once, i.e.

XX
N

Gi

xk,f
i,j = 1

+

Gu

j=1

X X
N

k
Tu,j
xk,f
u,j

+

k,k′ ,f k,k′ ,f
Cu,i
zu,i

u,u→i k′ 6=k

(9)

k=1

∀f, ∀i,

(8)

≤ sfv

Ck yk

k k,f
Ti,j
xi,j ≤ T

sfu

k=1

N

N

0
X X
N

The ILP model enforces the following constraints:
Area constraint: The total area occupied by the resources
in the target architecture should not exceed the maximum
allowed area,i.e.

XX

xk,f
i,j ≤ yk

Task dependency constraint: If two tasks (in the same
path)have to be scheduled sequentially, the task starting
earlier should finish before another one begins. For two adjacent tasks using different resources, the communication
delay should also be included in the total running time of
the earlier one, i.e.,

k=1

∀f, ∀i, sfi +

X
j=1

N

Ak yk ≤ A

′

xkv,j,f

Gi

∀f, ∀i, ∀k,

The objective of this ILP model is to find a task-to-resource
mapping with minimum cost:

X

≤

Task-to-resource mapping constraint: A resource is included in the target architecture if and only if it is used
by at least one task, i.e.

The ILP model has the following decision variables:
xk,f
i,j is equal to 1, when task i runs on resource k with algorithm j in flow, 0 otherwise yk is equal to 1, when resource
k is used in the target architecture, 0 otherwise dfu,v is equal
to 1, when task u starts no later than task v in flow f , 0
k,k′ ,f
otherwise zu,v
is equal to 1, when in flow f , task u runs
on resource k and task v runs on resource k′ , 0 otherwise sfi
is the starting time of task i in flow f

X

,f

j=1

k
Gi is the number of candidate algorithms for task i. Ti,j
is the running time for task i with algorithm j on resource
k. Each resource k is associated with an area Ak , whereas
k,k′ ,f
A is the maximum allowable area for the system. Ci,i
′
represents the communication time, if task i (in flow f ) is
assigned to resource k and task i′ (in flow f ) is assigned to
resource k′ and task i′ follows task i in the task graph associated with flow f . Each flow f is associated with a deadline
T and all the tasks in a flow must be completed with the
deadline.

Minimize

′

(5)

k=1 j=1

27

1
A

Competitive MAC under Adversarial SINR
Adrian Ogierman1 , Andrea Richa2 , Christian Scheideler1 , Stefan Schmid3 , Jin Zhang2
1

arXiv:1307.7231v1 [cs.NI] 27 Jul 2013

2

Department of Computer Science, University of Paderborn, Germany; {adriano,scheideler}@upb.de
Computer Science and Engineering, SCIDSE, Arizona State University, USA; {aricha,jzhang82@asu.edu}@asu.edu
3
TU Berlin & Telekom Innovation Labs, Berlin, Germany; stefan@net.t-labs.tu-berlin.de

Abstract—This paper considers the problem of how to efficiently share a wireless medium which is subject to harsh external
interference or even jamming. While this problem has already
been studied intensively for simplistic single-hop or unit disk
graph models, we make a leap forward and study MAC protocols
for the SINR interference model (a.k.a. the physical model).
We make two contributions. First, we introduce a new adversarial SINR model which captures a wide range of interference
phenomena. Concretely, we consider a powerful, adaptive adversary which can jam nodes at arbitrary times and which is only
limited by some energy budget. The second contribution of this
paper is a distributed MAC protocol which provably achieves a
constant competitive throughput in this environment: we show
that, with high probability, the protocol ensures that a constant
fraction of the non-blocked time periods is used for successful
transmissions.

I. I NTRODUCTION
The problem of coordinating the access to a shared medium
is a central challenge in wireless networks. In order to solve
this problem, a proper medium access control (MAC) protocol
is needed. Ideally, such a protocol should not only be able
to use the wireless medium as effectively as possible, but
it should also be robust against a wide range of interference problems including jamming attacks. Currently, the most
widely used model to capture interference problems is the
SINR (signal-to-interference-and-noise ratio) model [18]. In
this model, a message sent by node u is
Pcorrectly received
by node v if and only if Pv (u)/(N + w∈S Pv (w)) ≥ β
where Px (y) is the received power at node x of the signal
transmitted by node y, N is the background noise, and S is
the set of nodes w 6= u that are transmitting at the same
time as u. The threshold β > 1 depends on the desired
rate, the modulation scheme, etc. When using the standard
model for signal propagation,
then this expression results
P
in P (u)/d(u, v)α /(N + w∈S P (w)/d(w, v)α ) ≥ β where
P (x) is the strength of the signal transmitted by x, d(x, y) is
the Euclidean distance between x and y, and α is the pathloss exponent. In this paper, we will assume that all nodes
transmit with some fixed signal strength P and that α > 2 + 
for some constant  > 0, which is usually the case in an
outdoors environment [30].
In most papers on MAC protocols, the background noise N
is either ignored (i.e., N = 0) or assumed to behave like a
Gaussian variable. This, however, is an over-simplification of
the real world. There are many sources of interference producing a non-Gaussian noise such as electrical devices, temporary
obstacles, co-existing networks [34], or jamming attacks. Also,
these sources can severely degrade the availability of the

wireless medium which can put a significant stress on MAC
protocols that have only been designed to handle interference
from the nodes themselves. In order to capture a very broad
range of noise phenomena, one of the main contributions of
this work is the modeling of the background noise N (due
to jamming or to environmental noise) with the aid of an
adversary ADV(v) that has a fixed energy budget within a
certain time frame for each node v. More precisely, in our
case, a message transmitted by a node u will be successfully
received by node v if and only if
P/d(u, v)α
P
≥β
ADV(v) + w∈S P/d(w, v)α

(1)

where ADV(v) is the current noise level created by the adversary at node v. Our goal will be to design a MAC protocol
that allows the nodes to successfully transmit messages under
this model as long as this is in principle possible. Prior to our
work, no MAC protocol has been shown to have this property.
Model. We assume that we have a static set V of n wireless
nodes that have arbitrary fixed positions in the 2-dimensional
Euclidean plane so that no two nodes have the same position.
The nodes communicate over a wireless medium with a
single channel. We also assume that the nodes are backlogged
in the sense that they always have something to broadcast.
Each node sends at a fixed transmission power of P , and
a message sent by u is correctly
received by v if and only
P
if P/d(u, v)α /(ADV(v) + w∈S P/d(w, v)α ) ≥ β For our
formal description and analysis, we assume a synchronized
setting where time proceeds in synchronized time steps called
rounds. In each round, a node u may either transmit a message
or sense the channel, but it cannot do both. A node which is
sensing the channel may either (i) sense an idle channel, (ii)
sense a busy channel, or (iii) receive a packet. In order to
distinguish between an idle and a busy channel, the nodes use
a fixed noise threshold ϑ: if the measured signal power exceeds
ϑ, the channel is considered busy, otherwise idle. Whether a
message is successfully received is determined by the SINR
rule described above.
Physical carrier sensing is part of the 802.11 standard, and is
provided by a Clear Channel Assessment (CCA) circuit. This
circuit monitors the environment to determine when it is clear
to transmit. The CCA functionality can be programmed to be
a function of the Receive Signal Strength Indication (RSSI)
and other parameters. The ability to manipulate the CCA rule
allows the MAC layer to optimize the physical carrier sensing
to its needs. Adaptive settings of the physical carrier sensing
threshold have been used, for instance, in [35] to increase

spatial reuse.
In addition to the nodes there is an adversary that controls
the background noise. In order to cover a broad spectrum of
noise phenomena, we allow this adversary to be adaptive, i.e.,
for each time step t the adversary is allowed to know the state
of all the nodes in the system at the beginning of t (i.e., before
the nodes perform any actions at time t) and can set the noise
level ADV(v) based on that for each node v. To leave some
chance for the nodes to communicate, we restrict the adversary
to be (B, T )-bounded: for each node v and time interval I of
length T , a (B, T )-bounded adversary has an overall noise
budget of B · T that it can use to increase the noise level at
node v and that it can distribute among the time steps of I as
it likes. This adversarial noise model is very general, since in
addition to being adaptive, the adversary is allowed to make
independent decisions on which nodes to jam at any point in
time (provided that the adversary does not exceed its noise
budget over a window of size T ). In this way, many noise
phenomena can be covered.
Our goal is to design a symmetric local-control MAC
protocol (i.e., there is no central authority controlling the
nodes, and all the nodes are executing the same protocol)
that has a constant competitive throughput against any (B, T )bounded adversary as long as certain conditions (on B etc.)
are met. In order to define what we mean by “competitive”,
we need some notation. The transmission range of a node v is
defined as the disk with center v and radius r with P/rα ≥ βϑ.
Given a constant  > 0, a time step is called potentially busy
at some node v if ADV(v) ≥ (1 − )ϑ (i.e., only a little bit
of additional interference by the other nodes is needed so that
v sees a busy channel). For a not potentially busy time step,
it is still possible that a message sent by a node u within v’s
transmission range is successfully received by v. Therefore,
as long as the adversary is forced to offer not potentially busy
time steps due to its limited budget and every node has a least
one other node in its transmission range, it is in principle
possible for the nodes to successfully transmit messages. To
investigate that formally, we use the following notation. For
any time frame F and node v let fv (F ) be the number of time
steps in F that are not potentially busy at v and let sv (F ) be
the number of time steps in which v successfully receives
a message. P
We call a protocolPc-competitive for some time
frame F if v∈V sv (F ) ≥ c v∈V fv (F ). An adversary is
uniform if at any time step, ADV(v) = ADV(w) for all nodes
v, w ∈ V , which implies that fv (F ) = fw (F ) for all nodes.
Note that the scope of this paper is not restricted to the case
of a uniform jammer (cf Theorem 1.1).
Since the MAC protocol presented in this paper will be
randomized, our performance results typically hold with high
probability (short: w.h.p.): this means a probability of at least
1 − 1/nc for any constant c > 0.
Our Contribution. The contribution of this paper is
twofold. First of all, we introduce a novel extension of the
SINR model in order to investigate MAC protocols that
are robust against a broad range of interference phenomena.

Second, we present a MAC protocol called S ADE1 which can
achieve a c-competitive throughput where c only depends on 
and the path loss exponent α but not on the size of the network
or other network parameters. (In practice, α is typically in the
range 2 < α < 5, and thus c is a constant for fixed . [30]) Let
n be the number of nodes and let N = max{n, T }. Concretely,
we show:
Theorem 1.1: When running S ADE for at least
Ω((T log N )/ + (log N )4 /(γ)2 ) time steps, S ADE
2/(α−2)
) -competitive throughput for any
has a 2−O((1/)
((1 − )ϑ, T )-bounded adversary as long as (a) the adversary
is uniform and the transmission range of every node contains
at least one node, or (b) there are at least 2/ nodes within
the transmission range of every node.
On the other hand, we also show the following.
Theorem 1.2: The nodes can be positioned so that the
transmission range of every node is non-empty and yet no
MAC protocol can achieve any throughput against a (B, T )bounded adversary with B > ϑ, even if it is 1-uniform.
The two theorems demonstrate that our S ADE protocol
is basically as robust as a MAC protocol can get within
our model. However, it should be possible to improve the
competitiveness. We conjecture that a polynomial dependency
on (1/) is possible, but showing that formally seems to be
hard. In fact, a different protocol than S ADE would be needed
for that.
To complement our formal analysis and worst-case bounds,
we also report on the results of our simulation study. This
study confirms many of our theoretical results, but also shows
that the actual performance is often better than in the worstcase. For instance, it depends to a lesser extent on .
Paper Organization. The remainder of this paper is organized as follows. We present our algorithm in Section II,
and subsequently analyze its performance in Section III.
Simulation results are presented in Section IV. After reviewing
related work in Section V, we conclude our paper with a
discussion in Section VI.
II. A LGORITHM
The intuition behind S ADE is simple: Each node v maintains
a parameter pv which specifies v’s probability of accessing the
channel at a given moment of time. That is, in each round,
each node u decides to broadcast a message with probability
pv . (This is similar to classical random backoff mechanisms
where the next transmission time t is chosen uniformly at
random from an interval of size 1/pv .) The nodes adapt their
pv values over time in a multiplicative-increase multiplicativedecrease manner, i.e., the value is lowered in times when the
channel is utilized (more specifically, we decrease pv whenever
a successful transmission occurs) or increased during times
when the channel is idling. However, pv will never exceed p̂,
for some constant 0 < p̂ < 1 to be specified later.
In addition to the probability value pv , each node v maintains a time window threshold estimate Tv and a counter cv for
1 S ADE stands for SINR JADE , the SINR variant of the jamming defense
protocol in [31].

Tv . The variable Tv is used to estimate the adversary’s time
window T : a good estimation of T can help the nodes recover
from a situation where they experience high interference in the
network. In times of high interference, Tv will be increased
and the sending probability pv will be decreased.
With these intuitions in mind, we can describe S ADE in full
detail.
Initially, every node v sets Tv := 1, cv := 1, and pv := p̂. In
order to distinguish between idle and busy rounds, each node
uses a fixed noise threshold of ϑ.
The S ADE protocol works in synchronized rounds. In every
round, each node v decides with probability pv to send a
message. If it decides not to send a message, it checks the
following two conditions:
• If v successfully receives a message, then pv := (1 +
γ)−1 pv .
• If v senses an idle channel (i.e., the total noise created by
transmissions of other nodes and the adversary is less than
ϑ), then pv := min{(1 + γ)pv , p̂}, Tv := max{1, Tv − 1}.
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no idle step among
the past Tv rounds, then pv := (1 + γ)−1 pv and Tv := Tv + 2.
In order for S ADE to be constant competitive in terms of
throughput, the parameter γ needs to be a sufficiently small
value that depends very loosely on n and T . Concretely, γ ∈
O(1/(log T + log log n)).
Our protocol S ADE is an adaption of the MAC protocol
described in [31] for Unit Disk Graphs that works in more
realistic network scenarios considering physical interference.
The main difference in the new protocol is that in order to
use the concepts of idle and busy rounds, the nodes employ
a fixed noise threshold ϑ to distinguish between idle (noise
< ϑ) and busy rounds (noise ≥ ϑ): in some scenarios the
threshold may not be representative, in the sense that, since the
success of a transmission depends on the noise at the receiving
node and on β, it can happen that a node senses an idle or
busy channel while simultaneously successfully receiving a
message. In order to deal with this problem, S ADE first checks
whether a message is successfully received, and only otherwise
takes into account whether a channel is idle or busy. Another
change to the protocol in [31] is that we adapt Tv based on
idle time steps which allows us to avoid the upper bound on
Tv in the protocol in [31] so that our protocol is more flexible.
III. A NALYSIS
While the MAC protocol S ADE is very simple, its stochastic
analysis is rather involved: it requires an understanding of the
complex interplay of the nodes following their randomized
protocol in a dependent manner. In particular, the nodes’
interactions depend on their distances (the geometric setting).
In order to study the throughput achieved by S ADE, we will
consider some fixed node v ∈ V and will divide the area
around v into three circular and concentric zones.

Let DR (v) denote the disk of radius R around a given node
v ∈ V . In the following, we will sometimes think of DR (v)
as the corresponding geometric area on the plane, but we will
also denote by DR (v) the set of nodes located in this area.
The exact meaning will be clear from the context. Moreover,
whenever we omit R we will assume R1 as radius, where R1
is defined as in Definition 3.1.
Definition 3.1 (Zones): Given any node v ∈ V , our analysis
considers three zones around v, henceforth referred to as
Zone 1, Zone 2, and Zone 3: Zone 1 is the disk of radius
R1 around v, Zone 2 is the disk of radius R2 around v
minus Zone 1, and Zone 3 is the remaining part of the plane.
Concretely:
1) Zone 1 covers the transmission range of v, i.e., its radius
R1 is chosen
so that P/R1α ≥ βϑ, which implies that
p
α
R1 = P/(βϑ). Region DR1 (v) has the property that
if there is at least one sender u ∈ DR1 (v), then v will
either successfully receive the message from u or sense
a busy channel, and v will certainly receive the message
from u if the overall interference caused by other nodes
and the adversary is at most ϑ.
2) Zone 2 covers a range that we call the (critical) interference range of v. Its radius R2 is chosen in a way
so that if none of the nodes in Zone 1 and Zone 2
transmit a message, then the interference at any node
w ∈ DR1 (v) caused by transmitting nodes in Zone 3
is likely to be less than ϑ. Hence, if the current time
step is potentially non-busy at some w ∈ DR1 (v) (i.e.,
ADV(w) ≤ (1 − )ϑ), then the overall inference at w
is less than ϑ, which means that w will see an idle
time step. It will turn out that R2 can be chosen as
O (1/)1/(α−1) R1 .
3) Everything outside of Zone 2 is called Zone 3.
Whenever it is clear from the context, we use D1 , D2 , and
D3 instead of DR1 , DR2 , and the area covered by Zone 3,
respectively.
The key to proving a constant competitive throughput is
the analysis of the aggregate probability (i.e., the sum of
the individual sending probabilities pv ) of nodes in disks
D1 (v) and D2 (v): We will show that the expected aggregate
probabilities of D1 (v) and D2 (v), henceforth referred to by
p1 and p2 , are likely to be at most a constant. Moreover, our
analysis shows that while the aggregate probability p3 of the
potentially infinitely large Zone 3 may certainly be unbounded
(i.e., grow as a function of n), the aggregated power received
at any node w ∈ D1 (v) from all nodes in Zone 3 is also
constant on expectation.
A. Zone 1
P
To show an upper bound on p1 = u∈D1 (v) pu , i.e., the
aggregate probability of the nodes in Zone 1 of v, we can
follow a strategy similar to the one introduced for the Unit
Disk Graph protocol [31].
In the following, we assume that the budget B of the
adversary is limited by (1 − 0 )ϑ for some constant 0 = 2.
In this case, B is at most (1 − )2 ϑ. We first look at a

slightly weaker form of adversary. We say that a round t
is open for a node v if v and at least one other node w
within its transmission range are potentially non-busy, i.e.,
ADV(v) ≤ (1 − )ϑ and ADV(w) ≤ (1 − )ϑ (which also
implies that v has at least one node within its transmission
range). An adversary is weakly (B, T )-bounded if it is (B, T )bounded and in addition to this, at least a constant fraction of
the potentially non-busy rounds at each node is open in every
time interval of size T . We will show the following result:
Theorem 3.2: When running S ADE for at least
Ω((T log N )/0 + (log N )4 /(γ0 )2 ) time steps, S ADE
0 2/(α−2)
) -competitive throughput for any
has a 2−O((1/ )
0
weakly ((1 −  )ϑ, T )-bounded adversary.
In order to prove this theorem, we focus on a time frame
I of size F consisting of δ log N/ subframes I 0 of size f =
δ[T + (log3 N )/(γ 2 )] each, where f is a multiple of T , δ is
a sufficiently large constant, and N = max{T, n}. Consider
some fixed node v. We partition D1 (v) into six sectors of
equal angles from v, S1 , ..., S6 . Note that for any sector Si it
holds that if a node u ∈ Si transmits a message, then its signal
strength at any other node u0 ∈ Si is at least βϑ. Fix a sector
S and consider some fixed time frame F . Let us refer to the
sum of the sending probabilitiesPof the neighboring nodes of
a given node v ∈ S by p̄v := w∈S\{v} pw . The following
lemma, which is proven in [31], shows that pv will decrease
dramatically if p̄v is high throughout a certain time interval.
Lemma 3.3: Consider any node w in S. If p̄w > 5 − p̂
0
during all rounds
√ of a subframe I of I and at2 the beginning
0
of I , Tw ≤ F , then pw will be at most 1/n at the end of
I 0 , w.h.p.
Given this property of the individual probabilities, we can
derive an upper bound for the aggregate
probability of a Sector
P
S. In order to compute pS = v∈S pv , we introduce three
thresholds, a low one, ρgreen = 5, one in the middle, ρyellow =
5e, and a high one, ρred = 5e2 . The following three lemmas
provide some important insights about these probabilities. The
first lemma is shown in [31].
Lemma 3.4: Consider √any subframe I 0 in I. If at the
beginning of I 0 , Tw ≤ F for all w ∈ S, then there is at
least one round in I 0 with pS ≤ ρgreen w.h.p.
Lemma 3.5: For any subframe I 0 in I it holds that if pS ≤
ρgreen at the beginning of I 0 , then pS ≤ ρyellow throughout
I 0 , w.m.p.2 Similarly, if pS ≤ ρyellow at the beginning of I 0 ,
then pS ≤ ρred throughout I 0 , w.m.p. The probability bounds
hold irrespective of the events outside of S.
Proof: It suffices to prove the lemma for the case that
initially pS ≤ ρgreen as the other case is analogous. Consider
some fixed round t in I 0 . Let pS be the aggregate probability
at the beginning of t and p0S be the aggregate probability at the
(0)
end of t. Moreover, let pS denote the aggregate probability
of the nodes w ∈ S with a total interference of less than ϑ
(1)
in round t when ignoring the nodes in S. Similarly, let pS
denote the aggregate probability of the nodes w ∈ S with
2 With moderate probability, or w.m.p., means a probability of at least 1 −
log−Ω(1) n.

a single transmitting node in D1 (w) \ S and additionally an
(2)
interference of less than ϑ in round t, and let pS be the
aggregate probability of the nodes w ∈ S that do not satisfy the
first two cases (which implies that they will not experience an
idle channel, no matter what the nodes in S will do). Certainly,
(0)
(1)
(2)
pS = pS + pS + pS . Our goal is to determine p0S in this
case. Let q0 (S) be the probability that all nodes in S stay
silent, q1 (S) be the probability that exactly one node in S is
transmitting, and q2 (S) = 1−q0 (S)−q1 (S) be the probability
that at least two nodes in S are transmitting.
First, let us ignore the case that cv > Tv for a node v ∈ S
at round t. By distinguishing 9 different cases, we obtain the
(0)
(1)
following result: E[p0S ] ≤ q0 (S)· [(1+γ)pS +(1+γ)−1 pS +
(2)
(0)
(1)
(2)
(0)
pS ] +q1 (S) · [(1 + γ)−1 pS + pS + pS ] +q2 (S) · [pS +
(1)
(2)
pS + pS ] Just as an example, consider the case of q0 (S) and
(1)
pS , i.e., all nodes in S are silent and for all nodes in w ∈ S
(1)
accounted for in pS there is exactly one transmitting node in
D1 (w) \ S and the remaining interference is less than ϑ. In
this case, w is guaranteed to receive a message, so according
to the S ADE protocol, it lowers pw by (1 + γ).
The upper bound on E[p0S ] certainly also holds if cv > Tv
for a node v ∈ S because pv will never be increased (but
possibly decreased) in this case. For the rest of the proof we
refer the reader to [31].
Lemma 3.6: For any subframe I 0 in I it holds that if there
has been at least one round during the past subframe where
pS ≤ ρgreen , then throughout I 0 , pS ≤ ρred w.m.p., and the
probability bound holds irrespective of the events outside of
S.
Proof: Suppose that there has been at least one round
during the past subframe where pS ≤ ρgreen . Then we know
from Lemma 3.5 that w.m.p. pS ≤ ρyellow at the beginning of
I 0 . But if pS ≤ ρyellow at the beginning of I 0 , we also know
from Lemma 3.5 that w.m.p. pS ≤ ρred throughout I 0 , which
proves the lemma.
Now, define a subframe I 0 to be good if pS ≤ ρred
throughout I 0 , and otherwise I 0 is called bad. With the help
of Lemma 3.4 and Lemma 3.6 we can prove the following
lemma.
Lemma 3.7: For any sector S, the expected number of bad
subframes I 0 in I is at most 1/polylog(N ), and at most β 0 /6
of the subframes I 0 in I are bad w.h.p., where the constant
β 0 > 0 can be made arbitrarily small depending on the constant
δ in f . The bounds hold irrespective of the events outside of
S.
The proof can be found in [31]. Since we have exactly
6 sectors, it follows from Lemma 3.7 that apart from an
β 0 -fraction of the subframes, all subframes I 0 in I satisfy
P
0
v∈D1 (u) pv ≤ 6ρ throughout I w.h.p.
B. Zone 3
Next, we consider Zone 3. We will show that although the
aggregate probability of the nodes in Zone 3 may be high (for
some distributions of nodes in the space it can actually be as
high as Ω(n)), their influence (or noise) at node v is limited

if the radius of Zone 2 is sufficiently large. Thus, probabilities
recover quickly in Zone 1 and there are many opportunities
for successful receptions.
In order to bound the interference from Zone 3, we divide
Zone 3 into two sub-zones: Z3− , which
 contains all nodes from
Zone 3 up to a radius of O log2 n , and Z3+ , which contains
all remaining nodes in Zone 3. For Zone Z3− we can prove
the following lemma.
Lemma 3.8: At most an β-fraction of the subframes I 0 in
I are bad for some R1 -disk in Zone Z3− w.h.p., where the
constant β 0 > 0 can be made arbitrarily small depending on
the constant δ in f .
Proof: The claim follows
from the fact that the radius

of Zone Z3− is O log2 n and hence d = O log4 n disks
of radius R1 are sufficient to cover the entire area of Z3− .
According to Lemma 3.7, over all of these disks, the expected
number of bad subframes is at most 1/polylog(N ). Using
similar techniques as for the proof of Lemma 3.7 in [31],
it can also be shown that for each disk D, the probability
for D to have k bad subframes is at most 1/polylog(N )k
irrespective of the events outside of D. Hence, one can use
Chernoff bounds for sums of identically distributed geometric
random variables to conclude that apart from anP
β 0 /d-fraction
0
of the subframes, all subframes I in I satisfy v∈D pv ≤ 6ρ
throughout I 0 w.h.p. This directly implies the lemma.
Suppose that R2 = c·R1 . Lemma 3.8 implies that in a good
subframe the expected noise level at any node w ∈ D1 (v)
created by transmissions in Zone Z3− is upper bounded by
O(log2 n)

6ρred ·

X
d=(c−1)

2π(d + 1)
12πρred
1
√
≤
·
α
α − 1 (c − 2)α−2 R1α
2(dR1 )

which is at most ϑ/4 if c = O((1/)1/(α−2) ) is sufficiently
large. In order to bound the noise level at any node w ∈ D1 (v)
from Zone Z3+ , we prove the following claim.
Claim 3.9: Consider some √
fixed R1 -disk D. If at the beginning of time frame I, Tw ≤ F for all w ∈ D, then for all
time steps except for the first subframe in I, pD ∈ O(log n),
w.h.p.
Proof: Lemma 3.4 implies that there must be a time step
t in the first subframe of I with pD ≤ 6ρgreen w.h.p. Since for
pD ∈ Ω(log n) at least a logarithmic number of nodes in D
transmit and therefore every node sees a busy channel, w.h.p.,
and pD can only increase if a node sees an idle channel, pD
is bounded by O(log n) for the rest of I w.h.p.
The claim immediately implies the following result.
√ Lemma 3.10: If at the beginning of time frame I, Tw ≤
F for all w, then for all time steps except for the first
subframe in I, the interference at any node w ∈ D1 (v) due to
transmissions in Z3+ is at most ϑ/4 w.h.p.
Hence, we get:
√ Lemma 3.11: If at the beginning of time frame I, Tw ≤
F for all w, then at most an β-fraction of the subframes in
I contain time steps in which the expected interference at any
node w ∈ D1 (v) due to transmissions in Zone 3 is at least
ϑ/2.

C. Zone 2
For Zone Z2 we can prove the following lemma in the same
way as Lemma 3.8.
Lemma 3.12: At most an β-fraction of the subframes I 0
in I are bad for some R1 -disk in Zone 2, w.h.p., where the
constant β > 0 can be made arbitrarily small depending on
the constant δ in f .
D. Throughput
Given the upper bounds on the aggregate probabilities and
interference, we are now ready to study the throughput of
S ADE. For this we first need to show an upper bound on Tv
in order to avoid long periods of high pv values. Let J be a
time interval that has a quarter of the length of a time frame,
i.e., |J| = F/4. We start with the following lemma whose
proof is identical to Lemma III.6 in [32].
Lemma 3.13: If in subframe I 0 the number of idle time
steps at√v is at most k, then node v increases Tv by 2 at most
k/2 + f many times in I 0 .
Next, we show the following lemma.
√
Lemma 3.14: If at the beginning of J, Tv ≤ F /2 for all
2/(α−2)
) |J|
nodes v, then every node v has at least 2−O((1/)
time steps in J in which it senses an idle channel, w.h.p.
Proof: Fix some node v. Let us call a subframe I 0 in J
good if in Zone 1 and in any R1 -disk in Zone 2 of v, the
aggregate probability is upper bounded by a constant, and the
expected interference due to transmissions at v induced from
Zone 3 is at most ϑ/2 throughout I 0 . From Lemmas 3.7, 3.12,
and 3.11 it follows that there is an (1 − )-fraction
of good

subframes in J. Since R2 = O (1/)1/(α−2) R1 , for any time
step t in a good subframe I 0 the total aggregate probability

in Zones 1 and 2 of v is upper bounded by O (1/)2/(α−2) .
Hence, the probability that none of the nodes in Zones 1 and
2 of v transmits is given by
P
X
2/(α−2)
)
(1 − pw ) ≥ e−2 w∈Z1 ∪Z2 pw = 2−O((1/)
w∈Z1 ∪Z2

Due to the Markov inequality, the probability that the interference due to transmissions in Zone 3 is at least ϑ is at
most 1/2. These probability bounds hold independently of the
other time steps in I 0 . Moreover, the total interference energy
of the adversary in I 0 is bounded by |I 0 |(1 − )2 ϑ, which
implies that at most a (1 − )-fraction of the time steps in I 0
are potentially busy, i.e., ADV(v) ≥ (1 − )ϑ. Hence, for at
2/(α−2)
) -fraction of the time steps in I 0 , the
least a 2−O((1/)
probability for v to sense an idle channel is a constant, which
implies the lemma.
This allows us to prove the following lemma.√
Lemma 3.15: If√at the beginning of J, Tv ≤ F /2 for all
v, then also Tv ≤ F /2 for all v at the end of J, w.h.p.
Proof: From the previous lemma we know that every node
v senses an idle channel for Ω(|J|) time steps in J for any
constants  > 0 and α > 2. Tv is maximized at the end of
J if all of these idle time steps happen at the beginning of
J, which would get Tv down to 1 at some point. Afterwards,

T
Pv t can rise to a value of at most t for the maximum t with
i=1 2i ≤ |J| (because v increases Tv by 2 each time it sees
no
p in thepprevious Tv steps), which is at most
p idle channel
|J|. Since |J| = |F |/2, the lemma follows.
√
Since Tv can be increased at most (F/4) F /2 many times
in J, we get:
√ Lemma 3.16: If at the beginning of a time
√ frame I, Tv ≤
F /2 for all v, then √
throughout I, Tv ≤ F for all v, and
at the end of I, Tv ≤ F /2 for all v, w.h.p.
Hence, the upper bounds on Tv that we assumed earlier are
valid w.h.p. We are now ready to prove Theorem 3.2.
of Theorem 3.2: Recall that a time step is open for a node
v if v and at least one other node in D1 (v) are not potentially
busy. Let J be the set of all open time steps in I. Furthermore,
let k0 be the number of times v senses an idle channel in J
and let k1 be the number of times v receives a message in I.
From Lemma 3.14 and the assumptions in Theorem 3.2 we
2/(α−2)
) |I|.
know that k0 = 2−O((1/)
2/(α−2)

Case 1: k1 ≥ k0 /6. Then our protocol is 2−O((1/)
competitive for v and we are done.

)-

Case 2: k1 < k0 /6. Then we√know from Lemma 3.13 that pv
is decreased at most k0 /2 + F times in I due to cu > Tu . In
addition to this, pv is decreased at most k1 times in I due to a
received message. On the other hand, pv is increased at least
k0 times in J (if possible) due to an idle channel w.h.p. Also,
we know from our protocol that at the beginning of I,p
pv = p̂.
Hence, there must be at least (1 − 1/2 − 1/6)k0 − |F | ≥
k0 /4 rounds in J w.h.p. at which pv = p̂. Now, recall the
definition of a good subframe in the proof of Lemma 3.14.
From Lemmas 3.7, 3.12, and 3.11 it follows that at most a
β-fraction of the subframes in I is bad. In the worst case, all
of the time steps in these subframes are open time steps, which
sums up to at most k0 /8 if β is sufficiently small. Hence, there
are at least k0 /8 rounds in J that are in good subframes,
w.h.p., and at which pv = p̂, which implies that the other not
potentially busy node in D1 (v) has a constant probability of
receiving a message from v. Using Chernoff bounds, at least
k0 /16 rounds with successfully received transmissions can be
identified for v, w.h.p.
If we charge 1/2 of each successfully transmitted message
to the sender and 1/2 to the receiver, then a constant competitive throughput can be identified for every node in both
2/(α−2)
)cases above. It follows that our protocol is 2−O((1/)
competitive in F .
Now, let us consider the two cases of Theorem 1.1. Recall
that we allow here any ((1 − )ϑ, T )-bounded adversary.
of Theorem 1.1:
Case 1: the adversary is 1-uniform and ∀v : D1 (v) 6= ∅.:
In this case, every node has a non-empty neighborhood and
therefore all non-jammed rounds of the nodes are open. Hence,
the conditions on a weakly ((1 − )ϑ, T )-bounded adversary
are satisfied. So Theorem 3.2 applies, which completes the
proof of Theorem 1.1 a).

Case 2: |D1 (v)| ≥ 2/ for all v ∈ V .: Consider some
fixed time interval I with |I| being a multiple of T . For every
node v ∈ D1 (u) let fv be the number of non-jammed rounds
at v in I and ov be the number of open rounds at v in I. Let J
be the set of rounds in I with at most one non-jammed node.
Suppose that |J| > (1 − /2)|I|. Then every node in D1 (u)
must have more than (/2)|I| of its non-jammed rounds in J.
As these non-jammed rounds must be serialized
in J to satisfy
P
our requirement on J, it holds that |J| > v∈D1 (u) (/2)|I| ≥
(2/) · (/2)|I| = |I|. Since this is impossible, it must hold
that |J| ≤ P
(1 − /2)|I|.
P
Thus,
≥P ( v∈D1 (u) fv ) − |J|
≥
v∈D1 (u) ov
P
(1/2) v∈D1 (u) fv because v∈D1 (u) fv ≥ (2/)·|I| = 2|I|.
Let D0 (u) be the set of nodes v ∈ D1 (u) with
ov ≥ fv /4. That is, for each of these nodes, a
constant fraction
P of the non-jammed time
P steps is
open.
Then
o
<
(1/4)
0
v
v∈D1 (u)\D
P
P (u)
P v∈D1 (u) fv ,
so v∈D0 (u) ov ≥ (1/2) v∈D1 (u) ov ≥ (1/4)S v∈D1 (u) fv .
Consider now a set U ⊆ V of nodes so that u∈U D1 (u) =
V and for every v ∈ V there are at most 6 nodes u ∈ U with
v ∈ D1 (u). Note U is easy to construct in a greedy fashion for
arbitrary UDGs, and therefore for D1 (u) in the SINR model,
and also
set of constantP
density. Let
S known as a dominating
P
V 0 = u∈U D0 (u). Since v∈D0 (u) ov ≥ (1/4)P v∈D1 (u) fv
for every
P node
P u ∈ U , it follows
P thatP v∈V 0 ov ≥
(1/6) P
o
≥
(1/24)
0
v
u∈U
v∈D (u)
u∈U
v∈D1 (u) fv ≥
(1/24) v∈V fv . Using that together with Theorem 3.2, which
implies that S ADE is constant competitive w.r.t. the nodes in
V 0 , completes the proof of Theorem 1.1 b).
Finally, we show that S ADE is self-stabilizing, i.e., it can
recover quickly from any set of pv - and Tv -values.
E. Optimality
Obviously, if a jammer has a sufficiently high energy
budget, it can essentially block all nodes all the time. In the
following we call a network dense if ∀v ∈ V : D1 (v) ≥ 1.
Theorem 3.17: The nodes can be positioned so that the
transmission range of every node is non-empty and yet no
MAC protocol can achieve any throughput against a (B, T )bounded adversary with B > ϑ, even if it is 1-uniform.
Proof: Let us suppose the jammer uses an energy budget
B > ϑ. If every node v only has nodes right at the border of its
disk D1 (v) and the adversary continuously sets ADV(v) = B,
then v will not be able to receive any messages according to
the SINR model. Thus the overall throughput in the system is
0.
IV. S IMULATIONS
To complement our formal analysis and to investigate the
average-case behavior of our protocol, we conducted a simulation study. In the following, we consider two scenarios which
differ in the way nodes are distributed in the 2-dimensional
Euclidean space. In the first scenario, called U NI, the nodes
are distributed uniformly at random in the 2-dimensional plane
of size 25 × 25 units. In the second scenario, called H ET, we
first subdivide the 2-dimensional plane of size 25 × 25 units

into 25 sub-squares of size 5×5 units. For each sub-square we
then choose the number of nodes λ uniformly at random from
the interval [20, 1000] and distribute said nodes (uniformly at
random) in the corresponding sub-square. Consequently, each
sub-square potentially provides a different density, where the
attribute density represents the average amount of nodes on a
spot in the plane of the corresponding scenario. In order to
avoid boundary effects, for both U NI and H ET, we assume
that the Euclidean plane “wraps around”, i.e., distances are
computed modulo the boundaries.
While our formal throughput results in Section III hold for
any adversary which respects the jamming budget constraints,
computing the best adversarial strategy (i.e., the strategy which
minimizes the throughput of S ADE) is difficult. Hence, in
our simulations, we consider the following two types of
adversaries: (1) Regular (or random) jammer (R EG): given
an energy budget B per node, a time interval T , and a
specific 1 >  > 0, the adversary randomly jams each node
every th round (on average) using exactly B energy per
node. Additionally we make sure that the overall budget B is
perfectly used up at the end of T . (2) Bursty (or deterministic)
jammer (B UR): For each time period T , the adversary jams
all initial rounds at the node, until the budget B is used up.
The remaining rounds in T are unjammed. In other words,
the first T many rounds are jammed by the adversary using
exactly B energy per node.
If not stated otherwise, we use the jammer R EG and parameters α = 3,  = 31 , β = 2, Π = 8, T = 60, B = (1−)·β and
run the experiment for 3000 rounds. We will typically plot the
percentage of successful message receptions, averaged over all
nodes, with respect to the unjammed time steps. If not specified
otherwise, we repeat each experiment ten times with different
random seeds, both for the distribution of nodes in the plane as
well as the decisions made by our MAC protocol. By default,
our results show the average over these runs; the variance of
the runs is low.
Impact of Scale and α. We first study the throughput as a
function of the network
size.
√
√ Therefore we distribute n nodes
uniformly in the n × n plane. Figure 1 (top left) shows
our results under the R EG (or random) jammer and different
α values. First, we can see that the competitive throughput
is around 40%, which is higher than what we expect from
our worst-case formal analysis. Interestingly, for α = 3, we
observe a small throughput decrease for larger networks; but
for α > 3, the throughput is almost independent of the network
scale. (In the literature, α is typically modeled as 3 or 4.)
This partially confirms Theorem 1.1: a higher α renders the
transmissions and power propagation more local. This locality
can be exploited by S ADE to some extent.
Impact of Density. Next, we investigate how the performance of S ADE depends on the node density. We focus on
α = 3 and study both the R EG jammer as well as the B UR
(deterministic) jammer. Figure 1 (top right) shows that results
for the U NI scenario (n nodes distributed uniformly in the
25 × 25 plane, i.e., density n/625). The throughput is similar
under both jammers, and slightly declines for denser networks.

This effect is very similar to the effect of having larger (but
equidistant) networks.
However, S ADE suffers more from more heterogenous densities. The results for the scenario H ET are shown in Figure 1
(bottom left). While the throughput is generally lower, the
specific sub-square density plays a minor role.
Convergence Time. S ADE adapts quite fast to the given
setting, as the nodes increase and decrease their sending
probabilities in a multiplicative manner. Being able to adapt
quickly is an important feature, in particular in dynamic or
mobile environments where nodes can join and leave over
time, or where nodes are initialized with too high or low sending probabilities. Our distributed MAC protocol will adjust
automatically and “self-stabilize”.
Figure 1 (bottom right) shows representative executions
over time and plots the aggregate probability. Initially, nodes
have a maximum sending probability p̂ = 1/24. This will
initially lead to many collisions; however, very quickly, the
senders back off and the overall sending probabilities (the
aggregated probability) reduce almost exponentially, and we
start observing successful message transmissions. (Observe
that the aggregated “probability” can be higher than one, as it
is simply the sum of the probabilities of the individual nodes.)
The sum of all sending probabilities also converges quickly
for any other Π. However, for smaller powers, the overall
probability is higher. This is consistent with the goal of S ADE:
because for very large sending powers, also more remote nodes
in the network will influence each other and interfere, it is
important that there is only a small number of concurrent
senders in the network at any time—the aggregated sending
probability must be small. On the other hand, small powers
allow for more local transmissions, and to achieve a high
overall throughput, many senders should be active at the same
time—the overall sending probability should be high.
802.11a and Impact of Epsilon. We also compared the
throughput of S ADE to the standard 802.11 MAC protocol
(with a focus on 802.11a). For simplicity, we set the unit slot
time for 802.11 to 50 µs. The backoff timer of the 802.11
MAC protocol implemented here uses units of 50 µs. We
omit SIFS, DIFS, and RTS/CTS/ACK. Our results show that
802.11a suffers more from the interference, while it yields a
similar throughput for large . In fact, we find that for  close
to 0, 802.11a can even slightly outperform S ADE.
When varying , we find that the worst-case bound of
Theorem 1.1 may be too pessimistic in many scenarios, and
the throughput depends to a lesser extent on the constant .
V. R ELATED W ORK
Traditional jamming defense mechanisms typically operate
on the physical layer [25], [27], [36], and mechanisms have
been designed to both avoid jamming as well as detect jamming. Especially spread spectrum technology is very effective
to avoid jamming, as with widely spread signals, it becomes
harder to detect the start of a packet quickly enough in order
to jam it. Unfortunately, protocols such as IEEE 802.11b use
relatively narrow spreading [20], and some other IEEE 802.11

1.0
0.4

0.6

0.8

Deterministic Jammer
Random Jammer

0.0

0.2

Throughput

1.0
0.6
0.4
0.0

0.2

Throughput

0.8

alpha=3.0
alpha=4.0
alpha=5.0
alpha=6.0

100

300

500

700

900

0.16

0.48

0.80

40
35
30
25

Power=2
Power=4
Power=8

20
1.44

2.08

2.72

3.36

4.00

4.64

Sub-Square Density

Fig. 1.

Aggregate Probability

0.2

0.4

Power=2.0
Power=4.0
Power=8.0

0.80

1.44

Density

0.0

Throughput

0.6

Network Size

1.12

0

500

1000

1500

2000

2500

3000

Number of Rounds

Throughput as a function of network size (top left), density (top right), sub-grid density (bottom left), and power (bottom right).

variants spread signals by even smaller factors [5]. Therefore,
a jammer that simultaneously blocks a small number of
frequencies renders spread spectrum techniques useless in
this case. As jamming strategies can come in many different
flavors, detecting jamming activities by simple methods based
on signal strength, carrier sensing, or packet delivery ratios
has turned out to be quite difficult [24].
Recent work has investigated MAC layer strategies against
jamming in more detail, for example coding strategies [6],
channel surfing and spatial retreat [1], [38], or mechanisms to
hide messages from a jammer, evade its search, and reduce
the impact of corrupted messages [37]. Unfortunately, these
methods do not help against an adaptive jammer with full
information about the history of the protocol, like the one
considered in our work.
In the theory community, work on MAC protocols has
mostly focused on efficiency. Many of these protocols are
random backoff or tournament-based protocols [4], [7], [17],
[19], [23], [29] that do not take jamming activity into account
and, in fact, are not robust against it (see [2] for more details).
The same also holds for many MAC protocols that have been
designed in the context of broadcasting [8] and clustering [22].
Also some work on jamming is known (e.g., [9] for a short

overview). There are two basic approaches in the literature.
The first assumes randomly corrupted messages (e.g. [28]),
which is much easier to handle than adaptive adversarial
jamming [3]. The second line of work either bounds the
number of messages that the adversary can transmit or disrupt
with a limited energy budget (e.g. [16], [21]) or bounds the
number of channels the adversary can jam (e.g. [10], [11],
[12], [13], [14], [15], [26]).
The protocols in [16], [21] can tackle adversarial jamming
at both the MAC and network layers, where the adversary
may not only be jamming the channel but also introducing
malicious (fake) messages (possibly with address spoofing).
However, they depend on the fact that the adversarial jamming
budget is finite, so it is not clear whether the protocols would
work under heavy continuous jamming. (The result in [16]
seems to imply that a jamming rate of 1/2 is the limit whereas
the handshaking mechanisms in [21] seem to require an even
lower jamming rate.)
In the multi-channel version of the problem introduced
in the theory community by Dolev [13] and also studied
in [10], [11], [12], [13], [14], [15], [26], a node can only
access one channel at a time, which results in protocols
with a fairly large runtime (which can be exponential for

deterministic protocols [11], [14] and at least quadratic in the
number of jammed channels for randomized protocols [12],
[26] if the adversary can jam almost all channels at a time).
Recent work [10] also focuses on the wireless synchronization
problem which requires devices to be activated at different
times on a congested single-hop radio network to synchronize
their round numbering while an adversary can disrupt a certain
number of frequencies per round. Gilbert et al. [15] study
robust information exchange in single-hop networks.
Our work is motivated by the work in [3] and [2]. In [3] it
is shown that an adaptive jammer can dramatically reduce the
throughput of the standard MAC protocol used in IEEE 802.11
with only limited energy cost on the adversary side. Awerbuch
et al. [2] initiated the study of throughput-competitive MAC
protocols under continuously running, adaptive jammers, but
they only consider single-hop wireless networks. Their approach has later been extended to reactive jamming environments [32], co-existing networks [34] and applications such
as leader election [33].
The result closest to ours is the robust MAC protocol for
Unit Disk Graphs presented in [31]. In contrast to [31], we
initiate the study of the more relevant and realistic physical
interference model [18] and show that a competitive throughput can still be achieved. As unlike in Unit Disk Graphs,
in the SINR setting far-away communication can potentially
interfere and there is no absolute notion of an idle medium,
a new protocol is needed whose geometric properties must be
understood. For the SINR setting, we also introduce a new
adversarial model (namely the energy budget adversary).
VI. C ONCLUSION
This paper has shown that robust MAC protocols achieving
a constant competitive throughput exist even in the physical
model. This concludes a series of research works in this area.
Nevertheless, several interesting questions remain open. For
example, while our theorems prove that S ADE is as robust as
a MAC protocol can get within our model and for constant ,
we conjecture that a throughput which is polynomial in (1/)
is possible. However, we believe that such a claim is very
difficult to prove. We also plan to explore the performance of
S ADE under specific node mobility patterns.
Acknowledgments. The authors would like to thank
Michael Meier from Paderborn University for his help with
the evaluation of the protocol.
R EFERENCES
[1] G. Alnifie and R. Simon. A multi-channel defense against jamming
attacks in wireless sensor networks. In Proc. of Q2SWinet, pages 95–
104, 2007.
[2] B. Awerbuch, A. Richa, and C. Scheideler. A jamming-resistant MAC
protocol for single-hop wireless networks. In Proc. ACM PODC, 2008.
[3] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and
B. Thapa. On the performance of IEEE 802.11 under jamming. In
Proc. IEEE INFOCOM, pages 1265–1273, 2008.
[4] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E.
Leiserson. Adversarial contention resolution for simple channels. In
Proc. ACM SPAA, 2005.
[5] T. Brown, J. James, and A. Sethi. Jamming and sensing of encrypted
wireless ad hoc networks. In Proc. ACM International Symposium on
Mobile Ad hoc Networking and Computing (MOBIHOC), pages 120–
130, 2006.
[6] J. Chiang and Y.-C. Hu. Cross-layer jamming detection and mitigation
in wireless broadcast networks. In Proc. MOBICOM, pages 346–349,
2007.

[7] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki. Adversarial queuing
on the multiple-access channel. In Proc. ACM PODC, 2006.
[8] A. Czumaj and W. Rytter. Broadcasting algorithms in radio networks
with unknown topology. Journal of Algorithms, 60(2):115 – 143, 2006.
[9] S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski, C. Newport, F. Kuhn,
and N. Lynch. Reliable distributed computing on unreliable radio
channels. In Proc. 2009 MOBIHOC S3 Workshop, 2009.
[10] S. Dolev, S. Gilbert, R. Guerraoui, F. Kuhn, and C. C. Newport.
The wireless synchronization problem. In Proc. 28th Annual ACM
Symposium on Principles of Distributed Computing (PODC), pages 190–
199, 2009.
[11] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Gossiping in a multichannel radio network: An oblivious approach to coping with malicious
interference. In Proc. of the Symposium on Distributed Computing
(DISC), 2007.
[12] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Secure communication over radio channels. In Proc. 27th ACM Symposium on Principles
of Distributed Computing (PODC), pages 105–114, 2008.
[13] S. Dolev, S. Gilbert, R. Guerraoui, and C. C. Newport. Gossiping in a
multi-channel radio network. In Proc. 21st International Symposium on
Distributed Computing (DISC), pages 208–222, 2007.
[14] S. Gilbert, R. Guerraoui, D. Kowalski, and C. Newport. Interferenceresilient information exchange. In Proc. of the 28th Conference on
Computer Communications. IEEE INFOCOM., 2009.
[15] S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport.
Interference-resilient information exchange. In Proc. 28th IEEE International Conference on Computer Communications (INFOCOM), pages
2249–2257, 2009.
[16] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and suspicious sensors: On the efficiency of malicious interference in wireless
networks. In Proc. OPODIS, 2006.
[17] L. A. Goldberg, P. D. Mackenzie, M. Paterson, and A. Srinivasan.
Contention resolution with constant expected delay. J. ACM, 47(6),
2000.
[18] P. Gupta and P. Kumar. The capacity of wireless networks. IEEE
Transactions on Information Theory, 46(2):388 –404, 2000.
[19] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for
mulitiple accesschannels. SIAM Journal on Computing, 25(4), 1996.
[20] IEEE. Medium access control (MAC) and physical specifications. In
IEEE P802.11/D10, 1999.
[21] C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable broadcast in radio
networks: The bounded collision case. In Proc. ACM PODC, 2006.
[22] F. Kuhn, T. Moscibroda, and R. Wattenhofer. Radio network clustering
from scratch. In Proc. ESA, 2004.
[23] B.-J. Kwak, N.-O. Song, and L. E. Miller. Performance analysis of exponential backoff. IEEE/ACM Transactions on Networking, 13(2):343–355,
2005.
[24] M. Li, I. Koutsopoulos, and R. Poovendran. Optimal jamming attacks
and network defense policies in wireless sensor networks. In Proc. IEEE
INFOCOM, pages 1307–1315, 2007.
[25] X. Liu, G. Noubir, R. Sundaram, and S. Tan. Spread: Foiling smart
jammers using multi-layer agility. In Proc. IEEE INFOCOM, pages
2536–2540, 2007.
[26] D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer. Speed dating
despite jammers. In Proc. DCOSS, June 2009.
[27] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein. Using channel
hopping to increase 802.11 resilience to jamming attacks. In Proc. IEEE
INFOCOM, pages 2526–2530, 2007.
[28] A. Pelc and D. Peleg. Feasibility and complexity of broadcasting with
random transmission failures. In Proc. ACM PODC, 2005.
[29] P. Raghavan and E. Upfal. Stochastic contention resolution with short
delays. SIAM Journal on Computing, 28(2):709–719, 1999.
[30] T. Rappaport. Wireless communications. Prentice Hall PTR Upper
Saddle River, 2002.
[31] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A Jamming-Resistant
MAC Protocol for Multi-Hop Wireless Networks. In Proc. DISC, 2010.
[32] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Competitive and fair
medium access despite reactive jamming. In Proc. 31st International
Conference on Distributed Computing Systems (ICDCS), 2011.
[33] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Self-stabilizing leader
election for single-hop wireless networks despite jamming. In Proc.
12th ACM International Symposium on Mobile Ad Hoc Networking and
Computing (MOBIHOC), 2011.
[34] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Competitive and
fair throughput for co-existing networks under adversarial interference.
In Proc. 31st Annual ACM Symposium on Principles of Distributed
Computing (PODC), 2012.
[35] C. Scheideler, A. Richa, and P. Santi. An O(log n) Dominating Set
Protocol for Wireless Ad-Hoc Networks under the Physical Interference
Model. In Proc. ACM International Symposium on Mobile Ad hoc
Networking and Computing (MOBIHOC), 2008.
[36] M. K. Simon, J. K. Omura, R. A. Schultz, and B. K. Levin. Spread
Spectrum Communications Handbook. McGraw-Hill, 2001.
[37] A. Wood, J. Stankovic, and G. Zhou. DEEJAM: Defeating energyefficient jamming in IEEE 802.15.4-based wireless networks. In Proc.
SECON, 2007.
[38] W. Xu, T. Wood, and Y. Zhang. Channel surfing and spatial retreats:
defenses against wireless denial of service. In Proc. of Workshop on
Wireless Security, 2004.

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhou∗ , Anisha Mazumder∗ , Arunabha Sen∗ , Martin Reisslein† and Andrea Richa∗
∗ School of Computing, Informatics and Decision Systems Engineering
† School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

Abstract—Fiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ﬁnd the “shortest path”
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ﬁnd
a least weighted path (or “shortest path”) between a speciﬁed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the “length” of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the “length” of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 ©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiﬁcant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
• Polynomial time algorithm for single path routing (metric
1) in FiWi networks
• Polynomial time algorithm for single path routing (metric
2) in FiWi networks
• NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
• Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
• One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
• One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
• Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ﬁrst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafﬁc intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e ∈ E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 →1 v1 →2 v2 →3 v3 . . . →k vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + · · · + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the “length” (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

∀u∈P

∀u∈P

where wu represents the trafﬁc intensity at a generic FiWi
network node u, which may be an optical node in the ﬁber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafﬁc intensity “weight”
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 →1 v1 →2 v2 →3 . . . →k vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 →v1 →v2 → . . . →vk , between the nodes v0 and vk is given
by

w̄(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) ∗ max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) ∗ maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and w̄(Pv0 ,vk ) will be 2 ∗ w(e)
and CN T (Pv0 ,vk ) ∗ w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ﬁber networks [6],
wireless networks [7] and recently in integrated ﬁber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ﬁrst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
w̄(Pv0 ,vk ), we call an edge e ∈ Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), ∀e ∈ Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstra’s algorithm is that “subpath of a
shortest
path is shortest”. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 →1 v1 →2 v2 →3 v3 and P2 :
w4
w5
w3
v0 → v4 → v2 → v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 →1 v1 →2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 →4 v4 →5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that “subpath of a shortest path is shortest” no
longer holds in this path length metric.
As the assertion “subpath of a shortest path is shortest” no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstra’s algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deﬁne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstra’s algorithm does, we need to maintain
distance vector. We deﬁne distv be distance (length of shortest
path) from s to v, Πv be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deﬁne the function e∗ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
e∈P w(e)
and w+ (Pd ) = w(Pd ) + w(e∗ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ﬁxed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Ge∗ (Pd ) .
Proof: It is obvious that Pd still exists in Ge∗ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiﬁed Dijkstra’s Algorithm
1: Initialize ansv = ∞ for for all v ∈ V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = ∞, Πv = nil, maxedgev = 0 for
all v ∈ V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v ∈ E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Πv = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Πv = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
e∗ (Pd ) since they both belong to Ge∗ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(e∗ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiﬁed Dijkstra’s Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstra’s algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiﬁed Dijkstra’s Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v ∈ V , optimal
solution can be obtained by enumerating all possible crucial
edges e∗ (Pv ) and computing shortest path on Ge∗ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Ge∗ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Ge∗ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstra’s algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let e∗ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that w̄(P ) =
 our objective becomes to ﬁnd
∗
w(e)
+
CN
T
(Q)
∗
w(e
(Q))
is minimum.
e∈Q
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciﬁed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Πv by
Πcv , 0 ≤ c ≤ n. Each Πcv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., w̄(Pv ) is
minimum among all s-v path, then Pv is computed in Ge∗ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deﬁnition, Pv exists in Ge∗ (Pv ) and
CN T (Pv ) ≥ 1 since any path should go through at least one
crucial edge. Noting w̄(Pv ) = w(Pv )+CN T (Pv )∗w(e∗ (Pv )),
on one hand if we treat CN T (Pv ) as a ﬁxed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reﬂects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c − 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v ∈ V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = ∞ for for all v ∈ V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = ∞, Πcv = nil for all v ∈ V and all
0≤c≤n
5:
dist0s = 0
6:
for j = 1 to n − 1 do
7:
for k = 0 to j do
8:
for every node v ∈ V do
9:
if distkv = ∞ then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(e∗ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Πk+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Πku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n − 1 do
31:
ansv = min{ansv , distiv + i ∗ w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Ge∗ Pv . Then, by considering all possible Ge∗ , we could get
Pv in one of these layering. It takes O(m) to generate all Ge∗ ,
by Lemma 4, MSPA runs in v ∈ V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e ∈ E, a source node
s ∈ V , a destination node t ∈ V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) ≤ w(P2 ) ≤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiﬁcation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 −

fj,i,1 =

(i,j)∈E

(j,i)∈E





fi,j,2 −

(i,j)∈E

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e ∈ E, a source node
s ∈ V , a destination node t ∈ V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) ≤ w+ (P2 ) ≤ X  ?



fj,i,2 =

(j,i)∈E

⎧
⎪
⎨
⎪
⎩
⎧
⎪
⎨
⎪
⎩

1
−1
0

i=s
i=t
otherwise

1
−1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 ≤ 1
w1 ≥ fi,j,1 ∗ w(i, j)

∀(i, j) ∈ E

w2 ≥ fi,j,2 ∗ w(i, j)

fi,j,1 ∗ w(i, j)
M P ≥ w1 +

∀(i, j) ∈ E

∀(i, j) ∈ E

(i,j)∈E



M P ≥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 ∗ w(i, j)

(i,j)∈E

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) ≤
w+ (P2 ) ≤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance G’ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxe∈G(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) ≤ w+ (P2 ) ≤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 ∈ P1 and e02 ∈ P2 . Since w(e01 ) =
w(e02 ) = maxe∈E(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 − e01 and
P2 −e02 are two edge disjoint path in G, with length no greater
than X  − 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) ≤ w(P2 ) ≤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

∀(i, j) ∈ E

The following is a brief description of this ILP formulation.
The ﬁrst two equation represent ﬂow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deﬁne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ﬁnd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ﬂow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballe’s algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) ≤ 2w(P ).
Proof: By deﬁnition, w+ (P ) = w(P ) + w(e∗ (P )). Since
w(e (P )) ≤ w(P ), then w+ (P ) ≤ 2 ∗ w(P ).
∗

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s − t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) ≥ w+ (P2 ) and w+ (Q1 ) ≥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p ≤ q. Also, we have w+ (P1 ) = w(P1 ) + e∗ (P1 ) ≤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + e∗ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
≤ w opt
< q/2
≤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = ∞
2: for every Ge of G do
3:
Run Suurballe’s algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} ≥ 2,
1
2

w(e ). It sufﬁces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) ≥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) ≥ w(Q1 ) + w(e∗ (Q1 )) + w(Q2 ) + w(e∗ (Q2 ))
By deﬁnition, e is one of e∗ (Q1 ), e∗ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s → t}, P2 = {s → r → t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s → u1 → u2 →
... → uk−1 → uk → r → t}, Q2 = {s → r → v1 → v2 →
... → vk−1 → vk → t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 ≈ 4 when k is
sufﬁciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballe’s algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{e∗ (Q1 ), e∗ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(e∗ (P1 )) ≤ w(e ) and w(e∗ (P2 )) ≤
w(e ) since they both belong to Ge . Then anse ≤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ﬁnal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s → x1 → x2 →
... → x2k−1 → x2 k → t}, P2 = {s → r → t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s → u1 → u2 →
... → uk−1 → uk → r → t}, Q2 = {s → r → v1 → v2 →
... → vk−1 → vk → t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 ≈ 2 when k is
sufﬁciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, “Fiber-Wireless (FiWi) Access
Networks: A Survey”, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, “Fiber-Wireless (FiWi) Access Networks:
Challenges and Opportunities”, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, “ONU placement in ﬁber-wireless (FiWi)
networks considering peer-to-peer communications”, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, “A study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincations”, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, “FiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysis”, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, “On maximum
available bandwidth through disjoint paths”, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, “Multipath routing in wireless
mesh networks”, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,“Performance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networks”,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,“Mitigating Packet Reordering in
FiWi Networks”, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, “Complexity of Finding Two
Disjoint Paths with Min- Max Objective”, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, “Disjoint paths in a network”, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ﬁnding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

Theoretical Computer Science 671 (2017) 56–68

Contents lists available at ScienceDirect

Theoretical Computer Science
www.elsevier.com/locate/tcs

Universal coating for programmable matter
Zahra Derakhshandeh a,∗,1 , Robert Gmyr b,2 , Andréa W. Richa a,1 ,
Christian Scheideler b,2 , Thim Strothmann b,2
a
b

Computer Science, CIDSE, Arizona State University, USA
Department of Computer Science, Paderborn University, Germany

a r t i c l e

i n f o

Article history:
Received 19 September 2015
Received in revised form 13 January 2016
Accepted 25 February 2016
Available online 10 March 2016
Keywords:
Programmable matter
Self-organizing particle systems
Object coating

a b s t r a c t
The idea behind universal coating is to have a thin layer of a speciﬁc substance covering
an object of any shape so that one can measure a certain condition (like temperature
or cracks) at any spot on the surface of the object without requiring direct access to that
spot. We study the universal coating problem in the context of self-organizing programmable
matter consisting of simple computational elements, called particles, that can establish and
release bonds and can actively move in a self-organized way. Based on that matter, we
present a worst-case work-optimal universal coating algorithm that uniformly coats any object
of arbitrary shape and size that allows a uniform coating. Our particles are anonymous,
do not have any global information, have constant-size memory, and utilize only local
interactions.
© 2016 Elsevier B.V. All rights reserved.

1. Introduction
Today, engineers often need to visually inspect bridges, tunnels, wind turbines and other large civil engineering structures
for defects — a task that is both time-consuming and costly. In the not so distant future, smart coating technology could
do the job faster and cheaper, and increase safety at the same time. The idea behind smart coating (also coined smart
paint) is to have a thin layer of a speciﬁc substance covering the object so that one can measure a certain condition (like
temperature or cracks) at any spot on the surface of the object without requiring direct access to that spot. Also in nature,
smart coating occurs in various situations. Prominent examples are proteins closing wounds, antibodies surrounding bacteria,
or ants surrounding food in order to transport it to their nest. So one can envision a broad range of coating applications for
programmable matter in the future. We intend to study coating problems in the context of self-organizing programmable
matter consisting of simple computational elements, called particles, that can establish and release bonds and can actively
move in a self-organized way. As a basic model for these self-organizing particle systems, we will use the geometric version
of the amoebot model presented in [1,2].
1.1. Amoebot model
We assume that any structure the particle system can form can be represented as a subgraph of an inﬁnite graph
G = ( V , E ) where V represents all possible positions the particles can occupy relative to their structure, and E represents

*

Corresponding author.
E-mail addresses: zderakhs@asu.edu (Z. Derakhshandeh), gmyr@mail.upb.de (R. Gmyr), aricha@asu.edu (A.W. Richa), scheideler@upb.de (C. Scheideler),
thim@mail.upb.de (T. Strothmann).
1
Supported in part by the NSF under Awards CCF-1353089 and CCF-1422603.
2
Supported in part by DFG grant SCHE 1592/3-1.
http://dx.doi.org/10.1016/j.tcs.2016.02.039
0304-3975/© 2016 Elsevier B.V. All rights reserved.

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

57

Fig. 1. (a) shows a section of G eqt . Nodes of G eqt are shown as black circles. (b) shows ﬁve particles on G eqt . The underlying graph G eqt is depicted as a
gray mesh. A particle occupying a single node is depicted as a black circle, and a particle occupying two nodes is depicted as two black circles connected
by an edge. (c) depicts two particles occupying two non-adjacent positions on G eqt . The particles have different offsets for their head port labelings.

all possible atomic transitions a particle can perform as well as all places where neighboring particles can bond to each
other. In the geometric amoebot model, we assume that G = G eqt , where G eqt = ( V eqt , E eqt ) is the inﬁnite regular triangular
grid graph, see Fig. 1(a).
We brieﬂy recall the main properties of the geometric amoebot model. Each particle occupies either a single node or a
pair of adjacent nodes in G eqt , and every node can be occupied by at most one particle. Two particles occupying adjacent
nodes are connected by a bond, and we refer to such particles as neighbors. The bonds do not just ensure that the particles
form a connected structure but they are also used for exchanging information as explained below.
Particles move through expansions and contractions: If a particle occupies one node (i.e., it is contracted), it can expand
to an unoccupied adjacent node to occupy two nodes. If a particle occupies two nodes (i.e., it is expanded), it can contract
to one of these nodes to occupy only a single node. Fig. 1(b) illustrates a set of particles (some contracted, some expanded)
on the underlying graph G eqt . For an expanded particle, we denote the node the particle last expanded into as the head
of the particle and call the other occupied node its tail. A handover allows particles to stay connected as they move. Two
scenarios are possible here: (1) a contracted particle p can “push” a neighboring expanded particle q and expand into the
neighboring node previously occupied by q, forcing q to contract, or (2) an expanded particle p can “pull” a neighboring
contracted particle q to node v it occupies thereby causing q to expand into v, which allows p to contract.
Particles are anonymous but each particle has a collection of ports, one for each edge incident to the nodes occupied by
it, that have unique labels. Adjacent particles establish bonds through the ports facing each other. We also assume that the
particles have a common chirality, i.e., they all have the same notion of clockwise (CW) direction, which allows each particle
p to order its head port labels in clockwise order. However, particles do not have a common sense of orientation since they
can have different offsets of the labelings, see Fig. 1(c). W.l.o.g.,3 we assume that each particle labels its head ports from 0
to 5 in clockwise order. Whenever a particle p is connected to a particle q, we assume that p knows the label of q’s bond
that p is connected with.
Each particle has a constant-size shared local memory that can be read and written to by any neighboring particle. This
allows a particle to exchange information with a neighboring particle by simply writing it into the other particle’s memory.4
A particle always knows whether it is contracted or expanded, and in the latter case it also knows along which head port
label it is expanded. W.l.o.g. we assume that this information is also available to the neighboring particles (by publishing
that label in its local shared memory). Particles do not know the total number of particles, nor do they have any estimate
on this number.
We assume the standard asynchronous model from distributed computing, where the particle system progresses through
a sequence of particle activations, i.e., only one particle is active at a time. Whenever a particle is activated, it can perform
an arbitrary bounded amount of computation (involving its local memory as well as the shared memories of its neighbors)
and at most one movement. A round is deﬁned as the elapsed time until each particle has been activated at least once.
We count time according to the number of particle activations that have already happened since the start of the activation sequence. We assume the activation sequence to be fair, i.e., for any point in time t, every particle will eventually
be activated after t. The conﬁguration C of the system at time t consists of the nodes in G eqt occupied by the object and
the set of particles at the beginning of the particle activation at t; in addition, for every particle p, C contains the current
state of p, including whether the particle is expanded or contracted, its port labeling, and the contents of its local memory.
The work spent by the particles till time t is measured by the number of movements they have done until that point. (We
ignore other state changes since their energy consumption should be irrelevant compared to the energy for a movement.)
For more details on the model, please refer to [1].
1.2. Universal coating problem
For any two nodes v , w ∈ V eqt , the distance d( v , w ) between v and w is the length of the shortest path in G eqt from v
to w. The distance d( v , U ) between a v ∈ V eqt and U ⊆ V eqt is deﬁned as min w ∈U d( v , w ).
In the universal coating problem we are given an instance ( P , O ) where P represents the particle system and O the ﬁxed
object to be coated. Let V ( P ) be the set of nodes occupied by P and V ( O ) be the set of nodes occupied by O (when clear

3

Without loss of generality.
In [1,2], the model was presented as having a shared memory for each port that is visible only to the respective neighbor: The two variants of the
model are equivalent, in the sense that they can emulate each other trivially; we adopt the one here for convenience.
4

58

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

Fig. 2. An example of an object with a tunnel of width 1.

from the context, we may omit the V (·) notation). We call the set of nodes in G eqt neighboring O the surface (coating)
layer. Let n be the number of particles and B be the number of nodes in the surface layer. An instance is called valid if the
following properties hold:
1. The particles are all contracted and start in an idle state.
2. The subgraphs of G eqt induced by V ( O ) and V ( P ) ∪ V ( O ), respectively, are connected, i.e., we are dealing with a single
object and the particle system is connected to the object.
3. The subgraph of G eqt induced by V eqt \ V ( O ) is connected, i.e., the object O does not contain any holes.5
4. V eqt \ V ( O ) is 2( nB  + 1)-connected. In other words, O cannot form tunnels of width less than 2( nB  + 1).
Note that a width of at least 2 nB  is needed to guarantee that the object can be evenly coated. See Fig. 2 for an example
of an object with a tunnel of width 1. Since coating narrow tunnels requires speciﬁc technical mechanisms that complicate
the protocol and do not add much to the basic idea of coating, we decided to ignore narrow tunnels completely in favor of
a clean presentation.
A conﬁguration C is legal if and only if all particles are contracted and

min

v ∈ V eqt \( V ( P )∪ V ( O ))

d( v , V ( O )) ≥ max d( v , V ( O ))
v ∈V ( P )

i.e., no unoccupied node of G eqt is closer to the object than a particle in the system, implying that the particles are as close
to the object as possible: In other words, the contracted particles coat O as evenly as possible. A conﬁguration C is said to be
stable if no particle in C ever performs a state change or movement. An algorithm solves the universal coating problem if,
starting from any valid conﬁguration, it reaches a stable legal conﬁguration C in a ﬁnite number of rounds.
1.3. Our contributions
Our main contribution in this paper is a worst-case work-optimal algorithm for the universal coating problem on selforganizing particle systems. Our Universal Coating Algorithm seamlessly adapts to any valid object O , uniformly coating the
object by forming multiple coating layers (where each coating layer consists of equidistant particles to the object) if necessary. As stated in Section 1.1, our particles are anonymous, do not have any global information (including on the number of
particles n), have constant-size memory, and utilize only local interactions.
Our algorithm builds upon many primitives, some of which may be of interest on their own: The spanning forest primitive
organizes the particles into a spanning forest which is used to guide the movement of particles while preserving connectivity
in the system; the complaint-based coating primitive allows the ﬁrst layer to form, only expanding the coating of the ﬁrst
layer as long as there is still room and there are particles still not touching the object; the general layering primitive allows
the layer  to form only after layer  − 1 has been completed, for  ≥ 2; and a node-based leader election primitive that works
even as particles move and that is used to jumpstart the general layering process. One of the main contributions of our work
is to show how these primitives can be integrated in a seamless way, with no underlying synchronization mechanisms.
1.4. Related work
Many approaches have already been proposed that can potentially be used for smart coating. One can distinguish between active and passive systems. In passive systems the particles either do not have any intelligence at all (but just
move and bond based on their structural properties or due to chemical interactions with the environment), or they have
limited computational capabilities but cannot control their movements. Examples of research on passive systems are DNA
self-assembly systems (see, e.g., the surveys in [3–5]), population protocols [6], and slime molds [7,8]. We will not describe
these models in detail since we are focusing on active systems. In active systems, computational particles can control the
way they act and move in order to solve a speciﬁc task. Robotic swarms, and modular robotic systems are some examples
of active programmable matter systems.

5

If O does contain holes, we consider the subset of particles in each connected region of V eqt \ V ( O ) separately.

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

59

Especially in the area of swarm robotics the problem of coating objects has been studied extensively. In swarm robotics,
it is usually assumed that there is a collection of autonomous robots that have limited sensing, often including vision, and
communication ranges, and that can freely move in a given area. However, coating of objects is commonly not studied as a
stand-alone problem, but is part of collective transport (e.g., [9]) or collective perception (see respective section of [10,11] for
a summary of results). In collective transport a group of robots has to cooperate in order to transport an object. In general,
the object is heavy and cannot be moved by a single robot, making cooperation necessary. In collective perception, a group
of robots with a local perception each (i.e., only a local knowledge of the environment), aims at joining multiple instances
of individual perceptions to one big global picture (e.g. to collectively construct a sort of map). Some research focuses on
coating objects as an independent task under the name of target surrounding or boundary coverage. The techniques used
in this context include stochastic robot behaviors [12,13], rule-based control mechanisms [14] and potential ﬁeld-based
approaches [15]. Surveys of recent results in swarm robotics can be found in [16,17,10,11]; other samples of representative
work can be found in e.g., [18–22]. While the analytic techniques developed in the area of swarm robotics and natural
swarms are of some relevance for this work, the individual units in those systems have more powerful communication and
processing capabilities than the systems we consider, and they can move freely.
In a recent paper [23], Michail and Spirakis propose a model for network construction that is inspired by population
protocols [6]. The population protocol model relates to self-organizing particles systems, but is also intrinsically different:
agents (which would correspond to our particles) freely move in space and can establish connections to any other agent
in the system at any point in time, following the respective probabilistic distribution. In the paper the authors focus on
network construction for speciﬁc topologies (e.g., spanning line, spanning star, etc.). However, in principle, it would be
possible to adapt their approach also for studying coating problems.
1.5. Structure of the paper
Section 2 describes our Universal Coating algorithm. A formal correctness and a worst-case work analyses of the algorithm follow in Section 3. We present our concluding remarks in Section 4.
2. Universal coating algorithm
In this section we present our Universal Coating algorithm: In Section 2.1, we introduce some preliminary notions;
Section 2.2 introduces the algorithmic primitives used for the coating algorithm; and lastly Section 2.3 focuses on the
leader election process needed in certain instances of the problem.
2.1. Preliminaries
We deﬁne the set of states that a particle can be in as idle, follower, root, and retired. In addition to its state, a particle
may maintain a constant number of ﬂags (constant size pieces of information to be read by neighboring particles). While
particles are anonymous, when a particle p sets a ﬂag of type x in its shared memory, we will denote it by p .x (e.g.,
p .parent, p .dir, etc.), so that ownership of the respective ﬂag becomes clear. In our proposed algorithm, we assume that
every time a particle contracts, it contracts out of its tail. Therefore, a node occupied by the head of a particle p still is
occupied by p after a contraction.
We deﬁne a layer as the set of nodes v in G eqt that are equidistant to the object O . More speciﬁcally a node v is in
layer  if d( v , V ( O )) = ; in particular the surface coating layer deﬁned earlier corresponds to layer 1. Any root or retired
particle p stores a ﬂag p .layer indicating the layer number of the node occupied by the head of p. We say a layer is ﬁlled
or complete if all nodes in that layer are occupied with retired particles. In order to respect the particles’ constant-size
memory constraints, we take all layer numbers modulo 4. However, for ease of presentation, we will omit the modulo 4
computations in the text, except for the pseudocode description of the algorithms.
Each root particle p has a ﬂag storing a port label p .down pointing to an occupied node adjacent to its head in layer
p .layer − 1 or in the object if p .layer = 1. Moreover, p has two additional ﬂags, p .CW and p .CCW, which are also port
labels. Intuitively, if p continuously moves by expanding in direction p .CW (resp., p .CCW) and then contracting, it moves
along a clockwise (resp. counter-clockwise) path around the connected structure consisting of the object and retired particles.
Formally, p .CW is the label of the ﬁrst port to a node v in counter-clockwise (CCW) order from p .down such that either v
is occupied by a particle q with q.layer = p .layer, or v is unoccupied (in the latter, v may be a node on layer p .layer or
p .layer − 1). We deﬁne p .CCW analogously, following a clockwise (CW) order from p .down.
2.2. Coating primitives
Our algorithm can be decomposed into a set of primitives, which are all concurrently executed by the particles, as we
brieﬂy explained in Section 1.3. Namely the algorithm relies on the following key primitives: the spanning forest primitive, the
complaint-based coating primitive used to establish the ﬁrst layer of coating, the general layering primitive, and a node-based
(rather than particle-based) leader election primitive that works even as particles move, and that is used to jumpstart the

60

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

Fig. 3. Complaint-based coating primitive: Particles are shown as gray circles. In (a), a follower particle generates a complaint ﬂag (depicted as a black dot
within the particle) that is then forwarded to a super-root (b) causing the super-root to expand into an unoccupied node (c). After a series of handovers,
the follower particle that generated the complaint ﬂag can move to a position on the surface (d).

general layering primitive. One of the main contributions of our work is to show how these primitives can be put to work
together in a seamless way and with no underlying synchronization mechanisms.6
The spanning forest primitive (Algorithm 1) organizes the particles in a spanning forest, in which the roots of the trees
will be in state root and will determine the direction of movement which is speciﬁed by a port label p .dir; the remaining
non-retired particles follow the root particles using handovers. The main beneﬁt of organizing the particles in a spanning
forest connected to the surface is that it provides a relatively easy mechanism for particles to move, following the tree
paths, while maintaining connectivity in the system (see [1,25] for more details). All particles are initially idle. A particle p
becomes a follower when it sets a ﬂag p .parent corresponding to the port leading to its parent in the spanning forest (any
adjacent particle q to p can then easily check if q is a child of p). As the root particles ﬁnd ﬁnal positions according to
the partial coating of the object, they stop moving and become retired. Namely, a root particle p becomes retired when it
encounters another retired particle across the direction p .dir.
Algorithm 1 Spanning forest primitive.
A particle p a acts depending on its state as described below:
idle:
If p is connected to the object O , it becomes a root particle, makes the current node it occupies a leader candidate position, and starts
running the leader election algorithm described in Section 2.3. If p is connected to a retired particle, p also becomes a root particle. If
an adjacent particle p 	 is a root or a follower, p sets the ﬂag p .parent to the label of the port to p 	 , puts a complaint ﬂag in its local
memory, and becomes a follower. If none of the above applies, p remains idle.
follower:

If p is contracted and connected to a retired particle or to O , then p becomes a root particle. Otherwise, if p is expanded, it considers
the following two cases: (i) if p has a contracted child particle q, then p initiates Handover( p ); (ii) if p has no children and no idle
neighbor, then p contracts. Finally, if p is contracted, it runs the function ForwardComplaint( p , p .parent) described in Algorithm 3.

root:

If particle p is on the surface coating layer, p participates in the leader election process described in Section 2.3. If p is contracted, it
ﬁrst executes MarkerRetiredConditions( p ) (Algorithm 6), and becomes retired, and possibly also a marker, accordingly; if p does not
become retired, it calls LayerExtension( p ) (Algorithm 4). If p is expanded, it considers the following two cases: (i) if p has a contracted
child, then p initiates Handover( p ); (ii) if p has no children and no idle neighbor, then p contracts. Finally, if p is contracted, it runs
ForwardComplaint( p , p .dir) (Algorithm 3).

retired:

p clears a potential complaint ﬂag from its memory and performs no further action.

Recall that B denotes the number of nodes on the surface coating layer (layer 1). We need to ensure that once min{n, B }
particles are on layer 1, they stop moving and the coating is complete, independent of how B compares to n (i.e., whether
n ≤ B or not). In order to be able to seamlessly adapt to all possible surface conﬁgurations, we use our novel complaintbased coating primitive for the ﬁrst layer, which basically translates into having the root particles (touching the object) open
up one more position on layer 1 only if there exists a follower particle that remains in the system. This is accomplished by
having each particle that becomes a follower generate a complaint ﬂag, which will be forwarded by particles in a pipeline
fashion from children to parents through the spanning forest and then from a root q to another root at q.dir, until it arrives
at a root particle p with an unoccupied neighboring node at p .dir (we call such a particle p a super-root). Upon receiving a
complaint ﬂag, a super-root p consumes the ﬂag and expands into the unoccupied node at p .dir. The expansion will eventually be followed by a contraction of p, which will induce a series of expansions and contractions of the particles on the path
from p to a follower particle z, eventually freeing a position on the surface coating layer to be taken by z. In order to ensure
that the consumption of a complaint ﬂag will indeed result in one more follower touching the object, one must give higher
priority to a follower child particle in a handover operation, as we do in Algorithm 2. The complaint-based coating phase of
the algorithm will terminate either once all complaint ﬂags are consumed or when layer 1 is ﬁlled with contracted particles.
In either case, the particles on layer 1 will move no further. Fig. 3 illustrates the complaint-based coating primitive.
Once layer 1 is complete and if there are still follower particles in the system, the general layering primitive steps
in, which will build further coating layers. We accomplish this by electing a leader marker particle on layer 1 (via the
leader election primitive proposed in Section 2.3). This leader marker particle will be used to determine a “beginning”
(and an “end”) for layer 1 and allow the particles on that layer to start retiring according to the retired condition given in
Algorithm 6 (the leader marker particle will be the ﬁrst retired particle in the system). Once a layer  becomes completely
ﬁlled with retired (contracted) particles, a new marker particle will emerge on layer  + 1, and start the process of building

6

A video illustrating a fully asynchronous execution of our universal coating algorithm can be found in [24].

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

61

Fig. 4. General layering primitive: Retired particles are shown as black circles, other than (retired) marker particles which are shown in dark gray (the dark
gray arrows represent the marker edges); a root particle is depicted in light gray. Black arrows show the current direction of movement (given by the
dir ﬂag) for each particle (which becomes irrelevant once a particle retires). (a) The root particle p is located on layer  = 3; (b) particle p moves in CW
direction over retired particles on layer  − 1; (c) after a series of expansions and contractions following p .dir, p arrives at an unoccupied neighboring node
on layer  − 1; (d) since p .dir leads to a retired particle, p retires too.

Algorithm 2 Handover( p ).
1: if p .layer = 1 and p has a follower child q then
2:
if q is contracted then
3:
p initiates a handover with particle q
4: else
5:
if p has any contracted (follower or root) child q then
6:
p initiates a handover with particle q

Algorithm 3 ForwardComplaint( p , i ).
1: if p holds a complaint ﬂag and p’s parent does not hold a complaint ﬂag then
2:
p forwards the complaint ﬂag to the particle given by p .parent

this layer (i.e., start the process of retiring particles on that layer) according to Algorithm 6. A marker particle on layer  + 1
only emerges if a root particle p connects to the marker particle q on layer  via its marker port and if q veriﬁed locally
that layer  is completely ﬁlled (by checking whether q.CW and q.CCW are both retired).
With the help of the marker particles — which can only be established after layer 1 was completely ﬁlled (and hence,
we must have B ≤ n) — we can replace the complaint-based coating algorithm of layer 1 with a simpler coating algorithm
for the higher layers, where each root particle p just moves in CW or CCW direction (depending on its layer number) until
p encounters a retired particle on the respective layer and retires itself. More precisely, each contracted root particle p on
layer  tries to extend this layer by expanding into an unoccupied position on layer , or by moving into an unoccupied
position in layer  − 1 (when p .layer will change to  − 1 accordingly), following the direction of movement given by p .dir.
Fig. 4 illustrates this process. The direction p .dir is set to p .CW (resp., p .CCW) when p .layer is odd (resp., even). Alternating
between CCW and CW movements for the particles in consecutive layers ensures that a layer  is completely ﬁlled with
retired particles before particles start retiring in layer  + 1, which is crucial for the correctness of our layering algorithm.
Algorithm 4 LayerExtension( p ).
1:
2:
3:
4:
5:
6:
7:
8:
9:

Calculating p .layer, p .down and p .dir
The layer number of any node occupied by the object is equal to 0.
Let q be any neighbor of p with smallest layer number (modulo 4).
p .down ← p’s label for port leading to q
p .layer = (q.layer + 1) mod 4
clockwise ( p , p .down)
if p .layer is odd then
p .dir ← p .CW
else
p .dir ← p .CCW

Extending layer p .layer
10: if the position at p .dir is unoccupied, and either p is not on the ﬁrst layer, or p holds a complaint ﬂag then
11:
p expands in direction p .dir
12:
p consumes its complaint ﬂag, if it holds one

Algorithm 5 Clockwise( p , i ).
1: j ← i, k ← i
2: while edge j is connected to the object or to a retired particle with layer number p .layer − 1 do
3:
j ← ( j − 1) mod 6
4: p .CW ← j
5: while edge k is connected to the object or to a retired particle with layer number p .layer − 1 do
6:
k ← (k + 1) mod 6
7: p .CCW ← k

 Computes CW & CCW directions

62

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

Algorithm 6 MarkerRetiredConditions( p ).
First marker condition:
1: if p is leader particle then
2:
p becomes a retired particle
3:
p sets the ﬂag p .marker to be the label of a port leading to a node guaranteed not to be on layer p .layer — e.g., by taking the average direction of
p’s two neighbors in layer 1 (by now complete)
Extending Layer Markers:
4: if p is connected to a marker q and the port q.marker points towards p then
5:
if both q.CW and q.CCW are retired then
6:
p becomes a retired particle
7:
p sets the ﬂag p .marker to the label of the port opposite of the port connecting p to q
Retired Condition:
8: if edge p .dir is occupied by a retired particle then
9:
p becomes retired

2.3. Leader election primitive
In this section, we describe the process used for electing a leader among the particles that touch the object. Note that
only particles in layer 1 will ever participate in the leader election process. A leader will only emerge if B ≤ n; otherwise
the process will stop at some point without a leader being elected. As discussed earlier, a leader is elected on layer 1 to
provide a “checkpoint” (a marker particle) that the particles can use in order to determine whether the layer has been
completely ﬁlled (and a leader is only elected after this happens).
The leader election algorithm we use in this paper is a slightly modiﬁed version of the leader election algorithm presented in [1] that can tolerate particles moving around on the surface layer while the leader election process is progressing
(in [1], leader election runs on a system of static particles). Hence, for the purpose of universal coating, we will abstract the
leader election algorithm to conceptually run on the nodes in layer 1, and not on the particular particles that may occupy
these nodes at different points in time. The particles on layer 1 will simply provide the means for running the leader election process on the respective positions, storing and transferring all the ﬂags (which can be used to implement the tokens
described in [1]) that are needed for the leader competition and veriﬁcation. An expanded particle p on layer 1, whose tail
occupies node v in layer 1, that is about to perform a handover with contracted particle q will pass all the information
associated with v to q using the particles’ local shared memories. If a particle p occupying position v would like to forward
some leader election information to a node w adjacent to v that is currently unoccupied, it will wait until either p itself
expands into w, or another particle occupies node w. It is important to note that according to the complaint-based coating
algorithm that we run on layer 1, if a node v in layer 1 is occupied at some time t, then v will never be left unoccupied
after time t.
Here we outline the differences between the leader election process used in this paper and that of [1]:

• Only the nodes on layer 1 that initially hold particles start as leader node candidates. Other nodes in layer 1 will take

•
•
•

•

part in the leader node election process by forwarding any tokens between two consecutive leader node candidates, as
determined for the leader election process for a set of static particles forming a cycle in [1]. Note that layer 1 is a cycle
on G eqt .
The leader election process will determine which leader node candidate in layer 1 will emerge as the unique leader
node. The leader particle is then chosen as described below.
If particle p is expanded, it will hold the ﬂags and any other information necessary for the leader election process
corresponding to each node p occupies (head and tail nodes) independently. In other words, an expanded particle
emulates the leader election process for two nodes on the surface layer.
A particle p occupying node v forwards a ﬂag τ to the node w in CW (or CCW) direction along the surface layer only
if node w is occupied by a particle q (note that q may be equal to p, if p is expanded) and q has enough space in
its (constant-size) memory associated with node w; otherwise p continues to hold the ﬂag τ in its shared memory
associated with node v.
If p is expanded along an edge ( v , w ) and wants to contract into node w, there must exist a particle q expanding into
v (due to the complaint-based mechanism), and hence p will transfer all of its ﬂags currently associated with node v
to particle q.

After the solitude veriﬁcation phase in the leader election algorithm of [1] is complete, there will be just one leader
node v left in the system. Once v is elected a leader node, a contracted particle p occupying this position will check
if layer 1 is completely ﬁlled with contracted particles. To do so, when a contracted particle p occupies node v it will
generate a single CHK ﬂag which it will forward to its CCW neighbor q only if q is contracted. Any particle q receiving a
CHK ﬂag will also only forward the ﬂag to its CCW neighbor z if and only if z is contracted. If the CHK ﬂag at a particle q
ever encounters an expanded CCW neighbor, the ﬂag is held back until the neighbor contracts. Additionally, the particle at
position v sends out a CLR ﬂag to its CW neighbor as soon as it expands. This ﬂag is always forwarded in CW direction. If
a CLR and a CHK ﬂag meet at some particle, the ﬂags cancel each other out. If at some point in time, a particle p at node

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

63

v receives a CHK ﬂag from its CW neighbor in layer 1, it implies that layer 1 must be completely ﬁlled with contracted
particles (and the complaint-based algorithm for layer 1 has converged), and at that time this contracted particle p elects
itself the leader particle, setting the ﬂag p .leader. Note that the leader election process itself does not incur any additional
particle expansions or contractions on layer 1, only the complaint-based algorithm does.
3. Analysis
In this section we show that our algorithm eventually solves the coating problem, and we bound its worst-case work.
We say a particle p 	 is the parent of particle p if p 	 occupies the node in direction p .parent. Let an active particle be
a particle in either follower or root state. We call an active particle a boundary particle if it has the object or at least one
retired particle in its neighborhood, otherwise it is a non-boundary particle. A boundary particle is either a root or a follower,
whereas non-boundary particles are always followers. Note that throughout the analysis we ignore the modulo computation
of layers done by the particles, i.e., layer 1 is the unique layer of nodes with distance 1 to the object.
Given a conﬁguration C , we deﬁne a directed graph A (C ) over all nodes in G eqt occupied by active particles in C .
For every expanded active particle in C , A (C ) contains a directed edge from the tail to the head node of p. For every
non-boundary particle p, A (C ) has a directed edge from the head of p to p .parent, if p .parent is occupied by an active
particle, and for every boundary particle p, p has a directed edge from its head to the node in the direction of p .dir as it
would be calculated by Algorithm 4, if p .dir is occupied by an active particle. The ancestors of a particle p are all nodes
reachable by a path from the head of p in A (C ). For each particle p we denote the ancestor that has no outgoing edge
with p .superRoot, if it exists. Certainly, since every node has at most one outgoing edge in A (C ), the nodes of A (C ) can only
form a collection of disjoint trees or a ring of trees. We deﬁne a ring of trees to be a connected graph consisting of a single
directed cycle with trees rooted at it.
First, we prove several safety conditions, and then we prove various liveness conditions that together will allow us to
prove that our algorithm solves the coating problem.
3.1. Safety
Suppose that we start with a valid instance ( P , O ), i.e., all particles in P are initially contracted and idle and V ( P ) ∪ V ( O )
forms a single connected component in G eqt , among other properties. Then the following properties hold, leading to the
fact that V ( P ) ∪ V ( O ) stays connected at any time.
Lemma 1. At any time, the set of retired particles forms completely ﬁlled layers except for possibly the current topmost layer , which
is consecutively ﬁlled with retired particles in CCW direction (resp. CW direction) if  is odd (resp. even).
Proof. From our algorithm it follows that the ﬁrst particle that retires is the leader particle, setting its marker ﬂag in a
direction not adjacent to a position in layer 1. The particles in layer 1 then retire starting from the leader in CCW direction
around the object. Once all particles in layer 1 are retired, the ﬁrst particle to occupy the adjacent position to the leader via
its marker ﬂag direction will retire and become a marker particle on layer 2, extending its marker ﬂag in the same direction
as the ﬂag set by the marker (leader) on layer 1. Starting from the marker particle in layer 2, other contracted boundary
particles can retire in CW direction along layer 2. Once all particles in layer 2 are retired, the next layer will start forming.
This process continues inductively, proving the lemma. 2
The next lemma characterizes the structure of A (C ).
Lemma 2. At any time, A (C ) is a forest or a ring of trees. Any node that is a super-root (i.e., the root of a tree in A (C )) or part of the
cycle in the ring of trees is connected to the object or to a retired particle.
Proof. An active particle can either be a follower or a root. First, we show the following claim.
Claim 1. At any time, A (C ) restricted to non-boundary particles forms a forest.
Proof. Let A 	 (C ) be the induced subgraph of A (C ) by the non-boundary particles only. Certainly, at the very beginning,
when all particles are still idle, the claim is true. So suppose that the claim holds up to time t. We will show that it then
also holds at time t + 1. Suppose that at time t + 1 an idle particle p becomes active. If it is a non-boundary particle (i.e.,
a follower), it sets p .parent to a node occupied by a particle q that is already active, so it extends the tree of q by a new
leaf, thereby maintaining a tree. Edges can only change if followers move. However, followers only move by a handover or
a contraction, thus a handover can only cause a follower and its incoming edges to disappear from A 	 (C ) (if that follower
becomes a boundary particle), and an isolated contraction, can only cause a leaf and its outgoing edge to disappear from
A 	 (C ), so a tree is maintained in A 	 (C ) in each of these cases. 2
Next we consider A (C ) restricted to boundary particles.

64

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

Claim 2. At any time, A (C ) restricted to boundary particles forms a forest or a ring.
Proof. The boundary particles always occupy the nodes adjacent to retired particles or the object. Therefore, due to
Lemma 1, the boundary particles either all lie in a single layer or in two consecutive layers. Since the layer numbers
uniquely specify the movement direction of the particles, connected boundary particles within a layer are all connected in
the same orientation. Therefore, if these particles all lie in a single layer, they can only form a directed list or directed cycle
in A (C ), proving the claim. If they lie in two consecutive layers, say,  and  − 1, then  − 1 must contain at least one retired
particle, so the nodes occupied by the boundary particles in layer  − 1 can only form a directed list. If there are at least
two boundary particles in layer  − 1, this must also be true for the nodes occupied by the boundary particles in layer 
because according to Lemma 1 there must be at least two consecutive nodes in layer  − 1 not occupied by retired particles.
Moreover, it follows from the algorithm that p .dir of a boundary particle can only point to the same or the next lower layer
of p, implying that in this case A (C ) restricted to the nodes occupied by all boundary particles forms a forest. 2
Since a boundary particle p never connects to a non-boundary particle the way p .dir is deﬁned, and a follower without
an outgoing edge in A (C ) restricted to the non-boundary particles must have an outgoing edge to a boundary particle
(otherwise it is a boundary particle itself), A (C ) is a forest or a ring of trees. The second statement of the lemma follows
from the fact that every boundary particle must be connected to the object or a retired particle. 2
Finally, we investigate the structure formed by the idle particles.
Lemma 3. At any time, every connected component of idle particles is connected to at least one non-idle particle or the object.
Proof. Initially, the lemma holds by the deﬁnition of a valid instance. Suppose that the lemma holds at time t and consider
a connected component of idle particles. If one of the idle particles in the component is activated, it may either stay idle
or change to an active particle, but in both cases the lemma holds at time t + 1. If a retired particle that is connected to
the component is activated, it does not move. If a follower or root particle that is connected to the component is activated,
that particle cannot contract outside of a handover with another follower or root particle, which implies that no node
occupied by it is given up by the active particles. So in any of these cases, the connected component of idle particle remains
connected to a non-idle particle. Therefore, the lemma holds at time t + 1. 2
The following corollary is consequence of the previous three lemmas.
Corollary 1. At any time, V ( P ) ∪ V ( O ) forms a single connected component.
Lemma 4. At any time before the ﬁrst particle retires, in every connected component G of A (C ), the number of expanded boundary
particles in G plus the number of complaint ﬂags in G is equal to the number of non-boundary particles in G.
Proof. Initially, the lemma holds trivially. Suppose the lemma holds at time t and consider the next activation of a particle.
We only discuss relevant cases. If an idle particle becomes a non-boundary particle (i.e., it is not connected to the object
but joins a connected component), it also generates a complaint ﬂag. So both the number of non-boundary particles and the
number of complaint ﬂags increases by one for the component the particle joins. If a non-boundary particle expands as part
of a handover with a boundary particle, both the number of expanded boundary particles and the number of non-boundary
particles decrease by one for the component. If a boundary particle expands as part of a handover, that handover must be
with another boundary particle, so the number of expanded boundary particles remains unchanged for that component.
Since by our assumption there is no retired particle, all boundary particles are in layer 1. Hence, for a boundary particle
to expand outside of a handover, it has to consume a complaint ﬂag. This increases the number of expanded boundary
particles by one and decreases the number of complaint ﬂags by one. Finally, an expansion of a boundary particle outside of
a handover can connect two components of A (C ). Since the equation given in the lemma holds for each of these components
individually, it also holds for the newly built component. 2
3.2. Liveness
We say that the particle system makes progress if (i) an idle particle becomes active, or (ii) a movement (i.e., an expansion,
handover, or contraction) is executed, or (iii) an active particle retires. In the following, we always assume that we have a
fair activation sequence for the particles.
Before we show under which circumstances our particle system eventually makes progress, we ﬁrst establish some
lemmas on how particles behave during the execution of our algorithm.
Lemma 5. Eventually, every idle particle becomes active.

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

65

Proof. As long as an idle particle exists, there is always an idle particle p that is connected to a non-idle particle or the
object according to Lemma 3. The next time p is activated p becomes active according to Algorithm 1. Therefore, eventually
all particles become active. 2
The following statement shows that even though super-roots can be followers, they will become a boundary particle the
next time they are activated.
Lemma 6. In every tree of A (C ), every boundary particle in the follower state enters a root state the next time it is activated. In
particular, every super-root in A (C ) will enter the root state the next time it is activated.
Proof. Let p be a follower boundary particle. By deﬁnition p must have a retired particle or the object in its neighborhood.
Therefore, p immediately becomes a root particle once it is activated according to Algorithm 1. 2
Furthermore, the following lemma provides a relation between the movement of super-roots and the availability of
complaint ﬂags.
Lemma 7. For every tree of A (C ) with a contracted super-root p and at least one complaint ﬂag, p will eventually retire or expand to
p .dir, thereby consuming a complaint ﬂag, and after the expansion p may cease to be a super-root.
Proof. If p is not a root, it becomes one the next time it is activated according to Lemma 6. Therefore, assume p is a
root. If there is a retired particle in p .dir, p retires and ceases to be a super-root. If the node in p .dir is unoccupied, p
can potentially expand. According to Algorithm 3, complaint ﬂags are forwarded along the tree of p towards p. Once the
ﬂag reaches p, it will expand, thereby consuming the ﬂag. If p expands, it might have an active particle in its movement
direction and thus ceases to be a super-root. 2
Next, we prove the statement that expanded particles will not starve, i.e., they will eventually contract.
Lemma 8. Eventually, every expanded particle contracts.
Proof. Consider an expanded particle p in a conﬁguration C . By Lemma 5 we can assume w.l.o.g. that all particles in C are
active or retired. If there is no particle q with either q.parent = p or p occupying the node in q.dir, then p can contract once
it is activated. If such a q exists and it is contracted, p contracts in a handover (see Algorithm 2). If q exists and is expanded,
we consider the tree of A (C ) that p is part of. Consider a subpath in this tree that starts in p, i.e., ( v 1 , v 2 , . . . , v k ) such that
v 1 , v 2 are occupied by p and v k is a node that does not have an incoming edge in A (C ). Let v i be the ﬁrst node of this
path that is occupied by a contracted particle. If all particles are expanded, then clearly the last particle occupying v k−1 , v k
eventually contracts and we can set v i to v k−1 . Since v i is contracted it eventually performs a handover with the particle
occupying v i −2 , v i −1 . Now we can move backwards along ( v 1 , v 2 , . . . , v i −1 ) and it is guaranteed that a contracted particle
eventually performs a handover with the expanded particle occupying the two nodes before it on the path. So eventually q
is contracted, eventually performs a handover with p and the statement holds. 2
In the following two lemmas we will speciﬁcally consider the case that B ≤ n, i.e., the particles can coat at least one
layer around the object.
Lemma 9. If B ≤ n, layer 1 is completely ﬁlled with contracted particles eventually.
Proof. Consider a conﬁguration C such that layer 1 is not completely ﬁlled by contracted particles. Note that in this case
the leader election cannot have succeeded yet, which means that a leader cannot be elected, and therefore no particle can
be retired in conﬁguration C . So by Lemma 5 we can assume w.l.o.g. that all particles in conﬁguration C are active.
Since layer 1 is not completely ﬁlled by contracted particles, there is either at least one unoccupied node v on layer 1
or all nodes are occupied, but there is at least one expanded particle on layer 1. We show that in both cases a follower will
move to layer 1, thereby ﬁlling up the layer until all particles are contracted. In the ﬁrst case, let p be the super-root of a tree
in A (C ) that still has non-boundary particles, let ( p 0 = p , p 1 , . . . , pk ) be the boundary particles of the tree such that p i −1
occupies the node in p i .dir and let q be the non-boundary particle in the tree that is adjacent to some p j ∈ ( p 0 , . . . , pk ) such
that j is minimal. If a particle p i in ( p 0 , . . . , p j = q.parent) is expanded, it eventually contracts (Lemma 8) by a handover
with p i +1 , and by consecutive handovers all particles in ( p i +1 , . . . , p j ) eventually expand and contract until the particle
p j = q.parent expands. According to Algorithm 2, p j performs a handover with q. Therefore, the number of particles on
layer 1 has increased. If all particles in ( p 0 , . . . , q.parent) are contracted, then by Lemma 4 a complaint ﬂag still exists in the
tree. Eventually, p expands by Lemma 7. Consequently, we are back in the former case that a particle in ( p 0 , . . . , q.parent)
is expanded.

66

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

In the second case, let p 	 be an expanded boundary particle and let q	 be the non-boundary particle with the shortest
path in A (C ) to p 	 . By a similar argument as for the ﬁrst case, particles on layer 1 perform handovers (starting with p 	 )
until eventually the node in q	 .parent is occupied by a tail. Again, q	 eventually performs a handover and the number of
particles on layer 1 has increased. 2
As a direct consequence, we can show the following.
Lemma 10. If B ≤ n, a leader is elected in layer 1 eventually.
Proof. According to Lemma 9 layer 1 is eventually ﬁlled with contracted particles. Leader Election successfully elects a
leader node according to [1]. The contracted particle p occupying the leader node forwards the CHK ﬂag and eventually
receives it back, since all particles are contracted. Therefore, p becomes a leader. 2
Now we are ready to prove the two major statements of this subsection that deﬁne two conditions for system progress.
Lemma 11. If all particles are non-retired and there is either a complaint ﬂag or an expanded particle, the system eventually makes
progress.
Proof. If there is an idle particle, progress is ensured by Lemma 5. If an active particle is expanded Lemma 8 guarantees
progress. Finally, in the last case all particles are active, none of them is expanded and there is a complaint ﬂag. If layer 1
is completely ﬁlled, a leader is elected according to Lemma 10 and as a direct consequence the active particles on layer 1
eventually retire, guaranteeing progress. If layer 1 is not completely ﬁlled, there exists at least one tree of A (C ) with a
contracted super-root p that has an unoccupied node in p .dir and at least one complaint ﬂag. Therefore, progress is ensured
by Lemma 7. 2
Lemma 12. If there is at least one retired particle and one active particle, the system eventually makes progress.
Proof. Again, if there is an idle particle, progress is ensured by Lemma 5. Moreover, note that since there is at least one
retired particle, we can conclude that leader election has been successful (since the ﬁrst particle that retires is a leader
particle) and therefore layer 1 has to be completely ﬁlled with contracted particles. If there is still a non-retired particle on
layer 1, it eventually retires according to the Algorithm, guaranteeing progress.
So suppose that all particles in layer 1 are retired. We distinguish between the following cases: (i) there exists at least
one super-root, (ii) no super-root exists, but there is an expanded particle, and (iii) no super-root exists and all particles
are contracted. In case (i), Lemma 6 guarantees that a super-root will eventually enter root state, and therefore it will
eventually either expand (if p .dir is unoccupied) or retire (since p .dir is occupied by a retired particle). In case (ii), the
particle contracts according to Lemma 8. In case (iii) A (C ) forms a ring of trees, which can only happen if all boundary
particles completely occupy a single layer, so there is an active particle that occupies the node adjacent to the marker edge.
Since it is contracted by assumption, it retires upon activation. Therefore, in all three cases the system eventually makes
progress. 2
3.3. Termination
Finally, we show that the algorithm eventually terminates in a legal conﬁguration, i.e., a conﬁguration in which the
coating problem is solved. For the termination we need the following two lemmas.
Lemma 13. The number of times an idle particle is transformed into an active one and an active particle is transformed into a retired
one is bounded by O (n).
Proof. From our algorithm it immediately follows that every idle particle can only be transformed once into an active
particle, and every active particle can only be transformed once into a retired particle. Moreover, a non-idle particle can
never become idle again, and a retired particle can never become non-retired again, which proves the lemma. 2
Lemma 14. The overall number of expansions, handovers, and contractions is bounded by O (n2 ).
Proof. We will need the following fact, which immediately follows from our algorithm.
Fact 1. Only a super-root of A (C ) can expand to a non-occupied node, and every such expansion triggers a sequence of handovers,
followed by a contraction, in which every particle participates at most twice.

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

67

Consider any particle p. Note that only an active particle performs a movement. Let C be the ﬁrst conﬁguration in which
p becomes active. If it is a non-boundary particle (i.e., a follower), then consider the directed path in A (C ) from the head of
p to the super-root r of its tree or the ﬁrst particle r belonging to the ring in the ring of trees. Such a path must exist due
to Lemma 2. Let P = ( v 0 , v 1 , . . . , v m ) be a node sequence covered by this path where v 0 is the head of p in C and v m is
the ﬁrst node along that path with the object or a retired particle in its neighborhood. Note that by Lemma 2 such a node
sequence is well-deﬁned since v m must at latest be a node occupied by r. According to Algorithm 1, p attempts to follow
P by sequentially expanding into the nodes v 0 , v 1 , . . . , v m . At latest, p will become a boundary particle once it reaches v m .
Up to this point, p has traveled along a path of length at most 2n, and therefore, the number of movements p executes as
a follower is O (n).
Now suppose p is a boundary particle. Let C be the conﬁguration in which p becomes a boundary particle and let
 = p .layer. Suppose that  = 1. From our algorithm we know that at most n complaint ﬂags are generated by the particles,
and therefore by Lemma 7, there are at most n expansions in level 1 (the rest are handovers or contractions). Hence, it
follows from Fact 1 that p can only move O (n) times as a boundary particle.
Next consider the case that  > 1. Here we will need the following well-known fact.
Fact 2. Let B i be the length of layer i. For every i and every valid instance ( P , O ) allowing O to be coated by i layers it holds that
B i = B 0 + 6i.
If  = 2, there must be a retired particle in layer 1, and since the leader is the ﬁrst retired particle, Lemmas 9 and
10 imply that level  − 1 is completely ﬁlled with contracted particles. So p can only move along nodes of layer . Since
B −1 ≤ n, it follows from Fact 2 that B  ≤ n + 6. As long as not all particles in level  − 1 are retired, p cannot move beyond
the marker node in level . So p either becomes retired before reaching the marker node, or if it reaches the marker node,
it has to wait there till all particles in level  − 1 are retired, which causes the retirement of p. Therefore, p moves along
at most n + 6 nodes. If  > 2, we know from Lemma 1 that level  − 2 is completely ﬁlled with contracted particles. Since
B −2 ≤ n and B  = B −2 + 12, it follows that B  ≤ n + 12. Hence, p will move along at most n + 12 nodes in level  before
becoming retired or moving to level  − 1, and p will move along at most n + 6 further nodes in level  − 1 before retiring.
Thus, in any case, p performs at most O (n) movements as a boundary particle. Therefore, the number of movements
any particle in the system performs is O (n), which concludes the lemma. 2
Lemmas 13 and 14 imply that the system can only make progress O (n2 ) many times. Hence, eventually our system
reaches a conﬁguration in which it no longer makes progress, so the system terminates. It remains to show that when the
algorithm terminates, it is in a legal conﬁguration, i.e., the algorithm solves the coating problem.
Theorem 1. Our coating algorithm terminates in a legal conﬁguration.
Proof. From the conditions of Lemmas 11 and 12 we know that the following facts must both be true when the algorithm
terminates:
1. At least one particle is retired or there is neither a complaint ﬂag nor an expanded particle in the system (Lemma 11).
2. Either all particles are retired or all particles are active (Lemma 12).
First suppose that all particle are retired. Then it follows from Lemma 1 that the conﬁguration is legal. Next, suppose that
all particles are active and neither a complaint ﬂag nor an expanded particle is left in the system. Then Lemma 4 implies
that there cannot be any non-boundary any more, so all active particles must be boundary particles. If there is at least
one boundary particle in layer  > 1, then there must be at least one retired particle, contradicting our assumption. So all
boundary particles must be in layer 1, and since there are no more complaint ﬂags and all boundary particles are contracted,
also in this case our algorithm has reached a legal conﬁguration, which proves our theorem. 2
Recall that the work performed by an algorithm is deﬁned as the number of movements (expansions, handovers, and
contractions) of the particles till it terminates. Lemma 14 implies that the work performed by our algorithm is O (n2 ).
Interestingly, this is also the best bound one can achieve in the worst-case for the coating problem.
Lemma 15. The worst-case work required by any algorithm to solve the Universal Object Coating problem is (n2 ).
Proof. Consider the conﬁguration of particles which is a straight line connected
  to the object from an endpoint of the
line. A particle with distance i ≥ 1 to the object needs at least 2(i − 1 −
ﬁnal layer. Therefore, any algorithm requires at least 2
B ≥ 2.

2

n−1
i =1

(i − 1 −



i −1
B

i −1
B

) movements to become contracted on its
n−1
) ≥ i =1 (i − 1 − ( Bi )) = (n2 ) work assuming



68

Z. Derakhshandeh et al. / Theoretical Computer Science 671 (2017) 56–68

Hence, we get:
Theorem 2. Our algorithm requires worst-case optimal work (n2 ).
4. Conclusion
This paper presented a universal coating algorithm for programmable matter using worst-case optimal work. With minor
extensions our algorithms could allow the particles to cover only part of the surface, or to achieve some forms of sensing
(see [26] for more details). For example, once the ﬁrst layer of a coating is formed the particles could determine whether
the object is convex, or particles could determine whether the number of particles in the system is greater than or equal
to the length of the surface to be covered (i.e., whether n ≥ B). In future work it would be interesting to bound the parallel
runtime of our algorithm and to investigate its competitiveness, i.e., how does its work or runtime compare to the best
possible work or runtime for any given instance. Moreover, it would be interesting to implement the algorithm and evaluate
its performance either via simulations or hopefully at some point even via experiments with real programmable matter.
Acknowledgements
We would like to thank Joshua Daymude and Alexandra M. Porter for fruitful discussions on this topic and for helping
us review the manuscript.
References
[1] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R.A. Bazzi, A.W. Richa, C. Scheideler, Leader election and shape formation with self-organizing programmable
matter, in: DNA Computing and Molecular Programming — 21st International Conference, DNA 21, 2015, pp. 117–132.
[2] Z. Derakhshandeh, S. Dolev, R. Gmyr, A.W. Richa, C. Scheideler, T. Strothmann, Brief announcement: amoebot — a new model for programmable matter,
in: 26th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA ’14, 2014, pp. 220–222.
[3] D. Doty, Theory of algorithmic self-assembly, Commun. ACM 55 (12) (2012) 78–88.
[4] M.J. Patitz, An introduction to tile-based self-assembly and a survey of recent results, Nat. Comput. 13 (2) (2014) 195–224.
[5] D. Woods, Intrinsic universality and the computational power of self-assembly, in: Proc. Machines, Computations and Universality 2013, MCU 2013,
2013, pp. 16–22.
[6] D. Angluin, J. Aspnes, Z. Diamadi, M.J. Fischer, R. Peralta, Computation in networks of passively mobile ﬁnite-state sensors, Distrib. Comput. 18 (4)
(2006) 235–253.
[7] V. Bonifaci, K. Mehlhorn, G. Varma, Physarum can compute shortest paths, in: Proc. of SODA ’12, 2012, pp. 233–240.
[8] K. Li, K. Thomas, C. Torres, L. Rossi, C.-C. Shen, Slime mold inspired path formation protocol for wireless sensor networks, in: Proc. of ANTS ’10, 2010,
pp. 299–311.
[9] S. Wilson, T.P. Pavlic, G.P. Kumar, A. Buﬃn, S.C. Pratt, S. Berman, Design of ant-inspired stochastic control policies for collective transport by robotic
swarms, Swarm Intell. 8 (4) (2014) 303–327.
[10] M. Brambilla, E. Ferrante, M. Birattari, M. Dorigo, Swarm robotics: a review from the swarm engineering perspective, Swarm Intell. 7 (1) (2013) 1–41.
[11] I. Navarro, F. Matía, An introduction to swarm robotics, in: ISRN Robotics, Hindawi Publishing Corporation, 2012, p. 10.
[12] G.P. Kumar, S. Berman, Statistical analysis of stochastic multi-robot boundary coverage, in: Proc. of ICRA ’04, 2014, pp. 74–81.
[13] T.P. Pavlic, S. Wilson, G.P. Kumar, S. Berman, An enzyme-inspired approach to stochastic allocation of robotic swarms around boundaries, in: 16th
International Symposium on Robotics Research, ISRR 2013, 2013, pp. 16–19.
[14] L. Blázovics, K. Csorba, B. Forstner, H. Charaf, Target tracking and surrounding with swarm robots, in: IEEE 19th International Conference and Workshops
on Engineering of Computer-Based Systems, ECBS, 2012, pp. 135–141.
[15] L. Blázovics, T. Lukovszki, B. Forstner, Target surrounding solution for swarm robots, in: Information and Communication Technologies, 2012,
pp. 251–262.
[16] S. Kernbach (Ed.), Handbook of Collective Robotics — Fundamentals and Challenges, Pan Stanford Publishing, 2012.
[17] J. McLurkin, Analysis and implementation of distributed algorithms for multi-robot systems, Ph.D. thesis, Massachusetts Institute of Technology, 2008.
[18] D. Arbuckle, A. Requicha, Self-assembly and self-repair of arbitrary shapes by a swarm of reactive robots: algorithms and simulations, Auton. Robots
28 (2) (2010) 197–211.
[19] R. Cohen, D. Peleg, Local spreading algorithms for autonomous robot systems, Theoret. Comput. Sci. 399 (1–2) (2008) 71–82.
[20] S. Das, P. Flocchini, N. Santoro, M. Yamashita, On the computational power of oblivious robots: forming a series of geometric patterns, in: Proc. of the
29th Annual ACM Symposium on Principles of Distributed Computing, PODC 2010, 2010, pp. 267–276.
[21] X. Defago, S. Souissi, Non-uniform circle formation algorithm for oblivious mobile robots with convergence toward uniformity, Theoret. Comput. Sci.
396 (1–3) (2008) 97–112.
[22] T.-R. Hsiang, E. Arkin, M. Bender, S. Fekete, J. Mitchell, Algorithms for rapidly dispersing robot swarms in unknown environments, in: Proc. of the 5th
Workshop on Algorithmic Foundations of Robotics, WAFR, 2002, pp. 77–94.
[23] O. Michail, P.G. Spirakis, Simple and eﬃcient local codes for distributed stable network construction, in: ACM Symposium on Principles of Distributed
Computing, PODC ’14, 2014, pp. 76–85.
[24] http://sops.cs.upb.de.
[25] Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler, T. Strothmann, An algorithmic framework for shape formation problems in self-organizing particle
systems, in: Proceedings of the Second Annual International Conference on Nanoscale Computing and Communication, NANOCOM’ 15, Boston, MA, USA,
21-22 September 2015, 2015, pp. 21:1–21:2.
[26] Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler, T. Strothmann, Universal coating for programmable matter, CoRR arXiv:1601.01008.

Approximation Algorithms for the Mobile Piercing Set
Problem with Applications to Clustering in Ad-hoc
Networks
Hai Huang ∗

∗

Andréa W. Richa

Michael Segal

Department of Computer
Science and Engineering,
Arizona State University,
Tempe AZ, 85287-5406, USA
Tel: 1-480-965-1807
Fax: 1-480-965-2751

Department of Computer
Science and Engineering,
Arizona State University,
Tempe AZ, 85287-5406, USA
Tel: 1-480-965-7555
Fax: 1-480-965-2751

Communication Systems
Engineering Department,
Ben-Gurion University,
Beer-Sheva 84105, Israel
Tel: 972-8-6477234
Fax: 972-8-6472883

hai@asu.edu

aricha@asu.edu

segal@cse.bgu.ac.il

ABSTRACT

topology. While there is a vast literature on simulation results for the LCC and the lowest-id algorithms, these had
not been formally analysed prior to this work.
We also present an O(log n)-approximation algorithm for
the mobile piercing set problem for nonuniform disks (i.e.,
disks that may have diﬀerent radii), with constant update
time.

The main contributions of this paper are two-fold. First,
we present a simple, general framework for obtaining eﬃcient constant-factor approximation algorithms for the mobile piercing set (MPS) problem on unit-disks for standard
metrics in ﬁxed dimension vector spaces. More speciﬁcally,
we provide low constant approximations for L1 - and L∞ norms on a d-dimensional space, for any ﬁxed d > 0, and for
the L2 -norm on 2- and 3-dimensional spaces. Our framework provides a family of fully-distributed and decentralized algorithms, which adapts (asymptotically) optimally to
the mobility of disks, at the expense of a low degradation
on the best known approximation factors of the respective
centralized algorithms: Our algorithms take O(1) time to
update the piercing set maintained, per movement of a disk.
We also present a family of fully-distributed algorithms for
the MPS problem which either match or improve the best
known approximation bounds of centralized algorithms for
the respective norms and dimensions.
Second, we show how the proposed algorithms can be
directly applied to provide theoretical performance analyses for two popular 1-hop clustering algorithms in ad-hoc
networks: the lowest-id algorithm and the Least Cluster
Change (LCC) algorithm. More speciﬁcally, we formally
prove that the LCC algorithm adapts in constant time to
the mobility of the network nodes, and minimizes (up to low
constant factors) the number of 1-hop clusters maintained;
we propose an alternative algorithm to the lowest-id algorithm which achieves a better approximation factor without
increasing the cost of adapting to changes in the network

Categories and Subject Descriptors
C.2.1 [COMPUTER-COMMUNICATION NETWORKS]:
Network Architecture and Design—Wireless communication;
C.2.2 [COMPUTER-COMMUNICATION NETWORKS]:
Network Protocols; F.2.2 [ANALYSIS OF ALGORITHMS
AND PROBLEM COMPLEXITY]: Nonnumerical Algorithms and Problems—Geometrical problems and computations; F.2.m [ANALYSIS OF ALGORITHMS AND
PROBLEM COMPLEXITY]: Miscellaneous

General Terms
Algorithms, Theory

Keywords
piercing set, mobile ad-hoc networks, clustering, distributed
protocols, approximation algorithms

1. INTRODUCTION
The mobile piercing set (MPS) problem is a variation of
the (classical) piercing set problem that arises in dynamic
distributed scenarios. The MPS problem has many applications outside its main computational geometry domain, as
for example in mobile ad-hoc communication networks, as
we will see later.
We start by formalizing some basic deﬁnitions. A disk D
of radius r with center q in d with respect to Lp norm1 is
given by the set of points D = {z ∈ d : ||z − q||p ≤ r}.
Let q(D) denote the center of a disk D. A piercing set of

∗This work was supported in part by NSF CAREER Award
CCR–9985284.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Dial-M’02, September 28, 2002, Atlanta, Georgia, USA.
Copyright 2002 ACM 1-58113-587-4/02/0009 ...$5.00.

1

The Lp norm, for any ﬁxed p, of a vector z =
(z1 , z2 , · · · , zd ) in d is given by ||z||p = (|z1 |p + |z2 |p + · · · +
1
|zd |p ) p ; if p = ∞, then ||z||∞ = max(|z1 |, |z2 |, · · · , |zd |).

52

a given collection of disks D is a set of points P such that
for every disk D ∈ D, there exists a point p ∈ P such that
p ∈ D — i.e., P pierces every disk D ∈ D. The (classical)
k-piercing set problem seeks to ﬁnd whether a piercing set
P of cardinality k of D exists, and if so, produces it. If the
value of k is minimal over all possible cardinalities of piercing
sets of D then the set P is called a minimum piercing set of
D. The minimum piercing set problem asks for the minimum
piercing set of a given collection D. The piercing set problem
is closely related with the hitting set problem [19, 34].
We consider a dynamic variation of the classical piercing
set problem, which arises in mobile and distributed scenarios, where disks are moving in space. In the mobile piercing
set (MPS) problem, we would like to maintain a dynamic
piercing set P of a collection of mobile disks D such that,
at any time t, P is a minimum piercing set of the current
conﬁguration of the disks. In other words, P must adapt
to the mobility of the disks. Moreover, we would like to be
able to devise a distributed algorithm to solve this problem,
where the individual disks can decide in a distributed fashion (with no centralized control) where to place the piercing points. In this scenario, we assume that the disks are
able to detect whether they intersect other disks. We can
think about a disk as being the communication range of a
given mobile device (node), which resides at the center of
the disk: A disk can communicate with all of its adjacent
nodes by a broadcast operation within O(1) time. Below, we
will present applications of the mobile piercing set problem
in mobile networks.
In this paper, we focus on the case when the disks are all
of the same radius r — or equivalently, of same diameter 2r.
Hence, without loss of generality, in the remainder of this
paper, unless stated otherwise, we assume that 2r = 1, and
therefore that we have a collection of unit-diameter disks, or
unit-disks for short. In Section 5, we address an extension
of our algorithms to the nonuniform case, where the disks
may not all have the same radius.
In recent years, the technological advances in wireless
communications have led to the realization of ad-hoc mobile wireless networks, which are self-organizing and which
do not rely on any sort of stationary backbone structure.
These networks are expected to signiﬁcantly grow in size
and usage in the next few years. For scalability, specially
in order to be able to handle updates due to the constant
changes in network topology, clustering becomes mandatory.
As hinted above, mobile unit-disks can be used to model
an ad-hoc network where all mobile wireless nodes have the
same range of communication. Each mobile node’s communication range is represented by a disk in 2 (or 3 ) centered
at the node with radius equal to 1; a mobile node A can
communicate with mobile node B if and only if B is within
A’s communication range. The ad-hoc network scenario is a
direct application scenario for the unit-disk MPS problem,
since an ad-hoc network is fully decentralized and any algorithm running on such a network must adapt to mobility in
an eﬃcient way.
If all disks are of the same size, then the k-piercing set
problem is equivalent to the decision version of a well-known
problem: the geometric k-center problem [2]. The k-center
problem under Lp metric is deﬁned as follows: Given a set
S of n demand points in d , ﬁnd a set P of k supply points
so that the maximum Lp distance between a demand point
and its nearest supply point in P is minimized. The corre-

sponding decision problem is to determine, for a given radius
r, whether S can be covered by the union of k Lp -disks of
radius r, or in other words, to determine whether there exists a set of k points that pierces the set of n Lp -disks of
radius r centered at the points of S. In some applications,
P is required to be a subset of S, in which case the problem
is referred to as the discrete k-center problem. When we
choose the L2 metric, the problem is called the Euclidean
k-center problem, while for L∞ metric the problem is called
the rectilinear k-center problem. Since the Euclidean and
rectilinear k-center problems in 2 are NP-complete (see
e.g. [27, 30]) when k is part of the input, the planar unitdisk k-piercing set problem in 2 under L1 , L2 or L∞ norms
is also NP-complete. Unfortunately, an approximation algorithm for the k-center problem does not translate directly
into an approximation algorithm for the unit-disk piercing
set problem (and vice-versa), since an algorithm for the former problem will give an approximation on the radius of
the covering disks, while for the latter problem we need an
approximation on the number of piercing points. Still, the
two approximation factors are highly related [2].
The remainder of this paper is organized as follows. In
Section 1.1, we state our main contributions in this work.
In Section 2, we discuss more related work in the literature.
Section 3 proves some geometric properties of the piercing
set problem. We use the results in Section 3 to develop
the approximation algorithms presented in Sections 4 and
5: The algorithm introduced in Section 4 leads to lower
approximation factors, for the norms and dimensions considered, while the one in Section 5 adapts optimally to the
movement of disks. In Section 6, we relate the algorithms
presented for the MPS problem to clustering in ad-hoc networks. Finally, we present some future work directions in
Section 7.

1.1 Our results
In this paper we propose fully distributed (decentralized)
approximation algorithms for the unit-disk MPS problem
for some ﬁxed norms and dimensions. All of the approximation factors presented in this paper are with respect to the
number of points in a minimum piercing set.
For each algorithm, we are interested in computing the
cost associated with building an initial approximate piercing
set for the given initial conﬁguration of the collection of disks
— which we call the setup cost of the algorithm — and the
cost associated with updating the current piercing set due
to the movement of a disk — which we call the update cost
of the algorithm. Actually we charge the update costs per
event, as we explain below. We assume that all the costs that
do not involve communication between disks are negligible
when compared to the cost of a disk communicating with
its neighbors (through a broadcast operation). Therefore
we will only consider communication costs when evaluating
the algorithms considered.
In order to maintain an optimal or approximate piercing
set of the disks, there are two situations which mandate an
update on the current piercing set. The ﬁrst situation is
when the movement of a disk D results in having at least
one disk D of D unpierced (note that D may be D itself).
The second situation is when some piercing points in the
set maintained become “redundant”, and we may need to
remove them from the set. Thus, we say that an (update)
event is triggered (or happened) whenever one of the two

53

avoid the use of some sort of “centralization” (even if implicitly). In order to be able to apply the given framework
to a particular norm and dimension, one needs only to be
able to compute a set of piercing points which are guaranteed to pierce the immediate neighborhood of any disk D:
The number of such points will be used in bounding the
approximation factor of the algorithms proposed.
The second main contribution of this work is the application of the algorithms developed for the MPS problem to the
problem of ﬁnding an eﬃcient self-organizing one-hop underlying clustering structure for (wireless and mobile) ad-hoc
networks, as seen in Section 6. In fact, one can use the algorithms developed for the MPS problem to derive the first
theoretical performance analyses of the popular Least Cluster Change (LCC) algorithm proposed by Chiang et al. [7],
and of the lowest-id algorithm (discussed by Gerla and Tsai
in [15]), both in terms of the number of one-hop clusters
maintained and in terms of update and setup costs, thus
providing a deeper understanding of these two algorithms
and validating the existing simulation results for the same.
No previous formal analysis of either algorithm exists in the
literature. Namely, we show that the LCC algorithm has
the same approximation factor, setup and update costs as
the M-algorithm for L2 in 2 (or 3 ), and that the lowest-id
algorithm also maintains the same approximation factor as
the M-algorithm, while incurring higher update costs. We
propose an alternative algorithm that achieves same setup
and update costs as the lowest-id algorithm, but which has
better approximation factors.
Another contribution of our work addresses the MPS problem on nonuniform radius disks. Then, if the ratio between
the maximum and minimum disk radii is bounded by a polynomial on n = |D|, we present a fully-distributed O(log n)approximation algorithm for this problem, with constant update cost.

Table 1: Main results for 1D, 2D, and 3D with optimal
update costs
Space/Norm
1D
2D/L1 , L∞
2D/L2
3D/L1 , L∞
3D/L2

Approximation Factor, Setup/Update Cost
2-approximation, O(|P ∗ |)/O(1)
4-approximation, O(|P ∗ |)/O(1)
7-approximation, O(|P ∗ |)/O(1)
8-approximation, O(|P ∗ |)/O(1)
21-approximation, O(|P ∗ |)/O(1)

Table 2: Main results for 2D and 3D with better approximation factors
Space/Norm Approximation Factor, Setup/Update Cost
2D/L1 , L∞
2-approximation, O(|P ∗ |)/O(|P ∗ |)
2D/L2
4-approximation, O(|P ∗ |)/O(|P ∗ |)
3D/L1 , L∞
4-approximation, O(|P ∗ |)/O(|P ∗ |)
3D/L2
11-approximation, O(|P ∗ |)/O(|P ∗ |)

situations just described occurs.
The main contributions of this paper are two-fold. First,
we present a family of constant-factor approximation algorithms — represented by the M-algorithm — for the unitdisk MPS problem with (asymptotically) optimal setup and
update costs, for all the norms and space dimensions considered. Moreover, we achieve this without a signiﬁcant
increase in the approximation factor of the corresponding
best known approximation algorithms for the classical piercing set problem. Let P ∗ be a minimum piercing set. More
speciﬁcally, in d dimensions, we devise a 2d -approximation
algorithm under L1 or L∞ . For L2 norm, we devise a sevenapproximation algorithm in 2 , and a 21-approximation algorithm in 3 . All these algorithms have O(|P ∗ |) setup cost
and O(1) update cost. Note that any dynamic algorithm
that approximates the minimum piercing set of a collection
of mobile disks has setup cost Ω(|P ∗ |), and update cost Ω(1).
These algorithms are the ﬁrst constant-approximation algorithms for the unit-disk MPS problem, with asymptotically
optimal setup and update costs. We summarize these results
in Table 1.2
We also present a second family of fully distributed algorithms — represented by the A-algorithm — for L1 or L∞
norms in any space d , and for L2 norm in 2 and 3 . These
algorithms achieve the same, or better, constant approximation factors as the best known centralized algorithms for the
corresponding norm and space dimension, but have a poorer
update cost of O(|P ∗ |). These algorithms are, to the best
of our knowledge, the ﬁrst fully distributed (decentralized)
approximation algorithms which achieve the same approximation factors as their centralized counterparts. These algorithms are of interest since, for example, they provide an
alternative algorithm to the lowest-id clustering algorithm
in ad-hoc networks, which would achieve a four- (resp., 11-)
approximation factor in 2 (resp., 3 ) without an increase
on setup and update costs. We summarize these results in
Table 2.2
The simple framework presented for the M-algorithm, which
can handle mobility eﬃciently in a dynamic scenario, is an
important contribution of this work on its own. It avoids
the use of involved data structures, which in general cannot

2. RELATED WORK
The k-center and k-piercing problems have been extensively studied. In d dimensions, a brute-force approach leads
to an exact algorithm for the k-center problem with running
time O(ndk+2 ). For the planar case of the√Euclidean k-center
problem, Hwang et al. [25] gave an nO( k) time algorithm
improving Drezner [9] solution which runs in time O(n2k+1 ).
An algorithm with the same running time was presented in
Hwang et al. [24] for the planar discrete Euclidean k-center
problem. Recently, Agarwal and Procopiuc [1] extended and
simpliﬁed the technique by Hwang et al. [25] to obtain an
1−1/d )
time algorithm for computing the Euclidean knO(k
center problem in d dimensions.
Sharir and Welzl [33] explain a reduction from the rectilinear k-center problem to the k-piercing set problem (under
L∞ metric), using a sorted matrix searching technique developed by Frederickson and Johnson [13]. Ko et al. [27]
proved the hardness of the planar version of the rectilinear
k-center and presented an O(n log n) time 2-approximation
(on the covering radius) algorithm. (In fact, Ko et al. [27]
proved that, unless P = N P , the best approximation factor
that can be achieved in polynomial time for the rectilinear
k-center problem is 2.) Several approximation results (on
the radii of the disks) have been obtained in [11, 17, 21, 22].
For more results on the k-center problem, please refer to [2].
Regarding the k-piercing set problem in d , Fowler et

2
All the results are for unit-disks; Lp is equivalent to L∞
for any p in one dimension.

54

al. [12] proved the NP-completeness of ﬁnding the minimum value of k for a given set of n disks. Hochbaum and
d
Maas [20] gave an O(ld n2l +1 ) polynomial time algorithm
for the minimum piercing set problem with approximation
factor (1 + 1l )d for any ﬁxed integer l ≥ 1. Thus, for l = 1,
their algorithm yields an O(n3 ) time (sequential) algorithm
with performance ratio 2d . For the one-dimensional case,
Katz et al. [26] presented an algorithm that maintains the
exact piercing set of points for a collection of n intervals
in O(|P ∗ | log n) time, where P ∗ is a minimum piercing set.
Their solution can be adapted to obtain an algorithm with
distributed running time O(|P ∗ |) for computing a minimum
piercing set of n intervals. Nielsen [31] proposed a 2d−1 approximation algorithm that works in d-dimensional space
under L∞ metric in O(dn + n log c) time, where c is the size
of the piercing set found. This algorithm is based on the
divide-and-conquer paradigm.
Although not stated explicitly, the approximation on the
radius of the k-center problem in [1] implies a four-approximation
algorithm for the minimum piercing set problem for 2 and
L2 . Efrat et al. [10] introduced a dynamic data structure
based on segment trees which can be used for the piercing set
problem. They presented a sequential algorithm which gives
a constant factor approximation for the minimum piercing
set problem for “fat” objects with polynomial setup and update time. See [10] for the deﬁnition of “fatness” and more
details.
A large number of clustering algorithms have been proposed and evaluated through simulations in the ad-hoc network domain, as for example in [3, 4, 15, 28, 29, 32]. Gerla
and Tsai in [15] considered two distributed clustering algorithms, the lowest-id algorithm and the highest-degree algorithm, which select respectively the lowest-id mobile or the
highest degree mobile in a one-hop neighborhood as the clusterhead. A weight oriented clustering algorithm, more suitable to “quasi-static” networks, was introduced by Basagni
[4], where one-hop clusters are formed according to a weightbased criterion that allows the choice of the nodes that
coordinate the clustering process based on node mobilityrelated parameters. In [28], Lin and Gerla described a nonoverlapping clustering algorithm where clusters are able to
be dynamically reconﬁgured.
The LCC algorithm proposed by Chiang et al. [7] aims to
maintain a one-hop clustering of a mobile network with least
number of changes in the clustering structure, where clusters
will be broken and re-clustered only when necessary. In fact,
our algorithm for the MPS problem, when translated to a
clustering algorithm in the ad-hoc scenario, is essentially the
LCC algorithm, as discussed in Section 6.
Recently, researchers have investigated using geometric
centers as clusterheads in order to minimize the maximum
communication cost between a clusterhead and the cluster
members. Bepamyatnikh et al. [6] discussed how to compute
and maintain the one-center and the one-median for a given
set of n moving points on the plane (the one-median is a
point that minimizes the sum of all distances to the input
points). Their algorithm can be used to select clusterheads
if mobiles are already partitioned into clusters.
Gao et al. [14] proposed a randomized algorithm for maintaining a set of clusters based on geometric centers, for a
ﬁxed radius, among moving points on the plane. Their algorithms have expected approximation factor on the optimal
number of centers (or, equivalently, of clusters) of c1 log n for

55

√
intervals and of c2 n for squares3 , for some constants c1 and
c2 . The probability that there are more than c ln n times the
2
optimal number of centers is 1/nΘ(c ) for the case of intervals; for squares, the probability that there are more than
√
2
c n ln n times the optimal number of centers is 1/nΘ(c ) ln n ,
for constant c. An extension of this basic algorithm led
to a hierarchical algorithm, also presented in [14], based
on kinetic data structures [5]. The hierarchical algorithm
admits an expected constant approximation factor on the
number of discrete centers, where the approximation factor also depends linearly on the constants c1 and c2 . The
dependency of the approximation factor and the probability that the algorithm chooses more than a constant times
the optimal number of centers is similar to that of the nonhierarchical algorithm for the squares case. The constants
c1 and c2 , which have not been explicitly determined in [14],
can be shown to be very large (certainly more than an order
of magnitude larger than the corresponding approximation
constant presented in this paper), even if we allow the probability of deviating from the expected constant approximation on the number of centers (which depends linearly on
c1 and c2 ) not to be close to one. Their algorithm has an
expected update time of O(log3.6 n) (while the update cost
is constant in our algorithm), the number of levels used in
the hierarchy is O(log log n), with O(n log n log log n) total
space.
Har-Peled [18] found a scheme for determining centers in
advance, if the degree of motion of the nodes is known: More
speciﬁcally, if in the optimal solution the number of centers is k and r is the optimal radius for the points moving
with degree of motion , then his scheme guarantees a 2	+1 approximation (of the radius) with k	+1 centers chosen from
the set of input points before the points start to move.

3. GEOMETRY FOR THE PIERCING SET
PROBLEM
In this section, we prove some geometric properties of the
minimum piercing set problem. More speciﬁcally, we solve
the minimum piercing set problem on the neighborhood of a
disk, which will provide the basic building block for our approximation algorithms presented in the following sections.
The main step of the approximation algorithms is to select
an unpierced unit-disk and pierce all of its neighbors. By
repeating this procedure, we will eventually pierce all unitdisks and form a piercing set. The approximation factors
are determined by the number of piercing points chosen for
each selected unpierced unit-disk.
If two disks D and D intersect, we say that D is a neighbor of D . The neighborhood of a disk D, denoted by N (D),
is deﬁned as the collection of all disks that intersect D,
N (D) = {D : D ∩ D = ∅, D ∈ D}. Note that D ∈ N (D).
We are interested on the minimum number of points that
pierce all disks in the neighborhood of a given disk. However,
this number may vary, depending on the distribution of the
disks in the particular neighborhood in consideration. Thus,
we compute the minimum number (along with the ﬁxed positions) of points needed to pierce any possible neighborhood
of a disk. This number is called the neighborhood piercing
number. The neighborhood piercing number is tight in the
3
Disks in 1D correspond to intervals on the line; in 2D, Disks
under L∞ or L1 are called squares.

sense that for any set of points with smaller cardinality, we
can ﬁnd some conﬁguration of the neighborhood of a disk
which has an unpierced disk. The corresponding piercing
points are called the neighborhood piercing points. Clearly,
the piercing number is a function of both dimension d and
norm index p. Hence, we denote the neighborhood piercing
number for dimension d and norm index p as N (d, p), and
we use P N (D, d, p) to denote a corresponding set of neighborhood piercing points of a unit-disk D. 4 We prove in this
section that N (d, 1) = N (d, ∞) = 2d for all d ≥ 1, and that
N (2, 2) = 7. We also place an upper bound of 21 on N (3, 2).
For each of the norms and dimensions considered, we give a
corresponding set of neighborhood piercing points.
First we reduce the minimum piercing set problem into
an equivalent disk covering problem. Let D be a collection
of unit-disks and P be a set of points. Let P  be the set of
centers of all disks in D, and D be a collection of unit-disks
centered at points in P . Then P pierces D if and only if D
covers P  . Moreover, P is a minimum piercing set for D if
and only if D is a minimum set of disks (with respect to
cardinality) that covers P  . We deﬁne the unit-disk covering
problem to be the problem of ﬁnding the minimum k such
that there are k unit-disks whose union covers a given point
set.
We now reduce the problem of ﬁnding the neighborhood
piercing number to a unit-disk covering problem as follows.
For a unit-disk D, all the centers of unit-disks in N (D) are
located in the region G = G(D) = {z : ||z − q(D)||p ≤ 1},
where q(D) is the center of D. Conversely, a disk centered
at any point in G must intersect D. Therefore, we seek
for the minimum number of unit-disks that cover region G.
The centers of those disks serve as the set of neighborhood
piercing points P N (D). The tightness of N can be seen
from the fact that in the disk covering problem, we cannot
cover the entire region G with less than N disks, as proven
in the following lemma. Note that the region G is a disk of
radius 1, and that all of the disks that we use to cover G are
unit-disks (i.e., of radius 12 ).
For any Lp norm, in a d-dimensional space, the ratio of
the area of G to the area of a unit-disk is 2d . Thus, we
need at least 2d disks to cover G — i.e., N ≥ 2d for any
dimension d ≥ 1 and any norm Lp . The lower bound of 2d
is in fact tight for p = 1 or p = ∞, since in any dimension
d, the unit-disk D has 2d “corners” under these norms, and
the set of unit-disks centered at those “corners” cover the
entire region G.
The case p = 2 is more involved since we cannot pack
“spheres” as tightly as “hypercubes” without leaving uncovered points in the region G, if no intersection of disks is
allowed. Figure 1-(a) shows an optimal 7-disk covering with
disks centered at q,r,s,t,u,v and w, for the region G under
L2 norm in a 2 . Since we need at least six disks to cover
the boundary of region G, which is a circle of perimeter 2π,
those seven disks form a minimum cardinality covering. If
q = (x, y) is the center of the unit-disk
D, the coordinates
√
√
of the six other points are (x ± 34 , y ± 43 ), (x, y ± 23 ). This
completes the proof of Lemma 1 (a more formal proof of this
lemma can be found in [23]).

v
w

u

D
q

r

D
q

t

D
q

D’
q’

r

s

t q’
s

(a)
(b)
Figure 1: Piercing points for the neighborhood of (a)
an arbitrary disk (b) a top disk
l

l
D

r
D

Figure 2: Piercing points for the neighborhood of a
rightmost interval and an arbitrary interval
neighborhood piercing number for 2-dimension and L2 is
equal to 7.
For L2 norm in 3 , we were only able to place an upper
bound on the number of unit-disks needed to cover a disk
of diameter 2, hence placing an upper bound on N (3, 2).
A simple argument [23] suﬃces to verify that 20 unit-disks
centered at some evenly spaced points on the surface of G
plus a unit-disk D centered at the origin cover a disk G of
diameter two also centered at the origin. Hence we have
N (3, 2) ≤ 21. It remains an open problem to compute the
exact value of N (3, 2). The neighborhood piercing number
for L2 is closely related to the sphere packing and sphere
covering problems described in [8].
When compared to the results in the literature, the approximation factors based on the neighborhood piercing points
are not the best known. For example, we have shown that
N (1, ·) = 2, which leads to a two-approximation algorithm
for piercing unit-intervals on the line (see Section 5). In [16,
p. 193] (see also [26]), an exact solution (i.e., one-approximation)
for piercing unit-intervals is proposed. The idea there, shown
in Figure 2, is to start from the rightmost interval D, where
only one endpoint of D — the left endpoint l — is enough for
piercing all neighbors of D (since D has no neighbor to its
right). In order to be able to extend and generalize this idea
to other norms and higher dimensions, we need to deﬁne, the
halfspace of a disk D with orientation n, denoted by HD (n):
HD (n) = {z : (z − q(D)) · n ≥ 0}. For the one-dimensional
case, all of the centers of the neighboring disks of the rightmost interval D are located in the half space HD (−1) (to
the “left” of D), and only half of the neighborhood piercing points (i.e., only N (1)/2 points) are enough for piercing
N (D). More generally, in any d-dimensional space, there
exists an orientation n, such that we need roughly half of
the neighborhood piercing points to pierce all the neighbors
of disk D located in HD (n). The minimum number of piercing points needed for the halfspace HD (n), over all possible
orientations n, is called the halfspace neighborhood piercing
number, and is denoted by N . The set of corresponding
piercing points are called the halfspace neighborhood piercing points of D and are denoted by P N (D).
If P N (D) is symmetric with respect to the center of the
unit-disk D, then N =  N2  if the center of D does not belong to P N , or N =  N+1
 otherwise. Note that this is
2
the case for P N (d, 1), P N (d, ∞) and P N (2, 2). The set of
piercing points which correspond to the upper bound of 21

Lemma 1. The neighborhood piercing number is equal to
2d for d-dimensional space under L1 - and L∞ -norm. The
4
In general, we omit the parameters p, d, or D, whenever
clear from the context.

56

casting has O(1) cost, the running time of the distributed
A-algorithm is O(|P ∗ |). Theorem 1 states the main properties of the A-algorithm. This theorem actually extends
the results in [26] and in [31] — for L1 and L∞ norms in
d-dimensional spaces — to a more general distributed scenario, and also to the L2 norm in two- and three-dimensional
space.
We re-invoke the A-algorithm to maintain the piercing set
every time an event (as deﬁned in Section 1.1) happens. In
a distributed scenario, this can be done by ﬂooding a reset
message to unpierce all disks. Thus the update cost of the
A-algorithm is also O(|P ∗ |).

for N (3, 2) is not symmetric, but we can still ﬁnd an orientation such that 11 points are enough to pierce the halfspace
neighborhood of a disk with respect to the orientation. Figure 1-(b) illustrates halfspace neighborhood piercing points
— points q,r,s and t — for 2 under L2 norm. The orientation considered is n = (0, −1). Table 3 summarizes some
values of neighborhood piercing number and that on halfspace for lower dimensions and norms L1 , L∞ and L2 , where
we denote the minimum of N (n) as N and corresponding
P N (n) as P N . It follows from the upper bound on N (3, 2)
that N ≤ 11 for L2 and 3 . The corresponding halfspace
neighborhood piercing points are also a subset of the points
used for establishing the upper bound on N (3, 2). It also
remains an open question to determine the exact value of N
for L2 and 3 .
For an orientation n, if we order all unit-disks D in D according to the values q(D)· n, then a unit-disk D bearing the
smallest q(D)·n value satisﬁes the property that all its neighbors are located in the halfspace HD (n). Thus, by carefully
choosing the order in which we consider the neighborhoods
of disks to be pierced, we can use the halfspace neighborhood piercing points as the basis of the fully-distributed algorithms for the MPS problem presented in Section 4, which
match or improve the best known approximation factors of
the respective centralized algorithms.
The problem of computing N for other Lp metrics is more
involved and may not have many practical applications. A
method to estimate an upper bound on N and compute
the corresponding set of neighborhood piercing points for
arbitrary Lp metrics is discussed in [23] for completeness.

4.

Theorem 1. The approximation factor of the distributed
A-algorithm is N , and its setup and update costs are both
O(|P ∗ |).
Proof. For each piercing unit-disk D, we need at least
one point in the minimum piercing set P ∗ to pierce D. For
any two distinct piercing unit-disks D and E, the point in
P ∗ that pierces D cannot pierce E since no two (distinct)
piercing disks intersect. Thus we have at most |P ∗ | piercing
unit-disks. For each piercing unit-disk, we select N piercing
points. Hence the approximation factor follows. It takes
constant time to pierce the neighborhood of each piercing
unit-disk using a broadcast operation. Hence the running
time for both setup and update operations is O(|P ∗ |).

5. BETTER HANDLING OF MOBILITY
We now present the M-algorithm, a fully distributed constant approximation algorithm for the mobile piercing set
problem that adapts optimally to the mobility of disks: The
update cost of the M-algorithm is O(1). We break the Malgorithm into two parts: the M-Setup algorithm, which
builds an initial piercing set, and the M-Update algorithm,
which is in charge of adapting the piercing set maintained in
response to the mobility of disks (we will see later that the
M-Update algorithm may initiate a local call to M-Setup
as a subroutine at some of the disks). The M-algorithm
is more suitable for highly dynamic ad-hoc mobile network
scenarios.

BETTER APPROXIMATION FACTORS

In this section we present a family of constant-factor fullydistributed (decentralized) approximation algorithms for the
piercing set problem, which at least match the best known
approximation factors of centralized algorithms for the respective norms and dimensions. This algorithm introduces
some basic concepts which will be useful when developing the algorithms in Section 5. Also, the algorithm presented in this section directly translates into an alternative
to the lowest-id clustering algorithm for ad-hoc networks
(discussed in Section 6) which achieves a better approximation factor on the number of clusters maintained. The
algorithms in this section all follow a general algorithmic
framework, which we call the A-algorithm (for having better approximation factors) in contrast with the slightly looser
approximation factors of the other family of algorithms presented in Section 5 (represented by the M-algorithm) which
can better handle mobility.
Consider a set of unit-disks in a d-dimensional space under norm Lp . As shown in Section 3, we need at most N
piercing points to pierce the neighborhood of a unit-disk D
bearing the smallest q(D) · n among the (unpierced) disks
in its neighborhood, where n is an orientation that gives N .
We call such a disk D a top disk. Thus, at each step of the
algorithm, each top unpierced disk D elects itself as a piercing disk and selects the points in P N (D) as piercing points.
Since all the unpierced disks in N (D) are now pierced by
P N (D), we mark all the unpierced disks in N (D) as pierced,
and repeat the procedure above. After repeating this step
for at most |P ∗ | times, all the unit-disks in D are pierced
and a piercing set with cardinality at most N times |P ∗ | is
produced, as shown in Theorem 1. Provided that broad-

4

3

2

1

Before Movement
4’

Figure 3:

3’

2’

1’

After Movement

The movement of the rightmost interval
changes all piercing points

The key idea behind the M-algorithm is to break the
sequential running fashion of the A-algorithm. In the Aalgorithm, an ordering of the unit-disks is mandatory (even
if implicitly). As shown in Figure 3, in the worst-case, the
movement of one disk (the rightmost one in the ﬁgure) could
lead to a global update of all selected piercing disks, while the
cardinality of the minimum piercing set does not change. In
order to maintain a relatively stable piercing set, the desired
algorithm needs to be able to sever this “cascading eﬀect”

57

Table 3: Neighborhood piercing number and that on halfspace in 1D and 2D
N

1D
2

2D/L1
4

2D/L∞
4

2D/L2
7

PN
N

2 endpoints
1

4 “corners”
2

4 “corners”
2

{(0, 0), (± 34 , ± 43 ), (0, ±
4

PN

n

left endpoint
−1

left & top “corners”
(1, −1)

2 bottom “corners”
(0, −1)

{(0, 0), (± 34 , − 43 ), (0,
(0, −1)

√

√

√

3
)}
2

√

3
)}
2

(i), or at disks D and D for events of type (ii). The MUpdate procedure can be divided into two phases: In the
ﬁrst phase, we will unmark some of the disks as to be now
unpierced; in the second phase, we select piercing disks for
those unpierced disks. The second phase is executed by a
local call to M-Setup initiated at each unpierced disk.
The details of the M-Update procedure are as follows. If
we have an event of type (i), the M-Update will degrade disk
D to a normal disk and unpierce all disks that were currently
pierced by D (including D itself). Otherwise, if case (ii) applies, the M-Update will simply unpierce disk D . Each node
that is marked unpierced by the M-Update procedure will
invoke M-Setup locally. The M-Setup procedure invoked at
an unpierced disk F will ﬁrst check if any of its neighbors is
a piercing disk. If so, it marks itself pierced. Otherwise, if F
has the lowest label among its unpierced neighbors, it elects
itself as a piercing disk and marks all its unpierced neighbors as pierced. The M-Setup and M-Update algorithms are
shown in Figure 4.
As proven in Theorem 3, all unit-disks will be pierced
at the end of the calls to M-Setup, and the approximation
factor on the size of the piercing set maintained is still guaranteed to be N .

— i.e., the algorithm needs to be able to keep the updates
local. Lemma 2 shows that the cardinality of an optimal
piercing set cannot change by much, due to the movement
of a single disk. This property suggests that an update can
be kept local. This property guarantees that an update can
be kept local. The proof of this lemma is trivial, and can be
found in [23] for completeness.
Lemma 2. If at one time only one unit-disk moves, then
||P ∗ | − |P ∗∗ || ≤ 1, where P ∗ denotes a minimum piercing
set before the movement, and P ∗∗ denotes a minimum piercing set after the movement.
In the M-Setup algorithm, instead of choosing a disk with
respect to the ordering given by a direction n, we select arbitrary unpierced disks as piercing disks in each step, then
pierce the neighborhood of each selected disk D using the
points in P N (D). By repeating this procedure O(|P ∗ |)
times, we will generate a piercing set for D: Since now we
use N points to pierce the neighborhood of each selected
piercing disk, the approximation factor is roughly doubled
compared to that of the A-algorithm. However, this small
degradation in the approximation factor pays for an optimal
update strategy, as will be shown later.
In order to implement the above idea in a distributed fashion, we repeat the following procedure. Each disk D ﬁrst
checks if there are any piercing disks in its neighborhood.
If so, then D marks itself as pierced. Otherwise, each unpierced disk tries to become a piercing disk itself. In order
to guarantee that only one disk becomes a piercing disk in
an unpierced disk’s neighborhood — this is a key property
for proving the approximation factor of this algorithm —
a mechanism such as “lowest labeled neighbor wins” (assuming that each disk has a unique identiﬁcation label) is
required. Note that, unlike the A-algorithm, in the M-Setup
algorithm disks do not need to know the disks’ coordinates
(since no comparisons of the q(D) · n values are required),
which may be desirable in an ad-hoc network scenario. The
proof of Theorem 2 is analog to that of Theorem 1, and is
therefore omitted.

Theorem 3. The M-Update procedure maintains an N approximation of the MPS, with update cost of O(1) for each
event.
Proof. First we show that the running time of M-Update
is constant per event. Assume that at one time only one
event occurs. All the disks possibly aﬀected by the event
are located in the neighborhood of a disk D. Thus the operation of marking disks as unpierced (in the ﬁrst phase)
takes constant time. Since all nodes that invoked a call to
M-Setup were neighbors of a former piercing disk D, it follows that the calls to M-Setup will have at most a constant
number, N , of rounds of “lowest labeled neighbor wins” until
a valid set of piercing disks is restored. Therefore the total
time taken by each of the invoked M-Setup calls also takes
constant time. If several events occur at the same time, then
the ﬁnal eﬀect is the same as if a sequence of events occurs
in a row, and the update cost per event remains the same.
Now we show that the approximation factor maintained is
equal to N . Clearly the resulting piercing set is determined
by the collection of selected piercing unit-disks. We will
show that the updated collection of piercing disks produced
by the M-Update procedure could have been the initial collection of piercing disks produced by the M-Setup algorithm
(for a given ordering of the labels of the disks), thus proving
the claimed approximation factor. Assume that the collection E = {D1 , · · · , Dm } of selected piercing unit-disks before
the call to M-Update is invoked is an N -approximation on
the MPS. Let E  be the collection of selected piercing unitdisks after the call to M-Update is completed at all nodes

Theorem 2. The M-Setup generates a piercing set of cardinality within a factor of N of |P ∗ | in O(|P ∗ |) time.
As disks start moving in space, each disk needs to be
able to trigger an update procedure whenever an update
is necessary. To facilitate the following discussion, we call
a disk that is not a piercing disk a normal disk. When
a disk moves, the following events may make the current
piercing set invalid and trigger an update: (i) the boundaries
of two piercing disks D and E meet (thus D may become a
redundant piercing disk); (ii) the boundaries of one piercing
disk D and some normal disk D pierced by D separate
(thus at least one of the disks becomes unpierced). An MUpdate procedure is initiated at disk D in events of type

58

M-Setup
For each unmarked unit-disk D:
1. Repeat
2.
If there is piercing unit-disk in N (D) then
3.
IsMarked=True
4.
Elseif D bears the lowest label among all its
neighbors which attempt to become a
piercing disk then
5.
For each unmarked neighbor D  of D
6.
D’.IsMarked=True
7.
End
8.
End
9. Until the disk becomes marked

(which may involve calling the M-Setup algorithm locally).
One of the following four cases may occur:
Case 1. A normal unit-disk D moves and after the movement, it is still pierced by some piercing unit-disk in E . In
this case, the M-Update procedure never invokes M-Setup at
a node, and E  = E . Since we still need at least one piercing
point to pierce each of the selected piercing disks (no two
piercing disks overlap) and since E was an N -approximation
of the MPS, the approximation factor still holds.
Case 2. A normal unit-disk D moves, and after the movement, D is no longer pierced by a piercing unit-disk in
E . In this case, the M-Setup procedure invoked by the
call to M-Update will upgrade D to a piercing disk. Thus
E  = E ∪ {D }.
We prove the bound on the size of the piercing set maintained by showing that E  could have been obtained by a
general call to the M-Setup algorithm to the current conﬁguration (placement in space) of the disks if all disks were
currently unpierced, for a given assignment of labels to the
disks. Suppose that the labels of the disks in E  are smaller
than the labels of all other disks in D, and that label(D1 ) <
. . . < label(Dm ) < label(D ). Thus, on or before step i ≤ m,
disk Di will be selected by M-Setup to become a piercing
disk (since all Di ’s were unpierced disks in E initially, and no
two piercing disks intersect). After all disks D1 , . . . , Dm are
selected, only disk D is not pierced. Thus M-Setup must
select D to be a piercing disk. Hence E  is obtained, proving
the N -approximation factor on the cardinality of the set of
piercing points produced.
Case 3. A piercing unit-disk D moves and after the movement, D is pierced by some other piercing unit-disk E ∈
E . Let D = Di , where Di ∈ E . The M-Update will degrade D to a normal disk and unpierce all unit-disks previously pierced by D. The M-Update procedure then invokes local calls to M-Setup at all unpierced disks. For
each unit-disk D previously pierced by D, M-Setup will
ﬁrst check if there is another piercing disk that pierces D .
If so, D will be marked pierced. Otherwise, if there are
neighbors of D which still remain unpierced, then the MSetup algorithm will upgrade some normal disks to piercing disks. Let E  = {Dm+1 , · · · , Dm+k } be the collection of those upgraded piercing disks. Then we have E  =
(E − {Di }) ∪ E  as the new set of piercing disks. As in Case
2, if label(D1 ) < . . . < label(Di−1 ) < label(Di+1 ) < . . . <
label(Dm+k ) < label(E), for all disks E not in E  , the MSetup algorithm when applied to the current conﬁguration
of the disks in D, assuming all disks are unpierced at start,
will produce E  as the resulting set of piercing disks. Thus
the N -approximation factor follows.
Case 4. A piercing unit-disk D moves and after the movement, D is not pierced by any other piercing unit-disk E ∈ E .
Essentially the same as Case 3, but for the fact that we do
not degrade D to a normal disk.

M-Update
When a unit-disk D moves
1. If D is a piercing unit-disk, then
2.
If D’s boundary meets that of another piercing
unit-disk E, then
3.
remove the neighborhood piercing points of
D from P
4.
Unmark D and all normal unit-disks that
were marked by D
5.
End
6.
If D’s boundary separates from that of a normal
unit-disk D  , then
7.
Unmark D 
8.
End
9. Else (D is a normal unit-disk)
10.
If D’s boundary separates from that of D’s
piercing unit-disk, then
11.
Unmark D
12.
End
13. Call M-Setup

Figure 4: The M-Setup and M-Update algorithm
Corollary 1. By grouping the disks into O(log n) classes
such that each class contains disks of radii in [2i−1 , 2i ), we
have an O(log n) approximation for the MPS problem on
nonuniform disks with distributed update cost of O(1).
Proof. In each class, as we show below, N 2 points are
enough to pierce an arbitrary neighborhood. Since we have
O(log n) classes, and the piercing set for each class is a N 2 approximation of the overall minimum piercing set, the approximation factor is bounded by O(log n). Once a disk
moves, it only aﬀects the piercing set selected for one class,
thus the update cost is still constant. We now show that
N 2 points are in fact enough for covering a disk of diameter
2i+2 , using disks of diameter in [2i , 2i+1 ). In the worst case,
we need to cover a region of diameter 2i+2 with disks of diameter 2i . We can do this in two phases. First we cover the
region using N disks of diameter 2i+1 . Then for each disk
D of diameter 2i+1 , we cover D using N disks of diameter
2i .

A simple extension of the M-algorithm provides a polylog
approximation algorithm for the nonuniform case. If the collection contains disks of various radii, then we can guarantee
an N -approximation if at each step we ﬁnd the unpierced
disk of smallest radii in the collection and pierce all of its
neighborhood. However, we cannot guarantee having O(1)
update cost in this case. Without loss of generality, assume
that the minimum radius of a disk is equal to 1. If the
largest disk radius is bounded by a polynomial on n = |D|,
then we have the following Corollary:

6. APPLICATIONS TO CLUSTERING IN MOBILE NETWORKS
For the ad-hoc network scenario described in the introduction, where all nodes have equal range of communication, the
algorithms proposed for the mobile piercing set problem can

59

factor maintained is given by the A-algorithm presented in
Section 4 (when translated to an ad-hoc network scenario).
Since an update caused by a single event in the lowest-id
algorithm may incur Θ(|P ∗ |) cost, we may as well take advantage of the ordering of the disks used by the A-algorithm
and get a better approximation guarantee on the number of
clusters maintained.

be directly applied in order to obtain a one-hop clustering
of the network. A clustering of a network G is a partition
of the nodes of G into subsets (clusters) Ci , where for each
Ci , we elect a node v ∈ Ci as its clusterhead. A one-hop
clustering of G is a clustering of G such that every node in
the network can communicate in one-hop with the clusterhead of the cluster it belongs to. We can view the network
G as a collection of unit-disks in 2 (resp., 3 ) under L2 (as
discussed in the introduction).
The algorithm in Section 5 can be used to obtain an almost optimal (with respect to number of clusters) one-hop
clustering of a wireless network where all nodes have equal
communication range. We have that |P ∗ |/N (2, 2) = |P ∗ |/7
(resp., |P ∗ |/N (3, 2) ≥ |P ∗ |/21) is a lower bound on the
minimum number of one-hop clusters (and therefore on the
number of selected clusterheads) needed to cover the entire
network, since we need at least one piercing point for each
of the neighborhoods of a piercing disk, and since we use
at most seven (resp., 21) piercing points for each of these
neighborhoods in a MPS in 2 (resp., 3 ). The number of
piercing disks selected by the algorithm in Section 5 is at
most |P ∗ |. Since each of these piercing disks D corresponds
uniquely to a one-hop cluster C in the network (given by
all the disks pierced by D), and since the union of all these
clusters covers the entire network, we have that the number
of clusters is at most |P ∗ |, which is a seven-approximation
(resp., 21-approximation) on the minimum number of onehop clusters needed in 2 (resp., 3 ). This algorithm is also
suitable for maintaining such an optimal structure as nodes
start moving in space, with optimal update costs. The algorithm tends to keep the number of changes in the set of
selected clusterheads low.
In fact, the algorithm presented in Section 5, when translated to a clustering algorithm on ad-hoc networks, is essentially the same as the Least Cluster Change (LCC) algorithm
presented by Chiang et al. [7]. Therefore, in this paper we
provide a theoretical analysis of the performance of this popular clustering algorithm, validating the simulation results
that showed that the clusters maintained by this algorithm
are relatively stable. More speciﬁcally, we have proved that
this algorithm sustains a seven-approximation on the number of one-hop clusters maintained, while incurring optimal
setup and update costs.
A closer look at the lowest-id algorithm, investigated by
Gerla and Tsai in [15], shows that this algorithm corresponds to several applications of the M-Setup procedure of
Section 5. Every time a disk becomes unpierced, or two
piercing disks intersect, the lowest-id algorithm starts updating the clustering cover maintained in a fashion that may
correspond to an application of the M-Setup algorithm on
the current conﬁguration of the disks if all disks were unpierced — in the worst-case, the lowest-id algorithm may
generate a “cascading eﬀect” which correspond to an application of the M-Setup algorithm on a collection of all
unpierced disks, if the disk labels are given by the node
ids. Thus the setup and the worst-case update costs of the
lowest-id algorithm are both O(|P ∗ |), and the approximation on the number of clusters maintained is equal to seven
and 21, for 2 and 3 respectively.
If the mobile nodes know their coordinates in space, an alternative algorithm to the lowest-id algorithm, which achieves
same setup and update costs, but which has a better guarantee (of four in 2 , and 11 in 3 ) on the approximation

7. FUTURE WORK
There are many natural extensions of the work in this
paper. We would like to extend the one-hop clustering
structure to a full network clustering hierarchy. One idea
would be to apply the same algorithm presented to construct O(log n) clustering covers of the network: Clustering
i would be obtained by assuming that all disks have radius
equal to 2i , for i = 0, . . . , log n, where n = |D|. One problem with this strategy is that by artiﬁcially increasing the
communication ranges on the nodes in the network (radii
of the disks), a resulting cluster in the hierarchy may not
even be connected. Other directions for future work are (i)
to develop constant approximation algorithms for piercing
a collection of disks of diﬀerent radii; (ii) to extend any
results on nonuniform radius disks to ad-hoc network clustering — note that if we have nonuniform radius disks, we
can no longer guarantee symmetric communication between
nodes in the network; and (iii) to determine the exact neighborhood piercing number for L2 norm in three- (or higher)
dimensional spaces.

8. ACKNOWLEDGEMENT
We would like to express our thanks to Martin Ziegler for
valuable discussions on estimating N (3, 2).

9. REFERENCES
[1] Pankaj K. Agarwal and Cecilia M. Procopiuc. Exact
and approximation algorithms for clustering. In Proc.
9th ACM-SIAM Sympos. on Discrete Algorithms,
pages 658–667, 1998.
[2] Pankaj K. Agarwal and Micha Sharir. Eﬃcient
algorithms for geometric optimization. ACM Comput.
Surv., 30:412–458, 1998.
[3] S. Basagni. Distributed and mobility-adaptive
clustering for multimedia support in multi-hop
wireless networks. In Proc. IEEE Vehicular Tech.
Conf., pages 19–22, 1999.
[4] S. Basagni. Distributed clustering for ad-hoc
networks. In Proc. 1999 Int. Sympos. on Parallel
Architectures, pages 310–315, 1999.
[5] J. Basch, Leonidas J. Guibas, and J. Hershberger.
Data structures for mobile data. In Proc. 8th
ACM-SIAM Sympos. on Discrete Algorithms, pages
747–756, 1997.
[6] S. Bepamyatnikh, B. Bhattacharya, D. Kirkpatrick,
and M. Segal. Mobile facility location. In Proc. ACM
Int. Workshop on Discrete Algorithms and Methods
for Mobile Computing and Communications, pages
46–53, 2000.
[7] C.-C. Chiang, H.-K. Wu, W. Liu, and M. Gerla.
Routing in clustered multihop, mobile wireless
networks with fading channel. In Proc. IEEE
Singapore Int. Conf. on Networks, pages 197–211,
1997.

60

[8] J. H. Conway and N. J. A. Sloane. Sphere packings,
lattices, and groups. Springer, 1999.
[9] Zvi Drezner. The p-center problem: heuristic and
optimal algorithms. J. Oper. Res. Soc., 35:741–748,
1984.
[10] Alon Efrat, Matthew J. Katz, Franck Nielsen, and
Micha Sharir. Dynamic data structures for fat objects
and their applications. In Proc. Workshop on
Algorithms and Data Structures, pages 297–306, 1997.
[11] T. Feder and D. H. Greene. Optimal algorithms for
approximate clustering. In Proc. 20th Annu. ACM
Sympos. on Theory of Computing, pages 434–444,
1988.
[12] R. J. Fowler, M. S. Paterson, and S. L. Tanimoto.
Optimal packing and covering in the plane are
NP-complete. Inform. Process. Lett., 12(3):133–137,
1981.
[13] G. N. Frederickson and D. B. Johnson. Generalized
selection and ranking: sorted matrices. SIAM J.
Comput., 13:14–30, 1984.
[14] J. Gao, L. J. Guibas, J. Hershburger, L. Zhang, and
A. Zhu. Discrete mobile centers. In Proc. 17th ACM
Sympos. on Computational Geometry, pages 188–196,
2001.
[15] M. Gerla and J. T. C. Tsai. Multicluster mobile
multimedia radio networks. ACM-Baltzer J. Wireless
Networks, 1(3):255–256, 1995.
[16] M. Golumbic. Algorithmic Graph Theory. Academic
Press, New York, 1980.
[17] T. Gonzalez. Covering a set of points in
multidimensional space. Inform. Process. Lett.,
40:181–188, 1991.
[18] Sariel Har-Peled. Clustering motion. In Proc. 42nd
Annu. IEEE Sympos. on Foundations of Computer
Science, pages 84–93, 2001.
[19] R. Hassin and N. Megiddo. Approximation algorithms
for hitting objects by straight lines. Disc. Appl. Math.,
30:29–42, 1991.
[20] D. S. Hochbaum and W. Maas. Approximation
schemes for covering and packing problems in image
processing and vlsi. J. ACM, 32:130–136, 1985.
[21] D. S. Hochbaum and D. Shmoys. A best possible
heuristic for the k-center problem. Math. Oper. Res.,
10:180–184, 1985.
[22] D. S. Hochbaum and D. Shmoys. A uniﬁed approach

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

61

to approximation algorithms for bottleneck problems.
J. ACM, 33:533–550, 1986.
H. Huang, A. W. Richa, and M. Segal. Approximation
algorithms for the mobile piercing set problem with
applications to clustering. Technical Report
TR-01-007, Dept. of Computer Sci. and Eng., Arizona
State University, Tempe, AZ, 2001.
R. Z. Hwang, R. C. Chang, and R. C. T. Lee. The
generalized searching over separators strategy to solve
some np-hard problems in subexponential time.
Algorithmica, 9:398–423, 1993.
R. Z. Hwang, R. C. T. Lee, and R. C. Chang. The
slab dividing approach to solve the euclidean p-center
problem. Algorithmica, 9:1–22, 1993.
Matthew J. Katz, Frank Nielsen, and Michael Segal.
Maintenance of a piercing set for intervals with
applications. In Proc. 11th Int. Symp. on Algorithms
and Computation, pages 552–563, 2000.
M. T. Ko, R. C. T. Lee, and J. S. Chang. An optimal
approximation algorithm for the rectilinear m-center
problem. Algorithmica, 5:341–352, 1990.
C. R. Lin and M. Gerla. Adaptive clustering for
mobile wireless networks. IEEE J. on Sel. Areas in
Comm., 15(7):1265–1275, 1997.
A. B. McDonald and T. Znati. A mobility-based
framework for adaptive clustering in wireless ad-hoc
networks. IEEE J. on Sel. Areas in Comm., 17(8),
1999.
N. Megiddo and K. J. Supowit. On the complexity of
some common geometric location problems. SIAM J.
Comput., 13(1):182–196, 1984.
F. Nielsen. Fast stabbing of boxes in high dimensions.
In Proc. 8th Canad. Conf. in Computational
Geometry, pages 87–92, 1996.
R. Ramanathan and M. Steenstrup.
Hierarchically-organized, multihop mobile wireless for
quality-of-service support. Mobile Networks and
Applications, 3:101–119, 1998.
Micha Sharir and Emo Welzl. Rectilinear and
polygonal p-piercing and p-center problems. In Proc.
12th Annu. ACM Sympos. on Computational
Geometry, pages 122–132, 1996.
D. Zuckerman. NP-complete problems have a version
that’s hard to approximate. In Proc. 8th Annu. Struct.
Complexity Theory Conf., pages 305–312, 1993.

Theory Comput. Systems 32, 241–280 (1999)

Theory of
Computing
Systems
©

1999 Springer-Verlag
New York Inc.

Accessing Nearby Copies of Replicated Objects
in a Distributed Environment∗
C. G. Plaxton,1 R. Rajaraman,2 and A. W. Richa3
1 Department

of Computer Science, University of Texas,
Austin, TX 78712, USA
plaxton@cs.utexas.edu

2 College

of Computer Science, Northeastern University,
Boston, MA 02115, USA
rraj@ccs.neu.edu

3 Department

of Computer Science and Engineering, Arizona State University,
Tempe, AZ 85287, USA
aricha@asu.edu

Abstract. Consider a set of shared objects in a distributed network, where several
copies of each object may exist at any given time. To ensure both fast access to the
objects as well as efficient utilization of network resources, it is desirable that each
access request be satisfied by a copy “close” to the requesting node. Unfortunately, it
is not clear how to achieve this goal efficiently in a dynamic, distributed environment
in which large numbers of objects are continuously being created, replicated, and
destroyed.
In this paper we design a simple randomized algorithm for accessing shared
objects that tends to satisfy each access request with a nearby copy. The algorithm
is based on a novel mechanism to maintain and distribute information about object
locations, and requires only a small amount of additional memory at each node.
∗

A preliminary version of this paper appeared in the Proceedings of the 9th Annual ACM Symposium on
Parallel Algorithms and Architectures (SPAA), pages 311–320, June 1997. The first author was supported by
the National Science Foundation under Grant No. CCR-9504145. Part of this work was done when the second
author was at the University of Texas at Austin, with support from the National Science Foundation under Grant
No. CCR-9504145, and when he was at DIMACS Center, Rutgers University. DIMACS is an NSF Science
and Technology Center, funded under Contract STC-91-19999 and partially supported by the New Jersey
Commission on Science and Technology. Part of this work was done while the third author was at Carnegie
Mellon University, supported by National Young Investigator Award under Grant No. CCR-94-57766, and
ARPA Contract F33615-93-1-1330.

242

C. G. Plaxton, R. Rajaraman, and A. W. Richa

We analyze our access scheme for a class of cost functions that captures the hierarchical nature of wide-area networks. We show that under the particular cost model
considered (i) the expected cost of an individual access is asymptotically optimal,
and (ii) if objects are sufficiently large, the memory used for objects dominates the
additional memory used by our algorithm with high probability. We also address
dynamic changes in both the network and the set of object copies.

1.

Introduction

The advent of high-speed distributed networks has made it feasible for a large number
of geographically dispersed computers to cooperate and share objects (e.g., files, words
of memory). Indeed, the last few years have seen the emergence of large distributed
databases such as the World Wide Web. The distributed nature of the databases and the
rapidly growing demands of the users have in turn overloaded the underlying network
resources (e.g., links, memory space at the processors, buffer space at the links and
processors). In an attempt to minimize communication delays and to satisfy as many
users as possible, strategies for making efficient use of network resources when providing
access to shared objects have been devised.
As one might expect, the task of designing efficient algorithms for supporting access
to shared objects over wide-area networks is challenging, both from a practical and a
theoretical perspective. With respect to any interesting measure of performance (e.g.,
latency, throughput), the optimal bound achievable by a given network is a complex
function of many parameters, including edge delays, edge capacities, buffer space, communication overhead, and patterns of user communication. Ideally, we would like to take
all of these factors into account when optimizing performance with respect to a given
measure. However, such a task may not be feasible in general because the many network
parameters interact in a complex manner. For this reason, we adopt a simplified model
in which the combined effect of the detailed network parameter values is assumed to
be captured by a single function that specifies the cost of communicating a fixed-length
message between any given pair of nodes.
Accessing Shared Objects. Consider a set A of m objects being shared by a network
G, where several copies of each object may exist. A shared object may be replicated in
order to improve fault-tolerance or performance, for example. In this paper we consider
the basic problem of reading objects in A. Motivated by the need for efficient network
utilization, we seek algorithms that minimize the cost of the read operation. We do not
address the write operation, which involves the additional consideration of maintaining
consistency among the various copies of each object. The problem of consistency, although an important one, is separate from our main concern, namely, that of studying
locality. Our results for the read operation apply for the write operation in scenarios
where consistency is either not required or enforced by an independent mechanism.
We differentiate between shared and unshared copies of objects. A copy is shared
if any node can read this copy; it is unshared if only the node that holds the copy may
read it. We say that a node u inserts (resp., deletes) a copy of object A (that u holds) if
u declares the copy shared (resp., unshared).

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

243

We refer to the set of algorithms for read, insert, and delete operations as an access
scheme. Any access scheme that efficiently supports these operations incurs an overhead
in memory. It is desirable that this overhead be small, not only because of space considerations, but also because low overhead usually implies fast adaptability to changes in
the network topology or in the set of object copies.
The main source of difficulty in designing an access scheme that is efficient with
respect to both time and space is the competing considerations of these measures. For
example, consider an access scheme in which each node stores the location of each copy
of each object in the network. This allows very fast read operations since a node can
easily determine the location of the closest copy of any desired object. However, such an
access scheme is impractical because it incurs a prohibitively large memory overhead,
and every node of the network has to be informed whenever a copy of an object is
inserted or deleted. At the other extreme, one might consider an access scheme using no
additional memory. In this case insert and delete operations are fast, but read operations
are costly since it may be necessary to search the entire network in order to locate a copy
of some desired object.
Our Access Scheme. We design a simple randomized access scheme that exploits locality and distributes control information to achieve low memory overhead. The central
part of our access scheme is a mechanism to maintain and locate the addresses of copies
of objects. For a single object, say A, we can provide such a mechanism by the following
approach. We embed an n-node “virtual” height-balanced tree T one-to-one into the network. Each node u of the network maintains information associated with the copies of A
residing in the set of nodes that form the subtree of T rooted at u. Given the embedding
of T , the read operation may be easily defined as follows. When a node u attempts to
read A, u first checks its local memory for a copy of A or information about copies of A
in the subtree of T rooted at u. If this local check is unsuccessful, u forwards the request
for object A to its parent.
Naive extensions of the above approach to account for all objects require significant overhead in memory for control information at individual nodes. We overcome
this problem by designing a novel method to embed the different trees associated with
different objects. Our embedding enables us to define simple algorithms for the read,
insert, and delete operations, and to prove their efficiency for a class of cost functions
that is appropriate for modeling wide-area networks.
One important property of our access scheme is that it does not require locationdependent naming of the copies of the objects. Thus it avoids renaming a copy of an
object every time this copy migrates to another location in the network. Keeping track
of replicated copies of the same object may also pose a problem if we have locationdependent naming of the objects, since copies of the same object located at different
addresses in the network will have different names. Other properties of our scheme for
the restricted class of cost functions considered are that (i) due to its distribution of
control information and of shared data, our scheme is expected to avoid “hot-spots”
in the network (i.e., heavily accessed nodes); and (ii) due to its distribution of data,
combined with its support for object replication and fast adaptability to changes in the
network, our scheme is expected to scale well. Scalability is one of the most important
problems to be solved in today’s large-scale networks; for example, the World Wide

244

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Web, in spite of using scalable components (e.g., clients, servers, TCP/IP connections,
DNS), has serious problems of scalability as a whole.
The Cost Model. As indicated above, we assume that a given function determines the
cost of communication between each pair of nodes in the network. Our analysis is geared
toward a restrictive class of cost functions which we believe to be of practical interest.
The precise set of assumptions that we make with respect to the cost function are stated
in Section 2. Our primary assumption is that for all nodes x and costs r , the ratio of the
number of nodes within cost 2r of node x to the number of nodes within cost r of node x
is bounded from above and below by constants greater than 1 (unless the entire network
is within cost 2r of node x, in which case the ratio may be as low as 1).
There are some important observations we can make concerning this primary assumption on the cost function. First, a number of commonly studied fixed-connection
network families lead naturally to cost functions that satisfy this assumption. For example, fixed-dimension meshes satisfy this assumption if the cost of communication
between two nodes is defined as the minimum number of hops between them, and
constant-degree trees satisfy this assumption if the cost of communication between two
nodes is given by the distance between these nodes in a physical layout (e.g., a wide-area
layout, or a VLSI layout) of the tree.
Following the latter example, fat-tree topologies [12] satisfy our assumption if the
cost of communication between two nodes is determined by the total cost of a shortest path
between the nodes, where the cost assigned to individual edges grows at an appropriate
geometric rate as we move higher in the tree. Fat-trees are of particular interest here,
because of all the most commonly studied fixed-connection network families, the fattree captures the hierarchical structure of most wide-area networks, and may provide the
most plausible approximation to the structure of current such networks.
Even so, it is probably inappropriate to attempt to model the Internet, say, with any
kind of uniform topology, including the fat-tree. Note that our assumption on the cost
function is purely “local” in nature, and allows for the possibility of a network with a
highly irregular global structure. This may be the most important characteristic of our
cost model.
Performance Bounds. We show that our access scheme achieves optimality or nearoptimality in terms of several important complexity measures for the restricted class of
cost functions discussed above. In particular, our scheme achieves the following bounds:
• The expected cost for any read request is asymptotically optimal.
• The expected cost of an insert (resp., delete) operation at node u is O(C) (resp.,
O(C log n)), where C is the maximum cost of communicating a single word
message between any two nodes.
• If the number of objects that can be stored at each node is q, then the additional
memory required at each node is O(q log2 n) words whp,1 where a word is an
1 We use the abbreviation “whp” throughout the paper to mean “with high probability” or, more precisely,
“with probability 1 − n −c , where n is the number of nodes in the network and c is a constant that can be set
arbitrarily large by appropriately adjusting other constants defined within the relevant context.”

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

245

O(log n)-bit string. Thus, if the objects are sufficiently large—i.e., Ä(log2 n)
words—the memory for objects dominates the additional memory.
• The number of nodes that need to be updated upon the addition or removal of a
node is O(log n) expected and O(log2 n) whp.
An obvious shortcoming of our analysis is that it only applies to the restricted class of
cost functions discussed above. While we do not expect that all existing networks fall
precisely within this restricted class, we stress that (i) our access scheme is well defined,
and functions correctly, for arbitrary networks, and (ii) we expect that our access scheme
would have good practical performance on any existing network. (Although we have not
attempted to formalize any results along these lines, it seems likely that our performance
bounds would only degrade significantly in the presence of a large number of nontrivial
violations of our cost function assumptions.)
Related Work. The basic problem of sharing memory in distributed systems has been
studied extensively in different forms. Most of the earlier work in this area assumes that
each of the nodes of the network has knowledge of a hash function that indicates the
location of any copy of any object. Examples of such work include PRAM emulation
schemes for completely connected distributed-memory machines (e.g., [11] and [19]) or
bounded-degree networks (e.g., [17]), and algorithms for providing concurrent access to
a set of shared objects [16].
The basic problem of locating an object arises in every distributed system [14],
and was formalized by Mullender and Vitányi [15] as an instance of the distributed
matchmaking problem. Awerbuch and Peleg [3], and subsequently Bartal et al. [5], and
Awerbuch et al. [1] give near-optimal solutions in terms of cost to a related problem by
defining sparse-neighborhood covers of graphs. Their studies do not address the overhead
due to control information, and, hence, natural extensions of their results to our problem
may require an additional memory of m words at some node. However, we note that
their schemes are designed for arbitrary cost functions, whereas we have focused on
optimizing performance for a restricted class of cost functions.
In [4] Awerbuch and Peleg examine the problem of maintaining a distributed directory server, that enables keeping track of mobile users in a distributed network. This
problem can be viewed as an object location problem, where objects migrate in the
network.
In recent work, access schemes for certain Internet applications have been described
in [9], [10], and [20]. Some of the ideas in our scheme are similar to those in [20];
however, the two schemes differ considerably in the details. Moreover, the schemes of
[9] and [20] have not been analyzed. As in our study, the results of [10] concerning
locality assume a restricted cost model. However, their cost model, which is based on
the ultrametric, is different from ours. Also, their algorithms are primarily designed for
problems associated with “hot spots” (i.e., popular objects).
In [13] Maggs et al. investigate both the problem of determining the placement of
copies of the objects in the network, and the problem of devising an efficient access
scheme, with the main goal of keeping the edge congestion low. Their work considers
cost models that arise in some restricted network topologies, such as trees, meshes, and
clustered networks.

246

C. G. Plaxton, R. Rajaraman, and A. W. Richa

A closely related problem is that of designing a dynamic routing scheme for networks [2], [7]. Such a scheme involves maintaining routing tables at different nodes
of the network in much the same way as our additional memory. However, in routing
schemes the size of additional memory is a function of network size n, while in our problem the overhead is primarily a function of m, the number of objects. Straightforward
generalizations of routing schemes result in access schemes that require an additional
memory of m words at each node.
The remainder of this paper is organized as follows. Section 2 defines the model
of computation. Section 3 gives an informal overview of our access scheme. Section 4
presents a formal description of our access scheme. Section 5 contains a formal statement of the main results. Section 6 gives an informal overview of the analysis of our
access scheme. Section 7 presents some preliminary definitions that are used in our analysis. Sections 8 and 9 present a formal analysis and establish the main results. Finally,
Section 10 discusses directions for future research.

2.

Model of Computation

We consider a set V of n nodes, each with its own local memory, sharing a set A
of m = poly(n) objects. We define our model of computation by characterizing the
following aspects of the problem: (i) objects, (ii) communication, (iii) local memory,
(iv) local computation, and (v) complexity measures.
Objects. Each object A has a unique (log m)-bit ID. For i in [log m], let Ai denote
the ith bit of the ID of A.2 Each object A consists of `(A) words, where a word is an
O(log n)-bit string.
Communication. Nodes communicate with one another by means of messages; each
message consists of at least one word. We assume that the underlying network supports
reliable communication.
We define the cost of communication by a function c: V 2 7→ R. For any two nodes
u and v in V , c(u, v) is the cost of transmitting a single-word message from u to v.
We assume that c is symmetric and satisfies the triangle inequality. We also assume for
simplicity that, for u, v, and w in V , c(u, v) = c(u, w) if and only if v = w. (We make
the latter assumption for the sake of convenience only, and with essentially no loss in
generality, since an arbitrarily small perturbation in the cost function can be used to
break ties.)
The cost of transmitting a message of length ` from node u to node v is given by
f (`)c(u, v), where f : N 7→ R+ is any nondecreasing function such that f (1) = 1.
Given any u in V and any real r , let M(u, r ) denote the set {v ∈ V : c(u, v) ≤ r }.
We refer to M(u, r ) as the ball of radius r around u. We assume that there exist real
constants δ > 8 and 1 such that for any node u in V and any real r ≥ 1, we have
min{δ|M(u, r )|, n} ≤ |M(u, 2r )| ≤ 1|M(u, r )|.
2

For any positive integer x, we use [x] to denote the set {0, . . . , x − 1}.

(1)

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

247

Local Memory. We partition the local memory of each node u into two parts. The first
part, the main memory, stores objects. The second part, the auxiliary memory, is for
storing possible control information.
Local Computation. There is no cost associated with local computation. (Although the
model allows an arbitrary amount of local computation at zero cost, our algorithm does
not perform any particularly complex local operations.)
Complexity Measures. We evaluate any solution on the basis of four different complexity measures. The first measure is the cost of reading an object. The second measure
is the size of the auxiliary memory at any node. The remaining two measures concern
the dynamic nature of the problem: We address the complexity of inserting or deleting a
copy of an object, and of adding or removing a network node. The third measure is the
cost of inserting or deleting a copy of an object. The fourth measure is the adaptability,
which is defined as the number of nodes whose auxiliary memory is updated upon the
addition or removal of a node. (Our notion of adaptability is analogous to that of [7].)

3.

Informal Overview of the Access Scheme

The purpose of this section is to provide an informal high-level description of our access
scheme. A formal description of the access scheme is given in Section 4. (The reader
interested only in the formal description may skip the present section without loss of
continuity.)
We begin by considering a simplified version of our access scheme. In this simplified
scheme, each of the n nodes of the network is assigned a (log n)-bit label uniformly at
random. (Recall that each node also has a unique (log n)-bit ID which is independent
of this label.) These node labels are then used to construct a “neighbor table” at each
node. For each node x, each integer k between 1 and log n, and each k-bit string α, the
neighbor table at node x stores the (log n)-bit ID of the closest node y to x—i.e., the node
with minimum c(x, y)—such that the k-bit suffix of the label of y is equal to α. We call
this node y the (k, α)-neighbor of x. (If no such node y exists, then let k 0 be the largest
integer such that the label of some node matches α in the k 0 lowest bit positions, let S
denote the set of such nodes, and define the (k, α)-neighbor of x as the closest node in S
to x.) Thus the neighbor table of any node has a large number of entries, namely 2(n).
This large table size is the main technical deficiency of the simplified access scheme; for
ease of reference, we refer to this deficiency as Problem 1 in the present section. As we
develop our (nonsimplified) access scheme we will address Problem 1 by reducing the
size of the neighbor table to 2(log n). However, we ignore Problem 1 for the moment
and continue to consider the simplified scheme. Throughout the following discussion,
let A be an arbitrary object and let αk denote the k-bit suffix of the unique ID of object A.
To insert a copy of object A at node x, we place a pointer to this copy at up to log n
nodes of the network. (To delete a copy, we remove this set of pointers.) These nodes
are determined as follows: For each k between 1 and log n, we place a pointer at the
(k, αk )-neighbor of x. The probability that the k-bit suffix of the label of an arbitrary node
matches a particular k-bit string αk is 2−k . Consider a ball B around node x containing

248

C. G. Plaxton, R. Rajaraman, and A. W. Richa

exactly 2k nodes. Note that there is a constant probability (approximately 1/e) that no
node in B matches αk . Thus the radius of B is a lower bound (up to a constant factor) on
the expected distance from x to its (k, αk )-neighbor. Is this radius also an upper bound?
Not for an arbitrary metric, since (for example) the diameter of the smallest ball around
x containing 2k + 1 nodes can be arbitrarily larger than the diameter of the smallest ball
around x containing 2k nodes. Under the assumption of the left inequality of (1), however,
it can be shown that the radius of B provides a tight bound on the expected distance
from x to its (k, αk )-neighbor. Furthermore, the right inequality of (1) implies that the
expected distance from x to its (k, αk )-neighbor is geometrically increasing in k. The
latter observation turns out to be useful because it implies that the expected total distance
from x to its (i, αi )-neighbor, summed over all i such that 0 ≤ i ≤ k, is dominated by
the expected distance from x to its (k, αk )-neighbor, up to constant factors.
We now describe how to process a read request for object A at a node x in our
simplified access scheme. If x already has a copy of A, then it processes the read request
locally. Otherwise, x processes the read request as follows, where xk denotes the (k, αk )neighbor of x, 1 ≤ k ≤ log n. First, x consults x1 . If x1 has a pointer to a copy of A that
is no more than a constant factor further away from x1 than x, then x1 returns a pointer
to such a copy to x, x retrieves this copy, and the read is complete. If x1 does not have
such a pointer, x consults x2 , and so on. (In general, if xi has a pointer to a copy of A that
is no more than a constant factor further away from xi than x, then xi returns a pointer
to such a copy to x, x retrieves this copy, and the read is complete.) A deficiency of this
scheme is that we may fail to locate a pointer to A at x1 through xlog n even though there
are one or more copies of A in the network. Fortunately, this deficiency may be easily
rectified by making a slight modification to the definition of xlog n . Thus we ignore this
issue in the present informal discussion.
In general, if x completes the read operation for A by using a pointer found at xk , the
total cost of the read is proportional to the sum of the distances from x to xi , 1 ≤ i ≤ k,
and the distance from x to the copy of A retrieved. By the definition of the algorithm the
latter distance cannot dominate. Furthermore, as discussed earlier, the expectation of the
summation is dominated by the expected distance from x to xk . Our main objective is
to prove that the expected cost of the read operation is optimal, that is, proportional to
the distance to the closest copy of object A. Thus, it is sufficient for us to prove that the
distance from x to the closest copy of object A is proportional to the expected distance
from x to xk . Let y denote the node holding the closest copy of A to x, and let j be the least
integer such that the expected distance from x to x j is at least the distance from x to y.
The intuition underlying our claim that the expected distance from x to xk is proportional
to the distance from x to y is that, for all i ≥ j, there is a reasonable chance that xi is
the (i, αi )-neighbor of y (in addition to x). Our approach to formalizing this intuition
involves showing that, under the assumption of (1), the sets of nodes contained in two
balls of radius 2(r ) centered at nodes r apart (e.g., at x and y) have a substantial (i.e.,
constant fraction) intersection. We then argue that there is a constant probability that the
read operation terminates successfully at each successive node xi such that i ≥ j. Such
an argument yields the desired main theorem (which addresses the expected optimal cost
for the read operation) as long as the probability of success at each successive iteration is
sufficiently close to 1 to offset the geometrically increasing expected cost of visiting each
successive xi . (For example, if the expected distance from x to x i is 2i , then the success

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

249

probability has to be strictly greater than 1/2.) In the discussion that follows, we refer to
the problem of achieving a sufficiently large constant success probability as Problem 2.
The foregoing completes our description and discussion of our simplified access
scheme. We now sketch the design of our (nonsimplified) access scheme. We focus the
discussion on our approach to overcoming Problems 1 and 2, the two main technical
problems identified above. As mentioned above, we address Problem 1 by replacing the
O(n)-entry neighbor tables defined above with O(log n)-entry tables. The basic idea
underlying this exponential reduction in the table size is straightforward. Consider a ball
containing a set S of 2k nodes, and notice that for any (log n)-bit string α, many of the
(log n, α)-neighbors of the nodes in set S are likely to be the same. In fact, for any i ≥ k
and any i-bit string α, many of the (i, α)-neighbors of the nodes in set S are likely to
be the same. Furthermore, to the extent that such (i, α)-neighbors differ, it is not clear
that these differences are crucial to the performance of the algorithm. These observations
suggest that it may be possible to reduce the table size by allowing clusters of nodes such
as S to share common values. In Section 4 we present a simple method for achieving this
kind of sharing across clusters. An interesting and important feature of this method is that
it does not explicitly partition the network into clusters. (Such a global partition might
be difficult to maintain efficiently in a highly dynamic network.) Instead, our method is
completely distributed and derives its efficiency from simple properties of the randomly
assigned node labels. For example, we exploit the fact that in any fixed set of 2k nodes,
the expected number of occurrences of any given k-bit suffix is 1: It follows that the
random node labels can be used to partition the “shared” table entries within any cluster.
Reducing the table size in the above manner solves Problem 1 but introduces another
technical difficulty which we now describe and refer to as Problem 3. Using the revised
tables, it turns out that the algorithm for performing a read operation is changed in a
significant manner. In the simplified scheme described above, x successively contacts
nodes x 1 , x2 , etc. until a suitable pointer to the desired object A is found. The actual
access scheme is similar except that if a suitable pointer is not found at a node z, then
the read request is forwarded to a suitable neighbor of z, as opposed to a neighbor of x.
(The placement of pointers on an insertion is modified in a similar manner.) As a result,
the sequence of nodes along which the read request is forwarded in our access scheme
is different from the sequence of nodes x1 , x2 , etc., that is used in the simplified access
scheme. We denote the ith node in the new sequence by xi0 . Unfortunately, this change in
the access scheme tends to increase the expected number of iterations until a given read
operation succeeds. Informally, the reason for this increase is that the neighbors of xi0 are
defined in terms of distance from xi0 , and for the purposes of a read operation originating
at node x, distance from x is a more useful metric than distance from xi0 . The problem of
bounding the increase in the expected cost of a read operation due to forwarding, which
we refer to in the present section as Problem 3, is one of the main technical challenges
faced in our analysis.
Aside from the reduced neighbor tables alluded to above, there are two main differences between the simplified access scheme described in this section and the access
scheme of Section 4. These differences are designed to help us overcome Problems 2
and 3, respectively. In the following paragraphs we briefly sketch these differences and
provide some intuition underlying their relevance to Problems 2 and 3.
The first difference is that, in the access scheme of Section 4, we maintain a suf-

250

C. G. Plaxton, R. Rajaraman, and A. W. Richa

ficiently large constant number of neighbors in each table entry, rather than a single
neighbor. In other words, rather than storing the ID of the closest node with a label
satisfying a certain condition, we store the IDs of the d + 1 closest nodes satisfying this
condition, for some sufficiently large constant d. Accordingly, when a copy of an object
is inserted, we place pointers to the copy at d +1 times as many nodes as in the simplified
scheme. Furthermore, at each iteration of our algorithm for processing a read request,
we search for an appropriate pointer to a copy of the object at d + 1 nodes instead of
one. By leaving more pointers and expanding our search for pointers in this manner,
iterations at which a read operation would otherwise have had only a small constant
success probability can now have a success probability close to 1. Such an improvement
in the success probability is clearly helpful for dealing with Problem 2. Furthermore,
the cost of this change in the access scheme is at most a constant factor in terms of both
time and space.
The second difference is that, in the access scheme of Section 4, we view the random
node labels as base-2b numbers for some sufficiently large constant b, rather than as base2 numbers. Thus, for example, the read operation proceeds in dlog n/be iterations rather
than in log n iterations. As in the simplified access scheme, the ratio between the expected
0
and the expected distance from x to xi0 is a constant greater than 1,
distance from x to xi+1
i.e., these distances are geometrically increasing. However, for b > 0, this ratio is a much
larger constant. Increasing this ratio is helpful for dealing with Problem 3, informally
because the effect of forwarding becomes negligible: The distance from x to xi0 tends to
0
that the ratio between the distance
be so much smaller than the distance from x to xi+1
0
from x to xi+1 (which is important to analyze when bounding the cost of a read from x)
0
(which is easier to analyze because it is more closely
and the distance from xi0 to xi+1
related to the table entries used by the algorithm, due to forwarding) is typically close to
1. Consequently, we are able to show that the “drift” of the sequence x10 , x20 , etc., from the
sequence that we would get if requests were not forwarded and instead sent to neighbors
of x (as in the simplified scheme) is small enough that the increase in the expected cost
of a read operation due to forwarding is small.
4.

The Access Scheme

In this section we present our access scheme for shared objects. We assume that n is a
power of 2b , where b is a fixed positive integer to be specified later (see the beginning
of Section 7). For each node x in V , we assign a label independently and uniformly at
random from [n]. For i in [log n], let x i denote the ith bit of the label of x. Note that
the label of a node x is independent of the unique (log n)-bit ID of the node. For all x in
V (resp., A in A ), we define x[i] as the nonnegative integer with binary representation
x (i+1)b−1 · · · x ib (resp., A[i] denotes A(i+1)b−1 · · · Aib ), for i in [(log n)/b]. We also assign
a total order to the nodes in V , given by the bijection β: V → [n].
We partition the auxiliary memory of each node into two parts, namely the neighbor
table and the pointer list of the node.
• Neighbor table. For each node x, the neighbor table of x consists of (log n)/b
levels. The ith level of the table, i in [(log n)/b], consists of primary, secondary,
and reverse (i, j)-neighbors, for all j in [2b ].

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

Fig. 1.

251

The primary neighbor table of node x, for b = 1.

The primary (i, j)-neighbor y of x is such that y[k] = x[k] for all k in [i],
and either (i) i < (log n)/b − 1 and y is the node of minimum c(x, y) such that
y[i] = j, if such a node exists, or (ii) y is the node with the largest β(y) among
all nodes z such that z[i] matches j in the largest number of rightmost bits. Note
that the primary (i, j)-neighbor of a node x is guaranteed to exist, since x itself
is a candidate node. Let d be a fixed positive integer, to be specified later (see the
beginning of Section 7). Let y be the primary (i, j)-neighbor of x. If y[i] = j,
then let Wi, j denote the set of nodes w in V \{y} such that w[k] = x[k] for all
k in [i], w[i] = j, and c(x, w) is at most O(c(x, y)). Otherwise, let Wi, j be the
empty set.
The set of secondary (i, j)-neighbors of x is the subset U of min{d, |Wi, j |}
nodes u with minimum c(x, u) in Wi, j —that is, c(x, u) is at most c(x, w) for all
w in Wi, j \U , and for all u in U . Finally, a node w is a reverse (i, j)-neighbor of
x if and only if x is a primary (i, j)-neighbor of w.
In Figure 1 we illustrate the primary neighbors entries in the neighbor table of
node x for b = 1; suppose the level i neighbors of x in the table are given by (i)
above.
• Pointer list. Each node x also maintains a pointer list Ptr(x) with pointers to
copies of some objects in the network. Formally, Ptr(x) is a set of triples (A, y, k),
where A is in A, y is a node that holds a copy of A, and k is an upper bound on the
cost c(x, y). We maintain the invariant that there is at most one triple associated
with any object in Ptr(x). The pointer list of x may only be updated as a result of
insert and delete operations. All of the pointer lists can be initialized by inserting
each shared copy in the network at the start of the computation. We do not address
the cost of initializing the auxiliary memories of the nodes.
Let r be the node whose label matches (in terms of binary representation) the ID
of A in the largest number of rightmost bits. (In case of a tie between several nodes
r0 , . . . , rk , let r be the unique node ri maximizing β(ri ).) We call r the root node for
object A. The uniqueness of the root node for each A in A is crucial to guarantee the
success of every read operation.

252

C. G. Plaxton, R. Rajaraman, and A. W. Richa

In this section and throughout the paper, we use the notation hαik to denote the
sequence α0 , . . . , αk (of length k + 1). When clear from the context, k will be omitted.
In particular, a primary neighbor sequence for A is a maximal sequence huik such that
u 0 is in V , u k is the root node for A, and u i+1 is the primary (i, A[i])-neighbor of u i , for
all i. It is worth noting that the sequence huik is such that the label of node u i satisfies
(u i [i − 1], . . . , u i [0]) = (A[i − 1], . . . , A[0]), for all i. We now give an overview of the
read, insert, and delete operations.
• Read. Consider a node x attempting to read an object A. The read operation
proceeds by successively forwarding the read request for object A originating
at node x along the primary neighbor sequence hxi for A with x0 = x. When
forwarding the read request, node xi−1 informs xi of the current best upper bound
k on the cost of sending a copy of A to x. On receiving the read request with
associated upper bound k, node xi proceeds as follows. If xi is the root node
for A, then xi requests that the copy of A associated with the current best upper
bound k be sent to x. Otherwise, xi communicates with its primary and secondary
(i, A[i])-neighbors to check whether the pointer list of any of these neighbors has
an entry (A, z, k1 ) such that k1 is at most k. Then xi updates k to be the minimum
of k and the smallest value of k1 thus obtained (if any). IfPk is within a constant
factor of the cost of following hxi up to xi , that is, k is O( i−1
j=0 c(x j , x j+1 )), then
xi requests that the copy of A associated with the upper bound k be sent to x.
Otherwise, xi forwards the read request to xi+1 .
Relating to the more general description of the read operation of our scheme
described in Section 1, the tree T associated with object A is given by the following
rule: the parent of node x in T is the primary (i, A[i])-neighbor of x, where i is
the maximum index such that (x[i − 1], . . . , x[0]) = (A[i − 1], . . . , A[0])—in
other words, the parent of node x is the node xi in the primary neighbor sequence
hxi for A with x = x0 . Figure 2 illustrates the tree T and the sequence hxi.
• Insert. An insert request for object A generated by node y updates the pointer
lists of the nodes in some prefix of the primary neighbor sequence hyi for A with

Fig. 2.

The tree T associated with object A and the primary neighbor sequence for x = x0 .

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

253

y0 = y. When such an update arrives at a node
P yi by means of an insert message,
yi updates its pointer list if the upper bound i−1
j=0 c(y j , y j+1 ) on the cost of getting
object A from y is smaller than the current upper bound associated with A in this
list. In other words, yi updates Ptr(y
Pi ) if (A, ·, ·) is not in this list, or if (A, ·, k)
is in Ptr(yi ) and k is greater than i−1
j=0 c(y j , y j+1 ). Node yi forwards the insert
request to node yi+1 only if Ptr(yi ) is updated.
• Delete. A delete request for object A generated by node y eventually removes all
triples of the form (A, y, ·) from the pointer lists Ptr(yi ), where hyi is the primary
neighbor sequence for A with y0 = y, making the copy of A at y unavailable to
other nodes in the network. Upon receiving such a request by means of a delete
message, node yi checks whether the entry associated with A in its pointer list
is of the form (A, y, ·). If it is not, the delete procedure is completed and we
need not proceed further in updating the pointer lists in hyi. Otherwise, yi deletes
this entry from its pointer list, and checks for entries associated with A in the
pointer lists of its reverse (i − 1, A[i − 1])-neighbors. If an entry is found, yi
updates Ptr(yi ) by adding the entry (A, w, k + c(w, yi )), where w is the reverse
(i − 1, A[i − 1])-neighbor of yi with minimum upper bound k associated with A
in its pointer list. A delete message is then forwarded to yi+1 .
The read, insert, and delete operations are summarized in Figures 3–5. The messages
and requests in the figure are all with respect to object A. A read request is generated
by node x when x (= x0 ) sends a message Read(x, ∞, ·) to itself, if x does not hold a
copy of A. A read message Read(x, k, y) indicates that (i) a read request for object A
was generated at node x, and (ii) the current best upper bound on the cost of reading a
copy of A is k, the cost of accessing the copy at y. An insert (resp., delete) request is
generated when node y (= y0 ) sends a message Insert(y, 0) (resp., Delete(y)) to itself.
An insert message Insert(y, k) indicates to its recipient node z that the best known upper

Action of xi on receiving a message Read(x, k, y)
If i > 0 and xi [i − 1] 6= A[i − 1], or i = (log n)/b − 1 (that is, xi is the root for A), then
•

Node xi sends a message Satisfy(x) to node v such that (A, v, ·) is in Ptr(xi ), requesting it to
send a copy of A to x. If Ptr(xi ) has no such entry, then there are no shared copies of A.

Otherwise
•

•

•

•

Let U be the set of secondary (i, A[i])-neighbors of xi . Node xi requests a copy of A with
associated upper bound at most k from each node in U ∪ {xi+1 }.
Each node u in U ∪ {xi+1 } responds to the request message received from xi as follows: If there
Pi−1
c(x j , x j+1 ) is at most k,
exists an entry (A, v, qv ) in Ptr(u) and if qv0 = qv + c(xi , u) +
j=0

then u sends a success message Success(v, qv0 ) to xi .
Let U 0 be the set of nodes u from which xi receives a response message Success(u, ku ). If U 0 is
not empty, then xi updates (k, y) to be (k z , z), where z is a node with minimum ku over all u in
U 0.
Pi−1
If k = O( j=0 c(x j , x j+1 )), then xi sends a message Satisfy(x) to node y, requesting y to send
a copy of A to x. Otherwise, xi forwards a message Read(x, k, y) to xi+1 .

Fig. 3.

Action on receiving a message Read for object A.

254

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Action of yi on receiving a message Insert(y, k)
If (A, ·, ·) is not in Ptr(yi ), or (A, ·, k 0 ) is in Ptr(yi ) and k 0 > k, then
•

•

Fig. 4.

Node yi accordingly creates or replaces the entry associated with A in Ptr(yi ) by inserting
(A, y, k) into this list.
If yi [i − 1] = A[i − 1], then yi sends a message Insert(y, k + c(yi , yi+1 )) to yi+1 .
Actions on receiving a message Insert for object A.

bound on the cost incurred by bringing the copy of A located at y to the node z is k. We
assume that y holds a copy of A and that this copy is unshared (resp., shared) when an
insert (resp., delete) request for A is generated at y.
The correctness of our access scheme follows from the two points below:
1. The insert and delete procedures maintain the following invariants. For any A
in A and any y in V , there is at most one entry associated with A in the pointer
list of y. If y holds a shared copy of A and hyi is the primary neighbor sequence
for A with y0 = y, then (i) there is an entry associated with A in the pointer list
of every node in hyi, and (ii) the nodes that have a pointer list entry associated
with the copy of A at y form a prefix subsequence of hyi. The preceding claims
follow directly from the insert and delete procedures as described.
2. Every read request for any object A by any node x is successful. That is, it locates
and brings to x a shared copy of A, if such a copy is currently available. The
read operation proceeds by following the primary neighbor sequence hxi for A
with x0 = x, until either a copy of A is located or the root for A is reached. By
point 1 above, there exists a shared copy of A in the network if and only if the
root for A has a pointer to it.

5.

Performance Bounds

In this section, we state our main claims regarding the performance of our access scheme.
In Theorems 1–4 below, we state bounds on the cost of a read, the cost of an insert or
delete, the size of auxiliary memory, and the adaptability of our access scheme.

Action of yi on receiving a message Delete(y)
If (A, y, ·) is in Ptr(yi ), then
•

•

•

•

Fig. 5.

Let U be the set of reverse (i − 1, A[i − 1])-neighbors of yi . Node yi removes (A, y, ·) from
Ptr(yi ), and requests a copy of A from each u in U .
Each u in U responds to the request message from yi by sending a message
Success(v, qv + c(yi , u)) to yi if and only if (A, v, qv ) is in Ptr(u).
Let U 0 be the set of nodes u such that yi receives a message Success(u, ku ) in response to the
request message it sent. If |U 0 | > 0, then yi inserts (A, w, kw ) into Ptr(yi ), where w is the node
in U 0 such that kw ≤ ku , for all u in U 0 .
If yi [i − 1] = A[i − 1], then yi sends a message Delete(y) to yi+1 .
Actions on receiving a message Delete for object A.

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

255

Theorem 1. Let x be any node in V and let A be any object in A. If y is the node with
minimum c(x, y) that holds a shared copy of A, then the expected cost of satisfying a
read request for A by x is O( f (`(A))c(x, y)).
Let C denote max{c(u, v) : u, v ∈ V }. If a node x tries to read an object A for
which there is currently no shared copy in the network, then the expected cost of the
read operation is O(C).
Theorem 2. The expected cost of an insert operation is O(C), and that of a delete
operation is O(C log n).
Theorem 3. Let q be the number of objects that can be stored in the main memory of
each node. The size of the auxiliary memory at each node is O(q log2 n) words whp.
Theorem 4. The adaptability of our scheme is O(log n) expected and O(log2 n) whp.

6.

Informal Overview of the Analysis

In this section we give an informal overview of the analysis of our access scheme.
Section 6.1 gives an overview of the analysis of the read operation. Section 6.2 outlines
our analysis for the other performance bounds. The reader interested only in the formal
analysis of our access scheme may proceed directly to Section 7 without loss of continuity.
6.1.

The Read Operation

Suppose node x issues a read request for an object A for which the closest copy lies at
node y. (Recall that a closest node y to x is the node with minimum c(x, y) that holds
a copy of A.) Let hxi (resp., hyi) denote the primary neighbor sequence associated with
object A and node x = x0 (resp., y = y0 ). Since a copy of A was inserted at y prior to
the read request, it is guaranteed that any node yj in the sequence hyi has a pointer to a
copy of A that is at y or at a node closer to yj than y. The read operation proceeds by
forwarding the request along the nodes in the sequence hxi until a pointer to a copy of A
is obtained. In fact, when the read request is forwarded to a node xi in the sequence hxi,
xi checks both its auxiliary memory as well as its secondary neighbors for a pointer to a
copy of A. If the pointer is present at any of these nodes, then the object is fetched from
the appropriate node. Otherwise, the request is forwarded to the next “level.” Figure 6
illustrates the neighbor sequences and the forwarding process. In the figure a read request
for object A generated at node x = x0 is forwarded along hxi until a pointer to the copy of
A residing at node y = y0 is found at node y3 , which is also a secondary neighbor of x4 .
An important part of our analysis is to place an upper bound τ on the smallest level in
the neighbor sequence of x where a pointer to the object can be found. Of course, owing
to the randomization used by our access scheme, any such upper bound is probabilistic.
In other words, we can only characterize the probability distribution of τ . Moreover,
the probability distribution of τ does not directly yield the cost incurred for the read
operation since the cost incurred at any given level is itself a random variable. While we
expect the cost incurred at the highest level to be the dominating term in the overall cost,
we need to incorporate this intuition formally.

256

Fig. 6.

C. G. Plaxton, R. Rajaraman, and A. W. Richa

A read request for object A is forwarded until a pointer to a copy of A is found.

Balls Defined by the Primary Neighbor Sequences. We characterize the probability
distribution of τ by analyzing certain balls that are defined by the sequences hxi and hyi.
For any nonnegative integer i, let Ai (resp., Di ) denote the ball of smallest radius
S around
y
)
that
contains
x
(resp.,
y
).
Let
B
(resp.,
E
)
denote
the
set
xi (resp.,
i+1
i+1
i
i
0≤ j≤i A j
S i
(resp., 0≤ j≤i D j ). Let Ci denote the ball of smallest radius around xi that contains all
of the secondary (i, A[i])-neighbors of xi . If we select τ such that Cτ is a superset of
Dτ , then the node yτ +1 is in Cτ , implying that τ is an upper bound on the smallest level
at which a pointer to A is guaranteed to be found. Therefore, the cost incurred by the
forwarding and request messages is within a constant factor (that depends on the constant
d) of the sum of the radii of the Ci ’s, 0 ≤ i ≤ τ . Moreover, if yτ +1 has a pointer to a
copy of A at node z, then the insert operation ensures that c(yτ , z) is at most the sum of
the radii of the Di ’s, 0 ≤ i ≤ τ . Therefore, the total cost of the read operation is within
a constant factor of the sum of the radii of the Ci ’s and the Di ’s, 0 ≤ i ≤ τ .
It follows from the discussion in the preceding paragraph that we can estimate τ and
the total cost of the read operation by determining bounds on the sizes of the balls Ci and
Di . We proceed by identifying balls centered around x (resp., y) that are “expected” to
contain the balls Ai (resp., Di ) and Ci , and the set Bi (resp., E i ) for a given i. Consider
the balls Ci and Di . The expected size of each ball is 2(2(i+1)b ). By choosing b to be
sufficiently large, we ensure that the expected size of Ci (resp., Di ) grows rapidly enough
with i that the expected radius of Ci (resp., Di ) is much greater than the expected distance
from x to xi (resp., y to yi ). As a result, if the sizes of the balls at each level are close to their
respective expected values, then we see that Ci is a superset of a ball X i and a subset of a
ball X i0 , where both X i and X i0 consist of 2(2(i+1)b ) closest neighbors of x. Similarly, we
also expect that Di is a subset of a ball Yi which consists of 2(2(i+1)b ) closest neighbors
of y. Moreover, since the ball Ci includes d secondary neighbors of xi while Di is defined
only by the primary neighbor of yi , we can choose d sufficiently large such that X i is
a constant factor bigger than Yi . Let i ∗ be the smallest i such that the radius of X i is
2(c(x, y)). By (1), an appropriate choice of constants (depending on 1) in the definitions
of X i and Yi implies that, for all i greater than or equal to i ∗ , X i is a superset of Yi . Thus,
we can now define τ as the smallest i ≥ i ∗ for which both Ci ⊇ X i and Di ⊆ Yi hold.
Loosely speaking, X i and Yi may be thought of as the expected values of the balls that
are obtained during the processing of a read request and an insert request, respectively,

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

257

in the simplified access scheme described in Section 3. Recall that in the simplified
scheme, node x processes a read request by repeatedly querying appropriately chosen
neighbors of x that lie in increasingly larger balls around x. Similarly, node y processes
an insert request for a given object by repeatedly passing a pointer to the new object
copy to appropriately chosen neighbors of y in increasingly larger balls around y. As
mentioned in Section 3, while the simplified access scheme is relatively easy to analyze,
it suffers from the drawback that the memory required at the individual nodes is high.
Analyzing the Drift of Balls. In the foregoing discussion, we argued that if the behavior
of the access scheme at each level is close to the expected behavior, then Ci ⊇ X i and
Di ⊆ Yi hold for all i. It is quite likely, however, that the ball sizes may deviate from
their expected values at some level. An important part of our analysis is to quantify the
“drift” that the balls Ci and Di have with respect to the balls X i and Yi , respectively, and
thus bound the increase in the expected cost due to forwarding, a technical challenge
referred to as Problem 3 in Section 3. In fact, it helps us in our analysis to quantify the
drift of the sets Ai , Bi , and E i as well and thus provide a more accurate characterization
of our access scheme. The main difficulty is that it is not possible to place useful “high
probability” bounds on the amount of drift for each value of i. We instead obtain an
approximate probability distribution on the amount of drift. More precisely, we define a
nonnegative state variable si (resp., ti ) for the sequence hxi (resp., hyi) that captures the
amount of drift in the balls Ai , Bi , and Ci (resp., Di and E i ). The larger the values of
the state variables si and ti , the larger the drift. Thus, for example, when si is 0, Ci is a
superset of X i and Ci is a subset of X i0 . Similarly, if ti is 0, then the set Di is a subset of
Yi . Thus τ can now be defined as the smallest i ≥ i ∗ such that si = ti = 0.
The sequence (si , ti ) can be naturally viewed as a random walk on the two-dimensional integer lattice. One of our key claims concerns the characterization of the random
walk defined by (si , ti ). We show in Lemmas 8.6 and 8.7 that this random walk is biased
toward (0, 0). More precisely, we show that if si (resp., ti ) is positive then it decreases
with a probability that can be made arbitrarily close to 1 by selecting the constants b
and d in the algorithm to be sufficiently large. We note that a sufficiently large value for
b ensures that the probability that Ci is a subset of X i0 and Di is a subset of Yi is close
to 1 (thus addressing Problem 3 of Section 3). Similarly, a sufficiently large value for
d ensures that the probability that Ci is a superset of X i is close to 1 (thus addressing
Problem 2 of Section 3).
Bounding the Expected Total Cost. Lemmas 8.6 and 8.7 together characterize a single step of the random walk defined by (si , ti ). Using the two lemmas and standard
combinatorial techniques for analyzing random walks on a line, we then establish the
following claim (Lemma 8.17): for any i, the probability that j is the smallest index such
that (si+ j , ti+ j ) is (0, 0) decreases exponentially with j. One obvious corollary of the
preceding claim is that the expected value of τ is i ∗ + O(1). More importantly, however,
it allows us to bound the tail of the probability distribution of τ . That is, Lemma 8.17
implies that the probability that the read request terminates at level i ≥ i ∗ decreases
exponentially with i − i ∗ . On the other hand, the expected cost incurred at a level i increases exponentially with i. By choosing the constants involved judiciously, we ensure
that the product of the probability of reaching a level i > i ∗ to the expected cost incurred

258

C. G. Plaxton, R. Rajaraman, and A. W. Richa

at level i decreases exponentially with i − i ∗ , thus yielding the desired upper bound on
the expected total cost.
6.2.

Other Performance Bounds

We now present an informal overview of the analysis for the other performance bounds,
namely, that of the insert and delete operations, the auxiliary memory size, and the
adaptability of our access scheme. We first consider the insert and delete operations.
When an insert operation is issued at a node y, the node updates the pointer lists of the
nodes in some prefix of the primary neighbor sequence hyi. The desired upper bound
on the expected cost of an insert operation thus follows directly from the bounds on the
expected cost incurred at each level of the primary neighbor sequence that we establish
in the analysis of the read operation. For a delete operation issued for a copy of object A
at node y, an additional communication cost needs to be taken into account since a node
in the primary neighbor sequence hyi may also send a message to its reverse neighbors
to check for another copy of A. We show by standard Chernoff-type arguments that
the number of reverse neighbors for any node is O(log n) expected and O(log2 n) whp
(Corollary 9.2.1), thus leading to the desired bound for the delete operation.
As mentioned in Section 4, the auxiliary memory at any node consists of two parts.
The first part, the neighbor table, consists of the primary, secondary, and reverse neighbors. Since the number of primary and secondary neighbors of a node is O(log n), the
discussion in the preceding paragraph about the number of reverse neighbors indicates
that the size of the neighbor table is O(log n) expected and O(log2 n) whp. For bounding
the size of the second part, the pointer list, we first observe that if node u has a pointer to
an object A lying at another node v, there is an i such that (i) u is the primary (i, A[i])neighbor of v, and (ii) the label of node u matches the ID of A in the rightmost ib bits.
We show by means of a Chernoff-type argument that the number of nodes v that satisfy
condition (i) is O(2ib log n) whp (see Lemma 9.3). Moreover, the probability of condition (ii) occurring is at most 1/2ib . Since the number of objects lying at a node is at most
q and there are O(log n) possible values of i, we obtain that the number of pointers at a
node is O(q log2 n) whp. This yields the desired upper bound on the auxiliary memory
size.
Finally, since the number of nodes that need to be updated on the removal and the
addition of a node u is equal to the total number of neighbors of u, the desired bound on
the adaptability of the access scheme follows directly from the bound on the size of the
neighbor table.

7.

Preliminaries for the Analysis

In this section we present some preliminary definitions that are used in our analysis. We
first define the relationship between the several constants that appear in the model, the
access scheme, and the analysis. The constants δ and 1 appear in the model, b and d
appear in the access scheme, and γ and ε appear in the analysis. We choose b, d, γ , and
ε such that the following inequalities hold:
2b ≥ 12 γ 3 ,

(2)

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

d ≥ γ 2,
γ ≥ 12 ,
ε ≥ max{61/γ , 4e−γ /41 , 6(d + 1)/2b , 6(2e/2b )d/2 , 6(e1γ 2 /d)d/2 },
ε < (10 · 2db logδ 2e )−1 .

259

(3)
(4)
(5)
(6)

An assignment of values to the constants γ , b, d, and ε that satisfies the above inequalities
may be obtained as follows: Set γ equal to 2b/3 /12/3 , d equal to (e22b/3+1 )/11/3 , and
ε equal to (6e15/3 )/2b/3 . The preceding assignment satisfies (2) and (5) if b is set
sufficiently large. Equations (4) and (6) can be satisfied by setting b large enough so that
2b ≥ 18 and 2b/3−db logδ 2e > 60e15/3 . (Note that since we assume δ > 8 for our model,
b/3 − db logδ 2e can be set to an arbitrarily large constant by choosing b sufficiently
large.)
During the course of our analysis, we frequently study certain “local neighborhoods”
of a given node with respect to the cost function c. In Section 2 we defined the ball of
radius r around a node u, M(u, r ). We now define the ball of size k around node u,
N (u, k), for any u in V and any integer k in [1, n] as follows.3 Let N (u, k) denote the
unique set of k nodes such that for any v in N (u, k) and w not in N (u, k), c(u, v) is
less than c(u, w). For convenience, if k is greater than n, we let N (u, k) be V . As in the
case of the balls M(u, r ), we define the radius of N (u, k) to be the maximum value of
c(u, v) over all v in N (u, k).

8.

Analysis of the Read Operation

We begin our analysis of the read operation by establishing some useful properties of
balls in Section 8.1. The desired upper bound on the expected cost of an individual read
operation is shown in Section 8.2.
8.1.

Properties of Balls

In the proofs of the lemmas in this section, we make extensive use of (1) as well as
of the fact that the cost function is symmetric and satisfies the triangle inequality. We
first consider the smallest (resp., the largest) ball centered at a node v that contains
(resp., is contained in) some given subset of nodes. Given any subset S of V and some
node u in S, let n ⊆ (u, S) (resp., n ⊇ (u, S)) denote the largest (resp., smallest) integer k
such that N (u, k) is a subset (resp., superset) of S. Let N⊆ (u, S) and N⊇ (u, S) denote
N (u, n ⊆ (u, S)) and N (u, n ⊇ (u, S)), respectively.
Lemma 8.1. Let u belong to V , and let k0 and k1 denote positive integers such that
k1 ≥ 12 k0 . For any v in N (u, k0 ), n ⊆ (v, N (u, k1 )) is at least k1 /1 and N⊇ (v, N (u, k1 ))
is a subset of N (u, 1k1 ).
Proof. We first obtain a lower bound on n ⊆ (v, N (u, k1 )). Let r0 and r1 denote the radii
of N (u, k0 ) and N (u, k1 ), respectively. Since k1 ≥ 12 k0 , (1) implies that r1 − r0 ≥
3

For integers a and b, we let [a, b] denote the set {k ∈ Z : a ≤ k ≤ b}.

260

C. G. Plaxton, R. Rajaraman, and A. W. Richa

(r1 + r0 )/2. Let w be the node in N⊆ (v, N (u, k1 )) such that c(v, w) is maximum. A ball
of radius (r1 − r0 ) around v is contained in N (u, k1 ) (since v is contained in N (u, k0 )).
Thus r1 − r0 ≤ c(v, w). It follows that 2c(v, w) is at least 2(r1 − r0 ) ≥ r1 + r0 and
M(v, 2c(v, w)) is a superset of N (u, k1 ). We obtain a lower bound on n ⊆ (v, N (u, k1 ))
as follows:
n ⊆ (v, N (u, k1 )) = |M(v, c(v, w))|
≥ |M(v, 2c(v, w))|/1
≥ k1 /1.
We now place an upper bound on n ⊇ (v, N (u, k1 )). Let w be the node in
N⊇ (v, N (u, k1 )) such that c(v, w) is maximum. We have r1 − r0 ≤ c(v, w) ≤ r1 + r0 .
(We showed that r1 − r0 ≤ c(v, w) in the preceding paragraph, and the other inequality
follows from the triangle inequality.) It follows that 2(r1 −r0 ) is at least c(v, w), implying
that M(v, c(v, w)) is a subset of M(u, 2r1 ). Therefore,
n ⊇ (v, N (u, k1 )) = |M(v, c(v, w))|
≤ |M(u, 2r1 )|
≤ 1k1 .
Lemma 8.2 and Corollary 8.2.1 are used in Section 8.2. We refer to any predicate
on V that only depends on the label of v as a label predicate. Given any node u in V and
a label predicate P on V , let p(u, P) denote the node v such that (i) P(v) holds, and
(ii) for any node w such that P(w) holds, c(u, v) is at most c(u, w). (We let p(u, P)
be null if such a v is not defined.) Let P(u, P) be M(u, c(u, p(u,P))) if p(u, P) is not
null, and V otherwise.
Lemma 8.2 examines the relationship between the set P(u, P) and the probability
distribution of the labels of the nodes, for any given node u and label predicate P. For
u in V , and i in [(log n)/b], let λ≥i (u) denote the string of (log n − ib) bits given by
u[(log n)/b −1] · · · u[i +1]u[i]. For convenience, we let λ>i (u) denote λ≥i+1 (u). For all
i and allV
u in V , let Pi (u) hold if and only if u[i] = A[i]. For all i and all u in V , let P<i (u)
denote j∈[i] P j (u). Let P≤i (u), P>i (u), and P≥i (u) be defined similarly. We note that
for u and v in V , and nonnegative integers i and j, if (u 6= v) ∨ ((u = v) ∧ (i 6= j)),
then Pi (u) and P j (v) are independent random variables. Also, each of the predicates
defined above is a label predicate.
Lemma 8.2. Let S and S 0 be subsets of V , and let u belong to S. Let P be a label
predicate on V and for each v in S 0 , let λ≥0 (v) be chosen independently and uniformly
at random.
1. Given that P(u, P) ⊆ S, we have that (i) the variables λ≥0 (v), for all v in
S 0 \P(u, P), are independent and uniformly random, and (ii) for each node v in
P(u, P)\{ p(u, P)}, P(v) is false.
2. Given that P(u, P) 6⊆ S, we have that (i) the variables λ≥0 (v), for all v in
S 0 \N⊆ (u, S), are independent and uniformly random, and (ii) for each node v
in N⊆ (u, S), P(v) is false.

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

261

3. Given that P(u, P) ⊇ S, we have that (i) the variables λ≥0 (v), for all v in
S 0 \N⊇ (u, S), are independent and uniformly random, and (ii) for each node v in
N⊇ (u, S)\{ p(u, P)}, P(v) is false.
Proof. We first consider Part 1 of the lemma. Part 1(i) follows from the independence of
P(v) and P(w), for any two distinct nodes v and w. By the definition of P, P( p(u, P))
holds and for each node v in P(u, P)\{ p(u, P)}, P(v) is false. This proves Part 1(ii).
Parts 2 and 3 follow similarly. For Part 2, we note that the event P(u, P) 6⊆ S is equivalent
to the event that, for each node v in N⊆ (u, S), P(v) is false. For Part 3, we note that the
event P(u, P) ⊇ S is equivalent to the event that, for each node in N⊇ (u, S)\{n ⊇ (u, S)},
P(v) is false.
The following claim follows from repeated application of Part 1 of Lemma 8.2.
Corollary 8.2.1. Let S be an arbitrary subset of V , let i be in [(log n)/b − 1], and let
S 0 be a subset of V such that λ≥0 (u) is independently and uniformly random for each u
in S 0 . Given a sequence of nodes u 0 , . . . , u i such that, for all j in [i], u j+1 = p(u j , P≤ j )
and P(u j , P≤ j ) ⊆ S, we have
S
1. The variables λ≥0 (u), for all u in S 0 \ j∈[i] P(u, P≤ j ), are independent and
uniformly random.
2. The variable λ>i (u i ) is uniformly random and for each node u in
[
P(u j , P≤ j )\{u i },
j∈[i]

P≤i (u) is false.
8.2.

Cost of the Read Operation

In this section we place upper bounds on the cost of the read operation by establishing
Theorem 1. We first introduce some notation and prove a few elementary lemmas in
Section 8.2.1. The bulk of the analysis appears in Sections 8.2.2 and 8.2.3. Using the
tools developed in these two sections, we prove Theorem 1 in Section 8.2.4.
8.2.1. Preliminaries. Consider a read request originating at node x for an object A.
Let y denote a node that has a copy of A. In the following we show that the expected cost
of a read operation is O( f (`(A))c(x, y)). If y denotes the node with minimum c(x, y)
among the set of nodes that have a copy of A, then the preceding bound implies that the
expected cost is asymptotically optimal.
Let hxi and hyi be the primary neighbor sequences for A with x0 = x and y0 = y,
respectively. For any nonnegative integer i, let Ai (resp., Di ) denote the ball of smallest
radius
S around xi (resp.,Syi ) that contains xi+1 (resp., yi+1 ). Let Bi (resp., E i ) denote the
set 0≤ j≤i A j (resp., 0≤ j≤i D j ). Let Ci denote the ball of smallest radius around xi
that contains all of the secondary (i, A[i])-neighbors of xi . For convenience, we define
B−1 = E −1 = ∅.
It is useful to consider an alternative view of xi , yi , Ai , and Di . For any nonnegative
i, if xi+1 (resp., yi+1 ) is not the root node for A, then xi+1 (resp., yi+1 ) is p(xi , P≤i )
(resp., p(yi , P≤i )) and Ai (resp., Di ) is P(xi , P≤i ) (resp., P(yi , P≤i )).

262

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Let γ be an integer constant satisfying (2)–(6). For any nonnegative integer i and
j
j
any integer j, let X i (resp., Yi ) denote the ball N (x, γ j 2(i+1)b ) (resp., N (y, γ j 2(i+1)b )).
∗
Let i denote the least integer such that the radius of X i1∗ is at least c(x, y). Let ai (resp.,
bi ) denote the radius of X i1 (resp., Yi1 ).
Lemma 8.3. For all i such that i ≥ i ∗ , X i2 is a superset of Yi1 .
Proof. By the definition of i ∗ , ai is at least c(x, y). Therefore, M(y, 2ai ) is a superset
of X i1 . Hence, M(y, 2ai ) contains at least γ 2(i+1)b nodes and is a superset of Yi1 .
By (1), |M(x, 3ai )| is at most 12 |M(x, ai )| ≤ 12 |X i1 |. By (4), 12 |X i1 | ≤ 12 γ 2(i+1)b
≤ γ 2 2(i+1)b . Thus, M(x, 3ai ) is a subset of X i2 . Since M(x, 3ai ) is a superset of
M(y, 2ai ), which is a superset of Yi1 , the claim holds.
Lemma 8.4. For all i in [(log n)/b − 2] we have 2bb log1 2c ai ≤ ai+1 ≤ 2db logδ 2e ai and
2bb log1 2c bi ≤ bi+1 ≤ 2db logδ 2e bi . For i = (log n)/b − 2, we have ai+1 ≤ 2db logδ 2e ai and
bi+1 ≤ 2db logδ 2e bi . Also, ai ∗ and bi ∗ are both O(c(x, y)).
Proof. We prove the bounds for ai+1 and ai ∗ . The bounds for bi+1 and bi ∗ follow the same
1
| = 2b |X i1 |. Therefore,
lines. Since γ ≤ 2b by (4), for all i in [(log n)/b−2], we have |X i+1
bb log1 2c
ai ≤ ai+1 ≤ 2db logδ 2e ai . For
for all i in [(log n)/b − 2], it follows from (1) that 2
1
1
b
db logδ 2e
ai .
i = (log n)/b − 2, |X i+1 | ≤ 2 |X i |, and, hence, ai+1 ≤ 2
If i ∗ > 0, then ai ∗ (resp., bi ∗ ) is at most 2db logδ 2e c(x, y) = O(c(x, y)). Otherwise,
ai ∗ is O(2dlogδ γ e ) = O(c(x, y)), since δ and γ are constants.
We define two sequences hsi i and hti i of nonnegative integers as follows:

Ai ⊇ X i−1 , Ci ⊇ X i2 ,
0
if Bi ⊆ X i1 ,



1
Ai ⊇ X i−1 , Ci 6⊇ X i2 ,
1
if Bi ⊆ X i ,
si =
1

Ai 6⊇ X i−1 , and
2
if Bi ⊆ X i ,


3+ j
if 0 ≤ j ≤ i, Bi− j 6⊆ X i1 , Bi− j−1 ⊆ X i1 .
½
0
if E i ⊆ Yi1 ,
and
ti =
1+ j
if 0 ≤ j ≤ i, E i− j 6⊆ Yi1 , E i− j−1 ⊆ Yi1 .

(7)

(8)

The following intuition underlies the above definitions of si and ti . For any j, the
expected sizes of the balls Ai and Di are both 2(i+1)b . Thus, the expected sizes of the
balls Bi and E i are both at most 2(i+1)b+1 . Moreover, the expected size of Ci is at least
c1 2(i+1)b , where c1 is a constant that depends on d. The constant γ is chosen sufficiently
large and d is chosen sufficiently larger than γ such that the “expected behavior” of the
balls Ai , Bi , Ci , and E i is as follows: Bi ⊆ X i1 , Ai ⊇ X i−1 , Ci ⊇ X i2 , and E i ⊆ Yi1 . The
value of si (resp., ti ) indicates the degree to which the sizes of the balls Ai , Bi , and Ci
(resp., Di and E i ) deviate from this expected behavior. The larger the value of si , the
greater the deviation from the expected behavior.
Lemma 8.5.
is O(bi ).

If si is in {0, 1, 2}, then c(xi , xi+1 ) is O(ai ). If ti is 0, then c(yi , yi+1 )

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

263

Proof. The proof of the first claim follows from the observation that if si is in {0, 1, 2},
then Ai ⊆ Bi ⊆ X i1 . The proof of the second claim follows from the observation that if
ti is 0, then Di ⊆ E i ⊆ Yi1 .
8.2.2. Properties of hsi i and hti i. Our plan for determining an upper bound on the cost
of the given read operation for object A is as follows. Let τ be the smallest integer i ≥ i ∗
such that (si , ti ) = (0, 0). By the definitions of sτ and tτ , Cτ ⊇ X τ2 and Yτ1 ⊇ E τ ⊇ Dτ .
By Lemma 8.3, X τ2 ⊇ Yτ1 , thus implying that Cτ is a superset of Dτ . Thus, a copy of
A is located within τ forwarding steps along hxi. By the definition of the primary and
secondary neighbors, the total cost of all messages sent by node xi is O(c(xi , xi+1 )).
Since a copy of A is located within τ forwarding steps,
P the cost of all messages needed in
locating the particular copy of A that is read is O( 0≤ j<τ (dc(x j , x j+1 ) + c(yj , yj+1 ))).
The cost of reading the copy is at most f (`(A)) times the preceding cost. Since d is a
constant, the cost of reading A is at most
X
O( f (`(A))(c(x j , x j+1 ) + c(yj , yj+1 ))).
(9)
0≤ j<τ

P
The remainder of the proof concerns the task of showing that E[ 0≤ j<τ (c(x j , x j+1 )
+ c(y j , yj+1 ))] is O(c(x, y)). A key idea is to establish that the sequence hsi , ti i corresponds to a two-dimensional random walk that is biased toward (0, 0). Lemmas 8.6
and 8.7 below provide a first step toward formalizing this notion.
Lemma 8.6. Let i be in [(log n)/b − 1]. Given s j and t j for all j in [i] such that si−1
is at least 3, the probability that si is less than si−1 is at least 1 − ε2 . Given s j and t j for
all j in [i] such that ti−1 is at least 1, the probability that ti is less than ti−1 is at least
1 − ε2 .
Lemma 8.7. Let i be in [(log n)/b − 1]. Given s j and t j for all j in [i] such that si−1
is at most 3, the probability that si is 0 is at least 1 − ε. Given s j and t j for all j in [i]
such that ti−1 is at most 1, the probability that ti is 0 is at least 1 − ε.
In order to establish the above lemmas, we first introduce some additional notation.
For each i ≥ −1, we define Si and Ti as follows. Let S−1 = T−1 = ∅ and for i ≥ 0, let

if si ∈ {0, 1},
 Si−1 ∪ Bi ∪ (Ci ∩ N⊇ (xi , X i2 ))
Si−1 ∪ Bi
if si = 2,
Si =

otherwise;
Si−1 ∪ Bi−si +2 ∪ N⊆ (xi−si +3 , X i1 )
½
Ti−1 ∪ E i
if ti = 0,
Ti =
otherwise.
Ti−1 ∪ E i−ti ∪ N⊆ (yi−ti +1 , Yi1 )
The set Si (resp., Ti ) contains all of the nodes whose labels need to be examined to
determine the values of s0 through si (resp., t0 through ti ). Moreover, as we show in
Lemma 8.8, the particular values of s0 through si and t0 through ti bias the distribution
of only a suffix of the labels of the nodes in Si ∪ Ti .

264

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Lemma 8.8. Let i be in [(log n)/b − 1]. Given s j and t j for all j in [i], we have that:
1. The variables λ≥0 (u), for all u not in Si ∪ Ti , are independent and uniformly
random.
2. There exists a subset Si0 of Si of size at most d + 1 such that (i) the variables
λ>i (u), for all u in Si0 , are independent and uniformly random, and (ii) for each
node u in Si \Si0 , P≤i (u) is false.
3. There exists at most one node v in Ti such that (i) the variable λ>i (v) is uniformly
random, and (ii) for each node u in Ti \{v}, P<i (u) is false.
Proof. We prove Parts 1–3 for all i ≥ −1. The proof is by induction. For the induction
base we set i = −1. Part 1 follows directly from the random assignment of labels. For
0
to ∅, and the desired claim holds since S−1 is ∅. The claim of Part 3
Part 2, we set S−1
holds vacuously since T−1 is ∅.
For the induction hypothesis, we assume that Parts 1–3 of the lemma hold for i − 1.
We first consider different cases depending on the value of si .
(a) si = 3 + j, j ∈ [i]: The event si = 3 + j is equivalent to the event (Bi− j−1 ⊆
X i1 ) ∧ (Ai− j 6⊆ X i1 ). We first condition on the event Bi− j−1 ⊆ X i1 by invoking
Corollary 8.2.1 with the substitution (X i1 , V \(Si−1 ∪ Ti−1 ), i − j) for (S, S 0 , i).
We next condition on the event Ai− j 6⊆ X i1 by invoking Part 2 of Lemma 8.2 with
the substitution (xi− j , X i1 , V \(Si−1 ∪ Ti−1 ∪ Bi− j−1 ), P≤i ) for (u, S, S 0 , P). By
combining Part (i) of both invocations, we have (a.i) the variables λ≥0 (v), for all
v not in Si−1 ∪ Ti−1 ∪ Bi− j−1 ∪ N⊆ (xi− j , X i1 ), are independent and uniformly
random. By combining Part (ii) of both invocations, we have (a.ii) for each node
v in Bi− j−1 ∪ N⊆ (xi− j , X i1 ), P≤i (v) is false.
0
\(Bi− j−1 ∪ N⊆ (xi− j , X i1 )).
We set Si0 to Si−1
(b) si = 2: The event si = 2 is equivalent to the event (Bi ⊆ X i1 ) ∧ (Ai 6⊇ X i−1 ).
We first condition on the event Bi ⊆ X i1 by invoking Corollary 8.2.1 with the
substitution (X i1 , V \(Si−1 ∪Ti−1 ), i) for (S, S 0 , i). It follows from the preceding
invocation and the definition of Bi that (b.i) the variables λ≥0 (v), for all v not
in Si−1 ∪ Ti−1 ∪ Bi , are independent and uniformly random, and (b.ii) for each
node v in Bi \{xi+1 }, P≤i (v) is false.
0
\(Bi \{xi+1 }).
We set Si0 to Si−1
(c) si ∈ {0, 1}: The event si ∈ {0, 1} is equivalent to the event (Bi ⊆ X i1 ) ∧ (Ai ⊇
X i−1 ). We condition on the event Bi ⊆ X i1 by invoking Corollary 8.2.1 with the
substitution (X i1 , V \(Si−1 ∪Ti−1 ), i) for (S, S 0 , i). It follows from the preceding
invocation and the definition of Bi that (i) the variables λ≥0 (v), for all v not in
Si−1 ∪ Ti−1 ∪ Bi , are independent and uniformly random, and (ii) for each node
v in Bi \{xi+1 }, P≤i (v) is false.
Let Si0 equal the set {v ∈ Ci ∩ N⊇ (xi , X i2 ) : P≤i (v)}. By the definition of
Ci , |Si0 | is at most d + 1. If Ci 6⊇ X i2 , then Ci ⊆ N⊇ (xi , X i2 ) and it follows
from the definition of Ci that (c.i) the variables λ≥0 (v), for all v not in Si−1 ∪
Ti−1 ∪ Bi ∪ Ci , are independent and uniformly random, and (c.ii) the variables
λ>i (v), for all v in Si0 , are independent and uniformly random, and for each
node v in (Bi ∪ Ci )\Si0 , P≤i (v) is false. If Ci ⊇ X i2 , then Ci ⊇ N⊇ (xi , X i2 ) and

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

265

it follows from Part 3 of Lemma 8.2 that (c.i) the variables λ≥0 (v), for all v
not in Si−1 ∪ Ti−1 ∪ Bi ∪ N⊇ (xi , X i2 ), are independent and uniformly random,
and (c.ii) the variables λ>i (v), for all v in Si0 , are independent and uniformly
random, and for each node v in (Bi ∪ N⊇ (xi , X i2 ))\Si0 , P≤i (v) is false.
We thus obtain from (a.i), (b.i), and (c.i) and the definition of Si that (i) the variables
λ≥0 (u), for all u not in Si ∪Ti−1 , are independent and uniformly random. By the definitions
of si and ti , the particular values of si and ti are independent of the suffix λ>i (u) of any
node u. In particular, the variables λ>i (u), for all u in Si0 , are independent and uniformly
random. It follows from the preceding observation and claims (a.ii), (b.ii), and (c.ii) that
(ii) the bits of λ>i (u), for all u in Si0 , are independent and uniformly random, and, for
each node in Si \Si0 , P≤i (u) is false. We next consider two cases depending on the value
of ti .
(d) ti = 1 + j, j ∈ [i]: This case is similar to Case (a). The event ti = 1 + j
is equivalent to the event (E i− j−1 ⊆ Yi1 ) ∧ (Di− j 6⊆ Yi1 ). We first condition
on the event E i− j−1 ⊆ Yi1 by invoking Corollary 8.2.1 with the substitution
(Yi1 , V \(Si ∪ Ti−1 ), i − j) for (S, S 0 , i). We next condition on the event Di− j 6⊆
Yi1 by invoking Part 2 of Lemma 8.2 with the substitution (yi− j , Yi1 , V \(Si ∪
Ti−1 ∪ E i− j−1 ), P≤i ) for (u, S, S 0 , P).
By combining Part (i) of both invocations, we have (d.i) the variables λ≥0 (v),
for all v not in Si ∪Ti−1 ∪ E i− j−1 ∪ N⊆ (yi− j , Yi1 ), are independent and uniformly
random. By combining Part (ii) of both invocations, we have (d.ii) for each node
v in E i− j−1 ∪ N⊆ (yi− j , Yi1 ), P≤i (v) is false.
(e) ti = 0: This case is similar to Case (b). The event ti = 2 is equivalent to the event
E i ⊆ Yi1 . We invoke Corollary 8.2.1 with the substitution (Yi1 , V \(Si ∪ Ti−1 ), i)
for (S, S 0 , i) to obtain: (e.i) the variables λ≥0 (v), for all v not in Si ∪ Ti−1 ∪ E i ,
are independent and uniformly random, and (e.ii) for each node v in E i \{yi+1 },
P≤i (v) is false.
To complete the induction step, we consider each part of the statement of the lemma
separately:
1. By (i), (d.i), (e.i), and the definition of Ti , it follows that given s j and t j , j ∈ [i],
the variables λ≥0 (u), for all u not in Si ∪ Ti , are independent and uniformly
random.
2. This part follows directly from (ii) above.
3. By (d.ii) and (e.ii), it follows that given arbitrary values for s j and t j , j ∈ [i]: (i)
the variable λ>i (yi+1 ) is uniformly random, and (ii) for each node u in Ti \{yi+1 },
P≤i (u) is false.
Lemma 8.9 places upper bounds on the sizes of Si and Ti .
Lemma 8.9. Let i be a nonnegative integer. If si is in {0, 1}, Si is a subset of X i3 ;
otherwise, Si is a subset of X i1 . The set Ti is a subset of Yi1 .
Proof.

The proof is by induction on i. For convenience, we set i = −1 for the induction

266

C. G. Plaxton, R. Rajaraman, and A. W. Richa

base. Since S−1 = T−1 = ∅, the claims follow trivially. Let the claims of the lemma hold
for Si−1 and Ti−1 . We will show that Si ⊆ X i1 . The proof for Ti is along the same lines.
3
3
. Since γ 2 ≤ 2b by (2), X i−1
⊆ X i1 , hence
By the induction hypothesis, Si−1 ⊆ X i−1
1
implying that Si−1 ⊆ X i . We now consider three cases depending on the value of si .
If si is in {0, 1}, then Bi ⊆ X i1 . Moreover, by Lemma 8.1, N⊇ (xi , X i2 ) ⊆
N (x, 1γ 2 2(i+1)b ). Since 1 ≤ γ by (4), N (x, 1γ 2 2(i+1)b ) ⊆ X i3 . It thus follows that
Si−1 , Bi , and N⊇ (xi , X i2 ) are all subsets of X i3 . Thus, Si is a subset of X i3 .
If si is 2, then Bi ⊆ X i1 . It thus follows that Si−1 and Bi are both subsets of X i3 .
Thus, Si is a subset of X i3 .
If si is greater than 2, then Bi−si +2 ⊆ X i1 . Moreover, N⊆ (xi−si +3 , X i1 ) is a subset of
1
X i . Thus, Si is a subset of X i1 .
The following lemma is used in the proof of Lemma 8.6.
Lemma 8.10. Let i be in [(log n/b) − 1]. Given sk and tk for all k in [i] such that si−1
2
is 3 + j for some j in [i + 1], the probability that Bi− j−1 is a subset of X i−1
is at least
1 − ε 2 /2. Given sk and tk for all k in [i] such that ti−1 is 1 + j for some j in [i + 1], the
2
is at least 1 − ε2 /2.
probability that E i− j−1 is a subset of Yi−1
Proof. Let E denote the event that the 2i random variables sk and tk , k ∈ [i], take the
given values. We assume that E holds. We begin with the proof of the first claim. Since
1
1
, and Bi− j−2 is a subset of X i−1
.
si−1 is 3 + j, Bi− j−1 is not a subset of X i−1
By Part 1 of Lemma 8.8, it follows that given E, the variables λ≥0 (u), for all u not in
Si−1 ∪ Ti−1 , are independent and uniformly random. By Lemma 8.9, |Si−1 ∪ Ti−1 | is at
2
most γ 2ib+1 . By Lemma 8.1, since γ ≥ 12 by (4), n ⊆ (xi− j−1 , X i−1
) is at least γ 2 2ib /1.
2
) is at most
Therefore, the probability that Ai− j−1 is not a subset of N⊆ (xi− j−1 , X i−1
(1 − 1/2(i− j)b )(γ

2

/1−2γ )2ib

≤ e−(γ /1−2γ )2
≤ ε2 /2.
2

jb

(The second step makes use of the following inequalities: (i) γ ≥ 41, which is obtained
from (4), and (ii) e−γ /1 ≤ (4e−γ /41 )2 /2 ≤ ε2 /2, which is obtained from (5).) Since
2
2
2
) is a subset of X i−1
, the probability that Ai− j−1 is not a subset of X i−1
is
N⊆ (xi− j−1 , X i−1
1
2
2
at least 1−ε /2. Since Bi− j−2 is a subset of X i−1 ⊆ X i−1 and Bi− j−1 = Bi− j−2 ∪ Ai− j−1 ,
2
with probability at least 1 − ε2 /2.
we obtain that Bi− j−1 is a subset of X i−1
The proof of the second claim is analogous to the above proof and is obtained by
substituting (t, D, E, y, Y ) for (s, A, B, x, X ).
We are now ready to prove Lemmas 8.6 and 8.7.
Proof of Lemma 8.6. Let E denote the event that the 2i random variables s j and t j ,
j ∈ [i], take the given values. We assume that E holds. We begin with the proof of the
1
, and
first claim. Let si−1 be 3 + j for some j in [i]. Thus, Bi− j−1 is not a subset of X i−1
1
Bi− j−2 is a subset of X i−1 .

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

267

We show that Bi− j is a subset of X i1 with probability at least 1 − ε2 . We first invoke
2
with probability at least 1 − ε2 /2.
Lemma 8.10 to obtain (a) Bi− j−1 is a subset of X i−1
2
hold.
We now assume that E and the event that Bi− j−1 is a subset of X i−1
We now show that (b) the probability that Bi− j is a subset of X i1 is at least 1 − ε2 /2.
It follows from Part 1 of Lemma 8.8 that, given E, the variables λ≥0 (u), for all u not in
Si−1 ∪ Ti−1 , are independent and uniformly random. Thus, given E and the event that
2
2
, the variables λ≥0 (u), for all u not in Si−1 ∪ Ti−1 ∪ X i−1
,
Bi− j−1 is a subset of X i−1
1
1
.
are independent and uniformly random. By Lemma 8.9, Si−1 ⊆ X i−1 and Ti−1 ⊆ Yi−1
2
is at most γ (γ + 1)2ib . By Lemma 8.1,
Therefore, the size of the set Si−1 ∪ Ti−1 ∪ X i−1
since γ ≥ 12 by (4), n ⊆ (xi− j , X i1 ) is at least γ 2(i+1)b /1. Therefore, the probability that
Ai− j is not a subset of N⊆ (xi− j , X i2 ) is at most
(1 − 1/2(i− j+1)b )(γ /1−γ

2

/2b −γ /2b )2(i+1)b

≤ e−(γ /1−γ
≤ ε2 /2.

2

/2b −γ /2b )2 jb

(The second step makes use of the following inequalities: (i) 21(γ + 1) ≤ 12 γ 3 ≤ 2b ,
which is obtained from (2), and (ii) e−γ /21 ≤ (4e−γ /41 )2 /2 ≤ ε2 /2, which is obtained
from (5).) Thus, the probability that Bi− j is not a subset of X i1 is at most ε2 /2.
It follows from (a) and (b) above that with probability at least (1 − ε 2 ), si is less
than si−1 , thus establishing the first claim of the lemma. The proof of the second claim
is analogous to the above proof and is obtained by substituting (t, D, E, y, Y ) for
(s, A, B, x, X ).
Proof of Lemma 8.7. Let E denote the event that the random variables s j , t j , j ∈ [i],
take the given values. We assume that E holds. We begin with the proof of the first claim.
1
. If si−1 is 3, then, by Lemma 8.10, Bi−1 is
If si−1 is in {0, 1, 2}, Bi−1 is a subset of X i−1
2
a subset of X i−1 with probability at least 1 − ε2 /2. We now assume that Bi−1 is a subset
2
.
of X i−1
We first show that (a) the probability that Bi is a subset of X i1 is at least 1−ε/3+ε2 /2.
By Part 1 of Lemma 8.8, it follows that, given E, the variables λ≥0 (u), for all nodes u not
in Si−1 ∪ Ti−1 , are independent and uniformly random. By Lemma 8.9, |Si−1 ∪ Ti−1 | is
2
and 2b ≥ 12 γ (by (2)), n ⊆ (xi , X i1 )
at most γ 3 2ib+1 . By Lemma 8.1, since xi is in X i−1
(i+1)b
/1. Therefore, the probability that Ai is not a subset of N⊆ (xi , X i1 ) is
is at least γ 2
at most
(i+1)b

(1 − 1/2(i+1)b )2

(γ /1−2γ 3 /2b )

≤ e−(γ /1−2γ /2
≤ e−γ /21
≤ ε/3 − ε2 /2.
3

b

)

(The second inequality follows from the inequality 4γ 2 1 ≤ 2b , which is obtained from
(2). The last inequality follows from the inequalities (i) e−γ /21 ≤ ε/4, which is obtained
1
by (6).) This implies that
from (5), and (ii) ε/4 ≤ ε/3 − ε 2 /2, which holds since ε ≤ 10
1
the probability that Ai is not a subset of X i is at most ε/3 − ε2 /2.
We next show that (b) the probability that Ai is a superset of X i−1 is at least 1 − ε/3.
Since 12 γ 3 ≤ 2b by (2), Lemma 8.1 implies that n ⊇ (xi , X i−1 ) ≤ 12(i+1)b /γ . By
Lemma 8.8, we have (i) the variables λ≥0 (u), for all u not in Si−1 ∪ Ti−1 , are independent

268

C. G. Plaxton, R. Rajaraman, and A. W. Richa

and uniformly random, and (ii) there are at most d + 1 nodes in Si−1 ∪ Ti−1 for which
the predicate P<i holds. Therefore, the probability that Ai is a subset of N⊇ (xi , X i−1 ) is
at most (d + 1)/2b + 1/γ , which is at most ε/3. It follows that the probability that Ai
is not a superset of X i−1 is at most ε/3.
We finally show that (c) given that Bi is a subset of X i1 and Ai is a superset of X i−1 ,
the probability that Ci is a superset of X i2 is at least 1 − ε/3. We note that given E and the
two events that Bi is a subset of X i1 and that Ai is a superset of X i−1 the following two
claims hold: (i) the variables λ≥0 (u), for all u not in Si−1 ∪ Ti−1 ∪ X i1 , are independent
and uniformly random, and (ii) there exist at most d + 1 nodes in Si−1 ∪ Ti−1 for which
the predicate P<i holds.
We place an upper bound on the probability that Ci is not a superset of X i2 by placing
an upper bound on the probability that Ci is not a superset of N⊇ (xi , X i2 ), which is a
superset of X i2 . Let r0 (resp., r1 ) denote n ⊇ (xi , X i−1 ) (resp., n ⊇ (xi , X i2 )). By definition,
n ⊇ (xi , X i−1 ) is at least 2(i+1)b /γ . By Lemma 8.1, n ⊇ (xi , X i2 ) is at most 1γ 2 2(i+1)b .
We first show that the nodes in N⊇ (xi , X i2 ) are within a cost of d · c(xi , xi+1 ). We
2
. Moreover,
note that c(xi , xi+1 ) is at least the difference of the radii of X i−1 and X i−1
2
3
2
since N⊇ (xi , X i ) is a subset of X i , n ⊇ (xi , X i ) is at most the sum of the radii of X i3 and
2
. Since (4γ 4 )logδ 2 ≤ γ 2 ≤ d by (3), all of the nodes in N⊇ (xi , X i2 ) are within a cost
X i−1
of d · c(xi , xi+1 ) from xi .
It now follows that the probability that Ci is not a superset of X i2 is at most the
probability that there exist d nodes in N⊇ (xi , X i2 ) whose (i + 1)b rightmost bits match
a certain bit-sequence. This probability is at most
µ 2 (i+1)b ¶
µ
¶
1γ 2
d +1
b d/2
(1/2(i+1)b )d/2 ≤ (2e/2b )d/2 + (e1γ 2 /d)d/2
(1/2 ) +
d/2
d/2
≤ ε/3.
(The second step follows from the inequalities (2e/2b )d/2 ≤ ε/6 and (e1γ 2 /d)d ≤ ε/6,
both of which are derived from (5).)
It follows from (a)–(c) above that with probability at least 1 − ε, si is 0, thus
establishing the first claim of the lemma. The proof of the second claim is analogous to
the proof of (a) and is obtained by substituting (t, D, E, y, Y ) for (s, A, B, x, X ).
By the definitions of si and ti , it follows that 0 ≤ si+1 ≤ 3 if si ≤ 2, and 0 ≤ si+1 ≤
si + 1 otherwise. In addition, 0 ≤ ti+1 ≤ ti + 1, for all i. Let si0 equal 0 if si = 0, 1 if
0
, ti+1 } ≤ max{si0 , ti } + 1, for all
si ∈ {1, 2, 3}, and si − 2 otherwise. Hence 0 ≤ max{si+1
i. In Section 8.2.3 below, we analyze the random walk corresponding to the sequence
hmax{s 0 , t}i.
8.2.3. Random Walks. We begin the analysis of the random walk corresponding to the
sequence hmax{s 0 , t}i by proving several useful properties of certain random walks on a
line. These properties are stated in Lemmas 8.11–8.15. The main technical claim of this
section is Lemma 8.17.
Let W (U, F) be a directed graph in which U is the set of nodes and F is the set of
edges. For all u in U , let Du be a probability distribution over the set {(u, v) ∈ F}. It
is convenient to define PrDu [(u, v) : (u, v) 6∈ F] = 0. A random walk on W starting at

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

269

v0 and according to {Du : u ∈ U } is a random sequence hvi such that (i) vi is in U and
(vi , vi+1 ) is in F, for all i, and (ii) given any fixed (not necessarily simple) path u 0 , . . . , u i
in W and any fixed u i+1 in U , Pr[vi+1 = u i+1 | (v0 , . . . , vi ) = (u 0 , . . . , u i )] = Pr[vi+1 =
u i+1 | vi = u i ] = PrDui [(u i , u i+1 )].
Let H be the directed graph with node set N (the set of nonnegative integers) and
edge set {(i, j) : i ∈ N, 0 ≤ j ≤ i + 1}. Let H 0 be the subgraph of H induced by the
edges {(i + 1, i), (i, i + 1) : i ∈ N} ∪ {(0, 0), (1, 1)}.
Let p and q be reals in (0, 1] such that p ≥ q. We now define two random walks,
ω p,q and ω0p,q , on graphs H and H 0 , respectively. Both ω p,q and ω0p,q begin at node
0. The walk ω p,q = hwi is characterized by: (i) Pr[wi+1 ≤ j − 1 | wi = j] ≥ p,
for any integer j > 1, (ii) Pr[wi+1 = 0 | wi = j] ≥ q, for j equals 0 or 1, and
(iii) Pr[wi+1 = 2 | wi = 1] ≤ 1 − p. The walk ω0p,q = hw0 i is characterized by: (i)
0
0
= j − 1 | wi0 = j] = p, for all integers j > 1, (ii) Pr[wi+1
= 0 | wi0 = j] = q,
Pr[wi+1
0
0
for j equals 0 or 1, and (iii) Pr[wi+1 = 2 | wi = 1] = 1 − p. We note that Lemmas 8.6
and 8.7 imply that the sequence hmax{s 0 , t}i can be represented by the random walk ω p,q
with p = 1 − 2ε2 and q = 1 − 2ε.
We analyze the random walk ω p,q by first showing that ω p,q is more “favorable”
than ω0p,q with respect to the properties of interest. The random walk ω0p,q is easier to
analyze as it is exactly characterized by p and q. Lemmas 8.11 and 8.13 show that the
bias of ω p,q toward 0 is more than that of ω0p,q . Since the values of p and q are fixed
throughout the following discussion, we omit the subscript p, q in the terms ω p,q and
ω0p,q for convenience.
Lemma 8.11. For all i and k in N, for random walks ω and ω0 , we have Pr[wi ≤ k] ≥
Pr[wi0 ≤ k].
Proof. We prove the claim by induction on i. The base case i = 0 is trivial since
ω0 = ω00 = 0. Assume the claim holds for i and any k. If k ≥ 1, then we have
0
≤ k] = Pr[wi0 ≤ k − 1] + p Pr[k ≤ wi0 ≤ k + 1]
Pr[wi+1
= (1 − p) Pr[wi0 ≤ k − 1] + p Pr[wi0 ≤ k + 1]

and
Pr[wi+1 ≤ k] ≥ Pr[wi ≤ k − 1] + p Pr[k ≤ wi ≤ k + 1]
= (1 − p) Pr[wi ≤ k − 1] + p Pr[wi ≤ k + 1].
If k = 1, then we have
0
≤ 0] = q Pr[wi0 ≤ 0] + q Pr[wi0 = 1]
Pr[wi+1
= q Pr[wi0 ≤ 1]

and
Pr[wi+1 ≤ 0] ≥ q Pr[wi ≤ 0] + q Pr[wi = 1]
= q Pr[wi ≤ 1].
The lemma now follows by induction.

270

C. G. Plaxton, R. Rajaraman, and A. W. Richa

We now establish a probabilistic comparison of the number of steps it takes for the
random walks ω and ω0 to reach node 0 starting from a given node i. Let z i (σ ) be the
random variable denoting the number of steps taken to reach node 0 starting from node
i, for a random walk σ .
Lemma 8.12.
Proof.

For all ` and all i > 0, we have Pr[z i (ω0 ) ≤ `] ≤ Pr[z i−1 (ω0 ) ≤ `],

We use induction on `. The base case ` = 0 is trivial. Let ` ≥ 1. If i > 2, then

Pr[z i−1 (ω0 ) ≤ `] = p Pr[z i−2 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z i (ω0 ) ≤ ` − 1]
≥ p Pr[z i−1 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z i+1 (ω0 ) ≤ ` − 1]
= Pr[z i (ω0 ) ≤ `],
where the second step follows from the induction hypothesis. If i = 2, then we have
Pr[z 1 (ω0 ) ≤ `] = q + (1 − q − (1 − p)) Pr[z 1 (ω0 ) ≤ ` − 1]
+ (1 − p) Pr[z 2 (ω0 ) ≤ ` − 1]
≥ p Pr[z 1 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z 2 (ω0 ) ≤ ` − 1]
≥ p Pr[z 1 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z 3 (ω0 ) ≤ ` − 1]
= Pr[z 2 (ω0 ) ≤ `],
where the second step follows from the induction hypothesis. If i = 1, then we have
Pr[z 0 (ω0 ) ≤ `] = q + (1 − q) Pr[z 1 (ω0 ) ≤ ` − 1]
≥ q Pr[z 0 (ω0 ) ≤ ` − 1] + ( p − q) Pr[z 1 (ω0 ) ≤ ` − 1]
+ (1 − p) Pr[z 2 (ω0 ) ≤ ` − 1]
= Pr[z 1 (ω0 ) ≤ `],
where the second step follows from the induction hypothesis.
We now use Lemma 8.12 to argue that the random variable z(ω) is stochastically
dominated by the random variable z(ω0 ).
Lemma 8.13. For all i and ` in N, we have Pr[z i (ω) ≤ `] ≥ Pr[z i (ω0 ) ≤ `].
Proof. The proof is by induction on `. The base case ` = 0 is trivial. Let p j =
Pr[wi+1 ≤ j − 1 | wi = j], for j > 1, and q j = Pr[wi+1 = j | wi = j], for all j in N.
Note that the following inequalities hold: (i) p ≤ p j , for all j > 1, (ii) q ≤ min{ p1 , q0 },
and (iii) p ≥ q.
If i ≥ 2, then we have
Pr[z i (ω0 ) ≤ `] = p Pr[z i−1 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z i+1 (ω0 ) ≤ ` − 1]
≤ pi Pr[z i−1 (ω0 ) ≤ ` − 1] + (1 − pi ) Pr[z i+1 (ω0 ) ≤ ` − 1]
≤ pi Pr[z i−1 (ω0 ) ≤ ` − 1] + qi Pr[z i (ω0 ) ≤ ` − 1]

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

271

+ (1 − pi − qi ) Pr[z i+1 (ω0 ) ≤ ` − 1]
≤ pi Pr[z i−1 (ω) ≤ ` − 1] + qi Pr[z i (ω) ≤ ` − 1]
+ (1 − pi − qi ) Pr[z i+1 (ω) ≤ ` − 1]
= Pr[z i (ω) ≤ `].
(The second step holds because: (i) p ≤ pi , and (ii) Pr[z i−1 (ω0 ) ≤ `−1] ≥ Pr[z i+1 (ω0 ) ≤
` − 1], which follows from Lemma 8.12. The third step holds since Pr[z i (ω0 ) ≤ ` − 1] ≥
Pr[z i+1 (ω0 ) ≤ ` − 1] by Lemma 8.12. The fourth equation follows from the induction
hypothesis.) For i = 1, we have
Pr[z 1 (ω0 ) ≤ `] = q + ( p − q) Pr[z 1 (ω0 ) ≤ ` − 1] + (1 − p) Pr[z 2 (ω0 ) ≤ ` − 1]
≤ p1 + q1 Pr[z 1 (ω0 ) ≤ ` − 1] + (1 − p1 − q1 ) Pr[z 2 (ω0 ) ≤ ` − 1]
≤ p1 + q1 Pr[z 1 (ω) ≤ ` − 1] + (1 − p1 − q1 ) Pr[z 2 (ω) ≤ ` − 1]
= Pr[z 1 (ω) ≤ `].
(The second step holds because: (i) q ≤ p1 , (ii) 1 − p ≥ 1 − p1 − q1 , and (iii)
Pr[z 1 (ω0 ) ≤ ` − 1] ≥ Pr[z 2 (ω0 ) ≤ ` − 1], which follows from Lemma 8.12. The third
step follows from the induction hypothesis.)
For i = 0, we have
Pr[z 0 (ω0 ) ≤ `] = q + (1 − q) Pr[z 1 (ω0 ) ≤ ` − 1]
≤ q0 + (1 − q0 ) Pr[z 1 (ω0 ) ≤ ` − 1]
≤ q0 + (1 − q0 ) Pr[z 1 (ω) ≤ ` − 1]
= Pr[z 0 (ω) ≤ `].
(The second step holds because q ≤ q0 . The third step follows from the induction
hypothesis.) We have thus established the desired claim.
We now show that, in a probabilistic sense, the time to return to 0 is smaller for ω
than for ω0 . For any i, let τi (resp., τi0 ) denote the smallest j ≥ 0 such that wi+ j = 0
0
0
(resp., wi+
j = 0). We note that by letting hwi represent hmax{s , t}i, the terminating
∗
step τ is given by i + τi ∗ . Lemma 8.14 shows that, for any i, the random variable τi is
stochastically dominated by the random variable τi0 .
Lemma 8.14. For any i and j ≥ i, we have Pr[τi ≤ j] ≥ Pr[τi0 ≤ j].
Proof.

We have

Pr[τi ≤ j] =

X

Pr[wi = k] Pr[z k (ω) ≤ j]

0≤k≤i

≥

X

Pr[wi = k] Pr[z k (ω0 ) ≤ j]

0≤k≤i

≥

X

Pr[wi0 = k] Pr[z k (ω0 ) ≤ j]

0≤k≤i

= Pr[τi0 ≤ j].

272

C. G. Plaxton, R. Rajaraman, and A. W. Richa

(The first step follows from the definitions of τi and z i (ω). For the second step, we use
Lemma 8.13. For the third step, we first invoke Lemma 8.11 and then invoke Lemma A.1
with the substitution (i, k, Pr[wi = k], Pr[wi0 = k], Pr[z k (ω0 ) ≤ j]) for (m, i, pi , qi , n i ).
We note that one of the conditions for the latter invocation, namely, Pr[z k (ω0 ) ≤ j] is
nonincreasing with k, follows from Lemma 8.12. The fourth step follows from the
definitions of τi0 and z i (ω0 ).)
Lemma 8.14 indicates that we can obtain an upper bound on the time taken for the
random walk ω to return to 0 by deriving a corresponding bound for the random walk ω0 .
Indeed, we use Lemma 8.14 to obtain an upper bound on the length of any “excursion”
in ω. An excursion of length ` in a graph W with node set N is a walk that starts at node
0 and first returns to the start node at time `, for all ` in N. For all i such that wi = 0, let
`i (ω) be the random variable that gives the length of the excursion in ω starting at time
i. We note that, for all i, `i (ω) equals z 0 (ω).
The following lemma, which describes a probabilistic recurrence relation for the
length of an excursion in ω0 , is proved using a classical combinatorial result known as
Raney’s lemma [8], [18].
Lemma 8.15. Let p and q satisfy the inequality 1 − p ≤ ( p − q)2 . For all i and ` in N,
we have Pr[`i (ω0 ) = ` + 1 | wi0 = 0] ≤ max{1 − q, 5( p − q)} Pr[`i (ω0 ) = ` | wi0 = 0].
Proof.

Since ω0 is a random walk,

0
, wi0 ) = (u 0 , . . . , u i−1 , 0)] = Pr[`0 (ω0 ) = ` | w00 = 0]
Pr[`i (ω0 ) = ` | (w00 , . . . , wi−1

for any u 0 , . . . , u i−1 in N. For the remainder of the proof, we assume without loss of
generality that i is 0.
For ` = 1, the desired claim holds since Pr[`0 (ω0 ) = 2]/ Pr[`0 (ω0 ) = 1] = (1 − q).
We now consider ` ≥ 2. Let E j denote the event that the random walk does not reach
node 0 in the first j steps. That is, E j is the event that wk0 is nonzero for all k in [1, j].
For all j, let α j denote the probability that w0j+1 is 1 and E j+1 holds, given that w10 is 1.
For convenience, we assume that α−1 equals 1/( p − q). We obtain that
Pr[`0 (ω0 ) = `] = (1 − q) · α`−2 · q.

(10)

It thus follows that the ratio of Pr[`0 (ω0 ) = ` + 1] and Pr[`0 (ω0 ) = `] equals
α`−1 /α`−2 . The remainder of the proof is devoted to obtaining an upper bound on α j+1 /α j
for all j ≥ 0.
Let βm denote the probability that the following conditions hold given that w10 = 1:
0
= 1, and (iii) the edge (1, 1) is not traversed in any of the first
(i) E2m+1 holds, (ii) w2m+1
¡
¢
( p(1 −
2m + 1 steps. Using Raney’s lemma [8], [18], we have βm = (1/(2m + 1)) 2m+1
m
p))m . By the definitions of α j and βm , it follows that
X
βm · ( p − q) · α j−2m−1
αj =
0≤m≤b j/2c

µ
¶
2m + 1
1
( p(1 − p))m · ( p − q) · α j−2m−1 .
=
2m + 1
m
0≤m≤b j/2c
X

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

273

We now prove by induction on j ≥ 2 that α j+1 /α j is at most 5( p −q). The induction
base holds since α0 is 1 and α1 is 5( p − q). For the induction hypothesis, we assume
that α j+1 /α j is at most 5( p − q) for all j ≤ k − 1. If k is even, then we have
αk+1 /αk ≤ max αk−2m /αk−2m−1
0≤m≤k/2

≤ 5( p − q),
where the last step follows from the induction hypothesis. If k is odd, then we have
¶
µ
¶ ¶Á
½µ µ
k
k+2
1
1
( p − q)2 +
p
αk+1 /αk ≤ max
k (k − 1)/2
k + 2 (k + 1)/2
¶
¶
µ µ
k
1
( p − q) ,
k (k − 1)/2
¾
αk−2m
max
0≤m≤(k−3)/2c αk−2m−1
≤ max{5( p − q), 5( p − q)}
= 5( p − q),
where the second step follows from the induction hypothesis along with the inequalities
1 − p ≤ ( p − q)2 and
µ
¶
µ
¶
k+2
k
≤4
.
(k + 1)/2
(k − 1)/2
The claim of the lemma follows from the upper bound on αk+1 /αk and (10).
We are now ready to use the properties of the random walks ω and ω0 that are
stated in Lemmas 8.14 and 8.15 to analyze the random walk obtained by the sequence
hmax{s 0 , t}i. We set p = 1 − 2ε2 and q = 1 − 2ε. Lemmas 8.6 and 8.7 imply that ω
characterizes the random walk corresponding to the sequence hmax{s 0 , t}i. Consider the
random walk ω0 . We define a sequence hvi associated with hw0 i as follows: if w0j = 0,
then v j = G; otherwise, v j = B.
Lemma 8.16. Let i be in [(log n/b) − 1]. Given any fixed sequence hvii−1 of B, G
values, the probability that wi0 is 0 is at least 1 − 10ε.
Proof. Assume that v j = G. What is the probability that vi = G, i > j, if we
know that vk = B, for all integers k in the interval [ j + 1, i)? From Lemma 8.15, it
follows that this probability is at least 1 − 10ε. This is because Lemma 8.15 states that
1 − max{2ε, 10(ε − ε2 )} is a lower bound on the probability that there is an excursion
of length i − j starting at j in H 0 , given that there is an excursion of length at least i − j
1
by (6).)
starting at j in H 0 . (Note that 1 − p ≤ ( p − q)2 since ε < 10
Given any fixed B, G sequence hui j−1 , Pr[vi = G | (v0 , . . . , vi−1 ) = (u 0 , . . . , u j−1 ,
G, B, . . . , B)] = Pr[vi = G | (v j , . . . , vi−1 ) = (G, B, . . . , B)]. Since this holds for any
j > 0 and since wi0 = 0 if and only if vi = G, we have Pr[wi0 | (v0 , . . . , vi−1 ) =
(u 0 , . . . , u i−1 )] ≥ 1 − 10ε.

274

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Our main technical claim concerning the random walk ω now follows from Lemmas 8.14 and 8.16.
Lemma 8.17. For any i in [(log n)/b−1] and any nonnegative integer j, the probability
that τi is at least j is at most (10ε) j .
Proof. By Lemma 8.16, the probability that τi0 is at least j is at most (10ε) j . The desired
claim then follows from Lemma 8.14.
8.2.4. Proof of Theorem 1. We first derive upper bounds on E[c(xi , xi+1 )] and
E[c(yi , yi+1 )], for all i, using Lemma 8.17. Recall that ai and bi denote the radii of
X i1 and Yi1 , respectively, and i ∗ is the smallest integer i such that ai is at least c(x, y).
Lemma 8.18. For any i in [(log n)/b−1], E[c(xi , xi+1 )] = O(ai ) and E[c(yi , yi+1 )] =
O(bi ).
Proof. We first observe that c(xi , xi+1 ) (resp., c(yi , yi+1 )) is at most ak (resp., bk ),
where k is the least j ≥ i such that s j (resp., t j ) belongs to {0, 1, 2} (resp., {0}); if such
a j does not exist, then k is (log n)/b − 1. Thus, k is at most i + τi . By Lemma 8.17, it
follows that, for any j ≥ i, the probability that k ≥ j is at most (10ε) j−i . By Lemma 8.4,
we thus have
X
ai (10ε) j−i 2db logδ 2e( j−i) = O(ai )
E[c(xi , xi+1 )] ≤
j≥i

and
E[c(yi , yi+1 )] ≤

X

bi (10ε) j−i 2db logδ 2e( j−i) = O(bi ),

j≥i

since 10ε2db logδ 2e < 1 by (6).
We now use Lemmas 8.4, 8.17, and 8.18 to establish Theorem 1.
Proof of Theorem 1. By (9), the
P expected cost of the read operation is bounded by
the expected value of f (`(A)) 0≤i<τ O(c(xi , xi+1 ) + c(yi , yi+1 )). (Recall that τ is
theP
smallest integer i ≥ i ∗ such that (si , ti ) =P(0, 0).) We upper bound the two terms
E[ 0≤i<i ∗ (c(xi , xi+1 ) + c(yi , yi+1 ))] and E[ i ∗ ≤i<τ (c(xi , xi+1 ) + c(yi , yi+1 ))] separately. By Lemmas 8.4 and 8.18, the first term is O(ai ∗ + bi ∗ ). We upper bound the
second term as follows. Since τ is i ∗ + τi ∗ , we obtain from Lemma 8.17 that, for any
j ≥ 0, the probability that τ ≥ i ∗ + j is at most (10ε) j . Therefore,
#
"
X
X
(c(xi , xi+1 ) + c(yi , yi+1 )) ≤
j (10ε) j (ai ∗ + j + bi ∗ + j )
E
i ∗ ≤i<τ

j≥0

≤

X
j≥0

j (10ε) j 2 jdb logδ 2e (ai ∗ + bi ∗ )

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

275

= O(ai ∗ + bi ∗ )
= O(c(x, y)).
(The second step follows from Lemma 8.4. The third step holds since 10ε2db logδ 2e < 1
by (6). The fourth step follows from Lemma 8.4.)

9.

Analysis for Other Performance Bounds

We begin by establishing some useful properties of neighbors in Section 9.1. These
properties are then used to derive bounds on the cost of the insert and delete operations,
the size of the auxiliary memory, and the adaptability of our access scheme in Sections 9.2,
9.3, and 9.4, respectively.
9.1.

Properties of Neighbors

In this section we establish certain claims concerning the different types of neighbors
that are defined in Section 4. We differentiate between root and nonroot primary (i, j)neighbors. A root primary (i, j)-neighbor w of v is a primary (i, j)-neighbor w of v
such that either w[i] 6= j or i = (log n)/b − 1. A primary neighbor that is not a root
primary neighbor is a nonroot primary neighbor. Note that, for i < (log n)/b − 1, if u
is a root primary (i, j)-neighbor of v, then u[`] equals v[`], for each ` in [i], and there
is no node w in V such that w[i] equals j and w[`] equals v[`], for each ` in [i]. We
illustrate the difference between a root primary neighbor and a nonroot primary neighbor
by an example. Consider a network of n = 26 nodes. Thus, each node has a 6-bit label.
Let b equal 2. Consider nodes u and v with labels 100111, and 110011, respectively.
If v is a primary (1, 00)-neighbor of u, then v is a nonroot primary neighbor. On the
other hand, if v is a primary (1, 01) neighbor of u, then v is a root primary neighbor
since v[1] = 00 6= 01. Moreover, this case implies that there is no node in the network
with a label for which the four rightmost bits are 1101. (Note that the neighbor table is
constructed by matching rightmost bits.) Finally, we observe that since (log n)/b−1 = 2,
any primary (2, ·) neighbor is a root primary neighbor.
Lemma 9.1. Let u and v be in V , and let k denote |M(u, c(u, v))|. For any j in [2b ],
we have (i) for any i in [(log n)/b −1], the probability that u is a primary (i, j)-neighbor
(i+1)b
, and (ii) for any i in [(log n)/b], the probability that u is
of v is at most e−((k/1)−2)/2
(i+1)b
a root primary (i, j)-neighbor of v is at most e−n/2
.
Proof. Consider the ball M(v, c(v, u)). By equation (1), |M(v, c(v, u))| ≥
|M(v, 2c(v, u))|/1. Since M(v, 2c(v, u)) is a superset of M(u, c(u, v)), we have
|M(v, c(v, u))| ≥ k/1. The probability that a node w in M(v, c(u, v))\{u, v} does
not match the label of v in its (i + 1)b rightmost bits is at most 1 − 1/2(i+1)b . Since i is
less than (log n)/b − 1, the probability that u is a primary (i, j)-neighbor of v is at most
(i+1)b

(1 − 1/2(i+1)b )(k/1)−2 ≤ e−((k/1)−2)/2

.

276

C. G. Plaxton, R. Rajaraman, and A. W. Richa

If u is a root primary (i, j)-neighbor of v, then u[`] equals v[`] for each ` in [i] and
there is no node w in V such that w[i] equals j and w[`] equals v[`] for each ` in [i].
Therefore, the probability that u is a root primary (i, j)-neighbor of v is at most
(i+1)b

(1/2ib )(1−1/2(i+1)b )n−1 (1−1/2b ) ≤ (1/2ib )(1−1/2(i+1)b )n ≤ (1/2ib )e−n/2

.

Corollary 9.1.1. Let u and v be in V , let i be in [(log n)/b], and let j be in [2b ]. If u
is a primary (i, j)-neighbor of v, then v is in N (u, O(2ib log n)) whp.
Lemma 9.2 and Corollary 9.2.1 below establish bounds on the number of nodes v
such that u is a primary or secondary neighbor of v, and on the number of nodes v such
that v is a reverse neighbor of u, respectively. For any u in V , let au denote the total
number of triples (i, j, v) such that i belongs to [(log n)/b], j belongs to [2b ], v belongs
to V , and u is a primary or secondary (i, j)-neighbor of v. Lemma 9.2 is used in the
proof of Theorem 4, while Corollary 9.2.1 is used in the proofs of Theorems 2 and 3.
Lemma 9.2. Let u be in V and let i be in [(log n)/b]. Then the number of nodes for
which u is an ith level primary neighbor is O(log n) whp. Also, E[au ] = O(log n) and
au is O(log2 n) whp.
Proof. Given a node v in V , i in [(log n)/b−1], and j in [2b ], it follows from Lemma 9.1
(i+1)b
.
that the probability that u is a root primary (i, j)-neighbor of v is at most (1/2ib )e−n/2
b
Given a node v in V and j in [2 ], the probability that u is a root ((log n)/b, j)-primary
neighbor of v is at most 1/n.
Fix j in [2b ]. Let ` equal (log n − log log n)/b − Ä(1), where the constant in the
Ä(1) term is chosen sufficiently large. We consider two cases: i < ` and i ≥ `. If i < `,
then the probability that there exists v in V such that u is a root primary (i, j)-neighbor
of v is at most
(i+1)b

n(1/2ib )e−n/2

≤ ne−Ä(log n) = O(1/ poly(n)).

If i ≥ `, then, given v in V , the probability that u is a root primary (i, j)-neighbor of
v is at most 1/2`b = O((log n)/n). It follows from Chernoff bounds [6] that the number
of nodes for which u is a root primary (i, j)-neighbor is O(log n) whp.
We now consider secondary and nonroot primary neighbors. For any i in [(log n)/b],
if u is a secondary or nonroot primary (i, j)-neighbor of v, then j is u[i] and u is one of
the d + 1 nodes w in V with minimum c(v, w) whose rightmost ib bits match those of
v. We now fix u and i, set j to u[i], and obtain an upper bound on the probability that u
is one of the at most d + 1 nodes w with minimum c(v, w) and whose rightmost ib bits
match those of v.
Consider a node v in N (u, µk+1 2(i+1)b )\N (u, µk 2(i+1)b ), where µ is a real constant
to be specified later. If k equals zero, then the probability that u is a primary or secondary
(i, j)-neighbor of v is at most 1/2ib . Otherwise, consider the ball M(v, c(v, u)). By
the low-expansion condition (i.e., the right inequality of (1)), |M(v, c(v, u))| is at least
|M(v, 2c(v, u))|/1. We are given that M(u, c(u, v)) is a superset of N (u, µk 2(i+1)b ).
Since M(v, 2c(v, u)) is a superset of M(u, c(u, v)), we obtain that |M(v, c(v, u))| is at

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

277

least µk 2(i+1)b /1. The probability that u is a primary or secondary (i, j)-neighbor of v
is at most
µ k (i+1)b ¶
/1
µ 2
k (i+1)b
/1)−d
/(2ib 2(i+1)bd )
(1 − 1/2(i+1)b )(µ 2
d
d
≤ d(eµk 2(i+1)b /(1d))d e−µ
≤ 4d(eµk /(1d))d (e−µ
≤ 1/((2µ)k 2ib ).

k

/1

k

/1

(1 − 1/2(i+1)b )−d /(2ib 2(i+1)bd )

/2ib )

(The second step holds since d ≤ 2b ≤ 2ib and (1 − 1/2ib )−2 is at most 4. The third
k
step follows by choosing µ large enough with respect to 1 and d such that eµ /1 ≥
k k
d−1
k
d+1
for all k ≥ 1.)
(2 1 /d )(µ /1)
Thus, the expected number of nodes for which u is a secondary or nonroot primary
neighbor is at most
X
X
X
1/((2µ)k 2ib )
ib

i∈[(log n)/b], j=u[i] k≥0 v∈N (u,µk+1 2(i+1)b )\N (u,µk 2(i+1)b )

≤

X

2b µ = O(log n).

i∈[(log n)/b], j=u[i]

To obtain a high probability bound on the number of nodes for which u is a secondary
or nonroot primary neighbor, we proceed as follows. For any v not in
N (u, 2(2(i+1)b log n)), it follows from Lemma 9.1 that the probability that u is a secondary or nonroot primary (i, j)-neighbor of v is O(1/ poly(n)). For any v in
N (u, 2(2(i+1)b log n)), the probability that u is a secondary or nonroot primary (i, j)neighbor of v is at most 1/2(i+1)b . Therefore, the number of nodes for which u is a
secondary or nonroot primary neighbor is O(log2 n) whp.
The bounds on expectation and the high probability bounds together establish that
E[au ] is O(log n) and au is O(log2 n) whp.
Corollary 9.2.1. For any u in V , the total number of reverse neighbors of u is O(log2 n)
whp, and O(log n) expected.
Proof. The desired claim follows directly from Lemma 9.2, since v is a reverse (i, j)neighbor of u if and only if u is a primary (i, j)-neighbor of v.
Lemma 9.3 is used in the proof of Theorem 3. For a given a node u, it provides
a bound on the number of primary neighbor sequences that have u in the ith position.
For any u and v in V and i in [(log n)/b], v is said to be an i-leaf of u if there exists
a sequence v = v0 , v1 , . . . , vi−1 , vi = u, such that, for all j in [i], v j+1 is a primary
( j, v j+1 [ j])-neighbor of v j .
Lemma 9.3. Let u belong to V , and let i be in [(log n)/b]. Then the number of i-leaves
of u is O(2ib log n) whp.

278

C. G. Plaxton, R. Rajaraman, and A. W. Richa

Proof. We establish the lemma by showing that if v is an i-leaf of u, then v is in
N (u, O(2ib log n)) whp. By Corollary 9.1.1, we have that, for all j in [i], v j is in
N (v j+1 , c0 2( j+1)b log n) whp for some sufficiently large real constant c0 . We will prove
by induction on j in [i + 1] that v = v0 is in N (v j , c0 2 jb log n) whp.
The induction base follows trivially. For the induction step, assume that v is in
N (v j , c0 2 jb log n). By Corollary 9.1.1, v j belongs to N (v j+1 , c0 2 jb log n) whp. Let r0
and r1 denote c(v j+1 , v j ) and c(v j , v), respectively. The node v is contained in the ball
M(v j+1 , r0 + r1 ). If r0 is at least r1 , then |M(v j+1 , r0 + r1 )| is at most 1c0 2 jb log n by
(1). Otherwise, |M(v j+1 , r0 + r1 )| is at most |M(v j , 3r1 ), which is at most 12 c0 2 jb log n
by (1). We thus obtain that v is in N (v j+1 , 12 c0 2 jb log n). Since 12 < 2b (by (2)), we
obtain that v is in N (v j+1 , c0 2( j+1)b ).
Applying the above inductive claim with j = i, we obtain that v is in
N (u, O(2ib log n)) whp. This completes the proof.

9.2.

Insert and Delete Operations

Theorem 2 follows from Lemmas 8.4, 8.18, and 9.2.
Proof of Theorem 2. Consider an insert operation
P performed by x for any object. The
expected cost of the operation is bounded by E[ 0≤i<log n/b c(xi , xi+1 )], which by Lemmas 8.4 and 8.18 is O(a(log n)/b−1 ) = O(C).
We now consider the cost of the delete operation. By Lemma 9.2, for each i, the
number of reverse (i, j)-neighbors of xi for any j is O(log n) whp, where xi is the ith
node in the primary neighbor sequence of x. Therefore, thePexpected cost of the delete
operation executed by x is bounded by the product of E[ 0≤i<log n/b c(xi , xi+1 )] and
O(log n). By Lemma 8.18, it follows that the expected cost of a delete operation is
O(C log n).

9.3.

Auxiliary Memory

Proof of Theorem 3. We first place an upper bound on the size of the neighbor table
of any u in V . By definition, the number of primary and secondary neighbors of u is at
most (d + 1)2b (log n)/b, which is O(log n). By Corollary 9.2.1, the number of reverse
neighbors of u is O(log2 n) whp.
We next place an upper bound on the size of the pointer list of any u in V . The size
of Ptr(u) is at most the number of triples of the form (A, v, ·), where A is in A and v is in
V such that (i) there exists i in [(log n)/b] such that v is an i-leaf of u, (ii) A[ j] = u[ j]
for all j in [i], and (iii) A is in the main memory of v.
By Lemma 9.3, the number of i-leaves of u is O(2ib log n) whp. The probability
that A[ j] = u[ j], for all j in [i], is at most 1/2ib . Since the number of objects in
the main memory of any node is at most `, it follows that whp, |Ptr(u)| is at most
P
2
i∈[log n/b] O(` log n) which is O(` log n).
Combining the bounds on the sizes of the neighbor table and pointer list, we obtain
that the size of the auxiliary memory of u is O(` log2 n) whp.

Accessing Nearby Copies of Replicated Objects in a Distributed Environment

9.4.

279

Adaptability

Proof of Theorem 4. By Lemma 9.2, for any node u, the number of nodes for which u
is a primary or secondary neighbor is O(log n) expected and O(log2 n) whp. Moreover,
u is a reverse neighbor of O(log n) nodes since u has O(log n) primary neighbors.
Therefore, the adaptability of our scheme is O(log n) expected and O(log2 n) whp.

10.

Future Work

We would like to extend our study to more general classes of cost functions and determine tradeoffs among the various complexity measures. It would also be interesting
to consider models that allow faults in the network. We believe that our access scheme
can be extended to perform well in the presence of faults, as the distribution of control
information in our scheme is balanced among the nodes of the network.

Acknowledgments
The authors would like to thank Madhukar Korupolu and Satish Rao for several helpful discussions.

Appendix.

A Technical Lemma

Lemma A.1. Let m be a nonnegative integer and let hni be a sequence of m nonincreasingPreals. Let h pi
of m P
reals each such that (i) for all
Pand hqi be two sequences
P
j in [m], 0≤i≤ j pi ≥ 0≤i≤ j qi and (ii) 0≤i≤m pi = 0≤i≤m qi . Then we have
X

pi n i ≥

0≤i≤m

X

qi n i .

0≤i≤m

Proof. The proof is by induction on m. The induction basis is trivial. For the induction
hypothesis, we assume that the statement of the lemma holds for m. We now establish
the claim for m + 1:
X
X
pi n i = q0 n 0 + ( p0 − q0 )n 0 +
pi n i
0≤i≤m+1

1≤i≤m+1

≥ q0 n 0 + ( p0 − q0 + p1 )n 1 +
≥ q0 n 0 +
=

X

X

X

pi n i

2≤i≤m+1

qi n i

1≤i≤m+1

qi n i .

0≤i≤m

(The third step follows from the induction hypothesis and the inequalities n 0 ≥ n 1 and
note that the induction hypothesis
p0 ≥ q0 . We P
P can be invoked
P since p0 − q0 + p1 +
P
p
≤
q
and
p
−
q
+
p
+
p
=
i
i
0
0
1
i
2≤i≤ j
1≤i≤ j
2≤i≤m+1
1≤i≤m+1 qi .)

280

C. G. Plaxton, R. Rajaraman, and A. W. Richa

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]

[8]
[9]

[10]

[11]

[12]
[13]

[14]
[15]
[16]

[17]
[18]
[19]
[20]

B. Awerbuch, Y. Bartal, and A. Fiat. Distributed paging for general networks. Journal of Algorithms,
28:67–104, 1998.
B. Awerbuch and D. Peleg, Routing with polynomial communication space tradeoff. SIAM Journal on
Discrete Mathematics, 5:151–162, 1990.
B. Awerbuch and D. Peleg, Sparse partitions. In Proceedings of the 31st Annual IEEE Symposium on
Foundations of Computer Science, pages 503–513, 1990.
B. Awerbuch and D. Peleg. Online tracking of mobile users. Journal of the ACM, 37:1021–1058, 1995.
Y. Bartal, A. Fiat, and Y. Rabani. Competitive algorithms for distributed data management. In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, pages 39–50, May 1992.
H. Chernoff, A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of
observations. Annals of Mathematical Statistics, 23:493–509, 1952.
S. Dolev, E. Kranakis, D. Krizanc, and D. Peleg. Bubbles: adaptive routing scheme for high-speed
dynamic networks. In Proceedings of the 27th Annual ACM Symposium on Theory of Computing,
pages 528–537, May 1995.
R. L. Graham, D. E. Knuth, and O. Patashnik. Concrete Mathematics, Addison-Wesley, Reading, MA,
1989.
J. D. Guyton and M. F. Schwartz. Locating nearby copies of replicated Internet servers. In Proceedings
of the 1995 ACM SIGCOMM Conference on Applications, Technologies, Architectures, and Protocols
for Computer Communication, pages 288–298, August 1995.
D. Karger, E. Lehman, F. T. Leighton, M. Levine, D. Lewin, and R. Panigrahy. Consistent hashing
and random trees: distributed caching protocols for relieving hot spots on the World Wide Web. In
Proceedings of the 29th Annual ACM Symposium on Theory of Computing, pages 654–663, May 1997.
R. Karp, M. Luby, and F. Meyer auf der Heide, Efficient PRAM simulation on a distributed memory
machine. In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, pages 318–326,
May 1992.
C. E. Leiserson, Fat-trees: universal networks for hardware-efficient supercomputing. IEEE Transactions
on Computers C-34:892–900, 1985.
B. M. Maggs, F. Meyer auf der Heide, B. Vöcking, and M. Westermann. Exploiting locality for data
management in systems of limited bandwidth. In Proceedings of the 38th Annual IEEE Symposium on
Foundations of Computer Science, pages 284–293, October 1997.
S. J. Mullender, editor. Distributed Systems. Addison-Wesley, Reading, MA, 1993.
S. J. Mullender. and P. M. B. Vitányi. Distributed match-making. Algorithmica, 3:367–391, 1988.
C. G. Plaxton and R. Rajaraman. Fast fault-tolerant concurrent access to shared objects. In Proceedings
of the 37th Annual IEEE Symposium on Foundations of Computer Science, pages 570–579, October
1996.
A. G. Ranade. How to emulate shared memory. Journal of Computer and System Sciences, 42:307–326,
1991.
G. N. Raney, Functional composition patterns and power series reversion. Transactions of the American
Mathematical Society, 94:441–451, 1960.
E. Upfal and A. Wigderson. How to share memory in a distributed system. Journal of the ACM, 34:116–
127, 1987.
M. van Steen, F. J. Hauck, and A. S. Tanenbaum. A model for worldwide tracking of distributed objects.
In Proceedings of the 1996 Conference on Telecommunications Information Networking Architecture
(TINA 96), pages 203–212, September 1996.

Received December 19, 1997, and in final form October 16, 1998.

27

Scale-Free Compact Routing Schemes in Networks
of Low Doubling Dimension
GORAN KONJEVOD, Lawrence Livermore National Laboratory
ANDRÉA W. RICHA, Arizona State University
DONGLIN XIA, Google

We consider compact routing schemes in networks of low doubling dimension, where the doubling dimension
is the least value α such that any ball in the network can be covered by at most 2α balls of half radius. There
are two variants of routing-scheme design: (i) labeled (name-dependent) routing, in which the designer is
allowed to rename the nodes so that the names (labels) can contain additional routing information, for
example, topological information; and (ii) name-independent routing, which works on top of the arbitrary
original node names in the network, that is, the node names are independent of the routing scheme.
In this article, given any constant  ∈ (0, 1) and an n-node edge-weighted network of doubling dimension
α ∈ O(loglog n), we present
—a (1+)-stretch labeled compact routing scheme with log n-bit routing labels, O(log2 n/ loglog n)-bit packet
headers, and (( 1 ) O(α) log3 n)-bit routing information at each node;
—a (9 + )-stretch name-independent compact routing scheme with O(log2 n/ loglog n)-bit packet headers,
and (( 1 ) O(α) log3 n)-bit routing information at each node.
2

In addition, we prove a lower bound: any name-independent routing scheme with o(n(/60) ) bits of storage
at each node has stretch no less than 9− for any  ∈ (0, 8). Therefore, our name-independent routing scheme
achieves asymptotically optimal stretch with polylogarithmic storage at each node and packet headers.
Note that both schemes are scale-free in the sense that their space requirements do not depend on
the normalized diameter  of the network. We also present a simpler nonscale-free (9 + )-stretch nameindependent compact routing scheme with improved space requirements if  is polynomial in n.

r

CCS Concepts: Networks → Routing protocols;
design problems; Sparsification and spanners

r Theory of computation → Routing and network

Additional Key Words and Phrases: Scale free, labeled routing, name-independent routing, compact routing,
doubling dimension
ACM Reference Format:
Goran Konjevod, Andréa W. Richa, and Donglin Xia. 2016. Scale-free compact routing schemes in networks
of low doubling dimension. ACM Trans. Algorithms 12, 3, Article 27 (June 2016), 29 pages.
DOI: http://dx.doi.org/10.1145/2876055

1. INTRODUCTION

In this article, we study routing schemes, that is, distributed network algorithms capable of routing network packets from an arbitrary source to an arbitrary destination
node in the network. The objective of a routing scheme is to find an efficient path from
This work is supported by the National Science Foundation, under grant CCF-0830791.
Authors’ addresses: G. Konjevod, Lawrence Livermore National Laboratory, Livermore, CA 94550; email:
konjevod1@llnl.gov; A. W. Richa, Computer Science, CIDSE, Arizona State University, Tempe, AZ 85287;
email: aricha@asu.edu; D. Xia, Google, Kirkland, WA, 98053; email: dxia@google.com.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c 2016 ACM 1549-6325/2016/06-ART27 $15.00

DOI: http://dx.doi.org/10.1145/2876055

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:2

G. Konjevod et al.

the given source to any destination. We measure the quality of a path by its length, for
which the length of a path is given by the sum of the weights of its edges, thus consider
its efficiency to be expressed by the ratio of its length to the length of the shortest path
between the same source and destination. If every source stored a complete description
of the network, it would be easy to route along the shortest paths. This could even be
done if each source stored just the next hop of the shortest path to each destination in
its routing table, resulting in routing tables of size linear on the number of nodes. For
scalability, we would like to restrict the amount of storage available to each node to
be polylogarithmic in the size of the network. Hence, in this article, we study compact
routing schemes, defined to be those for which the size of the routing tables and packet
headers is only polylogarithmic in the size of the network.
A routing scheme consists of two parts:
—the preprocessing step, in which the routing tables are configured at every node, and
—the routing algorithm, which is used by the nodes to perform the actual routing of
packets.
In the routing algorithm, given a destination’s name, the source node sets up a
packet header and sends the packet to one of its neighbors, based on the destination’s
name and the local routing table. A relay node, upon the reception of a packet, decides
whether the packet has reached its destination and, if not, where to forward it, based
on the packet header and the local routing table.
There are two variants of routing scheme design:
(1) name-dependent (or labeled) routing, for which the designer is allowed to rename
the nodes so that the names (labels) can contain additional routing information,
for example, topological information; and
(2) name-independent routing, which works on top of the arbitrary original node names
in the network, that is, the node names are independent of the routing scheme.
A labeled routing scheme has the advantage of embedding information in the node
labels to facilitate routing, but it also requires the source node to know the designergiven label of the destination node, which is not always feasible. Given a labeled routing
scheme, it remains an issue to determine how (and where) the source will find the
label of the destination. Therefore, name-independent routing schemes are preferable,
especially in applications with intrinsic requirements on node names (e.g., distributed
hash tables [Abraham et al. 2004]), those that require randomly distributed node
names (e.g., Chord [Stoica et al. 2001]), or those that perform network operations such
as locating nearby copies of replicated objects and tracking of mobile objects [Abraham
et al. 2004; Awerbuch and Peleg 1990].
As indicated earlier, a fundamental trade-off in routing is between the quality of
routing paths and the space overhead introduced by the routing tables and packet
headers. Formally, the stretch of a routing scheme is the maximum ratio of the length of
the path by which a packet is delivered to the length of the shortest source–destination
path, over all source–destination pairs. The space requirement of a scheme refers to the
size of routing tables maintained at each node and the size of the packet headers used
by the scheme. Recall that a routing scheme is compact if the size of the routing table
at each node and the size of every packet header are bounded by a polylogarithmic
function of the number of nodes.
We now introduce a network parameter that strongly influences the design of routing
schemes. The normalized diameter  of a graph is the ratio of the largest to the smallest
shortest path distance in the graph. For many routing schemes [Awerbuch and Peleg
1990; Talwar 2004; Chan et al. 2005; Slivkins 2005a; Konjevod et al. 2006], the routing
table size at each node, the packet header size, or the routing label directly depend
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:3

on a polylogarithmic function of . While those schemes are compact for networks in
which  is polynomial in n, they do not scale well if  grows exponentially with n.
Hence, one would prefer a compact routing scheme that does not directly depend on
. We call a routing scheme scale-free if the space requirements for its routing tables,
packet headers, and routing labels are independent of the normalized diameter. While
the schemes we present as our main results in this article are scale-free, they have
simpler variants that are not, as the one we present in Section 3.3 and the nonscalefree algorithms presented in Abraham et al. [2006c].
Early on in the study of routing schemes, it was shown that a routing scheme with
stretch less than 2k + 1 on a general graph must have a space requirement of (n1/k)
bits at some nodes [Thorup and Zwick 2001; Abraham et al. 2006a]. (For more details on
previous work, refer to Section 1.3.) Thus, compact routing on general graphs requires
larger than constant stretch. In order to allow better results, one must restrict the
structure of the metric space induced by the network. Two classes of networks—growthbounded and low doubling dimension—have been particularly well studied, which we
briefly discuss here.
1.1. Bounded Doubling Dimension

In this section, we briefly discuss the class of bounded doubling dimension networks,
of which the growth-bounded networks are a subclass. A ball of radius r centered at a
node v is the set of nodes u such that the shortest-path distance from v to u is at most
r. Doubling dimension is formally defined as the least value α such that any ball can
be covered by at most 2α balls of half radius.
The bounded doubling dimension networks (doubling networks for short) are characterized by the condition that each ball of radius r can be covered by a constant number
of balls of radius r/2 (i.e., α is a constant). The growth-bounded networks are characterized by the condition that the number of nodes within distance 2r of any node is at most
a constant factor more than the number of nodes within distance r. Since these are
really properties of the induced metric space, we will often use the terms “network” and
“metric” interchangeably, always referring to the metric space induced by the nodes
and the shortest-path distance function of the (possibly weighted) network.
Many problems become easier in growth-bounded metrics and those of bounded
doubling dimension [Gupta et al. 2003; Krauthgamer et al. 2004; Krauthgamer and
Lee 2004; Talwar 2004; Slivkins 2005a, 2005b; Chan et al. 2005; Kleinberg et al. 2009;
Karger and Ruhl 2002; Cole and Gottlieb 2006], including metric embeddings, the
traveling salesman problem, compact data structures, distance estimation, and finding
nearest neighbors.
It is easy to see that every growth-bounded metric is also of finite doubling dimension, while the opposite may not be true. An example of a growth-bounded metric
is induced by the d-dimensional grid. However, if points are excluded from the grid
and we consider its subgraph, the resulting metric may not be growth bounded anymore. It will, however, still have bounded doubling dimension. Growth-bounded metrics
have been shown to provide a good approximation of communication cost metrics on
Internet-like networks. Bounded doubling metrics provide a natural generalization of
growth-bounded metrics, as illustrated by the example, thus the increased interest in
this class in recent years.
The distinction between growth-bounded and bounded doubling networks is also
reflected in the stretch of compact routing schemes for the two classes. On growthbounded networks, there exist name-independent compact routing schemes with
stretch arbitrarily close to 1, while such results are not possible for networks of
bounded doubling dimension. Even name-independent routing schemes with only constant stretch in doubling networks have eluded several researchers.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:4

G. Konjevod et al.

Our main result in this article is to present a lower bound and a matching constructive
upper bound (by means of a scale-free algorithm) on the stretch achievable by a nameindependent compact routing scheme in doubling metrics.
Intuitively, most of the compact routing schemes designed so far use a hierarchical
data structure to reduce the routing problem to a setting that is “almost” a regular
Euclidean grid. The problem becomes challenging in networks that are not growth
bounded, because such hierarchical data structures assume a regular increase in the
number of nodes with the distance from the source. This may not hold in doubling
networks; thus, the hierarchies must be based not only on the distance from the source,
but also on the number of nodes actually seen by moving up to that distance. This issue
was first addressed by Abraham et al. [2006c] using notions of dense and sparse levels.
We offer a different approach, more directly based on natural properties of doubling
metrics, and believe our results to be simpler as a consequence.
1.2. Our Contributions.

Our main results are a (9 + )-stretch scale-free name-independent compact routing
scheme for networks of low doubling dimension for any  > 0 (Theorem 1.1), and a
matching lower bound of (9 −   ) for any   > 0, on the stretch of any name-independent
compact routing scheme in doubling networks (Theorem 1.2). We formally define a
network to have low doubling dimension if α = O(loglog n) (hence, they form a superset
of bounded doubling networks).
THEOREM 1.1. Given any constant  ∈ (0, 1) and a weighted undirected graph G
with n nodes and doubling dimension α, we present a scale-free name-independent
routing scheme for G with (9 + )-stretch, O(log2 n/ log log n)-bit packet headers, and
(( 1 ) O(α) log3 n)-bit routing information at each node.
THEOREM 1.2. For any constant  ∈ (0, 8), there exists a weighted undirected graph G
with n nodes, doubling dimension α ≤ 6 − log , and normalized diameter  = O(21/ n)
such that any name-independent routing scheme on G that uses routing tables of size
2
o(n(/60) ) bits at each node has stretch at least 9 − .
Hence, by Theorems 1.1 and 1.2, our name-independent routing scheme is the first
name-independent scale-free compact routing scheme for networks of low doubling dimension with (asymptotically) optimal stretch, closing the gap left by Theorem 1.4 (in
which an optimal-stretch, but not scale-free, name-independent scheme is presented)
and in Abraham et al. [2006c] (in which a scale-free, but not optimal-stretch, nameindependent scheme is presented). By asymptotically optimal stretch, we mean that
the stretch of our algorithm is 9 +  for any fixed constant  ∈ (0, 1), while for any
  ∈ (0, 8) there is an instance, that is, a network, such that any name-independent
compact routing scheme has stretch at least 9 −   . Note that the graphs G that give
the lower bound in Theorem 1.2 allow for polynomially sized routing tables (rather
than just polylogarithmic in size) and have polynomial normalized diameter (thus, the
lower bound applies even if we restrict our attention to networks with low normalized
diameter).
In addition, we also present an optimal-stretch scale-free labeled compact routing
scheme for networks of low doubling dimension, as stated in Theorem 1.3:
THEOREM 1.3. Given any constant  ∈ (0, 1) and a weighted undirected graph G
with n nodes and doubling dimension α, we present a (1 + )-stretch labeled routing
scheme for G with log n-bit routing labels, O(log2 n/ loglog n)-bit packet headers, and
(( 1 ) O(α) log3 n)-bit routing information at each node.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:5

Our contributions for the labeled routing model are twofold. First, our algorithm is
the first (asymptotically) optimal-stretch scale-free compact labeled routing scheme for
networks of low doubling dimension that uses optimal log n-bit routing labels1 (hence,
embeds the minimal required amount of network-dependent routing information into
the routing labels). Second, our techniques are significantly simpler than the ones used
by Abraham et al. [2006c], who also present an asymptotically optimal-stretch scalefree labeled routing scheme (they use 2 O(α) log3 n-bit routing labels, though). Our labeled
routing scheme relies on a simple and unifying hierarchical network decomposition
technique using a ball-packing, rather than the complex sparse-dense decomposition
of Abraham et al. [2006c].
As an introduction to our scale-free name-independent routing scheme presented in
Section 4, we also present a simpler nonscale-free name-independent compact routing
scheme, as stated in Theorem 1.4, which is an improved version of our work in Konjevod
et al. [2006]. Note that, if the normalized diameter  is a polynomial in n, the space
bounds of the scheme in Theorem 1.4 are actually better than those of the scale-free
scheme in Theorem 1.1, since it only requires (( 1 ) O(α) log2 n)-bit routing information at
each node and O(log n)-bit packet headers.
THEOREM 1.4. Given any constant  ∈ (0, 1) and a weighted undirected graph G with
n nodes and doubling dimension α, we present a name-independent routing scheme for
G with (9 + )-stretch, O(log n)-bit packet headers, and (( 1 ) O(α) log  log n)-bit routing
information at each node.
We believe that some of the techniques introduced in this article are a major contribution on their own. The new techniques and data structures presented here enable us
to go beyond the results in both Theorem 1.4 and Abraham et al. [2006c], and obtain
an optimal-stretch scale-free name-independent scheme for networks of low doubling
dimension. In particular, we believe that our ball-packing decomposition, used in both
the name-independent and labeled routing schemes, will have an impact on other
problems that also rely on a hierarchical structure of r-nets (see Definition 2.1). In a
nutshell, both types of schemes rely on a global hierarchy of r-nets. In order to avoid a
dependence on , we cannot store information for all O(log ) layers of r-nets: we only
maintain information about O(log n) layers at each node, while packing balls are used
to account for the layers for which no information exists at a node.
1.3. Related Work

Not surprisingly, there has been a vast amount of research on efficient network routing
schemes. General overviews are available in Peleg [2000] and the surveys by Gavoille
[2001] and Gavoille and Peleg [2003].
There are lower bound results for both labeled and name-independent models. For
the labeled model, Thorup and Zwick [2001] showed that there exist graphs such that
every labeled routing scheme with stretch less than 2k + 1 for k = 1, 2, 3, 5 must have
(n1/k)-bit routing tables at some nodes. For the name-independent model, Abraham
et al. [2006a] showed that there exist graphs such that every name-independent routing
scheme with stretch less than 2k + 1 must have ((n log n)1/k)-bit routing tables on
some nodes. However, the graphs that they designed have large doubling dimension,
namely, (log n). In this article, for any fixed  ∈ (0, 8), we show that there exists
a tree with constant doubling dimension (no more than 6 − log ) and normalized
1 Following an extensive line of work for doubling metrics on optimal stretch labeled compact routing schemes,
as summarized in Table II.

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:6

G. Konjevod et al.
Table I. Name-Independent Routing Schemes in Networks with n Nodes, Doubling
Dimension α, and Normalized Diameter 
Reference

Stretch

Routing Table (in bits)

Header (in bits)

log  log n

O(1)

2 O(α)

O(1)

2 O(α) log4 n

Theorem 1.4

9+

( 1 ) O(α) log  log n

O(log n)

Theorem 1.1

9+

( 1 ) O(α) log3 n

log n
O( log
log n )

Abraham et al. [2006c]

O(log n)
2 O(α) log3 n
2

Table II. (1 + )-Stretch Labeled Routing Schemes in Networks with n Nodes, Doubling
Dimension α, and Normalized Diameter 
Reference

Routing Table (in bits)
1 α
O( α
) (log

Talwar [2004]
Chan et al. [2005]
Slivkins [2005a]

Abraham et al. [2006c]
Theorem 1.3

2+α

)

( α ) O(α) (log  log n)
( 1 ) O(α) (log  log n)
( 1 ) O(α) (log  log n loglog )
( 1 ) O(α) log  log n
( 1 ) O(α) log3 n
( 1 ) O(α) (log3 n)

Header (in bits)
2

O(α log )
O(α log
O(α log
2 O(α)

1

1


Label (in bits)
O(α log )

log )

O(α 2 log α log )

log )

O(α log

log n log( 1

log )

2 O(α)

1


log )

log n loglog 

O(log n)

log n

2 O(α) log3 n

2 O(α) log3 n

O(log2 n/ loglog n)

log n
2

diameter  = O(21/ n) such that any name-independent routing scheme with o(n(/60) )bit routing table at each node must have stretch larger than 9 − .
Awerbuch and Peleg [1992] pioneered the name-independent model, designing a
name-independent scheme with stretch O(k2 ) and Õ(n1/k log ) bits of storage per node.
(The Õ() notation denotes complexity similar to O() up to polylogarithmic factors.) The
stretch was improved to O(k) with the same space requirement in Abraham et al.
[2004]. In addition, Abraham et al. [2006b] presented a scale-free name-independent
routing scheme with O(k) stretch, and Õ(n1/k) routing tables, asymptotically optimal
for general graphs, given the lower bound for general graphs in Abraham et al. [2006a].
Constant-stretch name-independent compact routing schemes do exist for restricted
classes of graphs. Table I summarizes the results of constant stretch name-independent
compact routing schemes in networks of low doubling dimension. For growth-bounded
networks (a subclass of networks of constant doubling dimension), a randomized (1+)stretch compact routing scheme is known [Abraham and Malkhi 2005]. For unweighted
graphs, excluding fixed Kr,r minors (including trees and planar graphs), a (1+)-stretch
compact scheme is presented in Abraham and Gavoille [2006].
In the labeled (or name-dependent) model, Eilam et al. [1998] achieved stretch 5 with
Õ(n1/2 ) storage and O(log n)-bit labels, while Cowen [2001] proposed a stretch 3 labeled
routing scheme with Õ(n2/3 ) storage and O(log n)-bit labels. Furthermore, Thorup and
Zwick [2001] achieve stretch 2k − 1 using Õ(n1/k) routing tables and O(k log2 n)-bit
labels. For trees, optimal stretch labeled routing schemes with O(log2 n/ log log n) bits
of label, packet header, and storage were presented in Fraigniaud and Gavoille [2001]
and Thorup and Zwick [2001]. There are (1 + )-stretch labeled routing schemes with
polylogarithmic space for planar graphs [Thorup 2004], and graphs excluding a fixed
minor [Abraham and Gavoille 2006]. In addition, (1+)-stretch labeled compact routing
schemes are devised in networks of low doubling dimension; Table II summarizes those
results.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:7

In this article, we focus on designing scale-free compact routing schemes in both
name-independent and labeled models for networks of low doubling dimension. The
notion of a scale-free scheme was addressed in Slivkins [2005a]. Concurrent with the
work presented here, a scale-free constant-stretch name-independent routing scheme
with 2 O(α) log4 n-bit storage and 2 O(α) log3 n-bit packet headers was independently
proposed by Abraham et al. [2006c] for networks of low doubling dimension. Their
stretch factor, although constant, is very large and of limited practical interest. They
also present a scale-free (1 + )-stretch labeled scheme with ( 1 ) O(α) log4 n-bit storage
and ( 1 ) O(α) log3 n-bit labels and headers. Both their schemes rely on a sparse-dense
decomposition technique, which differentiates dense and sparse regions of the network.
Intuitively, they maintain two sets of routing schemes, one for dense and one for
sparse regions, that are applied alternatively depending on the density of the region of
the network considered. The sparse-dense decomposition technique is rather involved
and does not yield good stretch factors for the name-independent routing case, nor
does it yield optimal stretch if we use optimal-size routing labels in the labeled routing
case. In contrast, our unifying and simpler O(log n)-level hierarchical decomposition
technique based on ball packings “efficiently” covers dense and sparse regions of the
network alike. We carefully design data structures to combine the two hierarchical
network decompositions used by our algorithms, namely, r-nets and ball packings,
in order to eliminate the storage dependence on . The combined hierarchies also
eliminate the need for differentiated treatment of sparse and dense regions, allowing
for much simpler routing algorithms, thus optimal stretch in the name-independent
routing case and optimal routing label size in the labeled routing case.
1.4. Bibliographic Note

The results in this article first appeared in two conference publications by the same
authors: Konjevod et al. [2006] and Konjevod et al. [2007].
In Konjevod et al. [2006], we present a preliminary version of the nonscale-free nameindependent scheme presented in Section 3 and referred to in Theorem 1.4 (the protocol
presented in Section 3 is actually an improved version of the scheme in Konjevod et al.
[2006]). The protocol in Konjevod et al. [2006] was the first optimal-stretch nameindependent compact routing scheme in the literature. In addition, the lower bound
results presented in this work also appeared originally in Konjevod et al. [2006]. We
would like to mention that the results in Abraham et al. [2006c] (as summarized
in Tables I and II) were developed independently and concurrently with the work
in Konjevod et al. [2006].
In Konjevod et al. [2007], we present the scale-free optimal-stretch results presented
in Sections 4 and 5 and referred to in Theorems 1.1 and 1.3, respectively.
2. PRELIMINARIES

In this section, we present some important definitions and basic results, which will be
used in the following sections.
Let G = (V, E) be a connected, edge-weighted, undirected graph with n nodes,
shortest-path metric d, doubling dimension α ∈ O(loglog n), and arbitrary normalized diameter . Recall that the doubling dimension of G is the least value α such
that any ball in the graph can be covered by at most 2α balls of half radius, and the
normalized diameter  is the ratio of the largest to the smallest shortest-path distance
in the graph, that is,  = max d(u, v)/ minu=v d(u, v). A ball of radius r centered at node
u, denoted by Bu(r), is the set of nodes within distance r from u; that is, for any u ∈ V
and r > 0, Bu(r) = {x ∈ V : d(u, x) ≤ r}. Without loss of generality, assume that the
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:8

G. Konjevod et al.

minimum weight of an edge is 1, therefore  = max d(u, v), and that both n and  are
powers of 2.
First, we give the definition of r-nets, which capture the geometric structure, and
on which the hierarchies of our schemes are based to achieve constant stretch while
keeping polylogarithmic storage.
Definition 2.1 (r-NET). An r-net in a metric space (V, d) is a subset Y ⊆ V such that
any point in V is within distance at most r from Y , and any two points in Y are at
distance at least r.
For any finite metric and any r > 0, it is easy to show that an r-net exists and can be
constructed greedily. The following is a well-known result about r-nets:
LEMMA 2.2 ([GUPTA ET AL. 2003]). Let Y be an r-net of (V, d). For any u ∈ V and r  ≥ r,

we have that |Bu(r  ) ∩ Y | ≤ ( 4rr )α .
For any integer x > 0, let [x] denote the set {0, 1, . . . , x}.
We now construct 2i -nets Yi for all i ∈ [log ] as follows:
(1) The -net Ylog  is a singleton for an arbitrary node in V .
(2) Recursively construct the 2i -net Yi by greedily expanding Yi+1 with nodes to obtain
a 2i -net, for i = log  − 1, log  − 2, . . . , 0.
Following this construction, we have that
Ylog  ⊆ Ylog −1 ⊆ · · · ⊆ Y0 = V.

(1)

For any node u ∈ V , we define its zooming sequence, which is due to Slivkins [2005a],
as follows: (i) Let u(0) = u; (ii) recursively define u(i) to be the nearest node to u(i − 1) in
Yi for i from 1 to log  (if there are several such nearest nodes, use some arbitrary tiebreaking mechanism, provided that all nodes use the same tie-breaking mechanism,
e.g., selecting the node with the smallest id). From the definition of an r-net, for any
u ∈ V and any i ∈ [log ], we have that
i

k=1

d(u(k − 1), u(k)) ≤

i


2k < 2i+1 .

(2)

k=1

We now form a netting tree of r-nets {Yi } by building a path from each node u ∈ V along
its zooming sequence u(0), u(1), · · · , u(log )
, and denote the netting tree as T ({Yi }).
The definition of netting tree also appears in Har-Peled and Mendel [2005].
We next introduce the concept of a ball packing, which captures the combinatorial
structure of a network. Independently, the same definition also appears in Chan et al.
[2006]. We use ball packings to avoid the log  geometric factor incurred by a straight
application of the hierarchy of 2i -nets, thereby achieving scale-free schemes.
Let ru( j) be the radius of the ball centered at u of size 2 j , that is, |Bu(ru( j))| = 2 j , for
any node u ∈ V and j ∈ [log n]. For each j ∈ [log n], let B j be a ball packing as defined
in Lemma 2.3.
LEMMA 2.3 (PACKING LEMMA). For any j ∈ [log n], there exists a ball packing B j of G,
that is, a (maximal) set of nonintersecting balls, such that
(1) for any ball B ∈ B j , |B| = 2 j .
(2) for any node u, there exists a ball B ∈ B j centered at c such that the radius of B is
at most ru( j) (i.e., rc ( j) ≤ ru( j)) and d(u, c) ≤ 2ru( j).
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:9

Fig. 1. The routing path from u to v of the name-independent routing scheme.

PROOF. Consider the set of balls {Bu(ru( j)) : u ∈ V }. Greedily select balls from this
set in the order of shortest radius to longest to form a maximal set of nonintersecting
balls B j .
First, such a ball packing B j has Property (1), since |Bu(ru( j))| = 2 j for any node
u ∈ V . Second, for any node u ∈ V , if Bu(ru( j)) ∈ B j , Property (2) is trivially satisfied.
Otherwise, since B j is maximal, Bu(ru( j)) intersects some B ∈ B j with center c. The
radius of B is at most ru( j), that is, rc ( j) ≤ ru( j), since the balls are selected by increasing
radius. Moreover, since B and Bu(ru( j)) intersect and rc ( j) ≤ ru( j), we have that d(u, c) ≤
2ru( j). Thus, Property (2) follows.
3. A SIMPLER NAME-INDEPENDENT ROUTING SCHEME

In this section, we present a simpler name-independent compact routing scheme as
stated in Theorem 1.4, whose space requirements depend on the normalized diameter
, thereby not scale-free.
3.1. Main Idea

The name-independent routing scheme uses a (1 + )-stretch compact labeled routing scheme as its underlying routing scheme. In order to deliver a message to the
destination node, the routing algorithm searches for the routing label of the destination according to its name. Once the label is located, the routing algorithm uses the
underlying labeled routing scheme to deliver the message to the destination.
The preprocessing procedure of the routing scheme maintains a hierarchy of search
trees and stores the (name, label) pairs of nodes into these search trees. For any i ∈
[log ] and any node u in the 2i -net Yi , we maintain a search tree T (u, 2 j /) rooted at
u for the ball Bu(2i /), storing the (name, label) pairs of all nodes within the same ball.
The tree nodes of the search tree are exact nodes of Bu(2i /); the (name, label) pairs are
stored at the search tree so that each node stores exactly one pair. Each tree edge is
represented by its two endpoints recording each other’s routing label. The search tree
on the ball Bu(2i /) is organized using a hierarchy of r-nets such that the path from the
root u to any leaf has length 2i (1/ + O(1)). Hence, the standard tree-search procedure
on the search tree that starts from and reports back to the root has cost 2i+1 (1/ + O(1))
to retrieve the label of the queried node according to key, that is, its name, if the node is
in Bu(2i /), or to report the node is not in Bu(2i /). By Lemma 2.2, the number of search
trees containing any fixed node is ( 1 ) O(α) log , which guarantees the compact storage
at each node as long as the normalized diameter  is no more than the polynomial of n.
Assume that a source node u wants to deliver a message to a destination node v giving
the name of v. The routing algorithm, as illustrated in Figure 1, searches for the label
of v on search trees T (u(i), 2i /) along the zooming sequence {u(i)} of the source node
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:10

G. Konjevod et al.

u, for i from 0 until the first level j where v ∈ Bu(i) (2i /). Note that the total cost of tree
j
search procedures is i=0 2i+1 (1/ + O(1)) = 2 j+2 (1/ + O(1)); the total cost of the path
from u(0) to u( j) along u’s zooming sequence is O(2 j+1 ); and the cost of the path from
u( j) to v is no more than d(u, v) + O(2 j+1 ). Thus, the total routing cost is no more than
2 j+2 (1/ + O(1))+d(u, v). Since v’s label is not found at level j −1, we have that d(u, v) >
2 j−1 (1/ − O(1)). Therefore, the stretch of the routing algorithm is no more than 9+ O().
Independently, Abraham et al. [2006c, Theorem 1] achieve a nonscale-free nameindependent compact routing scheme with stretch 64 in networks of low doubling
dimension, which we call the AGGM scheme, while our name-independent compact
routing scheme has the optimal stretch 9 + O() given the lower bound result in Theorem 1.2. Both of the schemes have similar general ideas: (i) using an underlying
labeled routing scheme; (ii) using search trees to store (name, label) pairs of nodes;
(iii) maintaining a hierarchy of search trees based on the hierarchy of r-nets; and (iv)
searching for the label of the destination along the search trees from lower level to
higher. These general common ideas lead to the compact storage and constant stretch
in both schemes. However, there are several technical ideas in our scheme that lead
to the optimal stretch factor. First, for a node u ∈ Yi , we maintain a search tree on a
larger area Bu(2i /) than a typical i-level area with radius 2i , as in the AGGM scheme.
Hence, in our scheme, the cost from the root of level i − 1 to the root of level i, that is, 2i ,
is dominated by 2i /, the radius of the area at level i. Second, in our scheme, the search
tree on an area Bu(r) has height (1 + O())r, while the search tree of the AGGM scheme
on the same area has height 2r. Third, our scheme uses a labeled routing scheme with
stretch 1 + , while the AGGM scheme uses a 5/4-stretch labeled routing scheme by
setting  = 1/4. Forth, we use d(u, v) + O(2 j+1 ) to bound d(u( j), v), rather than 2 j /,
that is, the radius of the ball Bu( j) (2 j /).
3.2. Data Structure

In this section, we first define a search tree for a ball and provide procedures to store
and retrieve (key, data) pairs; for our name-independent scheme, we take the original
node name as the key and the node label of the underlying labeled routing scheme as the
data. Furthermore, we define the hierarchical data structures to maintain search trees.
For any u ∈ V , let id(u) denote the arbitrary original name of u, and let l(u) denote
the label given by the underlying labeled routing scheme. We maintain a hierarchy of
search trees to store (name, label) pairs and retrieve the label of a node given its name.
Thus, every time u wants to communicate with a node v given by its name id(v), u uses
id(v) to retrieve the label l(v), then routes to v using the underlying labeled scheme.
Specifically, our simpler name-independent routing scheme uses the labeled routing
scheme of Abraham et al. [2006c, Theorem 4] as the effective underlying labeled routing
scheme. For reference, we list the main results achieved by this labeled routing scheme.
THEOREM 3.1 ([ABRAHAM ET AL. 2006C]). Given any undirected edge-weighted graph
with n nodes, doubling dimension α, and normalized diameter , for any  ≤ 1/2,
there exists a (1 + )-stretch labeled routing scheme with log n-bit routing labels,
(log  log n/ O(α) )-bit routing information at each node, and O(log n)-bit packet header.
Note that our name-independent routing schemes are adaptable to any compact
labeled routing scheme with stretch 1 + . The different underlying labeled routing
schemes would increase the storage by a polylogarithmic factor, but would have no
affect on the stretch.
3.2.1. Search Tree.

Definition 3.2 (Search Tree). For any  ∈ (0, 1) and any 
ball Bc (r) in G, let U0 = {c},
and for 1 ≤ i ≤ log(r), let Ui be a 2log(r)−i -net of Bc (r) \ 0≤ j<i U j . Then, the search
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:11

tree on Bc (r), denoted by T (c, r), is formed by connecting each node v ∈ Ui to its nearest
node in Ui−1 for 0 < i ≤ log(r), and defining the weight on each edge (u, v) equal to
d(u, v) in G.
From the definition of r-net, we can derive the following bound on the height of the
search tree T (c, r):
log(r)

r+



2log(r)−i ≤ (1 + )r.

(3)

i=1

Let the two endpoints of each virtual edge in the search tree keep each other’s routing
label so that they can communicate using the underlying labeled scheme. Note that
{Ui } is a partition of Bc (r), and by Lemma 2.2, the root has the maximum degree in the
4r
tree, ( 2log(r)−1
)α = ( 1 ) O(α) . Hence, each node keeps ( 1 ) O(α) labels for the search tree.
Next, given a search tree T (c, r) with m nodes, Algorithm 1, which is executed during
the preprocessing procedure, shows how to store k (key, data) pairs in the search tree:
ALGORITHM 1: Store Data in the Search Tree T (c, r)
1: sort all pairs according to their keys into a list
2: for each new visited node during a depth-first traversal of T (c, r) do
3:
pick k/m new pairs from the list of Step 1, and store them at the current node
4: end for
5: Each node u in T (c, r) stores the range of keys of the pairs stored in u and its descendants in
T (c, r); furthermore, u stores the range information of all its children.

Finally, we define the search Algorithm 2 that is used for the routing algorithm to
search for the data in a search tree T (c, r), given the key. The procedure starts from
and reports back to the root c with cost 2(1 + )r. Since (1 + )(1 + O()) = 1 + O() for
 < 1, we will omit the (1 + ) factor in the cost of the search procedure.
ALGORITHM 2: SearchTree(key, T (c, r))
1: u ← c
2: while there exists a child u of u in T (c, r) such that the range of u contains key do
3:
go to node u and u ← u
4: end while
5: if u stores the data corresponding to the given key then
6:
report the data
7: else
8:
report error: there is no pair in T (c, r) with the given key
9: end if
10: go back from u to c along the tree T (c, r)

3.2.2. Hierarchical Search Structures. First, as defined in Section 2, we have Yi , a 2i -net,

for i ∈ [log ]. Recall that the netting tree T ({Yi }) is formed by connecting each node
u along its zooming sequence {u(i)}. For each virtual edge (u(i), u(i + 1)) of the netting
tree T ({Yi }) with u(i) = u(i + 1), let u(i) store the label of u(i + 1). By Equation (1) and
the fact that u(i) = u(i + 1), we have that u(i) ∈ Yk for all k ≤ i and u(i) ∈
/ Yk for all k > i.
Thus, the node u(i) stores only one label of the elements of its zooming sequence, that
of its parent u(i + 1) in the netting tree, though node u(i) may appear multiple times
in the netting tree. Thus, each node u ∈ V can now route packets along its zooming
sequence by using the underlying labeled routing scheme.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:12

G. Konjevod et al.

Second, for any i ∈ [log ] and any u ∈ Yi , we maintain a search tree T (u, 2i /) for
the ball Bu(2i /), storing the pairs (id(v), l(v)) of all nodes v in the same ball.
LEMMA 3.3 (STORAGE). The routing information at each node has log  log n/ O(α) bits.
PROOF.
By Theorem 3.1, the underlying labeled routing scheme requires
(log  log n/ O(α) ) bits at each node.
Each node maintains at most one label of its parent node in the netting tree.
The storage required to maintain search trees and store routing labels in them can
be bounded as follows. Since the maximum degree in any search tree is ( 1 ) O(α) , and the
size of each range and routing label is O(log n), each node in a search tree maintains
( 1 ) O(α) log n bits of range information and link information. Since each search tree
stores exactly the (name, label) pairs of its own nodes, each node of the search tree
stores O(log n) bits of data. For each i ∈ [log ], the number of u ∈ Yi such that the
search tree T (u, 2i /) contains a fixed node is ( 1 ) O(α) by Lemma 2.2. Therefore, the total
storage for maintaining search trees and storing routing labels at each node is no more
than ( 1 ) O(α) log  log n.
3.3. Routing Algorithm

Now, we are ready to describe our simpler name-independent routing scheme, and
prove its performance bounds.
The routing procedure is described in Algorithm 3. Assume that a source node u
wants to send a message to a destination node v, given the name id(v) of v.
ALGORITHM 3: Name-Independent Routing
1: set i ← 0
2: repeat
3:
go to u(i) using the underlying routing scheme
4:
perform a local search at u(i) by calling the procedure SearchTree(id(v), T (u(i), 2i /))
5:
set i ← i + 1
6: until the routing label l(v) of v is found
7: go to v from u(i) using the underlying labeled routing scheme.

Figure 1 illustrates an execution of Algorithm 3. The procedure SearchTree(id(v),
T (u(i), 2i /)) searches for the label of v in the ball Bu(i) (2i /) repeatedly until, at level j,
it finds the routing label of v in the ball Bu( j) (2 j /). Then, it calls the underlying labeled
routing scheme to route to v from u( j). The following lemma guarantees the stretch
bound.
LEMMA 3.4 (STRETCH). For any source node u and any destination node v in G, the
total routing cost of our algorithm is no more than (9 + O())d(u, v).
PROOF. As illustrated in Figure 1, let j be the index of the level at which v’s routing
j
label is found. Thus, the routing cost from u to v consists of i=1 d(u(i − 1), u(i)) for the
j
cost along the zooming sequence, i=0 2i+1 / for the search procedures, and d(u( j), v)
for the cost from u( j) to v. By the triangle inequality, we have that d(u( j), v) ≤ d(u, v) +
j
i=1 d(u(i − 1), u(i)). Hence, by Equation (2), we find that the total cost is at most
j

i=1

d(u(i − 1), u(i)) +

j


2i+1 / + d(u( j), v) ≤ 2 j+2 (1/ + 1) + d(u, v).

(4)

i=0

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:13

On the other hand, since v’s routing label is not found by SearchTree(id(v), T (u( j −
1), j − 1)), we have that d(u( j − 1), v) > 2 j−1 /. Thus, by the triangle inequality and
Equation (2), we have that
d(u, v) ≥ d(u( j − 1), v) −

j−1


d(u(i − 1), u(i)) ≥ 2 j−1 (1/ − 2).

(5)

i=1

Hence, by Equations (4) and (5), the routing cost is no more than


8(1/ + 1)
2 j+2 (1/ + 1) + d(u, v) ≤ 1 +
d(u, v) ≤ (9 + O())d(u, v),
(1/ − 2)

(6)

from which the lemma follows.
Proof of Theorem 1.4: The bounds on storage and stretch follow from Lemmas 3.3 and
3.4, respectively. By Theorem 3.1, the packet header size of the name-independent
routing scheme is O(log n) bits.
4. A SCALE-FREE NAME-INDEPENDENT ROUTING SCHEME

In this section, we improve the simpler name-independent routing scheme in Section 3
to be scale-free, that is, we remove the dependence of the space requirements on the
normalized diameter.
4.1. Main Idea

The nonscale-free storage of the simpler name-independent routing scheme of Section 3 results from the log -level hierarchy of search trees. A node, for example, the
singleton in Ylog  , might belong to (1/) O(α) log  search trees. In this section, we use
the ball-packing technique (Lemma 2.3) to reduce the number of original search trees
of Section 3 and achieve the scale-free storage.
First, for each j ∈ [log n], we maintain a ball packing B j (defined as in Lemma 2.3),
and create a search tree (defined as in Definition 3.2) on each ball B ∈ B j . The search
tree for B stores the pair (id(v), l(v)) for each node v in the ball Bc (rc ( j + 2)), where c is
the center of B, and recall that |Bc (rc ( j + 2))| = 2 j+2 . Since |B| = 2 j , each node of the
search tree on B stores 4 pairs. Let B be the union of all ball packings B j , ∀ j ∈ [log n],
log n
that is, B = j=0 B j . We call the search tree on B ∈ B the packing search tree.
Second, for each u ∈ Yi and i ∈ [log ], let IsPSTCovered(u, i) indicate whether
there is a packing search tree on a packing ball B ∈ B j for j ∈ [log n], such that
B ⊆ Bu(2i (1/ + 1)) and the search tree on B contains information of all nodes in
Bu(2i /), that is, Bc (rc ( j + 2)) ⊇ Bu(2i /). If IsPSTCovered(u, i) is true, without loss of
generality, let PB(u, i) denote such a packing ball B at minimum level j. If there are
multiple such packing balls, choose one that minimizes d(u, c). If IsPSTCovered(u, i) is
false, we maintain a search tree on the ball Bu(2i /) to store the information of nodes
in the ball itself. Let A denote the collection of such balls Bu(2i /) with search tree
maintained, that is, A = {Bu(2i /) : u ∈ Yi , i ∈ [log ], IsPSTCovered(u, i) = f alse}.
Now, we claim the following lemma, whose proof appears in Section 4.2.
LEMMA 4.1. For any v ∈ V , the number of search trees that contain v is at most
( 1 ) O(α) log n.
The routing algorithm for our scale-free name-independent routing scheme remains
the same as Algorithm 3 except that, for Line 4 of Algorithm 3, we search on either the search tree T (u(i), 2i /) if IsPSTCovered(u(i), i) is false or on the packing
search tree of PB(u(i), i) if IsPSTCovered(u(i), i) is true. In the latter case, note that
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:14

G. Konjevod et al.

PB(u(i), i) ⊆ Bu(i) (2i (1/ + 1)) and the search tree on PB(u(i), i) contains information of
all nodes in Bu(i) (2i /). Hence, the cost of the round trip between u(i) and the center
of PB(u(i), i) together with the cost of the searching procedure on the search tree of
PB(u(i), i) is 2i+1 (1/ + O(1)). Thus, by Lemma 3.4, the algorithm guarantees stretch
9 + O().
Our scale-free optimal-stretch name-independent compact routing scheme follows
the main framework of our nonscale-free optimal-stretch name-independent routing
scheme as in Section 3, and borrows the idea of the scale-free scheme in Abraham
et al. [2006c, Theorem 2], which we call the AGGM scale-free scheme, to remove the
scale dependence. The idea is to use a data structure that captures both the geometric
and combinatorial properties of the network. While they use a collection of landmarks,
which is originally due to Slivkins [2005a], we use a set of ball packings, which is
independently defined in Chan et al. [2006]. Note that the set of centers of balls in
a ball packing is a collection of landmarks. Hence, we are able to take advantage of
the packing balls’ geometric and the combinatorial properties directly; after building
a search tree on each packing ball to store information of nodes within the concentric
ball with quad size, we reduce the original search trees as in Section 3 to scale free
storage instantly. On the other hand, the AGGM scale-free scheme defines an area
for each landmark and maintains a single-source name-independent routing on each
such area, which brings in an additional factor on their routing stretch. In addition,
they maintain two sets of routing schemes, one for dense and one for sparse regions,
that are applied alternatively depending on the density of the region of the network
considered. Note that our routing algorithm is unaware of the density of the network,
and almost the same as in Section 3, though in our storage analysis, we use similar
analytic techniques as in Abraham et al. [2006c].
4.2. Storage Analysis

We show that the storage at each node is scale-free and compact.
First, we give the proof of Lemma 4.1. It is obvious that the number of packing search
trees that contain a fixed node is no more than log n, since the packing balls at the same
level are disjoint and there are log n-level ball packings. The following lemma counts
the number of search trees on balls in A that contain a fixed node.
LEMMA 4.2. For any node v ∈ V , the number of search trees on balls in A that contain
v is no more than (1/) O(α) log n.
Thus, we have Lemma 4.1.
In order to prove Lemma 4.2, we define dense level for a node, which is due to Abraham
et al. [2006c].
Definition 4.3 (Dense Level). For any node v ∈ V and any i ∈ [log ], we say that i is
a dense level for node v if
|Bv (2i+2 /)| ≥ 2|Bv (2i−2 )|.

(7)

Let IsDensev (i) indicate whether level i is dense for node v.
It is straightforward that the number of dense levels for any fixed node is
O(log 1 log n), as claimed in the following lemma:
LEMMA 4.4. For any node v ∈ V , the number of dense levels in [log ] is no more than
O(log 1 log n), that is,|{i ∈ log  : IsDensev (i) = TRUE}| ≤ O(log 1 log n).
The next lemma relates the search tree on balls in A that contain v to the dense
levels for node v.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:15

Fig. 2. A search tree on Bu(2i /) containing v implies that IsDensev (i) is true.

LEMMA 4.5. For any v ∈ V , if ∃u ∈ Yi , i ∈ [log ] such that v ∈ Bu(2i /) and
IsPSTCovered(u, i) is false, that is, there is a search tree on Bu(2i /) containing v, then
IsDensev (i) is true.
PROOF. Let j ∈ [log n] be the index such that 2 j ≤ |Bv (2i−2 )| < 2 j+1 . By the packing
lemma, there is a packing ball Bc (rc ( j)) ∈ B j with small radius (rc ( j) < 2i−2 ) and
near to v (d(v, c) ≤ 2i−1 ), as illustrated in Figure 2. Hence, the packing ball Bc (rc ( j)) is
contained in Bu(2i (1/ +1)). Since IsPSTCovered(u, i) is false, Bc (rc ( j +2)) does not cover
the whole ball Bu(2i /), which implies that rc ( j +2) < 2i+1 / +2i−1 . Thus, by the triangle
inequality, Bv (2i+2 /) contains Bc (rc ( j + 2)). Therefore, |Bv (2i+2 /)| ≥ 2 j+2 > 2|Bv (2i−2 )|,
that is, IsDensev (i) is true.
Thus, by Lemmas 4.4, 4.5, and 2.2, we have Lemma 4.2.
Now, we present the storage lemma:
LEMMA 4.6 (STORAGE). The routing information at each node has ( 1 ) O(α) log3 n bits.
PROOF. By Theorem 1.3, the underlying labeled routing scheme requires ( 1 ) O(α) log3 n
bits at each node.
The storage required to maintain search trees and store routing labels in them can
be bounded as follows. Since the maximum degree in any search tree is ( 1 ) O(α) and the
size of each range and routing label is O(log n), each node in a search tree maintains
( 1 ) O(α) log n bits of range information and link information. Since each node in a search
tree stores at most 4(name, label) pairs, it stores O(log n) bits of data. Since the number
of search trees containing any node is at most ( 1 ) O(α) log n by Lemma 4.1, the total
storage for maintaining search trees and routing labels at each node is no more than
( 1 ) O(α) log2 n.
By the definition of the netting tree, each node maintains at most one label of its
parent node that is not the node itself.
The following claim shows that, for any u ∈ V , the number of links to the center of
packing balls PB(u, i), for all i ∈ [log ] such that u ∈ Yi and IsPSTCovered(u, i) is true,
is no more than 4 log n. Therefore, each node requires O(log2 n) bits of storage for the
links.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:16

G. Konjevod et al.

LEMMA 4.7. For any u ∈ V , the number of packing balls PB(u, i), for all i ∈ [log ]
such that u ∈ Yi and IsPSTCovered(u, i) is true, is no more than 4 log n.
PROOF. We show that, for any j ∈ [log n], the number of different balls PB(u, i) ∈ B j
such that IsPSTCovered(u, i) is true is at most 4, from which the claim follows.
By contradiction, assume that there is an index j ∈ [log n] such that the number of different balls PB(u, i) ∈ B j such that IsPSTCovered(u, i) is true is at least
5. Let i0 < i1 < i2 < i3 < i4 be five of these indices. By definition of PB(u, ik),
we have that PB(u, ik) ⊆ Bu(2ik (1/ + 1)). Since ik < i4 for k < 4, we have that
Bu(2ik (1/ + 1)) ⊂ Bu(2i4 /). Thus, Bu(2i4 /) contains balls PB(u, ik) for k < 4. Since
all balls PB(u, ik) in B j are nonintersecting and have size 2 j , we have that |Bu(2i4 /)| >
2 j+2 . Thus,Bu(2i4 /)  Bc (rc ( j + 2)), where c is the center of PB(u, i4 ). This contradicts
the definition of PB(u, i4 ).
Therefore, each node stores ( 1 ) O(α) log3 n bits of routing information.
Proof of Theorem 1.1: The bounds on stretch and storage requirements at the nodes
follow from Lemmas 3.4 and 4.6, respectively. The packet header size of our nameindependent routing scheme is dominated by the packet header size of the underlying
labeled scheme, that is, O(log2 n/ log log n) bits, by Theorem 1.3.
4.3. Encoding the Scale-Dependent Index

Note that the description of Algorithm 3 depends on some scale-dependent index i ∈
[log ], which requires O(log log ) bits for its representation. In this section, we discuss
how to encode the scale-dependent index in our data structure and routing algorithm
to achieve a fully scale-free name-independent routing scheme.
First, for a search tree, each tree node is able to be unaware of its level on the tree
(which might be as large as log ), since the parent–child doubly linked relationship is
enough to maintain a tree structure and to support the tree-search algorithm.
Second, consider a node u ∈ Yk but u ∈
/ Yk+1 for a k ∈ [log ], that is, u(i) = u
for all i ∈ [k] but u(k + 1) = u. For any i ∈ [k], note that u maintains a link to the
root of a search tree that has information of all nodes in Bu(2i /). The search tree is
the search tree on the packing ball PB(u, i) if IsPSTCovered(u, i) is true, or it is the
search tree on Bu(2i /). By Lemmas 4.1 and 4.7, the number of such links at u for
all i ∈ [k] is at most ( 1 ) O(α) log n. Let Maxu denote the number of such links, and let
(rootu( j) : j = 1, . . . , Maxu) denote these links in a sequence. Let u record the label
of the parent node p of u(k) and the index j  such that rootp( j  ) represents the root of
the search tree at node p at level k + 1. Now, after the routing algorithm performs the
search at u using the link rootu( j) to the corresponding search tree and the routing
information of the destination node is not found, it stays at u and searches using the
next level link rootu( j + 1) if j ≤ Maxu or, if j > Maxu, it goes to the parent node p of
u(k) and searches at p using the link rootp( j  ) to its corresponding search tree, where
the index j  is prestored at u such that rootp( j  ) represents the root of the search tree
at node p at level k + 1. Therefore, the routing algorithm is able to be unaware of the
scale-dependent index.
5. A SCALE-FREE LABELED ROUTING SCHEME

In this section, we present our scale-free labeled routing scheme as in Theorem 1.3.
For simplicity, we will prove the stretch in terms of big-O, that is, stretch 1 + O().
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:17

5.1. Data Structures

First, as defined in Section 2, we have Yi , a 2i -net, for i ∈ [log ]. We define the label
function l : V → [log n] to be the enumeration of the leaves in a depth-first traversal of
the netting tree T ({Yi }). Note that the leaf set of T ({Yi }) is Y0 = V . For any i ∈ [log ]
and any node x ∈ Yi , by the properties of depth-first traversal, the labels of leaf nodes
of the subtree rooted at x in T ({Yi }) are a range of continuous integers, denoted by
Range(x, i). Thus, we have that l(u) ∈ Range(x, i) if and only if x = u(i).
Second, let the i th ring of u be the node set Xi (u) = Bu(2i /) ∩ Yi , which is due
to Slivkins [2005a], and R(u) = {i ∈ [log ] : ∃ j ∈ [log n], 6 ru( j) ≤ 2i ≤ ru( j)}. Then,
each node u stores the range information Range(x, i) for nodes x ∈ Xi (u) and i ∈ R(u),
and the log n-bit information to identify which neighbor of u is on the shortest path from
u to x. Note that |R(u)| = O( log n ), and by Lemma 2.2, we have that |Xi (u)| = (1/) O(α) .
Hence, the range information stored at each node is (1/) O(α) log2 n bits.
Third, let B j be a ball packing defined as in Lemma 2.3, for each j ∈ [log n]. For any
ball B ∈ B j with center c and any j ∈ [log n], let V (c, j) be the Voronoi region of c in the
Voronoi diagram of centers of balls in B j , that is, V (c, j) = {u ∈ V : d(u, c) ≤ d(u, c ),
for the centerc of any ball in B j }; and let Tc ( j) be a shortest path tree rooted at c and
spanning V (c, j). For each tree Tc ( j), we maintain a labeled routing scheme as follows,
and let l(v; c, j) denote the local routing label of v ∈ Tc ( j):
LEMMA 5.1 ([FRAIGNIAUD AND GAVOILLE 2001; THORUP AND ZWICK 2001]). For every
weighted tree T on n nodes, there exists a labeled routing scheme that, given any destination label, routes optimally on T from any source to the destination. The storage per
node, the label size, and header size are O(log2 n/ log log n) bits.
For each j ∈ [log n], each node u ∈ V stores the local routing label l(c; c, j) of the
center c, where c is the center of a ball B ∈ B j such that u ∈ V (c, j). Note that, by
Voronoi diagram properties, for each fixed j, the trees Tc ( j) are disjoint. Thus, the local
routing label information at each node is O(log3 n/ log log n) bits.
Finally, for each j ∈ [log n] and each center c of a ball in B j , we build a
search tree T  (c, rc ( j)) as in Definition 5.2 to store the (key, data) pairs of nodes
v ∈ Tc ( j) ∩ Bc (rc ( j + 1)), where the key is the global routing label l(v), and data is
the local routing label l(v; c, j) of v in the tree Tc ( j). Thus, given a key, that is, l(v), the
SearchTree(l(v), T  (c, rc ( j))) procedure, as defined in Section 3.2.1, retrieves the label
l(v; c, j) of v along the shortest path of the search tree.
The following definition is modified from Definition 3.2. The key idea of using a path
to span a very small scale for levels beyond log n is borrowed from Abraham et al.
[2006c].
Definition 5.2 (Search Tree II). For any  ∈ (0, 1) and any ball Bc (r) in
G, let U0 = {c},
and for 1 ≤ i ≤ min(log n, log(r)) let Ui be a 2log(r)−i -net of Bc (r)\ 0≤ j<i U j . Then,
the search tree II on Bc (r), denoted by T  (c, r), is formed by connecting each node v ∈ Ui
to its nearest node in Ui−1 for 0 < i ≤ min(log n, log(r)), and defining the weight
on each edge (u, v) equal to d(u, v) in G.
If log n ≥ log(r), the search tree II is the same as the search tree in Definition 3.2.
If log n < log(r), for any u ∈ Ulog n , let V (u) be the Voronoi region of u in the
Voronoi diagram of a set of sites Ulog n in Bc (r), thatis, V (u) = {x ∈ Bc (r) : d(x, u) ≤
d(x, u ), for any u ∈ Ulog n }. Link the nodes in V (u) \ 0≤ j≤log n U j into a path, connect
, for each u ∈ Ulog n .
it to u and define the weight on these edges equal to 2r
n
Note that the height of the search tree T  (c, r) is at most (1 + )r + 2r
· n = (1 + O())r.
n
The following lemma shows how to link the endpoints of each virtual edge.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:18

G. Konjevod et al.

Fig. 3. The routing path from u to v of the labeled routing scheme.

LEMMA 5.3. We can deliver packets along each virtual edge of the search tree T  (c, r),
with cost at most the weight of the edge, by maintaining 2 O(α) log2 n bits of information
per node.
PROOF. First, for any virtual edge (u, v), where u ∈ Ui−1 , v ∈ Ui , and 0 < i ≤
min(log n, log(r)), let each node x on the shortest path from u to v maintain the
next-hop information in both directions. Thus, u and v can communicate with each
other along the shortest path. Now, we bound the space requirement. On the direction
from v to u, since u is the nearest node in Ui−1 to v and x is on the shortest path
between u and v, u is also the nearest node in Ui−1 to x. Thus, x stores one entry
of the next-hop information to its nearest node in Ui−1 . On the direction from u to
v, by Lemma 2.2, the number of nodes v ∈ Ui whose nearest node in Ui−1 is a fixed
node u is 8α . Hence,x maintains at most 8α entries of next-hop information to those
vs. Since 0 < i ≤ min(log n, log(r)) and each next-hop information has size no
more than log n bits, each node in Bc (r) requires at most 2 O(α) log2 n bits of the next-hop
information.
 Second, consider the virtual edges on the path from u to link all nodes in V (u) \
0≤ j≤log n U j , for each u ∈ Ulog n . Let T (u) be a shortest-path tree rooted at u and
spanning V (u). We maintain a local labeled routing scheme given by Lemma 5.1 for
the tree T (u), and let the two endpoints of each of these virtual edgeskeep each other’s
local label. Since d(u, v) ≤ 2log(r)−log n ≤ rn for any v ∈ V (u) \ 0≤ j≤log n U j , the
. By the Voronoi diagram
routing cost along each of these virtual edges is at most 2r
n
properties, the trees T (u) for all nodes u ∈ Ulog n are disjoint. Thus, by Lemma 5.1,
each node in Bc (r) maintains O(log2 n/ log log n) bits of routing information for the local
labeled routing.
In summary, now, the two endpoints of each virtual edge in T  (c, r) can communicate
with each other with cost at most the weight of the edge, by maintaining 2 O(α) log2 n
bits of information per node.
LEMMA 5.4 (STORAGE). The routing information at each node is at most ( 1 ) O(α) log3 n
bits.
PROOF. Each node maintains ( 1 ) O(α) log2 n-bit range information, O(log3 n/ log log n)bit local routing label information, and 2 O(α) log3 n-bit data structures and
O(log3 n/ log log n)-bit data storage for search trees. Hence, the routing information
at each node is at most ( 1 ) O(α) log3 n bits.
5.2. Routing Algorithm

Assume that a source node u wants to send a packet to a destination node v given its
label l(v). The routing procedure is defined in Algorithm 4.
Figure 3 illustrates the routing path from u to v, which consists of the path u0 →
u1 → · · · → ut , then the routing path from ut to c, the search trail of SearchTree() in the
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:19

ALGORITHM 4: Labeled Routing Scheme
1: set k ← 0, u0 ← u, and i−1 ← +∞
2: set ik ← the minimal index in R(uk) such that there exists xk ∈ Xik (uk) with
l(v) ∈ Range(xk, ik)
3: if ik ≤ ik−1 and d(uk, xk) ≥ 2ik−1 / − 2ik then
4:
uk+1 ← the next hop along the shortest path from uk to xk and go to uk+1
5:
set k ← k + 1, and repeat Step 2
6: end if
7: set t ← k; j ← the index in [log n] such that rut ( j) ≤ 2it < rut ( j + 1); and c ← the center of a
ball B ∈ B j such that ut ∈ V (c, j)
8: route to c using the labeled tree routing on Tc ( j) [the label l(c; c, j) is stored at ut ]
9: SearchTree(l(v), T  (c, rc ( j))). [By Lemma 5.5, this retrieves l(v; c, j).]
10: route to v using the labeled tree routing on Tc ( j)

ball Bc (rc ( j)), and the routing path from c to v. Then, the following lemma guarantees
that the SearchTree() procedure in the ball Bc (rc ( j)) retrieves the local label l(v; c, j)
successfully.
LEMMA 5.5. The SearchTree(l(v), T  (c, rc ( j))) in Step 9 retrieves the label of v, where
j and c are defined in Step 8.
PROOF. First, since the if condition in Step 3 is not satisfied in Iteration t, we have
the following claim, which is proved later.
CLAIM 5.6. Let t ∈ [log ] and j ∈ [log n] be defined as in Line 7. Then, rut ( j)/(3) <
d(ut , v) < rut ( j + 1)/5.
Second, we show that v ∈ V (c, j), that is, v ∈ Tc ( j), and that v ∈ Bc (rc ( j + 1)). This
implies that the local label l(v; c, j) is stored in the search tree T  (c, rc ( j)).
For a contradiction, assume that v ∈ V (c , j), where c = c is a center of a ball B ∈ B j .
Thus, we have that
d(v, c ) ≤ d(v, c) ≤ d(v, ut ) + d(ut , c) ≤ d(ut , v) + 2rut ( j),

(8)



where the last inequality follows from Lemma 2.3. Since B and Bc (rc ( j)) are disjoint,
we have that
rc ( j) + rc ( j) < d(c, c ) ≤ 2d(v, c).

(9)

Thus by Equations (8) and (9), we have that
d(ut , c ) + rc ( j) ≤
≤
≤
<

(d(ut , v) + d(v, c )) + (2d(v, c) − rc ( j))
d(ut , v) + 3d(v, c)
4d(ut , v) + 6rut ( j)
rut ( j + 1),

(10)

where the last inequality follows from Claim 5.6. In addition, by Lemma 2.3 and
Claim 5.6, we have that
d(ut , c) + rc ( j) ≤ 3rut ( j) < rut ( j + 1).

(11)


Note that the ball center at ut with radius max(d(ut , c) + rc ( j), d(ut , c ) + rc ( j)) contains
both B and Bc (rc ( j)). Thus, it has size 2 j+1 , because B and Bc (rc ( j)) are disjoint and
both of size 2 j . Hence, rut ( j + 1) ≤ max(d(ut , c) +rc ( j), d(ut , c ) +rc ( j)), which contradicts
Equations (10) and (11). Therefore, v ∈ V (c, j).
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:20

G. Konjevod et al.

We now show that v ∈ Bc (rc ( j + 1)). Since balls Bc (rc ( j + 1)) and But (rut ( j + 1)) have
the same size, we have that
d(ut , c) + rc ( j + 1) ≥ rut ( j + 1).

(12)

By Lemma 2.3 and Claim 5.6, we have that
d(c, v) ≤ d(c, ut ) + d(ut , v) < rut ( j + 1) − d(ut , c).

(13)

Thus, by Equations (12) and (13), we have that d(c, v) ≤ rc ( j + 1), that is, v ∈
Bc (rc ( j + 1)). Therefore, the tree T  (c, rc ( j)) stores the local label l(v; c, j) of v, and
SearchTree(l(v), T  (c, rc ( j))) retrieves it.
Proof of Claim 5.6: Note that l(v) ∈ Range(xt , it ) in Line 2 implies that xt is the tth
element of v’s zooming sequence, that is, xt = v(it ). Let x  = v(it − 1). We have that
x  ∈ Xit −1 (ut ) because the if condition in Line 3 is not satisfied for k = t; the detailed
argument is given in the following two cases:
(1) Either it > it−1 . We have that d(ut , x  ) ≤ d(ut , xt−1 ) + d(xt−1 , x  ) ≤ 2it−1 / + d(xt−1 , x  ).
If it−1 = it − 1, then we have that xt−1 = x  and d(ut , x  ) < 2it −1 /. Otherwise, we
have that 2it−1 / ≤ 2it −2 / and d(xt−1 , x  ) ≤ 2it by the definition of r-nets. Hence,
d(ut , x  ) < 2it −1 / for  < 3/4.
(2) Or d(ut , xt ) < 2it −1 / − 2it . Since d(x  , xt ) ≤ 2it by the definition of r-net, we have
that d(ut , x  ) ≤ d(ut , xt ) + d(xt , x  ) ≤ (2it −1 / − 2it ) + 2it = 2it −1 /.
Therefore, we have that
(14)
d(ut , x  ) ≤ 2it −1 /,

/ R(ut ).
that is, x ∈ Xit −1 (ut ). Hence, by the minimality of it in Line 2, we have that it −1 ∈
Since rut ( j) ≤ 2it < rut ( j + 1), as in Line 7, by the definition of R(u) and it − 1 ∈
/ R(ut ),
we have that
rut ( j) < 2it −1 < rut ( j + 1) · /6.
(15)
Thus, by Equations (14), (15), and (2), we have that
d(ut , v) ≤ d(ut , x  ) + d(x  , v) < 2it −1 / + 2(it −1)+1 < rut ( j + 1)/5.


(16)


We now show that rut ( j)/(3) ≤ d(ut , v). Let i = log rut ( j) ∈ R(ut ). Since i < it by
Equation
(15), with the minimality of it , we have that v(i  ) ∈
/ Xi (ut ), that is, d(ut , v(i  )) >
i
2 / ≥ rut ( j)/(2). Thus, by Equation (2), we have that


d(ut , v) ≥ d(ut , v(i  )) − d(v(i  ), v) ≥ rut ( j)/(2) − 2i +1 > rut ( j)/(3).

(17)

The claim follows from Equations (16) and (17).
LEMMA 5.7 (STRETCH). For any source node u and any destination node v in G, the
total routing cost of the labeled routing scheme is no more than (1 + O())d(u, v).
PROOF. Since the cost of the SearchTree() on the ball Bc (rc ( j))) in Step 9 is bounded
by (2 + O())rc ( j), as illustrated in Figure 3, the total routing cost is no more than
t


d(uk, uk−1 ) + d(ut , c) + (2 + O())rc ( j) + d(c, v).

(18)

k=1

First, since d(ut , c) ≤ 2rut ( j) and rc ( j) ≤ rut ( j) by Lemma 2.3, and since rut ( j)/(3) <
d(ut , v) by Claim 5.6, we have that
d(ut , c) + (2 + O())rc ( j) + d(c, v)
≤ 2d(ut , c) + (2 + O())rc ( j) + d(ut , v)
≤ (1 + O())d(ut , v).

(19)

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:21


Second, we bound the cost of tk=1 d(uk, uk−1 ). Since xk is on the shortest path from
uk−1 to xk−1 and since d(uk, xk) ≤ d(uk, xk−1 ) + d(xk−1 , xk) by the triangle inequality, for
all k ≤ t, as illustrated in Figure 3, we have that
t−1


d(uk, uk+1 ) + d(ut , v)

k=0

≤ d(u0 , x0 ) +

t−1


d(xk, xk−1 ) + d(xt−1 , v)

(20)

k=1
i0 +2

≤ d(u, v) + 2

,

where the last inequality follows from d(u0 , x0 ) ≤ d(u, v) + d(x0 , v) and Equation (2).
Now, consider the total cost. If t = 0, the routing cost is given by Equation (19), and
is at most (1 + O())d(u, v). If t > 0, we have that d(u, x0 ) > 2i0 −1 / − 2i0 because the if
condition in Step 3 is not satisfied in Iteration 0. Hence, by Equation (2), we have that
d(u, v) ≥ d(u, x0 ) − 2i0 +1 ≥ 2i0 −1 / − 2i0 +2 .
Thus, by Equations (18) and (19), the total routing cost is no more than

 t

d(uk, uk−1 ) + d(ut , v)
(1 + O())
k=1

(21)

(22)

≤ (1 + O())d(u, v),
where the last inequality follows from Equations (20) and (21), and the equation (1 +
O())2 = (1 + O()). This completes the lemma.
Proof of Theorem 1.3: The bounds on stretch and storage at the nodes follow from
Lemmas 5.7 and 5.4, respectively. The packet header size of our labeled routing scheme
is dominated by the packet header size of the underlying labeled tree-routing scheme,
that is, O(log2 n/ log log n) bits, by Lemma 5.1.
6. LOWER BOUND FOR NAME-INDEPENDENT ROUTING SCHEMES

In this section, we present the proof of our lower bound as stated in Theorem 1.2. Note
that a name-independent routing scheme works on arbitrary original node names.
In Section 6.1, by taking advantage of the small number of different configurations
of routing tables compared to the number of different namings, we show that there
exist many namings such that the routing configuration for a large number of nodes is
identical for each of these namings. These identical namings will be called congruent
(See Definition 6.3 for a formal definition). We show that, given a fixed-source node and
destination name, the routing algorithm must follow the same initial steps for any two
congruent namings provided that the nodes visited by the routing algorithm during
these initial steps have the same routing configuration for both namings.
In Section 6.2, we build the counterexample, a tree, to be used in the lower bound
proof. First, from Section 6.1, it follows that there exists a specific target name such
that, for different congruent namings, it may be found in any branch of the tree. Second,
given one of these namings, a sequence of branches is defined according to the routing
path from the root to the node with the specific target name. We will use this sequence
to show that the stretch achieved by the algorithm cannot be less than 9 −  for any
fixed  ∈ (0, 8).
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:22

G. Konjevod et al.

6.1. Congruent Namings

Given an integer constant c ≥ 2 and a graph G = (V, E) with n nodes and a β-bit
routing table at each node, where β = o(n1/c ), consider any name-independent routing
scheme on G. First, we give some definitions. In this section, for any integer x > 0, let
[x] denote the set {0, 1, . . . , x − 1}.
Definition 6.1 (Naming). A naming  on nodes in V is a bijective function  : V → [n].
Let L denote the family of all namings.
For any naming  and any subset of nodes V  , let (V  ) be the set of names of nodes in
V  under naming , that is, (V  ) = {(v) : ∀v ∈ V  }. Note that, given a naming on V ,
the name-independent routing scheme configures the β-bit routing table at each node.
Therefore, it naturally determines a routing configuration function, as follows.
Definition 6.2 (Routing Configuration Function). A routing configuration function is
a function f of the form
f : L × V → [2β ].
Note that the preconfiguration process of the given routing scheme specifies the
routing configuration function f . Now, we give the definition of the set of congruent
namings.
Definition 6.3 (Set of Congruent Namings). Given a mapping g : V → [2β ] and a
subset of nodes V  ⊆ V , the set of namings congruent with respect to V  and g is the
set of namings L = { ∈ L : f (, v) = g(v), ∀v ∈ V  }.
Let {Vi : i = 0, 1, . . . ,
c} be a partition of V such that |V0 | = 1 and |Vi | = ni/c − n(i−1)/c
c
for 1 ≤ i ≤ c. Note that i=0
|Vi | = n. Then, we have the following lemma.
LEMMA 6.4. There exists a mapping g : V → [2β ] such that |Li | ≥ n!/2βn , where Li

is the set of namings congruent with respect to ij=0 V j and g, for 0 ≤ i ≤ c. Moreover,
by definition, L0 ⊇ L1 ⊇ · · · ⊇ Lc .
i/c

PROOF. We recursively define g by applying the pigeonhole principle.
(1) Define g on the node set V0 so that |L0 | ≥ n!/2β . Such an assignment exists since
the number of all namings is n!, that is, |L| = n!, and since there are 2β possible
values for the routing table at the single node of V0 .
i/c
(2) For 1 ≤ i ≤ c, recursively define g on the node set Vi so that |Li | ≥ n!/2βn . Such
(i−1)/c
i/c
(i−1)/c
)
and there are 2β(n −n
possible
an assignment exists since |Li−1 | ≥ n!/2βn
values for routing tables at all nodes of Vi .
Based on Lemma 6.4, the following lemma finds a specific name, which is used as
the destination name in the lower bound analysis. Its proof appears in Section 6.1.1.
LEMMA 6.5. There exists a name t ∈ [n] such that, for any 0 < i < c, there exist two
distinct namings 1 , 2 ∈ Li−1 with t ∈ 1 (Vi ) and t ∈
/ 2 (Vi ).
By Definition 6.3, for any naming l ∈ Li−1 , the configuration of the routing table is

the same, that is, f (l, v) = g(v), for every node v in i−1
j=0 V j . Thus, by Lemma 6.5, we
have Corollary 6.6.
COROLLARY 6.6. There exists a name t ∈ [n] such that, for any 0 < i < c and any

naming l ∈ Li−1 , the routing tables of nodes in i−1
j=0 V j cannot uniquely determine
whether the node named t belongs to Vi or not.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:23

Hence, no routing algorithm can be certain of correctly deciding whether the node
named t belongs to Vi or not without seeing some information from nodes outside of
i−1
j=0 V j .
6.1.1. Proofs.

PROOF OF LEMMA 6.5: For 0 < i < c,	
let Yi be the set of names used only for nodes in Vi
for all namings in Li−1 , that is, Yi = ∀∈Li−1 (Vi ); let Ni be the set of names never used
	
for any node in Vi for any naming in Li−1 , that is, Ni = ∀∈Li−1 (Vi ). Since Yi ⊆ (Vi )
and Ni ∩ (Vi ) = ∅, for any  ∈ Li−1 , we have that


n − |Yi | − |Ni |
|Vi |! · (n − |Vi |)!.
|Li−1 | ≤
(23)
|Vi | − |Yi |
This formula follows from
 two observations:
(1) The number of different sets of names

i |−|Ni |
that  may use for Vi is n−|Y
,
since
the
names in Yi are preselected, and those
|Vi |−|Yi |
in Ni are not allowed. (2) Once the set of names for Vi is selected, the number of such
different namings is at most |Vi |! · (n − |Vi |)!.
The following claim bounds the cardinality of Yi and Ni , which is proved later.
CLAIM 6.7. For any 0 < i < c, we have that |Yi | + |Ni | = o(n).
c−1
Since c is a constant, by Claim 6.7, we have that | i=1
(Yi ∪ Ni )| = o(n). Thus, let a
c−1
(Yi ∪ Ni ). For any 0 < i < c, since t ∈
/ Ni , there exists a naming 1 ∈ Li−1
name t ∈
/ i=1
such that t ∈ 1 (Vi ); since t ∈
/ Yi , there exists a naming 2 ∈ Li−1 such that t ∈
/ 2 (Vi ). It
completes the lemma.
PROOF OF CLAIM 6.7: Consider two cases depending on whether |Vi | ≤ (n− |Yi | − |Ni |)/2:
(1) If |Vi | > (n − |Yi | − |Ni |)/2, by Equation (6.7), we have that
 


2|Vi |
n − |Yi | − |Ni |
≤
.
|Li−1 | ≤
|Vi | − |Yi |
|Vi |
Since |Li−1 | ≥ n!/2βn

(i−1)/c

(24)

by Lemma 6.4, we have that

2βn

(i−1)/c

≥ 
2|V |
i
|Vi |

n!
|Vi |! · (n − |Vi |)!

n(n − 1) · · · (n − |Vi | + 1)
2|Vi |(2|Vi | − 1) · · · (|Vi | + 1)
|Vi |

n
≥
.
2|Vi |

=

(25)

Since |Vi | = ni/c − n(i−1)/c , we have that
βn(i−1)/c
n
ni/c −n(i−1)/c
≤
2
2(ni/c − n(i−1)/c )


βn(i−1)/c
=1+O
ni/c − n(i−1)/c


(26)

= 1 + o(1),
where the last two equations follow from e x = 1 + O(x) for small x and β = o(n1/c ).
This contradicts i < c. Omit this case.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:24

G. Konjevod et al.

iq+ j+1

iq+ j

Fig. 4. A tree consists of root u, (n pq − n pq )-node paths Ti, j for all i ∈ [ p] and j ∈ [q], and edges with
weight wi, j = 2i (q + j) connecting the root and the median node of each path Ti, j .

(2) If |Vi | ≤ (n − |Yi | − |Ni |)/2, by Equation (6.7), we have that


n − |Yi | − |Ni |
|Vi |! · (n − |Vi |)!
|Li−1 | ≤
|Vi |
Since |Li−1 | ≥ n!/2βn

(i−1)/c

2βn

(i−1)/c

(27)

by Lemma 6.4, we have that

n!
≥ 
n−|Y |−|N |
i
i
|Vi |! · (n − |Vi |)!
|Vi |
n(n − 1) · · · (n − |Vi | + 1)
(n − |Yi | − |Ni |) · · · (n − |Yi | − |Ni | − |Vi | + 1)
|Vi |

n
≥
n − |Yi | − |Ni |


|Yi | + |Ni | |Vi |
≥ 1+
n
=

(28)

Since |Vi | = ni/c − n(i−1)/c , we have that
1+

βn(i−1)/c
|Yi | + |Ni |
≤ 2 ni/c −n(i−1)/c
n
= 1 + o(1),

(29)

where the last equation follows from e x = 1 + O(x) for small x and β = o(n1/c ).
Therefore, |Yi | + |Ni | = o(n).
6.2. Lower Bound Proof

In this section, we start by building a graph G(V, E). Later, with the help of results
in Section 6.1, we will show that, for any name-independent routing scheme with
2
o(n(/60) )-bit routing tables at each node, the stretch on G cannot be smaller than 9 − .
As shown in Figure 4, the graph G is a tree with root u and an edge of length wi, j
connecting u to the median node of the path Ti, j for each i ∈ [ p] and j ∈ [q], where
iq+ j+1

iq+ j

p, q ∈ O(1/), wi, j = 2i (q + j) and Ti, j is a path on n pq − n pq nodes with edges of
weight 1/n. Note that {{u}, Ti, j : i ∈ [ p], j ∈ [q]} is a partition {Vi : i = 0, 1, . . . , c} of V
with |V0 | = |{u}| = 1 and |Viq+ j+1 | = |Ti, j | = n(iq+ j+1)/c −n(iq+ j)/c for i ∈ [ p], j ∈ [q], where
c = pq. Hence, by Corollary 6.6, for any routing scheme with o(n1/c )-bit routing table
at each node, there exists a name t ∈ [n] such that, for any 0 < i < c, the routing tables
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:25


of nodes in i−1
j=0 V j cannot uniquely determine whether the node named t belongs to
Vi or not.
There are several properties of the tree G. First, the length of the path Ti, j is o(1)
for i ∈ [ p], j ∈ [q] except i = p − 1 and j = q − 1, so that it is omitted compared
with wi, j . Second, wi, j+1 /wi, j = 1 + O(), which intuitively means that the branch Ti, j+1
has almost the same distance to the root u as the branch Ti, j . Third, wi+1, j /wi, j = 2,
∀i ∈ [ p − 1], ∀ j ∈ [q], which intuitively means that the weight of the edge between u
and the branch Ti, j increases doubly for i increasing one, and that i ranges from 0 to
p − 1 = O(1/).
2
Specifically, we set p = 72/ + 6 and q = 48/ − 4, which imply that o(n1/(60/) ) ⊂
o(n1/c ). We prove the lower bound by contradiction. Assume that there is a nameindependent routing scheme with an o(n1/c )-bit routing table at each node and stretch
less than 9 − . Given a naming  ∈ Lc−2 for which ∃v ∈ Vc−1 such that t = l(v), consider
the routing path of delivering a message from the root u to the node v named t. Let the
sequence of subtrees Txi ,yi : i = 0, . . . , m − 1
 be the subtrees visited on the routing
path such that (i) Tx0 ,y0 is the first subtree visited; and (ii) for 0 < i < m, Txi ,yi is the

first subtree visited with wxi ,yi > wxi−1 ,yi−1 . Let Ai = ij=0 wx j ,y j . The following claim
gives the relation between Ai and wxi ,yi .
CLAIM 6.8. For any i ∈ [m], we have that
(1) if i ≤ m − 3, then Ai ≤ (4 − /3)wxi ,yi ;
(2) if wxi+1 ,yi+1 > wxi ,yi +1 , then Ai+1 ≤ (4 − /3)wxi ,yi .
PROOF. For a fixed i ∈ [m], by Lemma 6.5, consider a naming  ∈ Vk−1 with t ∈ (Vk) =
(Txi ,yi +1 ), where k = xi q + yi + 2.
If, by contradiction, there exists i ≤ m−3 such that Ai > (4−/3)wxi ,yi , the routing cost
from u to the node named t, which is in Txi ,yi +1 , is at least 2Ai +wxi ,yi +1 > (8−2/3)wxi ,yi +
wxi ,yi +1 . It results in a stretch more than 9 − , because wxi ,yi /wxi ,yi +1 ≥ q/(q + 1).
Contradiction.
If, by contradiction, there exists i ∈ [log ] such that wxi+1 ,yi+1 > wxi ,yi +1 and Ai+1 >
(4 − /3)wxi ,yi , the routing cost from u to the node named t, which is in Txi ,yi +1 , is at
least 2Ai+1 + wxi ,yi +1 > (8 − 2/3)wxi ,yi + wxi ,yi +1 . It results in a stretch more than 9 − ,
because wxi ,yi /wxi ,yi +1 ≥ q/(q + 1). Contradiction.
Based on Claim 6.8, we have the following claim, whose proof appears in Section 6.2.1.
CLAIM 6.9. There exists i ≤ m − 4 such that Ai+1 > (4 − /4)wxi ,yi .
Let i be the index as defined in Claim 6.9. Note that, if wxi+1 ,yi+1 > wxi ,yi +1 , there
is a contradiction between Claims 6.9 and 6.8(2). If wxi+1 ,yi+1 = wxi ,yi +1 , we have that
w
q+yi
Ai+1 > (4 − /4) · wx xi ,y,yi wxi+1 ,yi+1 = (4 − /4) · q+y
wxi+1 ,yi+1 ≥ (4 − /3)wxi+1 ,yi+1 , where
i +1
i+1 i+1

the last inequality follows from q ≥ 48/ − 4. It contradicts with Claim 6.8(1).
In addition, we have the following lemma, whose proof appears in Section 6.2.1.
LEMMA 6.10. The doubling dimension α of G is no more than 6 − log .
Therefore, Theorem 1.2 follows.

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:26

G. Konjevod et al.

6.2.1. Proofs.

PROOF
wxi+1 ,yi+1

wx

,y

CLAIM 6.9: Let ri = wxAi,y and r̃i = wi+1x ,yi+1 for i ∈ [m]. Since Ai+1 = Ai +
i i
i i
= (ri + r̃i )wxi ,yi and Ai+1 = ri+1 wxi+1 ,yi+1 , we have that ri + r̃i = ri+1r̃i . Then,
OF

m−4

k=0

rk+1r̃k =

m−4


(rk + r̃k)

k=0

= r0 +

m−4


(rk+1 + r̃k) − rm−4

k=0

≥2

m−4



rk+1r̃k + r0 − rm−4

(30)

k=0

≥2

m−4



rk+1r̃k − 3,

k=0

 √

where the first inequality follows from the inequality k(xk + yk) ≥ 2 k xk yk, which
holds for all sequences of nonnegative numbers xk, yk, and the second inequality follows
because r0 = 1, and rm−4 ≤ 4 by Claim 6.8(1).
√
Now, by averaging, there exists i ∈ [0, m − 4] such that ri+1r̃i ≥ 2 ri+1
r̃i − 3/(m − 3).
√
√
By solving the quadratic equation
in
r
r̃
,
we
get
that
r
r̃
>
1
+
1 − 3/(m − 3).
i+1
i
i+1
i

Then, ri+1r̃i > 2 − 3/(m− 3) + 2 1 − 3/(m − 3) > 4 − 9/(m− 3). Note that ri+1r̃i = wAxi+1,y .
i i
By the following claim, we have that ri+1r̃i > 4 − 9/(m − 3) ≥ 4 − /4. It finishes the
proof.
CLAIM 6.11. m ≥ p/2.
PROOF. For any i ∈ [m], we have that wxi+1 ,yi+1 < 4wxi ,yi . Otherwise, by Lemma 6.5,
consider a naming  ∈ Vk−1 with t ∈ (Vk) = (Txi ,yi +1 ), where k = xi q + yi + 2. The
routing cost from u to the node named t, which is in Txi ,yi +1 , is at least 2(wxi ,yi +
wxi+1 ,yi+1 ) + wxi ,yi +1 ≥ 10wxi ,yi + wxi ,yi +1 . It results in a stretch more than 11 − O(),
because wxi ,yi /wxi ,yi +1 ≥ q/(q + 1). Contradiction. Thus wxi+1 ,yi+1 < 4wxi ,yi and by a
similar argument, we have that wx0 ,y0 ≤ w2,0 . Therefore, m ≥ p/2.
PROOF OF LEMMA 6.10: Let B be a ball of radius r centered at v for any r > 0 and any
node v ∈ V (G).
If u ∈ B, then B is contained in the path Ti, j that contains the center node v and
r < d(u, v). Thus, B can be covered by at most 3 balls of radius r/2, since Ti, j is a path.
If u ∈ B and d(u, v) ≥ r/2, then B can be covered by the ball centered at u of radius
r/2 and the ball centered at v of radius r/2.
If u ∈ B and d(u, v) < r/2, then B can be covered by the ball centered at u of radius
r/2 and the paths {Ti−1, j , Ti−1, j+1 , . . . , Ti, j }, where wi, j ≤ r < wi, j+1 . Then, the nodes
that belong to the ball B in each of these paths can be covered by the ball of radius
r/2 centered at the median node of the path since the length of any these paths is less
than 1, thereby less than r/2. Thus, B can be covered with no more than q + 2 balls of
radius r/2.
Therefore, the shortest-path metric of G is a doubling metric with dimension at most
log(q + 2) ≤ 6 − log  for q = 48/ − 4.
ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:27

We are now ready to conclude the proof of Theorem 1.2. Let k be the index as defined
> (4 − /4). Suppose that wx,y = bk. There are two cases
in Claim 6.9 such that Abk+1
k
depending on whether bk+1 = wx,y+1 .
x
wx,y
(q+y)
q
k+1
= Abk+1
> (4 − /4) 2x2(q+y+1)
≥ (4 − /4) q+1
≥
(1) If bk+1 = wx,y+1 , then we have Abk+1
k wx,y+1
4 − /3, since q ≥ 48/ − 4. On the other hand, by Claim 6.8, Ak+1 ≤ (4 − /3)bk+1 for
k + 1 ≤ m − 3, leading to a contradiction.
(2) If bk+1 = wx,y+1 , then bk+1 > wx,y+1 . Thus, by Claim 6.8(2), Ak+1 ≤ (4 − /3)bk <
(4 − /4)bk, a contradiction.
Therefore, the theorem follows.
7. CONCLUSION AND FUTURE WORK

In this article, for networks of low doubling dimension α ∈ O(loglog n), we presented (i) a scale-free (9 + )-stretch name-independent routing scheme that requires
(( 1 ) O(α) log3 n)-bit routing information at each node; and (ii) a scale-free (1 + )-stretch
labeled routing scheme with log n-bit routing labels that requires (( 1 ) O(α) log3 n)-bit
routing information at each node. In addition, for name-independent routing schemes,
we presented a matching lower bound that shows that our scale-free name-independent
routing scheme achieves asymptotically optimal stretch in networks of low doubling
dimension.
Since stretch 9 is asymptotically optimal for name-independent compact routing
schemes in networks of low doubling dimension by Theorem 1.2, and since (2k + 1)stretch routing schemes for general graphs require ((n log n)1/k)-bit storage at some
nodes [Abraham et al. 2006a], a natural open question is whether we can achieve better
stretch if we relax some of our routing requirements. That is, can we achieve better
stretch if we allow a small constant fraction of nodes to use larger space, or a small
constant fraction of source-destination pairs to incur larger routing stretch?
Abraham et al. [2006a] show that any name-independent routing scheme for general
graphs with o((n/(9k))1/k)-bit storage at each node has average stretch at least k/4+7/8.
Hence, an interesting question is whether a constant-stretch name-independent compact routing scheme for general graphs with relaxed guarantees exists. Furthermore, in
the labeled routing model, it may be interesting to investigate whether we can achieve a
(1 + )-stretch labeled routing scheme for general graphs with relaxed guarantees. The
strongest lower bound result for labeled routing in general graphs states that a labeled
scheme with stretch less than 3 requires (n1/2 )-bit storage at some nodes [Thorup and
Zwick 2001].
ACKNOWLEDGMENTS
The authors would like to thank anonymous reviewers for helpful comments and suggestions.

REFERENCES
Ittai Abraham and Cyril Gavoille. 2006. Object location using path separators. In Proceedings of the 25th
Annual ACM Symposium on Principles of Distributed Computing (PODC’06). ACM, New York, NY,
188–197. DOI:http://dx.doi.org/10.1145/1146381.1146411
Ittai Abraham, Cyril Gavoille, Andrew V. Goldberg, and Dahlia Malkhi. 2006c. Routing in networks with
low doubling dimension. In Proceedings of the 26th IEEE International Conference on Distributed Computing Systems (ICDCS’06). IEEE Computer Society, Washington, DC, 75–. DOI:http://dx.doi.org/10.
1109/ICDCS.2006.72
Ittai Abraham, Cyril Gavoille, and Dahlia Malkhi. 2004. Routing with improved communication-space tradeoff. In Distributed Computing: 18th International Conference (DISC’04), Vol. 3274. Springer, 305.

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

27:28

G. Konjevod et al.

Ittai Abraham, Cyril Gavoille, and Dahlia Malkhi. 2006a. On space-stretch trade-offs: Lower bounds. In Proceedings of the 18th Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA’06).
ACM, New York, NY, 207–216. DOI:http://dx.doi.org/10.1145/1148109.1148143
Ittai Abraham, Cyril Gavoille, and Dahlia Malkhi. 2006b. On space-stretch trade-offs: Upper bounds. In Proceedings of the 18th Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA’06).
ACM, New York, NY, 217–224. DOI:http://dx.doi.org/10.1145/1148109.1148144
Ittai Abraham and Dahlia Malkhi. 2005. Name independent routing for growth bounded networks. In Proceedings of the 17th Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA’05).
ACM, New York, NY, 49–55. DOI:http://dx.doi.org/10.1145/1073970.1073978
Ittai Abraham, Dahlia Malkhi, and Oren Dobzinski. 2004. LAND: Stretch (1 + ) locality-aware networks for DHTs. In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA’04). Society for Industrial and Applied Mathematics, Philadelphia, PA, 550–559. http://dl.acm.
org/citation.cfm?id=982792.982873
B. Awerbuch and D. Peleg. 1990. Sparse partitions. In Proceedings of the 31st Annual Symposium on
Foundations of Computer Science (SFCS’90). IEEE Computer Society, Washington, DC, 503–513 vol. 2.
DOI:http://dx.doi.org/10.1109/FSCS.1990.89571
Baruch Awerbuch and David Peleg. 1992. Routing with polynomial communication-space trade-off. SIAM
Journal on Discrete Mathematics 5, 2, 151–162. DOI:http://dx.doi.org/10.1137/0405013
Hubert T.-H. Chan, Anupam Gupta, Bruce M. Maggs, and Shuheng Zhou. 2005. On hierarchical routing in doubling metrics. In Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA’05). Society for Industrial and Applied Mathematics, Philadelphia, PA, 762–771.
http://dl.acm.org/citation.cfm?id=1070432.1070540
T.-H. Hubert Chan, Michael Dinitz, and Anupam Gupta. 2006. Spanners with slack. In Proceedings of the
14th Conference on Annual European Symposium - Volume 14 (ESA’06). Springer-Verlag, London, UK,
196–207. DOI:http://dx.doi.org/10.1007/11841036_20
Richard Cole and Lee-Ad Gottlieb. 2006. Searching dynamic point sets in spaces with bounded doubling
dimension. In Proceedings of the 38th Annual ACM Symposium on Theory of Computing (STOC’06).
ACM, New York, NY, 574–583. DOI:http://dx.doi.org/10.1145/1132516.1132599
Lenore J. Cowen. 2001. Compact routing with minimum stretch. Journal of Algorithms 38, 1, 170–183.
DOI:http://dx.doi.org/10.1006/jagm.2000.1134
Tamar Eilam, Cyril Gavoille, and David Peleg. 1998. Compact routing schemes with low stretch factor
(extended abstract). In Proceedings of the 17th Annual ACM Symposium on Principles of Distributed
Computing (PODC’98). ACM, New York, NY, 11–20. DOI:http://dx.doi.org/10.1145/277697.277702
Pierre Fraigniaud and Cyril Gavoille. 2001. Routing in trees. In Proceedings of the 28th International
Colloquium on Automata, Languages and Programming, (ICALP’01). Springer-Verlag, London, UK,
757–772. http://dl.acm.org/citation.cfm?id=646254.684110.
Cyril Gavoille. 2001. Routing in distributed networks: Overview and open problems. SIGACT News 32, 1,
36–52. DOI:http://dx.doi.org/10.1145/568438.568451
Cyril Gavoille and David Peleg. 2003. Compact and localized distributed data structures. Distributed Computing 16, 2–3, 111–120. DOI:http://dx.doi.org/10.1007/s00446-002-0073-5
Anupam Gupta, Robert Krauthgamer, and James R. Lee. 2003. Bounded geometries, fractals, and lowdistortion embeddings. In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS’03). IEEE Computer Society, Washington, DC, 534–. http://dl.acm.org/citation.
cfm?id=946243.946308
Sariel Har-Peled and Manor Mendel. 2005. Fast construction of nets in low dimensional metrics, and their
applications. In Proceedings of the 21st Annual Symposium on Computational Geometry (SCG’05). ACM,
New York, NY, 150–158. DOI:http://dx.doi.org/10.1145/1064092.1064117
David R. Karger and Matthias Ruhl. 2002. Finding nearest neighbors in growth-restricted metrics. In
Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC’02). ACM, New York,
NY, 741–750. DOI:http://dx.doi.org/10.1145/509907.510013
Jon Kleinberg, Aleksandrs Slivkins, and Tom Wexler. 2009. Triangulation and embedding using small sets of
beacons. Journal of the ACM 56, 6, Article 32, 37 pages. DOI:http://dx.doi.org/10.1145/1568318.1568322
Goran Konjevod, Andréa W. Richa, and Donglin Xia. 2006. Optimal-stretch name-independent compact routing in doubling metrics. In Proceedings of the 25th Annual ACM Symposium on Principles of Distributed
Computing (PODC’06). ACM, New York, NY, 198–207. DOI:http://dx.doi.org/10.1145/1146381.1146412
Goran Konjevod, Andréa W. Richa, and Donglin Xia. 2007. Optimal scale-free compact routing schemes
in networks of low doubling dimension. In Proceedings of the 18th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA’07). Society for Industrial and Applied Mathematics, Philadelphia, PA,
939–948. http://dl.acm.org/citation.cfm?id=1283383.1283484

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Scale-Free Compact Routing Schemes in Networks of Low Doubling Dimension

27:29

Robert Krauthgamer and James R. Lee. 2004. Navigating nets: Simple algorithms for proximity search.
In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA’04). Society for Industrial and Applied Mathematics, Philadelphia, PA, 798–807. http://dl.acm.org/citation.
cfm?id=982792.982913
Robert Krauthgamer, James R. Lee, Manor Mendel, and Assaf Naor. 2004. Measured descent: A new embedding method for finite metrics. In Proceedings of the 45th Annual IEEE Symposium on Foundations of
Computer Science. IEEE Computer Society, 434–443.
David Peleg. 2000. Distributed Computing: A Locality-sensitive Approach. Society for Industrial and Applied
Mathematics, Philadelphia, PA.
Aleksandrs Slivkins. 2005a. Distance estimation and object location via rings of neighbors. In Proceedings of
the 24th Annual ACM Symposium on Principles of Distributed Computing (PODC’05). ACM, New York,
NY, 41–50. DOI:http://dx.doi.org/10.1145/1073814.1073823
Aleksandrs Slivkins. 2005b. Distributed approaches to triangulation and embedding. In Proceedings of the
16th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA’05). Society for Industrial and
Applied Mathematics, Philadelphia, PA, 640–649. http://dl.acm.org/citation.cfm?id=1070432.1070522.
Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan. 2001. Chord: A
scalable peer-to-peer lookup service for Internet applications. In Proceedings of the 2001 Conference on
Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM’01).
ACM, New York, NY, 149–160. DOI:http://dx.doi.org/10.1145/383059.383071
Kunal Talwar. 2004. Bypassing the embedding: Algorithms for low dimensional metrics. In Proceedings of
the 36th Annual ACM Symposium on Theory of Computing (STOC’04). ACM, New York, NY, 281–290.
DOI:http://dx.doi.org/10.1145/1007352.1007399
Mikkel Thorup. 2004. Compact oracles for reachability and approximate distances in planar digraphs. Journal of the ACM 51, 6, 993–1024. DOI:http://dx.doi.org/10.1145/1039488.1039493
Mikkel Thorup and Uri Zwick. 2001. Compact routing schemes. In Proceedings of the 13th Annual
ACM Symposium on Parallel Algorithms and Architectures (SPAA’01). ACM, New York, NY, 1–10.
DOI:http://dx.doi.org/10.1145/378580.378581
Received April 2007; revised October 2008; accepted January 2016

ACM Transactions on Algorithms, Vol. 12, No. 3, Article 27, Publication date: June 2016.

Universal Coating for Programmable Matter
Zahra Derakhshandeha,1,∗, Robert Gmyrb,2 , Andréa W. Richaa,1 , Christian
Scheidelerb,2 , Thim Strothmannb,2
a

arXiv:1601.01008v1 [cs.ET] 5 Jan 2016

b

Computer Science, CIDSE, Arizona State University, USA
Department of Computer Science, Paderborn University, Germany

Abstract
The idea behind universal coating is to have a thin layer of a specific substance covering an object of any shape so that one can measure a certain
condition (like temperature or cracks) at any spot on the surface of the object without requiring direct access to that spot. We study the universal
coating problem in the context of self-organizing programmable matter consisting of simple computational elements, called particles, that can establish
and release bonds and can actively move in a self-organized way. Based on
that matter, we present a worst-case work-optimal universal coating algorithm that uniformly coats any object of arbitrary shape and size that allows
a uniform coating. Our particles are anonymous, do not have any global
information, have constant-size memory, and utilize only local interactions.
Keywords: Programmable Matter, Self-Organizing Particle Systems,
Object Coating
1. Introduction
Today, engineers often need to visually inspect bridges, tunnels, wind
turbines and other large civil engineering structures for defects — a task that
is both time-consuming and costly. In the not so distant future, smart coating
∗

Corresponding author
Email addresses: zderakhs@asu.edu (Zahra Derakhshandeh), gmyr@mail.upb.de
(Robert Gmyr), aricha@asu.edu (Andréa W. Richa), scheideler@upb.de (Christian
Scheideler), thim@mail.upb.de (Thim Strothmann)
1
Supported in part by the NSF under Awards CCF-1353089 and CCF-1422603.
2
Supported in part by DFG grant SCHE 1592/3-1.

Preprint submitted to Theoretical Computer Science

January 7, 2016

(a)
Figure 1:

(b)

(c)

(a) shows a section of Geqt . Nodes of Geqt are shown as black circles.
(b) shows five particles on Geqt . The underlying graph Geqt is depicted as a gray
mesh. A particle occupying a single node is depicted as a black circle, and a particle
occupying two nodes is depicted as two black circles connected by an edge. (c)
depicts two particles occupying two non-adjacent positions on Geqt . The particles
have different offsets for their head port labelings.

technology could do the job faster and cheaper, and increase safety at the
same time. The idea behind smart coating (also coined smart paint) is to have
a thin layer of a specific substance covering the object so that one can measure
a certain condition (like temperature or cracks) at any spot on the surface
of the object without requiring direct access to that spot. Also in nature,
smart coating occurs in various situations. Prominent examples are proteins
closing wounds, antibodies surrounding bacteria, or ants surrounding food
in order to transport it to their nest. So one can envision a broad range
of coating applications for programmable matter in the future. We intend
to study coating problems in the context of self-organizing programmable
matter consisting of simple computational elements, called particles, that
can establish and release bonds and can actively move in a self-organized
way. As a basic model for these self-organizing particle systems, we will use
the geometric version of the amoebot model presented in [1, 2].
1.1. Amoebot model
We assume that any structure the particle system can form can be represented as a subgraph of an infinite graph G = (V, E) where V represents all
possible positions the particles can occupy relative to their structure, and E
represents all possible atomic transitions a particle can perform as well as all
places where neighboring particles can bond to each other. In the geometric
amoebot model, we assume that G = Geqt , where Geqt = (Veqt , Eeqt ) is the
infinite regular triangular grid graph, see Figure 1(a).
We briefly recall the main properties of the geometric amoebot model.
Each particle occupies either a single node or a pair of adjacent nodes in
2

Geqt , and every node can be occupied by at most one particle. Two particles
occupying adjacent nodes are connected by a bond, and we refer to such
particles as neighbors. The bonds do not just ensure that the particles form
a connected structure but they are also used for exchanging information as
explained below.
Particles move through expansions and contractions: If a particle occupies one node (i.e., it is contracted ), it can expand to an unoccupied adjacent
node to occupy two nodes. If a particle occupies two nodes (i.e., it is expanded ), it can contract to one of these nodes to occupy only a single node.
Figure 1(b) illustrates a set of particles (some contracted, some expanded)
on the underlying graph Geqt . For an expanded particle, we denote the node
the particle last expanded into as the head of the particle and call the other
occupied node its tail. A handover allows particles to stay connected as
they move. Two scenarios are possible here: (1) a contracted particle p can
“push” a neighboring expanded particle q and expand into the neighboring
node previously occupied by q, forcing q to contract, or (2) an expanded particle p can “pull” a neighboring contracted particle q to node v it occupies
thereby causing q to expand into v, which allows p to contract.
Particles are anonymous but each particle has a collection of ports, one
for each edge incident to the nodes occupied by it, that have unique labels.
Adjacent particles establish bonds through the ports facing each other. We
also assume that the particles have a common chirality, i.e., they all have
the same notion of clockwise (CW) direction, which allows each particle p
to order its head port labels in clockwise order. However, particles do not
have a common sense of orientation since they can have different offsets of
the labelings, see Figure 1(c). W.l.o.g.3 , we assume that each particle labels
its head ports from 0 to 5 in clockwise order. Whenever a particle p is
connected to a particle q, we assume that p knows the label of q’s bond that
p is connected with.
Each particle has a constant-size shared local memory that can be read
and written to by any neighboring particle. This allows a particle to exchange
information with a neighboring particle by simply writing it into the other
particle’s memory.4 A particle always knows whether it is contracted or
3

Without loss of generality.
In [1, 2], the model was presented as having a shared memory for each port that is
visible only to the respective neighbor: The two variants of the model are equivalent, in
the sense that they can emulate each other trivially; we adopt the one here for convenience.
4

3

expanded, and in the latter case it also knows along which head port label it
is expanded. W.l.o.g. we assume that this information is also available to the
neighboring particles (by publishing that label in its local shared memory).
Particles do not know the total number of particles, nor do they have any
estimate on this number.
We assume the standard asynchronous model from distributed computing, where the particle system progresses through a sequence of particle activations, i.e., only one particle is active at a time. Whenever a particle is
activated, it can perform an arbitrary bounded amount of computation (involving its local memory as well as the shared memories of its neighbors) and
at most one movement. A round is defined as the elapsed time until each
particle has been activated at least once.
We count time according to the number of particle activations that have
already happened since the start of the activation sequence. We assume the
activation sequence to be fair, i.e., at any point in time, every particle will
eventually be activated. The configuration C of the system at the beginning
of time t consists of the nodes in Geqt occupied by the object and the set of
particles; in addition, for every particle p, C contains the current state of p,
including whether the particle is expanded or contracted, its port labeling,
and the contents of its local memory. The work spent by the particles till
time t is measured by the number of movements they have done until that
point. (We ignore other state changes since their energy consumption should
be irrelevant compared to the energy for a movement.) For more details on
the model, please refer to [1].
1.2. Universal Coating Problem
For any two nodes v, w ∈ Veqt , the distance d(v, w) between v and w is
the length of the shortest path in Geqt from v to w. The distance d(v, U )
between a v ∈ Veqt and U ⊆ Veqt is defined as minw∈U d(v, w).
In the universal coating problem we are given an instance (P, O) where P
represents the particle system and O the fixed object to be coated. Let V (P )
be the set of nodes occupied by P and V (O) be the set of nodes occupied by
O (when clear from the context, we may omit the V (·) notation). We call
the set of nodes in Geqt neighboring O the surface (coating) layer. Let n be
the number of particles and B be the number of nodes in the surface layer.
An instance is called valid if the following properties hold:
1. The particles are all contracted and start in an idle state.
4

Figure 2: An example of an object with a tunnel of width 1.

2. The subgraphs of Geqt induced by V (O) and V (P )∪V (O), respectively,
are connected, i.e., we are dealing with a single object and the particle
system is connected to the object.
3. The subgraph of Geqt induced by Veqt \ V (O) is connected, i.e., the
object O does not contain any holes.5
4. Veqt \ V (O) is 2(d Bn e + 1)-connected. In other words, O cannot form
tunnels of width less than 2(d Bn e + 1).
Note that a width of at least 2d Bn e is needed to guarantee that the object
can be evenly coated. See Figure 2 for an example of an object with a
tunnel of width 1. Since coating narrow tunnels requires specific technical
mechanisms that complicate the protocol and do not add much to the basic
idea of coating, we decided to ignore narrow tunnels completely in favor of a
clean presentation.
A configuration C is legal if and only if all particles are contracted and
min

v∈Veqt \(V (P )∪V (O))

d(v, V (O)) ≥ max d(v, V (O))
v∈V (P )

i.e., the particles are as close to the object as possible, which means that
they coat O as evenly as possible.
An algorithm solves the universal coating problem if, starting from any
valid configuration, it reaches a stable legal configuration C in a finite number
of rounds. A configuration C is said to be stable if no particle in C ever
performs a state change or movement.
5

If O does contain holes, we consider the subset of particles in each connected region
of Veqt \ V (O) separately.

5

1.3. Our Contributions
Our main contribution in this paper is a worst-case work-optimal algorithm for the universal coating problem on self-organizing particle systems.
Our Universal Coating Algorithm seamlessly adapts to any valid object O,
uniformly coating the object by forming multiple coating layers if necessary. As stated in Section 1.1, our particles are anonymous, do not have any
global information (including on the number of particles n), have constantsize memory, and utilize only local interactions.
Our algorithm builds upon many primitives, some of which may be of
interest on their own: The spanning forest primitive organizes the particles
into a spanning forest which is used to guide the movement of particles while
preserving connectivity in the system; the complaint-based coating primitive
allows the first layer to form, only expanding the coating of the first layer
as long as there is still room and there are particles still not touching the
object; the general layering primitive allows the layer ` to form only after
layer ` − 1 has been completed, for ` ≥ 2; and a node-based leader election
primitive elects a position (in Geqt ) to house a leader particle, which is used
to jumpstart the general layering process. One of the main contributions of
our work is to show how these primitives can be integrated in a seamless way,
with no underlying synchronization mechanisms.
1.4. Related work
Many approaches have already been proposed that can potentially be
used for smart coating. One can distinguish between active and passive systems. In passive systems the particles either do not have any intelligence
at all (but just move and bond based on their structural properties or due
to chemical interactions with the environment), or they have limited computational capabilities but cannot control their movements. Examples of
research on passive systems are DNA self-assembly systems (see, e.g., the
surveys in [3, 4, 5]), population protocols [6], and slime molds [7, 8]. We will
not describe these models in detail since we are focusing on active systems.
In active systems, computational particles can control the way they act and
move in order to solve a specific task. Robotic swarms, and modular robotic
systems are some examples of active programmable matter systems.
Especially in the area of swarm robotics the problem of coating objects has
been studied extensively. In swarm robotics, it is usually assumed that there
is a collection of autonomous robots that have limited sensing, often including vision, and communication ranges, and that can freely move in a given
6

area. However, coating of objects is commonly not studied as a stand-alone
problem, but is part of collective transport (e.g., [9]) or collective perception
(see respective section of [10, 11] for a summary of results). In collective
transport a group of robots has to cooperate in order to transport an object.
In general, the object is heavy and cannot be moved by a single robot, making cooperation necessary. In collective perception, a group of robots with a
local perception each (i.e., only a local knowledge of the environment), aims
at joining multiple instances of individual perceptions to one big global picture (e.g. to collectively construct a sort of map). Some research focuses on
coating objects as an independent task under the name of target surrounding
or boundary coverage. The techniques used in this context include stochastic
robot behaviors [12, 13], rule-based control mechanisms [14] and potential
field-based approaches [15]. Surveys of recent results in swarm robotics can
be found in [16, 17, 10, 11]; other samples of representative work can be
found in e.g., [18, 19, 20, 21, 22]. While the analytic techniques developed
in the area of swarm robotics and natural swarms are of some relevance for
this work, the individual units in those systems have more powerful communication and processing capabilities than the systems we consider, and they
can move freely.
In a recent paper [23], Michail and Spirakis propose a model for network
construction that is inspired by population protocols [6]. The population
protocol model relates to self-organizing particles systems, but is also intrinsically different: agents (which would correspond to our particles) freely
move in space and can establish connections to any other agent in the system
at any point in time, following the respective probabilistic distribution. In
the paper the authors focus on network construction for specific topologies
(e.g., spanning line, spanning star, etc.). However, in principle, it would be
possible to adapt their approach also for studying coating problems under
the population protocol model.
1.5. Structure of the paper
Section 2 describes our Universal Coating algorithm. Formal correctness
and worst-case work analyses of the algorithm follow in Section 3. We address
some applications of our universal coating algorithm in Section 4, and present
our concluding remarks in Section 5.

7

2. Universal Coating Algorithm
In this section we present our Universal Coating algorithm: In Section 2.1,
we introduce some preliminary notions; Section 2.2 introduces the algorithmic primitives used for the coating algorithm; and lastly Section 2.3 focuses
on the leader election process that is needed in certain instances of the problem.
2.1. Preliminaries
We define the set of states that a particle can be in as idle, follower,
root, and retired. In addition to its state, a particle may maintain a constant
number of flags (constant size pieces of information to be read by neighboring
particles). While particles are anonymous, when a particle p sets a flag of
type x in its shared memory, we will denote it by p.x (e.g., p.parent, p.dir,
etc.), so that ownership of the respective flag becomes clear. In our proposed
algorithm, we assume that every time a particle contracts, it contracts out
of its tail. Therefore, a node occupied by the head of a particle p still is
occupied by p after a contraction.
We define a layer as the set of nodes v in Geqt that are equidistant to
the object O. More specifically a node v is in layer ` if d(v, V (O)) = `;
in particular the surface coating layer defined earlier corresponds to layer
1. Any root or retired particle p stores a flag p.layer indicating the layer
number of the node occupied by the head of p. We say a layer is filled or
complete if all nodes in that layer are occupied with retired particles. In
order to respect the particles’ constant-size memory constraints, we take all
layer numbers modulo 4. However, for ease of presentation, we will omit the
modulo 4 computations in the text, except for in the pseudocode description
of the algorithms.
Each root particle p has a flag storing a port label p.down pointing to
an occupied node adjacent to its head in layer p.layer − 1 or in the object
if p − layer = 1. Moreover, p has two additional flags, p.CW and p.CCW ,
which are also port labels. Intuitively, if p continuously moves by expanding
in direction p.CW (resp., p.CCW ) and then contracting, it moves along
a clockwise (resp. counter-clockwise) path around the connected structure
consisting of the object and retired particles. Formally, p.CW is the label
of the first port to a node v in counter-clockwise (CCW) order from p.down
such that either v is occupied by a particle q with q.layer = p.layer, or v is
unoccupied (in the latter, v may be a node on layer p.layer or p.layer−1). We
8

layer 3
layer 2
layer 1

Figure 3: We illustrate the first three coating layers with respect to the given object
(represented by the nodes in Geqt shaded in black); we also illustrate the direction in
which these layers will be filled by our algorithm — CW for odd layers, and CCW for even
layers — as determined in Section 2.2.

define p.CCW analogously, following a clockwise (CW) order from p.down.
Figure 3 illustrates the different layers around an object, and also CW and
CCW traversals of those.
2.2. Coating Primitives
Our algorithm can be decomposed into a set of primitives, which are all
concurrently executed by the particles, as we briefly described in Section 1.3.
Namely the algorithm relies on the following key primitives: the spanning
forest primitive, the complaint-based coating primitive used to establish the
first layer of coating, the general layering primitive, and a node-based (rather
than particle-based) leader election primitive that works even as particles
move, and that is used to jumpstart the general layering primitive. One of
the main contributions of our work is to show how these primitives can be put
to work together in a seamless way and with no underlying synchronization
mechanisms.6
The spanning forest primitive (Algorithm 1) organizes the particles
in a spanning forest, in which the roots of the trees will be in state root
and will determine the direction of movement which is specified by a port
label p.dir; the remaining non-retired particles follow the root particles using
6

A video illustrating a fully asynchronous execution of our universal coating algorithm
can be found in [24].

9

handovers. The main benefit of organizing the particles in a spanning forest
connected to the surface is that it provides a relatively easy mechanism for
particles to move, following the tree paths, while maintaining connectivity
in the system (see [1, 25] for more details). All particles are initially idle. A
particle p becomes a follower when it sets a flag p.parent corresponding to
the port leading to its parent in the spanning forest (any adjacent particle q
to p can then easily check if q is a child of p). As the root particles find final
positions according to the partial coating of the object, they stop moving and
become retired. Namely, a root particle p becomes retired when it encounters
another retired particle across the direction p.dir.
Recall that B denotes the number of nodes on the surface coating layer
(layer 1). We need to ensure that once min{n, B} particles are on layer 1, they
stop moving and the coating is complete, independent of how B compares to
n (i.e., whether n ≤ B or not); in addition, we would like to efficiently coat
one more surface scenario, namely that of coating just a bounded segment
of the surface, as we explain in Section 4. In order to be able to seamlessly
adapt to all possible coating configurations, we use our novel complaintbased coating primitive for the first layer, which basically translates into
having the root particles (touching the object) open up one more position on
layer 1 only if there exists a follower particle that remains in the system. This
is accomplished by having each particle that becomes a follower generate a
complaint flag, which will be forwarded by particles in a pipeline fashion from
children to parents through the spanning forest and then from a root q to
another root at q.dir, until it arrives at a root particle p with an unoccupied
neighboring node at p.dir (we call such a particle p a super-root). Upon
receiving a complaint flag, a super-root p consumes the flag and expands into
the unoccupied node at p.dir. The expansion will eventually be followed by a
contraction of p, which will induce a series of expansions and contractions of
the particles on the path from p to a follower particle z, eventually freeing a
position on the surface coating layer to be taken by z. In order to ensure that
the consumption of a complaint flag will indeed result in one more follower
touching the object, one must give higher priority to a follower child particle
in a handover operation, as we do in Algorithm 2. The complaint-based
coating phase of the algorithm will terminate either once all complaint flags
are consumed or when layer 1 is filled with contracted particles. In either
case, the particles on layer 1 will move no further. Figure 4 illustrates the
complaint-based coating primitive.
Once layer 1 is complete and if there are still follower particles in the
10

Algorithm 1 Spanning Forest Primitive
A particle p a acts depending on its state as described below:
idle:
If p is connected to the object O, it becomes a root particle,
makes the current node it occupies a leader candidate position,
and starts running the leader election algorithm described in
Section 2.3. If p is connected to a retired particle, p also becomes a root particle. If an adjacent particle p0 is a root or
a follower, p sets the flag p.parent to the label of the port to
p0 , puts a complaint flag in its local memory, and becomes a
follower. If none of the above applies, p remains idle.
follower:

If p is contracted and connected to a retired particle or to
O, then p becomes a root particle. Otherwise, if p is expanded, it considers the following two cases: (i) if p has a
contracted child particle q, then p initiates Handover(p);
(ii) if p has no children and no idle neighbor, then p contracts. Finally, if p is contracted, it runs the function
ForwardComplaint(p, p.parent) described in Algorithm 3.

root:

If particle p is on the surface coating layer, p participates in
the leader election process described in Section 2.3. If p is contracted, it first executes MarkerRetiredConditions(p)
(Algorithm 6), and becomes retired, and possibly also a
marker, accordingly; if p does not become retired, it calls LayerExtension (p) (Algorithm 4). If p is expanded, it considers
the following two cases: (i) if p has a contracted child, then p
initiates Handover(p); (ii) if p has no children and no idle
neighbor, then p contracts. Finally, if p is contracted, it runs
ForwardComplaint(p, p.dir) (Algorithm 3).

retired:

p clears a potential complaint flag from its memory and performs no further action.

11

(a)

(b)

(c)

(d)

Figure 4: Complaint-based coating primitive: Particles are shown as grey circles. In
(a), a follower particle generates a complaint flag (depicted as a black dot within the
particle) that is then forwarded to a super-root (b) causing the super-root to expand into
an unoccupied node (c). After a series of handovers, the follower particle that generated
the complaint flag can move to a position on the surface (d).

Algorithm 2 Handover (p)
1: if p.layer = 1 and p has a follower child q then
2:
if q is contracted then
3:
p initiates a handover with particle q
4: else
5:
if p has any contracted (follower or root) child q then
6:
p initiates a handover with particle q

Algorithm 3 ForwardComplaint(p, i)
1: if p holds a complaint flag and p’s parent does not hold a complaint flag
then
2:
p forwards the complaint flag to the particle given by p.parent

12

(a)

(b)

(c)

(d)

Figure 5: General layering primitive: Retired particles are shown as black circles, other
than (retired) marker particles which are shown in dark grey (the dark grey arrows represent the marker edges); a root particle is depicted in light grey. Black arrows show the
current direction of movement (given by the dir flag) for each particle (which becomes
irrelevant once a particle retires). (a) The root particle p is located on layer ` = 3; (b)
particle p moves in CW direction over retired particles on layer ` − 1; (c) after a series of
expansions and contractions following p.dir, p arrives at an unoccupied neighboring node
on layer ` − 1; (d) since p.dir leads to a retired particle, p retires too.

system, the general layering primitive steps in, which will build further
coating layers. We accomplish this by electing a leader marker particle on
layer 1 (via the leader election primitive proposed in Section 2.3). This
leader marker particle will be used to determine a “beginning” (and an “end”)
for layer 1 and allow the particles on that layer to start retiring according
to the retired condition given in Algorithm 6 (the leader marker particle
will be the first retired particle in the system). Once a layer ` becomes
completely filled with retired (contracted) particles, a new marker particle
will emerge on layer ` + 1, and start the process of building this layer (i.e.,
start the process of retiring particles on that layer) according to Algorithm 6.
A marker particle on layer ` + 1 only emerges if a root particle p connects to
the marker particle q on layer ` via its marker port and if q verified locally
that layer ` is completely filled (by checking whether q.CW and q.CCW are
both retired).
With the help of the marker particles — which can only be established
after layer 1 was completely filled (and hence, we must have B ≤ n) — we
can replace the complaint-based coating algorithm of layer 1 with a simpler
13

coating algorithm for the higher layers, where each root particle p just moves
in CW or CCW direction (depending on its layer number) until p encounters
a retired particle on the respective layer and retires itself. More precisely,
each contracted root particle p on layer ` tries to extend this layer by expanding into an unoccupied position on layer `, or by moving into an unoccupied
position in layer ` − 1 (when p.layer will change to ` − 1 accordingly), following the direction of movement given by p.dir. Figure 5 illustrates this
process. The direction p.dir is set to p.CW (resp., p.CCW ) when p.layer is
odd (resp., even), as illustrated in Figure 3. Alternating between CCW and
CW movements for the particles in consecutive layers ensures that a layer
` is completely filled with retired particles before particles start retiring in
layer ` + 1, which is crucial for the correctness of our layering algorithm.
Algorithm 4 LayerExtension (p)
Calculating p.layer, p.down and p.dir
1: The layer number of any node occupied by the object is equal to 0.
2: Let q be any neighbor of p with smallest layer number (modulo 4).
3: p.down ← p’s label for port leading to q
4: p.layer = (q.layer + 1) mod 4
5: clockwise (p, p.down)
. Computes CW & CCW directions
6: if p.layer is odd then
7:
p.dir ← p.CW
8: else
9:
p.dir ← p.CCW
Extending layer p.layer
10: if the position at p.dir is unoccupied, and either p is not on the first
layer, or p holds a complaint flag then
11:
p expands in direction p.dir
12:
p consumes its complaint flag, if it holds one

2.3. Leader Election Primitive
In this section, we describe the process used for electing a leader among
the particles that touch the object. Note that only particles in layer 1 will
ever participate in the leader election process. A leader will only emerge
if B ≤ n; otherwise the process will stop at some point without a leader
14

Algorithm 5 Clockwise (p, i)
1: j ← i, k ← i
2: while edge j is connected to the object or to a retired particle with layer
number p.layer − 1 do
3:
j ← (j − 1) mod 6
4: p.CW ← j
5: while edge k is connected to the object or to a retired particle with layer
number p.layer − 1 do
6:
k ← (k + 1) mod 6
7: p.CCW ← k

Algorithm 6 MarkerRetiredConditions(p)
First marker condition:
1: if p is leader particle then
2:
p becomes a retired particle
3:
p sets the flag p.marker to be the label of a port leading to a node
guaranteed not to be on layer p.layer — e.g., by taking the average
direction of p’s two neighbors in layer 1 (by now complete)

4:
5:
6:
7:

8:
9:

Extending Layer Markers:
if p is connected to a marker q and the port q.marker points towards p
then
if both q.CW and q.CCW are retired then
p becomes a retired particle
p sets the flag p.marker to the label of the port opposite of the
port connecting p to q
Retired Condition:
if edge p.dir is occupied by a retired particle then
p becomes retired

15

being elected. As discussed earlier, a leader is elected on layer 1 to provide
a “checkpoint” (a marker particle) that the particles can use in order to
determine whether the layer has been completely filled (and a leader is only
elected after this happens).
The leader election algorithm we use in this paper is a slightly modified
version of the leader election algorithm presented in [1] that can tolerate particles moving around on the surface layer while the leader election process
is progressing (in [1], leader election runs on a system of static particles).
Hence, for the purpose of universal coating, we will abstract the leader election algorithm to conceptually run on the nodes in layer 1, and not on the
particular particles that may occupy these nodes at different points in time.
The particles on layer 1 will simply provide the means for running the leader
election process on the respective positions, storing and transferring all the
flags (which can be used to implement the tokens described in [1]) that are
needed for the leader competition and verification. An expanded particle p
on layer 1, whose tail occupies node v in layer 1, that is about to perform a
handover with contracted particle q will pass all the information associated
with v to q using the particles’ local shared memories. If a particle p occupying position v would like to forward some leader election information to a
node w adjacent to v that is currently unoccupied, it will wait until either p
itself expands into w, or another particle occupies node w. It is important
to note that according to the complaint-based coating algorithm that we run
on layer 1, if a node v in layer 1 is occupied at some time t, then v will never
be left unoccupied after time t.
Here we outline the differences between the leader election process used
in this paper and that of [1]:
• Only the nodes on layer 1 that initially hold particles start as leader node
candidates. Other nodes in layer 1 will take part in the leader node election
process by forwarding any tokens between two consecutive leader node
candidates, as determined for the leader election process for a set of static
particles forming a cycle in [1]. Note that layer 1 is a cycle on Geqt .
• The leader election process will determine which leader node candidate in
layer 1 will emerge as the unique leader node. The leader particle is then
chosen as described below.
• If particle p is expanded, it will hold the flags and any other information
necessary for the leader election process corresponding to each node p occupies (head and tail nodes) independently. In other words, an expanded
16

particle emulates the leader election process for two nodes on the surface
layer.
• A particle p occupying node v forwards a flag τ to the node w in CW
(or CCW) direction along the surface layer only if node w is occupied
by a particle q (note that q may be equal to p, if p is expanded) and q
has enough space in its (constant-size) memory associated with node w;
otherwise p continues to hold the flag τ in its shared memory associated
with node v.
• If p is expanded along an edge (v, w) and wants to contract into node w,
there must exist a particle q expanding into v (due to the complaint-based
mechanism), and hence p will transfer all of its flags currently associated
with node v to particle q.
After the solitude verification phase in the leader election algorithm of [1]
is complete, there will be just one leader node v left in the system. Once
v is elected a leader node, a contracted particle p occupying this position
will check if layer 1 is completely filled with contracted particles. To do so,
when a contracted particle p occupies node v it will generate a single CHK
flag which it will forward to its CCW neighbor q only if q is contracted.
Any particle q receiving a CHK flag will also only forward the flag to its
CCW neighbor z if and only if z is contracted. If the CHK flag at a particle
q ever encounters an expanded CCW neighbor, the flag is held back until
the neighbor contracts. Additionally, the particle at position v sends out
a CLR flag to its CW neighbor as soon as it expands. This flag is always
forwarded in CW direction. If a CLR and a CHK flag meet at some particle,
the flags cancel each other out. If at some point in time, a particle p at node
v receives a CHK flag from its CW neighbor in layer 1, it implies that layer
1 must be completely filled with contracted particles (and the complaintbased algorithm for layer 1 has converged), and at that time this contracted
particle p elects itself the leader particle, setting the flag p.leader. Note
that the leader election process itself does not incur any additional particle
expansions or contractions on layer 1, only the complaint-based algorithm
does.
3. Analysis
In this section we show that our algorithm eventually solves the coating
problem, and we bound its worst-case work.
17

We say a particle p0 is the parent of particle p if p0 occupies the node
in direction p.parent. Let an active particle be a particle in either follower
or root state. We call an active particle a boundary particle if it has the
object or at least one retired particle in its neighborhood, otherwise it is a
non-boundary particle. A boundary particle is either a root or a follower,
whereas non-boundary particles are always followers. Note that throughout
the analysis we ignore the modulo computation of layers done by the particles,
i.e., layer 1 is the unique layer of nodes with distance 1 to the object.
Given a configuration C, we define a directed graph A(C) over all nodes
in Geqt occupied by active particles in C. For every expanded active particle
in C, A(C) contains a directed edge from the tail to the head node of p.
For every non-boundary particle p, A(C) has a directed edge from the head
of p to p.parent, if p.parent is occupied by an active particle, and for every
boundary particle p, p has a directed edge from its head to the node in the
direction of p.dir as it would be calculated by Algorithm 4, if p.dir is occupied
by an active particle. The ancestors of a particle p are all nodes reachable
by a path from the head of p in A(C). For each particle p we denote the
ancestor that has no outgoing edge with p.superRoot, if it exists. Certainly,
since every node has at most one outgoing edge in A(C), the nodes of A(C)
can only form a collection of disjoint trees or a ring of trees. We define a ring
of trees to be a connected graph consisting of a single directed cycle with
trees rooted at it.
First, we prove several safety conditions, and then we prove various liveness conditions that together will allow us to prove that our algorithm solves
the coating problem.
3.1. Safety
Suppose that we start with a valid instance (P, O), i.e., all particles in P
are initially contracted and idle and V (P ) ∪ V (O) forms a single connected
component in Geqt , among other properties. Then the following properties
hold, leading to the fact that V (P ) ∪ V (O) stays connected at any time.
Lemma 1. At any time, the set of retired particles forms completely filled
layers except for possibly the current topmost layer `, which is consecutively
filled with retired particles in CCW direction (resp. CW direction) if ` is
odd (resp. even).
Proof. From our algorithm it follows that the first particle that retires is
the leader particle, setting its marker flag in a direction not adjacent to a
18

position in layer 1. The particles in layer 1 then retire starting from the
leader in CCW direction around the object. Once all particles in layer 1
are retired, the first particle to occupy the adjacent position to the leader
via its marker flag direction will retire and become a marker particle on
layer 2, extending its marker flag in the same direction as the flag set by
the marker (leader) on layer 1. Starting from the marker particle in layer 2,
other contracted boundary particles can retire in CW direction along layer
2. Once all particles in layer 2 are retired, the next layer will start forming.
This process continues inductively, proving the lemma.
The next lemma characterizes the structure of A(C).
Lemma 2. At any time, A(C) is a forest or a ring of trees. Any node that
is a super-root (i.e., the root of a tree in A(C)) or part of the cycle in the
ring of trees is connected to the object or to a retired particle.
Proof. An active particle can either be a follower or a root. First, we show
the following claim.
Claim 1. At any time, A(C) restricted to non-boundary particles forms a
forest.
Proof. Let A0 (C) be the induced subgraph of A(C) by the non-boundary
particles only. Certainly, at the very beginning, when all particles are still
idle, the claim is true. So suppose that the claim holds up to time t. We
will show that it then also holds at time t + 1. Suppose that at time t + 1
an idle particle p becomes active. If it is a non-boundary particle (i.e., a
follower), it sets p.parent to a node occupied by a particle q that is already
active, so it extends the tree of q by a new leaf, thereby maintaining a tree.
Edges can only change if followers move. However, followers only move by a
handover or a contraction, thus a handover can only cause a follower and its
incoming edges to disappear from A0 (C) (if that follower becomes a boundary
particle), and an isolated contraction, can only cause a leaf and its outgoing
edge to disappear from A0 (C), so a tree is maintained in A0 (C) in each of
these cases.
Next we consider A(C) restricted to boundary particles.
Claim 2. At any time, A(C) restricted to boundary particles forms a forest
or a ring.

19

Proof. The boundary particles always occupy the nodes adjacent to retired
particles or the object. Therefore, due to Lemma 1, the boundary particles
either all lie in a single layer or in two consecutive layers. Since the layer
numbers uniquely specify the movement direction of the particles, connected
boundary particles within a layer are all connected in the same orientation.
Therefore, if these particles all lie in a single layer, they can only form a
directed list or directed cycle in A(C), proving the claim. If they lie in two
consecutive layers, say, ` and `−1, then `−1 must contain at least one retired
particle, so the nodes occupied by the boundary particles in layer ` − 1 can
only form a directed list. If there are at least two boundary particles in
layer ` − 1, this must also be true for the nodes occupied by the boundary
particles in layer ` because according to Lemma 1 there must be at least two
consecutive nodes in layer `−1 not occupied by retired particles. Moreover, it
follows from the algorithm that p.dir of a boundary particle can only point
to the same or the next lower layer of p, implying that in this case A(C)
restricted to the nodes occupied by all boundary particles forms a forest.
Since a boundary particle p never connects to a non-boundary particle
the way p.dir is defined, and a follower without an outgoing edge in A(C)
restricted to the non-boundary particles must have an outgoing edge to a
boundary particle (otherwise it is a boundary particle itself), A(C) is a forest
or a ring of trees. The second statement of the lemma follows from the fact
that every boundary particle must be connected to the object or a retired
particle.
Finally, we investigate the structure formed by the idle particles.
Lemma 3. At any time, every connected component of idle particles is connected to at least one non-idle particle or the object.
Proof. Initially, the lemma holds by the definition of a valid instance. Suppose that the lemma holds at time t and consider a connected component
of idle particles. If one of the idle particles in the component is activated,
it may either stay idle or change to an active particle, but in both cases the
lemma holds at time t + 1. If a retired particle that is connected to the
component is activated, it does not move. If a follower or root particle that
is connected to the component is activated, that particle cannot contract
outside of a handover with another follower or root particle, which implies
that no node occupied by it is given up by the active particles. So in any of
20

these cases, the connected component of idle particle remains connected to
a non-idle particle. Therefore, the lemma holds at time t + 1.
The following corollary is consequence of the previous three lemmas.
Corollary 1. At any time, V (P )∪V (O) forms a single connected component.
Lemma 4. At any time before the first particle retires, in every connected
component G of A(C), the number of expanded boundary particles in G plus
the number of complaint flags in G is equal to the number of non-boundary
particles in G.
Proof. Initially, the lemma holds trivially. Suppose the lemma holds at time t
and consider the next activation of a particle. We only discuss relevant cases.
If an idle particle becomes a non-boundary particle (i.e., it is not connected to
the object but joins a connected component), it also generates a complaint
flag. So both the number of non-boundary particles and the number of
complaint flags increases by one for the component the particle joins. If
a non-boundary particle expands as part of a handover with a boundary
particle, both the number of expanded boundary particles and the number
of non-boundary particles decrease by one for the component. If a boundary
particle expands as part of a handover, that handover must be with another
boundary particle, so the number of expanded boundary particles remains
unchanged for that component. Since by our assumption there is no retired
particle, all boundary particles are in layer 1. Hence, for a boundary particle
to expand outside of a handover, it has to consume a complaint flag. This
increases the number of expanded boundary particles by one and decreases
the number of complaint flags by one. Finally, an expansion of a boundary
particle outside of a handover can connect two components of A(C). Since the
equation given in the lemma holds for each of these components individually,
it also holds for the newly built component.
3.2. Liveness
We say that the particle system makes progress if (i) an idle particle
becomes active, or (ii) a movement (i.e., an expansion, handover, or contraction) is executed, or (iii) an active particle retires. In the following, we
always assume that we have a fair activation sequence for the particles.
Before we show under which circumstances our particle system eventually makes progress, we first establish some lemmas on how particles behave
during the execution of our algorithm.
21

Lemma 5. Eventually, every idle particle becomes active.
Proof. As long as an idle particle exists, there is always an idle particle p
that is connected to a non-idle particle or the object according to Lemma 3.
The next time p is activated p becomes active according to Algorithm 1.
Therefore, eventually all particles become active.
The following statement shows that even though super-roots can be followers, they will become a boundary particle the next time they are activated.
Lemma 6. In every tree of A(C), every boundary particle in the follower
state enters a root state the next time it is activated. In particular, every
super-root in A(C) will enter the root state the next time it is activated.
Proof. Let p be a follower boundary particle. By definition p must have a
retired particle or the object in its neighborhood. Therefore, p immediately
becomes a root particle once it is activated according to Algorithm 1.
Furthermore, the following lemma provides a relation between the movement of super-roots and the availability of complaint flags.
Lemma 7. For every tree of A(C) with a contracted super-root p and at
least one complaint flag, p will eventually retire or expand to p.dir, thereby
consuming a complaint flag, and after the expansion p may cease to be a
super-root.
Proof. If p is not a root, it becomes one the next time it is activated according
to Lemma 6. Therefore, assume p is a root. If there is a retired particle
in p.dir, p retires and ceases to be a super-root. If the node in p.dir is
unoccupied, p can potentially expand. According to Algorithm 3, complaint
flags are forwarded along the tree of p towards p. Once the flag reaches p, it
will expand, thereby consuming the flag. If p expands, it might have an active
particle in its movement direction and thus ceases to be a super-root.
Next, we prove the statement that expanded particles will not starve, i.e.,
they will eventually contract.
Lemma 8. Eventually, every expanded particle contracts.

22

Proof. Consider an expanded particle p in a configuration C. By Lemma 5
we can assume w.l.o.g. that all particles in C are active or retired. If there is
no particle q with either q.parent = p or p occupying the node in q.dir, then
p can contract once it is activated. If such a q exists and it is contracted, p
contracts in a handover (see Algorithm 2). If q exists and is expanded, we
consider the tree of A(C) that p is part of. Consider a subpath in this tree
that starts in p, i.e., (v1 , v2 , . . . , vk ) such that v1 , v2 are occupied by p and vk
is a node that does not have an incoming edge in A(C). Let vi be the first
node of this path that is occupied by a contracted particle. If all particles
are expanded, then clearly the last particle occupying vk−1 , vk eventually
contracts and we can set vi to vk−1 . Since vi is contracted it eventually
performs a handover with the particle occupying vi−2 , vi−1 . Now we can
move backwards along (v1 , v2 , . . . , vi−1 ) and it is guaranteed that a contracted
particle eventually performs a handover with the expanded particle occupying
the two nodes before it on the path. So eventually q is contracted, eventually
performs a handover with p and the statement holds.
In the following two lemmas we will specifically consider the case that
B ≤ n, i.e., the particles can coat at least one layer around the object.
Lemma 9. If B ≤ n, layer 1 is completely filled with contracted particles
eventually.
Proof. Consider a configuration C such that layer 1 is not completely filled
by contracted particles. Note that in this case the leader election cannot have
succeeded yet, which means that a leader cannot be elected, and therefore
no particle can be retired in configuration C. So by Lemma 5 we can assume
w.l.o.g. that all particles in configuration C are active.
Since layer 1 is not completely filled by contracted particles, there is
either at least one unoccupied node v on layer 1 or all nodes are occupied,
but there is at least one expanded particle on layer 1. We show that in
both cases a follower will move to layer 1, thereby filling up the layer until
all particles are contracted. In the first case, let p be the super-root of a
tree in A(C) that still has non-boundary particles, let (p0 = p, p1 , . . . , pk )
be the boundary particles of the tree such that pi−1 occupies the node in
pi .dir and let q be the non-boundary particle in the tree that is adjacent to
some pj in(p0 , . . . , pk ) such that j is minimal. If a particle pi in (p0 , . . . , pj =
q.parent) is expanded, it eventually contracts (Lemma 8) by a handover with
pi+1 , and by consecutive handovers all particles in (pi+1 , . . . , pj ) eventually
23

expand and contract until the particle pj = q.parent expands. According
to Algorithm 2, pj performs a handover with q. Therefore, the number
of particles on layer 1 has increased. If all particles in (p0 , . . . , q.parent)
are contracted, then by Lemma 4 a complaint flag still exists in the tree.
Eventually, p expands by Lemma 7. Consequently, we are back in the former
case that a particle in (p0 , . . . , q.parent) is expanded.
In the second case, let p0 be an expanded boundary particle and let q 0
be the non-boundary particle with the shortest path in A(C) to p0 . By a
similar argument as for the first case, particles on layer 1 perform handovers
(starting with p0 ) until eventually the node in q 0 .parent is occupied by a tail.
Again, q 0 eventually performs a handover and the number of particles on
layer 1 has increased.
As a direct consequence, we can show the following.
Lemma 10. If B ≤ n, a leader is elected in layer 1 eventually.
Proof. According to Lemma 9 layer 1 is eventually filled with contracted
particles. Leader Election successfully elects a leader node according to [1].
The contracted particle p occupying the leader node forwards the CHK flag
and eventually receives it back, since all particles are contracted. Therefore,
p becomes a leader.
Now we are ready to prove the two major statements of this subsection
that define two conditions for system progress.
Lemma 11. If all particles are non-retired and there is either a complaint
flag or an expanded particle, the system eventually makes progress.
Proof. If there is an idle particle, progress is ensured by Lemma 5. If an
active particle is expanded Lemma 8 guarantees progress. Finally, in the
last case all particles are active, none of them is expanded and there is a
complaint flag. If layer 1 is completely filled, a leader is elected according
to Lemma 10 and as a direct consequence the active particles on layer 1
eventually retire, guaranteeing progress. If layer 1 is not completely filled,
there exists at least one tree of A(C) with a contracted super-root p that
has an unoccupied node in p.dir and at least one complaint flag. Therefore,
progress is ensured by Lemma 7.
Lemma 12. If there is at least one retired particle and one active particle,
the system eventually makes progress.
24

Proof. Again, if there is an idle particle, progress is ensured by Lemma 5.
Moreover, note that since there is at least one retired particle, we can conclude that leader election has been successful (since the first particle that
retires is a leader particle) and therefore layer 1 has to be completely filled
with contracted particles. If there is still a non-retired particle on layer 1, it
eventually retires according to the Algorithm, guaranteeing progress.
So suppose that all particles in layer 1 are retired. We distinguish between the following cases: (i) there exists at least one super-root, (ii) no
super-root exists, but there is an expanded particle, and (iii) no super-root
exists and all particles are contracted. In case (i), Lemma 6 guarantees that
a super-root will eventually enter root state, and therefore it will eventually
either expand (if p.dir is unoccupied) or retire (since p.dir is occupied by a
retired particle). In case (ii), the particle contracts according to Lemma 8.
In case (iii) A(C) forms a ring of trees, which can only happen if all boundary particles completely occupy a single layer, so there is an active particle
that occupies the node adjacent to the marker edge. Since it is contracted
by assumption, it retires upon activation. Therefore, in all three cases the
system eventually makes progress.
3.3. Termination
Finally, we show that the algorithm eventually terminates in a legal configuration, i.e., a configuration in which the coating problem is solved. For
the termination we need the following two lemmas.
Lemma 13. The number of times an idle particle is transformed into an
active one and an active particle is transformed into a retired one is bounded
by O(n).
Proof. From our algorithm it immediately follows that every idle particle can
only be transformed once into an active particle, and every active particle
can only be transformed once into a retired particle. Moreover, a non-idle
particle can never become idle again, and a retired particle can never become
non-retired again, which proves the lemma.
Lemma 14. The overall number of expansions, handovers, and contractions
is bounded by O(n2 ).
Proof. We will need the following fact, which immediately follows from our
algorithm.
25

Fact 1. Only a super-root of A(C) can expand to a non-occupied node, and
every such expansion triggers a sequence of handovers, followed by a contraction, in which every particle participates at most twice.
Consider any particle p. Note that only an active particle performs a
movement. Let C be the first configuration in which p becomes active. If
it is a non-boundary particle (i.e., a follower), then consider the directed
path in A(C) from the head of p to the super-root r of its tree or the first
particle r belonging to the ring in the ring of trees. Such a path must exist
due to Lemma 2. Let P = (v0 , v1 , . . . , vm ) be a node sequence covered by
this path where v0 is the head of p in C and vm is the first node along that
path with the object or a retired particle in its neighborhood. Note that by
Lemma 2 such a node sequence is well-defined since vm must at latest be a
node occupied by r. According to Algorithm 1, p attempts to follow P by
sequentially expanding into the nodes v0 , v1 , . . . , vm . At latest, p will become
a boundary particle once it reaches vm . Up to this point, p has traveled
along a path of length at most 2n, and therefore, the number of movements
p executes as a follower is O(n).
Now suppose p is a boundary particle. Let C be the configuration in
which p becomes a boundary particle and let ` = p.layer. Suppose that
` = 1. From our algorithm we know that at most n complaint flags are
generated by the particles, and therefore by Lemma 7, there are at most
n expansions in level 1 (the rest are handovers or contractions). Hence, it
follows from Fact 1 that p can only move O(n) times as a boundary particle.
Next consider the case that ` > 1. Here we will need the following wellknown fact.
Fact 2. Let Bi be the length of layer i. For every i and every valid instance
(P, O) allowing O to be coated by i layers it holds that Bi = B0 + 6i.
If ` = 2, there must be a retired particle in layer 1, and since the leader is
the first retired particle, Lemmas 9 and 10 imply that level `−1 is completely
filled with contracted particles. So p can only move along nodes of layer `.
Since B`−1 ≤ n, it follows from Fact 2 that B` ≤ n + 6. As long as not all
particles in level ` − 1 are retired, p cannot move beyond the marker node in
level `. So p either becomes retired before reaching the marker node, or if it
reaches the marker node, it has to wait there till all particles in level ` − 1 are
retired, which causes the retirement of p. Therefore, p moves along at most
n + 6 nodes. If ` > 2, we know from Lemma 1 that level ` − 2 is completely

26

filled with contracted particles. Since B`−2 ≤ n and B` = B`−2 +12, it follows
that B` ≤ n + 12. Hence, p will move along at most n + 12 nodes in level `
before becoming retired or moving to level ` − 1, and p will move along at
most n + 6 further nodes in level ` − 1 before retiring.
Thus, in any case, p performs at most O(n) movements as a boundary
particle. Therefore, the number of movements any particle in the system
performs is O(n), which concludes the lemma.
Lemmas 13 and 14 imply that the system can only make progress O(n2 )
many times. Hence, eventually our system reaches a configuration in which
it no longer makes progress, so the system terminates. It remains to show
that when the algorithm terminates, it is in a legal configuration, i.e., the
algorithm solves the coating problem.
Theorem 1. Our coating algorithm terminates in a legal configuration.
Proof. From the conditions of Lemmas 11 and 12 we know that the following
facts must both be true when the algorithm terminates:
1. At least one particle is retired or there is neither a complaint flag nor
an expanded particle in the system (Lemma 11).
2. Either all particles are retired or all particles are active (Lemma 12).
First suppose that all particle are retired. Then it follows from Lemma 1
that the configuration is legal. Next, suppose that all particles are active
and neither a complaint flag nor an expanded particle is left in the system.
Then Lemma 4 implies that there cannot be any non-boundary any more,
so all active particles must be boundary particles. If there is at least one
boundary particle in layer ` > 1, then there must be at least one retired
particle, contradicting our assumption. So all boundary particles must be
in layer 1, and since there are no more complaint flags and all boundary
particles are contracted, also in this case our algorithm has reached a legal
configuration, which proves our theorem.
Recall that the work performed by an algorithm is defined as the number
of movements (expansions, handovers, and contractions) of the particles till
it terminates. Lemma 14 implies that the work performed by our algorithm
is O(n2 ). Interestingly, this is also the best bound one can achieve in the
worst-case for the coating problem.

27

Figure 6: A worst-case configuration concerning work. The object is solid black and the
non-object particles are black dots . Here, all n particles lie on a straight line.

Lemma 15. The worst-case work required by any algorithm to solve the
Universal Object Coating problem is Ω(n2 ).
Proof. Consider the configuration depicted in Figure 6. A particle with distance i ≥ 1 to the object needs at least 2(i − 1 − i−1
) movements to
B
becomePcontracted on its final P
layer. Therefore, any algorithm requires at
i−1
i
2
least 2 n−1
) ≥ n−1
i=1 (i − 1 −
i=1 (i − 1 − ( B )) = Ω(n ) work assuming
B
B ≥ 2.
Hence, we get:
Theorem 2. Our algorithm requires worst-case optimal work Θ(n2 ).
4. Applications
In this section, we present other coating scenarios and applications of
our universal coating algorithm. Our algorithm can be easily extended to
also handle the case when one would like to cover only a certain portion of
the object surface. More concretely, assume that one would like to cover
the portion of the object surface delimited by two endpoint nodes. Basically
in that case, the algorithm can be modified slightly so that the particles
that eventually reach one of the endpoints of the surface segment retire and
become endpoint markers. The position of endpoint marker particles will be
propagated to higher layers, as necessary, such that the delimited portion of
the object is evenly coated.
Once the first layer is formed and a leader is elected (implying that
B ≤ n), one can trivially determine (i) whether the number of particles
in the system is greater than or equal to the size of the object boundary,
or (ii) whether the object O is convex; one could also potentially address
28

other applications that involve aggregating some (constant-size) collective
data over the boundary of the object O. Once all particles in layer 1 retire, a
leader will emerge and that leader can initiate the respective application. For
the first application, all particles may initially assume that B > n. Once a
leader is elected, it informs all other particles that B ≤ n. For the convexity
testing, the leader particle can generate a token that traverses the boundary
in CW direction: If the token ever makes a left turn (i.e., it traverses two
consecutive edges on the boundary at an outer angle of less than 180◦ ), then
the object is not convex; otherwise the object is convex.
5. Conclusion
This paper presented a universal coating algorithm for programmable
matter using worst-case optimal work. It would be interesting to also bound
the parallel runtime of our algorithm in terms of number of asynchronous
rounds, and to investigate its competitiveness — i.e., how does its work or
runtime compare to the best possible work or runtime for any given instance.
Moreover, it would be interesting to implement the algorithm and evaluate
its performance either via simulations or hopefully at some point even via
experiments with real programmable matter.
Acknowledgements
We would like to thank Joshua Daymude and Alexandra M. Porter for
fruitful discussions on this topic and for helping us review the manuscript.
References
[1] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. A. Bazzi, A. W. Richa,
C. Scheideler, Leader election and shape formation with self-organizing
programmable matter, in: DNA Computing and Molecular Programming - 21st International Conference, DNA 21, Boston and Cambridge,
MA, USA, August 17-21, 2015. Proceedings, 2015, pp. 117–132.
[2] Z. Derakhshandeh, S. Dolev, R. Gmyr, A. W. Richa, C. Scheideler,
T. Strothmann, Brief announcement: amoebot - a new model for programmable matter, in: 26th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA ’14, Prague, Czech Republic - June 23
- 25, 2014, 2014, pp. 220–222.
29

[3] D. Doty, Theory of algorithmic self-assembly, Communications of the
ACM 55 (12) (2012) 78–88.
[4] M. J. Patitz, An introduction to tile-based self-assembly and a survey
of recent results, Natural Computing 13 (2) (2014) 195–224.
[5] D. Woods, Intrinsic universality and the computational power of selfassembly, in: Proceedings Machines, Computations and Universality
2013, MCU 2013, Zürich, Switzerland, September 9-11, 2013., 2013, pp.
16–22. doi:10.4204/EPTCS.128.5.
[6] D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, R. Peralta, Computation in networks of passively mobile finite-state sensors, Distributed
Computing 18 (4) (2006) 235–253.
[7] V. Bonifaci, K. Mehlhorn, G. Varma, Physarum can compute shortest
paths, in: Proceedings of SODA ’12, 2012, pp. 233–240.
[8] K. Li, K. Thomas, C. Torres, L. Rossi, C.-C. Shen, Slime mold inspired
path formation protocol for wireless sensor networks, in: Proceedings of
ANTS ’10, 2010, pp. 299–311.
[9] S. Wilson, T. P. Pavlic, G. P. Kumar, A. Buffin, S. C. Pratt, S. Berman,
Design of ant-inspired stochastic control policies for collective transport
by robotic swarms, Swarm Intelligence 8 (4) (2014) 303–327.
[10] M. Brambilla, E. Ferrante, M. Birattari, M. Dorigo, Swarm robotics: a
review from the swarm engineering perspective, Swarm Intelligence 7 (1)
(2013) 1–41. doi:10.1007/s11721-012-0075-2.
[11] I. Navarro, F. Matı́a, An introduction to swarm robotics, in: ISRN
Robotics, Hindawi Publishing Corporation, 2012, p. 10.
[12] G. P. Kumar, S. Berman, Statistical analysis of stochastic multi-robot
boundary coverage, in: 2014 IEEE International Conference on Robotics
and Automation, ICRA 2014, Hong Kong, China, May 31 - June 7, 2014,
2014, pp. 74–81. doi:10.1109/ICRA.2014.6906592.
[13] T. P. Pavlic, S. Wilson, G. P. Kumar, S. Berman, An enzyme-inspired
approach to stochastic allocation of robotic swarms around boundaries,
in: 16th International Symposium on Robotics Research (ISRR 2013),
Singapore, Dec, 2013, pp. 16–19.
30

[14] L. Blázovics, K. Csorba, B. Forstner, H. Charaf, Target tracking and
surrounding with swarm robots, in: IEEE 19th International Conference and Workshops on Engineering of Computer-Based Systems,
ECBS 2012, Novi Sad, Serbia, April 11-13, 2012, 2012, pp. 135–141.
doi:10.1109/ECBS.2012.41.
[15] L. Blázovics, T. Lukovszki, B. Forstner, Target surrounding solution
for swarm robots, in: Information and Communication Technologies 18th EUNICE/ IFIP WG 6.2, 6.6 International Conference, EUNICE
2012, Budapest, Hungary, August 29-31, 2012. Proceedings, 2012, pp.
251–262.
[16] S. Kernbach (Ed.), Handbook of Collective Robotics – Fundamentals
and Challanges, Pan Stanford Publishing, 2012.
[17] J. McLurkin, Analysis and implementation of distributed algorithms for
multi-robot systems, Ph.D. thesis, Massachusetts Institute of Technology (2008).
[18] D. Arbuckle, A. Requicha, Self-assembly and self-repair of arbitrary
shapes by a swarm of reactive robots: algorithms and simulations, Autonomous Robots 28 (2) (2010) 197–211.
[19] R. Cohen, D. Peleg, Local spreading algorithms for autonomous robot
systems, Theoretical Computer Science 399 (1-2) (2008) 71–82.
[20] S. Das, P. Flocchini, N. Santoro, M. Yamashita, On the computational
power of oblivious robots: forming a series of geometric patterns, in:
Proceedings of the 29th Annual ACM Symposium on Principles of Distributed Computing, PODC 2010, Zurich, Switzerland, July 25-28, 2010,
2010, pp. 267–276.
[21] X. Defago, S. Souissi, Non-uniform circle formation algorithm for oblivious mobile robots with convergence toward uniformity, Theoretical
Computer Science 396 (1-3) (2008) 97–112.
[22] T.-R. Hsiang, E. Arkin, M. Bender, S. Fekete, J. Mitchell, Algorithms
for rapidly dispersing robot swarms in unknown environments, in: Proceedings of the 5th Workshop on Algorithmic Foundations of Robotics
(WAFR), 2002, pp. 77–94.
31

[23] O. Michail, P. G. Spirakis, Simple and efficient local codes for distributed
stable network construction, in: ACM Symposium on Principles of Distributed Computing, PODC ’14, Paris, France, July 15-18, 2014, 2014,
pp. 76–85.
[24] http://sops.cs.upb.de.
[25] Z. Derakhshandeh, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann, An algorithmic framework for shape formation problems in
self-organizing particle systems, in: To appear in 2nd ACM International Conference on Nanoscale Computing and Communication (ACM
NANOCOM 2015), 2015.

32

Accessing

Nearby

Copies

in a Distributed
C. Greg Plaxton

1

of Replicated

Objects

Environment

Rajrnohan Rajararnan 1

Andr&a W. Richa 2

Abstract

1

Consider a set of shared objects in a distributed
network, where several copies of each object may exist at
any given time. To ensure both fast access to the objects
as well as efficient utilization of network resources, it is
desirable that each access request be satisfied by a copy
“close” to the requesting node. Unfortunately,
it is not
clear how to efficiently achieve this goal in a dynamic,
distributed
environment
in which large numbers of objects are continuously being created, replicated, and destroyed,

The advent of high-speed networks has made it feasible
for a large number of geographically
dispersed computers to cooperate and share objects (e.g, files, words of
memory).
This has resulted in the implementation
of
large distributed
databases like the World Wide Web
on wide-area networks. The large size of the databases
and the rapidly growing demands of the users has in turn
overloaded the underlying network resources. Hence, an
important
goal is to make efficient use of network resources when providing access to shared objects.

In this

paper,

we design

a simple

randomized

Introduction

As one might expect, the task of designing efficient
algorithms for supporting
access to shared objects over
wide-area networks is extremely challenging, both from
a practical as well as a theoretical
perspective.
With
respect to any interesting measure of performance
(e.g.,
latency, throughput),
the optimal bound achievable by
a given network is a complex function of many parameters, including edge delays, edge capacities, buffer space,
communication
overhead, pat terns of user communication, and so on. Ideally, we would like to take all of
these factors into account when optimizing performance
with respect to a given measure. However, such a task

algo-

for accessing shared objects that tends to satisfy
each access request with a nearby copy. The algorithm is
based on a novel mechanism to maintain and distribute
information about object locations, and requires only a
smaIl amount of additional memory at each node. We
analyze our access scheme for a class of cost functions
that captures the hierarchical nature of wide-area networks. We show that under the particular cost model
considered: (i) the expected cost of an individual access
is asymptotically
optimal, and (ii) if objects are sufficiently large, the memory used for objects dominates
the additional memory used by our algorithm with high
probability.
We also address dynamic changes in both
the network as well as the set of object copies.
rithm

may not be feasible in general because

the many net-

work parameters
interact in a fairly complex manner.
For this reason, we adopt a simplified model in which
the combined effect of the detailed network parameter
values is assumed to be captured by a single function
that specifies the cost of communicating
a fixed-length
message bet ween any given pair of nodes.
We anticipate that analyzing algorithms
under this model will
significantly aid in the design of practical algorithms for
modern distributed
networks.

1Department of Computer Science, University of Texas at
Austin, Austin, TX 78712. Supported by the National Science
Foundation under Grant No. CCR–9504145.
Email: {phxton,
rraj}@cs .utexas. edu.
2School of Computer Science, Carnegie Mellon University,
Pittsburgh, PA 15213. Supported by Bruce Maggs’s National
Young Investigator Award under Grant No. CCR-94-57766.
Email: aricha@cs, mu. edu.

Accessing
shared
objects.
Consider a set A of
m objects being shared by a network G, where several
copies of each object may exist. In this paper, we conof r-eading objects in A. Motivated by the need for efficient network utilization,
we
seek algorithms that minimize the cost of the read operation. We do not address the write operation,
which
involves the additional consideration of maintaining con-

sider the basic problem

and/or fee
.$PA/l 97 Newport, Rhode lslm)d ( lS}\
COpyrlghL 1997 AChf ()-897’) I-89(1 -8’97/0(>,.S~.~U

pcm~lssioll

311

sistency among the various object copies. The problem
of consistency, although an important
one, is separate
from our main concern, namely, that of studying locality.
Our results for the read apply for the write in scenarios
where consistency either is not required or is enforced
by an independent
mechanism.

Saive

for

control

come

Our

information

this

problem

embedding

for read,
efficiency

of the above

require

bed the different

We differentiate
between shared and unshared copies
of objects.
A copy is shared if any node can read this
copy; it is unshared if only the node which holds the
copy may read it. We say that a node u znserts (resp.,
deietes) a copy of object A (that u holds) if u declares
the copy shared (resp., unshared).

significant

trees

associated

to account
in memory

and delete

nodes.

a novel
with

us to define

for a class of cost
wide-area

approach
overhead

at individual

by designing

enables

insert,

for modeling

operations,
functions

We

method
different

simple

objects.

algorithms

and to prove
that

overto em-

their

is appropriate

networks.

The cost model.
As indicated above, we assume
that a given function determines the cost of communication between each pair of nodes in the network. Our
analysis is geared towards a restrictive class of cost functions which we believe to be of practical interest.
The
precise set of assumptions that we make with respect to
the cost function is stated in Section 2. Our primary
assumption is that for all nodes z and costs r; the ratio
of the number of nodes within cost 2r of node x to the
number of nodes within cost r of node x is bounded from
above and below by constants greater than 1 (unless the
entire network is within cost 2r of node x, in which case
the ratio may be as low as 1).

We refer to the set of algorithms for read, insert,
and delete operations
as an access scheme.
Any access scheme that efficiently supports these operations
incurs an overhead in memory. It is desirable that this
overhead be small, not only because of space considerations, but also because low overhead usually implies
fast adaptability to changes in the network topology
in the set of object copies.

extensions

for all objects

or

The main difficulty in designing an access scheme that
is efficient with respect to both time and space is the
competing considerations
of these measures.
For example, consider an access scheme in which each node
stores the location of each copy of each object in the network. This allows very fast read operations since a node
can easily determine the location of the closest copy of
any desired object.
However, such an access scheme
is impractical because: (i) it incurs a prohibitively large
memory overhead, and (ii) every node of the network has
to be informed whenever a copy of an object is inserted
or deleted.
At the other extreme, one might consider
an access scheme using no additional memory. In this
case insert and delete operations are fast, but read operations are costly since it may be necessary to search the
entire network in order to locate a copy of some desired
object.

There are several important observations we can make
concerning this primary assumption
on the cost function.
First, a number of commonly
studied fixedconnection network families lead naturally to cost functions satisfying this assumption.
For example, fixeddimension meshes satisfy this assumption
if the cost of
communication
between two nodes is defined as the minimum number of hops between them. As another example, fat-tree topologies can be made to satisfy our
assumption
if the cost of communication
between two
nodes is determined by the total cost of a shortest path
between them, where the cost assigned to individual
edges grows at an appropriate geometric rate as we move
higher in the tree. The latter example is of particular
interest here, because of all the most commonly studied
fixed-connection
network families, the fat-tree may provide the most plausible approximation
to the structure
of current wide-area networks.

Our access scheme. We design a simple randomized access scheme that exploits locality and distributes
control information to achieve low overhead in memory,
The central part of our access scheme is a mechanism
to maintain and locate the addresses of copies of objects. For a single object, say A, we can provide such
a mechanism by the following approach. We embed an
n-node “virtual” height-balanced
tree T one-to-one into
the network. Each node u of the network maintains information associated with the copies of A residing in
the set of nodes that form the subtree of T rooted at
u. Given the embedding of T, the read operation may
be easily defined as follows. When a node u attempts
to read A, u first checks its local memory for a copy of
A or information
about copies of A in the subtree of T
rooted at u. If u is unable to locate any copy on the
basis of local information,
it forwards its request to its
parent in T,

Even so, it is probably inappropriate
to attempt to
model the Internet, say, with any kind of uniform topology, including the fat-tree. Note that our assumption on
the cost function is purely “local” in nature, and allows
for the possibility of a network with a highly irregular
global structure.
This may be the most important characteristic of our cost model,

Performance bounds. We show that our access
scheme achieves optimality or near-optimality
in terms
of several important
complexity measures for the restricted class of cost functions discussed above. In particular, our scheme achieves the following bounds:
. The expected cost for any read request
ically optimal.

312

is asymptot-

If the number of objects that can be stored at each
node is q, then the additional
memory
required
is
O(qlog2

n) words

bit string,
i.e., Q(log2

Thus,

whpl,

n) words,

nates the additional
The

number

the

addition

pected

where

if the objects

of nodes

a word is an O(log
are sufficiently

the memory

for objects

In recent work, access schemes for certain Internet applications have been described in [7, 8, 17]. Some of the
ideas in our scheme are similar to those in [17]; however, the two schemes differ considerably in the details.
Moreover, the schemes of [7] and [17] have not been analyzed. As in our study, the results of [8] concerning
locality assume a restricted cost model. However, their
cost model, which is based on the ultrametric,
is different from ours. Also, their algorithms
are primarily
designed for problems associated with “hot spots” (i.e.,
popular objects).

n)-

large,
domi-

memory.
that

need to be updated

upon

of a node is O(log n) exand O(log2 n) whp.
or

removal

The expected cost of an insert (resp., delete) operation at node u is O(C) (resp., O(Clog n)), where C
is the maximum cost of communicating
a single word
message between any two nodes.

A closely related problem is that of designing a dynamic routing scheme for networks [2, 5]. Such a scheme
involves maintaining routing tables at different nodes of
the network in much the same way as our additional
memory. However, in routing schemes the size of additional memory is a function of network size, i.e., n, while
in our problem the overhead is primarily a function of
m. Straightforward
generalizations
of routing schemes
result in access schemes that require an additional memory of m words at each node.

An obvious shortcoming
of our analysis is that it only
applies to the restricted class of cost functions discussed
above. While we do not expect that all existing networks
fall precisely within this restricted class, we stress that:
(i) our access scheme is well-defined, and functions correctly, for arbitrary networks, and (ii) we expect that our
access scheme would have good practical performance on
any existing network. (Although we have not attempted
to formalize any results along these lines, it seems clear
that our performance bounds would only degrade significantly in the presence of a large number of non-trivial
violations of our cost function assumptions. )

The remainder of this paper is organized as follows.
Section 2 defines the model of computation.
Section 3
formally describes our access scheme.
Section 4 contains a formal statement of the main results. Section 5
analyzes the algorithm and establishes the main results.
Section 6 discusses directions for future research.

Related work. The basic problem of sharing memory in distributed
systems has been studied extensively in different forms. Most of the earlier work in
this area, e.g., emulations
of PRAM on completelyconnected distributed-memory
machines (e.g., [9, 16] )
or bounded-degree
networks (e.g., [14]), and algorithms
for providing concurrent
access to a set of shared objects [12], assume that each of the nodes of the network
has knowledge of a hash function that indicates the location of any copy of any object.

2

Model

of Computation

We consider a set V of n nodes, each with its own local
memory, sharing a set A of m = poly(n) objects.
We
define our model of computation
by characterizing
the
following aspects of the problem: (i) objects, (ii) communication,
(iii) local memory, (iv) local computation,
and (v) complexity measures,
Objects.
Each object A has a unique (log m)-bit
identification.
For i in [log ~], we denote the ith bit of
the identification of A by A’. (For any positive integer

The basic problem of locating an object arises in every
distributed
system [10], and was formalized by Mullender and Vit6nyi [11] as an instance of the distributed
matchmaking
problem.
Awerbuch and Peleg [3], and
subsequently
Bart al et al. [4] and Awerbuch et al. [I],
give near-optimal
solutions in terms of cost to a related problem by defining sparse-neighborhood
covers
of graphs.
Their studies do not address the overhead
due to control information
and hence, natural extensions of their results to our problem may require an additional memory of m words at some node, However,
we note that their schemes are designed for arbitrary
cost functions, whereas we have focused on optimizing
performance for a restricted class of cost functions,

x, we use [z] to denote the set {O,
.,z – l}. ) Each
object A consists of /(A) words, where a word is an

O(log n)-bit

string.

Communication.
Nodes communicate
with one another by means of messages; each message consists of at
least one word. We assume that the underlying network
supports reliable communication.
We define the cost of communication
by a function
c : V2 s R. For any two nodes u and v in V, C(U, v)
is the cost of transmitting
a single-word message from
u to v. We assume that c is symmetric and satisfies the
triangle inequality.
We also assume for simplicity that
for u, v, and w in V, C(U, v) equals C(U, w) iff v equals
w.

1 We use the abbreviation
“whp” throughout the paper to
mean “with high probability”
or, more precisely, “with probability 1 – n–c, where n is the number of nodes in the network and
c is a constant that can be set arbitrarily large by appropriately
adjusting other constants defined within the relevant context. ”

The cost of transmitting
a message of length t from
node u to node v is given by f(l)c(u, v), where ~ : N ~

313

R+ is any non-decreasing

node exists, or (ii) y is the node with largest B(y) among
all nodes z such that z[i] matches j in the largest number
of rightmost bits. Let d be a fixed positive integer, to

function such that j(1) equals

1.
Given any u in V and any real r, let M(u1 r) denote
the set {v E V : C(U, v) S“ r}. We refer to M(u, r) as the
ball of radius r around u. We assume that there exist
real constants 8 > 8 and A such that for any node u in
V and any real r z 1, we have:
min{JIM(u,

r)l, n} < /M(u,2r)l

< AIM(u,

r)l

be specified

(1)

Local
Memory.
We partition the local memory of
each node u into two parts.
The first part, the main
memory, stores objects. The second part, the auxiliary
memory, is for storing possible control information.
Local

Computation.

with local computation.
(Although the model allows an
arbitrary amount of local computation
at zero cost, our
algorithm does not perform any particularly
complex
local operations.)
We evaluate any solution
complexity measures. The

first measure is the cost of reading an object. The second measure is the size of the auxiliary memory at any

node. The remaining twc} measures concern the dynamic
nature of the problem, where we address the complexity
of inserting or deleting a copy of an object and adding
or removing a network node. The third measure is the
cost of inserting or deleting a copy of an object. The
fourth measure is adaptobiiity,
which is defined as the
number of nodes whose auxiliary memory is updated
upon the addition or removal of a node. (Our notion of
adaptability
is analogous to that of [5]. )

3

The

Access

Let

y be the primary

(i, j)-neighbor

Each node x also maintains
a
Pointer
list.
pointer list Ptr(z) with pointers to copies of some objects in the network. Formally, Ptr(z) is a set of triples
(A, y, k), where A is in d, y is a node that holds a copy
of A, and k is an upper bound on the cost C(Z, y). We
maintain the invariant that there is at most one triple
associated with any object in FVr(z). The pointer list of
z may only be updated as a result of insert and delete
operations.
All the pointer lists can be initialized by inserting each shared copy in the network at the start of
the computation.
We do not address the cost of initializing the auxiliary memories of the nodes.

There is no cost associated

Complexity
measures.
on the basis of four different

later.

y[i] = j, then let Wi,j denote the set of nodes w
in V \ {y} such that w[k] = x[k], for k in [i], w[i] = j,
and C(Z, w) is at most d C(Z, y). Otherwise,
let Wi,j be
the empty set. The set of secondary (i, j) -neighbors of x
is the subset U of min{d, IWi,j I} nodes u with minimum
c(z, U) in Wi,j; that is, c(z, U) is at most c(z, w), for all
w in Wi,j, and for all IAin U. A node w is a reverse
(i, j)-neighbor
of z iff z is a primary (i, j)-neighbor
of
w.
of z. If

Let r be the node with highest ~(r) such that there
satisfying:
(i) r[k] = A[k] for all
k in [i], (ii) r[i] matches A[i] in the largest number of
rightmost bits, and (iii) if i < (log n)/b — 1, there is no
node y with y[k] = A[k] for all k in [i + 1]. We call r
the root node for object A. The uniqueness of the root
node for each A in A is crucial to guarantee the success
of every read operation.

exists i in [(log n)/b]

Scheme

In this section, we present our access scheme for shared
objects. We assume that n is a power of 2b, where b is a
fixed positive integer to be specified later. For each node
x in V, we assign a label independently and uniformly
at random from [n]. For i in Dog n], let xi denote the
ith bit of the label of z, Note that the label of a node
x is independent
of the (log n)-bit unique identification
of the node. For all z in V (resp., A in d ), we define
z[i] = X(i+l)b-l
. .xib
for i in [(log n)/b]. We also assign a total order to the
nodes in V, given by the bijection ,B : V + [n]. We
partition the auxiliary memory of each node into two
parts, namely the neighbor table and the pointer list of
the node,

(reSp.,
~[~]= Afi+l)b-l
.. Aib),

In this section and throughout
the paper, we use the
notation (a)k to denote the sequence (of length k + 1)
aO, al,...,
ak (of length k + 1). When clear from the
context, k will be omitted.
In particular,
a primary
neighbor sequence for A is a maximal sequence (u)k such
that U. is in V, uk is the root node for A, and ui+l is the
primary (i, A[i])-neighbor of ~i, for all i. It is worth noting that the sequence (u) is such that the label of node
ui satisfies (ui[i– 1], ., .,ui[O]) = (A[i– 1],. ., AIO]), for
all i. We now give an overview of the read, insert, and
delete operations,
Read.
Consider a node z attempting
to read an object A. The read operation proceeds by successively forwarding the read request for object A originating at node
z along the primary neighbor sequence (x) for A with
X. = z. When forwarding the read request, node Zi _ 1
also informs ~i of the current best upper bound k on the
cost of sending a copy of A to z. On receiving the read
request with associated upper bound k, node xi proceeds
as follows. If Zi is the root node for A, then Zi requests
that the copy of A associated with k be sent to z, Other-

For each
node
x,
the
Neighbor
table.
neighbor table of z consists of (log n) /b levels. The ith
level of the table, i in [(log n)/b], consists of primary, secondary, and reverse (i, j) -neighbors, for all j in [2b]. The
primary
(i, j)-neighbor
y of z is such that y[k] = x[k]
for all k in [i], and either: (i) i < (log rz)/b – 1 and y is
the node of minimum c(2, y) such that y[i] = j, if such a

314

A is generated

wise, xi communicates
with its primary and secondary
(i, A[i])-neighbors
to check whether the pointer list of
any of these neighbors has an entry (A, z, kl) such that
/cl is at most k. Then, xi updates k to be minimum of
k and the smallest value of kl thus obtained (if any),
If k is within a constant factor, of the cost of following
(z) UP to
O(xjD~ C(Zj, .Ej+I)), then Zi
requests that the copy of A associated with the upper
bound k be sent to x. otherwise,
xi forwards the read
request to z~+l.

(1) The insert and delete procedures maintain the following invariants.
For any A in A and any y in
V, there is at most one entry associated with A in
the pointer list of y. If y holds a shared copy of A
and (y) is the primary neighbor sequence for A with
YO = y, then: (i) there is an entry associated with
A in the pointer list of every node in (y), and (ii)
the nodes that have a pointer list entry associated
with the copy of A at y form a prefix subsequence
of (y). The preceding claims follow directly from
the insert and delete procedures as described.

z;,
that
k,kk

Insert.

An insert

request for object

A generated by

node y updates the pointer lists of some nodes that form
a prefix subsequence of the primary neighbor sequence
(y) for A with y. = y. When such an update arrives
at a node yi by means of an insert message, yi updates
its pointer list if the upper bound ~~~~ c(yj, Yj+l ) on

A from y, is smaller than the
with A in this list. In
other words, yi updates .Ptr(yi ) if (A, ., .) is not in this
list, or if (A, ~,k) is in Ptr(yi) and k is greater than
the cost of getting object

current

upper

bound

associated

~~~~ C(Y~,Y~+l) Node y; forwards the insert
to node yi+l only if PtrI yi ) is updated.

at y.

The correctness of our access scheme follows from the
two points below:

request

Delete.
A delete request for object A generated by
node y eventually removes all triples of the form (A, y, ~)
from the pointer lists Pt~(yi ), where (y) is the primary
neighbor sequence for A with y. = y, making the copy of
A at y unavailable to other nodes in the network, Upon
receiving such a request by means of a delete message,
node yi checks whether the entry associated with A in its
pointer list is of the form (A, y, .). In case it is not, the
delete procedure is completed and we need to proceed no
further in updating the pointer lists in (y). Otherwise,
Yi deletes this entry from its pointer list, and checks
for entries associated with A in the pointer lists of its
reverse (i —1, A[i — 1])-neighbors.
If an entry is found, yi
updates Ptr(yi ) by adding the entry (A, W, k + c(w, yi)),
where w is the reverse (i – 1, A[i – 1])-neighbor of yi
with minimum upper bound k associated with A in its
pointer list. A delete message is then forwarded to yi+l.
The read, insert, and delete procedures for an object
A are formally described in Figure 1, The messages
and requests in the figure are all with respect to object
A. A read request is generated by node x when x (= XO)
sends a message Read(x, m, .) to itself, if z does not hold
a copy of A, A read message Read (x, k, y) indicates a
read request for object .1 generated at node x, and that
the current best upper bound on the cost of reading
Ais k and such a copy resides at y. An insert (resp.,
delete) request is generated
when node y (= y.) sends
a message
Insert (y, O) ( resp., Delete(y) ) to itself, An
insert message Insert (y, k) indicates to its recipient node
z that the best known upper bound on the cost incurred
by z to read the copy of A located at y is k. We assume
that y holds a copy of A and that this copy is unshared
(resp., shared) when an Insert (resp., delete) request for

315

(2) Every read request for any object A by any node
That is, it locates and brings to
z is successful.
x a shared copy of A, if such a copy is currently
available. The read operation proceeds by following
the primary neighbor sequence (z) for A with Z. =
z, until either a copy of A is located or the root for
A is reached.
By point ( 1), there exists a shared
copy of A in the network if and only if the root for
A has a pointer to it.

4

Results

In this section, we formally state the main results of our
access scheme. In Theorems 1, 2, 3, and 4, we prove
bounds on the cost of a read, the cost of an insert or
delete, the size of auxiliary memory, and the adaptability of our access scheme. Let C denote max{c(u, v) :
U)vc v}.

Theorem 1 Let x be any node in V and let A be any
object In A. If y is the nearest node to x that holds a
shared copy of A, then the expected cost of a read operation is O(f(/(A))c(x,
y)).
When a node x tries to read an object A which has
currently no shared copy in the network, then the expected cost of the associated operation is 0(6’).
2 The expected cost of an insert operation
and that of a delete operation is O(Clog n).

Theorem

O(C),

is

3 Let q be the number of objects that can be
stored in the main memory of each node. The size of
the auxiliary memory at each node is O(q logz n) words
whp.

Theorem

4 The adaptability
expected and 0(log2 n) whp.

Theorem

of our scheme

is O(log n)

Action

of xi on receiwng a message Read(x,

k, y):

Actton

If i >0 and ~i[i-l] # .~[i-1], or i = (logn)/b – 1 (that
is, xi is the root for A) then:
Q Node ~i sends a message Satisfy(x) to node v such
that (A, v, .) is in l’tr(~i), requesting it to send a copy
of A to z, If Ptr(Xi) has no such entry, then there are
no shared copies of A.

If (A, y, ) is in Ptfiyi),
then:
. Let U be the set of reverse (i–l, A[i–1])-neighbors
of yi. Node yi removes (A, y, .) from Ptflyi
), and
requests a copy of A from each u in U.
Each u in U responds to the request message from

●

o Let U be the set of secondary (i, A[i])-neighbors of xi.
Node xi requests a copy of A with associated upper
bound at most k from each node in U U {~i+l }.
D Each node u in U U {~i+l } responds to the request
message received from ~i as follows: if there exists an
entry (A, v, go) in Ptr(u) and if q: = q. + c(~i, U) +

●

Satisfy(x) to node y, requesting y to send a copy of A
Otherwise, xi forwards a message Read(z, k, y)

to z.

tO X;+l .

Figure

5

1: Actions on receiving messages

Analysis

●

tO y~+l .

Action

of yi on receiving

a message

If (A,.,.) is not in Ptflyi),
and k’ > k, then:

Insert (y, k):

or (A, ,k’)

is in Ptr(yi)

. Node yi accordingly creates or replaces the entry
associated with A in Ptflyi ) by inserting (A, y, k)
into this list,
If yi [i–l]
= A[i–1] then yi sends a message
Insert(y, k + c(yi, yi+l)) to Yi+l

●

Read, Insert, and Delete for object A.

5.1 Let u, v, and w be in V and let k. and
kl be positive integers. if v is in N(M, ko) and w is in
N(u, kl), then w is in N(u, Ako+ A2kl).
■

Lemma

Given any subset S of V and some node u in S, let
q(u, S) (resp., T-(u, S)) denote the largest (resp., smallest ) integer k such that N(u, k) is a subset (resp., superset) of S. Let Q(u, S) and R(u, S) denote N(u, q(u, S))
and N(u, r(u, S)), respectively.

Several constants appear in the model, the algorithms,
and the analysis: d and A appear in the model, b and
d appear in the algorithms, y and c appear in the analysis.
We set b, d, -y, and E such that:
d, y << 2b,
E < 1/(10 . 2b10gJ2), and b sufficiently large to obtain
the desired results. We refer the reader to the full version of the paper [13] for the precise relationships among
the constants.
Properties

into Ptr(yi), where w is the node in U! such that
kW ~ kU, for all u in U’.
If yi [i–l] = A[i–1] then yi sends a message Delete(y)

set {k E Z : a ~ k ~ b}. ) We refer to N(u, k) as the ball
of size k around u. For convenience, if k is greater than
n, we let N(u, k) be V.

In this section, we analyze the access scheme described
in Section 3, and establish the main results described in
Section 4. Section 5.1 presents some useful properties
of balls. Section 5.2 presents properties of primary and
secondary neighbors. Section Ei.3 presents the proofs of
Theorems
1 and 2. Sections 5.4 and 5.5 present the
proofs of Theorems 3 and 4, respectively.
Due to space
constraints,
we omit most of the proofs in this abstract.
We refer the reader to the full version of the paper [13]
for complete proofs of the results stated in Section 4.

5.1

Let U’ be the set of nodes u such that yi receives
a message Success (u, k.) in response to the request
message it sent. If IU’ I >0 then yi inserts (A, w, kW)

k at most k, then u sends a success
~~~~ c(xj, Xj+l)
message Success (v, q:) to Xi.
o Let U’ be the set of nodes u from which xi receives a

response message Success (u, kti ). If U’ is not empty,
then Zi updates (k, g’] to be (k=, z), where z is a node
with minimum ku over all u in U’.
If k = o(~$~~ c(~j, ~j+l)) then ~~ sends a message

Success (v, q. + c(yi, u)) to
is in Ptr(u).

yi by sending a message
vi iff (A, v,q”)

Otherwise:

●

of yi on receiving a message Delete(y):

Lemma
5.2 Let u be in V, let S be a subset of V, and
let k be in [1, n]. Then N(u, k) is a subset (resp., su-

Wrset) of S ifl N(u, k) is a subset
superset of R(v, S)).

of Q(u, S) (resp.,
■

Lemma 5.3 Let u belong to V,

and let k. and kl
denote positive integers such that kl ~ A2 k..
For
any v in N(u, ko), q(v, N(u, kl)) is at least kl/A and
R(v, N(u, kl)) is a subset of N(u, Akl).
■

of Balls

Given any u in V and any integer k in [1, n], let IV(U, k)
denote the unique set of k nodes such that for any u
in N(u, k) and w not in N(u, k), C(U, V) is less than
C(U, w). (For integers a and b, we let [a, b] denote the

We refer to any predicate on V that only depends on
the label of v as a label predicate. Given any node u
in V and a label predicate ‘P on V, let p(u, P) denote

316

the node v such that: (i) T(v) holds, and (ii) for
node w such that P(u) holds, C(U, V) isat most c(u,
(Weletp(u,7)
benullif
such avis
not defined.)
P(u, T) be M(u, c(u, p(u, T))), ifp(u, ~) isnot null,
V, otherwise.

then Pi(~)

root

primary

~ S,

and uniformly

random,

R(u, S) \ {P(u, P)},

P(v)

we have:
A20(v)

(i)

for

and (ii) for each node v in

is false.

The following claim follows from repeated
of Part 1 of Lemma 5.4.

application

proofs of Theorems

is independently

and uniformly

at most e-niz(’+’)b.

■

2 and 3.

5.6 Let u be in V and Jet i be in [(log n)/b].
Then, the number of nodes of which u is an ith level primary neighbor is O(log n) whp. Also, E[aU] = O(log n)
and au is 0(log2 n) whp.
■

Corollary

5.6.1

reverse neighbors
O(log?z).

For any u in V, the total number of
of u is 0(log2 n) whp, and ezpected
■

For any u and v in V and i in [(log
said to be an i-leaf of u if there exists
v = vo)vl,
. . ..vl.
vivi = U, such that for
Vj+l is a primary (j, vj+l ~])-neighbor
of vj.
is used in the proof of Theorem 3.

n)/b], v is
a sequence
all j in [i],
Lemma 5.7

Lemma
5.7 Let u belong to V, and let
[(log n)/b].
Then the number of i-leaves
0(2ib log n) whp.

5.4.1
Let S be an arbitrary subset of V, let i
be in [(log n)/b – 1], and let S’ be a subset of V such that

u

is

Lemma

Corollary
A20(u)

of v

secondary (i, j)-neighbor of v. Lemma 5,6 is used in the
proof of Theorem 4, while Corollary 5.6.1 is used in the

each

is independently

(i, j)-neighbor

For any u in V, let au denote the total number of
triples (i, j, v) such that i belongs to [(log n)/b], j belongs to [2b], v belongs to V, and u is a primary or

2. Given that P(u, P) ~ S, we have: (i) for each node
v in S’ \ Q(u, S), ~:, o(v) is independently
and uniformly random, and-(’ii) for each node v in Q(u, S),
P(v) is false.
that P(u, P)

neighbor.

5.5.1
Let u and v be in V, let i be in
[(log rz)/b], and iet j be in [2b]. If u is a primary (i, j)neighbar of v, then v is in ll(u, 0(2iblog n)) whp.
■

1. Given that P(u, P) ~ S1 we have: (i) for each
node v in S’ \ P(u, ‘P), A20(v) is independently
and uniformly random, and (ii) for each node u in
P(u, P) \ {p(u, ?)}, ?(v) is false.

v in S’ \ R(u, S),

primary

Corollary

and

Let S and S’ be subsets of V and let u
belong to S. Let P be a label predicate on V and for
and
each v in S’, let A>. (v I be chosen independently
uniformly at mndom-.

node

is a nonroot

(i, j)-neighbor of v is at most e-[(klAj-2)/2(’+’)b,
and (ii)
for any i in [(log n)/b], the probability that u is a root

5.4

3. Given

neighbor

5.5 Let u and v be in V, and let k denote
IM(u, C(U, v))[. For any j in [2b], we have: (i) for any
i in [(log n)/b — 1], the probability that u is a primary

Pj (v) are independent random variables. Also, each of
the predicates defined above is a label predicate.)
Lemma

primary

Lemma

For u in V and i in [(log n)/b], let A>:(u) denote the
string of (log n– ib) bits given by u[(log~)/b–
1] . . ~u[i+
l]u[i]. For convenience, we let A>i (u) denote A2i+l (u).
For all i and all u in V, let ‘Pi(u) hold iff u[i] = A[i]. For
all i and all u in V, let ‘P<i(u) denote AjEI;]7j(U).
Let
‘F’~i(~)l P>i(u), and P2i(u) be defined similarly. (We
note that for u and v in V and nonnegative integers i
and j, if (u # v) V ((u = v) A (i # j)),

w of v such that w[i] # j
of v is a primary (i, j)-neighbor
or i = (log n) /b — 1. A primary
neighbor
that is not a

any
w).
Let
and

random for each

i be in
of u is
■

in S’. Given a sequence of nodes UO, u1, . . . . ui such

that for all j in [i], ~j+l = p(uj, P<j) and p(uj,p~j)
S, we have:
1. For each node u in S’ \ Uj~[i]P(u, P~j),
independently
and uniformly mndom.
2. The random
and uniformly
Uj~[i]P(tJj,

5.2

P<j)

variable
mndom
\ {Ui},

~~o(u)

5.3

G

is

A>i (ui) is independently
and for each node u in
P<i(u)
is false;
■

Properties of Neighbors

In this section, we establish certain claims concerning
the different types of neighbors that are defined in Section 3. We differentiate
between root and nonroot primary (i, j)-neighbors.
A root primary (i, j)-neighbor
w

Cost

of operations

Consider a read request originating
at node x for an
object A. Let y denote a node that haa a copy of A. In
the following,

we show that the expected

cost of a read

operation is 0( f (l(A) )c(z, y)). Letting y to be the node
with minimum C(Z, y) among the set of nodes that have

a copy of A, this bound implies that
is asymptotically
optimal.

the expected

cost

Let (z) and (y) be the primary neighbor sequences
for A with Z. = z and y. = y, respectively.
For any
nonnegative integer i, let Ai (resp., Di ) denote the ball
of smallest radius around ~i (resp., yi ) that contains
X;+l (resp., ~:+1). Let Bi (resp., Ei ) denote the set
UO<j<iAj
(resp,, Uo<j<illj).
Let Ci denote the ball of
---

317

O(c(z, y)). A key idea is to establish that the sequence
(si, ti) corresponds
to a two-dimensional
random walk
that is biased towards (O, O). Lemmas 5.11 and 5.12
provide the important first step towards formalizing this
notion.

smallest radius around ~i that contains all of the secondary (i, A[i] )-neighbors
of xi. For convenience, we
define B-l = E-l = 0.
It is useful to consider an alternative view of zi, Y:, Ai,
and D;. For any nonnegative i, if ~i+l (resp,, ~i+l ) is not
the root node for A, then Z;+l (resp., Yi+l ) is P(zi, ~<i)
(resp., p(yl, P<~)) and A, (resp., ~i) is

P(~i,~<i)

5.11
Let i be in [(log n)/b– 1]. Given arbitrary
well defined values for sj and tj for all j in [i] such that
si–l is at least 3, the probability that si is less than si-l
is at least 1 – ~2. Given arbitrary values for Sj and tj
for all j in [i] such that ti- ~ is at least 1, the probability
■
that ti is less than ti-l is at least 1 – E2.
Lemma

(rew.,

P(~i,’P<i)).

Let ~ be an integer constant that is chosen later appropriately.
For any nonnegative integer i and any integer j, let X: (resp., Y:) denote the ball N(z, 7~2ti+lJb)
(resp., IV(Y, -y~2(i+11b)). Let i* denote the least integer
such that the radius of X:. is at least C(Z, y). Let ai
(resp., bi) denote the radius of X: (resp., %1).
Lemma

For all i such that i z i*, X:

5.8

5.12 Let i be in [(log n)/b–1].
Given arbitrary
well defined values for sj and tj for all j in [i] such that
si–l is at most 3, the probability that si is O is at least
1 – E. Given arbitrary values for sj and tj for all j in
[i] such that ti-l is at most 1, the probability that ti is
9
O is at least 1 – E.

Lemma

is a superset

■

Of Yll

Lemma 5.9 For all i in [(log n)/b

– 2], we have
blog6 2ai and 2b10%A2bi < bi+l <
‘2b10gA2a; < (11+1 ~ 2
zb log, Zbi
For i = (logn)/b
– 2, we have ai+l <
‘i?b’oga
‘ai and bi+l s 2h10g’2b~. AIso, ai. and biq are
9
both ~(C(C, y)).
We define two sequences
integers as follows:

Lemma

o(ai).

(si ) and (ti) of nonnegative

If Si is in {O, 1,2},
then c(~i,~i+l)
is O, then c(~l, y~+l) is O(bi).

5.10

Ifti

By the definitions of si and ti,it follows that O ~
si+l ~ 3 if si ~ 2, and O ~ si+l ~ si + 1 otherwise. In
addition, O ~ ti+l< ti+ 1, for all i. Let s; equal O if
si = O, equal 1 if si c {1, 2, 3}, and equal si—2 otherwise.
Hence O s max{sj+l,ti+l}< max{s~,ti}+ 1,for all i.
We now analyze the random walk corresponding
to the
sequence (max{s’, t}).

Random Walks. Let W(U, F) be a directed graph in
which U is the set of nodes and F is the set of edges.
For all u in U, let D. be a probability distribution
over
the set {(u, v) c F} (let Prnu[(u, v) : (u, v) @ F] = O,
for convenience).
A random walk on W starting at V.
and according to {’D. : u E U } is a random sequence
(v) such that:
(i) vi is in U and (vi, vi+l) is in F,
for all i, and (ii) given any fixed (not necessarily simple) path Uo, . . . . ui in W and any fixed U:+l in U,
Pr[~i+l = Ui+l I (Vi), . . .,V; ) = (~0, . . . . Ui)] = Pr[~i+~ =
~i+l I vi = ~i] = I%.,
[(~i,
Ui+l)].

is
■

We now determine
an upper bound on the cost of
read for A as follows.
Let r be the smallest integer
i ~ i* such that (si, ti) = (O, O). By Lemma 5.8, C7 is
a superset of DT, implying that a copy of A is located
within r forwarding steps along (z). By the definition
of the primary and secondary neighbors, the cost of any
request (resp., forward) message sent by node ~i is at
most d ~c(~i, ~i+l) (resp., c(~i, ~i+l)). Since a COpy of
A is located within r forwarding
steps, by the definition of the algorithm,
the cost of all messages needed
in locating the particular
copy of A that is read is at

Let H be the directed graph with node set N and
edge set {(i, j) :i E N,O s j < i+ 1}. Let H’ be the
subgraph of H induced by the edges {(i + 1, i), (i, i +
l):i EN} U{(O, O), (l,l)}.
Let p and q be reals in (O, 1]. We now define two
random walks, wP,~ and w~,~, on graphs H and H’, respectively. The walk wP,~ = (w) is characterized
by: (i)
Pr[uli+~ ~ j – 1 I Wi = j] > p, for any integer j >1, (ii)
Pr[wi+l = O I ~i = j] ~ q, for j equal O or 1, and (iii)
Pr[~i+l = 2 I ~i = 1] s 1 – p. The walk ~~,g = (w’) is
characterized by: (i) Pr[w~+l = j–l I w; = j] = p, for all
integer j > 1, (ii) Pr[w~+l = O I w: = j] = q, for j equal
O or 1, and (iii) Pr[w~+l = 2 I w: = 1] = 1 – p. We note
that the sequence (max{s’, t}) represents
the random
walk wP,~ with appropriate
values for p and q, as determined by Lemmas 5.11 and 5.12. We analyze random
w~,~
walk WP,~ by first showing that wP,~ “dominates”

most WCo<j<T(d24~j,
zi+l) + c(Yj, Yj+l))). The cost
of reading the copy is at most f(t?(A)) times the preceding cost. Since d is a constant, the cost of reading A is

at most:

~
o(f(l(.4))(c(:cj,
zj+I)
og<T
The

remainder

of the

showing that E[~o<j<,(c(~j,

proof

+ c(Yj, Yj+I))

(2)

concerns the task of
xj+l) + c(~j, Yj+I))] is

318

with

respect

to the properties

walk w~,q is easier

to analyze

of interest.

The

as it is exactly

Lemma 5.19 For any i in [(log n)/b–

random

and E[c(yi, gi+l)] are both O(ai).

character-

by p and q. Lemmas 5.13 and 5.14 show that
bias of w~,q towards O is more than that ofw~,q.

the

ized

Lemma 5.13 For all i and k in N, for random
wP,q and w~,q, we have Pr[uq ~ k] ~ Pr[w~ ~ k].

We now use Lemmaa
Theorem 1.

all i and 1
Pr[Zi(LJp,q)
< f-l
> Pr[zi(w;,q) S 11

in

N,

we

~i+l)
+C(yi,
Yi+l)).
we

separately

We

place

ability

pr[~i(w~,q)

9)} pr[~i(~j,q)

=

t

+

i

1 I u:

= ~ I4

=

and
=

01

N,

we

a

that

~ >

E[~i.<i<~(c(~i,

have
■

f

in

S

max{l – q,5(p –

~i+l)

bound

on

E[~i.

+

+ c(yi, yi+l))].

<i<,

(c(~i

+ hi.).

J ~i+l)

+

i* + j is at most ( 10&)~. Therefore,
~i+l) + c(yi, yi+l))] is at most:

+ bi*+j)

~j(lo~)’(ai.+j
j>O

<

~j(loE)j2jb’”g‘‘(a;.+ hi. )
j~O

=

O(ai. + b~”),

since 10e2b10g~2 < 1. By Lemma
theorem follows.

5.9, the claim of the
■

Proof of Theorem
2:
Consider an insert operation
executed by x for any object.
The expected cost of
the operation is bounded by E[~o$i<lOg ~lb C(Zi, Xi+l)]j

The following claim is proved using Raney’s lemma [6,
15].
all

E[Zi*<i<7(c(~i,

(C(Zi, Z:+l)

Since
r is i’ + Ti* , by
~
fOllOWS.
5,18, we obtain that for any j ~ O, the prob-

Lemma

■

For

on ~[~o<i<i.

c(Yi, Yi+l))]

Lemma 5.15 For any i and j ~ i, we have Pr[ri ~

5.16

and

bounds

By Lemmaa 5.9 and 5.1% the first term is O(ai.

the notion of the domination
of
over
u’
For
any
i,
let
~i
(resp.,
r;)
denote
the
~P,q
, P,’J”
smallest J ~ O such that Wi+j = O (resp.,
10~+j
=
0).
We note that by letting (w) represent (max{s’, t}),the
terminating step ~ is given by i“ + Ti. .

Lemma

place

c(yi, yi+l))]

We now formalize

j] 2 Pdd < j].

5.9, 5.18, and 5.19 to establish

Proof of Theorem 1: By Equation 2, the expected
cost of the read operation is bounded by the expected
valueof f(l(A)) ~o<i<, O(c(~i,

walks
■

Let zi (w) be the random variable denoting the number
of steps taken to reach node O starting from node i, for a
random walk w. An excursion of length t! in a graph W
with node set N is a walk that starts at node O and first
returns to the start node at time t, for all t in N. For
all i such that ~i = O, let /i (w) be the random variable
that gives the length of the excursion in w starting at
time i. We note that for all i, Yi(u) equals Z.(u).

Lemma 5.14 For

1], E[c(~i, Zi+l)]
■

which

by Lemmaa

5.9 and

5.19 1s O(atioK~ji~-l)

=

o(c).
We now consider the cost of the delete operation.
By Lemma 5.6, for each i, the number of reverse
(i, j)-neighbors
of xi for any j is O(log n) whp, where
xi is the ith node in the primary neighbor sequence
of x.
Therefore,
the expected
cost of the delete
operation
executed
by x is bounded
by the prodand O(log n).
By
c(~i,
~i+l)]
uct of E[~O<i<logn/b
Lemma 5.19, ii follows that the expected cost of a delete
■
operation is O(C’ log n).

have

01

We now let u and w’ denote the random walks Up,g
and w~,~, respectively,
where p = 1 – Z&= and q =
1 – 2c. Lemmas 5.12 and 5.11 imply that w characterizes the random walk corresponding
to the sequence
(max{s’, t}). Consider the random walk w’. Assume
that at each step we only reveal whether w: = O or not.
We can define a sequence (u) associated with (w’) aa
follows: Vj = G’ iff w: = O, and vj = B otherwise.

5.4

Auxiliary

Memory

Proof of Theorem 3: We first place an upper bound
on the size of the neighbor table of any u in V. By defi-

5.17 Let i be in [(log n/b)– 1]. Given any jized
sequence (v)i– 1 of B, G values, the probability that w: is
■
O is at least 1 – 10.s.

nition, the number of primary and secondary neighbors
of u is at most (d+ l)2b(log n)/b, which is O(log n). By
Corollary 5.6.1, the number of reverse neighbors of u is
O(log2 n) whp.

Our main claim about
Lemmas 5.15 and 5.17.

We next place an upper bound on the size of the
pointer list of any u in V. The size of Ptr(u) is at most
the number of triples of the form (A, v, ), where A is in
A and v is in V such that: (i) there exists i in [(log n)/b]
such that v is an i-leaf of u, (ii) A[j] = u[j] for all j in
[i], and (iii) A is in the main memory of v.

Lemma

the random

walk w follows from

5.18 For any i in [(log n)/b – 1] and any nonnegative integer j, the probability that Ti is at least j is
at most (lO&)~.

Lemma

Using Lemma 5.18,
13[c(zi, ~i+l)] and E[c(yi

we derive an upper
, Yi+l
)] for all i.

bound

on

By
0(2ib

319

Lemma 5.7, the number
log n) whp. The probability

of i-leaves of u is
that A[j] = u[j], for

all j in [i], is at most l/2i~. Since the number of objects
in the main memory of any node is at most t, it follows that whp, l~tr(u)l is at most ~;6[10~n/b] ~(~ log’)
which is 0(1 logz n).
the
bounds
on the sizes of the
Combining
neighbor table and pointer list, we obtain that the size
of the auxiliary memor~” of u is 0(1 logz n) whp.
■
5.5

Adaptability

By Lemma 5.6, for any node u,
Proof of Theorem
4:
the number of nodes of which u is a primaryor secondary

neighboris O(log n) expected and 0(log2 n) whp. Moreover, u is a reverse
has C)(log
ability

of

neighbor

n) primary

of O(log

neighbors.

n) nodes

Therefore,

our scheme is O(log n) expected

since

u

Future Work

We would

like to extend our study to more general
classes of cost functions and determine tradeoffs among
the various complexity measures. It would also be interesting to consider models that allow faults in the network. We believe that our access scheme can be extended to perform well in the presence of faults, aa the
distribution
of control information in our scheme is balanced among the nodes of the network.

Acknowledgments
The authors would like to thank Madhukar Korupolu
and Satish RaQ for several helpful discussions.

[9] R. Karp,

M. Luby, and F. Meyer auf der Heide.
Efficient PRAM simulation on a distributed memory machine.
In Proceedings of the 2,/th Annual

Symposium

[1] B. Awerbuch,

Y. Bartal, and A. Fiat. Distributed
paging for general networks. In Proceedings of the

7th Annual ACM-SIAM
Symposium
on Discrete
Algorithms, pages 574-583, January 1996.
and D. Peleg. Routing with polynomial communication
space tradeoff. SIAM Journal
of Discrete Mathematics,
5:151–162, 1990.
and D. Peleg.

Sparse

partitions.

In

Symposium

pages 503–

513, 1990.
[4] Y. Bartal, A. Fiat,

and Y. Rabani.
Competitive
algorithms for distributed
data management.
In
Proceedings of the 2dth Annual ACM Symposium
on Theory of Computing, pages 39–47, May 1992.

[5] S. Dolev, E. Kranakis,
Bubbles:
dynamic

D. Krizanc, and D. Peleg.
Adaptative routing scheme for high-speed
networks. In Proceedings
of the 27th An-

nual ACM
ing,

Symposium

on the Theory

of Comput-

pages 528–537, 1995.

[6] R. L. Graham,

D. E. Knuth,

[11] S. J. Mullender
match-making.

Distributed

pages

Systems.

1993.
and P. M. B. Vitanyi. Distributed
Algorithmic,
3:367–39 1, 1988.

[12] C. G. Plaxton and R. Rajaraman.
Fast faulttolerant concurrent
access to shared objects.
In
Proceedings of the 37th Annual IEEE Symposium
on Foundations
of Computer Science, pages 570–
579, October 1996.
[13] C. G. Plaxton, R. Rajaraman,
and A. W. Richa.
Accessing nearby copies of replicated objects in a
distributed environment.
Technical Report TR-9711, Department of Computer Science, University of
Texas at Austin, April 1997.

Functional
[15] G. N. Raney.
and power series reversion.
Mathematical

Society,

composition
Transactions

94:441-451,

patterns
American

1960.

[16] E, Upfal and A. Wigderson. How to share memory
in a distributed
system. JA CM, 34: 116–127, 1987.

[2] B. Awerbuch

Proceedings
of the 31st Annual IEEE
on Foundations
of Computer
Science,

on Theory of Computing,

318-326, May 1992.
[10]S. J. Mullender, editor.

How to emulate shared memory.
[14] A. G. Ranade.
Journal of Computer and System Sciences, 42:307326, 1991.

References

[3] B. Awerbuch

Reading,

[8] D. Karger, E. Lehman, T. Leighton, M. Levine,
D. Lewin, and R. Panigrahy. Relieving hot spots on
the World Wide Web. In Proceedings of the 29th
Annual ACM Symposium on the Theory of Computing, May 1997.

Addison-Wesley,
6

Addison-Wesley,

[7] J. D. Guyton and M. F. Schwartz. Locating nearby
copies of replicated Internet servers. In Proceedings
of ACM SIGCOMM, pages 288–298, 1995.

ACM

the adapt-

and 0(log2 n)
■

whp

Concrete Mathematics.
MA, 1989.

and O. Patashnik.

320

[17] M. Van Steen, F. J. Hauck, and A. S. Tanenbaum.
A model for worldwide tracking of distributed
objects. In Proceedings
of TINA ’96, pages 203-212,
September 1996.

Brief Announcement:
Amoebot—A New Model for Programmable Matter
Zahra Derakhshandeh

Shlomi Dolev

Robert Gmyr

Arizona State University

Ben-Gurion University

University of Paderborn

zderakhs@asu.edu
Andréa W. Richa

dolev@cs.bgu.ac.il
Christian Scheideler

gmyr@mail.upb.de
Thim Strothmann

Arizona State University

University of Paderborn

University of Paderborn

aricha@asu.edu

scheideler@mail.upb.de

thim@mail.upb.de

ABSTRACT

microelectronic mechanical components, such that one can
anticipate integrating logic circuits, microsensors, actuators,
and communications devices on the same chip. Also, there
has been intriguing progress in understanding the biochemical mechanisms of individual cells such as the mechanisms
behind cell signaling and cell movement. Moreover, recent
results have demonstrated that, in principle, biological cells
can be turned into finite automata or even pushdown automata. Therefore, one can imagine to tailor-make biological
cells to operate as sensors and actuators, as programmable
delivery devices, and as chemical factories for the assembly
of nano-scale structures.
One can envision producing vast quantities of microscopic
computing elements to form programmable matter [8]. Programmable matter refers to matter which has the ability to
change its physical properties (shape, density, moduli, conductivity, optical properties, etc.) in a programmable fashion, based upon user input or autonomous sensing. This has
many applications like smart materials, autonomous monitoring and repair, and minimal invasive surgery, so there
is a high relevance of this topic to industry and society in
general. While programmable matter has just been science
fiction more than two decades ago, a large amount of research activities can now be seen in this field in the recent
years. These activities include research on passive systems
like DNA computing [4, 7, 10] as well as active systems like
swarm robotics [5, 6] and modular robotic systems [2, 3, 9].
Most related to our research is the recently proposed nubot
model [11].
We propose Amoebot, a new amoeba-inspired model for
programmable matter. In our model, the programmable
matter consists of particles that can bond to neighboring
particles and use these bonds to form connected structures.
Particles only have local information and have modest computational power. The particles achieve locomotion by expanding and contracting, which resembles the behavior of
amoeba [1].

The term programmable matter refers to matter which has
the ability to change its physical properties (shape, density, moduli, conductivity, optical properties, etc.) in a programmable fashion, based upon user input or autonomous
sensing. This has many applications like smart materials,
autonomous monitoring and repair, and minimal invasive
surgery, so there is a high relevance of this topic to industry and society in general. While programmable matter has
just been science fiction more than two decades ago, a large
amount of research activities can now be seen in this field
in the recent years. Often programmable matter is envisioned, as a very large number of small locally interacting
computational particles. We propose the Amoebot model, a
new model which builds upon this vision of programmable
matter. Inspired by the behavior of amoeba, the Amoebot
model offers a versatile framework to model self-organizing
particles and facilitates rigorous algorithmic research in the
area of programmable matter.

Categories and Subject Descriptors
F.m [Theory of Computation]: Miscellaneous

Keywords
programmable matter; self-organization; nano-computing;
mobile robots

1.

INTRODUCTION

Recent advances in microfabrication and cellular engineering foreshadow that in the next few decades it might be
possible to assemble simple information processing units at
almost no cost. Myriads of these small-scale units could be
combined to powerful systems capable of solving intricate
tasks. This vision of building cheap microscopic processing
units is supported by the progress made in manufacturing

2.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SPAA’14, June 23–25, 2014, Prague, Czech Republic.
ACM 978-1-4503-2821-0/14/06.
http://dx.doi.org/10.1145/2612669.2612712.

MODEL

Consider the equilateral triangular graph, see Figure 1. A
particle occupies either a single node or a pair of adjacent
nodes in this graph, and every node can be occupied by at
most one particle. Two particles occupying adjacent nodes
are defined to be connected.
Every particle has a state from a finite set Q. Connected
particles can communicate via the edges connecting them in

220

Figure 1: The left half of the figure depicts a section
of the infinite equilateral triangular graph. Nodes
are shown as black circles. The right half shows five
particles on the graph. When depicting particles
we draw the graph as a gray mesh without nodes.
A particle occupying a single node is depicted as a
black circle, and a particle occupying two nodes is
depicted as two black circles connected by an edge.

Figure 2: The three parts of the figure show a moving particle together with the labels seen by the particle. On the left, the particle occupies only a single
node. The particle then expands in the direction of
the edge labeled 4 resulting in the particle occupying two nodes as depicted in the middle. Since the
expansion changes the number of edges leaving the
particle, the edges have to be relabeled. The direction of the edge labeled 0 remains constant. Because
of the restriction for the label 0 mentioned in the
text, this uniquely defines the edge that will receive
the label 0 after the expansion. Next the particle
contracts out of one of the nodes it currently occupies towards the direction of the edge labeled 6 resulting in the particle occupying only a single node
as depicted on the right. Again, the edges leaving
the particle are relabeled.

the following way. A particle p holds a flag from a finite alphabet Σ for each edge that leaves p (i.e., all edges incident
in a node occupied by p except the edge between the occupied nodes if p occupies two nodes). A particle occupying
the node on the other side of such an edge can read this flag.
This communication process can be used in both directions
over an edge. In order to allow a particle p to address the
edges leaving it, the edges are labeled from the local perspective of p. This labeling starts with 0 at an edge leading to a
node that is only adjacent to one of the nodes occupied by
p and increases counter-clockwise around the particle. The
restriction for label 0 will be used below to uniquely define
which edge is labeled 0 when a particle moves.
Particles move through expansion and contraction: If a
particle occupies one node, it can expand into an unoccupied adjacent node to occupy two nodes. If a particle occupies two nodes, it can contract out of one of these nodes to
occupy only a single node. During these movements, the direction of the edge labeled 0 remains constant, even though
the edge itself might change. Figure 2 shows an example of
the movement of a particle. Besides executing expansions
and contractions in isolation, we allow pairs of connected
particles to combine these primitives to perform a coordinated movement: One particle can contract out of a certain
node at the same time as another particle expands into that
node. We call this movement a handover, see Figure 3. The
particles involved in a handover are defined to remain connected during its execution.
Computationally, particles resemble finite state machines.
A particle acts according to a transition function

Figure 3: Two particles performing a handover.
The set of movements is defined as
M ={idle} ∪
{expandi | i ∈ [0, 9]} ∪
{contracti | i ∈ [0, 9]} ∪
{handoverContracti | i ∈ [0, 9]}.
The movement idle means that p does not move, and expandi
and contracti are defined as mentioned above. The index
i specifies the edge that defines the direction along which
the movement should take place, as shown in the example
in Figure 2. The movement handoverContracti specifies a
contraction that can only be executed as part of a handover.
Summarizing, a transition function specifies a set of turns a
particle would like to perform based on the locally available
information.
A system of particles progresses by executing atomic actions. An action is either the execution of an isolated turn
for a single particle or the execution of a turn for each of
two particles resulting in a handover between those particles. Note that if a movement is not executable, the respective action is not enabled: For example, a particle occupying two nodes cannot expand although it might specify this
movement in a turn. As another example, a particle cannot
expand into an occupied node except as part of a handover.
Finally, an action consisting of an isolated turn involving the

δ : Q × Σ10 → P(Q × Σ10 × M ).
For a particle p the function takes the current state of p and
the flags p can read via its leaving edges as arguments. Here,
the i-th coordinate of the tuple Σ10 represents the flag read
via the edge labeled i when numbering the coordinates of
the tuple starting at 0. If for a label i there is no edge with
that label or if the respective edge leads to a node that is
not occupied, the coordinate of the tuple is defined to be ε.
The value ε ∈ Σ is reserved for this purpose and cannot be
set as a flag by a particle. The transition function maps to
a set of turns. A turn is a tuple specifying a combination of
a state to assume, flags to set, and a movement to execute.

221

movement handoverContracti is never enabled as this movement can only be performed as part of a handover. The
transition function is applied for each particle to determine
the set of enabled actions in the system. From this set, a
single action is arbitrarily chosen and executed. The process
of evaluating the transition function and executing an action
continues indefinitely.

3.

by the particles of a system. A second example is the class
of shape formation problems in which a system has to arrange to form a specific shape. Finally, in bridging problems
particles have to bridge gaps in given structures. We see
the coating problems as an algorithmic primitive for solving
other problems. For example, the formation of a shape can
be achieved by creating an initially small instance of that
shape which is then iteratively coated to form increasingly
large instances until the number of particles in the system
is exhausted.
Also, variants of the model might be of interest. Firstly,
in a physical realization particles may become faulty and a
system may not be well-initialized. For a system to handle such occurrences it may be necessary to allow particles to detect other faulty particles and to cope with them.
Such a model would require self-stabilizing algorithms. Secondly, the model may be extended from two to three dimensions For more information, including preliminary results on coating problems and more related work please
see http://amoebot.cs.upb.de.

DISCUSSION

The general Amoebot model as described in the previous
section can take various specific forms depending on how
systems of particles are initialized, what information particles keep track of in their state, and what information they
share over their leaving edges. For example, the model does
not specify how the labeling of the edges leaving a particle
is initially set up. Always assigning the label 0 to an edge
pointing upwards for every particle results in a full-compass
model, while assigning this label differently for each particle results in a no-compass model where particles only share
their sense of orientation. As an example for the influence
of communication, in the no-compass variant of the model
two particles can compare the relative rotation of the direction of their respective edges with label 0 by communicating,
but this is by no means mandatory. As a final example, the
particles can be set up to know whether they occupy one or
two nodes, they can keep track of this information during
the progression of the system, and they can communicate
this information over their leaving edges, but again none of
this is mandatory.
While the model allows for many variations, it also has
fixed core features. Some of these features represent our idea
on how programmable matter consisting of discrete particles
would work: The particles have constant memory (in particular, they have no identifiers) and modest computational
power. Particles act asynchronously and make their decision based on local information. Finally, particles can only
move themselves and not other particles because of limited
physical strength. However, the most striking feature of the
Amoebot model, namely the use of expansion and contraction as locomotion primitives together with handovers as
their combination, is motivated by the problems we want to
investigate using the model: We want to study problems in
which all particles in a system have to form a single connected component at all times. An example of why our type
of locomotion is favorable in this context is that it allows
arbitrarily long chains of (initially contracted) particles to
move along a common trajectory without losing connectivity. All particles of such a chain simply expand and contract
in alternation. The first particle in the chain determines the
trajectory by deciding the direction of its expansion, the expansion of the remaining particles is always in direction of
their predecessor in the chain. Furthermore, a contraction
is always in the same direction in the grid as the previous
expansion and all contractions are handover-contractions except the contractions of the last particle in the chain.

4.

5.

REFERENCES

[1] R. Ananthakrishnan and A. Ehrlicher. The forces
behind cell movement. International Journal of
Biological Sciences, 3(5):303–317, 2007.
[2] Z. Butler, S. Murata, and D. Rus. Distributed
replication algorithms for self-reconfiguring modular
robots. In Distributed Autonomous Robotic Systems 5,
pages 37–48. Springer, 2002.
[3] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita.
Generic decentralized control for lattice-based
self-reconfigurable robots. International Journal of
Robotics Research, 23(9):919–937, 2004.
[4] K. C. Cheung, E. D. Demaine, J. R. Bachrach, and
S. Griffith. Programmable assembly with universally
foldable strings (moteins). IEEE Transactions on
Robotics, 27(4):718–729, 2011.
[5] S. Kernbach, editor. Handbook of Collective Robotics –
Fundamentals and Challanges. Pan Stanford
Publishing, 2012.
[6] J. McLurkin. Analysis and Implementation of
Distributed Algorithms for Multi-Robot Systems. PhD
thesis, Massachusetts Institute of Technology, 2008.
[7] R. Nagpal, A. Kondacs, and C. Chang. Programming
methodology for biologically-inspired self-assembling
systems. Technical report, AAAI Spring Symposium
on Computational Synthesis, 2003.
[8] T. Toffoli and N. Margolus. Programmable matter:
concepts and realization. Physica D: Nonlinear
Phenomena, 47(1):263–272, 1991.
[9] J. E. Walter, J. L. Welch, and N. M. Amato.
Distributed reconfiguration of metamorphic robot
chains. Distributed Computing, 17(2):171–189, 2004.
[10] E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman.
Design and self-assembly of two-dimensional dna
crystals. Nature, 394(6693):539–544, 1998.
[11] D. Woods, H.-L. Chen, S. Goodfriend, N. Dabby,
E. Winfree, and P. Yin. Active self-assembly of
algorithmic shapes and patterns in polylogarithmic
time. In ITCS, pages 353–354, 2013.

RESEARCH CHALLENGES

We would like to use the Amoebot model to investigate
various problems in which the system of particles has to form
a single connected component at all times. As an example,
the class of coating problems might be considered. Here, the
surface of an object is to be coated as uniformly as possible

222

IEEE TRANSACTIONS ON MOBILE COMPUTING,

VOL. 5,

NO. 6,

JUNE 2006

653

Cluster Overlay Broadcast (COB): MANET
Routing with Complexity Polynomial in
Source-Destination Distance
Luke Ritchie, Student Member, IEEE, Hyo-Sik Yang, Member, IEEE, Andréa W. Richa, and
Martin Reisslein, Senior Member, IEEE
Abstract—Routing algorithms with time and message complexities that are provably low and independent of the total number of
nodes in the network are essential for the design and operation of very large scale wireless mobile ad hoc networks (MANETs). In this
paper, we develop and analyze Cluster Overlay Broadcast (COB), a low-complexity routing algorithm for MANETs. COB runs on top
of a one-hop cluster cover of the network, which can be created and maintained using, for instance, the Least Cluster Change (LCC)
algorithm. We formally prove that the LCC algorithm maintains a cluster cover with a constant density of cluster leaders with minimal
update cost. COB discovers routes by flooding (broadcasting) route requests through the network of cluster leaders with a doubling
radius technique. Building on the constant density property of the network of cluster leaders, we formally prove that, if there exists a
route from a source to a destination node with a minimum hop count of , then COB discovers a route with at most OðÞ hops from
the source to the destination node in at most OðÞ time and by sending at most Oð2 Þ messages. We prove this result for arbitrary
node distributions and mobility patterns and also show that COB adapts asymptotically optimally to the mobility of the nodes. In our
simulation experiments, we examine the network layer performance of COB, compare it with Dynamic Source Routing, and
investigate the impact of the MAC layer on COB routing.
Index Terms—One-hop clustering, algorithm/protocol design and analysis, message complexity, routing protocol, scalability, time
complexity, wireless mobile ad hoc network.

æ
1

INTRODUCTION

S

CALABLE routing is one of the key challenges in designing
and operating large scale mobile ad hoc networks
(MANETs). In order to ensure effective operation as the
total number of nodes in the MANET becomes very large,
the complexity of the employed routing algorithms should
be low and independent of the total number of nodes in the
network. An important consideration in the development of
scalable routing algorithms is that the complexity properties
of the scalable routing algorithms should be well understood and formally analyzed [1]. While simulations are very
useful in assessing routing protocols, they typically provide
only limited insight into the underpinnings and parameter
dependencies that govern the algorithm performance. As
discussed in more detail in Section 1.1, significant progress
has been made in recent years in developing and evaluating
algorithms and algorithm refinements to achieve scalable
MANET routing.

. L. Ritchie and M. Reisslein are with the Department of Electrical
Engineering, Arizona State University, Goldwater Center, MC 5706,
Tempe, AZ 85287-5706. E-mail: {Luke.Ritchie, reisslein}@asu.edu.
. H.-S. Yang is with the Division of Electronic and Electrical Engineering,
Kyungnam University, 449, Wolyoung-Dong, Masan, Korea, 631-701.
E-mail: hsyang@kyungnam.ac.kr.
. A. Richa is with the Department of Computer Science and Engineering,
Arizona State University, Box 878809, Tempe, AZ 85287-8809.
E-mail: aricha@asu.edu.
Manuscript received 4 May 2005; revised 10 Nov. 2005; accepted 6 Jan. 2006;
published online 17 Apr. 2006.
For information on obtaining reprints of this article, please send e-mail to:
tmc@computer.org, and reference IEEECS Log Number TMC-0129-0505.
1536-1233/06/$20.00 ß 2006 IEEE

Yet, some key challenges remain in the development and
evaluation of scalable MANET routing algorithms. In
particular, the existing MANET routing algorithms that
have been formally analyzed either:
incur, for the route discovery, a total elapsed time or
total number of messages exchanged that depend on
the overall network size, such as the total number of
nodes in the network or the total diameter (in terms of
number of wireless hops) of the network (see, for instance,
[2], [3], [4]), or
. make restrictive assumptions about the overall
network topology, such as limiting the network
density (see, for instance, [4], [5], [6]) or assume
knowledge of the locations of the nodes at any point
in time (location-aided routing) (see, for instance, [7],
[8], [9]).
For these reasons, the analyzed routing algorithms are of
limited use for very large MANETs consisting of a very
large number of nodes and having a very large diameter
and no location aid. As detailed in Section 1.1, a number of
enhancements to the existing routing protocols have
recently been proposed to improve their scalability. These
enhancements have demonstrated significant potential for
improving the scalability in simulations, but have not yet
been formally analyzed in the context of the routing
protocols.
In this paper, we address these two key shortcomings in
the state-of-the-art in scalable MANET routing that 1) the
existing formally analyzed algorithms do not scale well
.

Published by the IEEE CS, CASS, ComSoc, IES, & SPS

654

IEEE TRANSACTIONS ON MOBILE COMPUTING,

with the total network size and 2) scalability enhancing
refinements are formally not well understood. Toward
addressing these two points, we develop and formally
analyze Cluster Overlay Broadcast (COB), a highly scalable
reactive routing algorithm for very large MANETs. COB
incorporates the recently considered routing on top of the
clusters mechanism and the doubling radius broadcast
mechanism in a judicious manner in a low-complexity
reactive routing algorithm.
In brief, our approach is to form a one-hop clustering
(cluster cover) of the network and then to perform route
discovery by broadcasting route requests over the overlay
network formed by the cluster leaders. More specifically,
we employ the Least Cluster Change (LCC) algorithm to
establish and maintain a clustering structure of the network,
whereby a node in a given cluster can reach the leader of
the cluster in one hop. When a source node wants to send a
message to a destination node, the source node contacts its
cluster leader. The cluster leader then floods (broadcasts)
route requests over the overlay network of cluster leaders.
We employ long-haul transmissions, which have three times
the range of the regular (short-haul) transmissions for the
transmissions on the overlay network of cluster leaders. The
route requests are broadcast with a doubling radius
technique, i.e., the cluster leader first broadcasts the route
request with a time-to-live (TTL) of one long-haul transmission hop. If the destination node is reached, it responds
with an acknowledgment and the route discovery is
completed. Otherwise, after a timeout, the source node’s
cluster leader broadcasts the route request with a TTL of
two, then four, and so on.
We formally prove that COB has a route discovery
complexity—both in terms of total elapsed time and
number of message exchanges—that is polynomially
proportional to the minimum number of hops between the
source node and the destination node and adapts optimally
to mobility. More specifically, if  denotes the minimum
number of short-haul hops from the source node to the
destination node, then COB discovers a route with at most
OðÞ hops (which may include short-haul and long-haul
hops). This route discovery takes at most OðÞ time and
requires the sending of at most Oð2 Þ messages with COB.
We also show that COB requires only a constant amount of
storage in each node. These theoretical results, which hold
for arbitrary node mobility and node density, build on the
constant density of the overlay network of cluster leaders
and the doubling radius broadcast. To the best of our
knowledge, these results make COB the first MANET
routing algorithm for which both the time complexity and
the message complexity are polynomial in the minimum
hop distance between the source node and the destination
node and independent of the overall network dimensions
(total number of nodes, network diameter).
This paper is structured as follows: In the following
section, we review related work. In Section 2, we describe
the considered model of the MANET. In Section 3, we
discuss the properties of the algorithm used to maintain the
one-hop cluster cover and prove that the density of the
network of cluster leaders is constant for the considered
clustering algorithm. We also discuss the system model

VOL. 5,

NO. 6,

JUNE 2006

aspects related to the clustering and the transmissions
within and in between clusters. In Section 4, we introduce
and formally analyze the Cluster Overlay Broadcast (COB)
routing algorithm. In Section 5, we present simulation
results for the COB routing algorithm. We summarize our
conclusions in Section 6.

1.1 Related Work
The routing in MANETs has attracted a significant level of
interest in recent years (see, e.g., [2], [10], [11], [12], [13],
[14], [15], [16] for overviews). In general, the MANET
routing protocols can be classified into proactive routing
protocols, which maintain routing tables which are consulted when transmitting a packet toward its destination,
and reactive routing protocols, which find a route on
demand, i.e., in response to the generation of a message
for a specific destination. Reactive routing protocols are
typically more efficient for MANETs with a high level of
mobility (see, e.g., [16], [17]) and are the main focus of our
study.
The issue of scalable routing has recently begun to attract
significant interest and several studies have formally
analyzed and compared the complexity characteristics of
the existing routing algorithms (see, for instance, [2], [3], [4],
[6], [18], [19], [20], [21], [22]). It was found that most of the
existing reactive routing algorithms have message complexities that are OðNÞ, where N denotes the total number of
nodes in the network. That is, the complexities of the
routing algorithms depend on the overall size of the
network. It was also found that there are algorithms that
have lower complexity, but these generally make restrictive
assumptions about the network topology or require location
awareness. For instance, hierarchical state routing (HSR) [5],
[23] is a proactive routing algorithm that runs on top of a
clustering hierarchy and has a message complexity linear in
the average number of nodes in a cluster and the number of
hierarchical levels in the clustering. This makes HSR a low
complexity routing algorithm if the nodes are uniformly
distributed. In general, if the nodes are nonuniformly
distributed, the complexity may approach OðNÞ. Also,
HSR requires a multilevel clustering hierarchy, which needs
to maintain a list of all cluster members in each cluster
leader. This structure tends to become costly to maintain for
high levels of node mobility. In contrast, COB requires only
a simple two-level clustering hierarchy (consisting of
regular nodes and cluster leaders) and does not require
the cluster leaders to maintain membership lists; COB only
requires that each individual regular node knows who its
cluster leader is. Examples for routing algorithms that
employ location information are location aided routing [8],
the greedy perimeter stateless routing [7], and the scalable
location update routing protocol (SLURP) [9], which exploit
the location information to limit the geographic area over
which route requests are broadcast and thus achieve
complexities on the order of the geographic broadcast area.
COB, on the other hand, does not require any location
information and has time and message complexities that are
provably polynomial in the minimum source-destination
distance and independent of the overall network size.
A plethora of routing algorithms and routing algorithm
refinements have been developed and evaluated through
simulations. The ad hoc on-demand distance vector protocol

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

(AODV), for instance, which is one of the most prominent
reactive routing protocols, has been studied extensively
through simulations [17], which have provided invaluable
insights into its dynamics and led the development of
several refinements. In the adaptive routing using clusters
(ARC) approach [24], for instance, the AODV runs on top
of a clustering (overlay network) that is maintained with a
clustering algorithm that enforces a subset property. That
is, a cluster leader is only demoted to regular node status
when the cluster has become a subset of another cluster.
Further approaches for routing on top of a cluster cover or
a set of core nodes have been proposed (see, for instance,
[25], [26], [27], [28], [29], [30], [31]). Also, the doubling
radius technique has been simulated in the context of
AODV [32] and has been found to reduce the complexity.
Our work on COB, which also employs clustering and
doubling radius broadcast, complements the existing
simulation studies of these mechanisms in that we formally
analyze these techniques in the context of a routing
algorithm. Our theoretical analysis yields fundamental
insights into the mechanisms governing the complexities
of the routing algorithm. For instance, we find that it is
crucial that two cluster leaders are not within the shorthaul communications range, as is common with the subset
property. The transmission with two different transmission
ranges which we employ in COB has been evaluated
through simulations in [33].
A variety of other refinements have been proposed
which are complementary to our routing algorithm development and analysis. For instance, different mechanisms
for flooding the route requests that exploit the mobility of
the nodes have been proposed (see, for instance, [34], [35]).
These approaches assume that the nodes either do not move
very far or move quite extensively. In contrast, we do not
assume any specific mobility behavior. The feasibility of
routing based on dynamic addresses is examined in [36].
Techniques for further optimizing a route found by a route
discovery algorithm are explored in [37].

2

SYSTEM MODEL

We consider a large ad hoc network of mobile wireless
nodes (MANET) and let N denote the number of nodes. We
consider the problem of unicast routing in the MANET. In
particular, we focus on the problems of 1) discovering a
route from a source node x to a destination node y and
2) delivering a message M from x to y.
We consider a wireless system consisting of homogeneous
nodes with the capability to transmit with two different fixed
transmission ranges, namely, the (normalized) transmission
ranges one and three, which can, for instance, be achieved
with power control [33]. Following [33], we use the
terminology short-haul transmission to refer to a transmission
with transmission range one and long-haul transmission to
refer to a transmission with transmission range three. For the
analytical model, we view the short-haul transmission range
of each network node as a disk of radius one under Euclidean
norm in <2 (or <3 , depending on the particular network
application) centered at the node, i.e., each node is
represented by a unit-disk. Our main motivation for considering the two transmission ranges is to keep our

655

discussions of the routing protocol simple and focused on
the main underlying conceptual issues. When only shorthaul transmissions are employed, then transmissions between adjacent cluster leaders would need to be forwarded
by up to two gateway nodes. We conjecture that the
transmissions via the gateway nodes do not affect the
asymptotic complexity of the COB algorithm, but would
require a more detailed “bookkeeping” in the clustering. This
transmission via gateway nodes is left for future work.
In our analysis, we do not assume any specific distribution of the nodes. However, our COB route discovery
algorithm—as any other algorithm—can only find a route if
the network is connected, i.e., if there exists at least one
feasible route from the source to the destination node using
only short-haul transmissions with a fixed (normalized)
transmission range of one. (We note that an interesting
direction for future work is to consider a network scaling
scenario where the transmission ranges are adapted to keep
the network connected as the number of nodes changes.)
We do not assume any specific mobility model in our
analysis. We only initially assume, as is reasonable and
common, that the mobility of the nodes is on a time scale
slower than the route discovery [35].

3

CLUSTERING

AS A

BASIS

FOR

ROUTING

In this section, we present the salient points of the node
clustering as it relates to our routing protocol. Node
clustering in ad hoc networks has received a significant
amount of interest of its own (see, for instance, [38], [39],
[40], [41], [42], [43], [44], [45]). Our routing algorithm
builds upon some specific properties of the underlying
clustering structure. In particular, we employ the Least
Cluster Change (LCC) algorithm proposed by Chiang et al.
[46] for one-hop clusterings of MANETs. In the clustering,
we consider only the short-haul transmission range, i.e.,
each node can reach its cluster leader in one short-haul
transmission hop. The choice of the LCC clustering
algorithms is motivated by our previous work [47], where
we proved that the LCC algorithm is asymptotically
optimal or near-optimal with respect to: 1) the number of
clusters maintained and 2) the cost of an update. More
specifically, one would like to minimize the number of
clusters maintained since the smaller the number of
clusters maintained, the more efficient the clustering of
the network, in the sense that any routing, name lookup,
other levels of a clustering hierarchy, or any other network
function to be built on top of the one-hop clustering cover
would like to see a network of cluster leaders which is as
small as possible (i.e., to have a view of the network which
is as simplified as possible). Also, the fewer cluster leaders
maintained, the fewer cluster leader changes we expect to
see in the network. Thus, we expect a network of cluster
leaders which is relatively stable, which is a key property
for implementing efficient routing algorithms on top of
the one-hop clustering. We have proven in [47] that the
LCC algorithm maintains a seven-approximation on the
minimum possible number of clusters; this means that the
number of clusters maintained by the LCC algorithm is
at most seven times the minimum possible number in a
one-hop cluster cover of the network, at any point in time.

656

IEEE TRANSACTIONS ON MOBILE COMPUTING,

As far as the cost of an update is concerned, there is a
trade-off between the number of clusters maintained and
the update cost of an algorithm. For example, we have
shown in [47] that, if we were able to maintain a minimum
one-hop clustering of the network (note that this problem is
NP-hard and, therefore, maintaining such a minimum
clustering would most likely be infeasible), then the cost
of an update may be strictly proportional to the number of
nodes N in the network. On the other hand, an algorithm
that does not attempt to minimize the number of clusters
maintained could select every node in the network to be a
cluster leader, incurring zero update cost. Since this latter
approach is equivalent to having no clustering in the
network, the best we can hope for is to have constant
update cost, keeping each update as “local” as possible.
Indeed, we have proven in [47] that the LCC algorithm has
an asymptotically minimal update cost, namely, (a small)
constant.
We proceed by proving a very important property of the
LCC algorithm which follows from the fact that no two cluster
leaders fall within the short-haul communication range of
one another. (Theorem 1 below and the related corollaries
also hold for any other clustering algorithm that satisfies the
property that no two cluster leaders can communicate
directly via a short-haul transmission.) A 1/2-radius disk
centered at a node v is a disk of radius 1/2 centered at v.
Throughout, we employ standard asymptotic notation where
a function gðnÞ ¼ OðfðnÞÞ if there exist positive constants c
and n0 such that gðnÞ  c  fðnÞ for all n  n0 and gðnÞ ¼
ðfðnÞÞ if there exist positive constants c and n0 such that
gðnÞ  c  fðnÞ for all n  n0 .
Theorem 1. There are at most OðaÞ cluster leaders whose
1=2-radius disks are fully contained in an area A of total size a.
Proof. From the property of the LCC algorithm that no
two cluster leaders can communicate with one another
with a short-haul transmission, it follows that no cluster
leader is contained in the unit-disk centered at another
cluster leader. Hence, no two 1=2-radius disks centered at
the cluster leaders intersect with each other. Each of
these 1=2-radius disks covers a constant size area,
namely, an area of size =4, of the plane. Thus, if we
take an area A of size a in the network, we can see at
most 4a= 1=2-radius disks centered at the cluster leaders
which are fully contained in area A.
u
t
We define the density of a network as the maximum ratio
of the number of cluster leaders whose 1/2-radius disks are
fully contained in an area A to the total size a of area A, for
any area A on the Euclidean plane. The corollary below
follows directly from the theorem:
Corollary 1.1. The network consisting only of the nodes selected
as cluster leaders has constant density, namely, it has density
at most 4=.
Another corollary, which will be useful when proving
the complexity of our COB algorithm, follows from
Corollary 1.1:
Corollary 1.2. There are at most Oðr2 Þ cluster leaders in a disk D
of radius r  1 centered at a cluster leader v.

VOL. 5,

NO. 6,

JUNE 2006

Proof. The 1/2-radius disks of all cluster leaders contained
in D are fully contained in a disk D0 of radius r þ 1=2
centered at node v. The area of D0 is   ðr þ 12Þ2 . From
Corollary 1.1, we know that there are at most 4  ð  ðr þ
1 2
1 2
2
2Þ Þ= ¼ 4  ðr þ 2Þ ¼ Oðr Þ cluster leaders whose 1/2radius disks are fully contained in D0 . Hence, there are at
most Oðr2 Þ cluster leaders in D.
u
t
We remark that if a network is connected when only
short-haul transmissions are employed, then the network is
also connected when only 1) short-haul transmissions
between regular nodes and their cluster leaders as well as
2) long-haul transmissions between cluster leaders are
employed. To see this, note that, in a network connected
with short-haul transmissions, each node has a neighbor
that is no further away than the short-haul transmission
range. Thus, in a 1-short-haul-hop cluster cover of such a
network, where each node is within the short-haul
transmission range of its cluster leader, the maximum
distance between two adjacent cluster leaders1 is equivalent
to three times the short-haul transmission range, which, in
turn, is equal to the long-haul transmission range.
We also briefly note that, in wireless communications,
the energy consumption increases generally quadratically
with the transmission range. A long-haul transmission thus
consumes on the order of nine times more energy than a
short-haul transmission. In our clustering, at least two shorthaul transmissions (via at a gateway node) and at most
three short-haul transmissions (via two gateway nodes)
would be required to communicate from cluster leader to
adjacent cluster leader. Hence, the use of long-haul
transmissions consumes on the order of between three
and 4.5 times more energy than the use of only short-haul
transmissions. (We also note that there could be situations
in sparse networks where a long-haul transmission can
reach a cluster leader that is not adjacent, i.e., more than
three short-haul transmissions would be required to reach
that cluster leader; in such a situation, the use of long-haul
transmissions can actually lead to energy savings.) The
generally higher energy consumption with the long-haul
transmissions can be overcome by forwarding transmissions between adjacent cluster leaders with short-haul
transmissions via up to two gateway nodes, which is a
direction for future work.

3.1 System Model: Time Step
In our system model, we focus on the network layer and do
not consider a particular medium access control (MAC)
protocol. We define the time step as the maximum time
required 1) to conduct a short-haul transmission from a
regular node to its cluster leader or 2) to conduct a longhaul broadcast from a cluster leader that reaches all regular
nodes in the cluster headed by the cluster leader as well as
all adjacent cluster leaders, i.e., the cluster leaders within
the long-haul transmission range. We assume that the
processing of the route requests and acknowledgments in a
node takes negligible time (or is accounted for in the time
1. Two cluster leaders are defined to be adjacent if there exists a path
using only short-haul transmissions via at most two regular nodes between
the two cluster leaders.

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

step). We assume that the time step is a constant that is
independent from the total number of nodes in the network
and the distribution of the nodes in the network. This can be
reasonably achieved by employing a mix of time, frequency,
and code division multiple access. A cluster leader, for
instance, can impose a time division access method for the
transmissions from its regular nodes. Also, the transmissions from a cluster leader to its regular nodes and the
transmission from a cluster leader to its adjacent cluster
leaders (noting that the LCC algorithm ensures that there
are no more than 49 such adjacent cluster leaders, as seen by
setting r ¼ 3 in the proof of Corollary 1.2) can be conducted
in different frequency bands or with different CDMA codes
[33]. Nevertheless, it is to be understood that, in a real
wireless network, there are, in general, no absolute
(deterministic) guarantees for reaching nodes via wireless
transmission within a given time interval, as nodes may
experience shadowing or malfunction or other impairments
with a nonzero probability. The absolute performance
bounds derived for our network model thus correspond,
in general, to probabilistic performance characterizations in
real networks.

4

2.

Route discovery: Flooding (broadcasting) the route
request message on the cluster overlay network:

Node y, upon receiving a RREQ ðy; i; k; Ly ; xÞ,
where Ly is the cluster leader of node y, sends a
path acknowledgment message via a short-haul
transmission (Ly may be y itself, in which case,
we skip the actual sending of the acknowledgment message; also, if y is a cluster leader
itself, the RREQ message it receives does not
contain Ly in its fourth field).
b. Node Ly , upon receiving a path acknowledgment notice from node y, sends a long-haul
transmission path acknowledgment message
(ACK) of the form ðy; ‘Þ, where ‘ is the node in
the fourth field of the RREQ message stored at
Ly . Node Ly also marks itself as ACTIVEðx; yÞ.
c. Each cluster leader z, z 6¼ Lx , upon receiving an
ACK message ðy; ‘Þ, checks if z ¼ ‘. If so, then z
marks itself as ACTIVEðx; yÞ and sends an
ACK message ðy; ‘0 Þ via a long-haul transmission, where ‘0 is the node in the fourth field of
the RREQ stored at z.
Message transmission:
a.

In this section, we first describe our COB routing algorithm,
to be implemented on top of the clustering structure. We
then prove the performance bounds governing this routing
algorithm with respect to total elapsed time and total
number of messages exchanged. The routing algorithm
heavily relies on the constant density of the network of
cluster leaders in order to achieve a polynomial complexity
in terms of both time and number of messages exchanged.
Also, as we will see, other than requiring an underlying
clustering cover of the network at all times, the routing
algorithm presented is a purely on-demand algorithm.
Thus, in order for this routing algorithm to adapt to
mobility in an efficient way, all that is required is that the
underlying clustering structure be maintained efficiently
upon mobility. We have seen that the LCC algorithm—
which is our clustering algorithm of choice—adapts
optimally to mobility, namely, in Oð1Þ time per event [47].

1.

Node x starts by sending the message M and
destination y to its cluster leader Lx using a
short-haul transmission.
b. Suppose Lx receives the message ðy; MÞ from x
at time t ¼ 1. Node Lx forwards a route request
message (RREQ) of the form ðy; i; 2i ; Lx ; xÞ,
where 2i is the TTL of the message, at time
step 2  2i ¼ 2iþ1 , for i ¼ 0; 1; 2; . . . , to all of its
adjacent cluster leaders, using a long-haul
transmission.
c. Each cluster leader z receiving an RREQ ðy; i; k; ; Þ
for the first time checks whether z is y itself.
Otherwise, if k > 1, then z forwards an RREQ
with TTL equal to k  1 and its own id label, i.e.,
node z forwards the RREQ ðy; i; k  1; z; xÞ, to its
adjacent cluster leaders using a long-haul transmission. Node z keeps the just received RREQ for
broadcast round i and discards the stored RREQ
from round i  1, if any.
d. Each cluster leader that still has a stored RREQ
ðy; i; ; ; xÞ 2iþ1 time steps after the receipt of the
RREQ promptly discards the RREQ.
Route discovery: Acknowledging receipt of RREQ and
selecting ðx; yÞ-path:
a.

CLUSTER OVERLAY BROADCAST ROUTING

4.1 Description of COB Algorithm
In the description of the COB algorithm, we let Lz denote
the cluster leader of node z for any node z in the network.
We note that each node z in the network needs to be aware
of which node is its cluster leader Lz , which each node
learns during the establishment of the cluster cover. We also
note that COB does not require the cluster leader to
maintain a list of the regular nodes in its cluster.
Suppose a node x wants to send a message M to a node y
in the network. Node x initiates a route request for node y
as described below; all the other nodes in the network
follow the COB protocol described below:

657

3.

a.

If Lx receives an ACK ðy; Lx Þ, then Lx
Stops forwarding any RREQ messages
relative to M;
ii. Broadcasts the message ðy; M; xÞ to its
adjacent cluster leaders using a long-haul
transmission.
Each cluster leader z marked as ACTIVEðx; yÞ,
upon receiving a message ðy; M; xÞ forwards the
message ðy; M; xÞ via a long-haul transmission.
Upon forwarding the last packet carrying the
message M, z unmarks itself as ACTIVEðx; yÞ
and discards any ACK or RREQ messages it has
with respect to the message M.
i.

b.

658

IEEE TRANSACTIONS ON MOBILE COMPUTING,

Before we analyze the COB routing algorithm, we
discuss the key steps in more detail. First, note that the
successive broadcast rounds i, i ¼ 0; 1; 2; . . . , out of cluster
leader Lx in Step 1b are timed such that the next broadcast
round i þ 1 is only launched if the destination was not
reached in the current round i, which is ensured by setting
the timeout value for the next broadcast round to twice the
TTL field in the current broadcast. A second point to note
about Step 1b is that, for conceptual simplicity, in the
described algorithm, a given cluster leader uses only longhaul transmissions and these long-haul transmissions are
used to reach both the regular nodes around the cluster
leader as well as the adjacent cluster leaders. In particular,
broadcast round i ¼ 0 reaches all the regular nodes around
cluster leader Lx (i.e., the regular nodes for which Lx is the
cluster leader) as well as the cluster leaders adjacent to
(within the long-haul transmission range of) Lx . (Some
regular nodes from the clusters adjacent to Lx may also be
reached, but this is not relevant, as explained shortly. Also,
reaching the adjacent cluster leaders is only a side effect of
using the long-haul transmission. All we need to achieve in
round i ¼ 0 is to reach the regular nodes around Lx .)
Broadcast round i ¼ 1 reaches again all regular nodes
around Lx as well as all the regular nodes around the
cluster leaders that are adjacent to Lx (and, in turn, their
adjacent cluster leaders).
Instead of using only long-haul transmissions, the
broadcast rounds may be conducted with a mix of
1) short-haul transmissions to reach the regular nodes
around a given cluster leader and 2) long-haul transmissions to reach the adjacent cluster leaders. These two types
of transmissions can be conducted in different frequency
bands or with different CDMA codes, similar to [33], to
simplify the medium access control. When employing this
mix of short and long-haul transmissions, the broadcast
rounds proceed as follows: In round i ¼ 0, cluster leader Lx
broadcasts the RREQ with a short-haul transmission to all
of its regular nodes. In round i ¼ 1, cluster leader Lx
broadcasts the RREQ again with a short-haul transmission
to its regular nodes. In addition, Lx broadcasts the RREQ
with a long-haul broadcast to its adjacent cluster leaders,
which, in turn, broadcast the RREQ with short-haul
transmissions to their regular nodes, and so on in the
following rounds.
As an additional refinement, we can skip the repeated
short-haul transmissions, i.e., in round i ¼ 1, we skip the
short-haul transmission to the regular nodes around Lx
which have been reached in round i ¼ 0. In general, with
this additional refinement, only the cluster leaders
reached for the first time in a broadcast round, i.e., the
cluster leaders receiving route requests with TTL value
k ¼ 2i1 ; 2i1  1; . . . ; 1, forward the RREQ with a shorthaul transmission to their regular nodes. We note that
using the mix of short and long-haul transmissions and
the described additional refinement do not affect the
asymptotic complexity of COB (aside from affecting the
involved constants) as analyzed in the following section.
However, they tend to simplify the medium access
control [33].

VOL. 5,

NO. 6,

JUNE 2006

We remark about Step 2a that a regular node y processes
an RREQ as detailed in Step 2a only if the RREQ is received
from the node’s cluster leader Ly . RREQs directed to y that
are received from other cluster leaders (e.g., from long-haul
transmissions) are ignored.

4.2 Analysis of the COB Algorithm
The message complexity of a given distributed network
algorithm is given by the number of unit-size packet
transmissions throughout the execution of the algorithm.
The time complexity of a distributed algorithm is given by
the total elapsed time during the execution of the algorithm.
We let the jMj denote the size of a message M in number of
packets.
Theorem 2.
1.

2.

3.

Route Discovery: The time and message complexity of
the route discovery part of the COB routing protocol
are OðÞ and Oð2 Þ, respectively, where  is the
minimum hop distance between the source node x and
destination node y.
Message transmission: The time and message complexity of the actual message transmission from x to y are
both asymptotically optimal, i.e., OðjMjÞ.
Storage complexity: The COB routing protocol only
requires a constant amount of storage space at each
node in the network, which will be released once the
ðx; yÞ-routing is complete.

Proof. 1) Route discovery: We first prove the complexity of
the route discovery part of the COB algorithm. Suppose
node y was first reached during the ith broadcast round
originated at node Lx . Hence, the distance from Lx to Ly
must be at most 2i hops (only cluster leaders within 2i hops
from Lx are reached during the ith broadcast). The
broadcast rounds out of node Lx will end as soon as an
ACK is received by that node. The RREQ message that first
reached node y must have been sent before the ith round
was completed, i.e., at a time t  2iþ1 þ 2i ¼ Oð2i Þ, since
the ith broadcast commences at time step 2iþ1 and takes at
most 2i time steps to complete. The ACK sent out of node Ly
must have been sent at time t þ 1. Any ACK sent by the
algorithm goes from a node reachable from Lx in h hops to
a node reachable from Lx in h  1 hops (note that each
ACK has a specific node it is trying to reach, namely, the
one given by the “parent” field, i.e., the fourth field, in the
RREQ stored at the node sending the ACK). Thus, since Ly
is reachable from Lx after t0  2i time steps of the ith round,
it will take at most 2i time steps for the ACK originating at
Ly to reach Lx . Putting all these costs together, the route
discovery takes at most 2iþ1 þ 2i þ 2i þ 2 ¼ Oð2i Þ time
steps (the constant additive term comes from the fact that
there may be two additional communication steps
between Ly and y).
Since we know that y was not reached in the
ði  1Þth round, the long-haul distance between Lx and
Ly must be at least 2i1 long-haul hops, implying that the
short-haul distance between Lx and Ly must also be at
least 2i1 short-haul hops. Since any ðLx ; Ly Þ-short-haul
path of the form Lx ; x; . . . ; y; Ly is a candidate path for
being the path between Lx and Ly with the smallest

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

possible number of short-haul hops (which, as we have
seen, must be longer than or equal to 2i1 ), the short-haul
distance  between x and y has to be at least
2i1  2 ¼ ð2i Þ. Hence, the route discovery of the
COB algorithm takes time which is linearly proportional
on , i.e., OðÞ.
We now prove that the message complexity of the route
discovery is Oð2 Þ. In the ith broadcast round, each cluster
leader reached in this round sends at most one RREQ. All
cluster leaders reached in the ith broadcast round fit into a
disk of radius Oð2i Þ and, thus, by Corollary 1.2, there are
Oð22i Þ such cluster leaders. Hence, Oð22i Þ messages are
sent in round i. Hence, the totalP
number of messages sent
 2i
2
in all rounds of broadcast is Oð log
i¼0 2 Þ ¼ Oð Þ. Since
OðÞ ACKs are sent in the route discovery phase, the
message complexity follows.
2) Message transmission: The message transmission
phase only involves the nodes in the selected path from
Ly to Lx and each node in this path takes one time step to
forward each packet of M to the next node in the path.
We have seen that the selected path from Lx to Ly (and,
hence, the extension of this path that goes from x to y)
has OðÞ hops. Hence, the message and time complexity
of the message transmission phase are both OðjMjÞ.
3) Storage complexity: It remains to show that the
COB routing protocol only requires a constant amount of
storage space at each node in the network. Note that, at
any time during the execution of the algorithm, each
node (more specifically, each cluster leader) stores at
most one RREQ and has at most one ACTIVEðx; yÞ mark,
which are all of constant size. Also, the COB algorithm
does not use and, therefore, does not need to maintain
any cluster membership information at the cluster
leaders (neither does the LCC algorithm): The only
information necessary for the COB algorithm to work is
that each node z knows who its cluster leader Lz is,
which takes only a constant amount of space. All the
ACTIVEðx; yÞ marks are erased as the message M is
indeed transmitted from Lx to Ly and any RREQ is
released after all broadcasts from Lx are guaranteed to
have terminated.
u
t
Corollary 2.1. For fixed size messages, the overall time and
message complexity of the COB algorithm are OðÞ and
Oð2 Þ, respectively.
Proof. Adding up the asymptotic complexities of the route
discovery and message transmitting phases in view that
jMj ¼ Oð1Þ gives the result in this corollary.
u
t
Theorem 3. The COB routing protocol adapts asymptotically
optimally to the mobility of the nodes.
Proof. The COB algorithm is a purely on-demand algorithm,
provided we always maintain a clustering cover of the
network according to the LCC algorithm. Thus, any
updates upon mobility of the nodes must only be taken
care of at the clustering level. In [47], we have shown that
the update cost (total elapsed time of an update) of the
LCC algorithm is Oð1Þ, which is asymptotically optimal.
The number of messages exchanged during an update of
the clustering structure is linearly proportional to the
number of nodes left uncovered due to the mobility of a

659

node—i.e., if a node v moves and if, as a result, k nodes are
not covered by any cluster leader after the move (and
possible demotion of v as a cluster leader), then at
most OðkÞ messages will be sent in order to fix the
LCC clustering cover. Note that no deterministic algorithm can have a better message complexity for creating a
clustering cover of k nodes.
u
t
Combining the results in Theorems 2 and 3, we have that
the COB algorithm is the first routing algorithm for a
MANET of homogeneous nodes that, under the unit-disk
model, adapts optimally to mobility and that has time and
message complexities both polynomial on the distance
between source node and destination node.
We also note that the route found by the COB algorithm
is free from loops, which follows from the fact that an ACK
goes from a node reachable from Lx in h hops to a node
reachable from Lx in h  1 hops, as noted in the proof of
Theorem 2a. We furthermore note that the delay incurred
with COB is at most five times larger than the delay with
naive flooding over the network of cluster leaders. To see
this, consider a cluster leader Ly of the destination node that
is 2i þ 1 long-haul hops from the cluster leader Lx of the
source node. With our doubling radius broadcast, Ly is
reached in broadcast round i þ 1, which commences at
time 2iþ2 , and, in this broadcast round, it takes 2i þ 1 time
steps to reach Ly . Thus, the total delay to reach Ly is 2iþ2 þ
2i þ 1 time steps, which is ð5 þ 1=2i Þ=ð1 þ 1=2i Þ times larger
than the delay with naive flooding. We believe that this
larger delay is a reasonable trade-off for achieving a
bounded message complexity of Oð2 Þ.

4.3 Energy-Fair COB
In COB, the cluster leaders conduct all the intercluster
communication and do so using long-haul transmissions,
which consume more energy than short-haul transmissions,
as outlined at the end of Section 3. This may lead to unfairly
high energy consumption in nodes that act as a cluster
leader for long periods of time. To address this problem, we
propose a slight modification of the COB algorithm in order
to obtain an energy-fair COB algorithm, i.e., an algorithm
which aims at a fair usage of energy at all nodes.
The energy-fair COB algorithm is implemented as
follows: Instead of using the ID of a node as the tie-breaker
in the clustering algorithm, we use the remaining energy
level of a node as the tie breaker. Thus, when nodes
compete in a local region for becoming cluster leaders, the
one with the highest remaining energy level will win (if
there is still a tie between nodes with the same remaining
energy level, we can break this tie using the unique
node IDs). Whenever the power level of a cluster leader
node Lz drops below half of the power level of Lz at the
time it was elected cluster leader, then Lz demotes itself as a
leader node and starts a local update on the clustering of the
network. As discussed earlier, this local update will only
take Oð1Þ time. Note that, while the power levels of the
nodes in the network remain reasonably large, a cluster
leader will remain as cluster leader for a significant amount
of time and the local clustering updates due to energy drops
at the leader nodes will not be frequent. It is only when the
network comes to a very low energy level that there will be

660

IEEE TRANSACTIONS ON MOBILE COMPUTING,

a significant overhead due to frequent cluster leader swaps.
However, at this point, the network is basically at “the end
of its life” since the remaining energy at all nodes is indeed
coming to an end. We note that similar approaches have
been proposed in the context of clustering protocols, for
instance, in [48], [49].
The energy-fair COB algorithm is fair in terms of energy
usage to the nodes in the network, in the sense that the
nodes in a local neighborhood tend to converge to a
scenario where the energy levels of all the nodes fall in
between  and =2 for some energy level . If the network
communication patterns are uniform along the different
regions of the network, then we also expect this energy
level  to be roughly the same for the entire network.
We note that there are situations in which the energy
starvation of a node is unavoidable: Suppose there is a
region R of the network with very low density (e.g., if R is a
unit-disk and there are only a small constant number of
nodes in R) and suppose this region is a “bottleneck region”
in that it provides the only bridge between large, densely
populated parts of the network. Then, no matter how we
elect cluster leaders in R, we expect the energy consumption at the nodes in R to be much higher than that of the rest
of the network. This is because there are only a few nodes in
the bottleneck region R that can alternate in performing the
role of cluster leader.

5

SIMULATION RESULTS

In this section, we present simulation results to illustrate the
performance of the COB routing algorithm. We examine
three different aspects of the COB algorithms in the
simulations, namely, 1) the network layer performance of
COB, 2) the comparison of the network layer performance
of the well-known Dynamic Source Routing (DSR) algorithm with COB, and 3) the impact of the MAC layer on the
COB performance.

5.1 Network Layer Performance of COB
Our network layer simulation setup is similar to the route
discovery evaluation setup employed in [35] in that we
evaluate COB only with respect to the mobility process and
the size of the network. In particular, we consider an
idealized model of the MAC layer where transmissions
reach their destinations within one time step, as defined in
Section 3.1, and we simulate the route discovery sequentially. This ensures that we measure the network layer
performance of COB in isolation from any positive or
negative effects of the MAC layer or cross-traffic.
We conduct simulations for two scaling scenarios: 1) a
node density scaling scenario, where the area of the network
is a square of fixed size R ¼ 500 m by R ¼ 500 m and the
number of nodes N in the network is varied, and 2) a
network diameter scaling scenario, where we jointly scale
up the number of nodes N in the network and the diameter
of the network area. In particular, in the diameter scaling
scenario, we consider the configurations: N ¼ 250 nodes in
125 m by 125 m square area, N ¼ 500 nodes in 250 m by
250 square area, . . . , N ¼ 4; 000 nodes in 2,000 m by 2,000 m
square area. The goal of the diameter scaling is to investigate
the performance of the COB routing algorithm as the

VOL. 5,

NO. 6,

JUNE 2006

Fig. 1. Node density scaling: Length of discovered route as a function of
number of nodes N for different (fixed) network areas of R  R m2 and
short-haul transmission ranges P m.

shortest (short-haul) hop distance  between source and
destination increases. Clearly, it is computationally prohibitive to find the true shortest route, but it is reasonable to
assume that  scales approximately linearly with the
diameter of the network. Throughout, we consider the
two short-haul transmission ranges P ¼ 25 m and P ¼ 50 m
and corresponding long-haul transmission ranges of 75 m
and 150 m. We conduct simulations for both the random
walk (RW) and the random waypoint (RWP) mobility
models with a mobile speed of 10 m/sec; the pause time
for the random waypoint mobility model is 10 seconds.
Since a route discovery typically takes on the order of tens or
hundreds of milliseconds with the idealized MAC layer,
whereas changes in the cluster cover due to node mobility
take place on the time scale of typically tens of seconds, we
approximate the node positions as static during a given
route discovery. The practical deployment of the
COB routing protocol would, of course, require that node
changes in the cluster cover that affect an ongoing route
discovery are properly recovered from.
In the simulations, we consider the COB algorithm
employing only long-haul transmissions in the broadcast
rounds, as detailed in Section 4.1. We sequentially conduct
several stochastically independent route discoveries between randomly uniformly chosen source and destination
node pairs and collect statistics on the number of messages
transmitted and the time elapsed for the route discovery.
We also collect the statistics on the number of hops of the
route found by the COB algorithm. We continue each
simulation until the 95 percent confidence intervals are
smaller than 10 percent of the corresponding sample means
of the measures of interest.
In Figs. 1, 2, and 3, we consider the node density scaling
scenario and plot the delay for a route discovery (in time
steps) and the number of message transmissions per route
discovery. The plotted results are for the random walk
mobility model. We observe from Fig. 1 that, for a 500 m by
500 m square and a short-haul transmission range of 50 m,
the discovered route has on average four hops. Typically,
such a four hop route consists of one short-haul hop to go
from regular source node x to its cluster leader Lx , then
two long-haul hops to reach the cluster leader Ly of the
destination node y, and one more hop to go from Ly to y.
Generally, we observe that the length of the discovered

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

Fig. 2. Node density scaling: Delay for a route discovery as a function of
number of nodes N for different (fixed) network areas of R  R m2 and
short-haul transmission ranges P m.

route and the delay for the route discovery do not change
significantly as the number of nodes N in the fixed network
area increases, i.e., as the node density increases. An
exception is the 1,000 m by 1,000 m network with a shorthaul transmission range of 25 m, where the route length and
delay decrease as the number of nodes increases from 500 to
1,000. This effect is due to “uncovered” areas in the network
of 500 nodes that are not bridged by the short radio
transmission range of the relatively few nodes and require a
route around the uncovered area, i.e., the routes tend to be
more crooked and less straight in this scenario. In any case,
Theorem 2b ensures that the discovered route is asymptotically linear in the shortest possible route.
From Fig. 3, we observe that the number of messages
transmitted for a route discovery generally tends to initially
increase and then level off as the node density increases.
This effect is most pronounced for the network with the
large area and the small transmission range. This effect is
due to the initially increasing number of clusters as the
network becomes more populated. Once the entire network
area—or, more precisely, the entire disk centered at a given
source node with radius required to reach the given
destination node with the doubling radius technique—is
covered by clusters, there is no further increase in the
number of messages. Theorem 2a guarantees that the
number of messages is at most quadratic in the shortest

Fig. 3. Node density scaling: Number of message transmissions for a
route discovery as a function of number of nodes N for different (fixed)
network areas of R  R m2 and short-haul transmission ranges P m.

661

Fig. 4. Diameter scaling: Length of discovered route as a function of
number of nodes N with proportional (R  N) network area of R  R m2
for different (fixed) short-haul transmission ranges P m and mobility
patterns (random walk (RW) and random waypoint (RWP)).

source to destination hop distance irrespective of the overall
network size.
In Figs. 4, 5, and 6 we consider the diameter scaling
scenario for the random walk (RW) and random waypoint
(RWP) mobility models. We observe from Fig. 4 and Fig. 5
that the length of the discovered routes and the route
discovery delay increase linearly as we jointly scale up the
diameter and number of nodes in the network. Also, we
observe from Fig. 6 that the number of messages transmitted for a route discovery appears to increase quadratically with the diameter of the network, which, in turn,
gives a good indication of the shortest source-destination
hop distance . We note that, in our simulation set-up, the
source node and destination node are drawn uniformly
randomly on the network area. Thus, with expanding
network area, the source and destination nodes are
increasingly further apart, giving rise to the observed
scaling behaviors of the delay and message complexity,
which reflect our theoretical results (see Theorem 2a). It is
important to note that the time and message complexity of
COB depend only on the shortest source-destination
distance and not on the overall network dimensions.
We also observe that both the random walk and the
random waypoint mobility models result in the same
underlying asymptotic trends in the hop distance, delay,

Fig. 5. Diameter scaling: Delay for a route discovery as a function of the
number of nodes N with proportional (R  N) network area of R  R m2
for different (fixed) short-haul transmission ranges P m and mobility
patterns (random walk (RW) and random waypoint (RWP)).

662

IEEE TRANSACTIONS ON MOBILE COMPUTING,

Fig. 6. Diameter scaling: Number of message transmissions for a route
discovery as a function of the number of nodes N with proportional
(R  N) network area of R  R m2 for different (fixed) short-haul
transmission ranges P m and mobility patterns (random walk (RW)
and random waypoint (RWP)).

and message complexity. This is to be expected from our
analysis of the COB algorithm, which is general in that it
does not assume any specific mobility model, ensuring that
our theoretical results hold for any mobility behavior. The
somewhat lower hop distance, delay, and number of
messages for the random waypoint model observed in the
plots are due to the slight tendency for the nodes to more
densely populate the center of the network area with the
random waypoint model, resulting in somewhat lower
constants in the asymptotic scaling behavior.

5.2 Comparison with DSR
In this section, we compare the network layer performance
of the COB route discovery process with the well-known
Dynamic Source Routing (DSR) algorithm [50], [51]. We
consider the node density scaling scenario where N nodes
are uniformly distributed on an area of R ¼ 500 m by
R ¼ 500 m. Nodes are freely moving in the area according
to the random way point model with a randomly
distributed speed in the range from 10-20 m/s and a pause
time of 30 seconds. We consider the two transmission
ranges P ¼ 50 m and P ¼ 100 m in DSR, which we consider
to correspond to the short haul transmission ranges in COB.
We consider the two performance metrics’ normalized
routing load, which we define as the number of packets
(messages) transmitted per data packet delivered to the
destination, and mean delay, which we define as the
number of time steps required to deliver a data packet
from source to destination.
We observe from Figs. 7 and 8 that, as already observed
above, the delay and the normalized routing load (message
complexity) do not increase with the number of nodes in
COB. On the other hand, we observe that the delay in DSR
very slightly decreases as the number of nodes increases.
This effect is caused by the slightly more crooked routes
around uncovered areas with a small number of nodes,
which diminishes with increasing network density. Note
that DSR has a small delay for 100 nodes with a
transmission range of P ¼ 50 m. This is caused by the loss
of some data packets due to the network not always being
fully connected with only 100 nodes, whereby we count the
delay only for successfully delivered packets. In other

VOL. 5,

NO. 6,

JUNE 2006

Fig. 7. Network layer comparison of COB and DSR: Delay for packet
delivery as a function of number of nodes N for different (fixed)
transmission ranges P (in m); pause time = 30 s, network area
500  500 m2 , fixed.

words, the part of the network that is connected and allows
for successful delivery tends to have a somewhat smaller
diameter.
When comparing the delay values of COB and DSR, it is
important to keep in mind that, in the considered setting,
the transmission range in DSR corresponds to the shorthaul transmission range between a regular node and its
cluster leader in COB. The long-haul transmissions in COB
between adjacent cluster leaders have three times the range
of the transmissions in DSR. Hence, it may take three times
as long with DSR to traverse the same distance as traversed
with a long-haul transmission in COB. As observed in Fig. 7,
the delay in time step units with DSR is between two and
three times the delay with COB. This indicates that DSR
may approximately achieve the same delay performance as
a version of COB that uses short-haul transmissions via
gateway nodes between adjacent cluster leaders.
We observe from Fig. 8 that the normalized routing load
increases approximately linearly with the node density with
DSR, which is caused by the increasing number of nodes
within the transmission range of each given node, resulting
in a larger total number of transmitted messages in the route
discovery process. With increasing transmission range P ,
the normalized routing load decreases with both routing
approaches, which is primarily due to the shorter hop
distance between source and destination with the larger
transmission range. Considering the scaling behavior of the

Fig. 8. Network layer comparison of COB and DSR: Normalized routing
load as a function of number of nodes N for different (fixed) transmission
ranges P (in m); pause time = 30 s, network area 500  500 m2 , fixed.

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

Fig. 9. Network layer comparison of COB and DSR: Delay for packet
delivery as a function of pause time; transmission range P ¼ 100 m,
number of nodes N ¼ 500, network area 500  500 m2 , fixed.

number of transmitted messages with DSR in comparison
with COB reveals the benefit of the constant density
clustering used in COB and indicates that it may be
worthwhile to formally examine constant density clustering
in the context of DSR (initial simulation explorations of
clustering in conjunction with DSR are reported in [52], [53]).
Fig. 9 and Fig. 10 show the mean delay and normalized
routing load as a function of the pause time in the mobility
model for N ¼ 500 nodes and a transmission range of
P ¼ 100 m. Since COB discovers a route from scratch
whenever a node has data to send, the mean delay and
the routing load are essentially independent of the pause
time. With DSR, on the other hand, the delay and the
routing load decrease as the pause time increases. This is
because the cached routing table in DSR becomes stale less
frequently with increasing pause time. For large pause
times, the delay with DSR (which uses only short-haul
transmissions) approaches the delay with COB (which uses
short-haul transmissions between regular source/destination node and cluster leader and long-haul transmissions
between cluster leaders), indicating that DSR may in fact
give lower delays than a version of COB which uses only
short-haul transmissions (via up to two gateway nodes
between adjacent cluster leaders). This indicates that
employing DSR’s route caching mechanism on top of COB
may result in an overall improved routing performance (at

Fig. 10. Network layer comparison of COB and DSR: Normalized routing
load as a function of pause time; transmission range P ¼ 100 m, number
of nodes N ¼ 500, network area 500  500 m2 , fixed.

663

Fig. 11. Node density scaling: The length of the discovered route as a
function of the number of nodes N with 802.11 and idealized MAC layers;
network area 500  500 m2 , short-haul transmission range P ¼ 50 m,
fixed.

the expense of higher storage complexity in the nodes,
which would need to be formally examined in future work).

5.3 Performance of COB over 802.11 MAC Layer
To assess the interactions of COB with the MAC layer and
the performance with cross-traffic, we conducted simulations with a model of the 802.11 MAC layer that includes
cross-traffic, packet collisions, and movement during the
route discovery phase. In these simulations with the
MAC layer, we consider a short-haul transmission range of
P ¼ 50 m and corresponding long-haul transmission range
of 150 m. We consider the delivery of 512 Byte data packets
and set the transmission rate to 2 Mbps. Data packets are
generated at each node according to an independent Poisson
process with a rate that ensures that all cluster leaders, each
of which works on one RREQ broadcast at a time, are always
backlogged. Throughout, the nodes move according to the
random waypoint mobility model with 1 m/s. Following
[33], we assumed different frequency bands for the intracluster communication inside the individual clusters and
the intercluster communication among adjacent cluster
leaders. Our simulation model considers the distributed
coordination function (DCF) of 802.11, which employs
carrier sense multiple access with collision avoidance
(CSMA/CA). We did not employ request-to-send/clear-tosend (RTS/CTS) reservations for the RREQ packets to avoid
the reservation overhead for these short packets. If the
doubling radius flooding passed an estimated maximum
network diameter before reception of an ACK, the cluster
leader moved on to the next queued request.
We considered the same two scaling scenarios as in
Section 5.1 and report statistics on the number of hops in the
discovered route, the delay for the packet delivery and the
throughput (number of successfully delivered packets in
100 seconds of simulated network operation). Several
statistically independent network operation periods were
simulated and the obtained 95 percent confidence intervals
are displayed in the plots.
The first series of results in Figs. 11, 12, and 13 for the
node density scaling scenario indicate, in general, similar
trends for the 802.11 MAC layer results and the idealized
MAC layer results (which correspond to the network layer
performance results from Section 5.1). We observe from

664

IEEE TRANSACTIONS ON MOBILE COMPUTING,

VOL. 5,

NO. 6,

JUNE 2006

Fig. 12. Node density scaling: Delay for a packet delivery as a function of
number of nodes N with 802.11 and idealized MAC layers; network area
500  500 m2 , short-haul transmission range P ¼ 50 m, fixed.

Fig. 14. Diameter scaling: The length of the discovered route as a
function of the number of nodes N with proportional (R  N) network
area of R  R (in m2 ) with 802.11 and idealized MAC layer.

Fig. 11 that, on average, the discovered path lengths are
somewhat longer with the 802.11 MAC layer. This effect is
due to the collisions which may prevent routings from
discovering the shortest path first. The overall trend,
however, is for the discovered path length to remain
relatively constant in both scenarios as the node density
increases.
We observe from Fig. 12 that the packet delivery delay
initially increases with the node density and then flattens
out for a high node density. This behavior resembles the
behavior of the delays with the idealized MAC layer,
which are constant when scaling up the node density, as
shown in Fig. 2. The absolute values of the 802.11 MAC
layer delays are significantly larger than the delays with
the idealized MAC layer. Higher density networks require
our 802.11 MAC layer to spend more time rebroadcasting
to successfully complete each one-hop transmission. Importantly, the scaling behavior of the delay with the
802.11 MAC layer for increasing node density indicates
that Theorem 2a still holds in that the delay (time
complexity) is constant with respect to increasing node
density. However, a more efficient MAC layer could most
likely reduce the absolute values of the delays.
Fig. 13 gives the throughput in number of successfully
delivered packets in 100 seconds of network operation. We
observe an initial decrease in the throughout and then
flattening out for high node densities. This behavior is
primarily due to the increasing number of adjacent cluster

leaders with increasing node density. More specifically, as
the node density increases, so does, initially, the number of
adjacent cluster leaders. More adjacent cluster leaders result
in more collisions in the long-haul transmissions, which, in
turn, result in decreased throughput as seen in Fig. 13 (and
increased delay as seen in Fig. 12). As noted in Section 3, the
LCC cluster algorithm ensures that there are no more than
49 adjacent cluster leaders irrespective of the node density.
As the node density increases further, the number of
adjacent leaders approaches a maximum value and correspondingly the throughput approaches a constant level, as
observed in Fig. 13.
Next, we examine the diameter scaling scenario, in
which we jointly scale up the network area and number of
nodes to achieve an approximately linear increase of the
shortest hop distance . We observe from Fig. 14 that, with
both MAC models, the lengths of the discovered routes
increase approximately linearly, tracking the increase of the
shortest hop distance . We observe from Table 1 a
tendency toward linear scaling of the delay with both
models, whereby the absolute delay values are orders of
magnitude larger with the 802.11 MAC layer.
The throughout plotted in Fig. 13 again exhibits the
initial drop, which is primarily due to the increasing hop
distance (see Fig. 14), but then stabilizes even for large
networks and correspondingly large hop distances. For the
smallest simulated networks, the overlay network is quite
compact, and hop distances often include only one or
two long-haul transmissions. This vastly reduces congestion
due to cross traffic, allowing for higher throughput. For
TABLE 1
Diameter Scaling: Delay for a Packet Delivery as a Function of
Number of Nodes N with Proportional (R  N) Network Area of
R  R (in m2 ) with 802.11 and Idealized MAC Layer

Fig. 13. Network density and diameter scaling: Throughput of successfully delivered data packets in 100 s with 802.11 MAC layer.

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

larger networks, the overlay network of cluster leaders
grows proportionally with the increasing number of nodes
and network area in the diameter scaling scenario, allowing
for relatively stable throughput levels even with increasing
hop distance.

6

CONCLUSIONS

We have developed and formally analyzed the Cluster
Overlay Broadcast (COB) routing algorithm for MANETs.
COB runs on top a cluster cover of the network with a
constant density of cluster leaders, which we have proven
can be maintained by the Least Cluster Change (LCC)
algorithm. COB discovers routes with a doubling radius
broadcast on the overlay network of cluster leaders. We
note that the underlying mechanisms (routing on top of
cluster cover, doubling radius technique) have also been
examined in the context of the AODV routing protocol
through simulations (see, for instance, [24], [32]). We have
formally shown that, by exploiting the constant density of
the network of cluster leaders and the doubling radius
technique, COB has a time complexity that is linear in the
shortest source-destination hop distance and a message
complexity that is quadratic in the shortest source-destination distance. Importantly, we have also shown that COB
adapts optimally to the mobility of the nodes and has
constant storage complexity in the nodes. Our theoretical
results complement the existing simulation studies on
MANET routing mechanisms and provide insight into the
fundamental underpinnings of the performance of the
routing mechanisms employed in COB and other protocols,
such as AODV. To the best of our knowledge, COB is the
first MANET routing algorithm that has been formally
shown 1) to adapt optimally to the node mobility and 2) to
have time and message complexities that are polynomial in
the source-destination node distance and independent of
the overall network size (total number of nodes, total
diameter of network).
Our simulation results demonstrate that COB incurs
essentially a constant delay as the number of nodes in a
fixed network area (network density) is scaled up both
when considering only the network layer as well as the
network layer combined with an elementary 802.11 MAC
layer. Also, with increasing network density, the number of
message transmissions for a route discovery increases only
until the network area is fully covered with clusters and
then remains constant for further increasing node density.
Our simulation results have also demonstrated that the
delay scales linearly with the source-destination distance
and have indicated that the number of messages scales
quadratically with the source-destination distance.
There are several broad areas for exciting future work on
MANET routing and extensions to COB. One area is to
integrate MANET routing in general and the COB algorithm in particular, with higher layer notions of network
services, such as service discovery or location and context
aware services. In this context, it is very interesting to
examine the integration of routing with content distribution
mechanisms, e.g., for providing multimedia services to
MANET nodes. Another area is to develop cross-layer
designs that integrate several network layers, possibly

665

ranging from the application layer, including the network
layer, and reaching down to the medium access and
physical layers. Exploiting the specific characteristics of
the wireless medium access and physical layers, similar to
the approaches in [54], [55], [56], [57], appears especially
promising in these cross-layer designs. Throughout, we
believe it is vital to pay close attention to and formally
understand the scaling behaviors of the MANET protocols.

ACKNOWLEDGMENTS
The authors are grateful to the careful consideration of an
earlier version of this manuscript by the three anonymous
reviewers and their thoughtful comments on it, which
helped them to significantly improve the quality of this
paper. This work was supported in part by the US National
Science Foundation through grant Career ANI-0133252 and
Career 9985284 and by the Kyungnam University Research
Fund, 2005. Please direct correspondence to M. Reisslein. At
the time of this work, H.-S. Yang was with the Department
of Electrical Engineering, Arizona State University, Tempe.

REFERENCES
[1]
[2]
[3]
[4]
[5]

[6]
[7]
[8]
[9]
[10]

[11]
[12]
[13]

[14]
[15]

R. Wattenhofer, “Ad-Hoc and Sensor Networks: Worst-Case vs.
Average-Case,” Proc. Int’l Zurich Seminar Comm., pp. 156-159,
2004.
M. Abolhasan, T. Wysocki, and E. Dutkiewicz, “A Review of
Routing Protocols for Mobile Ad Hoc Networks,” Ad Hoc
Networks, vol. 2, no. 1, pp. 1-22, Jan. 2004.
X. Hong, K. Xu, and M. Gerla, “Scalable Routing Protocols for
Mobile Ad Hoc Networks,” IEEE Network, pp. 11-21, July/Aug.
2002.
C.A. Santivanez, B. McDonald, I. Stavrakakis, and R. Ramanathan,
“On the Scalability of Ad Hoc Routing Protocols,” Proc. IEEE
Infocom, pp. 1688-1697, June 2002.
A. Iwata, C.-C. Chiang, G. Pei, M. Gerla, and T.-W. Chen,
“Scalable Routing Strategies for Ad Hoc Wireless Networks,”
IEEE J. Selected Areas in Comm., vol. 17, no. 8, pp. 1369-1379, Aug.
1999.
R. Rajamaran, “Topology Control and Routing in Ad Hoc
Networks: A Survey,” SIGACT News, vol. 33, pp. 60-73, 2002.
B. Karp and H.T. Kung, “GPSR: Greedy Perimeter Stateless
Routing for Wireless Networks,” Proc. ACM MobiCom, pp. 243254, Aug. 2000.
Y.-B. Ko and N.H. Vaidya, “Location-Aided Routing (LAR) in
Mobile Ad Hoc Networks,” Wireless Networks, vol. 6, no. 4, pp. 307321, 2000.
S.-C.M. Woo and S. Singh, “Scalable Routing Protocol for Ad Hoc
Networks,” Wireless Networks, vol. 7, no. 5, pp. 513-529, 2001.
J. Broch, D. Maltz, D. Johnson, Y.-C. Hu, and J. Jetcheva, “A
Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols,” Proc. ACM Mobicom, pp. 85-97, Oct.
1998.
A. Boukerche, “Performance Evaluation of Routing Protocols for
Ad Hoc Wireless Networks,” Mobile Networks and Applications,
vol. 9, pp. 333-342, 2004.
I. Chlamtac, M. Conti, and J.J.N. Liu, “Mobile Ad Hoc Networking: Imperatives and Challenges,” Ad Hoc Networks, vol. 1, no. 1,
pp. 13-64, July 2003.
W. Choi and S.K. Das, “Design and Performance Analysis of a
Proxy-Based Indirect Routing Scheme in Ad Hoc Wireless
Networks,” Mobile Networks and Applications, vol. 8, no. 5,
pp. 499-515, 2003.
S.R. Das, R. Castaeda, and J. Yan, “Simulation-Based Performance
Evaluation of Routing Protocols for Mobile Ad Hoc Networks,”
Mobile Networks and Applications, vol. 5, no. 3, pp. 179-189, 2000.
J.J. Garcia-Luna-Aceves, M. Mosko, and C. Perkins, “Efficient OnDemand Loop-Free Routing in Ad Hoc Networks,” Proc. ACM
Symp. Principles of Distributed Computing (PODC), July 2003.

666

IEEE TRANSACTIONS ON MOBILE COMPUTING,

[16] S.-J. Lee, J. Hsu, R. Hayashida, M. Gerla, and R. Bagrodia,
“Selecting a Routing Strategy for Your Ad Hoc Network,”
Computer Comm., vol. 26, no. 7, pp. 723-733, May 2003.
[17] E.M. Belding-Royer and C.E. Perkins, “Evolution and Future
Directions of the Ad Hoc On-Demand Distance-Vector Routing
Protocol,” Ad Hoc Networks, vol. 1, no. 1, pp. 125-150, July 2003.
[18] S.J. Philip, J. Ghosh, S. Khedekar, and C. Qiao, “Scalability
Analysis of Location Management Protocols for Mobile Ad Hoc
Networks,” Proc. IEEE Wireless Comm. and Networking Conf.,
pp. 183-188, Mar. 2004.
[19] A. Sankar and Z. Liu, “Maximum Lifetime Routing in Wireless
Ad-Hoc Networks,” Proc. IEEE Infocom, Mar. 2004.
[20] A. Srinivas and E. Modiano, “Minimum Energy Disjoint Path
Routing in Wireless Ad-Hoc Networks,” Proc. ACM MobiCom,
pp. 122-133, 2003.
[21] J. Sucec and I. Marsic, “Clustering Overhead for Hierarchical
Routing in Mobile Ad Hoc Networks,” Proc. IEEE Infocom,
pp. 1698-1706, June 2002.
[22] H. Wu and A. Abouzeid, “Cluster-Based Routing Overhead in
Networks with Unreliable Nodes,” Proc. IEEE Wireless Comm. and
Networking Conf. (WCNC), Mar. 2004.
[23] G. Pei, M. Gerla, X. Hong, and C. Chiang, “A Wireless
Hierarchical Routing Protocol with Group Mobility,” Proc. IEEE
Wireless Comm. and Networking, pp. 1538-1542, Sept. 1999.
[24] E.M. Belding-Royer, “Multi-Level Hierarchies for Scalable Ad Hoc
Routing,” Wireless Networks, vol. 9, no. 5, pp. 461-478, 2003.
[25] C.-C. Chiang and M. Gerla, “Routing and Multicast in Multihop,
Mobile Wireless Networks,” Proc. IEEE Int’l Conf. Universal
Personal Comm., pp. 546-551, Oct. 1997.
[26] Z. Haas and M.R. Pearlman, “The Performance of Query Control
Schemes for the Zone Routing Protocol,” IEEE/ACM Trans.
Networking, vol. 9, no. 4, pp. 427-438, 2001.
[27] M. Jiang, J. Ji, and Y.C. Tay, “Cluster Based Routing Protocol,”
internet draft, work in progress, 1999.
[28] P. Krishna, N.H. Vaidya, M. Chatterjee, and D.K. Pradhan, “A
Cluster-Based Approach for Routing in Dynamic Networks,”
SIGCOMM Computer Comm. Rev., vol. 27, no. 2, pp. 49-64, 1997.
[29] R. Ramanathan and M. Steestrup, “Hierarchically Organized
Multihop Mobile Wireless Networks for Quality of Service
Support,” Mobile Networks and Applications, vol. 3, no. 1, pp. 101119, 1999.
[30] P. Sinha, R. Sivakumar, and V. Bharghavan, “CEDAR: A CoreExtraction Distributed Ad Hoc Routing Algorithm,” Proc. IEEE
Infocom, pp. 202-209, Mar. 1999.
[31] Y. Yi and M. Gerla, “Scalable AODV with Efficient Flooding Based
on On-Demand Clustering,” ACM Mobile Computing and Comm.
Rev., vol. 6, no. 3, pp. 98-99, 2002.
[32] S.-J. Lee, E.M. Belding-Royer, and C.E. Perkins, “Scalability Study
of the Ad Hoc On-Demand Distance Vector Routing Protocol,”
Int’l J. Network Management, vol. 13, no. 2, pp. 97-114, 2003.
[33] K. Xu and M. Gerla, “A Heterogeneous Routing Protocol Based on
a New Stable Clustering Scheme,” Proc. IEEE Milcom, pp. 838-843,
Oct. 2002.
[34] R. Castaneda and S. Das, “Query Localization Techniques for OnDemand Routing Protocols in Ad Hoc Networks,” Proc. ACM
MobiHoc, pp. 186-194, 1999.
[35] H. Dubois-Ferriere, M. Grossglauser, and M. Vetterli, “Age
Matters: Efficient Route Discovery in Mobile Ad Hoc Networks
Using Encounter Ages,” Proc. ACM MobiHoc, pp. 257-266, June
2003.
[36] J. Eriksson, M. Faloutsos, and S. Krishnamurthy, “Scalable Ad Hoc
Routing: The Case for Dynamic Addressing,” Proc. IEEE Infocom
2004, Mar. 2004.
[37] C. Gui and P. Mohapatra, “SHORT: Self-Healing and Optimizing
Routing Techniques for Mobile Ad Hoc Networks,” Proc. ACM
MobiHoc, pp. 279-290, 2003.
[38] K. Alzoubi, P.-J. Wan, and O. Frieder, “New Distributed
Algorithm for Connected Dominating Set in Wireless Ad Hoc
Networks,” Proc. 34th Ann. Hawaii Int’l Conf. System Science
(HICSS-35), 2002.
[39] D. Baker, A. Ephremides, and J.A. Flynn, “The Design and
Simulation of a Mobile Radio Network with Distributed Control,”
IEEE J. Selected Areas in Comm., vol. 2, no. 1, pp. 226-237, Jan. 1984.
[40] S. Basagni, “Distributed and Mobility-Adaptive Clustering for
Multimedia Support in Multi-Hop Wireless Networks,” Proc. IEEE
Vehicular Technology Conf., pp. 19-22, 1999.

VOL. 5,

NO. 6,

JUNE 2006

[41] G. Calinescu, I.I. Mandoiu, P.-J. Wan, and A.Z. Zelikovsky,
“Selecting Forwarding Neighbors in Wireless Ad Hoc Networks,”
Mobile Networks and Applications, vol. 9, no. 2, pp. 101-111, 2004.
[42] M. Chatterjee, S.K. Das, and D. Turgut, “WCA: A Weighted
Clustering Algorithm for Mobile Ad Hoc Networks,” Cluster
Computing, vol. 5, no. 2, pp. 193-204, 2002.
[43] A.B. McDonald and T.F. Znati, “A Mobility-Based Framework for
Adaptive Clustering in Wireless Ad Hoc Networks,” IEEE J.
Selected Areas in Comm., vol. 17, no. 8, pp. 1466-1487, Aug. 1999.
[44] X.-Y. Li, Y. Wang, P.-J. Wan, and O. Frieder, “Localized Low
Weight Graph and Its Applications in Wireless Ad Hoc Networks,” Proc. IEEE Infocom 2004, Mar. 2004.
[45] O. Younis and S. Fahmy, “Distributed Clustering in Ad-Hoc
Sensor Networks: A Hybrid, Energy-Efficient Approach,” Proc.
IEEE Infocom 2004, Mar. 2004.
[46] C.-C. Chiang, H.-K. Wu, W. Liu, and M. Gerla, “Routing in
Clustered Multihop, Mobile Wireless Networks with Fading
Channel,” Proc. IEEE Singapore Int’l Conf. Networks, pp. 197-211,
1997.
[47] H. Huang, A.W. Richa, and M. Segal, “Approximation Algorithms
for the Mobile Piercing Set Problem with Applications to
Clustering in Ad Hoc Networks,” Mobile Networks and Applications
(MONET), vol. 9, no. 2, pp. 151-161, Apr. 2004.
[48] A.D. Amis and R. Prakash, “Load-Balancing Clusters in Wireless
Ad Hoc Networks,” Proc. IEEE Symp. Application-Specific Systems
and Software Eng. Technology, pp. 25-32, Mar. 2000.
[49] A. Safwat, H. Hassanein, and H. Mouftah, “Power-Aware Fair
Infrastructure Formation for Wireless Mobile Ad Hoc Communications,” Proc. IEEE Globecom, pp. 2832-2836, Nov. 2001.
[50] D.B. Johnson, D.A. Malts, and J. Broch, “DSR: The Dynamic
Source Routing Protocol for Multi-Hop Wireless Ad Hoc Networks,” Ad Hoc Networking, C.E. Perkins, ed., chapter 5, pp. 139172, Addison-Wesley, 2001.
[51] D.B. Johnson, D.A. Malts, and Y.-C. Hu, “The Dynamic Source Routing Protocol for Mobile Ad Hoc Networks (DSR),” IETF Internet
Draft, technical report, July 2004, draft-ietf-manet-dsr-10.txt.
[52] M. Soyturk, E. Cayirci, and A.E. Harmanci, “Virtual Cell Layout
Based Dynamic Source Routing Algorithm for the Mobile
Subsystem of the Next Generation Tactical Communications
Systems,” Proc. IEEE Milcom, pp. 541-545, Oct. 2002.
[53] Y. Yi, M. Gerla, and T.J. Kwon, “The Selective Intermediate Nodes
Scheme for Ad Hoc On-Demand Routing Protocols,” Proc. IEEE
Int’l Conf. Comm., pp. 3191-3195, May 2002.
[54] A. Muqattash and M.M. Krunz, “A Distributed Transmission
Power Control Protocol for Mobile Ad Hoc Networks,” IEEE
Trans. Mobile Computing, vol. 3, no. 2, pp. 113-128, Apr.-June 2004.
[55] B. Sadeghi, V. Kanodia, A. Sabharwal, and E. Knightly, “Opportunistic Media Access for Multirate Ad Hoc Networks,” Proc.
ACM/IEEE Mobicom, Sept. 2002.
[56] J.A. Stine and G. de Veciana, “A Paradigm for Quality-of-Service
in Wireless Ad Hoc Networks Using Synchronous Signaling and
Node States,” IEEE J. Selected Areas in Comm., vol. 22, no. 7,
pp. 1301-1321, Sept. 2004.
[57] X. Yang and N. Vaidya, “On Physical Carrier Sensing in Wireless
Ad Hoc Networks,” Proc. IEEE Infocom, Mar. 2005.
Luke Ritchie received the BS and MS degrees
in electrical engineering from Arizona State
University (ASU), Tempe, in 2003 and 2004,
respectively. He is currently a PhD student at
ASU. His research interests are in the areas of
interaction between medium access control
(MAC) and routing in mobile ad hoc networks
(MANETs) and wireless sensor networks
(WSNs). He is a student member of the IEEE.

RITCHIE ET AL.: CLUSTER OVERLAY BROADCAST (COB): MANET ROUTING WITH COMPLEXITY POLYNOMIAL IN SOURCE-DESTINATION...

Hyo-Sik Yang received the BE degree from the
Department of Information and Communication
Engineering at Myongji University, Yongin,
Korea, in 1998 and the MS degree and the
PhD degree in electrical engineering from the
Arizona State University, Tempe, in 2000 and
2005, respectively. Currently, he is an assistant
professor in the Division of Electrical and
Electronic Engineering, Kyungnam University,
Masan, Korea. His research interests are wavelength division multiplexing (WDM) all-optical networks, WDM packet
switching, and WDM metropolitan area networks, including node
architecture, optimization, medium access control (MAC), and traffic
analysis, routing protocol in mobile ad hoc networks (MANETs), and
substation automation system using IEC 61850. He is a member of the
IEEE.
Andréa W. Richa received the BS degree in
computer science and the MS degree in
computer systems from the Graduate School in
Engineering (COPPE), both from the Federal
University of Rio de Janeiro, Brazil, in 1992 and
1990, respectively. She also received the MS
and PhD degrees from the School of Computer
Science at Carnegie Mellon University in 1995
and 1998, respectively. She has been an
associate professor in the Department of Computer Science and Engineering at Arizona State University, Tempe,
since August 2004. She joined this department as an assistant professor
in August 1998. Her main area of research is in network algorithms.
Some of the topics she has worked on include packet scheduling,
distributed load balancing, packet routing, mobile network clustering and
routing protocols, and distributed data tracking. Her data tracking (or
name lookup) algorithm has been widely recognized as the first
benchmark algorithm for the development of distributed databases in
peer-to-peer networking, having been referenced by more than
130 academic journal or conference publications to date and implemented as part of two of the current leading projects in peer-to-peer
networking. She was the recipient of a US National Science Foundation
CAREER Award in 1999. For a selected list of her publications, CV, and
current research projects, please visit http://www.public.asu.edu/aricha.

667

Martin Reisslein received the MSE degree from
the University of Pennsylvania, Philadelphia, in
1996 and the Dipl-Ing (FH) degree from the
Fachhochschule Dieburg, Germany, in 1994,
both in electrical engineering. He received the
PhD degree in systems engineering from the
University of Pennsylvania in 1998. During the
academic year 1994-1995, he visited the University of Pennsylvania as a Fulbright scholar.
From July 1998 through October 2000, he was a
scientist with the German National Research Center for Information
Technology (GMD FOKUS), Berlin, and lecturer at the Technical
University of Berlin. From October 2000 through August 2005, he was
an assistant professor at Arizona State University (ASU), Tempe.
Currently, he is an associate professor in the Department of Electrical
Engineering at ASU. He is editor-in-chief of the IEEE Communications
Surveys and Tutorials and has served on the technical program
committees of IEEE Infocom, IEEE Globecom, and the IEEE International Symposium on Computer and Communications. He has organized
sessions at the IEEE Computer Communications Workshop (CCW). He
maintains an extensive library of video traces for network performance
evaluation, including frame size traces of MPEG-4 and H.263 encoded
video, at http://trace.eas.asu.edu. He is corecipient of the Best Paper
Award of the SPIE Photonics East 2000—Terabit Optical Networking
conference. His research interests are in the areas of Internet Quality of
Service, video traffic characterization, wireless networking, optical
networking, and engineering education. He is a senior member of the
IEEE.

. For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

2015 IEEE 35th International Conference on Distributed Computing Systems

Competitive Strategies for Online Cloud
Resource Allocation with Discounts
The 2-Dimensional Parking Permit Problem
Xinhui Hu1 , Arne Ludwig2 , Andrea Richa1 , Stefan Schmid2,3
1

Computer Science, SCIDSE, Arizona State University, Tempe, AZ 85287, USA
TU Berlin, Germany 3 Telekom Innovation Laboratories (T-Labs), Germany

2

that its resource demand is not known in advance (e.g.,
it depends on the popularity of its website). In order
to ensure that its resource demand is satisﬁed at any
time, and in order to avoid a costly over-provisioning of
the service, additional resources must be bought in an
online manner. The online resource allocation problem
may further be complicated by the fact that the provider
offers discounts for larger and longer resource contracts.
The goal of the customer is to come up with a
smart resource renting strategy to satisfy its dynamic
and unpredictable resource demand, while minimizing
the overall costs of the bought resource bundles.
Our Contributions. This paper shows that at the heart
of efﬁcient cloud resource allocation lies a fundamental
algorithmic problem, and makes the following contributions. We ﬁrst observe that the problem of renting a
single resource over time can be seen as a 2-dimensional
variant of the well-known online Parking Permit Problem
(PPP). While in the classic parking permit problem,
only the contract durations need to be chosen, in the 2dimensional variant PPP2 introduced in this paper, also
the resource rates are subject to optimization.
Our main contribution is the deterministic online algorithm O N 2D whose performance is close to the one
of a clairvoyant optimal ofﬂine algorithm which knows
the entire resource demand in advance: given some
simplifying assumptions (stated in Section II), O N 2D
provably achieves a competitive ratio of O(k), where k
is the total number of available resource contracts; this
is asymptotically optimal in the sense that there cannot
exist any deterministic online algorithm with competitive
ratio o(k).
We also give a constructive proof that the ofﬂine
variant of the PPP2 problem can be solved in polynomial
time, by presenting a dynamic programming algorithm
O FF 2D accordingly. To the best of our knowledge,
O FF 2D is also the ﬁrst ofﬂine algorithm to efﬁciently
solve PPP and PPP2 for long enough request sequences.

Abstract—Cloud computing heralded an era where resources can be scaled up and down elastically and in an online manner. This paper initiates the study of cost-effective
cloud resource allocation algorithms under price discounts,
using a competitive analysis approach. We show that for
a single resource, the online resource renting problem can
be seen as a 2-dimensional variant of the classic online
parking permit problem, and we formally introduce the
PPP2 problem accordingly. Our main contribution is an
online algorithm for PPP2 which achieves a deterministic
competitive ratio of k (under a certain set of assumptions),
where k is the number of resource bundles. This is almost
optimal, as we also prove a lower bound of k/3 for any
deterministic online algorithm. Our online algorithm makes
use of an optimal ofﬂine algorithm, which may be of
independent interest since it is the ﬁrst optimal ofﬂine
algorithm for the 1D and 2D versions of the parking permit
problem. Finally, we show that our algorithms and results
also generalize to multiple resources (i.e., multi-dimensional
parking permit problems).

I. I NTRODUCTION
As the Internet becomes increasingly virtualized, resources can be allocated more ﬂexibly and at large
scale. Virtualization not only introduces an Internet-wide
resource market, but also new business models. For
example, a startup company running a webservice no
longer needs to invest in its own infrastructure, but can
dynamically lease cloud resources to provide the service
to its users in a cost-efﬁcient manner. Also a hybrid
model is possible where the cloud resources are just
used to complement a limited own infrastructure in peak
demand times (a.k.a. “cloud bursting”). New business
models are also introduced by resource brokering opportunities: a broker may lease a large amount of resources
from different providers and resell them to its customers
(at a higher price).
This paper studies the problem of a (cloud) customer
who rents resource bundles from a (cloud) provider, in
order to offer a certain service to its users (or to resell
the resources). The customer is faced with the challenge
1063-6927/15 $31.00 © 2015 IEEE
DOI 10.1109/ICDCS.2015.18

93

that resource contracts have a monotonically increasing
rate-duration product r × d, and will denote by Ci the
ith largest contract.
A speciﬁc contract instance of type Ci will be de(j)
(j)
noted by Ci , for some index j. Each instance Ci
(j)
of contract type Ci (r, d) has a speciﬁc start time ti ,
and we will sometimes refer to a contract instance
(j) (j)
by Ci (ti , ri , di ). The identiﬁers are needed since
multiple contracts of the same type can be “stacked”
in our model, but will be omitted if the contract is clear
from the context.
We will make three simplifying assumptions:
A1 Monotonic Prices: Prices are monotonically increasing, i.e., larger contracts are more expensive
than shorter contracts: π(Ci−1 ) < π(Ci ) since
ri−1 × di−1 < ri × di for any i.
A2 Multiplicity: The duration and resource rate of a
contract of type Ci (ri , di ) are perfect multiples
of the duration and rate, respectively, of contract
Ci−1 (ri−1 , di−1 ). That is, we assume that di =
x · di−1 and ri = y · ri−1 for ﬁxed bases x, y ≥ 2.
Moreover, without loss of generality (w.l.o.g.), we
will assume that the smallest contract has d1 = 1
and r1 = 1.
A3 Intervals: We assume that a contract of duration d
can only be bought at time t0 + i · d, where t0 = 0
is the start time.
Assumption A1 is natural: contracts which are dominated by larger, cheaper contracts may simply be ignored. Assumption A2 restricts the variety of available
contracts the customer can choose from, and constitutes
the main simpliﬁcation made in this paper. Assumption A3 mainly serves the ease of presentation: as we
will prove in this paper (and as it has already been shown
for the classic Parking Permit Problem [13]), an ofﬂine
or online algorithm limited by the interval model is at
most a factor of two off the respective optimal solution
in the general model.
Now, let O N be some online algorithm, let Ct (O N)
denote the set of contracts bought according to O N
at time t and let C≤t (O N) denote the set of contracts
bought according to O N through time t. We will use the
notation Ct∗ (O N) ⊆ C≤t (O N) to describe the set of active
contracts at time t: i.e., contracts Ci (ti , ri , di ) bought
by O N with ti ≤ t < ti + di . Likewise we denote the
set of contracts bought by an optimal ofﬂine algorithm
O FF to cover the demand preﬁx σ1 , ..., σt until time t
by C≤t (O FF).
Since a correct algorithm must ensure that there are
always sufﬁcient
 resources to cover the current demand,
the invariant C(r,d)∈C ∗ (O N) r ≥ σt must hold at any

Figure 1.
Overview of the model: A customer has to cover its
resource demand σ by buying contracts C = {C1 , C2 , . . . , Ck } from
the provider. Larger contracts Ci come with a price discount (price
π(Ci )).

O FF 2D is used as a subroutine in O N 2D.
Finally, we show that our algorithms and results also
generalize to multi-resource scenarios, i.e., to higherdimensional parking permit problems.
Paper Organization. The remainder of this paper
is organized as follows. Section II formally introduces
our model. Section III presents our online algorithm.
Section IV discusses an example and provides intuition
for the analysis, and Section V presents the general
analysis. We show that our algorithm is almost optimal
by deriving a lower bound in Section VI. Section VII
presents a polynomial-time optimal ofﬂine algorithm,
and Section VIII shows how to extend our results from
2-dimensions to D-dimensions (for a constant D). We
report on simulation results in Section IX, review related literature in Section X, and conclude our work in
Section XI.
II. M ODEL
We attend to the following setting (for an illustration,
see Figure 1). We consider a customer with a dynamically changing resource demand. We model this resource
demand as a sequence σ = (σt )t , where σt refers to
the resource demand at time t. We use σ
 to denote
maxt σt . The customer is faced with the challenge that
its future resource requirements are hard to predict, and
may change in a worst-case manner over time: We are in
the realm of online algorithms and competitive analysis.
In order to cover its resource demand, the customer
buys resource contracts from a (cloud) provider. For
ease of presentation, we will focus on a single resource
scenario for most of our paper; however, we will also
show that our results can be extended to multi-resource
scenarios. Concretely, we assume that the provider offers
different resource contracts C(r, d) of resource rate r
and duration d. We will refer to the set of available
contracts by C = {C1 , C2 , . . . , Ck }. Each contract type
has a price π(C) = π(r, d), which depends on its rate
r(C) = r and its duration d(C) = d. We will assume

t

94

moment of time t. We will use the one-lookahead
model [3] frequently considered in online literature, and
allow an online algorithm to buy a contract at time t
before serving the request σt ; however, O N does not have
any information at all about σt for t > t.
The goal is to minimize the overall price πσ (O N) =
C∈C≤t (O N) π(C). More speciﬁcally, we seek to be
competitive against an optimal ofﬂine algorithm and
want to minimize the competitive ratio ρ of O N: We
compare the price πσ (O N) of the online algorithm O N
under the external (online) demand σ, to the price
πσ (O FF) paid by an optimal ofﬂine algorithm O FF,
which knows the entire demand σ in advance. Formally, we assume a worst-case perspective and want
to minimize the (strict) competitive ratio ρ for any σ:
ρ = maxσ πσ (O N)/πσ (O FF).
We are interested in long demand sequences σ; in
particular, we will assume that the length of σ, |σ|, is at
least as large as the largest single demand σt .
Our problem is a new variant of the classic Parking
Permit Problem PPP [13], which we review quickly in
the following. In PPP, a driver has to choose between k
parking permits of different durations and costs in order
to satisfy all of his/her parking needs while minimizing
the total cost paid. More precisely, the driver has a
sequence of days when he/she needs a parking space at a
parking garage and there are k different parking permits,
where each parking permit Pi allows the driver to use
one parking space for di consecutive days at a cost of
ci . In the online version of the problem, the sequence of
days when the driver will need a parking space is not
known in advance.

Figure 2. Worst-case example where σt = 1 ∀t. While O FF 2D,
at time d5 , buys a single contract C5 , O N 2D is forced to buy all the
depicted contracts, in addition to C5 . For instance, O N 2D buys C1 in
every second time step.

active contracts in such a way that longer contracts are
embedded lower in the plane, i.e., the longest running
contract Ci (ri , di ) covers the demand from 1 to ri , the
next shorter contract Cj (rj , dj ) then covers the demand
ri + 1 to ri + rj , and so on. This guarantees a unique
mapping of a demand point p(time, demand) at time t
to a contract Ci for the ofﬂine algorithm.
Algorithm 1 Online Algorithm O N 2D
Input: Demand preﬁx σ≤t = σ1 , σ2 , ..., σt ; set of contracts C≤t−1 (O N 2D) bought by O N 2D through time
t−1
Output: Contracts to be bought at time t: Ct (O N 2D)
1: C≤t (O FF 2D) ← O FF 2D(σ1 , σ2 , ..., σt )
2: for C ∈ C≤t (O FF 2D) do
3:
if ∃ demand point p covered by C such that p is
not covered by C≤t−1 (O N 2D) then
4:
Ct (O N 2D).add(C)
5: return Ct (O N 2D)

III. C OMPETITIVE O NLINE A LGORITHM
This section presents the deterministic online algorithm O N 2D for the PPP2 problem. As a subroutine,
in order to determine which contracts to buy at time
t, O N 2D uses an optimal ofﬂine algorithm O FF 2D
that computes optimal contracts for a preﬁx σ≤t of
the demand through time t. In this section, we will
treat O FF 2D as a black box, but we will describe a
polynomial-time construction later in Section VII of the
paper.
In order to formally describe and analyze our algorithm, we propose a scheme that assigns bought contracts
to the 2-dimensional time-demand plane. Our model
requires that each point below the demand curve σ is
covered by a contract, i.e., the mapping of contracts to
demand points must be surjective.
We pursue the following strategy to assign contracts
to the time-demand plane: at any time t, we order the
set of active contracts by their duration, and stack the

Our online algorithm O N 2D (see Algorithm 1) is
based on an oracle O FF 2D computing optimal ofﬂine
solutions for the demand so far. O N 2D uses these
solutions to purchase contracts at time t. Concretely,
O N 2D mimics the ofﬂine algorithm in an efﬁcient way,
in the sense that it only buys the optimal ofﬂine contracts covering time t if the corresponding demand is
not already covered by contracts bought previously by
O N 2D: At each time t, O N 2D compares the set of
previously bought contracts C≤t−1 (O N 2D) with the set
of contracts C≤t (O FF 2D) that O FF 2D would buy for
an ofﬂine demand sequence σ1 , ..., σt ; O N 2D then only
buys the contracts C ∈ C≤t (O FF 2D) for the demand at
time t that is not covered by C≤t−1 (O N 2D).
95

IV. E XAMPLE

Proof: Consider the discussed worst-case sequence
σ, where O N 2D has to buy every contract (total cost
Π(Ci )) while O FF 2D can simply buy Ci at price π(Ci ).
We can show that Π(Ci ) ≤ i · π(Ci ) and hence
Π(Ci ) ≤ k · π(Ci ). According to the observed behavior
of O FF 2D, every second contract bought by O N 2D is
C1 (2i−2 times), every fourth is C2 (2i−3 times), etc.,
and ﬁnally O N 2D also buys Ci . See Figure 2 for an
example. Thus,

In order to provide some intuition of the behavior
of O N 2D, as a case study, we consider the special
scenario where contracts are perfect squares, i.e., Ci =
(2i−1 , 2i−1 ), and where the contract prices have a speciﬁc discount structure, namely π(Ci ) = 2 · Ci−1 , with
π(C1 ) = 1. This price function ensures that O FF 2D will
buy at most one Cj contract before it is worthwhile to
buy the next larger contract Cj+1 for the given time
interval.
Let us now study the maximal cumulative price Π(Ci ).
It is easy to see that under the price function above,
the demand sequence σ with a constant demand of one
unit per time, maximizes Π(Ci ) for Ci = (2i−1 , 2i−1 )
and π(Ci ) = 2 · Ci−1 : higher demands imply missed
opportunities to charge O N 2D for smaller contracts, as
already a demand of two at given time t renders it
worthwhile to buy C2 , and a demand of four renders
it worthwhile to buy C3 , etc.
With the given demand σ, O FF 2D will end up buying
each of the smaller contracts once before it buys the
next larger contract. The cumulative price
from
derived
i−1
σ according to this behavior is Π(Ci ) = j=1 Π(Cj ) +
π(Ci ). We prove this claim by induction over the contract types i. For the base case i = 1, Π(Ci ) = π(Ci )
holds trivially. Assuming the induction hypothesis for i
we have:
Π(Ci+1 ) =

i


Π(Ci ) = 2i−2 · π(C1 ) + 2i−3 · π(C2 ) + · · · + 1 · π(Ci )
= 2i−2 · 20 + 2i−3 · 21 + · · · + 2i−i · 2i−2 + 1 · 2i−1
≤ 2i−1 + 2i−1 + 2i−1 + · · · + 2i−1 + 2i−1
= i · 2i−1 = i · π(Ci )

V. A NALYSIS : U PPER B OUND
With these intuitions in mind, we now present a
general analysis of O N 2D. First, we derive some simple
properties of the contracts bought by the optimal ofﬂine
algorithm O FF 2D over time. Let us ﬁx an arbitrary
demand point p, i.e., a point below the σ-curve in the
time-demand plane. We make the following claim: if p is
covered by a certain contract C in C≤t (O FF 2D), p will
never be covered by a smaller contract C  in Ct (O FF 2D)
for any t > t. In other words, when considering a longer
ofﬂine demand sequence σ1 , . . . , σt , O FF 2D will never
buy a smaller contract than C to cover the demand point
p. This property of “growing contracts” together with the
assumption of disjoint intervals motivates the notion of
contract independence, which we formalize in the lemma
below:

Π(Cj ) + π(Ci+1 )

j=1

=

i−1


Π(Cj ) + Π(Ci ) + π(Ci+1 )

j=1
Hyp.

=

i−1

j=1

Π(Cj ) +

i−1


Lemma 1 (Contract Independence). Consider a demand
point pi covered by contract Ci ∈ C≤t (O FF 2D) and a
demand point pj covered by a distinct contract Cj ∈
C≤t (O FF 2D). Then there does not exist a contract C ∈
Ct (O FF 2D) for any t < t such that pi , pj are covered
by C. We say that the two contracts Ci and Cj are
independent.

Π(Cj ) + π(Ci ) + π(Ci+1 )

j=1

Due to the induction hypothesis,
cost of a quarter
the
i−1
of 2i+1 × 2i+1 is maximized for j=1 Π(Cj ) + π(Ci ).
In order to maximize the cost in the second quarter (at
the bottom of
the time-demand plane) O FF 2D would
i−1
need to buy j=1 Π(Cj ) again, and instead of buying
a second contract Ci , the pricing scheme requires the
purchase of contract Ci+1 . Therefore, buying the same
contracts
again (despite Ci ) must lead to Π(Ci+1 ) =
i
Π(C
j ) + π(Ci+1 ).
j=1
In summary, we have derived a worst-case sequence
σ for the considered price function, for which O N 2D is
k-competitive.

Independence between contracts is trivially ensured in
our model. This allows us to introduce a simple characterization of the scenarios maximizing the competitive
ratio.
Lemma 2. The competitive ratio is maximized in a
scenario where O FF 2D buys only one contract to satisfy
the entire demand σ.
Proof: By contradiction. Assume O FF 2D buys more
than one contract, say Ci and Cj . Now assume that over
time, O N 2D buys a set of (possibly smaller) contracts

Theorem 1. For the special setting considered in our
case study, O N 2D is k-competitive.

96

Ci , Ci , . . . to cover the demand points of Ci and
Cj  , Cj  , . . . to cover the demand points of Cj . Thus,
O N 2D pays π(Ci )+π(Ci )+. . . and π(Cj )+π(Cj  )+. . .
whereas O FF 2D pays π(Ci ) and π(Cj ); the resulting
competitive ratio is ρCi = (π(Ci ) + π(Ci ) + . . .)/π(Ci )
for the Ci part and ρCj = (π(Cj )+π(Cj  )+. . .)/π(Cj )
respectively. Since all contracts in O FF 2D are independent, the competitive ratio ρ of O FF 2D will be
max{ρCi , ρCj }, which would also be the case if the
larger contract was the only one bought by O FF 2D.

Π(Ci ) =  · Π(Ci−1 ) + π(Ci ) +


(p)
Cj ∈S 

≤  · (i − 1) · π(Ci−1 ) + π(Ci ) +

(p)

Π(Cj )


j · π(Cj )

(p)
Cj ∈S 

≤  · (i − 1) · π(Ci−1 ) + π(Ci ) + (i − 2)


(p)

Cj

π(Cj )

∈S 

≤  · (i − 1) · π(Ci−1 ) + π(Ci )+
+ (i − 2) [π(Ci ) −  · π(Ci−1 )]
=  · π(Ci−1 ) + (i − 1) · π(Ci )
π(Ci )
≤
· π(Ci−1 ) + (i − 1) · π(Ci )
π(Ci−1 )
= π(Ci ) + (i − 1) · π(Ci ) = i · π(Ci )

We hence want to show that O N 2D will never buy
too many small contracts to cover a demand for which
O FF 2D would later only buy one contract. Concretely, let
us ﬁx any contract Ci ∈ C≤t (O FF 2D), and let us study
the set of contracts S bought by O N 2D during the time
interval [0, t) which overlap with Ci in the time-demand
plane. Recall that S will only contain distinct instances
of the contracts (since O N 2D does not buy “repeated”
contracts) and it will be contained in ∪t <t Ct (O FF 2D).
By the interval and independence property, we know that
contracts in S are all “inside” Ci , i.e., do not exceed its
boundary in the plane. Accordingly, we can compute an
upper bound on the maximum cumulative price spent on
contracts in S by O N 2D while O FF 2D at time t only
bought a single contract Ci at price π. In the following,
let us refer
to this cumulative price paid by O N 2D by
Π(Ci ) = C∈S π(C).

With these results, we can derive the competitive
(j)
ratio. According to Lemma 3, for each contract Ci ∈
(j)
C≤t (O FF 2D), the accumulated cost Π(Ci ) is bounded
by i · π(Ci ). Therefore, summing up all the accumulated
costs of each contract in C≤t (O FF 2D), we get the
total cost of O N 2D at time t. Note that every contract
bought by O N 2D must be totally covered by contracts
in Ct (O FF 2D), since Ct (O FF 2D) is an optimal solution
for the entire demand sequence σ≤t and the contract
independence property holds. Since we have k different
contracts and for each contract Ci in Ct (O FF 2D), we
have Π(Ci ) ≤ i · π(Ci ) ≤ k · π(Ci ), and:
Theorem 2. O N 2D is k-competitive, where k is the total
number of contracts.
As we will show in Section VI, this is almost optimal.
Finally, observe that restricting O N 2D to Assumption A3 does not come at a large cost.

Lemma 3. The maximum cumulative price paid by
O N 2D to cover a contract Ci , Π(Ci ), is less than or
equal to i · π(Ci ), for any i ≥ 0.

Theorem 3. Let A LG1 be an optimal ofﬂine algorithm
for PPP2 , and let A LG2 be an optimal ofﬂine algorithm for PPP2 where we relax Assumption A3. Then
π(A LG2 ) ≤ π(A LG1 ) ≤ 2 · π(A LG2 ).

Proof: Consider a contract Ci ∈ C≤t (O FF 2D)
and S as deﬁned above. Let  be such that O N 2D
has bought  contracts Ci−1 to cover the area of Ci
π(Ci )
during time [0, t), where 0 ≤  ≤ π(C
. For all
i−1 )
other C ∈ S, we must have C ∈ {C1 , . . . , Ci−2 }. Let
{C ∈ S, s.t. C ∈ {C1 , . . . , Ci−2 }}. Hence we
S  =
have C∈S  π(C) ≤ π(Ci ) −  · π(Ci−1 ), since the area
covered by all contracts in S is at most equal to the area
covered by Ci , and given Assumption A1. We argue by
induction on i.

(m)

Proof: Consider any contract Ci (ri , di ) bought
by an optimal ofﬂine algorithm for PPP2 without Assumption A3. When time is divided into intervals of
(m)
will overlap in time with at most two
length di , Ci
(j)
(l)
contracts Ci and Ci of duration di . Therefore, we
can modify the optimal solution output by A LG2 by pur(m)
chasing those two contracts instead of Ci , eventually
transforming the optimal solution output by A LG2 into
a feasible solution for PPP2 (under Assumption A3).
Hence, we can guarantee that π(A LG2 ) ≤ π(A LG1 ) ≤
2 · π(A LG2 ).
Hence, since O N 2D is k-competitive under Assumption A3 (Theorem 2), and since the optimal ofﬂine cost is

Base case i = 1: If there is just one type of contract
C1 , the online algorithm will buy the same contracts as
the ofﬂine algorithm, and the claim holds trivially.
Inductive step i > 1: Assuming the induction hypothesis holds for all j < i, we have:
97

full interval,i.e., /di contracts for any i: π(O FF) ≤
k
π(Ci )(mi + j≥i nj ). In order to derive the lower bound
we ﬁrst prove a minimum cost of any algorithm O N on
intervals that start with a demand rate of 1.
Lemma 4. Any O N must pay at least π(Ci ) on each
interval of length di that starts with a demand rate of 1.
Proof: By induction on the different intervals 2i−1 .
For i = 1, each algorithm must at least buy a contract
of type C1 in order to cover that demand. Assume that
for i − 1, it holds and now let us argue for i. If O N
does not buy a contract of type Ci , we can divide the
volume into (2k)2 squares with side length di−1 each,
where 2k · di−1 = di . We let each of these 2k intervals
(at the bottom row) start with a demand of 1 which then
cost at least π(Ci−1 ) due to the induction hypothesis.
The total cost is at least 2k · π(Ci−1 ) = k · π(Ci ) for
every interval where O N does not buy a contract i and
at least π(Ci ) otherwise.
Consider now an interval of length (2k)i−1 where no
contract of type i or higher was bought. We know from
the induction that π(O N) ≥ mi · k · π(Ci ). We can derive
the following lower bound:

Figure 3. O N buys ni = 4 contracts Ci and ni+1 = 1 contract
Ci+1 over seven intervals of length di . In two of these seven intervals
O N buys several contracts smaller than Ci to cover the demand.

at most a factor of two lower without the interval model
(Theorem V), we have:
Corollary 1. O N 2D is 2k-competitive for the general
PPP2 problem without Assumption A3, where k is the
number of contracts.
VI. L OWER B OUND
Theorem 2 is essentially the best we can hope for:
Theorem 4. No deterministic online algorithm can
achieve a competitive ratio less than k/3.
The proof is the 2-dimensional analogon of the proof
in [13]. We consider a scenario where the next larger
available contract doubles in cost. With k being the
number of different contracts, each contract is 2k times
longer and has 2k times more rate, i.e., in our plane
representation contracts are squares covering an area
(2k)2i .

k · π(O FF) ≤

k

i=0

≤

k

i=0

π(Ci ) = 2i−1

≤

r1 = 1; ri = 2k · ri−1 = (2k)i−1

k


⎡
⎣π(Ci )(mi +
⎡
⎣ni

k


⎤
nj ) ⎦

j≥i
i


(1)
⎤

π(Ci ) + mi · π(Ci )⎦

(2)

j=1

[2ni · π(Ci ) + mi · π(Ci )]

(3)

i=0

≤ 3 · π(O N)

d1 = 1; di = 2k · di−1 = (2k)i−1

(4)

Inequality (1) is given by the cost estimation of O FF
against any O N buying only one kind of contracts.
Inequality (2) is a reorganization of the sum since π(Ci )
is multiplied by every nj , j ≥ i which is also given
after the reordering. Afterwards, we use the geometric
sum on the cost of the contracts to derive Inequality (3).
This
k leads to a lower bound of k/3 since π(O N) =
i=1 ni · π(Ci ) and π(O N ) ≥ mi · k · π(Ci ).

In the following, let us focus on a simple demand
which only assumes rates σt ∈ {0, 1} for all t. We
let the adversary schedule demand only when O N has
no valid contract. For each interval (2k)i where the
adversary asks for a 1-demand, O N can choose between
three options (see also Figure 3):
1 Eventual purchase of contract Ci . Assume that this
happens ni times.
2 Eventual purchase of larger contracts Cj , j > i.
k
Assume that this happens j>i nj times.
3 Never purchase contract Ci or any larger contracts.
Assume this happens mi times.
Therefore the sumof all contracts bought by O N is
k
given by π(O N) = i=1 ni · π(Ci ). Given an interval
of length , we estimate the cost of O FF by less than
buying multiples of only one kind of contract over the

VII. O PTIMAL O FFLINE A LGORITHM
So far, we have treated the optimal ofﬂine algorithm
on which O N 2D relies as a black box. In the following,
we show that ofﬂine solutions can indeed be computed in
polynomial time, and present a corresponding dynamic
programming algorithm O FF 2D.
The basic idea behind the ofﬂine algorithm O FF 2D
is that the optimal cost for any contract over a certain

98

Algorithm 3 Ofﬂine Algorithm for dk -length interval
Input: Precomputed matrix M over interval [t, t + dk ].
Output: Optimal total costs OPT[i, j, ·] for all intervals
within [t, t + dk ].
1: Initialize all entries in OPT to be 0.
2: Let σ̂ = M [i, j].
3: for i = 1 to dk do
4:
for λ = M [i, i] − 1 to 0 do
5:
OPT[i, i, λ]
←
σ , λ+r}]+π(r, d)}
minC(r,d)∈C {OPT[i, i, min{
6: for  = 2 to dk do
7:
for i = 1 to dk −  + 1 do
8:
j =i+−1
9:
for λ = M [i, j] − 1 to 0 do
10:
OPT[i, j, λ]
←
+
mini≤z<j {OPT[i, z, min{M [i, z], λ}]
OPT[z + 1, j, min{M [z + 1, j], λ}]}
11:
C  ← {C (x) (t(x) , r, d) ∈ C : t(x) = b · d for
some positive integer b and t(x) ≤ i < j ≤
t(x) + d}
12:
if C  is not empty then
13:
OPT[i, j, λ]
←
min{OPT[i, j, λ];
minC(r,d)∈C  OPT[i, j, min{σ̂, λ + r}] +
π(r, d)}
14: return OPT[1, dk , 0]

Algorithm 2 Pre-computation of matrix M for dk -length
Input: Demand sequence σt , . . . , σt+dk (over interval
[t, t + dk ]).
Output: Matrix M .
1: for i = 1 to dk do
2:
M [i, i] ← σt+i
3: for i = 1 to dk − 1 do
4:
for j = i + 1 to dk do
5:
M [i, j] ← max{M [i, j − 1], σt+j }
6: return M
interval is obtained either by splitting the cost at some
time, or by buying a long contract with a certain rate
r. In the following, recall that dk is the duration of the
largest contract Ck .
O FF 2D proceeds as follows: It splits time into intervals of length dk and solves each of these interval
separately using Algorithm 3. O FF 2D relies on the following data structures: For each dk -length time interval
I, we precompute the maximum demand within any
subinterval [i, j] of I, and store this information in
position M [i, j] of a dk ×dk matrix M (Algorithm 2). In
particular the maximum requested demand σ̂ in interval
I is stored in M [1, dk ]. A dk × dk × σ̂ matrix OPT is
used to compute the optimal cost. The entry OPT[i, j, λ]
indicates the optimal cost of covering a demand rate of
M [i, j] − λ over the interval [i, j] — i.e. λ indicates the
amount of covered demand for [i, j]. Initially, all entries
are set to 0.
Algorithm 2 pre-computes the matrix M over the dk length interval [t, t+dk ], where t = b·dk , for integer b ≥
0. Lines 1-2 initialize the matrix and store the demand
σt+i in entry M [i, i]. Lines 3-5 compute the maximum
demand within any time interval [t + i, t + j], 0 ≤ i ≤
j ≤ dk . The demand can be obtained by comparing the
demand at time t + j (i.e., σt+j ) with the maximum
demand between time t + i and t + j − 1, which has
already been computed by our algorithm.
After obtaining the matrix M over interval [t, t + dk ],
we can compute the optimal solution for the PPP2
problem over the same interval using Algorithm 3, as
we show in Theorem 5:

demand λ. Clearly, the claim is true for intervals [i, i]
( = 1) (Lines 3-5): If λ > 0 we need at least
one contract C(r, d) to ﬁnish covering the demand at
time i; the remaining demand at time i not covered
by C must be covered optimally by other contracts, as
previously computed in OPT[i, i, λ + r]. Now consider
a subinterval [i, j] of length  = j − i + 1 ≥ 2, where
1 ≤ i ≤ j ≤ dk . This interval is either split into two
non-overlapping subintervals of smaller length (Case I),
or a long contract of length equal to or greater than
 that completely covers [i, j] is bought, at a certain
demand rate r, where 0 ≤ r ≤ M [i, j] (Case II). Given
Assumption A2 and A3, for any instances of contracts
(y)
(q)
Cx and Cp , either the duration of one contract is fully
contained in the other, or the two contracts never overlap
in time: Hence, given that we consider all intervals
[i, j], including the ones that may correspond to actual
instances of contracts, it is enough to consider only these
two cases.
In Case I, we split the interval at time z such
that the solution OPT[i, z, min{M [i, z], λ}] + OPT[z +
1, j, min{M [z + 1, j], λ}] is minimized over all z between i and j (Line 10). Since the lengths of the two
subintervals z − i + 1 and j − z are both smaller than ,

Theorem 5. Algorithm 3 computes an optimal ofﬂine
solution for any given interval of length dk in time O(d3k ·
σ
), where σ
 is the maximum demand over the interval.
Proof: We assume, for the sake of simplicity and
without loss of generality, that t = 0 and the dk -length
interval we consider is [0, dk ].
Correctness: By induction over the length of the
subintervals  = j − i + 1 and the respective uncovered
99

OPT[i, z, λ] and OPT[z +1, j, λ] already store the cost of
optimal solutions for these subproblems, respectively, by
the induction hypothesis. Hence OPT[i, z, λ] + OPT[z +
1, j, λ] will yield the optimal solution for OPT[i, j, λ] if
Case I applies.
In Case II, we buy a long contract with rate r. First, we
need to check which contracts with longer durations can
cover [i, j] fully, and store the candidate contracts in C  .
A candidate contract C (x) (t(x) , r, d), where t(x) = b · d
according to Assumption A3, satisﬁes t(x) ≤ i < j <
t(x) +d. The algorithm picks the valid candidate contract
that minimizes π(r, d) plus the optimal cost of covering
the largest remaining demand M [i, j] − (λ + r) over
[i, j], which has been previously computed and stored in
OPT[i, j, λ + r] (Line 11).
By choosing the smaller value of Cases I and II, we
obtain the optimal cost for subproblem [i, j, λ] (Line 13).
Time Complexity: The total time complexity of O FF 2D
for the pre-computation part in Algorithm 2 is O(d2k ).
The ﬁrst part of Algorithm 3 in (Lines 3-5) takes O(dk ·
k·σ
) time, where σ
 is the maximum demand for the
whole time interval. The ﬁrst two loops of the second
part (Lines 6-7) take O(d2k ) time and the for-loop in Line
9 takes O(
σ ) time. The statement in Line 10 requires
O(dk ) time and Lines 11 and 13 take time O(k) each.
) for a
Therefore, the total time complexity is O(d3k · σ
subinterval with length dk .
Taking Theorem 5 into account for all intervals of
length dk in σ, and for a long enough demand sequence
σ (i.e., such that |σ| = Ω(
σ ), where σ
 is the maximum
demand over σ), we get the following corollary, which
expresses the total running time of the ofﬂine algorithm:

M matrix in Algorithm 2 which indicates the current
demand dimension β, M [i, j, β] and run the algorithm
β times for the pre-computation. We illustrate those
changes below in 3D.
Assume a scenario where a third dimension is added,
e.g. computational and network resources over time. The
contracts C(r, r , d) then cover r × r × d cuboids. In
order to adjust Algorithm 3, we add another loop after
Line 4 which goes through the maximum values λ of
the additional demand (for λ = M [i, i, 2] − 1 to 0 do)
and change the statement in Line 5 to: OPT[i, i, λ, λ ] ←
minC(r,r ,d)∈C  OPT[i, i, λ + r, λ + r ] + π(r, r , d). The
same loop must also be added after Line 9 and the
updates of the OPT matrix must be changed accordingly
in Lines 10 and 13.
The runtime of the pre-computation in Algorithm 2
would be increased by a factor of D (i.e., by the dimension of the problem) and still be negligible regarding
the overall runtime (assuming D is a constant). For
Algorithm 3 the runtime would increase by a factor
i , where σ i is the maximum demand for
of Πi≥2 σ
resource i, for i ≥ 1, leading to an overall runtime of
i ) for each interval dk .
O(d3k · Πi≥1 σ
No changes are needed regarding O N 2D. It still mimics O FF 2D’s behavior and given the Assumptions A2
and A3, the contract independence still holds for higher
dimensions. Hence, the proof for the competitive ratio
of k still applies.
IX. S IMULATIONS
We have conducted a small simulation study to complement our formal analysis. In this simulation, we
consider k square contracts where Ci (ri , di ) has rate and
duration ri = di = 2i−1 , for 1 ≤ i ≤ k. The price π
of a contract is a function of the rate-duration product
ri · di , and we study a parameter x to vary the discount.
Concretely, we consider a scenario where a twice as large
time-rate product is by factor (1 + x) more expensive,
i.e., π(2 · d · r) = (1 + x) · π(d · r); we set π(1) = 1.
To generate the demand σ, we use a randomized
scheme: non-zero demand requests arrive according to a
Poisson distribution with parameter λ, i.e., the time between non-zero σt is exponentially distributed. For each
non-zero request, we sample a demand value uniformly
at random from the interval [1, y].
Each simulation run represents 1000 time steps, and
is repeated 10 times.
Impact of the request model. We ﬁrst study how the
competitive ratio depends on the demand arrival pattern.
Figure 4 plots the competitive ratio ρ as a function of the
Poisson distribution parameter λ. The price model with
x = 0.5 is used, and there are k = 8 contract types. First,

Corollary 2. Algorithm O FF 2D runs in time O(|σ|2 d2k ).
Proof: By summing up the computation time of
	|σ|/dk 
 subintervals of length dk , we have an over) = O(|σ|2 d2k ), since
all complexity of O(|σ| · d2k · σ
|σ| = Ω(
σ ).
VIII. H IGHER D IMENSIONS
O FF 2D and O N 2D are designed for the twodimensional version of the PPP problem but they can
also be extended towards a D-dimensional version of the
problem, where each additional dimension (other than
the time duration dimension) would indicate the rate
at which you would buy a certain resource. Regarding
O FF 2D we need to do the following changes: For each
additional dimension we need to extend the dimension
of the optimal cost matrix OPT by one and add two
additional loops in O FF 2D’s Algorithm 3. Furthermore
we only need to add one additional dimension for the

100

4.5

4.5

y=128,k=8,λ=2

x=0.5,y=128,k=8
4
4

3.5

ρ

ρ3.5

3

2.5
2

3

1.5
2.5

1

Figure 4.

2

3

4

5

6

λ

7

8

9

1

10

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

x

Figure 5.

Effect of request distribution (Poisson λ).

Effect of discount x.

4.5
x=0.5,y=128,λ=2
4

we observe that the competitive ratio ρ is bounded by
approx. 5, which is slightly lower than what we expect
in the worst-case (cf Theorem 2). Another observation is
that the competitive ratio decreases as λ increases. This
can be explained by the fact that demand rates become
sparser for increasing λ, and hence less contracts will be
bought. Meanwhile, when the demand rates are sparse,
the ofﬂine algorithm will have less chance to buy a larger
contract. Put differently, the online algorithm will pay
relatively more compared to the ofﬂine algorithm for
small λ, as it purchases more small contracts.

3.5

ρ

3

2.5
2
1.5
1

3

4

5

6

7

8

9

k
Figure 6.

Impact of the price model. Different price models also affect the purchasing behavior of our online
algorithm. Figure 5 shows the competitive ratio ρ for
different values x. (For this scenario, we set y = 128,
k = 8 and λ = 2.) We see a tradeoff: for small x, until
x = 0.5, the competitive ratio increases and then begins
to decrease again. The general trend can be explained by
the fact that for small x, it is worthwhile to buy larger
contracts earlier, and it is hence impossible to charge
O N 2D much; for larger x, also an ofﬂine solution cannot
proﬁt from buying a large contract.

Effect of the number of contracts k

X. R ELATED W ORK
Cost reductions (due to economy-of-scale effects) are
one of the main motivations behind today’s trend to outsource infrastructure and software to the cloud. A large
body of literature in the ﬁeld focuses on resource allocation and scheduling problems. For a good overview, we
refer the reader to the surveys [2], [5].
Compared to the algorithmic problems of the resource
allocation and scheduling, the economical aspects are
less well-understood. Different economical cloud models
have been proposed and compared by various authors,
e.g., by Armbrust et al. [12], Pal et al. [14], or Dash et
al. [6]. Some of the studied pricing models have their
origins in the context of ISP-customer relationships [17]
and are also related to classic economic problems [8].
An interesting tradeoff between time and price has
been studied in [10], from a scheduling complexity
perspective. More generally, there are several interesting
proposals for novel adaptive resource and spot market

Impact of the number of contracts. Finally, Figure 6
shows the competitive ratio as a function of the number
of contracts k. (We ﬁx x = 0.5, y = 128 and λ = 2 in
this simulation.) The competitive ratio ﬁrst increases as
k increases but then stabilizes. This stabilization is due
to the fact that when we have eight or more contracts
(k ≥ 8), the largest contract can cover the maximum
rate. In the beginning, the ratio increases since the ofﬂine
algorithm buys larger and larger contracts, and the online
algorithm pays for many small contracts along the way.
101

pricing schemes, e.g., [1]. Our model is motivated by
architectures such as [4], [16] which allow for subrenting and recursion.
This paper assumed an online algorithm and competitive analysis perspective. Many online models, such as
ski-rental problems [3], facility location problems [9],
or buy-at-bulk [15] and rent-or-buy [11] problems, assume that once an item has been purchased, it remains
indeﬁnitely for no extra charge. The Parking Permit
Problem [13] and the Bahncard Problem [7] are the
archetype for online problems where purchases have
time durations which expire regardless of whether the
purchase is used or not.
The paper closest to ours is the Parking Permit
Problem (PPP) paper by Meyerson [13]. Formally, PPP
speciﬁes a set of k different types of parking permits
of a certain price πi and duration di . In [13], Meyerson
presents an asymptotically optimal deterministic online
algorithm with competitive ratio O(k) (together with
a lower bound of Ω(k)). The paper also discusses
randomized algorithms and an application to Steiner
forests. While we can build upon some of the techniques
in [13], the rate dimension renders the problem different
in nature, both from an online and an ofﬂine algorithm
perspective.

(which is needed for the concept of contract independence in the upper bound proof of Section V)
is a more promising direction for future research.
3) Randomized algorithms: It will be interesting to
study whether the randomized algorithms known
from the classic parking permit problem can also
be generalized to multiple dimensions.
Acknowledgments. This research was supported by
the BMBF (01IS12056).
R EFERENCES
[1] V. Abhishek, I. A. Kash, and P. Key. Fixed and market pricing
for cloud services. In Proc. NetEcon Workshop, 2012.
[2] M. Armbrust, A. Fox, R. Griﬁth, A. D. Joseph, R. H. Katz,
A. Konwinski, G. Lee, D. A. Patterson, A. Rabkin, I. Stoica,
and M. Zaharia. Above the clouds: A berkeley view of cloud
computing. In UC Berkeley Technical Report EECS-2009-28,
2009.
[3] A. Borodin and R. El-Yaniv. Online computation and competitive
analysis. Cambridge University Press, 1998.
[4] R. Buyya, C. S. Yeo, S. Venugopal, J. Broberg, and I. Brandic.
Cloud computing and emerging it platforms: Vision, hype, and
reality for delivering computing as the 5th utility. Elsevier FGCS,
25(6), 2009.
[5] N. M. K. Chowdhury and R. Boutaba. A survey of network
virtualization. Computer Networks, 54:862–876, 2010.
[6] D. Dash, V. Kantere, and A. Ailamaki. An economic model for
self-tuned cloud caching. In Proc. IEEE International Conference
on Data Engineering, pages 1687–1693, 2009.
[7] R. Fleischer. On the bahncard problem. Theor. Comput. Sci.,
268(1):161–174, 2001.
[8] S. Goyal and B. Giri. Recent trends in modeling of deteriorating
inventory. Elsevier EJOR, 134(1), 2001.
[9] S. Guha and K. Munagala. Improved algorithms for the data
placement problem. In Proc.13th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 106–107, 2002.
[10] T. A. Henzinger, A. V. Singh, V. Singh, T. Wies, and D. Zufferey.
A marketplace for cloud resources. In Proc. 10th ACM EMSOFT,
2010.
[11] A. Kumar, A. Gupta, and T. Roughgarden. A constant-factor
approximation algorithm for the multicommodity rent-or-buy
problem. In Proc. 43rd Symposium on Foundations of Computer
Science (FOCS), 2002.
[12] M. Armbrust et al. A view of cloud computing. Commun. ACM,
53(4):50–58, 2010.
[13] A. M. Meyerson. The parking permit problem. In Proc. 46th
Annual IEEE Symposium on Foundations of Computer Science
(FOCS), pages 274–284, 2005.
[14] R. Pal and P. Hui. Economic models for cloud service markets.
In Proc. ICDCN, 2012.
[15] F. S. Salman, J. Cheriyan, R. Ravi, and S. Subramanian. Buyat-bulk network design: Approximating the single-sink edge
installation problem. In Proc. 8th Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 619–628, 1997.
[16] G. Schaffrath, C. Werle, P. Papadimitriou, A. Feldmann, R. Bless,
A. Greenhalgh, A. Wundsam, M. Kind, O. Maennel, and
L. Mathy. Network virtualization architecture: Proposal and
initial prototype. In Proc. ACM SIGCOMM VISA, 2009.
[17] S. Shakkottai and R. Srikant. Economics of network pricing with
multiple isps. IEEE/ACM TON, 14(6), 2006.

XI. S UMMARY AND C ONCLUSION
This paper has shown that at the heart of efﬁcient
cloud resource allocation lies a fundamental algorithmic
problem, and we introduced the PPP2 problem, a 2dimensional variant of the online Parking Permit Problem PPP. We presented a deterministic online algorithm
O N 2D that provably achieves a competitive ratio of
k, where k is the total number of available contracts;
if we relax Assumption A3, the competitive ratio of
our algorithm is 2k. We also showed that O N 2D is
almost optimal in the sense that no deterministic online
algorithm for PPP2 can achieve a competitive ratio lower
than k/3. Finally, we proved that the ofﬂine version of
PPP2 can be solved in polynomial time.
We believe that our work opens interesting directions
for future research.
1) Optimality: The obvious open question regards the
gap between the upper bound k and the lower
bound k/3 derived in this paper.
2) Relaxing the assumptions: While the interval
model only comes at the cost of a small additional
approximation factor (a constant), it seems hard to
remove the assumption entirely but still being able
to compute optimal solutions in polynomial time:
we conjecture that the problem is NP-hard. We
believe that relaxing the multiplicity assumption

102

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

Competitive MAC under Adversarial SINR
Adrian Ogierman1 , Andrea Richa2 , Christian Scheideler1 , Stefan Schmid3 , Jin Zhang2
1

2

Department of Computer Science, University of Paderborn, Germany; {adriano,scheideler}@upb.de
Computer Science and Engineering, SCIDSE, Arizona State University, USA; {aricha,jzhang82@asu.edu}@asu.edu
3
TU Berlin & Telekom Innovation Labs, Berlin, Germany; stefan@net.t-labs.tu-berlin.de

Abstract—This paper considers the problem of how to efﬁciently share a wireless medium which is subject to harsh external
interference or even jamming. While this problem has already
been studied intensively for simplistic single-hop or unit disk
graph models, we make a leap forward and study MAC protocols
for the SINR interference model (a.k.a. the physical model).
We make two contributions. First, we introduce a new adversarial SINR model which captures a wide range of interference
phenomena. Concretely, we consider a powerful, adaptive adversary which can jam nodes at arbitrary times and which is only
limited by some energy budget. The second contribution of this
paper is a distributed MAC protocol which provably achieves a
constant competitive throughput in this environment: we show
that, with high probability, the protocol ensures that a constant
fraction of the non-blocked time periods is used for successful
transmissions. Our results also highlight an inherent difference
between the SINR model and unit disk graph models.

interference producing a non-Gaussian noise such as electrical
devices, temporary obstacles, co-existing networks [34], or
jamming attacks. Also, these sources can severely degrade the
availability of the wireless medium which can put a signiﬁcant
stress on MAC protocols that have only been designed to
handle interference from the nodes themselves. In order to
capture a very broad range of noise phenomena, one of
the main contributions of this work is the modeling of the
background noise N (due to jamming or to environmental
noise) with the aid of an adversary ADV that has a ﬁxed
energy budget within a certain time frame for each node v.
More precisely, in our case, a message transmitted by a node
u will be successfully received by node v if and only if

I. I NTRODUCTION

where ADV(v) is the current noise level created by the adversary at node v. Our goal will be to design a MAC protocol
that allows the nodes to successfully transmit messages under
this model as long as this is in principle possible. Prior to our
work, no MAC protocol has been shown to have this property.
Model. We assume that we have a static set V of n wireless
nodes that have arbitrary ﬁxed positions in the 2-dimensional
Euclidean plane so that no two nodes have the same position.
The nodes communicate over a wireless medium with a
single channel. We also assume that the nodes are backlogged
in the sense that they always have something to broadcast.
Each node sends at a ﬁxed transmission power of P , and a
message sent by u is correctly
 received by v if and only if
(P/d(u, v)α )/(ADV(v) + w∈S P/d(w, v)α ) ≥ β. For our
formal description and analysis, we assume a synchronized
setting where time proceeds in synchronized time steps called
rounds. In each round, a node u may either transmit a message
or sense the channel, but it cannot do both. A node which is
sensing the channel may either (i) sense an idle channel, (ii)
sense a busy channel, or (iii) receive a packet. In order to
distinguish between an idle and a busy channel, the nodes use
a ﬁxed noise threshold ϑ: if the measured signal power exceeds
ϑ, the channel is considered busy, otherwise idle. Whether a
message is successfully received is determined by the SINR
rule described above.
Physical carrier sensing is part of the 802.11 standard, and is
provided by a Clear Channel Assessment (CCA) circuit. This
circuit monitors the environment to determine when it is clear
to transmit. The CCA functionality can be programmed to be

The problem of coordinating the access to a shared medium
is a central challenge in wireless networks. In order to solve
this problem, a proper medium access control (MAC) protocol
is needed. Ideally, such a protocol should not only be able
to use the wireless medium as effectively as possible, but
it should also be robust against a wide range of interference problems including jamming attacks. Currently, the most
widely used model to capture interference problems is the
SINR (signal-to-interference-and-noise ratio) model [18]. In
this model, a message sent by node u is
correctly received
by node v if and only if Pv (u)/(N + w∈S Pv (w)) ≥ β
where Px (y) is the received power at node x of the signal
transmitted by node y, N is the background noise, and S is
the set of nodes w = u that are transmitting at the same
time as u. The threshold β > 1 depends on the desired
rate, the modulation scheme, etc. When using the standard
model for signal propagation,
then this expression results in

(P (u)/d(u, v)α )/(N + w∈S P (w)/d(w, v)α ) ≥ β where
P (x) is the strength of the signal transmitted by x, d(x, y) is
the Euclidean distance between x and y, and α is the pathloss exponent. In this paper, we will assume that all nodes
transmit with some ﬁxed signal strength P and that α > 2 + 
for some constant  > 0, which is usually the case in an
outdoors environment [30].
In most theory papers on MAC protocols, the background
noise N is either ignored (i.e., N = 0) or assumed to
behave like a Gaussian variable. This, however, is an oversimpliﬁcation of the real world. There are many sources of

P/d(u, v)α

≥ β,
ADV(v) + w∈S P/d(w, v)α

c 2014 IEEE
978-1-4799-3360-0/14/$31.00 

978-14799-3360-0/14/$31.00 ©2014 IEEE

2751

(1)

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

a function of the Receive Signal Strength Indication (RSSI)
and other parameters. The ability to manipulate the CCA rule
allows the MAC layer to optimize the physical carrier sensing
to its needs. Adaptive settings of the physical carrier sensing
threshold have been used to increase spatial reuse (e.g., [35]).
For simplicity, we will only consider a ﬁxed threshold.
In addition to the nodes there is an adversary that controls
the background noise. In order to cover a broad spectrum of
noise phenomena, we allow this adversary to be adaptive, i.e.,
for each time step t the adversary is allowed to know the state
of all the nodes in the system at the beginning of t (i.e., before
the nodes perform any actions at time t) and can set the noise
level ADV(v) based on that for each node v. To leave some
chance for the nodes to communicate, we restrict the adversary
to be (B, T )-bounded: for each node v and time interval I of
length T , a (B, T )-bounded adversary has an overall noise
budget of B · T that it can use to increase the noise level at
node v and that it can distribute among the time steps of I as
it likes. This adversarial noise model is very general, since in
addition to being adaptive, the adversary is allowed to make
independent decisions on which nodes to jam at any point in
time (provided that the adversary does not exceed its noise
budget over a window of size T ). In this way, many noise
phenomena can be covered.
Our goal is to design a symmetric local-control MAC
protocol (i.e., there is no central authority controlling the
nodes, and all the nodes are executing the same protocol)
that has a constant competitive throughput against any (B, T )bounded adversary as long as certain conditions (on B etc.)
are met. In order to deﬁne what we mean by “competitive”,
we need some notation. The transmission range of a node v is
deﬁned as the disk with center v and radius r with P/rα ≥ βϑ.
Given a constant  > 0, a time step is called potentially busy
at some node v if ADV(v) ≥ (1 − )ϑ (i.e., only a little bit
of additional interference by the other nodes is needed so that
v sees a busy channel). For a not potentially busy time step,
it is still possible that a message sent by a node u within v’s
transmission range is successfully received by v. Therefore,
as long as the adversary is forced to offer not potentially busy
time steps due to its limited budget and every node has a least
one other node in its transmission range, it is in principle
possible for the nodes to successfully transmit messages. To
investigate that formally, we use the following notation. For
any time frame F and node v let fv (F ) be the number of time
steps in F that are not potentially busy at v and let sv (F ) be
the number of time steps in which v successfully receives
a message. 
We call a protocolc-competitive for some time
frame F if v∈V sv (F ) ≥ c v∈V fv (F ). An adversary is
uniform if at any time step, ADV(v) = ADV(w) for all nodes
v, w ∈ V , which implies that fv (F ) = fw (F ) for all nodes.
Note that the scope of this paper is not restricted to the case
of a uniform jammer (cf Theorem 1.1).
Since the MAC protocol presented in this paper will be
randomized, our performance results typically hold with high
probability (short: w.h.p.): this means a probability of at least
1 − 1/nc for any constant c > 0.

Our Contribution. The contribution of this paper is
twofold. First, we introduce a novel extension of the SINR
model in order to investigate MAC protocols that are robust
against a broad range of interference phenomena. Second, we
present a MAC protocol called S ADE1 which can achieve a
c-competitive throughput where c only depends on  and the
path loss exponent α but not on the size of the network or other
network parameters. (In practice, α is typically in the range
2 < α < 5, and thus c is a constant for ﬁxed  [30].) Let n
be the number of nodes and let N = max{n, T }. Concretely,
we show:
Theorem 1.1: When running S ADE for at least
Ω((T log N )/ + (log N )4 /(γ)2 ) time steps, S ADE
2/(α−2)
) -competitive throughput for any
has a 2−O((1/)
((1 − )ϑ, T )-bounded adversary as long as (a) the adversary
is uniform and the transmission range of every node contains
at least one node, or (b) there are at least 2/ nodes within
the transmission range of every node.
On the other hand, we also show the following.
Theorem 1.2: The nodes can be positioned so that the
transmission range of every node is non-empty and yet no
MAC protocol can achieve any throughput against a (B, T )bounded adversary with B > ϑ, even if it is uniform.
The two theorems demonstrate that our S ADE protocol
is basically as robust as a MAC protocol can get within
our model. However, it should be possible to improve the
competitiveness. We conjecture that a polynomial dependency
on (1/) is possible, but showing that formally seems to be
hard. In fact, as we will show by a lower bound, a different
protocol than S ADE would be needed for that.
Additionally, this paper also shows that the SINR model is
fundamentally different from unit disk graph models: while
there exist MAC protocols that achieve a throughput polynomial in  by ensuring a constant cumulative sending probability
per disk, no such protocol can be polynomial-competitive
under SINR (see Theorem 3.18).
To complement our formal analysis and worst-case bounds,
we also report on the results of our simulation study. This
study conﬁrms many of our theoretical results, but also shows
that the actual performance for the cases considered in the
simulations is often better than in the worst-case.
II. A LGORITHM
The intuition behind S ADE is simple: Each node v maintains
a parameter pv which speciﬁes v’s probability of accessing the
channel at a given moment of time. That is, in each round,
each node u decides to broadcast a message with probability
pv . (This is similar to classical random backoff mechanisms
where the next transmission time t is chosen uniformly at
random from an interval of size 1/pv .) The nodes adapt their
pv values over time in a multiplicative-increase multiplicativedecrease manner, i.e., the value is lowered in times when the
channel is utilized (more speciﬁcally, we decrease pv whenever
1 S ADE stands for SINR JADE , the SINR variant of the jamming defense
protocol in [31].

2752

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

a successful transmission occurs) or increased during times
when the channel is idling. However, pv will never exceed p̂,
for some constant 0 < p̂ < 1 to be speciﬁed later.
In addition to the probability value pv , each node v maintains a time window estimate Tv and a counter cv for Tv . The
variable Tv is used to estimate the adversary’s time window
T : a good estimation of T can help the nodes recover from
a situation where they experience high interference in the
network. In times of high interference, Tv will be increased
and the sending probability pv will be decreased.
With these intuitions in mind, we can describe S ADE in full
detail.
Initially, every node v sets Tv := 1, cv := 1, and pv := p̂. In
order to distinguish between idle and busy rounds, each node
uses a ﬁxed noise threshold of ϑ.
The S ADE protocol works in synchronized rounds. In every
round, each node v decides with probability pv to send a
message. If it decides not to send a message, it checks the
following two conditions:
• If v successfully receives a message, then pv := (1 +
γ)−1 pv .
• If v senses an idle channel (i.e., the total noise created by
transmissions of other nodes and the adversary is less than
ϑ), then pv := min{(1 + γ)pv , p̂}, Tv := max{1, Tv − 1}.
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no idle step among
the past Tv rounds, then pv := (1 + γ)−1 pv and Tv := Tv + 2.
In order for S ADE to be constant competitive in terms of
throughput, the parameter γ needs to be a sufﬁciently small
value that depends very loosely on n and T . Concretely, γ ∈
O(1/(log T + log log n)).
Our protocol S ADE is an adaption of the MAC protocol
described in [31] for Unit Disk Graphs that works in more
realistic network scenarios considering physical interference.
The main difference in the new protocol is that in order to
use the concepts of idle and busy rounds, the nodes employ
a ﬁxed noise threshold ϑ to distinguish between idle (noise
< ϑ) and busy rounds (noise ≥ ϑ): in some scenarios the
threshold may not be representative, in the sense that, since the
success of a transmission depends on the noise at the receiving
node and on β, it can happen that a node senses an idle or
busy channel while simultaneously successfully receiving a
message. In order to deal with this problem, S ADE ﬁrst checks
whether a message is successfully received, and only otherwise
takes into account whether a channel is idle or busy. Another
change to the protocol in [31] is that we adapt Tv based on
idle time steps which allows us to avoid the upper bound on
Tv in the protocol in [31] so that our protocol is more ﬂexible.
III. A NALYSIS
While the MAC protocol S ADE is very simple, its stochastic
analysis is rather involved: it requires an understanding of the
complex interplay of the nodes following their randomized

protocol in a dependent manner. In particular, the nodes’
interactions depend on their distances (the geometric setting).
In order to study the throughput achieved by S ADE, we will
consider some ﬁxed node v ∈ V and will divide the area
around v into three circular and concentric zones.
Let DR (v) denote the disk of radius R around a given node
v ∈ V . In the following, we will sometimes think of DR (v)
as the corresponding geometric area on the plane, but we will
also denote by DR (v) the set of nodes located in this area.
The exact meaning will be clear from the context.
Deﬁnition 3.1 (Zones): Given any node v ∈ V , our analysis
considers three zones around v, henceforth referred to as
Zone 1, Zone 2, and Zone 3: Zone 1 is the disk of radius
R1 around v, Zone 2 is the disk of radius R2 around v
minus Zone 1, and Zone 3 is the remaining part of the plane.
Concretely:
1) Zone 1 covers the transmission range of v, i.e., its radius
so that P/R1α ≥ βϑ, which implies that
R1 is chosen

α
R1 = P/(βϑ). Region DR1 (v) has the property that
if there is at least one sender u ∈ DR1 (v), then v will
either successfully receive the message from u or sense
a busy channel, and v will receive the message from u
if the overall interference caused by other nodes and the
adversary is at most ϑ.
2) Zone 2 covers a range that we call the (critical) interference range of v. Its radius R2 is chosen in a way
so that if none of the nodes in Zone 1 and Zone 2
transmit a message, then the interference at any node
w ∈ DR1 (v) caused by transmitting nodes in Zone 3
is likely to be less than ϑ. Hence, if the current time
step is potentially non-busy at some w ∈ DR1 (v) (i.e.,
ADV(w) ≤ (1 − )ϑ), then the overall interference at
w is less than ϑ, which means that w will see an idle
time
 step. It will turn out that R2 can be chosen as
O (1/)1/(α−1) R1 .
3) Everything outside of Zone 2 is called Zone 3.
Whenever it is clear from the context, we use D1 , D2 , and
D3 instead of DR1 , DR2 , and the area covered by Zone 3,
respectively.
The key to proving a constant competitive throughput is
the analysis of the aggregate probability (i.e., the sum of
the individual sending probabilities pv ) of nodes in disks
D1 (v) and D2 (v): We will show that the expected aggregate
probabilities of D1 (v) and D2 (v), henceforth referred to by
p1 and p2 , are likely to be at most a constant. Moreover, our
analysis shows that while the aggregate probability p3 of the
potentially inﬁnitely large Zone 3 may certainly be unbounded
(i.e., grow as a function of n), the aggregated power received
at any node w ∈ D1 (v) from all nodes in Zone 3 is also
constant on expectation.
A. Zone 1


To show an upper bound on p1 = u∈D1 (v) pu , i.e., the
aggregate probability of the nodes in Zone 1 of v, we can
follow a strategy similar to the one introduced for the Unit
Disk Graph protocol [31].

2753

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

In the following, we assume that the budget B of the
adversary is limited by (1 −  )ϑ for some constant  = 2.
In this case, B is at most (1 − )2 ϑ. We ﬁrst look at a
slightly weaker form of adversary. We say that a round t
is open for a node v if v and at least one other node w
within its transmission range are potentially non-busy, i.e.,
ADV(v) ≤ (1 − )ϑ and ADV(w) ≤ (1 − )ϑ (which also
implies that v has at least one node within its transmission
range). An adversary is weakly (B, T )-bounded if it is (B, T )bounded and in addition to this, at least a constant fraction of
the potentially non-busy rounds at each node is open in every
time interval of size T . We will show the following result:
Theorem 3.2: When running S ADE for at least
Ω((T log N )/ + (log N )4 /(γ )2 ) time steps, S ADE
 2/(α−2)
) -competitive throughput for any
has a 2−O((1/ )

weakly ((1 −  )ϑ, T )-bounded adversary.
In order to prove this theorem, we focus on a time frame
I of size F consisting of δ log N/ subframes I  of size f =
δ[T + (log3 N )/(γ 2 )] each, where f is a multiple of T , δ is
a sufﬁciently large constant, and N = max{T, n}. Consider
some ﬁxed node v. We partition D1 (v) into six sectors of
equal angles from v, S1 , ..., S6 . Note that for any sector Si it
holds that if a node u ∈ Si transmits a message, then its signal
strength at any other node u ∈ Si is at least βϑ. Fix a sector
S and consider some ﬁxed time frame F . Let us refer to the
sum of the sending probabilitiesof the neighboring nodes of
a given node v ∈ S by p̄v := w∈S\{v} pw . The following
lemma, which is proven in [31], shows that pv will decrease
dramatically if p̄v is high throughout a certain time interval.
Lemma 3.3: Consider any node w in S. If p̄w > 5 − p̂

during all rounds
√ of a subframe I of I and at2 the beginning

of I , Tw ≤ F , then pw will be at most 1/n at the end of
I  , w.h.p.
Given this property of the individual probabilities, we can
derive an upper bound for the aggregate
probability of a sector

S. In order to compute pS = v∈S pv , we introduce three
thresholds, a low one, ρgreen = 5, one in the middle, ρyellow =
5e, and a high one, ρred = 5e2 . The following three lemmas
provide some important insights about these probabilities. The
ﬁrst lemma is shown in [31].
Lemma 3.4: Consider √any subframe I  in I. If at the
beginning of I  , Tw ≤ F for all w ∈ S, then there is at
least one round in I  with pS ≤ ρgreen w.h.p.
Lemma 3.5: For any subframe I  in I it holds that if pS ≤
ρgreen at the beginning of I  , then pS ≤ ρyellow throughout
I  , w.m.p.2 Similarly, if pS ≤ ρyellow at the beginning of I  ,
then pS ≤ ρred throughout I  , w.m.p. The probability bounds
hold irrespective of the events outside of S.
Proof: It sufﬁces to prove the lemma for the case that
initially pS ≤ ρgreen as the other case is analogous. Consider
some ﬁxed round t in I  . Let pS be the aggregate probability
at the beginning of t and pS be the aggregate probability at the
(0)
end of t. Moreover, let pS denote the aggregate probability
moderate probability, or w.m.p., means a probability of at least 1 −
log−Ω(1) n.
2 With

of the nodes w ∈ S with a total interference of less than ϑ
(1)
in round t when ignoring the nodes in S. Similarly, let pS
denote the aggregate probability of the nodes w ∈ S with
a single transmitting node in D1 (w) \ S and additionally an
(2)
interference of less than ϑ in round t, and let pS be the
aggregate probability of the nodes w ∈ S that do not satisfy the
ﬁrst two cases (which implies that they will not experience an
idle channel, no matter what the nodes in S will do). Certainly,
(0)
(1)
(2)
pS = pS + pS + pS . Our goal is to determine pS in this
case. Let q0 (S) be the probability that all nodes in S stay
silent, q1 (S) be the probability that exactly one node in S is
transmitting, and q2 (S) = 1−q0 (S)−q1 (S) be the probability
that at least two nodes in S are transmitting.
First, let us ignore the case that cv > Tv for a node v ∈ S
at round t. By distinguishing 9 different cases, we obtain the
(0)
(1)
following result: E[pS ] ≤ q0 (S)· [(1+γ)pS +(1+γ)−1 pS +
(2)
(0)
(1)
(2)
(0)
pS ] +q1 (S) · [(1 + γ)−1 pS + pS + pS ] +q2 (S) · [pS +
(1)
(2)
pS +pS ]. Just as an example, consider the case of q0 (S) and
(1)
pS , i.e., all nodes in S are silent and for all nodes in w ∈ S
(1)
accounted for in pS there is exactly one transmitting node in
D1 (w) \ S and the remaining interference is less than ϑ. In
this case, w is guaranteed to receive a message, so according
to the S ADE protocol, it lowers pw by (1 + γ).
The upper bound on E[pS ] certainly also holds if cv > Tv
for a node v ∈ S because pv will never be increased (but
possibly decreased) in this case. For the rest of the proof we
refer the reader to [31].
Lemma 3.6: For any subframe I  in I it holds that if there
has been at least one round during the past subframe where
pS ≤ ρgreen , then throughout I  , pS ≤ ρred w.m.p., and the
probability bound holds irrespective of the events outside of
S.
Proof: Suppose that there has been at least one round
during the past subframe where pS ≤ ρgreen . Then we know
from Lemma 3.5 that w.m.p. pS ≤ ρyellow at the beginning of
I  . But if pS ≤ ρyellow at the beginning of I  , we also know
from Lemma 3.5 that w.m.p. pS ≤ ρred throughout I  , which
proves the lemma.
Now, deﬁne a subframe I  to be good if pS ≤ ρred
throughout I  , and otherwise I  is called bad. With the help
of Lemma 3.4 and Lemma 3.6 we can prove the following
lemma.
Lemma 3.7: For any sector S, the expected number of bad
subframes I  in I is at most 1/polylog(N ), and at most β  /6
of the subframes I  in I are bad w.h.p., where the constant
β  > 0 can be made arbitrarily small depending on the constant
δ in f . The bounds hold irrespective of the events outside of
S.
The proof can be found in [31]. Since we have exactly
6 sectors, it follows from Lemma 3.7 that apart from an

subframes I  in I satisfy
β
 -fraction of the subframes, all

u∈D1 (v) pu ≤ 6ρ throughout I w.h.p.
B. Zone 3
Next, we consider Zone 3. We will show that although the
aggregate probability of the nodes in Zone 3 may be high (for

2754

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

some distributions of nodes in the space it can actually be as
high as Ω(n)), their inﬂuence (or noise) at node v is limited
if the radius of Zone 2 is sufﬁciently large. Thus, probabilities
recover quickly in Zone 1 and there are many opportunities
for successful receptions.
In order to bound the interference from Zone 3, we divide
−
Zone 3 into two sub-zones: Z
 contains all nodes from
 3 , 2which
Zone 3 up to a radius of O log n , and Z3+ , which contains
all remaining nodes in Zone 3. For Zone Z3− we can prove
the following lemma.
Lemma 3.8: At most an β  -fraction of the subframes I  in
I are bad for some R1 -disk in Zone Z3− w.h.p., where the
constant β  > 0 can be made arbitrarily small depending on
the constant δ in f .
Proof: The claim
follows
from the fact that


 the radius
of Zone Z3− is O log2 n and hence d = O log4 n disks
of radius R1 are sufﬁcient to cover the entire area of Z3− .
According to Lemma 3.7, over all of these disks, the expected
number of bad subframes is at most 1/polylog(N ). Using
similar techniques as for the proof of Lemma 3.7 in [31],
it can also be shown that for each disk D, the probability
for D to have k bad subframes is at most 1/polylog(N )k
irrespective of the events outside of D. Hence, one can use
Chernoff bounds for sums of identically distributed geometric
random variables (one for each D in Zone 3) to conclude that

all subframes I 
apart from an
β -fraction of the subframes,

in I satisfy v∈D pv ≤ 6ρ throughout I for all disks D in
Z3− w.h.p. This directly implies the lemma.
Suppose that R2 = c·R1 . Lemma 3.8 implies that in a good
subframe the expected noise level at any node w ∈ D1 (v)
created by transmissions in Zone Z3− is upper bounded by
O(log2 n)

6ρred ·



d=(c−1)

1
2π(d + 1)
12πρred
√
·
≤
α − 1 (c − 2)α−2 R1α
2(dR1 )α

which is at most ϑ/4 if c = O((1/)1/(α−2) ) is sufﬁciently
large. In order to bound the noise level at any node w ∈ D1 (v)
from Zone Z3+ , we prove the following claim.
Claim 3.9: Consider some √
ﬁxed R1 -disk D. If at the beginning of time frame I, Tw ≤ F for all w ∈ D, then for all
time steps except for the ﬁrst subframe in I, pD ∈ O(log n),
w.h.p.
Proof: Lemma 3.4 implies that there must be a time step
t in the ﬁrst subframe of I with pD ≤ 6ρgreen w.h.p. Since for
pD ∈ Ω(log n) at least a logarithmic number of nodes in D
transmit and therefore every node sees a busy channel, w.h.p.,
and pD can only increase if a node sees an idle channel, pD
is bounded by O(log n) for the rest of I w.h.p.
The claim immediately implies the following result.
√ Lemma 3.10: If at the beginning of time frame I, Tw ≤
F for all w, then for all time steps except for the ﬁrst
subframe in I, the interference at any node w ∈ D1 (v) due to
transmissions in Z3+ is at most ϑ/4 w.h.p.
Hence, we get:
√ Lemma 3.11: If at the beginning of time frame I, Tw ≤
F for all w, then at most an β-fraction of the subframes in

I contain time steps in which the expected interference at any
node w ∈ D1 (v) due to transmissions in Zone 3 is at least
ϑ/2, w.h.p.
C. Zone 2
For Zone Z2 we can prove the following lemma in the same
way as Lemma 3.8.
Lemma 3.12: At most an β-fraction of the subframes I 
in I are bad for some R1 -disk in Zone 2, w.h.p., where the
constant β > 0 can be made arbitrarily small depending on
the constant δ in f .
D. Throughput
Given the upper bounds on the aggregate probabilities and
interference, we are now ready to study the throughput of
S ADE. For this we ﬁrst need to show an upper bound on Tv
in order to avoid long periods of high pv values. Let J be a
time interval that has a quarter of the length of a time frame,
i.e., |J| = F/4. We start with the following lemma whose
proof is identical to Lemma III.6 in [32].
Lemma 3.13: If in subframe I  the number of idle time
steps at√v is at most k, then node v increases Tv by 2 at most
k/2 + f many times in I  .
Next, we show the following lemma.
√
Lemma 3.14: If at the beginning of J, Tv ≤ F /2 for all
2/(α−2)
) |J|
nodes v, then every node v has at least 2−O((1/)
time steps in J in which it senses an idle channel, w.h.p.
Proof: Fix some node v. Let us call a subframe I  in J
good if in Zone 1 and in any R1 -disk in Zone 2 of v, the
aggregate probability is upper bounded by a constant, and the
expected interference due to transmissions at v induced from
Zone 3 is at most ϑ/2 throughout I  . From Lemmas 3.7, 3.12,
and 3.11 it follows that there is an (1 − )-fraction
of good

subframes in J. Since R2 = O (1/)1/(α−2) R1 , for any time
step t in a good subframe I  the total aggregate
probability


in Zones 1 and 2 of v is upper bounded by O (1/)2/(α−2) .
Hence, the probability that none of the nodes in Zones 1 and
2 of v transmits is given by


2/(α−2)
)
(1 − pw ) ≥ e−2 w∈Z1 ∪Z2 pw = 2−O((1/)
w∈Z1 ∪Z2

Due to the Markov inequality, the probability that the interference due to transmissions in Zone 3 is at least ϑ is at
most 1/2. These probability bounds hold independently of the
other time steps in I  . Moreover, the total interference energy
of the adversary in I  is bounded by |I  |(1 − )2 ϑ, which
implies that at most a (1 − )-fraction of the time steps in I 
are potentially busy, i.e., ADV(v) ≥ (1 − )ϑ. Hence, for at
2/(α−2)
) -fraction of the time steps in I  , the
least a 2−O((1/)
probability for v to sense an idle channel is a constant, which
implies the lemma.
This allows us to prove the following lemma.√
Lemma 3.15: If√at the beginning of J, Tv ≤ F /2 for all
v, then also Tv ≤ F /2 for all v at the end of J, w.h.p.
Proof: From the previous lemma we know that every node
v senses an idle channel for Ω(|J|) time steps in J for any

2755

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

constants  > 0 and α > 2. Tv is maximized at the end of
J if all of these idle time steps happen at the beginning of
J, which would get Tv down to 1 at some point. Afterwards,
T
v t can rise to a value of at most t for the maximum t with
i=1 2i ≤ |J| (because v increases Tv by 2 each time it sees
no
 in theprevious Tv steps), which is at most
 idle channel
|J|. Since |J| = |F |/2, the lemma√follows.
Since √Tv can be increased at most F /2 many times
beyond F /2 in J, we get:
√ frame I, Tv ≤
√ Lemma 3.16: If at the beginning of a time
F /2 for all v, then √
throughout I, Tv ≤ F for all v, and
at the end of I, Tv ≤ F /2 for all v, w.h.p.
Hence, the upper bounds on Tv that we assumed earlier are
valid w.h.p. We are now ready to prove Theorem 3.2.
Proof of Theorem 3.2: Recall that a time step is open
for a node v if v and at least one other node in D1 (v) are
not potentially busy. Let J be the set of all open time steps
in I. Furthermore, let k0 be the number of times v senses an
idle channel in J and let k1 be the number of times v receives
a message in I. From Lemma 3.14 and the assumptions in
2/(α−2)
) |I|.
Theorem 3.2 we know that k0 = 2−O((1/)
Case 1: k1 ≥ k0 /6. Then our protocol is 2−O((1/)
competitive for v and we are done.

2/(α−2)

)-

Case 2: k1 < k0 /6. Then we√know from Lemma 3.13 that pv
is decreased at most k0 /2 + F times in I due to cu > Tu . In
addition to this, pv is decreased at most k1 times in I due to a
received message. On the other hand, pv is increased at least
k0 times in J (if possible) due to an idle channel w.h.p. Also,
we know from our protocol that at the beginning of I,
pv = p̂.
Hence, there must be at least (1 − 1/2 − 1/6)k0 − |F | ≥
k0 /4 rounds in J w.h.p. at which pv = p̂. Now, recall the
deﬁnition of a good subframe in the proof of Lemma 3.14.
From Lemmas 3.7, 3.12, and 3.11 it follows that at most a
β-fraction of the subframes in I is bad. In the worst case, all
of the time steps in these subframes are open time steps, which
sums up to at most k0 /8 if β is sufﬁciently small. Hence, there
are at least k0 /8 rounds in J that are in good subframes,
w.h.p., and at which pv = p̂, which implies that the other not
potentially busy node in D1 (v) has a constant probability of
receiving a message from v. Using Chernoff bounds, at least
k0 /16 rounds with successfully received transmissions can be
identiﬁed for v, w.h.p.
If we charge 1/2 of each successfully transmitted message
to the sender and 1/2 to the receiver, then a constant competitive throughput can be identiﬁed for every node in both
2/(α−2)
)cases above. It follows that our protocol is 2−O((1/)
competitive in F .
Now, let us consider the two cases of Theorem 1.1. Recall
that we allow here any ((1 − )ϑ, T )-bounded adversary.
Proof of Theorem 1.1:
Case 1: the adversary is uniform and ∀v : D1 (v) = ∅.:
In this case, every node has a non-empty neighborhood and
therefore all not potentially jammed rounds of the nodes

are open. Hence, the conditions on a weakly ((1 − )ϑ, T )bounded adversary are satisﬁed. So Theorem 3.2 applies,
which completes the proof of Theorem 1.1 a).
Case 2: |D1 (v)| ≥ 2/ for all v ∈ V .: Consider
some ﬁxed time interval I with |I| being a multiple of T .
For every node v ∈ D1 (u) let fv be the number of not
potentially jammed rounds at v in I and ov be the number
of open rounds at v in I. Let J be the set of rounds in
I with at most one not potentially jammed node. Suppose
that |J| > (1 − /2)|I|. Then every node in D1 (u) must
have more than (/2)|I| of its not potentially jammed rounds
in J. As these not potentially jammed rounds must be serializedin J to satisfy our requirement on J, it holds that
|J| > v∈D1 (u) (/2)|I| ≥ (2/) · (/2)|I| = |I|. Since this
is impossible, it must hold that |J| ≤ (1 − /2)|I|.


≥ ( v∈D1 (u) fv ) − |J|
≥
Thus,
v∈D1 (u) ov

(1/2) v∈D1 (u) fv because v∈D1 (u) fv ≥ (2/)·|I| = 2|I|.
Let D (u) be the set of nodes v ∈ D1 (u) with
ov ≥ fv /4. That is, for each of these nodes, a constant
fraction of 
the not potentially jammedtime steps is
open.
Then
v∈D1 (u)\D
  (u) ov < (1/4) v∈D1 (u) fv , so

o
≥
(1/2)
v∈D  (u) v
v∈D1 (u) ov ≥ (1/4) 
v∈D1 (u) fv .
Consider now a set U ⊆ V of nodes so that u∈U D1 (u) =
V and for every v ∈ V there are at most 6 nodes u ∈ U with
v ∈ D1 (u). Note U is easy to construct in a greedy fashion for
arbitrary UDGs, and therefore for D1 (u) in the SINR model,
and also
set of constant
density. Let
 known as a dominating

V  = u∈U D (u). Since v∈D (u) ov ≥ (1/4) v∈D1 (u) fv
for every
 u ∈ U , it follows
 that v∈V  ov ≥
 node
(1/6) u∈U v∈D (u) ov ≥ (1/24) u∈U v∈D1 (u) fv ≥
(1/24) v∈V fv . Using that together with Theorem 3.2, which
implies that S ADE is constant competitive w.r.t. the nodes in
V  , completes the proof of Theorem 1.1 b).
E. Near-Optimality
Obviously, if a jammer has a sufﬁciently high energy
budget, it can essentially block all nodes all the time. In the
following we call a network dense if ∀v ∈ V : |D1 (v)| > 1.
Theorem 3.17: The nodes can be positioned so that the
transmission range of every node is non-empty and yet no
MAC protocol can achieve any throughput against a (B, T )bounded adversary with B > ϑ, even if it is uniform.
Proof: Let us suppose the jammer uses an energy budget
B > ϑ. If every node v only has nodes right at the border of its
disk D1 (v) and the adversary continuously sets ADV(v) = B,
then v will not be able to receive any messages according to
the SINR model. Thus the overall throughput in the system
is 0.
In the following, we will prove that S ADE is also optimal
in the sense that no MAC protocol can achieve a throughput
polynomial in  if it aims to keep the aggregate sending probability in the transmission range constant. This is interesting
as it highlights a key difference between the unit disk graph
model and the SINR model: in the unit disk graph model, such
algorithms can exist (as shown in [31]).

2756

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

Theorem 3.18: The nodes can be positioned
so that for any

MAC protocol with the property ∀v : w∈D1 (v) pw = Θ(1),
2/(α−2)

)
.
the throughput cannot be higher than 2−Ω((1/)
∗
Proof: Let p denote the minimum aggregate probability
over all possible disks of radius R1 , and assume p∗ = Θ(1).
Suppose that the distance between an arbitrary node and its
nearest neighbor equals R1 , and suppose that R2 = c · R1 , for
some constant c we will compute below. The expected noise
level at any node w ∈ D1 (v) created by transmissions in Zone
Z3− is lower bounded by
O(log2 n)

p∗ ·


d=c

2π(d + 1)
1
≥ Θ(1) · α−2 α ,
α
(dR1 )
c
R1

which is 2ϑ for some c = Ω((1/)1/(α−2) ). Neglecting the
transmissions from Z3+ (they will only increase the interference further), we obtain R2 = Ω((1/)1/(α−2) · R1 ).
Fix a receiver v and consider a ((1 − )ϑ, T )-bounded
adversary. We ﬁrst show that the induced noise from nodes
in Z3− does not differ much from its expectation with a sufﬁciently large probability, and then we construct a (reasonable)
situation where a message sent by a node w in Z1 ∪ Z2 will,
most likely, not be received by v for a large amount of rounds.
Let xw be the amountof noise generated by node w in Z3− at
node v and let X = ∀w∈Z − xw . From above we know that
3
E[X] ≥ 2ϑ. We deﬁne η = max∀w∈Z − {xw }, and get
3
	

E[X]
· ( )2
P[X < (1 −  )E[X]] ≤ exp −
3η





2
= exp −Ω ( )2 · (1/) /α−2 ,


P
with η ≤ RPα = Ω((1/)α/(α−2)
= O α/(α−2) · βϑ .
α
2

 R1 ) 2/α−2 
. Therefore with
Thus P[X < 1.5ϑ]= exp
(1/)
 −Ω2/α−2

probability 1 − exp −Ω (1/)
the noise level induced
by Z3− is at least 1.5ϑ. Let w = v be any node that transmits
a message. By assumption the distance between v and w is at
least R1 . Then the expected signal to interference noise ratio
for v is given by (P/R1α )/((1 − )ϑ + 1.5ϑ) < β, and our
claim follows.
IV. S IMULATIONS
To complement our formal analysis and to investigate the
average-case behavior of our protocol, we conducted a simulation study. In the following, we consider two scenarios which
differ in the way nodes are distributed in the 2-dimensional
Euclidean space. In the ﬁrst scenario, called U NI, the nodes
are distributed uniformly at random in the 2-dimensional plane
of size 25 × 25 units. In the second scenario, called H ET, we
ﬁrst subdivide the 2-dimensional plane of size 25 × 25 units
into 25 sub-squares of size 5×5 units. For each sub-square we
then choose the number of nodes λ uniformly at random from
the interval [20, 120] and distribute said nodes (uniformly at
random) in the corresponding sub-square. Consequently, each
sub-square potentially provides a different density, where the
attribute density represents the average amount of nodes on a

spot in the plane of the corresponding scenario. In order to
avoid boundary effects, for both U NI and H ET, we assume
that the Euclidean plane “wraps around”, i.e., distances are
computed modulo the boundaries.
While our formal throughput results in Section III hold for
any adversary which respects the jamming budget constraints,
computing the best adversarial strategy (i.e., the strategy which
minimizes the throughput of S ADE) is difﬁcult. Hence, in
our simulations, we consider the following two types of
adversaries: (1) Regular (or random) jammer (R EG): given an
energy budget B per node, a time interval T , and a speciﬁc
constant 0 <  < 1, used by the adversary to randomly jam
each node every th round (on average) using exactly B
energy per node. Additionally we make sure that the overall
budget B is perfectly used up at the end of T . (2) Bursty
(or deterministic) jammer (B UR): For each time period T , the
adversary jams all initial rounds at the node, until the budget
BT is used up. The remaining rounds in T are unjammed.
In other words, the ﬁrst T many rounds are jammed by the
adversary using exactly B energy per node and round.
If not stated otherwise, we use the jammer R EG and
parameters α = 3,  = 13 , β = ϑ = 2, P = 8, T = 60,
B = (1−)·ϑ and run the experiment for 3000 rounds. We will
typically plot the percentage of successful message receptions,
averaged over all nodes, with respect to the unjammed time
steps. If not speciﬁed otherwise, we repeat each experiment
ten times with different random seeds, both for the distribution
of nodes in the plane as well as the decisions made by our
MAC protocol. By default, our results show the average over
these runs; the variance of the runs is low.
Impact of Scale and α. We ﬁrst study the throughput as a
function of the network
size. Therefore we distribute n nodes
√
√
uniformly in the n × n plane. Figure 1 (left-most) shows
our results under the R EG (or random) jammer and different
α values. First, we can see that the competitive throughput
is around 40%, which is higher than what we expect from
our worst-case formal analysis. Interestingly, for α = 3, we
observe a small throughput decrease for larger networks; but
for α > 3, the throughput is almost independent of the network
scale. (In the literature, α is typically modeled as 3 or 4.)
This partially conﬁrms Theorem 1.1: a higher α renders the
transmissions and power propagation more local. This locality
can be exploited by S ADE to some extent.
Impact of Density. Next, we investigate how the performance of S ADE depends on the node density. We focus on
α = 3 and study both the R EG jammer as well as the
B UR (deterministic) jammer. Figure 1 (left) shows that results
for the U NI scenario (n nodes distributed uniformly in the
25 × 25 plane, i.e., density n/625). The throughput is similar
under both jammers, and slightly declines for denser networks.
This effect is very similar to the effect of having larger (but
equidistant) networks.
However, S ADE suffers more from more heterogenous densities. The results for the scenario H ET are shown in Figure 1
(right). While the throughput is generally lower, the speciﬁc
sub-square density plays a minor role.

2757

300

500

700

Network Size

Fig. 1.

900

0.16

0.48

0.80

1.12

1.44

0.80

Density

40
35
30
25

Power=2
Power=4
Power=8

20

0.0

0.2
0.0
100

Aggregate Probability

0.6
0.4

Throughput

0.6

Power=2.0
Power=4.0
Power=8.0

0.2

1.0
0.8

Deterministic Jammer
Random Jammer

0.4

0.6
0.4
0.0

0.2

Throughput

0.8

alpha=3.0
alpha=4.0
alpha=5.0
alpha=6.0

Throughput

1.0

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

1.44

2.08

2.72

3.36

4.00

4.64

0

500

Sub-Square Density

1000

1500

2000

2500

3000

Number of Rounds

Throughput as a function of network size (left-most), density (left), sub-grid density (right), and power (right-most).

Convergence Time. S ADE adapts quite fast to the given
setting, as the nodes increase and decrease their sending
probabilities in a multiplicative manner. Being able to adapt
quickly is an important feature, in particular in dynamic or
mobile environments where nodes can join and leave over
time, or where nodes are initialized with too high or low sending probabilities. Our distributed MAC protocol will adjust
automatically and “self-stabilize”.
Figure 1 (right-most) shows representative executions over
time and plots the aggregate probability. Initially, nodes have
a maximum sending probability p̂ = 1/24. This will initially
lead to many collisions; however, very quickly, the senders
back off and the overall sending probabilities (the aggregated
probability) reduce almost exponentially, and we start observing successful message transmissions. (Observe that the
aggregated “probability” can be higher than one, as it is simply
the sum of the probabilities of the individual nodes.)
The sum of all sending probabilities also converges quickly
for any other P . However, for smaller powers, the overall
probability is higher. This is consistent with the goal of S ADE:
because for very large sending powers, also more remote nodes
in the network will inﬂuence each other and interfere, it is
important that there is only a small number of concurrent
senders in the network at any time—the aggregated sending
probability must be small. On the other hand, small powers
allow for more local transmissions, and to achieve a high
overall throughput, many senders should be active at the same
time—the overall sending probability should be high.
Impact of Epsilon and Comparison to Random Backoff
Schemes à la 802.11. We also compared the throughput of
S ADE to a simpliﬁed version of a 802.11 MAC protocol:
in that protocol version, we assume that nodes are perfectly
synchronized, and senders immediately know whether their
transmission was successful (i.e., at least one node successfully
received the message); if no node received the message, the
sender starts an exponential backoff mechanism. We set the
unit slot time for 802.11 to 50 μs (one round). The backoff
timer of the 802.11 MAC protocol implemented here uses units
of 50 μs. Our results show that 802.11a suffers more from the
interference, while it yields a similar throughput for large .
In fact, we ﬁnd that for  close to 0, 802.11a can even slightly
outperform S ADE.
When varying , we ﬁnd that the worst-case bound of

Theorem 1.1 may be too pessimistic in many scenarios, and
the throughput depends to a lesser extent on the constant .
V. R ELATED W ORK
Traditional jamming defense mechanisms typically operate
on the physical layer [25], [27], [36], and mechanisms have
been designed to both avoid jamming as well as detect jamming. Recent work has started to investigate also MAC layer
strategies against jamming, for example coding strategies [6],
channel surﬁng and spatial retreat [1], [38], or mechanisms to
hide messages from a jammer, evade its search, and reduce the
impact of corrupted messages [37]. These methods however
do not help against an adaptive jammer with full information
about the history of the protocol, like the one considered in
our work.
In the theory community, work on MAC protocols has
mostly focused on efﬁciency. Many of these protocols are
random backoff or tournament-based protocols [4], [7], [17],
[19], [23], [29] that do not take jamming activity into account
and, in fact, are not robust against it (see [2] for more details).
The same also holds for many MAC protocols that have been
designed in the context of broadcasting [8] and clustering [22].
Also some work on jamming is known (e.g., [9] for a short
overview). There are two basic approaches in the literature.
The ﬁrst assumes randomly corrupted messages (e.g. [28]),
which is much easier to handle than adaptive adversarial
jamming [3]. The second line of work either bounds the
number of messages that the adversary can transmit or disrupt
with a limited energy budget (e.g. [16], [21]) or bounds the
number of channels the adversary can jam (e.g. [10], [11],
[12], [13], [14], [15], [26]).
The protocols in [16], [21] can tackle adversarial jamming
at both the MAC and network layers, where the adversary
may not only be jamming the channel but also introducing
malicious (fake) messages (possibly with address spooﬁng).
However, they depend on the fact that the adversarial jamming
budget is ﬁnite, so it is not clear whether the protocols would
work under heavy continuous jamming. (The result in [16]
seems to imply that a jamming rate of 1/2 is the limit whereas
the handshaking mechanisms in [21] seem to require an even
lower jamming rate.)
Our work is motivated by the work in [3] and [2]. In [3] it
is shown that an adaptive jammer can dramatically reduce the

2758

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications

throughput of the standard MAC protocol used in IEEE 802.11
with only limited energy cost on the adversary side. Awerbuch
et al. [2] initiated the study of throughput-competitive MAC
protocols under continuously running, adaptive jammers, but
they only consider single-hop wireless networks. Their approach has later been extended to reactive jamming environments [32], co-existing networks [34] and applications such
as leader election [33].
The result closest to ours is the robust MAC protocol for
Unit Disk Graphs presented in [31]. In contrast to [31], we
initiate the study of the more relevant and realistic physical
interference model [18] and show that a competitive throughput can still be achieved. As unlike in Unit Disk Graphs,
in the SINR setting far-away communication can potentially
interfere and there is no absolute notion of an idle medium,
a new protocol is needed whose geometric properties must be
understood. For the SINR setting, we also introduce a new
adversarial model (namely the energy budget adversary).
VI. C ONCLUSION
This paper has shown that robust MAC protocols achieving
a constant competitive throughput exist even in the physical
model. This concludes a series of research works in this area.
Nevertheless, several interesting questions remain open. For
example, while our theorems prove that S ADE is as robust as
a MAC protocol can get within our model and for constant ,
we conjecture that a throughput which is polynomial in (1/)
is possible. However, we believe that such a claim is very
difﬁcult to prove. We also plan to explore the performance of
S ADE under speciﬁc node mobility patterns.
R EFERENCES
[1] G. Alniﬁe and R. Simon. A multi-channel defense against jamming
attacks in wireless sensor networks. In Proc. of Q2SWinet, pages 95–
104, 2007.
[2] B. Awerbuch, A. Richa, and C. Scheideler. A jamming-resistant MAC
protocol for single-hop wireless networks. In Proc. ACM PODC, 2008.
[3] E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and
B. Thapa. On the performance of IEEE 802.11 under jamming. In
Proc. IEEE INFOCOM, pages 1265–1273, 2008.
[4] M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E.
Leiserson. Adversarial contention resolution for simple channels. In
Proc. ACM SPAA, 2005.
[5] T. Brown, J. James, and A. Sethi. Jamming and sensing of encrypted
wireless ad hoc networks. In Proc. ACM International Symposium on
Mobile Ad hoc Networking and Computing (MOBIHOC), pages 120–
130, 2006.
[6] J. Chiang and Y.-C. Hu. Cross-layer jamming detection and mitigation
in wireless broadcast networks. In Proc. MOBICOM, pages 346–349,
2007.
[7] B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki. Adversarial queuing
on the multiple-access channel. In Proc. ACM PODC, 2006.
[8] A. Czumaj and W. Rytter. Broadcasting algorithms in radio networks
with unknown topology. Journal of Algorithms, 60(2):115 – 143, 2006.
[9] S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski, C. Newport, F. Kuhn,
and N. Lynch. Reliable distributed computing on unreliable radio
channels. In Proc. 2009 MOBIHOC S3 Workshop, 2009.
[10] S. Dolev, S. Gilbert, R. Guerraoui, F. Kuhn, and C. C. Newport.
The wireless synchronization problem. In Proc. 28th Annual ACM
Symposium on Principles of Distributed Computing (PODC), pages 190–
199, 2009.
[11] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Gossiping in a multichannel radio network: An oblivious approach to coping with malicious
interference. In Proc. of the Symposium on Distributed Computing
(DISC), 2007.

[12] S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Secure communication over radio channels. In Proc. 27th ACM Symposium on Principles
of Distributed Computing (PODC), pages 105–114, 2008.
[13] S. Dolev, S. Gilbert, R. Guerraoui, and C. C. Newport. Gossiping in a
multi-channel radio network. In Proc. 21st International Symposium on
Distributed Computing (DISC), pages 208–222, 2007.
[14] S. Gilbert, R. Guerraoui, D. Kowalski, and C. Newport. Interferenceresilient information exchange. In Proc. of the 28th Conference on
Computer Communications. IEEE INFOCOM., 2009.
[15] S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport.
Interference-resilient information exchange. In Proc. 28th IEEE International Conference on Computer Communications (INFOCOM), pages
2249–2257, 2009.
[16] S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and suspicious sensors: On the efﬁciency of malicious interference in wireless
networks. In Proc. OPODIS, 2006.
[17] L. A. Goldberg, P. D. Mackenzie, M. Paterson, and A. Srinivasan.
Contention resolution with constant expected delay. J. ACM, 47(6),
2000.
[18] P. Gupta and P. Kumar. The capacity of wireless networks. IEEE
Transactions on Information Theory, 46(2):388 –404, 2000.
[19] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for
mulitiple accesschannels. SIAM Journal on Computing, 25(4), 1996.
[20] IEEE. Medium access control (MAC) and physical speciﬁcations. In
IEEE P802.11/D10, 1999.
[21] C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable broadcast in radio
networks: The bounded collision case. In Proc. ACM PODC, 2006.
[22] F. Kuhn, T. Moscibroda, and R. Wattenhofer. Radio network clustering
from scratch. In Proc. ESA, 2004.
[23] B.-J. Kwak, N.-O. Song, and L. E. Miller. Performance analysis of exponential backoff. IEEE/ACM Transactions on Networking, 13(2):343–355,
2005.
[24] M. Li, I. Koutsopoulos, and R. Poovendran. Optimal jamming attacks
and network defense policies in wireless sensor networks. In Proc. IEEE
INFOCOM, pages 1307–1315, 2007.
[25] X. Liu, G. Noubir, R. Sundaram, and S. Tan. Spread: Foiling smart
jammers using multi-layer agility. In Proc. IEEE INFOCOM, pages
2536–2540, 2007.
[26] D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer. Speed dating
despite jammers. In Proc. DCOSS, June 2009.
[27] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein. Using channel
hopping to increase 802.11 resilience to jamming attacks. In Proc. IEEE
INFOCOM, pages 2526–2530, 2007.
[28] A. Pelc and D. Peleg. Feasibility and complexity of broadcasting with
random transmission failures. In Proc. ACM PODC, 2005.
[29] P. Raghavan and E. Upfal. Stochastic contention resolution with short
delays. SIAM Journal on Computing, 28(2):709–719, 1999.
[30] T. Rappaport. Wireless communications. Prentice Hall PTR Upper
Saddle River, 2002.
[31] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A Jamming-Resistant
MAC Protocol for Multi-Hop Wireless Networks. In Proc. DISC, 2010.
[32] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Competitive and fair
medium access despite reactive jamming. In Proc. 31st International
Conference on Distributed Computing Systems (ICDCS), 2011.
[33] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Self-stabilizing leader
election for single-hop wireless networks despite jamming. In Proc.
12th ACM International Symposium on Mobile Ad Hoc Networking and
Computing (MOBIHOC), 2011.
[34] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. Competitive and
fair throughput for co-existing networks under adversarial interference.
In Proc. 31st Annual ACM Symposium on Principles of Distributed
Computing (PODC), 2012.
[35] C. Scheideler, A. Richa, and P. Santi. An O(log n) Dominating Set
Protocol for Wireless Ad-Hoc Networks under the Physical Interference
Model. In Proc. ACM International Symposium on Mobile Ad hoc
Networking and Computing (MOBIHOC), 2008.
[36] M. K. Simon, J. K. Omura, R. A. Schultz, and B. K. Levin. Spread
Spectrum Communications Handbook. McGraw-Hill, 2001.
[37] A. Wood, J. Stankovic, and G. Zhou. DEEJAM: Defeating energyefﬁcient jamming in IEEE 802.15.4-based wireless networks. In Proc.
SECON, 2007.
[38] W. Xu, T. Wood, and Y. Zhang. Channel surﬁng and spatial retreats:
defenses against wireless denial of service. In Proc. of Workshop on
Wireless Security, 2004.

2759

An Algorithmic Framework for Shape Formation Problems
in Self-Organizing Particle Systems

1.

Zahra Derakhshandeh⇤

Robert Gmyr†

Andréa W. Richa⇤

Arizona State University, USA
zderakhs@asu.edu

University of Paderborn, Germany
gmyr@mail.upb.de

Arizona State University, USA
aricha@asu.edu

Christian Scheideler†

Thim Strothmann†

University of Paderborn, Germany
scheideler@upb.de

University of Paderborn, Germany
thim@mail.upb.de

INTRODUCTION

This material is based on work in progress.
Imagine that we had a piece of matter that can change
its physical properties like shape, density, conductivity, or
color in a programmable fashion based on either user input
or autonomous sensing. This is the vision behind what is
commonly known as programmable matter. Programmable
matter is the subject of many recent novel distributed computing proposals — ranging from DNA tiles, shape-changing
molecules, and synthetic cells, to reconfigurable modular
robotics — each pursuing solutions for specific application
scenarios with their own, special capabilities and constraints.
We envision systems of nano-sensors devices with very
limited computational capabilities individually, but which
can collaborate to reach a lot more as a collective. Ideally, those nano-sensor devices will be able to self-organize
in order to achieve a desired collective goal without the need
of central control or external (in particular, human) intervention. For example, one could envision using a system
of self-organizing nano-sensor devices to identify and coat
leaks on a nuclear reactor or to monitor environmental and
structural conditions in abandoned mines, on the exterior
of an airplane/spacecraft, or on a bridge, possibly also selfrepairing the structure — i.e., realizing what has been coined
as ”smart paint”. The applications in the health arena are
also endless, e.g., self-organizing nano-sensor devices could
be used within our bodies to detect and coat an area where
internal bleeding occurs, eliminating the need of immediate surgery, or they could be used to identify and isolate
tumor/malign cells. In many applications, there may be a
specific shape that one would like the system to assume (e.g.,
a disc, or a line, or even an arbitrary convex shape).
Hence, from an algorithmic point-of-view, we are interested in programmable matter consisting of self-organizing
⇤
This work was supported in part by the NSF under Awards
CCF-1353089 and CCF-1422603.
†
Supported in part by DFG grant SCHE 1592/3-1

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from Permissions@acm.org.
NANOCOM 2015 Boston, Massachusetts, USA
Copyright 2015 ACM ISBN 978-1-4503-3674-1/15/09 $15.00
DOI: http://dx.doi.org/10.1145/2800795.2800829.

particle systems (SOPS), where a particle is a simple computational element that can establish and release (communication or physical) bonds and can actively move in a selforganized way, and in general shape formation problems in
those systems.
We use the geometric amoebot model presented in [2] as
our basic model for SOPS: We assume that we have the
infinite regular triangular grid graph Geqt that represents
the relative positions that a connected set of particles may
assume — i.e., the set of nodes in Geqt represents all possible
positions of a particle (relative to the other particles in the
structure) and each edge represents a possible transition of
position (movement) of a particle.

2.

OUR CONTRIBUTIONS

In this paper, we present a general algorithmic framework
for shape formation problems in SOPS, which constitutes of
two basic algorithmic primitives: the spanning forest primitive and the snake formation primitive. We present concrete
applications of these primitives to two specific shape formation problems, namely to the problems of having the system
of particles self-organize to form the largest possible hexagonal shape (HEX) and triangular shape (TRI) (defined by a
set of inner angles), resp. Both the HEX and TRI formation algorithms are optimal with respect to work, which we
measure by the total number of particle movements needed
to reach the desired shape configuration. Our algorithms
rely only on local information (e.g., particles do not have
ids, nor do they know n, the total number of particles, or
have any sort of global coordinate/orientation system), and
require only constant-size memory particles.
Theorem 1. Our HEX (resp., TRI) algorithm correctly
decides HEX (resp., TRI) problem in worst-case optimal
O(n2 ) work, where n is the number of system particles.

3.

SHAPE FORMATION

We focus on solving shape formation problems in the geometric amoebot model starting from any well-initialized
connected configuration of particles. We formally define a
shape formation problem as a tuple M = (I, G) where I and
G are sets of connected configurations of particles. We say
I is the set of permitted initial configurations and G is the
set of goal configurations. We present a general algorithmic
framework for shape formation problems and then specifically investigate the Hexagonal Shape Formation (HEX) and
the Triangular Shape Formation (TRI) problems, where G
would be all configurations of contracted particles where the

positions of the set of particles induce the largest possible
complete hexagon, or the largest possible complete triangle
(given the set of its inner angles), respectively, on Geqt . We
say that an algorithm A decides a shape formation problem
M, if for any initial configuration from I, all executions of A
eventually reach one of the valid configurations in G without
losing connectivity, and whenever such a system configuration is reached for the first time, the system stays there and
all particles decide to perform no further actions.
Before we proceed, we provide some preliminaries. For all
algorithms we assume that there is a specific particle we call
the seed particle, which provides the starting point for constructing the respective shape. If a seed is not available, one
can be chosen using the leader election algorithm proposed
in [2]. We define the set of states that a particle can be in as
inactive, follower, root, and retired. Initially, all particles are
inactive, except the seed particle, which is always in a retired
state. In addition to its state, each particle p maintains a
constant number of (constant-size) flags.
Generally speaking, the shape formation algorithms we
propose for hexagonal and triangular shapes progress as follows: Particles organize themselves into a spanning set of
disjoint trees, where the roots of the trees are non-retired
particles adjacent to the partially constructed shape structure (consisting of all retired particles). Root particles lead
the way by moving in a predefined direction around the
current retired structure. The remaining particles (i.e., the
followers) follow behind the leading root particles, making
the system ”flatten” out towards the direction of movement.
Once the leading particles reach a valid position where the
shape can be extended (following the rules for the snake
formation for the particular shape), they stop moving and
change their state to retired as well. This process continues
until all particles become retired. Once a particle becomes
retired, it performs no further action. Note that the spanning forest component of this general approach is the same
for any shape formation algorithm: It is only in the rules
that determine the next valid position to be filled in the
shape structure being built that the respective algorithms
di↵er. We determine the next valid position to be filled
sequentially following a snake (i.e., a line of consecutive positions in Geqt ), that fills in the space of the respective shape
structure and scales naturally with the number of particles
in the system. We now briefly explain the general ideas
behind the HEX and TRI algorithms.
In order to characterize the snake formation for a given
shape formation problem, one only needs to specify the direction in which the line of particles forming the snake should
continue to grow, for each new particle added to the snake.
Hence once a particle finds the next valid position on the
snake, it will become retired and set its snake direction accordingly. Di↵erent rules for snake formation will realize
di↵erent shapes.

Hexagonal Shape.
In the HEX problem, the system of particles has to assume the shape of a hexagon (but for the outer layer, which
may not be completely full) in Geqt . The hexagon will be
built around the seed particle. (A hexagon in Geqt is actually a disk, formed by all nodes of within a certain distance
r from the seed node.) We will organize the particles according to a spiral snake structure which will incrementally
add new layers to the hexagon, scaling naturally with the

(a)

(b)
Figure 1: Snapshots of the (a) HEX algorithm and (b)
TRI algorithm. The seed is green, retired particles are
black, roots are red and followers are blue.

number of particles in the system. In a nutshell, the algorithm proceeds as follows: Initially, the seed particle, which
is retired, sets the snake direction flag to correspond to one
of its adjacent edges. Any particle adjacent to a retired particle becomes a root following the spanning forest algorithm.
Each root p moves in a clockwise fashion around the structure of retired particles until it finds the next position to
extend the hexagonal snake (i.e., a position connected to a
retired particle via an edge marked by the snake direction
flag) and becomes retired.

Triangular Shape.
In the TRI problem, the system of particles has to assume
a final triangular shape on Geqt (but for possibly the outer
layer). The snake formation rules for the TRI problem are
more complex than the ones we had for the HEX problem,
since we will need to explicitly take into account the formation of di↵erent layers of particles as we build the triangular
structure (this is implicitly taken care of by the spiral formation in the HEX algorithm). The TRI snake construction
will start from the seed particle, which will occupy one of
the triangle corners. The seed will mark two of its adjacent edges as eL and eR , which will determine the direction
along which two of the sides (L and R) of the triangle will be
formed (and hence also its inner angles). These directions
will be propagated by the particles that end up on L and R
resp. The seed also marks eR as the initial snake direction.
From there on, the triangle snake will be formed layer by
layer, alternating going ”to the left” and ”to the right” as
the snake places particles on R and L resp.
Fig. 1 depicts snapshots of the HEX and TRI algorithms.
Detailed descriptions of the algorithms, as well as correctness proofs, appear in [1]; for full simulation runs illustrating
both the HEX and TRI algorithms, see http://sops.cs.upb.de.

4.

REFERENCES

[1] Z. Derakhshandeh, R. Gmyr, A. W. Richa, C. Scheideler,
and T. Strothmann. An algorithmic framework for shape
formation problems in self-organizing particle systems.
arXiV, abs/1504.00744, 2015.
[2] Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. Bazzi,
A. W. Richa, and C. Scheideler. Leader election and shape
formation with self-organizing programmable matter. To
appear in proceedings of DNA21, 2015.

Brief Announcement: Dynamic Routing and Location
Services in Metrics of Low Doubling Dimension
Goran Konjevod

Andréa W. Richa

Donglin Xia

Arizona State University

Arizona State University

Arizona State University

goran@asu.edu

aricha@asu.edu

dxia@asu.edu

Categories and Subject Descriptors: C.2.2 [Computer
Communication Network]: Distributed Systems
General Terms: Algorithms, Design
Keywords: Dynamic Routing, Location Services, Doubling
Dimension
A routing scheme on the graph G = (V, E) is a distributed
algorithm that allows any source node to send packets to any
destination node along the links of E. A routing scheme
on a metric space (M, d) builds a graph G = (M, E) by
distributedly selecting the edges (u, v) to be in E and routes
only along the edges of E. The stretch of a routing path is its
length divided by the length of the shortest path between its
endpoints. The stretch of a routing scheme is the maximum
stretch of a routing path. A routing scheme is compact if the
routing table and packet header size are both polylog(|M |).
Compact routing research has recently focused on graphs
of low doubling dimension [10, 3, 8, 2, 5, 6, 7, 9], (the doubling dimension of a metric space is the minimum α such
that any ball of radius r can be covered by at most 2α balls
of radius r/2.) However, all of these schemes are static and
assume a fixed metric. Moreover, they assume a centralized
pre-configuration procedure to build the routing tables.
We finally cross the gap from static to dynamic (optimal
stretch) compact routing schemes, thus widening the applicability of such schemes to more realistic dynamic scenarios. We describe the first fully dynamic distributed compact
routing schemes with optimal-stretch in both labeled (1 + 
stretch) and name-independent models (9 +  stretch). More
precisely, our schemes work on a dynamic set of nodes V in
a metric of doubling dimension α = O(log log ∆) (where ∆
is the normalized diameter of the graph), use routing tables, label and packet headers of size polylogarithmic in the
network size and the normalized diameter. The number of
messages, amortized per operation, grows polylogarithmically with the network size and ∆. We support node join,
leave and move operations. Finally, the move operation is
locality-sensitive, in that the cost of the movement of a node
is proportional to the distance moved.
An important application of our result is the design of dynamic Distributed Hash Tables (DHTs) for highly-scalable
peer-to-peer systems. A DHT (or object/service location
scheme) is a dictionary data structure implemented in a distributed way, thus allowing efficient object location (lookup),
where objects may be data items (files), nodes, or services.
If each node publishes its own name as an object, DHT
design reduces to compact name-independent routing on the

shortest-path metric induced by the network: the dynamic
graph G maintained by the routing scheme will correspond
to the DHT overlay network. Our routing scheme generalizes to DHTs where nodes may hold multiple (copies
of) objects and where the network may contain duplicates
of any object. We also achieve constant stretch, polylogarithmic degree and storage space, and locality-sensitive node
move/join/leave and publish/unpublish operations.
Another application of our routing scheme is locating nodes
in mobile ad-hoc networks, using our locality-sensitive protocols for adapting to node movement, as well as join and
leave operations. Rigorous theoretical study of this problem
had been limited to close-to-uniform node distributions [4,
1]. Thus, our results imply constant stretch, low degree
compact node location schemes in mobile ad-hoc networks
of low doubling dimension which adapt near-optimally to
node move/join/leave operations.

1.

REFERENCES

[1] I. Abraham, D. Dolev, and D. Malkhi. LLS: a locality
aware location service for mobile ad hoc networks. In Proc.
2004 DIALM-POMC, 2004.
[2] I. Abraham, C. Gavoille, A. V. Goldberg, and D. Malkhi.
Routing in networks with low doubling dimension. In Proc.
26th ICDCS, page 75, 2006.
[3] H. T.-H. Chan, A. Gupta, B. M. Maggs, and S. Zhou. On
hierarchical routing in doubling metrics. In Proc. 16th
SODA, pages 762–771, 2005.
[4] R. Flury and R. Wattenhofer. MLS: an efficient location
service for mobile ad hoc networks. In Proc. 7th MobiHoc,
pages 226–237, 2006.
[5] G. Konjevod, A. W. Richa, and D. Xia. Optimal-stretch
name-independent compact routing in doubling metrics. In
Proc. 25th PODC, pages 198–207, 2006.
[6] G. Konjevod, A. W. Richa, and D. Xia. Optimal scale-free
compact routing schemes in networks of low doubling
dimension. In Proc. 18th SODA, pages 939–948, 2007.
[7] G. Konjevod, A. W. Richa, D. Xia, and H. Yu. Compact
routing with slack in low doubling dimension. In Proc. 26th
PODC, pages 71–80, 2007.
[8] A. Slivkins. Distance estimation and object location via
rings of neighbors. In Proc. 24th PODC, 2005.
[9] A. Slivkins. Towards fast decentralized construction of
locality-aware overlay networks. In Proc. 26th PODC,
pages 89–98, 2007.
[10] K. Talwar. Bypassing the embedding: algorithms for low
dimensional metrics. In Proc. 36th STOC, pages 281–290,
2004.

Copyright is held by the author/owner(s).
PODC’08, August 18–21, 2008, Toronto, Ontario, Canada.
ACM 978-1-59593-989-0/08/08 .

417

C OM BIN A TORIC A

COMBINATORICA 19 (3) (1999) 375–401

Bolyai Society – Springer-Verlag

FAST ALGORITHMS FOR FINDING O(Congestion + Dilation)
PACKET ROUTING SCHEDULES
TOM LEIGHTON, BRUCE MAGGS and ANDRÉA W. RICHA
Received July 8, 1996

In 1988, Leighton, Maggs, and Rao showed that for any network and any set of packets whose
paths through the network are fixed and edge-simple, there exists a schedule for routing the packets
to their destinations in O(c+d) steps using constant-size queues, where c is the congestion of the
paths in the network, and d is the length of the longest path. The proof, however, used the Lovász
Local Lemma and was not constructive. In this paper, we show how to find such a schedule in
O(m(c+d)(log P)4 (log log P)) time, with probability 1−1/P β , for any positive constant β, where
P is the sum of the lengths of the paths taken by the packets in the network, and m is the number
of edges used by some packet in the network. We also show how to parallelize the algorithm so
that it runs in N C. The method that we use to construct the schedules is based on the algorithmic
form of the Lovász Local Lemma discovered by Beck.

1. Introduction
I n this paper, we consider the problem of scheduling the movements of packets
whose paths through a network have already been determined. The problem is
formalized as follows. We are given a network with n nodes (switches) and m edges
(communication channels). Each node can serve as the source or destination of an
arbitrary number of packets (or cells or flits, as they are sometimes referred to).
Let N denote the total number of packets to be routed. The goal is to route the
N packets from their origins to their destinations via a series of synchronized time
steps, where at each step at most one packet can traverse each edge and each packet
can traverse at most one edge. Without loss of generality, we assume that all edges
in the network are used by the path of some packet, and thus that m gives the
number of such edges (all the other edges are irrelevant to our problem).

Mathematics Subject Classification (1991): 68M20, 68M10, 68M07, 60C05
Tom Leighton is supported in part by ARPA Contracts N00014-91-J-1698 and N00014-92J-1799. Bruce Maggs is supported in part by an NSF National Young Investigator Award under
Grant No. CCR–94–57766, with matching funds provided by NEC Research Institute, and by
ARPA Contract F33615–93–1–1330. Part of this research was conducted while Andréa Richa
was at Carnegie Mellon University, supported by NSF National Young Investigator Award under
Grant No. CCR–94–57766, with matching funds provided by NEC Research Institute, and ARPA
Contract F33615–93–1–1330.

c
0209–9683/99/$6.00 
1999
János Bolyai Mathematical Society

376

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

Figure 1 shows a 5-node network in which one packet is to be routed to each
node. The shaded nodes in the figure represent switches, and the edges between
the nodes represent channels. A packet is depicted as a square box containing the
label of its destination.
During the routing, packets wait in three different kinds of queues. Before the
routing begins, packets are stored at their origins in special initial queues. When a
packet traverses an edge, it enters the edge queue at the end of that edge. A packet
can traverse an edge only if at the beginning of the step, the edge queue at the
end of that edge is not full. Upon traversing the last edge on its path, a packet is
removed from the edge queue and placed in a special final queue at its destination.
In Figure 1, all of the packets reside in initial queues. For example, packets 4 and 5
are stored in the initial queue at node 1. In this example, each edge queue is empty,
but has the capacity to hold two packets. Final queues are not shown in the figure.
Independent of the routing algorithm used, the sizes of the initial and final queues
are determined by the particular packet routing problem to be solved. Thus, any
bound on the maximum queue size required by a routing algorithm refers only to
the edge queues.

5

4

1

2

3
1

5

4

2

3

Fig. 1. A graph model for packet routing

We focus on the problem of timing the movements of the packets along their
paths. A schedule for a set of packets specifies which move and which wait at each
time step. The length of a schedule is the number of time steps required to route all
the packets to their destinations according to the schedule. Given any underlying
network, and any selection of paths for the packets, our goal is to produce a schedule

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

377

for the packets that minimizes the length of the schedule and the maximum queue
size needed to route all of the packets to their destinations.
Of course, there is a strong correlation between the time required to route the
packets and the selection of the paths. In particular, the maximum distance, d,
traveled by any packet is always a lower bound on the time. We call this distance
the dilation of the paths. Similarly, the largest number of packets that must traverse
a single edge during the entire course of the routing is a lower bound. We call this
number the congestion, c, of the paths. Figure 2 shows a set of paths for the packets
of Figure 1 with dilation 3 (since the path followed by the packet going from node
5 to node 3 has length 3) and congestion 3 (since three paths use the edge between
nodes 1 and 2).

5

4

1

2

3
1

5

4

2

3

Fig. 2. A set of paths for the packets with dilatation d = 3 and congestion c = 3

Given any set of paths with congestion c and dilation d, in any network, it is
straightforward to route all of the packets to their destinations in cd steps using
queues of size c at each edge. In this case the queues are big enough that a packet
can never be delayed by a full queue in front, so each packet can be delayed at most
c − 1 steps at each of at most d edges on the way to its destination.
In [9], Leighton, Maggs, and Rao showed that there are much better schedules.
In particular, they established the existence of a schedule using O(c + d) steps and
constant-size queues at every edge, thereby achieving the naive lower bounds (up to
constant factors) for any routing problem. The result is highly robust in the sense
that it works for any set of edge-simple paths and any underlying network. (A
priori, it would be easy to imagine that there might be some set of paths on some

378

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

network that requires more than Ω(c+d) steps or larger than constant-size queues to
route all the packets.) The method that they use to show the existence of optimal
schedules, however, is not constructive. In other words, prior to this work, the
fastest known algorithms for producing schedules of length O(c+d) with constantsize edge queues required time that is exponential in the number of packets.
1.1. Our results
In this paper, we show how to produce schedules of length O(c + d) in O(m(c +
d)(log P)4 (log log P)) time steps, with probability at least 1−1/P δ , for any constant
δ > 0, where m is the number of distinct edges traversed by some packet in the
network. The schedules can also be found in polylogarithmic time on a parallel
computer using O(m(c + d)(log P)4 (log log P)) work, with probability at least 1 −
1/P δ .
The algorithm for producing the schedules is based on an algorithmic form
of the Lovász Local Lemma (see [6] or [20, pp. 57–58]) discovered by Beck [3].
Showing how to modify Beck’s arguments so that they can be applied to scheduling
problems is the main contribution of our work. Once this is done, the construction
of asymptotically optimal routing schedules is accomplished using the methods of
[9].
The result has several applications. For example, if a particular routing problem is to be performed many times over, then it may be feasible to compute an
asymptotically optimal schedule once using global control. This situation arises in
network emulation problems.
Typically, a guest network G can be emulated by a host network H by embedding G into H. An embedding maps nodes of G to nodes of H, and edges of
G to paths in H — an edge (x, y) of G is mapped to some path in H between the
nodes of H that x and y were mapped to. There are three important measures of
an embedding: the load, congestion, and dilation. The load of an embedding is the
maximum number of nodes of G that are mapped to any one node of H. The congestion of an embedding is the maximum number of paths corresponding to edges
of G that use any one edge of H. The dilation of an embedding is the length of the
longest path of H in the embedding. Let l, c, and d denote the load, congestion,
and dilation of the embedding, respectively.
Once G has been embedded in H, H can emulate G in a step-by-step fashion as
follows. Each node of H first emulates the local computations performed by the l (or
fewer) nodes mapped to it. This takes O(l) time. Then for each packet sent along
an edge of G, H sends a packet along the corresponding path in the embedding.
The algorithm described in this paper can be used to produce a schedule in which
the packets are routed to their destinations in O(c+d) steps. Thus, H can emulate
each step of G in O(l+c+d) steps with constant size queues, where l is the load of
this embedding.

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

379

Our packet routing result also has applications to job-shop scheduling. In particular, consider a scheduling problem with jobs j1 , . . . , jr , and machines m1 , . . . , ms ,
for which each job must be performed on a specified sequence of machines. In this
application, we assume that each job occupies each machine that works on it for
a unit of time, and that no machine has to work on any job more than once.
Of course, the jobs correspond to packets, and the machines correspond to edges.
Hence, we can define the dilation of the scheduling problem to be the maximum
number of machines that must work on any job, and the congestion to be the maximum number of jobs that have to be run on any machine. As a consequence of
the packet routing result, we know that any scheduling problem can be solved in
O(c + d) steps. In addition, we know that there is a schedule for which each job
waits at most O(c + d) steps before it starts running, and that each job waits at
most a constant number of steps in between consecutive machines. The queue of
jobs waiting for any machine will always have at most a constant number of jobs.
1.2. Related work
Scheideler recently presented in [18] an alternative simpler proof (than that of [9])
for the existence of O(c + d)-step schedules that only require edge queues of size 2.
The main idea in his proof is to decompose the problem in a different way by using
“secure edges”.
In [13], Meyer auf der Heide and Scheideler showed the existence of an off-line
protocol that only requires edge queues of size 1. However, the schedule produced
by this protocol has length O([d+c(log(c+d))(log log(c+d))] log log log(1+) (c+d)),
for any constant  > 0.
For the class of leveled networks, Leighton, Maggs, Ranade, and Rao [8] showed
that there is a simple on-line randomized algorithm for routing the packets to their
destinations within O(c + L + log N ) steps, with high probability, where L is the
number of levels in the network, and N is the total number of packets. (In a leveled
network with L levels, each node is labeled with a level number between 0 and L−1,
and every edge that has its tail on level i has its head on level i+1, for 0 ≤ i < L−1.)
Mansour and Patt-Shamir [12] showed that if packets are routed greedily on
shortest paths, then all of the packets reach their destinations within d + N steps.
These schedules may be much longer than optimal, however, because N may be
much larger than c. Meyer auf der Heide and Vöcking [14] devised a simple on-line
randomized algorithm that routes all packets to their destinations in O(c+d+log N )
steps, with high probability, provided that the paths taken by the packets are shortcut free (e.g., shortest paths).
Recently, Rabani and Tardos [16], and Ostrovsky and Rabani [15] extended
the main ideas used in [9] and in the centralized algorithm presented in this work
to obtain on-line local control algorithms for the general packet routing problem
that produce near-optimal schedules. More specifically, Rabani and Tardos [16]
show a randomized on-line algorithm that with high probability delivers all packets

380

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA
∗

in O(c + d((log ∗ N )O(log N ) ) + log6 N ) steps; Ostrovsky and Rabani [15] improved
on this result by presenting a randomized on-line algorithm that delivers all the
packets to their destinations in O(c+d+log(1+) N ) steps with high probability, for
arbitrary  > 0.
It was also in recent work that Srinivasan and Teo [21] answered a long-standing
question: Given source and destination nodes for each packet, can we select the
routing paths for the N packets, with congestion c and dilation d, in order to
approximate the minimum value of c + d (over all possible choices of paths) to
within a constant factor? (Finding the minimum value of c + d is NP-hard.) They
provided an algorithm that selects such paths in polynomial time; by applying
our algorithm on the selected paths, Srinivasan and Teo described the first off-line
constant factor approximation algorithm for routing N packets (if we are only given
the source and destination nodes of each packet) using constant-size queues. It is
interesting to note that there is still no known polynomial-time algorithm for which
the congestion c alone is asymptotically optimal: It was clever (and crucial) that
Srinivasan and Teo minimized the sum c + d rather than just c.
The problem of scheduling packets through given paths strongly relates to
network emulations via embeddings, as we discussed. Koch et al. in [7], and Maggs
and Schwabe in [11] address the problem of performing network emulations via
embeddings.
Shmoys, Stein, and Wein [19] give randomized and deterministic algorithms
that produce schedules of length within a polylogarithmic factor of that of an
optimal schedule for a more general job-shop scheduling problem than the one we
consider, when jobs are not assumed to have unit length and a machine may have
to work on the same job more than once.
1.3. Outline
The remainder of this paper is divided as follows. In Section 2, we give a very brief
overview of the non-constructive proof in [9]. We also introduce some definitions,
and present two important propositions and a new lemma that will be of later
use. In Section 3, we describe how to make the non-constructive method in [9]
constructive. In Section 4, we analyze the running time of the algorithm. The
propositions presented in Sections 2 and 3 are meant to replace (and are numbered
according to) some of the lemmas in [9].
In Section 5, we show how to parallelize the scheduling algorithm. We conclude
with some remarks in Section 6.
2. Preliminaries
In [9], Leighton, Maggs, and Rao proved that for any set of packets whose paths
are edge-simple and have congestion c and dilation d, there is a schedule of length
O(c+d) in which at most one packet traverses each edge of the network at each step,

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

381

and at most a constant number of packets wait in each queue at each step. (An
edge-simple path uses no edge more than once.) Note that there are no restrictions
on the size, topology, or degree of the network or on the number of packets.
The strategy for constructing an efficient schedule is to make a succession of
refinements to the “greedy” schedule, S0 , in which each packet moves at every step
until it reaches its final destination. This initial schedule is as short as possible: Its
length is only d. Unfortunately, as many as c packets may traverse an edge at a
single time step in S0 , whereas in the final schedule at most one packet is allowed
to traverse an edge at each step. Each refinement will bring us closer to meeting
this requirement.
The proof uses the Lovász Local Lemma ([6] or [20, pp. 57–58]) at each
refinement step. Given a set of “bad” events in a probability space, the lemma
provides a simple inequality that, when satisfied, guarantees that with probability
greater than zero, no bad event occurs. The inequality relates the probability that
each bad event occurs with the dependence among them. A set of bad events
A1 , A2 , . . . , Aq in a probability space has dependence at most b if every event is
mutually independent of some set of q − b − 1 other events. The lemma is nonconstructive; for a discrete probability space, it shows only that there exists some
elementary outcome that is not in any bad event.
Lemma [Lovász]. Let A1 , A2 , . . . , Aq be a set of “bad” events, each occurring with
probability p, and with dependence at most b. If 4pb < 1, then with probability
greater than zero, no bad event occurs.
Before proceeding, we need to introduce some notation. A T -frame — or a
frame of size T — is a sequence of T consecutive time steps in a given schedule.
The congestion of an edge g in a T -frame is the number of packets that traverse g in
this frame; the relative congestion of edge g in a T -frame is given by the congestion
of g in the frame divided by the frame size T . The frame congestion of a T -frame is
the largest congestion of an edge in the frame; the relative congestion of a T -frame
is the largest relative congestion of an edge in the frame.
2.1. A pair of tools for later use
In this section we re-state Lemma 3.5 of [9] and we prove Proposition 3.6, which
replaces Lemma 3.6 of [9]. Both will be used in the proofs in Section 3.
Lemma 3.5. [9] In any schedule, if the number of packets that traverse a particular
edge g in any y-frame is at most Ry, for all y between T and 2T − 1, then the
number of packets that traverse g in any y-frame is at most Ry, for all y ≥ T .
Proof. Consider a frame τ of size T 0 , where T 0 > 2T − 1. The first (bT 0 /T c − 1)T
steps of the frame can be broken into T -frames. In each of these T -frames, at most
RT packets traverse g. The remainder of the T 0 -frame τ consists of a single y-frame,
where T ≤ y ≤ 2T − 1, in which at most Ry packets traverse g.

382

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

The following proposition is basically a re-statement of Lemma 3.6 of [9] and
will be used here in place of this lemma. Proposition 3.6 applies when the number
of distinct edges traversed by the packets in the schedule considered, m0 , is at most
a polynomial in I (I as defined below).
Proposition 3.6. Suppose that (i) there are positive constants α1 , α2 , α3 , where
α1 ≥ α2 , α1 ≥ 2α3 , and α3 ≥ α2 ; (ii) I is greater than some sufficiently large
constant; and (iii) for all edges g, in a schedule of length I α1 (or smaller), the
relative congestion of edge g in frames of size I α2 or larger is at most ρg , where
ρg is a constant. Let m0 be the number of distinct edges traversed by the packets
in this schedule. Furthermore, suppose that each packet is assigned a delay chosen
randomly, independently, and uniformly from the range [0, I α2 ], and that if a packet
is assigned a delay of x, then x delays are inserted in the first I α3 steps and I α2 −x
delays are inserted in the last I α3 steps of the schedule.
1. Then for any constant ξ > 0, there exists a constant k1 > 0 such that with
probability at least 1 − m0 /I ξ the relative congestion of any edge g in any
α3
frame of size log2 I or larger, in between the
√ first and last I steps of the new
schedule is at most ρg (1 + σ), for σ = k1 / log I.
2. We can find such a schedule and verify whether it satisfies the conditions in 1.
in O(m0 I α1 (log2 I)) time.
Proof. To bound the relative congestions of each edge in frames of size log2 I or
larger, we need to consider all m0 edges and, by Lemma 3.5, all frames of size
between log2 I and 2 log2 I − 1.
As we shall see, the number of packets that traverse an edge g during a
particular T -frame τ has a binomial distribution. In the new schedule, a packet can
traverse g during τ only if in the original schedule it traversed g during τ or during
one of the I α2 steps before the start of τ . Since the relative congestion of edge g
in any frame of size I α2 or greater in the original schedule is at most ρg , there are
at most ρg (I α2 + T ) such packets. The probability that an individual packet that
could traverse g during τ actually does so is at most T /I α2 . Thus, the probability
p that ρ0g or more packets traverse an edge g during a particular T -frame τ is at
most
ρg (I α2 +T ) 

p≤

X

k=ρ0g

α2
 


ρg (I α2 + T )
T k
T ρg (I +T )−k
.
1− α
I α2
I 2
k

To estimate the area under the tails of this binomial distribution, we use the
following Chernoff-type bound [5]. Suppose that there are x independent Bernoulli
trials, each of which is successful with probability p0 . Let S denote the number of
successes in the x trials, and let µ = E[S] = xp0 . Following Angluin and Valiant [2],
we have
Pr[S ≥ (1 + γ)µ] ≤ e−γ

2 µ/3

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

383

for 0 ≤ γ ≤ 1. (Note that e denotes the base of the natural logarithm.)
In our application, x = ρg (I α2 + T ), p0 = T /I α2 , and so µ = ρg (I α2 + T )T /I α2 .
p
For γ = 3k0 / log I (where k0 is a positive constant to be specified later), ρg ≥ 1,
and T ≥ log2 I, we have Pr[S ≥ (1 + γ)µ] ≤ e−k0 ρg (I 2 +T )T /(I 2 log I) ≤ e−k0 log I ≤
p
e−k0 ln I = 1/I k0 . Set ρ0g T to be (1 + γ)µ = (1 + 3k0 / log I)ρg (I α2 + T )T /I α2 .
√
√
For I large enough, 2 log2 I/I α2 ≤ 1/ log I, and thus ρ0g ≤ ρg (1 + k1 / log I), for
√
√
some constant k1 ≥ 2 3k0 + 1. Let σ = k1 / log I. Then ρ0g ≤ ρg (1 + σ). Thus
α

α

p = Pr[S ≥ ρ0g T ] ≤ Pr[S ≥ (1 + γ)µ] ≤ 1/I k0 .
Since there are at most (I α1 +I α2 ) ≤ 2I α1 starting points for a frame, and log2 I
different size frames starting at each point, and there are at most m0 distinct edges
per frame, the probability that the relative congestion of any edge g is more than
ρ0g in any frame is at most 2m0 I α1 log2 I/I k0 ≤ m0 /I k0 −α1 −2 (since 2 log2 I ≤ I 2 ).
For any ξ > 0, we set k0 = ξ+α1 +2, which in turn sets k1 and σ, and completes the
proof of part 1.
We assign a random delay to each packet, and verify whether the conditions in
1. apply as follows. We construct the schedule by routing all the packets one step
at a time. At time t, for I α3 ≤ t ≤ (I α1 + I α2 − I α3 ), we compute the congestion of
edge g in a T -frame ending at t, for all edges g that are traversed by some packet
in the schedule, for all T ∈ [log2 I, 2 log2 I −1] using the following rules: if T = log2 I
then the congestion of g in a T -frame ending at time t can be computed by taking
the congestion of g in the T -frame ending at t−1, subtracting the number of packets
that traverse edge g at time t − T and adding the number of packets that traverse
g at time t; otherwise if t ≥ T , then the congestion of edge g in a T -frame that ends
at t is given by the congestion of g in a (T − 1)-frame that ends at t − 1 plus the
number of packets that traverse edge g at time t. The relative congestion of an edge
in a frame is given by the congestion of the edge in the frame divided by the size
of the frame. This can be done in time O(m0 (I α1 + I α2 ) log2 I) = O(m0 I α1 log2 I),
m0 being the number of distinct edges traversed in this schedule.
In the remainder of this paper, for a schedule of size polynomial in I, we
assume that we check for the congestions of all T -frames, log2 I ≤ T < 2 log2 I, of
the schedule as described in the proof of Proposition 3.6.
3. An algorithm for constructing optimal schedules
In this section, we describe the key ideas required to make the non-constructive
proof of [9] constructive. There are many details in that proof, but changes are
required only where the Lovász Local Lemma is used, in Lemmas 3.2, 3.7, and
3.9 of [9]. The non-constructive proof showed that a schedule can be modified by
assigning delays to the packets in such a way that in the new schedule the relative

384

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

congestion can be bounded in much smaller frames than in the old schedule. In
this paper, we show how to find the assignment of delays quickly. We will not
regurgitate the entire proof of [9], but only reprove those three lemmas, trying
to state the replacement propositions in a way as close as possible to the original
lemmas.
In Section 3.1, we provide Proposition 3.2, which is a constructive version of
Lemma 3.2 of [9]. In Sections 3.2 and 3.3, we provide three propositions which are
meant to replace Lemma 3.7 of [9]. Lemma 3.7 is applied O(log∗ (c + d)) times in
[9]. We will use Propositions 3.7.1 and 3.7.2 to replace the first two applications
of Lemma 3.7. The remaining applications will be replaced by Proposition 3.7.3.
In Section 3.4, we present the three replacement propositions for Lemma 3.9 of
[9]. Our belief is that a reader who understands the structure of the proof in [9]
and the propositions in this paper can easily see how to make the original proof
constructive. We analyze the running time of our algorithm in Section 4.

3.1. The first reduction in frame size
For a given set of N packets, let c and d denote the congestion and dilation of the
paths taken by these packets, and let P denote the sum of the lengths of these
paths. Note that m ≤ P ≤ mc, and that c, d < P, where m is the number of edges
traversed by some packet in the network. The following proposition is meant to
replace Lemma 3.2 of [9]. It is used just once in the proof, to reduce the frame size
from d to log P.

Proposition 3.2. For any constant β > 0, there is a constant α > 0, such that there
exists an algorithm that constructs a schedule of length d + αc in which packets
never wait in edge queues and in which the relative congestion in any frame of size
log P or larger is at most 1. The algorithm runs in O(m(c + d)(log P)) time steps,
and succeeds with probability at least 1 − 1/P β .

Proof. The algorithm is simple: Assign each packet an initial delay that is chosen
randomly, independently, and uniformly from the range [0, αc], where α is a constant
that will be specified later; the packet will wait out its initial delay and then travel
to its destination without stopping. The length of the new schedule is at most
αc + d.
To bound the relative congestion in frames of size log P or larger, we need
to consider all m edges and, by Lemma 3.5, all frames of size between log P and
2 log P − 1. We assume without loss of generality that c > 2 log P, since any frame
of length c or larger has relative congestion at most 1. For any particular edge g,
and T -frame τ , where log P ≤ T ≤ 2 log P − 1, the probability p that more than T

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

385

packets use g in τ is at most
p≤


   i 
c
X
c
T
T c−i
1−
αc
αc
i

i=T +1



  (T +1)
c
T
≤
T +1
αc
 e (T +1)
≤
α
since each of the at most c packets that pass through g has probability at most

T /αc of using g in τ , and since ab ≤ (ae/b)b , for any 0 < b ≤ a. The total number
of frames to consider is at most (αc+d) log P, since there are at most αc+d places
for a frame to start and log P frame sizes. Thus the probability that the relative
congestion of any edge is too large in any frame of size log P or larger is at most
m(log P)(αc + d)

 e log P
α

.

We bound the probability above by summing the probabilities that the relative
congestion is too large for all m(log P) edge-frame pairs. Using the inequalities
P ≥ c, P ≥ m, and P ≥ d, we have that for any constant β > 0, there exists a
constant α > 0, such that this probability is at most 1/P β .
Assigning the delays to the packets takes O(N ) time steps. Verifying whether
the relative congestion is at most 1 in any T -frame of size log P ≤ T ≤ 2 log P −1 can
be done in O(m(c+d)(log P)) time steps (as described in the last paragraph of the
proof of Proposition 3.6).
Before applying Proposition 3.7.1, we first apply Proposition 3.2 to produce a
schedule S1 of length O(c+d) in which the relative congestion in any frame of size
log P or larger is at most 1. For any positive constant β, this step succeeds with
probability at least 1 − 1/P β . If it fails, we simply try again.
3.2. A randomized algorithm to reduce the frame size
In this section, we prove two very similar propositions, Propositions 3.7.1 and 3.7.2,
which are meant to replace the first two applications of Lemma 3.7 of [9], which we
state below. In proving all these propositions, we use a constructive version of the
Lovász Local Lemma that applies to scheduling problems. Let I ≥ 0. We break a
schedule S into blocks of 2I 3 + 2I 2 − I consecutive time steps. The size of a block
is the number of time steps it spans.
Lemma 3.7. [9] In a block of size 2I 3 + 2I 2 − I, let the relative congestion in any
frame of size I or greater be at most r, where 1 ≤ r ≤ I. Then there is a way of

386

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

assigning delays to the packets so that in between the first and the last I 2 steps of
this block, the relative congestion in√any frame of size I1 = log2 I or greater is at
most r1 = r(1 + 1 ), where 1 = O(1)/ log I.
After applying Proposition 3.2 to reduce the frame size from d to log P, Propositions 3.7.1 and 3.7.2 are used once on each block (for I = log P and I = (log log P)2
respectively) to further reduce the frame size. Unlike Lemma 3.7 of [9], Propositions 3.7.1 and 3.7.2 may increase the relative congestion by a constant factor. In
general, we cannot afford to pay a constant factor at each of the O(log∗ (c + d))
applications of Lemma 3.7 of [9], but we can afford to pay it twice.
These two propositions avoid the use of exhaustive search, since they relate
to problem sizes that are still large: In these propositions we design separate
algorithms that use the fact that the problem sizes are sufficiently large in order
to guarantee a “good” solution with high probability. In the remainder of this
paper, we assume without loss of generality that P is not a constant. If P = O(1),
then we can find an optimal schedule in a constant number time steps by using
exhaustive search. For the application of Proposition 3.7.1, I = log P and r = 1.
With probability at least 1−1/P β , for any constant β > 0, we succeed in producing
a schedule S2 in which the relative congestion is O(1) in frames of size log2 I =
(log log P)2 or greater (if we should fail, we simply try again). In the application of
Proposition 3.7.2, I = (log log P)2 , and r = O(1); in the resulting schedule, S3 , the
relative congestion is O(1) in frames of size log2 ((log log P)2 ) = (log log log P)O(1) or
greater, with probability at least 1 − 1/P β , for any constant β > 0. At this point,
the problem sizes are small enough for using exhaustive search, and we start using
Proposition 3.7.3.
Proposition 3.7.1. Let the relative congestion in any frame of size I or greater be
at most r in a block of size 2I 3 + 2I 2 − I, where 1 ≤ r ≤ I and I = log P. Let q be
the number of distinct edges traversed by the packets in this block. Then, for any
constant β > 0,
1. there is an algorithm for assigning initial delays in the range [0, I] to the packets
so that in between the first and last I 2 steps of the block, the relative congestion
in any frame of size log2 I or greater is at most r0 , where r0 = 2r(1 + σ) and
√
σ = O(1)/ log I;
2. this algorithm runs in O(q(log P)4 (log log P)) time steps, with probability at
least 1 − 1/P β .
Proof. We define the bad event for each edge g in the network and each T -frame
τ , log2 I ≤ T ≤ 2 log2 I − 1, as the event that more than r0 T packets use g in τ . A
particular bad event may or may not occur — i.e., may or may not be true — in a
given schedule. If no bad event occurs, then by Lemma 3.5, the relative congestion
in all frames of size log2 I or greater will be at most r0 . Since there are log2 I

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

387

different frame sizes and there are at most (2I 3 + 2I 2 − I) + I = 2I 3 + 2I 2 different
frames of any particular size, the total number of bad events involving any one edge
is at most (2I 3 + 2I 2 ) log2 I < I 4 , for I greater than some large enough constant.
We now describe the algorithm for finding the assignment. In a first pass of
assigning delays to the packets, we process the packets one at a time. To each
packet, we assign a delay chosen randomly, independently, and uniformly from 0 to
I. We then examine every event in which the packet participates.
A packet can use an edge g in a T -frame τ only if it traversed edge g in τ
or in one of the I steps preceding τ in the original schedule. At most r(T + I)
packets use edge g in a frame of size (I + T ), since the relative congestion in this
frame is at most r in the original schedule. Thus at most r(T + I) packets can
traverse edge g in the new schedule (after delays are assigned to the packets). We
call these at most r(I +T ) packets the candidate packets to use edge g in τ . Let Cg
be the number of candidate packets to use g in τ that have been assigned delays
so far. We say that the event
√ for an edge g and a T -frame τ is critical if more
than Cg T /I + kr(I + T )T /(I log I) packets actually traverse edge g in τ , where k
is a positive constant to be specified later. Intuitively, an event becomes critical if
the number of packets assigned delays so far that traverse edge g in τ exceeds
√ the
expected number of such packets, Cg T /I, by an excess term kr(I +T )T /(I log I).
√
Since Cg ≤ r(T + I), we allow an excess of at most k/ log I times the expected
number of packets that would use edge g in the frame if all of the packets were
assigned delays. Hence, the maximum final excess allowed does not depend on Cg .
If a packet causes an event to become critical, then we set aside all of the other
packets that could also use g during τ , but whose delays have not yet been assigned.
The main difference between our algorithm and Beck’s [3] constructive version
of the Lovász Local Lemma is that we never allow the number of packets passing
through an edge in a T -frame to deviate from the expectation by more than a low
order term. In particular, we do not allow this number to deviate by a constant
factor times the expectation. In Beck’s algorithm, the random variable associated
with a bad event (in our case, the number of packets that traverse an edge in a T frame) may deviate from the expectation by a constant factor times the expectation.
We will deal with the packets that have been set aside later. Let P denote
the set of packets that have been assigned delays. As we shall see, after one pass
of assigning random delays to the packets, the problem of scheduling the packets
that have been set aside is broken into a collection of much smaller subproblems,
0
with probability at least 1 − 1/P β , for any constant β 0 > 0. Once the size of a
subproblem (given by the number of edges involved in the subproblem) gets small
compared to the frame length, we can try assigning random delays to the packets
that were set aside.
In this initial pass, we assign a random delay to each packet, and check whether
the event for an edge g traversed by the packet in this block and T -frame τ becomes
critical, for all edges g traversed by the packet in this block and for all frames of
size T in [log2 I, 2 log2 I − 1], by following the same procedure described in the last
paragraph of the proof of Proposition 3.6. Here the schedule length after we insert

388

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

the delays is 2(I 3 + I 2 ) = O(log3 P) (and so there are O(log3 P) starting points for
a T -frame τ ) and there are log2 I = (log log P)2 different frame sizes to consider.
The sum of the lengths of the paths traversed by the packets in this block is q.
Thus, a pass takes O(q(log P)3 (log log P)2 ) time steps. If a pass fails to reduce the
component size, we try again.
In order to proceed, we must introduce some notation. The dependence graph,
G, is the graph in which there is a node for each bad event, and an edge between
two nodes if the corresponding events share a packet. Let b denote the degree of
G. Whether or not a bad event for an edge g and a time frame τ occurs depends
solely on the assignment of delays to the packets that pass through g. Thus, the
bad event for an edge g and a time frame τ , and the bad event for an edge g 0
and a time frame τ 0 are dependent only if g and g 0 share a packet. Since at most
r(2I 3 +2I 2 −I) ≤ rI 4 (for I large enough) packets pass through g and each of these
packets passes through at most 2I 3 +2I 2 −I ≤ I 4 other edges g 0 , and since there are
at most (2I 3 + 2I 2 )(log2 I) ≤ I 4 time frames τ 0 , the dependence b is at most rI 12 .
For r ≤ I, we have b ≤ I 13 . Since the packets use at most q different edges in the
network, and for each edge there are at most I 4 bad events, the total number of
nodes in G is at most qI 4 . We say that a node in G is critical if the corresponding
event is critical. We say that a node is endangered if its event shares a packet with
an event that is critical. After each packet has been either assigned a delay or set
aside, let G1 denote the subgraph of G consisting of the critical and endangered
nodes and the edges between them. If a node is not in G1 , then all of the packets
that use the corresponding edge have already been assigned a delay, and the bad
event represented by that node cannot occur, no matter how we assign delays to
the packets not in P . Hence, from here on we need only consider the nodes in G1 .
Since different components are not connected by edges in G1 , no two components share a packet. Also, any two events that involve edges traversed by the same
packet share an edge in G1 , and so are in the same connected component. Thus
there exists a one-to-one correspondence between components of G1 and disjoint
sets in a partition of the packets not in P . Hence, we can assign the delays to the
packets in each component separately.
In the following claim, we show that, with high probability, the size of the
largest connected component U of G1 is at most I 52 log P, with high probability.
Hence we have reduced the maximum possible size of a component from qI 4 in G
to I 52 log P in G1 . Recall that the constant k (which we still need to specify) is
used to determine whether an event becomes critical.
Claim 1. For any constant β 0 > 0, there exists a constant k > 0 such that the size
of the largest connected component of G1 is at most I 52 log P with probability at
0
least 1 − 1/P β .
Proof. The trick to bounding the size of a largest connected component U is to
observe that the subgraph of critical nodes in U is connected in the cube, G31 , of

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

389

the graph G1 — i.e., in the graph in which there is an edge between two distinct
nodes u and v if and only if in G1 there is a path of length at most 3 between
u and v. In G31 , the critical nodes of U form a connected subgraph because any
path u, e1 , e2 , e3 , v in G1 that connects two critical or endangered nodes u and v by
passing through three consecutive endangered nodes e1 , e2 , e3 can be replaced by
two paths u, e1 , e2 , w and w, e2 , e3 , v of length 3 that each pass through e2 ’s critical
neighbor w. Let G2 denote the subgraph of G31 consisting only of the critical nodes
and the edges between them. Note that the degree of G2 is at most b3 , and if two
critical nodes lie in the same connected component in G2 , then they also lie in the
same connected component in G31 , and hence in G1 .
By a similar argument, any maximal independent set of nodes in a connected
component of G2 is connected in G32 . Note that if a set of nodes is independent
in G2 , then it is also independent in G31 and in G1 . The nodes in an independent
set in G1 do not share any packets, therefore the probabilities that each of these
nodes becomes critical are independent. Let G3 be the subgraph of G32 induced by
the nodes in a maximal independent set in G2 (any maximal independent set in G2
will do). The nodes in G3 form an independent set of critical nodes in G1 . The
degree of G3 is at most b9 .
Our goal now is to show that, for any constant β 0 > 0, there exists a constant
k > 0 such that the number of nodes in any connected component W of G3 is at
0
most log P, with probability 1−1/P β . To begin, with every connected component
W of G3 , we associate a spanning tree of W (any such tree will do). Note that, if
W and W 0 are two distinct connected components of G3 , then the spanning trees
associated with W and W 0 are disjoint.
Now let us enumerate the different trees of size ` in G3 . To begin, a node is
chosen as the root. Since there are at most qI 4 nodes in G3 , there are at most qI 4
possible roots. Next, we construct the tree as we perform a depth-first traversal of
it. Nodes of the tree are visited one at a time. At each node u in the tree, either
a previously unvisited neighbor of u is chosen as the next node to be visited, or
the parent of u is chosen to be visited (at the root, the only option is to visit a
previously unvisited neighbor). Thus, at each node there are at most b9 ways to
choose the next node. Since each edge in the tree is traversed once in each direction,
and there are ` − 1 edges, the total number of different trees with any one root is
at most (b9 )2(`−1) < b18` .
Any tree of size ` in G3 corresponds to an independent set of size ` in G1 ;
moreover, it corresponds to an independent set of ` critical nodes in G1 . We can
bound the probability that all of the nodes in any particular independent subset
U of size ` in G1 are critical
√ as follows. Let pC be the probability that more than
M = CT /I+kr(I+T )T /(I log I) packets use edge g in τ after C candidate packets

390

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

to use g in τ have been assigned delays. Then
pC ≤


   j 
C
X
C
T
T C−j
.
1−
I
I
j

j=M +1

√
For a fixed deviation (in our case, for kr(I+T )T /(I log I)) that does not depend on
C, the probability pC of exceeding this deviation is maximized when
√ C is maximized
— i.e., when C = r(I + T ), which makes M = r(T + I)T (1 + k/ log I)/I. Thus,
pC ≤ pr(I+T ) . Using the Chernoff-type bound as in the proof of Proposition 3.6,
√
with µ = r(I + T )T /I and γ = k/ log I, and k0 = k 2 /3, we have pC ≤ pr(I+T ) =
P r[S ≥ (1+γ)µ] ≤ e−γ µ/3 = e−(k r(I+T )T )/(3I log I) ≤ e−(k0 log I) ≤ e−(k0 ln I) = 1/I k0 ,
since T ≥ log2 I and r > 1. Thus the probability that the event for g and τ
becomes critical after C candidate packets to use g in τ have been assigned delays
is at most 1/I k0 . Since the nodes in U are independent in G1 , the corresponding
events are also independent. Hence the probability that all of the nodes in the
independent set are critical after all packets are assigned delays or put aside is at
most 1/I k0 ` . Thus the probability that there exists a tree of size ` in G3 is at most
2

2

qI 4 b18` /I k0 ` ≤ qI 4−(k0 −234)` (since there exists at most qI 4 b18` different trees of
size ` in G3 and b ≤ I 13 ). Since q ≤ P, we can make this probability less than
0

1/P β , for ` = log P and any fixed constant β 0 > 0, by choosing k to be a sufficiently
0

large constant. Hence, with probability at least 1 − 1/P β , the size of the largest
spanning tree in G3 will be log P.
We can now bound the size of the largest connected component in G1 . Since
(i) the largest connected component in G3 has at most ` nodes with probability
0

at least 1 − 1/P β , (ii) each of these ` nodes may have b3 neighbors in G2 , and
(iii) each node in G2 is either in G3 or is a neighbor of a node in G3 , the largest
connected component in G2 contains at most b3 ` nodes with probability at least
0
1 − 1/P β . As we argued before, the critical nodes in any connected component
of G1 are connected in G2 . Thus, the maximum number of critical nodes in any
connected component of G1 is at most b3 `. Since each of these nodes may have as
many as b endangered neighbors (and each endangered neighbor is adjacent to a
critical node), and since ` = log P, the size of the largest connected component in
G1 is at most b4 ` ≤ I 52 log P, with high probability.
Since I = log P in the scope of this lemma, the size of the largest connected
component in G1 is at most (log P)53 , for k large enough, with probability at least
0

1−1/P β , for any constant β 0 > 0. We still have to find a schedule for the packets not
in P . We now have a collection of independent subproblems to solve, one for each
component in the dependence graph. We can use Proposition 3.6 to find the initial
delays for these packets. Since each node in the dependence graph corresponds to

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

391

an edge in the routing network, a component with x nodes in the dependence graph
corresponds to at most x, and possibly fewer, edges in the routing network.
We apply Proposition 3.6 to each of the independent subproblems. In the
original block, let rg be the relative congestion of edge g with respect to the packets
not in P in frames of size I or larger, and let rgH be the relative congestion of
edge g with respect to the packets in subproblem H in frames of size I or larger
P
(rg = H rgH ). Let q H be the number of distinct edges associated with subproblem
H, for all H. Note that q H ≤ I 52 ` ≤ I 52 log P = I 53 , since I = log P. After applying
Proposition 3.6 to a subproblem H, the relative congestion of any edge g with
respect to the packets in H in frames of size log2 I or larger, in between the first
√
and last I 2 steps in the final schedule is at most rgH (1+k1 / log I), for some constant
k1 > 0, with probability at least 1−q H /I ξ ≥ 1−1/(log P)ξ−53 , for any constant ξ > 0.
Since the routing subproblems are mutually independent and disjoint, if we
apply Proposition 3.6 log P/(log log P) times to each of the at most N ≤ P
subproblems (note that each packet appears in at most one subproblem), then
for any constant ξ > 53, and P large enough, with probability at least
1 − N/(log P)(ξ−53) log P/(log log P) ≥ 1 − 1/P ξ−54 , the relative congestion of edge
g with respect to the packets not in P , in any frame of size log2 I or greater is at
√
√
P
most H rgH (1 + k1 / log I) = rg (1 + k1 / log I).
Applying Proposition 3.6 log P/(log log P) times for each subproblem takes
P
time O( H q H (I 3 + I 2 )(log2 I) log P/ log log P) = O(q(log4 P)(log log P)), since I =
P
log P and q ≥ H q H .
We now have schedules for the packets in P and for the packets not in P .
Fix any edge g traversed by some packet in the block and a T -frame τ , where
T ∈ [log2 I, 2 log2 I − 1]. The total number of candidate packets in P to use edge g
in τ after the delays have been assigned is given by Cg . The number of packets
in P that traverse edge g in τ in the resulting schedule is at most Cg T /I +
√
√
√
kr(I + T )T /(I log I) ≤ r(I + T )T (1 + k/ log I)/I ≤ rT (1 + (2k + 1)/ log I), since
√
(I + T )/I ≤ (1 + 1/ log I), for I large enough, and Cg ≤ r(I + T ). Hence the
relative congestion
√ of any edge g in τ with respect to the packets in P is at most
r(1 + (2k + 1)/ log I).
Now we consider the relative congestion of the packets not in P . With proba0
bility at least 1−1/P β −1/P (ξ−54) , for any positive constants β 0 and ξ, there exists
not in P that√
traverse any edge g in
a constant k1 such that the number of packets
√
the new schedule is at most rg T (1 + k1 / log I) ≤ rT (1 + k1 / log I).
Combining the relative congestions for packets in P and not in P , we get
√ that
the relative congestion of edge g in τ is at most 2r(1 + max(2k + 1, k1 )/ log I).
√
0
Choose σ = max(2k + 1, k1 )/ log I. Choose β such that 1/P β ≥ 1/P β + 1/P ξ−54 .
Hence, for any constant β > 0, there exist positive constants k and k1 such that

392

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

the relative congestion of edge g in τ is at most 2r(1 + σ), for any edge g, for any
T -frame τ , for any T ∈ [log2 I, 2 log2 I − 1], with probability at least 1 − 1/P β .
The number of time steps taken by the algorithm just described is
O(q(log log P + log P)(log3 P)(log log P)) = O(q(log 4 P)(log log P)).
Proposition 3.7.2. Let the relative congestion in any frame of size I or greater be
at most r in a block of size 2I 3 + 2I 2 − I, where 1 ≤ r ≤ I and I = (log log P)2 . Let
q be the number of distinct edges traversed by the packets in this block. Then, for
any constant β > 0,
1. there is an algorithm for assigning initial delays in the range [0, I] to the packets
so that in between the first and last I 2 steps of the block, the relative congestion
in any frame of size log2 I or greater is at most r0 , where r0 = 2r(1 + σ) and
√
σ = O(1)/ log I;
2. this algorithm runs in q(log P)(log log P)6 (log log log P)O(1) time steps, with
probability at least 1 − 1/P β .
Proof. The first part of the proof of this proposition is identical to the part where
we assign delays to the packets in P in the proof of the Proposition 3.7.1 (we let
I = (log log P)2 in that proof).
However, since I = (log log P)2 , we need to make an additional pass assigning delays to the packets in this proof, in order to reduce the component size in
the dependence graph to a polynomial in I. From there, we proceed by applying
Proposition 3.6 to each component separately, as we did in the proof of Proposition 3.7.1. In the first pass, we reduce the maximum component size in G1 from
qI 4 to I 52 log P (by taking ` = log P, as in Proposition 3.7.1), with probability at
0

least 1 − 1/P β , for any constant β 0 > 0. In the second pass, we reduce the component size from I 52 log P down to I 52 log log P ≤ I 53 (by taking ` = log log P in
the part of the proof of Proposition 3.7.1 where we assign delays to the packets
in P , and noting that the number of edges in any connected component is now at
most I 52 log P). For any component, this step will succeed with probability at least
0

1 − 1/(I 52 log P)β , for any constant β 0 > 0. To make this probability as high as it
was in the case I = log P, if a pass fails for any component, we simply try to reduce
the component size again, up to log P/(log log P) times. Then with probability at
0

least 1 − 1/P β , for any constant β 0 > 0, we have reduced the component size to at
most I 53 . Since (i) for each packet assigned a delay in these two passes, we have
to check whether the event for an edge g and a T -frame τ becomes critical, for all
edges g traversed by the packet, for all T -frames τ , for T ∈ [log2 I, 2 log2 I−1] (using
the procedure described in the last paragraph of Proposition 3.6), and since (ii) we
repeat the second pass O(log P/ log log P) times, the two passes take
O(q(I 3 + I 2 )(log2 I)(log P)/(log log P)) ≤

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

393

O(q(log log P)6 log2 ((log log P)2 ) log P/(log log P)) ≤
q(log P)(log log P)5 (log log log P)O(1)
time steps.
The second pass adds some packets to the set P . Let P1 and P2 denote the
number of packets assigned delays in the first and second pass, respectively. Then
the relative congestion due to these packets will be at most
p
p
[(P1 + P2 )T /I + 2kr(I + T )T /(I log I)]/T ≤ r(I + T )/I + 2kr(I + T )/(I log I) ≤
p
p
r[1 + T /I + 2k(I + T )/(I log I)] ≤ r(1 + (4k + 1)/ log I),
√
since T < 2 log2 I and 2 log2 I/I ≤ 1/ log I. If the two passes fail to achieve the
desired relative congestion, we try again.
Now we apply Proposition 3.6 up to log P/(log log log P) times, assigning delays
to the packets not in P , verifying at the end of√each application whether the
schedule obtained has relative congestion r(1+k1 / log I), for some constant k1 to
be specified later. Here we need to apply Proposition 3.6 up to log P/(log log log P)
times to each resulting component (rather than log P/(log log P) as in the proof of
Proposition 3.7.1) since the component size now is O(I 53 ) = (log log P)O(1) , and so
our bound on the failure probability for each component is only 1/(log log P)O(1)
(since the bound given by Proposition 3.6 is at best polynomially small in I and
I = (log log P)2 ), rather than 1/(log P)O(1) . The assignment of delays to the packets
not in P takes at most q(log log P)6 (log log log P)O(1) (log P) time steps. For any
constant β 0 > 0, there exists a constant k1 > 0 such that
√ we obtain a feasible schedule
for these packets with relative congestion r(1+k1 / log I) with probability at least
0

1 − 1/P β .
We have schedules for the packets
in P and for the
√ packets not in P , with
√
relative congestions r(1 + (4k + 1)/ log I) and r(1 + k1 / log I), respectively, with
0

probability at least 1 − 2/P β , for any constant β 0 > 0. The two schedules can be
found in at most q(log P)(log log P)6 (log log log P)O(1) time steps. When we merge
the two schedules, the resulting relative congestion may be as large as the sum of
the two relative congestions — that is, the resulting relative congestion may be
√
as large as 2r(1 + max(4k + 1, k1 )/ log I), with probability at least 1 − 1/P β , for
large enough positive constants k and k1 , for any fixed β > 0 (choose β 0 such that
√
0
1/P β > 2/P β ). Let σ = max(4k + 1, k1 )/ log I.

3.3. Applying exhaustive search
The remaining O(log∗ (c + d)) applications of Lemma 3.7 in [9] are replaced by
applications of the following proposition, which uses the same technique as Propo-

394

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

sitions 3.7.1 and 3.7.2, except that instead of using Proposition 3.6 for each component of the subgraph induced by critical and endangered nodes in the dependence
graph, it uses the Lovász Local Lemma and exhaustive search to find the settings
of the delays for the packets. Proposition 3.7.3 does not allow a constant factor increase in the relative congestion of the refined schedule, which prevents a
∗
2O(log (c+d)) blowup in the final relative congestion.
Proposition 3.7.3. Let the relative congestion in any frame of size I or greater be
at most r in a block of size 2I 3 +2I 2 −I, where 1 ≤ r ≤ I and I ≤ (log log log P)O(1) .
Let q be the number of distinct edges traversed by the packets in this block. Then,
for any constant β > 0,
1. there is an algorithm for assigning initial delays in the range [0, I] to the
packets so that the relative congestion in any frame of size log2 I or greater
in the resulting schedule is at most r0 ,
in between the first and last I 2 steps
√
0
where r = r(1 + σ) and σ = O(1)/ log I;
2. this algorithm runs in q(log P)(log log log P)O(1) (log log log log P)O(1) time
steps, with probability at least 1 − 1/P β .
Proof. This proof uses the Lovász Local Lemma to show that an assignment of
initial delays satisfying the conditions of the proposition exists, as we will see below.
We first assign delays to some packets by making three passes through the
packets using the algorithm of Proposition 3.7.1 (for making the initial pass assign(i)

ing delays to the packets in P ) in each pass. Let Cg , 1 ≤ i ≤ 3, be the number
of candidate packets to use edge g in τ that were assigned delays in the ith pass.
After the first pass, we have that (i) the number of packets assigned delays in this
√
(1)
pass that use edge g in the new schedule is at most Cg T /I +kr(I +T )/(I log I),
0

and (ii) with probability at least 1 − 1/P β , for any constant β 0 > 0, the size of the
largest component in the dependency graph is I 52 log P.
We need to make two more passes assigning delays to the packets, reducing
the size of the largest connected component first to I 52 (log log P), and then to
I 52 (log log log P) = (log log log P)O(1) (since I ≤ (log log log P)O(1) ), by taking ` =
log log P in the second pass and ` = log log log P in the third pass. If we fail to reduce
the component size as desired, the second pass is repeated up to log P/(log log P)
times, and the third pass is repeated up to log P/(log log log P) times. The number
of packets assigned delays in the second (resp., third) pass that traverse edge g in
√
(2)
(3)
the new schedule is at most Cg T /I +kr(I +T )/(I log I) (resp., Cg T /I +kr(I +
√
T )/(I log I)). As before, k is chosen large enough so that the failure probability
0

in each pass is at most 1/P β , for any constant β 0 > 0.
In each pass, we assign a random delay to each packet and check whether
the event for any edge g traversed by this packet and any T -frame τ , where
T ∈ [log2 I, 2 log2 I − 1], becomes critical, as we did in Propositions 3.7.1–2. Thus

395

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

each pass takes time O(q(I 3 +I 2 )(log2 I)) = q(log log log P)O(1) (log log log log P)O(1) .
0

For any constant β > 0, choose β 0 such that 1/P β > 3/P β . Hence, since we may
repeat the second and third passes up to log P/(log log P) and log P/(log log log P),
respectively, we succeed in reducing the component size to (log log log P)O(1) in
q(log P)(log log log P)O(1) (log log log log P)O(1) time steps, with probability at least
1 − 1/P β .
We now use the Lovász Local Lemma to show that there exists a way of
completing the assignment of delays (i.e., to assign delays to the packets not in P )
2
so that the relative
√ congestion in frames of size log I or greater in this block is at
most r(1 + O(1)/ log I). We associate a bad event with each edge and each time
frame of size log2 I through 2 log2 I−1. The bad event for an edge g and a particular
√
T -frame τ occurs when more than Mg = (r(I + T )− Pg )T /I + kr(I + T )T /(I log I)
packets not in P use edge g in τ , where Pg is the number of packets in P that
traverse edge g during τ after the delays have been assigned to the packets in P
(note that there are at most r(T + I) − Pg candidate packets not in P to use edge
g in τ ). As we argued in the proof of Proposition 3.7.1, the total number of bad
events involving any one edge is at most I 4 . We show that if each packet not in P
is assigned a delay chosen randomly, independently, and uniformly from the range
[0, I], then with nonzero probability no bad event occurs. In order to apply the
lemma, we must bound both the dependence of the bad events, and the probability
that any bad event occurs. The dependence b is at most I 13 , as argued before. For
any edge g and T -frame τ that contains g, where log2 I ≤ T ≤ (2 log2 I) − 1, the
probability pg that more than Mg packets not in P use g in τ , can be shown to
be at most 1/I 14 , for sufficiently large k, using exactly the same Chernoff-bound
argument that was used in Proposition 3.7.1. Thus, 4(maxg∈G {pg })b ≤ 4/I < 1 (for
I > 4). Hence, since maxg∈G {pg } is an upper bound on the probability of any bad
event occurring, by the Lovász Local Lemma, there is some way of assigning delays
to the packets not in P so that no bad event occurs.
Since at most r(T + I) packets pass through the edge associated with any
critical node, and there are at most (I + 1) choices for the delay assigned to
each packet, the number of different possible assignments for any subproblem
containing (log log log P)O(1) critical nodes is at most (I+1)r(I+T )(log log log P)

O(1)

4I 2 (log log log P)O(1)

≤

(since r < I and T < 2 log2 I). For I < (log log log P)O(1) and
I
P larger that some constant, this quantity is smaller than (log P)γ , for any fixed
constant γ > 0. Hence, we need to try out at most logγ P possible delay assignments.
After assigning delays to all of the packets, the number of packets that use an
edge g in any T -frame τ is at most
3
X
i=1

(i)

Cg T
kr(I + T )T
√
+
I
I log I

!
+

(r(I + T ) − Pg )T
kr(I + T )T
√
+
I
I log I

396

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

≤

4kr(I + T )T
r(I + T )T
√
+
I
I log I

with probability at least 1−1/P β , since each packet is assigned a delay exactly once,
(1)

(2)

(3)

and thus r(I + T )− Pg + Cg + Cg + Cg ≤ r(I + T ). Thus the relative congestion
in any T -frame, for log2 I ≤ T < 2 log2 I, is at most




T
4k
4k
r(I + T )
)=r 1+
)
(1 + √
(1 + √
I
I
log I
log I


(8k + 1)
= r(1 + σ),
=r 1+ √
log I
√
√
by taking σ = (8k + 1)/(I log I), since 2 log2 I/I ≤ 1/ log I, for I large enough.
We can bound the total number of time steps taken by the algorithm as
follows. The first three passes (including all repeated trials of the second and
third passes) take time q(log P)(log log log P)O(1) (log log log log P)O(1) , with probability at least 1 − 1/P β . After the third pass, we solve subproblems containing (log log log P)O(1) critical nodes exhaustively. For each subproblem, for each
of the at most logγ P possible assignment of delays to the packets in the subproblem, for each of the at most (I 3 + I 2 ) log2 I T -frames τ in the subproblem,
log2 I ≤ T < 2 log2 I, and for every edge g in τ , we check whether more than
Mg packets traverse g during τ (using the procedure described in the proof of
Proposition 3.6). This takes time O(q(I 3 + I 2 )(log2 I)(logγ P)), which is at most
q(log log log P)O(1) (log log log log P)O(1) (logγ P), for P large enough, for any fixed
γ > 0 (since I = (log log log P)O(1) ). In particular, for γ = 1, this quantity is
bounded by q(log P)(log log log P)O(1) (log log log log P)O(1) . Hence the algorithm
runs in q(log P)(log log log P)O(1) (log log log log P)O(1) time steps, with probability
at least 1 − 1/P β , for any constant β > 0.

3.4. Moving the block boundaries
Now we present the three replacement propositions for Lemma 3.9 of [9], which
bounds the relative congestion after we move the block boundaries (as in [9]). The
three propositions that follow are analogous to the three replacement propositions
— Propositions 3.7.1–3 — for Lemma 3.7 of [9]. The necessary changes in the
proof of Lemma 3.9 of [9], in places where the Lovász Local Lemma is used, are
also analogous to the changes made in the proof of Lemma 3.7 of [9], for the cases
I = log P, I = (log log P)2 , and I = (log log log P)O(1) . Therefore, we omit the proofs
of Propositions 3.9.1–3.

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

397

Suppose we have a block of size 2I 3 +2I 2 , obtained after the insertion of delays
into the schedule as described in Propositions 3.7.1, 3.7.2, or 3.7.3, according to
the current value of I. Then suppose we move the block boundaries as described
in [9]. Each Proposition 3.9.1–3 refers to a specific size of I. Note that in [9], the
steps between steps I 3 and I 3 + 2I 2 in the block are called the “fuzzy region” of
the block. We assume that the relative congestion in any frame of size I or greater
in the block is at most r, where 1 ≤ r ≤ I. Let q be the number of distinct edges
traversed by the packets in the block.
Proposition 3.9.1. For I = log P, for any constant β > 0,
1. there is an algorithm for assigning delays in the range [0, I 2 ] to the packets
such that in between steps I log3 I and I 3 and in between steps I 3 + 3I 2 and
2
2I 3 +3I 2 −I log3 I, the relative congestion
√ in any frame of size log I or greater
is at most 2r(1 + σ1 ), where σ1 = O(1)/ log I, and such that in between steps
2
I 3 and I 3 +3I 2 , the relative congestion
√ in any frame of size log I or greater is
at most 2r(1 + σ2 ), where σ2 = O(1)/ log I;
2. this algorithm runs in O(q(log P)4 (log log P)) time steps, with probability at
least 1 − 1/P β .
Proposition 3.9.2. For I = (log log P)2 , for any constant β > 0,
1. there is an algorithm for assigning delays in the range [0, I 2 ] to the packets
such that in between steps I log3 I and I 3 and in between steps I 3 + 3I 2 and
2
2I 3 +3I 2 −I log3 I, the relative congestion
√ in any frame of size log I or greater
is at most 2r(1 + σ1 ), where σ1 = O(1)/ log I, and such that in between steps
2
I 3 and I 3 +3I 2 , the relative congestion
√ in any frame of size log I or greater is
at most 2r(1 + σ2 ), where σ2 = O(1)/ log I;
2. this algorithm runs in q(log P)(log log P)6 (log log log P)O(1) time steps, with
probability at least 1 − 1/P β .
Proposition 3.9.3. For I = (log log log P)O(1) , for any constant β > 0,
1. there is an algorithm for assigning delays in the range [0, I 2 ] to the packets
such that in between steps I log3 I and I 3 and in between steps I 3 + 3I 2 and
2
2I 3 +3I 2 −I log3 I, the relative congestion
√ in any frame of size log I or greater
is at most r(1 + σ1 ), where σ1 = O(1)/ log I, and such that in between steps
2
I 3 and I 3 +3I 2 , the relative congestion
√ in any frame of size log I or greater is
at most r(1 + σ2 ), where σ2 = O(1)/ log I;
2. this algorithm runs in q(log P)(log log log P)O(1) (log log log log P)O(1) time
steps, with probability at least 1 − 1/P β .

398

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

4. Running time

Theorem 4.1. For any constant δ > 0, the algorithm for finding an O(c + d)-steps
schedule of the packets takes O(m(c+d)(log P)4 (log log P)) time steps overall, with
probability at least 1 − 1/P δ .
Proof. For any constant β > 0, we place an upper bound on the number of time steps
taken by the application of Proposition 3.2, followed by the applications of Propositions 3.7.1, 3.9.1, 3.7.2, and 3.9.2, then followed by the applications of Propositions 3.7.3 and 3.9.3. The application of Proposition 3.2 takes O(m(c + d) log P)
time steps, with probability at least 1−1/P β . Each of the Propositions 3.7.1–3, and
each of the Propositions 3.9.1–3 dealt with a single block. For any I, partitioning
the schedule into disjoint blocks and moving the block boundaries as described in
[9] take O(P) time. Let nI be the number of blocks in the partition of the schedule
for any given I.
We place an upper bound on the number of time steps taken by the applications of Propositions 3.7.1–3 and 3.9.1–3 as follows. Assume
P I the nI blocks in
the partition for I are numbered from 1 to nI . Note that n
i=1 qi = P, where qi
is the number of distinct edges traversed by the packets in block i in this partition, independent of I. Thus the applications of Proposition 3.7.1 and 3.9.1 take
O(P(log P)4 (log log P)) steps; and the applications of Propositions 3.7.2 and 3.9.2
take P(log P)(log log P)6 (log log log P)O(1) steps. For each partition of the schedule
for a given I ≤ (log log log P)O(1) , we apply Propositions 3.7.3 and 3.9.3 to every
block i in this partition, 1 ≤ i ≤ nI , taking overall time P(log P)(log log log P)O(1)
(log log log log P)O(1) . Since we will repartition the schedule O(log∗ (c + d)) times
after we bring I down to (log log log P)O(1) , the overall running time due to applications of Propositions 3.7.3 and 3.9.3 is
P(log P)(log log log P)O(1) (log log log log P)O(1) log∗ (c + d).
Choose δ > 0 such that 1/P δ ≥ O(log∗ (c + d))/P β . Hence, the total number
of time steps taken by the algorithm is O(m(c + d)(log P)4 (log log P)), for P large
enough, with probability at least 1−1/P δ , for any constant δ > 0 (Note that we use
the inequalities P ≥ c, P ≥ d, and P ≤ m(c + d)).

5. A parallel scheduling algorithm
At first glance, it seems as though the algorithm described in Section 3 is inherently
sequential. This is because the decision concerning whether or not to assign a delay

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES

399

to a packet is made sequentially. In particular, a packet is deferred (i.e., not assigned
a delay) if and only if the packet might be involved in an event — i.e., the packet
traverses an edge that corresponds to an event — that became critical because of
the delays assigned to prior packets.
In [1], Alon describes a parallel version of Beck’s algorithm which proceeds
by assigning values to all random variables (in this case delays to all packets) in
parallel, and then unassigning values to those variables that are involved in bad
events. The Alon approach does not work in this application because we cannot
afford the constant factor blow-up in relative congestion that would result from this
process.
Rather, we develop an alternative method for parallelizing the algorithm. The
key idea is to process the packets in a random order. At each step, all packets
that do not share an edge with an as-yet-unprocessed packet of higher priority are
processed in parallel.
To analyze the parallel running time of this algorithm, we first make a dependency graph G0 with a node for every packet and an edge between two nodes if the
corresponding packets can be involved in the same event. Each edge is directed towards the node corresponding to the packet of lesser priority. By Brent’s Theorem
[4], the parallel running time of the algorithm is then at most twice the length of
the longest directed path in G0 .
Let D denote the maximum degree of G0 . There are at most N DL paths
of length L in G0 . The probability that any particular path of length L has all
of its edges directed in the same way is at most 2/L! (the factor of 2 appears
because there are two possible orientations for the edges). Hence, with probability
near 1, the longest directed path length in G0 is O(D + log N ). This is because if
2  1.
L ≥ k(D + log N ), for some large constant k, then N DL · L!
Each packet can be involved in at most (2I 3 +2I 2 )(2I 3 +I 2 ) log2 I events, and
at most r(I + T ) ≤ O(I) packets can be involved in the same event. Hence, the
degree D of G0 is at most O(I 7 log2 I). By using the method of Proposition 3.2 as
a preprocessing phase, we can assume that c, d, and thus I, are all polylogarithmic
in P. Hence, the parallel algorithm runs in N C, as claimed.
6. Concluding remarks
O ur algorithm for packet scheduling can also be used to route messages that are
composed of sequences of packets. This is possible since our algorithm can easily
maintain the property that any two packets traveling along the same path to the
same destination always proceed in order.
The algorithms described in this paper are randomized, but they can be
derandomized using the method of conditional probabilities [17, 20].

400

TOM LEIGHTON, BRUCE MAGGS, ANDRÉA W. RICHA

References
[1]

N. Alon: A parallel algorithmic version of the Local Lemma, Random Structures
and Algorithms, 2(4) (1991), 367–378.

[2]

D. Angluin and L. G. Valiant: Fast probabilistic algorithms for hamiltonian
circuits and matchings, Journal of Computer and System Sciences, 18(2) (1979),
155–193.
J. Beck: An algorithmic approach to the Lovász Local Lemma I, Random Structures
and Algorithms, 2(4) (1991), 343–365.
R. P. Brent: The parallel evaluation of general arithmetic expressions, Journal of
the ACM, 21(2) (1974), 201–208.
H. Chernoff: A measure of asymptotic efficiency for tests of a hypothesis based on
the sum of observations, Annals of Mathematical Statistics, 23 (1952), 493–507.
P. Erdős and L. Lovász: Problems and results on 3-chromatic hypergraphs and
some related questions, in: Infinite and Finite Sets (A. Hajnal et al., eds.),
Volume 11 of Colloq. Math. Soc. J. Bolyai, pages 609–627, North Holland,
Amsterdam, The Netherlands, 1975.
R. R. Koch, F. T. Leighton, B. M. Maggs, S. B. Rao, A. L. Rosenberg,
and E. J. Schwabe: Work-preserving emulations of fixed-connection networks,
Journal of the ACM, 44(1) (1997), 104–147.
F. T. Leighton, B. M. Maggs, A. G. Ranade, and S. B. Rao: Randomized
routing and sorting on fixed-connection networks, Journal of Algorithms, 17(1)
(1994), 157–205.
F. T. Leighton, B. M. Maggs, and S. B. Rao: Packet routing and job-shop
scheduling in O(congestion + dilation) steps, Combinatorica, 14(2) (1994), 167–
180.
F. T. Leighton, B. M. Maggs, and A. W. Richa: Fast algorithms for finding
O(congestion + dilation) packet routing schedules, Technical Report CMU–CS–
96–152, School of Computer Science, Carnegie Mellon University, Pittsburgh,
PA, July 1996.
B. M. Maggs and E. J. Schwabe: Real-time emulations of bounded-degree networks, Information Processing Letters, 1998. To appear.
Y. Mansour and B. Patt-Shamir: Greedy packet scheduling on shortest paths,
Journal of Algorithms, 14 (1993), 449–65.
F. Meyer auf der Heide and C. Scheideler: Routing with bounded buffers and
hot-potato routing in vertex-symmetric networks, in: Proceedings of the Third
European Symposium on Algorithms, pages 341–354, 1995.

[3]
[4]
[5]
[6]

[7]

[8]

[9]

[10]

[11]
[12]
[13]

[14]

F. Meyer auf der Heide and B. Vöcking: A packet routing protocol for arbitrary
networks, in: Proceedings of the Twelfth Symposium on Theoretical Aspects of
Computer Science Volume 439 of Lecture Notes in Computer Science, pages
291–302. Springer–Verlag, Heidelberg, Germany, March 1995.

FINDING O(Congestion + Dilation) PACKET ROUTING SCHEDULES
[15]

[16]

[17]

[18]

[19]

[20]
[21]

401

R. Ostrovsky and Y. Rabani: Universal O(congestion +dilation +log1+ N ) local
control packet switching algorithms, in: Proceedings of the Twenty-Ninth Annual
ACM Symposium on Theory of Computing, pages 644–653, May 1997.
´ Tardos: Distributed packet switching in arbitrary networks, in:
Y. Rabani and E.
Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of
Computing, pages 366–375, May 1996.
P. Raghavan: Probabilistic construction of deterministic algorithms: Approximate
packing integer programs, Journal of Computer and System Sciences, 37(4)
(1988), 130–143.
C. Scheideler: Universal Routing Strategies for Interconnection Networks, Vol.
1390 of Lecture Notes in Computer Science, Springer–Verlag, Berlin, Germany,
1998.
D. B. Shmoys, C. Stein, and J. Wein: Improved approximation algorithms for
shop scheduling problems, in: Proceedings of the Second Annual ACM–SIAM
Symposium on Discrete Algorithms, pages 148–157, January 1991.
J. Spencer: Ten Lectures on the Probabilistic Method, SIAM, Philadelphia, PA,
1987.
A. Srinivasan and C.-P. Teo: A constant-factor approximation algorithm for
packet routing, and balancing local vs. global criteria, in: Proceedings of the
Twenty-Ninth Annual ACM Symposium on Theory of Computing, pages 636–
643, May 1997.

Tom Leighton

Bruce Maggs

Mathematics Department, and
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, MA 02139, U.S.A.
ftl@math.mit.edu

School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, U.S.A.
bmm@cs.cmu.edu

Andréa W. Richa
Department of Computer Science
and Engineering
Arizona State University
Tempe, AZ 85287, U.S.A.
aricha@asu.edu

An O(log n) Dominating Set Protocol for Wireless Ad-Hoc
Networks under the Physical Interference Model
∗

Christian Scheideler†

Andrea W. Richa

Paolo Santi

Institut für Informatik
Technische Universität
München, Germany

Dept. of Compute Science
and Engineering
Arizona State University, USA

Instituto di Informatica e
Telematica del CNR
Pisa, Italy

scheideler@in.tum.de

aricha@asu.edu

paolo.santi@iit.cnr.it

ABSTRACT

1. INTRODUCTION

Dealing with interference is one of the primary challenges to
solve in the design of protocols for wireless ad-hoc networks.
Most of the work in the literature assumes localized or hopbased interference models in which the effect of interference
is neglected beyond a certain range from the transmitter.
However, interference is a more complex phenomenon that
cannot, in general, be captured by localized models, implying that protocols based on such models are not guaranteed to work in practice. This paper is the first to present
and rigorously analyze a distributed dominating set protocol for wireless ad-hoc networks with O(1) approximation
bound based on the physical interference model, which accounts for interference generated by all nodes in the network.
The proposed protocol is fully distributed, randomized, and
extensively uses physical carrier sensing to reduce message
overhead. It does not need node identifiers or any kind of
prior information about the system, and all messages are
of constant size (in bits). We prove that, by appropriately
choosing the threshold for physical carrier sensing, the protocol stabilizes within a logarithmic number of communication rounds, w.h.p., which is faster than the runtime of any
known distributed protocol without prior knowledge about
the system under any wireless model that does not abstract
away collisions.

A major challenge in the design of protocols for wireless
multi-hop networks is related to modeling complex physical phenomena such as radio wave propagation and interference. On one hand, using oversimplified radio propagation/interference models leads to the design of protocols
that, although efficient in terms of computational complexity and message overhead, might display a considerably different behavior from what is expected when utilized in a
practical scenario. On the other hand, using very complex
radio propagation/interference models might hinder the design and analysis of efficient protocols. Hence, finding the
adequate compromise between model accuracy and computational/communication efficiency is at the heart of the successful design of protocols for wireless multi-hop networks.
The model accuracy/efficiency tradeoff has not been adequately addressed so far. Computer scientists have mostly
focused their attention on designing distributed protocols for
wireless multi-hop networks based on simplistic models, such
as the unit disk graph model (UDG) for radio wave propagation [17], and graph-based models for interference [29].
Classical graph-related problems such as distributed coloring, dominating set construction, clustering, etc., have been
successfully addressed in the past. However, these protocols
are guaranteed to work only if the models’ assumptions are
fulfilled, which is hardly the case in practice. In particular,
the UDG model assumes isotropic propagation of the radio signal with distance, which is unlikely to happen in the
real world due to phenomena such as scattering, reflection,
and diffraction of the radio signal. Concerning interference,
graph-based models assume that only nodes within d hops
from a certain receiver u can interfere with u, where d is a
small constant. A major shortcoming of these models is that
interference is modeled as a localized phenomenon, which is
not the case in practice. It is well known, in fact, that a
transmitter can corrupt message reception even at very large
distances, especially in large networks with several concurrent transmissions. A similar criticism applies to another
interference model commonly used in the design of protocols for wireless multi-hop networks, namely the protocol
interference model [12]. In this model, a transmission from
node v to node u is considered successful iff there is no other
transmitter within a certain (constant) range centered at u.
On the other hand, the communication engineering community has focused on deriving accurate models for radio wave
propagation and interference, but has devoted little effort to
the design of protocols with proven performance guarantees
based on these models.

Categories and Subject Descriptors

C.2.1 [Network Architecture and Design]: Wireless
Networks

General Terms

Algorithms, Theory

Keywords

Ad hoc networks, dominating set, physical interference model,
self-stabilization
∗
This work originated from a discussion at the Dagstuhl
Seminar on Geometry in Sensor Networks, 2007.
†
Supported by DFG grant SCHE 1592/1-1.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MobiHoc’08, May 26–30, 2008, Hong Kong SAR, China.
Copyright 2008 ACM 978-1-60558-073-9/08/05 ...$5.00.

91

A notable feature of our algorithm, which we call TWIN,
is that nodes do not need to have any a priori knowledge
about the other nodes, not even an estimate on their total
number. Also, distinct identification numbers of any form
are not required so that our protocol may even be applicable
to the important field of sensor networks.
In order to obtain a constant density dominating set under an arbitrary distribution of nodes, our algorithm has two
main, inter-dependent components: the density estimation
stage, during which each node obtains an estimate on the
number of nodes within its transmission range, and accordingly adjusts its transmission probability, and a leader election stage, which is done using the novel concept of “twin”
nodes introduced in Section 5. The notion of twins forces
leaders to wake up in a pair-wise fashion, so that they can
check whether the density of nodes becoming leaders is too
high. This ensures that the number of leader nodes within
the transmission range of a node never exceeds a constant,
so that the leader election process does not run into oscillation problems. Unfortunately, in some situations formation
of twins cannot be enforced (such as isolated nodes), so we
also need the concept of “single” leaders. However, our protocol privileges the “twin” status when electing leaders, and
our strategy will be to form singles only as a last resort. Using these concepts, we can construct a simple protocol with
the following performance.

In this paper, we try to bridge this gap by introducing
a fully distributed, randomized protocol called TWIN for
building a dominating set in logarithmic time, w.h.p.1 , in
a wireless multi-hop network based on more realistic radio
propagation and interference models with respect to those
commonly used in the distributed algorithms community. In
particular, we assume radio wave propagation obeys a costbased generalization of the log-distance path loss model,
which can be used to resemble a close approximation of the
well-known log-normal shadowing model. For what concerns
interference, we use the physical interference model introduced in [12], in which a message is correctly received if and
only if the SINR value at the receiver is larger than a certain
threshold.
A notable feature of the proposed algorithm is that, although it is a localized distributed protocol, it is guaranteed
to work properly under a ‘global’ interference model such as
the physical model. This is made possible by extensive use
of physical carrier sensing. Another positive feature of using
physical carrier sensing is reduction of the communication
overhead: the protocol is shown to converge to a constant
density dominating set (i.e., a dominating set with a constant number of nodes per unit square) within O(log n) communication rounds, w.h.p. Having such a dominating set is
important because it can be used as a building block for designing more complex distributed protocols. For instance, in
case of WSNs, once local leaders (nodes in the dominating
set) are elected, network-layer functionalities such as broadcast and convergecast can be more easily implemented on
top of them. Also application-layer functionalities such as
data aggregation significantly benefit from the existence of
a dominating set.

2.

Theorem 2.1. The TWIN protocol establishes a constant
density dominating set in O(log n) communication rounds,
w.h.p., where n is the number of nodes.
We also discuss how to make this protocol self-stabilizing,
i.e., to establish a constant density dominating set starting
from any initial state. Note that a self-stabilizing protocol
is able to tolerate dynamic network conditions due to, e.g.,
node failures, node mobility, and so on.
Interestingly, the runtime bound is only possible because
our protocol uses physical carrier sensing. It is known that
if physical carrier sensing is not available and nodes have
no estimate of the size of the network, then it takes Ω(n)
steps on expectation for a single message transmission to be
successful [15] in any protocol. Also interesting is the fact
that our O(log n) algorithm improves upon the best previous result in [19] under a much more restricted, bounded
interference model.

OUR CONTRIBUTIONS

We will focus on the dominating set problem. The classical dominating set problem is defined as follows. Given an
undirected graph G = (V, E), a subset U ⊆ V is called a
dominating set of G if all nodes v ∈ V are either in U or
have an edge to a node in U . The density of a dominating
set is the maximum over all nodes v ∈ U of the number of
neighbors that v has in U . In our context, the neighborhood of a node v consists of all nodes within its transmission range, i.e., all nodes that can receive a message from
v under the ideal case that only the background noise is
there. (In simple wireless models like the unit disk graph
model, this neighborhood consists of all nodes within a certain distance from v.) A constant density dominating set
is a constant approximation of a minimum dominating set
(i.e., a dominating set of minimum cardinality). The problem of finding a minimum dominating set has been shown
to be NP-complete even when restricted to unit disk graphs
[7] and, hence, approximation algorithms are of interest.
Our contributions are two-fold: i) we propose a new model
for wireless communication based on the physical interference model which incorporates physical carrier sensing and
which closely approximates the log-normal shadowing model;
and ii) we demonstrate how to develop and analyze algorithms on top of this model by presenting a local-control
algorithm for building a constant density dominating set,
which we define below.

3. RELATED WORK
Since interference is a major factor limiting performance
in wireless multi-hop networks, a lot of effort has been invested in deriving realistic interference models, and to analyze network performance under such models. A seminal
work in this area is [12], in which Gupta and Kumar study
the transport capacity of wireless networks under two different interference models, the physical and the protocol interference model. Contrary to the physical model, which is
used in this paper, the protocol interference model is a localized model, since decision on whether a certain communication is successful depends only on the presence of concurrent
transmitters within a bounded area centered at the receiver.
Another localized interference model commonly used in the
literature is the graph-based interference model, in which a
certain communication graph representing communication
links is assumed, and only links whose endpoints are up to

1
“w.h.p.” means with probability at least 1 − 1/nc for any
constant c > 0, where n is the number of network nodes.

92

a certain hop distance d on the communication graph from
link (u, v) can interfere with (u, v) [29].
Due to their simplicity and the fact that they somehow
resemble the behavior of the 802.11 MAC layer, localized
interference models have been mostly used in the literature
to design interference-aware protocols. This is the case, for
instance, for the protocols presented in [2, 21, 25, 26]. Given
the complexity of dealing with physical interference, only a
few protocols based on this model have been proposed so far.
For example, [10, 11, 14] consider the problem of scheduling
transmissions, but they provide solutions which are computationally infeasible even for a small size network. Only
recently, a computationally efficient algorithm for scheduling
transmissions under the physical interference model with a
provable approximation bound has been proposed [4]. The
physical interference model has been recently used in [9,
23] to study the complexity of scheduling a set of link demands in the shortest possible time and of one-shot scheduling (scheduling as many transmissions as possible in a single communication slot), which are both shown to be NPcomplete for wireless networks in a 2-dimensional Euclidean
space.
Various distributed algorithms have been proposed for
finding good approximations of minimum dominating sets in
arbitrary graphs (see, for example, [8, 19, 20, 22]). Alzoubi
et al. [3] presented the first constant approximation algorithm for the minimum connected dominating set problem in
unit-disk graphs with O(n) and O(n log n) time and message
complexity, respectively. Cheng et al. [5] proposed a polynomial time approximation scheme for the connected dominating set problem in unit-disk graphs. Huang et al. [13]
formally analyze a popular algorithm used for clustering in
ad-hoc mobile network scenarios. They show that this algorithm gives a 7-approximation for the minimum dominating
set problem in unit-disk graphs while being able to adapt to
the mobility of the nodes in the network.
Kuhn et. al. [19] presented a distributed algorithm that
computes a constant factor approximation of a minimum
dominating set in O(log2 n) time without needing any synchronization but it requires that nodes know an estimate of
the total number of nodes in the network. In [24], Parthasarathy and Gandhi also present distributed algorithms to
compute a constant factor approximation to the minimum
dominating set. The running time of their algorithm depends on the amount of information available to the nodes,
and nodes have to know an estimate of the size of the network. Both papers extend the unit-disk model taking into
account signal interference. A more realistic model taking physical carrier sensing into account was considered by
Kothapalli et al. [16], but their algorithm needs O(log4 n)
time steps, w.h.p., in order to construct a constant factor
approximation of a minimum dominating set.

4.

Interference Ratio (SINR) at which a data frame can still be
received with a reasonably low frame error rate. The minimum SINRs for 802.11b, for example, are 10dB for 11Mbps
down to 4dB for 1Mbps. These SINR values specify the
transmission range of the data transmission mechanism, i.e.
the maximum range within which data frames can still be received correctly. The transmission range, however, is highly
dependent on the environment.
In order to handle non-uniform environments, we propose
the following cost model. Assume we are given a set V of
mobile stations, or nodes, that are distributed in an arbitrary way in a 2-dimensional Euclidean space. For any two
nodes v, w ∈ V let d(v, w) be the Euclidean distance between v and w. Furthermore, consider any cost function c
with the property that there is a fixed constant θ ≥ 0 so
that for all v, w ∈ V ,
c(v, w) ∈ [(1 + θ)−1 · d(v, w), (1 + θ) · d(v, w)] .

(1)

The cost function c determines the transmission and interference behavior of nodes, and θ bounds the non-uniformity
of the environment. In particular, transmission from node
v to w is considered successful in our model (in absence of
interference) if and only if c(v, w) ≤ rt , where rt is the intended transmission range. Notice that we do not require
c to be monotonic in the distance, to satisfy the triangle
inequality, nor to be symmetric. This makes sure that our
model even applies to highly irregular environments. Similar
cost functions were also used in [16, 18, 23], for example.
Note that if c is not symmetric, then we have to rephrase
the definition of a dominating set to avoid ambiguities. In
that case, a dominating set is a set U of nodes so that for
every node v, either v ∈ U or v has a node u ∈ U with
c(u, v) ≤ rt (i.e., v can receive a message from u).
It is worth observing that, by properly setting the constant θ, the above model can be used to represent a channel
propagation model which is similar to the well-known lognormal shadowing model [27], in which the received power
at a distance of d relative to the received power at a reference distance of d0 (representing the distance at which the
signal strength starts to degrade) is given in dB as
−10 log(max{d/d0 , 1})α + Xσ

(2)

where α is the path loss coefficient and Xσ is a Gaussian
random variable with zero mean and standard deviation σ
(in dB) that models variability in signal loss with distance.
α usually ranges from 2 (free space) to 5 (indoors), and σ
from 2dB to 8dB.
Note that the original log-normal shadowing model cannot be represented through the notion of link cost defined
in (1), since the random component of signal propagation
(variable Xσ in equation (2)) has infinite support. This
implies that, in principle, it is possible to communicate to
nodes which are arbitrarily distant from the transmitter (or
to not being able to communicate to nodes which are arbitrarily close to the transmitter). Hence, the log-normal
shadowing model cannot be represented by any notion of
link cost which confines possible successful transmission to
a pair of nodes whose distance is within a constant factor
from the intended communication range. To circumvent this
problem, we consider a bounded version of the log-normal
shadowing model, in which the random component of signal
propagation is represented by a random variable Xσ′ with
bounded support. In particular, the support of Xσ′ is of the

COMMUNICATION MODEL

In this section, we present the communication model used
in the design of our algorithm. The radio propagation and
physical carrier sensing components of the model we use
were first proposed by Kothapalli et al. in [16]. The interference model component is similar to the one used in [23].

4.1 Signal propagation
To model message reception, we observe that every data
transmission mechanism has a minimum Signal-to-Noise-

93

cost model defined in (1), and by the interference model defined in the next sub-section.
Combining the physical interference model with the radio
propagation model introduced in the previous section, we
have that a message sent by node u to node v is correctly
received if and only if

form [−h · σ, +h · σ], where σ is the standard deviation of
variable Xσ , and h is a constant. The pdf of Xσ′ is obtained
from the pdf of Xσ by uniformly distributing the probability density of variable Xσ falling outside the support of Xσ′
into the interval [−h · σ, +h · σ]. It is easy to see that, by
increasing h, we have that the pdf of Xσ′ becomes arbitrarily
close to the pdf of the original variable Xσ . For instance, by
setting h = 3, we have that only 0.0027 of the probability
mass of variable Xσ falls outside [−3σ, 3σ], and the pdf of
Xσ′ is virtually indistinguishable from the pdf of Xσ .
It is easy to see that the above described bounded version
of the log-normal shadowing model can be represented by
hσ
the notion of link cost defined in (1) by setting θ = 10 10α −1,
where σ and α are the parameters of the propagation model.
For instance, by setting α = 3, σ = 6dB, and h = 3, we
obtain θ ≈ 1.5, implying that a transmission between nodes
u and v is always successful when d(u, v) < 0.399rt , and that
a successful transmission can only occur at distance ≤ 2.5rt .
In summary, our cost model can be used to cover a bounded
variant of the log-normal shadowing model, but in this case
the cost function c would be a random variable. In order to
be able to eventually arrive at a stable dominating set, we
will only consider fixed cost functions c in this paper.
In the following, we assume that nodes use some fixedrate communication mechanism and operate over a single
frequency band. When a node u is using a transmission
power of P (the same for every node), then the received
power at node v is equal to Pv (u) = P/ max{c(u, v)α , 1}
where α is the path-loss exponent2 . Similarly to [4, 12], we
will assume throughout the paper that α > 2 + ǫ for some
arbitrary constant ǫ > 0, which is usually the case in reality.

N+

In this paper, we model interference using the physical
interference model introduced in [12], which accounts for
the SINR at the receiver end of a link to determine whether
the transmission is successful. More specifically, a message
sent by node u to node v is correctly received if and only if
P (u)
Pv
≥β
w∈S Pv (w)

≥β .

Observe that the physical interference model, contrary to
the case of simpler interference models such as graph-based
and protocol models, accounts also for the interference generated by nodes which are far away from the intended receiver (say, node v) of a communication. Although the contribution to the interference level at v of a single far-away
transmitter can be relatively small, the overall contribution
of all far-away transmitters can be sufficiently high to drive
the SINR at v below the threshold and corrupt transmission.
This is the reason why protocols based on localized interference models that simply ignore interference beyond a certain
range from the transmitter are not guaranteed to work in a
real scenario, where actual message reception probability is
governed by SINR.
The fact that interference in the real world cannot, in principle, be confined within a bounded region from the transmitter poses a major challenge to the design of distributed
protocols for multi-hop networks. In fact, locality i.e., the
ability of designing protocols based on message exchange
only between nodes which are at most a few hops away from
each other in the network topology, is fundamental to ensure
that the designed distributed protocol runs effectively even
in large networks (e.g., sensor networks).
To get around this apparent contradiction between locality and the use of the physical interference model, we make
extensive use of physical carrier sensing. As we show in this
paper, by properly tuning the carrier sensing threshold it
is indeed possible to design localized, fully distributed protocols which are guaranteed to function correctly (w.h.p.)
under the physical interference model.

4.2 Interference model

N+

P
max{c(u,v)α ,1}
P
w∈S max{c(w,v)α ,1}

P

(3)

4.3 Physical carrier sensing

where Px (y) is the received power at node x of the signal
transmitted by node y, N is the background noise, S is the
subset of nodes in V \ {u, v} that are currently transmitting,
and β is a constant that depends on the desired rate, the
modulation scheme, etc.
It has been observed in the literature that, when using forward error correction mechanisms as proposed in the IEEE
802.11e MAC standard currently under development, the
transition between being able to correctly receive a data
frame with high probability and not being able to correctly
receive a data frame with high probability is very sharp.
As shown in [6], it can be less than 1 dB. Thus, the transmission range is an area with a relatively sharp border as
implied by (3) that in reality, however, may have a very irregular shape due to environmental effects. These features
(irregular coverage area, and sharp transition between low
and high reception probability) are well captured by the link

In this paper, we assume that nodes can perform physical
carrier sensing, and that they can set the carrier sensing
threshold to different values.
Physical carrier sensing is part of the 802.11 standard, and
is provided by a Clear Channel Assessment (CCA) circuit.
This circuit monitors the environment to determine when
it is clear to transmit. The CCA functionality can be programmed to be a function of the Receive Signal Strength Indication (RSSI) and other parameters. The RSSI measurement is derived from the state of the Automatic Gain Control (AGC) circuit. Whenever the RSSI exceeds a certain
threshold, a special Energy Detection (ED) bit is switched
to 1, and otherwise it is set to 0. By manipulating a certain
configuration register, this threshold may be set to an absolute power value of t dB, or it may be set to be t dB above
the measured noise floor, where t can be set to any value in
the range 0-127. The ability to manipulate the CCA rule allows the MAC layer to optimize the physical carrier sensing
to its needs. Adaptive setting of the physical carrier sensing threshold has been used, for instance, in [30] to increase
spatial reuse.

2
The max operator is used to account for the fact that the
log-distance path loss model holds only for distances beyond
a certain close-by distance d0 [27], w.l.o.g. assumed to be 1
in the following.

94

Parameter
θ
α
N
β
Pq

P
rt = α βN
rs = ρrt

Meaning
constant > 0 for defining link cost
path loss exponent (α > 2)
background noise
SINR thr. for correct message reception
nodes transmission power

Variable
T
D
d
acc(v)
pv

Meaning
threshold for physical carrier sensing in TWIN
upper bound on max twin density
upper bound on transmitters density
account variable for node v (acc(v) > 0 iff v is active)
tx probability for node v (stage 2)

nodes transmission range
range for physical carrier sensing (0 < ρ < 1)

p̂
γ

max tx probability value
increase/decrease step of node tx probability

Table 1: Network model parameters and TWIN algorithm variables.
In our network model, nodes can not only send and receive messages, but also perform physical carrier sensing.
Given some sensing threshold T (that can be flexibly set by
a node), a node v senses a busy channel if and only if
X
N+
Pv (w) ≥ T ,

round
stage 1
slot 1

w∈S

slot 2

slot 1

slot 2

slot 3

Proof. To prove the lemma, we need to show that C(α, δ)
is an upper bound for the total signal strength caused by the
nodes outside of the transmission range of u, for a maximum
density value δ. This follows along the lines of [4].

4.4 Summary of network model
Summarizing, we consider a wireless network where correct message reception at the receiver end of a transmission
is determined by the experienced SINR value computed according to equation (3), and radio signal propagation is expressed in terms of i) a fixed cost of a communication link as
defined in (1), and ii) a signal loss exponent α. Finally, we
assume that nodes can perform physical carrier sensing, and
that the threshold used to sense the channel can be chosen
among a (sufficiently large) set of possible values.
The main parameters of our network model, as well as
variables used in the TWIN protocol, are summarized in
Table 1.

The TWIN protocol works in rounds that are continuously
executed and synchronized among the nodes. A node can
be either inactive or active, and active nodes can be either
singles or twins, as will be explained later in this section.
The active nodes will eventually converge to a dominating
set.
Each round of the TWIN protocol consists of three stages
(see Figure 1). In stage 1, the active twins send out an
ACTIVE signal with a certain probability so that inactive
or active singles can learn about active twins in their vicinity.
In stage 2, those nodes v that have not yet found an active
twin in their vicinity probe the wireless medium and adjust
their probabilities pv so that within a certain number of
rounds the sum of the probabilities within any transmission
range of a node is within a constant on expectation. In stage
3, the non-twin nodes that were able to receive each other’s
signal in stage 2 acknowledge this to each other to be sure to
form active twins. They will then announce it to the other
nodes so that close-by nodes terminate the protocol.
In order to become an active single, each node v maintains an account acc(v) ≥ 0. Each time pv = p̂, the maximum transmission probability value a node can have, it
sets acc(v) := acc(v) + 4, and each time pv < p̂, it sets
acc(v) := max{acc(v) − 1, 0}. A node is an active single as
long as acc(v) > 0.
Next, we give the details of our TWIN protocol. Initially,
all nodes are inactive and acc(v) = 0 for every node v. The
probability values pv may be set to any value x with 0 <
x ≤ p̂. Each round works as follows

THE TWIN PROTOCOL

We assume that all nodes transmit with some fixed, uniform transmission power P . Let rt be the transmission range
of that power, i.e., under an ideal situation (only the background noise is there), v can transmit a message to node
w if and only if c(v, w) ≤ rt . In other words, rt satisfies
P/rtα ≥ βN .
Our dominating set protocol, called TWIN, is based on
two carrier sensing thresholds:
• Threshold Ts is defined so that whenever a node w with
c(v, w) ≤ rs sends a message for some sensing range
rs , node v will notice a busy channel. We assume that
rs = ρrt for some small constant 0 < ρ < 1 that
satisfies Ts = (P/(ρrt )α ) ≥ 4N so that the sensing
threshold is sufficiently far above the noise floor.
• Threshold Tn (d) is defined so that if the density of
transmitting nodes is at most some constant d, then a
node will only sense a busy channel if there is a sending
node within its transmission range.

• Stage 1: Announcing active twins
This stage consists of one time slot. In that time slot,
each active twin v decides with probability 1/D to
send out an ACTIVE signal, where the constant D
is an upper bound on the maximum density of twins
determined later. Each inactive or active single v that
receives an ACTIVE signal stops executing the protocol (since it is covered) and sets acc(v) := 0 (i.e., it
becomes inactive).

The following lemma bounds Tn (d) in terms of α and the
maximum density d of nodes we expect.
Lemma 5.1. If α > 2, then we can set Tn (d) = N +
C(α, d) with
C(α, δ) = δ · P ·

slot 1

Figure 1: Round of execution of the TWIN protocol.

where S is the subset of nodes in V − {v} that are currently
transmitting when node v is sensing the channel.

5.

stage 3

stage 2

2
π(1 + θ)α
· α
rtα−2
22 −2

• Stage 2: Guessing the right density
This stage consists of two time slots. Each inactive

95

or active single v still participating in the protocol
chooses one of the two time slots of this stage uniformly at random, say, slot s. For slot s, v decides to
send a PING signal with probability pv . If v sends
a PING signal, it senses the wireless channel with
threshold Ts in the alternative slot, s̄. Otherwise, it
senses the wireless channel with threshold Ts in both
slots. If it does not sense anything in either case, it
sets pv := min{(1 + γ)pv , p̂}, and otherwise it sets
pv := (1 + γ)−1 pv for some constants p̂ < 1 and 0 <
γ < 1. Whereas γ may be set to any constant value,
our analysis requires that p̂ ≤ 1/(240π(1 + θ)4 ), but
we did not try to optimize this bound. If pv = p̂, then
acc(v) := acc(v) + 4, (i.e., v becomes or remains an active single) and otherwise acc(v) := max{acc(v)−1, 0}.

protocol. Hence, when initially setting pv = p̂ for all v, then
the runtime bound is O(log n).
Let Rv be the current set of inactive or active singles
within the transmission range of node v and Rs (v) be the
current set of inactive or active singles within the sensing
range of node v with threshold T (i.e., whenever a node
w ∈ Rs (v) transmits a message, v will sense a busy channel with threshold T ). According to the definition of T ,
Rs (v) ⊆ Rv . We need a series of lemmas to prove the
theorem. The first lemma implies that after a logarithmic
number
Pof rounds a point is reached so that most of the
time,
w∈Rs (v) pw is bounded from above by a constant
(Lemma 5.3). With the help of this result we can prove an
upper
bound on the expected number of rounds in which
P
w∈Rs (v) pw is above a constant (Lemma 5.5). The two
lemmas can then be used to show that most of the time
the expected interference at a node caused by nodes outside
of its transmission range isPbelow T /2 − N (Lemma 5.6).
Hence, most of the time,
w∈Rv pw = O(1) and the interference caused by nodes outside of Rv together with the
noise is less than T (Lemma 5.7). That will allow us to
prove that within a logarithmic number of communication
rounds, any node v will either be an active single or have
an active twin within its transmission range (Lemma 5.8).
Finally, we show that within a logarithmic number of communication rounds there can only be a constant number of
active singles and twins within the transmission range of v
(Lemma 5.9). All of the lemmas hold w.h.p. Combining
them yields Theorem 2.1.
For the analysis of our protocol we need the following
general form of the Chernoff bounds [28].

• Stage 3: Forming new twins
This stage consists of three time slots. Every inactive
or active single v that sent a PING signal in some slot
s and received a PING signal in the alternative slot s̄
does the following. It sends an ACK signal in slot s of
this stage and listens to the wireless channel in slot s̄
of this stage. If it receives an ACK signal in slot s̄, it
becomes an active twin.
All nodes that just became a new active twin in this
stage send a NEW signal in the last time slot. All
remaining inactive or active singles sense the wireless
channel with threshold Tn (d), where d is the maximum
density at which new active twins can emerge. Each
node v that senses a busy channel stops executing the
protocol and sets acc(v) := 0 (since it has an active
twin within its transmission range).

Lemma 5.2. Consider P
any set of binary random variables
n
X1 , . . . , Xn , and let
QX = i=1 X
Qi . If there are probabilities
p1 , . . . , pn with E[ i∈S Xi ] ≤ Pi∈S pi for every set S ⊆
{1, . . . , n}, then it holds for µ = n
i=1 pi and any δ > 0 that
µ

δ
δ2 µ
e
− 2(1+δ/3)
≤
e
Pr[X ≥ (1 + δ)µ] ≤
(1 + δ)1+δ

Hence, altogether, each round consists of six time slots. The
assumption that the rounds are synchronized among the
nodes is not needed any more if six frequencies are available, one frequency for each of the six time slots of the protocol. In this case, we would only need the assumption that
the drift between the local clocks of the nodes is sufficiently
small for the protocol to work.
In order to avoid switching back and forth between the
two sensing thresholds all the time, TWIN may just use a
single threshold T := max{Ts , Tn (d)}. This is not a problem for the protocol as long as Tn (d) < P so that a single,
sufficiently nearby node is able to trigger a busy channel.
According to the bound in Lemma 5.1, Tn (d) < P is true if
rt is sufficiently large. With a much more careful analysis
that takes into account that all nodes sending NEW signals
must have received ACK signals before and the convexity of
the signal propagation function, it turns out that an upper
bound of 2(2π((1 + θ)/rt )α−1 + 1/β) suffices for the sensing
threshold in stage 3 so that a node only senses a busy channel if a NEW signal is sent by a node within its transmission
range. Thus, relatively small constants rt and β already suffice. We will defer the proof of that bound to the full version
of the paper. In the following analysis, we will just argue
with the T above.

If,Qon the other
Qside, there are probabilities p1 , . . . , pn with
E[ i∈S Xi ] ≥ P i∈S pi for every set S ⊆ {1, . . . , n}, then it
holds for µ = n
i=1 pi and any 0 < δ < 1 that

µ
2
e−δ
Pr[X ≤ (1 − δ)µ] ≤
≤ e−δ µ/2
(1 − δ)1−δ
We assume for stage 2 that, as a worst case, the set of
inactive and active singles participating in stage 2 is fixed.
In reality, it decreases monotonically, which is in our favor
when proving upper bounds on sums of access probabilities
in certain regions. In the following, the term ’node’ refers to
inactive or active singles still participating in the protocol.
Lemma 5.3. Let R be any region with the property that
for any two nodes v, w ∈ R it holds that v and w are within
the sensing range of each
P other. Consider any time interval
I in which initially
v∈R pv = φ0 and let Xφ be a random
P variable denoting the number of rounds in I in which
v∈R pv ≥ φ. Then it holds for any φ ≥ 2 and δ ≥ 2 that

5.1 Analysis of the TWIN protocol
Next, we prove the main result of this paper, i.e., we show
that our protocol constructs a constant density dominating
set within O(log(n + 1/ϕ)) rounds, w.h.p., where ϕ is the
lowest probability value a node v has at the beginning of the

Pr[Xφ ≥ (1 + δ)4|I|/eφ/2 + log1+γ ⌈φ0 /φ⌉]
φ/2

≤ (e/(1 + δ))−δ2|I|/e

96

Proof. For any v ∈ R and any round t ≥ 1 let pt (v)
denote the probability
P pv used by node v at the beginning
of round t. Let pt = v∈R pt (v). Consider some fixed round
t. The probability that v does not send a PING signal in
a specific time slot s in stage 2 of that round is equal to
(1/2)(1−pt (v))+(1/2) = 1−pt (v)/2 (where (1/2)(1−pt (v))
is the probability that v picks s but does not transmit a
PING signal in s and (1/2) is the probability that v does not
pick s). Thus, the probability that no node is transmitting
a PING signal at some time slot s is equal to
Y
Y −p (v)/2
(1 − pt (v)/2) ≤
e t
= e−pt /2
(4)
v∈R

the number of rounds r ∈ I with pr ≥ φ that these are at
most 2Y + log1+γ ⌈φ0 /φ⌉.
Inequality 6 and Claim 5.4 imply that
Pr[Xφ ≥ (1 + δ)4t/eφ/2 + log1+γ (1 + φ0 )]
≤ Pr[Y ≥ (1 + δ)2t/eφ/2 ]
φ/2

≤ (e/(1 + δ))−δ2t/e

for any δ ≥ 2, which completes the proof of Lemma 5.3.
Lemma 5.3 allows us to bound the expected number of
time steps in which pt ≥ φ for some time interval I. The
bound we present is not obvious since we want to avoid the
additive term of log1+γ ⌈φ0 /φ⌉ in Lemma 5.3, which can be
as large as log1+γ (np̂).

v∈R

If both slots in stage 2 are used by PING signals in R, then
pt+1 = (1 + γ)−1 pt , and therefore,
Pr[pt+1 = (1 + γ)−1 pt ] ≥ Pr[both slots used in stage 2]

Lemma 5.5. Let R be any region with the property that
for any two nodes v, w ∈ R it holds that v and w are within
the sensing range of each other. Consider any sufficiently
large time interval I with |I| = Ω(log 1+γ n) starting at a
sufficiently large time step t0 = Ω(log1+γ n) and let Xφ be
a random
P variable denoting the number of rounds in I in
which v∈R pv ≥ φ. Then it holds for any φ ≥ 2 that

Inequality (4) implies that the probability that at least one
of the slots in stage 2 is not used by a node in R is at most
2e−pt /2 . Hence, Pr[both slots used in stage 2] ≥ 1 − 2e−pt /2
which is at least 1 − 2e−φ/2 if pt ≥ φ. Thus, for pt ≥ φ it
holds that Pr[pt+1 = (1 + γ)−1 pt ] ≥ 1 − 2e−φ/2 .
Now, consider any interval I of t rounds, numbered from 1
to t. For round r ∈ {1, . . . , t} let the binary random variable
Yr be 0 if and only if pr < φ or it holds that pr ≥ φ and
pr+1 = (1 + γ)−1 pr . Irrespective of the previous rounds it
holds that
Pr[Yr = 0] = Pr[pr
Pr[pr
= Pr[pr
Pr[pr

E[Xφ ] ≤ 40|I|/eφ/2

P
Proof. For any time step t let pt =
v∈R pt (v). Initially, pt ≤ n. Hence, it follows from the proof of Lemma 5.3
that for any sufficiently large interval I ′ of size Θ(log1+γ n)
and any φ ≥ 2 there is a time step t ∈ I ′ with pt ≤ φ
w.h.p. Thus, if interval I starts at a sufficiently large time
step t0 = Ω(log1+γ n), there must be a time step t ≤ t0
with t0 − t = O(log1+γ n) and pt ≤ φ w.h.p. Let I ′ be the
interval starting at t and containing I. Then we know from
Lemma 5.3 that when defining Xφ with respect to I ′ ,

< φ] +
≥ φ ∧ pr+1 = (1 + γ)−1 pr ]
< φ] +
≥ φ] · Pr[pr+1 = (1 + γ)−1 pr | pr ≥ φ]

≥ Pr[pr < φ] + Pr[pr ≥ φ] · (1 − 2e−φ/2 )

≥ 1 − 2e−φ/2

Pr[Xφ ≥ (1 + δ)4|I ′ |/eφ/2 ] ≤ (e/(1 + δ))−δ2|I

which implies that for all r ∈ {1, . . . , t} and Q
sets S ⊆ {1, . . . , t}
with s < r for all s ∈ S, Pr[Yr = 0 |
s∈S Ys = 1] ≥
Q
1−2e−φ/2 and therefore Pr[Yr = 1 | s∈S Ys = 1] ≤ 2e−φ/2 .
From this it follows that
Y
E[
Yr ] ≤ (2e−φ/2 )|S|
(5)

φ/2

Pr[Xφ ≥ (1 + δ)5|I|/eφ/2 ] ≤ (e/(1 + δ))−δ2|I|/e

for any δ ≥ 2. When defining ℓ = (1 + δ)5|I|/eφ/2 for some
δ ≥ 3, it holds that

P
for any set S ⊆ {1, . . . , t}. Let Y = tr=1 Yr . From above
we know that E[Y ] ≤ 2t/eφ/2 . Due to (5) we can apply the
Chernoff bounds in Lemma 5.2 to Y , so
φ/2

(e/(1 + δ))−δ2t/e

|/eφ/2

for any δ ≥ 2. The same bound also applies to I. If |I ′ | ≤
(5/4)|I| (which is true if |I| is sufficiently large, w.h.p.), then
it follows that

r∈S

Pr[Y ≥ (1 + δ)2t/eφ/2 ] ≤

′

δ2|I|/eφ/2 =

(6)

2δ
ℓ ≥ (3/10)ℓ
5(1 + δ)

and therefore for any such ℓ, Pr[Xφ ≥ ℓ] ≤ (e/4)(3/10)ℓ ≤
2−ℓ/6 . Hence, for ℓ0 = 20|I|/eφ/2 ,
X
E[Xφ ] ≤
Pr[Xφ ≥ ℓ]

for any δ ≥ 2. To complete the proof, we need the following
claim. Recall the definition of φ0 in Lemma 5.3.
Claim 5.4. The number of rounds r ∈ I with pr ≥ φ is
at most 2Y + log1+γ ⌈φ0 /φ⌉.

ℓ≥1

≤ ℓ0 +

′

Proof. For any interval I = [t1 , t2 ] ⊆ I withPpt1 −1 < φ
2
and pr ≥ φ for all r ∈ I ′ it must hold that YI ′ = tr=t
Yr ≥
1
|I ′ |/2 because for every r ∈ I ′ with Yr = 0, pr+1 = (1 +
γ)−1 pr , and for any other r ∈ I ′ , pr+1 ≤ (1 + γ)pr . For the
initial probability pr of I we assumed that pr = φ0 . Hence,
for the first interval I ′ ⊆ I with pr ≥ φ for every r ∈ I ′ ,
YI ′ must be at least |I ′ |/2 − log1+γ ⌈φ0 /φ⌉ so that indeed
pr ≥ φ for every r ∈ I ′ . Thus, altogether it must hold for

≤ ℓ0 +

X

Pr[Xφ ≥ ℓ]

X

2−ℓ/6 ≤ 2ℓ0

ℓ≥ℓ0

ℓ≥ℓ0

Lemmas 5.3 and 5.5 allow us to prove the following lemma,
which bounds the number of rounds in which there is too
much non-local interference. Recall that the exponent for

97

the signal degradation is α for some constant α > 2 and
that P is the transmission power of every node. Let R̄v
represent the complement of Rv , i.e., the area outside of the
transmission range of v.

Moreover, the binary random variables satisfy the condition
of the first Chernoff bound in Lemma 5.2. Thus, for any
δ > 0,
Pr[Y ≥ (1 + δ)ǫ|I|] ≤ e−δ

Lemma 5.6. Consider any time interval I and let Y be
a randomPvariable denoting the number of rounds in I in
which P w∈R̄v pw /c(v, w)α ≥ T /2. For any constant ǫ >
0, Pr[Y ≥ ǫ|I|] can be made polynomially small in n if |I| =
Ω(log 1+γ n) is sufficiently large.

PFor a node v, a round t in I is called good if and only if
w∈Rv pt (w) ≤ g for some fixed constant g and the interference caused by nodes in R̄v is less than T − N (so that
the additional noise may not trigger a busy channel).
Lemma 5.7. For any constant ǫ > 0, at least (1 − ǫ)|I|
of the rounds in I are good for v, w.h.p., if g and T are
sufficiently large.
Proof. Fix some ǫ > 0. Lemma 5.3Pimplies that for
at least (1 − ǫ/2)|I| of the rounds in I,
w∈Rv pt (w) ≤ g
w.h.p. if the constant g is sufficiently large. Furthermore,
′
Lemma
P 5.6 implies that αfor at least (1−ǫ )|I| of the rounds in
I, P w∈R̄v pw /c(v, w) ≤ T /2, w.h.p., for some constant
ǫ′ > 0 that can be arbitrarily small. Using the Chernoff
bounds, only for an ǫ′ -fraction of these rounds the cumulative signal strength of the nodes in R̄v will exceed T − N
(given that T ≥ 4N ), w.h.p., so altogether there are at most
(ǫ′ (1 − ǫ′ ) + ǫ′ )|I| rounds in I in which the cumulative signal
strength of these nodes exceeds T − N , w.h.p. If ǫ′ > 0 is
sufficiently small, then ǫ′ (1 − ǫ′ ) + ǫ′ ≤ ǫ/2. In this case,
we
P would have at most an ǫ/2-fraction of the rounds with
w∈Rv pt (w) ≤ g and at most an ǫ/2-fraction of the rounds
in which the cumulative signal strength of the nodes in R̄v
exceeds T − N , which implies the lemma.

w∈R̄v

X 2π(i + 2/ρ + 1)
′
· 2(i + 2/ρ + 1)α /2 φ
2+α′
(r
t + i · rs /2)
i≥0

=

′
2π · 2φ X (i + 2/ρ + 1)1+α /2
(rs /2)2+α′ i≥0
(2/ρ + i)2+α′

≤

22+2α · 4πφ X
′
rs2+α
i≥0

′

≤



1
i + 2/ρ

/(2(1+δ/2))·ǫ|I|

which is polynomially small in n if |I| = Ω(log1+γ n) is sufficiently large.

Proof. For simplicity, suppose that c(u, v) = d(u, v) for
all pairs of nodes. Going back to the original cost model
would influence the bounds below by at most a (1 + θ)α
factor. In the following, let α′ = α − 2.
Consider R̄v to be cut into rings R0 , R1 , . . . where Ri covers the area between radius rt + irs /2 and rt + (i + 1)rs /2
around v. Each ring Ri can be cut into sectors Si,1 , . . . , Si,k
with k ≤ 2π(rt + (i + 1)rs /2)/(rs /2) = 2π(i + 2/ρ + 1) for
rs = ρrt so that every sector Si,j represents a region in which
nodes can sense each other. Suppose that pt (Si,j ) < φi for
′
all i, j with φi = (i + 2/ρ + 1)α /2 φ, where the constant φ is
specified later. Then
X
pw /d(v, w)α
≤

2

1+α′ /2

We are now ready to prove quick coverage by active nodes.

′
′
22(2+α ) · πφ 2  ρ α /2
· ′
2+α′
α 2
rs

Lemma 5.8. Let I be any time interval that is starting
after Ω(log 1+γ n) rounds with |I| = O(log1+γ (n+1/ϕ)) being
sufficiently large. If p̂ ≤ 1/(240π(1 + θ)4 ) then at the end of
I it holds for every node v that v is either an active single
or has an active twin within its transmission range, w.h.p.

′

Recall that T = P/rs2+α forPsome constant P . Hence, if ρ
is sufficiently small, then P w∈R̄v pw /d(v, w)α ≤ T /2.
It remains to show that most of the time this bound is
true, w.h.p. Consider any time interval I starting at some
sufficiently large round, and suppose that Y ≥ ǫ|I|. Then
there are d = ǫ|I| rounds t1 , . . . , td ∈ I so that for each tk
there is an Si,j with ptk (Si,j ) ≥ φi . In order to analyze
these events, let the random variable Yi,j to be defined as
the number of times ptk (Si,j ) ≥ φi in I. It follows from
Lemma 5.5 that

Proof.
We distinguish between two cases for
P
p′t = w∈Rv \{v} pt (w).
Case 1: p′t < 24π(1 + θ)4 p̂ for at least 7/8 of the rounds
in I. Let t be any of these rounds and suppose that t is
good. Then it follows from the proof of Lemma 5.3 that
Pr[pt+1 (v) = min{(1 + γ)pt (v), p̂}] is at least
Y
pt (v)
(1 − pt (w)/2) +

E[Yi,j ] ≤ 40|I|/eφi /2
Hence,

w∈Rv \{v}

E[Y ] =

XX
i

≤

X

≤

ǫ|I|

i

(1 − pt (v))

E[Yi,j ]

j

′

Y

w∈Rv \{v}

(1 − pt (w))
′

≥ pt (v)e−1.1pt /2 + (1 − pt (v))e−1.1pt
′
8
≥ e−1.1pt ≥
9

2π(i + 2/ρ + 1) · 40|I|/eφi /2

if p̂ ≤ 1/(240π(1 + θ)4 ). For round t, let the binary random
variable XP
t be 1 if and only if pt+1 (v) = min{(1+γ)pt (v), p̂}.
Let X = t∈I Xt . If at most 1/64 of the rounds in I are
bad (which is true according to Lemma 5.3 w.h.p.), it follows
that E[X] ≥ |I|(7/8 − 1/64)8/9 ≥ |I|(3/4 + 1/72). Furthermore, for any good round t with p′t < 18p̂ and any set S of

if the constant φ is large enough. Each Yi,j can be considered
as the sum of binary random variables in the flavor of the
proof of Lemma 5.3 (see the definition of the Yr ’s). Thus,
Y can be considered as the sum of binary random variables.
This sum is finite since p(Si,j ) ≤ n for every i, j, which
implies that we do not have to go beyond ring i with φi > n.

98

rounds in I since each of these nodes would have a probability of p̂ for at least 1/4 of the rounds in I. Hence, there
can at best be a constant number of nodes u ∈ Rv for which
case 1 applies. For the rest of them, case 2 must apply,
but in this case a twin will emerge within their transmission
range, w.h.p. Once a twin emerges within the transmission
range of a node v, stage 1 ensures that with constant probability v will receive an ACTIVE signal from that twin in a
round. Hence, these nodes will not be active singles within
O(log n) rounds, w.h.p., since their accounts will be set to 0
when they terminate the protocol.

prior rounds it holds that
Pr[Xt = 1 |

^

r∈S

(Xr = 1)] ≥ 8/9

Hence, we can use the Chernoff bounds to get that, if |I| ≥
c log n for a sufficiently large constant c, then X > |I|(3/4 +
1/144) w.h.p. If initially pv ≥ ϕ and |I| = O(log(n + 1/ϕ))
is large enough, then (1 + γ)|I|/144 ϕ ≥ p̂, which implies
together with X > |I|(3/4 + 1/144) that v would have
pt (v) = p̂ for at least a quarter of the rounds in I. When
using the accounting method in the protocol (see acc(v)),
it follows that v must be an active single at the end of I
(given that no node has become an active twin within its
transmission range).
Case 2: p′t ≥ 24π(1 + θ)4 p̂ for at least 1/8 of the rounds
in I. According to our cost model, the transmission region
of v must be inside a disk of diameter (1 + θ)r0 where r0 is
the ideal transmission√range for θ = 0. When cutting this
2
2⌉ sectors of equal angle and each
disk into ⌈2π(1 + θ)√
2
sector into ⌈(1 + θ) 2⌉ ring sections of equal width, then
4
we obtain
√ at most
√ 8π(1 + θ) ring sections with diameter at
most 2 ·r0 /( 2(1+θ)) = r0 /(1+θ). Thus, in each of these
sections, any pair of nodes can communicate with each other.
4
For each round t with p′t ≥ 24π(1
P + θ) p̂ there must be one
of these sections, say S, with w∈S∩Rv pt (w) ≥ 3p̂, which
implies
Pthat S can be decomposed
Pinto two groups S1 and S2
with w∈S1 ∩Rv pt (w) ≥ p̂ and w∈S2 ∩Rv pt (w) ≥ p̂. This
implies that if t is good (so the probabilities of the nodes
within the transmission range of any node in S1 or S2 sum
up to a constant), there is a constant probability > 0 that
the nodes in S will form a twin. Thus, if |I| ≥ c log n for a
sufficiently large constant c, then a twin will emerge within
the transmission range of v, w.h.p.

6. TOWARDS SELF-STABILIZATION
Note that the initial probabilities of the nodes can be set
in any way. From above we know that if the initial probabilities are at least ϕ, then the number of rounds the TWIN
protocol needs is O(log(n+1/ϕ)). Interestingly, also the initial acc(v) values of the nodes can be set in any way since the
proofs above do not make any assumptions on them besides
acc(v) ≥ 0. To see this, let us review the proof of Lemma 5.8
(the other lemmas are uncritical). In Lemma 5.8, there are
two cases. If case 1 is true, node v will become an active
single w.h.p. (which it will also be for very large acc(v)),
but if case 2 is true, then v will learn about an active twin
within its transmission range, w.h.p., and in this case it will
set acc(v) = 0 and terminate the protocol, so it will be an
inactive node at the end.
However, we still have two problems left. Initially, there
may be a non-constant density of twins, and the nodes should
not terminate the protocol at any point.
Termination is crucial for the TWIN protocol above because otherwise a non-constant density of twins may be created as the remaining nodes will continue to compete for
becoming a twin. The simplest way to get around this problem is to assume that the nodes have a round threshold of
R = Θ(log1+γ n). Whenever an inactive or active single receives an ACTIVE signal in stage 1 or senses a NEW signal
in stage 3, it does not participate in stage 2 for R many time
steps. This is equivalent to stopping the protocol, w.h.p., as
long as no active twin will become inactive again for some
reason or other events such as faults or mobility occur.
It remains to address the issue of twins with a non-constant
density. An approach that may work well in practice is the
following (which may be added as two additional time slots
in stage 1):
Each active twin v chooses one of two time slots uniformly
at random, say, slot s. For slot s, v sends out an ACTIVE
signal, and for the other slot, s̄, v senses the wireless medium
with threshold Ta that is sufficiently high so that it will not
be exceeded as long as the density of the twins does not
exceed a certain constant D. If v senses a busy medium, it
becomes inactive and sets acc(v) := 0.
Even though this approach just needs a single round to get
the density distribution of the active twins down to a constant, oscillation problems can occur in pathological cases
in which everywhere on expectation the cumulative signal
strength is slightly below Ta . The easiest approach to avoid
running into oscillation problems is that a twin sensing a
busy channel sends a reset signal that is flooded to its local k-neighborhood, e.g., with the help of surviving twins,
which have constant density once Ta has been applied so
that the flooding effect is kept local. Every active twin receiving or sensing a reset request is becoming inactive. The

Finally, we need to prove the following lemma.
Lemma 5.9. At the end of time interval I, each node v
can have at most a constant number of active singles and
twins within its transmission range, w.h.p.
Proof. First, consider the twins. According to the protocol, a node v will only participate in the twin creation if it
has not sensed a NEW signal in stage 3, which implies that
there cannot be twins within its sensing range defined by T .
Node v only becomes a twin if v sent a PING signal in stage
2 and also received one from some node w in that stage and
it also receives an ACK signal in stage 3 from some node
w′ (that does not necessarily have to be w). If v had sent a
PING signal together with a non-constant number of nodes
within its transmission range, then no other node within
its transmission range could have received a PING signal,
which means that v would not have received an ACK signal
in stage 3. Hence, only a constant number of nodes can become a twin within the transmission range of any node, and
any node becoming a twin does not have a twin within its
sensing range defined by T from previous rounds, so overall
the twins must have a constant density.
Next, we consider active singles. Recall the cases in the
proof of Lemma 5.8. Suppose that there are at least c many
nodes u ∈ Rv for which case 1 is true, which means that
they would all be active singles at the end of I, w.h.p. If the
constant c is sufficiently large, that would violate the case
1 assumption that pt < 24π(1 + θ)4 p̂ for at least 7/8 of the

99

2-neighborhood of a twin should suffice to make sure that
afterwards the density in its local area will not get beyond
what the original TWIN protocol would create, so oscillation problems will not occur in that area any more and the
dominating set will eventually stabilize.

[14]

7.

[15]

CONCLUSIONS

In this paper, we have introduced a fully distributed algorithm for constructing a constant density dominating set
under realistic interference and radio propagation models.
We believe the ideas and techniques presented in this paper
might prove very useful in the design of other distributed
protocols for wireless ad-hoc networks based on realistic
models, which is left for future work.

8.

[16]

[17]

REFERENCES

[1] http://standards.ieee.org/getieee802/download/
802.11a-1999.pdf
[2] M. Alicherry, R. Bathia, and L. Li, “Joint Channel
Assignment and Routing for Throughput
Optimization in Multi-Radio Wireless Mesh
Networks”, Proceedings of ACM International
Conference on Mobile Computing and Networking
(MobiCom), pp. 58–72, 2005.
[3] K. Alzoubi, P.J. Wan, O. Frieder, “New Distributed
Algorithm for Connected Dominating Set in Wireless
Ad Hoc Networks”, Proc. HICSS-35, 2002.
[4] G. Brar, D. Blough, and P. Santi, “Computationally
Efficient Scheduling with the Physical Interference
Model for Throughput Improvement in Wireless Mesh
Networks,” Proc. of ACM Mobicom, pp. 2–13, 2006.
[5] X. Cheng, D. Huang, W. Li, W. Wu, D.Z. Du, “A
Polynomial-Time Approximation Scheme for the
Minimum-Connected Dominating Set in Ad Hoc
Wireless Networks”, : Networks, vol. 42, 2003.
[6] S. Choi, “IEEE 802.11e MAC-level FEC Performance
Evaluation and Enhancement”, Proc. IEEE Globecom,
2002.
[7] B.N. Clark, C.J. Colbourn, D.S. Johnson, “Unit Disk
Graphs”, Discrete Mathematics, vol. 86, pp. 165–177,
1990.
[8] D. Dubhashi, A. Mei, A. Panconesi,
J. Radhakrishnan, A. Srinivasan, “Fast Distributed
Algorithms for (Weakly) Connected Dominating Sets
and Linear-Size Skeletons”, Proc. of ACM-SIAM
SODA, pp. 717–724, 2003.
[9] O. Goussevskaia, Y.V. Oswald, R. Wattenhofer,
“Complexity in Geometric SINR”, Proc. of ACM
MobiHoc, pp. 100–109, 2007.
[10] J. Gronkvist and A. Hansson, “Comparison Between
Graph-Based and Interference-Based STDMA
Scheduling,” Proc. of ACM MobiHoc, pp. 255–258,
2001.
[11] J. Gronkvist, “Traffic Controlled Spatial Reuse
TDMA in Multi-hop Radio Networks,” Proc. of
International Symposium on Personal, Indoor, and
Mobile Radio Communications, pp. 1203–1207, 1998.
[12] P. Gupta, P.R. Kumar, “The Capacity of Wireless
Networks,” IEEE Transactions on Information
Theory, vol. 46, no. 2, pp. 388–404, 2000.
[13] H. Huang, A.W. Richa, M. Segal, “Approximation
Algorithms for the Mobile Piercing Set Problem with

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]
[26]

[27]

[28]

[29]

[30]

100

Applications to Clustering in Ad Hoc Networks”,
ACM/Baltzer MONET, pp. 141–149, April 2004.
K. Jain, J. Padhye, V. Padmanabhan, and L. Qiu,
“Impact of Interference on Multi-Hop Wireless
Network Performance”, Proc. of ACM MobiHoc, pp.
66–80, 2003.
T. Jurdzinski, G. Stachoviak, “Probabilistic
Algorithms for the Wakeup Problem in Single-Hop
Radio Networks”, Proc. Int. Symposium on
Algorithms and Computation, pp. 535–549, 2002.
K. Kothapalli, C. Scheideler, M. Onus, A. Richa,
“Constant Density Spanners for Wireless Ad Hoc
Networks”, Proc. of ACM SPAA, pp. 116–125, 2005.
F. Kuhn, R. Wattenhofer, A. Zollinger, “Ad Hoc
Networks Beyond Unit Disk Graphs”, Proc. of ACM
DialM-POMC, 2003.
F. Kuhn, R. Wattenhofer, Y. Zhang, A. Zollinger,
“Geometric Ad Hoc Routing: Of Theory and
Practice”, Proc. of ACM PODC, pp. 25–32, 2003.
F. Kuhn, T. Moscibroda, R. Wattenhofer, “Radio
Network Clustering from Scratch”, Proc. of ESA,
2004.
F. Kuhn, R. Wattenhofer, “Constant-Time
Distributed Dominating Set Approximation”, Proc. of
ACM PODC, 2003.
P. Kyasanur and N. Vaidya, “Routing and Interface
Assignment in Multi-Channel Multi-Interface Wireless
Networks”, Proc. of IEEE WCNC, 2005.
M. Luby, “A Simple Parallel Algorithm for the
Maximal Independent Set Problem”, Proc. of ACM
STOC, pp. 1–10, 1985.
T. Moscibroda, R. Wattenhofer, A. Zollinger,
“Topology Control Meets SINR: The Scheduling
Complexity of Arbitrary Topologies”, Proc. of ACM
MobiHoc, pp. 310–321, 2006.
S. Parthasarathy, R. Gandhi, “Distributed Algorithms
for Coloring and Domination in Wireless Ad Hoc
Networks”, Proc. of FSTTCS, 2004.
B. Raman, “Channel Allocation in 802.11-based Mesh
Networks,” Proc. of IEEE Infocom, 2006.
A. Raniwala and T. Chiueh, “Architecture and
Algorithms for an IEEE 802.11-Based Multi-Channel
Wireless Mesh Networks”, Proc. of IEEE Infocom, pp.
2223–2234, 2005.
T.S. Rappaport, Wireless Communications: Principles
and Practice, 2nd edition, Prentice Hall, Upper Saddle
River, NJ, US, 2002.
C. Scheideler. Probabilistic Methods for Coordination
Problems. HNI-Verlagsschriftenreihe 78, University of
Paderborn, 2000. See also
www14.in.tum.de/personen/scheideler.
G. Sharma, R.R. Mazumdar, and N.B.Shroff, “On the
Complexity of Scheduling in Wireless Networks”,
Proc. of ACM Mobicom, pp. 227–238, 2006.
X. Yang, N.H. Vaidya, “Spatial Backoff Contention
Resolution for Wireless Networks”, Proc. of IEEE
WiMesh, 2006.

36

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

Continuous-Time Collaborative Prefetching of
Continuous Media
Soohyun Oh, Beshan Kulapala, Andréa W. Richa, and Martin Reisslein

Abstract—The real-time streaming of bursty continuous media,
such as variable-bit rate encoded video, to buffered clients over
networks can be made more efficient by collaboratively prefetching
parts of the ongoing streams into the client buffers. The existing
collaborative prefetching schemes have been developed for discrete time models, where scheduling decisions for all ongoing
streams are typically made for one frame period at a time. This
leads to inefficiencies as the network bandwidth is not utilized for
some duration at the end of the frame period when no video frame
“fits” into the remaining transmission capacity in the schedule. To
overcome this inefficiency, we conduct in this paper an extensive
study of collaborative prefetching in a continuous-time model. In
the continuous-time model, video frames are transmitted continuously across frame periods, while making sure that frames are
only transmitted if they meet their discrete playout deadlines. We
specify a generic framework for continuous-time collaborative
prefetching and a wide array of priority functions to be used for
making scheduling decisions within the framework. We conduct
an algorithm-theoretic study of the resulting continuous-time
prefetching algorithms and evaluate their fairness and starvation
probability performance through simulations. We find that the
continuous-time prefetching algorithms give favorable fairness
and starvation probability performance.
Index Terms—Client buffer, continuous media, continuous-time,
fairness, playback starvation, prefetching, prerecorded media,
traffic smoothing, video streaming.

I. INTRODUCTION
HE REAL-TIME streaming of continuous media over networks, such as the future Internet and next generation wireless systems, is a challenging problem mainly due to (i) the periodic playout deadlines, and (ii) the traffic variability. NTSC
video, for instance, has a periodic playout deadline (frame period) every 33 msec while PAL video has a 40 msec frame period, whereby a new video frame has to be delivered every frame
period to ensure continuous playback. A frame that is not delivered in time is essentially useless for the media playback and
results in interruptions of the playback. The continuous media
are typically compressed (encoded) to reduce their bit rates for
network transport. The efficient encoders, especially for video,
produce typically highly variable traffic (frame sizes), with ratios of the largest frame size to the average frame size for a given
video stream in the range between 8 and 15 [1]. As a result, allocating network resources based on the average bit rates would
result in frequent playout deadline misses since the larger frames

T

Manuscript received October 10, 2006; revised September 22, 2007. This
work is supported in part by the National Science Foundation under Grant No.
Career ANI-0133252.
The authors are with Arizona State University, AZ 85287-5706 USA (e-mail:
soohyun@asu.edu; beshan@asu.edu; aricha@asu.edu; reisslein@asu.edu)
Digital Object Identifier 10.1109/TBC.2007.910921

Fig. 1. J prerecorded video streams are multiplexed over a bottleneck link of
=
, and prefetched into client buffers of capacity B j bits,
capacity R
j
;
; J.

= 1 .. .

bits 1

()

could not be delivered in time, while allocating resources based
on the largest frame size would result in low average network
utilization.
To overcome these challenges, prefetching (work-ahead)
schemes have been developed that exploit the facts that (i)
a large portion of the media are prerecorded, and (ii) that
many of the media playback (client) devices have storage
space, by prefetching parts of an ongoing media stream. The
prefetching builds up prefetched reserves in the client buffers,
and these reserves help in ensuring uninterrupted playback.
The prefetching (smoothing) schemes studied in the literature
fall into two main categories: non-collaborative prefetching
schemes and collaborative prefetching schemes. Non-collaborative prefetching schemes, see for instance [2]–[19], smooth
an individual stream by pre-computing (off-line) a transmission schedule that achieves a certain optimality criterion (e.g.,
minimize peak rate or rate variability subject to client buffer
capacity). The streams are then transmitted according to the individually pre-computed transmission schedules. Collaborative
prefetching schemes [20]–[29], on the other hand, determine
the transmission schedule of a stream on-line as a function
of all the other ongoing streams. For a single bottleneck link,
this on-line collaboration has been demonstrated to be more
efficient, i.e., achieves smaller playback starvation probabilities
for a given streaming load, than the statistical multiplexing of
streams that are optimally smoothed using a non-collaborative
prefetching scheme [28]. We also note that there are transmission schemes which collaborate only at the commencement of
a video stream, e.g., the schemes that align the streams such
that the large intracoded frames of the MPEG encoded videos
do not collude [30].
A common characteristic of the existing collaborative
prefetching schemes is that they are designed based on a
discrete-time model, that is, the scheduling decisions are computed at discrete time instants. In particular, the majority of the

0018-9316/$25.00 © 2007 IEEE

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

exiting collaborative prefetching schemes calculate the transmission schedule on a per-frame period basis [20], [24]–[29].
These schemes essentially consider each frame period as a new
scheduling problem and attempt to fit as many video frames
as possible in the transmission capacity (link bit rate in bit/sec
frame period in sec) available in the currently considered
frame period. Frames are generally not scheduled across frame
periods. This leads to inefficiencies as some amount of available
transmission capacity at the end of a frame period goes unused
as no frame is small enough to fit into the remaining capacity.
Similar inefficiencies arise when frames are first smoothed
over an MPEG Group-of-Pictures (GoP) and then scheduled
using a JSQ like strategy executed on a per-frame period basis
[22], or when the scheduling decisions for the transmission of
individual frames are computed at discrete slot times [21], [23].
Another common characteristic of the existing collaborative
prefetching schemes is that they were primarily designed and
studied for minimizing the number of lost frames, i.e., the frame
loss probability. Since video frames requiring many bits for encoding have generally a larger impact on the delivered video
quality than video frames requiring only few encoding bits, the
number of lost bits, i.e., the information (bit) loss probability, is
an important metric when streaming compressed video.
In this paper we conduct an extensive study on collaborative
prefetching in a continuous-time model considering both the
frame loss probability and the information loss probability. The
continuous-time model overcomes the outlined inefficiency of
the discrete-time model by allowing for video frames to be
transmitted continuously across frame periods. The discrete
playout deadlines of the video frames need to be still considered, even in the continuous-time model, and our algorithms
ensure that frames that would not meet their deadline are
not transmitted. We specify a generic framework for continuous-time prefetching and an array of priority functions to be
used for making scheduling decisions within the framework.
The priority functions are based on numbers of transmitted
video frames and numbers of transmitted (or lost) video information bits. We conduct an algorithm-theoretic study of the
resulting continuous-time prefetching algorithms and evaluate
their fairness as well as frame and information starvation
probability performance through simulations. We find that the
continuous-time prefetching algorithms give favorable fairness
performance and significantly reduce the starvation probability
compared to discrete-time prefetching schemes.
This paper is organized as follows. In the following section
we describe the problem set-up and introduce the notations
used in the continuous-time modeling of the collaborative
prefetching. In Section III, we develop the generic framework
for continuous-time collaborative prefetching and introduce
a wide array of priority functions to be used within the
framework. In Section IV, we conduct an algorithm-theoretic complexity analysis of the prefetching framework. In
Section V, we conduct an algorithm-theoretic analysis of the
prefetching algorithm using the frame-based priority function
in the prefetching framework, while in Section VI we analyze the bit-based prefetching algorithms. In Section VII, we
present simulation results illustrating the fairness and starvation
probability performance of the continuous-time prefetching

37

algorithms and compare with the discrete time algorithms. In
Section VIII, we summarize our findings.
II. CONTINUOUS-TIME MODEL AND NOTATIONS
In our system model, which is illustrated in Fig. 1, a number
of prerecorded continuous media streams are stored in mass
storage in the server. We assume that the server is in one of the
following states, busy-with-transmission, busy-with-scheduling,
or idle. When the server is in the busy-with-transmission state, a
frame is being transmitted to the corresponding client. When the
transmission of the frame is complete, i.e., when the server has
sent out the last bit of the frame, the server becomes idle. If the
server has more frames to be delivered, then the server enters the
busy-with-scheduling state. The busy-with-scheduling state can
be masked by overlapped it with the preceding busy-with-transmission state, i.e., during transmission of a frame to client ,
the server computes the next frame to be transmitted. We assume
in this paper that the time that it takes to decide on the scheduling of the next frame is less than or equal to the transmission
time of the smallest video frame, which is reasonable given our
time complexity results for the prefetching algorithms. This allows for masking of the schedule computing time for all frames.
In our continuous-time model, frame deadlines are still
chosen from a discrete, evenly spaced set of time slots of length
, the common basic frame period of the videos in seconds.
However, scheduling and transmission of frames proceed in
a continuous-time fashion. A frame will be scheduled while
a previously scheduled frame is being transmitted, and right
after the current transmission ends (i.e., its last bit is sent out),
the newly scheduled frame will start being transmitted. Once
a video frame arrives at a client, it is placed in the client’s
prefetching buffer. For our model we assume that the time is
) when the server initiates scheduling
set to be 0 (i.e.,
and transmitting frames for clients. We also assume that at
, the first frame of each stream has deadline . In
time
other words, the first frame of a stream should arrive at the
to be decoded
corresponding client before or on time
and displayed during the first frame period
. If at time
, a complete video frame with deadline is not in the
prefetch buffer, the client suffers playback starvation and loses
the frame.
For simplicity of notation, which we summarize in Table I,
we assume that time is normalized by the frame period . The
frame with playback deadline , is removed from the buffer and
, and displayed during time
decoded at normalized time
. Each client displays the first frame (frame 1) of its
video stream during the time period [1, 2), and then removes the
and displays
second frame from its prefetch buffer at time
it during time period [2, 3), and so on. Formally, we let
denote the deadline of frame of stream and note that with
our assumptions,
.
If a video frame with deadline cannot be scheduled before
or at time
, where
denotes the size of frame
with deadline (or -th frame) of stream , then it is dropped
at the server and the client will suffer a frame loss during time
period
. We let
,
, denote the lowest
indexed frame for stream that is still on the server and has not
been dropped at time . In other words,
is the frame with
the earliest playout deadline that can still be transmitted to meet

38

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

TABLE I
DEFINITIONS OF NOTATIONS

its deadline. Let denote the earliest deadline frame among the
.
ongoing streams on the server at time , i.e.,
We assume that initially, at time
, all streams have
an infinite number of frames to be transmitted to the corresponding clients and their prefetch buffers are empty. We let
,
, denote the number of video frames that
have been transmitted to client up to time (and note the initial
condition
for
). Let
,
,
denote the number of bits that have been transmitted to client
up to time .
We let
,
, denote the number of video
frames of stream that have missed their playout deadline up
to time . The counter
is incremented by one whenever
client wants to retrieve a video frame from its buffer, but does
not find a complete frame in its buffer. Let
,
,
denote the number of bits of stream that have missed their
playout deadline up to time . We let
,
, denote the number of bits in the prefetch buffer of client at time
(and note that
for all clients
). Note
that the buffer constraint
must be satisfied for
all clients ,
, for all times . Although the case
of limited prefetch buffer capacities is more realistic, for ease
of analysis we initially consider the case of unlimited prefetch
buffer capacities (or sufficiently large buffer capacities).
We define the frame loss (starvation) probability of client
as
(1)
Similarly, we define the information loss probability of client
as
(2)
We define the average frame loss probability as
and the average information loss probability as
.

III. PREFETCHING FRAMEWORK AND PRIORITY FUNCTIONS
In our effort to study the problem of scheduling video frame
transmissions for the collaborative prefetching of continuous
media in continuous time, we first present a generic prefetching
framework. This framework is divided into two basic categories
according to the capacities of clients—unlimited buffer capacities and limited buffer capacities. We use the unlimited buffer
capacity scenario to study the effects of the limited network capacity on the video prefetching. With limited prefetch buffer
capacities, we can study the effects of both the limited network capacity and the limited client buffer capacity. In our algorithm-theoretic analysis of the proposed prefetching algorithms
we consider the unlimited prefetch buffer capacity, whereas in
our simulations we examine the effect of limited prefetch buffer
capacities.
In this section, we first present the generic scheduling frameworks for the continuous-time model and subsequently introduce the different priority functions that are employed within the
frameworks. Before we introduce the scheduling framework and
priority functions we briefly discuss the difficulties of finding
optimal solution to the continuous-time prefetching of continuous media. The objectives of a scheduling algorithm for the
continuous-time model are (i) to minimize the total number of
lost frames (or minimize the total number of lost bits) and (ii)
to treat the clients fairly. If we consider only objective (ii), fairness among clients, then it can be easily achieved by sending one
frame per stream. However, this problem becomes considerably
more involved when we consider the first objective: If we only
try to maximize the number of transmitted frames, then there is
an analogy between this problem and the standard job scheduling problem on a single processor. The standard job scheduling problem is defined as follows. There is a stream of tasks.
A task may arrive at any time and is assigned a value that reflects its importance. Each task has an execution time that represents the amount of time required to complete the task, and a
deadline by which the task is to complete execution. The goal
is to maximize the sum of the values of the tasks completed by

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

39

Fig. 2. Prefetch algorithm framework for case of unlimited client prefetch buffer capacity.

Fig. 3. Prefetch algorithm framework for case of limited client prefetch buffer capacity.

their deadlines. The analogy between the two scheduling problems is that the streams can be viewed as one stream where
some of tasks (frames) have the same deadline. Each frame is
assigned a value which is equal to one, and an execution time
which depends on the frame size. The off-line version of this
problem, where the time constraints of all tasks are given as
input, is known to be NP-hard [31]. A number of studies have
proposed on-line heuristics to handle situations when the system
is overloaded [31]–[33]. It is known that the earliest deadline
first policy gives the optimal solution when the system is underloaded [34].
A. Generic Prefetching Frameworks
Fig. 2 describes the generic prefetch algorithm framework for
the case of unlimited prefetch buffer capacity. In this case the
only consideration is the discrete deadline of each frame. The
priority of a stream will be defined by a priority function as detailed shortly. In the algorithm framework in Fig. 2, we neglect
the masking of the busy-with-scheduling state, i.e., assume the
schedule computation takes negligible time. If this time is significant, the algorithm execution would need to start earlier such
that it is completed when the server completes the current frame
transmission at time . The status of the various counters of numbers of transmitted or lost bits or frames at time is known at
this earlier time when the algorithm starts executing and to be
used in the algorithm’s calculations.
In the case of limited client prefetch buffer space, the server
must be aware of the clients’ prefetch buffers to prevent loss
caused by an overflow at the client side. Hence the server cannot
schedule frames for a client whose prefetching buffer is full even
though network capacity is available, as detailed in the algorithm framework in Fig. 3.
Importantly, the outlined generic prefetching frameworks
collaboratively consider all ongoing streams by computing the
priorities
for all streams
, and selecting
the stream with the highest priority for frame transmission.

B. Priority Functions
Within the presented algorithm framework we conduct an extensive study of prefetching algorithms that use different pri. We broadly categorize the priority funcority functions
tions into being based on the number of video frames or the
number of video information bits. The aim of the frame-based
priority function is primarily to minimize
, whereas the
bit-based priority functions aim at minimizing
.
In the remainder of this section we introduce the intuition behind the different priority functions, which are summarized in
Table II, and give a brief overview of their performance. The individual prefetching algorithms obtained by employing the different priority functions within the prefetching framework are
formally specified and analyzed in Sections V and VI and evaluated through simulations in Section VII.
1) Frame-Based Priority Function:
• Transmitted Frames (TF): If we maximize the number of
transmitted frames, we would expect to minimize frame
loss. Therefore, we propose a greedy local algorithm
that prioritizes the streams based on the number of frames
. With
transmitted so far, i.e., according to
the TF priority function, the stream with the least number
of transmitted frames has the highest priority. This algorithm strives to equalize the number of lost frames among
the clients. This algorithm has a good approximation ratio
when compared to an optimal offline algorithm when
considering long streams. We found from our simulations
that this algorithm produces essentially the same frame
loss probability for all clients. However, we have observed
that the information loss probabilities may slightly vary
among clients.
2) Bit-Based Priority Functions: Transmitted Bits:
• Normalized Transmitted Bits (NTB): This scheduling
policy is designed to minimize the loss probabilities by
maximizing the number of normalized transmitted bits,
i.e.,
, whereby the stream with the

40

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

TABLE II
SUMMARY OF CONSIDERED PRIORITY FUNCTIONS

smallest
has the highest priority. One shortcoming
of the priority function based on the average frame size
is that they may not accurately reflect the actual average data rate: The average frame size
represents the
average frame size of the entire video stream. However,
the average frame size in the part of the stream that has
been transmitted in the recent past and is currently considered for transmission may differ from the average frame
size over the entire stream. To overcome this problem we
propose the following alternative.
• Ratio of Transmitted Bits (RTB): This priority function is
based on the ratio of the number of transmitted bits to the
total number of bits that have been considered to be scheduled so far i.e.,
. This
priority function is designed to minimize the loss probabilities by maximizing the number of transmitted bits.
• Weighted Normalized Transmitted Bits (WNTB): The
drawback of the preceding bit-based priority functions is
that the frame deadlines are not directly taken into consideration, and hence they may cause increased information
loss by prefetching frames far in the future rather than
scheduling an imminent frame. As a consequence, unnecessary losses may occur and overall performance may be
degraded. Hence, we propose a weighing of the number of
transmitted bits by multiplying them with the number of
frames currently in the corresponding prefetch buffer, i.e.,
. If a client has some
frames in its prefetch buffer, it can display frames without
starvation for some time. In contrast, an empty prefetch
buffer implies that the first frame of the corresponding
stream should be immediately transmitted to avoid starvation. This WNTB priority function ensures that even
clients with a large number of successfully transmitted
bits can still be chosen by the algorithm for transmission
if they are close to starvation.
• Weighted Ratio of Transmitted Bits (WRTB) This priority
function combines the weighing with the normalization

by the actual number of bits considered for scheduling,
i.e., uses the priority function
.
3) Bit-Based Priority Functions: Lost Bits: To ensure an extensive evaluation of bit-based priority functions we also consider priority functions employing the number of lost bits. More
specifically, we consider the lost bits based counterparts of the
NTB, RTB, and WRTB policies.
• Normalized Lost Bits (NLB): The NLB priority function
considers the amount of lost bits normalized by the average
frame size of the video stream, i.e.,
.
The stream with the largest accumulated normalized lost
bits has the highest priority.
• Ratio of Lost Bits (RLB): The RLB priority function is
based on the ratio of the number of lost bits to the total
number of bits that have been considered to be scheduled
so far, i.e.,
. The stream
with the largest ratio has the highest priority.
• Weighted Ratio of Lost Bits (WRLB) This priority function
combines the weighing by the number of frames currently
in the prefetch buffer, achieved through the factor
,
with the RLB priority, i.e., the WRLB priority function is
.
IV. TIME COMPLEXITY OF GENERIC PREFETCH ALGORITHM
FRAMEWORK
In this section we analyze the computing time complexity of
the generic prefetch algorithm framework. It suffices to analyze
time complexity of the generic algorithm, since all the proposed
prefetch algorithms follow the basic framework of the generic
algorithm but use different priority functions, each of which can
be computed in constant time.
In the following, we compute the time taken for each step in
the algorithm frameworks presented in Figs. 2 and 3, thus determining the time complexity of the algorithm used for scheduling
the next frame for transmission. Since it takes only a constant
time to compute the priority of each stream (or client) at Step

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

41

Fig. 4. The least Transmitted Frames (TF) prefetch algorithm.

1 for all streams, it takes
time to compute the current
priorities for all streams. Finding the stream with the highest
priority takes
time at Step 2 and checking if the first frame
of the selected stream satisfies the deadline constraint takes another constant time. In a naive implementation of the algorithms,
we may need many iterations until we find a frame to be scheduled that can meet its deadline.
We can optimize the algorithms by initially checking the
deadline constraint for all streams and dropping any frames
whose deadline is smaller than , before we compute the priority
of each stream. Note that according to this implementation,
there cannot be any undropped frames with deadline smaller
at time (since the size of the previously scheduled
than
frame is at most
, ensuring that it can be
transmitted within one frame period) and hence the added complexity for checking the deadlines of the unscheduled frames is
since is a constant. The overall complexity
of the algorithms would then be
for one execution of
the continuous-time prefetch algorithm, which decides on the
scheduling of a video frame.
The computational complexity of the common discrete-time
prefetch algorithms is given in terms of the computational effort
required to compute the video frame transmission schedule for
a frame period of length . In order to facilitate the comparison of discrete-time and continuous-time prefetch algorithms
we characterize the computational complexity of the continuous-time prefetch framework for a frame period as follows.
denote the smallest frame among
We let
frames
the ongoing streams and note that at most
can be scheduled per frame period. Hence, the worst-case
computational complexity of the continuous-time prefetching
for computing the
framework is
schedule for a frame period. The corresponding complexities
for JSQ [28],
for DC
are
for BP [27]. Thus, continuous-time
[21], and
prefetching has comparable computational complexity to discrete-time prefetching.

V. SPECIFICATION AND ANALYSIS OF TF ALGORITHM
In this section we specify in detail and analyze the prefetch
algorithm obtained by employing the frame-based TF priority
function in the algorithm framework. The resulting least Transmitted Frames (TF) prefetch algorithm schedules frames as fol,
lows: At time , from among all ongoing streams
pick the streams that have the minimum number of transmitted
frames
. If there is more than one stream with the minimum
, then pick the stream that has the frame with the
earliest deadline and schedule this frame. If there is still a tie,
then pick the stream that has the smallest frame and schedule
its frame. The TF prefetch algorithm has the primary goal to
achieve small frame loss probabilities and is specified in Fig. 4.
The TF algorithm defines the priority of stream at time as

For the convenient calculation of this priority we define the
counter
to be the number of video frames of stream that
have already missed their playout deadlines up to time , including the frames that have not yet been identified and dropped
by the algorithm. This
counter, which is maintained as
specified in Fig. 4, is necessary since the number of dropped
is updated only when a client wants to retrieve
frames
a frame from its buffer. Frames that are still in the server but
cannot be delivered to the corresponding clients by their deadlines are hence not necessarily reflected by the counter
.
The equivalence
holds as long as the
frames for each stream are transmitted in their deadline order,
which is the case for our prefetch policies.
With the TF prefetch algorithm, the stream with the smallest
value has the highest priority. Note that if the value
of
is the same for all the streams, then the stream with
the most frame losses
has the highest priority. If any two
streams, say stream and , have the highest priority, then the
TF algorithm selects the stream with
. At the
end of the TF algorithm, one stream is selected to schedule and
transmit its lowest indexed frame.

42

A. Fairness Properties of TF Prefetch Algorithm
The following two claims show the fairness of the proposed
TF algorithm.
.
Lemma 1: At any time ,
Proof: Suppose that at time ,
. Let
, and analogously let
. For convenience we drop the subscript in the
following. Since
is equal to
, at time ,

Since
, stream
would be considered
prior to stream
. Suppose that frame
misses its
still has a lower
than
deadline. Then, stream
stream
, so that stream
would be considered prior to
stream
. If stream
is considered again, then stream
will schedule frame
at this time. Now, streams
and
have the same priority.
The above lemma shows that the proposed algorithm minimizes the difference between the maximum number of transmitted frames and the minimum number of transmitted frames.
We define the class of fair algorithms as being the set of algorithms for which Lemma 1 holds.
.
Corollary 1: At any time ,
Proof: Let
and
at time . Suppose that
at time
, where
. (Let us assume that only
.)
one frame has been transmitted during time period
I.e., during
, stream
drops frame
while
stream
does not drop any frame. There are two cases to be
considered.
is considered prior to stream
Case 1) Stream
during
.

Since the deadline of frame
is later than that
of frame
, stream
does not drop a frame
.
during time
Case 2) Stream
is considered prior to stream
during

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

Corollary 1 does not hold, could treat clients unfairly by dropping more frames for some clients than for other clients while
maintaining the number of transmitted frames equalized across
all clients.
B. Efficiency Properties of TF Prefetch Algorithm
We define a round to be a time interval during which each
stream schedules and transmits exactly one frame using the TF
algorithm. More specifically, we define the round such that all
streams have transmitted the same number of frames at the
beginning of a round, and each stream has increased its number
of transmitted frames by one at the end of the round. Intuitively,
if we minimize the time required to complete each round then
we minimize starvation. Without loss of generality, we suppose
that one round started at time 0 and it took time to complete this round. If
, then all streams would transmit
their frames—exactly one frame per stream—without any frame
where is an integer, then each stream
losses. If
will experience exactly
that transmits its frame at time
frame losses, and each stream that transmits its frame at
time
will experience exactly frame losses. Intuitively,
any fair algorithm must proceed in rounds. Otherwise, Lemma 1
would be violated. We label the rounds according to the number
of transmitted frames at the start of the round, i.e., round starts
when one of the streams has transmitted frames (while the
other streams still have transmitted
frames), and round
ends when all streams have transmitted exactly frames.
We analyze the amount of time required to complete one
round. We define to be the maximum value of the ratio of the
largest frame size to the smallest frame size over all streams.
. We show that the
i.e.,
proposed TF algorithm is an -approximation on minimizing
the time required to complete a round. More specifically, we
compare round for our proposed algorithm and an optimal alis the index of the scheduled frame
gorithm. Assume that
for stream in this round using the proposed algorithm. Let
denote the time required to transmit frame , i.e.,
. Then the total time taken to complete this round is
. Let
be the index of the scheduled frame for
stream that minimizes the total time for this round, i.e., the
. If no stream
optimal solution for this round is
. Let
be the
drops its frame during this round, then
set of streams such that
. Note that
since
. Thus,
(3)
(4)

We divide this case 2 into two sub-cases
(i)
and (ii)
. (i)
and
. Then
which
violates Lemma 1. (ii)
. If frame
misses its deadline, so does frame
.
Hence the corollary holds.
Lemma 1 and Corollary 1 taken together show that the TF
prefetching algorithm distributes the frame losses evenly among
clients. An algorithm for which Lemma 1 holds, but for which

(5)
(6)
(7)
(8)
which proves the following theorem.

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

43

Fig. 5. The NTB algorithm.

Theorem 1: The TF prefetch algorithm is an -approximation on minimizing the time required to complete a round, where
is the maximum ratio of the largest frame size to the smallest
frame size from the ongoing streams.
time to
Theorem 2: Any fair algorithm requires at most
complete a round with high probability.
Proof: We naturally assume that the capacity of the link
is large enough to accommodate the sum of the average frame
. We also assume that the probsizes, i.e., that
ability that the traffic from the ongoing streams exceeds the
link capacity is less than a small constant , i.e.,

frames per client. Thus, the number of transmitted frames for
each client according to the optimal algorithm is at most
times larger than the number of frames transmitted according to
our algorithm.
VI. SPECIFICATION AND ANALYSIS OF TRANSMITTED
BIT-BASED ALGORITHMS
In this section we consider the prefetch algorithms obtained
with the priority functions based on the number of transmitted bits in the prefetch algorithm framework. The resulting
bit-based prefetching algorithms have the primary goal to
achieve small information loss probabilities.

(9)
A. Normalized Transmitted Bits (NTB) Prefetch Algorithm
where
is the size of an arbitrary frame of stream .
We recall that we have defined a unit of time to be the frame
period with length . For example, one time unit starting at time
is the interval
and two time units starting at time
is the interval
and so on. We show that any fair
algorithm completes one round in
time units with high
probability as follows.
At least one stream could not send
its frame in
units
At least one stream could not send
its frame in the first time unit
At least one stream could not send
its frame in the second time unit
At least one stream could not send
its frame in the
-th time unit

Hence, with high probability it takes at most
time to send
exactly one frame per stream. This analysis holds for any scheduling algorithm as long as the algorithm is fair.
The preceding results imply that we have at most
frame losses per client during one round with high probability. At any time our algorithm has sent at least
frames per client and an optimal algorithm has sent at most

We define
bits for stream

to be the normalized number of transmitted
up to time , i.e.,
(10)

We rewrite the information loss probability of stream as
(11)
(12)
(13)
where (12) follows by noting that
is approximately equal
to the total number of bits in stream up to time (i.e., for large
:
). Hence maximizing
minimizes
the information loss probability.
We propose the least normalized transmitted bit (NTB)
prefetch algorithm that selects the next frame to be transmitted
at time by selecting the stream with
, i.e., we
employ the priority function
, as
detailed in Fig. 5. With the NTB algorithm, the stream with the
smallest
has the highest priority. If any two streams,
say stream and , have the same smallest
, then the
NTB policy selects the stream with the earlier playout deadline
. If there is again a tie, it selects the stream with the largest
frame.

44

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

B. Ratio of Transmitted Bits (RTB) Prefetch Algorithm
as

We rewrite the information loss probability of stream
follows

(17)
(18)
Fig. 6. Illustration of the difference between max r
for proof of Lemma 2.

(j ) and min r

(j )

1) Fairness Properties of NTB Prefetch Algorithm:
Lemma 2: The NTB algorithm satisfies at any time
,
, where
.
Proof: At the beginning of transmission (i.e., at time 0),
for all streams and thus the claim holds initially.
Suppose that after the -th frame transmission the claim holds.
-th frame transmission starts at time
Also suppose that the
and the transmission completed at time . From the algorithm
it follows that

Hence, maximizing the ratio of the number of transmitted bits
to the total number of bits minimizes the information loss probability of each stream. In this spirit, we propose the least Ratio
of Transmitted Bits (RTB) prefetch algorithm that schedules a
frame of the stream with smallest
for the next transmission.
The RTB policy is the same as the NTB policy, except the
way we define denominator of the priority function. The RTB
policy uses the ratio of the number of transmitted bits to the
while the
total number of bits to be transmitted up to time
NTB policy uses the normalized transmitted bits. Initially, we
for all . Let
be multiplied by current
set
time . Then

(14)
From the induction hypothesis, we have that
(15)
(16)
as illustrated in Fig. 6. From (14) and (16),
. Hence, at any time the claim holds.
2) Efficiency Properties of NTB Prefetch Algorithm: Suppose that there exists an optimal algorithm
that maximizes
the total number of transmitted normalized bits while the
transmitted normalized bits among clients are even. Let
be
the total number of transmitted normalized bits to all clients
up to time using algorithm
. Then each client has received roughly
normalized bits and has received roughly
actual bits up to time . Since we are fully utilizing
the bandwidth, the total amount of actual bits that our algorithm
has transmitted up to time is the same as the total amount of
actual bits that algorithm
has transmitted. Now, we claim
that at time ,
of our proposed NTB algorithm is no
less than
, where
.
Suppose that
, say
for some positive number . Since we must
have
,

which violates Lemma 2 and in turn proves the following theorem.
Theorem 3: At time ,
of the NTB prefetch algorithm is no less than
, where
is the total number
of transmitted normalized bits up to time using an optimal algorithm.

when is large. This implies that for a sufficiently large , the
priority of RTB is almost the same as that of NTB. Hence we expect that the RTB would perform as well as the NTB. In fact, our
simulation results indicate that the information loss probability
of RTB is typically lower than that of NTB. This appears to be
due to the fact that the RTB policy relies on the actual number
of transmitted bits, instead of the average number of transmitted
bits over long time horizons.
C. Weighted Normalized Transmitted Bits (WNTB) Prefetch
Algorithm
The weighted normalized transmitted bits (WNTB) algorithm
combines the NTB policy with a weighing according to the
number of prefetched frames. The WNTB priority function is
obtained by multiplying
with the number
of prefetched frames, i.e.,

The WNTB algorithm schedules a frame for the stream with the
smallest
for the next transmission. The priority function
of this hybrid algorithm considers the playout deadlines of the
frames in addition to the amount of information that has been
transmitted so far. A stream without prefetched frames (i.e.,
) has the highest priority. However, if more than one
client have zero prefetched frames, then the client that has received less bits should have higher priority. In order to permit
this differentiation, we use
for the weighing instead
. Similar reasoning leads to the weighted ratio of transmitted bits (WRTB) prefetch algorithm.
We conclude this section on the algorithm-theoretic analysis
of the transmitted bits based prefetch algorithms by noting that

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

45

TABLE III
VIDEO FRAME SIZE STATISTICS: PEAK TO MEAN RATIO, STANDARD DEVIATION (IN BIT), AND COEFFICIENT OF VARIATION (STANDARD DEVIATION NORMALIZED
BY MEAN FRAME SIZE). AVERAGE BIT RATE IS 64 kbps FOR ALL STREAMS

to the best of our knowledge the lost bits based prefetch algorithms are theoretically largely intractable.
VII. SIMULATION RESULTS
In this section we evaluate the proposed prefetch algorithms
through simulations using traces of MPEG-4 encoded video.
All used traces correspond to videos with a frame rate of 25
.
frames per second, i.e., the frame period is
The simulation implements the continuous-time model specified in Section II with frame playout deadlines that are discrete
integer multiples of the frame period
. The scheduling decisions are made on a continuous-time basis, driven by
the completion of the individual frame transmissions. That is, a
new frame transmission is scheduled to commence as soon as
the current frame transmission ends, irrespective of the discrete
frame playout deadlines. The peak to mean ratios, standard deviations, and coefficients of variation (standard deviation normalized by mean frame size) of the frame sizes of the video
traces used for the simulations are given in Table III. To study
the effects of video bit rate variability on the prefetch algorithms
we transformed the original video traces to obtain high variance video traces. This transformation increased the frame sizes
of large frames and decreased the sizes of small frames while
keeping the average bit rate fixed at 64 kbps. Throughout, we
refer the original video traces as the low variance video traces,
and the transformed video traces as the high variance video
traces. To simulate a mix of different video lengths (run times),
we generated for each video a set of 21 trace segments consisting of the full-length (108,000 frame) trace, the two halves
of the full-length trace, the three thirds, the four quarters, the
five fifth, and the six sixth of the full length trace. Each segment
was scaled to have an average bit rate of 64 kbit/sec.
For a detailed comparison of the different proposed continuous time prefetch algorithms, we first conduct steady state
simulations where all clients start at time zero with empty
prefetch buffers and there are always streams in progress,
all of which are collaboratively considered by the evaluated
prefetching mechanisms. Each client selects uniformly randomly a video title, one of the trace segments for the selected

title, and a uniform random starting phase into the selected
trace segment. The entire trace segment from the starting
phase frame to the frame preceding the starting phase frame
is streamed once (with wrap-around at the end of the trace
segment). When a stream terminates, i.e., the last frame of the
stream has been displayed at the client, the corresponding client
immediately selects a new video stream, i.e., random video title,
trace segment, and starting phase and begins transmitting the
new stream, whereby the prefetch buffer is initially empty. We
estimate the frame loss probabilities and the information (bit)
loss probabilities and their 90% confidence intervals for the
individual clients after a warm-up period using the method of
batch means. All simulations are run until the 90% confidence
intervals are less than 10% of the corresponding sample means.
For ease of discussion of the numerical results we normalize
the capacity of the bottleneck link by the 64 kbit/sec average bit
rate of the streams and denote this normalized capacity by .
Note that
, where is in units of bit/
and is the frame period in seconds.
A. Impact of Prefetch Buffer Size and Traffic Variability on
Continuous-Time Prefetch Algorithms
In this section we examine the effect of client prefetch buffer
size and the effect of video traffic variability on the proposed
continuous-time prefetch algorithms. We consider supporting
streams, each with buffer capacity , over a bottleneck
link of capacity
. Table IV shows the average frame
loss probability
and the average information probability
achieved by the different algorithms for different prefetch
buffer sizes for both low and high variability video streams.
We observe that generally the loss probability decreases as the
buffer size increases and is smaller for the low variance streams.
With a larger buffer, the clients can build up larger prefetched reserves, making starvation less likely. Also, the frame sizes of the
low variance are more uniform, making them generally easier to
schedule.
Examining more closely the loss probabilities for the different proposed prefetch algorithms, we first observe that for

46

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

TABLE IV
AVERAGE LOSS PROBABILITIES OF CONTINUOUS TIME PREFETCHING ALGORITHMS FOR DIFFERENT PREFETCH
BUFFERS B = 32, 64, 128, AND 256 KByte; J = 64 STREAMS IN STEADY-STATE TRANSMISSION OVER
AN R = 64 LINK. (a) AVERAGE FRAME LOSS PROBABILITY; (b) AVERAGE INFORMATION LOSS PROBABILITY

the frame-based TF algorithm, the frame loss probabilities for
the small buffers are significantly smaller than the information
loss probabilities. This is because the TF algorithm strives to
minimize the frame losses, and in doing so, tends to give preference to transmitting small frames and losing large frames.
Indeed, we observe from Table IV that for the TF algorithm,
the frame loss probability decreases while the information loss
probability slightly increases for the high variability streams
compared to the low variability streams. In the high variability
streams, there are relatively fewer mid-size frames, while there
are more small-sized and large-sized frames. As a consequence
the TF algorithm, which is purely focused on numbers of transmitted frames tends to transmit more small frames and lose
some more large frames. In contrast to the frame-based TF algorithm, we observe that for the bit-based algorithms, there is relatively little difference between the frame and the information
loss probability, with the information loss probabilities tending
to be generally slightly higher than the frame loss probabilities. This is because with the bit-based prefetch algorithms, the
larger frames have generally the same chance to be scheduled as
smaller frames, however, larger frames require more transmission time and therefore tend to be dropped more often when the
playout deadline is close.
Overall, we observe that counting the actual number of transmitted (or lost) bits (as done in the RTB, WRTB, RLB, and
WRLB policies) gives smaller loss probabilities than relying on
the long run average (as done in the NTB, WNTB, and NLB
policies). This is because the actual number of transmitted (or
lost) bits more closely reflects the current priority level of a
stream. Importantly, we observe that taking the number of currently prefetched frames (as done in the WNTB, WRLB, and
WRLB policies) into consideration results in significant reductions in loss probability compared to the corresponding policies
without the weighing. The number of prefetched frames gives
an immediate indication of how close or far—in terms of frame

periods—a client is to starvation and thus helps significantly in
selecting the client that is most in need of a frame. Combining
the counting of the actual number of transmitted (or lost) bits
with the weighing by the number of current prefetched frames
(as done in the WRTB and WRLB policies) gives the smallest
loss probabilities. We also observe that there is generally little
difference between considering the transmitted bits or the lost
bits.
B. Comparison Between Discrete-Time and Continuous-Time
Prefetch Algorithms
In this section we compare three representative discrete-time
prefetch algorithms, namely deadline credit (DC) [21], join-theshortest-queue (JSQ) [28], and bin packing (BP) [27], with the
continuous-time prefetch algorithms proposed in this paper. We
employ start-up simulations for this comparison since the DC
scheme is formulated for the start-up scenario in [21]. In the
start-up simulations, each of the streams starts at time zero
with an empty prefetch buffer and uniformly randomly selects a
full-length video trace and independently uniformly randomly
a starting phase into the selected trace. The full-length trace is
then streamed once and the frame and information loss probabilities for each stream are recorded. We run many independent replications of this start-up simulation until the 90% confidence intervals of the loss probabilities are less than 10% of
the corresponding means. In each replication, all streams start
with an empty prefetch buffer at time zero and select a random
full-length trace and starting phase.
For the DC scheme, we use a slot length of 1/2000th of the
frame period, i.e.,
slots. To convert the buffer occupation in bytes to the maximum deadline credit, we use the actual
sizes of the frames in the buffer. In the BP scheme, we use the
window size of 128 frames, which is a reasonable window size
for a prefetch buffer of
[27].

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

47

TABLE V
AVERAGE LOSS PROBABILITIES FOR DISCRETE- AND CONTINUOUS-TIME PREFETCH ALGORITHMS FROM START-UP
SIMULATIONS FOR DIFFERENT PREFETCH BUFFERS B = 32, 64, 128, AND 256 KByte; J = 64 STREAMS OVER
AN R = 64 LINK. (a) AVERAGE FRAME LOSS PROBABILITY; (b) AVERAGE INFORMATION LOSS PROBABILITY

1) Impact of Buffer Size and Traffic Variability: In Table V
we report the average loss probabilities for the discrete-time
and continuous-time prefetch algorithms for different prefetch
buffer sizes for both the low and high variability streams. In
Fig. 7 we plot the frame and information loss probabilities for
the individual clients for a prefetch buffer of
.
We first note that for this start-up scenario, the comparison
tendencies among the different continuous-time prefetch algorithms are generally unchanged from the steady-state scenario
considered in Section VII-A.
Turning to the comparison of continuous- and discrete-time
prefetch algorithms, we observe that for both the low and
high variability streams, the discrete-time prefetch algorithms
give significantly larger frame and information loss probabilities than the continuous-time algorithms. Typically, the
continuous-time prefetch algorithms reduce the starvation
probabilities by a factor of four or more compared to the
discrete-time algorithms. This improvement is primarily due to
utilizing the full bandwidth by scheduling video frames across
frame periods with the continuous-time prefetch algorithms.
The frame loss probabilities achieved by BP for small prefetch
buffers are an exception to this general trend in that BP achieves
frame loss probabilities as low as the continuous-time algorithms, but BP has information loss probabilities much larger
than the continuous-time algorithms. This behavior is caused by
BP’s strategy to look for small frames by temporarily skipping
large frames that do not fit into the remaining transmission capacity in a frame period and thus prefetching more future small

frames. In contrast, the JSQ algorithm does not skip frames,
but rather scans the next frame to be transmitted for all ongoing
streams to find frames that fit into the remaining transmission
capacity in a frame period. As observed from Table V, this JSQ
strategy results in larger frame loss probabilities, but smaller
information loss probabilities compared to BP.
Regarding fairness, we observe from Fig. 7 for both low and
high variability streams that the discrete-time prefetching algorithms generally provide the individual clients with approximately equal frame loss probabilities, but that the information
loss probabilities can vary significantly between clients. Indeed,
the discrete-time prefetch algorithms have primarily been designed to be fair in terms of the frame loss probability, while the
information loss probability has been largely ignored in the development of the discrete-time algorithms. In contrast, the continuous-time prefetch algorithms can provide fairness in terms
of the frame or information loss probability corresponding to the
adopted priority function. More specifically, the frame-based
TF prefetch policy meets the very tight fairness criterion of
Lemma 1, and gives consequently essentially identical frame
loss probabilities, as observed in Fig. 7. The individual information loss probabilities achieved with the TF algorithm vary
only slightly. Also, the bit-based prefetch algorithms give good
fairness both in terms of frame and information loss probability.
Overall, the continuous-time prefetch algorithms avoid the pronounced differences between the frame loss probability and the
information loss probability behaviors observed for BP by not
selectively prefetching frames with specific properties (such as

48

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

Fig. 7. Loss probabilities for individual clients for J = 64 streams over a link supporting R = 64 streams with B = 64 KByte buffer in each client. (a) Frame
loss probability for low variability streams; (b) frame loss probability for high variability streams; (c) information loss probability for low variability streams; (d)
information loss probability for high variability streams.

TABLE VI
AVERAGE LOSS PROBABILITIES FOR J = 64 LOW VARIABILITY STREAMS OVER LINK WITH DIFFERENT NORMALIZED CAPACITY R ; START-UP SIMULATION
WITH FIXED B = 64 KByte CLIENT BUFFERS

small number of bits). Instead, the continuous-time prefetch algorithms transmit the frames of a given stream in order of their
playout deadlines.
2) Impact of Utilization Level and System Size: In this section we analyze the impact of the utilization level, i.e., the ratio
of number of supported streams to normalized link capacity
, and the system size, i.e., the number of multiplexed streams
for a fixed utilization level, on frame and information loss.

and considering
We vary the utilization level by fixing
a range of normalized link capacities .
Considering first the overload scenarios with
in
Table VI, we observe that the continuous-time prefetching algorithms achieve information loss probabilities closer to the theoretical minimum of
than the discrete-time algorithms. Consistent with our earlier observations, the WRTB and
WRLB algorithms achieve information loss probabilities closest
to the theoretical minimum. We also observe that even slight

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

49

TABLE VII
AVERAGE LOSS PROBABILITIES FOR STREAMING J = R LOW VARIABILITY STREAMS TO B = 64 KByte CLIENTS

TABLE VIII
AVERAGE LOSS PROBABILITIES FOR 32 LOW VARIABILITY STREAMS AND 32 HIGH VARIABILITY STREAMS OVER AN R = 64 LINK

overload conditions with
result in fairly large instability
creases in the loss probabilities from the
limit scenario. The loss probabilities of the continuous-time algorithms increase by factors of around three to five, whereas the
discrete time algorithms, which have already larger loss probabilities at the
stability limit, experience relatively
smaller increases of the information loss probabilities by factors less than two. Further increasing the utilization by reducing
results roughly in a doubling of
the link capacity to
case. For the
the loss probabilities compared to the
extreme overload case of
, we observe information
loss probabilities close to the theoretical minimum of 1/2 and
correspondingly large frame loss probabilities.
Turning to the reduction in the utilization level by increasing
from 64 to 64.5, we observe that all algorithms, except DC
and BP, achieve reductions in the loss probabilities by roughly a
factor of two. The BP algorithm achieves a reduction by a factor
of close to two for the information loss probability; but the frame
loss probability, which is already very small for
, is only
very slightly further reduced. The DC algorithm can extract only
small reductions of the loss probabilities for the lower utilization. Overall, we conclude that the proposed continuous-time
prefetch algorithms achieve information loss probabilities close
to the theoretical minimum for overload conditions, and provide
significant reductions in the loss probability for even slight reductions on the utilization below the stability limit.
From Table VII we observe that the system size has a relatively minor impact on the performance of the prefetching al-

gorithms. Larger systems that multiplex more streams for the
same utilization level
have somewhat increased opportunities for statistical multiplexing and therefore achieve very
slightly smaller loss probabilities.
3) Impact of Heterogeneous Streams: In this section we examine the impact of heterogenous streams on the performance
of the prefetching algorithms. We first consider concurrently
streaming 32 low variability streams and 32 high variability
streams over an
link to clients with a
prefetch buffer using start-up simulations. We report the mean
frame and information loss probabilities experienced by the
group of 32 clients receiving low variability streams and
the group of 32 clients receiving high variability streams in
Table VIII.
We observe from the table that generally the prefetch algorithms provide relatively good fairness to the two groups of
video clients with both groups experiencing roughly equivalent
frame and information loss probabilities. In a few instances, the
high variability clients experience slightly higher loss probabilities, reflecting that the higher variability traffic is more challenging to schedule. The exception to this relatively good fairness performance for both loss probability measures is the TF
algorithm, which gives both groups of clients equivalent frame
loss probabilities, but significantly larger information loss probabilities to the high variability clients. This behavior is due to
the frame-based nature of the TF algorithm, which enforces the
same frame loss probability for all clients, but does not consider
the sizes of the video frames. The high variability streams contain relatively more large-sized frames, which are more difficult

50

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

Fig. 8. Loss probabilities for individual clients with clients 1 through 32 receiving low variability streams and clients 33 through 64 receiving high variability
streams over a link supporting R = 64 streams with B = 64 KByte buffer in each client. (a) Frame loss probability. (b) Information loss probability.

TABLE IX
AVERAGE LOSS PROBABILITIES FOR 32 STREAMS WITH 64 kbps AVERAGE BIT RATE AND 16 STREAMS WITH AND 128 kbps AVERAGE BIT RATE OVER AN
R = 64 LINK

to schedule, resulting in larger information loss for the high variability stream group.
For further insight into the loss probabilities experienced by
the individual clients we plot in Fig. 8 the frame and information loss probabilities for the individual clients. We observe
that the algorithms that provide good fairness among the groups
also provide good fairness among the individual clients within
a given group. Especially the continuous-time prefetch algorithms counting the actual numbers of bits and incorporating
weighing according to the number of prefetched frames (done
in WRTB and WRLB) achieve uniform information loss probabilities across the individual clients.
In a second heterogenous streaming scenario we consider
32 clients that receive video with an average bit rate of 64 kbps
and 16 clients that receive video with an average bit rate of
128 kbps. Note that the total average bit rate is equivalent to 64
clients concurrently receiving 64 kbps steams. We set
as in the preceding start-up simulations. Table IX shows the
average frame and information loss probabilities for the two
groups of clients. We observe that the frame-based algorithms
provide similar frame loss probabilities to the two groups, with
the DC and TF algorithms providing particularly strict fairness.
On the other hand, the bit-based algorithms provide similar in-

formation loss probabilities to the two groups. The algorithms
incorporating the long run average rates show slight differences,
while the algorithms considering the actual number of transmitted bits demonstrate very good fairness performance.
Lastly, comparing the loss probabilities for the heterogenous
scenarios in Tables VIII and IX with the corresponding loss
probabilities for the homogeneous scenario, e.g., the low variscenario in Table V, we
ability streams with
observe that the DC algorithm gives overall somewhat higher
loss probabilities for the heterogeneous scenarios. The other algorithms give overall roughly equivalent loss probability levels
for the homogeneous and heterogeneous scenarios.
VIII. CONCLUSION
We have developed and evaluated continuous-time algorithms for the collaborative prefetching of continuous media
with discrete playout deadlines, such as encoded video. The
continuous-time algorithms allow for the transmission of
video frames across frame period boundaries. In contrast,
previously studied collaborative prefetching schemes typically
scheduled video frames for transmission during a given frame
period and did not permit transmissions across the frame
period boundaries. This led to inefficiencies in the form of
unused transmission capacity at the end of a frame period,

OH et al.: CONTINUOUS-TIME COLLABORATIVE PREFETCHING OF CONTINUOUS MEDIA

when no frame could fit into the remaining capacity. The
continuous-time prefetching algorithms developed in this
paper overcome these inefficiencies and typically reduce the
starvation probabilities by a factor of four or more. We have
formally analyzed the fairness and efficiency characteristics of
the proposed continuous-time prefetching algorithms.
There are many exciting avenues for future work on continuous-time collaborative prefetching. One direction is to
consider the streaming from several distributed servers over a
common bottleneck to clients. In the present study the video
was streamed from a single server and all the scheduling decisions were made at that single server. In a multi-server scenario,
which could arise when streaming in parallel from several peers
in a peer-to-peer network, the different servers would need
coordinate their transmissions so as to achieve overall low
starvation probabilities and good fairness. Another direction
is to explore the continuous-time prefetching over wireless
links. In the present study, the bottleneck link was reliable, i.e.,
a video frame scheduled on the link was guaranteed to arrive
at the client. In contrast, wireless links are unreliable, i.e., a
transmitted frame may be dropped on the wireless link, and
possibly require retransmission.

REFERENCES
[1] F. H. Fitzek and M. Reisslein, “MPEG-4 and H.263 video traces for
network performance evaluation,” IEEE Network, vol. 15, no. 6, pp.
40–54, November/December 2001.
[2] C. Bewick, R. Pereira, and M. Merabti, “Network constrained
smoothing: Enhanced multiplexing of MPEG-4 video,” in Proceedings of IEEE International Symposium on Computers and
Communications, Taormina, Italy, July 2002, pp. 114–119.
[3] H.-C. Chao, C. L. Hung, and T. G. Tsuei, “ECVBA traffic-smoothing
scheme for VBR media streams,” International Journal of Network
Management, vol. 12, pp. 179–185, 2002.
[4] W.-C. Feng and J. Rexford, “Performance evaluation of smoothing algorithms for transmitting prerecorded variable-bit-rate video,” IEEE
Trans. Multimedia, vol. 1, no. 3, pp. 302–313, Sept. 1999.
[5] T. Gan, K.-K. Ma, and L. Zhang, “Dual-plan bandwidth smoothing
for layer-encoded video,” IEEE Trans. Multimedia, vol. 7, no. 2, pp.
379–392, Apr. 2005.
[6] M. Grossglauser, S. Keshav, and D. Tse, “RCBR: A simple and efficient
service for multiple time-scale traffic,” IEEE/ACM Trans. Networking,
vol. 5, no. 6, pp. 741–755, Dec. 1997.
[7] Z. Gu and K. G. Shin, “Algorithms for effective variable bit rate traffic
smoothing,” in Proceedings of IEEE International Performance, Computing, and Communications Conference, Phoenix, AZ, Apr. 2003, pp.
387–394.
[8] C.-D. Iskander and R. T. Mathiopoulos, “Online smoothing of VBR
H.263 video for the CDMA2000 and IS-95B uplinks,” IEEE Trans.
Multimedia, vol. 6, no. 4, pp. 647–658, Aug. 2004.
[9] M. Krunz, W. Zhao, and I. Matta, “Scheduling and bandwidth allocation for distribution of archived video in VoD systems,” Journal of
Telecommunication Systems, Special Issue on Multimedia, vol. 9, no.
3/4, pp. 335–355, Sept. 1998.
[10] M. Krunz, “Bandwidth allocation strategies for transporting variablebit-rate video traffic,” IEEE Communications Magazine, vol. 37, no. 1,
pp. 40–46, Jan. 1999.
[11] H. Lai, J. Y. Lee, and L.-K. Chen, “A monotonic-decreasing rate scheduler for variable-bit-rate video streaming,” IEEE Trans. Circuits and
Systems for Video Technology, vol. 15, no. 2, pp. 221–231, Feb. 2005.
[12] S. S. Lam, S. Chow, and D. K. Y. Yau, “A lossless smoothing algorithm
for compressed video,” IEEE/ACM Trans. Networking, vol. 4, no. 5, pp.
697–708, Oct. 1996.
[13] R. Sabat and C. Williamson, “Cluster-based smoothing for
MPEG-based video-on-demand systems,” in Proceedings of IEEE
International Conference on Performance, Computing, and Communications, Phoenix, AZ, Apr. 2001, pp. 339–346.
[14] J. Salehi, Z.-L. Zhang, J. Kurose, and D. Towsley, “Supporting stored
video: Reducing rate variability and end-to-end resource requirements
through optimal smoothing,” IEEE/ACM Trans. Networking, vol. 6, no.
4, pp. 397–410, Aug. 1998.

51

[15] A. Solleti and K. J. Christensen, “Efficient transmission of stored
video for improved management of network bandwidth,” International
Journal of Network Management, vol. 10, pp. 277–288, 2000.
[16] B. Vandalore, W.-C. Feng, R. Jain, and S. Fahmy, “A survey of application layer techniques for adaptive streaming of multimedia,” Real-Time
Imaging Journal, vol. 7, no. 3, pp. 221–235, 2001.
[17] D. Ye, J. C. Barker, Z. Xiong, and W. Zhu, “Wavelet-based VBR video
traffic smoothing,” IEEE Trans. Multimedia, vol. 6, no. 4, pp. 611–623,
Aug. 2004.
[18] D. Ye, Z. Xiong, H.-R. Shao, Q. Wu, and W. Zhu, “Wavelet-based
smoothing and multiplexing of VBR video traffic,” in Proceedings of
IEEE Globecom, San Antonio, TX, Nov. 2001, pp. 2060–2064.
[19] Z.-L. Zhang, J. Kurose, J. Salehi, and D. Towsley, “Smoothing,
statistical multiplexing and call admission control for stored video,”
IEEE Journal on Selected Areas in Communications, vol. 13, no. 6,
pp. 1148–1166, Aug. 1997.
[20] M. B. Adams and L. D. Williamson, “Optimum Bandwidth Utilization
in a Shared Cable System Data Channel,” United States Patent Number
6 124 878, filed December 1996, granted September 2000.
[21] Z. Antoniou and I. Stavrakakis, “An efficient deadline-credit-based
transport scheme for prerecorded semisoft continuous media applications,” IEEE/ACM Trans. Networking, vol. 10, no. 5, pp. 630–643, Oct.
2002.
[22] S. Bakiras and V. O. Li, “Maximizing the number of users in an interactive video-on-demand system,” IEEE Trans. Broadcasting, vol. 48,
no. 4, pp. 281–292, Dec. 2002.
[23] J. C. H. Yuen, E. Chan, and K.-Y. Lam, “Real time video frames allocation in mobile networks using cooperative pre-fetching,” Multimedia
Tools and Applications, vol. 32, no. 3, pp. 329–352, Mar. 2007.
[24] Y.-W. Leung and T. K. C. Chan, “Design of an interactive video-ondemand system,” IEEE Trans. Multimedia, vol. 5, no. 1, pp. 130–140,
Mar. 2003.
[25] F. Li and I. Nikolaidis, “Trace-adaptive fragmentation for periodic
broadcast of VBR video,” in Proceedings of 9th International Workshop on Network and Operating Systems Support for Digital Audio
and Video (NOSSDAV), Basking Ridge, NJ, June 1999, pp. 253–264.
[26] C.-S. Lin, M.-Y. Wu, and W. Shu, “Transmitting variable-bit-rate
videos on clustered VOD systems,” in Proceedings of IEEE International Conference on Multimedia and Expo (ICME), New York, NY,
July 2000, pp. 1461–1464.
[27] S. Oh, Y. Huh, B. Kulapala, G. Konjevod, A. W. Richa, and M.
Reisslein, “A modular algorithm-theoretic framework for the fair and
efficient collaborative prefetching of continuous media,” IEEE Trans.
Broadcasting, vol. 51, no. 2, pp. 200–215, June 2005.
[28] M. Reisslein and K. W. Ross, “High-performance prefetching protocols for VBR prerecorded video,” IEEE Network, vol. 12, no. 6, pp.
46–55, Nov/Dec 1998.
[29] H. Zhu and G. Cao, “A power-aware and QoS-aware service model
on wireless networks,” in Proceedings of IEEE Infocom, Hong Kong,
Hong Kong, Mar. 2004, pp. 1393–1403.
[30] M. Krunz and S. K. Tripathy, “Exploiting the temporal structure of
MPEG video for the reduction of bandwidth requirements,” in Proceedings of IEEE Infocom, Kobe, Japan, Apr. 1997, pp. 67–74.
[31] S. Baruah, G. Koren, B. Mishra, A. Raghunathan, L. Rosier, and D.
Shasha, “On-line scheduling in the presence of overload,” in Proceedings of the 32nd Annual Symposium on Foundation of Computer Science, 1991, pp. 100–110.
[32] T.-W. Lam and K.-K. To, “Performance guarantee for online deadline
scheduling in the presence of overload,” in Proceedings of the ACMSIAM Symposium on Discrete Algorithms (SODA), 2001, pp. 755–764.
[33] L. Sha, J. P. Lehoczky, and R. Rajkuma, “Solutions for some practical
problems in prioritized preemptive scheduling,” in Proceedings of the
Real-Time Systems Symposium, 1986, pp. 181–191.
[34] M. L. Dertouzos, “Control robotics: The procedural control of physical
processes,” in Proceedings of the IFIP Congress, 1974, pp. 807–813.

Soohyun Oh is a lecturer in the Department of
Computer Engineering at Hansung University,
Seoul, Korea. She received M.S. and Ph.D. degrees
in Computer Science from Arizona State University,
in 2001 and 2005, respectively. From July 2006
through February 2007 she joined Mobile Computing Laboratory at Sungkyunkwan University as
a researcher. Her research interests are continuous
media streaming, QoS, and packet routing.

52

IEEE TRANSACTIONS ON BROADCASTING, VOL. 54, NO. 1, MARCH 2008

Beshan Kulapala received the B.S. degree in electrical engineering from the University of Kentucky,
Lexington, in 2001, and received the M.S. degree in
electrical engineering from Arizona State University,
Tempe, in 2003. Since 2003 he has been a Ph.D. student in the Department of Electrical Engineering at
Arizona State University. His research interests are in
the area of video transmission over wired and wireless networks. He is a student member of the IEEE.

Andréa W. Richa is an Associate Professor at the
Department of Computer Science and Engineering
at Arizona State University, Tempe, since August
2004. She joined this department as an Assistant
Professor in August 1998. Prof. Richa received
her M.S. and Ph.D. degrees from the School of
Computer Science at Carnegie Mellon University,
in 1995 and 1998, respectively. She also earned an
M.S. degree in Computer Systems from the Graduate
School in Engineering (COPPE), and a B.S. degree
in Computer Science, both at the Federal University
of Rio de Janeiro, Brazil, in 1992 and 1990, respectively. Prof. Richa’s main
area of research is in network algorithms. Some of the topics Dr. Richa has
worked on include packet scheduling, distributed load balancing, packet
routing, mobile network clustering and routing protocols, and distributed data
tracking. Prof. Richa’s data tracking (or name lookup) algorithm has been
widely recognized as the first benchmark algorithm for the development of
distributed databases in peer-to-peer networking, having being references
by over 130 academic journal or conference publications to date, and being

implemented as part of two of the current leading projects in peer-to-peer
networking. Dr. Richa’s was the recipient of an NSF CAREER Award in 1999.
For a selected list of her publications, CV, and current research projects, please
visit http://www.public.asu.edu/aricha.

Martin Reisslein is an Associate Professor in the Department of Electrical Engineering at Arizona State
University (ASU), Tempe. He received the Dipl.-Ing.
(FH) degree from the Fachhochschule Dieburg, Germany, in 1994, and the M.S.E. degree from the University of Pennsylvania, Philadelphia, in 1996. Both
in electrical engineering. He received his Ph.D. in
systems engineering from the University of Pennsylvania in 1998. During the academic year 1994–1995
he visited the University of Pennsylvania as a Fulbright scholar. From July 1998 through October 2000
he was a scientist with the German National Research Center for Information
Technology (GMD FOKUS), Berlin and lecturer at the Technical University
Berlin. From October 2000 through August 2005 he was an Assistant Professor
at ASU. He served as editor-in-chief of the IEEE Communications Surveys and
Tutorials from January 2003 through February 2007 and has served on the Technical Program Committees of IEEE Infocom, IEEE Globecom, and the IEEE
International Symposium on Computer and Communications. He has organized
sessions at the IEEE Computer Communications Workshop (CCW). He maintains an extensive library of video traces for network performance evaluation,
including frame size traces of MPEG–4 and H.264 encoded video, at http://
trace.eas.asu.edu. His research interests are in the areas of Internet Quality of
Service, video traffic characterization, wireless networking, optical networking,
and engineering education. For a selected list of his publications, CV, and current research projects, please visit http://www.fulton.asu.edu/~mre.

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

Minimum Maximum Degree Publish-Subscribe
Overlay Network Design
Melih Onus

Andréa W. Richa

Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85281
Email: melih@asu.edu

Department of Computer Science and Engineering
Arizona State University
Tempe, AZ 85281
Email: aricha@asu.edu

Abstract—Designing an overlay network for publish/subscribe
communication in a system where nodes may subscribe to many
different topics of interest is of fundamental importance. For
scalability and efficiency, it is important to keep the degree of
the nodes in the publish/subscribe system low. It is only natural
then to formalize the following problem: Given a collection of
nodes and their topic subscriptions connect the nodes into a
graph which has least possible maximum degree and in such
a way that for each topic t, the graph induced by the nodes
interested in t is connected. We present the first polynomial
time logarithmic approximation algorithm for this problem and
prove an almost tight lower bound on the approximation ratio.
Our experimental results show that our algorithm drastically
improves the maximum degree of publish/subscribe overlay
systems.
We also propose a variation of the problem by enforcing that
each topic-connected overlay network be of constant diameter,
while keeping the average degree low. We present a heuristic
for this problem which guarantees that each topic-connected
overlay network will be of diameter 2 and which aims at keeping
the overall average node degree low. Our experimental results
validate our algorithm showing that our algorithm is able to
achieve very low diameter without increasing the average degree
by much.

I. I NTRODUCTION
In the publish/subscribe (pub/sub) communication
paradigm, publishers and subscribers interact in a decoupled
fashion. Publishers publish their messages through logical
channels and subscribers receive the messages they are
interested in by subscribing to the appropriate services, which
deliver messages through these channels.
A pub/sub system may be topic-based, if messages are
published to “topics”, where each topic is uniquely associated
with a logical channel. Subscribers in a topic-based system
will receive all messages published to the topics to which they
subscribe. The publisher is responsible for defining the classes
of messages to which subscribers can subscribe. In a contentbased system, messages are only delivered to a subscriber
if the attributes of those messages match constraints defined
by the subscriber; each logical channel is characterized by a
subset of these attributes. The subscriber is responsible for
classifying the messages.
This work was supported in part by NSF awards CCF-0830791 and CCF0830704.

Pub/sub communication systems are scalable and simple to
implement (see e.g., [1]–[4], [6]–[10], [15], [17], [19]).
Hence there are many applications which are built on top
of such systems, most notably a plethora of Internet-based
applications, such as stock-market monitoring engines, RSS
feeds [18], on-line gaming and several others. For a survey on
pub/sub systems, see [14].
In this paper, we will design a (peer-to-peer) overlay network for each pub/sub topic, in the sense that for each topic
t, the subgraph induced by the nodes interested in t will be
connected. This translates into a fully decentralized topicbased pub/sub system since any given topic-based overlay
network will be connected and thus nodes subscribed to a
given topic do not need to rely on other nodes (agents) for
forwarding their messages. Such an overlay network is called
topic-connected.
We can evaluate the complexity of a pub/sub overlay
network in terms of the cost of topic-based broadcasts on the
network. As in many other systems, a space-time trade-off
exists: On one hand, one would like the total time taken by
the broadcast (which directly depends on the diameter of each
topic-based subnetwork) to be as small as possible; on the
other hand, for memory and node bandwidth considerations,
one would like to keep the total degree of a node small.
Those two measures are often conflicting. For example, take
the simple scenario where all nodes are subscribed to the same
topic: A star overlay would result in the best possible diameter
but worst possible degree for the nodes. Even if we were to
maintain a balanced structure (e.g., a balanced binary tree)
for each topic, it is not clear how to achieve that without
letting the node degrees grow as large as the sizes of the node
subscription sets.
Some of the current solutions adopted in practice actually
fail at maintaining both the diameter and the node degrees
low. A naive, albeit popular, solution to topic connectedoverlay network design is to construct a cycle (or a tree
or any other separate overlay structure) connecting all nodes
interested in a topic independently for each given topic [19]:
This construction may result in a network with node degrees proportional to the nodes’ subscription sizes, whereas a
more careful construction, taking into account the correlations
among the node subscription sets might result in much smaller

978-1-4244-3513-5/09/$25.00 ©2009 IEEE

882

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

node degrees (and total number of edges).
Low node degrees are desirable in practice for scalability
and also due to bandwidth constraints. Nodes with a high
number of adjacent links will have to manage all these links
(e.g., monitor the availability of its neighbors, incurring in
heartbeats and keep-alive state costs, and connection state
costs in TCP) and the traffic going through each of the links,
without being able to take great advantage of aggregating the
traffic (which would also reduce the number of packet headers,
which can be responsible for a significant portion of the traffic
for small messages). See [12] for further motivation.
The node degrees and number of edges required by a topicconnected overlay network will be low if the node subscriptions are well-correlated. In this case, by connecting two nodes
with many coincident topics, one can satisfy connectivity of
many topics for those two nodes with just one edge. Several
recent empirical studies suggest that correlated workloads are
indeed common in practice [18].
In this work, we first consider the problem of devising
topic-based pub/sub overlay networks with low node degrees.
More specifically, we consider the following problem:
Minimum Maximum Degree Topic-Connected Overlay
(MinMax-TCO) Problem: Given a collection of nodes V , a set
of topics T , and the node interest assignment I, connect the
nodes in V into a topic-connected overlay network G which
has the least possible maximum degree.
We present a logarithmic approximation algorithm for this
problem. We also show that no polynomial time algorithm can
approximate MinMax-TCO problem within a constant factor
(unless P=NP), so our approximation guarantees are almost
tight. We further validate our algorithm with experimental
results.
We also propose a variation of the MinMax-TCO problem
by enforcing that each topic-connected overlay network be of
constant diameter, while keeping the average degree low (see
MinAv-TCO problem defined in the next section). We present
a heuristic for this problem which guarantees that each topic’s
induced overlay subnetwork will be of diameter 2 and which
aims at keeping the average node degree of the overall topicconnected overlay network low. We validate this algorithm
through experimental results.
A. Related Work
In [11], Chockler et al. introduced a closely related problem
to the MinMax-TCO problem, which we call MinAv-TCO
[In the original paper, this problem was called Min-TCO;
since it aims at minimizing the average degree of the overlay
network, and in order to avoid confusion with the MinMaxTCO problem considered in this paper, we will refer to the
problem considered by Chockler et al. in [11] as MinAv-TCO
in our work.]. The MinAv-TCO problem aims at minimizing
the average degree of the nodes rather than the maximum
degree. They present an algorithm, called GM, which achieves
a logarithmic approximation on the minimum average degree

of the overlay network. While minimizing the average degree is a step forward towards improving the scalability and
practicality of the pub/sub system, their algorithm may still
produce overlay networks of very uneven node degrees where
the maximum degree may be unnecessarily high: As we will
show in Section III, their algorithm may produce a network
with maximum degree |V | while a topic-connected overlay
network of constant degree exists for the same configuration
of I. Some of the high level ideas and proof techniques of [11]
have their roots in techniques used for the classical Set-Cover
problem. We benefit from some of the ideas in [11] and also
build upon the constructions for Set-Cover, extending and
modifying them to be able to handle the maximum degree
case.
To the best of our knowledge, minimizing max-degree or
diameter in topic-connected pub/sub overlay network design
had not been directly addressed prior to this work. The overlay
networks resulting from [2], [5], [10] are not required to
be topic-connected. In [4], [9], [12], [19], topic-connected
overlay networks are constructed, but they make no attempt to
minimize the average or maximum node degree. The first to
directly consider node degrees when building topic-connected
pub/sub systems were Chockler et al. in [11], as we mentioned
above.
B. Our Contributions
Our main contribution in this paper is the formal design
and analysis of the topic-connected overlay design algorithm
(MinMax-ODA) which approximates the MinMax-TCO problem within a logarithmic factor. The MinMax-ODA algorithm
is a greedy algorithm which relies on repeatedly using a greedy
approach for finding matchings that connect a large (close to
maximum) number of different connected components which
emerge for the given topics. We also show that no polynomial
time algorithm can approximate the MinMax-TCO problem
within a constant factor (unless P=NP), and so our MinMaxODA algorithm is almost tight. No previous algorithm with
sublinear approximation guarantees on the maximum degree
of a topic-connected pub/sub overlay network was known prior
to this work. Furthermore, we validate the performance of
MinMax-ODA with experimental results.
In addition, we present an algorithm, CD-ODA, which
builds a topic-based pub/sub network, where each topicconnected component is guaranteed to be of constant diameter
— more specifically of diameter 2 —and where we aim at
keeping the average degree low. While we do not have a formal
proof on any approximation guarantees on the average node
degree, we present steps of a possible formal proof, intuitions
and conjectures. Furthermore, we validate the performance of
CD-ODA with experimental results.
C. Structure of the paper
In Section II, we present some definitions and restate
the formal problem definition. In Section III, we present an
outline of the related problem of minimizing the average node

883

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

degree, namely the MinAv-TCO problem, and the corresponding logarithmic approximation algorithm GM proposed by
Chockler et al. [11], since some of the ideas presented will
be useful for the minMax-TCO problem. Section IV presents
our topic-connected overlay design algorithm MinMax-ODA,
whose approximation ratio is proved in Section V; in Section
VI, we present the hardness of approximation results for
MinMax-TCO. Section VIII addresses the CD-TCO problem
and presents our algorithm, CD-ODA, for the same. We
conclude the paper, also presenting some future work, in
Section IX.

Fig. 1. (a) Overlay with optimal max degree (b) Overlay constructed by GM

II. P RELIMINARIES
Let V be the set of nodes, and T be the set of topics. Let
n = |V |. The interest function I is defined as I : V × T →
{0, 1}. For a node v ∈ V and topic t ∈ T , I(v, t) = 1 if
and only if node v is subscribed to topic t, and I(v, t) = 0
otherwise.
For a set of nodes V , an overlay network G(V, E) is an
undirected graph on the node set V with edge set E ⊆ V × V .
For a topic t ∈ T , let Vt = {v ∈ V |I(v, t) = 1}. Given a
topic t ∈ T and an overlay network G(V, E), the number
of topic-connected components of G for topic t is equal to
the number of connected components of the subgraph of G
induced by Vt . An overlay network G is topic-connected if
and only if it has one topic-connected component for each
topic t ∈ T . The diameter of a graph is the length of the
longest shortest path in the graph. The degree of a node v in
an overlay network G(V, E) is equal to the total number of
edges adjacent to v in G.
Minimum Maximum Degree Topic-Connected Overlay
(MinMax-TCO) Problem: Given a collection of nodes V , a
set of topics T , and the node interest assignment I, connect
the nodes in V into a topic-connected overlay network G
which has least possible maximum degree.

step, add the edge which maximally reduces the total number
of topic-connected components.
The GM algorithm does not work well for the MinMaxTCO problem: The approximation ratio on the maximum
degree obtained by the GM algorithm may be as bad as Θ(n),
as we show in the lemma below.
Lemma 1. The GM algorithm can only guarantee an approximation ratio of at least Θ(n) for the MinMax-TCO problem,
where n is number of nodes in the pub/sub system.
Proof: Consider the example where we have n nodes
v1 , v2 , ..., vn , and n topics T = {t1 , t2 , ..., tn }. Node v1 is
interested in all topics in T and each vi is interested in
T − ti , 2 ≤ i ≤ n. The GM algorithm would produce an
overlay network with max degree n − 1. The overlay network
in Figure 1 (b), where E = {(v1 , vi )|1 < i ≤ n}, would
result from the GM algorithm – the maximum degree of
this overlay network is n − 1. The optimal solution for the
MinMax-TCO on the same configuration for the nodes V is


the overlay
 network G(V, E ), where E = {(vi , vi+1 )|3 ≤
i < n} {(vn , v2 ), (v1 , v2 ), (v1 , v3 )} (see Figure 1 (a)), which
has has maximum degree 2. Hence the approximation ratio of
the GM algorithm can be as large as (n − 1)/2 = Θ(n).
IV. OVERLAY D ESIGN A LGORITHM

III. M INAV-TCO PROBLEM AND G REEDY M ERGE (GM)
A LGORITHM
The MinAv-TCO problem was introduced by Chockler et.
al. [11] in which they aim at minimizing the average node
degree. In this section we present a formal definition of the
MinAv-TCO problem and outline the main techniques in the
corresponding Greedy Merge (GM) algorithm, which will be
useful for our approach to MinMax-TCO. We start with a
formal definition of the MinAv-TCO problem.
Minimum Topic Connected Overlay Problem (MinAv-TCO):
Given a collection of nodes V , a set of topics T , and a node
interest assignment I, connect the nodes in V into a topicconnected overlay network G which has the least possible total
number of edges (and hence the least possible average node
degree).
The Greedy Merge (GM) Algorithm [11]: Initially we a have
the set of nodes V and no edges between the nodes. At each

In this section we present our overlay design algorithm
(MinMax-ODA) for the MinMax-TCO problem. MinMaxODA starts with the overlay network G(V, ∅). At each iteration
of MinMax-ODA, a maximum weight edge — where the
weight of an edge (u, v) is given by the reduction on the
number of topic-connected components which would result
from the addition of (u, v) to the current overlay network
— among the ones which minimally increases maximum
degree of the current graph is added to edge set of the
overlay network. Let N C(V, E) denote total number of topic
connected components in the overlay network given by (V, E).
Steps 1-6 of MinMax-ODA build an initial weighted graph
G (V, E  , w) on V , where E  = V × V and w({u, v}) is
equal to the amount of decrease in the number of topicconnected components resulting from the addition of the edge
(u, v) to the current overlay network (represented by the edges
in OverlayEdges). Initially, this amount will be equal to the
number of topics that nodes u and v have in common.

884

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

Algorithm 1 Minimum Maximum Degree Overlay Design
Algorithm (MinMax-ODA)
1: OverlayEdges ← ∅
2: V ← Set of all nodes
3: G (V, E  ) ← Complete graph on V
4: for {u, v} ∈ E  do
5:
w{u, v} ← Number of topics that both of nodes u and
v have
6: end for
7: while G(V,OverlayEdges) is not topic-connected do
8:
Find maximum-weighted edge e on G (V, E  , w)
among the ones which increase the maximum degree
of G(V,OverlayEdges) minimally.

9:
OverlayEdges = OverlayEdges e
10:
E ← E − e
11:
for {u, v} ∈ E  do
12:
w{u, v}
←
NC(V ,
OverlayEdges)

NC(V ,OverlayEdges {u, v} )
13:
end for
14: end while

While at a first glance MinMax-ODA may look very similar
to GM, it actually can be shown to work in phases, where
in each phase a collection of edges that form matching of
the nodes in the pub/sub system that connects close to the
maximum number of connected components for the different
topics is selected, as we explain below. Such a matching
decomposition of the selected edges, crucial for our approximation ratio analysis, was not possible for GM.
At each iteration of the while loop, a maximum weight
edge among the ones which increase the maximum degree
of the current graph minimally is added to the set of overlay
edges. Note that the addition of an edge to OverlayEdges can
either increase the maximum degree by 1 or not increase it
at all. For ease of explanation, assume that we have an even
number of nodes in the pub/sub system. Since we start with
all nodes having equal degree (equal to 0), MinMax-ODA
will first select a set of edges that increases the degree of
every node to 1 (i.e., a matching), and then a set of edges
(another matching) that increases the degree of each node
to 2, etc. Note that some of these edges may have weight
0 (i.e., they do not really contribute to the construction of
the topic-connected components), but they will not affect the
final solution obtained (the 0-weight edges will be discarded
at the end of the algorithm). The crux in the analysis of this
algorithm is to show that each of these matchings will reduce
the number of connected components by a “large” amount.
Before we proceed in proving the approximation ratio on
the maximum degree guaranteed by MaxMin-ODA, we prove
that the algorithm terminates in O(|V |4 |T |) time.
Lemma 2. The MinMax-ODA algorithm terminates within
O(|V |2 ) iterations on the while loop.
Proof: At each iteration of the while loop, at least one
edge is added to the current overlay network. Hence the

algorithm will terminate in at most O(|V |2 ) iterations.
Lemma 3. The running time of MinMax-ODA is O(|V |4 |T |).
Proof: The weight initialization takes O(|V |2 |T |) time.
Updating the weight of each of the remaining edges takes
O(1) time( [11], Lemma 6.4). Finding the edge with max
weight will take at most O(|V |2 ) time. Since total weight
of the edges is O(|V |2 |T |) at the beginning and greater than
0 at the end, MinMax-ODA takes O(|V |2 |T |) ∗ O(|V |2 ) =
O(|V |4 |T |) time.
V. A PPROXIMATION R ATIO
In this section, we will prove that our overlay design
algorithm (MinMax-ODA) approximates the MinMax-TCO
problem within a logarithmic factor.
Theorem 1. The overlay network output by MinMax-ODA
has

maximum node degree within a factor of O(log( v∈V |{t ∈
T |I(v, t) = 1}|)) from the minimum possible maximum node
degree for any topic-connected overlay network on V .
Proof: At a high level, the proof follows the general
lines as the proof of the logarithmic approximation ratio for
the classic set cover problem (which was also the basis for
the approximation ratio proof of the GM algorithm for the
MinAv-TCO problem [11]). However, before we can apply
the set cover framework, we first need to carefully show
that MinMax-ODA works as if we had many applications
of a greedy matching algorithm that aims at reducing the
number of connected components maximally and then relate
our network overlay construction to a matching decomposition
of an optimal (i.e., a minimum maximum degree) overlay
network.
Assume we have an instance of the MinMax-TCO problem
and that G(V, Eopt ) is an optimum solution for this instance
with maximum degree dopt . We will use the following wellknown result in graph theory for the proof.
Lemma 4. Given a graph G(V, E) with maximum degree d,
we can divide the edge set E into k = d + 1 matchings Mi ,
1 ≤ i ≤ k.
Proof: We can color the edges of any graph with d + 1
colors such that any adjacent edge will have different color.
This is Vizing’s Edge Coloring Theorem (Theorem 5.3.2 in
[13]). Since each coloring class is a matching, we can divide
the edge set into d + 1 matchings.
Using the lemma above, we can divide the edge set Eopt
of the optimum solution into k = dopt + 1 matchings Mi ,
1 ≤ i ≤ k.
At the beginning of the algorithm,
 the total number of
connected components is Cstart = v∈V |{t ∈ T |I(v, t) =
1}| and at the end Cend = |{t|t ∈ T and ∃v ∈ V such
that I(v, t) = 1}|. Note that since we count the connected
components for each topic separately, once we get down to
Cend components, there must exist exactly one component for
each active topic t (i.e., each t such that there exists some

885

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

v with I(v, t) = 1) — i.e., the overlay network is topicconnected.
For ease of explanation, assume that we have an even
number of nodes in the pub/sub system (if there are an odd
number of nodes, one can always add a “dummy” node which
is not subscribed to any topic, without affecting the final
solution; or we can handle small deviations from a perfect
matching decomposition in the analysis). At each iteration of
the while loop, a maximum weight edge among the ones which
increases the maximum degree of the current graph minimally
is added to the set of overlay edges. At start all nodes have
degree 0. After a number of iterations the edges added will
form a perfect matching and then the next edge added will
increase the max degree of the graph by 1.
Let Si be the edge set of the ith matching added to the set
by the algorithm MinMax-ODA, 1 ≤ i ≤ k. Let ni be total
th
number of connected components
 before
 we
 add i matching,
so n1 = Cstart . Let SAi = S1 S2 ... Si−1 be the union
of all matchings found before the algorithm starts adding the
i-th matching.
The following lemma proves that each matching Si chosen
by our algorithm decrease the current total number of connected components at least (1/3)- optimally.

yl ≥ xl . So,

Lemma 5. The matching Si reduces the total number of
connected components of G(V, SAi ) by at least 1/3 of any
optimal matching which reduces by maximum amount.

From the inequalities (3) and (6), we will have

Proof: Let P be the edge set of the matching which
reduces the total number of connected components of the
G(V, SAi ) by the maximum amount, which we denote by
c. Let Q = {e1 , e2 , . . . , ej } be the edge set of the matching
Si that our algorithm finds. Let el = ul vl for 1 ≤ l ≤ j.
For ea and eb , if a < b, then ea is found before eb by
our algorithm. Let Q reduce the total number of connected

components of the
 G(V, SAi ) by c . Let G0 = G(V, SAi )
and Gl = Gl−1 el , for 1 ≤ l ≤ j. Let el reduce the total
number of connected components of Gl−1 by yl . Then,
c =


1≤l≤j

yl

ya ≥ yb , for 1 ≤ a ≤ b ≤ j

(1)
(2)

Let Xl be the set of edges in P which are incident to ul
or vl , 1 ≤ l ≤ j, but not incident to ul or vl 1 ≤ l ≤ l − 1.
Thus, Xl will have zero or one or two edges for 1 ≤ l ≤ j.
Let P0 = P and Pl = Pl−1 − Xl for 1 ≤ l ≤ j. Since Q is
a maximal matching, Pj = ∅. Let Xl reduce the total number
of connected components of Gl−1 by xl for 1 ≤ l ≤ j. Let Pl
reduce the total number of connected components of Gl by cl
for 0 ≤ l ≤ j.
If Xl has two edges, then our algorithm did not choose
one of these two edges at that step and choose el instead,
0 ≤ l ≤ j. Since our algorithm greedily choose the edges,
el reduces the total number of connected components of
Gl−1 by at least as much as each of the edges in Xl . Hence,
yl ≥ xl /2. Similarly, if Xl has one or zero edges, then

yl ≥

xl
2 ,

1≤l≤j



⇒

1≤l≤j

yl ≥

1
2


1≤l≤j

xl

(3)


Since Pl+1 = Pl − Xl+1 and Gl+1 = Gl el+1 ,
0 ≤ l ≤ j − 1, the amount that Pl reduces the total number
of connected components of Gl is smaller than sum of the
amount that Pl+1 reduces the total number of connected
components of Gl+1 and the amount that el+1 reduces the
total number of connected components of Gl and the amount
Xl+1 reduces the total number of connected components of
Gl . Hence,
c0 = c, cj = 0

(4)

cl+1 ≥ cl − (xl+1 + yl+1 ) for 0 ≤ l ≤ j − 1

(5)

If we add all the inequalities (4) and (5), we will have

1≤l≤j

3

xl +


1≤l≤j


1≤l≤j

yl ≥ c

yl ≥ c

(6)

(7)

From the inequalities (1) and (7), we will have
c ≥ c/3
Before MinMax-ODA starts adding the ith matching, we
have ni components and we know that if we add all the
k = dopt + 1 matchings Mj − SAi , 1 ≤ j ≤ k, to the
current solution, the total number of connected components
will be reduced to Cend . Therefore, there exists a matching
Mj − SAi which decreases the total number of connected
components by at least (ni − Cend )/k. Since our algorithm
always finds at least a (1/3)-optimal matching (Lemma 5),
the matching Si that our algorithm uses must decrease the
total number of connected components at that time by at least
(1/3) of this amount. Therefore,
ni − ni+1 ≥ (ni − Cend )/(3k)
⇒ ni+1 − Cend ≤ (1 − 1/(3k))(ni − Cend ).
Hence, the number of iterations for our algorithm MinMaxODA is less than or equal to the smallest m which satisfies
1 > (n1 − Cend )(1 − 1/(3k))m
⇒ m ≤ 3kln(Cstart − Cend )
⇒ m ≤ 3kln(Cstart )
⇒ m ≤ 3(dopt + 1) ∗ ln(Cstart )

886

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

VI. H ARDNESS OF M IN M AX -TCO P ROBLEM
In this section, we will show that no polynomial time
algorithm can approximate the MinMax-TCO problem within
a constant factor.
Theorem 2. There exists no polynomial time algorithm that
approximates the MinMax-TCO problem within a constant
factor unless P=NP.
Proof: The proof follows the proof of Theorem 5.3
in [11]. In fact, a more careful observation of the proof of
Theorem 5.3 in [11] shows that the proof basically shows the
hardness of approximation of the maximum degree (in [11],
the authors consider the problem of approximating the average
degree). For the sake of completeness, we present here the
sketch of the proof of Theorem 5.3 [11] when directly applied
to maximum degree. We first define the single node version
of the MinMax-TCO problem.
Single node version of the MinMax-TCO problem(SNMinMax-TCO): Given V , T , I, a node v ∈ V , connect
the nodes in V into an overlay network G which has least
possible degree for node v and G is topic-connected.

of Lemma 5.2 [11], the corresponding SN − M inM ax −
T CO(V, T, I, v) problem has a solution with degree sopt .
So, the algorithm B will find a solution to the corresponding
SN − M inM ax − T CO(V, T, I, v) problem with degree at
most c ∗ sopt . With using this solution, we can construct a
solution to minimum set cover problem with number of sets
at most c ∗ sopt as in Lemma 5.2 [11]. Thus, we have a
constant approximation algorithm C for the minimum set cover
problem.
Since it is trivial to show that MinMax-TCO is in NP, it
follows
Corollary 1. The MinMax-TCO problem is NP-complete.
VII. E XPERIMENTAL R ESULTS
The GM algorithm [12] and our MinMax-ODA algorithm
are implemented in Java. These two algorithms are compared according to maximum degree and average degree in
the resulting overlay graphs. Experimental results show that
MinMax-ODA improves the maximum degree of the overlay
network drastically at the cost of a small increase on the
average degree.
A. Maximum Node Degree

We can prove that SN-MinMax-TCO is NP-hard by reducing the minimum set cover problem to this problem. Then
we show how to reduce the SN-MinMax-TCO problem to the
MinMax-TCO problem, thus showing that the MinMax-TCO
problem is NP-hard.
Now, we are ready to prove our inapproximability result. We
will use the same reduction as in the proof of Theorem 5.3 in
[11]. Assume that there is an algorithm A which approximates
MinMax-TCO problem within a constant factor. We will
show we can use this algorithm A to find a constant factor
approximation to the minimum set cover problem, which is
known to be impossible unless P = NP.
Given an instance of SN − M inM ax − T CO(V, T, I, v)
problem, construct the corresponding instance of M inM ax −
T CO(V  , T  , I  ) problem as in the construction of proof of
Lemma 5.1 [11]. Let dopt denote the maximum degree of
an optimal solution for this instance of SN − M inM ax −
T CO(V, T, I, v) problem. As shown in proof of Lemma 5.1
[11], the corresponding M inM ax − T CO(V  , T  , I  ) problem also has a solution with degree dopt . So, the algorithm
A will find a solution to the corresponding M inM ax −
T CO(V  , T  , I  ) problem with degree at most c ∗ dopt . With
using this solution, we can construct a solution to SN −
M inM ax − T CO(V, T, I, v) problem with degree at most
c∗dopt as in Lemma 5.1 [11]. Thus, we have a constant approximation algorithm B for SN − M inM ax − T CO(V, T, I, v)
problem.
Given an instance of the minimum set cover problem
(U, S), construct the corresponding SN − M inM ax −
T CO(V, T, I, v) problem same as in the construction of proof
of Lemma 5.2 [11]. Denote sopt an optimal solution for this
instance of minimum set cover problem. As shown in proof

For these experiments, the number of nodes varies between
100 to 1000. In the first experiment (Figure 2(a)), the number
of topics is 100 and in the second experiment (Figure 2(b))
the number of topics is 200. We fixed number of subscriptions
to s = 10. Each node is interested in each topic uniformly
at random. This experimental setting is similar to previous
studies [12].
Figure 2(a) is a comparison of GM and MinMax-ODA
algorithm according to the maximum degree. The maximum
degree of the graph decreases for MinMax-ODA algorithm
when the number of nodes increases since MinMax-ODA
algorithm can find edges with higher correlation as the number
of nodes increases. Interestingly, the maximum degree of the
graph increases for the GM algorithm as the number of nodes
increases. Basically as the number of nodes increase, the GM
algorithm will assign more edges to same the nodes since
now we have more nodes with higher correlation for each
node. When we compare the results of GM and MinMaxODA algorithm, MinMax-ODA improves GM by factor 3 on
average (Figure 2(a)).
The same results are valid for Figure 2(b). When we compare Figure 2(a) and Figure 2(b), max node degree increases
slightly for both GM and MinMax-ODA since edges will have
less correlation when we increase the number of topics.
B. Average Node Degree
Experimental setting is same as previous subsection. Figure
3(a) is comparison of GM and MinMax-ODA algorithm according to average degree. The average degree of the graph
decreases for both GM and MinMax-ODA algorithms when
the number of nodes increases since algorithms can find edges
with higher correlation when the number of nodes increases.

887

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

(a)

(a)

(b)

(b)

Fig. 2. Maximum node degree for GM and MinMax-ODA (a) Number of
topics is 100 (b) Number of topics is 200

Fig. 3. Average node degree for GM and MinMax-ODA (a) Number of
topics is 100 (b) Number of topics is 200

When we compare the results of GM and MinMax-ODA
algorithm, GM is slightly better than MinMax-ODA, 7% on
average, (Figure 3(a)). Similar results are valid for Figure 3(b).

results of GM and MinMax-ODA algorithm, GM is slightly
better than MinMax-ODA, %10 on average, (Figure 4(b)).

C. Subscription Size
In these experiment, the number of nodes and the number of topics are fixed to 100. The subscription size varies
between 10 to 50. Each node is interested in each topic
uniformly randomly. Figure 4(a) is the comparison of GM and
MinMax-ODA algorithm according to the maximum degree.
The maximum degree of the overlay network decreases for
the MinMax-ODA algorithm as the subscription size increases
since the MinMax-ODA algorithm can find edges with higher
correlation when the subscription size increases. Interestingly,
the maximum degree of the overlay network increases for
GM algorithm as the subscription size increases. When the
subscription size increases, GM algorithm will assign more
edges to the same nodes since now we have more nodes
with higher correlation for each node. When we compare the
results of GM and MinMax-ODA algorithms, MinMax-ODA
improves GM by factor a 4 on average (Figure 2(a)).
Figure 4(b) is comparison of GM and MinMax-ODA algorithm according to the average degree. The average degree of
the overlay network decreases for both GM and MinMax-ODA
algorithms when subscription size increases since algorithms
can find edges with more correlation. When we compare the

VIII. C ONSTRUCTING C ONSTANT D IAMETER OVERLAYS
FOR P UBLISH -S UBSCRIBE
In this section, we study a new optimization problem
that constructs a constant diameter overlay network for
publish/subscribe communication with many topics. We
present an overlay network construction heuristic that
guarantees constant diameter and topic-connectivity which
are most important factors for efficient routing. The formal
problem is as follows:
Constant Diameter Topic-Connected Overlay (CD-TCO)
Problem: Given a collection of nodes V , a set of topics T ,
and the node interest assignment I, connect the nodes in V
into a topic-connected overlay network G which has least
possible average degree and constant diameter.
This problem aims at minimizing the average degree, as
does the MinAv-TCO problem introduced by Chockler et. al.
[11], with the additional requirement on diameter. We present
a heuristic for this problem and validate our heuristic via
experimental results.
We first show that the GM algorithm does not work well
for the CD-TCO problem. Second, we present our 2-diameter

888

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

neighbors with non-empty interest intersection is chosen. The
number of neighbors of a node u is equal to
nu = |{v ∈ V |∃t ∈ T, Int(v, t) = Int(u, t) = 1}|
We then put an edge between this node and each of its
neighbors, and remove all the topics in this node’s interest
assignment from the set of topics.

(a)

Algorithm 2 Constant Diameter Overlay Design Algorithm
(CD-ODA)
1: T ← Set of all topics
2: while T is not empty do
3:
For each node u, calculate number of nodes v such that
there exists a topic t in T and Int(u, t) = Int(v, t) = 1.
Denote this number by nu .
4:
Find node u with maximum nu .
5:
Put an edge between u and all nodes v such that there
exists a topic t ∈ T and Int(u, t) = Int(v, t) = 1.
6:
Remove all topics t from T such that Int(u, t) = 1.
7: end while

C. Analysis of Algorithms
Lemma 7. CD-ODA terminates within O(|V |2 ∗ |T |) time.
(b)
Fig. 4. Average and maximum node degree for different subscription size
(Number of nodes and number of topics is 100)

Lemma 8. CD-ODA generates a 2-diameter overlay for each
topic.
Proof: Since the algorithm generates a star for each topic,
each topic overlay network will have diameter 2.

algorithm and its performance evaluation.

D. Conjectures

A. The GM Algorithm and the CD-TCO Problem

Conjecture 1. CD-ODA approximates the constant diameter
overlay design problem within a logarithmic factor.

The GM algorithm does not work well for the CD-TCO
problem: The diameter obtained by the GM algorithm may be
as bad as Θ(n), as we show in the lemma below.
Lemma 6. The GM algorithm can have diameter of Θ(n),
where n is number of nodes in the pub/sub system.
Proof: Consider the example where we have n nodes
v1 , v2 , ..., vn , one topic t and every node is interested in t.
There exists many orderings of the edges of G for which
the GM algorithm would produce an overlay network with
diameter n − 1. For example, the overlay network , G(V, E),
where E = {(vi , vi+1 )|1 ≤ i < n} , can result from the GM
algorithm – the diameter of this overlay network is n − 1.
Another solution for the CD-TCO on the same configuration
for the nodes V is the overlay network G(V, E  ), where
E  = {(v1 , vi )|1 < i ≤ n} , which has diameter 2.
B. Constant Diameter Overlay Design Algorithm (CD-ODA)
We will present a greedy algorithm (CD-ODA) for the
problem. Our algorithm generates a star for each topic and
hence each topic-connected component will have diameter 2.
CD-ODA starts with the overlay network G(V, ∅). At each
iteration of the CD-ODA, a node which has maximum number

Our first intuition is that if there exists a k-edge overlay
network with constant diameter for a graph G, then there exists
a constant c such that there exists a c ∗ k-edge overlay with
diameter 2 for a graph G. This step will make the reduction
from constant diameter overlay to 2-diameter overlay.
For 2-diameter overlay, the most efficient graph for each
topic alone will have a star structure, since a star has diameter
2 and optimal number of edges (n-1 edges). Combining these
two steps, we have Conjecture 1.
Conjecture 2. There exists no polynomial time algorithm that
approximates constant diameter overlay design problem with
a constant factor unless P=NP.
Our intuition for Conjecture 2 is that the set cover problem
can be reduced to this problem as for the MinMax-TCO and
Min-TCO problems.
E. Experimental Results
The GM algorithm [12] and CD-ODA are implemented in
Java. These two algorithms are compared according to the
average degree in the resulting graph. Experimental results
show that CD-ODA improve the diameter of the overlay

889

This full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the IEEE INFOCOM 2009 proceedings.

algorithm which achieves a “good” trade-off between low
diameter and low node degree. This basically amounts to a
bicriteria optimization problem and we have to be able to
“quantify” the relative importance of optimizing over these
two parameters (e.g., in the CD-ODA algorithm we restrict
our attention to networks of diameter 2, while aiming at
maintaining the average degree low).
Two other important lines for future work would be to
design efficient distributed algorithms for the MinMax-TCO
problem, and to look at this problem under the line of a
dynamic configuration of the node set V and the interest
assignment I.
Fig. 5.

R EFERENCES

Average node degree for GM and CD-ODA
[1]

network drastically at the cost of a small increase on the
average degree. The diameter is always 2 for CD-ODA and it
may be θ(n) for GM algorithm. When we compare the results
of GM and CD-ODA according to average degree, CD-ODA
requires at most 2.3 times more edges.
In the experiment, the number of nodes varies between 100
to 1000. The number of topics is 100. We fixed number of
subscriptions to s = 10. Each node is interested in each topic
uniformly at random. This experimental setting is similar to
previous studies [11].
Figure 5 is a comparison of GM and CD-ODA according to
the average degree. The average degree of the graph decreases
for GM algorithm when the number of nodes increases since
GM algorithm can find edges with higher correlation as the
number of nodes increases. The average degree of the graph
slightly decreases for our algorithms. When we compare the
results of GM and CD-ODA, our algorithms requires at most
2.3 times more edges than GM (Figure 5).
IX. C ONCLUSIONS
In this paper, we study a new optimization problem
(MinMax-TCO) that constructs a practical and scalable overlay network for publish/subscribe communication with many
topics. We present a topic-connected overlay network design
algorithm (MinMax-ODA) which approximates the MinMaxTCO problem within a logarithmic factor. We also show that
the approximation factor of MinMax-ODA is almost tight,
since no constant-approximation polynomial-time algorithm
can exist for the MinMax-TCO problem (unless P=NP).
Our experimental results validate our formal analysis of the
MinMax-ODA algorithm, showing that the maximum degree
obtained by our algorithm clearly outperforms the maximum
degree obtained when GM is used.
We present a heuristic for constructing constant diameter
overlay networks. Our experimental results show that the
diameter obtained by our heuristics outperforms the diameter
obtained when GM is used while only increasing the average
degree by a factor of 2.3.
As future work, we would like to build upon our CDODA algorithm, by formally and experimentally evaluating
the hardness of obtaining a topic-connected overlay design

Oracle9i Application Developers Guide Advanced Queuing, Oracle,
Redwood Shores, CA.
[2] E. Anceaume, M. Gradinariu, A. K. Datta, G. Simon, and A. Virgillito,
A semantic overlay for self- peer-to-peer publish/subscribe, In ICDCS,
2006.
[3] S. Baehni, P. T. Eugster, and E. Guerraoui, Data-aware multicast. In
DSN, 2004.
[4] R. Baldoni, R. Beraldi, V. Quema, L. Querzoni, and S. T. Piergiovanni,
TERA: Topic-based Event Routing for Peer-to-Peer Architectures, 1st
International Conference on Distributed Event-Based Systems (DEBS).
ACM, 6 2007.
[5] R. Baldoni, R. Beraldi, L. Querzoni, and A. Virgillito, Efficient publish/subscribe through a self-organizing broker overlay and its application
to SIENA, The Computer Journal, 2007.
[6] S. Banerjee, B. Bhattacharjee, and C. Kommareddy, Scalable application
layer multicast, SIGCOMM Comput. Commun. Rev, 32(4):205-217,
2002.
[7] S. Bhola, R. Strom, S. Bagchi, Y. Zhao, and J. Auerbach, Exactly-once
delivery in a content-based publish-subscribe system. In DSN, 2002.
[8] A. Carzaniga, M. J. Rutherford, and A. L. Wolf, A routing scheme for
content-based networking., IEEE INFOCOM 2004, Hon Kong, China,
Mar. 2004.
[9] M. Castro, P. Druschel, A. M. Kermarrec, and A. Rowstron, SCRIBE: a
large-scale and decentralized application-level multicast infrastructure,
IEEE J. Selected Areas in Comm. (JSAC), 20(8):14891499, 2002.
[10] R. Chand and P. Felber, Semantic peer-to-peer overlays for publish/subscribe networks, In Euro-Par 2005 Parallel Processing, Lecture
Notes in Computer Science, volume 3648, pages 11941204. Springer
Verlag, 2005.
[11] G. Chockler, R. Melamed, Y. Tock and R. Vitenberg, Constructing
scalable overlays for pub-sub with many topics, Proc. of the 26th ACM
Symp. on Principles of Distributed Computing (PODC), 2007, pp. 109–
118.
[12] G. Chockler, R. Melamed, Y. Tock, and R. Vitenberg, SpiderCast: A
Scalable Interest-Aware Overlay for Topic-Based Pub/Sub Communication, 1st International Conference on Distributed Event-Based Systems
(DEBS). ACM, 6 2007.
[13] R. Diestel, Graph Theory, Springer-Verlag, 2nd edition, New York,
2000.
[14] P. T. Eugster, P. A. Felber, R. Guerraoui, and A. M. Kermarrec. The many
faces of publish/subscribe. ACM Computing Surveys, 35(2):114131,
2003.
[15] R. Guerraoui, S. Handurukande, and A. M. Kermarrec, Gossip: a gossipbased structured overlay network for efficient content-based filtering,
Technical Report IC/2004/95, EPFL, Lausanne, 2004.
[16] B. Korte, J. Vygen, Combinatorial Optimization Theory and Algorithms,
Springer-Verlag, 2nd edition, 2000.
[17] R. Levis, Advanced Massaging Applications with MSMQ and MQSeries.
QUE, 1999.
[18] H. Liu, V. Ramasubramanian, and E. G. Sirer. Client behavior and feed
characteristics of rss, a publish-subscribe system for web micronews. In
Internet Measurement Conference (IMC), Berkeley, California, October
2005.
[19] S. Voulgaris, E. Riviere, A. M. Kermarrec, and M. van Steen, Sub-2sub: Self-organizing content-based publish subscribe for dynamic large
scale collaborative networks, In IPTPS, 2006.

890

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

1331

Minimum Maximum-Degree Publish–Subscribe
Overlay Network Design
Melih Onus and Andréa W. Richa

Abstract—Designing an overlay network for publish/subscribe
communication in a system where nodes may subscribe to many
different topics of interest is of fundamental importance. For scalability and efficiency, it is important to keep the degree of the nodes
in the publish/subscribe system low. It is only natural then to formalize the following problem: Given a collection of nodes and their
topic subscriptions, connect the nodes into a graph that has least
possible maximum degree in such a way that for each topic , the
graph induced by the nodes interested in is connected. We present
the first polynomial-time logarithmic approximation algorithm for
this problem and prove an almost tight lower bound on the approximation ratio. Our experimental results show that our algorithm drastically improves the maximum degree of publish/subscribe overlay systems. We also propose a variation of the problem
by enforcing that each topic-connected overlay network be of constant diameter while keeping the average degree low. We present
three heuristics for this problem that guarantee that each topicconnected overlay network will be of diameter 2 and that aim at
keeping the overall average node degree low. Our experimental results validate our algorithms, showing that our algorithms are able
to achieve very low diameter without increasing the average degree
by much.
Index Terms—Communications
computing.

technology,

peer-to-peer

I. INTRODUCTION
N THE publish/subscribe (pub/sub) communication paradigm, publishers and subscribers interact in a decoupled
fashion. Publishers publish their messages through logical channels, and subscribers receive the messages they are interested in
by subscribing to the appropriate services, which deliver messages through these channels.
A pub/sub system may be topic-based if messages are published to “topics,” where each topic is uniquely associated with
a logical channel. Subscribers in a topic-based system will receive all messages published to the topics to which they subscribe. The publisher is responsible for defining the classes of

I

Manuscript received February 23, 2010; revised October 05, 2010; accepted
January 15, 2011; approved by IEEE/ACM TRANSACTIONS ON NETWORKING
Editor D. Rubenstein. Date of publication June 23, 2011; date of current version
October 14, 2011. This work was supported in part by the NSF under Awards
CCF-0830791 and CCF-0830704. A preliminary version containing some of the
results in this paper appeared at the IEEE Conference on Computer Communications (INFOCOM) Rio de Janeiro, Brazil, April 19–25, 2009, and the IEEE
International Conference on Distributed Computing Systems (ICDCS) Genoa,
Italy, June 21–25, 2010.
M. Onus is with the Department of Computer Engineering, Bilkent University, Ankara 06560, Turkey (e-mail: onus@cs.bilkent.edu.tr).
A. W. Richa is with the Computer Science and Engineering Program, School
of Computing Informatics and Decision Systems Engineering, Arizona State
University, Tempe, AZ 85281 USA (e-mail: aricha@asu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TNET.2011.2144999

messages to which subscribers can subscribe. In a content-based
system, messages are only delivered to a subscriber if the attributes of those messages match constraints defined by the subscriber; each logical channel is characterized by a subset of
these attributes. The subscriber is responsible for classifying the
messages.
Pub/sub communication systems are scalable and simple to
implement (see, e.g., [1], [3]–[5], [7]–[11], [16], [18], [23], [25],
[27], [30], and [31]). Hence, there are many applications that
are built on top of such systems, most notably a plethora of Internet-based applications, such as stock-market monitoring engines, RSS feeds [19], [30], online gaming, and several others.
Publish/subscribe schemes have been implemented by many industrial strength solutions (e.g., Altherr et al. [2], Talarian Corporation [28], Skeen [26], Tibco [29]). For a survey on pub/sub
systems, see [15].
In this paper, we design a (peer-to-peer) overlay network for
each pub/sub topic such that, for each topic , the subgraph induced by the nodes interested in is connected. This translates
into a decentralized topic-based pub/sub system in the sense
that any given topic-based overlay network will be connected,
and thus nodes subscribed to a given topic do not need to rely
on other nodes (agents) for forwarding their messages. Such an
overlay network is called topic-connected.
We can evaluate the complexity of a pub/sub overlay network in terms of the cost of topic-based broadcasts on the network. As in many other systems, a space–time tradeoff exists:
On one hand, one would like the total time taken by the broadcast (which directly depends on the diameter of each topic-based
subnetwork) to be as small as possible; on the other hand, for
memory and node bandwidth considerations, one would like to
keep the total degree of a node small. Those two measures are
often conflicting. For example, take the simple scenario where
all nodes are subscribed to the same topic: A star overlay would
result in the best possible diameter, but worst possible degree,
for the nodes. Even if we were to maintain a balanced structure
(e.g., a balanced binary tree) for each topic, it is not clear how
to achieve that without letting the node degrees grow as large as
the sizes of the node subscription sets.
Some of the current solutions adopted in practice actually fail
at maintaining both the diameter and the node degrees as low.
A naive, albeit popular, solution to topic-connected overlay network design is to construct a cycle (or a tree or any other separate overlay structure) connecting all nodes interested in a topic
independently for each given topic [31]. This construction may
result in a network with node degrees proportional to the nodes’
subscription sizes, whereas a more careful construction, taking
into account the correlations among the node subscription sets,

1063-6692/$26.00 © 2011 IEEE

1332

might result in much smaller node degrees (and total number of
edges).
Low node degrees are desirable in practice for scalability and
also due to bandwidth constraints. Nodes with a high number
of adjacent links will have to manage all these links (e.g., monitor the availability of its neighbors, incurring in heartbeats and
keep-alive state costs, and connection state costs in TCP) and
the traffic going through each of the links without being able to
take great advantage of aggregating the traffic (by aggregating
traffic, we also reduce the number of packet headers, which can
be responsible for a significant portion of the traffic for small
messages). See [13] for further motivation.
The node degrees and number of edges required by a topicconnected overlay network will be low if the node subscriptions
are well correlated. In this case, by connecting two nodes with
many coincident topics, one can satisfy the connectivity of many
topics for those two nodes with just one edge. Several recent
empirical studies suggest that correlated workloads are indeed
common in practice [19], [30].
In this paper, we first consider the problem of devising topicbased pub/sub overlay networks with low node degrees. More
specifically, we consider the following problem.
Minimum Maximum-Degree Topic-Connected Overlay
(MinMax-TCO) Problem: Given a collection of nodes , a
set of topics , and a node interest assignment , connect the
nodes in into a topic-connected overlay network that has
the least possible maximum degree.
We present a logarithmic approximation algorithm for this
problem. We also show that no polynomial-time algorithm can
asymptotically approximate MinMax-TCO problem within less
), so our approximathan a logarithmic factor (unless
tion guarantees are tight. We further validate our algorithm with
experimental results.
We also propose a variation of the MinMax-TCO problem
by enforcing that each topic-connected overlay network be of
constant diameter while keeping the average degree low (see
MinAv-TCO problem defined in the next section). We present
three heuristics for this problem that guarantee that each topic’s
induced overlay subnetwork will be of diameter 2 and that aims
at keeping the average node degree of the overall topic-connected overlay network low. We validate these algorithms
through experimental results.
A. Related Work
The results in this paper are an improved and extended
version of our results in [20]. The major extensions in this work
are the following. We extended the hardness of approximability
proof (Theorem 2) to show that one cannot approximate the
MinMax-TCO problem within an asymptotically better than
logarithmic ratio of optimal (unless
), rather than
a constant ratio (as it appears in [20]). We introduced new
and more thorough simulation results—in particular, all of
the simulation results for zipf distributions (Section VII-D)
and the more extensive simulation analysis for the uniform
distribution (Sections VII-A–VII-C). We also significantly
extended Section VIII by introducing two improved heuristics (Sections VIII-D–VIII-G) and corresponding simulations

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

(Section VIII-J), which provide a much more thorough approach to the problem than the very preliminary results
presented in [20].
In [12], Chockler et al. introduced a closely related problem
to the MinMax-TCO problem, which we call MinAv-TCO.1 The
MinAv-TCO problem aims at minimizing the average degree
of the nodes rather than the maximum degree. They present an
algorithm, called GM, which achieves a logarithmic approximation on the minimum average degree of the overlay network. While minimizing the average degree is a step forward
toward improving the scalability and practicality of the pub/sub
system, their algorithm may still produce overlay networks of
very uneven node degrees, where the maximum degree may
be unnecessarily high. As we will show in Section III, their
,
algorithm may produce a network with maximum degree
while a topic-connected overlay network of constant degree exists for the same configuration of . Some of the high-level
ideas and proof techniques of [12] have their roots in techniques
used for the classical Set-Cover problem. We benefit from some
of the ideas in [12] and also build upon the constructions for
Set-Cover, extending and modifying them to be able to handle
the maximum degree case.
To the best of our knowledge, minimizing the maximum degree or the diameter in topic-connected pub/sub
overlay network design had not been directly addressed
prior to this work [20], [21]. The overlay networks resulting
from [3], [6], [11] are not required to be topic-connected. In
[5], [10], [13], and [31], topic-connected overlay networks are
constructed, but they make no attempt to minimize the average
or maximum node degree. The first to directly consider node
degrees when building topic-connected pub/sub systems were
Chockler et al. in [12], as we mentioned.
B. Our Contributions
Our main contribution in this paper is the formal design
and analysis of the topic-connected overlay design algorithm
(MinMax-ODA) that approximates the MinMax-TCO problem
within a logarithmic factor. The MinMax-ODA algorithm is
a greedy algorithm that relies on repeatedly using a greedy
approach for finding matchings that connect a large (close
to maximum) number of different connected components
that emerge for the given topics. We also show that no polynomial-time algorithm can approximate the MinMax-TCO
problem within asymptotically less than a logarithmic factor
(unless
), and so our MinMax-ODA algorithm is
almost tight. No previous algorithm with sublinear approximation guarantees on the maximum degree of a topic-connected
pub/sub overlay network was known prior to this work. Furthermore, we validate the performance of the MinMax-ODA
with experimental results.
In addition, we present three heuristics that build a topicbased pub/sub network such that each topic-connected component is guaranteed to be of constant diameter—more specifically
1In the original paper, this problem was called Min-TCO. Since it aims at
minimizing the average degree of the overlay network, and in order to avoid
confusion with the MinMax-TCO problem considered in this paper, we will
refer to the problem considered by Chockler et al. in [12] as MinAv-TCO in our
paper.

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

1333

of diameter 2—and where we aim at keeping the average degree
low. While we do not have a formal proof on any approximation guarantees on the average node degree, we present steps of a
possible formal proof, intuitions, and conjectures. Furthermore,
we validate the performance of the algorithms via experimental
results.
C. Structure of the Paper
In Section II, we present some definitions and restate
the formal problem definition. In Section III, we present
an outline of the related problem of minimizing the average node degree, namely the MinAv-TCO problem, and
the corresponding logarithmic approximation algorithm GM
proposed by Chockler et al. [12], since some of the ideas
presented will be useful for the MinMax-TCO problem.
Section IV presents our topic-connected overlay design algorithm MinMax-ODA, whose approximation ratio is proven in
Section V. In Section VI, we present the hardness of approximation results for MinMax-TCO. Experimental results are
presented in Section VII. Section VIII addresses the CD-TCO
problem and presents our algorithms for the same. We conclude
the paper, also presenting some future work, in Section IX.
II. PRELIMINARIES
We will start by presenting a set of basic definitions. Let
be the set of nodes, and be the set of topics. Let
.
. For a
The interest function is defined as
node
and topic
,
if and only if node
is subscribed to topic , and
otherwise. For a set of
is an undirected graph on
nodes , an overlay network
the node set with edge set
. For a topic
, let
.
We now look at the number of connected components for each
and an overlay network
,
topic. Given a topic
the number of topic-connected components of for topic is
equal to the number of connected components of the subgraph
of induced by . An overlay network is topic-connected
if and only if it has one topic-connected component for each
. The diameter of a topic is the length of the longest
topic
shortest path between the nodes subscribed to this topic. The
diameter of a graph is equal to the maximum topic diameter.
is equal
The degree of a node in an overlay network
to the total number of edges adjacent to in .
We are now ready to formally state the MinMax-TCO
problem.
Minimum Maximum Degree Topic-Connected Overlay
(MinMax-TCO) Problem: Given a collection of nodes , a set
of topics , and the node interest assignment , connect the
nodes in into a topic-connected overlay network that has
least possible maximum degree.
III. MINAV-TCO PROBLEM AND GREEDY MERGE (GM)
ALGORITHM
The MinAv-TCO problem was introduced by
Chockler et al. [12], in which they aim at minimizing
the average node degree. In this section, we present a formal
definition of the MinAv-TCO problem and reproduce the
Greedy Merge (GM) algorithm, which will be useful for our

Fig. 1. (a) Overlay with optimal max degree. (b) Overlay constructed by GM.

approach to MinMax-TCO. We start with a formal definition
of the MinAv-TCO problem.
Minimum Topic Connected Overlay Problem (MinAv-TCO):
Given a collection of nodes , a set of topics , and a node
interest assignment , connect the nodes in into a topic-conthat has the least possible total
nected overlay network
number of edges (and hence the least possible average node
degree).
The Greedy Merge (GM) Algorithm [12]: Initially we have a
and no edges between the nodes. At each
the set of nodes
step, add the edge that maximally reduces the total number of
topic-connected components.
The GM algorithm does not work well for the MinMax-TCO
problem. The approximation ratio on the maximum degree ob, as we show
tained by the GM algorithm may be as high as
in the following lemma.
Lemma 1: The GM algorithm can only guarantee an approxfor the MinMax-TCO problem, where
imation ratio of
is number of nodes in the pub/sub system.
nodes
Proof: Consider the example where we have
, and topics
. Node
is interested in all topics in , and each
is interested in
. The GM algorithm would produce
. The overlay
an overlay network with max degree
network in Fig. 1(b), where
,
would result from the GM algorithm—the maximum de. The optimal sogree of this overlay network is
lution for the MinMax-TCO on the same configuration
is the overlay network
, where
for the nodes
[see Fig. 1(a)], which has maximum degree 2. Hence, the
approximation ratio of the GM algorithm can be as large as
.
In the example above, there is a large discrepancy between
the diameters of the overlay network produced by GM and the
optimal solution for MinMax ODA. We now present another example where GM performs poorly with respect to the maximum
degree, but where the diameters of the two respective solutions
are the same.
nodes
Consider the example where we have
, and
topics
. Node is interested in all topics in ,
is interested in topics
, and each node
each node
is interested in the topic
. A star

1334

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

network with center node can result from the GM algorithm.
and the diameter will be
The maximum degree will be
2. The optimal solution is a network with node connected to
all nodes
, and each node
connected to all
. The maximum degree will be
, and
nodes
the diameter will be 2.
IV. OVERLAY DESIGN ALGORITHM
In this section, we present our overlay design algorithm,
MinMax-ODA, for the MinMax-TCO problem. MinMax-ODA
. At each iteration of
starts with the overlay network
MinMax-ODA, a maximum weight edge—where the weight
is given by the reduction on the number of
of an edge
topic-connected components that would result from the addition
to the current overlay network—among the ones that
of
minimally increase the maximum degree of the current graph
is added to the edge set of the overlay network. Let
denote the total number of topic connected components in the
.
overlay network given by
Algorithm 1: Minimum Maximum-Degree Overlay Design
Algorithm (MinMax-ODA)
1: OverlayEdges
2:
Set of all nodes
Complete graph on
3:
4: for
do
Number of topics that both of nodes and
5:
subscribe to
6: end for
7: while ( , OverlayEdges) is not topic-connected do
8:
Find maximum-weighted edge on
among the ones which increase the maximum degree
of ( , OverlayEdges) minimally.
9:
OverlayEdges OverlayEdges
10:
11:
for
do
NC( , OverlayEdges)
12:
NC( , OverlayEdges
)
13:
end for
14: end while
15: Discard all weight 0 edges from OverlayEdges.
Steps 1–6 of the MinMax-ODA build an initial weighted
on , where
and
graph
is equal to the amount of decrease in the number of topic-connected components resulting from the addition of the edge
to the current overlay network (represented by the edges in
OverlayEdges). Initially, this amount will be equal to the
number of topics that nodes and have in common.
The MinMax-ODA algorithm uses the GM algorithm with
an additional constraint added to the edge selection process in
the GM algorithm. We choose the maximum weighted edge
among the ones that increase the maximum degree minimally.
While at a first glance the MinMax-ODA algorithm may look
very similar to the GM algorithm, it actually can be shown to
work in phases. In each phase, a collection of edges that forms

a matching of the nodes in the pub/sub system and that also connects close to the maximum number of connected components
for the different topics is selected, as we will explain. Basically,
edges chosen by our algorithm form
we show that the first
edges chosen by our algorithm
a matching, and the next
forms a matching, etc.
At each iteration of the while loop, a maximum weight edge
among the ones that increase the maximum degree of the current
graph minimally is added to the set of overlay edges. Note that
the addition of an edge to OverlayEdges can either increase the
maximum degree by 1 or not increase it at all. For ease of explanation, assume for now that we have an even number of nodes in
the pub/sub system. Since we start with all nodes having equal
degree (equal to 0), the MinMax-ODA algorithm will first select a set of edges that increases the degree of every node to 1
(i.e., a matching), and then a set of edges (another matching)
that increases the degree of each node to 2, etc. Note that some
of these edges may have weight 0 (i.e., they do not really contribute to the construction of the topic-connected components),
but they will not affect the final solution obtained (the 0-weight
edges will be discarded at the end of the algorithm). The crux
in the analysis of this algorithm is to show that each of these
matchings will reduce the number of connected components by
a “large” amount.
Before we proceed in proving the approximation ratio on the
maximum degree guaranteed by MinMAx-ODA, we prove that
time.
the algorithm terminates in
Lemma 2: The MinMax-ODA algorithm terminates within
iterations on the while loop.
Proof: At each iteration of the while loop, at least one edge
is added to the current overlay network. Hence, the algorithm
iterations.
will terminate in at most
Lemma 3: The running time of the MinMax-ODA is
.
time.
Proof: The weight initialization takes
Updating the weight of each of the remaining edges takes
time [12, Lemma 6.4 ]. Finding the edge with max weight will
time. Since total weight of the edges is
take at most
at the beginning and greater than 0 at the end, the
MinMax-ODA takes
time.
V. APPROXIMATION RATIO
In this section, we will prove that our overlay design algorithm (MinMax-ODA) approximates the MinMax-TCO
problem within a logarithmic factor.
Theorem 1: The overlay network output by the
MinMax-ODA has maximum node degree within a factor of
from the minimum possible maximum node degree for any
topic-connected overlay network on
with topic interest
assignment .
Proof: At a high level, the proof follows the general
structure of the proof of the logarithmic approximation ratio
for the classic set cover problem (which was also the basis
for the approximation ratio proof of the GM algorithm for the
MinAv-TCO problem [12]). However, before we can apply
the set cover framework, we first need to carefully show that

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

the MinMax-ODA works as if we had many applications of a
greedy matching algorithm that aims at reducing the number of
connected components maximally and then relate our network
overlay construction to a matching decomposition of an optimal
(i.e., a minimum maximum degree) overlay network.
Assume we have an instance of the MinMax-TCO problem
is an optimum solution for this instance
and that
with maximum degree
. We will use the following wellknown result in graph theory for the proof.
with maximum degree
Lemma 4: Given a graph
, we can partition the edge set into
matchings
.
Proof: We can color the edges of any graph with
colors such that any two adjacent edges will have different
colors by Vizing’s Edge Coloring Theorem [14, Theorem 5.3.2].
Since each coloring class is a matching, we can divide the edge
matchings.
set into
of the
Using the lemma above, we can divide the edge set
optimum solution into
matchings
.
At the beginning of the algorithm, the total number of connected components is
(i.e., there exists a singleton component for node and topic
such that is subscribed to ) and at the end
and
such that
(i.e., there exists exactly
one component for each topic in the system). Note that since
we count the connected components for each topic separately,
once we get down to
components, there must exist exactly
one component for each active topic (i.e., for each such that
there exists some with
), making the overlay network topic-connected.
For ease of explanation, assume that we have an even number
of nodes in the pub/sub system (if there are an odd number of
nodes, one can always add a “dummy” node that is not subscribed to any topic without affecting the final solution obtained
by the MinMax-ODA; or we can handle small deviations from a
perfect matching decomposition in the analysis). At each iteration of the while loop, a maximum weight edge among the ones
that increase the maximum degree of the current graph minimally is added to the set of overlay edges. At the start, all nodes
have degree 0. After a number of iterations, the edges added will
form a perfect matching, and then the next edge added will increase the max degree of the graph by 1.
Let be the edge set of the th matching added by the algorithm MinMax-ODA,
. Let be total number of connected components before we add th matching, so
.
Let
be the union of all matchings
found before the algorithm starts adding the th matching.
The following lemma proves that each matching
chosen
by our algorithm decreases the current total number of connected components by at least one third of the maximum possible amount. We say that a matching is optimal with respect to
the current configuration of if it reduces the number of connected components in by the maximum possible amount.
Lemma 5: The matching reduces the total number of connected components of
by at least 1/3 of the reduction on the number of connected components incurred by any
optimal matching.
Proof: Let be the edge set of an optimal matching for
, which reduces the number of connected compo-

1335

by . Let
, where
, for
, and where
implies that
is found before by our algorithm. Assume that reduces the
total number of connected components of the
by .
Let
and
, for
. We
use to denote the reduction in the number of connected com. Then
ponents incurred by the addition of to

nents of

(1)
for

(2)

be the set of edges in which are incident to
or
, but not incident to
or
.
Thus,
will have zero or one or two edges for
.
Let
and
for
. Since
is
a maximal matching,
. Let
reduce the total number
of connected components of
by for
. Let
reduce the total number of connected components of
by ,
for
.
has two edges, then our algorithm did not choose either
If
of these two edges at that step, but chose instead,
.
Since our algorithm greedily chooses the edges, reduces the
total number of connected components of
by at least as
much as each of the edges in . Hence,
. Similarly,
if
has one or zero edges, then
. Hence
Let

(3)
and
,
Since
the reduction by on the number of connected components of
incurred by the addition of is smaller than the sum of
,
the reduction on the number of connected components in
incurred by
, and
, the reduction on the number of connected components of
due to the addition of
, and
,
the reduction on the total number of connected components of
by the addition of
. Hence

for

(4)
(5)

If we add all the inequalities (4) and (5), we have that
(6)
From inequalities (3) and (6), we have that
(7)
From inequalities (1) and (7), we finally obtain that

Before the MinMax-ODA starts adding the th matching, we
have
components, and we know that if we add all the
matchings
, to the current solution, the total number of connected components will be reduced

1336

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

to
. Therefore, there exists a matching
that decreases the total number of connected components by at least
. Since our algorithm always finds at least a 1/3that our algooptimal matching (Lemma 5), the matching
rithm uses must decrease the total number of connected components at that time by at least one third of this amount. Therefore

Hence, the number of iterations for our algorithm
that
MinMax-ODA is less than or equal to the smallest
satisfies

implying that the degree of any node is at most a factor of
away from
.
VI. HARDNESS OF MINMAX-TCO PROBLEM
In this section, we will show that no polynomial-time algorithm can approximate the MinMax-TCO problem within less
). This is a tighter
than a logarithmic factor (unless
bound than the result in [20], where we showed that no constantapproximation polynomial-time algorithm was possible for this
problem.
The proof below follows the general lines of a proof presented
for [12, Theorem 5.3], which was adapted to handle the maximum node degree (instead of the average node degree as in
[12]) and to show a logarithmic lower bound on the approximation factor for the MinMax-TCO problem unless
(in [12], Theorem 5.3 proved a weaker, constant approximation
lower bound on the average node degree).
Theorem 2: There exists no polynomial-time algorithm that
approximates the MinMax-TCO problem within less than a log, unless
.
arithmic factor,
Proof: The proof follows in general lines the Proof of
[12, Theorem 5.3]. In fact, a more careful observation of the
Proof of [12, Theorem 5.3] shows that the proof basically
shows the hardness of approximation of the maximum degree,
and not the average degree, as claimed in [12].
For the sake of completeness, we present here the sketch of
the Proof of [12, Theorem 5.3] when directly applied to the
maximum degree. We first define the single-node version of the
MinMax-TCO problem.
Single-Node Version of the MinMax-TCO Problem
, and a node
,
(SN-MinMax-TCO): Given
connect the nodes in into an overlay network that has least
possible degree for node and that is topic-connected.
We can prove that SN-MinMax-TCO is NP-hard by reducing
the minimum set cover problem to this problem. We then
show how to reduce the SN-MinMax-TCO problem to the
MinMax-TCO problem, thus showing that the MinMax-TCO
problem is NP-hard.

Now, we are ready to prove our inapproximability result. We
will use the same reduction as in the Proof of [12, Theorem 5.3].
Assume that there is an algorithm A that approximates the
MinMax-TCO problem within a factor of . We will show that
we can use this algorithm A to find a -approximation to the
minimum set cover problem.
, construct
Given an instance
the corresponding instance
following the construction in the Proof of [12, Lemma 5.1].
denote the maximum degree of an optimal soLet
denotes the degree of node
in an
lution (that is,
optimal solution) for this instance of the SN-MinMax-TCO
problem. As shown in the Proof of [12, Lemma 5.1], the
also has
corresponding instance
. Hence, algorithm A will find a
a solution with degree
solution to
problem with degree at
. Using this solution, we can construct a solution
most
with degree at most
to
as in [12, Lemma 5.1]. Thus, we have a -approximation
algorithm B for the SN-MinMax-TCO problem.
In the minimum set cover problem, we are given an in, where is a set of elements and is a collection
stance
of smallest
of subsets of , and we would like to find
possible cardinality such that
. Given an
, construct
instance of the minimum set cover problem
instance folthe corresponding
lowing the construction as in the Proof of [12, Lemma 5.2]. Let
denote an optimal solution for this instance of the minimum
set cover problem. As shown in the Proof of [12, Lemma 5.2],
instance also
the corresponding
. Hence, algorithm B will find a
has a solution with degree
solution to
with degree at most
.
Using this solution, we can construct a solution to the minas in
imum set cover problem with cardinality at most
[12, Lemma 5.2]. Thus, we have a -approximation algorithm C
for the minimum set cover problem. Given that we cannot ap-approxproximate the minimum set cover within an
, and the
imation factor [24], unless
claim is proved.
Since it is trivial to show that MinMax-TCO is in NP, we have
the following.
Corollary 1: The MinMax-TCO problem is NP-complete.
VII. EXPERIMENTAL RESULTS
The GM algorithm [12] and our MinMax-ODA algorithm
are implemented in Java. These two algorithms are compared
according to maximum degree and average degree in the
resulting overlay graphs. Experimental results show that the
MinMax-ODA improves the maximum degree of the overlay
network drastically at the cost of a small increase on the average
degree and diameter.
A. Maximum Node Degree
For these experiments, the number of nodes varies between
1000–10 000. In the first experiment (Fig. 2), the number of
topics is 100, and in the second experiment (Fig. 3), the number
of topics is 200. We fixed the number of subscriptions to
.

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

Fig. 2. Maximum node degree for GM and MinMax-ODA (number of topics
is 100).

Fig. 3. Maximum node degree for GM and MinMax-ODA (number of topics
is 200).

Each node is interested in each topic uniformly at random. This
experimental setting is similar to previous studies [12].
Fig. 2 gives the comparison results for the GM and the
MinMax-ODA algorithms for the maximum degree metric. The
maximum degree of the graph decreases for the MinMax-ODA
algorithm when the number of nodes increases since the
MinMax-ODA algorithm can find more edges with higher
correlation as the number of nodes increases. Interestingly, the
maximum degree of the graph increases for the GM algorithm
as the number of nodes increases. The reason is a randomness of
the subscription patterns. When the number of nodes increases,
the chance for a node to find many neighbors with a big interest
overlap increases. Basically as the number of nodes increases,
the GM algorithm will assign more edges to the same nodes
since now we have more nodes with higher correlation for each
node. When we compare the results of the GM algorithm and
the MinMax-ODA algorithm, the MinMax-ODA algorithm
outperforms the GM algorithm by a factor of 10 on average
(Fig. 2).
The same results are valid for Fig. 3. When we compare
Figs. 2 and 3, the maximum node degree increases slightly for
both the GM algorithm and the MinMax-ODA algorithm since
edges will have less correlation when we increase the number
of topics.
B. Average Node Degree
We use the same experimental setting as in Section VII-A.
Fig. 4 is a comparison of the GM algorithm and the
MinMax-ODA algorithm for the average degree metric.

1337

Fig. 4. Average node degree for GM and MinMax-ODA (number of topics is
100).

Fig. 5. Average node degree for GM and MinMax-ODA (number of topics is
200).

The average degree of the graph decreases for both the GM
and the MinMax-ODA algorithms when the number of nodes
increases since both algorithms can find edges with higher
correlation. When we compare the results of the GM and the
MinMax-ODA algorithms, the GM algorithm is slightly better
than the MinMax-ODA algorithm, by 9% on average (Fig. 4).
Similar results are valid for Fig. 5.
C. Diameter
We use the same experimental setting as in Section VII-B.
The number of topics is 100. Fig. 6 is a comparison of the
GM algorithm and the MinMax-ODA algorithm for the diameter metric. The diameter of the graph increases for both
the GM and the MinMax-ODA algorithms when the number
of nodes increases since we have more nodes subscribed to
each topic. When we compare the results of the GM and the
MinMax-ODA algorithms, the GM algorithm is slightly better
than the MinMax-ODA algorithm, by 30% on average (Fig. 6).
D. Subscription Size
In these experiments, the number of nodes and the number
of topics are fixed to 100. The subscription size varies between
10 and 50. Each node is interested in each topic uniformly
at random. Fig. 7 is the comparison of the GM and the
MinMax-ODA algorithms for the maximum degree metric.
When the subscription size increases, the maximum degree of
the overlay network decreases for the MinMax-ODA algorithm
since the MinMax-ODA algorithm can find edges with higher

1338

Fig. 6. Diameter for GM and MinMax-ODA.

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

Fig. 9. Maximum node degrees for Zipf distribution.

Fig. 10. Average node degrees for Zipf distribution.
Fig. 7. Maximum node degree for different subscription sizes.

slightly better than the MinMax-ODA—by close to 10% on
average (Fig. 8).
E. Zipf Distribution

Fig. 8. Average node degree for different subscription sizes.

correlation. When the subscription size increases, the maximum
degree of the overlay network increases for the GM algorithm.
When we compare the current experimental results for the
GM and the MinMax-ODA algorithms, the maximum degree
obtained by the MinMax-ODA is a factor of 4 less than the
maximum degree obtained by the GM algorithm, on average
(Fig. 2).
Fig. 8 gives the comparison results for the GM and the
MinMax-ODA algorithms for the average degree metric. The
average degree of the overlay network decreases for both GM
and the MinMax-ODA algorithms as the subscription size
increases since both algorithms can find edges with higher
correlations. When we compare the average degree obtained by
the GM and the MinMax-ODA algorithms, the GM performs

For these experiments, the number of nodes varies between
1000 and 10 000. The number of topics is fixed at 100, and
. Each node subscribes to each
the subscription size at
topic with probability . Every time a node does not get exactly 10 subscriptions, we regenerate the node (i.e., we reassign
topics to the node according to the probabilities ). The value
of is distributed according to a Zipf distribution with
. This experimental setting is similar to previous
studies (e.g., [12]). Fig. 9 presents a comparison of the GM
and MinMax-ODA algorithms for the maximum degree metric,
while Fig. 10 is a comparison of the GM and MinMax-ODA algorithms for the average degree metric. The maximum degree
obtained by the MinMax-ODA algorithm is on average less than
1/7 of the maximum degree obtained by the GM algorithms, at
the expense of a small increase on the average degree.
VIII. CONSTRUCTING CONSTANT DIAMETER OVERLAYS FOR
PUBLISH–SUBSCRIBE
In this section, we study a variation of the low-degree topicconnected problems we considered so far, where we build a constant diameter overlay network for publish/subscribe communication with many topics [22]. We present three overlay network construction heuristics that guarantee constant diameter
and topic-connectivity, which are important factors for efficient
routing. We now formally define the problem.

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

Constant Diameter Topic-Connected Overlay (CD-TCO, or
-CD-TCO) Problem: Given a collection of nodes , a set of
topics , and node interest assignment , connect the nodes in
into a topic-connected overlay network that has least possible
average degree and diameter at most , where is a nonnegative
constant.
This problem aims at minimizing the average degree, as does
the MinAv-TCO problem introduced by Chockler et al. [12],
with the additional requirement on the diameter. We focus on
and present three heuristics for the 2-CD-TCO
the case
problem, which are validated via experimental results.
We first show that the GM algorithm and the MinMax-ODA
algorithm do not work well for the 2-CD-TCO problem. Second,
we present our 2-diameter algorithms and their performance
evaluations.
In the 2-CD-TCO problem, there are few topologies (of diameter at most 2) that each topic-connected component can
have. One such topology, which also minimizes the number of
edges required for connectivity in each component, is the star
topology. Hence, in all of our heuristics, we assume that the
topology of each 2-diameter component that emerges is a star
topology.
A. GM Algorithm and the MinMax-ODA Algorithm for the
CD-TCO Problem
The GM algorithm and the MinMax-ODA algorithm do not
work well for the CD-TCO problem: The diameter obtained by
the algorithms may be as bad as
, as we show in the following lemma.
Lemma 6: The GM algorithm and the MinMax-ODA algo, where
rithm can produce an overlay network of diameter
is number of nodes in the pub/sub system.
Proof: Consider the example where we have
nodes
, one topic , and every node is interested in
. There exist many orderings of the edges of
for which
the GM algorithm would produce an overlay network with
. For example, the overlay network
,
diameter
, can result from the GM
where
.
algorithm—the diameter of this overlay network is
The MinMax-ODA algorithm will also result in this network.
Another solution for the CD-TCO on the same configurais the overlay network
, where
tion for the nodes
, which has diameter 2.
B. Examples I and II
In Fig. 11, each node is subscribed to topics 1 and 2. In an opedges and we have only one star.
timal solution, we have
This example illustrates the intuition that we should have a minimum number of stars that satisfy all connectivity requirements.
Intuition 1: Construct minimum number of stars.
nodes
In Fig. 12, node is subscribed to topics 1–5.
nodes are subscribed
are subscribed to topics 1 and 2.
to topics 2 and 3.
nodes are subscribed to topics 3
nodes are subscribed to topics 4 and 5. In the
and 4.
edges, and we have only one star
optimal solution, we have

1339

Fig. 11. Example I.

Fig. 12. Example II.

with center node . From this example arises the intuition that
nodes for which there are many nodes with overlapping interest
are good candidates for being the center of stars.
Intuition 2: Nodes for which there are many nodes with overlapping interest assignment are good candidates for being the
center of stars.
C. Constant Diameter Overlay Design Algorithm (CD-ODA)
CD-ODA starts with the overlay network
. At each
iteration of the CD-ODA, a node that has the maximum number
of neighbors with nonempty interest intersection is chosen. The
number of neighbors of a node is equal to

We then put an edge between this node and each of its neighbors and remove all the topics in this node’s interest assignment
from the set of topics.
Algorithm 2: Constant Diameter Overlay Design Algorithm
(CD-ODA)
Set of all topics
1:
2: while is not empty do
3:
For each node , calculate number of nodes such
.
that there exists a topic in and
Denote this number by .
4:
Find node with maximum .
5:
Put an edge between and all nodes such that there
with
.
exists a topic
.
6:
Remove all topics from such that
7: end while
CD-ODA finds the optimal solution for Examples I and II in
Fig. 11.

1340

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

Fig. 13. CD-ODA on Example III.

Fig. 15. CD-ODA and CD-ODA-I in Example IV.

Fig. 14. Optimal solution for Example III.

Algorithm 3: Constant Diameter Overlay Design Algorithm I
(CD-ODA-I)

D. Example III
In Fig. 13, node is subscribed to topics 1–4. Node is subscribed to topics 1, 5, 9, 2, 6, and 10. Node is subscribed to
topics 3, 7, 11, 4, 8, and 12.
nodes are subscribed to
nodes are subscribed to topics 2, 6,
topics 1, 5, and 9.
nodes are subscribed to topics 3, 7, and 11.
and 10.
nodes are subscribed to topics 4, 8, and 12. For this
example, CD-ODA first puts edges between node and all the
other nodes, and then it puts edges between node and the first
nodes, and it puts edges between node
two sets of
and the last two sets of
nodes. Thus, CD-ODA uses
edges.
In the optimal solution (Fig. 14), there are edges between
nodes, there are edges
node and the first two sets of
nodes, and
between node and the last two sets of
there are edges
and
. In the optimal solution, only
edges are required. Let
us define the number of weighted neighbors of a node as

This example illustrates the intuition that nodes with many
weighted neighbors are good candidates for being the center of
stars.
Intuition 3: Nodes with many weighted neighbors are good
candidates for the center of stars.
Given examples I-III in Figs. 11, 12, and 14, respectively,
and intuitions 1–3, we designed the CD-ODA-I algorithm.
CD-ODA-I finds the optimal solution for Example III in
Fig. 14.
E. Constant Diameter Overlay Design Algorithm I
(CD-ODA-I)
. At each
CD-ODA-I starts with the overlay network
iteration of the CD-ODA-I, a node that has the maximum
number of weighted neighbors is chosen.
We add an edge between and each of its neighbors and then
remove the topics in this node’s interest assignment from the set
of topics.

1:
Set of all topics
2: while is not empty do
3:
For each node , calculate total number of weighted
.
neighbors. Denote this number by
.
4:
Find node with maximum
5:
Put an edge between and all nodes such that there
with
.
exists a topic
6:
Remove all topics from such that
.
7: end while
We can also find an example that CD-ODA performs better
than CD-ODA-I. Consider the following example. We have
nodes
. Nodes
and
are
. Nodes
subscribed to topics
are subscribed to topics
. Node
is subscribed to
. Node
is subscribed to topic . For this
topics
example, CD-ODA-I first puts edges between node
and nodes
, and then it puts edges
between node
and nodes
. Thus,
edges.
CD-ODA-I uses
For this example, CD-ODA puts edges between node
and nodes
, and then it puts
edges between nodes
and . Thus, CD-ODA uses only
edges.
F. Example IV
.
In Fig. 15, node is subscribed to topics
nodes (set ) are subscribed to
nodes (set ) are
nodes (set ) are subscribed to
subscribed to , and
.
nodes (set ) are subscribed to
nodes (set ) are subscribed to
, and
nodes (set
) are subscribed to
. For this example, CD-ODA and
CD-ODA-I first put edges between node and all the nodes in
, and then it puts edges between nodes
sets
and all nodes in
and , for
. Thus, CD-ODA
and CD-ODA-I use
edges.
In the optimal solution (Fig. 16), there are edges between
and all nodes in
and , for
.
nodes

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

1341

and
for all
CD-ODA-II first puts edges between nodes
. It puts an edge between nodes
and . Then,
.
it puts edges between node and nodes for all
It puts edges between node and nodes for all
.
edges. For this
Thus, CD-ODA-II uses
example, CD-ODA and CD-ODA-I puts edges between node
and all the other nodes. Thus, CD-ODA and CD-ODA-I use only
edges.
H. Analysis of Algorithms

Fig. 16. Optimal solution for Example IV.

There are edges between node and nodes , for
.
In the optimal solution, only
edges are required. Let us define the connection density of a
, as
node

From this example, we can have the intuition that nodes with
dense connections are good candidates for the center of stars.
Intuition 4: Nodes with dense connections are good candidates for the center of stars.
G. Constant Diameter Overlay Design Algorithm II
(CD-ODA-II)
CD-ODA-II also starts with the overlay network
. At
each iteration of the CD-ODA-II, a node that has maximum
is chosen.
connection density
We add an edge between a node with maximum density and
each of its neighbors and then remove the topics in this node’s
interest assignment from the set of topics.
Algorithm 4: Constant Diameter Overlay Design Algorithm II
(CD-ODA-II)
1:
Set of all topics
2: while is not empty do
3:
For each node , calculate its connection density.
Denote this number by .
4:
Find a node with maximum .
5:
Put an edge between and all nodes such that there
with
.
exists a topic
6:
Remove all topics from such that
.
7: end while
Given the example in Fig. 15 and Intuition 4, we designed the
CD-ODA-II algorithm. CD-ODA-II finds an optimal solution
for Example IV in Fig. 16.
We can also find an example that CD-ODA and CD-ODA-I
perform better than CD-ODA-II. Consider the following exnodes
.
ample. We have
, nodes
are subscribed to three topics
For all
. For all
, nodes
are subscribed to the
topic . Node is subscribed to all topics. For this example,

In this section, we show that all constant diameter algorithms
presented (CD-ODA, CD-ODA I, CD-ODA II) terminate in
time-steps and generate a 2-diameter overlay
for each topic.
Lemma 7: CD-ODA, CD-ODA-I, and CD-ODA-II terminate
time.
within
time, we can find the total weight
Proof: In
of the neighbors or the connection density for each node. We
choose a node with maximum weight/density. We remove all
the topics to which node is subscribed. We repeat this process
until all topics have been considered. At each iteration, at least
one topic is removed. Thus, the algorithm will take at most
=
time-steps.
Lemma 8: CD-ODA, CD-ODA-I, and CD-ODA-II generate
a 2-diameter overlay for each topic.
Proof: Since any of the algorithms generates a star for each
topic, each topic overlay network will have diameter 2.
I. Conjectures
Conjecture 1: All three algorithms approximate the constant
diameter overlay design problem within a logarithmic factor.
Our first intuition is that if there exists a -edge overlay network with constant diameter for a graph , then there exists a
-edge overlay with diconstant such that there exists a
ameter 2 for a graph . This step will make the reduction from
constant diameter overlay to 2-diameter overlay.
For a 2-diameter overlay, the most “efficient” graph for each
topic alone will have a star structure since a star has diameter 2
edges). Combining these
and optimal number of edges (
two steps, we have Conjecture 1.
Conjecture 2: There exists no polynomial-time algorithm
that approximates the constant diameter overlay design problem
.
within less than a logarithmic factor unless
Our intuition for Conjecture 2 is that the set cover problem
can probably be reduced to this problem in a similar fashion as
it was done for the MinMax-TCO and MinAv-TCO problems.
J. Experimental Results
The GM algorithm [12] and our algorithms are implemented
in Java. These algorithms are compared according to the average
degree in the resulting graph. The diameter is always 2 for our
, where is the number of nodes,
algorithms, and it may be
for the GM algorithm. When we compare the results of the GM
algorithm and our algorithms according to the average degree,
our algorithms require at most 2.3 times more edges than the
GM algorithm, while improving the diameter drastically.
1) Average Node Degree With Varying Number of Nodes:
For the first experiment, the number of nodes varies between

1342

Fig. 17. Average node degree for GM, CD-ODA, CD-ODA-I, and CD-ODA-II.

Fig. 18. Average node degree for GM, CD-ODA, CD-ODA-I, and CD-ODA-II.

100–1000. We fixed the number of topics at 100, and the number
. Each node is interested in each topic
of subscriptions at
uniformly at random. This experimental setting is similar to previous studies [12], [20], [23].
Fig. 17 gives the comparison results for the GM and our
algorithms according to the average degree. The average degree
of the graph decreases for the GM algorithm when the number
of nodes increases since there are more edges with higher
correlation. The average degree of the graph slightly decreases
for our algorithms. When we compare the results of GM and
CD-ODA-II, our algorithm requires at most 2.3 times more
edges than GM (Fig. 17). CD-ODA-II improves CD-ODA by a
factor of 3% on average, and CD-ODA-I by a factor of 2% on
average.
2) Average Node Degree With Varying Number of Topics: For
the second experiment, the number of nodes is 100, the number
of topics varies between 100 and 500, and the number of subscriptions is fixed at
. Each node is interested in each
topic uniformly at random. This experimental setting is also
similar to previous studies [12], [23].
Fig. 18 gives the comparison results for the GM and our algorithms for the average degree metric. The average degree of the
graph increases for our algorithms and the GM algorithm as the
number of topics increases since the edges will have lower correlations. When we compare the results of GM and CD-ODA-II,
our algorithm requires at most 1.9 times more edges than the
GM (Fig. 18). CD-ODA-II improves CD-ODA by a factor of
6% on average, and CD-ODA-I by a factor of 2% on average.

IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 19, NO. 5, OCTOBER 2011

Fig. 19. Average node degree for GM, CD-ODA, CD-ODA-I, and CD-ODA-II.

3) Average Node Degree With Varying Subscription Size: For
the third experiment, the number of nodes and the number of
topics are fixed at 100. The subscription size varies between
10 and 50. Each node is interested in each topic uniformly at
random. (The experimental setting is also similar to previous
studies [12], [23].)
Fig. 19 gives the comparison results for the GM and our algorithms for the average degree metric. The average degree of
the overlay network decreases for both GM and our algorithms
when the subscription size increases since all algorithms can
find edges with higher correlation. When we compare the results of GM and CD-ODA-II, our algorithm requires at most
1.8 times more edges than the GM (Fig. 19). CD-ODA-II improves CD-ODA by a factor of 20% on average, and CD-ODA-I
by a factor of 1% on average.
IX. CONCLUSION
In this paper, we study a new optimization problem
(MinMax-TCO) that constructs a practical and scalable
overlay network for publish/subscribe communication with
many topics. We present a topic-connected overlay network
design algorithm (MinMax-ODA) that approximates the
MinMax-TCO problem within a logarithmic factor. We also
show that the approximation factor of the MinMax-ODA is
almost tight since no less than logarithmic-approximation
polynomial-time algorithm can exist for the MinMax-TCO
).
problem (unless
Our experimental results validate our formal analysis of the
MinMax-ODA algorithm, showing that the maximum degree
obtained by our algorithm clearly outperforms the maximum
degree obtained when the GM algorithm is used.
We present three heuristics for constructing constant diameter overlay networks. Our experimental results show that the
diameter obtained by our heuristics outperforms the diameter
obtained when GM is used while only increasing the average
degree by a factor of 2.3.
As future work, we would like to build upon our CD-ODA
algorithm by formally and experimentally evaluating the hardness of obtaining a topic-connected overlay design algorithm
that achieves a “good” tradeoff between low diameter and low
node degree. This basically amounts to a bicriteria optimization
problem, and we have to be able to “quantify” the relative importance of optimizing over these two parameters (e.g., in the

ONUS AND RICHA: MINIMUM MAXIMUM-DEGREE PUBLISH–SUBSCRIBE OVERLAY NETWORK DESIGN

CD-ODA algorithm, we restrict our attention to networks of
diameter 2 while aiming at maintaining the average degree as
low).
Two other important lines for future work would be to design
efficient distributed algorithms for the MinMax-TCO problem
and to look at this problem under the line of a dynamic configuration of the node set and the interest assignment .
REFERENCES
[1] Oracle9i Application Developers Guide Advanced Queuing. Redwood Shores, CA: , Oracle.
[2] M. Altherr, M. Erzberger, and S. Maffeis, “iBusa software bus middleware for the Java platform,” in Proc. Int. Workshop Rel. Middleware
Syst., 1999, pp. 43–53.
[3] E. Anceaume, M. Gradinariu, A. K. Datta, G. Simon, and A. Virgillito,
“A semantic overlay for self- peer-to-peer publish/subscribe,” in Proc.
26th IEEE ICDCS, 2006, p. 26.
[4] S. Baehni, P. T. Eugster, and E. Guerraoui, “Data-aware multicast,” in
Proc. DSN, 2004, pp. 233–242.
[5] R. Baldoni, R. Beraldi, V. Quema, L. Querzoni, and S. T. Piergiovanni,
“TERA: Topic-based event routing for peer-to-peer architectures,” in
Proc. 1st ACM DEBS, 2007, pp. 2–13.
[6] R. Baldoni, R. Beraldi, L. Querzoni, and A. Virgillito, “Efficient publish/subscribe through a self-organizing broker overlay and its application to SIENA,” Comput. J., vol. 50, no. 4, pp. 444–459, 2007.
[7] S. Banerjee, B. Bhattacharjee, and C. Kommareddy, “Scalable application layer multicast,” SIGCOMM Comput. Commun. Rev., vol. 32, no.
4, pp. 205–217, 2002.
[8] S. Bhola, R. Strom, S. Bagchi, Y. Zhao, and J. Auerbach, “Exactly-once
delivery in a content-based publish-subscribe system,” in Proc. DSN,
2002, pp. 7–16.
[9] A. Carzaniga, M. J. Rutherford, and A. L. Wolf, “A routing scheme
for content-based networking,” in Proc. 23rd Annu. IEEE INFOCOM,
Hong Kong, Mar. 2004, vol. 2, pp. 918–928.
[10] M. Castro, P. Druschel, A. M. Kermarrec, and A. Rowstron, “SCRIBE:
A large-scale and decentralized application-level multicast infrastructure,” IEEE J. Sel. Areas Commun., vol. 20, no. 8, pp. 1489–1499, Oct.
2002.
[11] R. Chand and P. Felber, “Semantic peer-to-peer overlays for publish/
subscribe networks,” in Euro-Par 2005 Parallel Processing, Lecture
Notes in Computer Science. New York: Springer-Verlag, 2005, vol.
3648, pp. 1194–1204.
[12] G. Chockler, R. Melamed, Y. Tock, and R. Vitenberg, “Constructing
scalable overlays for pub-sub with many topics,” in Proc. 26th ACM
PODC, 2007, pp. 109–118.
[13] G. Chockler, R. Melamed, Y. Tock, and R. Vitenberg, “SpiderCast: A
scalable interest-aware overlay for topic-based pub/sub communication,” in Proc. 1st ACM DEBS, 2007, pp. 14–25.
[14] R. Diestel, Graph Theory, 2nd ed. New York: Springer-Verlag, 2000.
[15] P. T. Eugster, P. A. Felber, R. Guerraoui, and A. M. Kermarrec, “The
many faces of publish/subscribe,” Comput. Surveys, vol. 35, no. 2, pp.
114–131, 2003.
[16] R. Guerraoui, S. Handurukande, and A. M. Kermarrec, “Gossip: A
gossip-based structured overlay network for efficient content-based filtering,” EPFL, Lausanne, Tech. Rep. IC/2004/95, 2004.
[17] B. Korte and J. Vygen, Combinatorial Optimization Theory and Algorithms, 2nd ed. New York: Springer-Verlag, 2000.
[18] R. Levis, Advanced Massaging Applications With MSMQ and
MQSeries. Indianapolis, IN: QUE, 1999.
[19] H. Liu, V. Ramasubramanian, and E. G. Sirer, “Client behavior and
feed characteristics of RSS, a publish-subscribe system for web micronews,” in Proc. IMC, Berkeley, CA, 2005, pp. 29–34.
[20] M. Onus and A. W. Richa, “Minimum maximum degree publish-subscribe overlay network design,” in Proc. 28th Annu. IEEE INFOCOM,
Rio de Janeiro, Brazil, 2009, pp. 882–890.

1343

[21] M. Onus and A. W. Richa, “Brief announcement: Parameterized maximum and average degree approximation in topic-based publish-subscribe overlay network design,” in Proc. 21st ACM SPAA, Calgary, AB,
Canada, 2009, pp. 39–40.
[22] M. Onus and A. W. Richa, “Parameterized maximum and average degree approximation in topic-based publish-subscribe overlay network
design,” in Proc. 30th IEEE ICDCS, Genoa, Italy, 2010, pp. 644–652.
[23] V. Ramasubramanian, R. Peterson, and E. G. Sirer, “Corona: A high
performance publish-subscribe system for the World Wide Web,” in
Proc. 3rd NDSI, 2006, pp. 15–28.
[24] R. Raz and M. Safra, “A sub-constant error-probability low-degree test,
and a sub-constant error-probability PCP characterization of NP,” in
Proc. 29th Annu. ACM STOC, 1997, pp. 475–484.
[25] D. Sandler, A. Mislove, A. Post, and P. Druschel, “FeedTree: Sharing
Web micronews with peer-to-peer event notification,” in Proc. IPTPS,
2005, pp. 141–151.
[26] D. Skeen, “Vitrias publish-subscribe architecture: Publish-subscribe overview,” Sunnyvale, CA, 1998 [Online]. Available:
http://www.vitria.com
[27] D. Tam, R. Azimi, and H.-A. Jacobsen, “Building content-based publish/subscribe systems with distributed hash tables,” in Proc. 1st IDBISP2P, Berlin, Germany, 2003, pp. 138–152.
[28] “Everything you need to know about middleware: Mission-critical
interprocess communication,” Talarian Corporation, Los Altos, CA,
White paper, 1999 [Online]. Available: http://www.talarian.com/
[29] “TIB/Rendezvous,” Tibco, Palo Alto, CA, White paper, 1999.
[30] Y. Tock, N. Naaman, A. Harpaz, and G. Gershindky, “Hierarchical
clustering of message flows in a multicast data dissemination system,”
in Proc. 17th IASTED Int. Conf. Parallel Distrib. Comput. Syst., 2005,
pp. 320–327.
[31] S. Voulgaris, E. Riviere, A. M. Kermarrec, and M. van Steen, “Sub-2sub: Self-organizing content-based publish subscribe for dynamic large
scale collaborative networks,” in Proc. IPTPS, 2006, pp. 123–128.

Melih Onus received the B.S. degree in computer engineering from Bilkent University, Ankara, Turkey, in
2003, and the Ph.D. degree in computer science from
Arizona State University (ASU), Tempe, in 2009.
He is currently an Instructor with Bilkent University. His research interests are in the areas of distributed computing, computer networks, and theoretical computer science.

Andréa W. Richa received the B.S. degree in computer science and M.S. degree in computer systems
from the Federal University of Rio de Janeiro, Rio de
Janeiro, Brazil, in 1990 and 1992, respectively, and
the M.S. and Ph.D. degrees in computer science from
Carnegie Mellon University, Pittsburgh, PA, in 1995
and 1998, respectively.
She has been an Associate Professor in computer
science and engineering with Arizona State University, Tempe, since August 2004, having joined as an
Assistant Professor in August 1998. For a selected list
of her publications, CV, and current research projects, please visit http://www.
public.asu.edu/~aricha. Her main area of research is in network algorithms.
Some of the topics she has worked on include packet scheduling, distributed
load balancing, packet routing, ad hoc wireless network clustering and routing,
wireless network modeling, and distributed hash tables.
Dr. Richa was the recipient of a National Science Foundation (NSF)
CAREER Award in 1999.

Constant Density Spanners for Wireless Ad-Hoc Networks
Kishore Kothapalli

Christian Scheideler

Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{kishore,

Melih Onus

Andrea Richa

Department of Computer Science
Arizona State University
Tempe, AZ 85281, USA

scheideler}@cs.jhu.edu

{melih,

aricha}@asu.edu

ABSTRACT

General Terms

An important problem for wireless ad hoc networks has been to
design overlay networks that allow time- and energy-efficient routing. Many local-control strategies for maintaining such overlay networks have already been suggested, but most of them are based on
an oversimplified wireless communication model.
In this paper, we suggest a model that is much more general than
previous models. It allows the path loss of transmissions to significantly deviate from the idealistic unit disk model and does not
even require the path loss to form a metric. Also, our model is apparently the first proposed for algorithm design that does not only
model transmission and interference issues but also aims at providing a realistic model for physical carrier sensing. Physical carrier
sensing is needed so that our protocols do not require any prior information (not even an estimate on the number of nodes) about the
wireless network to run efficiently.
Based on this model, we propose a local-control protocol for establishing a constant density spanner among a set of mobile stations
(or nodes) that are distributed in an arbitrary way in a 2-dimensional
Euclidean space. More precisely, we establish a backbone structure by efficiently electing cluster leaders and gateway nodes so
that there is only a constant number of cluster leaders and gateway
nodes within the transmission range of any node and the backbone
structure satisfies the properties of a topological spanner.
Our protocol has the advantage that it is locally self-stabilizing,
i.e., it can recover from any initial configuration, even if adversarial nodes participate in it, as long as the honest nodes sufficiently
far away from adversarial nodes can in principle form a single connected component. Furthermore, we only need constant size messages and a constant amount of storage at the nodes, irrespective
of the distribution of the nodes. Hence, our protocols would even
work in extreme situations such as very simple wireless devices
(like sensors) in a hostile environment.

Algorithms, Theory

Keywords
ad hoc networks, spanner, dominating set, self-stabilization

1. INTRODUCTION
An important problem for wireless ad hoc networks has been
to design overlay networks that allow time- and energy-efficient
routing. Many local-control strategies for maintaining such overlay networks have already been suggested, but mostly high-level
wireless models have been used for their analysis. However, since
mobile ad-hoc networks have many features that are hard to model
in a clean way, it is not clear how well these strategies may actually
perform in practice. Major challenges are how to model wireless
communication and how to model mobility. Here, theoretical work
is still in its infancy. So far, people in the algorithms community
have mostly looked at static wireless networks (i.e. the wireless
stations are always available and do not move). Wireless communication has mostly been modeled using the packet radio network
or the even simpler unit disk model. In the packet radio network
model, the wireless units, or nodes, are represented by a graph, and
two nodes are connected by an edge if and only if they are within
transmission range of each other. Transmissions of messages interfere at a node if at least two of its neighbors transmit a message at
the same time. A node can only receive a message if it does not
interfere with any other message.
The packet radio network model is a simple and clean model that
allows to design and analyze algorithms for overlay networks with
a reasonable amount of effort. However, since it is a high-level
model, it does have some serious problems with certain scenarios
in practice because in reality the transmission range of a message
is not the same as its interference range. Consider, for example,
two nodes s and t and a set U of n nodes with all nodes in U
being within the transmission range of s but only one node in U ,
v, being within the transmission range of t. All other nodes in U
just interfere with t. Then no uniform protocol (i.e., at each step all
nodes try to access the wireless medium with the same probability)
can send a message from s to t in an expected o(n) number of
steps, whereas a constant expected time is easy to achieve if we can
ignore interference problems between U and t.
There are a limited number of papers in the theory area that use
a model that differentiates between the transmission range and the
interference range [1, 12, 13, 14, 22], but they still assume a disk
model in a sense that the transmission range and interference range
can be modeled by two distance values that hold irrespective of the
position of a node. We propose a more general model.

Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Architecture and Design—Wireless communication; F.2.8 [Analysis of
Algorithms and Problem Complexity]: Non-numerical Algorithms
and Problems—Computation of discrete structures

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SPAA’05, July 18–20, 2005, Las Vegas, Nevada, USA.
Copyright 2005 ACM 1-58113-986-1/05/0007 ...$5.00.

116

1.1 Wireless communication model

example, are 10dB for 11Mbps, 8dB for 5.5Mbps, 6dB for 2Mbps,
and 4dB for 1Mbps, and for 802.11a, 23dB is usually the minimum
SNR for 54Mbps. In the 802.11a standard [28], the minimum dB
values are defined as the received signal strength level at which the
frame error rate (FER) of a 1000-octet frame is less than 10%.
The SNRs above specify the transmission range of the data transmission mechanism, i.e. the maximum range within which data
frames can still be received correctly. The transmission range is
highly dependent on the environment. A reasonable model for
determining the transmission range is the log-normal shadowing
model [23, 30]. In this model, the received power at a distance of d
relative to the received power at a reference distance of d0 is given
in dB as

In order to motivate our model, we first review some commonly
used transmission techniques in wireless communication. We will
concentrate here on the IEEE 802.11 standard because IEEE 802.11based radio LANs are currently dominating the market and will
most probably do so also in the future. The IEEE 802.11 standard
distinguishes between a Physical (PHY) layer and a Medium Access Control (MAC) layer for the transmission of messages. The
802.11 MAC protocols are based on Carrier Sense Multiple Access
with Collision Avoidance (CSMA/CA).

Carrier sensing
The basic approach of the CSMA/CA scheme is as follows. Whenever a node receives a packet to be transmitted, it first listens to
the channel to ensure no other node is transmitting. If the channel
is clear, it transmits the packet. Otherwise, it uses an exponential back-off scheme until it either finds a time point in which the
channel is clear so that it can transmit its packet or aborts the transmission due to too many failed attempts.
In WLAN devices, there is usually just one antenna for both
sending and receiving, and hence the stations are not able to listen while sending. For this and other reasons there is no collision
detection capability like in the Ethernet. Therefore, acknowledgment packets (ACK) have to be sent from the receiver to the sender
to confirm that packets have been correctly received.
In wireless ad hoc networks that rely on a carrier-sensing random
access protocol, such as IEEE 802.11, the wireless medium characteristics generate complex phenomena such as the well-known
hidden-station and exposed-station problems. In order to handle
these problems, the MAC layer uses physical and virtual carrier
sensing techniques.
The physical carrier sensing part of the CSMA scheme is provided by a Clear Channel Assessment (CCA) circuit. This circuit
monitors the environment to determine when it is clear to transmit. It can be programmed to be a function of the Receive Signal
Strength Indication (RSSI) and other parameters. The RSSI measurement is derived from the state of the Automatic Gain Control
(AGC) circuit. Whenever the RSSI exceeds a certain threshold, a
special Energy Detection (ED) bit is switched to 1, and otherwise
it is set to 0. By manipulating a certain configuration register, this
threshold may be set to an absolute power value of t dB, or it may
be set to be t dB above the measured noise floor, where t can be set
to any value in the range 0-127. The ability to manipulate the CCA
rule allows the MAC layer to optimize the physical carrier sensing
to its needs.
Virtual carrier sensing is usually achieved by using two control
packets, Request To Send (RTS) and Clear To Send (CTS), which
are exchanged before the data transmission is taking place. More
precisely, before transmitting a data frame, the source station sends
an RTS packet to the receiving station announcing the upcoming
frame transmission. Upon receiving the RTS packet, the destination
replies by a CTS packet to indicate that it is ready to receive the data
frame. Both the RTS and CTS packets contain the total duration
of the transmission, i.e. the overall time needed to transmit the
data frame and the related ACK, so that other stations within the
transmission range of either the source or the destination stay silent
until the transmission is over.

−10 log(d/d0 )θ + Xσ
where θ is the path loss coefficient and Xσ is a Gaussian random
variable with zero mean and standard deviation σ (in dB) that models the influence of the background noise. θ usually ranges from 2
(free space) to 5 (indoors) [31].
When using forward error correction mechanisms as proposed in
the IEEE 802.11e MAC standard currently under development, the
transition between being able to correctly receive a data frame with
high probability and not being able to correctly receive a data frame
with high probability is very sharp. As shown in [6], it can be less
than 1 dB. Thus, in an ideal environment the transmission range is
an area with a relatively sharp border that in reality, however, may
be blurred due to environmental effects.
A limitation of the shadowing model is that it is only applicable
in uniform environments. In non-uniform environments, the signal
strength can exhibit a non-monotonic behavior. For example, it
can happen that the sender position A has a smaller distance to a
position B than to a position C and yet the strength of the signal
from A received at B is lower than the signal strength received at
C. This can even happen if B and C are close by.
For the interference and physical carrier sensing ranges there
does not seem to be a commonly accepted definition in practice.
So we will use a conservative model for these ranges to make sure
that our results in this model are meaningful in practice.

Our model
In our model, we assume that we are given a set V of mobile
stations, or nodes, that are distributed in an arbitrary way in a 2dimensional Euclidean space. For any two nodes v, w ∈ V let
d(v, w) be the Euclidean distance between v and w. Furthermore,
consider any cost function c with the property that there is a fixed
constant δ ∈ [0, 1) so that for all v, w ∈ V ,
• c(v, w) ∈ [(1 − δ) · d(v, w), (1 + δ) · d(v, w)] and
• c(v, w) = c(w, v), i.e. c is symmetric.
c determines the transmission and interference behavior of nodes
and δ bounds the non-uniformity of the environment. Notice that
we do not require c to be monotonic in the distance or to satisfy the
triangle inequality. This makes sure that our model even applies
to highly irregular environments. In Figure 1(a), for example, the
distance between u and v is greater than the distance between u and
w. Yet, the cost of communicating between u and w, c(u, w), is
bigger than c(u, v). Similar cost functions were also used in [21].
We assume that the nodes use some fixed-rate power-controlled
communication mechanism over a single frequency band. When
using a transmission power of P , there is a transmission range
rt (P ) and an interference range ri (P ) > rt (P ) that grow monotonically with P . The interference range has the property that

Transmission range, interference range, and physical
carrier sensing range
Every data transmission mechanism has a minimum signal-to-noise
ratio (SNR) at which a data frame can still be transmitted with a reasonably low frame error rate. The minimum SNRs for 802.11b, for

117

sense a transmission, then any superset of U will also do so. The
two sensing ranges are shown in Figure 1(b).
For simplicity, we will assume in the following that for the carrier sense ranges, rsi (T, P )/rst (T, P ) = ri (P )/rt (P ) for all relevant values of T .

rst (T,P)
a

u
w
c

w

b
rsi (T,P)

v

(a)

1.2 Our contributions

c

a

Our contributions are two-fold: we present a new model for
wireless networks, and we demonstrate how to develop and analyze algorithms on top of this model by presenting self-stabilizing
local-control algorithms for building constant density dominating
sets and spanners.
In our algorithms, the nodes do not have to have any a priori
knowledge about the other nodes, not even an estimate on their
total number. Also, fixed identification numbers of any form are
not required so that our protocols may even be applicable to the
important field of sensor networks. It is sufficient for us if the nodes
choose identification numbers so that there are no local conflicts
(which can be easily achieved with random, local-control coloring
strategies). We only require that the mobile hosts can synchronize
in rounds of constant length. This can be done, for example, with
the help of GPS signals or any form of beacons (that are sufficiently
far apart in time for a round of our protocols to complete).
In order to obtain a constant density spanner under an arbitrary
distribution of nodes, we proceed in two stages. First, we show that
there is a simple, distributed protocol to obtain a constant density
dominating set, and then we show how to extend this protocol in
order to also obtain a constant density spanner.
It is worth noting that our protocols only need a constant amount
of storage at each node, irrespective of the distribution of the nodes.
The constant only depends on the δ in our model. Moreover, our
protocols can self-stabilize even if some of the nodes show arbitrary adversarial behavior. We only require the honest nodes that
are outside a certain range of the adversarial nodes to be placed so
that they can in principle form a single connected component. So
our protocols would even work for very primitive devices in hostile
environments.

b

(b)

Figure 1: Figure (a) shows the notion of transmission range in
terms of cost of communication. Notice that node u can communicate directly with nodes v and a and c but not with nodes b
or w. Figures (b) shows the sensing ranges. When all nodes use
a transmission power P and node w uses a threshold of T , in
this example, node w can always sense transmissions of node a
while it may sense transmissions of node b and can never sense
transmissions of node c.

every node v ∈ V can only cause interference at nodes w with
c(v, w) ≤ ri (P ), and the transmission range has the property that
for every two nodes v, w ∈ V with c(v, w) ≤ rt (P ), v is guaranteed to receive a message from w sent out with a power of P (with
high probability) as long as there is no other node v ∈ V with
c(v, v  ) ≤ ri (P  ) that transmits a message at the same time with a
power of P  .
For simplicity, we assume that the ratio ρ = ri (P )/rt (P ) is a
fixed constant greater than 1 for all relevant values of P . This is
not a restriction because we do not assume anything about what
happens if a message is sent from a node v to a node w within v’s
transmission range but another node u is transmitting a message
at the same time with w in its interference range. In this case, w
may or may not be able to receive the message from v, so any worst
case must be assumed in the analysis. The only restriction we need,
which is important for any overlay network algorithm to eventually
stabilize, is that the transmission range is a sharp threshold. That
is, beyond the transmission range a message cannot be received any
more (with high probability). This is justified by the fact that when
using modern forward error correction techniques, the difference
between the signal strength that allows to receive the message (with
high probability) and the signal strength that does not allow any
more to receive the message (with high probability) can be very
small (less than 1 dB).
Nodes can not only send and receive messages but also perform
physical carrier sensing, which has not been considered before in
models proposed in the algorithms community. Given some sensing threshold T (that can be flexibly set by a node) and a transmission power P , there is a carrier sense transmission (CST) range
rst (T, P ) and a carrier sense interference (CSI) range rsi (T, P )
that grow monotonically with T and P . The range rst (T, P ) has
the property that if a node v transmits a message with power P and
a node w with c(v, w) ≤ rst (T, P ) is currently sensing the carrier
with threshold T , then w senses a message transmission (with high
probability). The range rsi (T, P ) has the property that if a node
v senses a message transmission with threshold T , then there was
at least one node w with c(v, w) ≤ rsi (T, P ) that transmitted a
message with power P (with high probability). More precisely, we
assume that the monotonicity property holds. That is, if transmissions from a set U of nodes within the rsi (T, P ) range cause v to

Constant density dominating set
Given an undirected graph G = (V, E), a subset U ⊆ V is called
a dominating set if all nodes v ∈ V are either in U or have an edge
to a node in U . A dominating set U is called connected if U forms
a connected component in G. The density of a dominating set is the
maximum over all nodes v ∈ U of the number of neighbors that v
has in U .
Given an arbitrary distribution of a set V of nodes in a 2-dimensional Euclidean space, let the graph Qr = (V, Er ) contain all
edges {v, w} with d(v, w) ≤ r. Suppose that we select a maximal independent set U in Qr . Then this is also a dominating set
of constant density because in the 2-dimensional Euclidean space a
node can have at most five neighbors within a distance of r that are
part of an independent set in Qr [3]. Note that a constant density
dominating set is also a constant factor approximation of a minimum dominating set, a well-studied problem in the algorithms and
wireless networking community.
Now, let us consider the graph Gr = (V, Er ) that contains all
edges {v, w} such that c(v, w) ≤ r. Since c(v, w) ≤ (1 + δ)
d(v, w), it follows from [3]:
FACT 1.1. Every node v can have at most five neighbors within
a Euclidean distance of r/(1 + δ) that are part of an independent
set in Gr .

118

Otherwise, there must be a pair v, w ∈ V with c(v, w) ≤ (1 +
δ)d(v, w) ≤ (1 + δ) · r/(1 + δ) = r that are in an independent
set in Gr , a contradiction. Furthermore, because c(v, w) ≥ (1 −
δ)d(v, w), a node can only be connected in Gr to nodes up to a
Euclidean distance of r/(1 − δ). Hence, it is easy to see that for
every node v there is a set Cv of neighbors of v in Gr of constant
size so that for every neighbor w of v in Gr there is a neighbor
w ∈ Cv with d(w, w ) ≤ r/(1+δ). Combining this with Fact 1.1,
we get:

Alzoubi et al. [4] presented the first constant approximation algorithm for the minimum connected dominating set problem in unitdisk graphs with O(n) and O(n log n) time and message complexity, respectively. Cheng et al. [5] proposed a polynomial time approximation scheme for the connected dominating set problem in
unit-disk graphs.
Huang et al. [15] formally analyze a popular algorithm used for
clustering in ad-hoc mobile network scenarios. They show that this
algorithm actually gives a 7-approximation for the minimum dominating set problem in unit-disk graphs, while adapting optimally to
the mobility of the nodes in the network.
Recently, Kuhn et. al. [19] presented a distributed algorithm that
computes a constant factor approximation of a minimum dominating set in O(log2 n) time without needing any synchronization but
it requires that nodes know an estimate of the total number of nodes
in the network. In [29], Parthasarathy and Gandhi also present distributed algorithms to compute a constant factor approximation to
the minimum dominating set. The running time of their algorithm
depends on the amount of information available to the nodes, and
nodes have to know an estimate of the size of the network. Both
papers extend the unit disk model taking into account signal interference.

FACT 1.2. For any independent set in Gr it holds that every
node v in Gr can have at most a constant number of neighbors in
this set, where the constant depends on δ.
Now, recall that any maximal independent set in a graph Gr is
also a dominating set in Gr , and according to the fact above, any
maximal independent set in Gr has a constant density (i.e., every
node only has a constant number of neighbors in that set). Hence,
in order to obtain a dominating set of constant density, it suffices
to design an algorithm that constructs a maximal independent set in
Gr . It turns out that constructing such a set is quite tricky, given the
uncertainties in our model, but we can construct something close to
that so that the following result holds.

Spanners

T HEOREM 1.3. For any desired transmission range r and any
initial situation, the dominating set protocol generates a constant
density dominating set in Gr in O(log4 n) communication rounds,
with high probability.

Suppose that we have a set of nodes V that are distributed in an
arbitrary way in a Euclidean space. For v, w ∈ V , let d(v, w denote
the Euclidean distance between v and w. The goal of the geometric
spanner problem is to find a graph G = (V, E) so that for each pair
of nodes v, w ∈ V there is a path in G from v to w whose length
is at most t · d(v, w) for some fixed constant t. In this case, G is
called a geometric t-spanner of G where t is the stretch factor.
For geometric spanners, several structures have been proposed.
Geometric spanners based on the Delaunay triangulation have been
studied, e.g., [11, 24, 33]. Spanners based on the Yao graph [36]
and Gabriel graph [10] are presented in [25, 35, 32].
For topological spanners, Dubhashi et. al. [9] presented a spanner with logarithmic stretch factor. Alzoubi et. al. [2] presented a
spanner with constant stretch factor of 5 where the protocol is very
similar to ours but uses a high-level model for wireless networks.
Our protocol for selecting gateway nodes also has similarities to
the protocols presented in [34, 11]. However both these papers are
based on high-level wireless models.

Hence, our protocol self-stabilizes within O(log4 n) rounds. Interestingly, this result is only possible because our protocol uses
physical carrier sensing. It is known that if physical carrier sensing is not available and the nodes have no estimate of the size of
the network, then it takes Ω(n) steps on expectation for a single
message transmission to be successful [17] in any protocol.

Constant density spanner
A subgraph H of a graph G is called a (topological) t-spanner of
G if for every pair of nodes v, w in G there is a path in H from v
to w whose length is at most t times the minimum length of a path
from v to w in G. In this case, t is also called the stretch factor of
H.
We then extend the dominating set protocol by additional protocols that connect the nodes in the dominating set via so-called
gateway nodes so that the following result holds.

1.4 Structure of the paper

T HEOREM 1.4. For any desired transmission range r and any
initial situation, the spanner protocol generates a constant density spanner in Gr in O(D log D log n + log4 n) communication
rounds, with high probability, where D is the maximum number of
nodes that are within the transmission range of a node.

We start with an overview of our protocol for the constant density
spanner problem. This protocol consists of three phases. A detailed
description and analysis of phase I is given in Section 3, which also
proves Theorem 1.3. Phases II and III are described and analyzed
in Sections 4.1 and 4.2. The paper ends with possible extensions
and open problems.

All of our protocols can self-stabilize even under adversarial behavior as long as the nodes outside a range of r = Θ(r) of adversarial nodes form a connected component in Gr .

2. OVERVIEW OF SPANNER PROTOCOL
In the following, rt denotes the desired transmission range and
Grt represents the graph with node set V and edge set Ert containing all edges {v, w} with c(v, w) ≤ rt .
Our spanner protocol for Grt consists of 3 phases:

1.3 Previous work
The problem of finding a minimum dominating set has been
shown to be NP-complete even when restricted to unit disk graphs
[7] and, hence, approximation algorithms are of interest. Recent research focused on developing distributed (rather than centralized)
algorithms for finding good approximations of minimum dominating sets in arbitrary graphs (see, for example, [9, 16, 20]). A simple
and elegant distributed approximation algorithm was proposed by
Luby [27].

• Phase I: The goal of this phase is to construct a constant density dominating set in Grt . This is achieved by extending
Luby’s algorithm [27] to our more complex model. Since
the dominating set resulting from phase I may not be connected, we need further phases to obtain a constant density
spanner.

119

Phase II

Phase II

Phase III Phase I

Phase I

Legend:

Phase III

Active Node

Round

Inactive node

Figure 2: Two consecutive rounds of the spanner protocol.

Gateway node
Gateway
Other edges

• Phase II: The goal of this phase is to organize the nodes of the
dominating set of phase I into color classes that keep nodes
with the same color sufficiently far apart from each other.
Only a constant number of different colors is needed for this,
where the constant depends on δ. Every node organizes its
rounds into time frames consisting of as many rounds as there
are colors, and a node in the dominating set only becomes
active in phase III in the round corresponding to its color.

Figure 3: The spanner of the original network.

has at least one node w ∈ U with c(v, w) ≤ rt and at most some
constant number of nodes w ∈ U with c(v, w) ≤ rt .
As mentioned earlier, if we want to reach the goal above in a
sub-linear number of steps without physical carrier sensing, then
a good approximation of log n is needed, where n = |V |. Since
our goal is to arrive at a dominating set without using any prior
knowledge of the network topology, physical carrier sensing has to
be used, which complicates the design as it has uncertainties (see
our model). To handle these uncertainties, we use a distributed
coloring strategy together with two different sensing ranges.
In our protocol, nodes can either be active or inactive. The active
nodes are the candidates for the dominating set. The nodes use two
different sensing thresholds, depending on their state. The sensing
threshold Ta has a CSI range of rt and the sensing threshold Ti has
a CST range of ri . To distinguish between these ranges, we speak
about an aCST/aCSI-range whenever we mean Ta and iCST/iCSIrange whenever we mean Ti .
Each node cuts the time into time frames of k rounds each for
some constant number k that is the same for every node. The
rounds are synchronized among the nodes but we do not require
the frames to be synchronized.
Initially, all nodes are inactive. Afterwards, each node executes
the following protocol in each round. In this protocol, each active
node has exactly one, fixed active round in a frame and a signal is
just a very simple message. Each item represents a communication
step.

• Phase III: The goal of this phase is to interconnect every pair
of nodes in the dominating set that is within a hop distance
of at most 3 in Grt with the help of at most 2 gateway nodes,
using the coloring determined in phase II to minimize interference problems. Constructions using gateway nodes were
also presented in [11, 34] but assuming a higher level model
of wireless networks.
Each phase has a constant number of time slots associated with
it, where each time slot represents a communication step. Phase I
consists of 3 time slots, phase II consists of 4 time slots, and phase
III consists of 4 time slots. These 11 time slots together form a
round of the spanner protocol (see also Figure 2). We assume that
all the nodes are synchronized in rounds, that is, every node starts
a new round at the same time step. As mentioned earlier, this may
be achieved via GPS or beacons.
The spanner protocol establishes a constant density spanner by
running sufficiently many rounds of the three phases. All of the
phases are self-stabilizing. More precisely, once phase I has selfstabilized, phase II will self-stabilize, and once phase II has selfstabilized, phase III will self-stabilize. In this way, the entire algorithm can self-stabilize from an arbitrary initial configuration.
It is not difficult to see that our spanner protocol results in a 5spanner of constant density: Consider any pair of nodes s and t in
Grt and let p = (s = v0 , v1 , . . . , vk = t) be the shortest path from
s to t in Grt . Then we can emulate p via the connected dominating
set by first going to a leader $0 of s, then (possibly via gateway
nodes) to a leader $1 of v1 , then to a leader $2 of v2 , and so on,
until we reach a leader $k of t, and finally to t. The length of this
path is at most 3k + 2 ≤ 5k for every k ≥ 1. Combining this with
the time bounds shown for the various phases in the sections below
results Theorem 1.4.
An important feature of our protocol is that all messages sent
are of constant length and the nodes only have to have a constant
amount of storage, irrespective of the density of the network. We
just need the assumption that a storage unit is large enough to store
the ID of a node. Hence, our protocol can be used with very simple
devices such as sensors.

3.

1. If v is active and in its active round, then v sends out an
ACTIVE signal.
If v is inactive and v did not sense any ACTIVE signal for the
last k rounds using a sensing threshold of Ta , v senses with
threshold Ti , and if it does not sense anything, it becomes
active and declares the current round number as its active
round. If v did sense some ACTIVE signal in one of the
last k rounds, it just performs sensing with threshold Ta and
records the outcome.
2. If v is active and is in its active round, then v sends out a
LEADER message containing its ID with some fixed probability p (determined later). If v decides not to send out
a LEADER message but it either senses a LEADER message with threshold Ta or receives a LEADER message, v
becomes inactive.

PHASE I: DOMINATING SET

Let P be some fixed transmission power with transmission range
rt and interference range ri for which we want to construct a dominating set of constant density. That is, given any set of nodes V ,
we want to find a subset U ⊂ V of nodes so that every node v ∈ V

In the following, let Hr,k = (V, E) be an undirected graph that
contains an edge between two nodes v and w if and only if v and w

120

are active and use the same active round (or color) k and c(v, w) ≤
r. A node v is called a leader if it is active and there is no other
active node w of the same color with c(v, w) ≤ rt . Since inactive
nodes sense with an iCST range of ri before they become active,
none of the inactive nodes w with c(v, w) ≤ ri will become active
in the active round of v. Hence, we get:

L EMMA 3.7. At any time step t, Hrt ,k consists of connected
components of active nodes where all nodes in a connected component were reactivated at the same round.
P ROOF. Suppose that there are two adjacent nodes, v and w, in
some active, connected component in Hrt ,k that were not reactivated at the same round. W.l.o.g. let v be the first node that became
active. Then w could not have become active because v is in its
iCST range, leading to a contradiction.

FACT 3.1. At any time, the set of leader nodes forms an independent set in Hrt ,k that is disconnected from all other active
nodes in Hrt ,k .

For the next lemma, given an active node v, we define ls(v) as
the bit sequence in which the ith bit is 1 if and only if v sent out a
LEADER message in round i since it joined its current component.
ls(v)i denotes the first i bits of ls(v).

In addition, a leader node uses an aCSI range of rt and will therefore not be affected by nodes outside of a range of rt . Hence, we
arrive at the following fact.

L EMMA 3.8. Every connected component of active nodes in
Hrt ,k needs at most O(log n) rounds, w.h.p., until every node in
it either becomes inactive or becomes a leader.

FACT 3.2. Once a node becomes a leader, it will stay a leader
as long as the cost function c does not change.
Furthermore, an inactive node v can only become active if in the
previous k rounds there was no active node w with c(v, w) ≤ rs ,
where rs is the CST range for threshold Ta , because otherwise v
would have sensed the ACTIVE signal of w in one of these rounds.
Hence, we also get:

P ROOF. Consider any connected component C of active nodes in
Hrt ,k at some time point t0 , and let C  be the union of the connected components of active nodes in Hrt ,k that have at least one
node within the interference range of a node in C.
Whenever a node becomes active after t0 , it cannot interfere with
the remaining nodes in C because it will be guaranteed to be outside of their interference range (and therefore also of their aCSI
range). Hence, we only need to focus on the remaining active nodes
in C ∪ C  .
We prove the lemma in two steps. First, we show that it only
takes O(log n) rounds, w.h.p., until there are no two active nodes
v and w in C ∪ C  where w is within the aCST range of v or vice
versa. Then we show that it only takes O(log n) further rounds,
w.h.p., until there are no two active nodes v and w in C that are
within the transmission range of each other.
The probability that for any two fixed, active nodes v and w it
holds that ls(v)i = ls(w)i is equal to pi . Hence, if i = c log1/p n,
then the probability that there are two nodes v and w in C ∪ C
with ls(v)i = ls(w)i that are within their aCST range is at most
n2 /pc log1/p n = n2−c . Thus, the probability that after c log1/p n
rounds there are still two nodes within the aCST range in C ∪ C
that are both active is polynomially small in n for c > 2.
Hence, after O(log n) rounds, there can only be at most some
constant number d of active nodes within the interference range of
any active node in C, where d depends on the ratio between the
interference range and the aCST range. Thus, when choosing p =
1/d, then the probability that exactly one of the active nodes within
the interference range of an active node v in C is transmitting a
LEADER message in a round is Θ(p). Therefore, it takes at most
O((1/p) log 1/p n) = O(d log d n) rounds until for every node v in
C that is still active there is no other active node in the transmission
range of v, with high probability.

FACT 3.3. There cannot be two leaders v and w with c(v, w) ≤
rs .
Since rt /rs is a constant, the facts above and Fact 1.2 imply that
the leaders must form a set of constant density in Grt . On the other
hand, the following lemma is true.
L EMMA 3.4. In any situation in which all active nodes are leaders but the leaders do not form a dominating set with respect to Grt ,
at least one inactive node will eventually become active.
P ROOF. From Facts 1.2 and 3.3 it follows that there can be at
most some constant number k of leaders within the iCSI range of
any node. Hence, if k > k then for every inactive node that does
not yet have a leader within its transmission range there must be at
least one round s in which there is no leader within its iCSI range.
Because the inactive node will continue to explore potential active
rounds in a round-robin fashion as long as it senses a transmission
with threshold Ti , it will eventually arrive at round s and become
active (unless some other inactive node close to it becomes active
before that).
On the other hand, the following result is easy to check.
L EMMA 3.5. Every connected component of active nodes in
Hrt ,k results in at least one leader.
Thus, the algorithm eventually arrives at a situation where there
is no inactive node that does not have a leader within its transmission range. At that point, the leaders must form a superset of a
maximal independent set in Grt . Thus, according to Facts 1.2, 3.3,
and 3.2 the leaders eventually form a static dominating set of constant density. It remains to prove how much time is needed to reach
such a state.

Next we give a lower bound on the number of leaders that emerge
from a connected component of active nodes in Hrt ,k . For the rest
of the proof, we assume w.l.o.g. that rt = 1 and ri = 1 + α for
some constant α > 0. We define the area covered by an active node
v as the area that is within the transmission range of v.

T HEOREM 3.6. If all nodes are initially inactive, after O(log4 n)
rounds of the algorithm, the leaders form a static dominating set of
constant density with respect to Grt , with high probability.

L EMMA 3.9. For any time step in which the currently existing
connected components of active, non-leading nodes cover an area
of A = Ω(log3 n), the number of leaders emerging from these
components is Ω(A/ log2 n), w.h.p.

P ROOF. The next two lemmata state important properties of connected components of active nodes in Hrt ,k . Notice that a leader
always represents a connected component by itself.

P ROOF. Consider any set C of connected components of active,
non-leading nodes that cover an area of A. Given any node v, let

121

Γ(v) denote the set of nodes w ∈ C with c(v, w) ≤ 1 and let
γ(v) = |Γ(v)|. Let H be the directed graph resulting from C by
connecting two active nodes v and w by an edge (v, w) if and only
if c(v, w) ≤ 1 and γ(w) ≥ 2γ(v). A node is called a sink if it
does not have any outgoing edges. H has the following important
property:

for some constant c, with high probability. Thus, after k stages of
O(log n) rounds each, the area covered by the unfinished nodes is
at most
„
«k
2
c
A0 ≤ e(c·k)/ log n A0 ,
1−
2
log n
with high probability. The right hand side is less than log3 n if k ≥
(log A0 )(log2 n)/c. Once an area of size O(log3 n) is reached, it
follows from Lemma 3.5 that it takes only O(log3 n) more stages
of O(log n) rounds each until there are no unfinished nodes any
more. Since A0 = O(n), it follows that the total runtime needed
for the set of active nodes to stabilize is O(log4 n).

C LAIM 3.10. Every node v in H has a directed path to a sink
s of length at most log n.
P ROOF. First of all, H cannot contain a directed cycle. Thus,
every directed path must eventually end in a sink. Suppose now
that some node v has a directed path p to a sink s of length more
than log n. Because of the definition of the edges, it follows that
γ(s) ≥ 2k · γ(v) > n · γ(v), which cannot happen because there
are only n nodes in the system.

The dominating set algorithm can be easily extended so that it
self-stabilizes [8] and it is robust against malicious behavior. Selfstabilization means that it can recover from any initial configuration.

Recall that our cost function must satisfy c(v, w) ∈ [(1 − δ)
d(v, w), (1 + δ)d(v, w)]. Thus, if we consider disks of radius (1 +
log n)/(1 − δ), around the sinks of H, then the complete area A
of active, non-leading nodes is covered. To extract out of all sinks
a set of sinks useful for our analysis below, we consider these sinks
one by one. For each sink s that has not already been eliminated,
eliminate all sinks s that are of distance at most 4 from s and add
s to a set S. At the end, we arrive at a set S of sinks of pairwise
distance at least 4 such that disks of radius r = (5 + log n)/(1 − δ)
around these sinks cover the entire area A. Thus, the area A can be
decomposed into areas of size at most a = πr2 each containing a
sink in S, and therefore |S| ≥ |A|/a. It is not difficult to show that
these sinks have the following property:

3.1 Self-stabilization
An extra rule is necessary to provide self-stabilization because
if the protocol above starts in a configuration violating Fact 3.3, it
may not succeed in establishing a dominating set.
Consider adding a third step to each round of the protocol above.
In this step, every active node sends a leader message with probability p and a transmission power so that its transmission range is
only equal to the aCST range. Adding now the rule that whenever
an active node receives a leader message in that step for a round
different from its active round, then it becomes inactive, we do not
have to assume anything about how the nodes are initially activated
in order to satisfy Fact 3.3. So we get:

C LAIM 3.11. For any sink s ∈ S, the expected number of active nodes in Γ(s) that become a leader is Θ(1).

C OROLLARY 3.12. For any initial situation, the extended protocol needs at most O(log4 n) rounds to arrive at a static dominating set of constant density with respect to Grt , w.h.p.

number
For any sink s, let the random variable Xs denote the P
of active nodes in Γ(s) that become leaders and let X = s Xs .
From Claim 3.11 it follows that E[X] ≥ α|S| for some constant
α > 0, and because the distance between any two sinks in S is
at least 4, the Xs variables are independent. Thus, we can use
Chernoff bounds to obtain
2

Pr[X ≤ (1 − ,)α|S|] ≤ e−

3.2 Robustness
Our dominating set algorithm is also highly robust against adversarial nodes. For any node v, let the r1 ⊕ r2 -range of v be defined
as the union of the r2 -ranges of all the nodes within the r1 -range
of v. Given any distribution of nodes, let A be the area covered
by the rii ⊕ rt -ranges of adversarial nodes, where rii is the iCSI
range of a node. Because in our protocol adversarial nodes can directly influence only nodes within their iCSI range, nodes beyond
the rt range of these nodes can only have leaders outside of A, and
leaders outside of A will stay leaders forever, one can show:

α|S|/2

for any , > 0. This is polynomially small if , = 1/2 and |S| =
Ω(log n) is sufficiently large. Hence, in this case,
»
–
α|A|
Pr [X ≤ α|S|/2] = Pr X ≤
2πr 2
is polynomially small, which completes the proof of the lemma.

C OROLLARY 3.13. If the honest nodes outside A are connected
in Grt , then after O(log4 n) rounds, the active honest nodes outside A form a dominating set of constant density with respect to
Grt , w.h.p.

Now, let us call a node unfinished if it is active but not a leader
or it is inactive and it does not have a leader within its transmission
range. We know that an unfinished node is either active or must
have at least one node within its iCSI range, rii , that was active
within the previous k rounds (because otherwise it would become
active). Hence, when drawing disks of radius rii /(1 − δ) around
all nodes that were active in at least one of the k previous rounds,
the entire area that the nodes can transmit messages to is covered.
Let A0 be the area covered by the transmission ranges of all the
nodes in the system. If A0 = Ω(log 3 n), then Lemma 3.8 and
Lemma 3.9 imply that after O(log n) rounds, the area covered by
the unfinished nodes is at most
„
«
c
A0
= 1−
A0 − c ·
A0
2
2
log n
log n

4. CONSTANT DENSITY SPANNER
In the next two subsections, we describe phases II and III in detail. We use the following notation. The constant d1 refers to the
number of active nodes that are within the interference range ri of
any node. The constant d2 refers to the number of active nodes that
are within the ri ⊕ ri -range of any node, and the constant g refers
to the maximum number of required gateway connections for any
active node. Finally, D refers to the density of the network, i.e.
the maximum number of nodes within the transmission range of a
node.

122

4.1 Phase II - Distributed Leader Coloring

Without the two types of signals BUSY and COLLISION and
the two different sensing thresholds the coloring achieved may fail
to be ri ⊕ ri distinct. For any active node $ in volatile state, the
threshold Tv and the BUSY signal helps to identify the presence of
active nodes in owner state with the same active round so that active nodes in owner state without another active node in owner state
within the rii ⊕ rii -range will also keep this property in the future
and are therefore safe from becoming volatile again. The COLLISION signal is necessary to resolve conflicts among close by active
nodes in owner state with the same active round, which can happen if volatile nodes become an owner in the same round, or this
may be part of the initial state when looking at self-stabilization. In
any case, the monotonicity assumption on the sensing in our model
is important to make sure that there will either never be a conflict
among owner nodes or immediately a conflict when a collision is
detected.

Similar to phase I, each node organizes the time into time frames
consisting of cd1 rounds for some constant c that is the same for
every node. Also here, the rounds are synchronized but frames do
not have to be synchronized among the nodes. We again assign
active nodes to distinct rounds using a coloring mechanism. While
the coloring in phase I was done with respect to Grt , we now need
a coloring of the active nodes with respect to Gri ⊕ri , that is, we
need the active nodes to be at least ri ⊕ ri apart in order to receive
the same color.
Every active node from phase I tries to own one of the rounds.
An active node u is said to own a round if no other active node
within its ri ⊕ ri range is using that round. Active nodes are in
one of the states {owner, volatile}. An active node is in owner
state if it already owns a round and is in volatile state if it is still
trying to own a round. Active nodes in owner state always send
their ID in the first time slot of their round. Initially, every active
node is volatile. Active nodes in volatile state choose an active
round from the cd1 possible rounds uniformly at random. Active
nodes in owner state use a sensing threshold To with CST range ri
and active nodes in volatile state use a sensing threshold Tv with a
CST range being equal to the CSI range of To , rii .
Active nodes do the following repeatedly. Every time a node reactivates, it sets its time stamp to 0. This time stamp is used by
active nodes in Phase III to compare entries.

4.2 Phase III - Gateway Discovery
In this section we describe the protocol for phase III. The goal of
this phase is for the active nodes from Phase I to discover gateway
connections to other leaders that are within a hop distance of at
most 3 in Grt .
During this phase, the active nodes use an aCST range of rt .
The active nodes use the rounds reserved in phase II to achieve
interference-free communication with the inactive nodes within their
transmission range. Each round consists of four time slots for communication in phase III, where each time slot represents a communication step as shown in Figure 2. In the first time slot, inactive
nodes send CLIENT messages and in the second time slot the active node sends a response accordingly; in the third and fourth time
slots, an inactive node u may broadcast to its (active and inactive)
neighbors all the information it has regarding possible gateways between the leader owning the reserved round and other leader nodes
it has heard about. For simplicity, we assume that all active nodes
are reactivated at the same time and hence that we can directly compare the time stamps with respect to the different active nodes. In
reality, each inactive node u would keep track of the offsets of the
(constant number of) time stamps it receives (in the corresponding
slots allocated to the different leaders in phase II) and use these
offsets when comparing time stamps from different leaders.
We first describe the data structures that are maintained during
this phase. Each inactive node u maintains a cache, called Pu ,
which has entries of the form ($, v, t ) where $ is an active node, v
is an inactive node (with u = v possibly), and t is the time stamp
with respect to $ at which the entry ($, v) is added to Pu . When
comparing entries in the cache, a ∗ acts as a wild card that matches
any value. The operation enqueue($, v, t ) on Pu is used to add the
new entry ($, v, t ) to Pu . Enqueue performs the following checks
before actually adding the new entry to Pu . When adding a new entry ($, v, t ), any entry of the form ($, ∗, t ) with t < t is evicted.
If no such entry exists and Pu is full, then the least recently added
entry (∗, ∗, t ), that is t = min{t|t < t and (∗, ∗, t) ∈ Pu }, is
evicted to make room for the new entry. The cache Pu has space
enough to store a constant, d2 , number of entries. Inactive nodes
also maintain a state that is either awake or asleep with respect to
each active node that is within their transmission range. The asleep
nodes just listen the channel and becomes awake when they receive
a FREE or a ACK message.
Each active node $ maintains a list, called G , and each entry
in G contains two fields. The first field has gateways represented
as quadruples of the form ($, u, v, $ ) where $ = $ and u = v
possibly, $ is an active node and u, v are inactive nodes. The second field contains the time stamp t at which the entry was added

1. Every active node in owner state that is in its active round
sends out a LEADER message containing its ID and its current time stamp and increases its time stamp by one afterwards.
2. Every active node in owner state that is in its active round decides with probability 1/2 to send out an OWNER message
either in the first or second substep of step 2.
3. Every inactive node that sensed a LEADER message with
threshold Tv sends out a BUSY signal. Every active node in
volatile state that senses a BUSY signal in its active round
chooses a new active round uniformly at random.
4. Every inactive node that sensed OWNER messages in both
substeps of step 2 with threshold To sends out a COLLISION
signal.
If an active node in owner state senses a COLLISION signal and sent an OWNER message in the second substep, it
changes into volatile state and chooses a new active round
uniformly at random.
If an active node in volatile state did not sense a BUSY or
COLLISION signal in its active round, it becomes an owner.
It is not difficult to show the following result:
T HEOREM 4.1. Once a stable set of active nodes is available,
it holds: If c ≥ 4, then all active nodes will be in owner state after
O(log n) rounds of the protocol, w.h.p.
The theorem implies that after O(log n) rounds, all active nodes
have chosen rounds so that for any two active nodes $ and $ with
the same round and any inactive node v within the interference
range of $, $ is outside of the interference range of v. Hence, $
can transmit messages to nodes within its transmission range without interference problems, and these nodes can transmit messages
to $ without causing interference problems at $. Both properties are
important for phase III to work correctly.

123

to G . The operation enqueue on G is used to add a new entry
(($, u, v, $ ), t ) to G . Before adding the new entry (($, u, v, $ ), t)
to G , any entry of the form (($, ∗, ∗, $ ), t ) is evicted from G for
t < t. If the list G is full, then the entry corresponding to t such
that t = min{t |t < t and (($, ∗, ∗, $ ), t ) ∈ G }, that is the
entry of G with smallest time stamp, is deleted to make room for
the new entry. (Similar to enqueue on Pu for inactive node u). The
list G has space enough to store a constant, g, number of entries.
In the following, $ refers to the ID of the active node that owns
the current slot and u is an inactive node that received the ID message from $ and the state of u is with respect to $.

P ROOF. We prove the convergence of phase III to a set of valid
gateway connections in O(D log n log D) rounds after phase I and
phase II have reached a stable state. Since, at that point the active
nodes have reserved rounds that are distinct within the ri ⊕ri range,
we can treat the actions of active nodes independent of each other.
Let (v, $) be an inactive node-active node pair such that v has to
send a CLIENT message to $. Node v has at most O(D) inactive
nodes in its interference range sending a CLIENT message to some
leader node. If more than one node in awake state, with respect to
$, decides to send a CLIENT message, then $ will send a collision
message. Since the collision message will be received by the inactive nodes, within rt range of $, awake nodes that decided not to
send a CLIENT message to $ in the previous slot will go to asleep
state.
Consider time to be partitioned into groups of consecutive rounds
such that each group ends with a round where the active node $
sends either an ACK message or a FREE message. (A group ending with an ACK message signifies a successful group and a group
ending with a FREE message is a failed group). Notice that at the
end of every group, whether successful or not, all the inactive nodes
within the rt range of $ go to awake state (by step 2 of the protocol).
It is not difficult to show that the expected number of rounds
in each group, successful or failed, is O(log D) and any group is
successful with constant probability. Due to symmetry reasons any
inactive node is equally likely to be send a CLIENT message in a
successful group. Thus, during any successful group, for a given
pair (v, $) ,

1. If u is awake then u sends out a CLIENT message of the
form CLIENT, $, u with probability 1/2.
2. Node $ responds with a reply in the next time slot which can
be of three forms. If $ receives a CLIENT message from node
u then $ adds u to N by calling enqueue(u) and also sends
an acknowledgment containing the ID of u as $, ACK, u.
If $ only senses a busy channel but does not receive any message, then $ sends a collision message of the form $, COL−
LISION. If $ does not receive any message and also does
not sense a busy channel, the $ sends a free channel message
of the form $, FREE.
If u is awake and decided not to send a CLIENT message
in the previous slot and receives a collision message then u
goes to asleep state. If u is asleep and receives a free channel
or an acknowledgement message then u becomes awake.

Pr[ v sends a CLIENT message successfully to $] ≥ 1/cD

3. If u is awake and receives an acknowledgment containing the
ID of u then u will store ($, u, t ) in Pu , where t is the current time stamp associated with $, by calling enqueue($, u, t ).

for some constant c > 1.
Using Chernoff bounds, for any given pair (v, $) the probability
that it takes more than Dk groups so that v sends a CLIENT message to $ successfully will be polynomially small for k = O(log n).
It can also be shown that each group has O(log D) rounds not only
on expectation but also with high probability. Thus any node v
requires at most O(D log n log D) rounds to send a CLIENT message to $ successfully w.h.p.
To proceed further, let $ and $ be active nodes, with d($, $ ) ≤ 3
and let ($, u, v, $ ) be a gateway between $ and $ . Notice that once
$ and $ receive CLIENT messages from u and v respectively, $
and $ can establish a gateway connection between them as successful CLIENT messages are followed by ADV and GATEWAY
messages in the next time slots reserved for this phase. Without
loss of generality, we assume that u sends the ADV message that v
receives and adds the entry ($, u, v, $ ) to the GATEWAY message
that v sends. Along with Fact 4.2 it holds that during every group
the probability that u gets an ACK message and sends the ADV
message is ≥ 1/c D for a constant c > 1. And similarly the probability that v gets an ACK message from $ and sends a GATEWAY
message is ≥ 1/c D. Thus, in each group,

Node u also deletes any entries of the form (∗, $) from Pu
(since $ is no longer inactive). Node u then broadcasts, in the
third time slot, a message ADV, $, u, t  to its neighbors.
4. Node u builds one GATEWAY message containing all quintuples of the form (($, u, vj , $j ), t) for each j such that $j =
$ with ($j , vj , tj ) ∈ Pu , where t = min{t , tj }, and sends
the message to its neighbors. The GATEWAY message is
sent in the fourth time slot.
If v is not active and received an ADV message ADV, $, u, t 
then it calls enqueue($, u, t ) on Pv . Node v also deletes any
entries of the form (u, ∗) or (∗, $) from Pv (as u is no longer
an active node nor is $ inactive).
If $ is active and receives a GATEWAY message containing
(($, u, v, $ ), t), then $ stores (($, u, v, $ ), t) in G by calling
enqueue(($, u, v, $ ), t).
Before we analyze the protocol, we start with the following fact,
which follows from the observation that a necessary condition for
an inactive node u to transmit in step 3 and step 4 is to receive an
ACK from an active node in step 2.

Pr[$ and $ discover a gateway connection] ≥ 1/c D
for some constant c > 1. Using calculations similar to the above,
it holds that $ and $ can establish a gateway connection in O(D log n
log D) rounds w.h.p.

FACT 4.2. During steps 3 and 4 of the protocol there are at
most a constant number d1 of nodes that are transmitting any message.

Note that, after phase II stabilizes and after we let phase III run
for O(D log n log D) time steps, time stamping will be enough to
guarantee that we will always keep information received at a leader
node $ about a valid gateway connection between leader nodes $
and $ , if at least one such connection exists (since we have at most
a constant number of leader nodes within cost 3rt from any given
leader node, and since we have at most a constant number of leader

Using this fact, we can prove the following theorem.
T HEOREM 4.3. In O(D log n log D) rounds, each active node
learns about a gateway to each of the currently active nodes in its
3-neighborhood with respect to Grt , w.h.p.

124

nodes adjacent to any inactive node, constant size Pu and G lists at
inactive nodes u and active nodes $ respectively will suffice).

5.

[17] T. Jurdzinski and G. Stachowiak. Probabilistic algorithms for the
wakeup problem in single-hop radio networks. In Proc. of the 13th
Int. Symposium on Algorithms and Computation, pages 535–549,
2002.
[18] J. Keil and W. Gutwin. The delaunay triangulation closely
approximates the complete euclidean graph. In 1st Workshop on
Algorithms and Data Structures (WADS), 1989.
[19] F. Kuhn, T. Moscibroda, and R. Wattenhofer. Radio network
clustering from scratch. In European Symposium on Algorithms
(ESA), 2004.
[20] Fabian Kuhn and Roger Wattenhofer. Constant-time distributed
dominating set approximation. In Proc. of the 22nd ACM Symposium
on Principles of Distributed Computing (PODC), pages 25–32, 2003.
[21] F. Kuhn, R. Wattenhofer, Y. Zhang and A. Zollinger. Geometric
ad-hoc Routing: Of theory and practice. In Proc. of the 22nd ACM
Symposium on Principles of Distributed Computing (PODC), 2003.
[22] Fabian Kuhn and Roger Wattenhofer and Aaron Zollinger Ad-hoc
networks beyond unit disk graphs. In Proc. of the 2003 Joint
Workshop on Foundations of Mobile Computing (DIALM-POMC),
pages 69–78, 2003.
[23] J. Kuruvila, A. Nayak, and I. Stojmenovic. Hop count optimal
position based packet routing algorithms for ad hoc wireless
networks with a realistic physical layer. In 1st IEEE International
Conference on Mobile Ad-hoc and Sensor Systems (MASS), 2004.
[24] X.-Y. Li, G. Calinescu, and P.-J. Wan. Distributed construction of
planar spanner and routing for ad hoc wireless networks. In IEEE
INFOCOM, pages 1268–1277, 2002.
[25] X.-Y. Li, P.-J. Wan, and Y. Wang. Power efficient and sparse spanner
or wireless ad hoc networks. In IEEE International Conference on
Computer Communications and Networks (ICCCN), pages 564–567,
2001.
[26] X.-Y. Li, P.-J. Wan, Y. Wang, and O. Frieder. Sparse power efficient
topology for wireless netowrks. In IEEE Hawaii International
Conference on System Sciences (HICSS), 2002.
[27] M. Luby A simple parallel algorithm for the maximal independent
set problem. In Proc. of the 17th ACM Symposium on Theory of
Computing (STOC), pages 1–10, 1985.
[28] IEEE P802.11a/D7.0-1999. Wireless lan medium access control
(mac) and physical layer (phy) specification: High speed physical
layer in the 6 ghz band. IEEE, New York, July 1999.
[29] S. Parthasarathy and R. Gandhi. Distributed algorithms for coloring
and domination in wireless ad hoc networks. In Proc. of FSTTCS,
2004.
[30] L. Quin and T. Kunz. On-demand routing in MANETs: The impact
of a realistic physical layer model. In Proc. of the 2nd International
Conference ADHOC-NOW, pages 37–48, 2003.
[31] T.S. Rappaport. Wireless Communications: Principle and Practice.
Prentice Hall, 1996.
[32] W-Z. Song, Y. Wang, ,X-Y. Li and O. Frieder Localized algorithms
for energy efficient topology in wireless ad hoc networks. In Proc. of
the 5th ACM Symposium on Mobile Ad Hoc Networking and
Computing (MobiHoc), pages 98–108, 2004.
[33] Y. Wang and X.-Y. Li. Localized construction of bounded degree and
planar spanner for wireless ad hoc networks. In Proc. of the 2003
Joint Workshop on Foundations of Mobile Computing
(DIALM-POMC), pages 59–68, 2003.
[34] Y. Wang and X.-Y. Li. Geometric Spanners for Wireless Ad Hoc
Networks. In IEEE Trans. on Parallel and Distributed Systems, pages
408–421, vol. 14, 2003.
[35] R. Wattenhofer, J. Li Li, P. Bahl, and Y.-M. Wang. Distributed
topology control for wireless multihop ad-hoc networks. In IEEE
INFOCOM ’01, pages 1388–1397, 2001.
[36] Andrew Chi Chih Yao. On constructing minimum spanning trees in
k-dimensional spaces and related problems. SIAM Journal on
Computing, 11(4):721–736, 1982.

FUTURE WORK

We feel that our model provides a realistic model for wireless
communication and it would therefore be highly interesting to see
how algorithms in our model perform in practice. Also, it would
be very interesting to develop protocols for other problems on top
of our wireless model (e.g. broadcasting and service discovery), in
particular, protocols that can self-stabilize under adversarial influence.

6.

REFERENCES

[1] M. Adler and C. Scheideler. Efficient communication strategies for
ad hoc wireless networks. Theory of Computing Systems,
33:337–391, 2000.
[2] K. Alzoubi, X.-Y. Li, Y. Wang, P.-J. Wan and O. Frieder Geometric
spanners for wireless ad hoc networks. In IEEE Transactions on
Parallel and Distributed Systems, pages 408-421, 2003.
[3] K. Alzoubi P.-J. Wan and O. Frieder. Distributed construction of
connected dominating set in wireless ad hoc networks. In IEEE
Infocom ’02, 2002.
[4] K. Alzoubi, P.-J. Wan, and O. Frieder. New distributed algorithm for
connected dominating set in wireless ad hoc networks. In Proc. of the
34th Annual Hawaii International Conference on System Science
(HICSS-35), 2002.
[5] X. Cheng, D. Huang, W. Li, W. Wu, and D. Z. Du. A
polynomial-time approximation scheme for the minimum-connected
dominating set in ad hoc wireless networks. Networks: An
International Journal, 42, 2003.
[6] S. Choi. IEEE 802.11e MAC-level FEC performance evaluation and
enhancement. In GLOBECOM ’02, 2002.
[7] B. N. Clark, C. J. Colbourn, and D. S. Johnson. Unit disk graphs.
Discrete Mathematics, 86:165–177, 1990.
[8] E. W. Dijkstra. Self stabilization in spite of distributed control.
Communications of the ACM, 17:643–644, 1974.
[9] D. Dubhashi, A. Mei, A. Panconesi, J. Radhakrishnan, and
A. Srinivasan. Fast distributed algorithms for (weakly) connected
dominating sets and linear-size skeletons. In Proc. of the 14th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA-03), pages
717–724, 2003.
[10] K.R. Gabriel and R.R. Sokal. A new statistical approach to
geographic variation analysis. Systematic Zoology, 18:259–278,
1969.
[11] J. Gao, L.J. Guibas, J. Hershberger, L. Zhang, and A. Zhu. Geometric
spanner for routing in mobile networks. In Proc. of the 2nd ACM
Symposium on Mobile Ad Hoc Networking and Computing
(MobiHoc ’01), pages 45–55, 2001.
[12] P. Gupta and P.R. Kumar. The capacity of wireless networks. IEEE
Transactions on Information Theory, IT-46(2):388–404, 2000.
[13] P. Gupta and P.R. Kumar. Internets in the sky: The capacity of three
dimensional wireless networks. Communications in Information and
Systems, 1(1):33–50, 2001.
[14] P. Gupta and P.R. Kumar. Towards an information theory of large
networks: An achievable rate region. In IEEE International
Symposium on Information Theory (ISIT 2001), 2001.
[15] H. Huang, A. W. Richa, and M. Segal. Approximation algorithms for
the mobile piercing set problem with applications to clustering in
ad-hoc networks. ACM Baltzer Journal on Mobile Networks and
Applications (MONET), pages 141–149, April 2004.
[16] L. Jia, R. Rajaraman, and R. Suel. An efficient distributed algorithm
for constructing small dominating sets. In Proc. of the 20th ACM
Symposium on Principles of Distributed Computing (PODC), 2001.

125

Report from Dagstuhl Seminar 16271

Algorithmic Foundations of Programmable Matter
Edited by

Sándor Fekete1 , Andréa W. Richa2 , Kay Römer3 , and
Christian Scheideler4
1
2
3
4

TU Braunschweig, DE, s.fekete@tu-bs.de
Arizona State University – Tempe, US, aricha@asu.edu
TU Graz, AT, roemer@tugraz.at
Universität Paderborn, DE, scheideler@upb.de

Abstract
This report documents the program and the outcomes of Dagstuhl Seminar 16271 “Algorithmic
Foundations of Programmable Matter”, a new and emerging field that combines theoretical work
on algorithms with a wide spectrum of practical applications that reach all the way from smallscale embedded systems to cyber-physical structures at nano-scale.
The aim of the Dagstuhl seminar was to bring together researchers from the algorithms
community with selected experts from robotics and distributed systems in order to set a solid base
for the development of models, technical solutions, and algorithms that can control programmable
matter. Both communities benefited from such a meeting for the following reasons:
Meeting experts from other fields provided additional insights, challenges and focus when
considering work on programmable matter.
Interacting with colleagues in a close and social manner gave many starting points for continuing collaboration.
Getting together in a strong, large and enthusiastic group provided the opportunity to plan
a number of followup activities.
In the following, we provide details and activities of this successful week.
Seminar July 3–8, 2016 – http://www.dagstuhl.de/16271
1998 ACM Subject Classification C.2.4 [Computer Communications Networks] Distributed Systems, F.1.2 [Computation by Abstract Devices] Modes of Computation, F.2.2 [Analysis of
Algorithms and Problem Complexity] Nonnumerical Algorithms and Problems, I.2.9 [Artificial Intelligence] Robotics, I.2.9 [Artificial Intelligence] Robotics – Modular Reconfigurable
Robots, I.6 [Simulation and Modeling] General
Keywords and phrases distributed algorithms, distributed systems, programmable matter, robotics, self-organization
Digital Object Identifier 10.4230/DagRep.6.7.1
Edited in cooperation with Thim Strothmann

Except where otherwise noted, content of this report is licensed
under a Creative Commons BY 3.0 Unported license
Algorithmic Foundations of Programmable Matter, Dagstuhl Reports, Vol. 6, Issue 7, pp. 1–14
Editors: Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler
Dagstuhl Reports
Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany

2

16271 – Algorithmic Foundations of Programmable Matter

1

Executive Summary

Sándor Fekete
Andréa W. Richa
Kay Römer
Christian Scheideler
License

Creative Commons BY 3.0 Unported license
© Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

Programmable matter refers to a substance that has the ability to change its physical
properties (shape, density, moduli, conductivity, optical properties, etc.) in a programmable
fashion, based upon user input or autonomous sensing. The potential applications are endless,
e.g., smart materials, autonomous monitoring and repair, or minimal invasive surgery. Thus,
there is a high relevance of this topic to industry and society in general, and much research
has been invested in the past decade to fabricate programmable matter. However, fabrication
is only part of the story: without a proper understanding of how to program that matter,
complex tasks such as minimal invasive surgery will be out of reach. Unfortunately, only
very few people in the algorithms community have worked on programmable matter so far,
so programmable matter has not received the attention it deserves given the importance of
that topic.
The Dagstuhl seminar “Algorithmic Foundations of Programmable Matter” aimed at
resolving that problem by getting together a critical mass of people from algorithms with a
selection of experts from distributed systems and robotics in order to discuss and develop
models, algorithms, and technical solutions for programmable matter.
The aim of the proposed seminar was to bring together researchers from the algorithms
community with selected experts from robotics and distributed systems in order to set a solid
base for the development of models, technical solutions, and algorithms that can control
programmable matter. The overall mix worked quite well: researchers from the more practical
side (such as Julien Bourgeois, Nikolaus Correll, Ted Pavlic, Kay Römer, among others)
interacted well with participants from the theoretical side (e.g., Jennifer Welch, Andrea
Richa, Christian Scheideler, Sándor Fekete, and many others). Particularly interesting to see
were well-developed but still expanding areas, such as tile self-assembly that already combines
theory and practice (with visible and well-connected scientists such as Damien Woods, Matt
Patitz, David Doty, Andrew Winslow, Robert Schweller) or multi-robot systems (Julien
Bourgeois, Nikolaus Correll, Matteo Lasagni, André Naz, Benoît Piranda, Kay Römer).
The seminar program started with a set of four tutorial talks given by representatives
from the different sets of participants to establish a common ground for discussion. From the
robotics and distributed system side, Nikolaus Correll and Julien Bourgeois gave tutorials
on smart programmable materials and on the claytronics programmable matter framework
respectively. From the bioengineering side, Ted Pavlic gave a tutorial on natural systems
that may inspire programmable matter. From the algorithmic side, Jacob Hendricks gave a
tutorial on algorithmic self-assembly. In the mornings of the remaining four days, selected
participants offered shorter presentations with a special focus on experience from the past
work and especially also open problems and challenges. Two of the afternoons were devoted
to discussions in breakout groups. Four breakout groups were formed, each with less than 10
participants to allow for intense interaction. Inspired by a classification of research questions
in biology into “why?” and “how?” questions presented in Ted Pavlic’s tutorial, the first
breakout session was devoted to the “why?” questions underpinning programmable matter,
especially also appropriate models of programmable matter systems (both biological or

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

3

engineered) suitable for algorithmic research. The second breakout sessions towards the
end of the seminar was devoted to a set of specific questions given by the organizers that
resulted from the discussions among the participants, they included both research questions
and organizational questions (e.g., how to proceed after the Dagstuhl seminar). After each
of the two breakout sessions, one participant of each of the four breakout groups reported
back the main findings of the discussions to the plenum, leading to further discussion among
all participants. One of the afternoons was devoted to a hike to a nearby village, where
the participants also visited a small museum devoted to programmable mechanical musical
devices.
The seminar was an overwhelming success. In particular, bringing together participants
from a number of different but partially overlapping areas, in order to exchange problems and
challenges on a newly developing field turned out to be excellent for the setting of Dagstuhl –
and the opportunities provided at Dagstuhl are perfect for starting a new community.
Participants were enthusiastic on a number of different levels:
Meeting experts from other fields provided additional insights, challenges and focus when
considering work on programmable matter.
Interacting with colleagues in a close and social manner gave many starting points for
continuing collaboration.
Getting together in a strong, large and enthusiastic group provided the opportunity to
plan a number of followup activities.
The latter include connecting participants via a mailing list, the planning and writing of
survey articles in highly visible publication outlets, and a starting point for specific scientific
workshops and conferences.
Participants were highly enthusiastic about the possibility of another Dagstuhl workshop
in the future; organizers will keep the ball rolling on this – most likely, for an application in
the coming spring, so that some more details can be worked out in the meantime.

16271

4

16271 – Algorithmic Foundations of Programmable Matter

2

Table of Contents

Executive Summary
Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler . . . . . .

2

Overview of Talks
Claytronics: an Instance of Programmable Matter
Julien Bourgeois . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

A Markov Chain Algorithm for Compression in Self-Organizing Particle Systems
Sarah Cannon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

Algorithm design for swarm robotics and smart materials
Nikolaus Correll . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

Dynamic Networks of Computationally Challenged Devices: the Passive Case
Yuval Emek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

Algorithms for robot navigation: From optimizing individual robots to particle
swarms
Sándor Fekete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

The Amoebot Model
Robert Gmyr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

Dances with Plants: Robot-supported Programmable Living Matter
Heiko Hamann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

Introduction to Modeling Algorithmic Self-Assembling Systems
Jacob Hendricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

Advantages, Limitations, Challenges of Tendon-Driven Programmable Chains
Matteo Lasagni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

Programmable Matter for Dynamic Environments
Othon Michail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

Energy Harvesting in-vivo Nano-Robots in Caterpillar Swarm
Venkateswarlu Muni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Algorithmic design of complex 3D DNA origami structures
Pekka Orponen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Algorithmic Foundations of Biological Matter: Faster, Cheaper, and More Out of
Control
Theodore P. Pavlic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

VisibleSim:Your simulator for Programmable Matter
Benoît Piranda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
On obliviousness
Nicola Santoro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Theory and practice of large scale molecular-robotic reconfiguration
Damien Woods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Distributed coordination of mobile robots in 3D-space
Yukiko Yamauchi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

3
3.1

5

Overview of Talks
Claytronics: an Instance of Programmable Matter

Julien Bourgeois (FEMTO-ST Institute – Montbéliard, FR)
Creative Commons BY 3.0 Unported license
© Julien Bourgeois
Joint work of Julien Bourgeois, Seth Copen Goldstein
Main reference J. Bourgeois, S. Copen Goldstein, “Distributed Intelligent MEMS: Progresses and Perspectives”,
IEEE Systems Journal, 9(3):1057–106, 2015.
URL http://dx.doi.org/10.1109/JSYST.2013.2281124
License

Programmable matter (PM) has different meanings but they can be sorted depending on
four properties: Evolutivity, Programmability, Autonomy and Interactivity. In my talk, I will
present the Claytronics project which is an instance of PM, evolutive, programmable, autonmous and interactive. In Claytronics, PM is defined as a huge modular self-reconfigurable
robot. To manage the complexity of this kind of environment, we propose a complete environment including programmable hardware, a programming langage, a compiler, a simulator,
a debugger and distributed algorithms.

3.2

A Markov Chain Algorithm for Compression in Self-Organizing
Particle Systems

Sarah Cannon (Georgia Institute of Technology – Atlanta, US)
Creative Commons BY 3.0 Unported license
© Sarah Cannon
Joint work of Sarah Cannon, Joshua J. Daymude, Dana Randall, Andréa W. Richa
Main reference S. Cannon, J. J. Daymude, D. Randall, A. W. Richa, “A Markov Chain Algorithm for Compression
in Self-Organizing Particle Systems”, in Proc. of the 2016 ACM Symp. on Principles of Distributed
Computing (PODC’16), pp. 279–288, ACM, 2016.
URL http://dx.doi.org/10.1145/2933057.2933107
License

One can model programmable matter as a collection of simple computational elements
(called particles) with limited (constant-size) memory that self-organize to solve systemwide problems of movement, configuration, and coordination. In recent work with Joshua J.
Daymude, Andrea Richa, and Dana Randall, we focused on the compression problem, in which
the particle system gathers as tightly together as possible, as in a sphere or its equivalent in
the presence of some underlying geometry. More specifically, we presented a fully distributed,
local, and asynchronous algorithm that leads the system to converge to a configuration with
small perimeter. Our Markov chain based algorithm solves the compression problem under
the geometric amoebot model, for particle systems that begin in a connected configuration
with no holes. I will give a brief overview of Markov chains, describe our Markov chain and
why it achieves particle compression, and show how it leads to a fully distributed, local, and
asynchronous protocol each particle can run independently. Furthermore, I’ll discuss how
Markov chains might be amenable for use in other programmable matter contexts

16271

6

16271 – Algorithmic Foundations of Programmable Matter

3.3

Algorithm design for swarm robotics and smart materials

Nikolaus Correll (University of Colorado – Boulder, US)
Creative Commons BY 3.0 Unported license
© Nikolaus Correll
Main reference M. A. McEvoy, N. Correll, “Materials that couple sensing, actuation, computation, and
communication”, Science, 347(6228), p. 1261689, 2015.
URL http://dx.doi.org/10.1126/science.1261689
License

“Programmable Matter” is a conjunction of a discrete program and continuous matter. Where
the line between the two needs to be drawn is currently unclear. One approach is to abstract
matter to the point where it can be treated exclusively by discrete models. Another approach
is to think about individual elements becoming so small that they are captured by continuous
physics such as fluid dynamics. In this tutorial, I argue for and explain a hybrid automata
model that consists of a discrete network of computers, which can sense and actuate on a
continuous material that the network is integrated in. Here, communication not only happens
through the network, but also in the material itself via sensor/actuator coupling. I illustrate
this approach using a series of “robotic materials” including a sensing skin, a shape-changing
beam, and a modular wall that can recognize gestures. These systems demonstrate a number
of algorithmic challenges ranging from networking, distributed control and optimization, and
programming. At the same time, each instance illustrates that material properties strongly
influence algorithmic design and vice versa.

3.4

Dynamic Networks of Computationally Challenged Devices: the
Passive Case

Yuval Emek (Technion – Haifa, IL)
Creative Commons BY 3.0 Unported license
© Yuval Emek
Joint work of Yuval Emek, Jara Uitto, Roger, Wattenhofer
Main reference Y. Emek, R. Wattenhofer, “Stone age distributed computing”, in Proc. of the 2013 ACM Symp.
on Principles of Distributed Computing (PODC’13), pp. 137–146, ACM, 2013.
URL http://dx.doi.org/10.1145/2484239.2484244
License

Motivated by applications in biology and nano-technology, the trend of applying the “distributed computing approach” to networks of message passing devices with weak computation
(as well as communication) capabilities is gaining momentum. So far, most of the advances
have been made under the assumption that (1) the network is static; or (2) the dynamic
behavior of the network is dictated by devices that can actively control their own motion. In
this talk, I’d like to discuss some research questions related to such networks that undergo
dynamic (adversarial) topology changes in which the devices only play a passive role.

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

3.5

7

Algorithms for robot navigation: From optimizing individual robots
to particle swarms

Sándor Fekete (TU Braunschweig, DE)
Creative Commons BY 3.0 Unported license
© Sándor Fekete
Joint work of Aaron Becker, Erik D. Demaine, Maximilian Ernestus, Sándor Fekete, Michael Hemmer, Alexander
Kröller, Dominik Krupke, SeoungKyou Lee, James McLurkin, Rose Morris-Wright, Christiane
Schmidt, S. H. Mohtasham
License

Planning and optimizing the motion of one or several robots poses a wide range of problems.
How can we coordinate a group of weak robots to explore an unknown environment? How
can we ensure that a swarm of very simple robots with local capabilities can deal with
conflicting global requirements? And how can a particle swarm perform complex operations?
We will demonstrate how an appropriate spectrum of algorithmic methods in combination
with geometry can be used to achieve progress on all of these challenges.

3.6

The Amoebot Model

Robert Gmyr (Universität Paderborn, DE)
Creative Commons BY 3.0 Unported license
© Robert Gmyr
Joint work of Zahra Derakhshandeh, Robert Gmyr, Andréa W. Richa, Christian Scheideler, Thim Strothmann
Main reference Z. Derakhshandeh, R. Gmyr, A. W. Richa, C. Scheideler, T. Strothmann, “Universal Shape
Formation for Programmable Matter”, in Proc. of the 28th ACM Symposium on Parallelism in
Algorithms and Architectures (SPAA’16), pp. 289–299, ACM, 2016.
URL http://dx.doi.org/10.1145/2935764.2935784
License

We envision programmable matter consisting of systems of computationally limited devices
that are able to self-organize in order to achieve a desired collective goal without the need for
central control or external intervention. Our formal investigation of programmable matter
is based on the Amoebot model. In this talk, I will give a brief introduction to this model.
Furthermore, I will give an overview of our work on three central problems, namely shape
formation, coating, and leader election.

3.7

Dances with Plants: Robot-supported Programmable Living
Matter

Heiko Hamann (Universität Paderborn, DE)
Creative Commons BY 3.0 Unported license
© Heiko Hamann
Joint work of Mostafa Wahby, Mohammad Divband Soorati, Heiko Hamann
Main reference H. Hamann, M. Wahby, T. Schmickl, P. Zahadat, D. Hofstadler, K. Støy, S. Risi, A. Faina, F.
Veenstra, S. Kernbach, I. Kuksin, O. Kernbach, P. Ayres, P. Wojtaszek, “flora robotica – Mixed
Societies of Symbiotic Robot-Plant Bio-Hybrids”, in Proc. of the IEEE Symp. on Computational
Intelligence (SSCI’15), pp. 1102–1109, IEEE, 2015.
URL http://dx.doi.org/10.1109/SSCI.2015.158
License

Besides standard self-reconfiguring modular robotics and self-assembly, robots can also be
mixed with other components to form heterogeneous systems. For example, combining
natural plants and distributed robot systems offers new approaches to programmable matter.
Instead of applying methods of synthetic biology, the idea here is to make use of natural

16271

8

16271 – Algorithmic Foundations of Programmable Matter

plant behaviors to control them. A second example is self-organized swarm construction of
possibly actuated structures. These approaches offer unique advantages, such as growth of
additional material for free, environmental safety, and simplicity, despite their limitations in
flexibility concerning possible structures and potential for reconfigurations.

3.8

Introduction to Modeling Algorithmic Self-Assembling Systems

Jacob Hendricks (University of Wisconsin – River Falls, US)
License

Creative Commons BY 3.0 Unported license
© Jacob Hendricks

This talk introduces theoretical tile based models of self-assembly. We first give the definition
of Winfree’s abstract Tile Assembly Model (aTAM) that was developed to study DNA based
molecular building blocks. This talk proceeds with examples of specific tile assembly systems
such as binary counters and Turing machine simulators as these demonstrate the possibility
of algorithmic self-assembly. Then we discuss a few specific topics in the field. These topics
include non-cooperative self-assembly, various models of self-assembly, common benchmarks
for determining the capabilities and limitations of models, simulation as a means of comparing
models, and finally, the notion of intrinsic universality. Topics have been selected to provide
a bird’s-eye view of a theoretician’s considerations about modelling self-assembling systems
using tile assembly models.

3.9

Advantages, Limitations, Challenges of Tendon-Driven
Programmable Chains

Matteo Lasagni (TU Graz, AT)
Creative Commons BY 3.0 Unported license
© Matteo Lasagni
Joint work of Matteo Lasagni, Kay Römer
Main reference M. Lasagni, K. Römer, “Force-guiding particle chains for shape-shifting displays”, in Proc. of the
2014 IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems (IROS’14), pp. 3912–3918, IEEE,
2014.
URL http://dx.doi.org/10.1109/IROS.2014.6943112
License

One of the first and most relevant questions when designing a shape-shifting material concerns
particles’ topology and hence their mobility. We can distinguish between detachable and
non-detachable topologies. Detachable topologies allow particles to temporarily detach from
the ensemble and freely migrate to different locations in order to obtain a shape-shift. In
contrast, non-detachable topologies constrain particles to occupy a fixed location where only
relative displacement between adjacent particles and/or particle deformation are allowed for
shape-shift. Despite the fact that detachable topologies allow the formation of literally any
shape, the complex architecture to enable particle migration, generally consisting of built-in
actuators and latching mechanisms, raises costs and limits the scalability of the whole system,
and causes inherent problems concerning particle power supply and communication. In our
work, we demonstrate how a nondetachable topology can allow the formation of arbitrary
complex shapes, thereby avoiding or at least limiting the above-mentioned problems. In
particular, scalability and cost-effectiveness derive from the concatenation of semi-active
particles without bulky built-in actuators and latches. Such particles, forming piecewise

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

9

bendable chains, exploit remotely generated forces to self-actuate and hence to control the
local curvature of the chain. Multiple chains can be combined to form a shape-shifting surface
to support novel applications like 3D tangible displays or programmable molds. One major
challenge concerns the actuation of the system. Without the support of optimal planning
strategies able to schedule proper particle actuation, unbearable actuation forces might
occur, for example, due to inconvenient leverage effects, with negative consequences for the
system stability and integrity. Starting from the current configuration and aiming at the final
target configuration, optimal planning techniques should explore the large set of possible
next configurations where only a limited number of particles can actuate, and determine in
which cases the intensity of the actuation forces is acceptable. The optimization problem
involves not only a single independent chain, but applies simultaneously to all chains. Due
to mechanical constraints, indeed, all chains need to actuate the same number of particles at
each reconfiguration step. This calls for models able to predict the behavior of the whole
system upon the application of specific control input in order to support optimal planning
algorithms. Such models need to be sufficiently accurate to be consistent with reality but
also computationally efficient to allow planning in reasonable time. An important question
concerns the determination of an acceptable trade-off between these two aspects.

3.10

Programmable Matter for Dynamic Environments

Othon Michail (CTI – Rion, GR)
Creative Commons BY 3.0 Unported license
© Othon Michail
Joint work of Othon Michail, Paul G. Spirakis
Main reference O. Michail, P. G. Spirakis, “Terminating population protocols via some minimal global knowledge
assumptions”, J. Parallel Distrib. Comput., Vol. 81–82, pp. 1–10, 2015.
URL http://dx.doi.org/10.1016/j.jpdc.2015.02.005
License

We discuss two recent theoretical models of programmable matter operating in a dynamic
environment. In the first model, all devices are finite automata, begin from the same initial
state, execute the same protocol, and can only interact in pairs. The interactions are
scheduled by a fair (or uniform random) scheduler, in the spirit of Population Protocols.
When two devices interact, the protocol takes as input their states and the state of the
connection between them (on/off) and updates all of them. Initially all connections are off.
The goal of such protocols is to eventually construct a desired stable network, induced by
the edges that are on. We present protocols and lower bounds for several basic network
construction problems and also universality results. We next discuss a more applied version
of this minimal and abstract model, enriched with geometric constraints, aiming at capturing
some first physical restrictions in potential future programmable matter systems operating
in dynamic environments.

16271

10

16271 – Algorithmic Foundations of Programmable Matter

3.11

Energy Harvesting in-vivo Nano-Robots in Caterpillar Swarm

Venkateswarlu Muni (Ben Gurion University of the Negev – Beer Sheva, IL)
Creative Commons BY 3.0 Unported license
© Venkateswarlu Muni
Joint work of Shlomi Dolev, Sergey Frenkel, Venkateswarlu Muni, Michael Rosenblit, Ram Prasadh Narayanan
License

Biological collaborative systems behavior is fascinating, urging researchers to mimic their
behavior through programmable matters. These matters constitute a particle system, wherein
the particles bind with the neighboring particles to swarm and navigate. Caterpillar swarm
inspired particle systems involves layered architecture with single to a predefined number
of layers. Through this work, a coordinated layered particle system inspired by caterpillar
swarm is discussed. We first propose a novel design for produce-able nano-particles that
uses electrodes to harvest electricity from the blood serum, energy that can be later used
for swarm inter and/or outer communication, moving, coordination, sensing and acting
according to a given (instructing) program. The benefit of moving and acting in a swarm is
demonstrated by a design of telescopic movement in pipes (e.g., blood vessels), wherein each
layer uses the accumulated speed of all layers below and moves faster, thus, mimicking the
faster motion of the caterpillar swarm.

3.12

Algorithmic design of complex 3D DNA origami structures

Pekka Orponen (Aalto University, FI)
Creative Commons BY 3.0 Unported license
© Pekka Orponen
Joint work of Erik Benson, Abdulmelik Mohammed, Johan Gardell, Sergej Masich, Eugen Czeizler, Pekka
Orponen, Björn Högberg
Main reference E. Benson, A. Mohammed, J. Gardell, S. Masich, E. Czeizler, P. Orponen, B. Högberg, “DNA
rendering of polyhedral meshes at the nanoscale”, Nature, 523(7561):441–444, 2015.
URL http://dx.doi.org/10.1038/nature14586
License

In a recent work (Nature 523:441–444, July 2015), we described a general methodology
and software pipeline for rendering 3D polyhedral mesh designs in DNA. In this talk, I
will first summarise the basic idea of Paul Rothemund’s DNA origami technique which
also underlies our approach, and then proceed to discuss the graph-theoretic concepts and
algorithmic ideas used in extending his technique from 2D patterns to 3D wireframe mesh
structures. The reliability and generality of the approach is demonstrated by a number of
electron microscopy images of synthesised nanostructures, including a 50-nm rendering of
the widely-used Stanford Bunny model. I will also touch on the challenges of using DNA as
a substrate for complex designs, and some related open questions.

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

3.13

11

Algorithmic Foundations of Biological Matter: Faster, Cheaper,
and More Out of Control

Theodore P. Pavlic (Arizona State University – Tempe, US)
License

Creative Commons BY 3.0 Unported license
© Theodore P. Pavlic

For at least the past 30 years, there has been much interest in developing programmable
matter solutions that have been inspired by related phenomena in nature. Visionaries in
computer science from the 1980’s promised that such life-like phenomena would be possible,
and yet the programmable matter of today still has limited capabilities. Rather than groups
of relatively inexpensive agents grouping together to form an intelligent and flexible collective
matter, the automation systems we have today combine relatively intelligent individual units
together into rigid and often static structures that have no ability to adapt to the surrounding
environment.
To make real progress in understanding the algorithms responsible for nature’s success,
computer scientists and engineers need to become familiar with the taxonomy of scientific
questions in biology. The outputs of biological evolution are shaped not only by adaptive
value (i.e., a design objective), but also by phylogeny (i.e., structures inherited from earlier
forms), ontogeny (i.e., the process of constructing the object), and the actual mechanism of
action that interacts with the environment. The former of these two pressures – adaptive
value and phylogeny – are the subject of the “Why” questions of biology, and those questions
must always be conditioned by the ancestral environment. The latter two of these pressures
– ontogeny and mechanism – are the subject of the “How” questions of biology, and those
questions must always be conditioned by the modern environment. This characterization of
biological questions is not unlike the ways in which computer scientists consider algorithms
and their implementations – in terms of design objectives, platforms, and algorithms that
operate on those platforms. However, taking the objective–platform–algorithm approach
alone with biological systems obscures details about evolution and ecology that are necessary
for understanding how a biological system could possibly be working and whether it is really
appropriate to take such an approach with an engineered system. Additionally, taking the
effort to understand adaptive value of biological phenomena can provide interesting new
motivations for problems that could be solved in engineered systems, albeit using totally
different mechanisms.
When learning about programmable matter from biological systems, there are some
biological model systems that are better matches than others. A flock of starlings produces
beautiful patterns of so-called “active matter” in three-dimensional space. However, such
patterns likely have no adaptive value themselves and are simply an epiphenomenon of
behaviors which were shaped by individual-level selection to reduce predation risk. As a
consequence, anyone trying to reverse engineer such patterns will likely not find much insight
in biology. However, army ant bivouacs that are large, self-healing, self-regulating balls of
ants in the middle of tropical forest may be a good model as this group-level phenomena is
likely under selective pressure by natural selection. Similarly, army ant bridges that move and
grow to some intermediate length have been shown to likely provide adaptive benefit to the
group. So army ants seem to be a more fitting model for programmable matter than starling
flocks. Ants, in general, are attractive targets as all ants evolved from an ancestor that was
a solitary wasp; in essence, ants are distributed wasps, with individual workers taking on
specialized tasks and aggregating together to form a colony that does what was once the work
of a single individual. Although this description of ants is relatively general, there are a wide

16271

12

16271 – Algorithmic Foundations of Programmable Matter

variety of different problem-solving methods that have evolved across ant taxa. Ants that use
pheromone trails as external spatial shared memory apparently vary in how important those
trails factor in their individual behaviors, and those variances allow some species of ants to
be able to better track changes in the environment than other ants (at the cost of not being
able to converge on group-level decisions as quickly). Ants that do not use pheromone trails
make foraging and nest-site selection decisions much more slowly, but they have a notable
ability to aggregate information from apparently irrational individuals that still produces
rational outcomes at the group level. Laboratory tests that induce irrationality in individuals
fail to generate the same irrationality in the groups. This kind of aggregation process would
be valuable in the development of programmable matter that gathers information from its
environment and processes it in a decentralized manner.
Beyond ants, there are other interesting systems that may be good models for programmable matter. One example is the slime mould, in particular the multinucleate slime
mould. The multinucleate slime mould is a single cell, formed by the fusion of multiple
cells, filled with nuclei and other organelles and surrounded by a flexible, growing membrane
that gives the whole unit amoeboid characteristics. Through a decentralized process that
dynamically induces pressure gradients around the amoeba-like macroscopic cell, it grows
and shrinks and can perform a variety of interesting computations – like finding the shortest
path through a maze, determining the right path through a Towers of Hanoi decision tree,
and even re-allocating its biomass across different food sources with different mixtures of
macronutrients so that the combined intake of those macronutrients is regulated to set levels.
While both ants and slime moulds appear somewhat social, it is rare to think about the
process of development as a decentralized, collective behavior or computation. However,
developmental biologists have shown that cells are effectively executing programs based on
internal state and local information from their microenvironment. Following these ideas,
some have found ways to re-grow limbs on vertebrates and even induce multiple heads to grow
on invertebrates. In the case of the example of the already regenerative planaria (flat worm),
modifications induced by a temporary stimulus are latched into the tissue and remembered
in all future generations. Those modifications can only be erased by applying drugs used in
humans for suppressing memories. This suggests that the somatic tissue all around the body
is itself a primitive neural network, capable of information processing and storage. Thus,
biological tissue may be a very fitting model system for programmable matter.
In this talk, I will elaborate on these ideas and provide more examples of other biological
systems that may provide useful insights when designing programmable matter that may
someday finally realize a dream that has been deferred for decades.

3.14

VisibleSim:Your simulator for Programmable Matter

Benoît Piranda (FEMTO-ST Institute – Montbéliard, FR)
Creative Commons BY 3.0 Unported license
© Benoît Piranda
Joint work of Julien Bourgeois, Dominique Dhoutaut, André Naz, Benoît Piranda, Pierre Thalamy, Thadeu Tucci
Main reference D. Dhoutaut, B. Piranda, J. Bourgeois, “Efficient simulation of distributed sensing and control
environments”, in Proc. of the 2013 IEEE Int’l Conf. on Internet of Things (iThings’13),
pp. 452–459, IEEE, 2013.
URL http://dx.doi.org/10.1109/GreenCom-iThings-CPSCom.2013.93
License

VisibleSim is a 3D simulator for distributed robots in a simulated environments. I propose a
short tutorial to write a first distributed code for VisibleSim.

Sándor Fekete, Andréa W. Richa, Kay Römer, and Christian Scheideler

3.15

13

On obliviousness

Nicola Santoro (Carleton University – Ottawa, CA)
Creative Commons BY 3.0 Unported license
© Nicola Santoro
Main reference P. Flocchini, G. Prencipe, N. Santoro, “Distributed Computing by Oblivious Mobile Robots”,
Synthesis Lectures on Distributed Computing Theory, Vol. 3, No. 2 , pp. 1-185, 2012.
URL http://dx.doi.org/10.2200/S00440ED1V01Y201208DCT010
License

The presence of some form of persistent memory (albeit small in size) is typically assumed
in “micro-level” computations (e.g., programmable matter). In contrast, obliviousness (i.e.,
total absence of persistent memory) is a common restrictions in “macro-level” computations
(e.g, autonomous mobile robots) in which I have been involved. On this regards, there are
two interesting research questions I would like to share:
Are meaningful oblivious computations possible at the micro-level?
What are the precise limits of near-obliviousness (i.e., memory-size thresholds)?

3.16

Theory and practice of large scale molecular-robotic
reconfiguration

Damien Woods (California Institute of Technology – Pasadena, US)
Creative Commons BY 3.0 Unported license
© Damien Woods
Joint work of Ho-Lin Chen, Moya Chen, David Doty, Scott Goodfriend, Nadine Dabby, Dhiraj Holden, Chris
Thachuk, Erik Winfree, Damien Woods, Doris Xin, Chun-Tao Yang, Peng Yin
License

The talk discussed the theory and practice of molecular robotics. I am interested in large
scale molecular reconfiguration, where dynamic self-assembling nanostructures change their
shape in response to environmental stimuli. I’m very much interested in models that have the
potential to be implemented in DNA: a shockingly-well understood and predictable material
for nanoscale self-assembly. Specifically, the talk focused attention on questions on the Nubot
model and on our initial progress on implementing this style of molecular robotics in the
wet-lab.

3.17

Distributed coordination of mobile robots in 3D-space

Yukiko Yamauchi (Kyushu University – Fukuoka, JP)
Creative Commons BY 3.0 Unported license
© Yukiko Yamauchi
Joint work of Taichi Uehara, Masafumi Yamashita
Main reference Y. Yamauchi, T. Uehara, M. Yamashita, “Brief Announcement: Pattern Formation Problem for
Synchronous Mobile Robots in the Three Dimensional Euclidean Space”, in Proc. of the 2016
ACM Symp. on Principles of Distributed Computing (PODC’16), pp. 447–449, ACM, 2016.
URL http://dx.doi.org/10.1145/2933057.2933063
License

We consider a swarm of autonomous mobile robots moving in the three-dimensional space
(3D-space). Each robot is anonymous, oblivious (memory-less), and has neither any access
to the global coordinate system nor any communication medium. Many researchers have
considered formation problems (point formation, circle formation, pattern formation, etc.)
in 2D-space, and it has been shown that the symmetry among the robots determines the
patterns that the robots can form. We would like to present our recent results on formation
problems in 3D-space, where we encounter rich symmetry represented by rotation groups.

16271

14

16271 – Algorithmic Foundations of Programmable Matter

Participants
Luca Becchetti
Sapienza University of Rome, IT
Julien Bourgeois
FEMTO-ST Institute –
Montbéliard, FR
Sarah Cannon
Georgia Institute of Technology –
Atlanta, US
Ioannis Chatzigiannakis
Sapienza University of Rome, IT
Nikolaus Correll
Univ. of Colorado – Boulder, US
David Doty
Univ. of California – Davis, US
Yuval Emek
Technion – Haifa, IL
Sándor Fekete
TU Braunschweig, DE
Robert Gmyr
Universität Paderborn, DE
Heiko Hamann
Universität Paderborn, DE
Jacob Hendricks
University of Wisconsin – River
Falls, US
Stephan Holzer
MIT – Cambridge, US
Irina Kostitsyna
Free University of Brussels, BE

Dominik Krupke
TU Braunschweig, DE

Kay Römer
TU Graz, AT

Fabian Daniel Kuhn
Universität Freiburg, DE

Trent Rogers
University of Arkansas –
Fayetteville, US

Matteo Lasagni
TU Graz, AT
Othon Michail
CTI – Rion, GR

Nicola Santoro
Carleton Univ. – Ottawa, CA
Christian Scheideler
Universität Paderborn, DE

Venkateswarlu Muni
Ben Gurion University of the
Negev – Beer Sheva, IL

Arne Schmidt
TU Braunschweig, DE

André Naz
FEMTO-ST Institute –
Montbéliard, FR

Robert Schweller
The University of Texas – Pan
American – Edinburg, US

Pekka Orponen
Aalto University, FI

Thim Frederik Strothmann
Universität Paderborn, DE

Matthew J. Patitz
University of Arkansas –
Fayetteville, US

Sebastian von Mammen
Universität Augsburg, DE

Theodore P. Pavlic
Arizona State University –
Tempe, US
Benoît Piranda
FEMTO-ST Institute –
Montbéliard, FR
Andréa W. Richa
Arizona State University –
Tempe, US

Jennifer L. Welch
Texas A&M University – College
Station, US
Andrew Winslow
Free University of Brussels, BE
Damien Woods
California Institute of Technology
– Pasadena, US
Yukiko Yamauchi
Kyushu Univ. – Fukuoka, JP

Telecommunication Systems 24:2–4, 123–138, 2003
 2003 Kluwer Academic Publishers. Manufactured in The Netherlands.

Models, Complexity and Algorithms for the Design of
Multi-fiber WDM Networks ∗
A. FERREIRA, S. PÉRENNES and H. RIVANO ∗∗

Herve.Rivano@sophia.inria.fr

Mascotte Project, CNRS-I3S–INRIA, France
A.W. RICHA

Department of Computer Science and Engineering, ASU, USA
N. STIER MOSES

Operations Research Center, MIT, USA
Received January 2000

Abstract. In this paper, we study multi-fiber optical networks with wavelength division multiplexing
(WDM). We extend the definition of the well-known Wavelength Assignment Problem (WAP) to the case
of k fibers per link and w wavelengths per fiber, generalization that we will call (k, w)-WAP. We develop
a new model for the (k, w)-WAP based on conflict hypergraphs. Furthermore, we consider two natural optimization problems that arise from the (k, w)-WAP: minimizing the number of fibers k given a number of
wavelengths w, on one hand, and minimizing w given k, on the other. We develop and analyze the practical
performance of two methodologies based on hypergraph coloring.
Keywords: optical network design, wavelength division multiplexing

Introduction
Wavelength division multiplexing (WDM) is one of the most promising optical network
technologies. This technology is capable of transmitting multiple signals through the
same fiber by using different wavelengths for each of them. As the interference among
signals is minimal, WDM permits an efficient use of the high bandwidth offered by
optical fibers. Our work focuses on WDM network design in real-life scenarios, both
from theoretical and practical perspectives. Of course, operators are interested in minimizing the costs incurred in the network configuration. This leads to a design problem
commonly known as the wavelength assignment problem (WAP), which received considerable attention in the literature [Robertson and Seymour, 22; Beauquier et al., 2;
Caragiannis et al., 3; Ramaswami and Sivarajan, 21].
∗ A preliminary version of this paper appears in Proceedings of the 10th International Conference on

Telecommunications, Tahiti, French Polynesia, 2003.

∗∗ Corresponding author. Mascotte Project, CNRS-I3S-INRIA, BP 93, 06902 Sophia Antipolis Cedex,

France. H. Rivano is also with France Telecom R&D.

124

FERREIRA ET AL.

Assume that an operator possesses a WDM network and clients who request to
route traffic between pairs of nodes. First, the operator establishes paths, called lightpaths in this context, to route each demand’s traffic. After lightpaths are determined
and before the network can start transmitting, the operator has to solve the WAP which
consists in assigning wavelengths to lightpaths. The assignment must be done in such
a way that any two paths that meet in a link are assigned different wavelengths, otherwise their signals would scramble making decodification impossible. In this paper, we
consider the problem of determining paths already solved and concentrate on the one of
assigning wavelengths. It is not hard to see that under the usual assumption that fibers
have unlimited capacity (i.e., they can transmit an arbitrary number of wavelengths), the
WAP is equivalent to the path coloring problem in standard graphs [Chlamtac et al., 4].
Moreover, these problems are also equivalent to the well-known graph coloring problem by mapping the n lightpaths to a graph of n vertices, where two vertices are adjacent
if their corresponding lightpaths meet in an arc. This reduction implies that for some
number δ > 0, there cannot exist an nδ -approximation algorithm for the WAP unless
P = NP [Hochbaum, 8]. As a consequence, a large fraction of the work in this field
concentrated on specific topologies like line networks, rings, trees and meshes; and on
specific communication patterns [Kumar, 10; Erlebach et al., 7].
From the telecommunications operator viewpoint, one of the largest costs incurred
while deploying an optical network stems from physically trench-digging to bury the optical fibers. Hence, to protect themselves from demand uncertainties and failures, operators usually install many fibers. Although frequently fibers are used independently, the
opportunity to exploit this redundancy gives rise to multi-fiber WDM networks (MWNs).
Unfortunately, the existing work on single-fiber networks cannot be extended to MWNs
in a straightforward manner. Indeed, the model used for the WAP on single-fiber networks fails to fully capture the benefits of having more fibers per link. In fact, in contrast
to single-fiber networks, the presence of multiple fibers adds an extra degree of freedom
when choosing wavelengths for paths: the same wavelength may be used in each of the
different fibers.
Margara and Simon [16], Li and Simha [13], and Choi et al. [5] studied some theoretical properties of the new setting. For instance, they proved that increasing the number
of fibers per link often simplifies the optical routing problem. In particular, although the
WAP is NP-complete on single-fiber undirected stars, it becomes polynomial if a second
fiber is available on each link. Note that using k fibers per link immediately allows a reduction of the number of wavelengths by a factor of k. In fact, multiple fibers may reduce
the number of required wavelengths even further: for all k and w, there exist a network
and a set of communication requests such that exactly w wavelengths are necessary to
solve the problem with k fibers per link while one wavelength is enough with k + 1
fibers. More generally, [Margara and Simon 17] proved that for any network N , there
exists a number k(N ) such that any set of paths in N admits a wavelength assignment
with k(N ) fibers per link.
Unfortunately, results of this flavor, which specifically determine the impact of having multiple fibers, either hold for very specific networks (e.g., [Margara and Simon, 16;

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

125

Li and Simha, 13]) or are not useful in practice [Margara and Simon, 17]. Nevertheless,
some practical results can be found in the literature too. For example, Baroni et al. [1]
consider path length constrained routing, wavelength assignment, wavelength conversion, and link failure restoration. They present an integer program and heuristics to
minimize the total number of fibers used in the network. In addition, Hyytiä and Virtamo [9] propose the utilization of metaheuristics as simulated annealing and tabu-search
for MWNs design. Both papers show that adding fibers could improve the network efficiency. Saad and Luo [23] address the dual problem of maximizing the number of
lightpaths that can be established on a given network. They use a Lagrangian decomposition of certain integer programs to obtain tractable heuristics.
Another problem previously considered was that of dynamic traffic in which lightpaths have to be set-up and released dynamically. Kuri et al. [11] addressed dynamic and
deterministic traffic. In this case, set-up and release times of lightpaths are assumed to
be known in advance, which allows a tabu-search metaheuristic to solve the optimization
problem off-line. Zhang and Qiao [25], and Li and Somani [14] study stochastic traffic
using an on-line approach. They show that multi-fiber networks are more efficient than
single-fiber networks with the same capacity per link (the capacity of a multi-fiber link
is the sum of the capacities of each fiber in the link). The use of multi-fiber links leads to
a performance that is equivalent to the one provided by limited wavelength conversion.
Most of the methodologies that we mentioned are based on heuristics and therefore it is not possible to compute performance guarantees for the solutions that they
provide. This issue cannot be addressed without introducing a more formal modeling
technique. For that reason, in this paper we propose an approach that mixes theory with
an operational viewpoint. We build a general framework and propose a new hypergraph
for modeling wavelength conflicts that arise in MWNs. To validate these concepts, we
illustrate our approach with experiments on two real-world backbone networks: the European C OST 239 and the Pan-American. Indeed, we solve the corresponding optimization problems with both exact integer programming formulations and approximation
algorithms with bounded factor. For the former, we consider commercial LP/IP solvers
while for the latter we implement the two algorithms that we propose. The first of them
is a randomized algorithm whose approximation guarantee depends on the logarithm
of the routing load, defined as the maximum number of paths going through an edge.
The second uses randomized rounding to get a good solution, followed by re-coloring to
make it feasible. Its guarantee is the best-known approximation factor for this problem,
namely the logarithm of the length of the longest path.
The organization of this paper can be summarized as follows. First, in section 1,
we precisely define the (k, w)-WAP in MWNs that we previously described informally.
Furthermore, we develop a model that generalizes the notion of a conflict hypergraph
used for single-fiber networks. This model captures more accurately the lightpath interdependencies in multi-fiber networks. With it, we build a bridge between results in
the literature about hypergraph coloring and the (k, w)-WAP. Then, in section 2, we
analyze the computational complexity of our problem. In fact, we prove that minimizing the number of wavelengths w is NP-hard, even in the case where the number of

126

FERREIRA ET AL.

fibers k is fixed in advance, answering an open question regarding the complexity of this
problem. In sections 3 and 4, we analyze the theoretical and practical performance of
different algorithms, some of which are exact while others are approximations based on
the hypergraph coloring problem. Finally, we conclude and present further directions of
research in section 5.
1.

Problem formulation

In this section, we formally define the (k, w)-WAP, the conflict hypergraph, and some
other related concepts. Let N be an instance of a MWN described by the graph
G = (N, L) and a set of communication paths P on G that join the origins of the
demands with their destinations. Assume further that each link in L contains k fibers
and that there are w wavelengths available. The (k, w)-WAP asks for an assignment of
wavelengths to paths satisfying that no more than k paths using one link are assigned
the same wavelength. Therefore, in the solution each wavelength will be assigned to at
most one of the fibers of a link, as required. To formalize the constraints, we define the
conflict hypergraph of the set of paths P as follows (see figure 1).
Definition 1. The conflict hypergraph H = (V , E) of the paths P is the hypergraph
given by a vertex v ∈ V for each path p ∈ P, and an hyperedge e ∈ E for every link
 ∈ L. The hyperedges of H contain the vertices corresponding to paths going through
the corresponding link; i.e., link  generates a hyperedge e = {v:  ∈ path represented
by v}.
The four main parameters of the hypergraph H can be expressed in terms of N
and P:
•
•
•
•

the number of vertices n := |V | = |P|;
the number of hyperedges m := |E| = |L|;
the rank t := max∈L |{P ∈ P:  ∈ P }|, which is called the load of P;
the maximum degree  := maxv∈V |{e ∈ E: e 
 v}|. Note that  
maxp∈P length(p), which is called the diameter of the routing.

Figure 1. A ring network and the corresponding conflict hypergraph.

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

127

Notice that a vertex coloring of the conflict hypergraph induces a feasible wavelength assignment to the paths if and only if no hyperedge contains more than k vertices
with the same color. This motivates the following definition.
Definition 2. Given a hypergraph H = (V , E) and a set of colors C = {1, . . . , c}, a
mapping f : V → C is a (k, c)-coloring if no hyperedge contains more than k vertices
with the same color; i.e., for all e ∈ E and all χ ∈ C, |{v ∈ e: f (v) = χ}|  k.
It is easy to see from definitions 1 and 2, that there is a one-to-one correspondence
between the (k, c)-colorings of the conflict hypergraph of P and the feasible wavelength
assignments to these paths. Since we can build the conflict hypergraph H in polynomial time, this implies a polynomial time reduction from the (k, w)-WAP to the (k, c)coloring problem. It is not trivial, however, that the converse is also true, since the
hypergraph coloring problem may seem at first to be a more general (and harder) problem than the WAP. In fact, the two are equivalent and because of that we use the terms
‘color’ and ‘wavelength’ interchangeably.
2.

Complexity of wavelength assignment in MWNs

In this section we prove the equivalence between the (k, w)-WAP and the (k, c)coloring problem. Furthermore, we prove that the decision version of the (k, w)-WAP is
NP-complete even in the case where k is fixed and present a lower bound on the number
of colors needed in a (k, c)-coloring of a (hyper)clique.
Theorem 1. The (k, c)-coloring problem is polynomially equivalent to the (k, w)-WAP
on MWNs.
Proof. Since we have argued in section 1 that there is a reduction from the (k, w)-WAP
to the (k, c)-coloring problem, the theorem would be proven if we show the converse.
Thus, we need to show that any hypergraph H is the conflict hypergraph of a set of
paths P on a network N , where the cardinalities of P and N are polynomial in the size
of H . For instance, let H = (V , E) be an arbitrary hypergraph, where V = {v1 , . . . , vn }
and E = {e1 , . . . , em }. We build a network N and a set of paths P of cardinality n as
follows (refer to figure 2 for an illustration).
• We create a first layer just containing a column of nodes x10 , . . . , xn0 from where paths
will originate.
• For each hyperedge ei ∈ E, we add one layer in N that contains:
– nodes yi and yi joined with an edge;
– a column of n nodes x1i , . . . , xni ;
– for each vertex vj ∈ ei , we add the edges (xji−1 , yi ) and (yi , xji );
/ ei , we add the edge (xji−1 , xji ).
– for each vertex vj ∈

128

FERREIRA ET AL.

Figure 2. Reduction from a hypergraph H to a network N discussed in theorem 1.

• For each vertex vj ∈ V , we add a path pj from xj0 to xjm through nodes xj1 , . . . , xjn−1
and also through nodes yi and yi when needed.
From construction, a conflict between two paths pj and pk can occur only on an
edge of the form (yi , yi ) such that both vj and vk belong to ei . Therefore, H is exactly
the conflict hypergraph of P on N . Last, polynomiality follows because |P| = n and
|N | = O(mn).

The graph coloring decision problem, which asks for an answer to the question
“Can a graph G be colored using c colors or less?” is known to be NP-complete. Noting
that the decision version of the (k, c)-coloring problem for k = 1 is exactly the coloring
problem, we get the following corollary.
Corollary 2. The decision version of the (k, w)-WAP on a MWNs is NP-complete for
an arbitrary k.
Moreover, and not too surprisingly, the problem remains difficult even with a
fixed k. Indeed, the following theorem implies that the decision version of the (k, w)WAP on a MWNs is NP-complete for any fixed k.
Theorem 3. The decision version of the (k, c)-coloring problem is NP-complete for any
fixed k.
Proof. To prove the claim we reduce the (k, c)-coloring problem to the graph coloring
problem. Indeed, assume we are given a graph G with n nodes and m edges. We must
prove that it is c-colorable if and only if a certain hypergraph H that will define from G
is (k, c)-colorable. Note that we can assume that c < n because otherwise the graphs can
always be colored. We will define H in such a way that it has a single (k, c)-coloring (up
to permutations of the colors) and each of the c colors repeated on exactly k + 1 nodes.
We now explain how to construct the mentioned hypergraph H .
First, we construct a hypergraph Q for which we can guarantee that there are
c nodes that span all the colors. Next, we extend Q to H in a way that it is colorable if
and only if G is colorable.

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

129

• Let Kn,t denote a hypergraph with n nodes that contains all possible hyperedges of
rank t. A valid (k, c)-coloring of the clique Kck,k+1 is any coloring that satisfies that
each color χ is assigned to exactly k nodes, where χ = 1, . . . , c .
• Consider the hypergraph Q = Kck,k+1 ∪ {v1 , . . . , vc }; fix a valid coloring for the
clique as described before; and color vi with color i, for i = 1, . . . , c. Last, add
to Q all the possible (k + 1)-hyperedges that would not contain k + 1 nodes of the
same color. By construction, it is clear that this coloring is the only possible one (up
to permutations), so every color is used k + 1 times.
• Let H = Q and add to H the node-set of G. Then, for each edge (i, j ) of G, we add
c hyperedges as follows: for each color χ include any k − 1 nodes that have color χ
in Q and the nodes corresponding to i and j in H .
For each edge e ∈ G, the set of c hyperedges in H constraints the (k, c)-coloring
problem to assign two different colors to the endpoints of e, which is exactly the constraint of the graph coloring problem. Then, if we can (k, c)-color the hypergraph, no
two nodes coming from adjacent vertices of G are assigned the same color. This means
that a coloring of H is valid for G and vice versa.
What remains to be seen is that the transformation is polynomial in the parameters.
The total number of nodes is c(k + 1) + n which is polynomial in the input. Notice also
that all the hyperedges we used have rank k + 1, therefore a (loose) upper bound for the
number of edges is that of the clique Kc(k+1)+n,k+1 , which amounts to

 

nk+1 (k + 2)k+1
c(k + 1) + n
n(k + 2)
.


k+1
k+1
(k + 1)!
For the first inequality we used the assumption that c < n. As this is certainly polynomial
in n and k is fixed, the claim follows.

2.1. A lower bound
Extending the notion of cliques in graphs, we can give a lower bound for the number of
colors needed in a (k, c)-coloring, by using (hyper)cliques, as follows.
Lemma 4. A (k, c)-coloring of Kn,t is feasible if and only if
 
 n
if t > k,
c
 k
1
otherwise.
Proof. The case in which t  k is straightforward since when the hyperedges have
rank less than k, there is no restriction on the coloring and therefore one color is enough.
For the case in which t > k, suppose that Kn,t can be colored with c colors.
Then, there exists a color χ such that there are n/c nodes colored with it. Since the
(k, c)-coloring was feasible and all hyperedges of rank t are present, we must have that
n/c  t. Moreover, there is a hyperedge that contains n/c vertices colored with χ,

130

FERREIRA ET AL.

from where n/c  k. The claim follows because the minimum number that satisfies
the previous condition is n/k.
For instance, due to the symmetry of the hypergraph, the only possible (k, c)coloring (up to permutation of the colors) is given by assigning colors to vertices uniformly.

The lemma above bounds the number of colors required to color any hypergraph
that contains Kn,t , yielding the following generalization of the fact that the chromatic
number of a graph is larger than the size of its maximum clique (just make t = 2 and
k = 1).
Corollary 5. Let H be a hypergraph containing Kn,t . If H can be (k, c)-colored, with
k < t, then c  n/k.

3.

Tools for designing MWNs

In this section, we will present two scenarios in the design of multi-fiber networks. The
number of fibers per link and the number of available wavelengths are two concurrent
optimization parameters and we have no informations on how to compare their costs.
Therefore we consider the two mono-criteria optimization problems where one of these
parameters is fixed. The equivalence between solving the WAP for P and computing
(k, c)-colorings of H allows us to concentrate on the latter. Hence we consider the
problems of finding the minimum k (respectively c) such that there is a feasible (k, c)coloring of H with c (respectively k) given. We address these two problems in sections 3.1 and 3.2, respectively.
3.1. Minimizing the number of fibers
We consider first the problem of minimizing the number of fibers when the number of
colors is given. This optimization is related to the situation an operator faces when
setting up a new network or when the operator tries to keep a set of free fibers for
robustness concerns or for renting them to other clients.
Using the hypergraph model, we can formulate the problem as a minimax integer
program [Srinivasan, 24]. For that, we define (0, 1)-integer variables xij , for all i ∈ V
and 1  j  c, such that xij = 1 if and only if node i is colored with color j and xij = 0
otherwise. The variable k is a common upper bound for the constraints defined by each
hyperedge. The optimal number of fibers can be found by solving IP 1.
Integer program 1.
minimize
subject to

k

c

(minimize # of fibers)
xic = 1 ∀ node i,

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS



xic  k

i∈H

xic ∈ {0, 1}

131

∀ color c, ∀ hyperedge H,
∀ color c, ∀ node i.

Srinivasan [24] showed that a randomized rounding of the optimal solution of the
LP relaxation of IP 1 produces, with positive probability, a solution that is feasible and
approximates the optimal up to a factor of the order of the logarithm of the hypergraph
degree. As said before, the degree of the conflict hypergraph of a set of paths equals to
the diameter of the corresponding routing. This result uses an extension of the Lovász
local lemma given in the same article. Inspired by this existential result, Lu [15] proposed a combinatorial and randomized algorithm which computes a solution within an
approximation ratio not too far from the one given by [Srinivasan, 24]. This algorithm
proceeds by successive recoloring phases. A first phase colors the vertices of the hypergraph randomly with a third of the set of colors. It then detects the hyperedges where
there are too many vertices with the same color and colors them once again randomly
with another third of the colors. Lu [15] shows then that with high probability the set
of vertices that are still to badly colored if small enough for an optimal, hence exponential, coloring to be done with the last third of colors. The analysis of the algorithm
shows that the approximation ratio is of the order of the logarithm of the load of the
routing.
Leighton et al. [12] proposed recently an enhancement of this algorithm. Their
idea comes from the remark that Lu’s algorithm behaves as if it was doing a randomized
rounding of a straightforward solution of the LP relaxation of IP 1 but considering the
colors by thirds of the set. This strategy leads to an inefficient use of the available colors
and to solutions that are worse than what promises the theoretical analysis of [Srinivasan 24]. Leighton et al. [12] want to build a solution close to the one Srinivasan showed
the existence and start their algorithm with a randomized rounding of the LP relaxation.
In order to be sure to get a good solution, they use recoloring techniques inspired by those
of [Lu, 15]. Using linear programming for the randomized rounding is costly in terms
of computing effort, but provides an efficient guide for constructing solutions within
the theoretical approximation ratio. Furthermore, we shall show in section 4 that the
recoloring phases are not used in practical situations since the randomized rounding
yields solutions that are close to optimality. More generally, the analysis of algorithms
based on randomized rounding are often very pessimistic in practice. This remark is
certainly to be related to the fact that the algorithms of both [Lu, 15] and [Leighton et al.,
12] can be derandomized using the conditional probability with pessimistic estimator
methodology.
3.2. Minimizing the number of wavelengths
Given the number k of fibers on each link of the network, we now would like to minimize the number of wavelength required for the WAP. As an example, this optimization
might model the interest of an operator who wants to upgrade the node equipments of an

132

FERREIRA ET AL.

existing network. In terms of hypergraph coloring, this means minimizing the number
of colors c such that a valid (k, c)-coloring of the hypergraph exists. We present an IP
formulation for this problem below. We define a variable xij for each node i and each
color j : xij = 1 if node i is colored with color j , and 0 otherwise. We have seen that
the number of colors is bounded by n/k if there are n nodes, this bound being tight
when the graph is an hyperclique. We also define a variable yj for each color j , yj = 1
if there is at least one node with color j . This variables allows for counting the number
of colors really used.
Integer program 2.
minimize
subject to


c

c

i∈H

yc

(minimize # of colors)

xic = 1

∀ node i,

xic  k

∀ color c, ∀ hyperedge H,

xic  yc
xic , yc ∈ {0, 1}

∀ color c, ∀ node i,
∀ color c, ∀ node i.

One could remark that IP 2 is not of high practical interest. Indeed, the program is
big with O(n2 ) variables and O(n2 m) constraints (we could reduce the number of constraints to O(nm) if the solver generates cuts automatically). Moreover, the symmetry
of the formulation is critical: Mehrotra and Trick [19] have shown that the branch-andbound techniques will waste a lot of time iterating trough similar solutions. The problem
arises because after a variable is constrained by the algorithm, a permutation of the set
of variables may still give a feasible solution of the same cost and this situation is uneasy to detect because of the yj variables. Nevertheless, this issue can be addressed by
automatic pruning techniques, as described by Margot [18].
4.

Implementation and performance evaluation

In order to validate the analysis of the presented algorithms and the relevance of our
model, we have implemented the two integer programs described in section 3, and the
randomized algorithms of Lu [15] and Leighton et al., 12. This allowed us to evaluate the
tradeoff between the performance and the running time of the exact version and the two
approximations. We also report our findings in the experience of solving the problem of
minimizing c. We ran tests on several instances and illustrate our results on two existing
continental backbone networks.
4.1. The instances
The first network we report on is depicted in figure 3. C OST 239 network interconnects
11 European capitals using 24 multi-fiber links. The demand matrix, which was provided

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

133

Figure 3. The C OST 239 Pan-European network.

Figure 4. The Pan-American network.

to us by France Telecom [20], is made of 176 requests, which cover all possible pairs of
cities. The maximum load of the given routing is 58, which is also a lower bound for the
number of colors in the single-fiber case.
The second network, depicted in figure 4, is a Pan-American backbone which interconnects 78 cities with 102 links. The demand matrix was generated with the wellknown gravitational model, where the weights of the cities represent their importance
and are proportional to the distance to 5 main population areas in the USA. Finally, the
demand between every two cities is proportional to the product of the two weights while
keeping the outgoing number of requests from every city equal to the weight. Using
different weights, we generated instances that were used for the benchmarks. We took
reliability issues into account, and computed the routing strategy through a minimum
cost disjoint paths problem for each origin-destination pair. For each origin and desti-

134

FERREIRA ET AL.

nation, the demand was randomly distributed among the disjoint paths with the shortest
total distance. We report on a relatively big instance with 2022 requests with load 520.
4.2. Results
The tests we ran have validated the theoretical analysis of the algorithms and some intuition on their practical behavior. Concerning the solving of IP 1 for minimizing the
number of fibers, the solver, CPLEX v7.5 found feasible solutions reasonably fast when
restricted to small instances: on the European C OST 239 network or the Pan-American
backbone with few available wavelengths the solver did not have difficulties in proving
optimality. It was expected, though, that when the instances grew bigger, the running
time was going to degrade because the underlying problem is NP-hard. Even if this
does not seem to be an issue for instances of reasonable size generated from real-world
networks, it is certain that an optimal optimization of networks with more than few tens
of available wavelengths is out of reach. Therefore approximate methodology are required when addressing networks deploying D - WDM technology with several hundreds
of wavelengths.
Our tests gave also some precisions on the behavior of the approximation algorithms. The algorithm of Lu [15] often produces solution within an approximation ration close to the one expected from its theoretical analysis. Moreover, the strategy of
dividing the set of wavelengths in 3 packs used one after the other has practical consequences on the behavior of the algorithm. The experience shows that the algorithm
behaves far better when the number of wavelength is a multiple of 3 and the number of
fibers computed by the algorithm decreases stepwise when the number of wavelengths
increases. Therefore, the practical approximation ratio varies within a wide range. The
algorithm of Leighton et al. [12] is far more efficient and we have seen that the recoloring procedures are never used in practice. Indeed, the randomized rounding always
gives solutions that are very close to the optimal.
Figure 5 shows optimal and approximate computations for C OST 239. It shows
the required number of fibers, as a function of the number of colors available. As said

Figure 5. Experiments on C OST 239.

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

135

Figure 6. Experiments on the Pan-American network: time and results.

above, the small size of this network allows IP 1 to be solved to optimality even with
a large number of colors. This allows for a precise comparison of the algorithms. The
results furthest from optimality are those of Lu’s algorithm, the stepwise dependence
of the number of fibers to the number of wavelength clearly appears. The algorithm of
Leighton et al. [12] produces solutions which are optimal up to an additive factor of 5.
On the other hand, because of the small size of the network, both approximate algorithms
finish almost immediately. Therefore, running times are not plotted for C OST 239 and
the running times are compared on the Pan-American network.
The big size of the instances on the Pan-American network highlights the asymptotic behaviors of the algorithms. Hence the running times of the different methodologies, depicted on the left-hand side of figure 6, clearly indicates that solving IP 1 is
exponential on the number of wavelengths and is not achievable after a certain threshold,
25 wavelengths on our computers (P IV 1 GHz, 512 Mo RAM). Moreover, this impossibility is also due to the explosion of the memory space required by the branch-andbound. The exponential growth of both the time and the memory shows that more modern computers will not solve much bigger instances and the optimization of networks
with several hundreds of wavelengths has to be done approximately. The randomized
rounding based algorithm of Leighton et al. [12] takes polynomial time and space which
may yield difficulties with networks of high capacity, but no real impossibility. What
is more surprising is that the running time of the algorithm of Lu [15] is almost constant and even slowly decreasing with the number of wavelengths. An explanation is
that increasing the number of wavelengths does not increase the combinatorial complexity of this algorithm. On the contrary, the number of nodes that enter the last coloring
phase may decrease with the number of wavelengths. As expected though, smaller running times are paid with the quality of approximation: the approximate number of fibers
given by Lu’s algorithm is around 3 times the optimal, while the algorithm of Leighton
et al. [12] stays close to the optimum up to an additive factor of 4. Thus, the tradeoff
between the quality of the approximation and the running time is obvious. We could use
randomized rounding to optimize a static network during an offline process where the
running time is not the main issue. Lu’s algorithm could be useful when time is an issue,
in online optimization process reacting to traffic modifications, for instance.

136

FERREIRA ET AL.

It is important to notice that in these instances, and often with real-world networks,
the number of colors equals to its lower bound, that is, the load of the network divided
by the number of fibers. It is known that pathological examples exist, but they do not
usually appear in real instances.
The biggest dependency of the running time of IP 2, which optimizes the number
of colors, is on the number of variables representing the colors. Initially, we used as
many colors as the number of requests, because that is an upper bound. Obviously,
this did not scale well when the size of the instances increased to real-world problems.
Instead, we performed a binary search for the upper bound of the colors. We relied on
the observation that when the bound is too small, the IP solver returns quickly that no
feasible solution exists. On the other hand, when the upper bound is not tight, it takes
too long to solve the first node of the branch-and-bound tree because there are too many
variables. With this strategy we got IPs of the correct size that could be handled by the
solver. As expected though, due to the symmetry in the formulation (the labeling of the
colors can be permuted without altering the solution), the enumeration of the nodes of
the branch-and-bound tree could not be always completed. In any case, we had a proof
of optimality: we found that when using one less color, the LP relaxation of the problem
was already not feasible. Therefore, showing a feasible solution with that many colors
was enough. Indeed, it would be interesting to characterize the integrality gap of that
problem.

5.

Conclusion

In this paper, we have proposed a framework to model the WAP in MWNs, reducing it
to a coloring problem on hypergraphs. Practically, the coloring problem appeared to be
tractable when there are few colors, since its straightforward IP formulation gives optimal solutions reasonably fast. Unfortunately, this is not the case for real-world instances
and the number of available wavelengths will dramatically increase with future D - WDM
and UD - WDM networks.
Furthermore, the heuristics that we implemented illustrate the tradeoff between
the quality of approximation and their running time. When an exact solution cannot be
computed in reasonable time and space, randomized rounding can be used to produce
very good solutions. When quicker solutions are required, one would rather follow
approaches based on Lu’s algorithm, which runs in quasi-constant time at the cost of
a multiplicative factor of 3 on the optimal solution.
It is also interesting to note that these hypergraph coloring algorithms may be useful in the context of radio ad-hoc network optimization. Indeed, the hypergraph structure
appears naturally when addressing capacity constrained radio coverage optimization.
Further work is still required in this direction.
Another interesting research direction is to study the design of MWNs in the case
where the routing is not fixed in advance. In such a case the lightpaths are not given, and
one needs to design both the routing and the wavelength assignment at once. We believe

MODELS, COMPLEXITY AND ALGORITHMS FOR WDM NETWORKS

137

that, as soon as k is large enough, this problem can be practically solved to optimality
[Coudert and Rivano, 6].

References
[1] S. Baroni, P. Bayvel, R.J. Gibbens and S.K. Korothy, Analysis and design of resilient multifiber
wavelength-routed optical transport networks, Journal of Lightwave Technology 17(5) (1999) 743–
754.
[2] B. Beauquier, J.-C. Bermond, L. Gargano, P. Hell, S. Pérennes and U. Vaccar, Graph problems arising
from wavelength-routing in all-optical networks, in: 2nd IEEE Workshop on Optics and Computer
Science, part of IPPS’97, Genova, Switzerland (IEEE Press, New York, 1997).
[3] I. Caragiannis, A. Ferreira, C. Kaklamanis, S. Pérennes and H. Rivano, Fractional path coloring with
applications to WDM networks, in: ICALP, eds. F. Orejas, P.G. Spirakis and J. van Leeuwen, Lecture
Notes in Computer Science, Vol. 2076 (Springer, New York, 2001) pp. 6–15.
[4] I. Chlamtac, A. Ganz and G. Karmi, Lightpath communications: An approach to high bandwidth
optical WAN’s, IEEE Transactions on Communications 40(7) (1992) 1171–1182.
[5] H. Choi, S. Subramaniam and H.-A. Choi, Optimal wavelength assignment algorithms for permutation
traffic in multifiber WDM ring networks, Photonic Network Communications 4(1) (2002) 37–46.
[6] D. Coudert and H. Rivano, Lightpath assignment for multifibers WDM optical networks with wavelength tranlators, in: Proc. IEEE Global Telecommunications Conf., Taipei, Taiwan, 2002, pp. 6–15.
[7] T. Erlebach, K. Jansen, C. Kaklamanis, M. Mihail and P. Persiano, Optimal wavelength routing on
directed fiber trees, Theoretical Computer Science 221(1/2) (1999) 119–137.
[8] D.S. Hochbaum, Approximation Algorithms for NP-Hard Problems (PWS, Boston, MA, 1997).
[9] E. Hyytiä and J. Virtamo, Wavelength assignment in multifibre in WDM-networks, Technical Report
COST257TD(99)04, Helsinki University of Technology (1999).
[10] V. Kumar, Approximating arc circular colouring and bandwidth allocation in all-optical ring networks,
in: First Internat. Workshop on Approximation Algorithms for Combinatorial Optimization Problems
(APPROX’98), 1998.
[11] J. Kuri, N. Puech, M. Gagnaire and E. Dotaro, Routing and wavelength assignment of scheduled
lightpath demands in a WDM optical transport network, in: Proc. of the 1st IEEE Internat. Conf. on
Optical Communications and Networks, Singapore, 2002.
[12] T. Leighton, C. Lu, S. Rao and A. Srinivasan, New algorithmic aspects of the Local Lemma with
applications to routing and partitioning, SIAM Journal on Computing 31(2) (2001) 626–641.
[13] G. Li and R. Simha, On the wavelength assignment problem in multifiber WDM star and ring networks, IEEE/ACM Transactions on Networking 9(1) (2001) 60–68.
[14] L. Li and A. Somani, A new analytical model for multifiber WDM networks, IEEE Journal on Selected
Areas in Communications 18(10) (2000) 2138–2145.
[15] C.-J. Lu, Deterministic hypergraph coloring and its applications, in: RANDOM, eds. M. Luby,
J.D.P. Rolim and M.J. Serna, Lecture Notes in Computer Science, Vol. 1518 (Springer, New York,
1998) pp. 35–46.
[16] L. Margara and J. Simon, Wavelength assignment problem on all-optical networks with k fibres per
link, in: ICALP, eds. U. Montanari, J.D.P. Rolim and E. Welzl, Lecture Notes in Computer Science,
Vol. 1853 (Springer, New York, 2000) pp. 768–779.
[17] L. Margara and J. Simon, Decidable properties of graphs of all-optical networks, in: ICALP, eds.
F. Orejas, P.G. Spirakis and J. van Leeuwen, Lecture Notes in Computer Science, Vol. 2076 (Springer,
New York, 2001) pp. 768–779.
[18] F. Margot, Pruning by isomorphism in branch-and-cut, in: IPCO, eds. K. Aardal and B. Gerards,
Lecture Notes in Computer Science, Vol. 2081 (Springer, New York, 2001) pp. 304–317.

138

FERREIRA ET AL.

[19] A. Mehrotra and M.A. Trick, A column generation approach for graph coloring, INFORMS Journal
on Computing 8(4) (1996) 344–354.
[20] Porto: Planification et Optimisation des Réseaux de Transport Optiques, Document under legal authorization of France Télécom R&D, RNRT project with the CNRS/INRIA/I3S project Mascotte, Alcatel
Corporate Research Center and France Télécom R&D (2000).
[21] R. Ramaswami and K.N. Sivarajan, Optical Networks: A Practical Perspective (Morgan Kaufman,
Los Altos, CA, 1998).
[22] N. Robertson and P.D. Seymour, Graph minors XIII. The disjoint paths problem, Journal of Combinatorial Theory Series B 63 (1995) 65–110.
[23] M. Saad and Z.-Q. Luo, A Lagrangian decomposition approach for the routing and wavelength assignment in multifiber WDM networks, in: Proc. of IEEE Global Telecommunications Conf., Taipei,
Taiwan, 2002.
[24] A. Srinivasan, An extension of the Lovász Local Lemma, and its applications to integer programming,
in: Proc. of the 13th Annual ACM–SIAM Symposium on Discrete Algorithms, Atlanta, GA, 1996, pp.
6–15.
[25] X. Zhang and C. Qiao, Wavelength assignment for dynamic traffic in multi-fiber WDM networks, in:
Proc. of the 7th Internat. Conf. on Computer Communications and Networks, Lafayette, LA, 1998,
pp. 479–585.

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Linearization: Locally Self-Stabilizing Sorting in Graphs
Melih Onus∗

Andrea Richa†

Abstract
We consider the problem of designing a distributed algorithm that, given an arbitrary connected graph G of nodes
with unique labels, converts G into a sorted list of nodes.
This algorithm should be as simple as possible and, for scalability, should guarantee a polylogarithmic runtime as well
as at most a polylogarithmic increase in the degree of each
node during its execution. Furthermore, it should be selfstabilizing, that is, it should be able to eventually construct
a sorted list from any state in which the graph is connected.
It turns out that satisfying all of these demands at the same
time is not easy.
Our basic approach towards this goal is the so-called
linearization technique: each node v repeatedly does the
following with its neighbors:

Christian Scheideler‡

to be self-stabilizing. In general, self-stabilization of a
system guarantees that regardless of a current system’s
state, the system will converge to a legal state in a finite
number of steps. Before presenting previous work on
self-stabilization, we start with some basic definitions.
Definition 1.1. A system is a pair S = (C, →) where
C is a set of states of the system and → is a binary
transition relation on C. A computation of S is a
non-empty sequence (c1 , c2 , . . .) such that for all i ≥ 0,
ci ∈ C and ci → ci+1 .

In distributed systems the above mentioned ci expresses a global state, which is the concatenation of the
local states of every process and the contents of every
• for its right (i.e., larger) neighbors w1 , . . . , w` in the or- communication channel. A commonly used definition of
der of increasing labels, v replaces {v, w1 }, . . . , {v, w` }
self-stabilization is:
by {v, w1 }, {w1 , w2 }, . . . , {w`−1 , w` }.

• for its left (i.e., smaller) neighbors u1 , . . . , uk in the order of decreasing labels, v replaces {v, u1 }, . . . , {v, uk }
by {v, u1 }, {u1 , u2 }, . . . , {uk−1 , uk }, and

As shown in this paper, this technique transforms any
connected graph into a sorted list, but there are graphs for
which this can take a long time. Hence, we propose several
extensions of the linearization technique and experimentally
evaluate their performance. Our results indicate that some
of these have a polylogarithmic performance, so there is hope
that there are distributed algorithms that can achieve all of
our goals above.

Definition 1.2. A system S is self-stabilizing with
respect to a set of legal states L ⊂ C if for any state
c ∈ C, any computation of the system starting with c
will eventually reach a state in L.

The subset L is problem-specific. Often, it is
assumed that L is closed, i.e. every non-faulty move
from a state in L ends in a state in L. An algorithm is
said to be self-stabilizing if it promotes self-stabilization
1 Introduction
of the system. A locally self-stabilizing algorithm is a
Due to their many applications, peer-to-peer systems self-stabilizing algorithm that runs locally at each node
have recently received a lot of attention both inside and that only assumes local knowledge of the network
and outside of the research community. Peer-to-peer by the nodes (e.g., the algorithm does not assume that
systems are usually dynamic in nature, that is, peers nodes know or have an estimate on the number of nodes
continuously enter and leave the system. Also, peers in the network).
It turns out that the crux of designing truly selfmay fail. In order to handle such a situation, distributed
stabilizing
algorithms for many structured overlay peeralgorithms are needed that can quickly and reliably
to-peer
networks
such as Chord and skip graphs is
recover a peer-to-peer system from any inconsistent
to
find
a
self-stabilizing
protocol that can convert an
state. Systems with such a property are also known
arbitrary connected graph into a sorted list or ring.
For scalability, such algorithms should converge to a
∗ Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-8809, USA. Email: sorted list in at most a polylogarithmic number of
rounds. A distributed algorithm for arranging the
melih@asu.edu
† Department of Computer Science and Engineering, Arinodes of an arbitrary connected graph into a sorted
zona State University, Tempe, AZ 85287-8809, USA. Email: list in a polylogarithmic number of rounds and with
aricha@asu.edu. This work was supported in part by NSF CApolylogarithmic work for each node is already known [1],
REER award no. 9985284.
‡ Institute for Computer Science, Technical University of Mubut the algorithm is not locally self-stabilizing. In fact,
nich, 85748 Garching, Germany. Email: scheideler@in.tum.de
under certain conditions it can run into a deadlock (i.e.,

99
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

the nodes cannot escape a certain invalid state). Hence,
there exists the need for developing truly (locally) selfstabilizing algorithms that can converge to a sorted list
from an arbitrary initial state in polylogarithmic time.
We are now ready to formally state the problem
we consider in this paper. We are given an arbitrary
connected graph G(V, E), |V | = n, with node labels
which induce a total order on V . For nodes u and
v, we say that u < v (resp., u > v), whenever the
label of node u is smaller (resp., greater) than that
of node v. For simplicity, we assume that all the
node labels are distinct. We consider the problem of
designing a distributed, locally self-stabilizing algorithm
that converts G into a sorted list H({v1 , . . . , vn }, EH ),
where vi ∈ V , and vi−1 < vi and {vi−1 , vi } ∈ EH for all
i.
The idea of self-stabilization in distributed computing first appeared in a classical paper by E.W. Dijkstra in 1974 [6] in which he looked at the problem of
self-stabilization in a token ring. Since Dijktra’s paper,
self-stabilization has been studied in many contexts, including communication protocols, graph theory problems, termination detection, clock synchronization, and
fault containment. For a recent survey see, e.g., [4],
and a very comprehensive list of papers can be found in
Herman’s Self-Stabilization Bibliography from 2002 [8].
For more information about self-stabilizing algorithms
we also recommend the book by Dolev [7].
Self-stabilization issues have also been considered
for peer-to-peer networks. In the technical report of the
Chord system [10], for example, several techniques such
as the weak stabilization protocol and the strong stabilization protocol are presented that allow the Chord
network to recover quickly from various kinds of degenerate states which the authors call pseudo-trees or loopy
states. However, the running time of these algorithms
depends on the degree of distortion of the degenerate
state, and can be as high as linear on the number of
nodes. Moreover, no technique is given that allows the
Chord network to recover from an arbitrary connected
state. Instead, to prevent Chord from ever running into
a state outside of pseudo-trees or loopy states, it is suggested that each node keeps Θ(log n) successor pointers instead of just one. In this way, nodes can recover
their successor pointers with high probability even if
the nodes in the system fail with constant probability.
Though this protects against faulty nodes, it is known
that this technique will not protect against adversarial
nodes [3], and therefore it is still important to understand how to recover from an arbitrary state.
In a technical report on skip graphs [2] the authors
propose a self-stabilization protocol in which each node
v periodically checks six locally checkable conditions. If

one of these conditions is violated, it is locally repaired.
It is shown that an overlay network forms a skip graph
if and only if all of the six conditions are satisfied for
all nodes. However, the local checking of the conditions
does not work for arbitrary states but only for states
in which the nodes form a degenerate version of a
skip graph that may be created due to node faults or
inconsistencies in concurrent join or leave operations.
As for the Chord repair mechanisms, the total running
time for repairing the conditions at each node depends
on the distortion of the degenerate state and can be as
high as linear.
Self-stabilizing protocols for organizing the nodes
into a sorted ring have already been proposed in [5, 9].
In the Iterative Successor Pointer Rewiring Protocol
(ISPRP) proposed in [5], each node aims at maintaining
correct successor pointers. If there is a local inconsistency, i.e., there are two nodes u and v with successor
pointers to a node w, w asks either u or v (depending on
their label) to repair its successor pointer. In the Ring
Network (RN) protocol proposed in [9], each node v periodically initiates the search for a closest successor by
sending a search request to a randomly chosen neighbor.
Any node w receiving such a request forwards it to the
neighbor of w closest to v. This is continued until there
is no such neighbor, or a neighbor is found that is closer
to v than its closest successor. In the latter case, v is
informed about it. Both protocols can have a very long
runtime. Hence, the question remains whether there are
simple self-stabilizing protocols with a polylogarithmic
runtime.
1.1 Our contributions We propose linearization as
the basic technique to sort any connected graph. This
technique works as follows: each node v repeatedly does
the following with its neighbors:
• for its left (i.e., smaller) neighbors u1 , . . . , uk in
the order of decreasing labels, v replaces {v, u1 },
. . . , {v, uk } by {v, u1 }, {u1 , u2 }, . . . , {uk−1 , uk },
and
• for its right (i.e., larger) neighbors w1 , . . . , w` in
the order of increasing labels, v replaces {v, w1 },
. . . , {v, w` } by {v, w1 }, {w1 , w2 }, . . . , {w`−1 , w` }.
As shown in this paper, the linearization technique
is self-stabilizing and transforms any connected graph
into a sorted list, but there are graphs for which this
can take a long time. Even for random graphs, the
pure linearization (PL) technique appears to perform
poorly. Our goal is to find self-stabilizing protocols that
are guaranteed to have a polylogarithmic runtime (with
high probability), or at least a polylogarithmic runtime

100
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

for random graphs. Hence, we propose several extensions of the linearization technique and experimentally
evaluate their performance. These are:

v < u (resp., u < v).
Figure 1 illustrates the left and right neighbors of a node
u with label 10.

• Linearization with memory: Each node only linearizes the edges that it got in the previous round
but also maintains (“remembers”) edges to all
nodes that it was connected to at any time. It uses
this node list to accelerate the linearization process
of the new edges. See Section 3 for details.
• Linearization with shortcut neighbors: Each node
only linearizes the edges that it got in the previous
round but also maintains edges to a certain set of
Figure 1: Left and right neighbors of node u
O(log n) other shortcut nodes that it learned about
over the time. Also, every node exchanges its set
of shortcut nodes with every node it is connected
In the pure linearization algorithm (PL), each node
with by a new edge. See Section 4 for details.
repeatedly converts its left neighborhood and its right
neighborhood into sorted lists at each round. More
Though linearization with memory (LM) still has a linspecifically, if a node u has left neighbors w1 < w2 <
ear runtime in the worst case, our experiments demon. . . < wk at the start of a left linearization step, it
strate that for random graphs the LM protocol signifconverts its left neighbors into a sorted list by removing
icantly outperforms the PL protocol. LM appears to
the edges {u, wi }, 1 ≤ i ≤ k − 1 and adding the edges
have a polylogarithmic runtime for these graphs, and
{wi , wi+1 }, 1 ≤ i ≤ k − 1 (Figure 2(a)). Similarly, if
the node degree seems to be polylogarithmic as well.
v < v2 < . . . < vj are the right neighbors of u at
Since linearization with shortcut neighbors (LSN) is 1
the start of a right linearization step, u removes the
based on the concept of a virtual space, also this proedges {u, vi }, 2 ≤ i ≤ j and adds the edges {vi , vi+1 },
tocol can perform poorly in the worst case (extremely
1 ≤ i ≤ j − 1 (Figure 2(b)) to the graph. Note that in
unevenly distributed node labels), but our experiments
a round every node first converts its left neighbors into
with LSN demonstrate that it is still much better than
a sorted list and then converts its right neighbors into a
the LM protocol for random graphs. Thus, LSN apsorted list. If at any step there is a conflict where a node
pears to be a good candidate for further, rigorous invesu wants to remove an edge {u, v} and an another node w
tigations towards the goal of finding a self-stabilizing
wants to add this edge {u, v}, the final outcome will be
protocol with polylogarithmic runtime.
that the edge {u, v} remains in the graph. Algorithm 1
summarizes the PL algorithm.
1.2 Structure of the paper In Section 2 we present
and analyze the pure linearization technique. After- Algorithm 1 Pure Linearization (PL)
wards, we study the linearization with memory techEach node u executes the following:
nique in Section 3 and the linearization with shortcut
1: for each round do
neighbors technique in Section 4. Section 5 presents and
2:
(Left linearization step) If w1 < w2 < . . . < wk
interprets the simulation results for all three techniques.
are
the current left neighbors of u, u removes the
The paper ends with conclusions.
edges {u, wi }, 1 ≤ i ≤ k − 1 and adds the edges
{wi , wi+1 }, 1 ≤ i ≤ k − 1 to the graph.
2 Pure Linearization
3:
(Right linearization step) If v1 < v2 < . . . < vj
In this section we present the pure linearization techare the current right neighbors of u, u removes
nique. We are given as input a connected undirected
the edges {u, vi }, 2 ≤ i ≤ j and adds the edges
graph G(V, E) with node labels. At any time t during
{vi , vi+1 }, 1 ≤ i ≤ j − 1 to the graph.
the execution of the algorithm, we will let G(V, E) de4: end for
note the current configuration of graph G, after edges
may have been inserted or removed from the original
The following two sections provide a formal analysis
edge set during prior time steps. We start with some
of the complexity and several other properties of the PL.
definitions:
Definition 2.1. At any time t, node v is a left (resp., 2.1 Analysis. Note that the PL algorithm is memoright) neighbor of node u if {u, v} ∈ E at time t and ryless, that is, it only needs to know the current state

101
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

(a)

Proof. Let V = {a1 , a2 , a3 , ..., an }, where ai < aj for all
i < j, i, j ∈ {1, 2, ..., n}. Recall that one round of the
PL algorithm consists of a left linearization step followed
by a right linearization step. The following three claims
will be used in the proof of Theorem 2.2. The claims
bound the number of left and right neighbors out of each
node after each round of the PL algorithm.
Claim 2.1. After k rounds an , an−1 , an−2 , ..., an−k+1
will each have exactly one left neighbor for every k =
1, 2, ..., n − 2.

(b)
Figure 2: Neighbors of node u after one round of pure
linearization.

of the connections in order to converge to a sorted list.
Hence, if we can show that the PL algorithm converges
to a sorted list for any connected graph, the PL algorithm is self-stabilizing. In the following two theorems
we will prove matching lower and upper bounds showing
that in the worst case the PL algorithm requires (n − 2)
rounds to converge to a sorted list.
Theorem 2.1. (Lower bound) The pure linearization algorithm requires at least (n−2) rounds to generate
a sorted list out of an arbitrary graph G.
Proof. Consider the example graph G(V, E), E =
{{i, i + 1}|i ∈ {2, 3, ..., n − 1}} ∪ {{n, 1}} illustrated in
Figure 3. After the first round, the edge {n, 1} is removed and the edge {n − 1, 1} is added to G. After the
second round, the edge {n − 1, 1} is removed and the
edge {n − 2, 1} is added to G. After the third round,
the edge {n − 2, 1} is removed and the edge {n − 3, 1} is
added to G. The PL algorithm will take (n − 2) rounds
to convert G to a sorted list since after the j th round,
the edge {n−j +1, 1} is removed and the edge {n−j, 1}
is added to G, 1 ≤ j ≤ n − 2.
t
u

Figure 3: Lower bound example for PL algorithm

Theorem 2.2. (Upper bound) After (n − 2) rounds,
the pure linearization algorithm will generate a sorted
list out of any graph G.

Proof. We will prove this claim by induction on k.
Base case: k = 1. We will show that after the first
round an will have exactly one left neighbor. During
the left linearization step of the first round, an will
remove all of its left neighbors except for one. Since an is
the largest node, it does not have any right neighbors.
Thus no node can add a left neighbor to an during a
left linearization step. Hence after the left linearization
step, an will have only one left neighbor, which we call
bn . If bn only has one right neighbor (an itself) after the
right linearization step, the left neighbor of an will not
change. If bn has more than one right neighbor, after
the right linearization step, the left neighbor of an will
change and it will be the second largest right neighbor
of bn . Thus, after the first round an will have only one
left neighbor.
Inductive Hypothesis:
After
k
rounds
an , an−1 , an−2 , ..., an−k+1 will each have exactly
one left neighbor, k < n − 2.
Inductive Step: We will show that after k + 1
rounds an , an−1 , an−2 , ..., an−k+1 , an−k will each have
exactly one left neighbor, k ≤ n − 2.
We will first show that after k + 1 rounds
an , an−1 , an−2 , ..., an−k+1 will still have one left neighbor each. From the inductive hypothesis, we know
that after k rounds an , an−1 , an−2 , ..., an−k+1 will
have only one left neighbor each. Consider node ai ,
n − k + 1 ≤ i ≤ n at round k + 1. Since ai only has
one left neighbor it will not do anything during the
left linearization step of this round. Since all nodes
aj > ai have only one left neighbor each, they cannot
add a left neighbor to ai . So, after the left linearization
step of round (k + 1), ai will still have only one left
neighbor (say bi ). If bi has no right neighbor smaller
than ai , the left neighbor of ai will not change after
right linearization step of round k + 1. If bi has a
right neighbor smaller than ai , the left neighbor of ai
will change and it will be the largest right neighbor
of bi which is less than ai . Thus, after k + 1 rounds
ai will have only one left neighbor, n−k +1 ≤ i ≤ n−1.

102
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Secondly, we will show that after k + 1 rounds
an−k will also have only one left neighbor. From
the inductive hypothesis, we know that after k rounds
an , an−1 , an−2 , ..., an−k+1 will have only one left neighbor each. We start by showing that an−k must have
at least one left neighbor after round k + 1. Then we
will show that an−k cannot have more than one left
neighbor after this round. By contradiction, assume
that after k rounds an−k has no left neighbor. Since
all nodes greater than an−k have only one left neighbor
each, all of the right neighbors of an−k must have only
one left neighbor which is an−k (if an−k has no right
neighbor the graph is disconnected.). Since the graph
is connected, an−k must have a path to a1 . However,
the right neighbors of an−k cannot have a left neighbor
smaller than an−k (since an−k is their only left neighbor)
and similarly the right neighbors of the right neighbors
of an−k cannot have a left neighbor smaller than an−k ,
and so on. Therefore, there is no path between an−k
and a1 which is a contradiction.
Now we show that an−k cannot have more than one
left neighbor after round (k + 1). We have just seen
that after k rounds an−k has at least one left neighbor.
During the left linearization step of round (k + 1), an−k
will remove all of its left neighbors except one. Since
all nodes aj > an−k have only one left neighbor, they
cannot add a left neighbor to an−k . So, after the left
linearization step, an−k will have only one left neighbor
(say bn−k ). If bn−k has no right neighbor smaller than
an−k , the left neighbor of an−k will not change during
the right linearization step of round (k + 1). If bn−k has
a right neighbor smaller than an−k , the left neighbor of
an−k will change and it will become the largest right
neighbor of bn−k which is less than an−k . Hence, after
k + 1 rounds an−k will have exactly one left neighbor,
completing the proof.
t
u

Proof. This statement is the dual of Claim 2.2 and its
proof is analogous. The main difference in the proof is
that we start the algorithm with a left linearization, but
this will not affect the proof significantly.
t
u
We are now ready to prove Theorem 2.2. We know,
by Claim 2.2 and Claim 2.3, that each node ai has
exactly one left and one right neighbor after n − 2
rounds of the PL algorithm. In addition, we will show
that the (only) left neighbor of node ai must be ai−1 ,
1 < i ≤ n. This is trivially true for i = 2. Assume
that the left neighbor of ai is ai−1 , and hence the (only)
right neighbor of ai−1 is ai , for all 1 ≤ i ≤ k. This
trivially implies that the left neighbor of ak+1 must be
ak (and the right neighbor of ak must be ak+1 ), and
hence a1 , ..., an is a sorted list of V .
t
u
2.2 Other properties of PL In the following
claims, we will present several properties of PL algorithm regarding the degree of the nodes and the required
number of rounds.
Claim 2.4. At each round of the PL algorithm, the
degree of a node v increases by at most an additive
constant.

Proof. Assume a node u has l left and r right neighbors
at the start of a round. After the left linearization step
of this round, node u will have (i) at most r+1 left
neighbors, since each of its right neighbors can add at
most one more left edge to u and only one of its previous
left neighbors will remain; and (ii) node u will have at
most r right neighbors, since each of its right neighbors
will either remain or be changed to another node. In
summary, after the left linearization step, node u will
have at most r + 1 left and r right neighbors. Similarly,
after the right linearization step of this round, node u
Claim 2.2. After n − 2 rounds all nodes except a1 will will have at most r + 1 left and r + 2 right neighbors.
have exactly one left neighbor each.
Following the same argument, after the left linearization
step of the next round, node u will have at most r+3 left
Proof. From Claim 2.1 we know that after n − 2 rounds and r + 2 right neighbors, and after the corresponding
an , an−1 , an−2 , ..., a3 will have one left neighbor each. right linearization step, node u will have r + 3 left and
We show that a2 will also have exactly one left neighbor r + 4 right neighbors. Hence, after t left and right
(a1 ). After n − 2 rounds, since the graph is connected, linearizations steps, node u will have at most r + 2t − 1
t
u
there must be a path between a2 and a1 . By contra- left and r + 2t right neighbors.
diction, assume a2 and a1 are not neighbors, implying
that a2 has only right neighbors. By Claim 2.1, the
An edge {u, w} is said to cross a node v if u ≤ v ≤
right neighbors of a2 only have right neighbors and so w. The following claim bounds the degree of a node v
on. Thus we cannot reach a1 from a2 , a contradiction. by the number of edges crossing the node.
t
u
Claim 2.5. The maximum degree of a node v during
Claim 2.3. After n − 2 rounds all nodes except an will the PL algorithm is bounded by the number of edges
have exactly one right neighbor each.
crossing it.

103
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Proof. Linearization ensures that for any edge (u, w)
converted into (u0 , w0 ) during a linearization step: u ≤
u0 ≤ w0 ≤ w.
3 Linearization with Memory
In this section we present an extension of the pure linearization technique called the linearization with memory (LM). In the linearization with memory algorithm,
nodes remember all of their old neighbors. While placing current neighbors into sorted lists, a node u will also
use its old neighbors in order to ”better place” its current neighbors within the final total order, as we will
show.
In the LM algorithm, nodes will place their current
left and right neighbors into sorted lists by reconnecting
them. When placing a left neighbor v into the sorted
list, a node u will remove the edge {u, v} and edge
{v, w} will be added, where w is the smallest node u
remembers (w may be a current neighbor of node u)
such that v < w. If v is the largest left neighbor of
node u, edge {u, v} remains. When placing a right
neighbor v into the sorted list, a node u will remove
the edge {u, v} and edge {w, v} will be added, where w
is the largest node u remembers (w may be a current
neighbor of node u) such that w < v. If v is the
smallest right neighbor of node u, edge {u, v} remains.
For example, consider a node u that currently has left
neighbors v1 < v2 < v3 < v4 and in addition remembers
the nodes w1 < w2 < w3 < w4 , where w1 < v1 < v2 <
w2 < v3 < w3 < w4 < v4 < u. The LM algorithm
would place u’s left neighbors into the sorted list by
removing the edges {u, v1 }, {u, v2 }, {u, v3 } and adding
the edges {v1 , v2 }, {v2 , w2 }, {v3 , w3 }, as illustrated in
Figure 4. Algorithm 2 summarizes the LM technique.
Algorithm 2 Linearization with Memory (LM)
Each node u executes the following:
1: for each round do
2:
Node u will remember all of its neighbors
3:
For each current left neighbor v of u, u replaces
edge {u, v} by edge {v, w}, where w is the smallest
node u remembers such that v < w, if such a node
w exists. Otherwise, u keeps the edge {u, v}.
4:
For each current right neighbor v of u, u replaces
edge {u, v} by edge {w, v}, where w is the largest
node u remembers such that w < v, if such a node
w exists. Otherwise, u keeps the edge {u, v}.
5: end for

(a)

(b)
Figure 4: Node u places its current left neighbors into
sorted list by reconnecting them.

on the average running time of the PL algorithm. The
major drawback of the LM technique is that the degree
of a node may increase very fast, since nodes ”never
forget” a neighbor (see Table 2). The linearization with
shortcut neighbors (LSN), introduced in Section 4, is a
refinement of the LM technique that aims at keeping
the degree of the nodes lower (and at speeding up the
LM process).
We now show that LM algorithm requires n − 2
rounds to generate a sorted list of the nodes in the worst
case. For any two nodes v and w, where u < w, let
|w−u| denote the cardinality of the set {v : u < v ≤ w}.
Theorem 3.1. The linearization with memory algorithm still requires n − 2 rounds to generate a sorted
list of the nodes in the worst-case.
Proof. The LM algorithm will behave just like the PL
algorithm for the graph in Figure 3 and hence the lower
bound of Theorem 2.1 also applies to the LM algorithm.
The following two claims are needed to prove that
the LM algorithm requires at most n − 2 rounds to
generate a sorted list of the nodes.

Claim 3.1. At any round of the algorithm, if a node
u remembers a node v < u (node v may be a current
neighbor of u), then there exists an increasing path
between u and v in the current graph, i.e. there exists
a path P = v1 , v2 , ..., vp , where v1 = v, vp = u, and vi
is a current left neighbor of vi+1 , 1 ≤ i ≤ j − 1 (also
While the LM technique alone still has a linear implying that v1 < v2 < ... < vj , j ≥ 2).
worst-case running time (see Corollary 3.1), its average
running time seems to be polylogarithmic as the experi- Proof. Let v be a left neighbor that u currently rememmental results in Section 5 show, improving significantly bers. We will prove by induction on |u − v|.

104
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Base case: |u − v| = 1. Node v is thus the largest
left neighbor node u has seen. According to the LM
algorithm, v must still be a current left neighbor of node
u, and hence P = vu.
Inductive Hypothesis: If |u − v| ≤ k, k ≥ 1, there
exists an increasing path between u and v in the current
graph.
Inductive Step: We will show that if |u − v| = k + 1,
then there exists an increasing path P between u and v
in the current graph.
If v is a current left neighbor of node u, P = vu and
we are done. Thus assume that node u only remembers
node v. Since v is not a current left neighbor of node
u, either u reconnected node v (i.e., u replaced edge
{u, v} by another edge in step 3 of the LM algorithm)
or v reconnected node u, at some prior time. Without
loss of generality, assume that u reconnected node v (the
proof is analogous if v reconnected u), by replacing edge
{u, v} by an edge {v, w}, where w was (and may still be)
a current neighbor of u and v < w < u. Hence |v − w|
and |w − u| are at most k (and u remembers both v and
w currently). By the induction hypothesis, there must
exist increasing paths P1 and P2 in the current graph
between v and w and between w and u, respectively.
Hence P1 P2 is an increasing path between u and v in
the current graph and the induction hypothesis follows.
t
u

only remembers at most 2(log n + 1) carefully selected
shortcut neighbors, which function as representatives of
exponentially increasing length intervals out of node v,
and (ii) at each round, a node exchanges information
about its shortcut neighbors with all of its current
or shortcut neighbors at that time. Without loss of
generality, we assume that each node has an arbitrary
label belonging to the interval [0, 1). Each node v
divides [v, 1) into the intervals Ii+ (v) = [v + 2i−1 /n, v +
2i /n), for all 1 ≤ i ≤ k such that v + 2k /n < 1 and
v + 2k+1 /n > 1; and the intervals I0+ (v) = [v, v + 1/n)
+
and Ik+1
(v) = [v + 2k /n, 1). Each node v divides [0, v]
into the intervals Ii− (v) = [v − 2i /n, v − 2i−1 /n), for all
1 ≤ i ≤ j such that v − 2j /n > 0 and v − 2j+1 /n < 0;
+
and the intervals I0− (v) = [v − 1/n, v] and Ij+1
(v) =
+
−
j
[0, v−2 /n). For each of the intervals Ii and Ii , node v
will remember one shortcut neighbor that belongs to the
interval, if such a node exists. Hence v will remember
at most 2(log n + 1) shortcut nodes. Those shortcut
neighbors that v remembers will aid v in placing its
current neighbors within the sorted list, in the same
way as the LM algorithm.

Figure 5: Intervals for node v

Claim 3.2. The graph remains connected after any
number of rounds of the LM algorithm.
Proof. From Claim 3.1, we know that if a node u Algorithm 3 Linearization with Shortcut Neighbors
remembers a node v, there exists an increasing path (LSN)
between u and v in the current graph (either v was a
Each node u executes the following:
left neighbor of u at some prior time, or u was a left
1: for each round do
neighbor of v, and the theorem above applies). Nodes
2:
Send your shortcut neighbors to your original and
will always remember their initial neighbors. So, from
shortcut neighbors
Claim 3.1, there exists a current neighbor path between
3:
Receive shortcut neighbors of your neighbors.
nodes and their initial neighbors. So, graph will always
4:
Check the shortcut neighbors u just received from
remain connected.
t
u
its neighbors and the current neighbors u has. If
there exists a node v that belongs to an interval
Claim 3.2 shows that the graph will remain conIi+ (u) or Ii− (u) which was not covered by any of
nected. Thus, Claims 2.1, 2.2, and Claim 2.3 all hold
the prior neighbors of node u (i.e., there is no
for the LM algorithm (taking into account only the curshortcut neighbor for this interval at node u), add
rent neighbors of a node, and not all the neighbors the
v to u’s shortcut neighbors list (if there are two or
node remembers). Hence, the upper bound in Theorem
more such nodes, select one of them arbitrarily).
2.2 also applies to the LM algorithm.
5:
Reconnect the current left neighbors of u, as det
u
scribed in Step 3 of the LM algorithm (node u
only remembers its current neighbors and short4 Linearization with Shortcut Neighbors
cut neighbors, at any time).
In this section we present the linearization with shortcut
6:
Reconnect the current right neighbors, as deneighbors (LSN) technique. This technique can be
scribed in Step 4 of the LM algorithm.
seen as a variation of the LM technique. The main
7: end for
ideas behind the LSN algorithms are that (i) a node v

105
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

The intuition behind the LSN algorithm is our
hope that by using geometrically decreasing intervals
near each node and collecting neighbors for these, the
network topology converges quickly against something
close to a hypercubic network. A hypercubic network
structure would have the advantage that nearest neighbors can be found much quicker (basically in logarithmic
time via binary search) than in arbitrary networks. In
fact, it is not difficult to show that linearization on top
of a hypercubic network would result in a sorted line
within O(log n) communication rounds. Algorithm 3
describes the LSN algorithm in detail.
The LSN algorithm also performs poorly in the
worst-case, since, like the PL and LM algorithm, it is
based on the concept of a virtual space (an extremely
unevenly distribution of node labels would hurt the
algorithm). However, it is not clear what the exact
worst-case complexity of the LSN algorithm is. The
upper bound given by Theorem 2.2 still applies to the
LSN algorithm. However, since nodes now exchange
their neighbor information with each other, the example
used in the proof of Theorem 2.1 no longer gives a lower
bound of n − 2.
5 Experimental Results
5.1 Regular Graphs All three algorithms, PL, LM,
and LSN, are implemented with Java. The runtimes of
the algorithms are first compared for connected random
graphs such that the expected degree of each node is
10. The random graphs are generated such that the
probability of existence of each edge is 10/(n−1), where
n is number of nodes in the graph. If the generated
graph is not connected, a new graph is generated. We
run experiments for graphs with up to 16000 nodes.
Each experiment is repeated 1000 times. We calculate
the average number of required rounds.
PL
LM
LSN

500
122
26.3
18.5

1000
275.7
45.41
25.97

2000
634.6
79.47
35.28

4000
1347
135.2
43.93

8000
2835
227.7
53.4

16000
5485.2
394.83
65.1

(a)

(b)
Figure 6: Average number of rounds for PL, LM and
LSN
maximum node degree is computed experimentally (Figure 7 and Table 2). As expected, the LM algorithm
has a greater average maximum degree than the LSN
algorithm. Still, the experimental results indicate that
the maximum node degree might be polylogarithmic for
both algorithms.

LM
LSN

50
25.9
12.9

100
35
17.1

200
46.1
22.1

400
58.38
27.15

1000
76.8
33.39

2000
92.1
37.74

Table 1: Average number of rounds for PL, LM and Table 2: Average maximum node degree for LM and
LSN (for each node avg number of neighbors is 10)
LSN (for each node avg number of neighbors is 10)
The experimental results show that the LSN algorithm improves over the LM algorithm and both LM and
LSN algorithms seem to have a polylogarithmic runtime
(Table 1 and Figure 6). The former observation is somewhat surprising when taking into account that the nodes
in LM may collect much more edges over time than LSN.
However, the fact that LSN uses a structured approach
of collecting edges still seems to give it an advantage
Next, for the LM and LSN algorithms the average

5.2 Power Law Graphs In recent work, it is
claimed that the Internet and certain unstructured P2P
network topologies satisfy the power law [11, 12, 13, 14].
Definition 5.1. (Power law) The number of nodes
that have degree k is proportional to 1/k α , where α is a
constant number and α > 1.

106
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

(a)
Figure 7: Average maximum node degree for LM and
LSN (for each node avg number of neighbors is 10)

We generated random power law graphs according
to the Chung-Lu model [15], with α = 2 and ran
some experiments. To generate these graphs, we first
generated the degrees of the nodes according to the
power law. Then we considered every node pair {i, j}
and selected an edge between them with probability
di ∗dj
2m where di is the desired degree of node i and m
is number of edges in the graph (Chung-Lu model [15]).
Thus, we have a graph with node i having expected
degree di .

PL
LM
LSN

500
170
13.9
12.1

1000
339
21.62
15.21

2000
699.2
30.08
18.12

4000
1420
39.74
22.97

8000
2786
54.15
28.08

16000
7142
62.67
38.167

Table 3: Average number of rounds for PL, LM and
LSN (graphs satisfy power law with α = 2)
As for regular random graphs, the experimental
results indicate that the LM and LSN algorithms seem
both to have an expected polylogarithmic runtime for
random power law graphs (Table 3 and Figure 8).
5.3 Bounded Locality We generated random
graphs with bounded locality B, i.e. each edge in the
graph crosses at most B nodes. Recall that an edge
{u, v} crosses a node w if u < w < v. We generated
random graphs with 1000 nodes and expected node
degree of 10. We ran experiments for different values of
B, by letting B = 125, 250, 500, 1000.
The experimental results also indicate that the
expected running time of the PL algorithm increases
linearly and that the expected running time of both the
LM and LSN algorithms increase polylogarithmicly with
B (Table 4, Figure 9).

(b)
Figure 8: Average number of rounds for PL, LM and
LSN (graphs satisfy power law with α = 2)

6 Conclusions
We presented distributed local self-stabilizing algorithms for converting an arbitrary graph into a sorted
list. In the pure linearization (PL) algorithm, each node
repeatedly converts its neighbors into a sorted list. We
show that the worst-case running time of the PL algorithm is linear. Hence, we further looked at proper
extensions of the PL algorithm, namely, the linearization with memory (LM) and linearization with shortcut
neighbors (LSN) algorithms. Both algorithms are based
on the idea of remembering old neighbors. Experimental results on random graphs, random power law graphs
and random graphs with bounded locality show that the
LM and LSN algorithms significantly improve the PL algorithm and that these algorithms seem to have a polylogarithmic time complexity in expectation. Moreover,
the LSN algorithm has better average runtime and maximum node degree than the LM algorithm. Ultimately,
we would like to provide a rigourous argument proving
the expected polylogarithmic behaviour of the LSN and
LM algorithms. However, this has proven to be a very
challenging task and is left for future work.

107
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

Downloaded 06/09/17 to 149.169.115.174. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

PL
LM
LSN

125
45.18
13.82
11.04

250
88.25
22
15.36

500
181.1
34.13
21.55

1000
275.7
45.41
25.97

Table 4: Average number of rounds for PL, LM and
LSN with bounded locality

Figure 9: Average number of rounds for PL, LM and
LSN with bounded locality

[10] I. Stoica, R. Morris, D. Liben-Nowell, D. Karger,
M.F. Kaashoek, F. Dabek and H. Balakrishnan, Chord:
A Scalable Peer-to-peer Lookup Service for Internet
Applications, Technical Report, MIT, 2002.
[11] Michalis Faloutsos, Petros Faloutsos and Christos
Faloutsos, On power-law relationships of the Internet
topology, SIGCOMM ’99: Proceedings of the conference on Applications, technologies, architectures, and
protocols for computer communication, 1999, pp. 251–
262.
[12] Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar Raghavan, Sridhar Rajagopalan, Raymie Stata,
Andrew Tomkins and Janet Wiener, Graph structure
in the web, 9th Int. WWW Conference, 2000.
[13] Albert-Laszlo Barabasi and Reka Albert, Emergence
of Scaling in Random Networks, Science, (286) 1999,
pp. 509–512.
[14] Matei Ripeanu, Ian Foster and Adriana Iamnitchi,
Mapping the Gnutella Network: Properties of LargeScale Peer-to-Peer Systems and Implications for System Design, IEEE Internet Computing Journal special
issue on peer-to-peer networking, vol. 6(1) 2002.
[15] F. Chung, L. Lu. Connected components in random
graphs with given degree sequences. Annals of Combinatorics 6 (2002) 125-145.

References
[1] D. Angluin, J. Aspnes, J. Chen, Y. Wu and Y. Yin,
Fast construction of overlay networks, Proc. of the 17th
ACM Symp. on Parallel Algorithms and Architectures
(SPAA), 2005, pp. 145–154.
[2] J. Aspnes and G. Shah, Skip graphs, Proc. of the 14th
ACM Symp. on Discrete Algorithms (SODA), 2003,
pp. 384–393.
[3] B. Awerbuch and C. Scheideler, Group Spreading: A
protocol for provably secure distributed name service,
Proc. of the 31th International Colloquium on Automata, Languages and Programming (ICALP), 2004,
pp. 183–195.
[4] J. Brzezinski and M. Szychowiak, Self-Stabilization in
Distributed Systems - a Short Survey, Foundations of
Computing and Decision Sciences, 25 (1), 2000.
[5] C. Cramer and T. Fuhrmann, Self-Stabilizing Ring Networks on Connected Graphs, Technical Report 2005-5,
System Architecture Group, University of Karlsruhe,
2005.
[6] E.W. Dijkstra, Self-stabilization in spite of distributed
control, Communications of the ACM, 17 (1974),
pp. 643–644.
[7] S. Dolev, Self-Stabilization, MIT Press, 2000.
[8] T. Herman, Self-Stabilization Bibliography: Access
Guide, University of Iowa, 2002.
[9] A. Shaker and D.S. Reeves, Self-stabilizing structured
ring topology P2P systems, 5th IEEE Int. Conference
on Peer-to-Peer Computing, 2005, pp. 39–46.

108
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited

A Markov Chain Algorithm for Compression in
Self-Organizing Particle Systems ∗
Sarah Cannon

†

Joshua J. Daymude

Georgia Institute of Technology
Atlanta, GA, USA

sarah.cannon@gatech.edu

‡

Arizona State University
Tempe, AZ, USA

jdaymude@asu.edu

§

‡

Dana Randall

Andréa W. Richa

Georgia Institute of Technology
Atlanta, GA, USA

randall@cc.gatech.edu

Arizona State University
Tempe, AZ, USA

aricha@asu.edu

ABSTRACT

ponentially small probability, the perimeter will be at least
β · pmax , where pmax is the maximum possible perimeter.

We consider programmable matter as a collection of simple
computational elements (or particles) with limited (constantsize) memory that self-organize to solve system-wide problems of movement, configuration, and coordination. Here,
we focus on the compression problem, in which the particle system gathers as tightly together as possible, as in a
sphere or its equivalent in the presence of some underlying
geometry. More specifically, we seek fully distributed, local,
and asynchronous algorithms that lead the system to converge to a configuration with small perimeter. We present a
Markov chain based algorithm that solves the compression
problem under the geometric amoebot model, for particle systems that begin in a connected configuration with no holes.
The algorithm takes as input a bias parameter λ, where
λ > 1 corresponds to particles favoring inducing more lattice triangles within the particle system. We show that for
all λ > 5, there is a constant α > 1 such that at stationarity with all but exponentially small probability the particles
are α-compressed, meaning the perimeter of the system configuration is at most α · pmin , where pmin is the minimum
possible perimeter of the particle system. We additionally
prove that the same algorithm can be used for √
expansion for
small values of λ; in particular, for all 0 < λ < 2, there is a
constant β < 1 such that at stationarity, with all but an ex-

Keywords
Self-organizing Particles; Compression; Markov Chains

1.

INTRODUCTION

Many programmable matter systems have recently been
proposed and realized—modular and swarm robotics, synthetic biology, DNA tiling, and smart materials form an incomplete list—and each is often tailored toward a specific
task or physical setting. In our work on self-organizing particle systems, we abstract away from specific settings and
instead describe programmable matter as a collection of simple computational elements (to be referred to as particles)
with limited computational power that each perform fully
distributed, local, asynchronous algorithms to solve systemwide problems such as movement, configuration, and coordination. Here we present an algorithm for compression, in
which the particle system gathers as tightly together as possible, as in a sphere or its equivalent in the presence of some
underlying geometry. This phenomenon is often found in
natural systems: fire ants form floating rafts by gathering
in such a manner, and honey bees communicate foraging
patterns within their hives. While each individual ant or
bee cannot view the group as a whole when soliciting information, it can take cues from its immediate neighbors
to achieve cooperation. It is with this motivation that we
present a distributed algorithm for compression in the amoebot model derived from a Markov chain process.
In the (geometric) amoebot model, more formally defined
in Section 2.1, particles with limited computational power
move among the vertices of the triangular lattice Γ (Figure 1(a)) by traveling along the edges of Γ. The compression
problem seeks to reorganize the configuration of a particle
system (via movements of particles) such that the system
converges to a configuration with small perimeter, where
we measure the perimeter of a configuration by the length
of the walk along its boundary. We say a particle system
is α-compressed, for α > 1, if the perimeter of the particle configuration is at most α times the minimum possible
perimeter for those particles.

∗A full version of this paper, including omitted proofs, is available
at www.arxiv.org/abs/1603.07991.
†Supported in part by NSF DGE-1148903 and a grant from the
Simons Foundation (#361047 to Sarah Cannon)
‡Supported in part by NSF awards CCF-1353089, CCF-1422603,
and REU-026935.
§Supported in part by NSF grant CCF-1526900.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

PODC’16, July 25-28, 2016, Chicago, IL, USA
c 2016 ACM. ISBN 978-1-4503-3964-3/16/07. . . $15.00

DOI: http://dx.doi.org/10.1145/2933057.2933107

279

1.1

Our results and techniques

tions typically are not required to be connected and can
have holes. Here, we focus on the connected a0nd hole-free
model to show compression can be achieved, even in an asynchronous distributed system where particles have constantsize memory and use only local information.

We present a Markov chain M for particle compression
under the geometric amoebot model that can be directly
translated into a fully distributed, local, asynchronous compression algorithm A. Both A and M take as input a bias
parameter λ (where λ > 1 makes induced lattice triangles
more favorable) and start from an arbitrary initial configuration for the particles that is connected and has no holes.
Markov chain M is carefully designed according to the distributed and local nature of the system, so that the particles
always stay connected and no holes form. Furthermore, we
prove M is reversible and ergodic, meaning many of the standard tools of Markov chain analysis can be applied. While
most of these proofs rely only on first principles, we emphasize they are far from trivial; working in a distributed setting
necessitated carefully defined protocols for local moves that
made proofs challenging.1
When the particles execute the local moves of M (by running A) for long enough, the configuration of the particles
converges to the stationary distribution of M. We prove for
all large enough λ there is a constant α = α(λ) > 1 such
that at stationarity, with all but exponentially small probability, the particles are α-compressed, meaning the perimeter
of the particle configuration√is at most α times the minimum perimeter (which is Θ( n) for systems of n particles).
We additionally show the counterintuitive result that λ > 1
is not enough
to guarantee compression. In fact, for all
√
0 < λ < 2, there is a constant β < 1 such that at stationarity with all but exponentially small probability the perimeter
is at least a β fraction of the maximum perimeter, which is
Θ(n) for systems of n particles. We call such a √
configuration
β-expanded. This implies that for any 0 < λ < 2, the probability that the particles are α-compressed is exponentially
small for any constant α.
The motivation underlying the design of this Markov chain
is from statistical physics, where ensembles of particles reminiscent of our amoebot model are used to study physical
systems. Like a spring relaxing, systems tend to favor configurations that minimize energy. The energy function is
determined by a Hamiltonian H(σ); each configuration σ
has weight w(σ) = e−B·H(σ) /Z, where where B = 1/T is inP
verse temperature and Z = τ e−B·H(τ ) is the normalizing
constant known as the partition function.
In our amoebot model, we assign each configuration σ a
Hamiltonian H(σ) = −t(σ), where t(σ) is the number of triangles in σ, i.e., the number of faces of the triangle lattice Γ
with all three vertices occupied by particles. Setting λ = eB ,
we get w(σ) = λt(σ). As λ gets larger (by increasing B, effectively lowering temperature), we favor configurations with
a large number of occupied triangles, causing increasingly
compressed configurations. Favoring edges with both endpoints occupied is an alternative metric we could consider,
but we prove for connected configurations on Γ without holes
the two measures are equivalent. Likewise, favoring shorter
perimeter is a third equivalent representation.
The key tool used to establish compression is a careful
Peierls argument, used in statistical physics to study nonuniqueness of limiting Gibbs measures and determining the
presence of phase transitions (see, e.g., [9]), and in computer science to establish slow mixing of Markov chains
(see, e.g., [2]). For standard Peierls arguments, configura1

1.2

Related Work

Our algorithm is derived from a carefully designed Markov
chain, enabling us to provide provable guarantees of its behavior. Random particle processes on the grid with hard
constraints (e.g., simple exclusion processes where no two
particles can occupy the same location) have been studied
in statistical physics, but we want results under the further
constraints of distributed computing. Our work exploits the
memoryless, stochastic nature of Markov chains to accomplish particle compression in the amoebot model, just one of
many distributed physical systems where compression-type
problems have been studied.
When considering physical systems and models, one can
differentiate between active and passive systems. Particles
in passive systems have no explicit control over their movements, and in some cases do not have any computational
power. One example is the DNA self-assembly work described in [19], where strands of DNA gather together to
form larger structures with certain patterns.
In active systems, particles have control over their behavior and—depending on the model—can achieve some directed locomotion. Swarm robotics is one example; different
variations of shape formation and collection problems have
been studied (e.g. [11, 17]), but always with more computational power or global knowledge of the system. Similarly,
pattern formation and creation of convex structures has been
studied in the cellular automata domain (e.g. [4, 8]), but differs from our model by assuming more powerful computational capabilities. The nubot model [20] addresses a framework for biomolecular-inspired models which—although allowing some non-local movements—provides additional ways
to create two dimensional shapes in polylogarithmic time.
Nature offers a variety of examples in which gathering and
cooperative behavior is apparent. For example, social insects
often exhibit compression-like characteristics in their collective behavior: fire ants form floating rafts [15], cockroach larvae perform self-organizing aggregation [13, 16], and honey
bees choose hive locations based on a decentralized process
of swarming and recruitment [3].
The rendezvous (or gathering) problem seeks to gather
mobile agents together on some node of a graph (see e.g. [1]
and the references within). In comparison, our particles follow the exclusion principle, and hence would not be able
to gather at a single node, and are computationally simpler
than the mobile agents considered.
Lastly, in [5, 6], algorithms for hexagon shape formation in the amoebot model were presented. Although a
hexagon satisfies a compressed configuration as we define
here, the Markov chain-based algorithm we present takes
a fully decentralized and local approach. This is naturally
self-stabilizing, forgoing the need for a seed particle that
may coordinate or initiate some underlying organization of
the set of particles, as required in [5] and even more critically
in [6].

Due to length constraints some proofs were omitted.

280

2.2

First, we introduce notation and terminology that will be
used throughout this paper. We call the collection of locations in Γ that are occupied by particles an arrangement;
note two arrangements are the same even if particles occupy
different locations within the arrangement. We can define
an equivalence relation on arrangements, where two arrangements are equivalent if one is a translation of the other. We
define a configuration to be an equivalence class of arrangements. If configuration σ is a rotation of configuration τ , we
still consider σ and τ to be distinct configurations. That is,
for the purpose of monitoring the particle system we maintain a global orientation of the particles, even though each
individual particle has no sense of global orientation.
We will let capital letters refer to particles and lower case
letters refer to locations on the triangular lattice Γ, e.g.,
“particle P at location `.” For a particle P (resp., location `), we use N (P ) (resp., N (`)) to denote the set of particles adjacent to P (resp., to `), where by adjacent we mean
connected by a lattice edge. Similarly, for a particle P (resp.,
location `), we will use n(P ) (resp., n(`)) to denote the six
locations in the neighborhood of P (resp., of `). For locations ` and `0 , by N (` ∪ `0 ) we mean (N (`) ∪ N (`0 )) \ {`, `0 };
the same holds for n(` ∪ `0 ).
By an edge or triangle of a configuration σ we mean, respectively, an edge or face of Γ such that all (respectively,
two or three) incident vertices are occupied by particles. The
number of edges of σ is e(σ) and the number of triangles is
t(σ). Throughout, by a path or a cycle we mean a path or cycle in the underlying graph Γ where all vertices are occupied
by particles, and in the case of a cycle, at least one location
inside the cycle is unoccupied. Two particles are connected
if there exists a path between them, and a configuration is
connected if all pairs of particles are. A hole in a configuration is a maximal finite component of adjacent unoccupied
locations. We specifically consider connected configurations
with no holes, and our algorithm, if starting at such a configuration, will maintain these properties.

(a)
(b)
Figure 1: (a) A section of the triangular lattice Γ; (b) expanded
particles (each denoted by its two occupied adjacent locations in
Γ and a thick line in between) and contracted particles (occupying
one location).

2.

BACKGROUND AND MODEL

We begin with the geometric amoebot model for programmable matter. We then define some properties of particle systems and discuss what it means for a particle system
to be compressed. We conclude with an overview of Markov
chains, which form the basis for our algorithm for compression to be presented in Section 3.

2.1

Terminology for Particle Systems

The Amoebot Model

In the amoebot model [7], programmable matter consists of
particles whose properties we now detail. An infinite undirected graph G = (V, E) represents the set of relative locations that the particles can occupy (given by V ) and the
set of all possible atomic transitions between locations in V
(given by E) [7]. We further assume the geometric variant of
the amoebot model, which imposes an underlying geometric
structure G = Γ, where Γ is the triangular lattice shown in
Figure 1(a) (also called the infinite regular triangular grid
graph, and denoted by Geqt in earlier work).
Each particle occupies either a single location (i.e., it is
contracted) or a pair of two adjacent locations on the graph
(i.e., it is expanded); Figure 1(b) illustrates expanded and
contracted particles on Γ. Each location can be occupied
by at most one particle. Particles achieve movement via a
series of expansions and contractions: a contracted particle
may expand into an adjacent unoccupied location to become
expanded, and completes its movement by contracting to
once again occupy only one location.
Two particles occupying adjacent nodes are said to be connected by a bond (and hence induce an edge in the particle
system), and we refer to them as neighbors. Particles are
anonymous, but can uniquely identify each one of their (six)
possible bonds and check which bonds lead to nodes occupied by neighboring particles. Additionally, the particles do
not share any global orientation or coordinate system.
Every particle has a constant-size, shared, local memory
which both it and its neighbors can read and write to for
communication. Because of the limitation on memory size,
particles cannot know neither the total size of the system
nor an estimate of it. Particles execute a sequence of atomic
actions, in each of which they do some local computation (in
our case, those may involve checking which of its adjacent
locations are occupied with particles) and an expansion or
contraction. We assume a fully asynchronous system, where
particles perform atomic actions concurrently and at different, possibly variable speeds; conflicts, which in our context
arise when two particles attempt to expand into the same
location, are resolved in an arbitrary manner. In order to
analyze such systems, we use the standard asynchronous
model from distributed computing, allowing us to evaluate
the progress of the system through a sequential series of individual particle activations, where every time a particle is
activated, it performs an atomic action.

2.3

Compression of Particle Systems

Our objective is to find a solution to the particle compression problem. There are many ways to formalize what
it means for a particle system to be compressed. For example, one could try to minimize the diameter of the system,
maximize the number of edges, or maximize the number of
triangles. We choose to define compression in terms of minimizing the perimeter. We prove for connected configurations
with no holes, minimizing perimeter, maximizing the number of edges, and maximizing the number of triangles are
all equivalent and are stronger notions of compression than
minimizing the diameter.
For a connected configuration σ of n particles with no
holes, the perimeter of σ, denoted p(σ), is the length of the
walk around the (single external) boundary of the particles.
In an abuse of notation, we use the term perimeter to refer
both to the length p(σ) of this walk and the walk itself. We
assume any walk W along the perimeter of a configuration is
in the clockwise direction. For a walk W in Γ with some no−
→
tion of direction, we say location ` is left of edge e = st ∈ W
traversed from s to t if `, s and t are the vertices of a triangular face of Γ with ` the next vertex counterclockwise around
this face from t. With this terminology, every edge traversed
in a clockwise walk W along the perimeter of σ has an unoc-

281

√
implies the diameter of the particle system is also O( n),
so our definition of α-compression is stronger than defining
compression in terms of diameter.
In order to minimize perimeter using only simple local
moves, we exploit the following relationship.

cupied location to its left. Specifically, for any consecutive
particles A, B, C in W, the locations in the clockwise span
of n(B) from A to C are always unoccupied and there is
at least one of them. Note an edge may appear twice in a
perimeter walk W; in this case, its length is counted twice
in p(σ). For a connected configuration of n particles without holes, the perimeter ranges from a maximum value of
2n − 2 when the particles are in their least compressed state
(a tree with no
√ induced triangles) to some minimum value
their most compmin (n) = Θ( n) when the particles are in√
pressed state. It is easy to see pmin (n) ≤ 4 n, and√we now
prove any configuration σ of n particles has p(σ) ≥ n; this
bound is not tight but suffices for our proofs.

Lemma 2.3. For a connected particle configuration σ with
no holes, t(σ) = 2n − p(σ) − 2.
Proof. We count particle-triangle incidences, of which
there are 3t(σ). Counting another way, every particle has six
incident triangles, except for those on the perimeter. Consider any traversal W of the perimeter; at each particle, the
exterior angle is 120, 180, 240, 300, or 360 degrees. These
correspond to the particle “missing” 2, 3, 4, 5, or 6 of its possible six incident triangles, or degree/60 missing triangles. If
W visits the same particle multiple times, count the appropriate exterior angle at each visit. The sum of exterior angles
along W is 180p(σ)+360, so in total particles on the perimeter are missing 3p(σ)+6 triangles. This implies 6n−3p(σ)−6
particle-triangle incidences, so 3t(σ) = 6n − 3p(σ) − 6.

Lemma 2.1. A connected configuration
√ with n ≥ 2 particles and no holes has perimeter at least n.
Proof. We argue by induction on n. A particle √
system
with two particles necessarily has perimeter 2 ≥ 2, as
claimed. Let σ be any particle configuration with n particles where n > 2, and suppose the lemma holds for all
configurations with less than n particles.
First, suppose there is a particle Q ∈ σ not incident to
any triangles of σ. This implies Q has one, two, or three
neighbors, none of which are adjacent. If Q has one neighbor, removing Q from σ yields a configuration√σ 0 with n − 1
particles and, by induction, perimeter at least n − 1. Thus
√
√
p(σ) = p(σ 0 ) + 2 ≥ n − 1 + 2 ≥ n.

Minimizing perimeter is also equivalent to maximizing
edges, as we now show.
Lemma 2.4. For a connected particle configuration σ with
no holes, e(σ) = 3n − p(σ) − 3.
Proof. The proof is nearly identical to that of Lemma 2.3,
counting particle-edge incidences instead.

If Q has two neighbors, removing Q from σ produces two
connected particle configurations σ1 and σ2 , where σ1 has n1
particles, σ2 has n2 particles, and n1 + n2 = n − 1. Thus
√
√
√
√
p(σ) ≥ n1 + n2 + 4 > n − 1 + 4 > n.

Corollary 2.5. For a connected particle configuration σ
with no holes, t(σ) = e(σ) − (n − 1).
Corollary 2.6. A connected particle configuration σ with
no holes and minimum perimeter is also a configuration with
the maximum number of triangles (and edges).

Similarly, if Q has three neighbors its removal produces three
particle configurations with n1 , n2 , and n3 particles where
n1 + n2 + n3 = n − 1 and we conclude
√
√
√
√
p(σ) ≥ n1 + n2 + n3 + 6 > n.

2.4

Markov chains

The distributed protocol for particle compression we present is based on a Markov chain, i.e., a memoryless stochastic
process defined on a finite set of states Ω. The transition
matrix P on Ω × Ω → [0, 1] is defined so that P (x, y) is the
probability of moving from state x to state y in one step, for
any pair x, y ∈ Ω. The t-step transition probability P t (x, y)
is the probability of moving from x to y in exactly t steps.
A Markov chain is ergodic if it is irreducible, i.e., for all
x, y ∈ Ω, there is a t such that P t (x, y) > 0, and aperiodic,
i.e., for all x, y ∈ Ω, g.c.d.{t : P t (x, y) > 0} = 1. Any finite,
ergodic Markov chain converges to a unique distribution π,
i.e., for all x, y ∈ Ω, limt→∞ P t (x, y) = π(y). In fact, for
any distribution π 0 such that π 0 (x)PP
(x, y) = π 0 (y)P (y, x)
(the detailed balance condition) and x∈Ω π 0 (x) = 1, π 0 is
the unique stationary distribution of M (see, e.g., [10]).
Given a desired stationary distribution π on Ω, the celebrated Metropolis-Hastings algorithm [12] defines appropriate transition probabilities. Starting at state x, pick a neighbor y in Ω uniformly with probability 1/(2∆), where ∆ is
the maximum number of neighbors of any state, and move
to y with probability min (1, π(y)/π(x)); with the remaining
probability stay at x and repeat. Using detailed balance, one
can verify if the state space is connected then π must be the
stationary distribution. While calculating π(x)/π(y) seems
to require global knowledge, this ratio can often be calculated using only local information when many terms cancel

Now, suppose every particle in σ is incident to some triangle of σ, implying there are at least n/3 triangles
√ in σ. An
equilateral triangle with side length 1 has area √3/4, so the
perimeter of σ encloses an area of at least A = 3n/12. By
the isoperimetric inequality, the minimum perimeter way of
enclosing this area, without regard to lattice constraints, is
with a circle of radius r and perimeter p, where
s
r
√
r
√
A
n 3
πn
r=
=
, p = 2πr = √ > n.
π
12π
3
√
As the perimeter of σ also
√ encloses an area of at least 3n/12,
it is of length at least n.
When n is clear from context we omit it and refer to pmin =
pmin (n) and pmax = pmax (n). We now define what it means
for a particle system to be compressed.
Definition 2.2. For any α > 1, a connected configuration σ with no holes is α-compressed if p(σ) ≤ α · pmin .
We prove in Section 4 that our algorithm, when executed
for a sufficiently long time, achieves α-compression with all
but exponentially small probability for any constant α >
1, provided n is sufficiently large. We note α-compression

282

out. In our case, the Metropolis probabilities are simply
min(1, λ|t(y)−t(x)| ); this difference is just the change in the
number of triangles incident to the moving particle, which
can be calculated with only local information.

3.

arbitrary starting configuration σ0 of contracted particles
that is connected and has no holes.
Algorithm A: Algorithm for compression run by particle P
If P is contracted:
1: Let ` denote P ’s current location.
2: Particle P chooses neighboring location `0 uniformly at
random from the six possible choices, and generates a
random number q ∈ (0, 1).
3: if `0 is unoccupied and P has no expanded neighboring
particle at ` then
4:
P expands to simultaneously occupy ` and `0 .
5:
if there are no expanded particles adj. to ` or `0 then
6:
P sets f lag = T RU E in its local memory
7:
else P sets f lag = F ALSE.

ALGORITHMS FOR COMPRESSION

Our objective is to demonstrate how stochastic algorithms
can provably achieve compression, focusing on self-organizing
particle systems on the triangular lattice Γ. Our algorithm
is based on Markov chain principles that will enable us to
prove rigorous results about the algorithm’s behavior. Remarkably, it does not even require the particles to communicate more than one bit of information to each other, even
though the amoebot model allows for such exchanges; at any
activation, a particle only needs to know which of its neighboring locations are occupied and if any of those neighbors
are expanded.
Our algorithm carefully maintains several critical properties throughout its execution. First, the particle system
stays connected and no holes form, even while particles decide where to move based only on local information. Additionally, any moves made are reversible: if a particle moves
to a new location, there is a nonzero probability that during
the next step, it moves back to its previous location. Finally,
the moves allowed by the algorithm (and respective Markov
chain) suffice to transform any configuration of particles into
any other configuration. These conditions are essential so
certain tools from Markov chain analysis can be applied.
In addition to the precise conditions needed to ensure connectivity and reversibility, our algorithm achieves compression by making particles more likely to move into a position
where they form more triangles with their neighbors. Specifically, a bias parameter λ controls how strongly the particles
favor being incident to triangles; λ > 1 corresponds to favoring triangles, while λ < 1 corresponds to disfavoring triangles. As Lemma 2.3 shows, locally favoring more triangles
is equivalent to globally favoring a shorter perimeter; this is
the relationship we exploit to obtain particle compression.

3.1

If P is expanded:
8: Let t be the number of triangles formed by P when it
was contracted at its original location `, and let t0 be
the number of triangles formed by P if it contracts at
the location it expanded into, `0 .
9: if (1) location ` does not have five neighboring particles,
(2) locations ` and `0 satisfy Property 1 or Property 2,
0
(3) q < λt −t , and (4) f lag = T RU E then
10:
P contracts to `0 .
11: else P contracts back to `.
Each particle P continuously runs Algorithm A, executing
Steps 1–7 if P is contracted, and Steps 8–11 if P is expanded.
Note a constant number of random bits suffice to generate q,
as only a constant precision is required (given that t0 −t is an
integer in [−6, 6] and λ is a constant). In Step 9, Condition
(1) ensures no holes form; Condition (2) ensures the particles stay connected and the corresponding Markov chain
M is reversible; Condition (3) ensures the moves happen
with probabilities such that M converges to the desired distribution; and Condition (4) ensures P is the only particle
in its neighborhood potentially moving to a new position,
`0 , since P last expanded. The claims regarding Conditions
(1)–(3) pertain to the underlying Markov process and will
be formalized in the next subsections. To understand why
Condition (4) suffices to ensure P is the only particle (potentially) moving to a new location in its immediate neighborhood, note that once P expands, any other particle P 0
expanding into a position adjacent to P will contract back to
its original position, since P 0 will set its own flag to FALSE
in Step 7. We assume any conflicts arising from two particles attempting to concurrently move into the same location
are resolved arbitrarily. Hence, any concurrent movements
will cover pairwise disjoint neighborhoods and the respective
actions will be mutually independent.
Following the classical asynchronous model [14], for any
starting configuration σ0 , we have that for any concurrent
execution of A that reaches a configuration σ, there is a
sequential execution of the same atomic actions that also
reaches σ, for all σ.

The Local Algorithm A

We start with two key properties that enable a particle to
move from location ` to location `0 . If ` and `0 are neighboring locations on Γ, let S = N (`) ∩ N (`0 ) be the set of
particles adjacent to both ` and `0 (i.e., |S| = 0, 1, or 2).
Property 1. |S| = 1 or 2 and every particle in N (` ∪ `0 )
is connected to a particle in S by a path through N (` ∪ `0 ).
Property 2. |S| = 0; ` and `0 each have at least one
neighbor; all particles in N (`) \ {`0 } are connected by paths
within this set; and all particles in N (`0 ) \ {`} are connected
by paths within this set.
These properties capture precisely the structure required to
maintain particle connectivity and prevent holes from forming. Additionally, both are symmetric for ` and `0 , necessary for reversibility. However, they are not so restrictive as
to limit the movement of particles and prevent compression
from occurring; we will see moves satisfying these properties
suffice to transform any configuration into any other.
We now present the local asynchronous algorithm that
each particle runs. Later we show how to view this algorithm
through the lens of our Markov chain M. Both A and M
take as an input a bias parameter λ > 1, and begin in an

3.2

The Markov Chain M

We now make explicit the Markov chain M corresponding
to Algorithm A. The state space of M is the set of all connected configurations of n contracted particles and no holes.

283

proximated using Poisson clocks with mean 1.2 That is,
each particle activates, and executes Algorithm A, at a random real time drawn from the exponential distribution e−t .
After each action, the particle then computes another random time drawn from the same distribution e−t and executes again after that amount of time has elapsed. The
exponential distribution is unique in that, if particle P has
just activated, it is equally likely that any particle will be the
next particle to activate, including particle P (see, e.g., [10]).
Moreover, the particles update without requiring knowledge
of any of the other particles’ clocks. Similar Poisson clocks
are commonly used to describe physical systems that perform updates in parallel in continuous time.

Transitions between states happen subject to the rules and
probabilities we now present.
Beginning at any connected, hole-free configuration σ0 of
contracted particles, repeat:
1. Select particle P uniformly at random from among all
particles; let ` be its location. Choose, uniformly at
random, `0 ∈ n(`) and q ∈ (0, 1).
2. If `0 is unoccupied, then P expands to simultaneously
occupy ` and `0 ; Else return to Step 1.
3. Let t be the number of triangles formed by P in position `, and let t0 be the number of triangles formed by
P in position `0 .

3.3

Invariants for Markov chain M

We begin by showing M maintains certain invariants. We
prove Conditions (1) and (2) in Step 4 of Markov chain M
ensure the particles remain in a connected configuration with
no holes, provided they start in such a configuration.

4. If (1) location ` does not have five neighboring particles, (2) locations ` and `0 satisfy Property 1 or Prop0
erty 2, and (3) q < λt −t then contract P to `0 .
5. Else contract P back to `.

Lemma 3.1. If the particles are initially connected, during the execution of Markov chain M they remain connected.

In M, we have paired up the consecutive actions that a
particle will take according to Algorithm A when contracted
and then expanded into one state transition, so that a transition corresponds to the complete movement of exactly one
(contracted) particle into a new (adjacent) location. Let P
be a particle that eventually moves from location ` to `0 at
some time t0 according to an execution of A. Condition (4)
in Step 9 and the condition in Step 3 of A ensure that if P
was activated and expanded at time t before being activated
once again and contracting to occupy `0 at time t0 > t, even
if other particles are activated between t and t0 no movement to a new location will occur in the neighborhood of P
in that time interval. Hence all activations of other particles
in the neighborhood of P in the interval (t, t0 ) can be ignored, justifying the pairing of actions in the Markov chain
M.
For every sequential execution of atomic actions that leads
to configuration σ 0 in A, there exists a sequence of transitions in M that reaches a configuration σ such that σ can
be obtained from σ 0 by preserving the locations of all contracted particles in σ 0 and by letting every expanded particle
in σ 0 contract according to the rules of A, for all corresponding pairs (σ 0 , σ). Conversely, every sequence of transitions
in M that reaches a configuration σ directly corresponds to
a sequence of atomic actions in A also leading to σ 0 = σ.
The perimeter of the respective σ 0 and σ differs by at most a
constant factor, and hence proving α-compression for σ also
implies α0 -compression for σ 0 , and vice-versa, for constants α
and α0 . Hence, we can use M, and respective Markov chain
tools and techniques, in order to analyze the correctness of
algorithm A.
We note that, under the assumptions of the asynchronous
model of distributed computing, one cannot typically assume the next particle to be activated is equally likely to be
any particle, as we assume in Step 1 of the description of M.
We make this assumption in order to be able to explicitly
calculate the stationary distribution of M so that we can
provide rigorous guarantees about its structure, but do not
expect the behavior of the system would be substantially
different if this requirement was relaxed.
To justify this random activation assumption, we note
that random sequences of particle activations can be ap-

Proof. Consider one iteration of M where a particle P
moves from location ` to location `0 . Let σ be the configuration before this move, and σ 0 the configuration after. We
show if σ is connected, then so is σ 0 .
A move of particle P from ` to `0 occurs only if ` and `0
are adjacent and satisfy Property 1 or Property 2. First,
suppose they satisfy Property 1. Let P1 , P2 6= P be particles,
and let Q be a path connecting them in σ. If P ∈
/ Q, then P1
and P2 remain connected by Q in σ 0 . If P ∈ Q, let N1
and N2 be the two particles on path Q before and after P ,
respectively. By Property 1, there exist paths in N (` ∪ `0 )
from N1 to a particle S1 ∈ S and from N2 to a particle
S2 ∈ S, possibly with S1 = S2 . A (not necessarily simple)
path from P1 to P2 in σ 0 is the union of Q from P1 to N1 ;
the path from N1 to S1 in N (` ∪ `0 ); if S1 6= S2 , the path
from S1 to P at location `0 to S2 ; the path from S2 to N2
in N (` ∪ `0 ); and Q from N2 to P2 . As P is connected to
S1 ∈ S and by the above argument S1 is connected to all
other particles, we conclude P is connected to every other
particle and thus σ 0 is connected.
Next, assume locations ` and `0 satisfy Property 2. Let
P1 , P2 6= P be particles; we show they are connected by a
path not containing P . Path Q connecting P1 and P2 in σ
exists, and suppose it contains P . Let N1 and N2 be the vertices on Q before and after P , respectively. Both N1 and N2
are neighbors of `, and by Property 2 all neighbors of ` are
connected by a path in N (`). Thus Q can be augmented to
form a (not necessarily simple) walk Q0 by replacing P with
a path from N1 to N2 in N (`). As P ∈
/ Q0 , this walk also
connects P1 and P2 in σ 0 . Additionally, because `0 has at
least one neighbor by Property 2, P remains connected to
all other particles in σ 0 and thus σ 0 is connected.
Lemma 3.2. If the particles begin in a connected configuration with no holes, during the execution of Markov chain
M they will never form a configuration with a hole.
Proof. Recall we assume a cycle in σ encircles at least
one unoccupied location; note a configuration has a hole if
2

The analysis can be modified to accommodate each clock
having its own constant mean; however, for the sake of ease
of presentation, we assume here that they are all i.i.d.

284

and only if it has a cycle encircling that hole. Let σ be a
particle configuration with particle P at location `, and σ 0
the same configuration with P at neighboring location `0 .
We assume σ has no cycles (i.e., no holes) and prove σ 0 has
no cycles (i.e., no holes).
We first show if a cycle is introduced in σ 0 , then P must
be on that cycle. Suppose this is not the case and σ 0 has
a cycle C with P ∈
/ C. If P is removed from location `0 , C
still exists. If P is then placed at `, yielding σ, then C
still exists unless it had enclosed exactly one unoccupied
location, `. However, this is not possible as any cycle in
σ 0 \P encircling ` would also necessarily encircle neighboring
unoccupied location `0 . This implies C is present in σ, a
contradiction, so we conclude any cycle in σ 0 must contain P .
By the conditions in Step 4 of Markov chain M that must
be met before a move occurs, particle P necessarily has fewer
than five neighbors in σ and locations ` and `0 satisfy Property 1 or Property 2. First, suppose they satisfy Property 2.
While P might momentarily create a cycle when it expands
to occupy both locations ` and `0 , it will then contract to
location `0 . Suppose P is part of some cycle C in σ 0 . Before
and after P on C are some neighbors N1 and N2 of P . By
Property 2, N1 and N2 are connected by a path in N (`0 ),
which doesn’t contain P . Replacing path N1 − P − N2 in cycle C by this path in N (`0 ) yields a (not necessarily simple)
cycle C 0 in σ 0 not containing P , a contradiction.
Suppose ` and `0 satisfy Property 1; recall location ` has
less than five neighbors in σ. Suppose there exists a cycle C
in σ 0 , which by definition encircles at least one unoccupied
location. If ` is unoccupied inside C, then so is at least one
of its neighbors; we conclude C encircles some unoccupied
location `00 6= `. Let N1 and N2 be the particles on cycle C
before and after P . If there exists a path between N1 and N2
in N (`0 ), the argument in the previous paragraph applies, so
we suppose this is not the case. It must be, without loss of
generality, that |S| = 2 and there exist paths in N (` ∪ `0 )
from N1 to S1 ∈ S and from N2 to S2 ∈ S, with S1 6= S2 .
There then exists cycle C 0 in σ, obtained from C by replacing
path N1 − P − N2 , where P is in location `0 , with the path
N1 −...−S1 −P −S2 −...−N2 , where P is in location `. This is
a valid (not necessarily simple) cycle in σ, as it still encircles
unoccupied location `00 6= `. This is also a contradiction, so
in all cases we find σ 0 has no cycles.

3.4

Figure 2: A particle configuration for which all valid moves of
Markov chain M satisfy Property 2; no particle has a valid move
satisfying Property 1 (darker lines represent induced edges of the
system). This demonstrates the subtlety of the Markov chain
rules we have defined.

Given a particle configuration σ with lowest leftmost particle S, we find a sequence of moves transforming σ into a
line of particles stretching down and left from S; particle S
never moves. We traverse the perimeter of σ, starting at S,
finding particles which we can eliminate, or move to this
line. We begin with a crucial definition.
Definition 3.3. An unoccupied location ` in Γ is a gap
of configuration σ if placing an additional particle P at `
results in a hole in configuration σ ∪ P .
Lemma 3.4. There exists a sequence of particle moves
transforming any configuration σ into a straight line.
Proof. Traverse the perimeter of σ starting from lowest
leftmost particle S. When possible, particles encountered
move via a sequence of valid particle moves backwards along
the perimeter to S, and then into a line stretching down
and left from S, eliminating them. This is possible precisely
when particles can move along the perimeter to S without
ever occupying a gap location. We prove if W is the longest
clockwise walk around the perimeter of σ starting at S that
is not adjacent to any gaps, then it is possible to either
eliminate one particle or execute a valid sequence of particle
moves that lengthens walk W. In fewer than 2n2 iterations,
all particles are eliminated: a particle must be eliminated at
least once every 2n steps, as the length of W can increase
at most p(σ) < 2n times in a row. After this, all particles
form a straight line stretching down and left from S.
We present one more lemma before proving M is irreducible.
Recall P (σ, τ ) is the probability of moving from configuration σ to state τ in one step of M.

Ergodicity of Markov chain M

We next show the carefully-defined moves of M suffice to
move from any configuration σ to any other configuration τ ,
necessary for showing M is ergodic and thus has a unique
stationary distribution.
We emphasize the details of this proof are far from trivial, and occupy ten pages in the full version of this paper∗ .
Figure 2 illustrates one difficulty. It depicts a particle configuration for which there exist no valid moves satisfying
Property 1; the only valid moves satisfy Property 2. Thus
if moves satisfying Property 2 are not included, the state
space of M is not connected. Our approach relies critically
on moves satisfying Property 2.
Here we outline and explain our approach to proving M
is ergodic. At a high level, we prove for any configuration σ
there exists a sequence of valid particle moves transforming σ into a straight line, and then prove M is reversible, implying for any other configuration τ there exists a sequence
of valid particle moves transforming that straight line into τ .

Lemma 3.5. For any two configurations σ and τ in Ω, if
P (σ, τ ) > 0, then P (τ, σ) > 0.
Proof. Let σ, τ ∈ Ω be any two configurations such that
P (σ, τ ) > 0. This means σ and τ differ by one particle P at
location ` in σ and at adjacent location `0 in τ .
Note in τ , particle P at location `0 has at most four neighbors. This is because l ∈ n(`0 ) is unoccupied as particle P
is instead at `0 , and at least one other location in n(`0 ) is
unoccupied as otherwise `0 would have been a hole in σ, impossible by Lemma 3.2. Because P (σ, τ ) > 0, Property 1 or
Property 2 must hold for ` and `0 . Both properties are symmetric with regard to the role played by ` and `0 . If Markov
chain M, in state τ , selects in Step 1 particle P , location
` ∈ n(P ), and a sufficiently small probability q, then because
Conditions 1, 2, and 3 are necessarily satisfied, particle P
moves to location `. This proves P (τ, σ) > 0.

285

Lemma 3.6. Markov chain M connects the state space of
all connected configurations without holes.

Proof. This follows from Lemma 2.4 and Corollary 3.9:
λ−p(σ)
λ−(3n−e(σ)−3)
π(σ) = P −p(σ) = P −(3n−e(σ)−3)
λ
σ
σλ

Proof. Let σ and τ be any two connected configurations
of n particles with no holes. By Lemma 3.4, there exists
a sequence
of moves transforming σ into a line with slope
√
1/ 3. By Lemmas 3.4 and 3.5, there exists a sequence of
valid moves transforming this line into τ .

=

3.6

Corollary 3.7. M is ergodic.

The stationary distribution π of M

We now know the Markov chain M is ergodic and finite,
so its stationary distribution is unique.
Lemma 3.8. The stationary distribution π of M is
π(σ) = λt(σ) /Z,
where Z =

P
σ

λt(σ) is the normalizing constant.

Proof. We confirm π is the stationary distribution by detailed balance. Let σ and τ be configurations in Ω with σ 6=
τ such that P (σ, τ ) > 0. By Lemma 3.5, also P (τ, σ) > 0.
Suppose particle P moves from location ` in σ to neighboring
location `0 in τ . Let t be the number of triangles on which Q
is incident when it is in location `, and let t0 be that number
when P is in location `0 . This implies t(σ) − t(τ ) = t − t0 .
Without loss of generality, let t0 < t. We see
1 1
1 1 t0 −t
· ·λ
and P (τ, σ) = · · 1.
n 6
n 6
We now show σ and τ satisfy the detailed balance condition:
P (σ, τ ) =

0

λt(σ) λt −t
λt(τ )
=
= π(τ )P (τ, σ).
Z
6n
Z · 6n
We conclude π is the stationary distribution of M.
π(σ)P (σ, τ ) =

4.

While it is natural to assume maximizing the number of
triangles in a particle configuration results in more compression, here we formalize this. We prove π can also be
expressed in terms of perimeter, which implies M converges
to a distribution weighted by the perimeter of configurations, a global characteristic, even though the probability of
any particle move is determined only by local information.

π(σ) = λ−p(σ) /Z,
P
σ

λ−p(σ) is the normalizing constant.

Proof. We use Lemma 2.3 and Lemma 3.8:

Lemma 4.1. The number of connected configurations with
no holes and perimeter k is at most 5k .

λ2n−p(σ)−2
λt(σ)
π(σ) = P t(σ) = P 2n−p(σ)−2
σλ
σλ

Proof. Consider any configuration σ. Let W be a clockwise traversal of its perimeter, beginning at the lowest leftmost particle of σ; for every edge e traversed in W, the location left of e is unoccupied. At each step of W, the perimeter
can continue straight, turn left by 60 degrees, turn right by
60 degrees, turn right by 120 degrees, or turn right by 180
degrees; the perimeter can never turn left by 120 or 180
degrees. Thus, at any point there are at most five possible locations for the next particle on W. We conclude that
there are fewer than 5k configurations of perimeter k, each
specified by the directions of the turns on W.

λ2n−2
λ−p(σ)
λ−p(σ)
= 2n−2 · P −p(σ) = P −p(σ) .
λ
λ
σ
σλ
The stationary distribution of M can also be expressed in
terms of edges.
Corollary 3.10. The stationary distribution π of M is
π(σ) = λe(σ) /Z,
where Z =

P
σ

ACHIEVING COMPRESSION

If M executes long enough, it will converge to its stationary distribution π; we will use the expression of π given in
Corollary 3.9. To simplify notation, we define the weight of
a configuration σ to be w(σ) = λ−p(σ) . For a set S ⊆ Ω,
we define w(S) as the sum of the weights of all configurations in S. We now show that, provided λ and n are large
enough, with all but exponentially small probability if M
is at stationarity then the particles are in an α-compressed
configuration. Constant α > 1 can be as close to 1 as desired, though smaller α requires larger λ. We begin with a
crucial counting lemma.

Corollary 3.9. The stationary distribution π of M is

where Z =

Convergence Time of Markov Chain M

We prove in Section 4 that when λ > 5, if Markov chain
M has converged to its stationary distribution, then with
all but exponentially small probability the particle system
will be compressed. However, we do not give explicit bounds
on the time required for this to occur; we give experimental evidence of convergence times in Section 6, but believe
proving rigorous bounds will be challenging.
A common measure of convergence time of a Markov chain
is the mixing time, the number of iterations until the distribution is within total variation distance ε of the stationary
distribution, starting from the worst initial configuration.
Bounding the mixing time of a Markov chain achieving compression is likely to be challenging because of the similarity to physical systems such as the Ising and Potts models,
common models for ferromagnetism. Algorithms that perform local updates are known to require exponential time for
many of these models precisely because of a type of compression of the systems [18]. However, mixing time most likley
is not the correct measure of our algorithm’s convergence.
Even if it takes exponential time for M to converge to its
stationary distribution, which is certainly plausible, it may
be true that the particles achieve α-compression after only a
polynomial number of steps. In fact, based on simulations,
it appears compression occurs in polynomial time; doubling
the number of particles consistently results in a ten-fold increase in iterations until compression starting from a straight
line of n particles, so we conjecture the number of iterations
until compression occurs is close to O(n3.3 ).

Proof. By Lemma 3.6, the state space is connected. M
is aperiodic as at each iteration there is a probability of at
least 1/6 that no move is made. Therefore M is ergodic.

3.5

λe(σ)
λ−3n+3
λe(σ)
· P e(σ) = P e(σ) .
−3n+3
λ
σλ
σλ

λe(σ) is the normalizing constant.

286

√
show that, provided n is large enough, for all 0 < λ < 2
there is a constant β such that with all but exponentially
small probability, if M is at stationarity then the particles
are β-expanded. This is notable because it implies, counterintuitively, that λ > 1 is not sufficient to guarantee particle
compression as one might first guess.

We note this bound is not tight; see the full version∗ for
stronger bounds, which lead directly to lower values of λ∗
below. We now prove our main result.
α

Theorem 4.2. For any α > 1, there exists λ∗ = 5 α−1 ,
n ≥ 0, and γ < 1 such that for all λ > λ∗ and n > n∗ ,
the probability that a random sample σ drawn according to
the stationary distribution π of M is not α-compressed is
exponentially small:
∗

P (p(σ) ≥ α · pmin ) < γ

√
n

Theorem
For all 0 < β < 1, there exists λ∗ =
√ 5.1.
∗
λ (β) < 2, n ≥ 0, and γ < 1 such that for all λ < λ∗
and n > n∗ , the particles achieve β-expansion with all but
exponentially small probability: for a configuration σ drawn
at random according to stationary distribution π,
∗

.

Proof. Let Sα be the set of configurations of perimeter
at least α · pmin . Let σmin be a configuration of n particles
achieving the minimum perimeter pmin . We show
π(Sα ) =

P (p(σ) < β · pmax ) ≤ γ

√
w(Sα )
w(Sα )
<
≤ γ n.
Z
w(σmin )

Proof. We let
λ < λ∗ = 5

The first equality is the definition of π; the next inequality
follows from the definitions of Z and w. We focus on the
last inequality. Using Lemma 4.1 and noting the weight of
any σ with p(σ) = k is λ−p(σ) = λ−k , we sum over Sα :
P2n−2
k −k
w(Sα )
k=dα·pmin e 5 λ
≤
−p
w(σmin )
λ min
=

2n−2
X

5(1−log5 λ)k+(log5 λ)(k/α)

k=dα·pmin e
2n−2
X

5(1−(1−1/α) log5 λ)k .

k=dα·pmin e
∗

w(Sα )
≤
w(σmin )

5

≤ 2n 5

√n
−2αc1

w(Sβ )
≤
N · w(σmax )

.

k=dα·pmin e

X

5k λ−k

X

5

λ
√
2

 βk



log λ
log5 2
k 1−log5 λ+ β5 − 2β

.

k=pmin

w(Sα )
w(σmin )
√
√n
≤ 2n 5−2αc1
< γ n.

Let c3 be such that
−c3 = 1 − log5 λ +

Corollary 4.3. For any λ > 5, there exists a value α =
log5 λ/(log5 λ − 1), n∗ ≥ 0, and γ < 1 such that for all n >
n∗ , a random sample σ drawn according to the stationary
distribution π of M satisfies
√
n



k=pmin

=

P (p(σ) ≥ α · pmin ) = π(Sα ) ≤

P (p(σ) ≥ α · pmin ) < γ

bβ·pmax c

bβ·pmax c

There exists γ < 1 and n∗ such that for all n ≥ n∗ ,

5.

2.

min

As λ > λ = 5
, then −c1 := 1 − (1 − 1/α)
√ log5 λ < 0.
Also k ≥ α · pmin , and by Lemma 2.1, pmin > n, so
√
−c1 ·2α n

√

√
Recalling that λ < 2 and k < β · pmax , we see

α
α−1

2n−2
X

=

We focus on proving the last of the inequalities above. Applying Lemma 4.1, we see
Pbβ·pmax c k −k
5 λ
w(Sβ )
k=p
≤ pmaxmin
/2 · λ−pmax
N · w(σmax )
2

pmax
bβ·pmax c
X
λ
5k λ−k √
=
.
2
k=p

Using the inequality pmin ≤ k/α, it follows that

=

log5 2
0−
2
0−1

√
w(Sβ )
w(Sβ )
<
≤ γ n.
Z
N · w(σmax )

π(Sβ ) =

5(1−log5 λ)k+(log5 λ)pmin .

2n−2
X

<5

.

Let Sβ be the set of configurations σ with p(σ) ≤ β · pmax .
Let σmax be a configuration achieving maximum perimeter
pmax = 2n−2. Note the number N of configurations achieving this maximum is at least 2n−1 = 2pmax /2 ; this is the
number of paths where every step is up or up-right, which
have no triangles and thus maximum perimeter. We show

k=dα·pmin e

w(Sα )
≤
w(σmin )

log5 2
β−
2
β−1

√
n

log5 2
log5 λ
−
.
β
2β

We want −c3 < 0, so we solve for λ and see −c3 < 0 precisely
when λ < λ∗ , a condition we know to hold. It follows that
w(Sβ )
≤
N · w(σmax )

.

bβ·pmax c

X
k=pmin
bβ·pmax c

USING M FOR EXPANSION

≤

A nice feature of our algorithm is that it also provably
achieves particle expansion for different values of bias parameter λ. We say a configuration σ is β-expanded for some
β < 1 if p(σ) > β · pmax , where√pmax = 2n − 2. We note
as pmax = Θ(n) and pmin = Θ( n) for a system of n particles, β-expansion and α-compression for any constants β
and α are mutually exclusive for sufficiently large n. We

5−c3 k

X

√

5−c3 ·2

n

≤ 2n5−2c3

√
n

.

k=pmin

Hence there exists n∗ ≥ 0 and γ < 1 such that for all n ≥ n∗ ,
w(Sβ )
N · w(σmax )

P (p(σ) ≤ β · pmax ) = π(Sβ ) ≤

√
n

≤ 2n5−c3 ·2·

287

√

<γ

n

.

(a)

(b)
(a)

(b)

Figure 4: 100 particles after (a) 10 million and (b) 20 million
iterations of M with bias λ = 2, starting with all particles in a
diamond shape of side lengths 10.

(c)

(d)

(e)

Figure 3: 100 particles in a line with occupied edges drawn, after

[6]

(a) 1 million, (b) 2 million, (c) 3 million, (d) 4 million, and (e) 5
million iterations of M with bias λ = 4.
[7]

√

Corollary 5.2. For all λ < 2, there exists a constant
0 < β < 1 such that with all but exponentially small probability a sample drawn according to stationary distribution π
of M is β-expanded.

[8]

Proof. This
√ follows immediately from the previous theorem; for λ < 2, there exists β such that λ < λ∗ (β).
[9]

6.

SIMULATIONS

In practice, Markov chain M yields good compression,
even beyond the values of λ for which our proofs apply. We
simulated M for λ = 4 on 100 particles that began in a line;
the configurations after 1, 2, 3, 4, and 5 million steps of M
are shown in Figure 3.
In contrast, λ = 2, while still favoring particles forming
triangles, does not appear to yield compression; see Figure 4, where even after 20 million simulated steps of M,
the particles have not compressed. We conjecture there is a
phase transition in λ, i.e., a critical value λc such that for
all λ < λc the particles do not compress and for all λ > λc
they do compress. Such phase transitions exist for similar
statistical physics
√ models (e.g., [2]). Our proofs indicate if
λc exists, then 2 ≤ λc ≤ 5; simulations suggest 2 < λc < 4.

[10]

7.

[16]

[11]

[12]

[13]

[14]
[15]

REFERENCES

[1] E. Bampas, J. Czyzowicz, L. Ga̧sieniec, D. Ilcinkas, and
A. Labourel. Almost optimal asynchronous rendezvous in
infinite multidimensional grids. In Distributed Comp.: 24th
Int. Symp., DISC 2010, pages 297–311, 2010.
[2] C. Borgs, J.T. Chayes, A. Frieze, J.H. Kim, P. Tetali,
E. Vigoda, and V.H. Vu. Torpid mixing of some MCMC
algorithms in statistical physics. In 40th IEEE Symp. on
Found. of Comp. Sci., FOCS 1999, pages 218–229, 1999.
[3] S. Camazine, K.P. Visscher, J. Finley, and S.R. Vetter.
House-hunting by honey bee swarms: Collective decisions
and individual behaviors. Insectes Sociaux, 46:348–360.
[4] A. Chavoya and Y. Duthen. Using a genetic algorithm to
evolve cellular automata for 2D/3D computational development. In Genetic and Evolut. Comp. Conf., GECCO 2006.
[5] Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler,
and T. Strothmann. An algorithmic framework for shape
formation problems in self-organizing particle systems. In

[17]

[18]

[19]

[20]

288

Proc. of the 2nd Ann. Int. Conf. on Nanoscale Computing
and Comm., NANOCOM’ 15, pages 21:1–21:2, 2015.
Z. Derakhshandeh, R. Gmyr, A.W. Richa, C. Scheideler,
and T. Strothmann. Universal shape formation for programmable matter. In To appear, 28th ACM Symp. on
Parallelism in Alg. and Arch., SPAA ’16, 2016. To appear.
Z. Derakhshandeh, R. Gmyr, T. Strothmann, R.A. Bazzi,
A.W. Richa, and C. Scheideler. Leader election and shape
formation with self-organizing programmable matter. In
21st DNA Comp. and Molec. Prog., DNA 21, pages
117–132, 2015.
A. Deutsch and S. Dormann. Cellular Automaton Modeling
of Biological Pattern Formation: Characterization,
Applications, and Analysis. Modeling and Simulation in
Science, Eng. and Technology. Birkhäuser Boston, 2007.
R.L. Dobrushin. The problem of uniqueness of a gibbsian
random field and the problem of phase transitions.
Functional Analysis and Its Applications, 2:302–312, 1968.
W. Feller. An Introduction to Probability Theory and Its
Applications, volume 1. Wiley, 1968.
P. Flocchini, G. Prencipe, N. Santoro, and P. Widmayer.
Arbitrary pattern formation by asynchronous, anonymous,
oblivious robots. Theoretical Computer Science,
407:412–447, 2008.
W. K. Hastings. Monte carlo sampling methods using
markov chains and their applications. Biometrika,
57:97–109, 1970.
R. Jeanson, C. Rivault, J.L. Deneubourg, S. Blanco,
R. Fournier, C. Jost, and G. Theraulaz. Self-organized
aggregation in cockroaches. Animal Behaviour, 69:169 –
180, 2005.
N. Lynch. Distributed Algorithms. Morgan Kauffman, 1996.
N.J. Mlot, C.A. Tovey, and D.L. Hu. Fire ants selfassemble into waterproof rafts to survive floods. Proc. of
the National Academy of Sci., 108:7669–7673, 2011.
C. Rivault and A. Cloarec. Cockroach aggregation:
Discrimination between strain odours in Blattella
germanica. Animal Behaviour, 55:177–184, 1998.
M. Rubenstein, A. Cornejo, and R. Nagpal. Programmable
self-assembly in a thousand-robot swarm. Science,
345:795–799, 2014.
L.E. Thomas. Bounds on the mass gap for finite volume
stochastic Ising models at low temperature. Comm. Math.
Phys., 126:1–11, 1989.
E. Winfree, F. Liu, L.A. Wenzler, and N.C. Seeman. Design
and self-assembly of two-dimensional DNA crystals. Nature,
394(6693):539–544, 1998.
D. Woods, H.L. Chen, S. Goodfriend, N. Dabby,
E. Winfree, and P. Yin. Active self-assembly of algorithmic
shapes and patterns in polylogarithmic time. In Proc. of
the 4th Conf. on Innovations in Theoretical Computer
Science, pages 353–354, 2013.

Wireless Personal Communications (2007) 43:605–621
DOI 10.1007/s11277-007-9252-9

c Springer 2007


MANET Routing with Provably Low Complexity Through Constant
Density Clustering and Route Request Broadcast
HYO-SIK YANG1 , LUKE RITCHIE2 , ANDRÉA W. RICHA3
and MARTIN REISSLEIN2
1
Department of Computer Engineering, Sejong University, Seoul, Korea
E-mail: hsyang@sejong.ac.kr
2
Department of Electrical Engineering, Arizona State University,Goldwater Center, MC 5706,TempeAZ
85287–5706,USA
E-mail: {Luke.Ritchie, reisslein}@asu.edu
3
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ, USA
E-mail: aricha@asu.edu

Abstract. As mobile ad hoc networks (MANETs) are emerging as important components in critical and large-scale
applications, it is crucial to develop MANET routing mechanisms with provably low complexity. In this paper, we
give a tutorial overview of the efficient use of elementary node clustering and route request broadcast mechanisms
for low-complexity MANET routing. We explain these mechanisms with illustrative examples and discuss their
theoretical performance characteristics. We demonstrate that node clustering with constant density and route request
broadcasting with a doubling radius technique over the network of cluster leaders can be employed for MANET
routing with theoretically proven low complexity. Moreover, we contrast these efficient elementary clustering and
route request broadcast mechanisms with clustering and route information accumulation mechanisms in the widely
studied AODV and DSR routing protocols and discuss the implications of these various mechanisms for scalable
MANET routing.
Keywords: 1-hop clustering, algorithm/protocol design and analysis, message complexity, routing protocol,
scalability, time complexity, wireless mobile ad hoc network

1. Introduction
As large-scale mobile ad hoc networks (MANETs) are envisioned for a variety of applications, including military, medical, and entertainment applications, it becomes important to
understand the fundamental performance characteristics of the central MANET functionalities. Routing, i.e., the process of finding a route from a source node to a destination node, is
one of the most central functionalities in a MANET. Indeed, a large number of studies have
contributed to a better understanding of MANET routing. However, the formal analysis of the
complexity of MANET routing has only recently attracted some attention, see e.g., [1–7]. It
has been found that the complexity of the existing MANET routing algorithms is typically
proportional to the overall network size, e.g., the total number of nodes in the network, unless
restrictive assumptions about the node locations or their mobility patterns are made.
In this paper, we give a tutorial overview of a recent line of algorithm-theoretic work [8, 9]
that has formally analyzed the elementary mechanisms of node clustering and route request

606

H.-S. Yang et al.

broadcast for low-complexity MANET routing. More specifically, we explain how (i) node
clustering with a specific constant density of cluster leaders (whereby density refers to the number of cluster leaders per unit area) that adapts optimally to node mobility can be combined
with (ii) broadcasting of route requests over the network of cluster leaders with a doubling
radius technique to achieve MANET route discovery with a complexity proportional to the
distance between the source and destination node and independent of the overall network size.
The Cluster Overlay Broadcast (COB) routing protocol [9] is build upon the combination of
these two elementary mechanisms, which are broadly applicable in MANET routing.
We explain the basic mechanisms of node clustering and route request broadcast with illustrative examples that provide insight into their complexity properties. We discuss the underpinnings of the formal algorithm-theoretic results for the node clustering and route request
broadcast mechanisms and outline their implications for efficient MANET routing. We further
illustrate the performance of COB with selected simulation results. We contrast the examined
efficient clustering and route request broadcast mechanisms with the clustering and route information accumulation mechanisms that have been proposed in the context of the well known
ad hoc on demand distance vector (AODV) and the dynamic source routing (DSR) protocols.
From these comparisons we identify lessons learned from the formal analysis of the basic
clustering and route request broadcast mechanisms for the design of low-complexity MANET
routing protocols.

1.1. R e l at e d W o r k
In general, MANET routing can be conducted over a flat network where all nodes participate
equally in the route discovery. In a dense network area containing many nodes per unit area,
the flat network approach becomes typically unscalable with many nodes exchanging route
requests and responses [10, 11]. Grouping nodes in clusters and conducting the route discovery primarily through the cluster leaders can mitigate these scalability problems. Research on
node clustering in MANETs over the past two decades has resulted in a wide variety of clustering algorithms, with the lowest-ID [12, 13] and Least Cluster Change (LCC) [14] algorithms
having been widely considered and used as benchmarks for comparisons [15]. The developed
algorithms have typically been evaluated through simulations which give valuable insights,
but provide only a limited understanding of the underlying fundamental performance limitations. As part of our algorithm theoretic line of work we have analyzed the mobile piercing
set problem [8], a computational geometry problem, which provides a formal framework for
studying clustering in MANETs. Our formal analysis provides fundamental insights into the
properties of common clustering algorithms, such as the LCC algorithm. These formal analysis results complement and underscore properties which have only been empirically observed
before and which we exploit for developing a route discovery approach with unprecedented
low complexity.
The elementary mechanism of broadcasting (i.e., flooding) route requests through parts of
the network is used in essentially all MANET routing protocols for discovering routes [16, 17].
Naive forwarding of the broadcast message by every node in a flat network results typically in many redundant copies, the so-called broadcast storm problem [18]. This broadcast
storm problem can be addressed by limiting the number of nodes that forward broadcast
messages [19, 20]. The strategy of first clustering the network and then having only cluster
leaders forward the route requests has been extensively examined through simulations, see for

MANET Routing with Provably Low Complexity

607

instance [14, 10, 21, 22]. With this strategy, the discovered routes traverse only cluster leaders,
forming so-called backbones [23].
We complement these existing studies with a tutorial overview of a formal analysis of the
basic mechanisms of node clustering and route request broadcast over the network of cluster
leaders that have been previously only examined through simulations. We explain that it is possible to significantly reduce the complexity of MANET route discovery by judiciously exploiting the insights from our formal analyses of clustering and route request broadcast. Specifically,
the complexity for route discovery can be proportional to the shortest source-to-destination
distance, and independent of the overall network size. This complexity is significantly lower
than those of previously formally analyzed routing protocols which had complexities directly
proportional to the overall network size [1–7] (which can be much larger than the shortest
source-to-destination distance).

2. Network Setup and Model
We follow the widely considered basic unit-disk model, which represents the nodes’ communication range as a circular disk, and make no other assumptions about the density or distribution
of the nodes within the network. Nodes may move in any pattern – we only assume they do
not move so quickly that wireless links are broken during the short time period it takes to
discover a route. Our architecture does not make any non-standard assumptions about the
medium access layer. As long as there is some reliability built into the broadcast mechanism,
then each transmission is received by all neighboring nodes with high probability within a
period of time we call a “time step.”
We employ commonly available capabilities to realize two transmission ranges (as also
employed in [22]): a shorter range for communication within clusters and a longer range for
communication between cluster leaders. Successful route discovery requires that the network
is connected with short-range transmissions, i.e., that there exists a route from any node in
the network to any other node with only short-range transmission. We consider long-range
transmissions that reach three times as far as short-range transmissions. This ensures in a
connected network that the leaders of adjacent clusters are always able to communicate in one
long-range hop, as will be illustrated in the next section on clustering. The two transmission
ranges do not mean that every node needs two transmitters: two power levels, or a “logical”
long-range channel implemented as a series of shorter transmissions would work equally well.
It is important that all nodes have this capability, since any of them may be required to function
as a cluster leader at some point.

3. Node Clustering
3.1. I n i t i a l C l u s tering and Properties of Clus ter Cover
Initially, all nodes begin with the same neutral status, see Figure 1(a). Then, each node broadcasts its ID number using the short transmission range. Any node which does not receive an
ID lower than its own ID marks itself as a leader, and signals all nodes within the short transmission range to become member nodes. In our example, nodes 0,1,2,4,5,7,8, and 13 have
the lowest IDs in their respective neighborhoods. This process is illustrated in Figure 1(b),

608

H.-S. Yang et al.

Figure 1. Illustration of clustering, routing over the overlay network, and update of cluster cover after node mobility.

where these first two steps are shown: leaders marked in black, members marked in white. At
this point, a few nodes are still unmarked, shown in gray. Since 19 and 32 have the lowest
unmarked IDs in their short-range neighborhoods, they then become cluster leaders as well, as
illustrated in Figure 1(c). Once the entire network is marked, the cluster leaders can function as

MANET Routing with Provably Low Complexity

609

an overlay network, see Figure 1(d), capable of reaching any member node in one short-range
hop – i.e., we have obtained a so-called one-hop cluster cover of the network.
The outlined algorithm corresponds to the LCC algorithm [14], which may use the lowestID clustering algorithm [12, 13] for the initial cluster formation. We have proven that for this
algorithm, the density of the overlay network of cluster leaders – i.e., the number of cluster
leaders per unit area – is no higher than a constant, which is independent of the number of
nodes or their distribution; specifically the cluster leader density is no higher than 4/π [9].
This result in turn can be used to show that a given cluster leader has no more than 49 other
cluster leaders within its long transmission range. These two results are important for two
main reasons. First, the bounded density of cluster leaders and bounded number of neighboring cluster leaders limit the number of cluster leaders that receive and forward route requests,
which allows us to bound the complexity of the routing, as discussed in Section 4.2. Second,
the limited number of neighboring cluster heads aids in the medium access control for the
long-range transmissions, as only a limited number of long-range transmission can collide.
The clustering algorithm does not require cluster leaders to maintain lists of the regular nodes
in their clusters. Instead, each regular member node only keeps track of which node it considers
its leader, resulting in low-storage overhead.
We have also proven that the density of the network of cluster leaders (i.e., of the one-hop
cluster cover) can not be much smaller than that achieved by the LCC algorithm. More specifically, if the LCC algorithm finds a one-hop cluster cover consisting of C cluster leaders,
it is absolutely not possible to achieve the one-hop cluster cover with less than C /7 cluster
leaders [8]. This result is important as it implies that no other algorithm, no matter how sophisticated, could find a one-hop cluster cover with significantly fewer (less than a seventh) of the
number of leaders declared by the LCC algorithm. Clearly, having as few cluster leaders as
possible is preferable to ensure that all network functions that operate on the overlay network
of cluster leaders, such as routing, have as simple a view of the network as possible. Our
theoretical results prove that the overlay network can not be much simpler than that found
with the LCC algorithm. We have also shown that the complexity of forming the initial cluster
cover with the LCC algorithm is linearly proportional to the number of formed clusters [8].
In summary, we have shown that the LCC clustering algorithm (which may employ the
lowest-ID technique for the initial clustering) forms an overlay network of cluster leaders,
whereby each node is within the short-range distance of a cluster leader. The density of the
network of cluster leaders (number of cluster leaders per unit area) is no higher than 4/π and
the density of a valid one-hop cluster cover can not be less than seven times smaller than that
achieved by the LCC algorithm. These two main results hinge critically upon the fact that no
two cluster leaders are ever within the short transmission range of each other. Any arbitrary
clustering technique that ensures that two cluster leaders are at least the short-range distance
apart has the 4/π upper bound on the cluster leader density and no valid one-hop cluster cover
can have less than one seventh of the cluster leaders declared by the considered clustering
technique. Also, these results hold regardless of the node distribution in the network, as long
as the entire network is connected by short-haul hops.
3.2. U p d at e o f C l u s tering Upon Node Mobility: Algorithm Outline
and Theoretical Res ults
There are two events that require an update of the clustering, i.e., a change in the nodes
designated as cluster leaders:

610

H.-S. Yang et al.

(i) A cluster leader moves and leaves at least one node uncovered (outside the short
transmission range of any existing cluster leader), or a regular node moves and becomes uncovered. In either case, the uncovered node(s) run the initial clustering procedure
outlined above. In the example illustrated in Figure 1(e), cluster leader 5 moves and leaves regular nodes 16, 34, 36, and 38 uncovered. These uncovered nodes then form a new
cluster with node 16 as the leader.
(ii) A cluster leader A moves within the short-haul transmission range of another cluster leader
B. Then the leader that moved (A) is demoted to a regular node. In Figure 1(e), cluster
leader 5 moved within the short-haul transmission range of cluster leader 32. Node 5 is
therefore demoted to a regular node within the cluster led by node 32.
If a regular node moves from within the short transmission range of one cluster leader to within
the short range of another leader, then no update of the clustering is required.
The outlined update mechanisms correspond to the LCC algorithm [14] with the subtle
modification that for update event (ii) the cluster leader that moved is demoted. (The LCC
specification in [14] allows either leader A or B to be demoted according to the lowest-ID,
highest-connectivity, or some other tie-breaking rule. Demoting the node that moved typically
results in somewhat less overhead, but the following complexity results hold also for any arbitrary tie-breaking rule.) We have proven that the updates required by both events (i) and (ii)
can be completed within a small constant amount of time, that is, independent of the number
of nodes [8]. We have also shown that when k nodes are uncovered due to an event of type
(i), then the number of messages required to update the clustering in linearly proportional to
k. No other deterministic algorithm can update the clustering with a number of messages, that
is, smaller than a constant times k [8]. Importantly, the updates according to the outlined LCC
algorithm preserve the properties (such as bounded constant density) of the one-hop cluster
cover.

4. Route Discovery Through Route Request Broadcast
4.1. D e s c r i p t i o n a n d I l l u s tration of Route Reques t Broadcas t over
Network of Clus ter Leaders
Routing begins when a cluster leader receives a new message, either from itself or via a short
range transmission from a cluster member. For an illustrative example, consider a message
generated by node 16 and destined to node 25. Node 16 sends the message to its cluster leader,
node 5. These transmissions from member to leader nodes always use the short-range. The
leader (node 5) then begins flooding the network with a route request (RREQ) for the message
using a version of an expanding ring search. First, it long-range broadcasts the RREQ with a
time to live (TTL) equal to 1 that only reaches as far as the adjacent cluster leaders. If a reply
does not come back in two time steps, the leader sends another RREQ with twice the previous TTL. This process continues with the doubling of TTL and waiting time until the RREQ
reaches its destination and the destination node (25) returns a route acknowledgement (ACK).
Note that the TTL limits the broadcast radius to no more than TTL long-range transmissions.
In the first round, the single RREQ broadcast by cluster leader node 5 reaches adjacent
cluster leaders (in our example, nodes 0, 7, and 32) and the rest of the regular nodes in its
cluster (nodes 34, 36, and 38). Other regular nodes (such as 11 or 31) are in range to hear this

MANET Routing with Provably Low Complexity

611

Table 1. Route Requests (RREQs) stored in cluster leaders 7, 19, and 2 during the third
broadcast round of the node 16-to-node 25 route discovery. The predecessor node entries
are used to pass back the acknowledgement for the discovered route to the source. The
incoming Time to Live (TTL) values help prevent routing loops
At node number
Source
Destination
Predecessor
Broadcast round
Incoming TTL
Source
Destination
Predecessor
Broadcast round
Incoming TTL
Source
Destination
Predecessor
Broadcast round
Incoming TTL

7

19

2

…

…

…

16
25
15
3
4

16
25
7
3
3

16
25
19
3
2

…

…

…

broadcast, but ignore any RREQ not from their respective cluster leader nodes. During the
second round, the TTL is set to two, and node 5 waits for four time steps before beginning the
next round. Leader nodes 1, 4, 8, 13, and 19 also receive the RREQ as well as the members
of clusters 0, 7, and 32. In the third broadcast round, the TTL is set to four and a RREQ
reaches node 2, the cluster leader for our considered destination node 25, by several paths. In
our example, we assume that the RREQ traveling through nodes 7 and 19 is the first to arrive.
Node 2 broadcasts the RREQ with a long-haul transmission reaching both its neighboring
cluster leaders and the regular nodes in its cluster, including the destination node 25.
During flooding, each intermediate cluster leader stores a copy of the first RREQ it receives
for a given route discovery during a given broadcast round. The RREQ stored at a given cluster leader contains the ID of the source node, the ID of the destination node, the ID of the
predecessor node from whom the RREQ was received, the number of the current broadcast
round, and the TTL value with which the RREQ was received. The RREQs stored at cluster
leaders 7, 19, and 2 during the third broadcast round are illustrated in Table 1.
Once node 2 receives the ACK from node 25 (via a short-range transmission), the ACK is
forwarded back along the path marked by the stored RREQs in nodes 2, 19, and 7. The total
round trip is eight hops, so it reaches node 5 before the next round begins. As intermediate
leaders forward the ACK, they mark the route as active, and this marked route is used to
forward the actual message from nodes 5 to 25. Because this is a purely reactive protocol,
very little information needs to be stored at any one time. At most, there will be one stored
RREQ for each route discovery currently underway and for each active route. Routing loops
are avoided by passing the ACK back to a predecessor node which was reached with a higher
incoming TTL field, i.e., the predecessor node is at least one-hop closer to the source.

612

H.-S. Yang et al.

The COB routing protocol [9] incorporates the efficient node clustering mechanism outlined
in Section 3 and the efficient route discovery mechanism outlined in this section. These mechanisms are broadly applicable to MANET routing and can be incorporated in a wide range of
MANET routing protocols. To fix ideas, we consider in the following COB as an instantiation
of a routing protocol based on the outlined efficient node clustering and route request broadcast
mechanisms.
4.2. T h e o r e t i c a l R e s u lt s f o r R o u t e R e q u e s t B r o a dcas t
We let  denote the length of the shortest possible source-to-destination path in short-range
hops. We have proven that the route discovery approach outlined in Section 4.1 finds a route
with a hop distance that is linearly proportional to the shortest source-to-destination hop distance  [9]. Note that this results holds even though we are employing long-range hops between
cluster leaders in the route discovery and the transmission of the actual message. To see this,
observe that in a connected network, one long-range hop between cluster leaders generally
corresponds to at most three short-range hops via at most two gateway nodes between cluster
leaders, thus a hop distance measured in short-range hops is linearly proportional to a hop
distance measured in long-range hops. The discovered path is also guaranteed to be loop-free.
The route discovery approach with RREQ broadcast over the constant-density network of
cluster leaders limits the time and message complexity of route discovery. Since the broadcast
radii are powers of two, the last round always accounts for more than half of the total overhead.
And since the overlay network has a bounded constant density, the overhead is a function of the
length  of the shortest source-to-destination path. More specifically, the number of broadcasts
during route discovery is within a constant factor of 2 , and the time spent is within a constant
of , i.e., in standard asymptotic notation they are O(2 ) and O(), respectively, [9]. These
formally proven performance bounds make COB the first formally analyzed MANET routing
protocol with time and message complexities that are linear, resp. quadratic, in the shortest
source-to-destination distance, and independent of the overall network size, such as the total
number of nodes in the network or the total hop-distance diameter of the network. Previous
formally analyzed routing protocols have complexities that depend on the total number of
nodes or the total hop distance diameter of the network [1, 2, 5], or require location aid, or
special mobility patterns. Our results for the route discovery approach outlined in Section 4.1
hold for any arbitrary mobility pattern and node distribution, and do not require location aid.
A key theoretical result for the outlined route discovery approach is that it adapts asymptotically optimally to the mobility of the nodes. To understand this result, note that the outlined
route discovery is purely on-demand, operating on top of a clustering cover of the network.
Thus, any changes due to node mobility are absorbed by updates of the cluster cover. As noted
above, we have shown that using the LCC algorithm, the total elapsed time of an update is a
small constant, and thus asymptotically optimal. We have also shown that (i) the number of
messages exchanged during an update of the clustering structure is linearly proportional to the
number of nodes left uncovered due to the mobility of a node, and that (ii) no deterministic
algorithm can update the cluster cover with a number of message exchanges, that is, less than
linearly proportional to the number of uncovered nodes. Thus, the COB routing protocol,
which routes on the network of cluster leaders, is asymptotically optimal in that no other
routing algorithm could adapt to node movements in less than a constant amount of time (i.e.,
a time that is independent of the number of nodes involved) or with fewer message exchanges
than a fixed constant times that number of involved nodes.

55

900

50

800

45
40

# of Messages

Delay (Time Steps)

MANET Routing with Provably Low Complexity

R=1000, P=25
R=1000, P=50
R=500, P=25
R=500, P=50

35
30
25

600
500
400
300
200

15

100
0

1000

2000

3000

4000

# of Nodes

(a) Delay

5000

6000

R=1000, P=25
R=1000, P=50
R=500, P=25
R=500, P=50

700

20

10

613

0

0

1000

2000

3000

4000

5000

6000

# of Nodes

(b) Number of message transmissions

Figure 2. Node density scaling: overhead for a route discovery as a function of number of nodes N for different
(fixed) network areas of R × R m2 and short transmission ranges P m.

4.3. S i m u l a t i o n R e s u lt s
In this section, we present selected simulation results that illustrate the key theoretical properties of the COB routing protocol, and refer to [9] for more details on the simulations. In
the presented simulations, we evaluate COB with respect to the mobility process and the size
of the network. We simulate the route discovery sequentially, randomly selecting a series of
source/destination pairs. This ensures that we measure the network layer performance of COB,
in isolation from any positive or negative effects of the MAC layer or cross-traffic. We include
simulations for two scaling scenarios: (i) a node density scaling scenario, where the area of
the network is a square of fixed size (R × R m2 ) and the number of nodes (N ) in the network
is varied, and (ii) a network diameter scaling scenario, where we jointly scale up the number
of nodes in the network and the diameter of the network area. The goal of the diameter scaling
is to measure the performance as the shortest hop distance  between source and destination
increases. Since scaling  directly is computationally prohibitive in a distributed simulation,
we make the reasonable assumption that  scales approximately linearly with the diameter
of the network, i.e., ∼ R. It is important to note that the time and message complexity of
COB depend only on the shortest source-destination distance and not on the overall network
dimensions.
We conduct simulations for both the random walk (RW) and the random waypoint (RWP)
mobility models with a mobile speed of 10 m/second. The pause time for the random waypoint mobility model is 10 seconds. In our simulation, a route discovery typically takes two
orders of magnitude less time than changes in the cluster coverage due to node movement,
so we can reasonably approximate the node positions as static during a given route discovery.
The practical deployment of the COB routing protocol would of course require a means for
recovering from node changes in the cluster coverage that affect an ongoing route discovery.
Recovery mechanisms similar to those in existing routing protocols could be adapted to COB.
In Figures 2(a) and 2(b), we consider the node density scaling scenario and plot the delay for
a route discovery (in time steps) and the number of message transmissions per route discovery.
Generally, we observe that the delay for the route discovery does not change significantly as
the node density increases. An exception is the sparsest network (R = 1000 m, P = 25 m,
N = 500), where there are significant “uncovered” areas in the network not reachable by any
node’s long transmission range. Here, many routes require a path around the uncovered area
–i.e., the routes tend to be more crooked and less straight in this scenario.

614

H.-S. Yang et al.
100
80
70

2000

# of Messages

Delay (Time Steps)

2500

P=25, RW
P=25, RWP
P=50, RW
P=50, RWP

90

60
50
40
30
20

P=25, RW
P=25, RWP
P=50, RW
P=50, RWP

1500
1000
500

10
0

0

500

1000

1500

2000

2500

3000

3500

4000

0

0

500

1000

1500

2000

2500

3000

3500

# of Nodes

# of Nodes

(a) Delay

(b) Number of message transmissions

4000

Figure 3. Diameter scaling: overhead for a route discovery as a function of number of nodes N with proportional
(R = N /2) network area of R × R m2 for different (fixed) short transmission ranges P m and mobility patterns
(random walk (RW) and random waypoint (RWP)).

From Figure 2(b) we observe that the number of messages transmitted for a route discovery
generally tends to initially increase and then level off as the node density increases. This effect
is most pronounced for the network with the large area and the small transmission range.
This is caused by an increasing number of clusters as the network becomes more populated,
until they approach the maximum density of clusters. Once the entire network area–or at least
the region involved in the doubling radius search–is covered by clusters, there is no further
increase in the number of messages.
In Figures 3(a) and 3(b), we consider the diameter scaling scenario for both mobility models. We observe from Figure 3(a) that the route discovery delay increases linearly as we jointly
scale up the diameter and number of nodes in the network (implicitly scaling up ). Also,
we observe from Figure 3(b) that the number of messages transmitted for a route discovery
appears to increase quadratically. We also observe that both the random walk and the random
waypoint mobility models result in the same underlying trends in the hop distance, delay, and
message complexity. This is exactly what we expect to see since the COB algorithm does not
assume any specific mobility model. The difference in results for the random waypoint model
seen in the plots is due to the slight tendency for the nodes to more densely populate the center
of the network area using that model, resulting in somewhat lower constants in the asymptotic
scaling behavior.

5. Discussion
In this section, we discuss the elementary node clustering and route request broadcast mechanisms examined in Sections 3 and 4 and contrast them with clustering and route information accumulation mechanisms that have been proposed in the context of extensively studied
on-demand ad hoc routing protocols, such as AODV [21] and DSR [24].
5.1. C l u s t e r i n g
Clustering has been proposed as an addition to both AODV and DSR and a number of other
routing protocols because it has some very useful properties that are quite generally applicable.
The most commonly cited advantage is that hierarchical routes tend to be more stable: links

MANET Routing with Provably Low Complexity

4

5
2

3

1

615

6

7

Figure 4. Worst case scenario of the ripple effect: if the central node gives up its leader status, up to six other nodes
formerly belonging to its cluster may be far enough apart to become new cluster leaders.

between clusters do not break as often as links between individual nodes. And as we have
seen, dividing a MANET into clusters can effectively limit the density of the overlay network.
Lowering the effective density of a MANET is useful because high-density networks generally tend to experience more collisions and more congestion. Limiting density also limits the
overhead of flooding the network with a route request by eliminating redundant broadcasts. It
can also have the side effect of turning the cluster leaders into bottlenecks which must do more
work and forward more traffic than the other nodes [25]. This is another reason for keeping
the processing and storage load as low as possible in the node clustering approach outlined in
Section 3.
It is important to note that a limited density of cluster leaders can only be guaranteed if the
cluster leaders are always a minimum distance apart. In the LCC algorithm, they are never
within the short communication range. Other clustering algorithms, such as the proposed
addition to AODV, Adaptive Routing using Clusters (ARC) [10], do not have this property.
ARC uses a subset rule to determine when cluster leaders change status. With the subset rule, a
cluster leader that moves within the short range of another leader only gives up its leader status
if all of its regular nodes also move within the short range of the other leader. This subset rule
was developed in [10] in order to avoid the “ripple effect.” This is the name for the situation
when one change in leader status triggers additional changes in the clustering.
For an illustration of the worst-case ripple effect we consider the two update events of the
LCC algorithm in the context of the scenario depicted in Figure 4. Suppose that node 2 in
the illustration is a cluster leader and the nodes 1, 3, 4, 5, 6, and 7 at the edge of the short
transmission range of node 2 are members of the cluster. Now suppose that node 2 moves
outside of the illustrated area and leaves its member nodes uncovered. According to update
event (i) from Section 3.2, the uncovered nodes organize themselves into clusters which results
in the worst case (if the nodes 1, 3, 4, 5, 6, and 7 are just slightly outside of each others short
range) in six new clusters. (In the best-case, when the nodes are within the short range only two
new clusters are formed.) Next, consider the update event (ii) by considering again Figure 4.
Suppose that node 1 is a cluster leader that has just moved from its old location outside the
illustrated area to just within the short range of the cluster leader 2, which has nodes 3, 4, 5,
6, and 7 as regular member nodes. With our update rule of demoting the node that moved,
node 1 demotes itself to a regular node joining the cluster led by node 2. With the lowest-ID
tie breaking rule allowed for by LCC, node 1 stays cluster leader and node 2 is demoted. The
demotion of node 2 leaves in the worst case the nodes 3, 4, 5, 6, and 7 uncovered (namely if

616

H.-S. Yang et al.

...

...

Layer 1

...

...

Layer 2

1

2
Figure 5. Each of the regular nodes (in white) in layer 1 is covered by exactly one leader (in black) one short-range
away in layer 2. With the subset rule of ARC, none of the cluster leaders can give up its leader status. With LCC,
between two and four leaders can cover all nodes.

nodes 3 and 7 are just slightly outside the short range of node 1). The uncovered nodes need
to organize themselves into clusters, which in the worst case results in five new clusters. We
see from these considerations, that the ripple effect in LCC is limited to a localized area and
does not propagate across the network.
Realistically, several factors limit the ripple effect to significantly fewer cluster leader
changes. If the node speed and clustering update interval take reasonable values, it is rare for
an entire disk to be uncovered at once. For a relatively low-node density, the simulations in [10]
show a higher, roughly proportional rate of cluster changes with LCC than with a subset rule.
However, for higher densities, it is more likely that members nodes will be within range of two
or more leaders, and even fewer nodes will be left uncovered by a leadership change. Changes
in leader status are also a less important issue for COB than for AODV, because losing a leader
does not mean also losing all of the route information it has accumulated (an aspect discussed
in the next section).
The subset rule of ARC can lead to a very large number of leaders and possibly to an
unbounded density of cluster leaders. To see this, consider the scenario depicted in Figure 5
where the cluster leaders are drawn in black and their members in white. More specifically,
each of the leader nodes in layer 2 has one member node in layer 1. With the subset rule, none
of the leaders can give up its leader status since each member node is only within the short
range of exactly one leader node. As the number of nodes in layers 1 and 2 grows very large
(while maintaining the depicted geometry), the number of cluster leaders becomes very large
and so does the density of cluster leaders, which in turn can lead to a very large message complexity for route discoveries. Note that for the depicted geometry, all nodes could be covered
with two clusters, one led by the middle node in layer 1, and one led by the middle node in
layer 2. In the worst case, four leader nodes would be required with LCC, one at each end
point of each layer.
5.2. A c c u m u l a t i o n o f R o u t e I n f o rmation
The route discovery outlined in Section 4, similar to the route discovery in DSR and AODV,
is reactive, in that it finds routes as they are needed. The key advantage of this approach over
proactive routing is the reduced routing load, which has a significant performance impact on
wireless links [26]. However, AODV and DSR do behave, in another sense, proactively by

MANET Routing with Provably Low Complexity

617

caching and maintaining route information for later use once it has been discovered. This has
the advantage of reducing the frequency of RREQ floods, at the price of increasing the overhead associated with storing and maintaining up-to-date route information. A comparison with
the route discovery approach of Section 4 illustrates how this design decision may ultimately
affect the scalability of a routing protocol.
In both DSR and AODV, each node maintains a route cache/route table. In AODV, this table
contains a separate entry for each destination, which in a network with N nodes may eventually grow to O(N ) in size. By virtue of source routing, DSR has access to a larger amount
of routing information than AODV. However, the size of an adaptive routing cache [27] may
increase over time, in the limiting case, to include information about every possible route
through a node (larger than O(N )). Each node in COB, on the other hand, stores just one entry
for each active route through that node.
The price COB pays for provably limited storage is a RREQ flood for every new route. But
as mentioned in Section 4, employing a doubling radius expanding ring search and limited
broadcast through the constant-density overlay network limits the RREQ sending overhead
to O(2 ). Starting fresh with each route also eliminates the additional complexity associated
with stale routes such as resending data lost on invalid paths and avoiding count-to-infinity
scenarios.
Even when a route is not stale or invalid, it still may not be the “best” route available.
Most routing protocols, including AODV, are designed to select the shortest route in terms
of the number of hops from source to destination. However, in a congested network, a route
with more hops may actually be faster in terms of end-to-end delay. The “shortest” path may
include critical links that are experiencing exceptional traffic loads. In this sense, the route
discovery approach of Section 4.1 employs a type of load balancing, using a RREQ flood to
select the route that replies first, regardless of hop count. (Although the doubling ring search
does ensure that the discovered route will be no more than twice the shortest number of hops.)
Networks with highly mobile nodes can also diminish the effectiveness of routing
tables/caches. If nodes move faster or more erratically, routes become stale even sooner.
More route update messages must be sent, and nodes will have to work harder to update their
information. While DSR outperforms AODV and COB in nearly-static networks, reliance on
past information becomes a liability in higher-mobility scenarios. On a related note, consider
the case of introducing additional nodes to the network or shutting down existing nodes. This
may occur for a variety of reasons: power-saving “sleep” cycles; damage, repair, or expansion
of network coverage; or a changing user population. All of these can be viewed as a special case
of mobility, since they involve nodes changing location. Protocols such as DSR and AODV
that rely on accumulation of route information over time will likewise be at a disadvantage in
these types of situations.

6. Conclusions
In summary, we have provided tutorial guidelines for the efficient use of the elementary mechanisms of node clustering and route request broadcast in MANET routing. We have explained
how these two basic mechanisms can form the basis for provably efficient route discovery
with a delay (time complexity) linearly proportional to the shortest source-destination hop
distance, i.e., O(), and a number of message exchanges (message complexity) quadratically proportionally to the shortest source-destination distance, i.e., O(2 ). In contrast, the

618

H.-S. Yang et al.

time and message complexities of previous formally analyzed routing protocols scaled with
the total network size (e.g., total number of nodes, diameter of network), which is typically
significantly larger than the shortest source-destination hop distance.
The examined basic node clustering and route request broadcast mechanisms are generally applicable to routing in a mobile ad hoc network. Comparison with clustering and route
information accumulation mechanisms extensively studied in the context of existing routing
protocols, such as DSR and AODV, has provided insights into the implications of these mechanisms for scalable MANET routing. As our comparison and formal analysis indicate, features
such as density-limiting clustering, minimal storage, and purely-reactive route discovery all
have provable benefits to scalability. And improved scalability is essential for expanding the
potential applications for ad hoc networks by eliminating constraints on network mobility,
density, and size.

References
1. M. Abolhasan, T. Wysocki, and E. Dutkiewicz, “A Review of Routing Protocols for Mobile Ad Hoc Networks”,
Ad Hoc Networks, Vol. 2, No. 1, pp. 1–22, 2004.
2. X. Hong, K. Xu, and M. Gerla, “Scalable Routing Protocols for Mobile Ad Hoc Networks”, IEEE Network,
Vol. 16, no. 4, pp. 11–21, 2002.
3. R. Rajamaran, “Topology Control and Routing in Ad Hoc Networks: A survey”, SIGACT News, Vol. 33,
pp. 60–73, 2002.
4. A. Sankar and Z. Liu, “Maximum Lifetime Routing in Wireless Ad-hoc Networks”, in Proc. of IEEE Infocom,
Hong Kong, 2004.
5. C.A. Santivanez, B. McDonald, I. Stavrakakis, and R. Ramanathan, “On the Scalability of Ad Hoc Routing
Protocols”, in Proc. of IEEE Infocom, New York, NY pp. 1688–1697, 2002.
6. A. Srinivas and E. Modiano, “Minimum Energy Disjoint Path Routing in Wireless Ad-hoc Networks”, in Proc.
of ACM MobiCom, San Diego, CA pp. 122–133, 2003.
7. J. Sucec and I. Marsic, “Hierarchical Routing Overhead in Mobile Ad Hoc Networks”, IEEE Transactions on
Mobile Computing, Vol. 3, No. 1, pp. 46–56, 2004.
8. H. Huang, A.W. Richa, and M. Segal, “Approximation Algorithms for the Mobile Piercing Set Problem with
Applications to Clustering in Ad Hoc Networks”, Mobile Networks and Applications (MONET), Vol. 9, No.
2, pp. 151–161, 2004.
9. L. Ritchie, H.-S. Yang, A.W. Richa, and M. Reisslein, “Cluster Overlay Broadcast (COB): MANET Routing
with Complexity Polynominal in Source-Destination Distance”, IEEE Transactions on Mobile Computing,
Vol. 5, No. 6, pp. 653–667, 2006.
10. E.M. Belding-Royer, “Multi-level Hierarchies for Scalable Ad Hoc Routing” Wireless Networks, Vol. 9, No.
5, pp. 461–478, 2003.
11. E.M. Belding-Royer and C.E. Perkins, “Evolution and Future Directions of the Ad Hoc On-demand Distancevector Routing Protocol” Ad Hoc Networks, Vol. 1, No. 1, pp. 125–150, 2003.
12. A. Ephremides, J.E. Wieselthier, and D.J. Baker, “A Design Concept for Reliable Mobile Radio Networks with
Frequency Hopping Signalling”, Proceedings of the IEEE, Vol. 75, No. 1, pp. 56–73, 1987.
13. M. Gerla and J.T.C. Tsai, “Multicluster Mobile Multimedia Radio Networks”, ACM-Baltzer Journal of Wireless
Networks, Vol. 1, No. 3, pp. 255–256, 1995.
14. C.-C. Chiang, H.-K. Wu, W. Liu, and M. Gerla, “Routing in Clustered Multihop, Mobile Wireless Networks
with Fading Channel,” in Proc. IEEE Singapore Int. Conf. on Networks, Singapore pp. 197–211, 1997
15. J.Y. Yu and P.H.J. Chong, “A survey of Clustering Schemes for Mobile Ad Hoc Networks”, IEEE Communications Surveys and Tutorials, Vol. 7, No. 1, pp. 32–48, 2005.
16. O. Liang, A. Sekercioglu, and N. Mani, “A Survey of Multipoint Relay based Broadcast Schemes in Wireless
Ad Hoc Networks”, IEEE Communications Surveys and Tutorials, Vol. 8, No. 4, pp. 30–46, 2006.
17. K. Viswanath, K. Obraczka, and G. Tsudik, “Exploring Mesh and Tree-based Multicast Routing Protocols for
MANETs”, IEEE Transactions on Mobile Computing, Vol. 5, No. 1, pp. 28–42, 2006.

MANET Routing with Provably Low Complexity

619

18. Y.-C. Tseng, S.-Y. Ni, Y.-S. Chen, and J.-P. Sheu, “The Broadcast Storm Problem in a Mobile Ad Hoc Network”,
Wireless Networks, Vol. 8, No. 2–3, pp. 153–167, 2002.
19. B. Williams, D. Metha, T. Camp, and W. Navidi, “Predictive Models to Rebroadcast in Mobile Ad Hoc
Networks”, IEEE Transactions on Mobile Computing, Vol. 3, No. 3, pp. 295–303, 2004.
20. J. Wu and F. Dai, “Efficient Broadcasting with Guaranteed Coverage in Mobile Ad Hoc Networks”, IEEE
Transactions on Mobile Computing, Vol. 4, No. 3, pp. 259–270, 2005.
21. S.-J. Lee, E.M. Belding-Royer, and C.E. Perkins, “Scalability Study of the Ad Hoc On-demand Distance Vector
Routing Protocol”, International Journal of Network Management, Vol. 13, No. 2, pp. 97–114, 2003.
22. K. Xu and M. Gerla, “A Heterogeneous Routing Protocol based on a New Stable Clustering Scheme”, in Proc.
of IEEE Milcom, Anaheim, CA pp. 838–843, 2002.
23. J. Wu and F. Dai, “Virtual Backbone Construction in MANETs using Adjustable Transmission Ranges”, IEEE
Transactions on Mobile Computing, Vol. 5, No. 9, pp. 1188–1200, 2006.
24. D.B. Johnson, D.A. Malts, and J. Broch, “DSR: The Dynamic Source Routing Protocol for Multi-hop Wireless
Ad Hoc Networks”, in Ad Hoc Networking, Chapter 5, C.E. Perkins, Ed. Reading, MA: Addison–Wesley, pp.
139–172, 2001
25. Y. Qin and J. He, “The Impact on Throughput of Hierarchical Routing in Ad Hoc Wireless Networks”, in Proc.
of IEEE ICC, Seoul, Korea pp. 3010–3014, 2005
26. C.E. Perkins, E.M. Belding-Royer, S.R. Das, and M.K. Marina, “Performance Comparison of Two On-demand
Routing Protocols for Ad Hoc Networks”, IEEE Personal Communications, Vol. 8, No. 1, pp. 16–28, 2001.
27. X. Yu and Z. Kedem, “A Distributed Adaptive Cache Update Algorithm for the Dynamic Source Routing
Protocol”, in Proc. of IEEE Infocom, Miami, FL, 2005, pp. 730–739.

Martin Reisslein is an Associate Professor in the Department of Electrical Engineering at
Arizona State University, Tempe. He received his Ph.D. in systems engineering from the
University of Pennsylvania in 1998. From July 1998 through October 2000 he was a scientist
with the German National Research Center for Information Technology (GMD FOKUS),
Berlin and lecturer at the Technical University Berlin. He maintains an extensive library of
video traces for network performance evaluation, including frame size traces of MPEG–4 and
H.263 encoded video, at http://trace.eas.asu.edu.

620

H.-S. Yang et al.

Andréa W. Richa is an Associate Professor at the Department of Computer Science and
Engineering at Arizona State University, Tempe, since August 2004. Prof. Richa received her
M.S. and Ph.D. degrees from the School of Computer Science at Carnegie Mellon University,
in 1995 and 1998, respectively. She also earned an M.S. degree in Computer Systems from
the Graduate School in Engineering (COPPE), and a B.S. degree in Computer Science, both at
the Federal University of Rio de Janeiro, Brazil, in 1992 and 1990, respectively. Prof. Richa’s
main area of research is in network algorithms. Prof. Richa’s data tracking (or name lookup)
algorithm has been widely recognized as the first benchmark algorithm for the development
of distributed databases in peer-to-peer networking, having being references by over 130 academic journal or conference publications to date, and being implemented as part of two of the
current leading projects in peer-to-peer networking. Dr. Richa’s was the recipient of an NSF
CAREER Award in 1999.

Luke Ritchie is a Ph.D. student at Arizona State University (ASU), Tempe. He received
B.S. and M.S. degrees in Electrical Engineering from Arizona State University in 2003 and
2004, respectively. His research interests are in the areas of interaction between medium
access control (MAC) and routing in mobile ad-hoc networks (MANETs) and wireless sensor
networks (WSNs).

MANET Routing with Provably Low Complexity

621

Hyo-Sik Yang is an assistant professor at Sejong University, Department of Computer Engineering, Seoul, Korea. Before he joined Sejong University, he was an assistant professor at
Kyungnam University, Masan, Korea. He joined Sejong University on Fall 2006. He was
a faculty research associate at Arizona State University (ASU), 2005. He received the B.
E. degree in the Department of Information and Communication Engineering form Myongji
University, Yongin, Korea, in 1998 and the M. S. and Ph. D. in Electrical Engineering from
Arizona State University, Tempe, AZ, U.S.A., in 2000 and 2005, respectively.
His research interests are wavelength-division-multiplexing (WDM) all-optical networks,
WDM packet switching, and WDM metropolitan area networks including node architecture,
optimization, medium access control (MAC), traffic analysis, and routing in mobile ad-hoc
networks (MANETs). He is also working on Substation Automation System (SAS) compying
with IEC 61850 - communication networks and system in substations.

Infinite Object Coating in the Amoebot Model
Zahra Derakhshandeh1 , Robert Gmyr2 , Andréa W. Richa1 ,
Christian Scheideler2 , Thim Strothmann2 , Shimrit Tzur-David3

arXiv:1411.2356v1 [cs.ET] 10 Nov 2014

1

Department of Computer Science and Engineering,
Arizona State University, USA
2

Department of Computer Science,
University of Paderborn, Germany

3

Department of Computer Science,
Ben-Gurion University, Israel

Abstract
The term programmable matter refers to matter which has the ability to change its physical properties (shape,
density, moduli, conductivity, optical properties, etc.) in a programmable fashion, based upon user input or
autonomous sensing. This has many applications like smart materials, autonomous monitoring and repair, and
minimal invasive surgery. While programmable matter might have been considered pure science fiction more than
two decades ago, in recent years a large amount of research has been conducted in this field. Often programmable
matter is envisioned as a very large number of small locally interacting computational particles. We propose the
Amoebot model, a new model which builds upon this vision of programmable matter. Inspired by the behavior
of amoeba, the Amoebot model offers a versatile framework to model self-organizing particles and facilitates
rigorous algorithmic research in the area of programmable matter. We present an algorithm for the problem of
coating an infinite object under this model, and prove the correctness of the algorithm and that it is work-optimal.

1

Introduction

Recent advances in microfabrication and cellular engineering foreshadow that in the next few decades it might be
possible to assemble myriads of simple information processing units at almost no cost. Microelectronic mechanical
components have become so inexpensive to manufacture that one can anticipate integrating logic circuits, microsensors, and communications devices onto nano-computational components. Imagine coating bridges and buildings
with smart paint that senses and reports on traffic and wind loads and monitors structural integrity. A smart-paint
coating on a wall could sense vibrations, monitor the premises for intruders, and cancel noise. There has also been
amazing progress in understanding the biochemical mechanisms in individual cells such as the mechanisms behind
cell signaling and cell movement [3]. Recently, it has been demonstrated that, in principle, biological cells can
be turned into finite automata [8] or even pushdown automata [35], so one can imagine that some day one can
tailor-make biological cells to function as sensors and actuators, as programmable delivery devices, and as chemical
factories for the assembly of nano-scale structures.
One can envision producing vast quantities of individual microscopic computational particles—whether microfabricated particles or engineered cells—to form programmable matter, as coined by Toffoli and Margolous [44].
These particles are possibly faulty, sensitive to the environment, and may produce various types of local actions that
range from changing their internal state to communicating with other particles, sensing the environment, moving to
a different location, changing shape or color, or even replicating. Those individual local actions may then be used to
change the physical properties, color, and shape of the matter at a global scale.
We propose Amoebot, a new amoeba-inspired model for programmable matter1 . In our model, the programmable
matter consists of particles that can bond to neighboring particles and use these bonds to form connected structures.
Particles only have local information and have modest computational power: Each particle has only a constantsize memory and behaves similarly to a finite state machine. The particles act asynchronously and they achieve
locomotion by expanding and contracting, which resembles the behavior of amoeba [3].
1.1 Our Contributions
Our proposed Amoebot model, presented in Section 3, offers a versatile framework to model self-organizing particles
and facilitates rigorous algorithmic research in the area of programmable matter. In addition, we present an algorithm
for the problem of coating an infinite object under this model in Section 5.2, and prove the correctness of the
algorithm (Theorem 1) and that the algorithm is work-optimal (Theorem 2).

2

Related Work

While programmable matter may have seemed like science fiction more than two decades ago, we have seen many
advances in this field recently. One can distinguish between active and passive systems. In passive systems the
particles either do not have any intelligence at all (but just move and bond based on their structural properties or due
to chemical interactions with the environment), or they have limited computational capabilities but cannot control
their movements. Examples of research on passive systems are DNA computing [1, 10, 14, 20, 38, 48], tile selfassembly systems in general [41, 42], population protocols [4], and slime molds [11, 36, 46]. We will not describe
these models in detail as they are only of little relevance for our approach. On the other hand in active systems,
there are computational particles that can control the way they act and move in order to solve a specific task. Selforganizing networks, robotic swarms, and modular robotic systems are some examples of active systems.
Self-organizing networks have been studied in many different contexts. Networks that evolve out of local, selforganizing behavior have been heavily studied in the context of complex networks such as small-world networks
[6, 33, 47]. However, whereas a common approach for the complex networks field is to study the global effect of
given local interaction rules, we aim at developing local interaction rules in order to obtain a desired global effect.
1

A preliminary version of our model was presented at the First Biological Distributed Algorithms (BDA) Workshop, co-located with
DISC, October 2013, and has appeard as a Brief Announcement at ACM SPAA 2014 [22, 21].

1

In the area of swarm robotics it is usually assumed that there is a collection of autonomous robots that have
limited sensing, often including vision, and communication ranges, and that can freely move in a given area. They
follow a variety of goals, as for example graph exploration (e.g., [27]), gathering problems (e.g., [2, 16]) , and shape
formation problem (e.g., [28]). Surveys of recent results in swarm robotics can be found in [32, 37]; other samples
of representative work can be found in e.g., [25, 7, 18, 19, 17, 31, 30, 43, 26, 39, 34, 5, 40]. Besides work on how
to set up robot swarms in order to solve certain tasks, a significant amount of work has also been invested in order to
understand the global effects of local behavior in natural swarms like social insects, birds, or fish (see e.g., [13, 9]).
While the analytical techniques developed in the area of swarm robotics and natural swarms are of some relevance
for this work, the underlying model differs significantly as we do not allow free movement of particles.
While swarm robotics focuses on inter-robotic aspects in order to perform certain tasks, the field of modular selfreconfigurable robotic systems focuses on intra-robotic aspects such as the design, fabrication, motion planning, and
control of autonomous kinematic machines with variable morphology (see e.g., [29, 50]). Metamorphic robots form
a subclass of self-reconfigurable robots that shares the characteristics of our model that all particles are identical and
that they fill space without gaps [15]. The hardware development in the field of self-reconfigurable robotics has been
complemented by a number of algorithmic advances (e.g., [12, 45]), but so far mechanisms that scale to hundreds or
thousands of individual units are still under investigation, and no rigorous theoretical foundation is available yet.
As in our model, the work in [24] also assumes that each system particle is a finite state machine with constantsize memory operating in an asynchronous distributed fashion. However, the work in [24] assumes a static network
topology and the problems considered only address what can be computed in an asynchronous network of randomized constant-size memory finite state machines, and not how desired topology/shape can be achieved in systems
where finite state machine particles can move (in addition to computing).
The nubot model [49], by Woods et al., was developped independently to our model (our model was originally
presented at the First Workshop on Biological Distributed Algorithms (BDA), October 2013 [22, 21]), and aims
at providing the theoretical framework that would allow for a more rigorous algorithmic study of biomolecularinspired systems, more specifically of self-assembly systems with active molecular components. While our model
shares many similarities with the nubot model at a high level, many of the assumptions underlying the nubot model
are different from ours: For example, in the nubot model, particles are allowed to replicate at will and are allowed
to drag many other particles as they move in space (which is pertinent in many molecular level systems, but which
would not be feasible in systems with very large numbers of nano-robots with weak bond structures); also the number
of states (and hence also the memory size) that a particle can be in is proportional to log n—where n relates to the
number of particles in the final desired configuration the system should assume—whereas in our model the number
of possible states is assumed to be constant.

3

Model

Consider the equilateral triangular graph Geqt , see Figure1. A particle occupies either a single node or a pair of

Figure 1: The left half of the figure depicts a section of the infinite equilateral triangular graph Geqt . Nodes are shown as
black circles. For four of the nodes the dual faces in the hexagonal tiling of the Euclidean plane are shown in gray. The right
half shows five particles on the graph. When depicting particles we draw the graph as a gray mesh without nodes. A particle
occupying a single node is depicted as a black circle, and a particle occupying two nodes is depicted as two black circles
connected by an edge.

2

adjacent nodes in Geqt , and every node can be occupied by at most one particle. Two particles occupying adjacent
nodes are defined to be connected and we refer to such particles as neighbors. The graph Geqt is the dual graph
of the hexagonal tiling of the Euclidean plane as indicated in Figure 1. So geometrically the space occupied by a
particle is bound by either one face or two adjacent faces in this tiling of the plane.
Every particle has a state from a finite set Q. Connected particles can communicate via the edges connecting
them in the following way. A particle p holds a flag from a finite alphabet Σ for each edge that is incident to p (i.e.,
all edges incident to a node occupied by p except the edge between the occupied nodes if p occupies two nodes). A
particle occupying the node on the other side of such an edge can read this flag. This communication process can be
used in both directions over an edge. In order to allow a particle p to address the edges incident to it, the edges are
labeled from the local perspective of p. This labeling starts with 0 at an edge leading to a node that is only adjacent
to one of the nodes occupied by p and increases counter-clockwise around the particle.
Particles move through expansion and contraction: If a particle occupies one node, it can expand into an unoccupied adjacent node to occupy two nodes. If a particle occupies two nodes, it can contract out of one of these nodes
to occupy only a single node. (Those two actions can be naturally physically realized on the dual hexagonal tiling of
the Euclidean plane.) Accordingly, we call a particle occupying a single node contracted and a particle occupying
two nodes expanded. Note that we can identify six directions in our graph corresponding to the directions of the
six edges incident to a node. The direction of the edge labeled 0 is defined to remain constant throughout all movement. We call this direction the orientation of a particle. Figure 2 shows an example of the movement of a particle.
Besides executing expansions and contractions in isolation, we allow pairs of connected particles to combine these
primitives to perform a coordinated movement: One particle can contract out of a certain node at the same time as
another particle expands into that node. We call this movement a handover, see Figure 3. The particles involved in
a handover are defined to remain connected during its execution.
Computationally, particles resemble finite state machines. A particle acts according to a transition function
δ : Q × Σ10 → P(Q × Σ10 × M ).
For a particle p the function takes the current state of p and the flags p can read via its incident edges as arguments.
Here, the i-th coordinate of the tuple Σ10 represents the flag read via the edge labeled i when numbering the coordinates of the tuple starting at 0. If for a label i there is no edge with that label or if the respective edge leads to a
node that is not occupied, the coordinate of the tuple is defined to be ε. The value ε ∈ Σ is reserved for this purpose
and cannot be set as a flag by a particle. The transition function maps to a set of turns. A turn is a tuple specifying a
state to assume, flags to set, and a movement to execute. The set of movements is defined as
M = {idle} ∪ {expandi | i ∈ [0, 9]} ∪ {contracti | i ∈ [0, 9]} ∪ {handoverContracti | i ∈ [0, 9]}.
The movement idle means that p does not move, and expandi and contracti are defined as mentioned above. The
index i specifies the edge that defines the direction along which the movement should take place, as shown in the

Figure 2: The three parts of the figure show a moving particle together with the labels seen by the particle. On the left, the
particle occupies only a single node. The particle then expands in the direction of the edge labeled 4 resulting in the particle
occupying two nodes as depicted in the middle. Since the expansion changes the number of edges incident to the particle, the
edges have to be relabeled. The direction of the edge labeled 0 remains constant. Next the particle contracts out of one of the
nodes it currently occupies towards the direction of the edge labeled 6 resulting in the particle occupying only a single node as
depicted on the right. Again, the edges incident to the particle are relabeled.

3

Figure 3: Two particles performing a handover.

example in Figure 2. Note that there are only two possible directions for a contract operation. The movement
handoverContracti specifies a contraction that can only be executed as part of a handover. In summary, a transition
function specifies a set of turns a particle would like to perform based on the locally available information.
A system of particles progresses by executing atomic actions. An action is either the execution of an isolated turn
for a single particle or the execution of a turn for each of two particles resulting in a handover between those particles.
Note that if a movement is not executable, the respective action is not enabled: For example, a particle occupying
two nodes cannot expand although it might specify this movement in a turn. As another example, a particle cannot
expand into an occupied node except as part of a handover. Finally, an action consisting of an isolated turn involving
the movement handoverContracti is never enabled as this movement can only be performed as part of a handover.
The transition function is applied for each particle to determine the set of enabled actions in the system. From this
set, a single action is arbitrarily chosen and executed. The process of evaluating the transition function and executing
an action continues as long as there is an enabled action.
Two actions are said to be independent if they do not involve nodes that are neighbors in Geqt . Each particle can
locally ensure that at most one action in its neighborhood is executed at any point in time. Hence, all of our results
also hold if a set of mutually independent actions was chosen to be concurrently executed at any point in time.

4

Morphing Problems

In general, we define a morphing problem as being a problem in which a system of particles has to morph into a
shape with specific characteristics (by changing the positioning of the particles in Geqt ) while sustaining connectivity.
Examples of morphing problems are the formation of geometric shapes and coating objects (i.e., surrounding a given
set of nodes). Before we can formally define morphing problems, we need some definitions.
We define the configuration of a particle as the tuple of its state, its flags, the set of nodes it occupies, and its
orientation. A system of particles progresses by performing atomic actions, each of which affects the configuration
of one or two particles. Therefore, a system progresses through a sequence of configurations where a configuration
of a system is the set of configurations of all its particles. We define the connectivity graph G(c) of a configuration
c as the subgraph of Geqt induced by the occupied nodes in c.
We can formally define a morphing problem as a tuple M = (I, G) where I and G are sets of connected
configurations. We say I is the set of initial configurations and G is the set of goal configurations. An algorithm
A, formally defined by a transition function δ, solves M if three conditions hold: Consider the execution of A on
a system in an arbitrary configuration from I. First, the system stays connected throughout the execution of A.
Second, the execution eventually reaches a configuration in which the transition function of each particle maps to
the empty set (we say A terminates). Third, when the execution terminates, the reached configuration is from G.

5

Infinite Object Coating

As a subclass of the class of morphing problems, one can consider coating problems in which an object is to be
coated (i.e., surrounded or engulfed) by the particles of a system as uniformly as possible. We investigate the Infinite
Object Coating problem where the object has an infinite surface and, accordingly, a uniform coating is accomplished
when all the particles of a system are directly connected to the object.

4

5.1 Problem Definition
In the Infinite Object Coating problem, an object can be represented by a set of contracted particles occupying nodes
in Geqt . These particles are in a special object state, and we refer to these particles as object particles. A transition
function must map to the empty set for an object particle, and no particle can switch into the object state. We denote
the number of non-object particles in a system by n. In the reminder of this paper, unless otherwise stated, when we
refer to a particle, we mean a non-object particle. We say a particle lies on the surface of the object if it is connected
to an object particle. Consider a connected induced subgraph C of Geqt . The subgraph C is called compact if
Geqt − C is 2-connected. An object that induces a compact subgraph in Geqt is a valid object. Intuitively, this
definition means that a valid object cannot have tunnels of width one, see Figure 4. Disallowing these tunnels allows
particles to move along the object in single file (as will be described in Section 5.2.1) without blocking each other
and therefore improves the clarity of presentation by avoiding boundary cases.

Figure 4: An example of an invalid object. The object occupies a half-plane except for the nodes marked by the solid line.
These nodes form a tunnel of width one in the object since removing the topmost of these nodes from Geqt − C disconnects
the two remaining nodes in that graph.

As presented in Section 4, a morphing problem is defined as a tuple (I, G) where I is a set of initial configurations and G is a set of goal configurations. For the Infinite Object Coating problem, I is the set of all connected
configurations consisting of a valid object together with a finite set of contracted particles. Every particle stores a
phase as part of its state and in an initial configuration every particle is in an inactive phase; we will elaborate on
phases in Section 5.2.2. Similarly, the set G is the set of all configurations consisting of a valid object together with
a finite set of contracted particles that all lie on the surface of the object.
The general Amoebot model as described in Section 3 can take various specific forms depending on how systems
of particles are initialized, what information particles keep track of in their state, and what information they share
over their edges. In the Infinite Object Coating problem, we do not make any assumption about the orientation of the
particles. Therefore, we work in a no-compass variant of the model. While particles do not share a common sense of
direction, they are able to keep track of directions by storing edge labels in their state and updating these labels upon
movement. The updates can be encoded in the transition function. We assume that a particle keeps track of whether
it is contracted or expanded; the particle also keeps track of which edge labels are incident to the occupied node that
is an endpoint of the edge labeled 0. Additionally, we assume that for an edge with label i the corresponding flag
always includes the index i, the information whether the edge is incident to the occupied node that is an endpoint
of the edge labeled 0, and whether the particle is contracted or expanded. Using this information, a particle that
reads a flag can compare its orientation to the orientation of the particle that set the flag and therefore particles can
exchange information about directions. Lastly, we assume that a particle keeps track of what edge labels specify
valid contraction indices.
For the sake of generality, the Amoebot model does not enforce any fairness condition on the execution of
actions. However, for the Infinite Object Coating problem we make the following assumption: Any set of of consecutive configurations in which a particle p could be affected by an enabled action, but is not, is finite.

5

5.2 Algorithm
Our algorithm for the Infinite Object Coating problem is a combination of three algorithmic primitives. First, particles lying on the surface of the object lead the way by moving in a common direction along the surface. Second, the
remaining particles follow behind the leading particles resulting in the system flattening out towards the direction
of movement. Third, particles on the surface check whether there are particles not lying on the surface and use this
information to eventually achieve termination. We present each of these primitives in detail in the following sections.
5.2.1 Moving Along a Surface
Our first algorithmic primitive solves a simple problem: We want all particles on the surface to move along the
surface in a common direction. However, before we can present our algorithm for this problem, we need some
definitions. For an expanded particle, we denote the node the particle last expanded into as the head of the particle
and call the other occupied node its tail. For a contracted particle, we define the single occupied node to be both the
head and the tail. The set of labels associated to the edges incident to the head can be encoded as part of the state,
and this information can be set upon expansion as part of the transition function. Therefore, a particle can always
distinguish the labels of edges incident to its head (head labels) from the labels of edges incident to its tail (tail
labels). Accordingly, we call edges that are labeled with a head label head edges and the remaining edges tail edges.
Combined with the information about valid contraction indices described in Section 5.1, a particle can deliberately
contract out of its tail or its head. In our algorithm, particles are only allowed to contract out of their tails so that the
fact that a particle contracts uniquely defines the contraction direction. Note that with this convention, the head of a
particle still is occupied by that particle after a contraction.
A particle on the surface can move along the surface in two directions. However, we want all particles on
the surface to move in a common direction. The simple procedure given in Algorithm 1, which is very similar to
the moving algorithm presented by Drees et al. [23], can be used to achieve this goal: A contracted particle uses
Algorithm 1 Movement along a Surface
let k be the size of the neighborhood of the particle (i.e., k = 6 or k = 10)
let i be the label of an edge connected to the object
while edge i is connected to the object do
i ← (i − 1) mod k
return i
the procedure to compute the direction of an expansion, and an expanded particle simply contracts according to
above definitions. The correctness of this approach is based on two facts. First, all particles share a common sense
of rotation (i.e., the edge labels always increase counter-clockwise around a particle). Second, according to our
definition of a valid object, an object must occupy a single consecutive sequence of nodes around a particle from the
local perspective of that particle and not all nodes around a particle belong to the object.
5.2.2 Spanning Forest Algorithm
As the name suggests, the spanning forest algorithm aims to organize the particles in a system as a spanning forest
where the particles that represent the roots of the trees in the forest are considered leaders whom the remaining
particles follow. Therefore, the movement of the system is dominated by the movement of the leaders. Every
particle that is connected to the surface becomes a leader, and leaders move along the surface as described in the
previous section. Algorithm 2 provides a detailed description of this approach.
In contrast to particles in phase inactive, we say followers and leaders are active. As specified in Algorithm 2,
the value d is only defined for followers. We denote the node in Geqt reached from a follower p via the edge labeled
d as u(p). The following lemmas demonstrate some properties that hold during the execution of the spanning forest
algorithm and will be used in Section 5.3 to analyze our complete algorithm.
6

Algorithm 2 Spanning Forest Algorithm
A particle is in one of three phases inactive, follow, and lead. Initially, all particles are assumed to be in phase
inactive. The phase of a particle is encoded as part of its state, and a particle indicates its phase as part of all its flags.
We call a particle in phase follow a follower and a particle in phase lead a leader. A follower stores a head label d
in its state and includes a follow indicator in the flag for the edge with label d. Depending on its phase, a particle p
behaves as described below. The transition function maps either to a set containing a single turn or to the empty set.
The specified conditions are to be checked in the given order. If a condition holds, the transition function maps to the
set containing only the respective turn. If none of the conditions holds, the transition function maps to the empty set.
inactive: If p is connected to the surface, it becomes a leader and executes the idle movement. If an adjacent
node is occupied by a leader or a follower, p sets d to point towards that node and becomes a follower.
follow:
If p is contracted and connected to the surface, it becomes a leader and executes the idle movement.
If p is contracted and there is an expanded particle p0 occupying the node reached via the edge labeled
d, p expands in direction d as an attempt to a handover and sets d to correspond to the contraction
direction of p0 . If p is expanded and a follow indicator is read from a contracted neighbor over a tail
edge, p executes a handover contraction and changes d to keep the direction constant. If p is expanded,
no follow indicator is read over a tail edge, and p has no inactive neighbor, p contracts and changes d
to keep the direction constant.
lead:
If p is contracted, it expands in the direction computed by Algorithm 1. If p is expanded and a follow
indicator is read from a contracted neighbor over a tail edge, p executes a handover contraction. If p
is expanded, no follow indicator is read over a tail edge, and there is no inactive neighbor, p contracts.
Lemma 1 For a follower p the node u(p) is occupied by an active particle.
Proof. Consider a follower p in any configuration during the execution of Algorithm 2. Note that p can only get into
phase follow from phase idle, and once it leaves the follow phase it will not switch to that phase again. Consider the
first configuration c1 in which p is a follower. In the configuration c0 immediately before c1 , p must be inactive and
it becomes a follower because of an active particle p0 occupying u(p) in c0 . The particle p0 still occupies u(p) in c1 .
Now assume that u(p) is occupied by an active particle p0 in a configuration ci , and that p is still a follower in the
next configuration ci+1 that results from executing an action a. If a affects p and p0 , the action must be a handover
in which p updates its value d such that u(p) changes but p0 again occupies u(p) in ci+1 . If a affects p but not p0 , it
must be a contraction in which u(p) does not change and is still occupied by p0 . If a affects p0 but not p, there are
multiple possibilities. The particle p0 might switch from phase follow to phase lead or it might expand, neither of
which violate the lemma. Furthermore, p0 might contract. If u(p) is the head of p0 , p0 still occupies u(p) in ci+1 .
Otherwise, p0 reads a follow indicator from p over a tail edge in ci and therefore the contraction must be part of a
handover. As p is not involved in the action, the handover must be between p0 and a third active particle p00 . It is
easy to see that after such a handover u(p) is occupied by either p0 or p00 . Finally, if a affects neither p nor p0 , u(p)
will still be occupied by p0 in ci+1 .
t
u
Based on Lemma 1, we define a successor relation on the active particles in a configuration c. Let p be a follower.
We say p0 is the successor of p if p0 occupies u(p). Analogously, we say p is a predecessor of p0 . Furthermore, we
define a directed graph A(c) for a configuration c as follows. A(c) contains the same nodes as G(c). For every
expanded particle p in c, A(c) contains a directed edge from the tail to the head of p, and for every follower p0 in c,
A(c) contains a directed edge from the head of p0 to u(p0 ).
Lemma 2 The graph A(c) is a forest, and if there is at least one active particle, every connected component of
inactive particles contains a particle that is connected to an active particle.
Proof. In an initial configuration c0 , all particles are inactive and therefore the lemma holds trivially. Now assume
that the lemma holds for a configuration ci . We will show that it also holds for the next configuration ci+1 that
7

results from executing an action a. If a affects an inactive particle p, this particle either becomes a follower or a
leader. In the former case p joins an existing tree, and in the latter case p forms a new tree in A(ci+1 ). In either case,
A(ci+1 ) is a forest and the connected component of inactive particles that p belongs to in ci is either non-existent
or connected to p in ci+1 . If a affects only a single particle p that is in phase follow, this particle can contract or
become a leader. In the former case, p has no predecessor p0 such that u(p0 ) is the tail of p and also p has no idle
neighbors. Therefore, the contraction of p does not disconnect any follower or inactive particle and, accordingly,
does not violate the lemma. In the latter case, p becomes a root of a tree which also does not violates the lemma. If a
involves only a single particle p that is in phase lead, p can expand or contract. An expansion trivially cannot violate
the lemma and the argument for the contraction is the same as for the contraction of a follower above. Finally, if
a involves two active particles in ci , these particles perform a handover. While such a handover can change the
successor relation among the nodes, it cannot violate the lemma.
t
u
The following lemma shows that the spanning forest algorithm achieves progress in that as long as the leaders
keep moving, the remaining particles will eventually follow them.
Lemma 3 An expanded particle eventually contracts.
Proof. Consider an expanded particle p in a configuration c. Note that p must be active. If there is an enabled action
that includes the contraction of p, that action will remain enabled until p contracts and therefore p will contract
eventually according to the fairness assumption we made in Section 5.1. So assume that there is no enabled action
that includes the contraction of p. According to the behavior of inactive particles, at some point in time all particles
in the system will be active. If the contraction of p becomes part of an enabled action before this happens, p will
eventually contract. So assume that all particles are active but still p cannot contract. If p has no predecessors, the
isolated contraction of p is an enabled action which contradicts our assumption. Therefore, p must have predecessors.
Furthermore, p must read at least one follow indicator over a tail edge and all predecessors from which it reads a
follow indicator must be expanded as otherwise p could again contract as part of a handover. Let p0 be one of the
predecessors of p. If p0 would contract, a handover between p0 and p would become an enabled action. We can apply
the complete argument presented in this proof so far to p0 and so on backwards along a branch in a tree in A(c) until
we reach a particle that can contract. We will reach such a particle by Lemma 2. Therefore, we found a sequence
of expanded particles that starts with p0 and ends with a particle that eventually contracts. The contraction of that
last particle will allow the particle before it in the sequence to contract and so on. Finally, the contraction of p will
become part of an enabled action and therefore p will eventually contract.
t
u
In the above lemmas, the direction of expansion of leaders is not used. Furthermore, the fact that only particles
on the surface become leaders is not used. Therefore, the algorithm works independently of the selection of leaders
and their expansion direction. This makes the spanning forest algorithm a reusable algorithmic primitive.
5.2.3 Complaining Algorithm
The algorithm so far achieves that the particles spread out towards one direction on the surface, which will be shown
formally in Section 5.3. However, the particles keep moving indefinitely even when all particles lie on the surface.
Since we require termination from an algorithm to solve the Infinite Object Coating problem, we need another
algorithmic primitive that ensures that once all particles are on the surface, they eventually stop moving. To achieve
this, we use the idea of complaining, see Algorithm 3. The algorithm extends Algorithm 2 by changing the set of
turns for leaders. The conditions in Algorithm 3 ought to be checked before the conditions given in Algorithm 2.
Note that a complaint indicator will be consumed by a leader p if it expands, contracts, or performs a handover.
That is, as long as all particles which forwarded the indicator have not moved up to p, p will not see a complaint
indicator. Furthermore, consider a follower q that reached the surface, but is not a leader yet. If q reads a complaint
indicator, it will not forward the indicator directly, but as soon as it turns into a leader. Moreover, if all particles are
8

Algorithm 3 Complaining Algorithm
Consider a leader particle p and let s be the direction returned by Algorithm 1, i.e., the direction that leaders use to
travel along the surface. Leaders can include a complaint indicator in a flag. If p is contracted and cannot expand or
perform a handover and sees a follow indicator or complaint indicator, it sends a complaint indicator in direction s
and performs the idle movement. If p is contracted and does not see a complaint indicator, it does not perform any
action. Otherwise p behaves according to Algorithm 2.
leaders, then no leader sees a follow indicator. We extend the notion of u(p) from Section 5.2.2 to leaders. The node
u(p) for a leader p is the node in the direction returned by Algorithm 1. Hence, the notion of successors (i.e., p0 is a
successor of p in some configuration c if p0 occupies u(p)) is now also applicable for leaders. If u(p) is unoccupied, p
has no successor. The descendants of a particle p are all nodes reachable by the successor relation (i.e., the successor
of, the successor of the successor, and so on). For each particle p we denote the descendant that has no successor
with a(p). For the next lemma consider a system that behaves according to Algorithm 2 and Algorithm 3.
Lemma 4 As long as a follower particle p exists, a descendant will eventually expand, and if all particles are
leaders the transition function of every leader eventually maps to the empty set.
Proof. For the first statement assume that even though the particle p exists, no descendant expands. Following
our assumption none of the descendants can expand, therefore they all have to be contracted, because an expanded
descendant would allow for a handover which involves an expansion. Therefore, a complaint indicator is created
by a leader particle that is a descendant of p and sees a follow indicator. This indicator is forwarded among the
descendants along the surface until a(p) sees it. Particle a(p) can always expand, which contradicts the assumption.
To prove the second statement, we look at the case in which all particles are leaders. We already mentioned
that no more follow indicators exist. Therefore, it is easy to see that all complaint indicators eventually vanish.
Accordingly, the transition function of all particles without a neighbor in direction s maps to the empty set. As a
result, the transition function of leaders that are neighbors to leaders without a neighbor in direction s will eventually
map to the empty set. This process continues until the transition function of every particle maps to the empty set. t
u
5.3 Analysis
Now, we can show that our algorithm as developed in the previous three sections solves the Infinite Object Coating
problem.
Theorem 1 Our algorithm solves the Infinite Object Coating problem.
Proof. First, we have to show that the algorithm maintains connectivity. So consider a system of particles in a
configuration during the execution of our algorithm. The object is by definition connected. A leader always lies on
the surface of the object according to Algorithm 2. A follower is always part of a tree in the spanning forest as shown
in Lemma 2. As every tree forms a connected component and is rooted in a leader, the set of object particles and
active particles forms a connected component. Finally, an inactive particle is always part of a connected component
of inactive particles that includes a particle that is connected to an active particle, again by Lemma 2. Therefore, all
particles in the system form a single connected component.
Next, we have to show that the algorithm terminates and that when it does, the system is in a goal configuration.
A common property of all goal configurations is that all particles lie on the surface. In our algorithm, every particle
p eventually activates. If p initially lies on the surface, it becomes a leader and remains on the surface. If p initially
does not lie on the surface, it becomes a follower. Let c be the first configuration in which p is a follower. Consider
the directed path in A(c) from the head of p to the first node on the surface. There always is such a path since every
follower belongs to a tree in A(c) by Lemma 2, every such tree is rooted in a leader, and a leader only occupies
nodes on the surface. Let P = (u0 , u1 , . . . , uk ) be that path where u0 is the head of p and uk lies on the surface.
9

Figure 5: Worst-case configuration concerning work. The object particles are shown in black and the non-object particles are
shown in red. The infinite object is a half-plane and the n non-object particles lie on a straight line.

According to Algorithm 2, p attempts to follow P by sequentially expanding into the nodes u1 , . . . , uk . By Lemma 4,
the algorithm does not terminate before p reaches the surface, and according to Lemma 3, p can actually execute all
movements required to follow P . Therefore, p eventually lies on the surface, becomes a leader, and remains on the
surface. According to Lemma 4 this means that for all particles the transition function eventually maps to the empty
set which implies termination.
t
u
Finally, we would like to measure how well our algorithm performs in terms of energy consumption. For this,
we consider the number of movements executed in a system until termination and call this measure work. When we
refer to movement in the context of work, we only mean expansions and contraction but not idle movements. We
count a handover as two movements. We ignore any computation a particle performs since in a physical realization
the energy consumption of computation is most likely negligible compared to the energy consumption of movement.
Lemma 5 The worst-case work required by any algorithm to solve the Infinite Object Coating problem is Ω(n2 ).
Proof. Consider the configuration depicted in Figure 5. The particle labeled
2i movements before
Pi requires at least
2 ) work.
2i
=
Ω(n
t
u
it lies contracted on the surface. Therefore, any algorithm requires at least n−1
i=0
Theorem 2 Our algorithm requires worst-case optimal work Θ(n2 ).
Proof. To prove the upper bound, we simply show that every particle executes O(n) movements. The theorem
then follows by Lemma 5. Consider a particle p. While p is inactive, it does not move. While p is a follower, it
moves along a path to the surface as described in the proof of Theorem 1. The length of this path is bound by 2n
and, therefore, the number of movements p executes while being a follower is O(n). While p is a leader it only
performs expansions if it reads a complaint indicator. Since a complaint indicator is consumed by an expansion (see
Section 5.2.3), a leader can see at most n − 1 indicators. Every expansion is followed by a contraction, therefore the
number of movements p executes while being a leader is as well O(n), which concludes the theorem.
t
u

6

Conclusion

In this work we have formally defined the Amoebot model and presented a work-optimal algorithm for the Infinite
Object Coating problem under this model. We want to use the Amoebot model to investigate various other problems
in which the system of particles forms a single connected component at all times. Other coating problems might be
considered, in particular when the object surface is finite and the surface of an object has to be coated as uniformly
as possible by the particles of a system (possibly with multiple layers of “coating”). A second example is the class of
shape formation problems in which a system has to arrange to form a specific shape, with or without a seed particle.
Finally, in bridging problems particles have to bridge gaps in given structures. We see the coating as an algorithmic
primitive for solving other problems. For example, the formation of a shape can be achieved by creating an initially
small instance of that shape which is then iteratively coated to form increasingly large instances until the number
of particles in the system is exhausted. Furthermore, we envision that our spanning forest algorithm (Section 5.2.2)
may in turn be a building block for other variations of the coating problems.
10

References
[1] L. M. Adleman. Molecular computation of solutions to combinatorial problems. Science, 266(11):1021–1024,
1994.
[2] Chrysovalandis Agathangelou, Chryssis Georgiou, and Marios Mavronicolas. A distributed algorithm for
gathering many fat mobile robots in the plane. In Proceedings of the 2013 ACM symposium on Principles of
distributed computing, pages 250–259. ACM, 2013.
[3] R. Ananthakrishnan and A. Ehrlicher. The forces behind cell movement. International Journal of Biological
Sciences, 3(5):303–317, 2007.
[4] D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in networks of passively mobile
finite-state sensors. Distributed Computing, 18(4):235–253, 2006.
[5] D. Arbuckle and A. Requicha. Self-assembly and self-repair of arbitrary shapes by a swarm of reactive robots:
algorithms and simulations. Autonomous Robots, 28(2):197–211, 2010.
[6] Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286(5439):509–
512, 1999.
[7] L. Barriere, P. Flocchini, E. Mesa-Barrameda, and N. Santoro. Uniform scattering of autonomous mobile
robots in a grid. Int. Journal of Foundations of Computer Science, 22(3):679–697, 2011.
[8] Y. Benenson, T. Paz-Elizur, R. Adar, E. Keinan, Z. Livneh, and E. Shapiro. Programmable and autonomous
computing machine made of biomolecules. Nature, 414(6862):430–434, 2001.
[9] A. Bhattacharyya, M. Braverman, B. Chazelle, and H.L. Nguyen. On the convergence of the hegselmannkrause system. CoRR, abs/1211.1909, 2012.
[10] D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall. On the computational power of DNA. Discrete Applied
Mathematics, 71:79–94, 1996.
[11] V. Bonifaci, K. Mehlhorn, and G. Varma. Physarum can compute shortest paths. In Proceedings of SODA ’12,
pages 233–240, 2012.
[12] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita. Generic decentralized control for lattice-based selfreconfigurable robots. International Journal of Robotics Research, 23(9):919–937, 2004.
[13] B. Chazelle. Natural algorithms. In Proc. of ACM-SIAM SODA, pages 422–431, 2009.
[14] K. C. Cheung, E. D. Demaine, J. R. Bachrach, and S. Griffith. Programmable assembly with universally
foldable strings (moteins). IEEE Transactions on Robotics, 27(4):718–729, 2011.
[15] G. Chirikjian. Kinematics of a metamorphic robotic system. In Proceedings of ICRA ’94, volume 1, pages
449–455, 1994.
[16] Mark Cieliebak, Paola Flocchini, Giuseppe Prencipe, and Nicola Santoro. Distributed computing by mobile
robots: Gathering. SIAM Journal on Computing, 41(4):829–879, 2012.
[17] R. Cohen and D. Peleg. Local spreading algorithms for autonomous robot systems. Theoretical Computer
Science, 399(1-2):71–82, 2008.

11

[18] S. Das, P. Flocchini, N. Santoro, and M. Yamashita. On the computational power of oblivious robots: forming
a series of geometric patterns. In Proceedings of 29th ACM Symposium on Principles of Distributed Computing
(PODC), 2010.
[19] X. Defago and S. Souissi. Non-uniform circle formation algorithm for oblivious mobile robots with convergence toward uniformity. Theoretical Computer Science, 396(1-3):97–112, 2008.
[20] E. D. Demaine, M. J. Patitz, R. T. Schweller, and S. M. Summers. Self-assembly of arbitrary shapes using
rnase enzymes: Meeting the kolmogorov bound with small scale factor (extended abstract). In Proceedings of
STACS ’11, pages 201–212, 2011.
[21] Zahra Derakhshandeh, Shlomi Dolev, Robert Gmyr, Andréa W. Richa, Christian Scheideler, and Thim Strothmann. Brief announcement: amoebot - a new model for programmable matter. In 26th ACM Symposium on
Parallelism in Algorithms and Architectures, SPAA ’14, Prague, Czech Republic - June 23 - 25, 2014, pages
220–222, 2014.
[22] Shlomi Dolev, Robert Gmyr, Andra W. Richa, and Christian Scheideler. Ameba-inspired self-organizing particle systems. CoRR, abs/1307.4259, 2013.
[23] Maximilian Drees, Martina Hüllmann, Andreas Koutsopoulos, and Christian Scheideler. Self-organizing particle systems. In IPDPS, pages 1272–1283, 2012.
[24] Yuval Emek and Roger Wattenhofer. Stone age distributed computing. In Proceedings of the 2013 ACM
symposium on Principles of distributed computing, pages 137–146. ACM, 2013.
[25] S. Fekete, C. Gray, and A.Kroeller. Evacuation of rectilinear polygons. In Proceedings of the 4th International
Conference Combinatorial Optimization and Applications (COCOA), pages 21–30, 2010.
[26] S.P. Fekete and C. Schmidt.
43(2):148–168, 2010.

Polygon exploration with time-discrete vision.

Computational Geometry,

[27] Paola Flocchini, David Ilcinkas, Andrzej Pelc, and Nicola Santoro. Computing without communicating: Ring
exploration by asynchronous oblivious robots. Algorithmica, 65(3):562–583, 2013.
[28] Paola Flocchini, Giuseppe Prencipe, Nicola Santoro, and Peter Widmayer. Arbitrary pattern formation by
asynchronous, anonymous, oblivious robots. Theoretical Computer Science, 407(1):412–447, 2008.
[29] T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss. Self organizing robots based on cell structures - cebot.
In Proceedings of IROS ’88, pages 145–150, 1988.
[30] T.-R. Hsiang, E. Arkin, M. Bender, S. Fekete, and J. Mitchell. Algorithms for rapidly dispersing robot swarms
in unknown environments. In Proceedings of the 5th Workshop on Algorithmic Foundations of Robotics
(WAFR), pages 77–94, 2002.
[31] B. Katreniak. Biangular circle formation by asynchronous mobile robots. In Proceedings of the 12th International Colloquium on Structural Information and Communication Complexity (SIROCCO), pages 85–99,
2005.
[32] S. Kernbach, editor. Handbook of Collective Robotics – Fundamentals and Challanges. Pan Stanford Publishing, 2012.
[33] J. Kleinberg. The small-world phenomenon: an algorithmic perspective. In Proceedings of STOC ’00, pages
163–170, 2000.
12

[34] P. Kling and F. Meyer auf der Heide. Convergence of local communication chain strategies via linear transformations. In Proceedings of the 23rd ACM Symposium on Parallelism in Algorithms and Architectures, pages
159–166, 2011.
[35] T. Krasinski, S. Sakowski, and T. Poplawski. Autonomous push-down automaton built on dna. Informatica,
36:263–276, 2012.
[36] K. Li, K. Thomas, C. Torres, L. Rossi, and C.-C. Shen. Slime mold inspired path formation protocol for
wireless sensor networks. In Proceedings of ANTS ’10, pages 299–311, 2010.
[37] J. McLurkin. Analysis and Implementation of Distributed Algorithms for Multi-Robot Systems. PhD thesis,
Massachusetts Institute of Technology, 2008.
[38] R. Nagpal, A. Kondacs, and C. Chang. Programming methodology for biologically-inspired self-assembling
systems. Technical report, AAAI Spring Symposium on Computational Synthesis, 2003.
[39] C. Parker and H. Zhang. Collective robotic site preparation. Adaptive Behavior, 14(1):5–19, 2006.
[40] M. Rubenstein and W. Shen. Automatic scalable size selection for the shape of a distributed robotic collective.
In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), 2010.
[41] Aaron Sterling. Distributed agreement in tile self-assembly. Natural Computing, 10(1):337–355, 2011.
[42] Aaron D Sterling. A limit to the power of multiple nucleation in self-assembly. In Distributed Computing,
pages 451–465. Springer, 2008.
[43] I. Suzuki and M. Yamashita. Distributed anonymous mobile robots: Formation of geometric patterns. SIAM
Journal on Computing, 28(4):1347–1363, 1999.
[44] Tommaso Toffoli and Norman Margolus. Programmable matter: concepts and realization. Physica D: Nonlinear Phenomena, 47(1):263–272, 1991.
[45] J. E. Walter, J. L. Welch, and N. M. Amato. Distributed reconfiguration of metamorphic robot chains. Distributed Computing, 17(2):171–189, 2004.
[46] S. Watanabe, A. Tero, A. Takamatsu, and T. Nakagaki. Traffic optimization in railroad networks using an
algorithm mimicking an amoeba-like organism, physarum plasmodium. Biosystems, 105(3):225–232, 2011.
[47] D. J. Watts and S. H. Strogatz. Collective dynamics of ’small-world’ networks. Nature, 393(6684):440–442,
1998.
[48] E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman. Design and self-assembly of two-dimensional dna
crystals. Nature, 394(6693):539–544, 1998.
[49] Damien Woods, Ho-Lin Chen, Scott Goodfriend, Nadine Dabby, Erik Winfree, and Peng Yin. Active selfassembly of algorithmic shapes and patterns in polylogarithmic time. In ITCS, pages 353–354, 2013.
[50] M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll, H. Lipson, E. Klavins, and G. S. Chirikjian. Modular
self-reconfigurable robot systems. IEEE Robotics Automation Magazine, 14(1):43–52, 2007.

13

A Jamming-Resistant MAC Protocol for
Multi-Hop Wireless Networks⋆
Andrea Richa1 , Christian Scheideler2 , Stefan Schmid3 , Jin Zhang1
1

arXiv:1007.1189v3 [cs.DC] 9 Jul 2010

2

Computer Science and Engineering, SCIDSE, Arizona State University
Tempe, AZ 85287, USA; {aricha,jzhang82}@asu.edu
Department of Computer Science, University of Paderborn, D-33102 Paderborn,
Germany,; scheideler@upb.de
3
Deutsche Telekom Laboratories, TU Berlin, D-10587 Berlin, Germany
stefan@net.t-labs.tu-berlin.de

Abstract. This paper presents a simple local medium access control
protocol, called Jade, for multi-hop wireless networks with a single channel that is provably robust against adaptive adversarial jamming. The
wireless network is modeled as a unit disk graph on a set of nodes distributed arbitrarily in the plane. In addition to these nodes, there are
adversarial jammers that know the protocol and its entire history and
that are allowed to jam the wireless channel at any node for an arbitrary (1 − ǫ)-fraction of the time steps, where 0 < ǫ < 1 is an arbitrary
constant. We assume that the nodes cannot distinguish between jammed
transmissions and collisions of regular messages. Nevertheless, we show
that Jade achieves an asymptotically optimal throughput if there is a
sufficiently dense distribution of nodes.

1

Introduction

The problem of coordinating the access to a shared medium is a central challenge
in wireless networks. In order to solve this problem, a proper medium access
control (MAC) protocol is needed. Ideally, such a protocol should not only be able
to use the wireless medium as effectively as possible, but it should also be robust
against attacks. Unfortunately, most of the MAC protocols today can be easily
attacked. A particularly critical class of attacks are jamming attacks (i.e., denialof-service attacks on the broadcast medium). Jamming attacks are typically easy
to implement as the attacker does not need any special hardware. Attacks of
this kind usually aim at the physical layer and are realized by means of a high
transmission power signal that corrupts a communication link or an area, but
they may also occur at the MAC layer, where an adversary may either corrupt
control packets or reserve the channel for the maximum allowable number of
slots so that other nodes experience low throughput by not being able to access
the channel. In this paper we focus on jamming attacks at the physical layer,
that is, the interference caused by the jammer will not allow the nodes to receive
⋆

A preliminary version of this article appeared at the 24th International Symposium
on Distributed Computing (DISC), 2010.

messages. The fundamental question that we are investigating is: Is there a MAC
protocol such that for any physical-layer jamming strategy, the protocol will still
be able to achieve an asymptotically optimal throughput for the non-jammed time
steps? Such a protocol would force the jammer to jam all the time in order to
prevent any successful message transmissions. Finding such a MAC protocol
is not a trivial problem. In fact, the widely used IEEE 802.11 MAC protocol
already fails to deliver any messages for very simple oblivious jammers that
jam only a small fraction of the time steps [3]. On the positive side, Awerbuch
et al. [2] have demonstrated that there are MAC protocols which are provably
robust against even massive adaptive jamming, but their results only hold for
single-hop wireless networks with a single jammer, that is, all nodes experience
the same jamming sequence.
In this paper, we significantly extend the results in [2]. We present a MAC
protocol called Jade (a short form of “jamming defense”) that can achieve a
constant fraction of the best possible throughput for a large class of jamming
strategies in a large class of multi-hop networks where transmissions and interference can be modeled using unit-disk graphs. These jamming strategies include
jamming patterns that can be completely different from node to node. It turns
out that while Jade differs only slightly from the MAC protocol of [2], the
proof techniques needed for the multi-hop setting significantly differ from the
techniques in [2].
1.1

Model

We consider the problem of designing a robust MAC protocol for multi-hop
wireless networks with a single wireless channel. The wireless network is modeled
as a unit disk graph (UDG) G = (V, E) where V represents a set of n = |V | honest
and reliable nodes and two nodes u, v ∈ V are within each other’s transmission
range, i.e., {u, v} ∈ E, if and only if their (normalized) distance is at most 1.
We assume that time proceeds in synchronous time steps called rounds. In each
round, a node may either transmit a message or sense the channel, but it cannot
do both. A node which is sensing the channel may either (i) sense an idle channel
(if no other node in its transmission range is transmitting at that round and its
channel is not jammed), (ii) sense a busy channel (if two or more nodes in its
transmission range transmit at that round or its channel is jammed), or (iii)
receive a packet (if exactly one node in its transmission range transmits at that
round and its channel is not jammed).
In addition to these nodes there is an adversary (who may control any number
of jamming devices). We allow the adversary to know the protocol and its entire
history and to use this knowledge in order to jam the wireless channel at will
at any round (i.e, the adversary is adaptive). However, like in [2], the adversary
has to make a jamming decision before it knows the actions of the nodes at the
current round. The adversary can jam the nodes individually at will, as long
as for every node v, at most a (1 − ǫ)-fraction of its rounds is jammed, where
ǫ > 0 can be an arbitrarily small constant. That is, v has the chance to receive
a message in at least an ǫ-fraction of the rounds. More formally, an adversary is

called (T, 1 − ǫ)-bounded for some T ∈ N and 0 < ǫ < 1, if for any time window
of size w ≥ T and at any node v, the adversary can jam at most (1 − ǫ)w of the
w rounds at v.
Given a node v and a time interval I, we define fv (I) as the number of time
steps in I that are non-jammed at v and sv (I) as the number of time steps in I in
which v successfully receives a message. A MAC protocol is called c-competitive
against some (T, 1 − ǫ)-bounded adversary if, for any time interval
P I with |I| ≥
K P
for a sufficiently large K (that may depend on T and n),
v∈V sv (I) ≥
c · v∈V fv (I). In other words, a c-competitive MAC protocol can achieve at
least a c-fraction of the best possible throughput.
Our goal is to design a symmetric local-control MAC protocol (i.e., there is
no central authority controlling the nodes, and all the nodes are executing the
same protocol) that has a constant-competitive throughput against any (T, 1−ǫ)bounded adversary in any multi-hop network that can be modeled as a UDG. In
order to obtain a more refined picture of the competitiveness of our protocol, we
will also investigate so-called k-uniform adversaries. An adversary is k-uniform
if the node set V can be partitioned into k subsets so that the jamming sequence
is the same within each subset. In other words, we require that at all times, the
nodes in a subset are either all jammed or all non-jammed. Thus, a 1-uniform
jammer jams either everybody or nobody in a round whereas an n-uniform
jammer can jam the nodes individually at will.
In this paper, we will say that a claim holds with high probability (w.h.p.) iff
it holds with probability at least 1 − 1/nc for any constant c ≥ 1; it holds with
moderate probability (w.m.p.) iff it holds with probability at least 1 − 1/(log n)c
for any constant c ≥ 1.
1.2

Related Work

Due to the topic’s importance, wireless network jamming has been extensively
studied in the applied research fields [1,5,6,22,26,27,28,30,31,37,38,39,40], both
from the attacker’s perspective [6,26,27,40] as well as from the defender’s perspective [1,5,6,27,28,30,38,40]—also in multi-hop settings (e.g. [21,32,42,43,44]).
Traditionally, jamming defense mechanisms operate on the physical layer
[28,30,36]. Mechanisms have been designed to avoid jamming as well as detect
jamming. Spread spectrum technology has been shown to be very effective to
avoid jamming as with widely spread signals, it becomes harder to detect the
start of a packet quickly enough in order to jam it. Unfortunately, protocols
such as IEEE 802.11b use relatively narrow spreading [20], and some other IEEE
802.11 variants spread signals by even smaller factors [5]. Therefore, a jammer
that simultaneously blocks a small number of frequencies renders spread spectrum techniques useless in this case. As jamming strategies can come in many
different flavors, detecting jamming activities by simple methods based on signal strength, carrier sensing, or packet delivery ratios has turned out to be quite
difficult [27].
Recent work has also studied MAC layer strategies against jamming, including coding strategies [6], channel surfing and spatial retreat [1,41], or mecha-

nisms to hide messages from a jammer, evade its search, and reduce the impact
of corrupted messages [38]. Unfortunately, these methods do not help against
an adaptive jammer with full information about the history of the protocol, like
the one considered in our work.
In the theory community, work on MAC protocols has mostly focused on
efficiency. Many of these protocols are random backoff or tournament-based protocols [4,7,17,18,25,34] that do not take jamming activity into account and, in
fact, are not robust against it (see [2] for more details). The same also holds for
many MAC protocols that have been designed in the context of broadcasting [8]
and clustering [24]. Also some work on jamming is known (e.g., [9] for a short
overview). There are two basic approaches in the literature. The first assumes
randomly corrupted messages (e.g. [33]), which is much easier to handle than
adaptive adversarial jamming [3]. The second line of work either bounds the
number of messages that the adversary can transmit or disrupt with a limited
energy budget (e.g. [16,23]) or bounds the number of channels the adversary can
jam (e.g. [10,11,12,13,14,15,29]).
The protocols in [16,23] can tackle adversarial jamming at both the MAC
and network layers, where the adversary may not only be jamming the channel
but also introducing malicious (fake) messages (possibly with address spoofing).
However, they depend on the fact that the adversarial jamming budget is finite,
so it is not clear whether the protocols would work under heavy continuous
jamming. (The result in [16] seems to imply that a jamming rate of 1/2 is the
limit whereas the handshaking mechanisms in [23] seem to require an even lower
jamming rate.)
In the multi-channel version of the problem introduced in the theory community by Dolev [13] and also studied in [10,11,12,13,14,15,29], a node can only
access one channel at a time, which results in protocols with a fairly large runtime (which can be exponential for deterministic protocols [11,14] and at least
quadratic in the number of jammed channels for randomized protocols [12,29]
if the adversary can jam almost all channels at a time). Recent work [10] also
focuses on the wireless synchronization problem which requires devices to be
activated at different times on a congested single-hop radio network to synchronize their round numbering while an adversary can disrupt a certain number of
frequencies per round. Gilbert et al. [15] study robust information exchange in
single-hop networks.
Our work is motivated by the work in [3] and [2]. In [3] it is shown that an
adaptive jammer can dramatically reduce the throughput of the standard MAC
protocol used in IEEE 802.11 with only limited energy cost on the adversary
side. Awerbuch et al. [2] initiated the study of throughput-competitive MAC
protocols under continuously running, adaptive jammers, but they only consider
single-hop wireless networks. We go one step further by considering multi-hop
networks where different nodes can have different channel states at a time, e.g.,
a transmission may be received only by a fraction of the nodes. It turns out
that while the MAC protocol of [2] can be adopted to the multi-hop setting
with a small modification, the proof techniques cannot. We are not aware of any

other theoretical work on MAC protocols for multi-hop networks with provable
performance against adaptive jamming.
1.3

Our Contributions

In this paper, we present a robust MAC protocol called Jade. Jade is a fairly
simple protocol: it is based on a very small set of assumptions and rules and has
a minimal storage overhead. In fact, in Jade every node just stores a constant
number of parameters, among them a fixed parameter γ that should be chosen
so that the following main theorem holds:
Theorem 1. When running Jade for Ω((T log n)/ǫ+(log n)4 /(γǫ)2 ) time steps,
Jade has a constant competitive throughput for any (T, 1 − ǫ)-bounded adversary
and any UDG w.h.p. as long as γ = O(1/(log T +log log n)) and (a) the adversary
is 1-uniform and the UDG is connected, or (b) there are at least 2/ǫ nodes within
the transmission range of every node.
Note that in practice, log T and log log n are rather small so that our condition
on γ is not too restrictive. Also, a conservative estimate on log T and log log n
would leave room for a superpolynomial change in n and a polynomial change
in T over time.
On the other hand, we can also show the following result demonstrating that
Theorem 1 essentially captures all the scenarios (within our notation) under
which Jade can have a constant competitive throughput.
Theorem 2. If (a) the UDG is not connected, or (b) the adversary is allowed
to be 2-uniform and there are nodes with o(1/ǫ) nodes within their transmission
range, then there are cases in which Jade is not constant competitive for any
constant c independent of ǫ.
Certainly, no MAC protocol can guarantee a constant competitive throughput if the UDG is not connected. However, it is still open whether there are
simple MAC protocols that are constant competitive under non-uniform jamming strategies even if there are o(1/ǫ) nodes within the transmission range of
a node.

2

Description of Jade

This section first gives a short motivation for our algorithmic approach and then
presents the Jade protocol in detail.
2.1

Intuition

The intuition behind our MAC protocol is simple: in each round, each node u
tries to send a message with probability pu with pu ≤ p̂ for some small constant 0 < p̂ < 1. Consider the unit disk D(u) around node u consisting of

u’s neighboring
nodes as well as u.1 Moreover, let N (u) = D(u) \ {u} and
P
p = v∈N (u) pv . Suppose that u is sensing the channel. Let q0 be the probability that the channel is idle at u and let q1 be the probability
that exactly
Q
one node in N (u) is sending a message. It holds that q0 = v∈N (u) (1 − pv ) and
Q
P
q1 = v∈N (u) pv w∈N (u)\{v} (1 − pw ). Hence,
q1 ≤

X

v∈N (u)

pv

1
1 − p̂

Y

(1−pw ) =

X
q0 · p
pv
, q1 ≥
1 − p̂
v∈N (u)

w∈N (u)

Y

(1−pw ) = q0 ·p.

w∈N (u)

Thus we have the following lemma, which has also been derived in [2] for the
single-hop case.
Lemma 1. q0 · p ≤ q1 ≤

q0
1−p̂

· p.

By Lemma 1, if a node v observes that the number of rounds in which the
channel is idle is essentially
P equal to the number of rounds in which exactly one
message is sent, then p = v∈N (v) pv is likely to be around 1 (if p̂ is a sufficiently
small constant), which would be ideal. Otherwise, the nodes know that they need
to adapt their probabilities. Thus, if we had sufficiently many cases in which an
idle channel or exactly one message transmission is observed (which is the case if
the adversary does not heavily jam the channel and p is not too large), then one
can adapt the probabilities pv just based on these two events and ignore all cases
in which the wireless channel is blocked, either because the adversary is jamming
it or because at least two messages interfere with each other (see also [19] for
a similar conclusion). Unfortunately, p can be very high for some reason, which
requires a more sophisticated strategy for adjusting the access probabilities.
2.2

Protocol Description

In Jade, each node v maintains a probability value pv , a threshold Tv and a
counter cv . The parameters p̂, γ > 0 in the protocol are fixed and the same for
every node. p̂ may be set to any constant value so that 0 < p̂ ≤ 1/24, and γ
should be small enough so that the condition in Theorem 1 is met.
Initially, every node v sets Tv := 1, cv := 1 and pv := p̂. Afterwards, the Jade
protocol works in synchronized rounds. In every round, each node v decides with
probability pv to send a message. If it decides not to send a message, it checks
the following two conditions:
– If v senses an idle channel, then pv := min{(1 + γ)pv , p̂}.
– If v successfully receives a message, then pv := (1 + γ)−1 pv and Tv :=
max{Tv − 1, 1}.
1

In this paper, disks (and later sectors) will refer both to 2-dimensional areas in the
plane as well as to the set of nodes in the respective areas. The exact meaning will
become clear in the specific context.

Afterwards, v sets cv := cv + 1. If cv > Tv then it does the following: v sets
cv := 1, and if there was no round among the past Tv rounds in which v sensed a
successful message transmission or an idle channel, then pv := (1 + γ)−1 pv and
Tv := min{Tv + 1, 21/(4γ) } .
As we will see in the upcoming section, the concept of using a multiplicativeincrease-multiplicative-decrease mechanism for pv and an additive-increaseadditive-decrease mechanism for Tv , as well as the slight modifications of the
protocol in [2], marked in italic above, are crucial for Jade to work.

3

Analysis of Jade

In contrast the description of Jade, its stochastic analysis is rather involved as it
requires to shed light onto the complex interplay of the nodes all following their
randomized protocol in a highly dependent manner. We first prove Theorem 1
(Sections 3.1 and 3.2) and then prove Theorem 2 (Section 3.3). In order to show
the theorems, we will frequently use the following variant of the Chernoff bounds
[2,35].
Lemma 2. Consider any set of binary random
Q1 , . . . , Xn . Suppose
Q variables X
that there are values p1 , . . . , pn ∈ [0, 1] P
with E[ i∈S Xi ] ≤P i∈S pi for every set
n
n
S ⊆ {1, . . . , n}. Then it holds for X = i=1 Xi and µ = i=1 pi and any δ > 0
that
µ

δ2 µ
eδ
≤ e− 2(1+δ/3) .
P[X ≥ (1 + δ)µ] ≤
1+δ
(1 + δ)
Q
Q
If, on the other hand, it holds that E[ i∈S Xi ] ≥ i∈S pi for every set S ⊆
{1, . . . , n}, then it holds for any 0 < δ < 1 that
P[X ≤ (1 − δ)µ] ≤



e−δ
(1 − δ)1−δ

µ

≤ e−δ

2

µ/2

.

Throughout the section we assume that γ = O(1/(log T + log log n)) is sufficiently small.
3.1

Proof of Theorem 1

We first look at a slightly weaker form of adversary. We say a round t is open for
a node v if v and at least one other node in its neighborhood are non-jammed
(which implies that v’s neighborhood is non-empty). An adversary is weakly
(T, 1 − ǫ)-bounded for some T ∈ N and 0 < ǫ < 1 if the adversary is (T, 1 − ǫ)bounded and in addition to this, at least a constant fraction of the non-jammed
rounds at each node are open in every time interval of size w ≥ T .

Theorem 3. When running Jade for Ω([T + (log3 n)/(γ 2 ǫ)] · (log n)/ǫ) rounds
it holds w.h.p. that Jade is constant competitive for any weakly (T, 1−ǫ)-bounded
adversary.
Proof. First, we focus on a time frame F consisting of (α log n)/ǫ subframes
of size f = α[T + (log3 n)/(γ 2 ǫ)] each, where f is a multiple of T and α is a
sufficiently large constant. The proof needs the following three lemmas. The first
one is identical to Claim 2.5 in [2]. It is true because only successful message
transmissions reduce Tu .
Lemma 3. If in a time interval I the number of rounds in which a node u
successfully
receives a message is at most r, then u increases Tu in at most
p
r + 2|I| rounds in I.

The second lemma holds for arbitrary (not just weakly) (T, 1 − ǫ)-bounded
adversaries and will be shown in Section 3.2.
P
Lemma 4. For every node u, v∈D(u) pv = O(1) for at least a (1 − ǫβ)-fraction
of the rounds in time frame F , w.h.p., where the constant β > 0 can be made
arbitrarily small.
The third lemma just follows from some simple geometric argument.
Lemma 5. A disk of radius 2 can be cut into at most 20 regions so that the
distance between any two points in a region is at most 1.

Consider some fixed node u. Let J ⊆ F be the set of all non-jammed open
rounds at u in time frame F (which are a constant fraction of the non-jammed
rounds at u because we have a weakly
P (T, 1 − ǫ)-bounded adversary). Let p be
a constant satisfying Lemma 4 (i.e., w∈D(v) pw ≤ p). Define DD(u) to be the
disk of radius 2 around u (i.e., it has twice the radius of D(u)). Cut DD(u)
into 20 regions R1 , . . . , R20 satisfying Lemma 5, and let vi be any node in region
Ri (if such a node exists), where vi = u if u ∈ Ri . According to Lemma 4 it
holds
for each i that at least a (1 − ǫβ ′ /20)-fraction of the rounds in F satisfy
P
′
′′
w∈D(vi ) pw ≤ p for any constantPβ > 0, w.h.p. Thus, at least a (1 − ǫβ )fraction of the rounds in F satisfy w∈D(vi ) pw ≤ p for every i for any constant
β ′′ > 0, w.h.p. As D(v) ⊆ DD(u) for all v ∈ D(u) and u has at least ǫ|F | nonjammed rounds in F , we get the following lemma, which also holds for arbitrary
(T, 1 − ǫ)-bounded adversaries
P
Lemma 6. At least a (1 − β)-fraction of the rounds in J satisfy v∈D(u) pv ≤ p
P
and w∈D(v) pw = O(p) for all nodes v ∈ D(u) for any constant β > 0, w.h.p.

Let us call these rounds good. Since the probability that u senses
P the channel
is at least 1− p̂ and the probability that the channel at u is idle for w∈D(u) pw ≤
Q
Q
p is equal to v∈N (u) (1 − pv ) ≥ v∈N (u) e−2pv ≥ e−2p , u senses an idle channel
for at least (1 − p̂)(1 − β)|J|e−2p ≥ 2β|J| many rounds in J on expectation if β is
sufficiently small. This also holds w.h.p. when using the Chernoff bounds under

the condition that at least (1 − β)|J| rounds in F are good (which also holds
w.h.p.). Let k be the number of times u receives a message in F . We distinguish
between two cases.
Case 1: k ≥ β|J|/6. Then Jade is constant competitive for u and we are done.
Case 2: kp
< β|J|/6. Then we know from Lemma 3 that pu is decreased at most
β|J|/6 + 2|F | times in F due to cu > Tu . In addition to this, pu is decreased
at most β|J|/6 times in F due to a received message. On the other hand, pu
is increased at least 2β|J| times in J (if possible) due to an idle channel w.h.p.
Also, we know from the Jade protocol that at thepbeginning of F , pu = p̂.
Hence, there must be at least β(2 − 1/6 − 1/6)|J| − 2|F | ≥ (3/2)β|J| rounds
in J w.h.p. at which pu = p̂. As there are at least (1 − β)|J| good rounds in
J (w.h.p.), there are at least β|J|/2 good rounds in J w.h.p. in which pu = p̂.
For these good rounds, u has a constant probability to transmit a message and
every node v ∈ D(u) has a constant probability of receiving it, so u successfully
transmits Θ(|J|) messages to at least one of its non-jammed neighbors in F (on
expectation and also w.h.p.).
If we charge 1/2 of each successfully transmitted message to the sender and
1/2 to the receiver, then a constant competitive throughput can be identified for
every node in both cases above, so Jade is constant competitive in F .
It remains to show that Theorem 3 also holds for larger time intervals than
|F |. First, note that all the proofs are valid as long as γ ≤ 1/[c(log T + log log n)]
for a constant c ≥ 2, so we can increase T and thereby also |F | as long as this
inequality holds. So w.l.o.g.
we may assume that γ = 1/[2(log T + log log n)].
p
In thispcase, 21/(4γ) ≤ |F |, so our rule of increasing Tv in Jade implies that
Tv ≤ |F | at any time. This allows us to extend the competitive throughput
result from a single to any sequence of polynomial in n many time frames F ,
which completes the proof of Theorem 3.
⊓
⊔
Now, let us consider the two cases of Theorem 1. Recall that we allow here
any (T, 1 − ǫ)-bounded adversary and not just a weakly bounded.
Case 1: the adversary is 1-uniform and the UDG is connected. In this
case, every node has a non-empty neighborhood and therefore all non-jammed
rounds of the nodes are open. Hence, the conditions on a weakly (T, 1 − ǫ)bounded adversary are satisfied. So Theorem 3 applies, which completes the
proof of Theorem 1 a).
Case 2: |D(v)| ≥ 2/ǫ for all v ∈ V . Consider some fixed time interval I
with |I| being a multiple of T . For every node v ∈ D(u) let fv be the number
of non-jammed rounds at v in I and ov be the number of open rounds at v in
I. Let J be the set of rounds in I with at most one non-jammed node. Suppose
that |J| > (1 − ǫ/2)|I|. Then every node in D(u) must have more than (ǫ/2)|I|
of its non-jammed rounds in J. As these non-jammed rounds must be serialized

P
in J to satisfy our requirement on J, it holds that |J| >
v∈D(u) (ǫ/2)|I| ≥
(2/ǫ) · (ǫ/2)|I|
=
|I|.
Since
this
is
impossible,
it
must
hold
that
|J|
≤ (1 − ǫ/2)|I|.
P
P
P
f
)
−
|J|
≥
(1/2)
o
≥
(
Thus,
v∈D(u) fv because
v∈D(u) v
v∈D(u) v
P
′
v∈D(u) fv ≥ (2/ǫ) · ǫ|I| = 2|I|. Let D (u) be the set of nodes v ∈ D(u)
with ov ≥ fv /4. That is, for each of P
these nodes, a constant fraction
of the
P
non-jammed time steps is open. Then v∈D(u)\D′ (u) ov < (1/4) v∈D(u) fv , so
P
P
P
fv .
v∈D′ (u) ov ≥ (1/2)
v∈D(u) ov ≥ (1/4)
v∈D(u)S
Consider now a set U ⊆ V of nodes so that u∈U D(u) = V and for every
v ∈ V there are at most 6 nodes u ∈ U with v ∈ D(u) (U is easy to construct
in a greedy fashion for arbitrary
S UDGs and also
Pknown as a dominating
P set of
constant density). Let V ′ = u∈U D′ (u). Since v∈D′ (u) ov ≥ (1/4) v∈D(u) fv
P
P
P
for every node u ∈ U , it follows that v∈V ′ ov ≥ (1/6) u∈U v∈D′ (u) ov ≥
P
P
P
(1/24) u∈U v∈D(u) fv ≥ (1/24) v∈V fv . Using that together with Theorem 3, which implies that Jade is constant competitive w.r.t. the nodes in V ′ ,
completes the proof of Theorem 1 b).
3.2

Proof of Lemma 4

In order to finish the proof of Theorem 1, it remains to prove Lemma 4. Consider
any fixed node u. We partition u’s unit disk D(u) into six sectors of equal angles
from u, S1 , ..., S6 . Note that all nodes within a sector Si have distances of at
most 1 from each other, so they can directly communicate with each other (in
D(u), distances can be up to 2). We will first explore properties of an arbitrary
node in one sector, then consider the implications for a whole sector, and finally
bound the cumulative sending probability in the entire unit disk.
Recall the definition of a time frame, a subframe and f in the proof of Theorem 3. Fix a sector S in D(u) and consider some fixed time frame F . Let us
refer to the sumPof the probabilities of the neighboring nodes of a given node
v ∈ S by p̄v := w∈S\{v} pw . The following lemma shows that pv will decrease
dramatically if p̄v is high throughout a certain time interval.
Lemma 7. Consider a node v in a unit disk D(u). If p̄v > 5 − p̂ during all
rounds of a subframe I of F , then pv will be at most 1/n2 at the end of I, w.h.p.
Proof. We say that a round is useful for node v if from v’s perspective there is an
idle channel or a successful transmission at that round (when ignoring the action
of v); otherwise the round is called non-useful. Note that in a non-useful round,
according to our protocol, pv will either decrease (if the threshold Tv is exceeded)
or remain the same. On the other hand, in a useful round, pv will increase (if v
senses an idle channel), decrease (if v senses a successful transmission) or remain
the same (if v sends a message). Hence, pv can only increase during useful rounds
of I. Let U be the set of useful rounds in I for our node v. We distinguish between
two cases, depending on the cardinality |U|. In the following, let pv (0) denote
the probability of v at the beginning of I (which is at most p̂). Suppose that
f ≥ 2[(3c ln n)/γ]2 for a sufficiently large constant c. (This lower bound coincides
with our definition of f in the proof of Theorem 3.)

Case 1: Suppose that |U| < (c ln n)/γ, that is, many rounds are blocked and
pv can increase only rarely. As there are at least (3c ln n)/γ occasions in I in
which cv > Tv and |U| < (c ln n)/γ, in at least (2c ln n)/γ of these occasions v
only saw blocked channels for Tv consecutive rounds and therefore decides to
increase Tv and decrease pv . Hence, at the end of I,
pv ≤ (1 + γ)|U |−2c ln n/γ pv (0) ≤ (1 + γ)−c ln n/γ pv (0) ≤ e−c ln n = 1/nc .
Case 2: Next, suppose that |U| ≥ (c ln n)/γ. We will show that many of these
useful rounds will be successful such that pv decreases. Since pv ≤ p̂ ≤ 1/24
throughout I, it follows from the Chernoff bounds that w.h.p. v will sense the
channel for at least a fraction of 2/3 of the useful rounds w.h.p. Let this set of
useful rounds be called U ′ . Consider any round t ∈ U ′ . Let q0 be the probability
that there is an idle channel at round t and q1 be the probability that there
is a successful transmission at t. It holds that q0 + q1 = 1. From Lemma 1 we
also know that q1 ≥ q0 · p̄v . Since p̄v > 5 − p̂ for all rounds in I, it follows that
q1 ≥ 4/5 for every round in U ′ . Thus, it follows from the Chernoff bounds that
for at least 2/3 of the rounds in U ′ , v will sense a successful transmission w.h.p.
Hence, at the end of I it holds w.h.p. that
′

pv ≤ (1 + γ)−(1/3)·|U | pv (0) ≤ (1 + γ)−(1/3)·(2c/3) ln n/γ pv (0) ≤ e−(2c/9) ln n = 1/n2c/9 .
Combining the two cases with c ≥ 9 results in the lemma.

⊓
⊔

Given this property of the individual probabilities, we can derive a bound
for
P the cumulative probability of an entire sector S. In order to compute pS =
v∈S pv , we introduce three thresholds, a low one, ρgreen = 5, one in the middle,
ρyellow = 5e, and a high one, ρred = 5e2 . The following three lemmas provide
some important insights about these probabilities.
Lemma 8. For any subframe I in F and any initial value of pS in I there is at
least one round in I with pS ≤ ρgreen w.h.p.
Proof. We prove the lemma by contradiction. Suppose that throughout the entire
interval I, pS > ρgreen . Then it holds for every node v ∈ S that p̄v > ρgreen − p̂
throughout I. In this case, however, we know from Lemma 7, that pv will decrease
to at most 1/n2 at the end of I w.h.p. Hence, all nodes v ∈ S would decrease
pv to at most 1/n2 at the end of I w.h.p., which results in pS ≤ 1/n. This
contradicts our assumption, so w.h.p. there must be a round t in I at which
pS ≤ ρgreen .
⊓
⊔
Lemma 9. For any time interval I in F of size f and any sector S it holds
that if pS ≤ ρgreen at the beginning of I, then pS ≤ ρyellow throughout I, w.m.p.
Similarly, if pS ≤ ρyellow at the beginning of I, then pS ≤ ρred throughout I,
w.m.p.
Proof. It suffices to prove the lemma for the case that initially pS ≤ ρgreen
as the other case is analogous. Consider some fixed round t in I. Let pS be the

cumulative probability at the beginning of t and p′S be the cumulative probability
(0)
at the end of t. Moreover, let pS denote the cumulative probability of the nodes
(1)
w ∈ S with no transmitting node in D(w)\S in round t. Similarly, let pS denote
the cumulative probability of the nodes w ∈ S with a single transmitting node
(2)
in D(w) \ S, and let pS be the cumulative probability of the nodes w ∈ S that
experience a blocked round either because they are jammed or at least two nodes
(0)
(1)
(2)
in D(w) \ S are transmitting at t. Certainly, pS = pS + pS + pS . Our goal
is to determine p′S in this case. Let q0 (S) be the probability that all nodes in S
stay silent, q1 (S) be the probability that exactly one node in S is transmitting,
and q2 (S) = 1 − q0 (S) − q1 (S) be the probability that at least two nodes in S
are transmitting.
When ignoring the case that cv > Tv for a node v ∈ S at round t, it holds:
(0)

(1)

(2)

E[p′S ] = q0 (S) · [(1 + γ)pS + (1 + γ)−1 pS + pS ]
(1)

(0)

(2)

+q1 (S) · [(1 + γ)−1 pS + pS + pS ]
(0)

(1)

(2)

+q2 (S) · [pS + pS + pS ]
This is certainly also an upper bound for E[p′S ] if cv > Tv for a node v ∈ S
because pv will never be increased (but possibly decreased) in this case. Now,
consider the event E2 that at least two nodes in S transmit a message. If E2
holds, then E[p′S ] = p′S = pS , so there is no change in the system. On the other
hand, assume that E2 does not hold. Let q0′ (S) = q0 (S)/(1 − q2 (S)) and q1′ (S) =
q1 (S)/(1 − q2 (S)) be the probabilities q0 (S) and q1 (S) under the condition of
¬E2 . Then we distinguish between three cases.
(0)
Case 1: pS = pS . Then
E[p′S ] ≤ q0′ (S) · (1 + γ)pS + q1′ (S) · (1 + γ)−1 pS
= ((1 + γ)q0′ (S) + (1 + γ)−1 q1′ (S))pS .
From Lemma 1 we know that q0 (S) ≤ q1 (S)/pS , so q0′ (S) ≤ q1′ (S)/pS . If pS ≥
ρgreen , then q0′ (S) ≤ q1′ (S)/5. Hence,
E[p′S ] ≤ ((1 + γ)/6 + (1 + γ)−1 5/6)pS ≤ (1 + γ)−1/2 pS
since γ = o(1). On the other hand, p′S ≤ (1 + γ)pS in any case.
(1)

Case 2: pS = pS . Then
E[p′S ] ≤ q0′ (S) · (1 + γ)−1 pS + q1′ (S)pS
= (q0′ (S)/(1 + γ) + (1 − q0′ (S)))pS = (1 − q0′ (S)γ/(1 + γ))pS .
Now, it holds that 1 − xγ/(1 + γ) ≤ (1 + γ)−x/2 for all x ∈ [0, 1] because from
the Taylor series of ex and ln(1 + x) it follows that
(1 + γ)−x/2 ≥ 1 − (x ln(1 + γ))/2 ≥ 1 − (x(1 − γ/2)γ)/2

and
1 − xγ/(1 + γ) ≤ 1 − (x(1 − γ/2)γ)/2
for all x, γ ∈ [0, 1] as is easy to check. Therefore, when defining ϕ = q0′ (S), we
get E[p′S ] ≤ (1 + γ)−ϕ/2 pS . On the other hand, p′S ≤ pS ≤ (1 + γ)ϕ pS .
(2)

Case 3: pS = pS . Then for ϕ = 0, E[p′S ] ≤ pS = (1 + γ)−ϕ/2 pS and p′S ≤ pS =
(1 + γ)ϕ pS .
(0)

(1)

(2)

Combining the three cases and taking into account that pS +pS +pS = pS ,
we obtain the following result.
(0)

(1)

(2)

Lemma 10. There is a φ ∈ [0, 1] (depending on pS , pS and pS ) so that
E[p′S ] ≤ (1 + γ)−φ pS

and

p′S ≤ (1 + γ)2φ pS .

(1)

Proof. Let a = (1 + γ)1/2 , b = (1 + γ)ϕ/2 for the ϕ defined in Case 2, and
(2)
(1)
(0)
c = 1. Furthermore, let x0 = pS /pS , x1 = pS /pS and x2 = pS /pS . Define
φ = − log1+γ ((1/a)x0 + (1/b)x1 + (1/c)x2 ). Then we have
(0)

(1)

(2)

E[p′S ] ≤ (1 + γ)−1/2 pS + (1 + γ)−ϕ/2 pS + pS = (1 + γ)−φ pS .
(0)

We need to show that for this φ, also p′S ≤ (1 + γ)2φ pS . As p′S ≤ (1 + γ)pS +
(2)
(1)
(1 + γ)ϕ pS + pS , this is true if
a2 x0 + b2 x1 + c2 x2 ≤

1
((1/a)x0 + (1/b)x1 + (1/c)x2 )2

or
((1/a)x0 + (1/b)x1 + (1/c)x2 )2 (a2 x0 + b2 x1 + c2 x2 ) ≤ 1

(2)

To prove this, we need two claims whose proofs are tedious but follow from
standard math.
Claim. For any a, b, c > 0 and any x0 , x1 , x2 > 0 with x0 + x1 + x2 = 1,
(ax0 + bx1 + cx2 )2 ≤ (a2 x0 + b2 x1 + c2 x2 )
Claim. For any a, b, c > 0 and any x0 , x1 , x2 > 0 with x0 + x1 + x2 = 1,
((1/a)x0 + (1/b)x1 + (1/c)x2 )(ax0 + bx1 + cx2 ) ≤ 1
Combining the claims, Equation 2 follows, which completes the proof.

⊓
⊔

Hence, for any outcome of E2 , E[p′S ] ≤ (1 + γ)−ϕ pS and p′S ≤ (1 + γ)2ϕ pS for
some ϕ ∈ [0, 1]. If we define qS = log1+γ pS , then it holds that E[qS′ ] ≤ qS − ϕ.
For any time t in I, let qt be equal to qS at time t and ϕt be defined as ϕ
at time t. Our calculations above imply that as long as pS ∈ [ρgreen , ρyellow ],
E[qt+1 ] ≤ qt − ϕt and qt+1 ≤ qt + 2ϕt .

Now, suppose that within subframe I we reach a point t when pS > ρyellow .
Since we start with pS ≤ ρgreen , there must be a time interval I ′ ⊆ I so that
right before I ′ , pS ≤ ρgreen , during I ′ we always have ρgreen < pS ≤ ρyellow ,
and at the end of I ′ , pS > ρyellow . We want to bound the probability for this to
happen.
Consider some fixed interval I ′ with the properties above, i.e., with pS ≤
ρgreen right before I ′ and pS ≥ ρgreen at the first round of I ′ , so initially,
pS ∈ [ρgreen , (1 + γ)ρgreen ]. We use martingale theory to bound the probability
that in this case, the properties defined above for I ′ hold. Consider the rounds
in I ′ to be numbered from 1 to |I ′ |, let qt and ϕt be defined as above, and let
Pt−1
qt′ = qt + i=1 ϕi . It holds that
′
E[qt+1
] = E[qt+1 +

t
X
i=1

ϕi ] = E[qt+1 ] +

t
X

t
X

ϕi ≤ qt − ϕt +

i=1

i=1

ϕi = qt +

t−1
X

ϕi = qt′ .

i=1

Moreover, it follows from Inequality (1) that for any round t, p′S ≤ (1 + γ)2ϕt pS .
′
Therefore, qt+1 ≤ qt +2ϕt , which implies that qt+1
≤ qt′ +ϕt . Hence, we can define
a martingale (Xt )t∈I ′ with E[Xt+1 ] = Xt and Xt+1 ≤ Xt + ϕt that stochastically dominates qt′ . Recall that a random variable Yt stochastically dominates a
random variable Zt if for any P
z, P[Yt ≥ z] ≥ P[Zt ≥ z]. In that
P case, it is also
straightforward to show that i Yi stochastically dominates i Zi , which we
will need in the following. Let T = |I ′ |. We will make use of Azuma’s inequality
to bound XT .
Fact 4 (Azuma Inequality) Let X0 , X1 , . . . be a martingale satisfying the
property that Xi ≤ Xi−1 + ci for all i ≥ 1. Then for any δ ≥ 0,
2

PT

2

PT

2

P[XT > X0 + δ] ≤ e−δ /(2 i=1 ci ) .
PT
Thus, for δ = 1/γ + i=1 ϕi it holds in our case that
P[XT > X0 + δ] ≤ e−δ

/(2

This implies that
P[qT′ > q0′ + δ] ≤ e−δ

2

/(2

i=1

PT

i=1

ϕ2i )

ϕ2i )

.

,

for several reasons. First of all, stochastic dominance holds as long as pS ∈
[ρgreen , ρyellow ], and whenever this is violated, we can stop the process as the
requirements on I ′ would be violated, so we would not have to count that probability towards I ′ . Therefore,
P[qT > q0 + 1/γ] ≤ e−δ

2

/(2

PT

i=1

ϕ2i )

.

Notice that qT > q0 + 1/γ is required so that pS > ρyellow at the end of I ′ , so
PT
the probability bound above is exactly what we need. Let ϕ = i=1 ϕi . Since
PT
ϕi ≤ 1 for all i, ϕ ≥ i=1 ϕ2i . Hence,


δ2
(1/γ + ϕ)2
ϕ
1
.
+
≥
≥
PT
2ϕ
2ϕγ 2
2
2 i=1 ϕ2i

This is minimized for 1/(2ϕγ 2 ) = ϕ/2 or equivalently, ϕ = 1/γ. Thus,
P[qT > q0 + 1/γ] ≤ e−1/γ

Since there are at most f2 ways of selecting I ′ ⊆ I, the probability that there
exists an interval I ′ with the properties above is at most
 
1
f −1/γ
e
≤ f 2 e−1/γ ≤
2
logc n
for any constant c if γ = O(1/(log T + log log n)) is small enough.

⊓
⊔

Lemma 11. For any subframe I in F it holds that if there has been at least one
round during the past subframe where pS ≤ ρgreen , then throughout I, pS ≤ ρred
w.m.p.
Proof. Suppose that there has been at least one round during the past subframe
where pS ≤ ρgreen . Then we know from Lemma 9 that w.m.p. pS ≤ ρyellow at
the beginning of I. But if pS ≤ ρyellow at the beginning of I, we also know from
Lemma 9 that w.m.p. pS ≤ ρred throughout I, which proves the lemma.
⊓
⊔
Now, define a subframe I to be good if pS ≤ ρred throughout I, and otherwise
I is called bad. With the help of Lemma 8 and Lemma 11 we can prove the
following lemma.
Lemma 12. For any sector S, at most ǫβ/6 of the subframes I in F are bad
w.h.p., where the constant β > 0 can be made arbitrarily small depending on the
constant α in f .
Proof. From Lemma 8 it follows that for every subframe I in F there is a time
point t ∈ I at which pS ≤ ρgreen w.h.p. Consider now some fixed subframe I in
F that is not the first one and suppose that the previous subframe in F had at
least one round with pS ≤ ρgreen . Then it follows from Lemma 11 that for all
rounds in I, pS ≤ ρred w.m.p. (where the probability only depends on I and its
preceding subframe), i.e., I is good. Hence, it follows from the Chernoff bounds
that at most ǫβ/7 of the odd-numbered as well as the even-numbered subframes
after the first subframe in F are bad w.h.p. (if the constant α is sufficiently
large). This implies that overall at most ǫβ/6 of the subframes in F are bad
w.h.p.
⊓
⊔
From Lemma 12 it follows
Pthat apart from an ǫβ-fraction of the subframes,
all subframes I in F satisfy v∈D(u) pv ∈ O(1) throughout I, which completes
the proof of Lemma 4.
3.3

Limitations of the Jade Protocol

One may ask whether a stronger throughput result than Theorem 1 can be
shown. Ideally, we would like to use the following model. A MAC protocol is

called strongly c-competitive against some (T, 1−ǫ)-bounded adversary if, for any
sufficiently large time interval and any node v, the number of rounds in which v
successfully receives a message is at least a c-fraction of the total number of nonjammed rounds at v. In other words, a strongly c-competitive MAC protocol can
achieve at least a c-fraction of the best possible throughput for every individual
node. Unfortunately, such a protocol seems to be difficult to design. In fact, Jade
is not strongly c-competitive for any constant c > 0, even if the node density is
sufficiently high.
Theorem 5. In general, Jade is not strongly c-competitive for a constant c > 0
if the adversary is allowed to be 2-uniform and ǫ ≤ 1/3.
Proof. Suppose that (at some corner of the UDG) we have a set U of at least 1/p̂
nodes located closely to each other thatPare all within the transmission range
of a node v. Initially, we assume that u∈U pu ≥ 1, pv = p̂ and Tx = 1 for
all nodes x ∈ U ∪ {v}. The time is partitioned into time intervals of size T . In
each such time interval, called T -interval, the (T, 1 − ǫ)-bounded adversary jams
all but the first ǫT rounds at U and all but the last ǫT rounds at v. It follows
directly from Section
2.3 of [2] that if T = Ω((log3 n)/(γ 2 ǫ)), then for every node
p
u ∈ U , Tu ≤ α T log n/ǫ w.h.p. for some sufficiently large constant α. Thus,
Tu ≤ γT /(β log n) w.h.p. for any constant β > 0 if T is sufficiently large. Hence,
between the last non-jammed round at U and the first non-jammed round at v
in a T -interval, the values Tu are increased (and the values pu are decreased) at
least β(log n)/(6γ) times. Thus, at the first non-jammed round at v, it holds for
every u ∈ U that
pu ≤ p̂ · (1 + γ)−β(log n)/(6γ) ≤ p̂ · e−(β/6) log n ≤ 1/nβ/6
P
and, therefore, u∈U pu = O(1/n2 ) if β ≥ 18. This cumulative probability will
stay that low during all of v’s non-jammed rounds as during these rounds the
nodes in U are jammed. Hence, the probability that v receives any message
during its non-jammed rounds of a T -interval is O(1/n2 ), so Jade is not ccompetitive for v for any constant c > 0.
⊓
⊔
Also, in our original model, Jade is not constant competitive if the node
density is too low.
Theorem 6. In general, Jade is not c-competitive for a constant c independent
of ǫ if there are nodes u with |D(u)| = o(1/ǫ) and the adversary is allowed to be
2-uniform.
Proof. Suppose that we have a set U of k = o(1/ǫ) nodes located closely to
each other that are all within the transmission range of a node v. Let T =
Ω((log3 n)/(γ 2 ǫ)). In each T -interval, the adversary never jams v but jams all
but the first ǫT rounds at U . Then Section 2.3 of [2] implies that for every node
u ∈ U , Tu ≤ γT /(β log n) w.h.p. for any constant β > 0 if T is sufficiently large.
The nodes in U continuously increase their Tu -values and thereby reduce their pu
values during their jammed time steps. Hence, the nodes in U ∪ {v} will receive

1
Uniform distribution
Gauss distribution

0.9
0.8

Throughput

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
200

400

600

800

1000

1200

1400

1600

1800

Network Size

Fig. 1. Top: Throughput as a function of network size. Bottom left: Convergence
behavior for multi-hop networks (uniform distribution). For the plot, we used
n = 500. Note that the start-up phase where the sending probabilities are high is
short (no more than 50 rounds). Bottom right: Convergence of Tv for multi-hop
networks (uniform distribution). For the plot, we used n = 500.

at most ǫT · |U | + (ǫT + O(T / log n)) = ǫT · o(1/ǫ) + (ǫ + o(1))T = (ǫ + o(1))T
messages in each T -interval on expectation whereas the sum of non-jammed
rounds over all nodes is more than T .
⊓
⊔
This implies Theorem 2. Hence, Theorem 1 is essentially the best one can
show for Jade (within our notation).
3.4

Simulations

In order to complement our theoretical insights, we conducted some experiments.
First, we present our throughput results for a sufficiently large time interval, and
then we discuss the convergence behavior. For our simulations, as in our formal
analysis, we assume that initially all nodes v ∈ V have a high sending probability
of pv = p̂ = 1/24. The nodes are distributed at random over a square plane of
4 × 4 units, and are connected in a unit disk graph manner (multi-hop). We
simulate the jamming activity in the following way: for each round, a node is
jammed independently with probability (1 − ǫ). We run the simulation for a
sufficiently large number of time steps indicated by the Theorem 1, i.e., for
([T + (log3 n)/(γ 2 ǫ)] · (log n)/ǫ rounds, where ǫ = 0.3, T = 200, and γ = 0.1.
Figure 1 (top) shows the throughput competitiveness of Jade for a scenario where different numbers of nodes are distributed uniformly at random

over the plane and a scenario where the nodes are distributed according to a
normal/Gaussian distribution N (0, 1). In both cases, the throughput is larger
when the density is higher. This corresponds to our formal insight that a constant competitive throughput is possible only if the node density exceeds a certain threshold. For example, this holds in case there are 60 nodes in the 4 × 4
plane (density of 3.75), as there are at least 3.75π ≈ 12 > 2/ǫ ≈ 7 uniformly
distributed nodes in one unit disk. As can be seen in the figure, when the number
of nodes is larger than 60, the throughput falls in a range between 20% and 35%.
Convergence time is the second most important evaluation criterion. We
found that already after a short time, a constant throughput is achieved; in
particular, the total sending probability per unit disk approaches a constant
value quickly. This is due to the nodes’ ability to adapt their sending probabilities fast, see Figure 1 (bottom left ). The figure also illustrates the high correlation
between success ratio and aggregated sending probability.
Finally, we have also studied the average of the Tv values over time. While
initially, due to the high sending probabilities, the Tv intervals are large (up to
around 5 if n = 500), they decline quickly, similarly to our observations made
in the previous plots. The average of Tv values stabilize in an interval [2, 4], as
shown in Figure 1 (bottom right ).

4

Conclusion

This paper has presented the first jamming-resistant MAC protocol with provably good performance in multi-hop networks exposed to an adaptive adversary.
While we have focused on unit disk graphs, we believe that our stochastic analysis is also useful for more realistic wireless network models. Moreover, although
our analysis is involved, our protocol is rather simple.

References
1. G. Alnifie and R. Simon. A multi-channel defense against jamming attacks in
wireless sensor networks. In Proc. of Q2SWinet ’07, pages 95–104, 2007.
2. B. Awerbuch, A. Richa, and C. Scheideler. A jamming-resistant MAC protocol for
single-hop wireless networks. In Proc. of PODC ’08, 2008.
3. E. Bayraktaroglu, C. King, X. Liu, G. Noubir, R. Rajaraman, and B. Thapa. On
the performance of IEEE 802.11 under jamming. In Proc. of IEEE Infocom ’08,
pages 1265–1273, 2008.
4. M. A. Bender, M. Farach-Colton, S. He, B. C. Kuszmaul, and C. E. Leiserson.
Adversarial contention resolution for simple channels. In Proc. of SPAA ’05, 2005.
5. T. Brown, J. James, and A. Sethi. Jamming and sensing of encrypted wireless ad
hoc networks. In Proc. of MobiHoc ’06, pages 120–130, 2006.
6. J. Chiang and Y.-C. Hu. Cross-layer jamming detection and mitigation in wireless
broadcast networks. In Proc. of MobiCom ’07, pages 346–349, 2007.
7. B. S. Chlebus, D. R. Kowalski, and M. A. Rokicki. Adversarial queuing on the
multiple-access channel. In Proc. of PODC ’06, 2006.

8. A. Czumaj and W. Rytter. Broadcasting algorithms in radio networks with unknown topology. Journal of Algorithms, 60(2):115 – 143, 2006.
9. S. Dolev, S. Gilbert, R. Guerraoui, D. Kowalski, C. Newport, F. Kuhn, and
N. Lynch. Reliable distributed computing on unreliable radio channels. In Proc.
2009 MobiHoc S3 Workshop, 2009.
10. S. Dolev, S. Gilbert, R. Guerraoui, F. Kuhn, and C. C. Newport. The wireless
synchronization problem. In Proc. 28th Annual ACM Symposium on Principles of
Distributed Computing (PODC), pages 190–199, 2009.
11. S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Gossiping in a multi-channel
radio network: An oblivious approach to coping with malicious interference. In
Proc. of the Symposium on Distributed Computing (DISC), 2007.
12. S. Dolev, S. Gilbert, R. Guerraoui, and C. Newport. Secure communication over
radio channels. In Proc. 27th ACM Symposium on Principles of Distributed Computing (PODC), pages 105–114, 2008.
13. S. Dolev, S. Gilbert, R. Guerraoui, and C. C. Newport. Gossiping in a multi-channel
radio network. In Proc. 21st International Symposium on Distributed Computing
(DISC), pages 208–222, 2007.
14. S. Gilbert, R. Guerraoui, D. Kowalski, and C. Newport. Interference-resilient information exchange. In Proc. of the 28th Conference on Computer Communications.
IEEE Infocom 2009., 2009.
15. S. Gilbert, R. Guerraoui, D. R. Kowalski, and C. C. Newport. Interference-resilient
information exchange. In Proc. 28th IEEE International Conference on Computer
Communications (INFOCOM), pages 2249–2257, 2009.
16. S. Gilbert, R. Guerraoui, and C. Newport. Of malicious motes and suspicious
sensors: On the efficiency of malicious interference in wireless networks. In Proc.
of OPODIS ’06, 2006.
17. L. A. Goldberg, P. D. Mackenzie, M. Paterson, and A. Srinivasan. Contention
resolution with constant expected delay. J. ACM, 47(6), 2000.
18. J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for mulitiple
accesschannels. SIAM Journal on Computing, 25(4), 1996.
19. M. Heusse, F. Rousseau, R. Guillier, and A. Duda. Idle sense: An optimal access
method for high throughput and fairness in rate diverse wireless lans. In Proc.
SIGCOMM, 2005.
20. IEEE. Medium access control (MAC) and physical specifications. In IEEE
P802.11/D10, 1999.
21. K. Jain, J. Padhye, V. N. Padmanabhan, and L. Qiu. Impact of interference
on multi-hop wireless network performance. In Proc. 9th Annual International
Conference on Mobile Computing and Networking (MobiCom), pages 66–80, 2003.
22. S. Jiang and Y. Xue. Providing survivability against jamming attack via joint
dynamic routing and channel assigment. In Proc. 7th Workshop on Design of
Reliable Communication Networks (DRCN), 2009.
23. C. Koo, V. Bhandari, J. Katz, and N. Vaidya. Reliable broadcast in radio networks:
The bounded collision case. In Proc. of PODC ’06, 2006.
24. F. Kuhn, T. Moscibroda, and R. Wattenhofer. Radio network clustering from
scratch. In Proc. of ESA ’04, 2004.
25. B.-J. Kwak, N.-O. Song, and L. E. Miller. Performance analysis of exponential
backoff. IEEE/ACM Transactions on Networking, 13(2):343–355, 2005.
26. Y. Law, L. van Hoesel, J. Doumen, P. Hartel, and P. Havinga. Energy-efficient
link-layer jamming attacks against wireless sensor network mac protocols. In Proc.
of SASN ’05, pages 76–88, 2005.

27. M. Li, I. Koutsopoulos, and R. Poovendran. Optimal jamming attacks and network
defense policies in wireless sensor networks. In Proc. of Infocom ’07, pages 1307–
1315, 2007.
28. X. Liu, G. Noubir, R. Sundaram, and S. Tan. Spread: Foiling smart jammers using
multi-layer agility. In Proc. of Infocom ’07, pages 2536–2540, 2007.
29. D. Meier, Y. A. Pignolet, S. Schmid, and R. Wattenhofer. Speed dating despite
jammers. In Proc. DCOSS ’09, June 2009.
30. V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein. Using channel hopping
to increase 802.11 resilience to jamming attacks. In Proc. of Infocom ’07, pages
2526–2530, 2007.
31. R. Negi and A. Perrig. Jamming analysis of MAC protocols. Technical report,
Carnegie Mellon University, 2003.
32. G. Noubir. On connectivity in ad hoc networks under jamming using directional
antennas and mobility. In Proc. 2nd International Conference on Wired/Wireless
Internet Communications (WWIC), pages 186–200, 2004.
33. A. Pelc and D. Peleg. Feasibility and complexity of broadcasting with random
transmission failures. In Proc. of PODC ’05, 2005.
34. P. Raghavan and E. Upfal. Stochastic contention resolution with short delays.
SIAM Journal on Computing, 28(2):709–719, 1999.
35. J. Schmidt, A. Siegel, and A. Srinivasan. Chernoff-Hoeffding bounds for applications with limited independence. SIAM Journal on Discrete Mathematics,
8(2):223–250, 1995.
36. M. K. Simon, J. K. Omura, R. A. Schultz, and B. K. Levin. Spread Spectrum
Communications Handbook. McGraw-Hill, 2001.
37. D. Thuente and M. Acharya. Intelligent jamming in wireless networks with applications to 802.11b and other networks. In Proc. of MILCOM ’06, 2006.
38. A. Wood, J. Stankovic, and G. Zhou. DEEJAM: Defeating energy-efficient jamming in IEEE 802.15.4-based wireless networks. In Proc. of SECON ’07, 2007.
39. W. Xu, K. Ma, W. Trappe, and Y. Zhang. Jamming sensor networks: attack and
defense strategies. IEEE Network, 20(3):41–47, 2006.
40. W. Xu, W. Trappe, Y. Zhang, and T. Wood. The feasibility of launching and
detecting jamming attacks in wireless networks. In Proc. of MobiHoc ’05, pages
46–57, 2005.
41. W. Xu, T. Wood, and Y. Zhang. Channel surfing and spatial retreats: defenses
against wireless denial of service. In Proc. of Workshop on Wireless Security, 2004.
42. S. Ye, Y. Wang, and Y. Tseng. A jamming-based MAC protocol for wireless
multihop ad hoc networks. In Proc. IEEE 58th Vehicular Technology Conference,
2003.
43. S.-R. Ye, Y.-C. Wang, and Y.-C. Tseng. A jamming-based MAC protocol to improve the performance of wireless multihop ad-hoc networks. Wirel. Commun.
Mob. Comput., 4(1):75–84, 2004.
44. J. Zander. Jamming in slotted ALOHA multihp packed radio networks. IEEE
Transactions on Networking, 39(10):1525–1531, 1991.

RandomizedProtocolsfor Low-CongestionCircuit Routingin Multistage
InterconnectionNetworks
Richard Cole*

Bruce M. Maggst

Andrea W. Richat

Friedhelm Meyer auf der HeideS

Klaus Schrijderq

amesh K. Sitaramanll

Berthold V&king*

switching, store-and-forwardrouting, and wormhole routing. With
circuit switching, eachmessagemust first lock down (i.e., reserve)
a path (i.e., circuit) in the network from its input node to its output
node. The path is then used to transmit the messagethrough the
network. In contrast, with store-and-forwardrouting and wormhole routing pathsarenot reservedbefore transmission,
Circuit-switching has enjoyed widespreadpopularity since its
early use in telephony and subsequentlyin the design of parallel
computers. Recent trends in network design emphasizethe need
for providing quality of service (QoS) guaranteesfor communication. To provide guaranteesas opposedto just best-effort service,
network resourcesmustbe reservedbefore communicationbegins.
Consequently,severalmodemhigh-speedmultimedia switchesand
ATMs reservea (virtual) circuit through the network for eachcommunication request[37,38].

Abstract

In tbis paperwe study randomizedalgorithms for circuit switching
on multistage networks related to the butterfly. We devise algorithms that route messagesby constructing circuits (or paths) for
the messageswith small congestion,dilation, and setuptime. Our
algorithms are basedon the idea of having eachmessagechoosea
route from two possibilities, a techniquethat haspreviously proven
successfulin simpler load balancing settings. As an application of
our techniques,we proposea novel design for a dataserver.
1

Michael Mitzenmacher§

Introduction

In this paper,we devisealgorithms for routing messagesin circuitswitching networkswhereeachmessagechoosesfrom two possible
routes,an ideathat hasbeenapplied with greatsuccessin otherload
balancing situations [12,17,26,27].
Underlying every parallel computeris a nehvork that delivers
messagesbetweenprocessorsor betweenprocessorsand memory
modules. Similar networks are found in the switches that route
telephone calls and intemet traffic. ‘&pically, a messageis sent
from its input node (source) to its output node (destination)via a
path in the network. Methods for routing messagesinclude circuit-

1.1

l CourantInstitute, New York University, New York, NY 10012(emaik
coleQcs.nyu.edu).Suppoxtedby NSF grant CCR-9503309.
tSchoo1of ComputerScience,CarnegieMellon University,Pittsburgh,
PA 15213(email: bmm,aricha@cs.cmu.edu).Supportedin part by the Air
Forcehfateciel Command(AFhIC) andARPAunderContractF196828-93C-0193. by ARPA Contract NOOOW95-l-1246,and by an NSF National
Young Irkestigator Award, No. CCR-94-57766.with matching fundsprovided bv NEC Resach Institute and Suu Microsystems. The viws and
conclusionscuntaiml here are those of the autbofi and should not be interpretedas necessarilyrepresentingthe official policies or endorsements,
either expressor implied, of AFhfC, ARPA, ChIU. or the U.S. Government.
&paxtment of Mathematics and Computer Science, and Heinz Niidorf Institute, University of Paderbom,33095Paderbom.Germauy(emaik
fmadb.voe&ing@uni-piderbom.de). Supportedin part by DFG-S&derforschunnsbereich376 andbv EU ESPRIT Long Term ResearchPro&? 20244

Circuit routing algorithms

and their performance

In a circuit-switched network, a messagearrives requestinga path
from its sourceto its destination. A routing algoritlun determines
which of manypossiblepathsis lockeddown for eachmessage,WC
measurethe performanceof a routing algorithm in terms of three
parameters:congestion,dilation, and setuptime.
Congestionanddilation arepropertiesof the pathslocked down
for the messagesby the routing algorithm. The congcstiorrof a set
of paths is defined to be the maximum number of paths that pass
through any link in the network. Congestion is a measureof the
maximum numberof pathsthat must be simultaneously supported
by a link of the network, and hencedeterminesthe bandwidth that
a link should possess.The dilarion of a set of paths is defined to
be the maximum Iength of a path in the set. Dilation is n melure
of maximumdistance(in links) that a messagemust tmvel to reach
its destination.Finally, the seruprime is the time takenby the routing algorithm to allocate paths through the network. This is the
time overheadinvolved in path selectionbefore the actual message
transmissionsbegin.
The goal of this paperis to deviserouting algorithms with small
congestion,dilation, and setuptime.
1.2

Network and problem definitions

The resultsin this paperapply to variantsof a popular type of multlstageinterconnectionnetwork called the 6ulterfIy network. Buttcrfly networksandits variantshavebeenwidely usedfor packetrouting in a number of commercialand experimentalnetworks [7, 15,
28,291. More recently, severalproposeddesignsfor the switching
fabric of scalablehigh-speedATM networks use the butterfly and
its variants for routing virtual circuits [37,38].
We define an n-input butte& network B, as follows, An ninput butterfly hasn(log n + 1) nodesarrangedin log n + 1 lcvcls
of n nodeseach.’ An example of an n-input butterfly (n = 8)
with depth log n (log n = 3) is show in Figure 1. Each node has a
distinct label (2u,i) where i is the level of the node (0 s i 4 log n)
‘Throughout this paper we use log rato denoteloga n.

378

of future messagearrivals. We assumethat at any time, the messagesbeing routed form a partial permutation; that is, each input
and output nodecorrespondto at most one routed message.

level
0

I

2

3

000
001
010
011

1.3

IO W
100

101
110
Ill
Figure 1: An d-input butterfly network.
nnd w = uJleu2, . . zut,,sR is a log n-bit binary numberthat denotes
the row of the node, All nodesof the form (w, i), 0 5 i C logn,
arc said to belong to row 2~. ‘Iwo nodes (w,i) and (w’;i’) are
linked by an edgeif i’ = i I- 1 and either zuand w’ areidentical or
w nnd w’ differ only in the bit in position i’. (The bit positions are
numbered1 through log n.) We call the first type of edgea straight
ed,qeand the seconda crossedge. The nodeson level 0 are called
the btprrlsof the network, and the nodeson level log it are called
the orrtprrls, Sometimesthe level 0 node in each row is identified
with the level log n nodein the samerow, In this case,the butterfly
B, la said to wrap around.
WCdellne a randomly-wired buttefly RB, as follows. Network RB,, has the sameset of nodes and edgesas Bn, except
that lhe crossedgesincident on the input nodesof RB,, are permuted randomly according to the following rule. Let d = logn
Bach node (WI , , , wd, 0) of RB,, is connectedby a crossedgeto
nqdc (wi , , , wi, 1) if and only if wr # w: and ~,,r (wz . . . wd) =
w2 , , , ul;, where 00 and or are randompermutationsof the set of
(lo&n - 1).bit numbers.
WCdefine a two-fold bttttetjly B& as follows. Network BB,,
consistsof two copies of B,, placed one after the other such that
cnch output nodein the ilrst copy is identified with the corresponding input nodeof the secondcopy with the samerow number.Note
that BB,, is a multistagenetwork with n rows and 2logn+ 1 levels, The nodesin level 0 arecalled the inputs of BB,, andthe nodes
in level 2 log a arc called the outputsof BB,. Also, observethat a
routing algorithm on BB,, can be simulatedby making two passes
through a butterfly B, that wraps around.
It is important to contrastthe BB, network with anothercommon variant of the butterfiy, the BeneSnehvork.An n-node Be&
network consistsof two copiesof B, placed“back-to-back” such
that eachoutput node of the first copy is identified with the correspondingoutput node of the secondcopy.
In this paper,we study n canonical circuit routing problemthat
ia known as the permutation routing problem. In a permutation
rorrlhg
problerrr nt most one messageoriginates at each input of
the network and nt mostone messageis destinedfor eachoutput of
the network.
WCdistinguish two kinds of permutationrouting problems:static
and dynamic, In n sruric probletn, all the messagesthat constitute
R permutation routing problem are present at time 0, before the
routing begins, The routing algorithm constructspaths for all the
messagesin a “batch” mode. All the messagesare delivered to
their respectivedestinationsbeforethe routing of the next batch of
messagesbegins. In contrast,in a dynamicproblem, messagesare
injectedor deletedone by one. The routing algorithm routesa path
for eachinjected messagein an on-line fashion with no knowledge

379

Previous work

There are severaldifferent sub-areasof researchthat relate to our
work We provide a summaryof the mostrelevant.
Routing in Butterfly Networks. There is a vast literature on routing in butterfly networks [20,21]. Much of the early work focuses
on store-and-forwardrouting [I, 23,24,31,35,39,41,42]. More
recently, there has been progressin analyzing wormhole routing
algorithms [IO, 11, 13,361. Since we presentno new results in
thesetwo routing methods,we focus only on the butterfly circuitswitching literature.
In two early papers,Beizer [S] and Ben& [9] showedthat any
static permutationrouting problem can be routed with congestion
1 and dilation 2logn on an n-input BeneSnetwork. Subsequently,
Waksman1431provided an elegantalgorithm that takesO(n log n)
time to determineall the paths, but requires global knowledge of
the sourceand destinationof all the messages.Later, Nassimi and
Sahni [30] showedhow to implementWaksman’salgorithm in parallel on the Benti and related networks in time O(log4 n). However,their algorithm is complex and requiresthe BeneSnetwork to
emulatea completenetwork by executing a seriesof sorting routines.
Although the Ben& network and the BB,, are closely related
in structure,whether or not it is possible to route an arbitrary permutation routing problem in an offline fashion with congestion 1
on the BB, is a long-standingopen problem.
In this paper,we deviserouting algorithms that minimize congestion. A complementaryapproachaimsto maximize throughput.
Previouswork hasstudiedthe model whereeachlink cansupportat
most q paths,and the goal is to maximize the number of messages
that lock down paths.Kruskal and Snir [I91 showedthat if eachinput in a butterfly network B,, sendsa messageto a randomly chosen
output, and at most one messagecan use any edgeof the network
(i.e., q = l), then the expectednumberof messagesthat succeedin
locking down pathsto their destinationsis @(n/log n). Koch [IS]
generalizedthe result of Kruskal and Snir by showing that if each
edge can support q messages,q I 1, then the expectedfraction
of messagesthat succeedin locking down pathsis Q(n/ log’/9 n).
Maggs and Sitaraman[24] generalizedthe previous two results by
showing that, by making two passesthrough a butterfly, it is possible to route an n(n/ log’/9 n) fraction of any permutation (rather
than only a randompermutation),with high probability.
Use of Randomness. An early example of the use of randomization for circuit-switching in butterfly networks is the work of
Valiant [41,42]. Valiant showedthat any permutationrouting problem can be transformedinto two randomproblemsby first routing
a path for eachmessageto a randomintermediatedestination, and
then on to its true destination.This implies that we can route paths
for a (static or dynamic) permutation routing problem on a twofold butterfly BB,, with congestionQ(log n/ log log n), and dilation 2logn. Note that the paths for each messagecan be set up
independentlywithout completeknowledge of the permutation in
O(log n) time. We show how to userandomizationto route permutations with substantiallysmaller congestionand the samedilation.
Ranade[34] observedthat a smaller amount of randomnessis
sufficient to implement Valiant’s algorithm. Note that eachswitch
has two input links and two output links. Ranadenoted that it is
sufficient that eachswitch in the first log n levels of BB,, shuntsa
messagefrom each input link to a random (and distinct) outgoing
link. Thus, messagesare sent to randombut not independentdestinations using one randombit per switch. The first logn levels of
such a BBn constitute ajIip network. A flip network was subse-

~.,_ --__-

.-

quently used in [24] in the context of circuit routing. We use flip
networksin our routing algorithms in Section2.
Randomnesscan be used in constructing the network itself.
The use of randomnessto design multistage networks datesback
to Ikeno[lB], and Bassalygo and Pinsker [5]. Networks such as
the randomly-wired multibutterfly are known to have good routing
and fault toleranceproperties [3O,221. Recentresults provide algorithms for routing circuits for any permutationrouting problem
with congestion1 in multibutterfly and multi-Bend networkswith
set-uptime O(log n) [2,32]. Unlike thesenetworks,our resultsin
Section2 apply to commonly-usednetworkslike Bn and BE& that
require neither randomwiring nor expanders.
Balls-and-bins problem. Our approachto circuit routing is influencedby recent advancesin the classical balls-and-binsproblem.
It is well known that if n balls are tossedrandomly into n bin:., the
mclsimumnumber of balls in any bin will be @(logn/ log log n)
with high probability. Azar et al [4] consider the following dynamic protocol for throwing n balls into n bins: for eachball pick
two bins independentlyanduniformly at random,and placethe ball
in the bin with the smallerload at the time of placement.They show
tlyatafter all balls are placedin bins, the maximum load of any bin
is @(log log n), with high probability.
Static protocolsfor the balls-and-binsproblemwere developed
in 1171,[12], and [27] and appliedto PRAh1simulations.They consider variantsof the following process.Initially, eachball chooses
two randombins. In a round, eachball not yet allocated accesses
its two bins. Each bin with at most c accessingballs acceplsall
of them. The other balls try again in the next round. This protocol guaranteesmasimum load c Even for constantc, the protocol
allocatesall balls, with high probability, using only O(log log n)
rounds.
We apply similar “two-choice” algorithms to circuit routing.
Note that this is a more complex situation. Thinking of eachmessageas a ball and eachnetwork edgeas a bin, we seethat finding
a path for each messagecorrespondsto placing each ball in several dependentbins. Thesedependenciessubstantially incre%ethe
difficulty of the analysis.
Circuit routing in general topology networks. Dynamic circuitswitching has been estensively studied in an on-line competitive
framework for arbitrary nehvork topologies. (See [33] for a survey). Resultsare known for minimizing congestion[3] and for the
mmimizing throughput [ 141.This frameworkcanincorporatemore
generalparameterssuch as the circuit bandwidth and circuit holding time. However, these results do not yield routing algorithms
with congestionsmaller than Q(log n) for the regularly-structured
mu&stage nehvorksthat are the focus of this paper.

man’salgorithm, which achievescongestion1 on a Benegnetwork,
we requiresubstantiallysmallersetuptime. Furthermore,we do not
require complete knowledge about the permutation being routed
and our routing algorithm can be implementedon the network itself. Comparingour result to the algorithm of Nassimi and Sahni
[303,our algorithm is much simpler and faster,although their algorithm achievessmaller congestion.
Dynamic Permutation Routing. In Section 2.2, we analyze the
minimum algorithm for routing any dynamic permutation routing
problem on network BB,. The congestionis O(loglogn) with
high probability, the dilation is 2 log n, and the setuptime for each
new messageis O(logn). Prior to this work, every known algorithm for the dynamic permutationrouting problem on the butterfly
and relatednetworksrequired R(log n/ log log n) congestion.Our
algorithm is optimal in that any routing algorithm on BBn that
considersonly a constantnumber of alternate paths per message
must incur R(log log n) congestion[4].
Data Server Architecture. As an application of our techniques,
in Section 3, we presenta proposal for the architecture of a data
server. The data serverutilizes network RB,, to connect n users
to n disks. Each user is associatedwith a distinct input node and
eachdisk is associatedwith a distinct output node of RB,,. Objects
(typically large, e.g. movies) aredistributed amongthe disks.
A canonicaltaskperformedby the dataserveris the following.
Given n requeststo objects,one per user, theserequestsmust be
satisfiedby providing a path from eachuser to a disk that contains;
their requestedobject. The congestionof the paths must be minimized. Besidescongestion,another important performancemetric is disk contention, which is often a bottleneck. We define disk
contention to be the maximum number of simultaneousrequests
that any disk must satisfy. In Section 3, we devise algorithms that
achieveboth small congestionand small disk contention.
The standardtechniqueof storing the objectsby independently
and randomly distributing them to the n disks yields congestion
and disk contention 8(logn/loglogn),
with high probability. To
achievelower congestionand disk contention, we storetwo copies
of the sameobject on two disks.
2
2.1

Routing in the two-fold

butterfly

Static routing in BB,,

We describea simple, efficient algorithm for routing permutations
on the two-fold butterfly BBn. Recall that the two-fold butterfly BB, has n inputs at level 0 and n outputs at level 24 where
d = log n. Given a permutation?r,our routing algorithm connects
eachinput nodei to the correspondingoutput node n(i); eachpair
(i, n(i)) of input andoutput nodesis called a reqrcest.Our random1.4 Our results
ized algorithm routes pathssuch that the maximum congestionon
an edgeis Q(log log n/ log log log n), with high probability. FurWe introduce two new protocols for circuit-routing: the collision
thermore,the time requiredby the algorithm to set up all the paths
protocol and the minimum protocol. In contrastto Valiant’s algois at most 0(log n log log n/ log log log n), with high probability.
rithm, which picks one random path for each message,theseproThe c-collision algorithm. We usethe collision protocol described
tocols choosenvo random(but not independent)pathsp and p’ for
below to perform the routing. The c-collision protocol initially
each messagedl. The collision protocol uses a suitably chosen
choosesat randomtwo possiblepathsfor eachrequest.Eventually
threshold c, and allocateseither p or p’ to messageM, provided
one of thesepathswill serveasthe required connection.
the congestionof the allocated path is at most c In contrast,the
The two random pathsfor each requestare chosenas follows.
nuikmzprokxol allocatesto M the path with the smallercongesThenodesonlevelsO,...,d/2-landd$d/2$1,...,2dare
tion. As mentionedpreviously, protocols of this flavor have been
flipped randomly. In particular, each input and output node mnps
utilized and analyzed in simpler settings. We estend thesetechthefirst par/zp of a requestto its stra$ht edgeand its sccmd pafh
niques to circuit-routing.
p’ to its cross edgewith probability Z,and with probability 4 the
Static Permutation Routing. In Section2.1, we showthe collision
order is reversed. Similarly, each node on levels 1,. . . , d/2 - I
algorithm routes any permutation on the hvo-fold butterfly BB,
2d - 1 with probability 8 connectsits input
with congestionO(log log n/ log log log n), with high probability,
andd+d/2+1,...,
anddilation 2 log n. The setuptime is O(log n log log n/ log log log n). straight edgewith its output straight edgeand its input crossedge
Our routing algorithm achievesa substantially smaller congestion
with its output crossedge,and with probability 4 the connections
arereversed.Note that theserandomchoicescompletely determine
bound than Valiant’s algorithm. Comparingour result with Waks-

3so

Ihc WO pathsp andp’ of eachrequest,becausethereis exactly
onepathconnectinga nodeon leveld/2 with a nodeon leveld +
d/2 in a BB, network.For a pathp, theotherpathp’ cmnecting
the sameinput and outputnodesis calledthe Mu’y of p. The
randomswitchingensuresthat any edgeon the levels1, . . . , d/2
by atmostoneof therandomlynndd+d/2-t-l,..,, 2d is traversed
gcncrntedpaths. However,eachedgeon the interior levels,i.e.,
onewith “top” nodeon oneof thelevelsd/2 -I- 1,. . . , d + d/2,
ia potentiallytrnversedby severalof thesepaths. We call these
edgescollision edges,andwesaythattwopathsthatcrossthesame
collialonedgecollide.
The c-collisionalgorithmproceedsin roundsto selecta path
for eachrequestas follows. Initially all pathsareactive andnot
sclcc~cd,A pathp is eligible to be selectedif for eachedgee E
p the numberof nctivepathstraversinge is at mostc. If p and
its buddyp’ arc botheligible to be selected,only oneis selected
arbitrarily,A pathp ceasesto beactivein a roundifp is selectedor
thebuddyof p is selectedin thatround.Thealgorithmterminates
whenthereareno moreactivepaths.
Eachroundof the c-collisionalgorithmcanbe implemented
usingn store-and-forward
algorithmasa subroutine:in a firstpass,
for eachactivepath,a packetis sentalongthepathfromlevel0 to
level 2d, During this pass,for eachedge,the numberof packets
trnverslngtheedgeis counted.Then,in a secondpass,all packets
areroutedbackwardalongtheir respectivepathsfromlevel2d to
level 0, During this passthe congestionfor eachactivepathis
computed,Notethat,in thismodel,whencomputingthesetuptime
thepacketsandedgesof thenetworkcanactin parallel,andhence
a roundmaycompletein o(n) time.
The c-collisionalgorithmselectsa pathp in a roundonly if
p collideswith no morethanc - 1 otheractivepathson any of
the edgesin p, This impliesthat any edgethat is includedin at
leastoneselectedpathis includedin at mostc - 1 otherselected
or activepaths,As a consequence,
thecongestionof all selected
pnthrtIs at mostc, Notethatthealgorithmasdescribed
is notguaranteedto terminate,However,in Theorem2.1, we showthatif c
ia ouftlcientlylarge,the algorithmwill terminatewith maximum
congestionat mostc, aftera smallnumberof rounds,with high
probability,In practice,wemayterminatethealgorithmaftersome
fixednumberof rounds:all requeststhatstill havetwo activepaths
at thetcrminntionpointmaychooseonearbitrarily,andin thiscase
we fail to guarantee
congestionc.

Eachnodeu in the witnesstreecorresponds
to a requestwith
two associated
paths,oneof which collideswith oneof the paths
associated
with eachsibling andthe parentof 2)(unlessv is the
root),andtheotherof whichcollideswith oneof thepathsassociatedwith eachof thechildrenof v (unlessv is a leaf). Wecall the
first paththe uppurh of v andthe otherpaththedown purh of v.
Theuppathof therootandthedownpathsof theleavesaredefined
to beemptypaths.Notethatby theterm“collision represented
by
nodev” we meanthecollisionof the downpathof v with theup
pathsof thechildrenof v in thewitnesstree.Finally,to giveeach
treea uniquerepresentation,
weassumethatchildrenof a nodeare
listedin increasingorderfromleft to right basedon theinputnode
numberof thecorresponding
request.
The requestscorresponding
to the nodesof a witnesstreeare
not necessarilypairwisedistinct. Furthermore,the up anddown
pathsof distinctrequestsmayoverlapin therandomly-flipped
levels,sothata randomly-flipped
switchcanbeincludedin morethan
oneof thesepaths. Hence,the collisioneventsrepresented
by a
witnesstreearenot necessarilystochasticallyindependent.Note
that,if theywerestochastically
independent,
it wouldberelatively
straightforward
to arguethetheorem.
Pruning the witnesstree.Theintuitivereasonwhy thedependenciesdonotaffectthefinalconclusionis thatthereareonly O(log n)
nodesin thewitnesstree,hencethedependencies
are“rare”. In orderto handledependencies.
we prunenodesfromthewitnesstree
asnecessary.
This pruningis doneby a traversalthroughthe tree
visiting theinternalnodesin breadth-first-search
orderstartingat
theroot. Whena nodev is visitedduringthistraversal,thedependenciesbetweenthe collisionrepresented
by v andthe collisions
represented
by nodesvisitedbeforev arechecked.If the dependenciessignificantlyaffectour calculations,thenodesbelowv are
pruned,andtheseprunednodesareexcludedfrom thesubsequent
traversal.
Thedetailedpruningrulesfollow. For a nodev visitedduring
thetraversal,let B(v) denotethesetof nodesvisitedbeforev. Furthermore,let I’(v) denotethesetof nodesthat arechildrenof the
nodesin B(v). thatarenotprunedbeforev is visited,andthatare
not in B(v) themselves.For the root r of the witnesstree,B(r)
andJ?(r) areemptysinceour traversalstartsat r. Wedistinguish
two pruningrules:
1. If a pathassociated
with oneof v’s non-prunedchildrentraversesa randomly-ffippcd
switchthat is alsotraversedby a
pathassociated
with a nodeu fromI’(v) thenthec subtrees
Thcorcm 2.1 fir arty constunt e > 0 and c such rhut c! = (1 +
rootedat thechildrenof v areremovedfromthetree,andthe
e) elog n, lhc probability thur the c-collision ulgotithm on BB,,
c subtrees
rootedat thechildrenof u arealsoremovedfrom
lukcs wore than t = Q(log log n/ log log log n) rounds to selecta
the tree. The nodev is calleda pruning node. The nodeu
palhfor every mqaestis at mostn-c’4f1+o(1). Furthcrmorc, each
thatcausedthe pruningis calledthe conflicting node of v.
rortrtdcm be computedin time O(log n), with high probability.
2. If a pathassociated
with oneof v’s non-prunedchildrentraProof, First,weshowthatif thealgorithmdoesnotterminateafter
versesa randomly-flipped
switchthatis alsotraversedby a
t rounds,we canconstructa “witnesstree”. Next,we showhow
path
associated
with
a
node
u fromB(v) thenthec subtrees
the witnesstreecanbe prunedto avoidstochasticdependencies.
rootedat the childrenof v areremovedfrom the tree. The
Flnnlly,weshowby enumeration
thattheprobabilityof occurrence
nodesv andu areagaincalledpruningandconflictingnodes
of n prunedwitnesstreeis at mostn-C/4+1+0(1).
respectively.
Conotructlng a witnesstree.Fix apermutation
n to berouted,and
thesettingsof therandomly-flipped
switchesonthelevels0,. . . , d/2- Whenthereis morethanonechoicefor a conflictingnodefor a
certainpruningnodewe makethe choicearbitrarily,so that each
thetwo pathschosen
1 andd + d/2 -I- 1,. . , , 2d. This determines
pruningnodecanbe associated
with exactlyoneconflictingnode.
for eachrequest,Assumethatthereis a requestwith pathsp and
Furthermore,
thesecondpruningruleis considered
only if theconp’, andneitherpathhasbeenselectedby roundt, wheretheproper
ditionsfor thefirstpruningrulearenot met.
vnlueat’t is to bedeterminedlater.Thenp collideswith at leastc
pathsof’otherrequestsin roundt at someedgee. Let ~1,. . . ,pC
Notethatthepruningrulesensurethat,for everynodev visited
aftertherootr, thesubgraph
inducedby B(v) W(v) is connected;
denotethec pathsthat collidewith p in roundt at e. Theroot of
thewitnesstreeIs therequestcorresponding
top andtherequests thatis, B(v) U I’(v) inducesa subtreeof thefull witnesstreewith
root r. Also, whena nodev is visited,up to 2c subtreesof maxcorresponding
to pl , . . . , pCareits children.Thepathspl, . . . ,pe
nndtheirbuddiespi, . . . ,pL werenotselectedat roundt - 1. Apimumheightt - 2 couldbeprunedfromthetree. Thesesubtrees
plying the argumentrecursivelyto pi:, . . . ,pi we canconstructa
do not includeany nodefrom B(v) u r(v). Hence,the subtree
completec-arytreeof heightt. This treeis calledthewirncsstree.
inducedby thissetonly growsduringthetraversal.

381

We continue the pruning processtill either there are no more
nodesto visit or there are 6 = [c/2] pruning nodes. In the latter
case,we apply a final pruning. If v is the Kth pruning node, we
removefrom the tree all nodesnot included in B(v) U I’(v). This
effectively stopsthe pruning processat the 6th pruning node.
The witnesstreepruned in this fashion is called theprunedwirnessfree. Let m denotethe number of internal nodesin this tree,
and m’ 5 K denotethe number of pruning nodes. Let ~11,. . . , v,
denotethe internal nodesandw , . . . , wmt the pruning nodesin order of visitation, respectively. Furthermore,let ui denotethe conflictin? node of ,wt, for 1 5 i 5 m’. The prunedtree possessesthe
folIowing properties.

number of randomly flipped switches coveredby all paths of I<.
As a consequence,the probability that the c-collision processtakes
morethan t rounds can be boundedby

c

c

TcT

KEXT
\

p(K)
J

*

=: E(T)

.

We aim to give an upper bound on E(T), for a iiscd tree shape
T E 1. E(T) is equal to the expected member of active witness
free configurations with tree shapeT. Note that the tree shapeT

only restricts the number of admissible configurations, that is, it
definesthe setXT, but doesnot influence the probability for a given
configuration K E XT to becomeactive. This probability depends
only on p(K), and, hence,on the overlapping of the paths in the
randomizationIeveIs.
In the following, we utilize Properties2 and 5 that govern how
pathsmay overlap to computeE(T). Instead of summing over all
admissible configurations in XT and multiplying each individual
configuration with its probability, we considerthe nodesof the witnesstree one by one and calculate an upper bound on the expected
number of configurations for each individual node. In particular,
we considerfirst all the internal tree nodesand then all the pruning
nodes;both setsof nodesare consideredin the order of visitation,
Define the configuration of an internal node vi to consistof the
down path of v~iand the up pathsof the children of vi, for 1 S i 5
m. Furthermore,define the conjguration of a pnming node uy to
be the down path of wi and the two pathsbelonging to the colliding
requestOfwit for 1 5 i < m’. A collection of node configurations
is said to be admissible,if they are a subsetof an admissible tree
configuration. Note that a collection of admissible configurations
for all internal and all pruning nodes(in conjunciion with the tree
shape)completely definesthe configuration of the witness tree.
For an internal node vf and a collection K of contiyrations
for the nodes VI , . . . , vi-1, let E&(vi, ZC) denote the espected
number of active configurations for vi under the assumptionthat
the configurations in K are active. Note that ZCalready specifics
the requestassociatedwith vt. (For the root VI we assumethat I\’
specifiesonly this request.)Let Z&n(vi) be the masimumover all
K).
configurationsK of Z&(Vi,

1) Any internal node u representsa collision of the down path
of v and the cup pathsof the children of v.
2) For any internal node v, the pruning ensuresthat the up paths
of the children of v do not sharea randomly-flipped switch
with a path associatedwith a nodein B(v) Ur(v) exceptfor
the down path of v. (As a consequence,all nodesof the tree
correspondto distinct requests.)
3) The down path of a pruning nodev either collides with a path
p that is associatedwiviththe conflicting nodeU, or it collides
with a pathp such that p or its buddy sharesa randomswitch
with a path associatedwith U. This path p is denotedthe
conflicting path of v.
4) The down path of a pruning node wi is not the conflicting
path of a pruning node 201:with b < i. (This can be proved
as foIIows. For contradiction, assumethe opposite. Then
Wi = ok and Wi E I’(wr:). Hence,the subtreebelow Wi is
removedwhen WI:is visited. This meansthat wi hasno nonpruned children when Wi is visited and ConsequentIy,wi is
not a pruning node.)
5) For each pruning node wl, the down path p of wi sharesat
most 5c mndomly flipped switches with up and down paths
associatedwith any other node and conflicting pathsassociated with the pruning nodes WI,. . . ,wi. (This is because,
according to Properties2 and 4. the down path of wi is not
equivalentto any suchup, down, or conflicting path. Furthermore,accordingto Property2, the down path of wi doesnot
sharea random switch with any other up or down path, except for the up and down pathsof the siblings of wi, and the
up pathof w(. With eachof these2c- 1 paths.the down paths
overlaps at most twice in the randomization IeveIs, once in
eachof the butterflies in EBn. The sameholds for the i conflicting pathsassociatedwith 2~1,. . . , Wi. Thus, there are at
most 4c - 2 + 2~. 5 5c overlappingswith thesepathsin the
randomizationlevels.)

Lemma 2.2

EC&(?&)

< log n/c! .

Proof. Webound the expectednumberof active configurations for
vi by choosing the down path p of vi arbitrarily nnd then deriving
an upper bound on the expectednumber of choices of active up
pathsPI , . . . ,pC of the children of vvtthat fulfill Properties1 and 2.
The expectednumber of active down paths p is at most one.
This is because,there are severaldifferent paths in BB,, that connect the two input and output nodeswhich are given by the conliguration K. However,at most two of them are active, and the configuration K determineswhich of them is the up path und which is
the down path of vi.
Given path p, there are d = log n possiblechoices for the collision edgeat which the down path collides with pl, . . , , pC. Let c
denotethis edgeand f?the level of this edge. \V.l.o.g., we assume
thatd/2$-1<15d.
We calculatean upper bound on the expectednumberof active
UPpathsPI , . . . , pCtraversinge and fulfilling Property 2. Property
2 ensuresthat pl , . . . , pC use only unrevealed random switches.
Therefore, we assumefor the following that all switches are unrevealed. Note that this doesnot decreasethe number of admissible configurations, and, hence, not decreasethe expectednumber
of active configurations for pl , . . . , pC. The main problem in cnlculating the number of active configurations for ~1,. . , ,p, is to

Bounding the probability of occurrence of a pruned witness
tree. We bound the probability of occurrenceof a pruned witness
tree via enumeration. Define the tree shape to be a description of
the topoloa of the tree including the pruning and the conflicting
nodes. Define an admissible witness tree configuration to be a tree
shapewith associatedrequests,up and down paths,and conflicting
pathswhich eventually,i.e., for somesetting of the randomswitching, matchesto a prunedwitness tree. In particular, any admissible
witness tree configuration hasto fulfill the 5 propertiesabove.
Let 7 denotethe setof tree shapescorrespondingto at leastone
admissiblewitness tree configuration, and let XT denotethe set of
all admissiblewitness tree configurations with tree shape2’ E 7.
An admissibleconfiguration K is said to be active if the outcome
of the randomswitching correspondsto all pathsof the configuration. Hence,eachadmissibleconfiguration K has a probability to
becomeactive, which is just 2-pcK) with p(K) denoting the total

382

handleoverlappingsamongthesepathsandoverlappingbetween
lhcscpathsandthedownpathp in therandomization
levels.
The numberof nodeson level0 from whiche canbe reached
in 2’-‘. Weselectan input nodefor eachof thepi’s from these
JIO~CS,
The numberof possiblewaysto choosethesec nodesis
(a’:‘) because
therequestsassociated
with thechildrenof a node
areorderedaccordingto theID’s of theinputnodes.Let sl,. . . , se
denotethe sourcenodesof the pathspl, . . . ,pc on level 0 and
nodesof thesepaths
dl = T(Q),
* 4,) dc = n(sc) thedestination
on level2d,
NextWCchooseanintermediate
destinationd; for eachpathpt
onnodelcvcl d-l-l. Foreverypi, therearc(eventually)severalpossibilitiesto choosetheseintermediate
destination.However,independentfromtheotherpathsof theconfigurationof vi, thenumber
of nctlvcdestinationsis at mostone. Hence,theexpectednumber
of activeIntermediate
destinations
is at mostone.
Now nssumctheintermediatedestinations
arefixed. Notethat
this alsofixesthepathfromlevel d + .t to level2d. It remainsto
consldcrthenumberof activeconfigurations
of c pathspi,. . . , pi
suchlhat pi connectssi anddi andtraversese. Pathspi,. . . ,pk
nndp donotoverlapin therandomization
levels.Thiscanbeshown
asfollows, If two pathssharea randomswitchs thenthesepaths
arriveandleaves on differentedges.Purthermore,
thesepathsdo
notoverlapat anyotherswitchwith distancelessthand -I- 1 from
n, Hence,twopathsthattraverseedgee cannothaveuseda random
owltchwith distancelessthand-l- 1 fromthetwoswitchesadjacent
to c, nndconsequently,
theycannotmeetona randomswitchonthe
levels0,..,,d/2-lorthclevelsd-t-d/2$1,...,d+f?.
Thenumberof differentpathsconnectingst with d; andtraversing c in one,Thus,thenumberof admissibleconfigurationfor the
pi’s is at mostone.All pathsin theadmissibleconfigurationdonot
sharea randomlyflippedswitchwith anotherpathfromtheconfigurationof ~1. Hence,thenumberof unrevealedrandomswitches
traversed
by eachof thesepathsis d/2 $ (d $ t) - (d + d/2) = .t.
Exceptfor theswitchon level0, all of theseswitchesmustcorrespondto thecourseof therespective
path.Theprobabilityfor this
eventis 2-(‘-‘). As a consequence,
theprobabilitythatall Icpaths
arcactivehi at most2-c’(e-1),
Puttingit nll together,theexpectednumberof activeconfigurationsfor II( Is
Ze-l , pv-‘1
5 ; )
C.
( C >
whichcompletestheproofof Lemma2.2.

Nowsupposethecollisionpathis fixed. The downpathof uri
collideswith this path. First, we assumethat the collision is in
levelf?,with d/2+ 1 I -! 5 d. Let e denotetherespective
collision
edge.Thereis at mostoneadmissiblecoursefor thedownpathof
Wi fromits sourcenodeon level0, whichis determinedby K, to
thecollisionedgee.
The courseof the downpathfrom level 0 to level e is determinedby tberandomlyflippedswitches.Property5 ensuresthat
at most5c of theswitchestraversedby the downpathareshared
witb other pathsin K. Hence,at least1- 5c of the randomly
flippedswitchesdeterminingthe courseof the pathfrom level 0
to level 1 areindependentof K, andconsequently,
the probability that the downpathof wi is equivalentto the only admissible
pathin theselevelsis 2-‘+“. Summingoverall collisionlevels
k’,with d/2 + 1 I e 5 d, yieldsanupperboundon theprobability thatthe switchesalongthe collisionpatharesetappropriately
of 2--d/2csc.Sincethesameboundholdsalsofor collisionswhen
d + 1 5 e I 3d/2, theprobabilitythatthedownpathis equivalent
to the only admissiblepathis at most2-d’2+5c+1. As a consequence,theexpectednumberof activeconfigurationsfor wi is at
most2-d/2’5e+1 - 4 . (log n + 1) = 25c+3. (log n + l)/&.
B
Theboundfor &,u(Vi) ontheexpectednumberof activeconfigurationsfor aninternalnodeVi is independent
of theconfigurationsof the internalnodes~1,. . . , Vi-l. Furthermore,thebound
for Epmne(wi) on the expectednumberof activeconfigurations
for a pruningnodewi is independentof the configurationson all
internalnodesandthepruningnodesWI,. . . uri-I. Consequently,
theseboundsareindependent
estimationsof expectedvaluesand
canbe multipliedin orderto get anupperboundon the expected
numberof all configurations.Sincethenumberof choicesfor the
initial configurationK in E(vl, K) specifyingtherequestassociatedwith the root is 71,we get the following upperboundon the
expectednumberof activewitnesstreeconfigurations.

d‘

I

Now we give an upperboundon the expectednumberof the
activeconfigurationsfor the pruningnodes.For a pruningnode
1u(anda collectionit’ of configurations
for all internalnodesand
thepruningnodes~1,. , . , 2~1-1,let Eprune(~t,K) denotetheexpectednumberof activeconfigurations
for wi undertheassumption
thatall configurations
in I< areactive.Let Eprune(~f)bethemaximumoverall configurationsI< of Eprune(~f,I<).
Lcmmn23 Enruno(ref)< 25c+3+(logn f l)/JZ .
Prook’.The conflictingpathp of pruningnodeUJU~
is eitherassociatedwith theconflictingnodeU( or p or its buddysharesa randomlyfljppedswitchwith a pathassociated
to UY.Thetreeshape
npecit’ies
~1,andthe configurationI< fixesthe requestassociated
with ut, For any consistentsettingof the randomswitches,the
numberof pathssharinga randomlyflippedswitchwith the two
pathsbelongingto this requestis at most2 * (logn + 1) (inclualvethe two pathsthemselves).Consequently,
for anysettingof
theswhchcs,thenumberof candidates
for thecollisionrequestis
nt most2 * (log n + l), and,hence,thenumberof candidates
for
thecollisionpathis at most4 * (logn +i-1).

383

(3)
,< n . pt+s

.
(

<

25c+3* (logn + 1) lc
fi
>

n-c/4+l+o(l)

for 6 = rc/21 = 0 (log log n/ ldg log log n) anda suitablylarge
t = 8 (loglogn/logloglogn).
Equation1 is an immediateconsequence
of Lemma2.2 and
Lemma2.3.
Equation2 is basedontberelationshipbetweenm andm’: The
full witnesstreeincludesc disjointsubtreesof heightt - 1. For
eachof the m’ pruningnodes,somenodesfrom at mosttwo of
thesesubtreesareremoved.Consequently,
at leastc - 2771’
of the
subtreesremainuntouched.Sinceeachof themincludeat least
weget
C‘-* internalnodes,
m~(c-22m’).C’-*>_(K-7n’).ct-*

.

Applyingthis equationandsubstitutingc! = (1 -I- e) . log n yields
logn m

(I >
C.

I

u+ 4

-c*--2.()s-m’)

(

25c+3 - (log 92+ 1) h-m’

5

6

>

is, the algorithm only connectsan input-output pair if neither is already involved in a connection. Without loss of generality we may
assumethat the sequenceof requestsincludes only valid arrival ond
departureevents. An input-output pair is said to esist at eachtime
b betweenits arrival and departure.
The minimum algorithm. To solve the dynamic routing problem
on the two-fold butterfly BB,, we initialize BB, as in Section
2.1. Let sf denotean arrival event. A path for the corresponding
requestri is chosenasfollows. For an edgee in the collision levels,
define c(e) to be the number of paths that traversee at time i. The
algorithm examinesthe two pathsp and p’ that connect the input
to the output of ri. The congestion c(p) of a path p is detined to
be mas,a,(c(e)). If c(p) 5 c(p’), path p is chosenfor rcqucstrr;
otherwise,pathp’ is chosen.

2

fort I log, Iogr+,. n + 2 = Q(loglogn/logloglogn).
Equation 3 results from a bound on the numberof different tree
shqes. In particular, there are at most

K (c”- l)/(c- 1) < pt
j
CC
>j=O
possiblechoicesfor the at most K pruning nodesamongthe (ct WC - 1) internal nodesof the witness tree,and at most

( >
,t+1

_

1

c-l

,’

<

Theorem 2.5 At any time t, the probability that the corrgestiotlis
greater than $(log log n) is at mostn-e(‘08*0~ n),

@-l)

-

Proof. The proof is similar to that of Theorem2.1.
Constructing a witness tree. First, we fix the settingsof the randomlyflipped switches.This determinestwo choicesof pathsfor eachrcquest. Assumethat there is an edge e with congestionlarger than
4c at sometime t, where c = [log log nl, Let p denote the last
path mappedto edgee on or before time t. When p was mapped
to e there were already 4c other paths present at this edge. Let
pl, . . . ,pdc denote these paths such that pr was mappedto e at
time step ti with tr < tr+l. The root of the tree is the request
correspondingto p and the requestscorrespondingto pl, . . . , psi0
are its children. Now we considerthe buddiespi,. . . , pit of these
paths. Path pi traversesan edgewith congestion at least i - 1 at
time steptt, becausethe congestionof pi is not larger than the congestion of pi at time i, and when pi was mappedto e them were
already i - 1 other paths presentat this edge. As a consequence,
we can constructa treeby applying the argumentaboverecursively
to PG, - - - ,PL
The tree constructedaboveis irregular in that nodeshavevarying degrees.However,it containsa c-ary tree of height c, which WC
call the witness tree,with the following properties.

possibilities to choosethem’ conflicting nodesamongthe (@Orl)/(c + 1) < ct+l nodesof the full witness tree. Since specifying
thesenodescompletely determinesthe shapeof the tree, the total
numberof different tree shapesis at most c~* + &+l) 5 c~~‘+~.
Wehavealreadyshownthat &,,
E(T) is an upperboundon
the probability that the c-collision processtakesmorethan t rounds.
Hence,this probability is at mostn --c’4+*+0t1). It remainsto show
that determining which paths becomeinactive each round can be
done in time O(logn), with high probability. Recall that, in our
model, this computationis accomplishedby sendinga packetback
and forth along eachactive path through the nehvork using a storeand-forwardalgorithm. According to [23], such a computationcan
be done in time O(congestion + dilation), with high probability,
using only constant size buffers at each edge. Note here that the
congestionwe wish to bound is the congestioncausedusing this
store-and-fonvardscheme,not the congestionunder the collision
algorithm. However,this congestionis easily bounded. Let C denote the congestionof all 2n paths.
Lemma 2.4 C 5 o-log n/log log n, wifhprobability n-“+“(l).

l

Proof. The congestionin the randomizationlevels is 1. Therfore,
we only have to consider the collision levels. The probability that
a fised collision edge is traversedby at least C paths is at most
l/C!. This bound follows analogouslyto the proof of Lemma2.2.
Hence,the probability that one of the 2 - n - log n collision edges
hascongestionC is at most
2 - n - log n - l/C! 5 n-a+ot’)

forC>cr-logn/loglogn.

l

Each internal node on levels 1,. . . , c - 2 has two children
that are internal nodes and c - 2 children that are leaves,
and eachinternal node on level c - 1 has c children that are
leaves.

Pruning the witness tree. The pruning is done by a breadth-first
traversalof the tree. Weusethe samedefinitions for B(v) andI’(v)
asin Section2.1. However,the pruning rules are slightly different.
When a node u is visited, the following rules are applied.

,
I

Applying Lemma 2.4 yields that each round can be computedin
time O(log n), with high probability. This completesthe proof of
Theorem2.1.
I
2.2

The node on level 0, i.e., the root, has c children that are
internal nodes.

1. If a path associatedwith one of 2r’snon-prunedchildren tmversesa randomly-flipped switch that is also traversedby a
path associatedwith a node u from B(v) U l?(v) then all
nodesbelow v arepruned. Node u is denotedthe conflicting
node of v. Note that the down path of v either sharesa collision edgewith a path p that is associatedwith u, or it shares
a collision edgewith a path p such that p or its buddy shams
a randomswitch with a path associatedwith U. This path p
is denotedthe conjpictingpath of v.

Dynamic routing in BB,

We now describea simple algorithm that routespathsdynamically
in the network BB,, where the dynamic model is specifiedas follows. As before, a request is an input-output pair. An oblivious
adversaryspecifiesan infinite sequence61, ~2, , . . of requests.The
requestQ must be handled at time step i. If at time i neither the
input nor the output of ui is already locked, then the algorithm
must establish and lock a path in the network between the input
and output of og: This is an arrival. If a locked path betweenthe
input-output pair already exists, then the path is released:This is
a departure. In all other casesthe requestmay be ignored. That

2. Dependingon the conflicting path p we apply a further pruning. For each node u E I’(v) such that either the input or
output node of ‘u,coincides with the input or output node of
pathp, we prune all the nodesbelow u. The first pruning rule
ensuresthat there is at most one requestin B(v) U I’(v) incident on eachinput and output of the network, even though

354

the requestsin B(u)Ur( v) existat possiblynon-overlapping
times. Thus, at mosttwo nodes,call them21andu’, get

pruneddueto an applicationof this rule. Nodesu andTV’
arc detlnedto be the conflictingnodesof v. (For simplicity, vie pretendthateachpruningnodev hastwo conflicting
nodesu nndu’; if this is notthecasewesimplyset21andu’
to bethesamenode.)Thesecondpruningrule ensuresthat
Properties
4 and5 asstatedin Section2.1holdfor thepruned
witnesstree- specifically,thedownpathof a pruningnode
cannotsharemorethantwo randomly-flipped
switcheswith
Rgivenconflictingpath.

wheren = [$I = 8 (loglogn).
Equation1 followsfrom the relationshipbetweenm andm’:
Eachof thec childrenof therootof thefull witnesstreeis a rootof
a subtreewith 2”-’ - 1 internalnodes.Foreachof them’ pruning
nodes,nodesfromatmost3 of thesesubtrees
areremoved.Thus,at
leastc- 3m’ of thesubtrees
remainuntouched.As a consequence,
m 2 (c- 3m’) . (2’-’ - 1) 2 (K - m’) . (2’-’ - 1) .

Applyingthisequationandsubstitutingc = [log Iogral yields
logn m ~ 2-(2C-‘-l).(K-m’)
C.1

Ilounding the probability of occurrence of a pruned witness
free, The termstree shape, admissible configuration, andactive
cor&quruliorr aredefinedasin Section2.1.Let ‘T denotethesetof
all treeshapes,and,for T E T, let E(T) denotethe expected
numberof activewitnesstree configurationswith tree shapeT.
Let vr, , , , , urn be the m internalnodesof T. Furthermore,for

n collectionK of configurationsfor the nodesVI,. . . , ~1-1, let
Z&ca~t(~t,
I<) denotetheexpectednumberof activeconfigurations
for vt undertheassumption
that I< is active,andlet E,a(vr) denotethemaximumoverall configurations
K of &,,u(vf, K).

for sufficientlylargen.
Equation2 resultsfroma boundon thenumberof differenttree
shapes.In particular,thereareat most

possiblewaysof chooosingtheatmostKpruningnodesfromtheat
there
mostc - 2’-’ internalnodesof thewitnesstree.Furthermore,
areat most
(c2s2c-1)2~'

IAWl,..,, W,I denotethem’ pruningnodesof T, andlet tli
andn{ denotetheconflictingnodesassociated
with WGFor a collectionI< of contigurations
for thenodesvr, . . . , urnand‘~1,. . . , wf-1,
let ,!$rm,no(w~,
1~7)denotetheexpectednumberof activeconfigurationsfor wt undertheassumption
thatK is active.Furthermore,
let &,n(wt) denotethe maximumoverall configurationsK of
IO*

.

Proof, Thepruned witnesstreedescribed
abovefulfills Properties
3, 4 and5 statedin Section2.1. Hence,theproofof Lemma2.3,
which is bancd only on thesethreeproperties,holdsalsofor this
I
lemma,

The probabilitythat the congestionexceeds4c is at mostthe
probabilitythata prunedwitnesstree.exists,Thelatterprobability
lo at most

p*.p--1).2x

This completes
theproofof Theorem2.5.
3

Ii

A proposal for a data server

Wepresentanapplicationof ourtechniques
to thedataserverarchitecmreproposedintheintroduction.For eachinput nodea’,let 01
betheobjectrequested
by theuseratinputnodei of therandomlywiredbutterflyRB,,. Weassumethat oi # oj for a’# j. Each
objectis storedon two disks: the first disk is chosenuniformly
andrandomlyfrom the first n/2 disks,while the seconddisk is
chosenuniformlyandrandomlyfrom thelast 7~/2disks. Wecall
the two instancesof objectoi the copies ofoi. For an objectof,
let dr(oi) andda(or) be the disksstoringthe copiesof of. As in
Section2, we definetwo pathsp andp’ startingat input nodea’:p
connectsinput nodei with outputnodedr(oi), andp’ connectsa’
with &(or). Sincethecopiesof objectoi arelocatedin different
sub-butterflies,
p andp’ areedgedisjointpaths.Unlike Section2.
we mustminimizenot only congestion,but alsothecontentionat
theoutputnodes,i.e., themaximumnumberof requestsany disk
hasto serve.
3.1

n-c/wto(')

I

23CA

prunedwitnessconstructed
herefulfills Properties1 and2 asstated
I
in Section2.1.

<

K-d

l

Proof, The proof is identicalto that of Lemma2.2, sincethe

Lemma 27 IZpruno(wf) < 2cc+3v(log n f l)/fi

25c+3- (logn + 1)
fi
(
>

possibilitiesto choosethe2m’ contlictingnodesfromtheat most
c2 .2’-’ nodesof the full witnesstree. Multiplying the bounds
yieldsthatthetotalnumberof differenttreeshapesis at mostco”

Lemmn 2.6 EC,,!1
(vi) 5 log n/c! .

&oll(l/h

<
-

(4

Wecontinuethe pruningprocesstill eitherthereareno more
nodesto visit or thereareK = j-c/31pruningnodes.In thelatter
cnsc,we applya linal pruning. If v is the &h pruningnode,we
removefromthetreeall nodesnot includedin B(v) U l?(v). The
remainingtreeis calledthepruhedwitnesstree.

Static routing

For thestaticselectionof pathswe usea modifiedversionof the
collisionprotocolof Section2. Initially, all pathsareactive and
notselected.For a pathp connectinginputnodea’andoutputnode
&(o~), k E {1,2}, let A(p) be the destination of p. A pathp is
selectedif for eachedgee E p thenumberof activepathsplusthe
numberof selectedpathstraversinge is at moste,Md thenumber
of activepathsplusthenumberof selectedpathswith destination
A(p) is at mostE If p andits buddyp’ areboth eligible to be
selected,oneis chosenarbitrarily.A pathp ceasesto beactivein a
roundif p is selectedor thebuddyof p, p’, is selectedin thatround.
Thealgorithmterminates
whenthereareno moreactivepaths.

,

385

Theorem 3.1 For any E 2 5 and c! 2 2 log n, the probability that
the collision algorithm on RBn takes more than t = log, log(n/ log n)
rounds to select a path for evev request is at most n-E’2+1+0(1).

Proof. The proof is similar to that of Theorem2.1 in Section2.1.
Constructing a witness tree. For each input node i fix its requestedobject oi. Fix the randompermutationsno and ~1 usedto
define the randomly-wired butterfly RB,,, and fix the randomdisks
dl(oi) and &(oi), for i = 0,. . . , n - 1. For eachrequest,this determinestwo paths. We say that two paths p and fi edge-collide,
if p and fi both traversean edge e. They are said to disk-collide if
p and fi have the samedestination node on the output level. ‘%o
pathsthat either edge-or disk-collide are said to simply collide.
Assume that there is a requestwith paths p and p’, and neither path has been selectedby round t, where the value of t is
to be determinedlater. Then p either edge-collided with c other
pc in round t or p disk-collided with E other paths
paths Pl,...,
pl , . . . , pa in round t. If p is involved in an edge-collision (resp.,
disk-collision), the root of the witness tree is the requestcorresponding to p and the requestscorrespondingto pl , . . . , pc (resp.,
pl, . . . , pa) are the children. Now pl, . . . ,pC (resp., pl, . . . ,pe)
and their buddiespi,. . . ,pi (resp.,pi,. . . ,pk) must havebezn active in round t - 1. Applying the sameargumentrecursively to
,
PI ,..., P’C(rev.. pi ,... , pi) we can construct a tree of height t.
This tree is called wifness tree.
Each node in the witness tree is a requestwith two paths, a
down path and an up path. Somenodesin the treecorrespondingto
disk collisions have degreec while others correspondingto edge
collisions have degreec >_ E The rightmost c - E children of a
noderepresentingan edgecollision arecalled supefluous nodes. In
order to bound the numberof nodesin the witnesstree,all subtrees
rooted at a child of a superfluousnode are removed. (we will not
refer to this as “pruning’ in the sequel.) Note that a superiluous
nodedoesnot representa collision.
Pruning the witness tree. As in Section2.1, the nodesof a witness
tree do not necessarilycorrespondto distinct requests. However,
the situation here is less complex becausethere are no randomlyflipped switchesthat could be sharedby different paths.Thus, it is
sufficient to ensurethat the requestsin the pruned witness tree are
distinct.
The pruning is done by a breadth-first traversalof the witness
tree. Let B(w) and l?(v) be defined asin Section2. When a nodev
is visited, we use the following pruning rules:

Any internal nodev representsa collision of the down pathof
2)with the up pathsof the children of w. The down path of a
pruning nodew collides with a path p that is associatedwith
its conflicting node U. The path p is called the cottflictins
path of w.
All nodesof the tree correspondto different requests.In particular, pruning nodewt doesnot representthe samerequest
asa conflicting node uj, 1 5 j 5 i.
Bounding the probability of occurrence of a pruned witness
tree. We define the tree sbpe to be a description of the topology
of the pruned tree including the degree(c or Z) of the inner nodes,
the pruning and the conflicting nodes. An admissible witrress tree
conjiguration is a tree shapewith associatedrequests,up and down
paths, and conflicting paths, which eventually, i.e. for some sctting of the randompermutationsdefining the RB, and the random
choicesfor the &(ot), &(o& 0 5 i 5 n - 1, matchesa pruned
witness tree. In particular each admissiblewitness tree configuration has to fulfill the two propertiesstatedabove. An admissible
configuration is active if the outcomeof the randomchoicescorrcspondsto all pathsin the configuration.
The set of tree shapescorrespondingto at least one admissible
witness tree configuration is denotedby 7. As in Section 2.1, we
bound the expectednumber of active witness tree configurations
E(T), for an arbitrary T E 7. Let &II and Eprunobe as defined
in Section2.1.
Lemma 3.2 J&,u(v~) 5 max{ 9,

$}.

Proof. We first bound the expectednumber of active confiyrations for vi representingan edge collision. In this casevi has c
children. Fix the randompermutation~0 and ~1 usedto define the
randomly-wired butterfly.
The expectednumberof active down pathsfor vi is at mostone.
Given path p, there are d = log n possibilities to chosean edgee
at which p collides with the up paths ~1,. . . ,pc of the children
of wi. Let e be the level of e. Since ~0 and ~1 are fixed, there
are at most (2’,-‘) possibilities to chosec pathspossibly attaining
e. Dependingon the randomchoicesof the destinationseachsuch
path attainse with probability 2-@-‘), Thus, the expectednumber
of active configurationsfor vi is
2e-i

.2-4w

< d
- cl

1. If a path associatedwith one of v’s non-prunedchildren is
alsoassociatedwith a nodeu in l?(v), then the subtreesrooted
at the children of u are removedfrom the tree, and tha subtreesrooted at the children of u are also removedfrom fhe
tree. The node w is called a pruning node. The node u is
denotedthe conjicting node of v.

Similarly, the expectednumberof active configurations for 211representinga disk collision (vi has Echildren) is boundedby

2. If a path associatedwith one of v’s non-prunedchildren is
associatedwith a node u from B(w) then the subtreesrooted
at the children of v areremovedfrom the tree. The nodev is
called apruning node. The node u is denotedthe conflicting
node of v.

sincethere are n pathspossibly having the samedestination as the
down path of vt and each such path actually has this destination
n
with probability 2/n.

de (

c >

2”
n
a0 B (1n

p
<-,
- El

Lemma 3.3 Epruno(wf) < d/n.

We continue the pruning processtill either there are no more
nodes to visit or there are ti = [$I pruning nodes. In the latter
case,we apply a final pruning. If u is the Hth pruning node, we
removefrom the tree all nodesnot included in B(v) U l?(v). This
effectively stopsthe pruning processat the Kth pmning node. The
remaining tree is called the pruned witness tree.
Let .uu1,..., w,,, be the m internal nodes and let WI,. . . , W~I
be the m’ pruning nodesin the order of visitation. Further, let ui
denotethe conflicting node of wi, for 1 5 i 5 nz’. The pruned
witness tree possessesthe following properties:

Proof. Let ui be the conflicting node of w(. Assume that W(
representsan edge collision. Since ~1 is associatedwith 2 paths,
thereare2d possibilities to choosethe edgee on which the collision
takesplace. We distinguish two cases.If the up path p of w[ WCS
a crossedgein level 0, then its buddy p’ startsby using a straight
edgein level 0. Thus the level 1 node attained by p’ is a random
node. If p usesa straight edgein level 0, then p’ usesa crossedge
in level 0, and thus attainsa randomnode in level 1.

386

In the level5subsequentto level 1, the courseof the down
pathp’ of ‘WIdependsonly on the randomchoiceof its destination A(#), henceat everylevelp’ attainsa randomedgeandthus
theprobabilityfor p’ to collidewith edgee is 1/(2n). As a conoequcncc
theexpectednumberof activeconfigurations
at pruning
nodew( ia nt most2d &
Now,assumethatwi represents
a diskcollision.Thenlet A(p)
bethedestinationof thedownpathp of wt. Theprobabilityfor the
pathsassociated
to 211to havedestinationA(p) is l/n. Thusthe
cx ectednumberof activeconfigurationsat wt is at most2/a 5
d Pn, if d 2 2,
I
l

Lemma 3.5 E,ll(Vf)

< logn/c!.

Lemma 3.6 Epmne(wf) _< d - +.

Finally, we boundthe probabilitythat thecongestionexceeds4c,
wherec = e(log log n), by boundingtheprobabilitythata pruned
witnesstreeexists.

A5 In Section2.1weproceed
by boundingtheexpected
number
of activewitnesstreeconfigurations.
m

Tt27

Tel-

f=l

ln’

(21 n.c4” .zzus. 14 lc
0n

j=l

,<

n-c/3+l+o(l),

whereEquations1 and2 arejustifiedasin Section2.2.

I

References

parallelcommunication.In ProUl R. Aleliunas.Randomized

ceedings of the ACM SIGACT-SGOPS Symposium on Principles of Distributed Computing, pages60-72,August1952.

PI S.Arora,T. Leighton,andB. Maggs.On-linealgorithmsfor

Bquntion1 followsfroma boundrelatingm andm’. Equation2 is
obtainedby boundingthenumberof treesin 7, takingintoaccount
thefacteachinternaltreenodecaneitherrepresentanedgecollioionor n diskcollision,Bothof theseboundsarederivedin a fashion olmilnrto theircounterparts
in Section2.1.Equation3 follows
fromthefactthatt = log, lo&/d) andmm{?, $} < l/2.
I
3.2

Dynamic routing

pathselectionin a non-blockingnetwork. SIAMJournal on
Computing, 25(3),pages600-625.June1996.
131J. Aspncs,Y. Azar,A. Fiat, S. Plotkin. and0. Waarts.Online machineschedulingwith applicationsto loadbalancing
andvirtual circuit routing. In Proc.25thAnnualACM SymposiumonTheoryof Computing,pages24&249,November
1994.
141Y. Azar,A. Broder,A. Karlin, andE. Upfal.Balancedallocations. In Proceedings of the 26th Annual ACM Symposium on
the Theory of Computing, pages593-602,May 1994.
PI L. A. BassalygoandM. S. Pinsker.Complexityof an optimumnonblockingswitchingnetworkwithoutreconnections.
Problems of Information Transmission, 9:64-66, 1974.
[61 K. Batcher.Sortingnetworksandtheir applications.In Pro-

The modelis similarto theadversarymodel
In Section2.2. An obliviousadversaryconstructsaninfinite
Gcquence
of events,whereeacheventis eitheraninput requesting
an objector nn input releasingan object.At anygiventimeeach
object Is accessed
by at mostoneinput,andeachinputaccesses
at
mostoneobject.
The mlnlmum algorithm. Let a request(i,o) arriveat time t,
wherei is aninput nodeando is theobjectrequested
by i. For an
edgec, detincc(c) to bethenumberof pathsthattraversee at time
t, andZ(i) to be the numberof pathswith destinationi at timet.
(Welenvcthet implicit asthemeaningwill beclear.)Thecongestionc(p) of a pathp is definedto bemax{max,&c(e)), e(A(p))},
whereA(p) is thedestinationof p. Thealgorithmexamines
thetwo
pnthsp andp’ thatconnectinputnodei with thetwooutputnodes
(11(o)anddz(o) thatstoreobjecto. Therequestis fulfilled by p if
c(p) g c(p’), otherwisethe requestis fulfilled by p’.

volume32,pages307-314,196s.
Overview.BBN ReportNo.
t71 ButterflyTh ParallelProcessor
6148,Version1,Cambridge,
MA, March1986.
181B. Beizer.Theanalysisandsynthesisof signalswitchingnetworks. In Proceedings of the Symposium on Mathematical
Theory of Automata, pages563-576,Brooklyn, NY, 1962.
BrooklynPolytechnicInstitute.
multistageconnecting
PI V. E. Ben& Optimalrearrangeable
networks.Bell System Technical Journal, 43:1641-1656,
July
1964.
UOI R. Cole,B. Maggs,andR. Sitaraman.On thebenefitof sup
portingvirtual channelsin wormholerouters.In Proceedings

Thcorcm 3.4 At any time t, the probability that the congestion excecdsQ(log log n) is at most n-‘(‘Og log n).

1111R. Cypher,F. Meyer auf der Heide, C. Scheideler,and

Model dcacription.

uoed

we constructa witnesstreeobeying
themoditlcntlonsmadein Section2.2to thewitnesstreeconstruction of Section2.1. Weprunethe treeusingthe rulesin Section
3.1 (moditlednsin Section2.2)usingat mostK = [c/31 pruning
nodes,Theproofsof thefollowinglemmasaresimilarto theproofs
of thecorresponding
lemmasin Section3.1.
ProoP,

A5 in Section 3.1,

387

ceedings of the AFIPS Spring Joint Computing Conference,

of the 8th Annual ACM Symposium on Parallel A1gon.thm.s
andArchitectures, pages131-141,June1996.

B. V&king. Universalalgorithmsfor store-and-forward
and
wormholerouting. In Proceedings of the 2Sth Annual ACM
Symposium on the Theory of Computing, May 1996.
[12] M. DietzfelbingerandF. Meyerauf der Heide. Simple,efficientsharedmemorysimulations.In Proceedings of the 5th
Annual ACM Symposium on Parallel Algorithms andArchitectures, pages110-119.June1993.

[13] S. Felperin, R Raghavan,E. Upfal. A theory of wormhole
routing in parallel computers.In Proceedings 33rd IEEESymposium on Foundations qf Computer Science, pages563572,
1992.
WI J.A. Garay,and I. Gopal. Call preemptionin communication
networks.In Proc.INFOCOM ‘92, Vo144,pages1043-1050,
Florence,Italy, 1992.
WI A. Gottlieb. An overview of the NYU UltracomputerProject.
In J. J. Dongarra, editor, Experimental Parallel Computing
Architectures, pages25-95. Elsevier SciencePublishers, B.
Y!, Amsterdam,The Netherlands,1987.
Wl Ikeno. A limit on crosspoint number. IRE Trans. on Info.
Theory. 5, pages187-196,1959.
[17] R. Karp, M. Luby, and E Meyer auf der Heide. Efficient
PRAM simulation on a distributed memory machine. Algorithmicu 16, pages245-231, 1996.
WI R. R. Koch. Increasingthe sizeof a network by a constantfactor can increaseperformanceby more than a constantfactor.
In Proceedings of the 29th Annual Symposium on Ftiutuiations of Computer Science, pages221-230. IEEE Computer
Society Press,October 19SS.
WI C. P.Kruskal and M. Snir. The performanceof multistageinterconnectionnetworks for multiprocessors. ZEEE Trunsactions on Computers, C-32(12):1091-109s. December19S3.
F.T. Leighton. Introduction to Parallel Algorithms and Architectures: Arrays l Trees l Hypercubes. Morgan Kaufmann,
SanMateo, CA, 1992.
WI T. Leighton. Methods for messagerouting in parallel machines. In Proceedings of the 24th Annual ACM Symposium
on the Theory of Computing, pages77-96, May 1992.
WI E T. Leighton and B. M. hlapgs. Fast algorithms for routing
around faults in multibutterflies and randomly-wired splitter
networks. IEEE Transactions on Computers, C-41(5):57&
587, May, 1992.
[23] F. T. Leighton, B. M. hlaggs, A. G. Ranade, and S. B.
Rao. Randomized routing and sorting on fixed-connection
networks. Journal of Algorithms, 17(1):157-205,July 1994.
B. M. Maggs and R. K. Sitaraman. Simple algorithms for
routing on butterfly networks with boundedqueues. In Pro-

[301D. Nassimi and S. Sahni. Parallel algorithms to set up the
Benel permutationnetwork. IEEE Transactions on Computers, C-31(2):14S-154,Feb, 19S2.
[311 N. Pippenger.Parallel communicationwith limited buffers. In

Proceedings of the 2SthAnnual Symposium on Foundations of
Computer Science, pages127-136. IEEE Computer Society

Press,October 1984.

1321N. Pippenger. Self-Routing Superconcentrators.Jo~rmalof
Computer and System Sciences, 52(1):53-60, Feb, 1996.

[331 S.Plotkin. Competitiverouting in ATM networks.IEEEJour-

nal on Selected Areas in Communication, pages1126-l 136,
August 1995.
[341 A. G. Ranade. Constrainedrandomization for parallel communication. Technical ReportYALEWDWTR-5 11,Dcpnrtmentof ComputerScience,Yale University, New Haven,CT,
1987.
[351 A. G. Ranade. How to emulate sharedmemory. J. Camp.
Syst. Scis. 42, pages307-326, 1991.
[36] A. Ranade,S. Schleimer,and D. S. Willterson. Nearly tight
boundsfor wormhole routing. In Proceedings of the35th Annual Symposium on Foundations of Computer Science, 1994.

[37] R. Rooholamini, V. Cherkasslry,and M. Garver. Finding the
right ATM switch for the market. IEEE Computer, pages1625, April 1994.
[3S] J. Turner and N. Yamanaka. Architectural Choices in Large
ScaleATM Switches. Technical Report WUCS-97-21. Department of Computer Science, Washington University St.
Louis, MO. 1997.
[39] E. Upfal. Efficient schemesfor parallel communication.Jorrrnul of the ACM, 31(3):507-517,July 19S4.
[40] E. Upfal. An O(log N) deterministic pa&et routing scheme,
Journal of the ACM, 39(l), pages55-70, Jan 1992.
[41] L. G. Valiant. A schemefor fast parallel communication.
SIAM Journal on Computing, 11(2):350-361,May 1962.
[42] L. G. Valiant and G. J. Brebner. Universal schemesfor parallel communication. In Proceedings of the 13th Anwtal ACM
Symposium on Theory of Computing pages263-277, May
1981.
[43] A. Waksman.A permutationnetwork. Journal of the ACM,
15(1):159-163,January 1968.

ceedings qf the 24th Annual ACM Symposium on Theory of
Computing, pages150-161,May 1992.

P53 F. Meyer auf der Heide andB. Wcldng. A packetrouting protocol for arbitrary nehvorks. In Proceedings of the 12th Symposium on Theoretical Aspects of Computer Science, pages
291-302, March 1995.
WI M. Mitzenmacher,“Load Balancing and Density Dependent
Jump Markov Processes”,Proc. of the 37’h IEEE Synp. on
Foundations of Computer Science, pages213-222, 1996.
[27] F. Meyer auf der Heide, C. Scheideler,and V. Stemann.Esplaiting storageredundancyto speedup randomizedshared
memory simulations. In Theoret. Comput. Sci. 162, pages
245-2S1,March 1996.
PS3 T, Nakata, S. Matsushita, N. Tanabe, N. Kajihara,
H. Onozuka,Y. Asano, and N. Koike. Parallel programing
on Cenju: A multiprocessorsystemfor modular circuit simulation. NEC Research d Development, 32(3):421-429,July
1991.
K291G. F. Pfister,W C. Brantley, D. A. George,S. L. Harvey,W. J.
Kleinfelder, K. P.hlcAuliffe. E. A. Melton, V. A. Norton, and
J. Weiss. An introduction to the IBM ResearchParallel ProcessorPrototype(RP3). In J. J. Dongarra,editor, Ekperimental Parallel Computing Architectures, pages 123-140. Elsevier SciencePublishers,B. V., Amsterdam,The Netherlands,
19s7.

388

4038

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 12, NO. 8, AUGUST 2013

Coping with a Smart Jammer in Wireless Networks:
A Stackelberg Game Approach
Dejun Yang, Guoliang Xue, Fellow, IEEE, Jin Zhang, Andrea Richa, and Xi Fang

Abstract— Jamming defense is an important yet challenging
problem. In this paper, we study the jamming defense problem
in the presence of a smart jammer, who can quickly learn the
transmission power of the user and adaptively adjust its transmission power to maximize the damaging effect. We consider
both the single-channel model and the multi-channel model.
By modeling the problem as a Stackelberg game, we compute
the optimal transmission power for the user to maximize its
utility, in the presence of a smart jammer. For the singlechannel model, we prove the existence and uniqueness of the
Stackelberg Equilibrium (SE) by giving closed-form expressions
for the SE strategies of both the user and the player. For the
multi-channel model, we prove the existence of the SE. We design
algorithms for computing the jammer’s best response strategy
and approximating the user’s optimal strategy. Finally, we
validate our theoretical analysis through extensive simulations.
Index Terms—Jamming, Stackelberg Game.

I. I NTRODUCTION

W

IRELESS networks are highly vulnerable to jamming
attacks, since jamming attacks are easy to launch.
Attacks of this kind usually aim at the physical layer and are
realized by means of a high transmission power signal that
corrupts a communication channel, as shown in Fig. 1. We
are interested in defending against smart jammers, who can
quickly learn the transmission pattern of the users and adjust
their jamming strategies so as to exacerbate the damage. Since
jammers need to consider transmission cost, transmitting with
the maximum power may not be the optimal strategy. As a
first step along this line, we study the battle between a single
user (a transmitter-receiver pair) and a single smart jammer
(a malicious transmitter). This problem arises, for example,
in military operations, where one radio station transmits data
to another in a hostile environment. In this paper, we aim to
derive the optimal power control for the user in the presence
of a smart jammer.
Game theory is a natural tool to model and address this
problem. Jamming defense can be considered a game, where
both the user and the jammer are players. Previous works

Manuscript received October 9, 2012; revised February 28, 2013; accepted
May 28, 2013. The associate editor coordinating the review of this paper and
approving it for publication was Y. Guan.
The authors are with Arizona State University, Tempe, AZ 85287 (e-mail:
{dejun.yang, xue, zhang.jin, andrea.richa, xi.fang}@asu.edu).
This research was supported in part by ARO grant W911NF-09-1-0467
and NSF grants CCF-0830704, CCF-1116368, CNS-1115129, and ECCS0901451. The information reported here does not reflect the position or the
policy of the federal government.
A preliminary version of this paper appeared in [19], where the singlechannel model is studied.
Digital Object Identifier 10.1109/TWC.2013.071913121570

Fig. 1.

Jamming in wireless networks.

[1, 2] have been done on this topic by proving the existence
of Nash Equilibria and computing a Nash Equilibrium. A
Nash Equilibrium (NE) is the status where no player has an
incentive to change its strategy unilaterally so as to increase
its own utility. However, Nash Equilibrium is not the best
solution to the problem studied in this paper, because the
rationality of Nash Equilibrium is based on the assumption
that all players take actions simultaneously. In our model, the
jammer is intelligent in the sense that it can quickly learn the
user’s transmission power and adjust its transmission power
accordingly. Stackelberg game serves the purpose of modeling
this scenario. In this game, players, including one leader and
one follower, are in a hierarchical structure. The leader takes
actions first, and then the follower takes actions accordingly.
Similar to the Nash Equilibrium in the standard game, there is
Stackelberg Equilibrium in this game. Different from the Nash
Equilibrium, Stackelberg Equilibrium is the optimal strategy
of the leader, given the fact that the follower would take
actions according to the leader’s strategy, together with the
optimal strategy of the follower corresponding to the leader’s
optimal strategy.
To the best of our knowledge, this paper is the first to
study the power control problem in the presence of a smart
jammer. As an initial step, we consider a single user and
a single jammer (the more challenging scenario with multi
users/jammers is a subject of future research). Both the user
and the jammer can adjust their transmission power levels.
We consider both the single-channel model and the multichannel model. We model the power control problem with
a smart jammer as a Stackelberg game [14], called Power
Control with Smart Jammer (PCSJ) game. In this game, the
user is the leader and the jammer is the follower. The user
is aware of the jammer’s existence and has the knowledge
of jammer’s intelligence, based on which the user chooses
an optimal strategy so as to maximize its own utility, while

c 2013 IEEE
1536-1276/13$31.00 

YANG et al.: COPING WITH A SMART JAMMER IN WIRELESS NETWORKS: A STACKELBERG GAME APPROACH

the jammer plays its best response strategy given the user’s
strategy. For the single-channel model, we derive closed-form
expressions for the jammer’s best response strategy and the
user’s optimal strategy, which together constitute the unique
Stackelberg Equilibrium (SE). For the multi-channel model,
we design an algorithm for computing the jammer’s best
response strategy, given the user’s strategy. We also develop
two algorithms to approximate the user’s optimal strategy and
thus the SE strategies.
The rest of this paper is organized as follows: In Section II,
we briefly describe the related works. In Section III, we introduce the system model and the Stackelberg game formulation.
In Section IV, we study the PCSJ game under the singlechannel model. In Section V, we study the PCSJ game under
the multi-channel model. In Section VI, we present numerical
results. We conclude this paper in Section VII.
II. R ELATED W ORK
Due to the importance of jamming defense, wireless network jamming has been extensively studied in the past few
years. Many jamming defense mechanisms have been proposed on both the physical layer [8–10, 17, 18] and the MAC
layer [11, 12] to detect jamming, as well as to avoid it. Spread
spectrum technologies have been shown to be very effective
to avoid jamming. With enough bandwidth or widely spread
signals, it becomes harder to detect the start of a packet
quickly enough in order to jam it.
Since jamming activities can be considered as a player
(the jammer) playing against another player (the user), game
theory is an appropriate tool to deal with this kind of problem.
Many previous works have studied jamming defense with
game theory formulations [1, 2, 7, 13, 16, 20]. In [1],
Altman et al. studied the jamming game in wireless networks
with transmission cost. In this game, both the user and
the jammer take the power allocation on channels as their
strategies. The utility of the user is the weighted capacity
minus transmission cost. The utility of the jammer is the
negative of the user’s weighted capacity minus transmission
cost. The authors proved the existence and uniqueness of Nash
Equilibrium. In addition, they provided analytical expressions
for the equilibrium strategies. In [2], the same group of
authors extended the jamming problem to the case with several
jammers. The difference from [1] is that they did not consider
transmission cost and they considered SINR and −SINR as
the utility values for the user and the jammers, respectively.
They showed that the jammers equalize the quality of the best
sub-carriers for transmitter on as low level as their power
constraint allows, meanwhile the user distributes its power
among these jamming sub-carriers. In [13], Sagduyu et al.
considered the power-controlled MAC game, which includes
two types of players, selfish and malicious transmitters. Each
type of user has a different utility function depending on
throughput reward and energy cost. They also considered
the case where the transmitters have incomplete information
regarding other transmitter’s types, modeled as probabilistic
beliefs. They derived the Bayesian Nash Equilibrium strategies
for different degrees of uncertainty, and characterized the
resulting equilibrium throughput of selfish nodes.

4039

The jamming problems have also been studied in cognitive
radio networks [7, 16, 20]. The anti-jamming game in this
scenario is often modeled as a (stochastic) zero-sum game,
where the sum of the utility values of the jammer(s) and
the secondary user is zero. In [20], Zhu et al. assumed the
transition between idle and busy states of the channel to be
Markovian. They considered a single secondary user and a
single jammer in the cognitive radio system. The strategy of
the user is the channel selected to transmit on, while the
strategy of the jammer is the channel selected to jam. The
utility of the user is 1 if the selected channel is not occupied
by the primary user and not jammed by the jammer. They
considered mixed strategies and proved the conditions for the
uniqueness of the Nash Equilibrium. They also showed that
the secondary user can either improve its sensing capability
to confuse the jammer or choose to communicate under states
where the available channels are less prone to jamming, in
order to improve its utility value. In [7], Li and Han studied
the problem of defending primary user emulation attack, which
is similar to the jamming attack in wireless networks. There is
only one jammer and one or multiple secondary users in their
models. The strategy of each secondary user is the channel
selected to transmit on, while the strategy of the jammer is
the channel selected to jam. The utility of each secondary
user is a reward if it senses a channel and the jammer is not
jamming. They computed the unique Nash Equilibrium and
analyzed the efficiency. In [16], Wu et al. first investigated
the case where a secondary user can access only one channel
at a time and then extended to the scenario where secondary
users can access all the channels simultaneously. For the
former case, the secondary user uses channel hopping as its
defense strategy. The utility of the secondary user is equal
to a communication gain, if the transmission is successful,
minus cost and a significant loss when jammed. They found
an approximation to the Nash Equilibrium by letting the user
and jammers iteratively update their strategies against each
other. For the latter case, the secondary user could allocate
power to several channels. The utility of the secondary user
is equal to the total number of successful transmissions. They
showed that the defense strategy from the Nash Equilibrium
is optimal.
In all the previous works on jamming defense, the authors
assumed that the users and the jammers take actions simultaneously. In this paper, we study the power control problem
in the presence of a smart jammer, which has more power
compared to the jammer model studied before. To the best of
our knowledge, we are the first to address this problem.
III. S YSTEM M ODEL AND G AME F ORMULATION
In this section, we present the system model and formulate
the problem to be studied.
A. System Model
Our system consists of a user (i.e., a transmitter-receiver
pair), and a jammer (i.e., a malicious transmitter), as illustrated
in Fig. 1. The user (jammer, respectively) has control over its
own transmission power. This problem arises, for example, in
military operations, where one radio station transmits data to

4040

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 12, NO. 8, AUGUST 2013

another radio station in a hostile environment. We consider two
models in this paper: single-channel model and multi-channel
model.
Single-channel model: Let P denote the transmission
power of the user and J denote the transmission power of the
jammer. In addition, we assume that the user and the jammer
transmit with cost E and C per unit power. As in [2, 13], we
adopt SINR as the reward of the user in our model. Hence,
the utility of the user is
us (P, J) =

αP
− EP,
N + βJ

(1)

and the utility of the jammer is
vs (P, J) = −

αP
− CJ,
N + βJ

(2)

where N is the background noise on the channel, and α > 0
and β > 0 are fading channel gains of the user and the
jammer, respectively. Note that we have omitted power constraint in this model. If power constraint is added, we can
still derive closed-form expressions for the SE. However, the
corresponding analysis is more complicated, involving ad-hoc
discussions of many cases. Hence, we choose to concentrate
on this model, which allows us to to emphasize the main
contributions without excessive ad-hoc analysis.
Multi-channel model: We assume that there are n available
channels. Let αi ∈ (0, 1] and βi ∈ (0, 1] denote the fading
channel gains of the user and the jammer on channel i, respectively. Let P̂ > 0 and Jˆ > 0 denote the total transmission
power of the user and the jammer, respectively. Let Pi and Ji
denote the transmission power allocated to channel i by the
user and the jammer, respectively. Let P = (P1 , P2 , . . . , Pn )
and J = (J1 , J2 , . . . , Jn ) denote the transmission power
vectors
n P is feasible
n of the user and the jammer, respectively.
ˆ
if
i=1 Pi ≤ P̂ , and J is feasibleif
i=1 Ji ≤ J.
n
≥ 0, i=1 Pi ≤ P̂ } and
Let P = {(P1 , P2 , . . . , Pn )|Pi 
n
ˆ denote the
J = {(J1 , J2 , . . . , Jn )|Ji ≥ 0, i=1 Ji ≤ J}
sets of feasible power vectors of the user and the jammer,
respectively. Similarly to the single-channel model, we assume
that the user and the jammer transmit with cost E and C per
unit power. The utility of the user is
um (P, J) =

n

i=1

n

αi Pi
−E
Pi .
N i + β i Ji
i=1

(3)

The utility of the jammer is
vm (P, J) = −

n

i=1

TABLE I
A UTILITY MATRIX : THE FIRST NUMBER IN EACH CELL IS THE UTILITY OF
P LAYER A, WHILE THE SECOND IS THE UTILITY OF P LAYER B.
Player B

Player A

U
D

L

R

3,2
4,3

6,5
8,2

B. Basics of Stackelberg Game
In game theory, Stackelberg game [4] is a tool to model
the scenario where hierarchy of actions exists between two
types of players: one is leader, and the other is follower.
The leader makes its move first. After the leader chooses a
strategy, the follower always chooses a best response strategy
that maximizes its utility. Knowing this reaction from the
follower, the leader strategically chooses a strategy to maximize its utility. This optimal strategy of the leader, together
with the corresponding best response strategy of the follower,
constitutes a Stackelberg Equilibrium (SE) of the game.
We illustrate these concepts using a simple example given
in Table I. Note that this example is just for the illustration
of an SE and not an instance of the problem studied. Assume
that Player A is the leader, and Player B is the follower. If A
plays strategy U , B would play strategy R, as it gives player
B a utility of 5 (as opposed to a utility of 2 should B play
strategy L). This leads to a utility of 6 for player A. If A plays
strategy D, B would play strategy L, as it gives player B a
utility of 3 (as opposed to a utility of 2 should B play strategy
R). This leads to a utility of 4 for player A. Hence A would
play strategy U , since doing so would result in a utility of 6
compared to 4 by playing strategy D. As explained before,
B would play R if A plays U . Therefore the Stackelberg
Equilibrium of this game is (U , R).
C. Stackelberg Game Formulation
In our model, the jammer is smart and can adjust its
transmission power based on the user’s transmission power.
Based on this fact, we model the power control problem in
the presence of a smart jammer as a Stackelberg game, called
Power Control with Smart Jammer (PCSJ) game. In this game,
both the user and the jammer are players, of which the user is
the leader, and the jammer is the follower. The strategy of each
player is its transmission power. The utility of the user (resp.
the jammer) is defined in (1) (resp. (2)) for the single-channel
model and (3) (resp. (4)) for the multi-channel model.

n


αi Pi
−C
Ji .
N i + β i Ji
i=1

(4)

In this paper, we deal with a smart jammer, who can
quickly learn the user’s transmission power and adjust its
transmission power accordingly to maximize its utility. The
user’s transmission power can be accurately learned using
physical carrier sensing and location knowledge. We are
interested in determining the transmission power of the user
such that its utility is maximized, in the presence of a smart
jammer. We call this problem the power control problem with
a smart jammer.

IV. PCSJ UNDER S INGLE -C HANNEL M ODEL
In this section, we study the PCSJ game under the singlechannel model. First, we compute the best response strategy of
the jammer, for a given strategy of the user. Then we compute
the optimal strategy of the user, based on the knowledge of
the best response strategy of the jammer.
A. Jammer’s Best Response Strategy
Assume that the user’s strategy P is given. Then the
jammer’s best response strategy can be computed by solving
the following optimization problem.

YANG et al.: COPING WITH A SMART JAMMER IN WIRELESS NETWORKS: A STACKELBERG GAME APPROACH

αP
− CJ.
max vs (P, J) = −
J≥0
N + βJ

u ( P , J ( P ))
DC
4EE

(5)

(D  EN ) CN

DE

Thus we have the following lemma.
Lemma 1. Let P be a given strategy of the user. Then the
corresponding optimal strategy of the jammer is

2
0,
P ≤ CN
αβ ,
√
J(P ) =
(6)
αβP
2
C −N
, P > CN
β
αβ .

αβP
∂vs (P, J)
=
− C.
∂J
(N + βJ)2

DC
4EE 2

CN 2

DE

(a) E ≤

P

α
2N

u ( P , J ( P ))

Proof: To find the maximum value of vs (P, J), we
differentiate vs (P, J) with respect to J and set the resulting
derivative equal to 0,
0=

4041

(D  EN ) CN

DE

P

CN 2

(7)

DE

(b)

Considering the constraint J ≥ 0, we have the optimal strategy
of the jammer in (6).

α
2N

<E≤

α
N

u ( P , J ( P ))

P

CN 2

B. User’s Optimal Strategy

DE
(D  EN ) CN

The user is aware of the existence of the jammer and
knows that the jammer will play its best response strategy
to maximize its own utility. Therefore, the user can derive the
jammer’s strategy based on Lemma 1. To compute the user’s
optimal strategy, we solve the following optimization problem.
max us (P, J(P )) =
P ≥0

αP
− EP,
N + βJ(P )

Fig. 2.

⎧
⎨( α − E)P,
N
us (P, J(P )) = 	 αCP
⎩
β − EP,

P ≤
P >

CN 2
αβ ,
CN 2
αβ .

α
N

User’s utility function for different values of E.

α
α
Case-2: 2N
< E ≤ N
. In this case, we can verify
2
CN
αC
that αβ > 4βE 2 . As illustrated in Fig. 2(b), us (P, J(P ))
2
)CN
when P = CN
achieves its maximum value of (α−EN
αβ
αβ .
2

(9)

Proof: Plugging (6) into the objective function (8), we
have

(c) E >

(8)

where J(P ) is given in (6).
The optimal strategy of the user is given in the following
Lemma.
Lemma 2. The optimal strategy of the user is
⎧
αC
α
⎪
⎨ 4βE22 , E ≤ 2N ,
α
α
P SE = CN
αβ ,
2N < E ≤ N ,
⎪
⎩
α
0,
E> N
.

DE

(10)

Hence us (P, J(P )) is a linear function in P for 0 ≤ P ≤
CN 2
CN 2
αβ , and is a strictly concave function in P for P > αβ .
Note that the derivative of us (P, J(P )) with respect to P in
2
the range P > CN
αβ is given by


1 αC
∂us (P, J(P ))
=
− E.
(11)
∂P
2 βP
αC
Setting equation (11) to 0, we obtain P = 4βE
2.
To compute the maximum value of (10), we consider three
disjoint cases.
2
α
Case-1: E ≤ 2N
. In this case, we can verify that CN
αβ ≤
αC
4βE 2 . As illustrated in Fig. 2(a), us (P, J(P )) achieves its
αC
αC
maximum value of 4βE
when P = 4βE
2.

α
αC
Case-3: E > N
. In this case, we also have CN
αβ > 4βE 2 .
As illustrated in Fig. 2(c), us (P, J(P )) achieves its maximum
value of 0 when P = 0.
This proves the lemma.
Lemmas 1 and 2 lead to the following theorem.

Theorem 1. The strategy pair (P SE , J SE ) is the Stackelberg
Equilibrium of the PCSJ game, where
⎧
αC
α
⎪
⎨ 4βE22 , E ≤ 2N ,
α
α
P SE = CN
αβ ,
2N < E ≤ N ,
⎪
⎩
α
0,
E> N
,
and


J

SE

=

α
2E −N

β

0,

,

E≤
E>

α
2N ,
α
2N .

Remark. Note that the user needs to have the knowledge
of β to compute P SE . This can be achieved as follows: The
user randomly selects its initial transmission power P [0] > 0.
It then keeps increasing its transmission power to P [i] until the
received jamming signal is non-zero. For example, it can set
P [i] = 2i × P [0] for i > 0. The received jamming signal can
be measured by taking advantage of the delay in jamming’s
decision making. According to (6), we have βJ(P [i] ) =

4042

	

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 12, NO. 8, AUGUST 2013

2
C (βJ(P [i] )+N )
− N . Hence we have β =
, where
αP [i]
βJ(P ) is the received jamming signal.

αβP [i]
C
[i]

V. PCSJ UNDER M ULTI -C HANNEL M ODEL
In this section, we study the PCSJ game under the multichannel model.

max{x, 0}. Then the best response strategy of the jammer
is J(P) = (J1 (P), J2 (P), . . . , Jn (P)), where
⎧ 
+
αi βi Pi
⎪
⎨
C+λ0 −Ni
, 1 ≤ i ≤ k,
βi
(16)
Ji (P) =
⎪
⎩
0,
k + 1 ≤ i ≤ n,
and

A. Jammer’s Best Response Strategy

λ0 =

Given the user’s strategy P, the problem of power allocation
for the jammer can be formulated as a convex optimization
problem as follows.

max vm (P, J) = −
J∈J

n

i=1

ˆ
0,
π(0) < J,
ˆ otherwise.
the unique root of π(λ) = J,

In addition, J(P) can be computed in O(n log n) time.

(12)
min f (J) =
J

In the following, Theorem 2 guarantees the existence of the
jammer’s best response strategy, and Theorem 3 computes the
jammer’s best response strategy, when the user’s strategy is
given.
Theorem 2. Let P be user’s strategy. There exists a unique
J(P) such that vm (P, J(P)) is maximized.
Proof: Since vm (P, ·) is a continuous function on the
compact set J , it can achieve its maximum value at some
J ∈ J [15].
Without loss of generality, we assume that Pi > 0 for 1 ≤
i ≤ k and Pi = 0 for k < i ≤ n. It is obvious that, in
any optimal solution of the optimization problem (12), we
have Ji (P) = 0 for k < i ≤ n. Otherwise, we can increase
the value of vm (P, J) by setting Ji (P) = 0, contradicting the
optimality of J. The optimization problem then becomes
max vm (P, J) = −
J

k

i=1

k


αi Pi
−C
Ji
N i + β i Ji
i=1

(13)

s.t.
k


ˆ Ji ≥ 0, for all i ∈ [1, k].
Ji ≤ J,

i=1

The first order partial derivative of vm (P, J) with respective
to Ji , for i ∈ [1, k], is
∂vm (P, J)
αi βi Pi
=
2 − C,
∂Ji
(Ni + βi Ji )

(17)
2

Proof: We convert the optimization problem (13) into a
standard form of convex optimization problem [3]:

n


αi Pi
−C
Ji .
N i + β i Ji
i=1



(14)

and the second order partial derivatives of vm (P, J) are

2αi βi2 Pi
i = j,
− (N +β
∂ 2 vm (P, J)
3,
i
i Ji )
(15)
=
∂Ji ∂Jj
0,
i = j.
The Hessian matrix is negative definite [3]: 2 vm (P, J) ≺ 0,
implying that the objective function (13) is strictly concave,
and there is a unique solution to the optimization problem.
Theorem
user’s strategy. We define π(λ) =
  3. Let Pbe
+
αi βi Pi
k
−N
i
C+λ
for λ ∈ [0, ∞), where [x]+ =
i=1
βi

k

i=1

k


αi Pi
+C
Ji
N i + β i Ji
i=1

(18)

s.t.
k


Ji − Jˆ ≤ 0,

i=1

−Ji ≤ 0, ∀i ∈ [1, k].
The first order partial derivative of f (J) with respective to
Ji , for i ∈ [1, k], is
αi βi Pi
∂f (J)
=−
+ C,
∂Ji
(Ni + βi Ji )2

(19)

and the second order partial derivatives of f (J) are

2αi βi2 Pi
i = j,
∂ 2 f (J)
3,
(N
i +βi Ji )
=
∂Ji ∂Jj
0,
i = j.

(20)

The Hessian matrix is positive definite [3]: 2 f (J)  0,
implying that the objective function (18) is strictly convex.
Since the constraints of the optimization problem are also
convex, we know that the Karush-Kuhn-Tucker (KKT) conditions [3] are necessary and sufficient for optimality.
We define the Lagrangian as


 k
k


ˆ
LJ (J, λ) = vm (P, J) + λ0
Ji − J +
λi Ji , (21)
i=1

i=1

where λi ≥ 0, 0 ≤ i ≤ k, are the Lagrange multipliers. The
KKT conditions for the optimal solution of (18) are given by
∂LJ (J, λ)
= 0, ∀i ∈ [1, k],
∂Ji
k

Ji − Jˆ ≤ 0,

(22)
(23)

i=1

−Ji
λi


 k

λ0
Ji − Jˆ

≤ 0, ∀i ∈ [1, k],
≥ 0, ∀i ∈ [0, k],

(24)
(25)

= 0,

(26)

= 0, ∀i ∈ [1, k].

(27)

i=1

−λi Ji

Combining (22), (23), and (27), we have (16) and (17).

YANG et al.: COPING WITH A SMART JAMMER IN WIRELESS NETWORKS: A STACKELBERG GAME APPROACH

Our algorithm for computing J(P) is given in Algorithm 1.
Lines 1–8 compute the value of λ0 satisfying (17). Line 9
ˆ Line 2
computes J(P) according to (16). When π(0) < J,
of Algorithm 1 computes λ0 = 0, which is consistent with
ˆ
the
 first case in (17). When π(0) ≥ J > 0, we have


αi βi Pi
C+λ

−Ni

+



computes the
if and only if

values {λi0 }ki=1 , such that
λ < λi0 . Line 5 sorts these

λi01 ≤ λi02 ≤ · · · ≤ λi0k . Hence π(λ) =

αi βi Pi
C+λ

−Ni

> 0
βi
values
such that


k

j=l

αi βi Pi
j
j
j
C+λ

−Nij

βij

i

for λ ∈ [λ0l−1 , λi0l ). This also implies that π(λi0k ) = 0, and
π(λ) is strictly decreasing for λ ∈ [0, λi0k ]. Hence there is a
ˆ Lines 6 and 7
unique λ0 ∈ (0, λi0k ) such that π(λ0 ) = J.
compute this value.
Line 5 in Algorithm 1 takes O(k log k) time. The rest of
the algorithm takes O(k) time. Since k ≤ n, the running time
of Algorithm 1 is O(n log n).
This completes the proof.
Algorithm 1: Computation of J(P)
input : The power vector P of the user
1 if π(0) < Jˆ then
2
λ0 ← 0;
3 else
4
λi0 ← αiNβi2Pi − C, for i ← 1 to k ;
6

7

Sort {λi0 }ki=1 such that λi01 ≤ λi02 ≤ · · · ≤ λi0k ;
i
Find r ∈ [1, k] such that π(λ0r−1 ) ≥ Jˆ > π(λi0r ),
i0
where λ0 = 0;



2
k
αij Pij
ˆ + k Nij
J
λ0 ←
− C;
j=r
j=r βi
βi
j

8

end

9

Ji (P) ←

10

⎧ 
+
αi βi Pi
⎪
⎨
C+λ0 −Ni
⎪
⎩

Define

βi

0,
return J(P);

vm (P̄, J(P̄)) − vm (P̄, J ) > 0.

(28)

3 = vm (P̄, J(P̄)) − vm (P̄, J ).

(29)

Since vm (P, J) is a continuous function on P ×J , it is continuous at points (P̄, J(P̄)) and (P̄, J  ). Since {(P[κ] , J(P̄))}
converges to (P̄, J(P̄)), and {(P[κ] , J(P[κ] ))} converges to
(P̄, J ), there exists an integer K such that
|vm (P[κ] , J(P̄)) − vm (P̄, J(P̄))| < , when κ ≥ K,

(30)

and
|vm (P[κ] , J(P[κ] )) − vm (P̄, J )| < , when κ ≥ K.

(31)

Therefore, for all κ ≥ K, we have (using (30), (29), and
(31))
vm (P[κ] , J(P̄))

>
=

vm (P̄, J(P̄)) − 
(vm (P̄, J ) + 3) − 

>
=

vm (P[κ] , J(P[κ] )) −  + 3 −  (34)
(35)
vm (P[κ] , J(P[κ] )) + .

(32)
(33)

This is in contradiction with the assumption that J(P[κ] ) is the
best response strategy of the jammer. This proves the lemma.

i

5

Since J(P̄) is the unique optimal strategy of the jammer for
the strategy P̄ of the user, we have

> 0 for at least one i ∈ [1, k]. Line 4

βi

4043

j

, 1 ≤ i ≤ k,

;

k + 1 ≤ i ≤ n,

B. User’s Optimal Strategy
We first rigorously prove the existence of the user’s optimal
strategy. This implies the existence of SEs of the PCSJ
game. We then design algorithms for approximating the user’s
optimal strategy.
Lemma 3. Let {P[κ] } be a sequence in P converging to a
point P̄ in P. Then the sequence {J(P[κ] )} converges to J(P̄).
Proof: To the contrary, assume that {J(P[κ] )} does not
converge to J(P̄). Since {J(P[κ] )} is contained in the compact
set P, it must have a sub-sequence {J(P[sκ ] )} converging to a
point J = J(P̄). Clearly {P[sκ ] } converges to P̄ since {P[κ] }
converges to P̄. Hence {(P[sκ ] , J(P[sκ ] ))} converges to (P̄, J ).
Without loss of generality, we assume that {(P[κ] , J(P[κ] ))}
converges to (P̄, J ).

Lemma 4. um (P, J(P)) is a continuous function in P.
Proof: By (3), um (P, J) is continuous in the variables
(P, J). From Lemma 3, J(P) is continuous in P. Hence
um (P, J(P)) is continuous in P.
Theorem 4. There exists PSE ∈ P such that (PSE , J(PSE ))
is a Stackelberg Equilibrium of the PCSJ game.
Proof: We know that um (P, J(P)) is a continuous function in P. Since the set P is compact, um (P, J(P)) achieves
its maximum at some point PSE ∈ P [15]. This proves the
theorem.
Based on the analytical results of the jammer’s best response
strategy given user’s strategy, the user can optimize its strategy
P to maximize its utility um (P, J), being aware that its
decision will affect the jammer’s strategy. From the user’s
prospective, its objective is to solve the following optimization
problem.
max um (P, J(P)) =
P∈P

n

i=1

n

αi Pi
−E
Pi ,
Ni + βi Ji (P)
i=1

(36)

where Ji (P) is derived from Theorem 3.
Although Theorem 4 proves the existence of an SE of
PCSJ, computing an SE is challenging. The reason is that
the objective function in (36) is not concave.
Non-Concavity of um (P, J(P)): Define g(P)
=
um (P, J(P)). We use an example to show that there
exists P[1] and P[2] such that


P[1] + P[2]
g(P[1] ) + g(P[2] )
>g
.
2
2

4044

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 12, NO. 8, AUGUST 2013

In this example, n = 2, α1 = α2 = 0.6, β1 = 0.5, β2 = 0.2,
N1 = N2 = 0.2, P̂ = 10, Jˆ = 4, E = 0.1, and C = 1. We
set P[1] = (4, 3) and P[2] = (5, 4). Using Algorithm 1 and
g(P[1] ) = 4.49089,
the definition of um (P, J(P)),
 [1] we[2] have

[2]
P +P
= 4.93331. Hence we
g(P ) = 5.57603, and g
2

 [1]
[1]
[2]
[2]
)
show that g(P )+g(P
.
> g P +P
2
2
Algorithm 2: SE-SA
input : Algorithm parameters I, T , σ, and δ
1 Randomly initialize P, Pbest ← P;
2 repeat
3
for i ← 1 to I do
4
Pnew ← neighbor(P);
5
Randomly select r from (0, 1);
6
if um (Pnew , J(Pnew )) ≥ um (P, J(P)) or
r ≤ e(um (Pnew ,J(Pnew ))−um (P,J(P)))/T then
7
P ← Pnew ;
8
if um (Pnew , J(Pnew )) > um (Pbest , J(Pbest ))
then
9
Pbest ← Pnew ;
10
end
11
end
12
end
13
T ← σT ;
14 until T ≤ 1;
We propose two algorithms to approximate the optimal
strategy of the user. The first is simulated annealing [6],
denoted by SE-SA and presented in Algorithm 2. The second
is a mesh-based hill-climbing algorithm, denoted by SEMESH and presented in Algorithm 3.
Since simulated annealing has been widely used in the
literature, we do not give detailed description of SE-SA, and
refer the readers to [5]. The algorithm finds a global optimal
solution with probability 1 when the number of iterations
goes to infinity [5]. The algorithm parameters are as follows.
T > 0 is the initial temperature. σ ∈ (0, 1) is the annealing
parameter. I is the number of iterations to be performed
at each temperature. δ is the parameter used for generating
perturbations. For any feasible power vector P ∈ P, the
function neighbor(P) generates a perturbation P  ∈ P in
the following way. For each i, let Pi = [Pi + δi ]+ , where
δi is a random number uniformly drawn from [−δ, δ]. If
n
Pi


P̂
n
.
i=1 Pi > P̂ , set Pi =
i=1 Pi
In SE-MESH, we first narrow down the searching space
P to the points (δ1 , δ2 , . . . , δn ) on a mesh with space 
between lines. We then select the top t points that have highest
values of u(P, J(P)). Starting from each of these t points,
we apply a searching strategy similar to that used in SE-SA
(Lines 3–12) except that only the point resulting in a higher
u(P, J(P)) is accepted in SE-MESH. In addition, for each
point P, the searching process terminates if we could not find
a neighbor yielding higher u(P, J(P)) after I iterations.
Remark 1. Similar to the single-channel model, the user
needs to know the value of βi for 1 ≤ i ≤ n to compute
PSE . To achieve this, the user can simply use the method in
Section IV for each channel and compute βi .

Algorithm 3: SE-MESH
input : Algorithm parameters , t, I, and δ
1 Pbest ← 0;

∗
2 Let P ← {(δ1 , δ2 , . . . , δn )|δi ∈ Z , 1 ≤ i ≤ n} ∩ P,
∗
where Z is the set of nonnegative integers;

3 Compute u(P, J(P)) for each P ∈ P ;



4 Let P [1], P [2], . . . , P [t] denote the top t power
transmission vectors with highest u(P, J(P));
5 for i ← 1 to t do
6
P ← P  [i];
7
if um (P, J(P)) > um (Pbest , J(Pbest )) then
8
Pbest ← P;
9
end
10
cnt ← 0;
11
while cnt < I do
12
Pnew ← neighbor(P);
13
if um (Pnew , J(Pnew )) ≥ um (P, J(P)) then
14
P ← Pnew ;
15
cnt ← 0;
16
if um (Pnew , J(Pnew )) > um (Pbest , J(Pbest ))
then
17
Pbest ← Pnew ;
18
end
19
else
20
cnt ← cnt + 1;
21
end
22
end
23 end

Remark 2. Since P is compact and um (P, J(P)) is a continuous function on P by Lemma 4, um (P, J(P)) is uniformly
continuous on P. Therefore, there exists a Lipschitz constant
L > 0, such that |um (P, J(P)) − um (P , J(P ))| ≤ L||P − P ||
[15]. Therefore, as  approaches zero, the solution computed
by SE-MESH converges to the optimal solution. More importantly, we have um (P , J(P )) ≥ um (Popt , J(Popt )) − nL,
where P is the transmit power computed by SE-MESH,
Popt is optimal transmission power of the user, and n is the
dimension of P vector.
VI. S IMULATIONS
In this section, we validate the theoretical insights of the
PCSJ game through extensive simulations.
A. Simulation Setup
For the single-channel model, five variables determine the
players’ strategies and their utility values, which are N , α,
β, C, and E. Among these five variables, only α and β, i.e.,
fading channel gains of the user and the jammer, may vary
significantly due to the change of players’ physical locations.
Hence, we explore the relations of user and jammer’s utility
values with respect to different values of α and β. We set α
and β to be in the range of [0.1, 0.9]. Moreover, let C = E = 1
(as in [1]), and N = 0.2.
For the multi-channel model, we have n ∈ [2, 12] and P̂ =
Jˆ = 10. We assume that αi is randomly distributed over (0, 1]

YANG et al.: COPING WITH A SMART JAMMER IN WIRELESS NETWORKS: A STACKELBERG GAME APPROACH

B. Result Analysis
Figs. 3 and 4 show the results of the single-channel model.
Specifically, Figs. 3(a) and 3(b) show the impact of α on the
players’ utility values with β = 0.5, for different scenarios.
We observe that SE leads to the highest utility values for the
user. The fact that the utility at SE is higher than that at NE is
consistent with the results in [19]. Recall that α is the fading
channel gain of the user. Therefore the larger α is, the closer
the transmitter is from the receiver. Hence, as α increases,
user’s utility increases, as shown in Fig. 3(a), while jammer’s
utility decreases, as shown in Fig. 3(b). For the user, both
NE and MISJUDGE result in higher utility than both RAND
and UNAWARE. It is because the user prepares for the worst
case where the jammer has intelligence. In RAND, the user
randomly sets its power, which results in a negative utility
when α = 0.1, 0.2 even without the jammer. Therefore, the
utility of the user in RAND is lower than that in UNAWARE
when α = 0.1, 0.2. However, when α > 0.2, the user’s utility
in RAND is always higher than that in UNAWARE, due to
the unawareness of the jammer’s existence in UNAWARE.
For the jammer, SE leads to the highest utility. Again, the
higher utility in SE compared to NE is consistent with the
results in [19]. Compare to RAND, jammer’s utility is higher
in UNAWARE where jammer has intelligence. In addition,
MISJUDGE is higher than RAND, because the user assumes
the existence of the jammer in MISJUDGE, but sets power
randomly in RAND. Another observation is that MISJUDGE
results in higher utility than UNAWARE when α ≥ 0.4. It is
because good channel condition (i.e. large value of α) makes
the user transmit with the maximum power in UNAWARE.
Figs. 4(a) and 4(b) show the impact of β on the players’
utility values with α = 0.5, for different scenarios. Again,

0

NE
SE
RAND
UNAWARE
MISJUDGE

−2
v (P, J)

us(P, J)

5

−4
−6

s

0

−5

−8
−10
−12

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
α

NE
SE
RAND
UNAWARE
MISJUDGE

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
α

(a) User’s utility

(b) Jammer’s utility

Impact of α on players’ utility values.

NE
SE
RAND
UNAWARE
MISJUDGE

5

−4
−6

s

0

0
−2
v (P, J)

Fig. 3.

us(P, J)

and βi is randomly distributed over (0, 0.5], for all 1 ≤ i ≤ n.
Same as the single-channel model, we have C = E = 1 and
Ni = 0.2 for all 1 ≤ i ≤ n. For the parameters of SE-SA,
we set I = 1000, T = 100, σ = 0.6, and δ = 0.25. For the
parameters of SE-MESH, we set  = 1, t = 100, I = 1000,
and δ = 0.25.
We compare the SE of the PCSJ game with the following
scenarios:
• Power Control with Standard Jammer (NE) [19]: The
jammer set its power without knowing the user’s. Thus
both the user and the jammer set their power simultaneously.
• Random Power Control (RAND): Both the user and
the jammer randomly set their power, regardless of the
existence of the other, as long as the power allocation is
feasible.
• Power Control While Being Unaware of Jammer’s Existence (UNAWARE): The user maximizes its utility,
without the knowledge of the smart jammer’s existence.
The smart jammer still maximizes it utility with its
intelligence.
• Power Control with Misjudgement (MISJUDGE): The
user assumes the intelligence of the jammer, while the
jammer is just a regular one using random transmission
power.

4045

−5
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
β

(a) User’s utility

Fig. 4.

NE
SE
RAND
UNAWARE
MISJUDGE

−8
−10
−12

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
β

(b) Jammer’s utility

Impact of β on players’ utility values.

SE leads to the highest utility values for the user. Note that
the jammer’s SE utility increases while the user’s SE utility
decreases due to the fact that the jammer’s influence on the
receiver gets stronger as β increases. We also have the similar
observations about the relationship among different scenarios
as in Figs. 3 and 4.
Figs. 5 through 8 show the results of the multi-channel
model. For SE-SA, three parameters I, T , and σ need to be
decided. Fig. 5 shows um (P, J(P)) as a function of the number
of iterations with our parameter settings. Plugging I, T , and σ
into Algorithm 2, we know that there are logσ T1 ∗I = 10000
iterations in total. We observe that the algorithm stops making
improvement after 3000 iterations.
Fig. 6 shows the comparison between SE-SA and SEMESH. In particular, Fig. 6(a) shows the user’s utility and
Fig. 6(b) shows the running time. Although SE-MESH performs a little better than SE-SA, the running time of SEMESH grows exponentially in 1/. Actually, the running time
of SE-MESH is dominated by the number of mesh points we
evaluate in Line 3 in Algorithm 3, which is Θ((n+ 1)P̂ / ). To
have a better idea on how this scales, we plot it as a function of
n and  in Fig. 7. As we can see, when n = 12, the number of
mesh points is more than 10100 for  = 0.1, which is beyond
the computability of current PC machines. Hence SE-SA is
the recommended approach.
Fig. 8 shows the impact of n on players’ utility values under
the multi-channel model. We observe that SE has the best
performance for the user, followed by RAND at the second
and UNAWARE at the bottom. In general, the user’s utility
increases when there are more channels. The reason is that the
user has a better chance to allocate power to channels with
better channel gains, i.e., αi . Another observation is that the
jammer has lower utility values in UNAWARE than SE when
there are less than 7 channels, while higher utility values when
there are more than 7 channels. This is because the number
of channel does not affect the jammer’s utility as much in
UNAWARE as it does in SE. The user will always only use
the channel(s) with the best channel gain(s) if it is unaware

4046

IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, VOL. 12, NO. 8, AUGUST 2013

Number of mesh points

6.6

6

0

u(P, J)

6

0.4

SE−SA
SE−MESH

2
2 3 4 5 6 7 8 9 10 11 12
n

0.3

0

10
12

Fig. 7.
10

0.2

10

8

n

SE−SA
SE−MESH

5

6

4

2

SE−SA
RAND
UNAWARE
MISJUDGE

0

0

−5
2

3

4

5

6

7
n

8

9 10 11 12

Comparison between SE-SA and SE-MESH.

of the smart jammer’s existence. In contrast, the user has
more flexible where there are more channels in SE, with the
knowledge of the jammer’s intelligence. Regarding the user’s
utility in MISJUDGE compared to other scenarios, we have
the similar observations as in the single-channel model. For
the jammer, it has higher utility in UNAWARE than it does
in MISJUDGE. It is because unlike the single-channel model,
where the user can transmit with the maximum power at one
channel, the user allocates power equally to channels with
the same condition under the multi-channel in UNAWARE.
The even power distribution allows the jammer to attack the
channels with better channel conditions for the jammer and
thus to improve its utility. This advantage of the jammer is
enhanced when the number of channels increases.
VII. C ONCLUSIONS
In this paper, we have studied the problem of optimal
power control in the presence of a smart jammer, who can
quickly learn the transmission power of the user and adjust
its transmission power to maximize the damaging effect. We
have considered both the single-channel and the multi-channel
models. We modeled the problem as a Stackelberg game,
called PCSJ game. For the single-channel model, we proved
the existence and the uniqueness by giving the closed-form
expressions for the Stackelberg Equilibrium (SE). For the
multi-channel model, we proved the existence and designed
algorithms for approximating an SE.
ACKNOWLEDGMENT
We thank the associate editor and the anonymous reviewers
for their helpful comments on an earlier version of this paper.
R EFERENCES
[1] E. Altman, K. Avrachenkov, and A. Garnaev, “A jamming game in
wireless networks with transmission cost,” in Proc. 2007 NET-COOP,
pp. 1–12.

2 3 4 5 6 7 8 9 10 11 12
n

(a) User’s utility

(b) Running time

Fig. 8.

0.4

0.2

0

0.6

0.8

1.0

ε

Scalability of SE-MESH in log-scale.

0.1

(a) User’s utility

Fig. 6.

100

10

10000

Convergence of the simulated annealing algorithm.

4

0

8000

um(P, J)

8

4000
6000
Iteration

Running time (secs)

Fig. 5.

2000

200

10

SE−SA
RAND
UNAWARE
MISJUDGE

0
vm(P, J)

6.2

s

u (P, J)

6.4

−10
−20
−30

2 3 4 5 6 7 8 9 10 11 12
n

(b) Jammer’s utility

Impact of n on players’ utility values.

[2] E. Altman, K. Avrachenkov, and A. Garnaev, “Jamming in wireless
networks: the case of several jammers,” in Proc. 2009 GameNets, pp.
585–592.
[3] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge University Press, 2004.
[4] D. Fudenberg and J. Tirole, Game Theory. The MIT Press, 1991.
[5] V. Granville, M. Krivanek, and J.-P. Rasson, “Simulated annealing: a
proof of convergence,” IEEE Trans. Pattern Analysis Machine Intelligence, vol. 16, no. 6, pp. 652–656, June 1994.
[6] S. Kirkpatrick, C. Gelatt Jr, and M. Vecchi, “Optimization by simulated
annealing,” Science, vol. 220, no. 4598, pp. 671–680, 1983.
[7] H. Li and Z. Han, “Dogfight in spectrum: combating primary user
emulation attacks in cognitive radio systems—part II: unknown channel
statistics,” IEEE Trans. Wireless Commun., vol. 10, no. 1, pp. 274–283,
Jan. 2011.
[8] A. Liu, P. Ning, H. Dai, and Y. Liu, “USD-FH: jamming-resistant
wireless communication using frequency hopping with uncoordinated
seed disclosure,” in Proc. 2010 MASS.
[9] X. Liu, G. Noubir, R. Sundaram, and S. Tan, “SPREAD: foiling smart
jammers using multi-layer agility,” in Proc. 2007 IEEE INFOCOM, pp.
2536–2540.
[10] V. Navda, A. Bohra, S. Ganguly, and D. Rubenstein, “Using channel
hopping to increase 802.11 resilience to jamming attacks,” in Proc. 2007
INFOCOM, pp. 2526–2530.
[11] A. Richa, C. Scheideler, S. Schmid, and J. Zhang, “A jamming-resistant
MAC protocol for multi-hop wireless networks,” in Proc. 2010 DISC.
[12] A. Richa, C. Scheideler, S. Schmid, and J. Zhang, “Competitive and fair
medium access despite reactive jamming,” in Proc. 2011 ICDCS.
[13] Y. Sagduyu, R. Berry, and A. Ephremides, “MAC games for distributed
wireless network security with incomplete information of selfish and
malicious user types,” in Proc. 2009 GameNets, pp. 130–139.
[14] V. Stackelberg, Marketform und Gleichgewicht. Oxford University Press,
1934.
[15] W. R. Wade, An Introduction to Analysis, 4th ed. Pearson, 2010.
[16] Y. Wu, B. Wang, K. Liu, and T. Clancy, “Anti-jamming games in multichannel cognitive radio networks,” IEEE J. Sel. Areas Commun., vol. 30,
no. 1, pp. 4–15, Jan. 2012.
[17] W. Xu, W. Trappe, Y. Zhang, and T. Wood, “The feasibility of launching
and detecting jamming attacks in wireless networks,” in Proc. 2005
MobiHoc, pp. 46–57.
[18] W. Xu, T. Wood, and Y. Zhang, “Channel surfing and spatial retreats:
defenses against wireless denial of service,” in Proc. 2004 Workshop
Wireless Security.
[19] D. Yang, J. Zhang, X. Fang, G. Xue, and A. Richa, “Optimal transmission power control in the presence of a smart jammer,” in Proc. 2012
IEEE Globecom, pp. 5506–5511.
[20] Q. Zhu, H. Li, Z. Han, and T. Bas andar, “A stochastic game model for
jamming in multi-channel cognitive radio systems,” in Proc. 2010 IEEE
ICC, pp. 1–6.

YANG et al.: COPING WITH A SMART JAMMER IN WIRELESS NETWORKS: A STACKELBERG GAME APPROACH

Dejun Yang (S’2008) received his B.S. from Peking
University, Beijing, China, in 2007. Currently he is a
Ph.D. candidate in the School of Computing, Informatics, and Decision Systems Engineering (CIDSE)
at Arizona State University. He will be joining the
Department of Electrical Engineering & Computer
Science at Colorado School of Mines as the Ben L.
Fryrear Assistant Professor in the Fall of 2013. His
research interests include economic and optimization approaches to networks, crowdsourcing, smart
grid, big data, and cloud computing. He has received
Best Paper Awards at IEEE MASS’2011, IEEE ICC’2011, 2012, and a Best
Paper Award Runner-up at IEEE ICNP’2010.
Guoliang Xue (Member 1996, Senior Member
1999, Fellow 2011) is a Professor of Computer
Science at Arizona State University. His research
interests include survivability, security, and resource
allocation issues in networks. He has published
extensively in top-tier conferences, including ACM
MOBICOM, ACM MOBIHOC, IEEE INFOCOM,
IEEE ICNP, and ISOC NDSS, as well as top-tier
journals, including IEEE/ACM T RANSACTIONS
ON N ETWORKING , IEEE J OURNAL ON S ELECTED
A REAS IN C OMMUNICATIONS, IEEE T RANSAC TIONS ON M OBILE C OMPUTING , IEEE T RANSACTIONS ON C OMPUTERS ,
IEEE T RANSACTIONS ON C OMMUNICATIONS, IEEE T RANSACTIONS ON
W IRELESS C OMMUNICATIONS, IEEE T RANSACTIONS ON V EHICULAR
T ECHNOLOGY, SIAM Journal on Computing, SIAM Journal on Optimization, and Operations Research. He is an Associate Editor of IEEE/ACM
T RANSACTIONS ON N ETWORKING and IEEE Network, and is a past associate
editor of IEEE T RANSACTIONS ON W IRELESS C OMMUNICATIONS . He was
a Keynote Speaker at IEEE LCN’2011, and served as a TPC co-Chair of
IEEE INFOCOM’2010. He is an IEEE Fellow.

4047

Jin Zhang (S’2010) received the B.E. degree in
network engineering from Beijing University of
Posts and Telecommunications, Beijing, China, in
2008, and the Ph.D. degree in computer science
from Arizona State University, Tempe, AZ, USA,
in 2012. He is a software engineer at Google Inc.,
Kirkland, WA, USA. His research interest is in the
area of designing and analyzing efficient medium
access protocols that are robust against adversarial
jamming in wireless networks.
Andréa W. Richa (M’2000) is an Associate Professor in Computer Science at Arizona State University (ASU), Tempe, AZ. She received her M.S.
and Ph.D. degrees from the School of Computer
Science at Carnegie Mellon University, in 1995 and
1998, respectively; and an M.S. degree in Computer
Systems from COPPE and a B.S. degree in Computer Science, both at the Federal University of Rio
de Janeiro, Brazil, in 1992 and 1990, respectively.
Prof. Richa’s work on network algorithms has been
widely cited, and includes work on distributed load
balancing, packet routing, wireless network modeling and topology control,
wireless jamming, data mule networks, underwater optical networking, and
distributed hash tables (DHTs). Dr. Richa was the recipient of an NSF
CAREER Award in 1999, is currently an Associate Editor of IEEE T RANS ACTIONS ON M OBILE C OMPUTING , and has served as keynote speaker and
program\general chair of several prestigious conferences. For a selected list
of her publications and other accomplishments, and current research projects,
please visit www.public.asu.edu/˜aricha.
Xi Fang (S’2009) received the B.S. and M.S. degrees from Beijing University of Posts and Telecommunications, Beijing, China, and received the Ph.D.
degree in computer science from Arizona State University, Tempe, AZ, USA in 2013. He is a software
development engineer at Microsoft Inc., working
on Microsoft big data platform. His research interests include big data, cloud computing, wireless
networks, and smart grids. Dr. Fang has received
Best Paper Awards at IEEE ICC 2012, IEEE MASS
2011, IEEE ICC 2011, and was a Best Paper Award
runner-up at IEEE ICNP 2010.

Towards Jamming-Resistant and Competitive Medium
Access in the SINR Model
Andrea Richa, Jin Zhang
Computer Science and
Engineering, SCIDSE
Arizona State University
Tempe, AZ 85287, USA

Christian Scheideler

Department of Computer
Science
University of Paderborn
D-33102 Paderborn, Germany

{aricha,jzhang82}@asu.edu

scheideler@upb.de

Stefan Schmid

Deutsche Telekom
Laboratories & TU Berlin
D-10587 Berlin, Germany

stefan@net.t-labs.tuberlin.de

ABSTRACT

Keywords

The efficient coordination of medium access is arguably one
of the most relevant applications of distributed computing.
Recently, progress has been made in the design of robust
medium access (MAC) protocols that guarantee a competitive throughput against a powerful jammer which can block
the medium an arbitrary constant fraction (1−ε) of the time.
These MAC protocols exploit the remaining ε-fraction optimally in the sense that a significant part is used for successful
transmissions. However, so far these throughput guarantees
only hold for rather simplistic interference models such as
Unit Disk Graphs.
This paper reports on our first insights on the design of a
robust medium access protocol SinrMac for the more realistic physical interference model which takes into account the
signal to interference plus noise ratio (SINR) at the receiver.
This model is more difficult, as there is no longer an objective distinction of idling and busy time periods which can
be used to dynamically adjust the wireless nodes’ backoff
periods. We discuss an approach that introduces individual “idle/busy thresholds” which are adapted dynamically
and, unlike the multiplicative backoff periods, in an additive
manner. We find that a reasonable convergence speed (and
throughput) can be achieved if there exists some meaningful
upper bound τ̂ on the noise level in the network; surprisingly,
however, our first simulation results indicate that adaptive
changes of the idly/busy thresholds do not yield a better
throughput than static thresholds set to τ̂ .

Distributed Control, Medium Access, Wireless, SINR, Interference, Jamming, Throughput

1.

Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed
Systems; F.2.2 [Analysis of Algorithms and Problem
Complexity]: Nonnumerical Algorithms and Problems

General Terms
Algorithms, Design

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
S3’11, September 19, 2011, Las Vegas, Nevada, USA.
Copyright 2011 ACM 978-1-4503-0868-7/11/09 ...$10.00.

33

INTRODUCTION

Medium access is a central challenge in wireless computing. In addition to the complication that the participants (or
nodes) of a wireless network may gather in an ad-hoc fashion, join and leave arbitrarily over time, or even be mobile,
communication may be disrupted by external interference
from co-existing networks, microwaves, or even jammers.
Despite the topic’s apparent relevance, researchers still do
not well understand how to design efficient MAC protocols
that guarantee a provably high throughput.
Recently, Awerbuch et al. presented first distributed algorithms that provide performance guarantees against a powerful adversary who can jam the medium an arbitrary constant (1−ε)-fraction of any time window of size T : more formally, for some given T ∈ N and 0 < ε ≤ 1, their (T, 1 − ε)bounded adversary can jam at most (1 − ε)w of the time
steps, for any time window of size w ≥ T . This adversary is
even allowed to be adaptive in the sense that it has complete
knowledge of the protocol’s execution history. While the
first results applied to a single-hop network only [1] (where
it is also possible to elect a leader in a self-stabilizing manner [5], and a modified protocol is even competitive against
reactive jamming [4]), the findings were subsequently generalized to Unit Disk Graphs (UDG) [3].
The analysis of these randomized distributed protocols is
already far from trivial, and it is seems difficult to go beyond
these simplistic interference models. The next big step forward would certainly be a result on the widely used and more
realistic Signal-to-Interference-plus-Noise-Ratio (SINR) model.
A crucial difference from the previous models such as the
UDG model is the fact that in the SINR model, nodes cannot always objectively distinguish an idle medium from a
busy one. This however was a central assumption of the
MAC protocols presented so far as it was used to adjust the
nodes’ backoff periods: in times of an idling medium, the
medium access probability was increased, and in times of a
busy medium, the medium access probability was decreased.
We report on our endeavor to generalize Awerbuch et al.’s
results to the SINR model. Concretely, we describe a first
algorithm where each node maintains a noise threshold to
determine whether the channel is idle or busy, and then
adjust its access probability and noise threshold accordingly
in an adaptive fashion.

2. MODEL

B. Such a jammer is called a (B, T )-bounded adversary: in
every time interval of size w ≥ T , the adversary can add
B · w/T to the noise level N of each node.
Our goal is to design a symmetric local-control MAC protocol (i.e., there is no central authority controlling the nodes,
and all the nodes are executing the same protocol) that has
a “competitive” throughput against any (B, T )-bounded adversary in any multi-hop network that can be modeled by
SINR. Intuitively, we want to call a MAC protocol competitive if the number of successful message receptions at the
nodes is a “large” fraction of the messages that would have
been received if the adversarial contributions to the noise N
are subtracted in the SINR formula for the corresponding
time steps.

We assume that the wireless nodes V (n = |V | many) are
distributed arbitrarily in the 2-dimensional Euclidean plane,
and that they communicate over a wireless network with a
single channel. We also assume the nodes are backlogged in
the sense that they always have something to broadcast. The
SINR model defines a parameter called minimum signal-tointerference-plus-noise ratio (SINR) at which a data frame
can still be received with a reasonably low frame error rate.1
In other words, these SINR values specify the transmission
range of the data transmission mechanism, i.e., the maximum range within which data frames can still be received
correctly. In the following, we assume that each node sends
at a transmission power of one, and a message sent from u
to v is received correctly if and only if

3.

d(u, v)−α
P
SIN R =
≥ β1 ,
N + w∈S d(w, v)−α

THE MAC PROTOCOL

Basically, the SinrMac protocol we propose is a random
backoff protocol, but with a twist: the nodes do not only
backoff once their messages collide, but maintain a “backoff
counter” which is adapted over time and reflects the current
channel state (see also [1]). Rather than storing the backoff
counter itself, each node v in SinrMac stores a medium
access probability pv (between 0 and some upper bound
p̂ < 1). The idea is that in times of an idling channel,
pv is increased (message transmissions become more likely),
whereas in times of a busy medium, pv is decreased. Unfortunately, unlike in the UDG model, such a distinction is not
possible in the SINR model, because absolute silence on the
channel no longer exists due to background noise and the
jammer. Hence, it is hard to tell from a node’s point of view
that the noise it senses at a particular time step is due to
background noise, message collisions, adversarial jamming,
or any combination of these.
In SinrMac, each node v maintains pv (in some sense,
the inverse of a random backoff timer), a noise threshold
estimate τv to distinguish between idle and non-idle time
periods, plus a time window threshold Tv , and a counter cv .
(The threshold Tv is necessary since an accurate estimation
of T allows v to adjust its pv correctly and in a timely manner.) Finally, the nodes share a common small factor γ with
which the cumulative sending probabilities are adjusted, and
a constant value c, which is used to additively adjust τv . In
the following, let Nv be the noise level (background noise
plus concurrent transmissions plus jamming) at node v.
In order to find a good equilibrium and achieve a high
throughput, the pv and τv values need to converge to meaningful values quickly. This constitutes a non-trivial challenge. If there are no successful message transmissions, a
node v cannot decide whether τv is too high or too low.
Fortunately, however, in practice one may determine some
reasonable upper bound τ̂ for τv , as, e.g., (1) the RSSI register (i.e., Received Signal Strength Indicator which measures
the power of a received radio signal) is of limited size and
constitutes a natural upper bound, or as (2) according to [2],
a constant density of transmitter nodes in the network implies that interference from far-away nodes can be bounded
by a constant. Given such an upper bound, it seems feasible
to come up with MAC protocols which find a good equilibrium (in terms of pv and τv values in a certain region), even
in the presence of adversaries.
Our solution, the SinrMac protocol, is formally described
in Algorithm 1. The algorithm is essentially interpreting any
noise floor smaller than τv as an idle channel and increases

where N captures the (e.g., thermal) noise, S is the set of
nodes with concurrent transmissions, and β1 is the SINR
threshold.
For our formal description and analysis, we assume a synchronized setting where time proceeds in time steps called
rounds. In each round, a node u may either transmit a message (at a certain power level) or sense the channel, but it
cannot do both. A node which is sensing the channel may
either (i) sense an idle channel, (ii) sense a busy channel, or
(iii) receive a packet.
In the UDG model, the three cases can easily be distinguished in the following manner: idle means no other node in
a node u’s transmission range is transmitting at that round
and the channel is not jammed, busy means two or more
nodes in u’s transmission range transmit at that round or
the channel is jammed, and successful reception occurs if
exactly one node in u’s transmission range transmits at that
round and the channel is not jammed. In the SINR model,
things are more complicated. In order to distinguish between an idle and a busy channel, a node may use a certain
threshold β2 : if the measured signal power exceeds β2 , a
channel is considered busy, otherwise idle. Whether a message is successfully received is determined by the SINR rule
described above. (There is at most one successful reception
at any moment of time.)
We assume that in addition to the nodes there is an adversary: the idea is that our conservative definition of adversary subsumes many different forms of intentional and
unintentional interference. Concretely, like in [1], we want
to allow the adversary to know the protocol and its entire
history and to use this knowledge in order to jam the wireless
channel at will at any round (i.e, the adversary is adaptive).
However, unlike in previous works [1], the adversary is not
bounded over time in the sense that it can only jam a subset of the time periods, but with respect to energy: for each
time period of length T , the adversary has a certain energy
budget to disrupt communications. Rather than assuming
some jammer locations in the Euclidean plane from which it
can transmit at different energy levels, we propose a model
where the jammer has a certain budget Bv for each wireless node v ∈ V . Henceforth, we assume that this budget
is the same for every node and we will simply refer to it by
1
For example, the minimum SINR for 802.11b are 10dB for
11Mbps down to 4dB for 1Mbps.

34

the sending probabilities accordingly; if on the other hand
the noise is relatively high, the sending probabilities are reduced, but only after Tv rounds where the channel was not
idle.
In SinrMac, each node adapts τv additively and pv multiplicatively, based on the channel states. Concretely, we
decrease τv by 2c if there is not much noise (Nv < τv ), but
only increase it by c otherwise: thus, in an equilibrium, we
strive for a 2 : 1 ratio of busy to idle time periods.
Algorithm 1 SinrMac
1: Initially, every node v sets Tv := 1, cv := 1 and pv := p̂.
2: Afterwards, the protocol proceeds in synchronized
rounds:
3: v decides with probability pv to send a message
4: if v decides not to send a message then
5:
v senses the channel
6:
if a message is successfully received then
7:
pv = pv /(1 + γ)
8:
else if Nv < τv then
9:
τv := max{τv − 2c, 0}
10:
pv := min{(1 + γ)pv , p̂}
11:
Tv := max{Tv − 1, 1}
12:
else if Nv ≥ τv then
13:
τv := min{τv + c, τ̂ }
14:
if cv ≥ Tv then
15:
cv := 1
16:
if no idle channel in past Tv rounds then
17:
pv := pv /(1 + γ)
18:
Tv := Tv + 2
19:
end if
20:
end if
21:
end if
22: end if

Figure 1: Normalized throughput as a function of
the network size and under different τv adaption
schemes. The result is averaged over 5 runs.

periments, we find that if p̂ = 1/2, fixing τv at a lower level,
i.e., τv = 4, gives the best throughput, since in this case p̂ is
much higher, and hence there are more collisions and busy
time periods. Here, being able to identify the busy channels
and decrease access probabilities accordingly is crucial for
the protocol to achieve a good throughput.

5.

CONCLUSION

This paper described a preliminary MAC protocol for the
SINR model under jamming activities. Surprisingly, we
found that our adaptive idle/busy threshold adaption strategy often performs worse than a static strategy. In our future work, we plan to rigorously evaluate different adapting
schemes for τv , and study our algorithm under more sophisticated and worst-case adversaries, not only empirically but
hopefully also by deriving performance proofs. Obviously,
in this process, changes to the protocol presented here may
be required.

4. FIRST RESULTS
Although intuitively, adapting τv seems to be crucial to
accurately react to the channel states and converge to a
good throughput, our first experiments indicate that static
τv values (fixed at the maximal possible reception power)
are often better. In the following, we report on our preliminary simulation study to evaluate the performance of our
protocol in terms of throughput and as a function of the
network size. We define throughput as the number of messages successfully received in the whole network per round.
In our network, nodes are distributed on a square grid (i.e.,
the number of nodes is n = a × a for some parameter a),
and we allow the simplistic adversary to evenly allocate its
jamming budget (B = 200, no other noise) among T time
steps, i.e., N = B/T per round. The transmission power for
all nodes is set to 4, the SINR ratio is β1 = 0.5, and T = 50.
We set c = 0.1, and consider p̂ = {1/24, 1/2}.
We evaluate four different schemes for adapting τv : the
first one initializes τv = 1 and adapts τv based on “idle”
and “busy” channel states afterwards (see Algorithm 1); the
other three schemes use a fixed τv (from {1, 4, 40}).
Figure 1 show an exemplary dependency of the throughput on the different τv schemes when p̂ := 1/24. We see that
an adaptive τv value approach performs worse than a strategy fixing τv at a high level: τv = 40 lets pv stay close to p̂,
which is still fine since 1/24 is relatively small. In other ex-

6.

REFERENCES

[1] B. Awerbuch, A. Richa, and C. Scheideler. A
jamming-resistant MAC protocol for single-hop wireless
networks. In Proc. of PODC ’08, 2008.
[2] D. Blough, C. Canali, G. Resta, and P. Santi. On the
impact of far-away interference on evaluations of
wireless multihop networks. In Proc. MSWIM, 2009.
[3] A. Richa, C. Scheideler, S. Schmid, and J. Zhang. A
jamming-resistant MAC protocol for multi-hop wireless
networks. In Proc. DISC, 2010.
[4] A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Competitive and fair medium access despite reactive
jamming. In Proc. ICDCS, 2011.
[5] A. Richa, C. Scheideler, S. Schmid, and J. Zhang.
Self-stabilizing leader election for single-hop wireless
networks despite jamming. In Proc. MobiHoc, 2011.

35

New Approximation

Techniques

for Some Ordering
And&a

Satish Rao*

Problems

W. Richat

Abstract

1

We describe logarithmic
times optimal
approximation
algorithms
for the NP-hard
graph optimization
problems of minimum
linear arrangement,
minimum
containing
interval
graph,
and minimum
storage-time
product.
This improves on the best previous approximation bounds of Even, Naor, Rao, and Schieber for
these problems by an Q(log log n) factor.
Even, Naor, Rao, and Schieber defined “spreading
metrics” for each of the ordering problems above (and
to other problems);
for each of these problems,
they
provided a spreading metric of volume W, such that W
is a lower bound on the cost of a solution to the problem.
They used this spreading
metric to find a solution of
cost O(W log n log log n) (for simplicity,
assume that
all tasks have unit processing
time, in the minimum
storage-time
product problem).
In this paper, we show
how to find a solution within a logarithmic
factor times
W for these problems.
We develop a recursion
where at each level we
identify
cost which,
if incurred,
yields subproblems
with reduced spreading metric volume. Specifically,
we
present a divide-and-conquer
strategy where the cost of
a solution to a problem at a recursive level is C plus the
cost of a solution to the subproblems
at this level, and
where the spreading metric volume on the subproblems
is less than the original volume by Q(C/ log n). This
ensures that the resulting
solution has cost O(logn)
times the original spreading metric volume.
We note that this is an existentially
tight bound on
the relationship
between the spreading metric volume
and the true optimal values for these problems.
For planar graphs, we combine a structural
theorem
of Klein, Plotkin, and Rao with our new recursion technique to show that the spreading metric cost volumes
are within an O(loglogn)
factor of the cost of an optimal solution for the minimum
linear arrangement,
and
the minimum containing
interval graph problems.

We describe approximation
algorithms
that apply to
the NP-hard
graph optimization
problems
of minimum linear arrangement,
minimum
containing
interval
graph, and minimum
storage-time
product [4]. An crapproximation
algorithm
is an algorithm
that finds a
solution to the problem whose cost is at most Q times
the cost of an optimal solution to the problem.
All the ideas that we use for approximating
the
minimum
containing
interval graph and the minimum
storage-time
product
problems can be illustrated
by
the algorithms
for the minimum
linear arrangement
problem.
Thus, we restrict
our exposition
primarily
to the minimum linear arrangement
problem, which we
define as follows: Let G be a graph with associated edge
weights. Informally,
a minimum
linear arrangement
of
G is an embeddingi
of G in the linear array such that
(i) we have a one-to-one mapping from the nodes of G
to the nodes of the linear array, and (ii) the weighted
sum of the edge lengths - that is, the cost of the linear
arrangement
- is minimum.
The length of an edge
in the embedding
is given by the distance between its
endpoints on the linear array.
Finding a minimum linear arrangement
is NP-hard,
even for the case when all the edges have unit weight.
We present a polynomial
time O(log n)-approximation
algorithm for the minimum linear arrangement
problem
on a graph with n nodes.
This improves
the best
previous
approximation
bound of Even, Naor, Rao,
and Schieber [l] f or this problem
by a O(loglogn)
factor.
If the graph is planar
(or, more generally,
if it excludes
Ii’,,, as a minor,
for fixed r, where
I(I-,, is the r x r complete bipartite
graph), we obtain
an O(loglogn)
approximation
factor for the minimum
linear arrangement
problem,
using a variation
of the
algorithm
presented for the general case. We obtain
this improvement
by combining the techniques used for
the general case with the algorithm
presented by Klein,
Plotkin,
and Rao [7] for finding separators
in graphs
that exclude fixed li’,,,-minors.

*Research
supported
by NEC Research
Institute,
4 IndepenNJ
dence
Princeton,
08540.
Email:
Way,
satish@research.nj.nec.com.
t Research
supported
in part by NSF National
Young Investigator Award
No. CCR-9457766,
ARPA
Contract
F33615-93-1-1330,
NEC Research Institute,
and DIMACS
graduatestudent
program.
School of Computer
Science,
Carnegie
Mellon
University,
Pittsburgh, PA 15213. Email:
aricha@cs.cmu.edu.

Introduction.

‘An
embedding of a graph G into a graph H maps nodes of
G to nodes of H, and edges of G to paths in H. Typically,
a
guest network
G is emulated
by a host network
H by embedding
G into H. (For a more complete
discussion
of emulations
and
embeddings,
see [s] .) .

211

212
Using techniques from [12], we can view the minimum containing
interval graph problem (which we define formally
later) as a “vertex version” of the minimum linear arrangement
problem
(see [l]). Thus, we
also obtain
an O(log n)-approximation
algorithm
for
general graphs, and an O(log log n)-approximation
algorithm for graphs that exclude fixed K,,,-minors
for
this problem.
This improves on previous bounds of
O(lognloglogn)
[l] for general graphs, and O(logn)
for graphs that exclude fixed I(,,,-minors.
We can also use techniques
from [12] to extend
our ideas to produce an O(logT)-approximation
for the
minimumstorage-time
product problem (where T is the
sum of the processing times of all tasks), improving
on
a previous approximation
bound of O(log T log log T) in

Dl.
Alon and Seymour [13] proved that there exists a
logarithmic
gap between the spreading metric cost volumes, and the true optimal values for certain instances
of the problems of minimum
linear arrangement,
minimum containing
interval graph, and minimum storagetime product.
Thus we provide an existentially
tight
bound on the relationship
between the spreading metric cost volumes and the true optimal values for these
problems.
1.1
Previous
Work.
Leighton and Rao [9] presented
an O(log n)-approximation
algorithm
for balanced partitions of graphs.
Among other applications,
this provided O(log2 n)-approximation
algorithms
for the minimum feedback arc set, and for the minimum-cut
linear
arrangement
problem.
Hansen [5] used the ideas in [9]
to present O(log2 n)-approximation
algorithms
for the
minimum linear arrangement
problem, and for the more
general problem of graph embeddings
in d-dimensional
meshes. Ravi, Agrawal,
and Klein [12] presented polynomial time approximation
algorithms
that deliver a
solution with cost within an O(log n 1ogT) factor from
optimal
for the minimum
storage-time
product problem, where T is the sum of the processing times of all
tasks, and within an O(log2 n) factor from optimal for
the minimum
containing
interval graph. The minimum
storage-time
product problem also generalizes the minimum linear arrangement
problem.
Seymour
[13] was the first to present a directed
graph decomposition
divide-and-conquer
approach that
does not rely on balanced cuts. He presented a polynomial time O(log n log log n)-approximation
algorithm
for the minimum feedback arc set problem. Even, Naor,
Rao, and Schieber [l] extended
the spreading
metric
approach used by Seymour to obtain polynomial
time
O(log n log log n)-approximation
algorithms for the minimum linear arrangement,
and the minimum
contain-

ing interval graph problems, and an O(log T loglogT)approximation
algorithm
minimum
storage-time
product problem.
Even et al.
actually
showed similar approximation
results for a broader class of graph
optimization
problems,
namely the ones that satisfy
their “approximation
paradigm”:
A graph optimization problem where their divide-and-conquer
approach
is applicable,
and for which a spreading
metric exists satisfies this paradigm.
They presented
polynomial
time
O(min{log
kV log log IV, log lc log log IE})approximation
algorithms
for these problems,
where k
denotes the number of “interesting”
nodes in the’problem instance (clearly Ic 5 n), and W is the lower bound
on the cost of a solution to the optimization
problem
provided by a spreading metric. Examples of such problems, besides the ones already mentioned,
are: graph
embeddings in d-dimensional
meshes, symmetric
multicuts in directed networks, multiway
separators
and p
separators (for small values of p) in directed graphs. For
a detailed description
of each of those problems, see [l].
Even, Naor, Rao, and Schieber [2] extended
the
spreading metric techniques to graph partitioning
problems. They used simpler recursions that yield a logarithmic approximation
factor for balanced cuts and multiway separators.
However, they were not able to extend
this simpler technique to obtain a logarithmic
approximation bound for the other problems considered in [l].
1.2
Spreading
Metrics
and Our Recursion.
Our
algorithms use an approach that relies on spreading metrics. Spreading metrics have been used in recent divideand-conquer
techniques to obtain improved approximation algorithms
for several graph optimization
problems
that are NP-hard [l]. These techniques perform the divide step according to the cost of a solution to the subproblems generated,
rather than according to the size
of such subproblems.
A spreading
metric on a graph is an assignment
of lengths to the edges or nodes of the graph that has
the property
of “spreading
apart” (with respect to the
metric lengths) the nontrivial
subgraphs.
The volume
of the spreading metric is the sum, taken over all edges
(resp., nodes), of the length of each edge (resp., node)
multiplied
by its weight.
Even, Naor, Rao, and Schieber[l]
showed how to
find a spreading metric of volume W such that W is a
lower bound on the cost of a solution to the optimization
problem.
Our techniques
are based on showing that
a spreading
metric of volume W can be used to find
a solution with cost O(Wlogn)
(O(WlogT),
for the
minimum storage-time
product problem) for any of the
ordering problems considered.
In this paper, we develop a recursion where at each

213
level we identify cost which, if incurred, yields subproblems with reduced spreading
metric volume.
Specifically, we present a divide-and-conquer
strategy where
the cost of a solution to a problem at a recursive level
is C plus the cost of a solution to the subproblems,
and where the spreading
metric volume on the subproblems is less than the original volume by fl(C/ log n)
prod(resp., fl(C/ 1ogT) for th e minimum storage-time
uct problem).
We will show that this ensures that the
resulting
solution
has cost O(logn)
(resp., O(logT))
times the original spreading metric volume.
The recursion is based on divide-and-conquer;
that
is, we find an edge set which removal divides the graphs
into subgraphs,
and then recursively
order the subgraphs. The cost of a recursive level is the cost associated with the edges in the cut selected at this level. Previous recursive methods and analyses proceeded by finding a small cutset of edges where the maximum spreading metric volumes of the subproblems
were quickly reduced.
We proceed by finding a sequence of cutsets
whose total cost can be upper bounded, say by a quantity C, and whose total spreading
metric volume is
R(C/logn)
(resp., fl(C/logT)),
as stated above. The
crux of the argument
is that the cost associated with
a.n edge in a cutset can be bounded by the number of
nodes between the previous and the next cutset in the
sequence.
We point out that the methods
in [l] applied
t,o more problems,
including
the d-dimensional
graph
embedding,
a.nd the minimum feedback arc set problems
[13]. We could not extend our methods to these other
problems, since we were unable to find a suitable bound
on the cost of a sequence of cutsets associated with any
of these problems.
Finally,
for planar graphs and other graphs that
exclude some fixed minors, we combine a structural
t,heorem of Klein, Plotkin
and Rao [7] with our new
recursion t,echniques, to show that the spreading metric
cost volumes are within an O(loglogn)
factor of the
cost of the optimal
solution for the minimum
linear
arrangement,
and the minimum
containing
interval
graph problems.
1.3
Overview.
We present a formal definition of the
problems
in Section 2. In Section 3, we define the
spreading metric used for the minimum linear arrangement problem.
In Section 4, we present a polynomial
time O(log n)-approximation
algorithm
for the minimum linear arrangement
problem on an arbitrary
graph
with n nodes and nonnegative
edge weights.
In Section 5, we show how to improve this approximation
factor to O(log log n), in case the graph has no fixed I(,,,In Section 6, we
minors - e.g., the graph is planar.

briefly discuss an algorithm
and its analysis for approximating the minimum storage-time
product.
2

The

Problems.

In this section, we present the definitions
problems considered in this paper.

of the ordering

2.1
Minimum
Linear
Arrangement.
The minimum linear arrangement
(MLA) problem is defined as
follows:
Given an undirected
graph G(V, E), with n
nodes, and nonnegative
edge weights w(e), for all e in
E, we would like to find a linear arrangement
of the
n} that minimizes the sum, over
nodes rr: V+
{l,...,
all (i, j) in E, of the weighted edge lengths ]u(i) -u(j)].
In other words, we would like to minimize the cost
c
44
(i,j)eE

A Idi)

In the context of VLSI layout,
the length of the interconnection

- 4.91.

]~(i) - r(j)] represents
between i and j.

2.2
Minimum
Storage-Time
Product.
The minimum storage-time
product problem arises in a manufacturing
or computational
process, in which the goal
is to minimize the storage-time
product of the process:
We want to minimize the use of storage over time, assuming storage is an expensive resource. Let G( V, E) be
an acyclic directed graph on n nodes with edge weights
w(e), for all e in E, and node weights T(U), for all 21in
V. The nodes of G represent tasks to be scheduled on a
single processor. The time required to process task v is
given by r(u). Th e weight on edge (u, w) , w(u, w), represents the number of units of storage required to save the
intermediate
results generated by task u until they are
consumed at task v. The minimum
storage-time
product problem consists of finding a topological
ordering of
the nodes cr : V + { 1, . . , n} that minimizes

This problem generalizes the MLA problem.
When
all tasks have unit execution
time, it becomes the
directed
MLA problem.
It is also a generalization
of the single-processor
scheduling
problem,
if we are
minimizing
the weighted sum of completion
times (this
problem is NP-complete
[4, problem SS13, page 2401).
2.3
Minimum
Containing
Interval
Graph.
We
first introduce
the interval graphs.
An interval
graph
is a graph whose vertices can be mapped to distinct
intervals in the real line such that two vertices in the

214
graph have an edge between them iff their corresponding
intervals overlap.
A completion
of a graph G into an
interval graph results in an interval graph with same
node set as G that contains G as a subgraph.
We use the following
characterization
of interval
A n undirected
graphs, due to [ll].
graph G(V, E) on
n nodes is an interval
graph iff there exists a linear
ordering D : V + { 1, . . . , n} of the nodes in V such that:
If an edge (u,zt) is in E, where u(u) < a(w), then every
edge (w,w), for w such that C(U) < g(w) < o(o), also
belongs to E. Note that following this characterization,
given any ordering
u, there exists a unique way of
completing
G into an interval graph by adding as few
edges to G as possible.
The cost of a completed graph is given by the total
number of edges in the (completed)
graph.
This cost
can be viewed as the sum over vertices of the maximum
backward stretch of the vertex -i.e.,
of the distance to
the farthest lower numbered node to which the vertex
is connected.
This is very similar to the MLA problem,
except that the nodes are stretched
along the order
rather than the edges (see [l]).
Thus, our techniques
also apply to this problem.
This problem arises in several areas, from computer
science, to biology (see [lo]), to archeology
(e.g., when
finding a consistent
chronological
model for tool use
while making as few assumptions
as possible [6]).
3

Spreading

Metric.

In this section, we define the spreading metric used in
the algorithms
for the MLA problem presented in Sections 4 and 5. Analogous functions are used when approximating
the minimum
storage-time
product problem (as presented in Section 6), and the minimum containing interval graph problem (see [l]).
Here we present spreading metrics in the context of
the MLA problem (see [l] for a more general definition).
A spreading metric is a function e : E + Q that assigns
rational
lengths to every edge in E, and that can be
computed in polynomial
time. It also has to satisfy the
following
two properties.
The volume of a spreading
metric ! is given by xeEE w(e)e(e).
1. Lower
metric

bound: The minimum volume of a spreading
is a lower bound on the cost of a MLA of G.

2. Diameter guarantee:
Let the distances be measured
with respect to the lengths l(e).
The distances
induced by the spreading metric “spread” the graph
and all its nontrivial
subgraphs.
In this application,
this translates to “The diameter of every nontrivial
connected subgraph U of V is s2(]U])“.
A solution e to (3.1-3.2) is a spreading metric for
the MLA (see [l]). Let Y denote the set of all nontrivial

connected

subgraphs

(3.2)

of V.

l(e)

>

0, Ve E E

where dist(u,u)
is the length of a shortest path from
u to w according to the lengths l(e).
Note that (3.1)
actually
implies that e(e) > 1, for .a11 e in E (simply
consider the subsets U that consist of a single edge and
its endpoints).
A solution
f! to (3.1-3.2)
that
minimizes
eEE
w(e)!(e)
is
a
lower
bound
on
the
cost
of a MLA
c
(see [l]). We will use this fact later, when proving Theorems 4.1 and 5.1.
Let C be a spreading
metric
of volume
W =
eEE
w(e)!?(e)
that
satisfies
(3.1-3.2).
In
the
remaining
c
of this paper all the distances in G are measured with
respect to e.
4

The

Algorithm.

We now present our O(log n)-approximation
algorithm
for the MLA problem on general graphs. Let G( V, E) be
a graph with nonnegative
edge weights w(e). Assume
w.1.o.g. that G is connected
(otherwise
consider each
connected component of G separately),
and that all the
edge weights w(e) are at least 1. For integers z and y,
we use the notation [x, y] to denote the set {CC,. . , y-l};
andby[z],wedenotetheset
{O,...,a:--1).
In this paragraph,
we introduce the notion of a level
according to e: Fix a node w in V. An edge (CC,y) is at
level i with respect to zi iff dist(w, Z) is at most i and
dist(v, y) is greater than i, for any i E N. Note that an
edge may be at more than one level. Let the weight of
level i, denoted by pi, be the sum of the weights of the
edges at level i. W.l.o.g., we will assume that 1ogW is
an integer. Let ok = 2”) for all Ic in [(log W) + 11. Level
i has index k, Ic in [log W], iff pi belongs to the interval
Ik = (ok, (Yk+r]. It follows from (3.1) that we must have
at least n/4 distinct levels with nonzero weight.
Since
there are log W distinct level indices, there must be at
least n/(4 1ogW) 1evels with same index Ic, for some k.
Let K be the exact number of levels of index k.
In a recursive step of the algorithm,
we cut along
the sequence of K levels of index k - i.e., we remove the
edges that are at those levels, even if they are also at
some other level of index different from k. For all i, let
level ai be the ith level of index k, in increasing order of
distances to w. Let Hi be the subgraph induced by the
nodes that are at distance greater than a; and at most
ai+l from V; let He (resp., Ii,) be the subgraph induced
by the nodes that are at distance at most al (resp.,
greater than a,) from w. Let ni denote the number of
nodes in Hi. We recurse on each Hi, obtaining
a linear

215
arrangement ui for the ni nodes in this subgraph. We
combine the linear arrangements obtained for the Hi’s,
obtaining a linear arrangement u for G, as follows:
(a(l),

. . ,0(n)) = (~O(l),~. I oo(no), . . . ,0,(l),

ok < pa, 5 (Yk+i, for all i, we obtain:
C(W)

5

C(W-

5

c[W-

&I

hdw

5

c[W-

G]

log

5

CwlOgw$

5

cWlogW

&)

+

ak+l

C(ni-1

+

ni)

-

&]

+

2ak+ln

+

hk+ln

. . , a,(n,))

This algorithm runs in polynomial time.
’ We use a charging scheme to account for the length
of an edge e in the linear arrangement for G obtained by
our algorithm (note that we will account for the length
rather than for
of the edge in the linear arrangement,
the spreading metric length of the edge). If some edge
e in level ai belongs to some other level of index k,
say level aj, then this edge also belongs to every level
of index k between ai and aj. W.1.o.g. assume that
i < j. Edge e will be “stretched over” all the nodes in
Hi U . U Hj-1, and may be “stretched over” some of
the nodes in Hi-1 and Hj, in the linear arrangement
produced by our algorithm. Hence the length of such
an edge in the final linear arrangement will be at most
ni-1 + . . + nj. Suppose we charge nP-i + np for the
portion of the edge that is stretched over the nodes in
HP-1UH,, when considering level ap, for allp in [i,j+l].
Then the total charge associated with edge e is equal to
ni-1 + 2(ni + . . . nj-l)+nj
>ni-l+...+nj:
Thatis,
.edge e will be charged at least as much as its length in
a final linear arrangement.
We will now compute an upper bound on the cost
of a linear arrangement obtained by our algorithm. Let
C(Z) be the maximum cost of a linear arrangement
obtained by our algorithm for a subgraph of G whose
volume of the spreading metric ! is at most 2. Since the
sum of the weights of all edges in level ai is pa,, and since
we charge for the length of an edge as described in the
preceding paragraph, we derive the following recurrence
relation for C(W):

CYk+ln[:!-

W

c

8logW

1%WI

for a sufficiently large constant c. The second inequality follows from the induction hypothesis; the fourth
inequality follows Since ok+1 = 2ok.

We still need to show how to bring the approximation factor down from O(log W) to O(log n). The basic
ideas used here follow [3].
Our goal will be to reduce, by resealing and rounding down weights, our original input graph G to an
“equivalent” input graph G’ whose spreading metric volume is a polynomial in n. Consider the set E’ of edges e
such that ul(e) 5 W/(mn).
Since an edge has length at
most n in any linear arrangement for G, the contribution
of the edges in E’ to a MLA of G is at most W. Suppose
we delete all those edges, and apply a papproximation
algorithm to the resulting graph: We thus obtain a linear arrangement of G - by simply adding those edges
back into the linear arrangement found - with cost that
is within a (p + 1) factor of the cost of a MLA of G.
We now round down each weight w(e), for all e in
E \ E’, to its nearest multiple of W/(mn).
The error
incurred by this rounding procedure is again at most W.
Furthermore, we scale the rounded weights by W/(mn),
obtaining new weights for the edges that are all integers
in the interval [0, mn]. Note that we have only changed
the units in which the weights are expressed. Hence
i=l
i=l
we obtain a polynomial time (p + 2))approximation
algorithm for the MLA problem on G with weights w(e),
We now show that C(W) = O(Wlogn).
Since W
by solving the MLA problem on G’ = G \ E’ with
is a lower bound on the cost of a linear arrangement of
integral weights that belong to [0, mn]. The volume
G, and since C(W) is an upper bound on the cost of a
W’ of the spreading metric for the latter problem is
MLA of G, the cost of the linear arrangement obtained
at most a polynomial in n. By Lemma 4.1, we have
is thus O(log n) times the cost of a MLA. We first prove
C(W’) 5 cW’log(W’)
= c’W’ log n, for some constant
the following lemma:
cI . Resealing the edge weights back by multiplying
LEMMA 4.1. C(W) 5 cWlogW,
for some conWV
by W/(mn), we obtain a MLA for the original
stant c.
weights on G with cost at most c’W log n.
Proof:
We will use induction on W (w.l.o.g., we
We can choose .!!* such that !* satisfies (3.1-3.2),
can assume that W is a rational number.) The base case and whose volume W’ minimizes CeEE ur(e)e(e), over
W = 0 corresponds to a totally disconnected graph (a all spreading metrics fi that satisfy (3.1-3.2).
Even,
graph with no edges); therefore C(0) = 0.
Naor, Rao, and Schieber [l] showed that W’ is a lower
Combining the recurrence relation for C(W) with
bound on the cost of a MLA. Hence, by Lemma 4.1

216
and the considerations
that follow
proved the following theorem:

this lemma,

we have

THEOREM 4.1. The cost of a solution to the MLA
problem,
obtained by our algorithm
for the spreading
metric e* on G, is within an O(logn)
factor times the
cost of a MLA of G.
5

Graphs

with

Excluded

Minors.

In this section we show how to obtain, in polynomial
time, an O(log log n)-approximation
bound for the MLA
problem on a graph G with no fixed I(,,,-minors
- e.g.,
on a planar graph G. We denote the r x r complete
bipartite
graph by Ii’,,, .
that
each
there
such
does
(i, j)
ofG.

DEFINITION
5.1. Let H and G be graphs. Suppose
(i) G contains disjoint connected subgraphs A,,, for
node w of H; and (ii) for every edge (u,u) in H,
is a path P(,,U) in G with endpoints in A, and A,,,
that any node in P(,,U) other than its endpoints
not belong to any A,, w in H, nor to any P(i,j),
in H \ (u, w). Then U,A, is said to be an H-minor

Klein, Plotkin,
and Rao [7], showed how to decompose (in polynomial
time) a graph with no 1<7,,-minors
into connected components
of small diameter.
In our
application,
this implies that each connected component has at most a constant fraction of the nodes in G,
as shown in the next section.
5.1
The Algorithm.
We recursively
solve the problem, as we do in the general case. We combine the
partial solutions returned
by each recursive step, and
charge for each edge removed at a cut step in the same
way as in the algorithm
of Section 4. It is in the way we
decompose the graph before a recursive step that the
algorithm
of Section 4 differs considerably
from the one
presented in this section. Before each recursive step, we
will perform a series of shortest path levelings, to be defined soon, on each induced connected subgraph,
until
we can guarantee that the original graph has been decomposed into subgraphs that contain at most a fixed
fraction
(strictly
less than one) of the nodes each. In
the algorithm
of Section 4, we perform only one shortest path leveling before each recursive step.
The algorithm
proceeds in rounds. In each round we
have a cut step, which corresponds to the series of cuts
performed during the round, and a recursive step, which
consists of recursing on the connected components that
result from the cut step. Let G(V, E) be a graph on
n nodes that excludes I(,,, as a minor, for some fixed
r > 0. Let J! be a spreading metric for G of volume W
that satisfies (3.1-3.2).
A cut step in G will produce a series of subgraphs of

G, G = Go,. . ., Gt , t < r, where each Gi+i results from
a shortest path levelling of Gi. Fix a node v in Gi. A
shortest path leveling (SPL) of Gi rooted at v consists
of an assignment of levels to the edges of Gi as follows:
An edge (z, y) is at level j of this SPL iff dist(w,x)
(in
Gi) is at most j and dist(v, y) (in Gi) is greater than j,
for all j E N. (An edge may be at more than one level.)
We will cut along a sequence of levels of the SPL;
one of the connected components resulting from this cut
procedure will be Gi+l.
Let n(Gi) denote the number
of nodes in Gi. Let s = n/b, where b is a constant
to be specified later.
The spreading
metric diameter
guarantee implies that this SPL has at least n(Gi)/4
levels. We will see later that n(Gi) = e(n), and that
we can choose b such that n(Gi)/4
2 2s (we need
b 2 8). We group the levels of this SPL into bands of
2s consecutive levels as follows: Alternate
coloring the
bands “blue” and “red”, in increasing order of the levels.
W.l.o.g., assume that the subgraph induced by the blue
bands has at least n(Gi)/2
nodes. We have 2s cuts of
the following type: For 0 5 j 5 2s - 1, a leveled cut j
consists of all the edges in the jth level (with respect to
distance from w) of every red band. That is, if the band
consisting of the first 2s levels is colored blue, then the
leveled cut j consists of the levels 2s + j, 6s + j, . . ., for
all j.
Now we group the leveled cuts according to their indices. Let ,8k = W2k/(slog7t),
for all k in [l, 2loglogn].
Let ,8e = 0 and P(siogiogn) = W. The weight of leveled cut j is the sum of the weights of the levels in the
cut (the weight of a level being the sum of the weights
of the edges at that level).
Leveled cut j has indee
k, k in [2 log log n], iff the weight of cut j belongs to
the interval Ik = (@k, ,&+I].
There must exist at least
as/(2 log logn) leveled cuts with same index ki (since
there exist at least 2s distinct leveled cuts).
If ki > 0, then we cut along these at least
s/(log log n) leveled cuts of index ki, and recurse on the
resulting
connected components.
In this case, we let
t = i, and the cut step of this round is complete. Otherwise, we first cut along only one of the leveled cuts
of index ki = 0 (chosen arbitrarily).
Then we check
whether there exists a connected component
Gi+r of Gi
nodes. In case no such compowith more than n(Gi)/2
nent exists, we let 2 = i (the cut step of this round is
complete),
and we recurse. If i = r - 1, we also recurse.
Otherwise,
we proceed by performing
a SPL on Gi+i,
following the procedure just described, with i = i + 1.
The number of nodes in Gi, n(G;), is proportional
to n, for all i in [r].
This follows since n(Gi+i)
2
n(Gi)/2,
by the choice of Gi+i, and since r is a constant.
Suppose we just performed
a series of r SPL’s and
corresponding
cut procedures.
The last cut performed,

217
on G+r,
generated
a collection
of connected components of G,-1. Klein, Plotkin, and Rao [7] proved that
the distance in G between any pair of nodes in any such
component
is O(s) (where the constant in the O(.) notation depends only on r). Thus, for a suitably chosen
constant b, we can ensure that the distance between any
pair of nodes is at most n/6, in any such component.
It follows from the result by Klein, Plotkin, and Rao
that any connected component that results from this cut
step has at most 2n/3 nodes, as we now show. Fix any
node u in G. It follows from (3.1), that any subgraph
of G on (n - Z) nodes that contains u has a node at
distance at least (n - x)/4 from u. Suppose we start
with the graph G, and proceed by removing one node
at a time, choosing always a node that has maximum
distance to u among the remaining
nodes. Thus, we
need to remove at least one-third
of the nodes before
we are left only with nodes that are within distance n/6
from u in G. This implies that any resulting connected
component of G,-1 has at most 2n/3 nodes. Any other
resulting connected component (of G\G,.-1)
has at most
n/2 nodes, by the choice of the Gi’s.
We distinguish
between two types of cut steps: if kt
is equal to 0, then we have a cut step of type I in this
round; otherwise
kt is not 0, and the cut step in this
round is of type II. Note that kt = 0 implies kj = 0, for
all j in [t].
Let C(Z, Z) denote the maximum
cost of a linear
arrangement
obtained by our algorithm
for a subgraph
of G with z nodes, whose volume of the spreading metric
L is at most 2.

LEMMA 5.1. C(W,n)
constant

<

cWloglogn,

for

some

c.

Proof:
We use induction on W and n (w.l.o.g., assume
that W is a rational number).
The base cases for W = 0
or n = 0 are trivial.
Suppose we perform a cut step of
type I. Let the connected
components
resulting
from
this step be He,. . . , Hp. Then:

inequality
follows. The last inequality
follows for a sufficiently large constant c.
If the cut step performed
was of type II, then we
performed
a series of t 5 T SPL’s and respective cut
procedures.
The last term on the right-hand
side of
the first inequality
below accounts for the first (t - 1)th
leveled cuts used. The second term on the right-hand
side of that inequality
accounts for the tth leveled cut
used. The charging scheme for the edges removed in
the tth leveled cut of this cut step is analogous to the
scheme presented in Section 4.

C(W,n) 2 C(W- J&L

log log n ’

+(r

2

gC(W,ni)
i=o

5

C

+r-$&n

cWi log log(2n/3)

cWloglogn-

5

cWloglogn

CW

-+3logn

pks
-,n)+(f-+3)Pkn

5

C(W-

5

cw-

5

cwloglogn+,dkn(r+3-

5

cw log log 72

log log n

&)

log

log 12 + (r -k 3)pkn

c log log n
b log log n )

As in Section 4, we choose a spreading
metric e*
that satisfies (3.1-3.2), and whose volume W’ is a lower
bound on the cost of a MLA of G. By Lemma 5.1, we
obtain the following theorem:

THEOREM 5.1. Given a graph G on n nodes that
excludes fixed I<,,, - minors, the cost of a solution to the
MLA problem, obtained by the algorithm
presented in
this section for the spreading metric l’, is within an
O(loglogn)
f ac t or t imes the cost of a MLA of G.
Approximating

Storage-Time

Product.

In this section, we sketch an approach to approximating
the storage-time
product for a directed acyclic graph
+ z

i
5

1)2Wn
slogn

when c 2 b(r+ 3). Th e second inequality
above follows
from pk 2 2w/(SlOgn),
and from ,&+I = 2&, 0 < k <
210glogn - 1.
w

6
C(W,n)

-

n) + Wk+1n

2brW
log n

where Wi and ni are the volume and number of nodes,
respectively,
associated with component
Hi. We have
shown that every ni is at most 2n/3.
The second inequality
above follows by induction.
Note that
Ioglog(2n/3)
5 log log n - l/(3 logn):
Thus the third

GP’, E).
We use a spreading metric defined as follows. Let
ER = {(u, w)l(v, u) E E}. We define G’ = (V, E U ER).
Let V denote the set of all nontrivial
strongly connected
subgraphs
of G’.
Find e : E -+ Q that minimizes
while satisfying the constraints
W = LEE
w(eMe),

c ucu r(uP(u, n) > CUEUr(u) ) VUEV,VVEU
WI

.e(i,j)

2

I

4
+~(j),

V(i,j)

EE

218
where 6(u, v) is equal to (dist(u, v)+dist(v,
u)). Here we
define dist(u, V) to be the length of a shortest path from
u to w in G’ according to the lengths k(e) for e E E, and
where each e E ER has length 0. As shown in [l], the
value of IV is a lower bound on the optimal cost of a
solution to the storage-time
product problem.
Given the spreading
metric constraints
above we
can apply the algorithm
of Section 4 to this problem as
follows.
Let T = CvEV r(w). There is a node v such
that the out-tree, or the in-tree rooted at v has depth
R(T).
Thus, we can find a sequence of K = Q(T/ log W)
levels (as defined in Section 4) whose weights pal, . ..pa.
are within a factor of two of each other.
Laying out the resulting
pieces successively,
we
obtain a solution where the cost is bounded by

PI G.

Even, J. Naor, S. Rao, and B. Schieber.
Spreading metric based approximate
graph partitioning
algorithms.
In Proceedin.gs of the 8th Annual
ACMSIAM Symposium
on Discrete
Algorithms,
pages 639648, January
1997.
Ap[31 G. Even, 3. Naor, B. Schieber, and M. Sudan.

proximating

PI

i=l

i=l

where ri is the sum of r(w) over all nodes w that
lie between levels aiand ai (~0 and rK are defined
accordingly).
This
recursion
can be upper
bounded
by
0( W log I&‘), as in Section 4. This cost can be reduced
to 0( W log T) using the standard techniques that were
used in Section 4 to reduce O(logW)
to O(logn).
7

Conclusion.

We provided
an existentially
tight bound on the relationship between the spreading metric cost volumes and
the true optimal values for the problems of minimum linear arrangement,
minimum
containing
interval graph,
and minimum
storage-time
product.
It would be interesting
to extend our techniques to
obtain O(logn)
approximation
factors for other problems.
In particular,
it seems natural
to extend our
techniques
to improve the best known approximation
factors for other problems that satisfy the “approximation paradigm”
of [l]. We would then provide an existentially
tight bound - on the ratio between the value
of an optimal solution and the spreading metric volume
- for any such problem.
However, since the approach used here depends on
the structure of graph ordering problems, new ideas may
be required.
References
[l]

G. Even, J. Naor, S. Rao, and B. Schieber.
Divideand-conquer
approximation
algorithms
via spreading
metrics.
In Proceedings
of the 36th Annual Symposium
on Foundations
of Computer
Science, pages 62-71,
October
1995.

minimum

feedback sets and multicuts

in

directed graphs. In 4th IPCO, pages 14-28, 1995. Full
version appears in IBM Research
Report
RC 20074
(88796).
and D. S. Johnson.
Computers
[41 M. R. Garey
and Intractability:
A Guide to the Theory
of NPCompleteness.
Freeman, NY, 1979.
Approximation
algorithms
for geometric
[51 M. Hansen.
embeddings
in the plane with applications
to parallel
processing
problems.
In Proceedings
of the 30th Annual Symposium
on Foundations
of Computer
Science,
pages 604-609, October
1989.
D. G. Kendall.
Incidence matrices, interval graphs, and
seriation
in archeology.
Pacific
J. Math., 28:565-570,
1969.
and S. Rao.
Excluded
minors,
[71 P. Klein, S. Plotkin,
network
decomposition
and multicommodity
flow.
In
Proceedings
of the 25th Annual
ACM Symposium
on
the Theory
of Computing,
pages 682-690,
October
1993.
R. Koch, T. Leighton,
B. Maggs, S. Rao, and A. Rosenberg. Work-preserving
emulations
of fixed-connection
networks.
In Proceedings
of the 21st Annual
ACM
Symposium
on Theory of Computing,
pages 227-240,
May 1989.
F. T. Leighton
and S. Rao.
An approximate
maxflow min-cut
theorem
for uniform
multicommodity
flow problems with applications
to approximation
algorithms.
In Proceedings
of the 29th Annual
Symposium
on Foundations
of Computer
Science, pages 422-431.
IEEE Computer
Society Press, October
1988.
J. Meidanis
and J. C. Setubal.
Introduction
to Computational
Molecular
Biology.
PWS Publishing
Co.,
Boston, MA, 1997.
G. Ramalingam
and C. Pandu Rangan.
A unified
approach
to domination
problems
in interval
graphs.
Information
Processing
Letters, 27:271-174,
1988.
R. Ravi, A. Agrawal,
and P. Klein. Ordering
problems
approximated:
single processor
scheduling
and interval graph completion.
In Proceedings
of the 18th International
Colloquium
on Automata,
Languages
and
Programming,
pages 751-762, July 1991.
Packing directed
circuits
fractionally.
1131 P. D. Seymour.
Combinatorics,
15(2):281-288,
1995.

PI

PI

[lOI
WI

P21

A Distributed Polylogarithmic Time Algorithm for
Self-Stabilizing Skip Graphs
∗

Riko Jacob

Andrea Richa

Dept. Computer Science
Technische Universität
München
D-85748 Garching bei
München, Germany

Department of Computer
Science and Engineering
Arizona State University
Tempe, Arizona
AZ 85287-8809, USA

aricha@asu.edu
jacob@in.tum.de
Christian Scheideler
Stefan Schmid
Hanjo Täubig
Department of
Computer Science
University of Paderborn
D-33102 Paderborn
Germany

Dept. Computer Science
Technische Universität
München
D-85748 Garching bei
München, Germany

Dept. Computer Science
Technische Universität
München
D-85748 Garching bei
München, Germany

scheideler@upb.de

schmiste@in.tum.de

taeubig@in.tum.de

ABSTRACT

make use of peer-to-peer technology, including file sharing, streaming and gaming tools. A distinguishing feature of these networks
is that they typically have an open clientele, allowing machines to
join and leave at any time and concurrently. If no countermeasures
are taken, the dynamic membership changes can degenerate the network, rendering central operations such as routing inefficient. In
an effort to gain deeper insights into (and deal with) these dynamics, researchers have studied different approaches. In theory, the
dominating approach has been to make sure that an overlay network
never leaves a certain set of legal states so that at any time, information can be efficiently exchanged between its members. This is
mostly achieved through redundancy in the overlay network topology, which can significantly increase its maintenance overhead, so
the rate of changes such networks can sustain might be rather limited. However, a high change rate can happen due to heavy churn
or join-leave attacks. Also, partitions of the underlying physical
network or DoS attacks may push the overlay network into some
illegal state. In this case, the overlay network may get into a state
in which it is highly vulnerable to further changes, so proper recovery mechanisms are needed to get it back into a legal state as
quickly as possible. Some results are known in this direction, but
most of the proposed protocols only manage to recover the network
from a restricted class of illegal states (e.g., [2, 5, 34]). Those few
results known for truly self-stabilizing networks either just show
eventual self-stabilization (e.g., [12]) or do not provide sublinear
bounds on the convergence time (e.g., [19, 31]). Our work is the
first that demonstrates that sublinear, in fact, polylogarithmic recovery time is possible. More precisely, we present a self-stabilizing
algorithm for a proper extension of the skip graph [3] (as the original skip graph is not locally checkable). Skip graphs are very useful
for scalable overlay networks. They have logarithmic diameter and
degree and constant expansion w.h.p. [4]. Also, like in hypercubic
networks, no extra routing tables have to be maintained for fast, low
congestion routing. Before we delve into the details of our solution,
we discuss related work and present our model.

Peer-to-peer systems rely on scalable overlay networks that enable
efficient routing between its members. Hypercubic topologies facilitate such operations while each node only needs to connect to
a small number of other nodes. In contrast to static communication
networks, peer-to-peer networks allow nodes to adapt their neighbor
set over time in order to react to join and leave events and failures.
This paper shows how to maintain such networks in a robust manner.
Concretely, we present a distributed and self-stabilizing algorithm
that constructs a (variant of the) skip graph in polylogarithmic time
from any initial state in which the overlay network is still weakly
connected. This is an exponential improvement compared to previously known self-stabilizing algorithms for overlay networks. In
addition, individual joins and leaves are handled locally and require
little work.

Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed Systems; F.2.2 [Analysis of Algorithms and Problem Complexity]:
Nonnumerical Algorithms and Problems

General Terms
Algorithms, Theory

1.

INTRODUCTION

Peer-to-peer computing is one of the most intriguing networking paradigms of the last decade. Numerous Internet applications
∗

This work was supported by NSF award CCF-0830704 and by the
DFG-Project SCHE 1592/1-1.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’09, August 10–12, 2009, Calgary, Alberta, Canada.
Copyright 2009 ACM 978-1-60558-396-9/09/08 ...$10.00.

1.1

Related Work

There is a large body of literature on how to maintain peer-topeer networks efficiently, e.g., [3, 8, 10, 18, 22, 28, 30, 32, 34].
Recently, the first structured overlay networks have also found their
way into the practical world; for instance, the Kademlia [29] over-

131

lay is used in the popular Kad network which can be accessed with
eMule clients. An interesting and flexible overlay structure are skip
graphs [3, 7, 20, 21, 22]. These networks are based on the classical skip list data structure and allow for efficient, low-congestion
routing while requiring a small node degree only. Due to the typically very dynamic nature of peer-to-peer systems, there is a need to
maintain the overlay topology or—in case of catastrophic events—
recover it from arbitrary connected states. While many results are
already known on how to keep an overlay network in a legal state,
not much is known about self-stabilizing overlay networks.
In the field of self-stabilization, researchers are interested in algorithms that are guaranteed to eventually converge to a desirable system state from any initial configuration. The idea of selfstabilization in distributed computing first appeared in a classical
paper by E.W. Dijkstra in 1974 [15] in which he looked at the problem of self-stabilization in a token ring. Since Dijkstra’s paper, selfstabilization has been studied in many contexts, including communication protocols, graph theory problems, termination detection,
clock synchronization, and fault containment. For a survey see, e.g.,
[11, 16, 24].
Also general techniques for self-stabilization have been considered. Awerbuch and Varghese [9] showed that every local algorithm
can be made self-stabilizing if all nodes keep a log of the state transitions until the current state. Since then several other methods have
emerged including various local and global checking and correction techniques [6, 13, 25, 35, 36]. Also so-called time-adaptive
techniques [23, 26, 27] as well as local stabilizers [1] have been
presented which can recover any distributed algorithm in O(f ) time
depending only on the number f of faults. This, however, does not
hold any more if faults include changes in the topology. In this case,
a single fault may require the involvement of all nodes in the system and is therefore quite expensive to fix. Thus, people have also
looked at so-called superstabilizing protocols, which are protocols
that can handle a single topology change as well as arbitrary state
faults with low overhead (e.g., [17]).
Interestingly, though much attention has been given to selfstabilizing distributed computing, even in the context of dynamic
networks, the problem of designing self-stabilizing networks has
only been given very little attention. The general techniques mentioned above are not applicable here as they have not been designed
to actively perform local topology changes (network changes are
only considered as faults or dynamics not under the control of the
algorithm). Even though logging techniques such as [9] to convert
non-self-stabilizing algorithms into self-stabilizing algorithms can
also be applied to self-stabilizing networks, they usually need some
non-local knowledge of the network (such as its size) to bound the
state space which can make self-stabilization very expensive. Our
goal instead was to find dedicated, much more light-weight algorithms for self-stabilizing networks.
Some preliminary work in this direction has already been done.
In the technical report of the Chord network [34], protocols are described which allow the topology to recover from certain degenerate
states. Similarly, it is also known how to repair skip graphs from certain degenerate states [3] but the problem of recovering them from
an arbitrary connected state has remained open. This is not surprising as the neighborhood information alone is not sufficient for
the Chord network as well as skip graphs to locally verify the correctness of the topology. Hence, additional information would be
needed, which significantly complicates the self-stabilization process.
In order to recover scalable overlays from any initial graph, researchers have started with simple non-scalable line and ring networks. The Iterative Successor Pointer Rewiring Protocol [14] and
the Ring Network [33] organize the nodes in a sorted ring. However, the runtime of both protocols is rather large. Aspnes et al. [2]
describe an asynchronous protocol which turns an initially weakly

connected graph into a sorted list. Unfortunately, their algorithm is
not self-stabilizing. In a follow-up paper [5], a self-stabilizing algorithm is given for the case that nodes initially have out-degree 1.
In [31], Onus et al. present a local-control strategy called linearization for converting an arbitrary connected graph into a sorted list.
However, the algorithm is only studied in a synchronous environment, and the strategy may need a linear number of communication
rounds. Clouser et al. [12] formulated a variant of the linearization technique for asynchronous systems in order to design a selfstabilizing skip list. Gall et al. [19] combined the ideas from [12,
31] and introduced a model that captures the parallel time complexity of a distributed execution that avoids bottlenecks and contention.
Two algorithms are presented together with an analysis of their distributed runtime in different settings. No sublinear time bounds are
shown there either.
To the best of our knowledge, this is the first paper to describe
a self-stabilizing algorithm for a scalable overlay network (in our
case, skip graphs) in sublinear time. In fact, the skip graph construction terminates in a polylogarithmic number of communication
rounds. In addition to being able to recover quickly from an arbitrary connected state, we also show that when the network forms the
desired topology, our algorithm efficiently supports join and leave
events, which incur only a polylogarithmic amount of work to fix.
This means (in contrast to considering a completely new starting
situation and recovering the structure in polylogarithmic time) that
only a small part of the nodes are involved in repairing the overlay
topology.

1.2

Model

We represent an overlay network as a directed graph G = (V, E),
where |V | = n. Each node is assumed to have a unique identifier
(or short: ID) v.id ∈ U that is immutable, where U is the (ordered) universe of all identifiers. At any time, each node can inspect its own state and the state of its current neighbors. Beyond
that, a node does not know anything, including the current size n
of the overlay network. Only local topology changes are allowed,
i.e., a node may decide to cut a link to a neighbor or ask two of its
neighbors to establish a link. The view and the influence of a node
are essentially local. The decisions to cut or establish links are controlled through actions (which we will also call rules). An action has
the form label : guard → commands where guard is a Boolean
predicate over the state of the executing node and its neighbors and
commands is a sequence of commands that may affect the state of
the executing node or request a new edge between two neighbors.
This is done via an insert(v, w) request by which a node asks its
neighbor v to establish an edge to neighbor w. An action is called
enabled if and only if its guard is true.
For simplicity, we assume that time proceeds in rounds, and all
requests generated in round i are delivered simultaneously at the
beginning of round i + 1. In other words, we assume the standard
synchronous message-passing model with the restriction that a node
can only communicate with nodes that it has currently links to. In
each round, all actions that are enabled are executed by the nodes.
If two actions executed by the same node are in conflict with each
other, any one of them may win and the other is discarded. Our goal
is to minimize the number of rounds needed in the worst case (over
all initial states in which the network is weakly connected) until the
overlay network has reached its desired structure. We make this a
bit more precise by defining what we mean by self-stabilization.
When using the synchronous message-passing model, the global
state of the system at the beginning of each round is well-defined.
A computation is a sequence of states such that for each state si at
the beginning of round i, the next state si+1 is obtained after executing all actions that were fired in round i. In our context, we call
a distributed algorithm self-stabilizing if from any initial state (from
which a legal state is still reachable) it eventually reaches a legal

132

state in which no more actions are enabled, i.e., once the overlay
network reaches its desired topology, it does not change anymore.
Our goal will be to find a self-stabilizing algorithm that needs as few
rounds as possible for this.

1.3

fore, we propose a slight extension of the skip graph that we call
SKIP+ .
The definition of SKIP+ requires us to also define (extended)
predecessors and successors on level i with a specific value in the
next bit. For any i ≥ 0 and x ∈ {0, 1} define pred∗i (v, x) =
pred(v, {w | pre i+1 (w) = pre i (v)◦x}) and similarly succ∗i (v, x)
(where operator ◦ means concatenation of bit strings). Let
low ∗i (v) = min{pred∗i (v, 0).id , pred∗i (v, 1).id } and high ∗i (v) =
max{succ∗i (v, 0).id , succ∗i (v, 1).id }, and let v.range∗ [i] ⊆ U be
defined as [low ∗i (v), high ∗i (v)]. With this, the SKIP+ graph has the
neighbor set Ni∗ (v) of v at level i as the set of all nodes w with
pre i (w) = pre i (v) and w.id ∈ v.range∗ [i].

Our Contributions

We present a variant of the skip graph, called SKIP+ , that can be
locally checked for the correct structure. For this graph, we present
a distributed self-stabilizing algorithm that arrives at SKIP+ for any
initial state in which the nodes are weakly connected in O(log2 n)
rounds. This is an exponential improvement over all previous results
on the number of communication rounds needed to arrive at a scalable overlay network. We also show that a single join event (i.e., a
new node connects to an arbitrary node in the system) or leave event
(i.e., a node just leaves without prior notice) can be handled by our
algorithm with polylogarithmic work, demonstrating that our algorithm is not just useful for the worst case but also for the case where
the overlay network is already forming the desired topology (which
is the standard case in the literature).

1.4

D EFINITION 2.2 (SKIP+ G RAPH ). Assume we are given a set
of nodes together with associated IDs and random strings. In the
corresponding SKIP+ graph every node v is connected
to exactly
S
the nodes in Ni∗ (v) for all i ≥ 0, i.e., N (v) = i≥0 Ni∗ (v).
Figure 1 illustrates the connections in SKIP+ . The white (resp.
black) nodes in the figure illustrate the nodes v at level i for which
v.rs[i + 1] = 0 (resp. v.rs[i + 1] = 1). The total sorted order of
the nodes according to their identifiers is shown at the bottom; the
SKIP+ structure at level 0 is depicted in the middle, whereas the
top part of the figure (top two connected components of black and
white nodes) correspond to level 1. Note that skip graph edges of
level i + 1 appear in the SKIP+ graph already on level i.

Paper Organization

In the rest of this paper we present and analyze our self-stabilizing
algorithm for SKIP+ graphs. The paper ends with a conclusion.

2.

ALGORITHM

We first introduce the skip graph SKIP+ we want to construct and
then present our algorithm ALG+ .

2.1

rs=11..
rs=10..

i=1

The SKIP+ Graph

rs=01..
rs=00..

We start with the definition of the skip graph. In skip graphs, the
identity of a node v consists of two components: v.id , a unique but
otherwise arbitrarily chosen identifier, and v.rs, a (pseudo-)random
bit string of sufficient length that was uniformly chosen at random
when the node entered the system. Both parts are assumed to be
immutable.
For a node v and a subset W ⊆ V of nodes define the predecessor
of v in W pred(v, W ) to be the node u ∈ W such that u.id =
max{w.id | w ∈ W and w.id < v.id }. By the assumption that no
two nodes share the same id , this is well defined. If such a u does
not exist, set pred(v, W ) := ⊥ and define ⊥.id = −∞. Similarly
define the successor of v in W succ(v, W ) to be u ∈ W such that
u.id = min{w.id | w ∈ W and w.id > v.id }, or if this does not
exist succ(v, U ) := > and define >.id = ∞. Here, −∞ and +∞
are resp. the lowest and largest elements in the identifier space U
not allowed as identifiers of real nodes.
The definitions needed for the ideal skip graph are marked by a
superscript ∗ to distinguish them from analogous definitions used in
the algorithm, which are all based on the current local views of the
nodes.
For any i ≥ 0, let pre i (v) denote the first i bits of v.rs (i.e.,
the prefix of v.rs of length i) and v.rs[i] represent the ith bit of
v.rs. Now define the level-i predecessor of v by pred∗i (v) :=
pred(v, {w | pre i (w) = pre i (v)}), and the level-i successor of v
by succ∗i (v) := succ(v, {w | pre i (w) = pre i (v)}).

Since the skip graph is a subgraph of SKIP+ (on the same set of
nodes), SKIP+ has a logarithmic diameter and constant expansion.
Also, it is not too difficult to see that the maximum degree remains
logarithmic w.h.p.

D EFINITION 2.1 (S KIP G RAPH ). Assume we are given a set
of nodes together with associated IDs and random strings. In the
corresponding skip graph, every node v is connected exactly to
pred∗i (v) and succ∗i (v) for every i ≥ 0 (except for the case of ⊥
and >).

2.2

rs=1...

i=0
rs=0...

Figure 1: Visualization of SKIP+ connections.

D EFINITION 2.3 (H EIGHT H ). The height HG of a SKIP+
graph G is defined as the maximal number of non-trivial levels. A
non-trivial level is a level which consists of more than one node,
that is, HG = max{|ρ| : |Vρ | ≥ 2} where Vρ = {v ∈ V | ρ =
pre |ρ| (v)}.
It is straightforward to show that HSKIP+ is in O(log n) w.h.p.
The goal is to establish the SKIP+ graph as the target topology
using bi-directed edges (this is why the edges in Figure 1 are undirected), but during the construction, the network has to deal with
directed edges.

The ALG+ Algorithm

In the description and analysis of our algorithm, we will make use
of the following definitions.
D EFINITION 2.4 (G RAPH Gρ ). Given any directed graph
G = (V, E) currently formed by the nodes and any prefix ρ, the
graph Gρ = (Vρ , Eρ ) is a subgraph of G where Eρ = {(u, v) ∈
E : u, v ∈ Vρ }. The nodes in Vρ are called ρ-nodes and the edges
in Eρ are called ρ-edges.

Given unique node identifiers, the skip graph is uniquely defined.
It is not difficult to see that the skip graph has logarithmic diameter
and maximum degree and allows hypercubic-style routing between
any pair of nodes in O(log n) time, w.h.p. However, the nodes cannot locally verify the correctness of the skip graph topology. There-

133

D EFINITION 2.5 (C ONNECTED ρ-C OMPONENT ). Given a
prefix ρ, we will refer to a weakly connected component of nodes
in Gρ as a connected ρ-component. A pair of nodes in such a
component is called ρ-connected.

or its neighbors. Recall that we assume the synchronous messagepassing model. At the beginning of a round i, every node receives
all the requests to establish an edge that were generated in the previous round. After a preprocessing step in which each node updates
its neighborhood and the state information about its neighborhood,
a set of three types of actions is processed, which we also call rules
here. For readability, we will present the rules in words, but transforming them into the formal terminology of our model is straightforward. We note that the preprocessing step is separated from the
actions to ensure a deterministic state transition in the synchronous
message-passing model. In an asynchronous model, the preprocessing step would be continuously performed in the background at any
time. For each node u we do the following in a round:

For any node v let N (v) be its current outgoing neighborhood and v.range[i] be its current range at level i, which is
based on its current view of predi (v, x) and succi (v, x), where
predi (v, x) is the node such that predi (v, x).id = max{w.id |
w ∈ N (v) and w.id < v.id and pre i+1 (w) = pre i (v) ◦ x} and
succi (v, x) is the node such that succi (v, x).id = min{w.id |
w ∈ N (v) and w.id > v.id and pre i+1 (w) = pre i (v) ◦ x}. For
each level i, v.range[i] ⊇ v.range∗ [i], i.e, the current range will
always be a superset of the desired range in the target topology (as
defined in SKIP+ ). We will see that as long as no faults or adversarial changes happen during the self-stabilization process, ALG+
monotonically converges to the desired ranges for every i.
ALG+ distinguishes between stable edges and temporary edges.
Node v considers an edge (v, w) to be temporary if from v’s point of
view (v, w) does not belong to SKIP+ and so v will try to forward it
to some of its neighbors for which the edge would be more relevant.
Otherwise, v considers (v, w) to be a stable edge and will make
sure that the connection is bi-directed, i.e., it will propose (w, v)
to w. There is a binary flag v.F (w) for each neighbor w that states
whether the edge to it is stable. The flag turns out to be important
when a stable edge destabilizes (i.e., converts into a temporary edge)
because this triggers the introduction of several temporary edges
that are needed for our proofs to go through. The other conversion,
from temporary to stable, essentially boils down to introducing also
the other direction of the edge. More details will be given later.
The intuition behind the ALG+ algorithm is as follows. The
algorithm has two main phases: The first phase proceeds in a
bottom-up (i.e., from level 0 upwards) fashion, forming connected
ρ-components for every prefix ρ. This will be accomplished by letting each node v find another node w of the opposite color, i.e., such
that pre i (w) = pre i (v) and v.rs[i + 1] 6= w.rs[i + 1] for all levels
i ≥ 0 (we will call w a buddy of v). We can show that once all
nodes in Vρ have formed a single connected component and have
found a buddy, then connecting all nodes which are at most three
hops away in the ρ-component results in a single connected ρ0- and
ρ1-component. This will be accomplished by Rules 1 (where new
nodes in the range of a node are discovered and where ranges may
be refined) and Rules 3 (where an efficient variation of a local transitive closure is performed) below.
Once the connected ρ-components are formed, the second phase
of the algorithm will form a sorted list out of each ρ-component.
This is accomplished in a top-down fashion by merging the two
already sorted ρ0- and ρ1-components into a sorted ρ-component
until all nodes in the bottom level form a sorted list.
Of course, this “division into phases”-intuition oversimplifies
what is really going on in our algorithm. Whereas for the sake of
simplicity, we can think of the execution of the phases of the algorithm as being perfectly synchronized, with all nodes waiting for the
connected components at level i to converge before the components
at level i+1 are formed in the first phase, and with the sorting of the
components at level i only starting after we have successfully sorted
the components at level i+1 in the second phase (and of course with
the second phase only kicking in after the first phase is completed),
all actions in our algorithm may be enabled at any time, causing the
phases to be intertwined in the real execution. Hence, the main challenge in this paper is to show that nevertheless the actions transform
any initially connected graph into SKIP+ in O(log2 n) rounds.
Now we are ready to present ALG+ . The local state of a node v
consists of v.id , v.rs, N (v) (which imply the edges and ranges of v)
and its flags. We assume that every node v knows (besides its own
local state) the current local state of all nodes w ∈ N (v). Hence, the
actions of a node v may be based on any local state information of v

Preprocessing.
First, a node u processes all insert(u, v) requests from the previous round where v ∈ V \ ({u} ∪ N (u)) (the others would be
thrown away, but our algorithm avoids issuing such requests in the
first place). This is done by adding v to N (u) and setting its flag
u.F (v) to 0 (temporary). Then u makes sure that its state is valid,
i.e., the flags carry binary values and N (u) is a set of nodes v 6= u
that are all alive (otherwise, u removes that node from N (u)). Now
u determines for every i its current predecessors predi (u, 0) and
predi (u, 1) and its current successors succi (u, 0) and succi (u, 1)
(within N (u)). This allows u to update its range information. The
updated local state is exchanged between the nodes so that the rules
below are based on up-to-date information.
D EFINITION 2.6
sidered stable, if

(S TABLE E DGES ). Every edge (u, v) is con-

• its endpoints mutually fall on each other’s range at some level
pre i (u) = pre i (v), i.e., v.id ∈ u.range[i] and u.id ∈
v.range[i], for some i. In this case (u, v) is defined to be
stable on all levels j ≥ i with pre j (u) = pre j (v). Or
• v = predi (u, x) or u = predi (v, x), or v = succi (u, x) or
u = succi (v, x), for some level i and bit x ∈ {0, 1}. In this
case (u, v) is stable only on level i.
This second kind of stable edges is needed to stay in touch with
a “buddy” (see below), in order to forward temporary edges. Our
algorithm guarantees that once a node has a buddy to the left (or to
the right), it will always have such a buddy in the future.
D EFINITION 2.7 (L EVEL OF T EMPORARY E DGE ). We define
the level of a temporary edge (u, v) as the length of the longest
common prefix of u and v.
All of the rules below are only activated if the resulting action
changes the graph or the state, i.e., if the to be inserted edge does
not already exist or the flag changes its value.

Rule 1a: Create Reverse Edges.
For every stable edge (u, v), u sets F (v) = 1 (if it has not already
done so) and initiates an insert(v, u) request.

Rule 1b and Rule 1c: Introduce Stable Edges.
For every stable neighbor v (the edge (u, v) is considered stable
as defined in Preprocessing) of a node u, for every i ≥ 0 and every
node w ∈ N (u), w 6= v with pre i (v) = pre i (w) and w.id ∈
v.range[i], node u initiates insert(v, w) (Rule 1b) and insert(w, v)
(Rule 1c).

134

Rule 2: Forward Temporary Edges.

L EMMA 3.4. Assume that u and v are σ-V-linked via w at
time t. Then, at time t + 1 the nodes u and v are σ-connected,
i.e., in the same connected component of Gσ .

Every temporary edge (u, v) is forwarded to a stable neighbor
of u that has the largest common prefix with v.rs. (Such an edge
exists because otherwise (u, v) would be a stable edge.)

P ROOF. Assume (w.l.o.g.) that u, v have label σ = ρ1, and w
has label ρ0. By Rule 3b, all stable ρ1-neighbors of w are in the
same ρ1-component C at time t + 1. If (w, u) is a temporary edge,
this is forwarded to one of the stable ρ1 neighbors of w, and hence
u is also in the ρ1-component C. By the same reasoning v is also
in C at time t + 1, which proves the lemma.

Rule 3a: Introduce All.
For all nodes u ∈ V whose set of stable neighbors is different
from the previous round, u initiates insert(v, w) for all neighbors w
of u. (In particular, if an edge destabilizes, both incident nodes will
introduce their neighbors.)

L EMMA 3.5. Consider any ρ ∈ {0, 1}∗ and x ∈ {0, 1}. Let
node a with label ρx be a ρ-buddy of u at time t and b a ρ-buddy
of u at time t0 ≥ t. Then a and b are in the same ρx-component at
time t0 + 1.
P ROOF. If than t = t0 , this follows from Lemma 3.4. Otherwise
the proof is an induction over time. Let a = at , at+1 , . . . , at0 =
b be ρ-buddies of u at the respective times. With Lemma 3.1, it
suffices to show that ai−1 and ai are in the same ρx-component at
time i + 1. Because no rule deletes or forwards stable edges, the
edge (u, ai−1 ) exists at time i and Lemma 3.4 can be applied.

Rule 3b: Linearize.
For every level i, u identifies the stable neighbors v1 , . . . , vk with
v1 .id < v2 .id < . . . < vk .id that have exactly the first i bits in
common with u.rs and initiates insert(v1 , v2 ), insert(v2 , v3 ),. . .,
insert(vk−1 , vk ) for them.

3.

Note that the σ in “σ-V-linked” refers to the nodes that are linked,
not the node that is providing the link. Similarly, we define a bridge
by the prefix of the nodes that are connected by this bridge, not by
the nodes that provide the bridge.

ANALYSIS

We first analyze the bottom-up phase and then tackle the topdown phase.

3.1

D EFINITION 3.6 (T EMPORARY /S TABLE B RIDGE ). Consider
two nodes a, b with prefix σ = ρx that are in different connected
components of Gσ . Then, for k ≥ 1, a and b are connected by
a (σ, k)-bridge if there is a node c with stable edge (c, a) and a
node d with stable edge (d, b), where c, d have prefix ρx̄ and have
the edge (c, d) at level |ρ| + k (i.e., pre |ρ|+k (c) = pre |ρ|+k (d)). If
(c, d) is stable, this is a stable bridge; if (c, d) is temporary, it is a
temporary bridge.

Bottom-up Phase

L EMMA 3.1. Consider any bit string ρ ∈ {0, 1}∗ . Suppose that
nodes a and b are ρ-connected at time t0 . Then a and b are also
ρ-connected at any time t ≥ t0 .
P ROOF. We prove the lemma by induction over the time steps.
Consider any edge e = (u, v) (temporary or stable) at time t ≥ t0
with u, v ∈ Vρ . The only rule that may remove this edge is Rule 2,
all other rules only create edges. If the edge e is forwarded by Rule 2
to a node w, node w must have a shared prefix with u that extends ρ,
and all three nodes u, v, w remain connected in Gρ at the next time
step.

The following definition is needed to show connectedness moving up the levels by the rules. It is central to the bottom-up proof
that eventually for all prefixes ρ we have that Gρ consists of only
one connected component. In the process where the connectedness
of Gρ0 follows from Gρ being connected, the bridges play an important role. First, as long as a bridge is temporary, the level increases
in every step, but then, once it becomes a stable bridge, the level of
the used bridges decreases.

In the following lemma, an edge (u, v) is said to be to the right
(resp. left) if u.id < v.id (resp. u.id > v.id ).
L EMMA 3.2. Assume a node u has a stable ρ-edge to the right
(left) at time t0 . Then at any time t > t0 , node u will have a stable
ρ-edge to the right (left).

D EFINITION 3.7 ((σ, k)- PRE - COMPONENT ). Two nodes a
and b, both with prefix σ = ρx for some x ∈ {0, 1}, are (directly)
(σ, k)-pre-connected if (1) Gρ is connected, if (2) every node in Gρx
knows at least one node from Gρx and vice versa,1 and if (3) there
is an edge (irrespective of this being temporary or stable), between
a and b, or if they are σ-V-linked, or if there exists a stable (σ, k0 )bridge with k0 ≤ k between them. The transitive closure of this
(undirected) relation defines the (σ, k)-pre-component.

P ROOF. By induction over time. A stable ρ-edge (u, v) only
destabilizes because the |ρ|-range of u has become smaller, and
there now is another ρ-node w stably connected to u, and w is between u and v.
A ρ-buddy of a node u is a stable neighbor v (the edge (u, v)
exists and is considered stable) with pre |ρ| (u) = pre |ρ| (v) = ρ
and u.rs[|ρ| + 1] 6= v.rs[|ρ| + 1]. Our algorithm ensures that once
a node has a left (or right) buddy at time t, then it will also have a
left (or right) buddy at all times t0 ≥ t. In the following, the V in
V-linked does not refer to a set V but to the V-shaped situation of
two nodes being linked indirectly via precisely two edges to a third
node.

Requirements (1) and (2) will be made precise in the subsequent
analysis and especially Lemma 3.14 where the connectivity of Gρ is
proved by induction. Note that for k = 0, the σ-links and the (σ, 0)pre-component (but without stable bridges) yield the transitive hull
of the σ-V-links. Figure 2 illustrates the situation.
L EMMA 3.8. Assume nodes a, b are in the same (σ, k)-precomponent at time t0 , k ≥ 1. Then a and b are in the same (σ, k)pre-component at any time t > t0 .

D EFINITION 3.3 (σ-V- LINK ). Consider any ρ ∈ {0, 1}∗ and
x ∈ {0, 1}. Assume there are two nodes u, v with prefix ρx = σ
and one node w with prefix ρx̄. If u, v ∈ N (w), we say that u and v
are σ-V-linked via w.

1

135

Throughout this paper, · will denote the logical negation.

ρ1

ρ1τ ρ1τ

u.id ∈
/ v.range[i]. Due to the definition of the ranges, v must have
both a stable predecessor (relatively to the current neighborhood)
with last bit 0 and a predecessor with last bit 1 that lie between u
and v: w0 = predi (v, 0) and w1 = predi (v, 1). Let w ∈ {w0 , w1 }
such that pre i+1 (u) = pre i+1 (w) = pre i+1 (v). See Figure 3.

ρ0

w=w1 v

u
ρ0τ1

Figure 2: The shaded nodes belong to the same (ρ0, k)-precomponent. In this figure, |τ | = k − 1, temporary edges are
dashed and stable edges are solid.

w’
ρ0τ0

ρ1

P ROOF. Assume w.l.o.g. σ = ρ1. By induction over time, and
the definition of the pre-component being a transitive closure, it is
sufficient to argue that nodes a, b with prefix ρ1 that are directly
(σ, k)-pre-connected at time t are in the same (σ, k)-pre-component
at time t + 1.
If a, b are directly linked, we can employ Lemma 3.1, and the
case that they are σ-V-linked is due to Lemma 3.4. The remaining
(and only interesting) case is that a and b have a (σ, k)-bridge via a
stable edge e = (u, v) (then there are also stable edges (u, a) and
(v, b)) at time t. Let a0 and b0 be σ-nodes that are stable neighbors
of u and v respectively. Then a and a0 are σ-V-linked and are in the
same σ-component at time t+1 by Lemma 3.4. The same reasoning
shows that b and b0 are in the same σ-component at time t + 1.
If e remains stable, a0 and b0 have a (σ, k)-stable bridge also at
time t + 1. Otherwise e destabilizes and by Rule 3a a temporary
edge (v, a0 ) or (u, b0 ) is present at time t + 1, so a0 and b0 are σV-linked via v or u, and hence in the same (σ, k)-pre-component at
time t + 1.

w2

pj

p j+1
u.range[i+1]
v.range[i+1]

u.range[i]

v.range[i]

Figure 3: Situation for i + 1 > |σ| in proof of Lemma 3.9 at time
t. Here we use x = 0.

u

w

v

c
ρx

u

µ

v

ρx

ρx

ρx

w’

L EMMA 3.9. Assume that nodes a, b are in the same (σ, k)-precomponent at time t, k > 1. Then, a and b are σ-connected at time
t + 4.

Figure 4: The two situations for i + 1 = |σ| in proof of
Lemma 3.9.

P ROOF. If a and b are in the same (σ, k)-pre-component (assume
again that σ = ρx for some x ∈ {0, 1}), there must exist a path
a = p1
p2
...
p` = b where pj is connected to pj+1 either
directly, via a σ-V-link, or via a stable (σ, k)-bridge. We only allow
a bridge between pj and pj+1 if these two nodes are not in the same
(σ, 0)-pre-component.
Consider this path and assume pj and pj+1 are connected by a
bridge over an edge e = (u, v) of length λ (w.r.t. node identifiers, u
and v having prefixes of the form ρxτ y where |τ | = k − 1 and y ∈
{0, 1}). Then, whenever possible, we replace e by two edges via
an intermediate node w if for the lengths it holds that |(u, w)| < λ
and |(w, v)| < λ. Thus, in our path, a new node p∗ —w’s buddy—
is inserted, maintaining our “path property”. Note that this process
terminates (the number of “turning points”—the local extrema of
the identifiers along the path—does not increase).
Consider a bridge edge (u, v) on this path with shortest bridges.
First, consider the case that (u, v) is unilaterally stable because of
the nearest neighborhoods. That is, w.l.o.g. assume v is u’s nearest
neighbor but for some level, u is not in v’s range. Then, it holds that
v must know two closer nodes than u (due to the range definition),
one of which will be proposed to u in the next round (hence (u, v)
will no longer be stable), triggering Rule 3a at t + 2, from which the
claim follows.
From now on it remains to consider the case where u and v mutually include each other in their ranges on some level. First assume
that (u, v) is stable on a level > |σ|—the other case where (u, v)
is stable on level |σ| is treated later. Let us refer to the lowest level
where (u, v) is stable as i+1, that is, where pre i+1 (u) = pre i+1 (v)
but either u.id ∈
/ v.range[i] or v.id ∈
/ u.range[i] (or both, cf Definition 2.6), so |σ| + k ≥ i + 1 > |σ|. W.l.o.g., assume that

However, (w, u) 6∈ E at time t, due to our selection of the shortest bridge path: (u, w) and (w, v) would imply a path (see your
“path property” described before) with shorter bridges. Thus, at
time t + 1, (u, w) is created by Rule 1b. Now, we will show that
this triggers u to fire Rule 3a at t+2, either because (u, w) becomes
stable or because (u, v) becomes instable. (Note that (u, v) must be
stable at time t, since the edge forms a bridge.) Observe that it is
not possible that (u, v) remains stable and (u, w) is instable, since
both are within u’s range on level i + 1, as u and w have a common
prefix ρ = pre i+1 (w). Thus, at time t + 1, Rule 3a is triggered at
u, and thus, at t + 2, a and b are σ-V-linked and connected in round
t + 3.
Observe that if w has a buddy w0 closer to u, see Figure 3, the
same arguments apply, as w0 will be introduced to v, which triggers
Rule 3a at v.
It remains to study the case where (u, v) is stable on level i = |σ|
(see Figure 4). Assume u is stably connected to pj and v is stably
connected to pj+1 (u, v ∈ Gρx ). We distinguish two cases: Either
there is a node c ∈ Gρx between u and v or not. First, assume there
is such a node c, and w.l.o.g., assume c is stably connected to v on
level |ρ|. (Observe that due to our path selection, c ∈
/ Gρx , and
hence it holds that c ∈ Gρx .) We will show that in this case, a new
connection (u, c) is proposed. Thus, there cannot be both edges
(u, c) and (c, v), otherwise the bridge would not be necessary, as
the nodes must be in the same (σ, 0)-pre-component. Therefore,
either (u, v) destabilizes at time t + 1, triggering Rule 3a at t + 2, or
(u, c) or (c, v) must be proposed according to Rule 1b. By the same
reasoning as above, the claim follows for time t + 4 in this case.
If there is no such node c, consider the buddy of u or v closest
to position µ = (u.id + v.id )/2. That is, let B denote the set of

136

all buddies of u and v on level i, i.e., B = (N (u) ∪ N (v)) ∩ Gρx .
Let w ∈ B be the buddy which minimizes |µ − w|. Without loss of
generality, assume w is the buddy of u. Now let w0 be the buddy of v
which is located on the same side of µ as w. Observe that v.range[i]
is defined by w0 due to our assumption that c does not exist and
since w0 is further away from µ and hence also from v. Therefore,
w ∈ v.range[i]. On the other hand, if no such w0 exists, then
v.range[i] is not bounded, and the claim follows trivially. Thus,
a and b are σ-V-linked in round t + 2. Therefore, the claim also
follows for the case i + 1 = |σ|, which concludes the proof.

distance of u in Gρ to the closest node in Vρ0 is cut in half in each
round. Thus, it takes at most O(log n) rounds into a node in Vρ0 is
a direct neighbor of u, which implies the lemma.
Lemma 3.13 can also be proved by observing that in every round,
due to the “pointer doubling” operations, the diameter of the connected component formed by nodes without a buddy is cut in half.
L EMMA 3.14. Assume Gρ is connected at time t. Then, at time
t + (H − |ρ|) + O(log n) the graph Gρ0 is connected and so is Gρ1 .
P ROOF. Define k = H − |ρ|. At time t0 = t + O(log n), by
Lemma 3.13, every ρ0 node has a stable connection to a ρ1 node and
vice versa. Let a and b be two ρ1 nodes. Because Gρ is connected at
time t, it is so at time t0 , and there is a path a = u1 , u2 , . . . , um = b
in Gρ at time t0 . If ui is a ρ0 node, define vi to be one of its ρ1
buddies, otherwise set vi = ui . Then, by definition, at time t0 the
nodes vi and vi+1 are either directly connected, or connected by a
temporary (ρ1, k)-bridge. Hence, by Lemma 3.12 (or Lemma 3.1),
at time t0 + k the nodes vi and vi+1 are in the same (ρ1, k)-precomponent, and by Lemma 3.9, Lemma 3.8, and Lemma 3.1 vi and
vi+1 are in the same connected component of Gρ1 at time t0 + k +
O(1). With this and the symmetric argument for Gρ0 the claim of
the lemma follows.

L EMMA 3.10. A temporary edge (u, v) of level ` at time t is
either transformed into a stable edge or forwarded and changed by
this into a temporary edge of level at least ` + 1.
P ROOF. Let ρ be the common prefix of u and v with |ρ| = `,
and assume w.l.o.g. that u has label ρ0 and v has label ρ1. If (u, v)
is temporary, it either becomes stable by Rule 1a or there must be
a stable ρ1-node w ∈ N (u) between u and v In this case, Rule 2
replaces (u, v) with (w, v) for such a w, which is a temporary edge
of level at least ` + 1.
The following lemma follows from the previous one because no
temporary edge has a level higher than the height H.

3.2

L EMMA 3.11. Every temporary edge becomes stable after H
time steps.

Top-down Phase

In the previous section, it has been shown that in O(log2 n)
rounds, all nodes sharing a given prefix ρ have discovered each other
and are connected (via other nodes with prefix ρ). In addition, each
node is connected—on each level—to at least one “buddy” having
the opposite last bit of the corresponding prefix. We now prove that
after these properties have been achieved, the final SKIP+ topology
is established in only O(log n) rounds.
The analysis of the top-down phase is done by induction overloading. For our induction step, we need the concept of finished
levels.

Next, the case of temporary bridges is investigated.
L EMMA 3.12. Assume two ρx-nodes a, b for some x ∈ {0, 1}
are directly connected by a temporary ρx-bridge, i.e., there is a temporary edge (u, v) and stable edges (u, a) and (v, b). Then, for
k = H − |ρ|, a and b are (ρx, k)-pre-connected at time t + k.
P ROOF. W.l.o.g. let x = 1. By the rules of temporary edges, the
destination v never changes. Let bi be a ρ-buddy of v, i.e., the stable
edge (v, bi ) exists at time t + i, b0 = b. At time t + i + 1, there is
at least the temporary edge (v, bi ), and bi and bi+1 are ρ1-V-linked,
and hence, by Lemma 3.8, bi is (ρ1, k)-pre-connected with b also at
time t + k.
Let ui be the starting point of the temporary edge at time t + i,
i.e. (ui , v) be the temporary edge, and let ai (with a0 = a) be some
stable neighbor of ui at that time, i.e., the stable edge (ui , ai ) exists
at time t + i. At time t + i the stable edge (ui , ui+1 ) exists (because
the temporary edge was forwarded along this edge). Further, let ci
be some stable neighbor of ui+1 at time t + i. If this does not exist,
set ci = ai , and note that at time t + i + 1 the stable edge (ui+1 , ai )
must exist because of Rule 1b. By definition, ai and ci have a stable
(ρ1, k0 )-bridge for k0 ≤ k and are hence (ρ1, k)-pre-connected. At
time t + i + 1 at least the temporary edge (ui+1 , ci ) exists, and
hence ci and ai+1 are ρ1-V-linked, so by Lemma 3.8 ai is (ρ1, k)pre-connected with a at time t + k.
By Lemma 3.10 the level of (ui , v) is at least |ρ| + i. Hence for
some j ≤ k the stable edge (uj , v) exists at level smaller than H at
time t + j. Hence, at time t + j the nodes aj and bj are (ρ1, k)-preconnected, and we can conclude from Lemma 3.8 that a and b are
(ρ1, k)-pre-connected at time t + k.

D EFINITION 3.15
finished if and only if

(i-F INISHED ). We say that a graph is i-

1. ∀ρ with |ρ| = i, it holds that Gρ contains all edges of the
SKIP+ -graph as stable edges;
2. ∀u ∈ V let ρ = pre i (u) and ∀j < i: if v is a level j rightbuddy of u (current closest node on the right with pre j (v) =
pre j (u) and v.rs[j + 1] 6= u.rs[j + 1]), then for all ∀w
with pre i (w) = ρ (i.e., w ∈ Gρ ) and w lying between u
and v, i.e., w.id ∈ [u.id , v.id ], it holds that u and w are
connected by a stable edge. If there is no such buddy v, then u
is connected to all nodes to the right of u in Gρ ; and similarly
to the left.
Figure 5 shows an example.
Observe that after the bottom-up phase, the “top label” is finished trivially: This label forms a graph Gρ with more than one
node, whereas Gρ1 and Gρ0 are trivial, i.e., consist of a single node.
Clearly, for a top label ρ the graph Gρ consists of precisely two
nodes. In addition, once the graph Gρ is connected, it contains all
edges of the SKIP+ -graph as stable edges.
The following reasoning shows that the levels will finish one after
the other starting from the highest level (hence this phase’s name),
where each level takes constant time only.

L EMMA 3.13. Consider any bit string ρ ∈ {0, 1}∗ . Suppose
that Gρ is weakly connected at time t0 . Then every node u ∈ Vρ
will have a neighbor in Vρ0 and Vρ1 within O(log n) rounds.
P ROOF. Consider any node u that does not have a neighbor in
Vρ0 or Vρ1 . In this case, u.range[i] = U for i = |ρ|, which implies
together with Rule 1b and Rule 1c that every node v ∈ N (u) ∩ Vρ
with missing ρ-buddy will introduce every node w ∈ N (v) ∩ Vρ
to u. Suppose w.l.o.g. that u is still missing a node in Vρ0 . Then
the neighbor introduction of Rule 1b and Rule 1c implies that the

L EMMA 3.16. Assume at time t the graph is i-finished. Then, at
time t + 3, the graph is (i − 1)-finished
P ROOF. Figure 6 illustrates the situation. We consider a node
u. We will show that at time t + 1, u knows its closest level-i
neighbor w in the direction of the old buddy (that must exist due

137

u.rs=σ=ρ1...

Finally, observe that after all levels are finished, all non-SKIP+
edges must be temporary. They will be forwarded towards the highest level and disappear in logarithmic time (in H rounds). Therefore, we conclude that the top-down phase takes logarithmic time
only.

rs=ρ...
ρ0...

3.3

Figure 5: Visualization of the i-finished concept: node u is
connected to all nodes having prefix ρ between its buddies on
level i. Note that the distances to the buddies may not decrease
monotonously towards lower levels!

T HEOREM 3.17. Given an arbitrary weakly connected graph,
the self-stabilizing algorithm ALG+ constructs SKIP+ in O(log2 n)
rounds, w.h.p.

4.

to Lemma 3.14 of the bottom-up phase). Having established this, it
follows directly that at time t + 2, w will inform u about all other
neighbors in the desired region (Rule 1b and Rule 1c). At time t+3,
node u will be informed about its neighbors on the opposite side of
the level i − 1 interval by the corresponding buddy, establishing our
induction invariant.

L EMMA 4.1. The degree of a node v in SKIP+ is O(log n)
w.h.p.

Case I
ρ0
w

ρ1

NODE JOIN/LEAVE

In this section we study the amount of work it takes to handle a
node departure (leave) or the inclusion of a new node (join). We
show that once we reach a valid SKIP+ graph, our algorithm can
efficiently support any single join or leave operation in O(log4 n)
work and O(log n) rounds w.h.p. For proving this result, we first
need to bound the degree of a node in SKIP+ .

w=v

ρ1

Combining Bottom-up and Top-down

From the previous discussion, we can draw the following conclusions. From Lemma 3.14, by summing up over all levels, it follows that the bottom-up phase lasts for at most O(log2 n) rounds
w.h.p. The subsequent top-down phase takes time O(log n) (cf
Lemma 3.16) w.h.p. Thus, we have derived the following theorem.

P ROOF. Recall the definition of a SKIP+ graph and consider any
node v. For any level i, let the random variable XiR be defined as

u

v

w

ρ1

v

XiR = max{|{w ∈ Ni∗ (v) | w.id > v.id}| − 1, 0}.

Case II
ρ0

u

ρ0

w´

w

ρ1

u

w´

In order to bound the probability that XiR = k for some k > 0,
we consider three cases. If v does not have k + 1 nodes in Ni∗ (v)
to the right of it, then Pr[XiR = k] = 0. If v has exactly k + 1
nodes in Ni∗ (v) to the right of it, then XiR = k if and only if for the
k closest successors w of v in Ni∗ (v) it holds that w.rs[i + 1] =
succ∗i (v).rs[i + 1], so Pr[XiR = k] = 1/2k−1 . Finally, if v has
more than k + 1 nodes in Ni∗ (v) to the right of it, then XiR = k
if and only if the closest k successors w of v in level i have the
property that w.rs[i + 1] = succ∗i (v).rs[i + 1] and the (k + 1)th
closest successor of v, w0 , satisfies w0 .rs[i + 1] 6= succ∗i (v).rs[i +
1], so Pr[XiR = k] = 1/2k . In any case, Pr[XiR = k] ≤ 1/2k−1
for any k > 0, and this probability bound holds independently of the
P
R
other levels. Hence, for XR = H
i=1 Xi , where H = Θ(log n) is
an upper bound on the number of levels that holds w.h.p., it follows
that
!
d
1
Pr[XR = d] ≤
.
H 2d−H

v

Case III
ρ0
w´

u

Figure 6: Proof of Lemma 3.16.

In order to prove that u knows its closest neighbor w at time t+1,
we distinguish three cases. From the bottom-up phase, we know
that u already has a buddy on one side. Without loss of generality,
assume this buddy is on the right of u. Let this buddy node be
denoted by v.
Case I: If v is already the closest node to the right, the claim holds
trivially (v = w).
We know that w also has a buddy, which can either be on the right
(Case II) or on the left (Case III) of w (and hence also u). Let w’s
buddy (take any if w has two) be denoted by w0 .
Case II: Assume w0 is also on the right of w. We distinguish two
cases: Either v is left of w0 or right of w0 . If v is left of w0 , by
our induction hypothesis, w must have a stable edge to node v. By
Rule 1b and Rule 1c, v will introduce w to u at t + 1 (edge (w, u)),
and the claim follows. The case where v is right of w0 is analogue:
the roles of v and w0 are simply switched.
Case III: Now assume w has a buddy w0 on the left. We distinguish two cases. If u has another buddy u0 on the left as well, the
same arguments as in Case II show that either u0 or w0 will introduce
the corresponding neighbors at time t + 1. If, on the other hand, u
does not have a buddy on the left, then by our induction hypothesis,
w0 must have a stable edge to u, over which u is introduced to w in
the next round as well.

If d = c · H, we get
d
H

!

1
2d−H

≤

(ec)H
1
≤ c0
2cH−H
n

for some constant c0 that can be made arbitrarily large if the constant
c is sufficiently large. Hence, the number of v’s neighbors to the
right is at most O(log n) w.h.p. A similar argument applies to the
left neighbors of v, which implies the claim.
T HEOREM 4.2. When a node v leaves the system, it takes
O(log n) rounds of the algorithm and O(log4 n) total work w.h.p.
for the graph to converge back to a valid SKIP+ structure.

138

the nodes of Vl (resp. Vr ) with maximum common prefix equal to i
with v. Moreover, let Wl (resp. Wr ) be the set of all nodes to the left
(resp. right) of v of maximum cardinality so that for all w0 ∈ Wl
(resp. w0 ∈ Wr ), the maximum common prefix with v is equal to i
and there is no node w00 in between the nodes of Wl (resp. Wr ) with
maximum common prefix more than i with v. Let vl , vr , wl and wr
be the closest neighbors in Vl , Vr , Wl and Wr to v. Let us assume
for now that vl , vr , wl and wr exist. Recall that w considers v to be
a stable neighbor. Suppose w.l.o.g. that v is to the right of w. We
distinguish between the following cases.
Case 1: wl .id < vl .id < vr .id < wr .id. In this case, all nodes
in {wl } ∪ Vl ∪ Vr ∪ {wr } have to connect to v and vice versa,
and besides these, no other connections are needed to fully integrate
v into level i. Since w = wl and w therefore knows all nodes in
Vl ∪ Vr ∪ {wr } by the SKIP+ definition, one round of applying
Rule 3a (which is caused by (w, v)) suffices to fully integrate node
v into level i.
Case 2: vl .id < wl .id < vr .id < wr .id. In this case, all nodes
in {vl } ∪ Wl ∪ Vr ∪ {wr } have to learn about v and vice versa to
fully integrate v into level i. Since w is a node in Wl , w has links to
all nodes in {vl } ∪ Wl ∪ Vr ∪ {wr }, so again one round of applying
Rule 3a suffices to fully integrate node v into level i.
Case 3: wl .id < vl .id < wr .id < vr .id. In this case, all nodes
in {wl }∪Vl ∪Wr ∪{vr } have to learn about v and vice versa to fully
integrate v into level i. Since w = wl , w has links to all nodes in
Vl ∪{wr }. Hence, one round of Rule 3a introduces v to the nodes in
Vl ∪ {wr } and vice versa. Afterwards, wr will apply Rule 3a since
its stable neighborhood changed due to v, so wr will introduce v to
Wr ∪ {vr } and vice versa, which completes the integration of v into
level i.
Case 4: vl .id < wl .id < wr .id < vr .id. In this case, all nodes
in {vl } ∪ Wl ∪ Wr ∪ {vr } have to learn about v and vice versa
to fully integrate v into level i. As w is any node in Wl , w knows
about {vl } ∪ Wl ∪ Wr ∪ {vr }, so one round of applying Rule 3a
suffices to fully integrate node v into level i.
The remaining cases in which vl , vr or wr do not exist are similar and dropped here. Hence, it takes at most two rounds to fully
integrate v into level i.
Once v is fully integrated into a level i, it knows the closest predecessor and successor w in SKIP+ with maximum prefix match at
least i + 1 (if it exists). Since each of these nodes will consider v
to be a stable neighbor in level i + 1, we can use similar case distinctions as above to show that v will be fully integrated into level
i + 1 in at most two further rounds. Node v also knows its closest
predecessor and successor w in SKIP+ with maximum prefix match
at least i. Since each of these nodes will consider v to be a stable
neighbor in level i − 1, we can also use similar case distinctions as
above to show that v will be fully integrated into level i−1 in at most
two further rounds. Using these arguments inductively implies that
v will be fully integrated into the SKIP+ graph in O(log n) time.
It remains to bound the work. The first part (creating and forwarding temporary edges),2 just consumes O(log2 n) work. Each time a
node destabilizes, O(log2 n) new edges are created. Certainly, only
nodes that will consider v to be their stable neighbor (and vice versa)
will destabilize, and we know from Lemma 4.1 that the degree of v
in SKIP+ will be O(log n) in the end w.h.p. Hence, altogether at
most O(log3 n) new edges are created. These either merge with stable edges, become a new stable edge or become a temporary edge.
Each of the temporary edges will need at most O(log n) applications of Rule 2 until it merges with a stable edge. This yields the
claim.

P ROOF. Certainly, only the nodes that were directly connected
to node v will need to adapt their current set of neighbors upon the
departure of v (since the departure of v could not possibly alter the
neighborhoods or ranges of other nodes, given that v is directly connected to all nodes in its range for all levels). By Lemma 4.1, the size
of the entire neighborhood of node v (across all levels) is O(log n)
w.h.p., so only O(log n) nodes need to change their neighborhood.
In order to show that these O(log n) nodes can quickly adapt their
neighborhoods, we distinguish between several cases for every level
i. In these cases, let Vl (resp. Vr ) be the set of all left (resp. right)
neighbors w ∈ Ni∗ (v) with w.rs[i + 1] = v.rs[i + 1] and let Wl
(resp. Wr ) be the set of all left (resp. right) neighbors w ∈ Ni∗ (v)
with w.rs[i + 1] 6= v.rs[i + 1]. Certainly, Vl ∪ Vr ∪ Wl ∪ Wr =
Ni∗ (v). Let vl , vr , wl and wr be the closest neighbors in Vl , Vr , Wl
and Wr to v. Let us assume for now that vl , vr , wl and wr exist.
Case 1: wl .id < vl .id < vr .id < wr .id. In this case, all neighborhoods are correct once v has been removed, so no further edges
are needed by the nodes.
Case 2: vl .id < wl .id < vr .id < wr .id. In this case, all nodes
in {vl } ∪ Wl \ {wl } have to learn about vr and vice versa to update
the neighborhoods. The other nodes just have to remove v from their
neighborhood. Since wl has edges to all nodes in {vl , vr } ∪ Wl , a
single round of applying Rule 3a suffices to update all neighborhoods correctly.
Case 3: wl .id < vl .id < wr .id < vr .id. This case is just the
reverse of Case 2.
Case 4: vl .id < wl .id < wr .id < vr .id. In this case, all nodes
in {vl } ∪ Wl have to learn about Wr ∪ {vr } and vice versa. Since
wl knows {vl } ∪ Wl ∪ {wr } and wr knows {wl } ∪ Wr ∪ {vr },
after one round of applying Rule 3a, all nodes in {vl } ∪ Wl learn
about wr and all nodes in Wr ∪ {vr } learn about wl . Since the
stable neighborhoods of wl and wr just got updated, wl and wr will
trigger another round of "introduce all" by Rule 3a, son nodes in
{vl ∪ Wl ∪ Wr ∪ {vr } will have updated their neighborhoods by
the second round.
The other cases when some of the nodes vl , vr , wl and wr do
not exist are very similar and dropped here. Hence, after at most
two rounds, all (stable) neighborhoods have been updated. Since
only O(log n) nodes need to change their neighborhood, and each
of these nodes inserts at most O(log2 n) edges due to Rule 3a in
each round, at most O(log3 n) edges are inserted in total. These
either merge with stable edges, become a new stable edge or become
a temporary edge. Each of the temporary edges will need at most
O(log n) applications of Rule 2 until it merges with a stable edge.
Hence, altogether the time is bounded by O(log n) and the work is
bounded by O(log4 n).
T HEOREM 4.3. Assume a new node v joins the system by establishing an edge to a node u which is currently in SKIP+ . It will take
O(log n) rounds of the algorithm and O(log4 n) total work w.h.p.
for the graph to converge back to a valid SKIP+ structure.
P ROOF. Upon learning about node u, node v immediately considers edge (v, u) as stable (since u is currently the only predecessor
or successor node v knows). That will prompt the insertion of edge
(u, v) by Rule 1a. If u considers (u, v) as a temporary edge, then it
forwards the edge via Rule 2 to a node u0 with a longer prefix match
with v than u. This leads, after at most H steps, to a stable edge
(w, v). Till then, v keeps inserting the edge (u, v) in each round (as
it considers u to be a stable neighbor), so there will be a string of
temporary edges moving upwards from u. Besides Rule 2, no other
rule will be applied at this point by the old nodes in SKIP+ .
Suppose that w is the first node that considers the edge (w, v) to
be stable. Let i be the maximum level such that pre i (v) = pre i (w).
Let Vl (resp. Vr ) be the set of all nodes to the left (resp. right) of v in
SKIP+ of maximum cardinality so that for all w0 ∈ Vl (resp. w0 ∈
Vr ), pre i+1 (w0 ) = pre i+1 (v) and there is no node w00 in between

2
By “first part” we mean the part where v contacts u until a first
node is met that considers the edge to v to be stable. The sequence
of nodes involved here will continuously forward temporary edges
upwards until v ceases to insert the edge (u, v). This happens once
v is not a nearest neighbor of u anymore for some level.

139

5.

CONCLUSION
[18] P. Druschel and A. Rowstron. Pastry: Scalable, distributed
object location and routing for large-scale peer-to-peer
systems. In Proc. 18th IFIP/ACM International Conference
on Distributed Systems Platforms (Middleware), 2001.
[19] D. Gall, R. Jacob, A. Richa, C. Scheideler, S. Schmid, and
H. Täubig. Modeling scalability in distributed
self-stabilization: The case of graph linearization. Technical
Report TUM-I0835, Technische Universität München,
Computer Science Dept., Nov. 2008.
[20] M. T. Goodrich, M. J. Nelson, and J. Z. Sun. The rainbow
skip graph: A fault-tolerant constant-degree distributed data
structure. In Proc. 17th Annual ACM-SIAM Symposium on
Discrete Algorithm (SODA), pages 384–393, 2006.
[21] N. Harvey and J. Munro. Deterministic SkipNet. Inf. Process.
Lett., 90(4):205–208, 2004.
[22] N. J. A. Harvey, M. B. Jones, S. Saroiu, M. Theimer, and
A. Wolman. Skipnet: A scalable overlay network with
practical locality properties. In Proc. 4th USENIX Symposium
on Internet Technologies and Systems (USITS), pages
113–126, 2003.
[23] T. Herman. Observations on time adaptive self-stabilization.
Technical Report TR 97-07, University of Iowa, 1997.
[24] T. Herman. Self-stabilization bibliography: Access guide.
University of Iowa, December 2002. See
ftp://ftp.cs.uiowa.edu/pub/selfstab/bibliography/.
[25] S. Katz and K. Perry. Self-stabilizing extensions for
message-passing systems. Distributed Computing,
7(1):17–26, 1993.
[26] S. Kutten and B. Patt-Shamir. Time-adaptive self stabilization.
In Proc. of the 16th ACM Symposium on Principles of
Distributed Computing (PODC), pages 149–158, 1997.
[27] S. Kutten and D. Peleg. Fault-local distributed mending. In
Proc. of the 14th ACM Symposium on Principles of
Distributed Computing (PODC), pages 20–27, 1995.
[28] D. Malkhi, M. Naor, and D. Ratajczak. Viceroy: A scalable
and dynamic emulation of the butterfly. In Proc. 21st PODC,
pages 183–192, 2002.
[29] P. Maymounkov and D. Mazières. Kademlia: A peer-to-peer
information system based on the xor metric. In Proc. 1st
International Workshop on Peer-to-Peer Systems (IPTPS),
pages 53–65, 2002.
[30] M. Naor and U. Wieder. Novel architectures for P2P
applications: the continuous-discrete approach. ACM
Transactions on Algorithms (TALG), 3, 2007.
[31] M. Onus, A. Richa, and C. Scheideler. Linearization: Locally
self-stabilizing sorting in graphs. In Proc. 9th ALENEX, 2007.
[32] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Schenker. A scalable content-addressable network. In Proc.
ACM SIGCOMM, pages 161–172, 2001.
[33] A. Shaker and D. S. Reeves. Self-stabilizing structured ring
topology P2P systems. In Proc. 5th IEEE International
Conference on Peer-to-Peer Computing, pages 39–46, 2005.
[34] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and
H. Balakrishnan. Chord: A scalable peer-to-peer lookup
service for internet applications. Technical Report
MIT-LCS-TR-819, MIT, 2001.
[35] G. Varghese. Self-Stabilization by local checking and
correction. PhD thesis, MIT, 1992.
[36] G. Varghese. Self stabilization by counter flushing. In Proc. of
the 13th ACM Symposium on Principles of Distributed
Computing (PODC), 1994.

This paper described the first self-stabilizing algorithm that
quickly establishes a scalable peer-to-peer topology out of any state
in which this is in principle possible. Our work opens many important directions for future research. In particular, so far, we do not
have a polylogarithmic bound on the enabled actions per node and
round. Hence, we want to explore the corresponding complexities
further and come up with the necessary algorithmic modifications.

6.

REFERENCES

[1] Y. Afek and S. Dolev. Local stabilizer. Journal of Parallel and
Distributed Computing, 62(5):745–765, 2002.
[2] D. Angluin, J. Aspnes, J. Chen, Y. Wu, and Y. Yin. Fast
construction of overlay networks. In Proc. 17th Annual ACM
Symposium on Parallelism in Algorithms and Architectures
(SPAA), pages 145–154, 2005.
[3] J. Aspnes and G. Shah. Skip graphs. ACM Transactions on
Algorithms, 3(4):37, Nov. 2007.
[4] J. Aspnes and U. Wieder. The expansion and mixing time of
skip graphs with applications. In Proc. 17th ACM Symposium
on Parallelism in Algorithms and Architectures (SPAA), pages
126–134, 2005.
[5] J. Aspnes and Y. Wu. O(log n)-time overlay network
construction from graphs with out-degree 1. In Proc.
International Conference on Principles of Distributed Systems
(OPODIS), volume 4878 of LNCS, pages 286–300, 2007.
[6] B. Awerbuch, B. Patt-Shamir, and G. Varghese. Self
stabilization by local checking and correction. In Proc. of the
32nd IEEE Symp. on Foundations of Computer Science
(FOCS), 1991.
[7] B. Awerbuch and C. Scheideler. Peer-to-peer systems for
prefix search. In Proc. of the 22nd ACM Symposium on
Principles of Distributed Computing (PODC), 2003.
[8] B. Awerbuch and C. Scheideler. The hyperring: A
low-congestion deterministic data structure for distributed
environments. In Proc. 15th Ann. ACM-SIAM Symposium on
Discrete Algorithms (SODA), pages 318–327, 2004.
[9] B. Awerbuch and G. Varghese. Distributed program checking:
A paradigm for building self-stabilizing distributed protocols.
In Proc. 32nd Annual IEEE Symposium on Foundations of
Computer Science (FOCS), pages 258–267, 1991.
[10] A. Bhargava, K. Kothapalli, C. Riley, C. Scheideler, and
M. Thober. Pagoda: A dynamic overlay network for routing,
data management, and multicasting. In Proc. 16th Annual
ACM Symposium on Parallelism in Algorithms and
Architectures (SPAA), pages 170–179, 2004.
[11] J. Brzezinski and M. Szychowiak. Self-stabilization in
distributed systems - a short survey. Foundations of
Computing and Decision Sciences, 25(1), 2000.
[12] T. Clouser, M. Nesterenko, and C. Scheideler. Tiara: A
self-stabilizing deterministic skip list. In Proc. 10th Int. Symp.
on Stabilization, Safety, and Security of Distributed Systems
(SSS), 2008.
[13] A. Costello and G. Varghese. Self-stabilization by window
washing. In Proc. of the 15th ACM Symposium on Principles
of Distributed Computing (PODC), pages 35–44, 1996.
[14] C. Cramer and T. Fuhrmann. Self-stabilizing ring networks on
connected graphs. Technical Report 2005-5, System
Architecture Group, University of Karlsruhe, 2005.
[15] E. Dijkstra. Self-stabilization in spite of distributed control.
Communications of the ACM, 17:643–644, 1974.
[16] S. Dolev. Self-Stabilization. MIT Press, 2000.
[17] S. Dolev and T. Herman. Superstabilizing protocols for
dynamic distributed systems. Chicago Journal of Theoretical
Computer Science, 4:1–40, 1997.

140

Tight

Analyses

Bhaskar

of Two Local

Andr6a

F. T. Leighton

Ghosh 1

S. Muthukrishnan

C. Greg

5

W. Richa

Load Balancing

3

Robert

This

presents

balancing

an

algorithm.

network

examines

neighbors

and

analysis
At

the

sends

of

each

number

the

step,

following
each

of tokens

a token

to each

node

neighbor

in

a

of its

with

at

fewer tokens, where d is the maximum degree of any node in the network. We show that within
O (A/cr) steps, the algorithm reduces the maximum difference in tokens between any two nodes to at most
O ( (dz log n) /a), where A is the maximum difference between the number tokens at any node initially and the
average number of tokens, n is the number of nodes in
the network, and a is the edge expansion of the network.
The time bound is tight in the sense that for any graph
wit h edge expansion a, and for any value A, there exists an initial distribution of tokens with imbalance A for
which tkle time to reduce the imbalance to even A/2 is at
least Q (A/a). The bound on the final imbalance is tight
in the sense that there exists a class of networks that can
be locally balanced everywhere (i.e., the maximum difference in tokens between any two neighbors is at most
least

2d +

2d),

1

while

the global

rthermore,

lh]

imbalance

we show

that

remains
upon

Q((dz

reaching

with

of 0((d2 log n)/a), the time for this
algorithm to locally balance the network can be as large
as Q (n 1/2). We extend our analysis to a variant of this
algorithm for dynamic and asynchronous networks. We
also present tight bounds for a randomized algorithm in
which each node sends at most one token in each step.
a global

imbalance

CT

06520.

joint

of Computer
Supported

study.

Email:

2 Department
Science,
Contracts
ftl@math.
t school
I’ittsburgh,

ONR

ghosh@cs.
of

MIT,

by

Science,

Cambridge,

init.
of
PA

Grant

edu.
ComPuter
15213.

New
and

Haven,

Award,

a Yale/IBM

for

Supported

Computer

ported

by

ARPA

oretical

Email:

ence

NOO014-92-J-1799.

Email:

{aricha.

Carnegie

Mellon

brmn}~cs

University,
emu.

by

Rutgers

by

an

NSF
and

National
by

Young

ARPA

Investigator

Contract

Email:

{diz,

F33615-93-

Grant

by

No.

Research

Research
tional
fice

at
Science

of

548

Science
NSF

Sup-

and

The-

Foundation

Contract

Sci-

STC-8809648.

edu.
Science,

University

of

Texas

at

78712.

the

rraj}@cs
Texas

Naval

of

.utexas

Advanced

No.

Computer

Institute,
Princeton

. edu.
Research

Olden

University

Reserach,
edu.
part
by

CCR-9457799.

Science,
35

Foundation,

ret@cs.
princeton.
‘supported
in
Award,

08855.

Program

under

ARP-93-O03658-461.

8Department
NEC

NJ

Mathematics

National

rutgers.

TX

Discrete

under

Computer

plaxton,

7Supported

a

Center,

of

Austin,

Piscataway,

for

Science,

muthu@dimacs.

Austin,

Permission to copy without fee all or parl of this material is
granted provided that the copies are not made or distributed for
direct commercial advantaqe, the ACM copyright notice and the
title of the publication a,nd, Its date appear,, and notice is given
that copym IS by perrwsslon of the Association of Computing
Machinery. o cop otherwise, or to republish, requires
a fee and/or+ sDecI“rIC Permission.
Nevada, USA
STOC’ 95, L&-Veg&
@ 1995 ACM 0-89791 -718-9/95/0005..$3.50

Center

Technology

‘Department

edu.

University,

DIMACS,

Computer
and

Email:
Science,

part

CCR-9457766,

5DIMACS,

Laboratory

02139.

and

in

No.

1-1330.

edu.
and

MA

University,

4-91-J-1576

yale.

Mathematics

NOO014-91-J-1698

Yale

6Y9

Introduction

4 Supported
1 Department

Zuckerman

We analyze the algorithms in this paper in terms
of tokens, i.e., the maximum
of the initial imbalance
difference between the number of tokens at any node and
the average number of tokens, which we denote A, the
number of nodes in the graph, which we denote n, the
maximum degree of the graph, d, and the node and edge
p,
expansion of the graph. We define the node expansion
of a graph G to be the largest value such that every set
S of n/2 or fewer nodes in G has at least plSl neighbors
outside of S. We define the edge expanszon a, of a graph

log n)/a).

a state

David

8

6’7

A natural way to balance the workload in a distributed
system is to have each work station periodically poll
the other stations to which it is connected, and send
some of its work to stations with less work pending.
This paper analyzes the effectiveness of this local load
balancing strategy in the simplified scenario in which
each work station has a collection of independent unitsize jobs (called tokens) that can be executed on any
other work station. We model a distributed system as
a graph, where nodes correspond to work stations, and
edges correspond to connections between stations, and
we assume that in one unit of time, at most one token
can be transmitted across an edge of the graph in each
direction.
Our analysis addresses only the static load
balancing aspect of this problem; we assume that each
processor has an initial collection of tokens, and that
no tokens are created or destroyed while the tokens are
being balanced.

load

at each

M. Maggs 34

R. Rajaraman

6Y7

E. Tarjan

1

paper

Bruce

2

Plaxton

Abstract

Algorithms

Grant
Contract
an

Princeton

Princeton,

supported

in

No.
No.

NSF

University,

Street,

part

CCR-8920505,
NO014-91-J-1463.

National

Young

NJ
by
and

and
08544.
the

Na-

the

Of-

Email:

Investigator

G to be the largest value such that for every set S of n/2
or fewer nodes in G, there are at least a [S[ edges in G
!rit,ll one endpoint in S and the other not in s.
The
I Ile

time

the

final

performance
that

it

balance

of an algorithm
takes
that

to
it

balance

is characterized
the

tokens,

We

say

achieves.

The multi-port algorithm is simpler and deterministic. At each step, a token is sent from node u to node v
across edge (u, v) if at the beginning of the step node u
contained at least 2d + 1 more tokens than node v. This
algorithm was first analyzed in [2].

by

and

that

larly, in terms of node expansion, there exist classes of
graphs where the time to balance to within even A/2
tokens is at least !il(dA/p).

by

an al-

(or just balances) to within t
tokens if the maximum difference in the number of tokens between any two nodes in the graph is at most t.
balances
to within t toWe say that an algorithm locally
kens if the maximum difference in the number of tokens
between any neighboring nodes in the graph is at most

gurithln

globally

balances

As in the single-port case, we analyze the multi-port
algorithm in terms of both edge expansion and node expansion. In terms of edge expansion, the algorithm balances to within 0(d2 log n/a) tokens in O(A/a)
steps.
This bound is tight in the sense that for any network
with edge expansion CY,and any value A, there exists an
initial distribution of tokens with imbalance A such that
the time to reduce the imbalance to even A/2 is fl(A/a).
In terms of node expansion, the algorithm balances to
within O(dlog n/p) tokens in O(A/p) time. This bound
is tight in the sense that for many values of d, n, and p,
and any value A, there exists an n-nocle degree-d graph
with node expansion p and an initial dlistribution of tokens with imbalance A for which the time to balance to
within A/2 tokens is fl(A/p).

(.

\Ve analyze two different types of algorithms in this
paper, single-port and multi-port.
In the szngle-port
lnodel,
a node may transmit or receive at most one token
model, a node may
in one unit of time. In the multt-port
simultaneously transmit or receive a token across all of
its edges (there may be as many as d) in a single unit
of time Not surprisingly, the load balancing algorithms
ruu
faster in the multi-port model. In practice, however,
single-port nodes may be prefered to multi-port
nodes
Ixx-ause they are easier and less costly to build.
1.1

Our

Both the single-port and multi-port
algorithms will
eventually locally balance the network, the single-port
algorithm to within one token, and the multi-port
alHowever, even after
gorithm to within 2d tokens.
reducing the global imbalance to a small value, the
time for either of these algorithms to reach a locally
balanced state can be quite large. h particular, we
show that after reaching a state tha~ is globally balanced to within O (d log n/p) tokens, the multi-port algorithm may take another f2(n1/2) steps to reach a state
that is locally balanced to within 2d tokens. For networks with large node expansion and small degree, e.g.,
p = ~(l) and d = 0(1), and small initial imbalance,

results

‘1’llis paper analyzes the simplest and most natural local
:dgorithms in both the single-port and multi-port modClh.
Ill the single-port algorithm, a matching is randomly
cl]osen at each step. First, each (undirected) edge in
the network is independently selected to be a candidate
with probability l/4d.
Then each candidate edge (u, v)
for

which

there

is another

from

the

candidates

form

a matching

edge

in

(u, v)

A/l,

set

candidate

IS removed

edge

of candidates.

if u and

Al

in the

v have

the

(u, c) or (y, v)
The

remaining

graph.

For

same

number

each

e.g., A = O(d log2 n/p), the time to lc]cally balance the
network, fl(n112), may be much larger than the time,
O(A/p) = 0(dlog2 n/p2) = 0(log2 n) to reach a state
that is globally balanced to within O (d log n/p) tokens.
We prove similar bounds in terms of eclge expansion and
also for the single-port algorithm.

of

is sent across (u, v). Otherwise,
a token is sent from the node with more tokens to the
Ilocle with fewer. This algorithm was first analyzed in
[15]
tokens,

then

nothing

We analyze the performance of the single-port algorithm in terms of both the edge expansion and the
Ilode expansion of the graph. In terms of edge expans1o]1, we show that the single-port algorithm balances
O(dlogn/a)
tokens
in O(dA/cr)
steps, with
to within
Iligh probability.
In terms of node expansion, the final imbalance is O(log n/p), and the time is O(dA/L),
wit h high probability.
(To compare these bounds, note
are tight
in the
that p ~ a ~ alp.) The time bounds
sense that for many values of n, d, a, and A, there is an
n-node degree-d graph with edge expansion CYor node
expansion ,u and an initial placement of tokens with imbalance A where the time (for any algorithm) to balance
Simito within even A12 tokens is at least f2(dA/a).

Thus far we have described a network model in which
the nodes are synchronized by a global clock (i.e., a synchronous
network), and in which the edges are assumed
not to fail. With minor modifications, however, the load
balancing algorithms can be made to work in both asynchronous and dynamic networks. In a dynamac network,
the set of edges in the network may vary at each time
step. In any time step, a jive edge is one that can transmit one message in each direction. We assume that at
each time step, each node in a synchronous dynamic
network knows which of its edges are live, In an asynchronous
network, the topology is fixed, but an adversary determines the speed at which each edge operates

549

been extended in [9, 18, 32]. These algorithms either use
strong expanders to approximately balance the network,
or the AKS sorting network [3] to perfectly balance the
network. Thus, they do not work on networks of arbitrary topology. Also, these algorithms work by setting
up fixed paths through the network on which load is
moved and therefore fail when the network changes. In
contrast, our local algorithm works on any arbitrary dynamic network that remains connected.

at every instant of time. For every undirected edge between two nodes, we allow at most two messages to be
in transit at any instant in time. These messages may
travel in opposite directions across the edge, or both
nlay travel in one direction, while no message travels in
tile opposite direction. An edge is said to be live for a
un i t interval of time if every message that was in transit
across the edge (in either direction) at the beginning of
the interval is guaranteed to reach the end of the edge
by the end of the interval. We analyze the performance
of the multi-port load balancing algorithm under the assulnption that at each time step, the set of live edges has
sorlle edge expansion a, or node expansion p.

On arbitrary topologies, load balancing has been
studied under two models.
In the first model, any
amount of load can be moved across a link in any time
step [8, 12, 15, 19, 35]. The second model is the one
that we adopt here, namely one in which at most one
unit load can be moved across a link in each time step.
Load balancing algorithms on the second model were
first proposed and analyzed in [2] for the multi-port variant and in [15] for the single-port variant. The upper
bounds established by them are suboptimal by a factor
of Q(W) or f2(log(nA)) in general. Our result here is
an improved, in fact, an optimal bound for the same
problem.

We also study the off-line load balancing problem, in
which every node has knowledge of the global state of
the network. This problem on static synchronous networks has been studied in [28]. We use their results to
obtain tight bounds on off-line load balancing in terms
of edge expansion and node expansion. In particular,
we prove that any network can be balanced off-line to
\vithin two tokens in at most [(1 + p)A/pl
steps in the
single-port model. Moreover, there exists a network and
an initial token distribution
for which any single-port
off-line algorithm takes more than ((1 + p)A/pl
steps
to balance the network to within one token. Similarly,
in d~e multi-port
model, any network can be balanced
off-line to within d tokens in at most [A/al
steps. It is
easy to observe that for any network G there exists an
i Ilitial token distribution
such that any algorithm will
take at least [A/cr] steps to balance G to within one
token.
1.2

Previous

and related

As remarked earlier, our multi-port
results (and
those in [2]) hold even for dynamic or asynchronous networks. In general, work on dynamic and asynchronous
networks has been limited. In work related to load balancing for instance, an end-to-end communication problem, namely one in which messages are routed from a
single source to a single destination, has been studied
in [1, 7] on dynamic networks.
Our scenario is substantially more involved since we are required to move
load between several sources and destinations simultaneously. Another result on dynamic networks is the recent analysis of a local algorithm for the approximate
multicommodity
flow problem [5, 6]. While their result
has several applications including the end-to-end communication problem mentioned above, it does not seem
to extend to load balancing. Our result on load balancing is related to their work in the technique; however,
our algorithm and analysis are simpler and we obtain
optimal bounds for our problem.

work

Load balancing has been studied extensively since it
comes up in a wide variety of settings including adapI,lve mesh partitioning
[17, 37], fine grain functional
programming [16], job scheduling in operating systems
[13, 24], and distributed game tree searching [22, 25]. A
r) LIrn ber of models have been proposed for load balanci ng, differing chiefly in the amount of global information
used by the algorithm [2, 11, 12, 15, 26, 30]. On these
]Iloclels, algorithms have been proposed for specific appl icat ions; also, proposed heuristics and algorithms have
been analyzed using simulations and queuing–theoretic
(echrriques [27, 34, 36]. In what follows, we focus on
and
on prior
Lnodels that allow only local algorithms
work

that

al]cil]g
Local

takes

an

analytical

approach

to

the

load

The convergence of local load balancing algorithms
is related to that of random walks on Markov Chains.
Indeed the convergence bounds in both cases depend on
the expansion properties of the underlying graph and
they

problem.
algorithms

restricted

to

particular

are

established

using

potential

function

arguments.

There are however two import ant differences.
First,
the analysis of the rapid convergence of random walks
[21, 29] relies on averaging arbitrary probabilities across
any edge. This corresponds to sending an arbitrary (possibly nonintegral) load along an edge which is forbidden
in our model. In this sense, the analysis in [12] (and all
references in the unbounded capacity model) are similar to the random walk analysis. Second, our argument
uses an exponential potential function. The analyses in

bal-

networks

been studied on counting networks [4, 23], hyper(:ubes [20, 33], and meshes [17, 28]. Another class of
rletworks on which load balancing has been studied is
the class of expanders. Peleg and Upfal [31] pioneered
this study by identifying certain small-degree expanders
:is being suitable for load balancing.
Their work has
have

550

[12,

21,

tions.

29],

Our

sis were
using

in

contrast,

potential

necessary

since

quadratic

results

use

quadratic

function

load

func-

amortized

integer

analy-

of previous

functions

local

potential

our

a number

potential

[2, 15] for

and

attempts

yielded

i, we denote

aa S~. Consider
T to be specified
at the

suboptimal

the

balancing.

start

start

Outline

The

of this

t ion 2 contains
performance
analyzes

the

In Section

definitions.

Section

single-port

algorithm.

performance

state

can

of the

Finally,

load

that

time

large,

is well

balanced

to dynamic

Section

to

the

Section

3.2

if the

6 presents

to space

limitations,

many

this

of

Leen

omitted

in

may

be found

version
in

the

on off-

proofs

paper.

where

u e

tokens
assuime
from

expansion

V

by

by

p.

that

(V, E)

with

a, we denote

w(v).

We

For

simplicity,

to every

minus

the

=

denote

heights

the

of all its

number

this

assign

edges,

height

Illclex

a partition

node

height

with
the

i is an integer

,S’>,] be Ui>j
\Ve define
index
bad

Si.

Similarly

index

i to

that
index

and

is not

we define

be good

good

i, we have

IS>,I

3

indices

we

.s >0

lS2il/(1

indices.

for

nl]

static

the

The

Ill this

section,

algorithm

for

any i.

Let

is described

Scj,

and

index.

S<j.

For

any
Since

Iwdes,

3.1
degree

balance

A,

the

0( (d log n)/a)
probubilzty.
Before
f:orcli ng to

For
d,

an
edge

that

at least

synchronous

in

greater

half

with

height

step

how

Imany

load

G

a,

initial

they

and

with

balances
steps,

the

set

contain.

step

token

with
into

It

a

follows

function,

and

the

increases,

that

the

increases.

any

sum

While

arriving
h + 1.

never

to be

is the

send:~ its
token

potential

In

when

of the

the

follow-

j’ is good,

network

before

potential

step

the

is at least

the

step,

where

later.

For

the

with

of height

because

the

of good

indices

on the

number

greater

than

greater
view

the

the total

potential

In

such

at most

j

is zero,

fraction

the
if the

then

is a constant

in

fraction

of nodes

The
(1)

a case

times

addition,

greater

and bad indices

analysis

than

In

Equation

If

potential

i.

a step.

i.

the total

1.

in such
in

of tokens

decreasing

in

exponential
satisfies

the

above.

indices

this

j +
there

defined

described

exponen-

a constant

a constant

to S<3

bound

then

551

is essentially

j’ is good,

we have

than

in-

for every

x is slower

i, then

and

all of the

th~nurnber
with

that

0(1)

set S>% grows

hence

greater

of tokens

d =

situation,

i “dominates”

at height

when

In this

increasing

of tokens

tokens

all indices

ac-

every

in which

decreasing

of S>j

In general,

high

scenario

exponentially

at height

drop,

sequence

i.e.,

i, and

an informal

let, us assume

expander,

size of the

at height

SYj

n

within

of nodes

the

we present

simplicity,

j are bad.

j,

potential
send

3.1,

For

of @(z)

the

im-

with

Theorem

i grows

to tokens

function

network

tokens

constant

network.

to be specified

decreasing

due

potential

balancing

arbztrary

we partition

a potential

network

height

potential

proof.

than

growth

amortized

every

of the

due to toke~s

1.1.

algorithm

node

never

constant

than

with

every

O((dA)/a)

steps.

(1)

the

any

during

Consider

tially

the

net-

expanszon

stngle-port
tokens

n]]

in which

<j,

in

in potential

proving

Q(l).

properties
Theorem

that

growth of S>i with

are good.

in Section

network

is a constant-degree

i greater

An

can be at most

the single-port

every

of the

of the

dices

model

we analyze

that

T/4

1 is a real

tokens

of a token

decrease

potential
single-port

all

height

is a real

A =

works
3.1

steps

as follows:

of the

definition

Before

where.

+ a/(2d)).

It follows

c >

h is assigned

an &vz fraction

rank

crlS>,l/2d.

a bad

in [1, 2[log(l+al(2~))

Analysis

<

/[zdJ) n ~ 1, there

bad

n]

(k%(l+LY/(2d))

<

S2j,

lSil

is called

l,\’> 0[/(l+a/(2d))10gIl+0

of the

if

time

of a token

{Si},

be empty

time

h we associate

the

we show

is the maximum

by

in any

[1, 2(log(l+~/(z~))

in at least

Similarly,

of the

expected

tokens.

S, may

that

outline

of V given

at

loss of gen-

are good

in

of those

potential

a token,
height.

G
Consider

With~out

R is defined

where

of

maximum

ing,

of

paper

a unique

of a node

lS<O]

Since at least half of

nl]

at height

The

potentials

potential

of tokens

average

at v. The

height

m

number

the

We

token

p. The

n nodes,

throughout

p is an integer.

[1, w(v)]

is its rank
amol)g

G

later.

transmitting

Complete

[14].

from

network

edge

ilr]d

at

v = a/ (cd),

of the

have

fact
any

half

~ : N ~

~ n/2

j

j is good

token

or

otherwise,

Preliminaries

For

every

steps,

net-

bounds

of the

Hence

IS>OI ~ n/2

steps,

an index

in at least

where

specified

proofs
2

is good

exists

with

either

is true.

5

balancing.

Due

the

former

p + i tokens

algorithm,

that

ifx

Section

asynchronous

tight

that

With

half

the

the

network

globally.

and

there

of ~(h),

half

having

of the

It holds

in [1,2 (log(l+ai(zd))

t,

IS>OI < n/2.

a locally

later.

assume

step

algorithm.
reach

even

Sec-

3.1 analyzes

multi-port

the

be quite

that

extensions

works.
line

as follows.

some

in a state

describes

is organized

of the

4, we show

balanced
starts

paper

T steps

of at least

the indices

remainder

set of nodes

first

of at least

erality,
1.3

the

the

of bad
j

that
than

is trivially

than
that

indices.

in reverse
for

each

or equal

j’ could
respects
We
and

index

i we can

otherwise,

any

consider

order

to i as bad.

preserved;

form

the upper
show

the
by

an

“view”

If i is bad,
there

is a

significant

potential

t,h is drop
order

to maintain

are

bad.

which

all

i

the

across

the

to rearrange

the

We then
indices

view

that

invoke

(S<;,

all indices

the

greater

cut

S>i),

the poten~ial

greater

argument

than

j

are

and

the

charges

in

the

sum

of S>i

for

the

and

Let

@t denote

IIII’i]

complete

potential

of the

set of tokens
in S<,.

mi

=

Illil.

i’ +

1, can

i’ < k < i.
relate

the
from

total

Let

a node
that

a node

drop

node

Si to

3.1,

as the
drop

in S>i

to

section

=

a node

M

at

nodes

p,

let

a(p)

v

after)

=

be

the

in S>J.

end

(resp.,

~

set

(Note

in

S>J

(resp.,

of tokens
that

tokens

also
b(p))

belong

that

are

l?[17j+l],

any

that

start

to

be the

M.)

height

from

For

tial

any

of S>

a lower

(d(~+

~

1)

-4(0)

~

that

(d(~+

1)

–d(~))

j<i<a(p)

=

(2PZ1V(1+”)*)+(P2+V)’+1)

=

k
third

We

i > j.)

There

Proofi

The

i z t, the

claim

the

the

+ V)j+l.

terms

=

debt,

index

j.

and

in the summa= V(l+v)i

we use to prove
we can

potential

drop

to every
below,

1 is the

with
pay

for

17[Q],

We consider

where

by I’,

analysis,

of cuz~(h)

that

on 13[W].
order,

lSil

s alS>i
S>,

alluded
Theorem

each
all

proof

the indices

maximum

token

i in [j, 1], we maintain
which

is the difference

edge e E E,

we place

from

the

fol-

<

Since

that

there

to S<i.

By

Lemma

mi
s

we

there

For
base

‘le

= O

+ v)?,

since

of–c.

consider

two

cases,

If i is good,

are at most

~lS>i

property
a lS>i //2

3.2 we have

i.

the

= O, we have

or bad.

are at least

0 such

on
for

(~v)lS>ll(l

expansion

9

+ V)i.

induction

choice

i is good

by the

& >

(&.v)lS2il(l

Therefore,

Since

step

l/(8d).

constant
<

reverse

1 by our

induction

l/2d.

a real
I?[I’i]

-I- V)l

the probability

is at least

trivially.

i = !.

we find

E[rt]

3.1.
at

place

in [j+

poten-

on E[ri],

any

matching

is by

holds

(W2)I,SZI(1

to Si,

13[~i]

1/2 edges

of the
edges

dethen

graph

from

S>i

~ aIS,i]/(16d).

we have
=

E[ri+l]

+ (eu2)lS2il(l

+ v)’

<

EII’i+l]

+ (&u2)lS2il(l

+ v)’

charges

and thus

total

to

token

the

bound

is obtained

exists

on whether

from

is the

that

we have

< l/c

the

pending

~(i+l)–~(i)

amortized

that

a charge

bound

Corresponding

rl

For

in the

all i > j,

Therefore,

We show

lower

given

)

we rearrange

section,

the expected

term,

+mj(l

W

using

in reverse

v)*

describe

in this
h.

total

across

we define

the upper

([15])

3.3

for

v = cr/(cd)

also use the equation

associate

we upper

of [15].

3.2

For
rniv(l+

step

We now

height

drop

(1 + v)=(p)

on l??[mi]

case we consider

earlier

words,

the expected

V)k,

~

to prove

e is selected

Thus,

f’or all

that

be viewed

P:Q(P)>J
to step t.

prior

j

lemma

Lemma

b(p)~i<a(p)

~
p6M,

and

i.e.,

expected
i > j,

=

bound

Lemma
that

p@M,

@t–l

In order

of p before

– fb(b(p)))

~

+

that

moved

lowing

((b(a(p))

PEM,

tions

In other

show

can

“V2)(P:ZJ1+
‘=(’V2)(PA:’+V

t.

step

=

the

We will
1,/]

by

adding

‘i=

t, W, to the

pcM

(Irl

i.

and

We also define

Let

klien

indices.

~mkv(l+

for

of i-drops

Note
fronl

r~+l

in [i+

i and

k>z

+mj(l+v)j+l.

al]d

at height

than

is calculated

from

indices

by the

for

vi

3.1

P1’c)ofi

that
for

Formally,

)7/:s.
Lemma

the

ri

O(~lS>il(l—
+ V)i).
It follows
from this
and the informal
argument
outlined
earlier

be paid

v c Si,,

of k-drops

step

of i-drop’s

of bad

greater

to a token

notion

in

an

node

sum

we use this

potential

due

p has

can

at height

i’ > i. Hence

by

bound

in this

sum

that

13[1’i]

upper

be the

to a node

a token

from

u c

Mi

in S>i

potential

be expressed

In Lemma
the

from

t >0.

if p moves

Thus,

on an edge

step

We say

—~(i),

in S<2.

moving

after

are sent

of @(i+l)

a node

i >

that

Let

i-drop

network

bound

the

for

due to tokens

is such

as a sequence

t of the algorithm.

step

to tokens

the

the charges

case in

bad

due

of i’-drops

subtracting

than

proof.
Consider

to

drop

can be used

–c~21S>i\(l
a

~

qri+l] –

– E[mi]v(l

+ v)’

+ V)i/16
(V2)lS~il(l

+ v)i(f(c,

a,d)

– c)

1, /?]
<

height.

(&V)lS>il(l
-(v2)IS2,1(1

a “debt”
between

<

552

(EV)ls>il(l

+ V)i+l
+ V)i(f(C,
+ P)i((l

CX,d) -

s)

+ U) – v(f(C,

~,d)

– &)/f),

~vt]ere

j’(c,

a, d)

ecluality

follows

N/(2d)).

The

tion

=

+ a/(2d))).

the

fourth

hypothesis.
The

c/(16(1

from

fact

(The

that

inequality

lS>~l

follows

z

from

third

in-

lS2il/(1

+

the

@t < 1 with
steps,

~ll,S>ll/(2d).

T’

induc-

ity.

lSi I ~

the average,

)

second

is

when

i

is

bad.

Thus

after

<

E[rz+,] + (EV2)[S2,1(1
+ v)’
(&r/) \s>il(l + V)’+1 + (wz)ls~zl(l + Vy

<

(&v)ls~*l(l

-1-v) ’((l

T’

and

the

potential

3.2

The

values

complete

for

c and

the

s such

induction

that

the

step

by determining

Ilold,

and

thus

establish

(cr/d)+4
and

(e.g.,

hence

cv/2)+v
the induction

constant

<1

(2)

<1

(3)

c = 5).

step.

greater

For this

v < (CV — cu2)/4.

section,

+ cv/2)

+ u

Since

than

‘I’l]us,

Equation

that

j(c,

Equation

(2).

choice

A,

E[rj+l]

Lemma

~

~;[?j~j]

z

~,

example,

c = 5 and

3.3 with

(cu)lS2j+11(l

~lS>jl/16d,

and

a

~

orem

=

<

E[rj+l]

+

V)j+l.

by

the

& = 5/48.)

If

1.

<

(&u)lS>j

s

r/lS>jl(l+

~

o,

[(l

For every

between

j’ is good

definitions

at least

exists

an index
T/4

kens,

then

@t denote

where

the

–

definitions

W and

r

=

of

W

step t, we have EII’]
(I), _, ( 1 – &r/2), where the
matching

[U[CD{+T]
over

<

all

the

setting

T

0,/4

By

@t+~
dard

z

most

@t/2

is at

00 + log n)l

most
steps,

~

n(l

+ v)~tl/v,

log

n –

log

u

Therefore,

@T/ >

for
for T’

is good

any

@t

v =

By

potential

drop

follows
The

proofs

to establish

the

T

is

steps.

4

Local

load

section,

in

probability

constant
<

= 0( Ad/cr+d2

using
in

Section

a > 0.
(A

+

that

is good

in

kens,

for

n, d and

any

algorithm

(to

p.

within

is Q(dnl/o).

time

to within

running

=

state.

Finally,

of Q (,unll’
we prove

at

reach

a locally

balanced

load

balancing

more

this

bound

in [15], i.e.,

token)

one token

but

an upper
from

to

global

that

from

have

in the

being

lo-

an expected

a locally

bound

the

take

than

we show
awi~y

to-

for

may

rather

) for reaching

time

the expected

algorithm

be one step

to
of [’2]

O(d log n/p)

We extend

state

be modof a.

algorithm
f2(nl/2)

Furthermore,

case, we may

balanced

locally

one

3.2

potential

k)e expensive

multi-port

this

3.1

Theorem
~3.2 can

to within

steps

Let

t of the

j is good,

c,n the

can

presented

c.

step

of # instead

take

to-

to Lemmas

3.1 and

can

at

other

chosen
after

bound

that
the

token

to all

&v2d2@t_1.

balancing

stan-

balanced

on the

a globally

time

to

balanced

state.

since

l)(v)

holds

2, there

t in which

in terms

1.1

load

T’

probability

nl]

step

lower

using

globally

cally

that

n/2

Section

analogous

balancing

than

single-port

<

claims
in every

we show

2d tokens

balancing

By

E[@t+~]

tokens.
Without

[S>,0 I ~
By

appropriately

of Theorems

local

expectation

for

bounds

perform

the

that

of the network

this

of additional

have

of The-

set of nodes

algorithm.

steps.

is at least

from

ified

described

~

dur-

to that

p + d – 1 + 2id

a potential

that

number

log@O

to within

■

Si be the

[1, 2[log1+a/(2~)

proving

the

this

and

assume

cz/cd2

single-port

1 with

n

tmbal-

balances

steps.

of zero

we

that

load

with

initial

of (1 + v) ~ tc, every

t. Thus,

show

G

and

and

E[@t]
~
over the

Therefore,

we can

we have

have

j’

obtain

network
a,

i, let

of these

in

therefore,
is taken

the

1/2.

[10],

multi-port

a potential

3.3 we show

drop.

we
If

in
, we

inequality,

+ I/na)

00

step

matchings

bounds

r,

W.

where

[(4 In 4)/(~v2)]
Markov’s

O(l/(*~)Q

in

– cvz)T/4,

random
=

Chernoff

8aT’((log

selected

@f(l

–

~ O, and
expectation

IIIg

ra]ldorr]

and

j

and

easily

– c/16)

cv2@t_1

3.1,

1.1.

multi-port

the potential

algorithm.

In

13y

redefining

steps.

assign

that

+v)J+l/(16d)

we
half

h ~ 2jd,

within

O~_l

prob-

Theorem

3.2 is similar

integer

of the

of generality,

We

of 17, I’j+l,

– alS>jl(l

V)J+l(E

that

high

deterministic

in O(A/cr)

p – d + 2id

T steps

height

~

+ v)~+l/(16d)

+ v)~+l

the

algorithm

of Theorem

at least

+ Uy+l

– alS>31(l

with

arguments

arbitrary

proof

before

to establish

i = j + 1, it follows

E[rj+l] – E[WL3](I

below

to show

proves

expansion

tokens

3,1,

loss

we have

/;[1’] =

above
This

an

edge

the multi-port

having

d, we find

set e = c/48

d,

log n)/cr)

The

to

+ cv/2)
+ cv/2)

Since

We now

n/2

~ I <

in Section

For

degree

ance

of c, v < (c–4)/c,

(1 +cu/2)/(1

is satisfied.

(For

Applying

aI)d

(3)

a, d) ~ c/24.

of tokens

argument

a ~ d,

or equal

(1 +2v+cv’/2)/(l

<

the

we analyze

3.2

nodes,

Therefore,

=

number

that

probabil-

model

described

Consider
(1 + v)/(1

repeat

multi-port

algorithm

0((d2
we can set c to be any

]S< – 2 ]Ogl+m,(zdj

implies

high

inequalities

a,d) -s)/&)

(1 +v)/(1+

in the

appropriately.

Theorem

((1+V) - V(f(c,

balance

then

which
O with

~ ] =

+v)/(l+cv/2)+v)
In this

We now

probability

we use an averaging

steps

ability,

high

IS> 2 bgl+a/(2d)

To establish

case

We have

I“’[l’t] <

we have
after

+

log n/a2),

for

553

We

now

G’,

the

defrne
network

a graph
used

for

G

which

will

be

showing

the

lower

the

basis

bounds

on

~ime

graph

for

local

load

balancing.

with

node

on n nodes,

(Iegree

cl.

.Y Q V.

Let

Let

neighbors.
least

IV(X)

Let

have

k be the
know
that

Ri

[Ril

index

;~~~?f’

Ri

iiode

of Lo;

let

G’

=

can not
the

=

i <

k

Add

[lode

and

Li

in

G – UkJ~ RJ~. Let

~ single

denote

ed~~

Note

that

the

addition
is Q(p).

all the results

in this

of the

expansion.

our

arguments

oi)e

call

network,

This

more

construct

for

only
of

G

and

so

the

Li al distribution
use

w(v)

.Agaln
tl}e

than

sets

denote

Ri’s

and

(note

Our

{L$,

para-

\Ve also

L

L, M

the same

as the

R,

X!

half
that

number

k +

Rl,

the

local

that

number

of nodes

{~(z)

algorithm

balance

locally

may

G’.

e from

moved

node

in

the

A 1“ for
7)(!
all

‘I’l IeII

For
all

all

ZE

Let

L,

L,

~ .C, let

w is globally

is Ilot

balanced

locally

are

+-~+
in

nodes
order

!2(1, we need
aI least

at least

and

that

any

node

any

node

in

then

at time

equal

for

let

w(z)
=

by at most

in Z is
following

in
G’

to move
to the

see Figure

+p)%-l

R,
to

as there
be

from
number

locally
7? to L

are

2(*

tokens

the

first

(ii)

We

of nodes

have

been

2d tokens

t’— 1, etc.
Li

had

at

have

through

the

ever

k.

Hence

token

than

moves

have
Thus

on any

at least
come

more
that

node

in L,

of tokens

on any

at time

t’each

the initial

Q

from

w(v’)

any

we must

2d tokens

number

i <

than

t’– i. Note

step

of tokens

O s

more

In general

at most

time

one more

show

w(y)

that

we can

– W(X)

Q steps.

s

2d,

“extra”
M

across

to

L,

edge

~ m–

(k+

only

have:

value

tokens
all

in

those

e, taking

2)d+

at

1 during

the

argument

fact

is sufficient

w(v)

~ m –

QX ~ Ri, Vy c Ri+l)VRi

A counting

in 7? and

It follows
steps,

– i)d.

ified

to

that

no

to show

that

claim
within

edge

on the
token

c

number

is ever

sent

this.

required

only

in terms

edge

e.

each

iteration

554

Any

time

since

for

of how

of
for

v sends

in any

above

can

height

model

of at most

or Li ‘s, and

consider

lower

on the

bound

balanced

tokens
with

single-port

e being

selected

mod(divide

m by 2d on each node

the

many

Q

Q steps.

be easily

Ri’s

a locally

a token

of the first

before

single-port

differences

is selected
the

either

balanced

the

above

to reach
edge

that

balanced
locally

consider

“ad~acent”

steps

(ii)

arguments

of tokens

in (i)),

expected

the

to hold

in R U R&,

between

(i) and
2d-locally

so G’ is not

in order

ization

of tokens

in 7Z across

and

the number

since

=
We

from

is not

Note

l)d.

+p)*-l

a number

all

must

%? to M

not

balanced

must

Q steps.

to v’ or%!

tokens,

.C.

in

time.

also

and

from

that

~ +

2d tokens

in

v’

sent

most

L~+l

no token

of tokens

to

1.

= (1

and

all

have

z in 7?,

O(y)

to within

a time,

on v’ must

must

is, we have

Q total

kd

= m–2(i–

2(i+

–

tokens

ever
for

in L1 at time

2d from

for

That

“extra”

M.

such

= m–
m

no token
initially,

of its neighbors

tfthe number

in L;+l,

Since

at
step

have

le~t

number

node

u)(z)

to within

2d).
(1

let

W(Z)

balanced

— ZV(V’) > /cd (~

There

c M,

every

that

at least
< w(v’),

=m–(k+2)d+lat

had

than

Z.

R~}.

steps

m is an integer

R,i cM,

L*,

z E Li,

IJ{II, it

z c Ri,

f2(n1f2)

for

where

L1

in L2 at time

sets).

of nodes

at

one token

node

node

k/2

in

saw

w(z)

~ w(v’)

t’–l<t–landw(v’)

z c L1 has at least

the

one

v’ has

Since

(we
w(z)

to any

2)d + 1 in the

t. Then

2 “extra”

v,

a token

of w(z).

of sets

in 7? for

take

Suppose

= 171+ 1 initially,

I)J ~ 2kd.

that

least)

72 after

The

to(u)

(at

differs

case.

I’or

z c L1 ) the

any

proceeds

> m – (k + 2)d + 1 for

initially.

time

to L1 and

1 consecu-

RJA,...,

number

v.

group

. . .. R%_l}

L~),
is such

We

of

at any

Lz

any

node

We

groups

of

{Ro,

the

on

1 distinct

2k +

7? =

and

fact

and

L~_l,R[(=

is roughly

use the

L

tokens

a group

. . .. L$_l}.

for

in

of

k is even.

into

G’.

of

z E L1,
from

these

than

claim

m – (k+

step

it had

tokens

t{,and v’ did not send a token to any node in L1 at
t’+ 1. This implies that v’ had at most 2d tokens
implies
that
more than any node in L1 at time t,which

to the ini-

nodes

assume

~,

...,

the

number

we have

L~+l,

choice

of sets

Li’s
and

that

{LO, LI,
=

the

over

we

sets,

(1 ve sets
L =

tokens

time

than

no token

time

a is the edge expansion

according

at some

s

that

of our

2d

M

from

0, durign

fewer

w(v’)

at
take

time

of making

previous

w(v’)
that

more

eattime

of

that

moves

i >

proof

Since
will

G’ to within

implies

had

The

l?.
e, it

balance

E X,

in Li

Suppose

t – 1. Actually

Nevertheless,

the

time

traversed

of the

in terms

sake

clear.

cases below

simplicity,

t’onsecutive

.Vt

of

to

for

two

all

moves

of 6’,
We present

first

node

show

Q steps.

for

in terms

the

and

where

We first

the

v to VI and

section

G as in

p = a/d,

(i)

not

rather

intuitive

as follows:

Li

in

no token

what

Li-l,

We

first

nodes
traverse

that

Q steps,

in Li - 1 initially.

v’ be the

edges

first
L~ to
every

$
can

to locally

node

to

expansion

the

2

to prove

Let

1. For

Q

a time

z fl(nljz)

from

2 tokens

of new

is done

a graph

grap]lsuch that

node

at

It is easy

corresponding

e connecting

of G’

expansion

the

have

since

be isomorphic

Rh

Q(Q)

moves

wlog

p <

time

we
token

steps

~ n/2.

for

one

to 7? in

~ z z,

n/p),

G – U~~~~j

at

~ n/2.

Suppose

most

tokens.

if we

U R-2),

Rj{

p

have

+ p)

by the

its edge

must

here we assume

= O(log

e.

any

at least

In general

I U$~~

expansion

We state

ROURI

that

be a

for

have

– (R:-I

be reduced

node

must

Then

such

let

G + e.

c E, x c X},

n/ log(l+p));

of simplicity,
Lk

(V, E)

p and maximum

in RO U RI.

not

k = logn/log(l
Let

G =

,u(l+p)i-lif lU~~~Rjl

k = O(log

the sake

= {v}

= N(Ri-1)

~

largest

that

y)

I?.

= IV(l?o).

neighbors

recursively

we must

{yl(z,

Then

RI

P( 1 + p)

define

=

v E V.

Let

expansion

need

one token
randomnumber

state

to traverse

O(i)

probability
algorithm.
at some

of

is given

Thus
time

step

the
at
the
is

f)(d).

Hence

time

we have

for G’,

I{e]]s, to reach
llt tile
The

local

the

[/,,

i~

all

: E L,,

(note

that

Li

eA4,1et

Li,

for

●J4),

Rj

to within

O(~)

tokens

1 token,

since

The

and

intuition

y E R;
for

this

]]) the

“right

direction”

anced

state

in a single

llul~lber
first

of tokens

step,

it will

If every

node

(JVC can

assume

I]eigllbor
time

in

step.

take

every

between

tokens

will

clearly

move

“up”,

The

be

Q(l R$_l

2.

tokens

in R;_l

local

nodes

in R~_l

continue

and

size

is

l/d)

(each

node

in R%_2).

Using

a similar

in

results

I]]g

node

care

of randomization)

see that

no

token

as in

ever

moves

the

from

and

sees

analysis

previous
M

to

at

(tak-

case,

Z

net-

that

if the

tokens,

then

to within

1 to-

high

probability.

during

each

direction

step

each

live.

The

and

we
that

node

e can

transmit
of its

The

[2].

(1)

For

node

each

live

u sends

otherwise

step

of the

u does

the

neighbor

a message

u sends

are
syn-

on dynamic
here

is

u maintains

every

neighbor

of the

algorithm

algorithm,

which

we

e“(v)

>

following:

w of

u,

consisting

a message

edges

node

of v for

in

at each

presented

Every

height

of the

static

to work

of e“ (w) at the start

In every

that
for

algorithm

as in

edges

ad,jacent

of the

)

dy-

a message

algorithm

value

D S, each

the

t. We assume

e“ (v)

call

the

to extend

An edge e c E

w of u.

(The

relates

models,

can be modified

same

netnet-

model,

which

networks.

synchronous

dynamically.

balancing

networks

multi-

networks.

if

knows

net-

local

synchronous

of [2] that

step

load

the

static

synchronous
t

of the

asynchronous

or succeed

step

multi-port

a~ynchronous

an estimate

12d,

u,( (J’) <
balanced,
for each
— m — $ + 1 or L is not locally
.L?(@)”initial
steps.
Thus,
once any of these to-

as for

asyn-

fcjr the

and

on dynamic

and

during

local

results

use a result

fail

and

a variant

sense

dynamic

may

live

that

to asynchronous

essentially

a match-

R~_l

then

is arbitrary.
least, one

same

synchronous

then

prove

synchronous

chronous

across

“down”,

We

our

is optimal

in the

In the

a single

R;_2

of such

with

3.2 to dynamic

algorithm

network

in R+

tokens

moving

expected

We first

our

bal-

a state.

in

A

balances

dynamic

we extend

works.

namic

move

node

balance

section,
of Section

port

has a distinct

if we move

to

works

in the

such

the

we show

steps

networks

model

if a large

some

subsequent

drop

steps

to within

locally

a token

in a potential

0(nA2/d)

balanced

algorithm

1, then

2d +

Similarly,

chronous

works.

a locally

to reach

balanced.

Extension

In this

all

1, for

direction”

with

node

hand,

the

tl)ese

will

time

~

within

G is globally

5

locally

See Figure

Otherwise,

Thus,

is globally

E
$);

>

if all

d.

ken in 0(ndA2)

=

For

it is not

– w(v)]

(u, v) resulting

~ k);

w(z)

– w(y)

“wrong

reaches

other

[lever
ing

step.

is matched

G’

the

least

the single-port

wisglobally

we reach

in the

a long

that

R+),

On

a [Ilatching

time

in R+ _l

but

case is that
initially,

move

~

i).

[w(u)
along

let

= m–(i–

W(Z)

UR$_2.

the

forallz

Thus

to within

have

let

= m–l;

= rn-(~+i).

we

that

c ~),

=m–(~–

balanced

x E R$_l

at

graph

Here

z ~ R&_l,

letw(z)

w(z)

L, c L, let w(z)

Ri

let w(z)

balanced
any

of

fl(pnlfz)

we

all
such

that

c R$,

or

Suppose

m is an integer

forallz
~+1

step

that

is transmitted

1 token

balance.

model.

z E Ra, i ~ $ – 2 (note

ZE

(u, v) such

to-

to within

one

local

of tokens:

= m + 1 (where

fbr

state

take

reach

single-port

In-($-i-–l);

expected
O(y)

work

may
to

distribution

for all

f2(dnli2)

to within

model.

time

following

take

balanced

algorithm

expected
consider

it will
balanced

a locally

single-port

w(z)

that

if G’ globally

if

w(u)

of w(u)

consisting

and

only

a token,

of w(u).

of the

kcIls reaches
they

v, it will

eventually

either

all such

n~ore token

Rk_2

the

z

cx[jected
is Q(d),

,*

and

time
the

(o wltl)ln

for

that

edge

expected

1 token

Convergence

v’.

7? to be locally
in 7? anymore

(1)

(And

the

initial

For

e“ (w)

balanced

each

to
if the

tokens
step

that
will

moved

reach

between

from

v’.

St.

Since

e to be in the

selected

for

G’

to reach

since

the

local

that

balance

locally

balanced

G is globally
subsequent

balances

t l~e potential

@ of the

the

is globally
At

steps

to within

network
balanced
any

step,

state.

is good

IS>OI

the

s

to

within

if there

Similarly,

multi-port

(w(v)
A

exists

e?(v)

Define
– p)z.

height

If index
of p after
height

for every

tokens,

(l+v)=

ifz

token

at height

555

j

j

is good

For

any

step

t,

~ 24jd–
the

1 Id,

total

potential

T/2

at the start
start

p, let
0.

start

For

nl]

of which
of step

ht (p)

t,

denote

convenience,

of D S by

ho (p).

ht (u) for every

node

u and

v = a/(cd2),

where

c >0

Let

later.
and

h, we associate

of at least

at the
>

as

loss of

[1, [2 logl+oi(z~j

token
t

having

tokens

without

start

in

of p at the

to be specified

an edge

@t denote

at the

v,

if the

by one.

the set of nodes

the steps

edge (u, v).

and

is increased

index

half

neighbor

p + 12d – 1 + 24id

for t ~ O, we define

is a constant

If

and

an

in at least

the

message

w(u)

lS>O I < n/2

n/2.

a live

the

of D S. We assume

exists

t a good step.

we denote

to within

2d tokens.

as ~V~v

the

We now

balanced

that

from

i, we denote
24id

T steps

There

steps.

matching

integer

Consider

to

a token,

p – 12d+

we call

a locally
graph

algorithm

@ = 0(nA2).

~ 4, and

received

according

contains

For every

(1 +p)2

time

in O(nAz/d)

network

1 <

message

is updated

message

or v has one

is .fl(pnllz).

A tokensl

tlletl

e onto

initially.)

all

in

for

are not

it had

traverse

generality

lI{~_il

l)rove

since

eventually

to

Ii+_,

will,
tokens

than

Hence

eventually

We (define

q$(x) to be

O otherwise.

With

a potential

of ~(h).

Let

after

step

of the

network

every

6

1.

We

analyze

sis over

the

D S by

means

of the

steps

of

an

amortized

algorithm.

Let

Et

drop

In this

ht-1(~)
Z
potential

section,

models.

d) – f#)(h,_l(v) + d)).

–

(U!.)6E{

the live

S1(NI, of Q during

following

tellti

al drops

step

euery

stc~] t we have

Tile

edges

jt

of G have

of DS,

crucial

lemma

to the

actual

an edge expan-

then

~ Q(v2d2@t–1).

for

every

the

step

5.2

For

any

mitaal

amortized

po-

distribution

and

any

LO prove

sues that
gains,
actual

p min{lx],

lzl}/(1

lost

prior

drop

when

decreases

actual

by a factor

an edge

for

(i)

and

By

5.1

6.2

5.3

7/(~u2d?)
such

t

5.1
degree

For

an

that

are good.

d and

initial

0(d2(log

There

A,

G

and

5.2.

there

is

either
value.

A

simple

the

variant
for

M alysis

of

DS,

asynchronous

then

load

m O(A/a)

for

the

USM1 for

asynchronous

boll rids.

Hence,

the

dynamic

multi-port

gorithm

balances

to within

O(A/cr)

on synchronous

in
As

to yield
local

0(d2

obtains

tight

the

denote

in a maximum

(V, E)

with

we haue

there

that

m(X)

node

?n(X)

>

a set X

exzsts

< KIX[/(1+

Any

on

terms

of

the

the

offline

function

we obtain

network

A

+ p)/pl

bounds

in

6.1,

imbalance

6.2 implies

A

G
can

[A(I

such

with

node

be globally

+ p)/pl

G and

steps

that

for

all

of graphs,

that

any

expanbalanced

steps.

an inittal

More-

load

distri-

algorzthm

to balance

our

local

takes

G to withzn

single-port

networks.

e.g.,

one

the

local

which

the

local

algorithm

is not

cube.

The

local

algorithm

balances

while

there

exists
for

proof

an

the

is optimal.

of Lemma

t( s t

Lemma

Any

for

exists

expanders,

for

A

for

network

optimal

O(A@i@)

hypercube

result

there

constant-degree

algorithm

algorithm

However,

which

The

with

n

edges

is the

hyper-

in fl (A log n)
time

load

time,

balancing

[33].

6.2 can be modified
the

multi-port

to establish

model.

A

tokens

in at most

[A/cr]

G,

balance

to

can

be

in

[2],

load

log n/a)

case

can

same

balancing
tokens

network

imbalance

steps

there
A

such

exists
that

to completely

can

G with
steps.

an initial
any

edge expansion

be globally

Moreover,
load

algorithm

baiance

balanced
for

d

every

net-

with

im-

distribution
takes

a and

to within

at least

[A/al

■

G.

9

[2],

the

6.3

initial
work

the dy-

shown

synchronous

networks

such

in at most

the following

balances

steps.

as suggested
networks.

V]/2

zmbalance

least

at

=

of V,

Moreover,

Lemma

optimal

a class

Lemma

de fifled

under

for

of this

if the live
a,

algozvthm

tokens

(V, E)

of V, let ~

■

Lemma

exists

network

zmbalance

num-

networks.

9

edge expansion

multz-port
n)/a)

that

G

X

ezzsts a network

[A(l

token.

is not

a threshold

such

X

of edges

network
subset

([28])

with

at least

should

network

2nz#(24jd)}.

arbitrary

t of G have

step

rIurtI zc synchronous
u!ttll[l~

t steps

@t, ~ max{@O/2,

Theorem
ut euely

integer

be any

of the jirst

that

nodes,

Let

G =

subset

balancing

2 tokens

bution
no

[28]

initial

there

algorithm
Lemma

minimum

~.

+ p).

invoking

p and

over,

(ii).

steps,

of the
below

is-

joins

tokens

su2d2))

potential

of 2 or falls

that

We show

Lemmas

0(1/(

when

We use a part

from

within

(ii)

“many”

steps.

two

any
any

of load

Lemma

(i) potential

and

24d tokens.

follows

that
the

across

to account

result

setting:

height,

to occur,

in previous

show

a step

events

height

main

\V’e first

occurs

by at least

above

potential
‘1’be

gains

multi-port

is synchronous.

any

(I+p)l

1 of

to within

in the dynamic

drop

~

to address

on the

network

and

problem

and

on arbitrary

the number
X

balancing

■

sion

5.2, we need

a token

differing

of the

II ave

arise

when

potential

rlodes
ally

may

i.e.,

Lemma

For

For

p and

m.

I n order

6.1

complexity

2 ‘ b’)

network

denote

expansion

Theorem

t’ > 0, we have

– 2@0 – 2rn#(24jd).

load

bounds

the

between

of size at most

drops.

load

balanc-

good

p).
Lemma

m(X)

Lemma

relates

the

model.

matching

N

potential

load

single-port

to balance

consider

single-port
V \ X,

If

5.1

tight

required
that

the

for both

We derive

of steps

We first

)L(u)>h(v)

Lemma

setting

We assume

((#)(h&l(u)

off-line

we analyze

in the off-line
ber

~

on

set

of

v, = ;

bounds

ing

analy-

be the

{(u U) : (u, ~) is live, u ~ S>j and ht-I(u)
24d}.
For every step t, we assign an amortized

Tight

6.3 implies

is asymptotically

that
optimal

the
for

local
all

multi-port

algorithm

networks.

References

be

time

[1] Y. Afek, E. Gafni, and A. Rosen. The

al-

applications
in dynamic
networks.
ACM
Symposium
on Principles
pages 35–46, August
1992.

in time

networks.

556

slide

mechanism

In Proceecl~ngs
oj Distributed

Oj

the

with
I fth

Computing,

[2]

W’. Aiello,
B. Awerbuch,
B. Maggs,
and S. Rae.
Approxinlate load balancing
on dynamic
and asynchronous
networks.
[n Proceedings
oj the 25th Annual
ACM
Sympostum
on the
7’heory

OJ Computing,

pages

632–641,

[~]

~1. Ajtai,
J. Korn16s,
a“d E.
parallel
steps.
Cornbtnatorica,

[-t]

J. Aspnes,
and

Herlihy,

multiprocessor

Annual

ACM

348–358,
[5]

M.

May

Shavit.

co-ordination.

Theory

oj

[21]

ACM
oj the 29rd

Computing,

[22]

pages

flow.

In Proceedings
oj Computer

[23]

[7]

Annual

[8]

p]

Sympostum

pages 358–363,

e92ce,

the Theory

on Foundations

October

and

Numerical
Methods.
Cliffs, NJ, 1989.

Chapter

A

M.

and

perfect

token

J. N. Tsitsiklis.

Frieze,

[IO]

H. Chernoff.
a hypothesis

[I I]

Y. C, Chow

In Proceedings

W.

in a heterogeneous

Tmnsactzons

Dynamic

multiple

[1 I]

oj the 19th

Inter-

load

system.

for distributed

[29]

IEEE
1980.

Adaptive

[30]

Corn-

[32]

and

D. Zuckerman.

ing algorithms.

Tight

Colmputer

Science,
March

[1 ~] B. Ghosh
parallel

Report

Carnegie

Mellon

Algor-athrns

University,

balancSchool

Dynamic

networks

and Architectures,
and

[33]

of

4ppl~cuttons,
A.

IHeirich

P. Hudak.

ance.
able

volume
and

Pittsburgh,

load balancing

by random

Research

Symposium

pages

226–235,

Implementing

1, pages

Report

on

[34]

In

on Parallel
June

1994.

functional

[35]

pro-

1< T.

489–503,

1989.

A parabolic
Caltech-CS-TR-93-

Computation

Herley.

I,ljormatzon
[19]

A

note

Processing

S. H. Hosseini,

B.

Lab,

on

the

token

Letters,

Computing,

1. JAJzi and

Litow,

10:160-166,
K.

W.

with

net-

Symposium

May

1992.

gradient
model load
on So.ftruare llngi-

balancing

for

distributed

oj the 6th In-

Symposium,

IEEE

A dynamic

provable

B. Monien,

networks:

pages

543–549,

ACM

Symposium
686-689,

F. Meyer

auf

on Parallel

der

Heide,

B.

Conductance

the

Symposium

Annual
pages

526–531,

C. Xu,

SE-11

Theoretical
Peleg

and

Journal

C.

Plaxton.

G.

and

Distributed

in

oj the
Process-

and

R.

Wzmka.

of Markov

October

Science,

Upfal.
Load

The

1985.

packet

token

routing

prob-

1987.

distribution

18:229-243,

drafting
on Sojtware

53:231-293,

balancing,

In Proceedings

Algorithms

Otto’ber

generalized

on Computtngy

oj

Compute~

Distributed

Transactions

~1161,

The

oj

1989.

IEEE

(10):115

chains

In Proceedings

on Foundations

Computer
E.

1993.
convergence

T. B. Gendreau.

and E. Upfal.

SIAM

and

of expanders.

for load balancing.

D. Peleg

balancing

In Proceedings

Osterdiekhoff,

and

treatment

Ni,

Load

Al-

1993.

token distribution.
In Proceedings
oj the
Colloquium
on Automata,
Languages
and

- a combinatorial

L. M.

June

1991.

pages 398–409,

Mihail.
30th

study.

bal-

In Pro-

on Parallel

164–172,

F. Ramme.

December

load

performance.

Symposium

pages

and

distributed

good

A comparative

ing, pages

D.

417–428,

problem.

1989.

selection

oj the 1989

and Architectures,

and
ACM

sorting

011

Symposium

pages 64–73,

Pasadena,

CA,

M.

Ryu.

distribution

Malkawi,

problem.

1991.
and

coloring
based distributed
of Parallel
and Distributed

1990.
Load

balancing

and

routing

Simulations

of three

scheduling

algorithms.

job

8:199-217,

1984.

R. Subramanian
load balancing.

on

557

ada)ptive,
Computer

June

decentralized
Networks,

and I. D. Scherson.
An analysis of diffusive
In Proceedings
oj the 1994 ACM
SympoAlgorithms

and

Architectures,

A. N. Tantawi
and D. Towsley.
Optimal
static
ing in distributed
computer
systems.
Journal

[37]

R. D. Williams.

32:445–465,

March

J. McPherson,

J. Stankovic.
controlled,

[36]

theory
of load bal25, Caltech
ScaL

28:329-334,

K. Vairavan.
Analysis
of a graph
load b=lamcimgalgorithrn.
Journal
[20]

B. Monien.

sium on Parallel
225, June 1994.

1993.
[18]

Liiling,

on Para//el

matchings.

ACM

S. Taylor.

Concurrent

and

and Architectures,

M.

ACM

In Proceedings

Processing

oj the 5th Annual

R.

Load

algorithms.

algorithm

the hypercube.

grams on a hypercube
multiprocessor.
In Proceedings
of the
;th Conference
on Hypercubes,
Concurrent
Computers
and
[17]

May

1989.

oj the 6th Annual

B. Goldberg

load

1995.

distributed

Proceedings

R. E. Tarjan,

of two local

290–300,

1987.

ceedings

lem.

S. Muthukrishnan,
Richa,

CMU-CS-95-131,

and S. Muthukri.hnan.
and

Maggs,
A. W.

analyses

Technical

PA

15213,

B. M.

R. Rajaraman,

235-241,

Small depth counting
Annual

pages

Monien.

bound

Engineering,

load shar-

B. Ghosh,

F. T, Leighton,

pages

M. Keller.
The
IEEE
Transactions

Parallel

algorithm

and J. Zahorjan.

pages

1992.

Science,

memory

Distributed

[31]

G, Plaxton,

and

B.

Strongly
adaptive
20th International

of
oj

bal-

November

and

and

Programming,
dynamic

R.

gorithms

3rd

and Prograrnfor tests
Annals

oj Computing,

of Computing,

13(1):32–38,

R. Luling

large

ing in homogeneous
distributed
systems.
IEEE
Transactions
on .$ojtware
Eng~neering,
SE–12(5):662–675,
1986.
(’

[I(,]

Near-

C–28(5):57-68,

load balancing

E. Lazowska,

[26]

Prentice–

E. Upfal.

processor

multiprocessors.
Journal
oj Parallel
putzng, 2(7):279–301,
1989.
D. Eager,

Sci-

1952.
for

Theory

Lin and
method.

ternational

[28]

Models

on Computers,

[1 ‘2] G. Cybenko.

[13]

7,

Languages

23:493–507,

Kohler.

on

Liiling

March

[27]

on Automata,
1992.

Statistics,
and

R.

and Distributed

A measure of asymptotic
efficiency
based on the sum of observations.

/\lathemat!cal
ancing

Parallel

E. Shamir,

distribution.

natlonu!
Collogutum
n~~ng, pages 308–317,

[25]

ancing

C’omputatzon:
Hall, Englewood
A.

F. C. H.
balancing

1989.

D. P. Bertsekas

Broder,

[24]

comoj the

oj Computer

oj Computing,

M. Klugerman and C. G. Plaxton.
oj the 24th
works. In Proceedings

neering,

oj Computing,

and N. Shavit.
End-to-end
overhead.
In Proceedings

on Theory

R. Karp and Y. Zhang. A randomized parallel branch-andoj the ;?Oth Annual
ACM
bound procedure.
In Proceedings

branch

B. Awerbuch,
Y. Mansour,
munication
with polynomial
30th

Symposium
1988.

on the Theory

[(>] B. Awerbuch
and T. Leighton.
Improved
approximation
algorit}~ms for the multi-commodity
flow problem
and local competitive
routing
in dynamic
networks.
In Proceedings
of the
on

and

1988.

approx-

oj the 34th Annual
Symposium
on Foundations
Sczence, pages 459–468,
October
1993.

26th Annual
ACM Sympostum
pages 487–496,
May 1994.

oj Parallel

May

local-control

for multi-commodity

.Journal
1992.

networks

Symposium
A simple

networks.
14:431-435,

in clogn

1991.
and T. Leighton.

algorithm

and related
Computing,

M. R. Jerrum
and A. Sinclair.
Conductance
and the rapid
mixing
property
for Markov
chains:
the approximation
of
the permanent
resolved.
In Proceedings
oj the 20th Annual

Counting

In Proceedings

on

Distributed

1993.

Szemer+di.
Sorting
3:1–19, 1983.
N.

Symposium

B, Awerbuch
imation

and

May

the hypercube

April

pages

220–

load balancoj the A CAf,

1985.
Performance

gorithms

for

unstructured

Practice

and Ezperierace,

of dynamic
mesh

load

calculations.

3(5):457–481,

1’391.

balancing
Concurrency.

al-

m-kd

Figure

1: The

initial

tokens

distribution

on G’ for

the

first

case.

m-k/2
m-(kf2-1)

m-(kL2+1)

m-1
m-(k-1)

m-1

m-k
m+ 1

m-(k-1)

m-(kf2+2)

m-(kjZ-3)
m-(kf2.-2)

m-(k/Z+l)
m-kjZ

00
L]

v’

(

~

. . . ~W1

~m

~+1

..

I-k.,

Lk = Rk

Rk.]

t .,

m-(k/2.

Rw+,

RUZ

Rw.l

“ ‘“

R2

00

R]

o~~
@~
)
000

/
e

Figure

2: The

initial

tokens

distribution

558

on G’

for

the

second

case,

“

1)

Improved Leader Election for Self-Organizing
Programmable Matter
Joshua J. Daymude

Robert Gmyr

Andréa W. Richa

Arizona State University

Paderborn University

Arizona State University

arXiv:1701.03616v2 [cs.ET] 10 Feb 2017

jdaymude@asu.edu
gmyr@mail.upb.de
aricha@asu.edu
Christian Scheideler
Thim Strothmann
Paderborn University

Paderborn University

scheideler@upb.de

thim@mail.upb.de

ABSTRACT
We consider programmable matter that consists of computationally limited devices (which we call particles) that are
able to self-organize in order to achieve some collective goal
without the need for central control or external intervention. We use the geometric amoebot model to describe such
self-organizing particle systems, which defines how particles
can actively move and establish or release bonds with one
another. Under this model, we investigate the feasibility
of solving fundamental problems relevant to programmable
matter. In this paper, we present an efficient local-control
algorithm which solves the leader election problem in O(n)
asynchronous rounds with high probability, where n is the
number of particles in the system. Our algorithm relies only
on local information (e.g., particles do not have unique identifiers, any knowledge of n, or any sort of global coordinate
system), and requires only constant memory per particle.

ber of rounds with high probability1 — an improvement over
our previous work, which only achieved its runtime bound
in expectation and was only specified as a high level (synchronous) algorithm. Moreover, due to its description as a
local control protocol which is executed by each particle independently, this algorithm is easier to implement than that
of the previous work2 .

1.1

Amoebot model

We represent any structure the particle system can form
as a subgraph of the infinite graph G = (V, E), where V
represents all possible positions the particles can occupy relative to their structure, and E represents all possible atomic
transitions a particle can perform as well as all places where
neighboring particles can bond to each other. In the geometric amoebot model, we assume that G = Geqt , where Geqt is
the infinite regular triangular grid graph. Figure 1(a) illustrates the standard planar embedding of Geqt .

Keywords
Leader Election; Self-Organizing Systems; Programmable
Matter

1.

INTRODUCTION

The vision for programmable matter is to create some material or substance that can change its physical properties
like shape, density, conductivity, or color in a programmable
fashion based on either user input or autonomous sensing of
its environment. Many realizations of programmable matter
have been proposed — including DNA tiles, shape-changing
molecules, synthetic cells, and reconfiguring modular robots
— each of which is pursu solutions applicable to its own
situation, subject to domain-specific capabilities and constraints. We envision programmable matter as a more abstract system of computationally limited devices (which we
refer to as particles) which can move, bond, and exchange
information in order to collectively reach a given goal without any outside intervention. Leader election is a central and
classical problem in distributed computing that is very interesting for programmable matter; e.g., most known shape
formation techniques for programmable matter suppose the
existence of a leader particle/seed (examples can be found
in [39] for the nubot model, [34] for the abstract tile self assembly model and [21, 22] for the amoebot model). In this
paper, we continue our research in [23] by presenting a local
control protocol that solves leader election in a linear num-

(a)

(b)

(c)

Figure 1: (a) shows a section of Geqt . Nodes of Geqt
are shown as black circles. (b) shows five particles
on Geqt . The underlying graph Geqt is depicted as
a gray mesh. A particle occupying a single node is
depicted as a black circle, and a particle occupying
two nodes is depicted as two black circles connected
by an edge. (c) depicts two particles occupying two
non-adjacent positions on Geqt . The particles have
different offsets for their head port labelings.
We recall the main properties of the geometric amoebot
model from [23]. Each particle occupies either a single node
or a pair of adjacent nodes in Geqt , and every node can be
occupied by at most one particle. Two particles occupying
adjacent nodes are connected by a bond, and we refer to such
particles as neighbors. The bonds not only ensure that the
1

An event occurs with high probability (w.h.p.), if the probability of success is at least 1−n−c , where c > 1 is a constant.
2
Given the complexity of the algorithm in [23], we opted to
present it at global scale without specifying the exact local
rules each particle would run.

particle system forms a connected structure, but also are
used for exchanging information as explained below.
Particles move through expansions and contractions: if a
particle occupies one node (i.e., it is contracted ), it can expand to an unoccupied adjacent node to occupy two nodes.
If a particle occupies two nodes (i.e., it is expanded ), it can
contract to one of these nodes to occupy only a single node.
Figure 1(b) illustrates a set of particles (some contracted,
some expanded) on the underlying graph Geqt . For an expanded particle, we denote the node the particle last expanded into as the head of the particle and call the other
occupied node its tail. For a contracted particle, the single
node occupied by the particle is both its head and its tail.
A handover allows two particles to stay connected as they
move. Two scenarios are possible: (1) a contracted particle
p can “push” a neighboring expanded particle q and expand
into a node previously occupied by q, forcing q to contract,
or (2) an expanded particle p can “pull” a neighboring contracted particle q to a node v it occupies, causing q to expand
into v and allowing p to contract.
Particles are anonymous; they have no unique identifiers.
Instead, each particle has a collection of ports — one for
each edge incident to the node(s) the particle occupies —
that have unique labels from the particle’s local perspective. We assume that the particles have a common chirality
(i.e., a shared notion of clockwise direction), which allows
each particle to label its ports in clockwise order. However,
particles do not have a common sense of global orientation
and may have different offsets for their port labels, as in
Figure 1(c). Adjacent particles establish bonds through the
ports facing each other. Whenever particles p and q share a
bond, we assume that p knows the label of q’s port which the
bond is incident to. Furthermore, we assume that p knows
whether q’s port belongs to the head or tail of q.
Each particle has a constant-size local memory that can
be read and written to by any neighboring particle. Particles exchange information with their neighbors by simply
writing into their memory. A particle always knows whether
it is contracted or expanded, and in the latter case it also
knows along which head port label it is expanded. We assume that this information is also available to its neighbors
(by publishing that label in its local memory). Due to the
constant-size memory constraint, particles know neither the
total number of particles in the system nor any estimate of
this number.
We assume the standard asynchronous model from distributed computing, where the particle system progresses
through a sequence of particle activations; i.e., only one particle is active at a time. Whenever a particle is activated, it
can perform an arbitrary bounded amount of computation
involving its local memory and the memories of its neighbors and can perform at most one movement. We define a
round to be complete once each particle has been activated
at least once.
The configuration C of the particle system at the beginning of time t consists of (i) the occupied nodes of Geqt , and
(ii), for each particle p, whether p is expanded or contracted,
its port labeling, and the contents of its local memory. For
more details on the model, please refer to [23].

1.2

Related work

A variety of work related to programmable matter has recently been proposed and investigated. One can distinguish

between active and passive systems. In passive systems,
the computational units either have no intelligence (moving
and bonding is based only on their structural properties or
interactions with their environment), or have limited computational capabilities but cannot control their movements.
Examples of research on passive systems are DNA computing [1, 6, 13, 37], tile self-assembly systems (e.g., the surveys
in [24, 33, 38]), population protocols [3], and slime molds [7,
31]. We will not describe these models in detail as they are
of little relevance to our approach. Active systems, on the
other hand, are composed of computational units which can
control the way they act and move in order to solve a specific task. We discuss prominent examples of active systems
here, as they are more comparable to our work.
In the area of swarm robotics, it is usually assumed that
there is a collection of autonomous robots that can move
freely in a given area and have limited sensing, vision, and
communication ranges. They are used in a variety of contexts, including graph exploration (e.g., [25]), gathering problems (e.g., [2, 15]), shape formation problems (e.g., [26, 35]),
and mimicking the collective behavior of natural systems
to better understand the global effects of local behavior
(e.g., [10]). Surveys of recent results in swarm robotics can
be found in [29, 32]; other samples of representative work
can be found in [4, 5, 16, 17, 18, 28, 30]. While the analytic techniques developed in swarm robotics and natural
swarms are of some relevance to this work, the individual
units in those systems have more powerful communication
and processing capabilities than in the systems we consider.
The field of modular self-reconfigurable robotic systems focuses on intra-robotic aspects such as design, fabrication,
motion planning, and control of autonomous kinematic machines with variable morphology (e.g., [27, 40]). Metamorphic robots form a subclass of self-reconfigurable robots that
share some of the characteristics of our geometric amoebot model [14]. Hardware development in the field of selfreconfigurable robotics has been complemented by a number
of algorithmic advances (e.g., [8, 35, 36]), but mechanisms
that automatically scale from a few to hundreds or thousands of individual units are still under investigation, and
no rigorous theoretical foundation is available yet.
The nubot model [11, 12, 39] by Woods et al. aims to provide the theoretical framework that would allow for more rigorous algorithmic studies of biomolecular-inspired systems,
specifically of self-assembly systems with active molecular
components. While there are similarities between such systems and our self-organizing particle systems, key differences
prohibit the translation of the algorithms and other results
under the nubot model to our systems; e.g., there is always
an arbitrarily large supply of “extra” particles that can be
added to the nubot system as needed, and the nubot model
includes a (non-local) notion of rigid-body movement.
The amoebot model [19] is a model for self-organizing programmable matter that aims to provide a framework for rigorous algorithmic research for nano-scale systems. In [23],
the authors describe a leader election algorithm for a abstract (synchronous) version of the amoebot model that decides the problem in expected linear time. Recently, a universal shape formation algorithm [22], a universal coating
algorithm [20] and a Markov chain algorithm for the compression problem [9] were introduced, showing that there is
potential to investigate a wide variety of problems under this
model.

1.3

Problem Description

We consider the classical problem of leader election in
a distributed system. A (distributed) algorithm solves the
leader election problem if it reaches a configuration such that
exactly one particle has irreversibly declared itself a leader
(e.g., by entering a terminal leader state), and all other particles will never become leaders. According to this problem
definition, the algorithm must terminate for the leader particle but does not have to terminate for non-leader particles,
though we will show in Section 4 how to achieve termination
for all particles. Throughout the paper, we assume that the
particle system is connected. For convenience and simplicity of presentation, we additionally assume all particles are
initially contracted; however, the description could be extended to handle configurations in which some particles are
initially expanded.

1.4

Our Contributions

In this paper, we present a randomized leader election algorithm for programmable matter that requires O(n) asynchronous rounds with high probability, where n is the number of particles in the system. We present our algorithm in
Section 2 and analyze its correctness and runtime in Section 3. In Section 4, we show how to adapt the algorithm
such that (i) all non-leader particles are in a specific nonleader state once the leader has been elected and (ii) we
achieve a success probability of 1.
This new leader election algorithm has the following advantages over our algorithm in [23]: (i) we describe a full
local-control protocol as opposed to merely sketching how a
high level protocol could be executed by individual particles,
(ii) correctness and runtime guarantees are proven for the
local-control protocol as opposed to doing so only for a simplified synchronous version of the algorithm as in [23], and
(iii) the asynchronous runtime guarantees hold with high
probability as opposed to just in expectation, which was the
case for the aforementioned synchronous version.

2.

ALGORITHM

Before describing the leader election algorithm in detail,
we give a short high-level overview. The algorithm consists
of six phases. These phases are not strictly synchronized
among each other; i.e., at any point in time, different parts
of the particle system may be executing different phases.
The first phase is boundary setup (see Section 2.1). In this
phase, each particle individually checks whether it is part
of a boundary of the particle system. Only the particles on
a boundary participate in leader election. Particles occupying a common boundary organize themselves into a directed
cycle. The remaining phases operate on each boundary individually. In the segment setup phase (see Section 2.2) the
boundaries are subdivided into segments: each particle flips
a fair coin, and particles that flip heads become candidates
which compete for leadership whereas particles that flip tails
become non-candidates which assist the candidates in their
competition. A segment consists of a candidate and all noncandidates following it along the boundary up to the next
candidate. The identifier setup phase (see Section 2.3) assigns a random identifier to each candidate. The identifier
of a candidate is stored distributively among the particles of
its segment. In the identifier comparison phase (see Section 2.4), the candidates use their identifiers to compete
for leadership using a token passing scheme. Whenever a

candidate sees an identifier that is higher than its own, it
revokes its candidacy. Whenever a candidate sees its own
identifier, the solitude verification phase (see Section 2.5) is
triggered. In this phase, the candidate checks whether it is
the last remaining candidate on the boundary. Finally, after
a candidate determines that it is indeed the last remaining
candidate on its boundary, it initiates the inside-outside-test
phase (see Section 2.6) to determine whether it occupies the
unique outer boundary of the system. If so, it becomes the
leader. Otherwise, it revokes its candidacy.

2.1

Boundary Setup

The first phase of the algorithm organizes the particle
system into a set of boundaries. This approach is directly
adopted from [23]. Nevertheless, we give a full description of
this phase to introduce important notation and fully specify
the leader election algorithm as a local-control protocol.
Let A ⊆ V be the set of nodes that are occupied by particles. According to the problem definition, the induced subgraph Geqt |A is connected. Consider the graph Geqt |V \A
induced by the unoccupied nodes in Geqt . We call a connected component of Geqt |V \A an empty region. Let N (R)
be the neighborhood of an empty region R in Geqt . Note that
all nodes in N (R) are occupied. We refer to N (R) as the
boundary corresponding to R. Since Geqt |A is a finite graph,
exactly one empty region has infinite size while the remaining empty regions have finite size. We define the boundary
corresponding to the infinite empty region to be the unique
outer boundary and refer to a boundary that corresponds to
a finite empty region as an inner boundary.
The particles determine the boundaries of the system in
an implicit and distributed way. The goal is to organize
the particles occupying a boundary into a directed cycle.
The particles set up the boundaries instantly using only local information and without any communication. Figure 2
shows all possible neighborhoods of a particle (up to rotation, which cannot be perceived by a particle) and the
corresponding results of the boundary setup phase. First,
a particle p checks for the two special cases depicted in the
top-most part of the figure. If p has no neighbors, it is the
only particle in the system (since the system is connected).
Thus, it immediately declares itself the leader and terminates. If all neighboring nodes of p are occupied, p is not
part of any boundary and terminates without participating
in the leader election process any further.
If these special cases do not apply, then p has at least
one occupied node and one unoccupied node in its neighborhood. Interpret the neighborhood of p as a directed ring
of six nodes that is oriented clockwise around p. Consider
a maximal sequences of unoccupied nodes (v1 , . . . vk ) in this
ring. Such a sequence is part of some empty region and
hence corresponds to a boundary that includes p. Let v0
be the node before v1 and let vk+1 be the node after vk in
the ring. Note that by definition, v0 and vk+1 are occupied.
Particle p implicitly arranges itself as part of a directed cycle
spanning the boundary by considering the particle occupying v0 to be its predecessor and the particle occupying vk+1
to be its successor for that boundary. It does this individually for each maximal sequence of unoccupied nodes in its
neighborhood. The resulting cases are depicted in Figure 2.
The remaining phases of the leader election algorithm operate exclusively on boundaries. Furthermore, they are executed on each boundary individually. The executions within

of unoccupied nodes in its neighborhood. Each agent is assigned the predecessor and successor that was determined by
the particle for the corresponding sequence. This effectively
connects the set of all agents into disjoint cycles spanning
the boundaries of the system, as in Figure 3. We refer to the
predecessor and successor of an agent a as a.pred and a.succ,
respectively. Note that a particle can now occur up to three
times (as different agents) on the same boundary. While we
can ignore this for most of the phases of our algorithm, we
will return to this fact when describing the solitude verification phase in Section 2.5.

Figure 3:
Boundaries and agents. Particles are
depicted as gray circles and the agents of a particle
are shown as black dots inside of the circles. After
boundary setup, the agents form disjoint cycles that
span the boundaries of the system. The solid arrows
represent the unique outer boundary and the dashed
arrows represent the two inner boundaries.

2.2

Figure 2: Possible results of the boundary setup
phase depending on the neighborhood of a particle.
In the special cases shown in the top-most part of the
figure the particle does not set up any boundaries.
The remainder of the figure is organized according
to the number of boundaries a particle is part of
from its local perspective. For each boundary, the
figure shows an arrow that starts at the predecessor
and ends at the successor for that boundary.

different boundaries do not interact with each other. Note
that a particle can have up to three maximal sequences of
unoccupied nodes in its neighborhood (see Figure 2). As
a consequence, a particle can be part of up to three distinct boundaries. However, a particle cannot locally decide
whether two distinct sequences belong to two distinct empty
regions or to the same empty region. To guarantee that the
executions on distinct boundaries are isolated, we let the
particles treat each sequence as a distinct empty region. For
each such sequence, a particle executes an independent instance of the same algorithm. We say a particle acts as a
number of distinct agents, one for each maximal sequence

Segment Setup

Since this phase and all following phases operate on each
boundary individually, we only consider a single boundary
for remainder of the algorithm description. The goal of this
phase is to divide the boundary into segments. To achieve
this, each agent flips a fair coin. The agents for which the
coin flip comes up heads become candidates and the agents
for which the coin flip comes up tails become non-candidates.
In the following phases, candidates compete for leadership
among each other while non-candidates assist the candidates
in their competition. A segment is a maximal consecutive
sequence of agents (a1 , a2 , . . . ak ) such that a1 is a candidate,
ai is a non-candidate for i > 1, and ai = ai−1 .succ for
i > 1. The maximality condition implies that the agent ak+1
following ak has to be a candidate. We say the segment
(c = a1 , a2 , . . . ak ) belongs to candidate c and refer to it
as c.seg. In the following phases, each candidate uses its
segment as a distributed memory.

2.3

Identifier Setup

After the segments have been set up, each candidate generates a random identifier by assigning a random digit to
each agent in its segment. These identifiers are used in the
next phase to elect a leader. Note that the term identifier
is slightly misleading in that two distinct candidates can
have the same identifier (in fact, this will happen quite frequently). Nevertheless, we hope that the reader agrees that
the way these values are used in the following phase makes
this term an apt choice.
To generate a random identifier, a candidate c sends a
token along its segment in the direction of the boundary.

During its traversal of the segment, the token assigns a value
chosen uniformly at random from [1, r − 1] to each visited
agent where r is a constant that is fixed in the analysis. The
identifier generated in this way is a number with radix r
consisting of |c.seg| digits where c holds the most significant
digit and the last agent of c.seg holds the least significant
digit. We refer to the identifier of a candidate c as c.id.
When comparing identifiers of different lengths, we assume
that the shorter identifier is (virtually) padded with the appropriate number of leading zeros so that longer identifiers
are always strictly greater than shorter identifiers.
After generating its random identifier, each candidate creates a copy of that identifier which is stored in reversed digit
order in its segment. This step is required as a preparation
for the next phase. To achieve this, we use a single token
that moves back and forth along the segment and copies
one digit at a time. More specifically, we reuse the token
described above that generated the random identifier. Once
this token reaches the end of the segment, it starts copying
the identifier: It reads the digit of the agent at the end of
the segment and then moves to the beginning of the segment. There, it stores a copy of that digit in the candidate
c. It then reads the digit of c and move back to the end of
the segment where it stores a copy of that digit in the agent
at the end of the segment. It proceeds in a similar way
with the second and the second to last agent and so on until
the identifier is completely copied. Afterwards, the token
moves back to c to inform the candidate that the identifier setup is complete and to start the identifier comparison.
Note that for ease of presentation we deliberately opted for
a simple algorithm over a fast algorithm for copying the reversed identifier. As we will show in Section 3, the runtime
of this simple algorithm is dominated by the runtime of the
following phase so that the overall asymptotic runtime of
the leader election algorithm does not suffer.

2.4

Identifier Comparison

In this phase, each candidate compares its own identifier
with the identifier of every other candidate on the boundary. A candidate whose identifier is not the highest identifier withdraws its candidacy, whereas a candidate with the
highest identifier eventually progresses to solitude verification. To achieve the comparison, the non-reversed copies of
the identifiers remain stored in their respective segments as
is, while the reversed copies move along the boundary as a
sequence of tokens. More specifically, a digit token is created for each digit of a reversed identifier. A digit token
created by the last agent of a segment (i.e. the most significant digit of a reversed identifier) is marked as a delimiter
token. Once created, the digit tokens traverse the boundary
against the direction of the cycle spanning it. Each agent is
allowed to hold at most two tokens at a time. The tokens
are not allowed to overtake each other, so whenever an agent
stores two tokens, it keeps track of the order they were received in and forwards them accordingly. At most one token
is forwarded at a time. We define the token segment of a
candidate c as the sequence of digit tokens created by the
agents in c.seg. We refer to the identifier stored in a token
segment as the token identifier of that segment.
A candidate c compares c.id with a token identifier when
the digit tokens of the corresponding token segment traverse
c.seg. The digit tokens are never stalled by this comparison.
The comparison of c.id with a token identifier has one of

three possible results: (i) the token segment is longer than
c.seg or the lengths are equal and the token identifier is
strictly greater than c.id, (ii) the token segment is shorter
than c.seg or the lengths are equal and the token identifier
is strictly smaller than c.id, or (iii) the lengths are equal and
the identifiers are equal. In the first case, c does not have the
highest identifier and withdraws its candidacy. In the second
case, c is still possibly a candidate with the highest identifier and therefore remains a candidate. Finally in the third
case, c initiates solitude verification. Identifier comparison
and solitude verification are executed in parallel and operate
independently of each other. Note that solitude verification
might be triggered quite frequently, especially for candidates
with short segments. We describe how to deal with this fact
in the following section.
To explain the token passing scheme of the identifier comparison, we consider a candidate c at the very beginning of
the identifier comparison phase. Both an agent and a token
can be either active or inactive. Each agent is initially active. Tokens are initially inactive, but become active once
they are forwarded by a candidate into a new segment. Let c0
be the candidate succeeding c in direction of the cycle. Since
the digit tokens move against the direction of the cycle, the
token segment of c0 moves along c.seg. When an active agent
receives an active token, both turn inactive and we say the
agent and the token matched. When matching, the agent
compares its digit with the digit of the token and stores the
result of the comparison for future reference. During this
process, the agent holding the least significant digit of c.id
matches with the token holding the least significant digit of
c0 .id, the agent holding the second to least significant digit
of c.id matches with the token holding the second to least
significant digit of c0 .id, and so on. The lengths of the identifiers are compared as follows: If the delimiter token of c0 is
matched with c, both segments have the same length. If the
delimiter token of c0 is matched with another agent of c.seg
(and is therefore inactive once it reaches c), c0 .seg is shorter
than c.seg. Finally, if a non-delimiter token of c0 is matched
with c, the segment of c0 .seg is longer than c.seg. Consequently, c can distinguish these three cases once it holds the
delimiter token of c0 . For the case that the segment lengths
are equal, c can compare its identifier with the token identifier of c0 as follows. As described above, the agents of c.seg
store the results of the comparisons of the individual digits
of c.id and c0 .id. When the delimiter token of c0 traverses
c.seg, it keeps track of the comparison result of the most
significant digit for which c.id and c0 .id differ. It can do so
because during its traversal it sees the consecutive digit-wise
comparisons starting with the least significant digit. Once c
receives the delimiter token of c0 , it can use the information
stored within it to decide whether the identifiers are equal
or, if not, which identifier is larger.
It remains to describe how the agents and tokens are prepared for subsequent comparisons. More concretely, we have
to specify when inactive agents and tokens are activated
again, and when the comparison results stored in the agents
are deleted. As described above, an inactive token is activated again when is forwarded by a candidate into a new
segment. The remaining tasks are the responsibility of the
delimiter tokens: When a delimiter token enters an agent,
it triggers the computation described above and then activates the agent and deletes the comparison result the agents
stored. Finally, note that candidates that withdrew their

candidacy still take part in the identifier comparison to a
certain degree: Since the agents in its segment still match
with incoming tokens, the candidate has to keep activating
these tokens when it forwards them to the segment of the
preceding candidate. However, candidates that withdrew
their candidacy will never progress to solitude verification
and are treated as non-candidates in the solitude verification phase.

2.5

Solitude Verification

Whenever a candidate c sees a segment identifier that
equals c.id during the identifier comparison phase, it executes solitude verification to determine whether it is the
only remaining candidate on the boundary. Note that the
solitude verification phase runs in parallel with the identifier comparison phase and does not interfere with it. The
protocol given in this section is based on the same ideas as
the protocol for solitude verification given in [23]. However,
the new protocol given here is much simpler.
From a global point of view, c needs to determine whether
the next candidate in direction of the cycle is itself or a another agent. Therefore, the solitude verification does not
only span c.seg but also the following segments that belong
to former candidates: namely, agents that initially were candidates but withdrew their candidacy in the identifier comparison phase. We refer to the sequence of agents up to the
next active candidate as the extended segment c.seg∗ . The
basic idea of the algorithm is the following. We treat the
edges that connect the agents on the boundary as vectors
in the two-dimensional Euclidean plane. A candidate c can
only be the last candidate on the boundary if the vectors
corresponding to the edges traversed when going from c to
the next active candidate sum up to the zero vector (i.e.,
both occupy the same node). To determine whether this
is the case, c locally defines a two-dimensional coordinate
system (e.g., the coordinate system depicted in Figure 5)
and uses a token passing scheme to determine whether the
x- and y-coordinates of the vectors respectively sum up to
0. In the following we only describe the protocol for the
x-coordinates; the protocol is executed in parallel for the
y-coordinates.
First, c sends an activation token through c.seg∗ to the
next active candidate. Whenever the token moves right (i.e.,
in positive direction of the x-axis as locally defined by c),
it creates a +1 token that is sent back along the boundary
towards c, and whenever the token moves left, it creates a
−1 token that is also sent back towards c. The +1 tokens
and −1 tokens move independently of each other. However,
a token of either type cannot overtake another token of the
same type. Each agent can hold at most two tokens of each
type. The tokens are forwarded as close as possible to c;
a token is settled if it cannot move any further towards c.
Each token holds a bit that specifies whether it is settled.
The bit is initially false. It is set to true if the token is held
by c itself or if the token is held by an agent a such that
a.pred already holds two settled tokens of the same type.
Consequently, both token types accumulate in a sequence
of consecutive agents starting from c. After the activation
token completely traverses c.seg∗ , it moves back towards c
while staying behind the +1 and −1 tokens. When it first
encounters an agent a in front of it which holds a settled
token, it moves to a and waits until all tokens at a are either
forwarded or settled. At this point, the activation token can

compare the total number of +1 and −1 tokens in c.seg∗
(corresponding to the sum of the x-coordinates of the vectors
of c.seg∗ ) by using the following local rule: the sum is zero
if and only if a holds the same number of settled +1 and
−1 tokens. The activation token then moves back to c and
reports the result. Once c receives the result for both the
x- and the y-coordinate, it knows whether it is the only
remaining agent on the boundary.
There are some details that have to be handled carefully
for the algorithm to work correctly. First of all as mentioned
in Section 2.1, a particle can occur up to three times (as different agents) on the same boundary. Therefore, there can
be distinct agents on the same boundary that occupy the
same node in the grid. If an extended segment reaches from
one of these agents to the other, an agent could wrongly
determine that it is the only agent on the boundary. To
avoid this pitfall, each particle assigns a locally unique agent
identifier from {1, 2, 3} to each of its agent in an arbitrary
way. When the activation token reaches the end of the an
extended segment, it reads the agent identifier of the candidate at the end of the extended segment and carries it back
to c. It is easy to see that c is the only agent on the boundary if and only if the vectors sum up to the zero vector and
the agent identifier stored in the activation token matches
the agent identifier of c. Furthermore, a solitude verification may be triggered before a previous solitude verification
finishes. In this case, the new solitude verification is simply
ignored. Finally, a candidate that is eliminated in the identifier comparison phase while a solitude verification is running
does not directly withdraw its candidacy, but waits until its
ongoing solitude verification is finished and then withdraws.

2.6

Inside-Outside-Test

Once a candidate determines that it is the only remaining
candidate on a boundary, it initiates the inside-outside-test
to determine whether it lies on the unique outer boundary
of the particles system. If this is the case, it declares itself the leader. Otherwise, it revokes its candidacy. To do
so, we make use of the observation that from a global perspective the outer boundary is oriented clockwise while an
inner boundary is oriented counter-clockwise (see Figure 3).
This is a direct consequence of the way the predecessor and
successor of an agent are defined (see Section 2.1).
The candidate can distinguish between these two situations by a simple token passing scheme that was introduced
in [23]. It sends a token along the the boundary that sums
up the angles of the turns it takes, see Figure 4. When the

Figure 4: The angle between the directions a token
enters and exits an agent.
token returns to the candidate its value represents the external angle of the polygon corresponding to the boundary

while respecting the rotation of the cycle. Consequently, it
is 360◦ for the outer border and −360◦ for an inner border.
The token can represent the angle as an integer k such that
the angle is k · 60◦ . Furthermore, to distinguish the two possible final values of k it is sufficient to store k modulo 5, so
that the token only requires 3 bits of memory.

3.

ANALYSIS

We now turn to the analysis of the leader election algorithm. We first show the correctness of the algorithm in
Section 3.1 and then analyze its runtime in Section 3.2.

3.1

Correctness

To show the correctness of the algorithm we have to prove
that eventually a unique leader is established. Note that no
leader can be established on an inner boundary: even if the
algorithm reaches the point at which there is exactly one
candidate c on some inner boundary, c will not claim leadership because it cannot pass the inside-outside-test described
in Section 2.6. Therefore, we can focus exclusively on the
behavior of the algorithm on the unique outer boundary.
We first show that w.h.p. the identifiers of the candidates
on the outer boundary are such that one identifier is strictly
larger than all other identifiers. Let L be the length of the
outer boundary. We have the following lower bound on L.
√
Lemma 1. L > n.
Proof. We define a coordinate system by picking an arbitrary node of Geqt as the origin and choosing an x- and
y-axis as depicted in Figure 5. Consider the axis-aligned

Proof. Pick an arbitrary agent a1 on the boundary and
define a = (a1 , a2 , . . . , aL ) where ai = ai−1 .succ for 2 ≤
i ≤ L. Define b = (b1 , b2 , . . . , bL ) where bi = 1 if the coin
flip of ai came up heads and bi = 0 otherwise. We assume
n ≥ 2 and define m = 0.25 log n. We first show that b
contains a subsequence of length at least m such that all
elements of the subsequence are 0. For this, we divide b into
consecutive subsequences
of length m. By Lemma 1, we
√
get k = bL/mc > b n/mc such subsequences. Let b(i) =
(b(i−1)m+1 , . . . , bim ) be the i-th such subsequence where 1 ≤
i ≤ k. Let Ei be the event that all elements of b(i) are 0.
We have Pr[Ei ] = (1/2)m = n−1/4 . Since the events Ei are
independent, we have
"
#

k
\
Pr
Ei = 1 − n−1/4
i


b√n/(0.25 log n)c
< 1 − n−1/4

0.25√n/(0.25 log n)
< 1 − n−1/4

n1/4 n1/4 / log n
−1/4
=
1−n
1/4

≤ e−n

/ log n

.

Therefore, we have that for n sufficiently large the entries of
one of the subsequences b(i) are all 0, w.h.p. It follows that
for n sufficiently large, b contains a subsequence of at least
m entries that are 0, w.h.p.
It remains to show that at least one element
of b is 1.
√
This holds with probability 1 − 2−L < 1 − 2− n , according
to Lemma 1. Therefore, for n sufficiently large there is a
segment of size at least m, w.h.p.
Finally, we show that one of the identifiers of maximal
length is indeed strictly larger than all other identifiers.
Lemma 3. For n sufficiently large, there is a candidate
c∗ such that c∗ .id > c.id for every candidate c 6= c∗ , w.h.p.

Figure 5:
Coordinate system and bounding box
(dashed) of a particle system.
bounding box of a particle system with n particles, see Figure 5. We define the length of a side of this bounding box as
the number nodes it spans. One of the
√ sides of this bounding box has to be of length at least n since otherwise the
number of particles in the system would be strictly smaller
than n. Let k be the length of a longest side
the bounding
√ of √
box. By definition, we have L ≥ 2k ≥ 2 n > n.
Recall that when identifiers of different lengths are compared, the shorter identifier is (virtually) padded with leading zeros and therefore a longer identifier is always strictly
larger than a shorter identifier. The following lemma gives
us a lower bound on the length of the longest segment which
is equivalent to the number of digits in the longest identifier.
Lemma 2. For sufficiently large n, there is a segment of
length at least 0.25 log n, w.h.p.

Proof. Let C be the set of candidates, let M be the set
of candidates with maximal segment length, and let c∗ ∈ M
be some candidate with the highest identifier among the
candidates in M . Since c∗ has maximal segment length, we
have c∗ .id > c.id for all c ∈ C \ M . It remains to show that
c∗ .id > c.id for all c ∈ M with c 6= c∗ . This is the case if the
identifier of c∗ is unique. By definition, the identifiers of the
candidates in M all consist of the same number of digits. By
Lemma 2, this number of digits is at least 0.25 log n. Each
digit is chosen independently and uniformly at random from
the interval [1, r − 1]. Therefore, for any candidate c ∈ M
with c 6= c∗ the probability that c.id = c∗ .id is at most
(r − 1)−0.25 log n . Applying the union bound gives us that
the probability that there exists such a candidate is at most
(|M | − 1) · (r − 1)−0.25 log n < n1−0.25 log(r−1) , which shows
the lemma.
Based on Lemma 3, we can now prove the correctness of
the algorithm.
Theorem 1. The algorithm elects a leader, w.h.p.
Proof. Let c∗ be the candidate with the highest identifier on the outer boundary which is well-defined by Lemma 3.

According to the protocol, c∗ never withdraws its candidacy.
Furthermore, for each candidate c 6= c∗ , the token segment
of c∗ eventually traverses c.seg and therefore c eventually
withdraws its candidacy. So eventually, c∗ is the only remaining candidate on the boundary. Once this is the case,
the token segment of c∗ eventually traverses c∗ .seg which
causes c∗ to trigger the solitude verification phase. Since
c∗ is the only remaining candidate, it eventually passes the
solitude verification. It then executes the inside-outside-test,
eventually determines that it lies on the outside boundary,
and declares itself the leader.

3.2

Runtime

As in the proof of correctness, we can focus on the unique
outer boundary. The first two phases of the algorithm,
namely boundary setup and segment setup, require only
local information. Hence, these phases can be completed
instantly by each particle upon its first activation. Therefore, the segments are established after one round. For the
identifier setup, which is the next phase of the algorithm,
we have the following lemma.
Lemma 4. For a segment of length `, the identifier setup
takes O(`2 ) rounds.
Proof. In the identifier setup phase, a token that is created by the candidate first traverses the segment to establish
the random digits of the identifier. Once it reaches the end
of the segment, the token creates a copy of the identifier that
is stored in reverse order in the segment. For this, the token moves in an alternating fashion back and forth through
the segment. Finally, once the copying is complete, the token moves back to the candidate. It is easy to see that the
overall number of steps taken by the token is O(`2 ).
After the first round, the segment is established and therefore the token can move without being stalled. In each further round, the token can take at least one step along its
trajectory. Hence, after O(`2 ) rounds, the identifier setup
for the segment is complete.
To bound the number of rounds required to complete the
identifier setup phase for all segments on the outer boundary,
we have to bound the maximal length of a segment.
Lemma 5. The longest segment length is O(log n), w.h.p.
Proof. Consider an agent a and a constant α ∈ N. The
probability that a becomes a candidate with a segment of
length at least α log n is at most (1/2)α log n = n−α . Since
there are n particles in the system and each particle represents at most 3 agents, there are at most 3n agents on the
outer boundary. Applying the union bound shows that the
probability that there is a segment of length at least α log n
is at most 3n1−α , which shows the lemma.
By combining the previous two lemmas, we get the following corollary.
Corollary 1. All segments complete the identifier setup
phase within O(log2 n) rounds, w.h.p.
After the identifiers are generated, they are compared in
the identifier comparison phase. In this phase, a set of
tokens, one for each agent on the boundary, traverses the

boundary. Each agent stores at most two tokens. The tokens are not allowed to overtake each other, so agents maintain the order of the tokens when forwarding them. Upon
activation, an agent a forwards a token whenever a.succ can
hold an additional token. At most one token is forwarded
at a time. We define the number of steps a token took as
the number of times it was forwarded from one agent to the
next since its creation. Let T be the first round in which all
segments finished the identifier setup phase. We have the
following lemma.
Lemma 6. At the beginning of round T +i for i ∈ N, each
token of the identifier comparison took at least i steps.
Proof. We bound the number of steps a token took by
comparing the asynchronous execution of the token passing
scheme with a fully synchronous token passing scheme in
which tokens move in lockstep. This approach is very similar to the movement schedules introduced in [22]. For the
synchronous execution, we assume that each token is initially stored at the agent that created it. We refer to this
point in time in the synchronous execution as round 0. The
tokens then move in lockstep along the boundary so that in
every round each agent stores exactly one token. For a token
t let si (t) be the number of steps t took by the beginning
of round i of the synchronous execution. By definition, we
have si (t) = i. Similarly, let ai (t) be the number of steps
t took at the beginning of round T + i of the asynchronous
execution of the actual token passing scheme. We show by
induction on i that ai (t) ≥ si (t) for all tokens t.
The statement holds for i = 0 by definition. Suppose that
the statement holds for some round i ≥ 0. We show that
the statement holds for round i + 1. Consider a token t.
If ai (t) > si (t) then ai+1 (t) ≥ ai (t) ≥ si (t) + 1 ≥ si+1 (t).
So assume ai (t) = si (t). To show ai+1 (t) ≥ si+1 (t), we
have to show that t is forwarded in round T + i of the asynchronous execution. Recall that in an asynchronous round,
each particle is activated at least once and therefore also the
agent holding t is activated at least once. Since the tokens
do not overtake each other, their order along the boundary (considering both a token’s position on the boundary as
well as which token is forwarded first if an agent holds two
tokens) is well-defined and remains unchanged in both the
synchronous and the asynchronous execution. Let t0 be the
next token along the boundary from t. Since in the synchronous execution t0 started one agent ahead of t and we
have ai (t0 ) ≥ si (t0 ) = si (t) = ai (t) by the induction hypothesis and our assumption, t0 is at least one agent ahead of t in
the asynchronous execution. By an analogous argument one
can see that the token t00 following t0 is at least two agents
ahead of t in the asynchronous execution. Since the tokens
preserve their order, there are no other tokens in between
t, t0 , and t00 . Therefore, the agent to which t should be forwarded in round i of the asynchronous execution holds at
most one token (which is t0 ). Furthermore, even if the agent
holding t at the beginning of round i holds an additional
token, t is forwarded first because the order of the tokens is
preserved. Therefore, once the agent holding t is activated,
it indeed forwards t.
Next, we analyze the runtime of the solitude verification
phase.
Lemma 7. For an extended segment of length `, the solitude verification phase takes O(`) rounds.

Proof. The token passing scheme of the solitude verification is executed independently for the x- and the y-axis.
We consider one of these executions and show that it takes at
most O(`) rounds. First, the activation token moves through
the extended segment and creates +1 and −1 tokens. Since
the activation token moves through the extended segment
unhindered, this traversal takes O(`) rounds. Then, the activation token moves back towards the candidate but now
stays behind the tokens it created. The two types of tokens
created by the activation token move back towards the candidate independently of each other. However, two tokens of
the same type cannot overtake each other. Once all tokens
of both types settled, the activation token can move back to
the candidate unhindered which takes another O(`) rounds.
It remains to determine the time until all tokens settle.
Our line of argument is similar to the one in Lemma 6.
Consider a single token type. Without loss of generality, we
refer to the round in which the activation token creates the
last token of the considered type as round 0. We consider
the following synchronous execution. In round 0 of the synchronous execution, each token is stored at the agent that
created it. The tokens then move in lockstep towards the
candidate. Apart from the synchronous movement, the algorithm for the synchronous execution works the same way as
the actual algorithm: Each agent can store two tokens and
the tokens move as close to the candidate as possible. We
assign the numbers 0, . . . , ` − 1 to the agents of the extended
segments starting with 0 at the candidate. For a token t let
si (t) be the number assigned to the agent that holds t at
the beginning of round i of the synchronous execution. Similarly, let ai (t) be the number of the agent that holds t at
the beginning of round i of the asynchronous execution. We
show by induction on i that ai (t) ≤ si (t) for all tokens t.
The statement holds for i = 0 by definition. Assume that
the statement holds for some round i ≥ 0. We show that
the statement hold for round i + 1. Consider a token t. If
ai (t) < si (t) then ai+1 (t) ≤ ai (t) ≤ si (t) − 1 ≤ si+1 (t).
So assume ai (t) = si (t). We need the following two observations: First, when an agent holds two tokens in the synchronous execution, both tokens must be at their final position. Second, since the tokens do not overtake each other,
their order along the extended segment is well-defined and
remains unchanged in both the synchronous and the asynchronous execution. Let t0 be the next token from t towards
the candidate. If there is no such token, the lemma holds.
Otherwise we have si (t0 ) ≤ si (t) by the ordering of the tokens. We distinguish the following three cases.
1. If si (t0 ) = si (t) then by our first observation both t
and t0 are at their final position. Therefore, t is not
forwarded in the synchronous execution.
2. If si (t0 ) = si (t) − 1 then we have to distinguish two
subcases. If the agent si (t0 ) holds two tokens in the
synchronous execution then t0 is at its final position
and hence t is also at its final position. Therefore, t
is not forwarded in the synchronous execution. If the
agent si (t0 ) holds only t0 in the synchronous execution
then it holds at most one token (which is t0 ) in the
asynchronous execution. Therefore, t is forwarded in
the asynchronous execution.
3. If si (t0 ) < si (t) − 1 then the agent ahead of t holds no
token because there is no token between t and t0 by def-

inition. Therefore, t is forwarded in the asynchronous
execution.
In all three cases we have ai+1 (t) = si+1 (t) which concludes
the induction.
It is easy to see that the synchronous execution moves
all tokens to their final position in O(`) rounds. So by the
inductive argument given above, the asynchronous execution
requires at most O(`) rounds to do the same. Once all tokens
reach their final position, it takes at most O(`) additional
rounds until each token sets the bit that shows that it is
settled to true.
The last phase of the algorithm, the inside-outside-test,
is only executed once a candidate has determined that it is
the last candidate on the boundary. The following lemma
provides an upper bound for the runtime of this phase.
Lemma 8. The inside-outside-test takes O(L) rounds.
Proof. The token calculating the angle has to traverse
the outer boundary, which has length L. Since the token takes at least one step in each round, this takes O(L)
rounds.
Finally, we can show the following theorem.
Theorem 2. For a particle system with an outer boundary of length L, the algorithm elects a leader in O(L) rounds,
w.h.p.
Proof. We show that after O(L) rounds the unique candidate c∗ with the highest identifier (which exists w.h.p.)
declares itself to be the leader. According to Corollary 1,
all segments complete the identifier setup within O(log2 n)
rounds and thereby all tokens of the identifier comparison
have been created. By Lemma 6, after an additional L
rounds all of these tokens completed an entire pass along
the boundary. Therefore, the identifier comparison phase
achieved a pairwise comparison between all identifiers. By
this point, every candidate except for c∗ either already withdrew its candidacy or is flagged to withdraw its candidacy
after it completes its current execution of the solitude verification phase. Since the maximum length of an extended
segment of any candidate is L, the candidates that still execute solitude verification withdraw their candidacy after at
most O(L) rounds according to Lemma 7. At this point, c∗
has to start its final solitude verification. However, it might
already be in the middle of a solitude verification execution
that might fail because it was started too early. If this is
the case, it takes at most O(L) rounds for the solitude verification to fail and within L additional rounds, the tokens
of the identifier comparison completed another pass of the
boundary and trigger the final solitude verification. The final solitude verification takes another O(L) rounds as does
the following inside-outside-test according to Lemma 8. So
after overall O(L) rounds, c∗ determines that it is the only
remaining candidate on the outer boundary and therefore
declares itself the leader.

4.

EXTENSIONS

Termination for All Particles.
According to the problem description, the leader is the
only particle for which the algorithm must terminate. Any

non-leader particle is allowed to execute the algorithm indefinitely. In the given algorithm, it is even possible for
particles to enter an infinite loop: consider a particle system
with an empty region R of size 1. With constant probability,
all six agents on the boundary corresponding to R become
candidates and get the same identifier. It is easy to see that
the algorithm never terminates for these agents.
Depending on the application, however, it may be desirable to have an algorithm that terminates for all particles.
This can be achieved as follows. After the leader is elected, it
broadcasts a termination message through the particle system. A particle receiving this message forwards it to each
of its neighbors and then terminates its execution of the algorithm. Clearly, the runtime of such a broadcast is linear
in the diameter D of the particle system. Therefore, the
overall runtime of the algorithm is O(D + L). Note that D
is not an upper bound for L and vice-versa; i.e., it is easy
to design concrete instances of √
particle systems such
√ that
either (i) L = O(n) and D = O( n) or (ii)L = O( n) and
D = O(n).

Leader Election with Probability 1.
The given algorithm elects a leader with high probability.
We can extend the algorithm such that it elects a leader
with probability 1 and still terminates in time O(L) w.h.p.
by combining it with a second algorithm. Consider the following leader election algorithm: the particles set up the
boundaries as before, each agent is initially a candidate, and
the candidates alternate between the following two phases.
In the first phase, a candidate flips a coin and sends the result to both its preceding and its succeeding candidate. A
candidate withdraws its candidacy if its coin flip came up
tails and the coin flips of both its predecessor or its successor
came up heads. It is not hard to see that as long as there
is more than one candidate on a boundary, the number of
candidates reduces in this phase with a probability that is
lower bounded by a constant. Furthermore, the last remaining candidate competes with itself and can therefore not
withdraw its candidacy. In the second phase, a candidate
executes solitude verification. Once a candidate determines
that it is the last remaining candidate for that boundary, it
executes the inside-outside-test. If the candidate lies on the
outside boundary, it declares itself the leader; otherwise, it
withdraws its candidacy. It is not hard to see that the given
algorithm elects a leader with probability 1. By running
this trivial leader election algorithm in parallel to the original leader election algorithm while carefully handling the
case that both algorithms elect a leader, we get an algorithm that elects a leader with probability 1 and terminates
in time O(L) w.h.p.

5.

CONCLUSION

In this paper we presented a randomized leader election algorithm for programmable matter which requires O(n) asynchronous rounds with high probability. The main idea of
this algorithm is to use coin flips to set up random identifiers for each leader candidate in such a way that at least one
candidate has an identifier of logarithmic length. We then
use a token passing scheme that allows us to compare all
identifiers on one boundary with each other in a pipelined
fashion, respecting the constant-size memory constraints at
each particle. We exploit the geometric properties of the
triangular grid only in the last two phases of our algorithm,

in which a candidate tests whether it is the last candidate
on a boundary and whether it is on the outer boundary. If a
candidate passes both of these tests, it irreversibly declares
itself the unique leader.

6.

REFERENCES

[1] L. M. Adleman. Molecular computation of solutions to
combinatorial problems. Science, 266(11):1021–1024,
1994.
[2] C. Agathangelou, C. Georgiou, and M. Mavronicolas.
A distributed algorithm for gathering many fat mobile
robots in the plane. In Proceedings of the 2013 ACM
symposium on Principles of distributed computing,
pages 250–259. ACM, 2013.
[3] D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and
R. Peralta. Computation in networks of passively
mobile finite-state sensors. Distributed Computing,
18(4):235–253, 2006.
[4] D. Arbuckle and A. Requicha. Self-assembly and
self-repair of arbitrary shapes by a swarm of reactive
robots: algorithms and simulations. Autonomous
Robots, 28(2):197–211, 2010.
[5] L. Barriere, P. Flocchini, E. Mesa-Barrameda, and
N. Santoro. Uniform scattering of autonomous mobile
robots in a grid. Int. Journal of Foundations of
Computer Science, 22(3):679–697, 2011.
[6] D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall.
On the computational power of DNA. Discrete
Applied Mathematics, 71:79–94, 1996.
[7] V. Bonifaci, K. Mehlhorn, and G. Varma. Physarum
can compute shortest paths. In Proceedings of SODA
’12, pages 233–240, 2012.
[8] Z. J. Butler, K. Kotay, D. Rus, and K. Tomita.
Generic decentralized control for lattice-based
self-reconfigurable robots. International Journal of
Robotics Research, 23(9):919–937, 2004.
[9] S. Cannon, J. J. Daymude, D. Randall, and A. W.
Richa. A markov chain algorithm for compression in
self-organizing particle systems. In Proceedings of the
2016 ACM Symposium on Principles of Distributed
Computing, PODC 2016, Chicago, IL, USA, July
25-28, 2016, pages 279–288, 2016.
[10] B. Chazelle. Natural algorithms. In Proc. of
ACM-SIAM SODA, pages 422–431, 2009.
[11] H.-L. Chen, D. Doty, D. Holden, C. Thachuk,
D. Woods, and C.-T. Yang. Fast algorithmic
self-assembly of simple shapes using random agitation.
In DNA Computing and Molecular Programming
(DNA 20), pages 20–36. Springer, 2014.
[12] M. Chen, D. Xin, and D. Woods. Parallel computation
using active self-assembly. In DNA Computing and
Molecular Programming, pages 16–30. Springer, 2013.
[13] K. C. Cheung, E. D. Demaine, J. R. Bachrach, and
S. Griffith. Programmable assembly with universally
foldable strings (moteins). IEEE Transactions on
Robotics, 27(4):718–729, 2011.
[14] G. Chirikjian. Kinematics of a metamorphic robotic
system. In Proceedings of ICRA ’94, volume 1, pages
449–455, 1994.
[15] M. Cieliebak, P. Flocchini, G. Prencipe, and
N. Santoro. Distributed computing by mobile robots:

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]
[25]

[26]

[27]

[28]

Gathering. SIAM Journal on Computing,
41(4):829–879, 2012.
R. Cohen and D. Peleg. Local spreading algorithms for
autonomous robot systems. Theoretical Computer
Science, 399(1-2):71–82, 2008.
S. Das, P. Flocchini, N. Santoro, and M. Yamashita.
On the computational power of oblivious robots:
forming a series of geometric patterns. In Proceedings
of PODC 2010, pages 267–276, 2010.
X. Defago and S. Souissi. Non-uniform circle
formation algorithm for oblivious mobile robots with
convergence toward uniformity. Theoretical Computer
Science, 396(1-3):97–112, 2008.
Z. Derakhshandeh, S. Dolev, R. Gmyr, A. W. Richa,
C. Scheideler, and T. Strothmann. Brief
announcement: amoebot - a new model for
programmable matter. In 26th ACM Symposium on
Parallelism in Algorithms and Architectures, SPAA
’14, Prague, Czech Republic - June 23 - 25, 2014,
pages 220–222, 2014.
Z. Derakhshandeh, R. Gmyr, A. Porter, A. W. Richa,
C. Scheideler, and T. Strothmann. On the runtime of
universal coating for programmable matter. In DNA
Computing and Molecular Programming - 22nd
International Conference, DNA 22, Munich,
Germany, September 4-8, 2016, Proceedings, pages
148–164, 2016.
Z. Derakhshandeh, R. Gmyr, A. W. Richa,
C. Scheideler, and T. Strothmann. An algorithmic
framework for shape formation problems in
self-organizing particle systems. In Proceedings of the
Second Annual International Conference on Nanoscale
Computing and Communication, NANOCOM’ 15,
Boston, MA, USA, September 21-22, 2015, pages
21:1–21:2, 2015.
Z. Derakhshandeh, R. Gmyr, A. W. Richa,
C. Scheideler, and T. Strothmann. Universal shape
formation for programmable matter. In Proceedings of
the 28th ACM Symposium on Parallelism in
Algorithms and Architectures, SPAA 2016, Asilomar
State Beach/Pacific Grove, CA, USA, July 11-13,
2016, pages 289–299, 2016.
Z. Derakhshandeh, R. Gmyr, T. Strothmann, R. A.
Bazzi, A. W. Richa, and C. Scheideler. Leader election
and shape formation with self-organizing
programmable matter. In DNA Computing and
Molecular Programming (DNA 21), pages 117–132,
2015.
D. Doty. Theory of algorithmic self-assembly.
Communications of the ACM, 55(12):78–88, 2012.
P. Flocchini, D. Ilcinkas, A. Pelc, and N. Santoro.
Computing without communicating: Ring exploration
by asynchronous oblivious robots. Algorithmica,
65(3):562–583, 2013.
P. Flocchini, G. Prencipe, N. Santoro, and
P. Widmayer. Arbitrary pattern formation by
asynchronous, anonymous, oblivious robots.
Theoretical Computer Science, 407(1):412–447, 2008.
T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss.
Self organizing robots based on cell structures - cebot.
In Proceedings of IROS ’88, pages 145–150, 1988.
T.-R. Hsiang, E. M. Arkin, M. A. Bender, S. P.

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

Fekete, and J. S. B. Mitchell. Algorithms for rapidly
dispersing robot swarms in unknown environments. In
J.-D. Boissonnat, J. Burdick, K. Goldberg, and
S. Hutchinson, editors, Algorithmic Found. Robot. V,
volume 7 of Springer Tracts in Advanced Robotics,
pages 77–94. Springer-Verlag, 2003.
S. Kernbach, editor. Handbook of Collective Robotics –
Fundamentals and Challanges. Pan Stanford
Publishing, 2012.
P. Kling and F. Meyer auf der Heide. Convergence of
local communication chain strategies via linear
transformations. In Proceedings of the 23rd ACM
Symposium on Parallelism in Algorithms and
Architectures, pages 159–166, 2011.
K. Li, K. Thomas, C. Torres, L. Rossi, and C.-C.
Shen. Slime mold inspired path formation protocol for
wireless sensor networks. In Proceedings of ANTS ’10,
pages 299–311, 2010.
J. McLurkin. Analysis and Implementation of
Distributed Algorithms for Multi-Robot Systems. PhD
thesis, Massachusetts Institute of Technology, 2008.
M. J. Patitz. An introduction to tile-based
self-assembly and a survey of recent results. Natural
Computing, 13(2):195–224, 2014.
P. W. K. Rothemund and E. Winfree. The
program-size complexity of self-assembled squares
(extended abstract). In Proceedings of the
Thirty-Second Annual ACM Symposium on Theory of
Computing, May 21-23, 2000, Portland, OR, USA,
pages 459–468, 2000.
M. Rubenstein, A. Cornejo, and R. Nagpal.
Programmable self-assembly in a thousand-robot
swarm. Science, 345(6198):795–799, 2014.
J. E. Walter, J. L. Welch, and N. M. Amato.
Distributed reconfiguration of metamorphic robot
chains. Distributed Computing, 17(2):171–189, 2004.
E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman.
Design and self-assembly of two-dimensional DNA
crystals. Nature, 394(6693):539–544, 1998.
D. Woods. Intrinsic universality and the
computational power of self-assembly. In Proceedings
of MCU 2013, pages 16–22, 2013.
D. Woods, H.-L. Chen, S. Goodfriend, N. Dabby,
E. Winfree, and P. Yin. Active self-assembly of
algorithmic shapes and patterns in polylogarithmic
time. In ITCS, pages 353–354, 2013.
M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll,
H. Lipson, E. Klavins, and G. S. Chirikjian. Modular
self-reconfigurable robot systems. IEEE Robotics
Automation Magazine, 14(1):43–52, 2007.

Mobile Networks and Applications 10, 9–17, 2005
 2005 Springer Science + Business Media, Inc. Manufactured in The Netherlands.

Dynamic Coverage in Ad-Hoc Sensor Networks
HAI HUANG ∗ and ANDRÉA W. RICHA ∗,∗∗
Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85287-8809, USA

MICHAEL SEGAL
Communication Systems Engineering Department, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel

Abstract. Ad-hoc networks of sensor nodes are in general semi-permanently deployed. However, the topology of such networks continuously changes over time, due to the power of some sensors wearing out, to new sensors being inserted into the network, or even due
to designers moving sensors around during a network re-design phase (for example, in response to a change in the requirements of the
network). In this paper, we address the problem of how to dynamically maintain two important measures on the quality of the coverage
of a sensor network: the best-case coverage and worst-case coverage distances. We assume that the ratio between upper and lower transmission power of sensors is bounded by a polynomial of n, where n is the number of sensors, and that the motion of mobile sensors can
be √
described as a low-degree polynomial function of time. We maintain a (1 + ε)-approximation on the best-case coverage distance and
a ( 2 + ε)-approximation on the worst-case coverage distance of the network, for any fixed ε > 0. Our algorithms have amortized or
worst-case poly-logarithmic update costs. We are able to efficiently maintain the connectivity of the regions on the plane with respect to the
sensor network, by extending the concatenable queue data structure to also serve as a priority queue. In addition, we present an algorithm
that finds the shortest maximum support path in time O(n log n).
Keywords: coverage, ad hoc sensor network, kinetic data structure

1. Introduction
Ad-hoc sensor networks are emerging as a new sensing paradigm and have thus received massive research interest recently. Usually sensor nodes are semi-permanently deployed,
since the sensors themselves barely have any moving capacity. However, the topology of such networks continuously
changes over time due to a variety of reasons: For example,
a sensor node may wear out due to its very limited battery
power; a new sensor node may be inserted into the network;
or the layout of a sensor network may need to be changed
in order to improve the quality of the network coverage in
response to a change in the network requirements, which is
accomplished by changing the placement of current (or inserting, deleting) sensors in network.
In this paper, we address the problem of how to dynamically maintain two important measures on the quality of the
coverage of a sensor network: the best-case coverage distance
and the worst-case coverage distance of the network. We also
address a closely related problem, namely that of finding a
shortest maximum support path.
In a sensor network, each sensor bears the ability to detect
objects around it. The coverage of a sensor is limited by its
energy level. Assuming that a sensor’s detecting ability is
omnidirectional, we can model the coverage of a sensor as a
disk (under 2-norm on the Euclidean plane1 ) centered at the
sensor. The radii of such disks are determined by the energy
∗ This work was supported in part by NSF CAREER Award CCR-9985284.
∗∗ Corresponding author.
1 A disk of radius r centered at (x, y) under 2-norm in R2 is the set of points

(p, q) such that (p − x)2 + (q − y)2  r.

level of the sensors. The coverage area (or simply coverage)
of the sensor network is the union of all such disks.
A sensor network is often used to detect intruders. An intruder may start at a point S, follow an arbitrary trajectory
(path) on the plane, and stop at some other point T on the
plane. In some applications, a sensor network may need to
keep track of the intruder at all times, as the intruder follows
its trajectory; in some other applications, the network’s function may be simply to detect the presence of an intruder, in
which case the network only needs to cover some part of the
trajectory. Thus, given two points S and T , two relevant types
of trajectories on the plane are proposed [10]: the maximum
breach path and the maximum support path. (In [10], these
paths are called maximal breach path and maximal support
path, respectively.)
The maximum breach path measures the vulnerability of a
sensor network by, as the name suggests, completely avoiding the coverage area of the sensor network: It is a trajectory
between the start point S and the stop point T that stays “as
far away” from the sensors as possible. On the other hand,
the maximum support path measures the efficiency of the network coverage: This path is a trajectory between S and T
which stays “as close to the sensors” as possible. The distance
of a point P to the sensor network is defined as the smallest
Euclidean distance from P to one of the sensor nodes. A maximum breach path from S to T is a path from S to T such that
the minimum distance from a point P in the path to the sensor network is maximized: this distance is called the worstcase coverage distance of the network. Similarly, a maximum
support path from S to T is a path such that the maximum
distance of a point P in the path to the sensor network is min-

10

imized: this distance is called the best-case coverage distance
of the network.
When the topology of a sensor network changes, the quality of its coverage most probably will be affected. We would
like to maintain an assessment on the quality of the network
coverage – which, as explained above, can be done by maintaining the worst-case and best-case coverage distances – efficiently at all times. This would give a clear indication on
how effective the network coverage is at any given point in
time, possibly calling for the insertion of new nodes in the
network (e.g., when the coverage deteriorates due to node
failures) or to a network re-design phase. Whenever necessary, the actual paths which give the best-case and worst-case
coverage distances can be retrieved. As we will see later, in
sections 4 and 5, our algorithms for maintaining the worstcase and best-case coverage distances have poly-logarithmic
update and query costs, as defined later. To the best of our
knowledge, this is the first work which formalizes and addresses this problem in a dynamic scenario.
For a moment, let us assume that all sensors have the same
energy power and thus all disks have the same radius r. We
call such a sensor network a uniform sensor network with coverage radius r. In a uniform sensor network, all of the paths
whose minimum distance of a point in the path to a sensor
is larger than the coverage radius are equivalent, in the sense
that the sensors in the network will not be able to detect an intruder using any such path. Similarly, all of the paths whose
maximum distance of a point in the path to a sensor is smaller
than the coverage radius are equivalent, in the sense that any
such path is entirely contained in the coverage area of the network. The worst coverage radius (see [10]) is defined to be
the maximum coverage radius r such that there exists a trajectory P between given points S and T which does not intersect
the interior region of the area covered by the uniform sensor
network (i.e., P may “touch” the coverage area, intersecting
it at a discrete number of points only). We can think of the
worst-coverage radius as being the maximum energy that can
be assigned to the sensor nodes which still would not prevent
an intruder from escaping from S to T without being detected
(for simplicity, we assume that a sensor will not be able to
detect an intruder who only touches its coverage area). Correspondingly, the best coverage radius (see [10]) is defined
to be the minimum coverage radius r such that there exists
a trajectory between S and T that is totally covered by the
uniform sensor network.
We introduce uniform sensor networks as a merely conceptual tool in order to facilitate the presentation of our approximation algorithms and their analyses, following a similar approach as Li et al. [9]. (The actual sensor network in consideration has nodes with arbitrary energy levels and therefore
is not assumed to be uniform.) In fact, if we think of a uniform sensor network built on top of the placement of the sensor nodes currently deployed in the general sensor network in
consideration, the worst-coverage radius of the uniform network is indeed equal to the worst-case coverage distance of
the general sensor network, and the best-coverage radius is
indeed equal to the best-case coverage distance.

HUANG ET AL.

In order to dynamically maintain the best- and worst-case
coverage distance efficiently, we need to maintain some information on the current topology of the sensor network; when
the network topology changes, we need to update this information. We also perform queries for the current best-case
and worst-case coverage distances, based on the information
maintained. Hence, the cost (or running time) of our algorithms are measured in terms of their respective update cost
– i.e., the cost to update the topology information, which is
charged per “relevant” topology change in the network – and
the query cost, which is the cost incurred when answering
a query for the current best-case or worst-case coverage distance.
In sections 4 and 5, we formally define a “relevant topology change” – which will henceforth be called an event – for
the problems of maintaining the best-case and worst-case coverage distances, respectively.
The remainder of the paper is organized as follows. Section 1.1 states our results. In section 2, we present some related work in the literature. Section 3 covers some preliminaries and sketches the basic framework of our solutions.
We present the low constant approximation algorithms for the
best- and worst-case coverage distance in sections 4 and 5 respectively. In section 6 we address the closely related problem
of efficiently finding a shortest maximum support path. Section 7 concludes the paper with some possible lines for future
work.
1.1. Our results
In this section, we summarize the main results of this paper.
One of the main contributions of this work is to take into account the dynamic nature of sensor networks, and to propose
a framework which can be used to continuously monitor the
quality of the network coverage. Let n denote the current
number of sensors in the network.
In the following sections, we present two algorithms to
maintain low constant approximations on the best-case and
worst-case coverage distances. Both algorithms have low
update and query costs. Namely, our algorithms achieve a
(1 + ε)-approximation
on the best-case coverage distance,
√
and a ( 2 + ε)-approximation on the worst-case coverage distance, for any fixed ε > 0. The amortized update cost per event of the best-case coverage distance algorithm is O(log3 n), and the respective query cost is worst-case
O(log n). For the worst-case coverage algorithm, the update
cost per event is worst-case O(log2 n) and the query cost is
worst-case O(1). A formal definition of an event for each of
the problems considered follows in sections 4 and 5, respectively.
As a byproduct of our algorithm for maintaining the worstcase coverage distance, we extend the concatenable queue
data structure to also serve as a priority queue. All the operations on this extended data structure have worst-case O(log n)
running time.
We also present an O(n log n) algorithm for computing
an exact shortest maximum support path between two given

DYNAMIC COVERAGE IN AD-HOC SENSOR NETWORKS

points S and T , improving on the best-known previous results
by Li et al. [9]. A shortest maximum support path from S
to T is a maximum support path from S to T such that the
Euclidean length of the trajectory followed in this path is minimum. In [9], two algorithms are presented for computing the
maximum support path: One algorithm computes an exact
shortest maximum support path in O(n2 log n) time; the other
algorithm provides a 2.5-approximation on the shortest maximum support path in O(n log n) time. One should note that
the algorithms presented by Li et al. can be implemented in
a distributed fashion (we use the communication complexity
as the time bound for the sequential versions of their algorithms), whereas the algorithms presented in this paper are all
centralized.
The update costs of our algorithms for approximately
maintaining the best- and worst-case coverage distances are
much cheaper than maintaining the best- or worst-case coverage distances using the best-known algorithms in the literature prior to this work. In fact, the best previously known
algorithm for maintaining the best-case (resp., worst-case)
coverage distance maintains the exact distance by repeatedly re-computing the maximum support path (resp., maximum breach path) using the O(n log n) algorithm by Li
et al. [9] (resp., the O(n2 log n) algorithm by Meguerdichian
et al. [10]) each time an event occurs. To the best of our
knowledge, this is the first work that explicitly addresses
the problems of dynamically maintaining (approximations of)
these two distances.

2. Related work
Meguerdichian et al. [10] considered the problems of finding
the maximum breach path and the maximum support path on
a sensor network. They [10] present an O(n2 log ) runtime
algorithm for the maximum breach path problem, where n is
the number of sensors in the sensor network, and  is the difference between the highest and the lowest weight of an edge
in the Voronoi Diagram of the sensor network. Their algorithm for computing the maximum support path has the same
running time as their maximum breach path algorithm. The
O(log ) factor can be easily converted into O(log n) in the
algorithm that solves the maximum breach path problem if we
perform a binary search over a sorted list of the radii of sensors instead of using a linear search as in [10]. The algorithms
presented in [10] heavily rely on geometric structures such as
the Voronoi Diagram and Delaunay triangulation of the network, which cannot be constructed efficiently in a distributed
manner.
Li et al. [9] prove the correctness of the algorithms given
in [10]. They also show how to find a maximum support
path in O(n log n) time using a centralized algorithm, or with
O(n log n) communication complexity bits in a distributed
fashion. In addition, Li et al. [9] present two algorithms for
computing a shortest (with respect to the Euclidean length of
the trajectory followed in this path) maximum support path:
an algorithm that computes an exact shortest maximum sup-

11

port path with O(n2 log n) worst-case communication complexity, and an algorithm that computes a 2.5-approximation
of a shortest maximum support path (i.e. the total length of
the obtained path is at most 2.5 times the length of a shortest maximum support path) with O(n log n) communication
complexity.
Meguerdichian et al. [11] proposed an exposure-based formulation for analyzing the coverage of paths taken by polygonal objects: they define a path-dependent “integral”, which
consists of the trajectories of all the points of a polygonal object (the polygonal object is able to rotate), and not only of
the trajectory of the object’s center point.
Recently, Zhang and Hou [13] proved that if the communication rage of a sensor is at least twice its sensing range,
a complete coverage of a convex area implies connectivity
among the working set of nodes and derive optimality conditions under which a subset of working sensor nodes can be
chosen for full coverage. Wang et al. [12] designed a Coverage Configuration Protocol (CCP) that can provide different degrees of connected coverage and present a geometric
analysis of the relationship between coverage and connectivity. Huang and Tseng [8] present an algorithm with runtime
of O(n2 log n) that decides whether every point in a given service area is covered by at least one sensor.

3. Preliminaries
Before heading into the technical details of our algorithms,
we introduce some basic concepts which will be used in both
sections 4 and 5. The first concept we introduce is that of
growing disks, which will help us translate our problems into
graph connectivity problems.
The growing disks concept was previously proposed in [9].
We restate it in terms of the coverage radius of a uniform sensor network as defined in section 1. (In section 1, we saw how
the coverage radius of a virtual uniform overlay sensor network directly relates to the worst-case and best-case coverage
distances of the actual network.) Assume we have a uniform
sensor network with coverage disks centered at the sensors.
Define U (r) to be the region on the plane composed of the
union of all of the coverage disks when the coverage radius
is r. Let U (r) be the complement of the region U (r). At the
very beginning, we set the coverage radius to be equal to 0.
Then U (r) is the union of discrete singletons. As the coverage radius grows, the disks centered at the sensors become
larger and might get connected into larger regions. Therefore,
U (r) might get disconnected into separate regions. For any
two given points S and T , the best coverage radius is the minimum r such that S and T are in the same connected region of
U (r), while the worst coverage radius is the minimum r such
that S and T belong to two disconnected regions in U (r).
Hence, the best and worst coverage radius problems translate
to connectivity problems on U (r) and U (r), respectively. Figure 1 illustrates these ideas. We will further translate the best
and worst coverage radius problems into graph connectivity
problems.

12

HUANG ET AL.

denoted by rmax . Let R = rmax /rmin . We need to maintain logα (R) copies of U (ri ) or U (ri ). If updating the relevant connectivity information for each U (ri ) or U (ri ) takes
time g(n), the overall update time is logα (R) · g(n). The update time is poly-logarithmic on n provided that g(n) is polylogarithmic on n, and that R is bounded by a polynomial on n.

4. Dynamic best-case coverage distance
(a)

(b)
Figure 1. Best and worst coverage radii. (a) Best-coverage radius: minimum r such that S and T are connected in U (r). (b) Worst-coverage radius:
minimum r such that S and T are disconnected in U (r).

We first show how to translate the best coverage radius
problem into a graph connectivity problem. A uniform disk
graph is the intersection graph of disks with uniform radius r
(see [4]). In this graph, disks are vertices and there is an edge
between two vertices if and only if the corresponding disks
intersect.2 The connectivity of U (r) is naturally modeled by
that of a uniform disk graph of radius r, denoted by G(U (r)).
The best coverage radius is the minimum r such that the vertex corresponding to the disk containing S is connected to that
corresponding to the disk containing T in G(U (r)).
We also translate the worst coverage radius problem into a
graph connectivity problem. However this case is rather more
involved and we delay its presentation to section 5.
When r is fixed, suppose that we have a poly-logarithmic
running time query to check whether the region in either U (r)
or U (r) containing S is connected to that containing T . Then
we can build an α-approximation algorithm, α > 1, for either
the best or the worst coverage radius problem, as we show in
the next paragraph.
For the best coverage radius, consider the sequence of
U (ri ), such that ri = αri−1 . Let i be such that S and T are
connected in U (ri ) but not in U (ri−1 ). Since the best coverage radius falls in the interval [ri−1 , ri ] and since ri is at most
αri−1 , we know that ri is an α-approximation on the best coverage radius. A similar argument on the sequence of U (ri )’s
gives an α-approximation of the worst coverage radius.
Assume sensors occupy some space and cannot overlap.
Then there is a constant lower bound on the coverage radius,
denoted by rmin . Due to the limited battery power, we assume
that there is a constant upper bound on the coverage radius,
2 If we rescale one unit to be 2r, then a uniform disk graph is a unit-disk

graph.

In this section, we present our (1 + ε)-approximation algorithm to maintain the best-case coverage distance following
the framework presented in section 3. Recall that, as shown
in section 3, finding the best-case coverage distance for given
points S and T is equivalent to finding the minimum r such
that S and T are connected in G(U (r)). Thus our main goal
is to devise an approach to maintain the connectivity of the
uniform disk graph G(U (r)) such that both the update cost
and the query cost are poly-logarithmic on n, where n is the
number of sensors in the network.
Holm et al. [7] showed that the connectivity of a graph
can be maintained in amortized poly-logarithmic update cost,
whereas each query takes worst-case O(log n/ log log n) time.
Guibas et al. [5] used Holm et al.’s algorithm to maintain connectivity on a unit-disk graph. The update cost in [5,7] is
charged per edge insertion or deletion. In order to be able
to detect when uniform disks meet or separate on the plane
(corresponding to an edge insertion or deletion on a unit-disk
graph, respectively), Guibas et al. [5] introduced a kinetic data
structure specially tailored to handle this scenario.
The kinetic data structure framework was first proposed by
Basch et al. [2,3] to deal with dynamics. Their main contribution is a method to maintain an invariant of a set of moving
objects in a discrete manner. They introduce the idea of keeping certificates as triggers for updates. When an object moves
and a certificate fails, the consistency of the kinetic data structure is invalidated and an update is mandatory. Each failure of
a certificate incurs a setup of up to a constant number of new
certificates. Hence we are allowed to monitor the dynamics
of a set of objects discretely and efficiently. The kinetic data
structure requires that we know the flight plan (a specification
of the future motion) [2,5] of all disks, and that the trajectory
of each disk can be described by some low-degree algebraic
curve. We have the freedom to change the flight plan of a disk
at any time. Basch [2] shows that kinetic data structures can
efficiently support the dynamic operations of inserting and
deleting objects into the system, provided those operations
do not occur too often. The details of kinetic data structures
are beyond the scope of this paper. Please refer to [2,3,5] for
more information.
The kinetic data structure utilized in [5] can be viewed as a
discrete event monitor. The events we need to monitor in order to maintain accurate connectivity information on G(U (r))
are when two disks meet or separate. In [5], two types of
certificates are set up and the data structure allows us to determine a priori the time when an event will occur. When an
event occurs, the topology of the uniform disk graph G(U (r))

DYNAMIC COVERAGE IN AD-HOC SENSOR NETWORKS

changes and an update on the connectivity information is triggered. Hence the update cost is the cost to update the connectivity information of G(U (r)) per event. When a certificate
fails and an event occurs, it takes constant time for the kinetic
data structure to process the failure (due to the setup of at most
a constant number of new certificates). We do not explicitly
take this cost into account when computing the update cost of
the maintenance of the connectivity information of G(U (r)),
since it would not change the asymptotic bound on the update
cost.
We adapt the main theorem in [5, theorem 5.4], to better
serve our purposes. The uniform disk graph G(U (r)) corresponds to a unit-disk graph if we rescale one unit to be equal
to 2r.
Lemma 1 (Adapted from [5, theorem 5.4]). In [5], an algorithm to dynamically maintain the connectivity of G(U (r)) is
presented. The update cost is amortized O(log2 n) per event.
The query cost is worst-case O(log n/ log log n).
We still need to show how to determine which disks contain the given points S and T , at any given time. We sort all
sensors according to their distances to the fixed point S. We
maintain a binary heap on this ordering. Once the ordering
changes, we update the heap in O(log n) time. This introduces a new type of event – namely, when a sensor changes
its location and needs to be re-inserted in this ordering – besides the other two events defined earlier. The update cost
for this event is O(log n). To check which disk contains S,
we find the closest sensor p to S. We check if the distance
from p to S is larger than the coverage radius. If so, then S is
not contained in any disk. Otherwise, we know that the disk
centered at p contains the point S. This query takes constant
time. We maintain the ordering of the sensors with respect
to T in a similar way.
Combining the result in this section with the algorithmic
framework presented in section 3, we have our (1 + ε)approximation algorithm (for any ε > 0) for the bestcase coverage distance by maintaining log1+ε R copies of
G(U (r)), for r = 1, (1+ε), (1+ε)2 , . . . . We perform a query
operation by doing a binary search on the log1+ε R copies of
G(U (r)).

13

worst-case coverage distance. We first present a (1 + ε)approximation algorithm (for any ε > 0) for a simplified sensor network model, where the coverage disks
√ are considered
under infinity-norm. Since there is √
only a 2 gap between
infinity-norm and 2-norm, a (1 + ε/√2)-approximation factor for infinity-norm dilates into a ( 2 + ε)-approximation
factor when applied to the 2-norm scenario, for any fixed
ε > 0. The infinity-norm of a vector v = (x1 , . . . , xd ) in a
d-dimensional space is defined as v∞ = max(|x1 |,
. . . , |xd |). Under infinity-norm, the distance between two
points on the plane is the maximum of the difference of
their x coordinates and the difference of their y coordinates.
Hence the coverage region of a sensor is square shaped and
its boundary is composed of four line segments. As we will
see later, this simple boundary shape allows for an efficient
maintenance scheme.
Recall the solution framework presented in section 3. The
core of our algorithm is to check, for any two given points S
and T , whether the region in U (r) containing S is connected
to that containing T . If we can maintain some information
such that each query on connectivity of regions takes only
poly-logarithmic time, the cost of update against mobility is
also poly-logarithmic.
In our algorithm, regions in U (r) are represented by their
boundaries. Only one region in U (r) may be infinite in area.
We call such an unbounded region the outer face. All of the
other (bounded) regions are called inner faces. Since we consider the infinity-norm, each disk is represented by a square
on the plane. Thus the boundary of any inner face is a simple cycle composed of a sequence of line segments, while the
boundary of the outer face comprises several simple cycles.
To differentiate these cycles, we call a cycle that is the bound-

(a)

Theorem 1. Our algorithm dynamically maintains a (1 + ε)approximation, for any ε > 0, of the best-case coverage
distance. The update cost of this algorithm is amortized
O(log2 n · log1+ε R) per event and the query cost is worstcase O((log n/ log log n) · log log1+ε R).
Corollary 1. If ε > 0 is fixed, then our algorithm has amortized O(log3 n) update cost per event, and worst-case O(log n)
query cost.
(b)
5. Dynamic worst-case coverage distance
√
In this section, we present our ( 2 + ε)-approximation
algorithm, for any ε > 0, to dynamically maintain the

Figure 2. Representation in G(U (r)). (a) A square is represented by 8 vertices. (b) Dynamics of vertices and edges when squares overlap. Vertices C,
C  , and E and E  are relocated. Edges (C, C  ) and (E, E  ) are removed, and
edges (C, E  ) and (E, C  ) are inserted.

14

HUANG ET AL.

tion for each G(U (r)) in [7] is amortized O(log2 n) and the
query cost is O(log n/ log log n), implying an overall amortized update cost of O(log3 n) and worst-case query cost of
O(log n), with an approximation factor of (1 + ε), for any
fixed ε > 0. However, G(U (r)) is the union of simple disjoint cycles, each uniquely defining a region in U (r) and thus
it allows for a more efficient update and query scheme, at the
expense of a small degradation in the approximation factor.
As we will show later, we can maintain the connectivity of all
G(U (r)) in overall worst-case update cost of O(log2√n), with
worst-case query cost of O(1), while maintaining a ( 2 + ε)approximation on the worst-case coverage distance, for any
fixed ε > 0.
In the remainder of this section, we first describe the dynamics of the connectivity graph. Then we define three types
of events which mandate updates. Our update cost is charged
per event. Following that, we present a data structure, which
is an extension of concatenable queues [1], to maintain the
connectivity of the graph efficiently. Finally, we present our
major result on the worst-case coverage distance.
Figure 3. Outer- and inner-cycles. When the outer cycle in (a) breaks into
two cycles, it can either break into an outer and an inner cycle, as shown
in (b); or it can break into two outer cycles, as shown in (c).

ary of an inner face an inner cycle, and a cycle on the boundary of the outer face an outer cycle. Figure 3 illustrates some
of these concepts. The shaded areas in the figure define U (r),
and the unshaded areas define U (r). In (b), U (r) is divided
into two regions, the unbounded region is the outer face, the
bounded region is the inner face. The boundary of the inner
face is an inner cycle and that of the outer face is an outer
cycle. In (c), the boundary of the outer face consists of two
disjoint outer cycles.
Below we describe a method which translates the connectivity of regions in U (r) into a graph connectivity problem.
The first step is to represent outer cycles and inner cycles by
a graph. There are only vertical line segments and horizontal line segments in both outer and inner cycles, and those
line segments only meet at their endpoints. Hence we can
draw a graph such that the vertices are the endpoints and the
edges are the line segments. We call this graph the connectivity graph G(U (r)). (For convenience, the connectivity graph
will actually be implemented in a slightly different way, as we
explain in section 5.1.)
Every outer or inner cycle is a cycle in the graph and any
two of them are disjoint, i.e., disconnected in the graph. This
coincides with the fact that any two distinct inner faces are
disconnected, and that any inner face is disconnected from
the outer face.
The connectivity of G(U (r)) is thus analogous to that of
U (r): Two regions are connected in U (r) if and only if their
boundary cycles are connected in the graph, or they are both
part of the outer face boundary. Thus we could apply the algorithm proposed by Holm et al. [7], which dynamically maintains graph connectivity, to maintain the connectivity of the
regions in U (r). The update cost per edge insertion or dele-

5.1. Dynamics of cycles
In this section, we first formally define the representation we
use for the connectivity graph G(U (r)). Second, we address the dynamics of the connectivity graph. And finally,
we present an algorithm for maintaining the connectivity information on the regions of U (r).
The boundary of a standalone square is the simplest cycle
in G(U (r)). We represent a square by eight vertices and eight
edges as shown in figure 2. For every corner X of a square,
we introduce two vertices X and X . Hence we have O(n)
vertices and edges in G(U (r)), where n always denotes the
current number of sensors in the network. The extra vertices
help us to efficiently maintain the graph when squares start to
move and overlap on the plane (including when sensors are
added or removed from the network). In the following, we
will show that the dynamics of sensors will not change the
O(n) bound on the number of vertices and edges.
When two squares meet, at most two pairs of line segments of their boundaries intersect. Without loss of generality, suppose a vertical edge B  C intersects with a horizontal edge E  F at a point Z, and the new boundary comprises
edges B  Z and ZF . Then we simply relocate vertices C and
E  to Z, insert an edge CE  and remove edges CC  and EE 
from G(U (r)). Figure 2 illustrates this operation. Note that
we do not introduce any new vertex or remove any old vertex. In fact, since G(U (r)) contains no information of the
vertex’s location, we do not need to perform any “relocation”
of a vertex when we operate on G(U (r)). The cases of a vertical edge intersecting with a vertical edge, and of a horizontal
edge intersecting with a horizontal edge are analogous, and
can thus be also handled by at most two edge insertions and
at most two edge deletions. Since we never change the number of vertices in the graph, and since each vertex has degree
at most 2, the O(n) upper bound on number of vertices and
edges in G(U (r)) always hold. The following fact follows:

DYNAMIC COVERAGE IN AD-HOC SENSOR NETWORKS

Fact 1. When two squares meet or separate, up to four edge
insertions and deletions are needed to update the connectivity
graph G(U (r)).
When the topology of the network changes, cycles in
G(U (r)) may also undergo changes. A cycle may break into
two smaller cycles; or two cycles may merge into a longer
cycle. Both these operations impose changes on the connectivity of G(U (r)). Cycles break or merge only when two sensors’ coverage disks meet or separate. Hence we need to detect the time when those happen in order to trigger an update.
When a cycle breaks, it could break into an outer cycle and
an inner cycle (as shown in figure 3). We need to differentiate
outer cycles from inner cycles since all outer cycles define
the same region, namely the outer face. In order to determine
whether a cycle is an outer cycle, one only needs to identify
the topmost edge of the cycle: If the topmost edge of the cycle
is the top boundary of a square, then the cycle is an outer
cycle; otherwise, the topmost edge of a cycle is the bottom
boundary of a square, and the cycle is an inner cycle. Hence
we need to maintain the topmost edge of each cycle as sensors
move. The topmost edge of a cycle may change only when
two horizontal line segments swap their y position. Therefore
we also need to monitor these line segment swaps.
Recall that the original problem we aim to solve is to check
whether the region containing a given point S is connected to
that containing T . We need to determine which region contains a given point and also to update this information as sensors move. As described in section 4, we sort all sensors according to the distance from the fixed point S and maintain a
binary heap on this ordering, with update cost O(log n) on the
heap. In order to check which region S belongs to, we need
to find the cycle representing the region. Again we find the
closest sensor p to S and check if the distance is smaller than
the radius of the coverage disk of p. If so, then the point S
does not belong to any region of U (r). Otherwise, we check
the eight vertices of the square representing the closest sensor
to S, find the closest one of these vertices to S, and the cycle
containing this closest vertex represents the region containing S. This query takes constant time. We maintain a similar
data structure for T . Thus we also need to monitor and detect the time when two sensors swap their relative position in
these orderings.
We summarize all of the above in the following three
types of events, which we need to monitor in order to trigger mandatory updates, as sensors move on the plane:
(I) Two vertical line segments swap their x position,
(II) Two horizontal line segments swap their y position, and
(III) Two sensor swap their position in the orderings of the
sensor’s distance to the given points S and T .
When events (I) or (II) occurs, we can check in constant
time whether two coverage disks meet or separate. If they do,
we check whether the event leads to a cycle break or merge,
and update the data structure accordingly. When event (II) occurs, we can check whether the two horizontal line segments

15

belong to the same cycle. If so, we may also need to update
the topmost edge of the cycle. When event (III) occurs, we
update the orderings with respect to distances to S and T .
We use the kinetic data structure as defined in [3] as our
event monitor (unlike G(U (r)), G(U (r)) is not a unit-disk
graph and therefore the results in [5] do not apply). Each
event can be detected and processed in constant time.
In the following, we present our update scheme. We will
also show that the update cost per event is O(log n). We store
a cycle as a sequence of consecutive edges. In section 5.2
we introduce a data structure which supports the following
operations on sequences of edges:
INSERT – insert an edge into a sequence
DELETE – delete an edge from a sequence
CONCATENATE – concatenate a sequence to the end of another sequence
SPLIT – split a sequence into two sequences
SWAP – swap the y position of two edges
MAX – return the topmost edge of a sequence
MEMBER – return the representative edge of a sequence
Each of these operations can be executed in worst-case
running time O(log n), as stated in lemma 4.
The update per type (I) or (II) event is as follows. When
squares move and the shape of a cycle changes, up to a constant number of INSERT and DELETE operations are needed
to update the cycle per event. When two edges in a cycle exchanges their y position, we execute SWAP to update the y
position per event. We can execute MAX to know whether a
cycle is an outer cycle or not. Recall that a cycle is an outer
cycle if and only if the topmost edge of the cycle is the top
boundary line segment of a square. Cycle merges or breaks
can be carried out by a constant number of CONCATENATE
and SPLIT operations. Since only a constant number of INSERT, DELETE, CONCATENATE, SPLIT, SWAP and MAX
operations are executed per event, the update cost per event is
worst-case O(log n). As we have explained earlier, the update
cost per type (III) event is also O(log n).
A data structure that supports the operations above can also
be used for efficiently performing a connectivity check. Assume that S and T are not covered by any sensor in U (r)
and therefore both belong to U (r). We can find the closest
vertices u and v to points S and T , respectively, in constant
time. Then we check if u and v belong to the same cycle by
performing two MEMBER operations. If so, then S and T
belong to the same region in U (r). Otherwise, we need to
check whether the closest cycles to S and T are both outer
cycles by two executions of the MAX operation. If both of
them are outer cycles, then both S and T belong to the outer
face, and hence are in the same region. Otherwise, S and T
belong to two disconnected regions. This procedure can be
implemented in O(log n) time.
We summarize all of the above in lemma 2.
Lemma 2. For any two given points S and T , we maintain a
data structure with O(log n) update cost per event such that

16

the query to check whether the region in U (r) containing S is
connected to that containing T takes O(log n) time.
Combining lemma 2 with the algorithmic framework presented in section 3, we have our (1 + ε)-approximation algorithm, for any ε > 0, for the worst-case coverage distance
under infinity-norm, as stated in the lemma below. If every
time we perform an update operation, we keep track of the
smallest ri such that S and T are disconnected in G(U (ri )),
then each query operation can be performed in O(1) time.
Lemma 3. Under infinity-norm, our algorithm dynamically
maintains a (1 + ε)-approximation of the worst-case coverage distance for any ε > 0. The update cost is worst-case
O(log n · log1+ε R) per event, and the query cost is worstcase O(1).
√
Hence the ( 2+ε)-approximation algorithm for the worstcase coverage distance under 2-norm follows:
√
Theorem 2. Our algorithm dynamically maintains a ( 2+ε)
-approximation of the worst-case coverage distance, for any
ε > 0. The update cost is worst-case O(log n · log1+ε/√2 R)
per event, and the query cost is worst-case O(1).
Corollary 2. If ε > 0 is fixed, then our algorithm has worstcase O(log2 n) update cost per event, and worst-case O(1)
query cost.
5.2. Extended concatenable queue
In this subsection we introduce a data structure that supports the operations INSERT, DELETE, CONCATENATE,
SPLIT, SWAP, MAX and MEMBER efficiently. The data
structure is an extension of the concatenable queue data structure [1]. In [1], a concatenable queue is implemented by a
2–3 tree (a Red–Black tree would also work, for example),
and all the data is stored at the leaf nodes. A concatenable queue supports the operations INSERT, DELETE, CONCATENATE, SPLIT and MEMBER, and each operation takes
time O(log n) in the worst case. In the following paragraphs,
we will show how to also implement the SWAP and MAX
operations on a concatenable queue in O(log n) time.
We associate each edge’s y coordinate to the corresponding leaf node in the 2–3 tree. To each internal node t, we associate the maximum y coordinate of a leaf node in the subtree
rooted at t. This is done by comparing all the y coordinates
associated to t’s children in the tree, taking constant time per
internal node. When the y coordinate of an edge changes, and
a SWAP operation is invoked, it takes at most O(log n) time to
climb up the tree and update all the internal nodes on the way
up. Starting from any given edge on a cycle, it takes O(log n)
time to reach the root of the 2–3 tree where we can find the
topmost edge of the cycle. Hence the O(log n) running time
of MAX follows.
We need also to justify that the above modification does
not increase the running time of all other operations. Per each

HUANG ET AL.

INSERT or DELETE, it takes an additional O(log n) time to
update the y coordinate of all internal nodes due to the edge
insertion or deletion. Both CONCATENATE and SPLIT are
implemented by up to O(log n) joins or breaks of trees at the
root node. Since updating the y coordinate at the root node
takes constant time (by comparing all the children of the root),
we incur at most an additional O(log n) time per CONCATENATE or SPLIT. Thus the asymptotic running time of INSERT, DELETE, CONCATENATE, and SPLIT remains unchanged. The running time of MEMBER is not affected by
SWAP or MAX operations.
Lemma 4. The extension of the concatenable queue data
structure supports the operations of INSERT, DELETE,
CONCATENATE, SPLIT, SWAP, MAX and MEMBER.
Each operation has worst-case running time of O(log n).
6. Exact shortest maximum support path
We consider the problem of finding a maximum support path
between S and T such that the Euclidean length of the trajectory followed by this path is minimum. Below we present
an O(n log n) runtime solution, thus improving on the bestknown previous results by Li et al. [9]. One should note that
the algorithms presented in [9] can be implemented in a distributed fashion, whereas the algorithm we present in this section is intrinsically centralized.
We proceed as follows. First we compute the best coverage
radius rbest using the algorithm of Li et al. [9] in O(n log n)
time. Next, we obtain a collection of uniform disks by setting
the radius of each sensor to be rbest . Let U denote the union
of all these uniform disks. Define the complement region of
the union C = R2 \ U .
The problem of finding a shortest maximum support path
is equivalent to the problem of finding a shortest S, T -path
in R2 avoiding C, since we are seeking for a maximum support path and rbest is the best coverage radius. (Since rbest is
the best coverage radius, any maximum support path is contained in U ; in fact any path from S to T in U is a maximum
support path.) A shortest maximum support path can only
contain straight line segments as edges, otherwise the path
would not be shortest. Therefore, we can replace each arc in
C by a straight line segment. In such fashion we obtain a new
set of obstacles C  as a collection of polygonal objects with
possible “holes” that have a total O(n) number of vertices.
We can remove these “holes” and obtain slightly larger number of disjoint polygonal objects by cutting the existing objects with segments that connect the vertices of the holes and
the external boundary in an arbitrary fashion. Note that the
number of total vertices of the disjoint polygonal objects has
not changed, i.e., it is still O(n). Thus, our problem translates
to that of finding a shortest path in R2 that avoids the polygonal obstacles in C  . The idea now is to use an algorithm by
Hershberger and Suri [6], which finds a shortest path between
S and T on the Euclidean plane that avoids polygonal obstacles in O(n log n) time. Hence the total running time of our
algorithm is O(n log n).

DYNAMIC COVERAGE IN AD-HOC SENSOR NETWORKS

As described in section 2, two algorithms are presented
for computing the maximum support path by Li et al. in [9]:
One algorithm computes an exact shortest maximum support path in O(n2 log n) time; the other algorithm provides a
2.5-approximation on the shortest maximum support path in
O(n log n) time. Our algorithm improves on the running time
of the former and on the approximation factor of the latter algorithm. One should note, however, that the algorithms presented by Li et al. can be implemented in a distributed fashion
(we use the communication complexity as the time bound for
the sequential versions of their algorithms), whereas our algorithm is centralized in nature.

17

[12] X. Wang, G. Xing, Y. Zhang, C. Lu, R. Pless and C.D. Gill, Integrated
coverage and connectivity configuration in wireless sensor networks,
in: Proc. of the 1st ACM Conf. on Embedded Networked Sensor Systems
(2003).
[13] H. Zhang and J.C. Hou, Maintaining sensing coverage and connectivity
in large sensor networks, Technical Report UIUCDCS-R-2003-2351,
UIUC (2003).

Hai Huang is a Ph.D. student in the Department of
Computer Science and Engineering at Arizona State
University. He has received a M.S. in the same
department under the supervision of Prof. Andrea
W. Richa in 2003. He received a B.A. and a M.A.
in the Mathematics Department at Tsinghua University, P. R. China, in 1996 and 1999, respectively. His
current research work focus on clustering and routing problems in mobile ad-hoc networks, and on the
chordal graph completion problem with applications

7. Future work
In this paper, we present poly-logarithmic dynamic algorithms to maintain approximations of two relevant measures
– namely, the best- and worst-case coverage distances – of the
quality of the network coverage in wireless sensor networks.
An interesting open question is whether we can maintain exact best-case and worst-case coverage distances for Euclidean
metric with poly-logarithmic update time.
Acknowledgement
We express our thanks to Micha Sharir for his comments.
References
[1] A.V. Aho, J.E. Hopcroft and J.D. Ullman, The Design and Analysis of
Computer Algorithms (Addison-Wesley, Reading, MA, 1974).
[2] J. Basch, Kinetic data structures, Ph.D. dissertation, Stanford University (1999).
[3] J. Basch, L.J. Guibas and J. Hershberger, Data structures for mobile
data, in: Proc. of 8th ACM–SIAM Symposium on Discrete Algorithms
(1997) pp. 747–756.
[4] B.N. Clark and C.J. Colbourn, Unit disk graphs, Discrete Math. 86
(1990) 165–177.
[5] L.J. Guibas, J. Hershberger, S. Suri and L. Zhang, Kinetic connectivity
for unit disks, Discrete Comput. Geom. 25 (2001) 591–610.
[6] J. Hershberger and S. Suri, An optimal algorithm for Euclidean shortest
paths in the plane, SIAM J. Comput. 28(6) (1999) 2215–2256.
[7] J. Holm, K. de Lichtenberg and M. Thorup, Poly-logarithmic deterministic fully-dynamic graph algorithms I: Connectivity and minimum
spanning tree, Technical Report DIKU-TR-97/17, Department of Computer Science, University of Copenhagen (1997).
[8] C.-F. Huang and Y.-C. Tseng, The coverage problem in a wireless sensor networks, in: Proc. of the 2nd ACM Internat. Conf. on Wireless
Sensor Networks and Applications (2003) pp. 115–121.
[9] X.-Y. Li, P.-J. Wan and O. Frieder, Coverage in wireless ad-hoc sensor
networks, IEEE Trans. Comput. 52 (2003) 1–11.
[10] S. Meguerdichian, F. Koushanfar, M. Potkonjak and M.B. Srivastava,
Coverage problems in wireless ad-hoc sensor networks, in: Proc. of the
20th IEEE INFOCOM (2001) pp. 1380–1387.
[11] S. Meguerdichian, F. Koushanfar, G. Qu and M. Potkonjak, Exposure in
wireless ad-hoc sensor networks, in: Proc. of the 7th ACM MOBICOM
(2001) pp. 139–150.

to scientific computing.
E-mail: hai@asu.edu

Andréa W. Richa joined the Department of Computer Science and Engineering at Arizona State University in 1998, where she is now an Associate Professor. She received her M.S. and Ph.D. degrees
from the School of Computer Science at Carnegie
Mellon University, in 1995 and 1998, respectively.
She also earned an M.S. degree in computer systems
from the Graduate School in Engineering (COPPE),
and a B.S. degree in computer science, both at the
Federal University of Rio de Janeiro, Brazil, in 1992
and 1990, respectively. Prof. Richa’s main area of research is in network
algorithms. Some of the topics Dr. Richa has worked on include packet
scheduling, distributed load balancing, packet routing, mobile network clustering and routing protocols, and distributed data tracking. Prof. Richa’s data
tracking (or lookup) algorithm has been widely recognized as the first benchmark algorithm for the development of distributed databases in peer-to-peer
networking, having received over 55 academic journal or conference publications, and being implemented as part of two of the current leading pojects
in peer-to-peer networking. Dr. Richa’s was the recipient of an NSF CAREER Award in 1999. For a selected list of her publications, CV, and current
research projects, please visit http://www.public.asu.edu/∼aricha.
E-mail: aricha@asu.edu

Michael Segal was born at October 12, 1972 in
USSR. In 1991 he immigrated to Israel and started
to study computer science in Ben-Gurion University
of the Negev. He finished his B.Sc., M.Sc. and Ph.D.
degrees in 1994, 1997, and 1999, respectively. During a period of 1999–2000 Dr. Michael Segal held a
MITACS National Centre of Excellence Postdoctoral
Fellow position in University of British Columbia,
Canada. Dr. Segal joined the Department of Communication Systems Engineering, Ben-Gurion University, Israel in 2002 where he holds now a position of senior lecturer and
serves as department’s Deputy Chairman. His primary research is algorithms
(sequential and distributed), data structures with applications to optimization
problems, mobile wireless networks, communications and security.
E-mail: segal@cse.bgu.ac.il

Ad Hoc Networks 7 (2009) 1347–1369

Contents lists available at ScienceDirect

Ad Hoc Networks
journal homepage: www.elsevier.com/locate/adhoc

Evaluation of physical carrier sense based spanner construction and
maintenance as well as broadcast and convergecast in ad hoc networks
Luke Ritchie a, Sapna Deval a, Martin Reisslein a,*, Andrea W. Richa b,1
a
b

Dept. of Electrical Engineering, Arizona State University, Goldwater Center, MC 5706, Tempe AZ 85287-5706, United States
Dept. of Computer Science and Engineering, Arizona State University, Box 878809, Tempe, AZ 85287-8809, United States

a r t i c l e

i n f o

Article history:
Received 31 October 2007
Received in revised form 23 September 2008
Accepted 27 January 2009
Available online 6 February 2009

Keywords:
Ad hoc network
Backbone network
Broadcast
Physical carrier sensing
Resilience
Variable sensing ranges

a b s t r a c t
While the use of physical carrier sensing for medium access control in ad hoc wireless networks is well established, exploiting physical carrier sensing directly for network layer
functions is largely unexplored. We conduct extensive simulation evaluations of recently
proposed algorithms that directly exploit physical carrier sensing for backbone network
(spanner) construction, broadcast, and convergecast in wireless ad hoc networks. Our algorithms accommodate interference ranges larger than transmission ranges, explicitly incorporate the medium access control and packet collisions, and do not require any prior
knowledge of the network. For spanner construction, our algorithms include three self-stabilizing phases that establish leader nodes able to reach all nodes in one hop, assign the
leaders non-interfering transmission rounds, and connect the leaders through gateway
nodes. We evaluate the backbone construction and maintenance as well as broadcast
and convergecast through simulations. We ﬁnd that over 75% of the control messages
for backbone network construction are received from physical carrier sensing. While the
number of backbone nodes is relatively large, the backbone is very robust, quickly self-stabilizing, and only a fraction of the backbone nodes are used for broadcast.
Ó 2009 Elsevier B.V. All rights reserved.

1. Introduction
Sending data from one node to all others (broadcast)
and gathering data from all nodes at a single sink (convergecast) are two fundamental operations in wireless ad
hoc networks, including sensor networks. Broadcast is necessary for distributing program updates, route requests
and data queries. Convergecast is used to collect data from
sensors and maintain base station connectivity in multihop networks. Finding the optimal tree for broadcast/convergecast operations is an NP-hard problem, even when
the network is assumed to be a simple bi-directional graph

* Corresponding author. Tel.: +1 480 965 8593; fax: +1 480 965 8325.
E-mail addresses: Luke.Ritchie@asu.edu (L. Ritchie), Sapna.Deval@
asu.edu (S. Deval), reisslein@asu.edu (M. Reisslein), aricha@asu.edu
(A.W. Richa).
URL: http://www.fulton.asu.edu/~mre (M. Reisslein).
1
Tel.: +1 480 965 7555; fax: +1 480 965 2751.
1570-8705/$ - see front matter Ó 2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.adhoc.2009.01.004

with all edge weights known ahead of time. However, this
level of simpliﬁcation is not realistic: nodes may interfere
with each other even when they are too far apart to communicate reliably, and no node in an ad hoc network can
be expected to possess global knowledge about the other
nodes’ connectivities.
In recent algorithm-theoretic work [1,2] we have proposed algorithms for constructing backbone networks so
as to enable efﬁcient broadcast/convergecast in wireless
ad hoc networks.2 Distinctions of our algorithms from
existing approaches are that our algorithms consider a
general network model with non-uniform transmission
and interference ranges as well as interference ranges
longer than transmission ranges. Previously existing algorithms were mainly developed for simpler, less detailed
network models, such as the unit-disk model, which con2
For brevity we refer to the algorithms proposed in [1,2] as our
algorithms throughout this manuscript.

1348

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

siders only a disk-shaped transmission range and ignores
interference, and the packet radio model, which considers
disk-shaped transmission and interference ranges whereby
both ranges are equal. Furthermore, existing approaches
exploit physical carrier sensing primarily for efﬁcient
medium access control, whereas our algorithms exploit
physical carrier sensing for constructing a backbone
network—a network layer task—while explicitly incorporating the medium access control. The backbone network
connects all nodes with an approximately minimum number of hops and forms the basis for efﬁcient broadcast/
convergecast.
The main contribution of this paper is an original simulation study of the backbone network (spanner) construction and broadcast/convergcast algorithms proposed in
[1,2]. Our algorithm-theoretic work [1,2] provided only
asymptotic performance bounds, but did not examine the
actual performance of the algorithms for typical network
scenarios. In this paper, we evaluate the actual performance
of the backbone network construction algorithms for typical network scenarios through simulations and contrast
our approach with existing methods. We ﬁnd that in a network of nodes without any neighbor or topology knowledge, our algorithms utilize mainly carrier sensing
information to very quickly establish leader nodes that
can reach all nodes through non-interfering transmission
rounds. These non-interfering transmission rounds are then
exploited to minimize collisions when sending detailed
node address information to link the leader nodes into a
backbone network. Over 65% of the information bits for
backbone network construction are sent through the noninterfering transmission rounds, thus avoiding collisions.
Our simulations provide detailed insights into the performance of the individual phases of our algorithms. Furthermore, the simulation results indicate that our overall
approach compares favorably with existing strategies that
rely on a neighbor discovery protocol in conjunction with
backbone network construction algorithms utilizing neighbor lists as well as existing strategies that form a backbone
network from nodes with different prior knowledge (such
as number of neighbors) and without exploiting physical
carrier sensing. We also ﬁnd that while our backbones
are relative large, containing close to 50% of the nodes in
a 900 node network, the backbones are robust, remaining
functional even when roughly two thirds of the backbone
nodes are lost. Also, broadcast involves typically less than
about half of the backbone nodes in the 900 node network.
This paper is structured as follows: in the following
subsection, we review related work. In Section 2, we present our network model, including the physical carrier sensing ranges. In Section 3, we give an overview of the spanner
construction algorithm proposed in [1]. In Section 4, we
present our simulation evaluations of the spanner construction, spanner resilience, as well as broadcast and convergecast. We summarize our conclusions in Section 5.
1.1. Related work
Broadcasting (in the sense of network-wide ﬂooding) in
ad hoc networks has been studied extensively; see, for instance [3–9]. While simulations, as employed in this paper,

have been the primary means of performance evaluation,
there have been a few mathematical analyses. Chlamtac
and Weinstein [10] and Bar-Yehuda et al. [11] were among
the ﬁrst to present algorithms with formally analyzed
complexity for respectively the centralized and distributed
cases of broadcasting in wireless ad hoc networks. Tseng
et al. [12] analyzed the difﬁculties of relying on simple
ﬂooding as a broadcast mechanism. Gandhi et al. [13]
proved that minimum latency broadcast in ad hoc networks is NP-hard, and provide a broadcast algorithm with
constant latency based on a unit-disk network model. A
probabilistic analysis of two broadcast schemes is conducted in [14]. Simulation-based studies include work by
Stojmenovic et al. [15], in which an efﬁcient broadcasting
scheme based on a dominating set is developed. Recently,
Wu and Dai [16] presented an efﬁcient broadcasting
scheme for mobile networks, based on distributing topology information through ‘‘hello” messages with a larger
than normal transmission range. These existing approaches have primarily been developed and evaluated
for the unit-disk and packet radio models where the interference range does not exceed the transmission range. Jakllari et al. [17] have recently proposed a cross-layer ad hoc
networking framework using cooperative transmissions
with space–time block coding at the physical layer. They
consider different transmission and interference ranges
that depend on the achieved physical layer diversity gain.
In [18] they examined this cooperative transmission strategy in the context of the counter-based broadcasting strategy proposed in [12], which does not involve a backbone or
other topology control mechanisms. In contrast, we examine a suite of network layer protocols that ﬁrst form a
backbone network, which then can be used for broadcast,
convergecast, and other network layer functions, while
considering interference ranges larger than transmission
ranges.
Convergecast and the related problem of data aggregation in wireless sensor networks have been considered in
numerous studies, e.g., [19–24], which assume different
sets of network conditions and performance priorities.
Interference and channel contention at the MAC layer,
which we explicitly incorporate into our algorithm designs,
are generally ignored in these existing convergecast
approaches.
The problem of ﬁnding an optimal backbone (overlay)
network for broadcast or convergecast is usually expressed
in terms of minimum connected dominating sets and spanning sets. Finding these sets is an NP-complete problem
even with a basic unit-disk model of the network. Distributed algorithms that approximate the minimum connected
dominating set with polynomial or polylogarithmic running time include, e.g., [15,25–29]. The ﬁrst phase of our
algorithm (described in Section 3.1) is an extension of
the dominating set algorithm [30]. Comprehensive simulation comparisons of clustering and overlay network formation algorithms developed for unit-disk and packet radio
models are reported in [31]. The comparison study [31]
takes MAC packet collisions into consideration and classiﬁes the algorithms according to their level of localization,
i.e., over how many hops does the information for forming
the overlay travel, a classiﬁcation also employed in [32].

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Approaches with a high level of localization that require
information from within only a 2-hop local neighborhood,
such as approaches based on [15,33], are compared with
approaches that have lower levels of localization, such as
algorithms based on [34,35]. It is found that highly localized approaches tend to give the best performance. Our approach, which is developed for a more realistic network
model with interference ranges larger than transmission
ranges, is highly localized. In our backbone construction,
information never travels further than twice the interference range (via two physical carrier sensing communication hops in phase two, see Section 3.2) or three times
the transmission range (via three actual packet transmissions in phase three, see Section 3.3).
In work by Kuhn et al. [36] and Parthasarathy and Gandhi [37,13], distributed algorithms are presented that compute constant factor approximations of a minimum
dominating set in poly-logarithmic time. Both extend the
unit disk model taking interference into account, but nodes
need to know an estimate of the size of the network. In
contrast, our approach does not require any estimates of
network density, nor the total number of nodes in the
network.
Physical carrier sensing has been studied from a variety
of perspectives. In single-hop communication, physical
carrier sense is used in many random-access schemes, such
as 802.11’s version of Carrier Sense Multiple Access with
Collision Avoidance (CSMA/CA). Many lines of work focus
on the MAC layer — studying or optimizing the effect of
CSMA on throughput and power consumption, see e.g.,
[38–47]. The problem of topology control in ad hoc networks, see e.g., [48,49], has some similarity to our work
in goals and approaches. For example, Muqattash and
Krunz [50,51] use virtual and physical carrier sense with
power control to increase throughput and energy efﬁciency. Similarly, Tavli and Heinzelman [52] employ a
cross-layer approach spanning the MAC and network layer
to minimize energy for broadcast. However, to the best of
our knowledge, our protocol is the ﬁrst to directly exploit
physical carrier sense for the network layer task of distributed spanner construction.

1349

sion fails due to interference, the interfering transmission
must have originated at a node closer than r i =ð1  dÞ to
the receiving node. Not all transmissions from nodes within r i will cause interference, but transmissions from nodes
outside of it never will. In a typical network, r i is 2–3 times
larger than rt ; our examples and simulations are generally
based on a ratio of r i =r t ¼ 2. Due to the d factor, the actual
transmission area can be an arbitrary shape with approximate radius rt .
We also assume that each node can perform physical
carrier sensing to detect when the medium is busy. In
802.11, this is based on Received Signal Strength Indicator
(RSSI) measurements. In a similar manner to rt and r i , we
deﬁne the certain-sensing range r st and the non-sensing
range rsi for the carrier sense operation: signals traveling
less than r st =ð1  dÞ are sensed with probability close to
one, whereas signals traveling farther than r si =ð1  dÞ are
sensed with close to zero probability, and in between with
arbitrary probability. We further assume that these ranges
can be tuned by adjusting the SNR threshold at the receiver. In 802.11, the threshold can be adjusted through the
Clear Channel Assessment rules. As with transmission
ranges, rsi =r st  2–3 typically, and for ease of discussion,
we set r si =r st ¼ ri =r t ¼ 2.
This model is a close match for the actual performance
of current wireless interfaces. Forward error correction
mechanisms allow for relatively sharp cutoffs between
the area where messages are almost always received
(transmission range; a range r such that the communication
cost r  ð1  dÞ is less than the transmission range rt , i.e.,
r  ð1  dÞ < r t ), where they may still interfere (interference
range; r t < r  ð1  dÞ < r i ), and where they never interfere
(r  ð1  dÞ > r i ). (For generalizations to communication
costs that are not monotonic in the distance r we refer to
[1].) We refer to a pair of nodes as connected if there exists
a path between the two nodes (possibly including hops
traversing intermediate nodes) such that the communication cost r  ð1  dÞ is less than the transmission range rt
for each hop along the path. We refer to a set of nodes as
connected if each pair of nodes in the set is connected.

3. Spanner construction algorithm
2. Network model
Our model makes very few assumptions about the connectivity among nodes in the network. Unlike the relatively restrictive unit disk or packet radio models, the
minimum assumptions for our algorithms can accommodate a number of real world scenarios and form the basis
for a robust cross-layer design.
Assuming that all the nodes operate in the same frequency band and use the same data rate, there will be a
transmission range rt that increases with transmission
power. For simplicity of discussion, we will assume all
nodes use the same ﬁxed power and have a transmission
range deﬁned by the same r t . Any two nodes closer together than r t =ð1  dÞ (where d; 0 6 d < 1 is a constant that
represents the non-uniformity factor of the network) can
reliably communicate, while nodes farther apart cannot.
We deﬁne the interference range ri , such that if a transmis-

The algorithm presented in [1] constructs an overlay
network in three phases. In Phase I, the distributed election
of ‘‘leader” nodes creates a dominating set: every node in
the network is either a leader or within the transmission
range r t of at least one leader. Phase II consists of a distributed assignment of leader time slots such that each leader
can communicate with neighboring non-leader nodes
without interfering with other leaders’ transmissions.
(The same non-interfering assignment will be used later
to reduce collisions during broadcast and convergecast.)
In Phase III, leaders select gateway nodes to form a spanning set—a connected set of leaders and gateways.
Selecting a minimum dominating set or spanning set is
NP-hard, but our distributed algorithm selects a spanner
with bounded density in poly-logarithmic time.
The timing structure of the algorithm is organized
according to locally synchronized ‘‘rounds”, such that each

1350

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

...

Phase I

Phase II

Phase III

Data Phase I
Period

Phase II

Phase III

...

One Round
Fig. 1. Illustration of algorithm timing structure: a round contains slots for each phase of the algorithm. k rounds make up one frame.

round contains 10 slots to accommodate phases I–III (see
Fig. 1). A sequence of k rounds forms a ‘‘frame”, where
the parameter k will be determined later (see Section
4.3). The beginning of each frame is not required to be synchronized between nodes, only the round/slot timing.
More speciﬁcally, the beginning and duration of each individual round and slot must be synchronized among the
nodes. However, each node may start its frame, i.e., the sequence of k consecutive rounds forming a frame, with a
different round.
An important feature of the algorithm is that all three
phases run in parallel. Each round contains slots for all
three phases, as illustrated in Fig. 1. It is common in articles describing spanner construction algorithms made up
of multiple phases to speak as if each phase does not begin
until the previous phase has completed. This simpliﬁes discussion, but in practice this type of synchronization would
be infeasible without some means for every node to know
that each phase has stabilized. In contrast, our algorithm is
designed to run in parallel. Once each phase has stabilized,
the next phase can self-stabilize without any global signal
that this transition has occurred. Running in parallel is also
important for the algorithm to adapt to mobility. The time
between rounds is used for sending data messages (Fig. 1).
If one or more nodes move, the algorithm has the ability to
re-stabilize during subsequent rounds.
3.1. Leader election (Phase I)
The leader election algorithm works roughly as follows.
Every node attempts to become a leader by announcing its
candidacy for leadership if it has heard no other leader
announcements. The timing of these announcements requires no prior knowledge about neighboring nodes, not
even the number of nodes. In particular, a node listens
and, if it has not heard from another potential leader for
one full frame, it becomes ‘‘active” in the current round
and sends out a leader message with a constant probability
p (that is independent of the density of the network) in the
second slot of that round every frame. If a leader candidate
chooses to listen (with probability 1  p) and senses or receives another leader message, it reverts to an inactive
state. As long as a node remains active, it sends an active
message in the ﬁrst slot of that round in every frame,
and possibly (with probability p) a leader message in the
second slot. The algorithm can be proven to quickly converge to a stable and complete leader set for any reasonable p value [1].
More speciﬁcally, during the leader election phase, two
different carrier sensing thresholds are used, depending on
the state of the node. In the inactive listening state, a smaller listening range is used. In particular, the non-sensing
range is set equal to the transmission range r si ¼ r t , which

is sufﬁcient for assessing whether there are other leader
candidates nearby. The idea is that a node should avoid
becoming part of the leader set if there is another leader
within its certain-sensing range r st ; this rule puts an upper
limit on the number of members the leader set will include. Only transmissions from nodes within the rt range
can possibly be heard, and transmissions from within
rst ¼ r t =2 will almost certainly be heard (see Fig. 2a).
If, after listening for a full frame, a node has heard nothing with the smaller range, it uses a larger sensing range
during the round it is attempting to claim. A range set large
enough to sense a busy medium reliably at the interference
range, r st ¼ ri (see Fig. 2b), has the effect of setting a lower
limit on the distance between leaders with the same active
slot. Based on this property, it can be shown that the leader
set is stable; i.e., once a node becomes the only active node
for a given round within its transmission range r t , it will remain active.
It is of course possible that multiple nodes will attempt
to become active during the same round. The probabilistic
exchange of leader messages in the second slot allows candidate leader nodes to discover other such nodes within
their transmission radius rt .
At the end of the leader election phase, all of the nodes
will be ‘‘covered” by the transmission ranges of leader
nodes. No two leaders will be closer than r t =2, and no
two leaders active in the same round will be within each
others’ transmission range rt . In Phase II of the overlay network construction algorithm, the active rounds of the leader nodes are re-assigned such that interference of two
leaders at any other node is impossible.
3.2. Non-interfering transmission round assignment
(Phase II)
The goal of the second phase is to assign each leader
node a non-interfering transmission round. In other words,
Phase II is a distributed algorithm for creating TDMA
assignments among leader nodes. The general requirement
for non-interference is that no two nodes within an
ri þ rt ¼ 3r t range transmit simultaneously, i.e., A’s transmission range does not overlap with B’s interference range,
and vice versa.
Basically, we are concerned with two nodes that cannot
‘‘hear” each other transmitting at the same time—a version
of the hidden terminal problem. If two sources are out of the
transmission range (‘‘hidden” from each other), they may
cause a collision (interfere) if they transmit simultaneously. For example, if two nodes A and B, that are further
than the transmission range r t apart, transmit during the
same slot, there could be a collision at node C that is within
the interference range ri of both A and B. Listening only for
complete transmissions with range rt is insufﬁcient to

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

1351

Fig. 2. Illustration of effect of different carrier sensing ranges at an inactive node A. (The transmission, interference, and sensing ranges are drawn as
idealized circles for ease of explanation; the model allows for non-uniform ranges as detailed in Section 2.)

avoid this problem, because nodes such as A and B that
cannot reliably communicate can still cause collisions. Instead, our algorithm uses another strategy to extend the
effective listening range: leader nodes set their certainsensing range r st ¼ r i , and non-leader nodes, also listening
with certain-sensing range rst ¼ r i , alert the leader nodes of
sensed transmissions. As a result, at a range of 2r i between
active leaders in the ﬁnal TDMA assignment, interference is
impossible.
The hidden terminal problem has been extensively
studied, and proposed solutions include physical carrier
sense, virtual carrier sense, and out-of-band control. Virtual carrier sense (an RTS/CTS-style handshake) is relatively effective for unicast transmissions. But for
broadcast transmissions that solution becomes unacceptably complicated as we must wait for acknowledgments
from an indeterminate number of nodes. Our use of variable sensing ranges is an efﬁcient tactic in this case because it makes use of existing capabilities while keeping
the algorithm relatively simple.
More speciﬁcally, the Phase II algorithm uses variable
sensing ranges as follows: initially every leader chooses a
random round, setting its threshold such that r st ¼ r i , with
the goal of eventually becoming the sole ‘‘owner” of a
round. Leaders transmit a message claiming their round
in the second slot while non-leaders sense with a range
r st ¼ ri and report back collisions to all leaders within the
interference range ri during the fourth slot. Leaders will
therefore be able to hear about collisions in their round
caused by any other leader within an ri þ ri ¼ 4rt range
through two carrier sense communication hops. The ﬁrst
slot is used by leaders who have successfully become owners to send an ownership message in their active round. For
this exchange, non-leaders sense with a range rst ¼ 2r i and

respond in the third slot, notifying leader nodes in the volatile, non-owner state that the current round is occupied.
This larger range ensures that leaders in the owner state
remain owners. In both cases, when a leader hears about
a collision or an occupied round, it randomly selects a
new round. This algorithm soon converges to a stable set
of non-interfering leader transmission round assignments.
In Phase III, these non-interfering leader transmission
rounds are exploited to time the transmissions of leader
and gateway node information so as to minimize collisions.
3.3. Connecting leader nodes through gateway nodes
(Phase III)
The third and ﬁnal phase is selection of gateway nodes
that connect nearby leaders. For a given leader, every leader within a 3r t range may be reachable by relaying messages through up to two gateway nodes. This implies that
the spanner has a maximum stretch factor of 5: the shortest
path between two nodes using only spanner links is no
more than 5 times the shortest path using all network
links. (In our extensive simulations presented in the next
section we never encountered an actual stretch factor larger than 1.4.) Both leader and non-leader nodes maintain
caches of local gateway information, which eventually converge to lists of gateway and leader nodes that will be used
to route messages between nearby leaders.
More speciﬁcally, non-leader nodes initiate a four step
process proceeding over four slots. In the ﬁrst slot, each
non-leader sends with a certain probability a ping to locate
an active leader within transmission range rt . If a leader responds with an acknowledgment in the second slot, the
non-leader prepares gateway advertisements for the third
and fourth slots. The third slot is used to pass leader infor-

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

mation to other non-leaders, while the fourth slot is used
to pass gateway information to the leaders themselves.
Non-leaders that randomly choose not to send the initial
ping listen during this slot. In a provably bounded number
of rounds, all nodes have complete gateway information
with high probability.

4. Performance evaluation
We present results for six simulation studies conducted
for static ad hoc wireless networks with a custom-built
simulator based on OMNeT++: (A) a study of how sensitive
spanner stabilization time is to the probability p used in
Phase I; (B) a study of the degree to which non-uniformity
(as deﬁned by the parameter d) affects stabilization time;
(C) a study of frame length; (D) a study of spanner construction; (E) a study of spanner resilience; (F) a study of
the broadcast mechanisms developed for the spanner;
and (G) a study of the convergecast mechanisms for the
spanner.
4.1. Effect of p value used in Phase I on stabilization time
In several slots, our algorithm behaves probabilistically
with respect to the timing for sending messages. A message for spanner construction may or may not be sent in
a given slot depending on certain probabilities. In general,
this probabilistic behavior is used to mix up the sets of
senders and receivers for a given slot. A node that could
not hear a message in one slot because it was sending will,
with high probability, hear it later. In these cases, the probabilities are part of a strategy for multiple access.
The only such probability that was not explicitly ﬁxed
in [1] is the value of p used in slot 2 of Phase I. In this slot,
active leader candidates send a message with probability p
and consequently listen with probability 1  p. The value
of p that can be analytically proven to perform well, with
high probability, for all network sizes is p ¼ 1=d. The constant d is equal to the maximum number of leader nodes
within the interference area of a node, which is bounded
for any arbitrary network as shown shortly in this section.
When p is chosen this way, the convergence time for Phase
I is guaranteed to remain roughly the same, even as the
network size and the network density increase. However,
this ‘‘optimal” value of p does not necessarily guarantee
the minimum stabilization time for a given network size,
merely that the time will be bounded for all networks. In
fact, we expect that values of p signiﬁcantly larger than
1=d work well in most cases.
Fig. 3 shows the results from simulations that illustrate
this effect. Consistent with typical simulation parameter
settings in the literature, we consider a 200  200 m
square network area with N = 100–900 nodes, each with
transmission range r t ¼ 30 m. Furthermore, following the
typical wireless propagation characteristics (see Section
2), we set the interference range to twice the transmission
range, i.e., r i ¼ 60 m (and correspondingly rsi ¼ 2rst ). For all
considered p values, the stabilization time for Phase I is relatively stable for the considered wide range of node densities; especially for the mid-range of p, Phase I stabilization

15

Time in frames

1352

10
N=100
N=200
N=300
N=400
N=500
N=600
N=700
N=800
N=900

5

0

0

0.25

0.5

0.75

1

probability p
Fig. 3. Stabilization time for Phase I for various values of p and N. The ﬁrst
data point is p ¼ 0:017  1=60.

time is independent of the network density. For our simulation setting, we have estimated a value of d ¼ 60, as explained shortly. The shortest stabilization times occur for
p ¼ 0:25, rather than for p ¼ 1=60. Since the times for
Phase I stabilization comprise only a small fraction of the
overall spanner stabilization time, and neither the p value
nor the Phase I stabilization time affect the subsequent
phases, we arbitrarily select the mid-range value p ¼ 0:5
for the subsequent simulations.
The value of d ¼ 60 was estimated for our simulation
scenario as follows: By deﬁnition, d is the maximum number of leader nodes that can be within the interference
range r i of a node. If we consider the case in our simulation
where r i ¼ 2r t is a uniform circle, and leaders may be as
close as rs ¼ r t =2 to each other, then the maximum value
of d can be obtained by viewing this as a disk packing problem. Since disk packing traditionally deals with non-overlapping disks, we note that the problem of placing nodes
no closer than rs is equivalent to packing non-overlapping
disks with radius rs =2. Also, since a node at the maximum
distance of r i has half of its transmission range beyond r i ,
our bounding circle has a radius of ri þ rs =2 ¼ 9r s =2. Therefore, the problem is equivalent to packing unit disks into a
circle of radius 9. In this case, the densest known packing
(a curved hexagonal packing [53]) allows 61 disks to ﬁt inside the larger circle. But our simulation is also bounded by
the number of rounds in a frame, k ¼ 60. Since each leader
node in the spanner must claim a unique non-interfering
round, the algorithm cannot create more than 60 stable
leaders in the deﬁned radius.
The constant d thus estimated is invariant with respect
to network size, node density, and any other topology
characteristic. Again, in [1], we formally prove that the p
chosen according to the constant d indeed works for any
network topology, and the simulation results validate the
theoretical results of [1].
4.2. Non-uniformity and d effects of sensing and interference
ranges
Our spanner construction algorithm is based on a physical model that allows for two types of non-uniformity. The

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

ﬁrst type of non-uniformity involves a range where interference or carrier sensing may occur with arbitrary probability. These ranges are ð0; ri Þ and ðrst ; r si Þ, respectively. In
order to conduct a worst-case analysis of interference for
this type of non-uniformity, we let all transmissions within
ð0; ri Þ interfere. For carrier sensing, the situation is somewhat different. Multiple overlapping transmissions may
increase the probability of detecting a busy carrier within
the arbitrary range ðrst ; r si Þ. As a simulation model for the
carrier sensing range, we require at least two nodes to be
transmitting within the ðrst ; r si Þ range in order for sensing
to occur. (Only one transmitting node is required within
the ð0; rst Þ range.) This simple model takes into account
the effect of multiple transmissions on carrier sensing,
and can be further reﬁned in future work to simulate received power levels with ﬁner granularity.
The second type of non-uniformity involves a non-uniform shape for each of these ranges: r t ; ri ; rst , and r si are
each deﬁned in terms of a cost function cðu; v Þ which
may take on values that are some factor d larger or smaller
than the Euclidean distance d between the nodes u and v.
For a detailed discussion on how the cost function deﬁnes
the transmission, interference, and sensing ranges we refer
to [1]. The worst case for this type of non-uniformity occurs when d takes on large values (close to 1). Separate
simulations for different d values are not necessary, as
the effect of scaling up d is equivalent to scaling up the
node density as explained in the following.
If the cost function takes on values larger and smaller
than the Euclidean distance with roughly equal probability, then the expected average transmission area is the
average of the area of the maximum and minimum transmission ranges corresponding to the cases when
cðu; v Þ ¼ ð1  dÞd and cðu; v Þ ¼ ð1 þ dÞd, respectively. More
speciﬁcally, the average area of two discs with radii
r t =ð1 þ dÞ and r t =ð1  dÞ. This average area is equal to
pr2t ð1 þ d2 Þ=ð1  d2 Þ2 , which is a monotonically increasing
function for d 2 ½0; 1Þ. Thus, increasing d effectively increases the expected average transmission area.
Increasing the expected average transmission area has
the effect of increasing D, the largest number of nodes
within the transmission range of any node. The stabilization time and (by extension) byte overhead are bounded
by D, which we can think of as the ‘‘maximum density”
of the network. Therefore, we expect increasing d to have
a similar effect on overhead as scaling up D. In particular,
our experiments that scale up the density of the network
(in terms of increasing the total number of simulated
nodes N in a ﬁxed area) show the equivalent effects of scaling up D.
To demonstrate this effect, we conduct a veriﬁcation
experiment with N ¼ 300 nodes. We model the effect of
the cost function and d in the following manner: all edges
with length d 6 rt =ð1 þ dÞ exist in the network graph, and
edges with r t =ð1 þ dÞ < d < r t =ð1  dÞ exist with probability 1=2. Note that the cost function is a general model that
can accommodate a wide range of propagation models; we
have chosen this model since it exhibits high variability
and is reasonably simple to simulate. We observe from
Fig. 4 that the stabilization times indeed increase with
increasing values of d; these increases of the stabilization

1353

Fig. 4. Stabilization time for increasing values of d for N ¼ 300; the
plotted 95% conﬁdence intervals are obtained from 20 independent
replications.

time are well within the increases in stabilization time observed for increasing N values in Fig. 10d.
4.3. Frame length and measurement of overhead and
information exchange
The spanner construction process makes use of both
carrier sensing and packet transmission for information
exchange between nodes. Additionally, carrier sensing allows the algorithm to form contention-free time slots for
each leader node, greatly reducing the number of packet
transmissions that are subject to collision. In order to measure the number of information bits exchanged through
each mechanism, we use the rules described below for
each communication slot. Each round consists of 10 slots,
as illustrated in Fig. 1.
The duration of the individual slots depend on the information contained in them, which is either presence/absence of sensed physical carrier (neglected in Byte count),
node address (counted as 6 Bytes), or time stamp (counted
as 2 Bytes); in addition, we count 6 Bytes of overhead for
each slot. Thus, the lengths of the control messages sent
in the individual slots and the resulting phase lengths are
as described below. See Table 1 for a summary.
During Phase I, the spanner algorithm uses a simple signal (indicating the presence of a transmission) in the ﬁrst
slot, and a message containing a node ID in slot 2. This
gives 6 þ 12 ¼ 18 Bytes. Note that all messages sent during
Phases I and II only require carrier sensing, so these signals
represent 1 bit of information sent for each transmitting
node and 1 bit received for each sensing node. Similarly,
during Phase II, the spanner algorithm uses a message containing an ID and a timestamp during the ﬁrst slot, and
simple signals during the other three slots. This gives
14 þ 6 þ 6 þ 6 ¼ 32 Bytes for Phase II. The constant factor
in the algorithm’s running time could potentially be reduced by only using the minimal 6 Byte messages throughout Phases I and II, giving a total of 6  6 ¼ 36 Bytes for
both phases. However, the messages received in slot 2 of
Phase I and slot 1 of Phase II can be used to collect

1354

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Table 1
Summary of slot lengths and information exchanged
Phase

Slot

Message
vulnerable
to collision

Exchanged
information
(bits/msg)

Message length
(Bytes)

I

1
2

No
Noa

1
1 or 48a

6
12

II

1
2
3
4

Noa
No
No
No

1 or 64a
1
1
1

14
6
6
6

1
2
3
4

Yes
No
Yes
No

96 or 1b
50 or 98
112
208n

18
18
20
6 + 26n

III

a
Phase I slot 2 and Phase II slot 1 can be used to collect information in
advance for Phase III, provided the messages do not collide. When this
mechanism is used in simulation, the larger number of information bits is
recorded.
b
In Phase III slot 1, active leader nodes may only sense a busy or free
carrier when the number of client nodes transmitting is not equal to 1.
This carrier sensing determines the leaders’ reply type in the next slot.

information in advance for Phase III, reducing the overall
stabilization time. This reﬁnement is incorporated into
our simulation experiments and evaluations of information
exchanged count these Bytes. Our stabilization time estimates are based on the longer slot lengths.
At the start of Phase III, assuming the previous phases
have stabilized, each leader owns a contention-free time
slot. However, intra-cluster transmissions from non-leader
nodes are still subject to collisions. Therefore, the ﬁrst two
slots of Phase III contain a built-in contention mechanism.
Non-leader nodes compete to receive an ACK message
from the active leader in response to their CLIENT messages. Therefore, we measure the overhead needed for this
slot by counting the total number of CLIENT messages sent
versus the number of CLIENT messages that receive ACKs.
The non-acknowledged CLIENT messages are not retransmissions in the usual sense because not all CLIENT messages need to be received in order for the spanner to
stabilize. The 12 Bytes of information sent in each CLIENT
message only need to be received from a subset of the
non-leader nodes. On the leader’s side, if only one client
message is received, 12 Bytes of information are received;
and if several client messages collide, 1 bit of information
is sensed. Including the header, the total length of this slot
is 18 Bytes.
In the second slot of Phase III, the leader replies to its
non-leader nodes with one of three types of responses
(ACK, COLLIDE, or FREE), which are of variable size. The
header must therefore contain 2 bits of information to distinguish response type, and depending on the type, an
additional 6 or 12 Bytes of payload information are sent
and received. Because it is sent by a leader node, this transmission is not subject to collision. In any case, the total
length of the slot is 18 Bytes, assuming the 2 bits of message type are absorbed into the header.
In the third slot of Phase III, a non-leader node which
has received an ACK may send an ADV message containing
14 Bytes of topology information to other non-leaders.

Fig. 5. Three leader nodes (gray squares A, D, and F) and three regular
nodes (gray circles B, C, and E) with the relative distances depicted may
result in a collision in Phase III slot 3.

Because these transmissions are between non-leaders,
they are subject to collisions. Consider the example illustrated in Fig. 5, with nodes B and E having been authorized
to send ADV messages by the leader nodes A and F, respectively. (Note that it is possible for both leader nodes A and
F to be active in the same round because they are more
than r i þ r i apart. Leader node D must be using a different
active round.) Node C is one potential recipient of the ADV
broadcast from B, but the reception at node C is subject to
possible interference from node E. However, while collisions are possible in this slot, the spacing of active leaders
established in Phase II greatly reduces the number of actual collisions (see Table 2 in Section 4.4.2).
In the fourth slot of Phase III, the node which sent an
ADV will also broadcast a GATEWAY message to leader
nodes. A GATEWAY message can always be received by
the leader node that authorized it, due to the r i þ r i spacing
of active leader nodes. Therefore, these messages are not
subject to collisions. The size of the gateway message depends on the number n of 26 Bytes entries it contains.
We observed in our simulations (see Fig. 6) average maxima ranging from 6 to 11.25 entries when slot 4 was used
with network sizes ranging from 100 to 800 nodes, and we
conservatively set n ¼ 12 for our delay estimates.
We calculated the values in Fig. 6 using the following
method. In each frame, the simulation averaged the number of gateway entries over all nodes and all rounds. Then,
we took the maximum of these samples over all frames of
all 20 runs for each value of N. Since these are maximum
and not mean values, we do not include conﬁdence
intervals.
The duration of a frame also depends on the k, the number of 10-slot rounds in each frame. When choosing a good
value for k, there is a tradeoff between high values of k
which give nodes a large number of ‘‘colors” to use in the
distributed coloring of Phase II, and low values of k which
shorten the frame length. We observe in Fig. 7 that values
of k < 60 do not allow spanner construction to complete in
dense networks (large N with ﬁxed area). For values of
k P 60, the time it takes the spanner to stabilize generally
increases as the frame length increases. Therefore, to minimize stabilization time, we should choose the smallest value of k that allows spanner construction to complete in
dense networks. For the values we examined, the number
of rounds per frame that meets these criteria is k ¼ 60,
which we use in all other simulations.
Based on a value of k ¼ 60, a 1 Mbit/s transmission rate
and a 20 ls spacing between slots, a frame is 215.5 ms
long. We note that our algorithms could be further reﬁned

1355

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369
Table 2
Average number of transmitted (tx), sensed (sx), collided, and received (rx) messages per node.
N
Phase I
# of msgs tx
# of msgs sx
=# of bits sx
# of msgs rx
# of bits rx
Phase II
# of msgs tx
# of msgs sx
=# of bits sx
# of msgs rx
# of bits rx

100

200

300

400

500

600

700

800

900

40  16
296
112
0:10
0:02
4:9  0:95

32  17
478
244
0:063
0:008
3:0  0:37

24  8:6
509
190
0:046
0:008
2:2  0:37

15  9:6
412
266
0:026
0:007
1:3  0:34

14  5:6
482
213
0:028
0:002
1:3  0:12

12  3:9
487
175
0:024
0:003
1:1  0:15

11  4:3
495
208
0:021
0:003
1:0  0:14

12  5:2
636
295
0:014
0:004
0:68  0:17

8:8  3:1
474
199
0:016
0:002
0:77  0:11

188
78
1204
433
20  8:2
1259
528

271
107
1416
474
27  11
1740
692

330
77
1551
311
32  7:7
2072
490

333
131
1520
495
33  13
2082
823

321
110
1451
425
31  11
2002
688

362
80
1593
300
35  7:6
2268
489

505
239
2078
842
49  22
3111
1440

389
148
1647
526
38  15
2452
944

666
279
2586
920
65  27
4162
1741

Phase III slot 1
# of msgs tx
# of msgs sx
=# of bits sx
# of msgs rx
Kilobits rx
# of collisions

40  5:0

110  13

173  16

257  24

334  26

379  35

424  49

510  51

538  71

14  1:8
38  4:6
3:7  0:44
8:1  0:93

23  2:5
52  5:5
5:0  0:52
17  1:9

25  2:1
47  4:4
4:5  0:43
20  1:7

26  2:2
43  3:7
4:1  0:35
21  1:8

26  1:9
40  3:7
3:8  0:35
21  1:8

25  1:9
36  3:8
3:4  0:36
21  1:6

23  2:6
31  3:5
3:0  0:34
20  1:6

24  2:2
31  2:9
2:9  0:28
20  1:9

21  2:4
26  3:1
2:5  0:30
18  2:1

Phase III slot 2
# of msgs tx
# of msgs rx
Kilobits rx

24  3:0
16  2:0
1:3  0:15

34  3:7
70  8:1
4:1  0:46

36  3:0
116  9:2
6:3  0:50

36  2:9
157  13
8:3  0:70

35  2:5
188  14
10  0:74

33  2:6
207  18
11  0:94

30  3:3
218  25
11  1:3

31  2:8
254  24
13  1:2

27  3:1
245  29
13  1:5

Phase III slot 3
# of msgs tx
# of msgs rx
Kilobits rx
# of collisions

9:98  1:2
34  4:2
3:8  0:47
0:04  0:01

12:0  1:3
104  11
12  1:2
0:03  0:009

10:9  0:92
157  13
18  1:4
0:18  0:08

10:1  0:82
204  17
23  1:9
0:068  0:02

9:03  0:67
236  17
26  2:0
0:026  0:009

8:04  0:72
257  22
29  2:5
0:36  0:1

7:01  0:72
265  28
30  3:2
0:19  0:08

7:04  0:66
306  29
34  3:3
0:16  0:07

6:01  0:68
298  34
33  3:8
0:17  0:08

Phase III slot 4
# of msgs tx
# of msgs rx
Kilobits rx

9:98  1:2
9:98  1:2
28  4:0

12:0  1:3
12:0  1:3
50  6:1

10:9  0:92
10:9  0:92
54  5:4

10:1  0:82
10:1  0:82
55  5:3

9:03  0:67
9:03  0:67
53  6:6

8:04  0:72
8:04  0:72
48  6:2

7:01  0:72
7:01  0:72
42  4:9

7:04  0:66
7:04  0:66
46  5:7

6:01  0:68
6:01  0:68
39  5:2

84  10

168  19

230  21

313  29

386  30

428  38

468  54

554  55

577  75

14  1:8
98  11
37  4:9
30  4:1

23  2:5
238  25
71  8:1
54  6:5

25  2:1
331  27
82  7:6
60  5:8

26  2:2
414  34
90  8:1
63  6:0

26  1:9
472  35
94  10
63  7:4

25  1:9
508  44
91  10
59  7:0

23  2:6
521  57
86  9:5
53  6:1

24  2:2
598  56
96  10
58  6:8

21  2:4
576  67
87  10
52  6:6

312
82
1515
480
118  11
38  4:8

471
127
1916
712
265  28
73  8:1

584
82
2085
483
363  25
84  7:5

645
151
1936
731
426  57
88  12

721
116
1958
617
504  36
96  9:5

803
82
2105
423
543  45
93  9:8

985
217
2596
892
570  51
89  9:0

929
171
2305
790
606  85
94  14

1252
283
3081
998
641  72
91  11

Phase III totals
# of msgs tx
# of msgs sx
=# of bits sx
# of msgs rx
Kilobits rx
Kilobits rx not
vulnerable to
collision
Totals
# of msgs tx
# of msgs sx
=# of bits sx
# of msgs rx
Kilobits rx

in future work to reduce the number of gateway entries,
thus shortening the frame duration.
4.4. Performance results for spanner construction
To demonstrate the performance of spanner construction, we present two types of results: (i) detailed samplepath data providing insights into the evolutions of the different phases of our algorithm, and (ii) aggregate data providing insights into the overall algorithm performance,
both for 100 up to 900 node networks. For each simulation

we conducted 20 independent runs, each starting with an
independent random placement of nodes without any
information about neighbors or topology in a 200
200 m square ﬁeld. We use a transmission range of
rt ¼ 30 m and an interference range of r i ¼ 60 m.
4.4.1. Sample-path simulations
We plot in Fig. 8a–c the means for the number of leader
nodes determined by Phase I, the number of leader nodes
owning non-interfering transmission rounds determined
by Phase II, and the number of gateway nodes determined

1356

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

12

Gateway Entries

10
8
6
4
2
0

200

400

600

800

# Nodes
Fig. 6. Average maximum number of GATEWAY entries when slot 4 is
used.

4

x 10
3.5

Time in rounds

3
2.5
2
1.5

k=80
k=70
k=60
k=50
k=40

1
0.5
0

0

200

400

600

800

1000

# Nodes
Fig. 7. Stabilization time for various values of number of rounds per
frame k and number of nodes N (95% conﬁdence intervals from 20
replications).

by Phase III of our algorithm, respectively, as a function of
time in frames. Each line depicts data for a different number of nodes, ranging from 100 at the bottom to 900 at the
top.
We observe from Fig. 8a and b that the leader nodes and
the ownership of non-interfering transmission rounds are
completely stabilized in all cases within approximately
10 and 15 frames, respectively, whereas the gateway nodes
stabilize hundreds of frames later. Speciﬁcally for the
N ¼ 300 node scenario, the leader nodes and the ownership of non-interfering transmission rounds stabilize within about two and eight frames, respectively, whereas the
gateway nodes stabilize after close to 215 frames. We
therefore conclude that for this scenario (increasing nodes
in a ﬁxed area), the completion times for Phases I and II are
small and relatively constant. On the other hand, we observe from Fig. 8c that the time it takes for Phase III to
complete is relatively large and increases with the number
of nodes/density. We also note that most of the gateway
nodes have been discovered well before the spanner is

completely stable. If we consider it sufﬁcient that 90% or
95% of possible gateway nodes are established, the overall
stabilization time is much smaller. Speciﬁcally, as examined in detail in Fig. 10d, for the N ¼ 300 scenario, the
backbone is completely constructed after about 46.4 s,
while 90% of the gateway nodes have been discovered after
approximately 86 frames (18.5 s), whereas 95% and 98% of
the gateway nodes are established after 127 frames
(27.3 s) and 162 frames (34.9 s), respectively, indicating
that the spanner is close to complete at these earlier times.
Comparisons with existing backbone formation algorithms are complicated due to a number of crucial differences and can therefore only give a rough indication of
the relative performance of our algorithm. For instance,
two extensively studied algorithms in [31], namely an
algorithm based on [15,33] and an algorithm based on
[34], assume that each node has a complete list of other
nodes in its transmission range, whereas we do not assume
any prior neighbor or topology knowledge by the nodes.
Creating such neighbor lists requires neighbor discovery
through the exchange of hello messages and is examined
in recent studies, see e.g., [54–59], which report time durations for completing the neighbor discovery ranging from
several seconds to a minute, with delays on the order of
10s of seconds being most typically reported. Also, our approach produces a backbone network in conjunction with a
schedule of non-interfering (MAC packet collision-free)
transmission rounds for the leader nodes in the backbone
network thus greatly facilitating the use of the backbone,
whereas the exiting approaches only identify the nodes
in the backbone, but do not further facilitate the use of
the backbone. For 300 nodes with neighbor lists and a
30 m transmission range in a 200 by 200 m area, [31] reports average protocol durations of a little less than 2 s
for the [15,33] based algorithm and about 12 s for the
[34] based algorithm.
Comparing the overall strategies for constructing a
backbone starting from a network of nodes without any
topology information, we note that the [15,33,34] based
approaches in [31] ﬁrst employ a neighbor discovery protocol to obtain the neighbor lists at each node, and then
construct the backbone relatively quickly based on the detailed node address information in the neighbor lists. On
the other hand, our approach starts from nodes without
any neighbor or topology information and ﬁrst employs
physical carrier sensing ðIÞ to establish leader nodes that
can reach every node in the network in one hop, and ðIIÞ
to assign these leader nodes non-interfering transmission
rounds. With this ‘‘infrastructure” which is put in place
very quickly, as observed from Fig. 8a and b, we then (in
our Phase III) exchange detailed node address information
to learn the detailed local topology for the establishment of
the gateway nodes. This detailed node address exchange in
our Phase III makes Phase III by far the most time consuming phase, as observed in Fig. 8c, but is aided by the underlying ‘‘infrastructure” that minimizes collisions for the long
control messages with the detailed gateway information
sent in the third and fourth slot of Phase III.
The comparison study [31] also considers an algorithm
based on [35], which requires nodes to know the number
of neighbors (obtained through techniques such as [60]),

1357

60

60

55

55

# Owner Nodes

# Leader Nodes

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

50
45
40

50
45
40

35

35
0

10

20

30

40

0

20

40

60

80

Time in frames

Time in frames

(a) Number of leader nodes

(b) Number of leader nodes owning non-interfering
transmission rounds

# Gateway Nodes

400

300

200

100

0
0

200
400
Time in frames

600

(c) Number of gateway nodes
Fig. 8. Detailed sample-path simulations: evolution of numbers of leader and gateway nodes backbone creation as a function of time in frames for 100
(bottom curve), 200, 300, . . ., 900 nodes (top curve).

and reports a duration of 77 s for the 300 node network.
The [35] based algorithm requires the propagation of control information over several transmission hops, resulting
in a low degree of localization, and involves the construction of a spanning tree. In contrast, our approach is highly
localized and does not involve spanning trees, resulting in
signiﬁcantly faster backbone construction in a network of
nodes without any topology information.
The parallel phases in our self-stabilizing algorithm ensure a robust backbone that automatically recovers from
changes in the network, such as might occur due to node
mobility and node failure. In other words, our algorithm
has built-in functionality for both creating and maintaining
a backbone network. For a reasonably fair comparison of
the overhead (in terms of number of transmitted Bytes
per node) with existing backbone creation algorithms, we
report in Fig. 9a–f the means and 95% conﬁdence intervals
of the numbers of per-frame and cumulative transmitted
Bytes contributing toward backbone creation. Speciﬁcally,
we count the transmissions of Phase I, from the start of
the simulation until the leaders are stable. Similarly, we
count the transmissions of Phase II from the start of the

simulation until the round ownerships are stable. Then
we count the Phase III transmissions from when the round
ownerships are stable until the gateways are stable. We
observe from Fig. 9a, c, and e a brief spike in the per-frame
transmissions for the completion of the leader election and
non-interfering transmission round assignment. The transmissions then slowly taper off as the gateway nodes are
found. Referring back to Fig. 8c, we note that most of the
gateways are discovered early in the stabilization process.
Comparing Fig. 9b, d, and f we observe that the trend is
similar for large and small networks: a brief period where
transmissions occur rapidly and a longer period where
transmissions are less frequent, eventually tapering off.
We also note that the rate of transmissions during gateway
discovery, indicated by the slope in these ﬁgures, is almost
identical for N ¼ 300 and N ¼ 700. This trend will be investigated further in the next section.
4.4.2. Aggregate simulations
In this section we present aggregate simulation results
characterizing the created backbone network and the effort required for creating the backbone. We consider net-

1358

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

5000
125
4000

3000

Bytes

Bytes

100
75

2000

50

1000

25
0
0

25

50

75

0

100

0

Time in frames

25

50

75

100

Time in frames

(a) Number of transmitted Bytes by a node for backbone
creation per frame for

(b) Cumulative number of transmitted Bytes by a node
for backbone creation for
10000

175
150

8000

6000

Bytes

Bytes

125
100
75

4000

50
2000
25
0

0
0

100

200

300

0

100

200

300

Time in frames

Time in frames

(d) Cumulative number of transmitted Bytes by a node
for backbone creation for

(c) Number of transmitted Bytes by a node for backbone
creation per frame for

15000

200
175
150

10000

Bytes

Bytes

125
100

5000

75
50
25

0
0

0

200

400

600

Time in frames
(e) Number of transmitted Bytes by a node for backbone
creation per frame for

0

200

400

600

Time in frames
(f) Cumulative number of transmitted Bytes by a node
for backbone creation for

Fig. 9. Detailed sample-path simulations: evolution of number of transmitted Bytes for backbone creation as a function of time in frames for 100, 300, and
700 nodes.

works with 100–900 nodes, whereby each node has the
transmission and interference ranges r t ¼ 30 m and

ri ¼ 60 m (and r si ¼ 2r st ). Each data point in the following
plots represents the average of 20 runs, each with indepen-

1359

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

dent random node placement, with the error bars representing 95% conﬁdence intervals. In these aggregate simulations, we deﬁne a spanner to be stable in terms of its
sample-path data from the previous section. Intuitively,
we deﬁne a spanner to be stable when the remainder of
the curve shown in Fig. 8c is ‘‘ﬂat”. Speciﬁcally, we allow
each run to continue until the time t 1 when the status of
each node (leader/owner state and gateway list length)
has remained unchanged for 10 frames. In general, this
overestimates the stabilization time. We then calculate
the true stabilization time to be the time t s such that the
variance from the mean for the number of gateway nodes
from ts to t 1 is less than 0.01.
In Fig. 10a we plot the mean and 95% conﬁdence intervals for the number of leader nodes. In Fig. 10b we plot the
node degree deﬁned as the number of connections each
leader has to other leaders through attached outgoing
gateway links. Smaller leader set size and node degree
indicate that the overlay network will be able to distribute

data messages more efﬁciently, with fewer total transmissions. We observe that the number of leader nodes
(Fig. 10a) and the number of neighbors each leader node
has in the backbone (Fig. 10b) initially both increase, then
level off toward a maximum for denser networks. This
means that the quality of the backbone network generated
by our algorithm will not degrade in highly dense networks. The node degrees achieved in our simulations are
in the same range as those in the simulations in [25]. For
a 100 node unit-disk model network with similar area
and transmission range as our network, the algorithms in
[25] give average node degrees between 1.2 and 4 and
maximum node degrees between 4 and 41, compared to
average and maximum node degrees of approximately
6.6 and 13.4 with our algorithm. This is one indication that
our algorithm constructs a fairly efﬁcient backbone, while
considering a more detailed physical layer model.
In Fig. 10c, we plot the total size of our spanner, including leader nodes and gateway nodes. These represent large

70

30

40
30
20

0
0

200

400

600

800

15
10

0

1000

0

200

400

600

800

1000

# Nodes
(b) Overlay network density: Maximum and average
n umber of outgoing gateway node links at a leader node

# Nodes
(a) Number of leader nodes
500

500

400

spanner stable
95% gateways
90% gateways

400

Time in frames

Size of Spanner

20

5

10

300

200

100

0

maximum
average

25

50

Node Degree

Size of Dominating Set

60

300

200

100

0

200

400

600

800

# Nodes
(c) Number of leader and gateway nodes

1000

0

0

200

400

600

800

1000

# Nodes
(d) Time efficiency: Average length of time (in frames)
to complete spanner formation. Using our time estimate
of 1 frame = 215.5 ms, the 800 node network stabilizes
in ≈85 s.

Fig. 10. Aggregate simulation: backbone network characteristics as well as required backbone formation time for 100–900 nodes.

1360

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

percentages of the total network size and large numbers of
gateway nodes compared to other spanner construction
algorithms. The backbones for the 300 node networks in
[31] contain 60–75 nodes, compared to on average 54 leader nodes and 186 gateway nodes for a total of 240 nodes
in the backbone with our algorithm. One main reason for
our relatively high number of gateway nodes is that in an
effort to keep complexity low and topology information
exchanges local, our algorithm does not force two leader
nodes A and B to use the same gateway node(s) for communication in both directions between A and B. More speciﬁcally, from additional simulations for 300 node
networks we found that for almost a third of all A$B connections, the A!B connection has one gateway node while
the B!A connection has two gateway nodes. Typically, one
third of such A$B connections do not share any gateway
node, i.e., employ three unique gateway nodes to connect
leaders A and B in both directions. Furthermore, typically
more than half of all A$B connections employ two gateway nodes in each direction, whereby in turn more than
half of such connections traverse at least one different
gateway node in the different directions. For close to a
tenth of all connections, node A identiﬁed unique nodes
C and D as its gateway path to B, while B identiﬁed unique
nodes E and F as its gateway path to A. Moreover, roughly
4/5 of all one-node connections use a different gateway in
each direction. These results indicate that integrating a
gateway ‘‘pruning” algorithm, e.g., [33], is one possibility
for future work on this topic. However, we also note that
redundant gateway connections make the spanner more
robust to node loss and do not hinder the broadcast or convergecast algorithms, as we explore in subsequent
sections.
In Fig. 10d, we plot the stabilization time for the spanner construction alongside graphs of the points in time
when 90% and 95% of the total gateways have been discovered. We observe that the stabilization time increases
somewhat slower than linearly with the network size,
which matches our theoretically shown poly-logarithmic
complexity results [1]. This type of efﬁciency is important
in any backbone formation algorithm, but stabilization
time is especially important when nodes become mobile.
As indicated in the previous section, the 90% and 95% lines
show that the spanner is mostly complete well before it is
completely stable. Using our time estimate of 1 frame = 215.5 ms, even the 800 node network is 90% complete in approximately 40 s.
In Fig. 11a, we plot the total number of Bytes transmitted per node for the backbone creation during the stabilization of the spanner. For better comparison with
algorithms that do not run in parallel, we only count packets sent during ‘‘active” phases of the algorithm (as in the
previous section). We observe that similar to stabilization
time, communication cost increases somewhat slower
than linearly with the network size. Again, this matches
our theoretically shown poly-logarithmic complexity results [1]. Speciﬁcally, for N ¼ 300, we observe from Figs.
9d and 11a a cumulative overhead of 8 104  606 (for
95% conﬁdence interval) transmitted Bytes per node for
the backbone creation, which compares to less than
1000 Bytes with the [15,33] based approach, roughly

7500 Bytes with the [34] based approach, and 36,000 Bytes
with the [35] based algorithm in [31]. It is very important
to keep in mind in these comparisons that [31] considers
interference ranges equal to transmission ranges, whereas
we consider interference ranges twice as large as the transmission ranges. To illustrate the impact of the larger interference range, we simulated the N ¼ 300 node network
with equal transmission and interference ranges
rt ¼ r i ¼ 30 m for which the cumulative overhead is
roughly cut in half to 3 475  525 Bytes per node for the
backbone creation. Further, with the rt ¼ ri setting, the stabilization times for the leader nodes and round ownerships
are slightly reduced, while the gateway nodes still require
around 215 frames to stabilize. It is also important to note
in the comparisons that the algorithms examined in [31]
assume knowledge of neighbor lists or number of neighbors, whose establishment requires signiﬁcant transmission overhead [54–59]. In contrast, our algorithms do not
require any prior knowledge. Since the energy consumed
for the overlay network construction is generally proportional to the exchanged control trafﬁc, the results in
Fig. 11a indicate that the energy consumed to set up the
overlay network increases close to linearly with the network size.
In Fig. 11b, we plot the average number of Bytes transmitted per node per frame during Phase III of the spanner
construction. As mentioned in the previous section, even as
we increase network size, the rate of Bytes sent per node is
relatively constant during this phase. The rate is actually
higher for sparser networks (N < 400) because a larger
percentage of the total nodes are being identiﬁed as
gateways.
In Fig. 11c, we plot the relative use of each slot during
spanner formation. We observe relatively small numbers
in most slots, since only a small fraction of nodes use these
slots. The exceptions are slots 5 and 7. Slot 5 is the third
slot of Phase II, and all inactive nodes within r i of an owner
node broadcast a busy message. As in all other plots, only
messages sent during ‘‘active” phases of the spanner construction are counted. If we counted Phase II maintenance
activity during the entire spanner formation time, slot 5
and not slot 7 would represent the largest fraction of communication overhead. Slot 7 is the ﬁrst slot of Phase III,
when potential gateway nodes contend for the attention
of active leader nodes. Unlike slot 5, only inactive nodes
covered by leaders in their active round transmit in this
slot, and fewer Bytes are transmitted during slot 7 each
round. However, the relatively long duration of Phase III allows this slot to contribute more toward the total overhead
in the long run. Slot 10 is used to transmit large gateway
list messages, but the contention mechanism of Phase III
effectively limits the number of nodes sending these in
each round.
In Table 2, we present the average number of transmitted, sensed, collided, and received messages per node, broken down by phases and slots. For each statistic, messages
are totaled over the time period each phase is active during
spanner construction, averaged across 20 runs, and divided
by the total number of nodes in the network. For this analysis, we deﬁne a collision to be an event in which a node
that could have received a message is only able to sense

1361

20

50

15

40

30

Bytes

kiloBytes

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

10

20

5
10
0

0

200

400

600

800

0

1000

0

200

400

600

800

1000

# Nodes

# Nodes

(a) Communication cost: Average number of Bytes sent (b) Communication cost: Average number of Bytes sent
per node during spanner creation
per node per frame during gateway discovery
10
100
200
300
400
500
600
700
800
900

kiloBytes

8

6

4

2

0
1

2

3

4

5

6

7

8

9

10

Slot Number

(c) Cost per slot: Average number of Bytes sent per slot
per node during spanner creation
Fig. 11. Aggregate simulation: backbone formation overhead for 100–900 nodes.

a busy carrier. We emphasize that these collisions do not
imply retransmissions: spanner formation merely depends
on the fact that sufﬁcient topology information is received
with high probability.
Also, in Table 2, we compare information reception that
is due to physical carrier sensing only versus fully receiving
a message by expressing both quantities in bits. While the
majority (over 75%) of messages sent are used for carrier
sensing, each of these messages conveys only one bit of
information. The larger gateway discovery messages in
Phase III comprise the majority (over 90%) of information
bits received.
Examining the numbers of messages transmitted for the
three phases in Table 2 for increasing number of nodes N,
we observe a decrease for Phase I, while there is an increase for Phase II and the totals for Phase III. The decreasing number of transmitted Phase I messages for increasing
node density is due to the probabilistic leader election
mechanism of [1], which scales back the number of messages transmitted per node, despite using the same p value

for all considered network densities. On the other hand,
the increasing numbers of Phases II and III messages for
increasing node density conform with the poly-logarithmic
complexities of these algorithms [1]. It is interesting to observe that the increase in the messages for Phase III is
mainly due to slot 1 of Phase III; the numbers of transmitted messages for slot 2 stay roughly constant, while the
numbers of transmitted slot 3 and 4 messages (which are
the longest messages exchanged by the algorithms slightly
decrease with increasing node density).
We further observe from the results for Phases I and II
in the table that these phases rely largely on sensed information. Recall from Section 4.3 that Phases I and II only require sensed information for forming the leader set and
the non-interfering transmission rounds, but through intact message reception can collect information in advance
for Phase III. We observe that Phase I contributes very little
toward the information collection for Phase III, while
Phase II makes substantial information collection
contributions.

1362

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Examining closer the transmitted and received messages in slots 3 and 4 of Phase III, we observe that the number of messages transmitted in slot 3 is equal to the
number of transmitted messages in slot 4, which in turn
is equal to the number of received messages in slot 4.
These results conﬁrm the expected behavior of the algorithms in that any node that sends a so-called ADV message in slot 3, sends a so-called GATEWAY message in
slot 4 [1]. Furthermore, a slot 4 GATEWAY message is sent
from a gateway node to its leader node, and is not vulnerable to collision. A possible reﬁnement over the algorithm
speciﬁed in [1] could be achieved by having other inactive
leader nodes ‘‘overhearing” the GATEWAY message transmission from a gateway node to its leader. However, additional simulations (not included in detail here due to space
constraints) indicated that this reﬁnement does not significantly reduce stabilization time.
Turning to the amount of control information exchanged through physical carrier sensing relative to the
information exchanged through intact message receptions
we make the following observations from Table 2. First, we
observe from the total numbers of messages sensed and
messages received that over 75% of the messages are
sensed (out of the total of sensed plus received messages).
Furthermore, we observe from the Phase III totals that the
majority (typically over 65%) of all bits received (from received messages) in Phase III are sent during slots that
are not vulnerable to collision. These results conﬁrm that
the output of Phase II’s carrier sensing operations, namely
a non-interfering schedule of leader transmission rounds,
helps to reduce overhead in Phase III by greatly reducing
the number of potential collisions. Furthermore, actual collisions are also reduced by the schedule of leader transmission rounds, as seen in slot 3 of Phase III.
4.5. Performance results for spanner resilience
To test the resilience of the spanner we conducted two
types of simulations: one evaluating the passive resilience
of the spanner, without any spanner maintenance; and another evaluating active resilience, measuring how well the
spanner maintenance function of the algorithm performed.
Throughout we continued to use the same simulation settings as before. We simulated 100–900 nodes with the
transmission and interference ranges r t ¼ 30 m and
ri ¼ 60 m (and r si ¼ 2r st ).

4.5.1. Passive resilience
To examine the passive resilience of the constructed
backbone network, we followed the approach of Basagni
et al. [31] and their ‘‘robustness” metric: we measured
the number of randomly selected nodes in the backbone
that could be removed while maintaining its two main
properties: (i) being a connected network, and (ii) being
a dominating set of the overall network. The backbone is
deﬁned to be connected when a path exists between every
pair of nodes in the backbone. The backbone is deﬁned to
be dominating when every node in the network is at most
one hop away from a backbone node. While this robustness metric allows us to make quantitative comparisons
with existing results, it is not directly applicable to our network model since we ﬁrst construct a set of leader nodes
that dominate the network, and then add gateway nodes
that connect the leaders but have no role in dominating
inactive nodes. Our gateway nodes would need to be ‘‘promoted” to leader status in cases where the local leader
node was disabled. We allow for this type of ‘‘easy repair”
of the spanner in our deﬁnition of ‘‘passive” resilience, so
that the robustness metric of [31] becomes applicable to
our model.
Table 3 ﬁrst gives the average total number of nodes in
our backbone, as well as the respective numbers of leader
and gateway nodes, prior to disabling any nodes. Throughout, we report 95% conﬁdence intervals based on 75 replications. Next, the table gives the average total number of
randomly selected backbone nodes that could be disabled
while the remaining backbone nodes still formed a functional backbone; the split of the disabled nodes into leader
and gateway nodes is also provided. Table 3 furthermore
gives the average number of ‘‘easy repairs”, i.e., promotions
of gateway nodes to leader nodes, required to maintain the
functionality of the backbone after the disabling of backbone nodes. Comparing the numbers for overall backbone
size and disabled nodes, we observe that for all network
sizes a large fraction of the nodes in the backbone can be
lost while maintaining the connected and dominating
properties. However, unlike the original spanner, this
‘‘damaged” backbone would not be guaranteed to have a
stretch factor of 5, and would require active maintenance
to regain full capability. Easy repairs alone only handle
the loss of the dominating property, and cannot guarantee
these other properties of the spanner.

Table 3
Backbone robustness: the number of nodes that can be disabled in a backbone network without compromising its connectedness or dominating property, for
various network sizes. Easy repairs represent instances when a gateway node was promoted to act as a dominating node.
N

100

200

300

400

500

600

700

800

900

Nodes in bb.
# leaders
# gateways

92  1:7
41  1:1
51  1:5

173  3:5
50  0:93
123  3:1

240  3:0
54  1:0
186  2:8

285  5:0
55  0:99
230  3:9

335  8:5
57  1:1
278  7:8

359  12
58  1:4
301  11

392  9:0
59  1:3
333  8:4

422  12
60  1:4
362  11

437  15
61  1:7
376  14

Avg. disabl.
# leaders
# gateways
Easy repairs

8  1:7
3:9  0:88
4:3  0:93
0.283

59  5:8
18  1:7
41  4:2
3.507

114  6:4
27  1:6
87  5:0
9.867

169  8:3
32  1:8
137  6:77
16.32

213  7:8
37  1:5
176  6:5
21.11

253  7:2
40  1:2
213  6:4
24.83

267  9:9
38  1:7
228  8:4
26.99

250  15
39  2:3
211  13
26.95

300  9:3
42  1:3
257  8:3
33.67

% of fail.
Not conn. (%)
Not domin. (%)

93.3
6.7

94.7
5.3

80.0
20.0

77.3
22.7

62.7
37.3

68.0
32.0

60.0
40.0

41.3
58.7

45.3
54.7

1363

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Compared to the other protocols with a high degree of
localization evaluated in [31], our algorithm produces a
large backbone: approximately 80% of the nodes in our
N ¼ 300 node network belong to the backbone, while 20–
25% of the nodes belong to the backbone with the protocols
of [15,33–35]. However, our spanners are also signiﬁcantly
more robust: almost half of the nodes in our backbone can
be lost while maintaining spanner properties. On the other
hand, the spanners produced by the algorithms in [15,33–
35] cannot generally survive the loss of more than 8
backbone nodes in the N ¼ 300 node network. Importantly,
as we will see in Section 4.6, our broadcast and convergecast algorithms efﬁciently use only a fraction of the
gateway nodes, effectively reducing the size of the active
backbone.
From Table 3, we also ﬁnd that the relative number of
failures due to loss of connectivity versus loss of dominating set property decreases for larger network sizes. As
pointed out in Section 4.4.2, our spanner does not force
two leaders to use the same set of gateways for communication between the two in both directions. In a denser network, there is greater possibility of a leader node
uncovering a few inactive nodes when it gets disabled,
while the backbone has a higher chance of remaining connected through the higher number of gateways.
4.5.2. Active resilience
We present in this subsection a brief preliminary evaluation of the active resilience of our spanner, a more comprehensive active resilience evaluation is left for future
work. We disable a number of the nodes in the backbone
and measure the time it takes for the spanner to re-stabilize. Compared to our passive resilience scenario, this is a
more demanding condition than merely requiring the
spanner to be connected and a dominating set after removing nodes. Here, we are measuring the time until the spanner is fully repaired with all the possible gateway
connections that contribute to the low stretch factor property of the backbone. We expect this process to take some

50

4.6. Broadcast
To evaluate the broadcast mechanism, we sequentially
simulate the broadcasting of single-packet messages from
one source node to all other nodes in the network. We conduct 90 independent replications and present 95% conﬁdence intervals. For each replication, the source node is
chosen at random from among all network nodes, includ-

n=1
n=5
n=10

40

200

150

Bytes Tx

Time in Frames

time, even when we remove fewer nodes than the previous
section’s results seem to permit.
For this evaluation, we focus on the removal of backbone nodes. In our spanner formation, broadcast, and convergecast algorithms the leaders do not maintain cluster
membership lists. Therefore, regular nodes may join, leave,
or move within the coverage area of different leader nodes
without incurring any repair costs to the backbone.
Fig. 12a shows the average time (95% conﬁdence intervals based on 15 replications) it takes the spanner to restabilize is primarily dependent on the number of removed
nodes n and not the total size of the network N. These results demonstrate that the spanner repair is conducted
mainly locally, involving only the immediate neighborhood of the removed nodes and not the entire network.
Fig. 12 shows the average per-node overhead during spanner maintenance (95% conﬁdence interval based on 45 replications), which is independent of the number of disabled
nodes. We see that the overhead for active repair is roughly
equal to the peak spanner construction overhead of
200 Bytes per frame per node. This is consistent with the
earlier results since we are now counting messages for
all three phases of the spanner formation algorithm as they
run in parallel to repair the backbone. We note that by
extending the data period in the round (see Fig. 1) whereby
a frame contains k rounds, i.e., k data periods, the relative
overhead for spanner maintenance can be reduced, at the
expense of reduced maintenance responsiveness. A detailed examination of these trade-offs of active resilience
is left for future work.

30

20

50

10

0

100

0
0

200

400

600

# Nodes
(a) Average time for the spanner to re-stabilize

0

200

400

600

# Nodes
(b) Average number of bytes transmitted per frame per
node during spanner repair, which is independent of the
number of disabled nodes

Fig. 12. Backbone robustness: time and Bytes spent while the spanner re-stabilizes after n ¼ 1; 5; 10 nodes have been disabled.

1364

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

ing leader, gateway, and inactive nodes. As before, we simulated 100–900 nodes with the transmission and interference ranges rt ¼ 30 m and r i ¼ 60 m (and rsi ¼ 2r st ).
A broadcasting frame contains k four-slot rounds, using
the non-interfering round assignments from Phase II. The
ﬁrst slot is reserved for leaders to forward the broadcast
message. The second and third slots are used by gateway
nodes to probabilistically initiate an RTS-CTS exchange.
Gateway nodes that have received a broadcast data message send an RTS with probability p. The fourth slot is reserved for gateways to forward the message after a
successful CTS. We proved in [2] that selecting p ¼ 1=2d
guarantees bounded time to deliver the message to all
nodes, where d is the number of leaders within the interference range r i of any node. Since we estimated in Section
4.1 a maximum possible value of d ¼ 60, we conservatively
set p ¼ 0:01 for these simulations.
If we consider the maximum size for a broadcast message of 1500 Bytes, and that the RTS and CTS frames are
20 and 14 Bytes, respectively (as in 802.11), then the dura-

tion of a broadcast frame can be estimated as in Section
4.3. Based on k ¼ 60 rounds/frame, a 1 Mbit/s transmission
rate and a 20 ls spacing between slots, a broadcast frame
is approximately 1.46 s long.
In Fig. 13a, we see that the time it takes for the message
to reach all nodes is relatively constant for dense networks
and is somewhat higher for sparse networks. While our
small value of p results in lower performance for sparse networks, it does establish a density-independent upper
bound on broadcast latency. Using our estimate of 1
frame = 1.46 s, the broadcast message reaches all nodes in
our N P 300 node networks in less than 8.8 s. Our results
are comparable with those in [13] in that both algorithms
achieve constant broadcast latency for ﬁxed-area networks,
and they therefore approximate the minimum latency.
Figs. 13b and c show the average number of times the
data message or control messages (RTS and CTS) were
transmitted, respectively. We note that the overhead levels
off for large networks, which corresponds to a decreasing
overhead/node ratio for denser networks.

20

250

200

Total Data Msg

Time in Frames

15

10

5

150

100

50

0

0

200

400

600

800

0

1000

0

200

400

600

800

1000

# Nodes

# Nodes

(a) Average length of time (in frames) to complete (b) Average number of times the data message was
broadcast operation. Using our estimate of 1 frame = transmitted during each broadcast operation
1.46 s, in the networks with
nodes, all nodes receive the broadcast message in less than 8.8 s.

gateway
leader

1250
1000

Total Data Msg

Total Ctrl Msg

150

750
500

100

50

250
0

0

200

400

600

800

1000

0

0

200

400

600

800

1000

# Nodes

# Nodes

(c) Average number of control messages (RTS and CTS) (d) Average number of times the data message was
transmitted during each broadcast operation
transmitted by leader and gateway nodes during each
broadcast operation
Fig. 13. Broadcasting overhead for N = 100–900 nodes.

1365

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Finally, Fig. 13d shows the average number of times the
broadcast message was transmitted by leader nodes and
gateway nodes during broadcast operations. The broadcast
protocol allows leader nodes to transmit exactly once, but
a given gateway node may retransmit the same broadcast
packet multiple times. Thus, the number of data packet
transmissions plotted in Fig. 13d gives the number of leader nodes and an upper bound on the number of gateway
nodes involved in the broadcast. More speciﬁcally, Table 4
gives the average number of backbone nodes involved in a
given broadcast. Here we see an important property of the
broadcast mechanism. Even though the N ¼ 900 node networks have on average 437 backbone nodes, on average
only 188 of these nodes are used during a broadcast. This
effectively mitigates the negative effects of selecting a
large number of gateway nodes, because only a fraction
are active in broadcasting.
Comparisons of our physical carrier sensing based
broadcast mechanism with existing broadcast approaches
are complex, mainly because the existing approaches do
not consider interference ranges larger than transmission
ranges and also do not conduct the broadcast over a selfstabilized (active resilience) backbone. To give some indications of the performance of existing algorithms, we note
that the most efﬁcient of the broadcast protocols studied in
[15] also uses a fraction of the nodes in a network that decreases with increasing network density. Their ‘‘gateway”
protocol uses only 45% of all network nodes for broadcasting in a network with degree ¼ 10. In our N ¼ 200 node
network, which is a similar degree, we use 52% of all nodes
to achieve network-wide broadcast. In [8], several broadcast algorithms using varying amounts of topology and
location information are evaluated through simulation.
The densest network evaluated has a degree of 21.2
(roughly the density of our N ¼ 400 node network) and between 18 and 35 of 110 nodes retransmit the broadcast
message for the various protocols, compared to 36% of
the nodes in our N ¼ 400 network.
We also note that the transmissions by leader nodes in
our backbone, which account for close to half of the transmissions for broadcasts in small networks and about one
quarter of the transmissions in large networks, are not subject to collisions due to the non-interfering round assignments from Phase II. More speciﬁcally, in slot 1, leaders
relay the data message in their reserved active round,
which means that all neighboring nodes can receive it
without interference. In slot 2, gateway nodes with the
broadcast packet send an RTS message with probability p,
which may result in a collision. In slot 3, a leader or gateway that successfully receives an RTS sends a CTS signal,
which the originating gateway receives through physical
carrier sensing, and is thus not vulnerable to collision. In
slot 4, only gateways that sense a CTS signal relay the

broadcast message, again without collision. We observe
from Table 4 that as a result of primarily our spanner infrastructure, less than 5% of the Bytes transmitted during
broadcast are vulnerable to collisions.
4.7. Convergecast
We deﬁne convergecast to be a many-to-one operation
in which each of m source nodes has a packet to send to a
sink node. In our evaluation of the convergecast algorithm,
for each replication, m source nodes and the sink are chosen uniformly at random from among all network nodes,
including leader, gateway, and inactive nodes. We conduct
15 replications each for six random networks, for a total of
90 replications and report 95% conﬁdence intervals.
Throughout, we continue to use the same simulation settings as before, that is, we simulated 100–900 nodes with
the transmission and interference ranges r t ¼ 30 m and
ri ¼ 60 m (and r si ¼ 2r st ).
Convergecast proceeds in two phases. The ﬁrst phase is
a tree-building operation, which is basically the broadcast
operation with a ROUTE message instead of a broadcast
data message. Tree-building frames similarly contain k
rounds, using the non-interfering round assignments from
Phase II. The difference is that both leaders and gateways
participate in the probabilistic sending of RTS messages
whenever they receive a ROUTE message reporting a shorter path to the sink. We therefore need only three slots for
tree building: one for sending an RTS with probability p,
one for responding with a CTS signal, and one for sending
ROUTE messages.
The second phase is gathering using the completed tree.
Convergecasting uses its own four-slot round, again making use of the non-interfering round assignments. The ﬁrst
slot is reserved for leaders to forward the convergecast
message. The second and third slots are used by gateway
and inactive nodes to probabilistically initiate an RTS-CTS
exchange. Non-leader nodes that have a convergecast data
message in their queue send an RTS with probability p. The
fourth slot is reserved for these non-leader nodes to forward a messages after a successful CTS. We proved in [2]
that selecting p ¼ 1=d guarantees bounded time to deliver
all messages to the sink. For contrast with our broadcast
results, we aggressively set p ¼ 0:05 for these simulations.
Unlike spanner construction, these two phases cannot
run in parallel, since we assume the tree is complete before
gathering begins. One possible distributed method to transition between these phases would be for source nodes to
start a timer upon receiving a ROUTE message during tree
building. With a timer duration set using estimates of tree
building latency (see Fig. 14), the source node could be reasonably certain the tree was complete before gathering begins. It would also be possible to build a tree for a given

Table 4
Average number of nodes involved in a broadcast and average number of kBytes transmitted during a broadcast operation of a 1500 Bytes packet.
N

100

200

300

400

500

600

700

800

900

# of broadcast nodes
kBytes tx not vuln. to coll.
kBytes tx vuln. to coll.

71  1:1
155  9:3
6:1  0:70

106  1:5
205  12
7:6  0:92

129  2:8
239  9:5
8:3  0:80

143  3:2
264  9:1
8:1  0:62

162  3:7
301  12
9:6  0:85

166  3:7
313  11
9:9  0:90

175  3:3
320  8:5
9:3  0:46

183  4:2
337  11
10:5  0:78

188  4:6
344  11
10:2  0:57

1366

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

Fig. 14. Convergecasting overhead for N = 100–900 nodes.

sink and re-use it for several iterations of gathering, as long
as there are no signiﬁcant changes in topology.

To obtain an estimate of convergecast latency, we assume that the ROUTE message is a simple 20 Bytes control

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

packet (it only needs to contain the address of the sender
and an advertised number of hops to the sink), and that
the size of the RTS, CTS, and data packets are 20, 14, and
1500 Bytes, respectively, as in broadcasting. Using similar
assumptions, a tree building frame is 29.52 ms, and a convergecast frame is approximately 1.46 s.
In Fig. 14a, we plot the time it takes to build a convergecasting tree for each network size. Using our estimate of 1
frame = 29.5 ms, it takes less than 177 ms to build a tree
for a given sink. For the N P 200 node networks, the estimated delay is less than 90 ms. In Fig. 14b, we plot the time
it takes to gather messages from m ¼ 5 and m ¼ 10 source
nodes over an established tree. The total delay for a convergecast is obtained by adding the tree building delay
from Fig. 14a and convergecast delay from Fig. 14b. As
indicated in our theoretical results [2], the time to complete convergecast depends both on the number of messages and the number of nodes.
In Figs. 14c and d, we compare the use of data messages
and control messages, counting RTS, CTS, and ROUTE messages separately for the tree building and gathering phases.
While the number of data message forwards appears to be
relatively constant for each value of m, the number of tree
building control messages increases roughly linearly with
N and is nearly the same for both values of m. This is because tree building involves all nodes in the network and
does not depend on m. On the other hand, the control message overhead for convergecasting is largely independent
of the number of nodes N. Furthermore, the number of
times we forward the larger data message toward the sink
is limited—it depends mostly on the distance from source
to sink [2].
We note that the overhead (time and data messages
sent) to gather messages from m ¼ 10 sources is signiﬁcantly less than double the time to gather messages from
m ¼ 5 sources. This is very likely due to the fact that we
are sampling the source–destination distance non-uniformly by choosing a new sink node and m new source
nodes uniformly at random for each convergecast
operation.
The data forwarding operations are further broken
down in Figs. 14e and f, which show the types of nodes
responsible for forwarding convergecast messages. We
note that since the gathering tree is built using only the
backbone nodes, inactive nodes will always have a backbone node as a parent in the tree. This effectively limits
the number of data transmissions by inactive nodes to only
the ﬁrst hop of convergecasting. We also note that each
node involved in the convergecast transmits a given data
message only once.

5. Conclusion
We have evaluated the actual performance of recently
proposed physical carrier sensing based backbone
(spanner) construction algorithms [1] and broadcast/convergecast algorithms [2] for ad hoc networks through simulations for typical network scenarios; only asymptotic
performance bounds existed previously. We have found
that through judiciously adjusting the carrier sense thresh-

1367

old used in physical carrier sensing, our algorithms are able
to accommodate interference ranges larger than transmission ranges, packet collisions at the MAC layer, and nodes
without any prior knowledge of neighbors while achieving
good network layer performance. Our algorithms achieve
this performance by exchanging typically over 75% of the
messages required for the overlay construction through
physical carrier sensing. Physical carrier sensing is extensively exploited to quickly (in about 2.4 s in a typical 300
node network) ﬁnd leader nodes that can reach all nodes
with non-interfering transmission rounds. The exchange
of detailed node address information for linking the leader
nodes into a backbone network is the most demanding
part of the backbone construction (requiring close to 47 s
in a typical 300 node network) but is signiﬁcantly facilitated by the non-interfering transmission rounds; less than
35% of the information bits for backbone formation are
transmitted in packets that are vulnerable to collisions.
While the constructed backbones are relatively dense,
e.g., the backbone contains up to 80% of the nodes in the
300 node network and close to 50% of the nodes in the
900 node network, the backbones are very robust, remaining functional when roughly half of the backbone nodes in
the 300 node network and close to two thirds of the backbone nodes in the 900 node network fail. For the 900 node
network, our broadcast mechanism uses less than about
half of the backbone nodes, i.e., less than about one quarter
of the network nodes, to deliver a 1500 Bytes broadcast
message with 1 Mb/s transmission rate within approximately 8.8 s to all network nodes. Our convergecast mechanisms deliver data messages from 10 source nodes
towards a common sink by transmitting the data messages
through less than 10% of the nodes in a network with 500
or more nodes.
Directions for future work include evaluating the active
resilience of the spanner in the context of mobile ad hoc
networks as well as developing routing and route maintenance algorithms exploiting the physical carrier sensing
with adjustment of the carrier sensing threshold.
Acknowledgement
Supported in part by the National Science Foundation
through Grant Career ANI-0133252.
References
[1] K. Kothapalli, C. Scheideler, M. Onus, A. Richa, Constant density
spanners for wireless ad hoc networks, in: Proceedings of ACM
Symposium on Parallelism in Algorithms and Architectures (SPAA),
Las Vegas, NV, July 2005, pp. 116–125.
[2] M. Onus, A. Richa, K. Kothapalli, C. Scheideler, Efﬁcient broadcasting
and gathering in wireless ad hoc networks, in: Proceedings of
International Symposium on Parallel Architectures, Algorithms and
Networks (I-SPAN), Las Vegas, NV, December 2005, pp. 346–351.
[3] A. Chiganmi, M. Baysan, K. Sarac, R. Prakash, Variable power
broadcast using local information in ad hoc networks, Ad Hoc
Networks 6 (5) (2008) 675–695. Jul..
[4] A. Keshavarz-Haddad, V. Ribeiro, R. Riedi, Color-based broadcasting
for ad hoc networks, in: Proceedings of IEEE International
Symposium on Modeling and Optimization in Mobile, Ad Hoc and
Wireless Networks, April 2006, pp. 1–10.
[5] T.J. Kwon, M. Gerla, V.K. Varma, M. Barton, T.R. Hsing, Efﬁcient
ﬂooding with passive clustering—an overhead-free selective forward

1368

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369

mechanism for ad hoc/sensor networks, Proceedings of the IEEE 91
(8) (2003) 1210–1220.
W. Lou, J. Wu, Toward broadcast reliability in mobile ad hoc
networks with double coverage, IEEE Transactions on Mobile
Computing 6 (2) (2007) 148–163.
W.-Z. Song, X.-Y. Li, Y. Wang, O. Frieder, W. Wang, Localized topology
control for unicast and broadcast in wireless ad hoc networks, IEEE
Transactions on Parallel and Distributed Systems 17 (4) (2006) 321–
334.
B. Williams, T. Camp, Comparison of broadcasting techniques for
mobile ad hoc networks, in: Proceedings of ACM MOBIHOC,
Lausanne, Switzerland, June 2002, pp. 194–205.
Y. Yi, M. Gerla, T.J. Kwon, Efﬁcient ﬂooding in ad hoc networks: a
comparative performance study, in: Proceedings of IEEE
International Conference on Communications (ICC), May 2003, pp.
1059–1063.
I. Chlamtac, O. Weinstein, The wave expansion approach to
broadcasting in multihop radio networks, IEEE Transactions on
Communication 39 (3) (1991) 426–433.
R. Bar-Yehuda, O. Goldreich, A. Itai, On the time-complexity of
broadcast in multi-hop radio networks: an exponential gap between
determinism and randomization, Journal of Computer and System
Sciences 45 (1) (1992) 104–126.
Y.-C. Tseng, S.-Y. Ni, Y.-S. Chen, J.-P. Sheu, The broadcast storm
problem in a mobile ad hoc network, Wireless Network 8 (2/3)
(2002) 153–167.
R. Gandhi, S. Parthasarathy, A. Mishra, Minimizing broadcast latency
and redundancy in ad hoc networks, in: Proceedings of the Fourth
ACM International Symposium on Mobile Ad Hoc Networking and
Computing, 2003, pp. 222–232.
H. Zhang, Z.-P. Jiang, Modeling and performance analysis of ad hoc
broadcasting schemes, Performance Evaluation 63 (12) (2006)
1196–1215.
I. Stojmenovic, M. Seddigh, J. Zunic, Dominating sets and neighbors
elimination-based broadcasting algorithms in wireless networks,
IEEE Transactions on Parallel and Distributed Systems 13 (1) (2002)
14–25.
J. Wu, F. Dai, Efﬁcient broadcasting with guaranteed coverage in
mobile ad hoc networks, IEEE Transactions on Mobile Computing 4
(3) (2005) 259–270.
G. Jakllari, S. Krishnamurthy, M. Faloutsos, P. Krishnamurthy, O.
Ercetin, A cross-layer framework for exploiting virtual MISO links in
mobile ad hoc networks, IEEE Transactions on Mobile Computing 6
(6) (2007) 579–594.
G. Jakllari, S. Krishnamurthy, M. Faloutsos, P. Krishnamurthy, On
broadcasting with cooperative diversity in multi-hop wireless
networks, IEEE Journal on Selected Areas in Communications 25
(2) (2007) 484–496.
E.J. Duarte-Melo, M. Liu, Data-gathering wireless sensor networks:
organization and capacity, Computer Networks 43 (4) (2003) 519–
537.
C. Intanagonwiwat, R. Govindan, D. Estrin, J. Heidemann, F. Silva,
Directed diffusion for wireless sensor networking, IEEE/ACM
Transactions on Networking 11 (1) (2003) 2–16.
A. Kesselman, D.R. Kowalski, Fast distributed algorithm for
convergecast in ad hoc geometric radio networks, Journal of
Parallel and Distributed Computing 66 (4) (2006) 578–585.
Y. Yu, B. Krishnamachari, V. Prasanna, Energy-latency tradeoffs for
data gathering in wireless sensor networks, in: Proceedings of IEEE
INFOCOM, vol. 1, March 2004, pp. 244–255.
S. Upadhyayula, V. Annamalai, S. Gupta, A low-latency and energyefﬁcient algorithm for convergecast in wireless sensor networks, in:
Proceedings of IEEE GLOBECOM, December 2003, pp. 3525–3530.
Y. Zhang, Q. Huang, Coordinated convergecast in wireless sensor
networks, in: Proceedings of IEEE Military Communications
Conference (MILCOM), October 2005, pp. 1152–1158.
K. Alzoubi, X.-Y. Li, Y. Wang, P.-J. Wan, O. Frieder, Geometric
spanners for wireless ad hoc network, IEEE Transactions on Parallel
and Distributed Systems 14 (4) (2003) 408–421.
X. Cheng, D. Huang, W. Li, W. Wu, D.Z. Du, A polynomial-time
approximation scheme for the minimum-connected dominating set
in ad hoc wireless networks, Networks: An International Journal 42
(4) (2003) 202–208.
J. Gao, L. Guibas, J. Hershberger, L. Zhang, A. Zhu, Geometric spanners
for routing in mobile networks, IEEE Journal on Selected Areas in
Communications 23 (1) (2005) 174–185.

[28] Y. Wang, W. Wang, X.-Y. Li, Efﬁcient distributed low-cost backbone
formation for wireless networks, IEEE Transactions on Parallel and
Distributed Systems 17 (7) (2006) 681–693.
[29] J. Wu, F. Dai, Virtual backbone construction in MANETs using
adjustable transmission ranges, IEEE Transactions on Mobile
Computing 15 (9) (2006) 1188–1200.
[30] M. Luby, A simple parallel algorithm for the maximal independent
set problem, SIAM Journal on Computing 15 (4) (1986) 1036–
1055.
[31] S. Basagni, M. Mastrogiovanni, A. Panconesi, C. Petrioli, Localized
protocols for ad hoc clustering and backbone formation: a
performance comparison, IEEE Transactions on Parallel and
Distributed Systems 17 (4) (2006) 292–306.
[32] H. Liu, X. Jia, P.-J. Wan, X. Liu, F. Yao, A distributed and efﬁcient
ﬂooding scheme using 1-hop information in mobile ad hoc
networks, IEEE Transactions on Parallel and Distributed Systems
18 (5) (2007) 658–671.
[33] F. Dai, J. Wu, An extended localized algorithm for connected
dominating set formation in ad hoc wireless networks, IEEE
Transactions on Parallel and Distributed Systems 15 (10) (2004)
908–920.
[34] S. Basagni, Distributed clustering for ad hoc networks, in:
Proceedings of International Symposium on Parallel Architectures,
Algorithms, and Networks (I-SPAN), Perth, Australia, June 1999, pp.
310–315.
[35] P.-J. Wan, K. Alzoubi, O. Frieder, Distributed construction of
connected dominating sets in wireless ad hoc networks, ACM
Mobile Networks and Applications 9 (2) (2004) 141–149.
[36] F. Kuhn, T. Moscibroda, and R. Wattenhofer, Radio network
clustering from scratch, in: Proceedings of European Symposium
on Algorithms (ESA), 2004.
[37] S. Parthasarathy, R. Gandhi, Distributed algorithms for coloring and
domination in wireless ad hoc networks, in: Proceedings of
Foundations of Software Technology and Theoretical Computer
Science, 2004, pp. 447–459.
[38] R. Bruno, C. Chaudet, M. Conti, E. Gregori, A novel fair medium access
control for 802.11-based multi-hop ad hoc networks, in:
Proceedings of the 14th IEEE Workshop on Local and Metropolitan
Area Networks (LANMAN), September 2005, pp. 1–6.
[39] J. Deng, B. Liang, P.K. Varshney, Tuning the Carrier Sensing Range of
IEEE
802.11
MAC,
in:
Proceedings
of
IEEE
Global
Telecommunications Conference (Globecom), November/December
2004, pp. 2987–2991.
[40] T.-S. Kim, H. Lim, J.C. Hou, Improving spatial reuse through tuning
transmit power, carrier sense threshold, and data rate in multihop
wireless networks, in: Proceedings of ACM MobiCom, September
2006, pp. 366–377.
[41] H. Ma, H. Alazemi, S. Roy, A stochastic model for optimizing physical
carrier sensing and spatial reuse in ad hoc networks, in: Proceedings
of IEEE International Conference on Mobile Ad hoc and Sensor
Systems, November 2005, pp. 615–622.
[42] K. Sanzgiri, I.D. Chakeres, E.M. Belding-Royer, Determining intraﬂow contention along multihop paths in wireless networks, in:
Proceedings of First International Conference on Broadband
Networks, October 2004, pp. 611–620.
[43] E. Wong, R. Cruz, A spatio-temporal model for physical carrier
sensing wireless ad hoc networks, in: Proceedings of IEEE Sensor and
Ad Hoc Communications and Networks (SECON), September 2006,
pp. 276–285.
[44] X. Yang, N. Vaidya, On physical carrier sensing in wireless ad hoc
networks, in: Proceedings of IEEE INFOCOM, Miami, FL, March 2005,
pp. 2525–2535.
[45] Y. Yang, J.C. Hou, L.-C. Kung, Modeling of physical carrier sense in
multi-hop wireless networks and its use in joint power control and
carrier sense adjustment, in: Proceedings of IEEE Infocom, May 2007,
pp. 2331–2335.
[46] H. Zhai, Y. Fang, Physical carrier sensing and spatial reuse in
multirate and multihop wireless ad hoc networks, in: Proceedings of
IEEE Infocom, April 2006, pp. 1–12.
[47] J. Zhu, X. Guo, L.L. Yang, W.S. Conner, Leveraging spatial reuse in
802.11 mesh networks with enhanced physical carrier sensing, in:
Proceedings
of
the
IEEE
International
Conference
on
Communications, June 2004, pp. 4004–4011.
[48] R. Ramanathan, R. Rosales-Hain, Topology control of multihop
wireless networks using transmit power adjustment, in:
Proceedings of IEEE INFOCOM, 2000, pp. 404–413.

L. Ritchie et al. / Ad Hoc Networks 7 (2009) 1347–1369
[49] C.-C. Shen, C. Srisathapornphat, R. Liu, Z. Huang, C. Jaikaeo, E.L. Lloyd,
CLTC: a cluster-based topology control for ad hoc networks, IEEE
Transactions on Mobile Computing 3 (1) (2004) 18–32.
[50] A. Muqattash, M. Krunz, A distributed transmission power control
protocol for mobile ad hoc networks, IEEE Transactions on Mobile
Computing 3 (2) (2004) 113–128.
[51] A. Muqattash, M. Krunz, POWMAC: a single-channel power-control
protocol for throughput enhancement in wireless ad hoc networks,
IEEE Journal on Selected Areas in Communications 23 (5) (2005)
1067–1084.
[52] B. Tavli, W. Heinzelman, Energy and spatial reuse efﬁcient
network-wide real-time data broadcasting in mobile ad hoc
networks, IEEE Transactions on Mobile Computing 5 (10) (2006)
1297–1312.
[53] B.D. Lubachevsky, R.L. Graham, Curved hexagonal packings of equal
disks in a circle, Discrete and Computational Geometry 18 (2) (1997)
179–194.
[54] G. Alonso, E. Kranakis, R. Wattenhofer, P. Widmayer, Probabilistic
protocols for node discovery in ad hoc, single broadcast channel
networks, in: Proceedings of IEEE International Parallel and
Distributed Processing Symposium, April 2003, pp. 1–8.
[55] L. Galluccio, G. Morabito, S. Palazzo, Analytical evaluation of a
tradeoff between energy efﬁciency and responsiveness of neighbor
discovery in self-organizing ad hoc networks, IEEE Journal on
Selected Areas in Communications 22 (7) (2004) 1167–1182.
[56] V. Giruka, M. Singhal, Hello protocols for ad hoc networks: overhead
and accuracy tradeoffs, in: Proceedings of IEEE International Symposium on a World of Wireless Mobile and
Multimedia Networks (WoWMoM), Taormina, Italy, June 2005, pp.
354–361.
[57] E.B. Hamida, G. Chelius, E. Fleury, Revisiting neighbor discovery with
interferences consideration, in: Proceedings of ACM International
Workshop on Performance Evaluation of Wireless Ad Hoc, Sensor
and Ubiquitous Networks, Terromolinos, Spain, October 2006, pp.
74–81.
[58] M.J. McGlynn and S.A. Borbash, Birthday protocols for low energy
deployment and ﬂexible neighbor discovery in ad hoc wireless
networks, in: Proceedings of ACM MobiHoc, Long Beach, CA, 2001,
pp. 137–145.
[59] T. Salonidis, P. Bhagwat, L. Tassiulas, R. LaMaire, Distributed
topology construction of Bluetooth wireless personal area
networks, IEEE Journal on Selected Areas in Communications 23
(3) (2005) 633–643.
[60] I. Cidon, O. Mokryn, Propagation and leader election in a multihop
broadcast environment, in: Proceedings of 12th International
Symposium on Distributed Computing, 1998, pp. 104–118.

Luke Ritchie received the BS and PhD
degrees in electrical engineering from Arizona State University (ASU), Tempe, in 2003
and 2007, respectively. He is currently a
network R&D engineer at DataSoft Corp.,
Scottsdale, AZ. His research interests are in
the areas of interaction between medium
access control (MAC) and routing in mobile
ad hoc networks (MANETs) and wireless
sensor networks (WSNs). He is a student
member of the IEEE.

1369

Martin Reisslein is an Associate Professor in
the Department of Electrical Engineering at
Arizona State University (ASU), Tempe. He
received the Dipl.-Ing. (FH) degree from the
Fachhochschule Dieburg, Germany, in 1994,
and the M.S.E. degree from the University of
Pennsylvania, Philadelphia, in 1996. Both in
electrical engineering. He received his Ph.D. in
systems engineering from the University of
Pennsylvania in 1998. During the academic
year 1994-1995 he visited the University of
Pennsylvania as a Fulbright scholar. From July
1998 through October 2000 he was a scientist with the German National
Research Center for Information Technology (GMD FOKUS), Berlin and
lecturer at the Technical University Berlin. From October 2000 through
August 2005 he was an Assistant Professor at ASU. He served as editor-inchief of the IEEE Communications Surveys and Tutorials from January
2003 through February 2007 and has served on the Technical Program
Committees of IEEE Infocom, IEEE Globecom, and the IEEE International
Symposium on Computer and Communications.
He has organized sessions at the IEEE Computer Communications
Workshop (CCW). He maintains an extensive library of video traces for
network performance evaluation, including frame size traces of MPEG-4
and H.264 encoded video, at http://trace.eas.asu.edu. His research interests are in the areas of Internet Quality of Service, video trafﬁc characterization, wireless networking, optical networking, and engineering
education.

Andrea W. Richa is an Associate Professor at
the Department of Computer Science and
Engineering at Arizona State University,
Tempe, since August 2004. She joined this
department as an Assistant Professor in
August 1998. Prof. Richa received her M.S. and
Ph.D. degrees from the School of Computer
Science at Carnegie Mellon University, in 1995
and 1998, respectively. She also earned an
M.S. degree in Computer Systems from the
Graduate School in Engineering (COPPE), and
a B.S. degree in Computer Science, both at the
Federal University of Rio de Janeiro, Brazil, in 1992 and 1990, respectively.
Prof. Richa’s main area of research is in network algorithms. Some of the
topics Dr. Richa has worked on include packet scheduling, distributed
load balancing, packet routing, mobile network clustering and routing
protocols, and distributed data tracking. Prof. Richa’s data tracking (or
name lookup) algorithm has been widely recognized as the ﬁrst benchmark algorithm for the development of distributed databases in peer-topeer networking, having being references by over 130 academic journal
or conference publications to date, and being implemented as part of two
of the current leading projects in peer-to-peer networking. Dr. Richa’s
was the recipient of an NSF CAREER Award in 1999. For a selected list of
her publications, CV, and current research projects, please visit http://
www.public.asu.edu/ aricha.

Broadcasting in Unreliable Radio Networks
Fabian Kuhn

Nancy Lynch

Calvin Newport

Faculty of Informatics,
University of Lugano

Computer Science and
Artificial Intelligence Lab, MIT

Computer Science and
Artificial Intelligence Lab, MIT

fabian.kuhn@usi.ch
lynch@csail.mit.edu
cnewport@csail.mit.edu
Rotem Oshman
Andrea Richa
Computer Science and
Artificial Intelligence Lab, MIT

Computer Science and
Engineering, SCIDSE, Arizona
State University

rotem@csail.mit.edu

aricha@asu.edu

ABSTRACT

1.

Practitioners agree that unreliable links, which sometimes deliver
messages and sometime do not, are an important characteristic of
wireless networks. In contrast, most theoretical models of radio
networks fix a static set of links and assume that these links work
reliably throughout an execution. This gap between theory and
practice motivates us to investigate how unreliable links affect theoretical bounds on broadcast in radio networks.
To that end we consider a model that includes two types of links:
reliable links, which always deliver messages, and unreliable links,
which sometimes fail to deliver messages. We assume that the reliable links induce a connected graph, and that unreliable links are
controlled by a worst-case adversary. In the new model we show an
Ω(𝑛 log 𝑛) lower bound on deterministic broadcast in undirected
graphs, even when all processes are initially awake and have collision detection, and an Ω(𝑛) lower bound on randomized broadcast
in undirected networks of constant diameter. This separates the new
model from the classical, reliable model. On the positive√side, we
give two algorithms that tolerate unreliability: an 𝑂(𝑛3/2 log 𝑛)time deterministic algorithm and a randomized algorithm which
terminates in 𝑂(𝑛 log2 𝑛) rounds with high probability.

A fundamental feature of radio networks is the presence of unreliable links, which sometimes deliver packets and sometimes do
not. Unreliable links can be caused by radio communication gray
zones [24], multipath propagation, and interference from unrelated
networks or electromagnetic devices. As the authors note in [26],
something as simple as opening a door can change the connection
topology of a network, and it is common in real network deployments to occasionally receive packets from distances significantly
longer than the longest reliable link [4]. Unreliable links are so
pervasive that virtually every ad hoc radio network deployment of
the last five years uses link quality assessment algorithms, such as
ETX [13], to cull unreliable connections from those considered by
higher-layer protocols. By contrast, many theoretical models of radio networks assume a fixed communication topology consisting
only of reliable links.
In this paper, we explore the impact, in terms of algorithmic time
complexity, of introducing unreliability into a theoretical model for
radio networks. We consider a dual graph model, in which there
are two types of communication links: reliable links that always
deliver messages, and unreliable links that sometimes deliver messages and sometimes do not. The unreliable links are an abstraction
that captures a variety of realistic phenomena. Our goal is to produce a model that is simple enough to be amenable to theoretical
analysis, yet still captures the diversity of complex radio behaviors
necessary to keep results applicable to real world deployment.
As a first step towards understanding the effects of unreliability we study the fundamental problem of network-wide message
broadcast in the dual graph model. Broadcast is a powerful primitive: it can be used to simulate a single-hop network on top of a
multi-hop network, greatly simplifying the design and analysis of
higher-level algorithms. The broadcast problem has been extensively studied in a variety of models and settings, but mostly in
reliable models (see Section 2.2 for an overview of existing work).
We show that broadcast in the presence of unreliable links is strictly
harder than broadcast in the reliable model. For example, in undirected reliable graphs it is possible to broadcast in 𝑂(𝑛) rounds
[2, 5], while we show that unreliable links increase the round complexity to Ω(𝑛 log 𝑛) under the same assumptions. For randomized algorithms the stretch is even worse: in the reliable model it
is possible to complete a broadcast in 𝑂(𝐷 log(𝑛/𝐷) + log2 𝑛)
rounds with high probability in graphs of diameter 𝐷 [20], while
we show that there is a dual graph network of diameter 2 in which
randomized broadcast requires Ω(𝑛) rounds (this result appeared
originally in [22] as a brief announcement). On the other hand, we
show that broadcast can still be solved with reasonable efficiency

Categories and Subject Descriptors
F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems—computations on discrete structures;
G.2.2 [Discrete Mathematics]: Graph Theory—graph algorithms;
G.2.2 [Discrete Mathematics]: Graph Theory—network problems

General Terms
Algorithms, Theory

Keywords
unreliable networks, broadcast, dual graphs

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
PODC’10, July 25–28, 2010, Zurich, Switzerland.
Copyright 2010 ACM 978-1-60558-888-9/10/07 ...$10.00.

336

INTRODUCTION

√
in the dual graph model: we give an 𝑂(𝑛3/2 log 𝑛) deterministic
algorithm for broadcast in directed dual graphs, and a randomized
algorithm that broadcasts in 𝑂(𝑛 log2 𝑛) rounds with high probability. A lower bound from [11] implies that our deterministic algorithm is optimal up to a polylogarithmic factor for directed dual
graphs; a gap remains for undirected graphs.

2.

(CR3) When 𝑝 sends, it always receives its own message; when
two or more messages reach 𝑝 and 𝑝 does not send, it hears
silence (⊥).
(CR4) When 𝑝 sends, it always receives its own message; when
two or more messages reach 𝑝 and 𝑝 does not send, it receives either ⊥ or one of the messages. (Which of these it
receives is controlled by the adversary.)

MODELS FOR RADIO NETWORKS

After process 𝑝 receives, it changes state before beginning the
next round.
Another important modelling decision is whether or not all processes start in the same round. Here we consider two rules: the
synchronous start rule has every process begin in the first round of
the execution; the asynchronous start rule activates each process
the first time it receives a message, either from the environment or
from another process.
In our upper bound results, we use the weakest assumptions, that
is, collision rule CR4 and asynchronous start; our lower bounds
use the strongest assumptions, collision rule CR1 and synchronous
start. In each case, this serves to strengthen the results.
The definitions above also apply when 𝒜 is probabilistic. But
now, in addition, we can define probability distributions on executions based on the random choices made by the processes of 𝒜.
To do this, we specify a particular class of (deterministic or probabilistic) adversaries. Recall that, in general, an adversary may
choose the 𝑝𝑟𝑜𝑐 mapping, the processes that are reached by each
message, and (for collision rule CR4), the particular collision behavior. An adversary class defines precisely what the adversary is
allowed to choose and what information is available to it in making its choices. For algorithm 𝒜 and any particular adversary in
the specified class, we can generate an execution probabilistically
using the random choices of the processes of 𝒜 together with the
adversary’s choices. In this way, we obtain a probability distribution on executions. Then for algorithm 𝒜 and an entire class of adversaries, we obtain a collection of such probability distributions,
one for each adversary in the class. In our lower bound results, we
consider very restricted adversaries, whereas our algorithms work
with respect to more powerful adversaries.

Many different models for wireless networks have been considered in the literature; we refer the reader to [28, 29] for a comprehensive survey. In this section we introduce our dual graph model.
Then we briefly review several other models and explain how they
compare to the dual graph model.

2.1

The Dual Graph Model

Fix any 𝑛 ≥ 2. We define a dual graph network, or simply a
network, to be a pair (𝐺, 𝐺′ ) consisting of two directed graphs,
𝐺 = (𝑉, 𝐸) and 𝐺′ = (𝑉, 𝐸 ′ ), where 𝑉 is a set of 𝑛 nodes and
𝐸 ⊆ 𝐸 ′ . The set 𝐸 represents the set of reliable communication
links and 𝐸 ′ the set of all links, both reliable and unreliable. We assume that 𝑉 includes a distinguished source node 𝑠, and that every
other node is reachable in 𝐺 from 𝑠. We call a network undirected
if for every edge (𝑢, 𝑣) in 𝐸 (resp. 𝐸 ′ ), the edge (𝑣, 𝑢) is also in 𝐸
(resp. 𝐸 ′ ).
We define an algorithm 𝒜 to be a collection of 𝑛 processes,
which are either deterministic or probabilistic automata. (See [27]
for one possible definition of automata that satisfy our requirements.) We assume that each process 𝑝 ∈ 𝒜 has a unique identifier
ID𝑝 from a totally ordered set 𝐼, ∣𝐼∣ = 𝑛. We often write “process
𝑖” to mean the process with identifier 𝑖.
In order to define how algorithm 𝒜 executes on network (𝐺, 𝐺′ ),
we must associate processes with graph nodes. Formally, our definition of an execution presupposes a bijection 𝑝𝑟𝑜𝑐 from 𝑉 to 𝒜.
We assume that an adversary controls the definition of 𝑝𝑟𝑜𝑐. The
distinction between graph nodes and processes is important for our
lower bound results in Sections 4 and 6. However, we generally
blur this distinction in our upper bounds in Sections 5 and 7, writing, for example, “node 𝑣 sends” when we really mean “process
𝑝𝑟𝑜𝑐(𝑣) sends”.
An execution of algorithm 𝒜 on network (𝐺, 𝐺′ ) with a mapping 𝑝𝑟𝑜𝑐 proceeds in synchronous rounds, 1, 2, . . .. In each round,
some input may arrive at each process 𝑝𝑟𝑜𝑐(𝑣) from the external
environment. Then 𝑝𝑟𝑜𝑐(𝑣) may or may not send a message. If
it sends, its message reaches the processes at all of 𝑣’s outgoing
neighbors in 𝐺, some arbitrary subset of 𝑣’s outgoing neighbors in
𝐺′ that are not outgoing neighbors in 𝐺, and 𝑣 itself. The subset of
𝐺′ -neighbors that the messages reaches is chosen by the adversary.
When no messages reach a process 𝑝, it receives ⊥, indicating
silence. When exactly one message reaches 𝑝, it receives the message. When two or more messages reach 𝑝, it experiences a collision. Collisions can be handled in several ways; we list the possible
collision rules in order of decreasing strength (from the algorithmic
point of view).

2.2

Other Models

The standard static model The most common theoretical model for radio networks features a single network graph 𝐺,
which is static and captures both transmission and interference. A
collision occurs at a node when two or more of its neighbors send
simultaneously; typically collision rule 3 is assumed, that is, no
collision detection is available. The communication graph may be
directed or undirected.
For directed graphs with no collision detection and asynchronous
starts, the best deterministic upper bound, obtained
by combining
{
}
the algorithms from [20, 12], is 𝑂(𝑛 min log2 𝐷, log 𝑛 ); the
best lower bound is Ω(𝑛 log 𝑛/ log(𝑛/𝐷)) [20]. In [12], a randomized algorithm is given which requires 𝑂(𝐷 log(𝑛/𝐷) + log2 𝑛)
rounds with high probability. This matches the randomized lower
bounds of [23, 1], which also hold for undirected networks with
synchronous start. In undirected communication graphs with synchronous start it is possible to broadcast in 𝑂(𝑛) rounds [2, 5].
This is clearly optimal in 𝑛, and [21] shows that this bound is tight
even for networks of constant diameter. The Ω(𝑛 log 𝑛) lower
bound in Section 6 applies even for undirected graphs with synchronous start, giving a clear separation between the models. The
construction may appear superficially similar to the Ω(𝐷 log 𝑛)
lower bound of [3], but it differs significantly (the lower bound
of [3] does not apply when spontaneous wakeup is allowed).

(CR1) If two or more messages reach 𝑝 (including its own message, if it sends), then 𝑝 receives ⊤, indicating collision
notification.
(CR2) When 𝑝 sends, it always receives its own message, regardless of whether or not another message reaches it. (This
amounts to assuming that a process cannot sense the medium
while it is sending.) If two or more messages reach 𝑝 and 𝑝
does not send, then it receives collision notification (⊤).

337

Explicit-interference models Several works (e.g., [15, 16])
model a network using two graphs, a transmission graph 𝐺𝑇 and
an interference graph 𝐺𝐼 . It is typically assumed that 𝐺𝑇 ⊆ 𝐺𝐼 .
Unlike transmission edges, interference edges can only cause collisions, and messages cannot be conveyed along them. (In contrast,
in the dual graph model all edges can convey messages.) A collision occurs at node 𝑢 when at least one of its 𝐺𝑇 -neighbors and
at least one of its 𝐺𝐼 -neighbors broadcast together. The transmission and interference graphs are both static. A completely different
approach is the SINR model [18, 25, 17], in which processes receive messages only when the ratio of the signal to the sum of the
noise and other signals exceeds some threshold. The SINR model
is geometric: the strength of the signal is assumed to degrade as a
function of the distance between the processes. We refer to [30] for
a recent treatment of interference in wireless networks.

We say that algorithm 𝒜 solves the broadcast problem in network (𝐺, 𝐺′ ) provided that, in any execution of 𝒜 in (𝐺, 𝐺′ ), with
any assignment 𝑝𝑟𝑜𝑐 of processes to nodes, the message eventually
arrives at all processes. We say that 𝒜 solves the broadcast problem
within 𝑘 rounds in network (𝐺, 𝐺′ ) provided that, in any execution
of 𝒜 in (𝐺, 𝐺′ ), with any assignment 𝑝𝑟𝑜𝑐 of processes to nodes,
the message arrives at all processes within 𝑘 rounds.
Now consider a probabilistic algorithm 𝒜 and a fixed adversary
class. Recall that 𝒜 generates a collection of probability distributions on executions, one for each adversary in the specified class.
For any 𝑞, 0 ≤ 𝑞 ≤ 1, we say that probabilistic algorithm 𝒜 solves
the broadcast problem in network (𝐺, 𝐺′ ) with probability 𝑞 provided that the following holds: When 𝒜 executes in (𝐺, 𝐺′ ), using
any adversary in the specified class, with probability at least 𝑞, the
message eventually arrives at all processes. We say that 𝒜 solves
the broadcast problem within 𝑘 rounds in (𝐺, 𝐺′ ) with probability
𝑞 provided that: When 𝒜 executes in (𝐺, 𝐺′ ), using any adversary in the specified class, with probability at least 𝑞, the message
arrives at all processes within 𝑘 rounds.
We say that network (𝐺, 𝐺′ ) is 𝑘-broadcastable, where 𝑘 is a
positive integer, if there exist a deterministic algorithm 𝒜 and a
mapping 𝑝𝑟𝑜𝑐 such that, in any execution of 𝒜 in (𝐺, 𝐺′ ) with
𝑝𝑟𝑜𝑐, with collision rule CR1 and synchronous starts, the message arrives at all processes within 𝑘 rounds. In other words, 𝑘broadcastable captures the intuitive notion that there is a way to
resolve the contention in the network such that the message can
be propagated to all nodes in 𝑘 rounds. Note that, if (𝐺, 𝐺′ ) is a
directed or undirected 𝑘-broadcastable network, then the distance
from the source to each other node in 𝐺 must be at most 𝑘.

Models that feature uncertainty The closest model to
the dual graph model in the literature is the dynamic-fault model
of [11], in which edges of the directed communication graph can
fail and recover dynamically during the execution. If one takes 𝐺′
to be the entire graph and 𝐺 to be the subgraph induced by edges
that never fail, the model of [11] is equivalent to dual graphs, except for one aspect: in [11] it is not assumed that 𝐺 is connected,
and instead the broadcast is only required to reach those processes
that are reachable from the source in 𝐺. It is shown in [11] that
deterministic oblivious algorithms require Ω(𝑛2 ) rounds to broadcast in dynamic-fault graphs; however, the notion of obliviousness
used there is a very strong one, and does not allow the behavior of
processes to depend on the round in which they first
√hear the message. In contrast, in Section 5 we give an 𝑂(𝑛3/2 log 𝑛) broadcast algorithm in which processes use no information except the
current round and the round in which they first receive the message
(and their label). The authors of [11] give a deterministic oblivious
algorithm that requires 𝑂(𝑛 min {𝑛, Δ log 𝑛}) rounds in dynamicfault graphs
√ of in-degree Δ. This algorithm outperforms ours when
Δ = 𝑜( 𝑛/ log 𝑛); however, it requires that all processes know
(an upper bound on) the in-degree Δ of the interference graph 𝐺′ ,
whereas our algorithm requires no such knowledge.
In addition, [11] shows an Ω(𝑛3/2 ) lower bound for non-oblivious
deterministic broadcast in directed dynamic-fault graphs. This lower
bound carries over to the dual graph model,
√ and implies that the
algorithm we give in Section 5 is within 𝑂( 𝑛) of optimal for directed graphs. In [10], the requirement on broadcast is strengthened
to require it to reach all processes, even those that are not connected
to the source by a fault-free path. For the stronger broadcast to be
possible, it is assumed that in every round there is some functioning link between a process that has the message and a process that
does not. This model does not admit a deterministic algorithm, but
the authors give an 𝑂(𝑛2 / log 𝑛) expected-time randomized algorithm.
Tables 1, 2 summarize the best known upper and lower bounds
for broadcast in the classical and dual graph models, assuming synchronous start (SS), asynchronous start (AS), directed (D) or undirected (U) communication graphs. Results shown in bold are presented in the current paper.

3.

4.

BOUNDS FOR 2-BROADCASTABLE NETWORKS

In [22], three of the authors proved the following theorem, which
provides a lower bound on the number of rounds required for broadcast in an undirected 2-broadcastable network. The lower bound
assumes collision rule CR1 and synchronous starts.
T HEOREM 1. Let 𝑛 ≥ 3. There exists a 2-broadcastable undirected network (𝐺, 𝐺′ ) such that there do not exist a probabilistic algorithm 𝒜 and integer 𝑘, 1 ≤ 𝑘 ≤ 𝑛 − 3, where 𝒜 solves
broadcast within 𝑘 rounds in (𝐺, 𝐺′ ) with probability greater than
𝑘/(𝑛 − 2).
The lower bound is matched by a deterministic round-robin broadcast strategy, which succeeds (deterministically) in 𝑂(𝑛) rounds in
(directed or undirected) graphs of constant diameter, and hence, in
𝑘-broadcastable networks for any constant 𝑘.

5.

DETERMINISTIC UPPER BOUND

We describe a deterministic
algorithm that solves the broadcast
√
problem in 𝑂(𝑛3/2 log 𝑛) time. To strengthen the upper bound
we assume the weakest assumptions from Section 2: a directed
dual graph, Rec. Rule 4, and√
asynchronous start. For simplicity we
assume that 𝑛 ≥ 3 and that 𝑛/ log 𝑛 is a power of 2.
Our algorithm follows the standard broadcast strategy of cycling
through selection objects of exponentially increasing sizes; c.f., [6,
7]. A selection object is a broadcast schedule for every node, parameterized by the number of nodes participating, which guarantees that if the correct number of nodes participate, each node will
be isolated and will be the only node to broadcast in some round.
Broadcast algorithms that follow this strategy are typically concerned with isolating all frontier nodes, nodes adjacent to some
node that does not have the message yet.

THE BROADCAST PROBLEM

The broadcast problem requires the dissemination of a message
from the process at the distinguished source node 𝑠 to all processes.
We assume that the message arrives at the source process prior to
the first round of execution. We assume that the processes treat the
message like a black box; i.e., that they behave the same regardless
of the message contents.

338

SS + U
SS + D
AS + U
AS + D

Classical model (𝐺 = 𝐺′ )
𝑂(𝑛)[5]
Ω(𝑛)[21]
{
}
𝑂(𝑛 min log 𝑛, log2 𝐷 )
Ω(𝑛 log 𝑛/ log(𝑛/𝐷))[20]
[20, 12]

Dual graphs (𝐺 ∕= 𝐺′ )
Ω(𝒏 log 𝒏)
Ω(𝑛3/2 )[11]
3/2 √
𝑶(𝒏
log 𝒏)
Ω(𝒏 log 𝒏)
Ω(𝑛3/2 )[11]

Table 1: Bounds on deterministic broadcast
Classical model (𝐺 = 𝐺′ )
2

𝑂(𝑛 log(𝑛/𝐷) + log 𝑛)[12]

Dual graphs (𝐺 ∕= 𝐺′ )
2

Ω(𝑛 log(𝑛/𝐷) + log 𝑛)[23, 1]

𝑶(𝒏 log2 𝒏)

?

Table 2: Bounds on randomized broadcast (for any combination of assumptions with no collision detection)
In the reliable model, when a frontier node 𝑢 is isolated and
broadcasts alone, all of 𝑢’s neighbors receive the message. Thereafter, node 𝑢 is no longer a frontier node; even if 𝑢 continues broadcasting, its transmissions cannot interfere with the progress of the
message, because all its neighbors already have the message. Thus,
in the algorithms of, e.g., [6, 7], nodes continue to cycle through selective families forever, and never stop broadcasting. The different
selector sizes are used to ensure that at least one selector matches
the size of the frontier, ensuring that all frontier nodes will be isolated.
In the dual graph model the situation is more complicated; there
is no clear-cut “frontier”. Suppose that node 𝑢 has some 𝐺′ -neighbors
that have not received the message, but all of its 𝐺-neighbors already have the message. Informally, node 𝑢 no longer contributes
to the progress of the algorithm, because the adversary can prevent
it from getting the message out to new nodes (its 𝐺′ -neighbors); in
this sense 𝑢 is no longer a frontier node. However, 𝑢 can still interfere with the progress of the algorithm, because its broadcasts can
cause collisions at nodes that do not have the message. Due to this
difficulty, we allow processes to participate in each selection object exactly one, limiting the interval during which they can cause
interference. This strategy has the additional advantage that nodes
eventually stop broadcasting.
In the following, we use the notation [𝑘, 𝑘′ ], where 𝑘′ ≥ 𝑘 ≥ 1,
to indicate the interval {𝑘, ..., 𝑘′ }, and use [𝑘], where 𝑘 ≥ 1, to
indicate [1, 𝑘]. We continue by defining Strongly Selective Families
(SSFs), the selection objects used in our algorithm.

The Strong Select Algorithm Assume without loss
of generality that processes have a access to a global round
counter1 . The algorithm divides the rounds into contiguous
groups of length 2𝑠max − 1 called epochs. The first round
of each epoch is dedicated to the smallest SSF ℱ1 ; the next
two rounds are dedicated to ℱ2 ; the next four rounds to ℱ3 ,
and so on. In general, we go through 2𝑠−1 sets of each SSF
ℱ𝑠 in each epoch.
When a process first receives a message, it waits, for
each 𝑠 ∈ [𝑠max ], until ℱ𝑠 cycles back to ℱ𝑠 [1]. It
then participates in the SSF ℱ𝑠 for a single iteration,
broadcasting in any round in which it is included in the
corresponding SSF set. That is, after it starts participating, in round( 𝑟 of epoch 𝑒 process
) 𝑖 broadcasts iff
𝑖 ∈ ℱ⌊log 𝑟⌋+1 [ (𝑒 − 2) ⋅ 2⌊log 𝑟⌋ + 𝑟 mod ℓ⌊log 𝑟⌋+1 +
1]. After participating in one complete iteration of an SSF,
the process stops participating in that family. Each process
participates in exactly one iteration of each SSF used in the
algorithm.
For a given SSF ℱ𝑠 , we use the term iteration to describe a complete cycle through ℱ𝑠 [1], . . . , ℱ𝑠 [ℓ𝑠 ]. Note that each iteration of
ℱ𝑠 is spread out over ℓ𝑠 /2𝑠−1 epochs. We also remark that in a
given epoch it could happen that a process participates in some selector families but not in others, because it is waiting for those other
selector families to cycle back to their first set.
Analysis. Fix a network (𝐺, 𝐺′ ) and an execution 𝛼 of the algorithm in the network. Define 𝑓 (𝑛) to be the log-factor in the size of
the SSFs: formally, 𝑓 (𝑛) is a function such that 𝑓 (𝑛) = 𝑂(log 𝑛)
and for each SSF ℱ𝑠 used by the algorithm, ℓ𝑠 ≤ 𝑘𝑠2 𝑓 (𝑛).
The proof involves an amortization argument, where (roughly
speaking) we show that in every sufficiently long interval the algorithm always makes progress: either many new nodes receive the
message for the first time, and a lot of progress is made; or few
nodes receive the message for the first time, but then these nodes
only have to contend with each other, and they will quickly be isolated and get the message out to other nodes. To formalize this, we
define the density of an interval [𝑟, 𝑟′ ], denoted den(𝑟, 𝑟′ ), to be the
number of processes that receive the message for the first time in
the interval, divided by 𝑟′ − 𝑟 + 1:

D EFINITION 2 ([8]:). Let 𝑘 ≤ 𝑛. A family ℱ of subsets of
[𝑛] is (𝑛, 𝑘)-strongly selective if for every non-empty subset 𝒵 of
[𝑛] such that ∣𝒵∣ ≤ 𝑘 and for every element 𝑧 ∈ 𝒵 there is a set 𝐹
in ℱ such that 𝒵 ∩ 𝐹 = {𝑧}.
Erdös et. al. provide an upper bound on the size of these objects [14]:
T HEOREM 3 ([14]). For any 𝑛 ≥ 3 and for{𝑘 ≥ 2, there}exist
(𝑛, 𝑘)-strongly selective families of size 𝑂(min 𝑛, 𝑘2 log 𝑛 ).
√
Let 𝑠max := log( 𝑛/ log 𝑛). For each 𝑠 ∈ [𝑠max ], let ℱ𝑠
be an (𝑛, 𝑘𝑠 )-SSF of size ℓ𝑠 = 𝑂(𝑘𝑠2 log 𝑛), where 𝑘𝑠 = 2𝑠 .
(By [14] we know such families exist.) We fix some total ordering ℱ𝑠 [1], ..., ℱ𝑠 [ℓ𝑠 ] on the ℓ𝑠 sets that comprise each family ℱ𝑠 .
Furthermore, we assume that ℱ𝑠max is the round robin sequence,
which isolates every node in the graph. Thus, ℱ𝑠max is an (𝑛, 𝑛)SSF. (We can assume this because ℓ𝑠max = Θ(𝑘𝑠2max log 𝑛) =
Θ(𝑛).) We now define our algorithm, which we call strong select.

# processes that receive the message
for the first time during [𝑟, 𝑟′ ]
.
(1)
den(𝑟, 𝑟 ) :=
𝑟′ − 𝑟 + 1
1
This can be achieved by having the source node label every message with its local round counter; when a node is awakened by
receiving a message, it adopts the round number on that message.
′

339

Given an SSF ℱ𝑠 , let 𝑐𝑠 (𝑟, 𝑟′ ) denote the number of complete
iterations of ℱ𝑠 that fit in the interval [𝑟, 𝑟′ ]. Finally, we fix two
constants that are used throughout the proof: we define a density
threshold
1
1
√
𝜌 :=
,
=
12𝑓 (𝑛)2𝑠max
12𝑓 (𝑛) 𝑛/ log 𝑛

Formally, we show by backwards induction on 𝑠 that for all
𝑠 = 𝑠max , . . . , 1, process 𝑖 did not receive the message by round
𝑇 − 2ℓ′𝑠 . Here, as in the proof of Lemma 4, we define ℓ′𝑠 =
ℓ𝑠 (2𝑠max − 1)/2𝑠−1 to be the number of rounds required for a
complete iteration of ℱ𝑠 . Note that 𝑇 − 2ℓ′𝑠 may be negative, in
which case the claim trivially holds.
Induction base: for 𝑠 = 𝑠max , suppose that 𝑇 − 2ℓ′𝑠max ≥ 0
and suppose by way of contradiction that node 𝑖 received the message by round 𝑇 − 2ℓ′𝑠max . Since ℱ𝑠max cycles back every ℓ′𝑠max
rounds, node 𝑖 started participating in ℱ𝑠max no later than round
𝑇 − ℓ′𝑠max ; by round 𝑇 it has had enough time to participate in a
full iteration of ℱ𝑠max . However, recall that ℱ𝑠max is an (𝑛, 𝑛)SSF; any node that participates in a full iteration of ℱ𝑠max is isolated. Since we assumed that 𝑖 has not been isolated by round 𝑇 , it
cannot have received the message by round 𝑇 − 2ℓ′𝑠max .
Induction step: suppose that node 𝑖 did not receive the message
by round 𝑇 − 2ℓ′𝑠 , and suppose by way of contradiction that 𝑖 received the message by round 𝑇 − 2ℓ′𝑠−1 ≥ 0. Observe that since
ℓ′𝑠−1 = 22(𝑠−1) 𝑓 (𝑛)(2𝑠max −1)/2𝑠−2 and ℓ′𝑠 = 22𝑠 𝑓 (𝑛)(2𝑠max −
1)/2𝑠−1 , we have ℓ′𝑠 = 2ℓ′𝑠−1 : two iterations of ℱ𝑠−1 fit inside
every iteration of ℱ𝑠 . Since process 𝑖 did not get the message by
round 𝑇 − 2ℓ′𝑠 = 𝑇 − 4ℓ′𝑠−1 , and we assumed for contradiction
that it got it by round 𝑇 − 2ℓ′𝑠−1 , it participates in one of the last
min {4, 𝑐𝑠−1 (1, 𝑇 )} iterations of ℱ𝑠−1 . From Lemma 4, process 𝑖
is isolated, yielding a contradiction. This concludes the induction.
We have shown that √
process 𝑖 did not get the message by round
𝑇 − 2ℓ′1 = 𝑇 − 8𝑓 (𝑛) 𝑛/ log 𝑛 > 𝑇 − 1/𝜌. Since we assumed
that 𝑖 did get the message prior to round 𝑇 , it follows that 𝑖 got the
message for the first time in the interval [max {1, 𝑇 − 1/𝜌 + 1} , 𝑇 ],
contradicting Lemma 5. This completes the first part of the proof;
we can now conclude that every process receives the message no
later than round 𝑇 .
To conclude the proof, consider the interval [1, 𝑋], where we
√
√
define 𝑋 = 𝑛/𝜌 = 12𝑛3/2 𝑓 (𝑛)/ log 𝑛 = 𝑂(𝑛3/2 log 𝑛). If
den(1, 𝑋) ≥ 𝜌, then 𝑛 processes receive the message during the
interval [1, 𝑋]. On the other hand, if den(1, 𝑋) < 𝜌, then by definition 𝑇 ≤ 𝑋, so again all processes receive the message no later
than round 𝑋. In both cases the broadcast
√ is complete by round 𝑋,
and the algorithm terminates in 𝑂(𝑛3/2 log 𝑛) rounds.

and let 𝑇 be the smallest round such that den(1, 𝑇 ) < 𝜌, that is,
the round in which the density over the entire execution first drops
below 𝜌. We will eventually show that the algorithm terminates no
later than round 𝑇 .
We begin by showing that each process that participates in one
of the last iterations of some SSF ending by round 𝑇 is isolated.
L EMMA 4. Consider the last 𝑐 := min {4, 𝑐𝑠 (1, 𝑇 )} iterations
of ℱ𝑠 in the interval [1, 𝑇 ], for some 𝑠 ∈ [𝑠max ]. Every process that
participates in one of these 𝑐 iterations broadcasts alone at some
point during the iteration.
P ROOF. Let 𝑃 be the number of processes that participate in one
of the 𝑐 last SSFs. Let ℓ′𝑠 = ℓ𝑠 (2𝑠max − 1) /2𝑠−1 be the number of
rounds required to complete an iteration of ℱ𝑠 : family ℱ𝑠 contains
ℓ𝑠 sets spread out over ℓ𝑠 /2𝑠−1 epochs (with 2𝑠−1 sets from ℱ𝑠
in each epoch), and each epoch requires 2𝑠max − 1 rounds. Any
process that participates in one of these 𝑐 iterations must receive
the message for the first time in the interval [𝑇 ′ , 𝑇 ] where 𝑇 ′ =
max {1, 𝑇 − 6ℓ′𝑠 + 1}. Therefore, if we denote by 𝑅 the number
of processes that receive the message for the first time in [𝑇 ′ , 𝑇 ],
then 𝑃 ≤ 𝑅. Note also that den(𝑇 ′ , 𝑇 ) < 𝜌, otherwise we would
have den(1, 𝑇 ′ ) < 𝜌 and 𝑇 would not be minimal. It follows that
(1)

𝑃 ≤ 𝑅 ≤ den(𝑇, 𝑇 ′ ) ⋅ (𝑇 − 𝑇 ′ + 1) < 𝜌 ⋅ 6ℓ′𝑠
=

6𝑘𝑠2 𝑓 (𝑛) (2𝑠max − 1)
12𝑓 (𝑛)2𝑠max ⋅ 2𝑠−1

(𝑘𝑠 =2𝑠 )

<

𝑘𝑠 .

We have shown that the total number of participants in any of
the last 𝑐 iterations is less than 𝑘𝑠 ; therefore, the number of participants in each individual iteration is also less than 𝑘𝑠 (because each
process participates in just one iteration). From the definition of an
SSF, each participant in any of the last 𝑐 iterations will be selected
to broadcast alone in the network.

A Note on
Solutions The (𝑛, 𝑘)-SSFs of
{ Constructive
}
size 𝑂(min 𝑛, 𝑘2 log 𝑛 ) used in strong select are derived from
an existential argument [14]. The smallest-size constructive definition of an (𝑛, 𝑘)-SSF, from
} by Kautz and Singel{ a 1964 paper
ton [19], is of size 𝑂(min 𝑛, 𝑘2 log2 𝑛 ). Replacing the SSFs in
our algorithm with the√variant from [19] would increase our time
complexity by only a log 𝑛-factor.

L EMMA 5. No process receives the message for the first time in
the interval [𝑇 ′ , 𝑇 ], where 𝑇 ′ = max {1, 𝑇 − 1/𝜌 + 1}.
P ROOF. If one or more processes receives a message in this interval, then den(𝑇 ′ , 𝑇 ) ≥ 𝑇 −𝑇1 ′ +1 ≥ 𝜌, contradicting the minimality of 𝑇 .
T HEOREM
6. The strong select algorithm solves broadcast in
√
𝑂(𝑛3/2 log 𝑛) rounds in any directed (or undirected) network
(𝐺, 𝐺′ ), with collision rule 4 and asynchronous starts.

6.

DETERMINISTIC LOWER BOUNDS

In this section, we present two lower bounds for deterministic
broadcast algorithms. For both algorithms, we assume collision
rule CR1 and synchronous starts. The following bound is a straightforward adaptation of the result presented as Theorem 4.2 of [9]:

P ROOF. We first show that every process receives the message
by the end of round 𝑇 .
Assume for contradiction that some node has not received the
message by round 𝑇 . Since all nodes are reachable from the source
in 𝐺, there exist two nodes 𝑖, 𝑗 such that 𝑖 has the message by round
𝑇 and 𝑗 does not, and (𝑖, 𝑗) ∈ 𝐸. This means that process 𝑖 has not
been isolated prior to round 𝑇 ; we will show that process 𝑖 cannot
have received the message prior to round 𝑇 , deriving a contradiction. The proof involves repeatedly using Lemma 4 to show that
process 𝑖 cannot have received the message by the last iteration of
selector families of decreasing size, pushing forward the round in
which process 𝑖 first received the message until eventually we exceed round 𝑇 ′ = 𝑇 − 1/𝜌, obtaining a contradiction to Lemma 5.

√
T HEOREM 7. There exists a 𝑛-broadcastable directed net′
work (𝐺, 𝐺 ), such that every deterministic algorithm 𝒜 that solves
the broadcast problem in (𝐺, 𝐺′ ) has an execution in which it takes
Ω(𝑛3/2 ) rounds until the message arrives at all processes.
It follows √
that our upper bound in Section 5 is tight to within a
factor of 𝑂( log 𝑛). However, this lower bound construction depends heavily on the fact that the network is directed. If the graph

340

1. If more than one process sends, then all messages sent reach
everywhere, and all processes receive ⊤.
2. If a single process 𝑗 ∈ 𝐴𝑘 sends alone, then its message
reaches exactly the processes with ids in 𝐴𝑘 ∪ {𝑖, 𝑖′ }, so
exactly these receive it.
3. If a single process 𝑗 ∈ 𝐼 − (𝐴𝑘 ∪ {𝑖, 𝑖′ }) sends alone, then
the message reaches all processes, so they all receive it.
4. If either 𝑖 or 𝑖′ sends alone, then the message reaches all
processes, so they all receive it. (We include this rule for
completeness; this case will not arise within the number of
rounds we will consider.)
5. If no process sends, then all processes receive ⊥.

were undirected, processes could provide feedback to their neighbors when they receive the message; this would break the reduction
to the SSF lower bound which is at the core of the lower bound
from [9].
We proceed with an Ω(𝑛 log 𝑛) lower bound that handles undirected networks. It is unknown whether this bound is tight.
T HEOREM 8. There exists an undirected network (𝐺, 𝐺′ ), such
that every deterministic algorithm 𝒜 that solves the broadcast problem in (𝐺, 𝐺′ ) has an execution in which it takes Ω(𝑛 log 𝑛) rounds
until the message arrives at all processes.
In the following proof, we say a process is about to be isolated
after a given finite execution if it will send in the next round, and is
the only process that will do so.

These rules are designed so that, until either 𝑖 or 𝑖′ sends alone,
only the nodes in 𝐴𝑘 ∪ {𝑖, 𝑖′ } will have the broadcast message. It is
easy to verify that the adversary can always follow the rules above
regardless of the process assignment to nodes 2𝑘 + 3, . . . , 𝑛 − 1
(which we have not yet committed to at this point).
Having defined 𝛽𝑖,𝑖′ for all possible pairs {𝑖, 𝑖′ }, we must choose
the pair {𝑖, 𝑖′ } that will actually be assigned to layer 𝐿𝑘 and used
to define 𝛼𝑘+1 . We do this by constructing a sequence of candidate
sets of process identifiers, 𝐶0 , 𝐶1 , . . . , 𝐶log(𝑛−1)−2 , where 𝐶0 =
𝐼 − 𝐴𝑘 , and each candidate set in the sequence is a subset of the
previous one. Informally speaking, the identifiers in each 𝐶ℓ are the
candidates that remain after we take into account behavior through
round ℓ. The process ids 𝑖 and 𝑖′ will be elements of 𝐶log(𝑛−1)−2 .
We begin with 𝐶0 = 𝐼 − 𝐴𝑘 and construct the remaining candidate sets inductively. Observe that ∣𝐶0 ∣ = ∣𝐼 − 𝐴𝑘 ∣ ≥ 𝑛−1
,
2
because we apply this construction for only 𝑛−1
stages
and
add
4
only two processes to 𝐴𝑘 at each stage.
We maintain the following inductive property for each candidate
set 𝐶ℓ (where 0 ≤ ℓ ≤ log(𝑛 − 1) − 2).

P ROOF. Let the set 𝑉 of nodes be {0, 1, . . . , 𝑛 − 1}, where
0 is the source node. We assume for simplicity that 𝑛 − 1 is a
power of 2, 𝑛 − 1 ≥ 4. We divide the nodes into layers 𝐿𝑘 ,
, where 𝐿0 = {0} and for each 𝑘, 1 ≤ 𝑘 ≤ 𝑛−1
,
𝑘 = 0, . . . , 𝑛−1
2
2
𝐿𝑘 = {2𝑘 − 1, 2𝑘}.
We construct a dual graph (𝐺, 𝐺′ ) with vertex set 𝑉 . The reliable graph, 𝐺, is a complete layered graph, with edge set 𝐸 given
by:
{{0, 𝑢} ∣ 𝑢 ∈ {1, 2}} ∪ {{𝑢, 𝑣} ∣ ∃𝑘 : 𝑢, 𝑣 ∈ 𝐿𝑘 and 𝑢 ∕= 𝑣}
∪ {{𝑢, 𝑣} ∣ ∃𝑘 : 𝑢 ∈ 𝐿𝑘 and 𝑣 ∈ 𝐿𝑘+1 } .
The unreliable graph, 𝐺′ , is the complete graph over 𝑉 : 𝐸 ′ =
{{𝑢, 𝑣} ∣ 𝑢 ∕= 𝑣}. Note that by design, when process 𝑝𝑟𝑜𝑐(𝑢)
trasmits, where 𝑢 ∈ 𝐿𝑘 , its message can reach the processes at
any subset of the nodes that includes 𝐿𝑘−1 (if 𝑘 > 0) and 𝐿𝑘+1 (if
).
𝑘 < 𝑛−1
2
We assume that the identifier set 𝐼 includes a distinguished identifier 𝑖0 that is assigned to node 0, that is, that 𝑝𝑟𝑜𝑐(0) = 𝑖0 .
We construct an execution 𝛼 and mapping 𝑝𝑟𝑜𝑐 in stages num. At Stage 𝑘, 1 ≤ 𝑘 ≤ 𝑛−1
, the construction
bered 1, 2, . . . , 𝑛−1
4
4
assigns processes to the nodes (2𝑘−1 and 2𝑘) in layer 𝐿𝑘 , and constructs a longer prefix 𝛼𝑘 of 𝛼. For any 𝑘, let 𝐴𝑘 be the set of identifiers of processes that are assigned to nodes in layers 𝐿0 , . . . , 𝐿𝑘 ,
by the end of Stage 𝑘. Our construction will ensure that, by the end
of 𝛼𝑘 , exactly the processes with identifiers in 𝐴𝑘 have received
the broadcast message. Moreover, 𝛼𝑘 ends with some process in
𝐴𝑘 about to be isolated.
As a base case for this construction, in Stage 0 we construct an
execution 𝛼0 in which all 𝐺′ -edges are used in every round, ending
with the first round after which 𝑖0 is about to be isolated. There
must be some such round, since otherwise no process other than
process 𝑖0 will ever receive the message. We define 𝐴0 = {𝑖0 }.
Note that by the end of 𝛼0 , only 𝑖0 has the message, because it has
not yet sent alone.
− 1, which assigns
Now we describe Stage 𝑘 + 1, 0 ≤ 𝑘 ≤ 𝑛−1
4
processes to the two nodes (2𝑘 + 1 and 2𝑘 + 2) in layer 𝐿𝑘+1 , and
extends 𝛼𝑘 to 𝛼𝑘+1 . For each pair of processes {𝑖, 𝑖′ } ⊆ 𝐼 − 𝐴𝑘 ,
we define an extension 𝛽𝑖,𝑖′ of 𝛼𝑘 , in which we assign processes 𝑖
and 𝑖′ to 𝐿𝑘 , arbitrarily assigning one of the two processes to 2𝑘+1
and the other to 2𝑘 + 2. We first define 𝛽𝑖,𝑖′ for any {𝑖, 𝑖′ }, and
then describe how we choose the particular pair 𝑖, 𝑖′ that is used to
construct 𝛼𝑘+1 . For convenience we number the rounds of 𝛽𝑖,𝑖′
after 𝛼𝑘 as 0, 1, . . ..
In round 0 of 𝛽𝑖,𝑖′ , we know that exactly one process sends, and
it belongs to 𝐴𝑘 . The adversary allows this message to reach (and
so, to be received by), exactly the processes in 𝐴𝑘 ∪ {𝑖, 𝑖′ } (by
using the appropriate 𝐺′ edges). Thereafter, we use the following
adversary rules to determine where messages reach. Collisions are
handled according to CR1, our strongest rule.

Property 𝑷 (ℓ).
(1) ∣𝐶ℓ ∣ ≥

𝑛−1
.
2ℓ+1

(2) Let 𝑗 ∈ 𝐼, and let {𝑖1 , 𝑖′1 } and {𝑖2 , 𝑖′2 } be two pairs of elements
of 𝐶ℓ . Suppose that 𝑗 is either in neither subset or in both. Then
process 𝑗 receives the same values (either ⊥, ⊤, or an actual
message) in rounds 1, . . . , ℓ of 𝛽𝑖1 ,𝑖′1 and 𝛽𝑖2 ,𝑖′2 .
(3) Let 𝑖, 𝑖′ ∈ 𝐶ℓ . Then neither 𝑖 nor 𝑖′ sends alone at any of
rounds 1, . . . , ℓ of 𝛽𝑖,𝑖′ .
Part (1) of 𝑃 (ℓ) will be used to ensure that we can extend Stage
𝑘 to Ω(log 𝑛) rounds. Part (2) ensures that neither of the processes
assigned to layer 𝐿𝑘 learns the identity of the other process, and
also that none of the processes assigned to layers greater than 𝑘
learns the identities of the processes assigned to layer 𝑘. Part (3)
says that the candidates that remain after round ℓ have not yet sent
alone, after 𝛼𝑘 .
Suppose we already have a set 𝐶log(𝑛−1)−2 satisfying 𝑃 (log(𝑛−
1) − 2). Conditions (1) and (3) together imply that there exist
𝑖, 𝑖′ ∈ 𝐶log(𝑛−1)−2 such that neither 𝑖 nor 𝑖′ sends alone in any of
rounds 1, . . . , log(𝑛 − 1) − 2 of 𝛽𝑖,𝑖′ . We arbitrarily choose one
such pair {𝑖, 𝑖′ }, and define 𝛼𝑘+1 to be the prefix of 𝛽𝑖,𝑖′ ending at
the first time when either 𝑖 or 𝑖′ is about to be isolated; this extends
𝛼𝑘 by at least log(𝑛 − 1) − 2 rounds.

Inductive construction of 𝑪0 , . . . , 𝑪log(𝒏−1)−2 . Property 𝑃 (0) is clearly true for 𝐶0 . Suppose we have already constructed 𝐶ℓ , where 0 ≤ ℓ ≤ log(𝑛 − 1) − 3, such that 𝑃 (ℓ) holds,
and let us construct 𝐶ℓ+1 . We begin by defining two sets:

341

3. Let 𝑖, 𝑖′ ∈ 𝐶ℓ+1 . Then neither 𝑖 nor 𝑖′ sends alone at any of
rounds 1, . . . , ℓ + 1 of 𝛽𝑖,𝑖′ .

∙ 𝑆ℓ+1 is the set of remaining candidates 𝑖 ∈ 𝐶ℓ such that
if we assign 𝑖 to layer 𝐿𝑘 , then 𝑖 will send in round ℓ + 1.
Formally, 𝑆ℓ+1 is defined to be the set of ids 𝑖 ∈ 𝐶ℓ such that
for some 𝑖′ ∈ 𝐶ℓ , 𝑖′ ∕= 𝑖, process 𝑖 sends in round ℓ + 1 of
𝛽𝑖,𝑖′ . (By Part 2 of 𝑃 (ℓ), this set is equivalent to what what
we obtain if we replace “for some 𝑖′ ” with “for every 𝑖′ ”.)

P ROOF. For Part 1, note that ∣𝐶ℓ ∣ ≥ 2𝑛−1
ℓ+1 , by Part 1 of 𝑃 (ℓ).
If ∣𝐶ℓ ∣ is even, the result then follows by easy calculations based
on the three cases in the definition of 𝐶ℓ+1 from 𝐶ℓ . If ∣𝐶ℓ ∣ is
odd, then the calculation is straightforward for Cases 1 and 2(a).
The argument for Case 2(b) is slightly more involved. We know
that ∣𝐶ℓ ∣ ≥ 2𝑛−1
We know that 2𝑛−1
ℓ+1 .
ℓ+1 is even, because ℓ ≤
log (𝑛 − 1) − 3. Since ∣𝐶ℓ ∣ is odd, we have ∣𝐶ℓ ∣ ≥ 2𝑛−1
ℓ+1 + 1.
Also, since ∣𝑆ℓ+1 ∣ < ∣𝐶2ℓ ∣ , we have ∣𝑆ℓ+1 ∣ ≤ ∣𝐶ℓ2∣−1 . So we have

∙ 𝑁ℓ+1 is the set of remaining candidates 𝑖 ∈ 𝐶ℓ that will send
in round ℓ + 1 if we do not assign them to layer 𝐿𝑘 . That
is, 𝑁ℓ+1 is the set of nodes such that for some 𝑗, 𝑗 ′ ∈ 𝐶ℓ
where 𝑖 ∕∈ {𝑗, 𝑗 ′ }, process 𝑖 sends in round ℓ + 1 of 𝛽𝑗,𝑗 ′ .
(As above, by Part 2 of 𝑃 (ℓ), this also holds if we replace
“for some 𝑗, 𝑗 ′ ” with “for every 𝑗, 𝑗 ′ ”.)

∣𝐶ℓ+1 ∣ = ∣𝐶ℓ ∣ − ∣𝑆ℓ+1 ∣ − 1 ≥ ∣𝐶ℓ ∣ −

Note that for every 𝑖 ∈ 𝐶ℓ − (𝑆ℓ+1 ∪ 𝑁ℓ+1 ), process 𝑖 will not
send in round ℓ + 1 regardless of whether or not it is assigned to
layer 𝐿𝑘 .
Now we are ready to define 𝐶ℓ+1 . We consider cases based on
the sizes of 𝑆ℓ+1 and 𝑁ℓ+1 .

By the lower bound on 𝐶ℓ , the right-hand side is
≥

In this case we omit two such processes from the candidate set: we define 𝐶ℓ+1 := 𝐶ℓ − {𝑗, 𝑗 ′ }, where 𝑗, 𝑗 ′
are the two smallest elements of 𝑁ℓ+1 .
∣𝐶ℓ ∣
.
2

Then we set 𝐶ℓ+1 :=

Case III: ∣𝑁ℓ+1 ∣ ≤ 1 and ∣𝑆ℓ+1 ∣ <
𝐶ℓ − (𝑆ℓ+1 ∪ 𝑁ℓ+1 ).

∣𝐶ℓ ∣
.
2

Then we set 𝐶ℓ+1 :=

That is, if at least two processes would send in round ℓ + 1 if
they did not receive the message in round 0, then we omit two
such processes from the new candidate set. This guarantees that,
in the remaining executions we will consider, they will not receive
the message in round 0 and will therefore send in round ℓ + 1, so
everyone will receive ⊤ in round ℓ + 1.
On the other hand, if at most one process would send in round
ℓ+1 if it did not receive the message in round 0, then we determine
the candidates based on the number of processes that would send
in round ℓ + 1 if they did receive the message in round 0. If at least
half would send in round ℓ + 1, we include exactly those that would
send. This ensures that, in the remaining executions, at least two
of these will receive the message in round 0 and will send in round
ℓ + 1, again causing everyone to receive ⊤ in round ℓ + 1.
The remaining case is where at most one process would send in
round ℓ + 1 if it did not receive the message in round 0, and strictly
fewer than half would send in round ℓ + 1 if they did receive the
message in round 0. In this case, we include exactly those that
would not send if they received the message, omitting the possible
single process that would send if it did not receive the message.
This ensures that, in the remaining executions, the processes that
receive the message at slot 0 will not send at slot ℓ + 1. Other
processes, however, may send at slot ℓ + 1.

Case III: ∣𝑁ℓ+1 ∣ ≤ 1 and ∣𝑆ℓ+1 ∣ < ∣𝐶2ℓ ∣ . Here we must carefully consider which processes send in round ℓ + 1. We know that
neither 𝑖1 nor 𝑖′1 sends in round ℓ + 1 of 𝛽𝑖1 ,𝑖′1 , and neither 𝑖2 nor
𝑖′2 sends in round ℓ + 1 of 𝛽𝑖2 ,𝑖′2 . Also, we know that each process
in 𝐴𝑘 chooses whether/what to send based on its own state after
𝛼𝑘 , its receipt of the message in round 0, and whatever values it
receives in rounds 1, . . . , ℓ. All of this information is the same in
𝛽𝑖1 ,𝑖′1 and 𝛽𝑖2 ,𝑖′2 , using Part 2 of Property 𝑃 (ℓ) (here, each element
of 𝐴𝑘 is always in neither of the two sets). Therefore, it behaves
the same in round ℓ + 1 of both executions.
We now consider two sub-cases.
Subcase IIIa: ∣𝑁ℓ+1 ∣ = 0. Then no process in 𝐼 − (𝐴𝑘 ∪
{𝑖1 , 𝑖′1 }) sends in round ℓ + 1 of 𝛽𝑖1 ,𝑖′1 , and no process in 𝐼 −
(𝐴𝑘 ∪ {𝑖2 , 𝑖′2 }) sends in round ℓ + 1 of 𝛽𝑖2 ,𝑖′2 . Since neither 𝑖1
nor 𝑖′1 sends in round ℓ + 1 in 𝛽𝑖1 ,𝑖′1 , and neither 𝑖2 nor 𝑖′2 sends in
round ℓ + 1 in 𝛽𝑖2 ,𝑖′2 , it follows that in this subcase, no process in
𝐼 − 𝐴𝑘 sends in round ℓ + 1 of 𝛽𝑖1 ,𝑖′1 or 𝛽𝑖2 ,𝑖′2 .
We are left to consider the processes in 𝐴𝑘 . If no process in
𝐴𝑘 sends in round ℓ + 1 then 𝑗 receives ⊥ in both 𝛽 executions.
If two or more processes in 𝐴𝑘 send in round ℓ + 1, then by the
adversary rules, both messages reach all processes, so 𝑗 receives ⊤
in both executions. If exactly one process in 𝐴𝑘 sends, then by the
adversary rules, the message reaches exactly the processes in 𝐴𝑘 ∪
{𝑖1 , 𝑖′1 } in 𝛽𝑖1 ,𝑖′1 , and reaches exactly the processes in 𝐴𝑘 ∪{𝑖2 , 𝑖′2 }
in 𝛽𝑖2 ,𝑖′2 . Since 𝑗 is either in both sets 𝐴𝑘 ∪ {𝑖1 , 𝑖′1 } and 𝐴𝑘 ∪
{𝑖2 , 𝑖′2 } or neither, the message reaches 𝑗 either in both executions
or in neither execution. Thus, either 𝑗 receives the message in both
executions, or it receives ⊥ in both executions.
Subcase IIIb: ∣𝑁ℓ+1 ∣ = 1. Then a single process 𝑛1 ∈ 𝑁ℓ+1
sends in round ℓ + 1 of both 𝛽 executions. This follows because

C LAIM 9. Property 𝑃 (ℓ + 1) holds for 𝐶ℓ+1 . That is,
1. ∣𝐶ℓ+1 ∣ ≥

( 2𝑛−1
𝑛
ℓ+1 + 1) − 1
= ℓ+2 ,
2
2

as needed.
Part 3 follows from Part 3 of 𝑃 (ℓ) and the cases in the definition
of 𝐶ℓ+1 .
In remains to show Part 2; for this, fix 𝑗, 𝑖1 , 𝑖′1 , 𝑖2 , 𝑖′2 as in the
hypotheses. Part 2 of 𝑃ℓ implies that 𝑗 receives the same values in
the first ℓ rounds; we consider what happens in round ℓ + 1. We
consider cases as in the definition of 𝐶ℓ+1 .
Case I: ∣𝑁ℓ+1 ∣ ≥ 2. Then in both 𝛽𝑖1 ,𝑖′1 and 𝛽𝑖2 ,𝑖′2 , two processes in 𝑁ℓ+1 do not receive the message in round 0 and so send
at round ℓ + 1. It follows that 𝑗 receives ⊤ in round ℓ + 1 in both
executions.
Case II: ∣𝑁ℓ+1 ∣ ≤ 1 and ∣𝑆ℓ+1 ∣ ≥ ∣𝐶2ℓ ∣ . Then both 𝑖1 and 𝑖′1
send in round ℓ + 1 in 𝛽𝑖1 ,𝑖′1 and both 𝑖2 and 𝑖′2 send in round ℓ + 1
in 𝛽𝑖2 ,𝑖′2 , so again 𝑗 receives ⊤ in round ℓ + 1 in both executions.

Case I: ∣𝑁ℓ+1 ∣ ≥ 2, that is, there are at least two processes that
would send in round ℓ + 1 if they are not assigned to
layer 𝐿𝑘 .

Case II: ∣𝑁ℓ+1 ∣ ≤ 1 and ∣𝑆ℓ+1 ∣ ≥
𝑆ℓ+1 .

∣𝐶ℓ ∣ − 1
∣𝐶ℓ ∣ − 1
−1=
.
2
2

𝑛−1
.
2ℓ+2

2. Let 𝑗 ∈ 𝐼, and let {𝑖1 , 𝑖′1 } and {𝑖2 , 𝑖′2 } be two pairs of elements of 𝐶ℓ+1 . Suppose that 𝑗 is either in neither subset or
in both. Then process 𝑗 receives the same values (either ⊥,
⊤, or an actual message) in rounds 1, . . . , ℓ+1 of 𝛽𝑖1 ,𝑖′1 and
𝛽𝑖2 ,𝑖′2 .

342

we have explicitly omitted 𝑛1 from 𝐶ℓ+1 , ensuring that it does not
receive the message in round 0 in 𝛽𝑖1 ,𝑖′1 or 𝛽𝑖2 ,𝑖′2 , which implies
that it sends in round ℓ + 1. By the adversary rules, we know that
𝑛1 ’s message reaches 𝑗 in both executions.
Now we consider the processes in 𝐴𝑘 . If no process in 𝐴𝑘 sends
in round ℓ + 1, then 𝑗 receives the message from 𝑛1 in round ℓ + 1
in both executions. If one or more processes from 𝐴𝑘 sends, then
by the adversary rules, their messages reach all processes. So then
𝑗 receives ⊤ in both executions (because the 𝐴𝑘 message(s) collide
with the 𝑛1 message).
Combined, these cases establish Part 2 of 𝑃 (ℓ + 1), thus completing the proof of the claim.

round numbers, where 𝑡1 = 0, and 𝑡𝑖 is the round in which the
𝑖th node receives the message. (That is, 𝑡2 is the round in which
the first node that is not the source receives the message, and so
on.) Note that the wake-up pattern of an execution determines the
broadcasting probabilities of the nodes in every round; therefore, to
reason about broadcast probabilities it is sufficient to reason about
all possible wake-up patterns (including ones that cannot occur in
any execution of the algorithm).

Claim 9 implies that 𝑃 (log (𝑛 − 1) − 2) holds for 𝐶log (𝑛−1)−2 .
Therefore, there exist two identifiers 𝑖, 𝑖′ ∈ 𝐶log (𝑛−1)−2 such that
neither 𝑖 nor 𝑖′ sends alone at any of the first log (𝑛 − 1) − 2 slots
of 𝛽𝑖,𝑖′ . (Use Part 1 to show that ∣𝐶log (𝑛−1)−2 ∣ ≥ 2, and Part 3
to show that the processes in this set do not send alone.) We then
define 𝛼𝑘+1 to be the prefix of 𝛽𝑖,𝑖′ that ends just before the first
round where either 𝑖 or 𝑖′ sends alone. This gives us an extension
of at least log (𝑛 − 1) − 2 slots. Note that only processes in 𝐴𝑘 ∪
{𝑖, 𝑖′ } have the broadcast message by the end of 𝛼𝑘+1 .
For the entire construction, we begin with 𝛼0 and construct successive extensions 𝛼1 , 𝛼2 , . . . , 𝛼 𝑛−1 . Since only two new pro4
cesses receive the message in each stage, by the end of 𝛼 𝑛−1 , some
4
processes have still not received the message. The resulting execution is Ω(𝑛 log 𝑛) rounds long, which yields our lower bound.

P ROOF. Let 𝑊 = 𝑡1 ≤ ⋅ ⋅ ⋅ ≤ 𝑡𝑛 be a wake-up pattern that
maximizes the number of busy rounds, and among those wake-up
patterns that maximize the number of busy rounds, minimizes the
number of free rounds before the last busy round. We argue that
this wake-up pattern has no free rounds between the busy rounds,
that is, rounds 1, . . . , 𝐵(𝑛) are all busy rounds.
For the sake of contradiction, suppose that there is a free round
before the last busy round, and let 𝑟0 be the last free round before
the last busy round. By definition, 𝑃 (𝑟0 ) < 1, and since round
𝑟0 + 1 must be busy, we also have 𝑃 (𝑟0 + 1) ≥ 1. The sum of
the broadcast probabilities can only increase from one round to the
next if some new node receives the message for the first time; thus,
there is some node 𝑖0 ∈ [𝑛] such that 𝑡𝑖0 = 𝑟0 .
Consider the alternative wake-up pattern 𝑊 ′ = 𝑡′1 ≤ ⋅ ⋅ ⋅ ≤ 𝑡′𝑛 ,
where 𝑡′𝑖 = 𝑡𝑖 if 𝑖 < 𝑖0 and otherwise 𝑡′𝑖 = 𝑡𝑖 − 1. Let us use
𝑃 (𝑡), 𝑃 ′ (𝑡) to denote the sum of the probabilities induced by wakeup patterns 𝑊 and 𝑊 ′ in round 𝑡, respectively. Further, let 𝑝𝑥 (𝑡)
be the sending probability in round 𝑡 of a node that first receives
the message in round 𝑥 (as defined in the algorithm). Because the
wake-up patterns 𝑊, 𝑊 ′ are the same up to round 𝑟0 − 2, we have
𝑃 (𝑡) = 𝑃 ′ (𝑡) for all 𝑡 < 𝑟0 . For 𝑡 ≥ 𝑟0 , we have

7.

L EMMA 10. Let 𝐵(𝑛) be the maximum number of busy rounds
induced by any wake-up pattern. Then there is a wake-up pattern
for which rounds 1, . . . , 𝐵(𝑛) are all busy.

RANDOMIZED UPPER BOUND

In this section we give a simple randomized algorithm for broadcast, which completes in 𝑂(𝑛 log2 𝑛) rounds with high probability.
We assume a directed communication graph and collision rule CR4,
the weakest rule.
The randomized algorithm we describe is symmetric: all processes execute the same algorithm (and in particular, they do not
use unique identifiers). For simplicity in notation, in this section
we assume that the processes are indexed by 1, . . . , 𝑛.

𝑃 ′ (𝑡) =

𝑖=1

Algorithm Harmonic Broadcast Nodes begin partic-

≥

ipating immediately after they receive the message. If node
𝑣 receives the broadcast message for the first time in round
𝑡𝑣 , then in all rounds 𝑡 > 𝑡𝑣 it transmits the message with
probability 𝑝𝑣 (𝑡), given by
∀𝑡 > 𝑡𝑣

:

𝑝𝑣 (𝑡) :=

𝑛
∑

𝑛
∑

𝑖0 −1

𝑝𝑡′𝑖 (𝑡) =

∑
𝑖=1

𝑝𝑡𝑖 (𝑡) +

𝑛
∑

𝑝𝑡𝑖 (𝑡 + 1)

𝑖=𝑖0

𝑝𝑡𝑖 (𝑡 + 1) = 𝑃 (𝑡 + 1).

𝑖=1

Therefore, if round 𝑡 > 𝑟0 is busy for 𝑊 , then round 𝑡 − 1 is busy
for 𝑊 ′ , and the total number of busy rounds in 𝑊 ′ is at least the
same as in 𝑊 . Furthermore, round 𝑟0 (which was free for 𝑊 ) is
busy for 𝑊 ′ , because round 𝑟0 + 1 is busy for 𝑊 . It follows that
𝑊 ′ has fewer free rounds before the last busy round than 𝑊 does,
but it has at least as many busy rounds, contradicting the choice
of 𝑊 . (Recall that 𝑊 was chosen to be a wake-up pattern that
maximizes the total number of busy slots, and among these wakeup patterns, minimizes the number of free time slots before the last
busy slot.)

1
.
1 + ⌊ 𝑡−𝑡𝒯𝑣 −1 ⌋

Hence, for the first 𝒯 rounds after receiving 𝑚, nodes transmit the
message with probability 1; in the next 𝒯 rounds the message is
transmitted with probability 1/2, then the probability becomes 1/3,
and so on. The parameter 𝒯 ≥ 1 in the algorithm is an integer
parameter that will be fixed later. For 𝑡 ≤ 𝑡𝑣 , we define 𝑝𝑣 (𝑡) := 0.
For convenience, we assume that the sender 𝑠 receives 𝑚 at time 0,
i.e., 𝑡𝑠 = 0 and 𝑠 starts broadcasting 𝑚 in round 1.
Analysis. For every 𝑡 ≥ 1, we define
∑
𝑃 (𝑡) :=
𝑝𝑣 (𝑡)
(2)

The following lemma bounds the total number of busy rounds
induced by any wake-up pattern.
L EMMA 11. The total number of busy rounds for any wake-up
pattern is at most 𝑛 ⋅ 𝒯 ⋅ 𝐻(𝑛).

𝑣∈𝑉

P ROOF. Consider an arbitrary 𝑛-node wake-up pattern 𝑊 =
𝑡1 ≤ ⋅ ⋅ ⋅ ≤ 𝑡𝑛 . We show that there has to be
∑a𝑛 free round by time
𝑡𝑓 (𝑛) := 𝑛 ⋅ 𝒯 ⋅ 𝐻(𝑛) where 𝐻(𝑛) =
𝑖=1 1/𝑖, 𝐻(0) = 1
denotes the harmonic sum. Together with Lemma 10, this implies
the claim.
We prove that there is a free time round by time 𝑡𝑓 (𝑛) by induction on 𝑛. For 𝑛 = 1 the claim is immediate.

to be the sum of the transmitting probabilities in round 𝑡. We say
that round 𝑡 is busy if 𝑃 (𝑡) ≥ 1, and otherwise we say that round 𝑡
is free.
We begin by bounding the number of busy rounds in any execution from above. We define the wake-up pattern of an execution
to be a non-decreasing sequence 𝑊 = 𝑡1 ≤ 𝑡2 ≤ ⋅ ⋅ ⋅ ≤ 𝑡𝑛 of

343

𝒯
. Let 𝑞 be the probability that there
isolated is larger than 4(𝒯 +𝜏
−1)
′
is no free round 𝑡 ∈ [𝑡𝑣 +1, 𝑡] in which 𝑣 transmits alone. As there
are at least ⌈𝜏 /2⌉ free rounds, the probability 𝑞 is bounded by
(
)⌈𝜏 /2⌉
𝒯 ⋅𝜏
𝒯
−
𝑞 < 1−
< 𝑒 8(𝒯 +𝜏 −1)
4(𝒯 + 𝜏 − 1)
12 ln(𝑛/𝜖)
𝒯 2
𝜖
< 𝑒− 8 ⋅ 3 ≤ 𝑒− 12
= .
𝑛

Thus, let 𝑛 > 1. For 𝑖 ∈ [𝑛], let 𝑣𝑖 be the node that wakes up
(receives the message) at time 𝑡𝑖 , and let 𝜏𝑖 be the first free round
when using the 𝑖-node wake-up pattern 𝑡1 , . . . , 𝑡𝑖 (that is, the prefix
of 𝑊 in which nodes 𝑣𝑖+1 , . . . , 𝑣𝑛 are never awakened). By the
induction hypothesis, 𝜏𝑖 ≤ 𝑡𝑓 (𝑖) for all 𝑖 < 𝑛. We want to show
that 𝜏𝑛 ≤ 𝑡𝑓 (𝑛).
Let us first consider the case where 𝑡𝑖+1 ≥ 𝜏𝑖 for some 𝑖 ∈
[𝑛 − 1]. In this case, round 𝜏𝑖 remains free when we consider the
complete wake-up pattern 𝑊 ; thus, 𝜏𝑛 = 𝜏𝑖 ≤ 𝑡𝑓 (𝑖) ≤ 𝑡𝑓 (𝑛).
Next, consider the case where 𝑡𝑖+1 ≤ 𝜏𝑖 − 1 ≤ 𝑡𝑓 (𝑖) − 1 for all
𝑖 ∈ [𝑛 − 1]. For any 𝑖 ∈ [𝑛], at time 𝑡𝑓 (𝑛), the sending probability
of node 𝑣𝑖 is
)
(
1
⌊
⌋
𝑝𝑣𝑖 𝑡𝑓 (𝑛) =
𝑡𝑓 (𝑛)−𝑡𝑖 −1
1+
𝒯
1

≤
1+

⌊

𝑡𝑓 (𝑛)−(𝑡𝑓 (𝑖−1)−1)−1
𝒯

⌋ <

The first inequality follows from Lemma 12 and from (3); the second inequality follows because for all 𝑥 ∈ ℝ we have (1 − 𝑥) <
𝑒−𝑥 . Finally, the third and fourth inequalities follow from 𝜏 ≥ 2𝒯
and from the fact that 𝒯 ≥ 12 ln(𝑛/𝜖), respectively.
Finally, we are ready to prove the main theorem, showing that
the broadcast completes in 𝑂(𝑛 log2 𝑛) rounds with probability at
least 1 − 𝑛−𝑂(1) .

1
.
(𝑛 − 𝑖 + 1)𝐻(𝑛)

T HEOREM 14. If 𝒯 = ⌈12 ln(𝑛/𝜖)⌉ for some 𝜖 > 0, the algorithm solves broadcast by time 𝑇 = 2⋅𝑛⋅𝒯 ⋅𝐻(𝑛) with probability
at least 1 − 𝜖. For 𝜖 = 𝑛−𝑂(1) , we get 𝑇 = 𝑂(𝑛 log2 𝑛).

For the sum of transmitting probabilities, we therefore obtain
𝑛
𝑛
(
) ∑
(
) ∑
𝑃 𝑡𝑓 (𝑛) =
𝑝𝑣𝑖 𝑡𝑓 (𝑛) <
𝑖=1

=

𝑖=1

1
(𝑛 − 𝑖 + 1)𝐻(𝑛)

P ROOF. For any node 𝑣, let 𝑡𝑣 be the round in which 𝑣 first
receives the message, or ∞ if 𝑣 never receives the message. Let
𝑡′𝑣 be the first round after 𝑡𝑣 in which the number of free rounds
greater than 𝑡𝑣 is equal to the number of busy rounds after 𝑡𝑣 . By
Lemma 13, node 𝑣 has been isolated by round 𝑡𝑣 with probability
at least 1 − 𝜖/𝑛. By a union bound argument, the probability that
every node 𝑣 has been isolated by 𝑡′𝑣 (assuming 𝑡′𝑣 is finite) is at
least 1 − 𝜖. We will show that whenever this event occurs, all nodes
receive the message before the first time in which the total number
of free rounds in the execution equals the total number of busy
rounds. Together with Lemma 11, this proves the theorem.
Let 𝜏 be the first round in which over the entire interval [1, 𝜏 ],
the number of free rounds equals the number of busy rounds, and
suppose by way of contradiction that every node 𝑣 was isolated no
later than round 𝑡′𝑣 (if round 𝑡′𝑣 is finite) but some node has not
received the message. Let 𝑈 ⊆ 𝑉 be the non-empty set of nodes
that have not received the message by round 𝜏 − 1. Since 𝐺 is
broadcastable, there exists a directed edge (𝑣, 𝑢) where 𝑢 ∈ 𝑈 and
𝑣 ∈ 𝑉 ∖ 𝑈 . If we can show that 𝑡′𝑣 ≤ 𝜏 , then by our assumpion that
𝑣 is isolated by round 𝑡′𝑣 , process 𝑢 receives the message by round
𝜏 , contradicting the choice of 𝑢.
To that end, assume by way of contradiction that 𝑡′𝑣 > 𝜏 (or 𝑡′𝑣 is
infinite), that is, the number of free rounds in the interval [𝑡𝑣 , 𝜏 ] is
smaller than the number of busy rounds. By choice of 𝜏 we know
that the number of free rounds in the interval [1, 𝜏 ] is at least the
number of busy rounds in the interval [1, 𝜏 ]. It follows that the
number of free rounds in [1, 𝑡𝑣 ] exceeds the number of busy rounds
in [1, 𝑡𝑣 ], contradicting the minimality of 𝜏 .

𝐻(𝑛)
= 1.
𝐻(𝑛)

Hence, round 𝑡𝑓 (𝑛) is free, as required.
We say that a process is isolated in a round if it is the only process transmitting in that round. In the following, we show that a
process that broadcasts in a free round is isolated with high probability, and that as soon as the number of free rounds since a process
received the message is large enough, that process is isolated with
high probability.
L EMMA 12. Let 𝑡 ≥ 1 be a free round and assume that node 𝑣
transmits in round 𝑡 with probability 𝑝𝑣 (𝑡). The probability that 𝑣
is isolated in round 𝑡 is at least 𝑝𝑣 (𝑡)/4.
P ROOF. Because 𝑡 is a free round, all transmitting probabilities
are smaller than 1 and thus for all 𝑢 ∈ 𝑉 we have 𝑝𝑢 (𝑡) ≤ 1/2.
Let 𝑞 be the probability that none of the nodes in 𝑉 ∖ {𝑣} send in
round 𝑡. We have
∏ (
∏ ( 1 )𝑝𝑢 (𝑡) ( 1 )𝑃 (𝑡)
)
1
>
> .
𝑞=
1−𝑝𝑢 (𝑡) ≥
4
4
4
𝑢∈𝑉 ∖{𝑣}

𝑢∈𝑉 ∖{𝑣}

In the last two steps we used the fact that for 0 ≤ 𝑥 ≤ 1/2 it
holds that 1 − 𝑥 ≥ (1/4)𝑥 , and that 𝑃 (𝑡) < 1, because 𝑡 is a free
round. The probability that 𝑣 is isolated in round 𝑡 is 𝑝𝑣 (𝑡) ⋅ 𝑞 >
𝑝𝑣 (𝑡)/4.
L EMMA 13. Consider a node 𝑣, and let 𝑡𝑣 be the time when 𝑣
first receives the message. Further, let 𝑡 > 𝑡𝑣 be such that at least
half of the rounds 𝑡𝑣 + 1, . . . , 𝑡 are free. If 𝒯 ≥ 12 ln(𝑛/𝜖) for
some 𝜖 > 0, then with probability larger than 1 − 𝜖/𝑛 there exists
a round 𝑡′ ∈ [𝑡𝑣 + 1, 𝑡] such that 𝑣 is isolated in round 𝑡′ .

8.

P ROOF. Let 𝜏 = 𝑡−𝑡𝑣 . Note that 𝜏 ≥ 2𝒯 because 𝑣 sends with
probability 1 in the first 𝒯 rounds (a nd hence the first 𝒯 rounds are
not free). In round 𝑡, the transmitting probability of 𝑣 is
𝑝𝑣 (𝑡) =

1+

1
⌊ 𝜏 −1 ⌋ ≥
𝒯

1
𝒯
=
.
𝒯 +𝜏 −1
1 + 𝜏 𝒯−1

CONCLUSION

In this paper we introduce dual graphs, a new model for radio
networks. Unlike most traditional models for radio networks, the
dual graph model allows for dynamic interference and unreliable
communication. Like traditional models, the dual graph model includes a graph 𝐺 of reliable communication links; but in addition,
unreliable links are represented in the form of a second graph 𝐺′ ,
whose edges can be deployed against the algorithm by a worstcase adversary. Algorithms for the dual graph model are therefore
resilient to interference and noise.
In the current paper we showed that for the broadcast problem,
resilience to link failures comes at the cost of higher round complexity: a lower bound of Ω(𝑛 log 𝑛) holds for a setting in which

(3)

Because the transmitting probability is non-increasing, by Lemma
12, for every free round 𝑡′ ∈ [𝑡𝑣 + 1, 𝑡], the probability that 𝑣 is

344

the reliable model admits an 𝑂(𝑛)-round deterministic algorithm.
√
Our deterministic upper bound, at 𝑂(𝑛3/2 log 𝑛) rounds, does
not yet match this lower bound; nevertheless, we gave reasonably
efficient deterministic and randomized algorithms for broadcast.
A significant part of the difficulty comes from the fact that the
network topology is unknown to the processes at the time of the
broadcast. In future work we intend to explore repeated broadcast
in dual graphs, where we hope to improve long-term efficiency by
learning the topology of the graph. Topology control in dual graphs
is another interesting area for future research.

9.

[15] F. Galčík. Centralized communication in radio networks with
strong interference. In SIROCCO ’08: Proceedings of the
15th international colloquium on Structural Information and
Communication Complexity, pages 277–290, Berlin,
Heidelberg, 2008. Springer-Verlag.
[16] F. Galčík, L. Gasieniec, and A. Lingas. Efficient broadcasting
in known topology radio networks with long-range
interference. In PODC ’09: Proceedings of the 28th ACM
symposium on Principles of distributed computing, pages
230–239, New York, NY, USA, 2009. ACM.
[17] O. Goussevskaia, T. Moscibroda, and R. Wattenhofer. Local
broadcasting in the physical interference model. In M. Segal
and A. Kesselman, editors, DIALM-POMC, pages 35–44.
ACM, 2008.
[18] P. Gupta and P. Kumar. The capacity of wireless networks.
IEEE Transactions on information theory, 46:388–404.
[19] W. Kautz and R. Singleton. Nonrandom binary superimposed
codes. IEEE Transactions on Information Theory,
10(4):363–377, 1964.
[20] D. R. Kowalski and A. Pelc. Broadcasting in undirected ad
hoc radio networks. Distrib. Comput., 18(1):43–57, 2005.
[21] D. R. Kowalski and A. Pelc. Time complexity of radio
broadcasting: adaptiveness vs. obliviousness and
randomization vs. determinism. Theor. Comput. Sci.,
333(3):355–371, 2005.
[22] F. Kuhn, N. Lynch, and C. Newport. Brief announcement:
Hardness of broadcasting in wireless networks with
unreliable communication. In The Annual ACM Symposiun
on Principles of Distributed Computing (PODC), pages
330–331, 2009.
[23] E. Kushilevitz and Y. Mansour. An Ω(𝐷 log(𝑁/𝐷)) lower
bound for broadcast in radio networks. SIAM J. Comput.,
27(3):702–712, 1998.
[24] H. Lundgren, E. Nordstr
"o, and C. Tschudin. Coping with Communication Gray
Zones in IEEE 802.11b Based Ad Hoc Networks. In the
International Workshop on Wireless Mobile Multimedia,
2002.
[25] T. Moscibroda and R. Wattenhofer. The complexity of
connectivity in wireless networks. In INFOCOM. IEEE,
2006.
[26] C. Newport, D. Kotz, Y. Yuan, R. Gray, J. Liu, and C. Elliott.
Experimental Evaluation of Wireless Simulation
Assumptions. Simulation, 83(9):643, 2007.
[27] C. Newport and N. Lynch. Modeling Radio Networks. In the
International Conference on Concurrency Theory
(CONCUR), 2009.
[28] D. Peleg. Time-efficient broadcasting in radio networks. In
DISC ’07: Proceedings of the 21st international symposium
on Distributed Computing, pages 3–4, Berlin, Heidelberg,
2007. Springer-Verlag.
[29] S. Schmid and R. Wattenhofer. Algorithmic models for
sensor networks. In IPDPS. IEEE, 2006.
[30] P. von Rickenbach, R. Wattenhofer, and A. Zollinger.
Algorithmic models of interference in wireless ad hoc and
sensor networks. IEEE/ACM Trans. Netw., 17(1):172–185,
2009.

REFERENCES

[1] N. Alon, A. Bar-Noy, N. Linial, and D. Peleg. A lower bound
for radio broadcast. J. Comput. Syst. Sci., 43(2):290–298,
1991.
[2] R. Bar-Yehuda, O. Goldreich, and A. Itai. On the
time-complexity of broadcast in radio networks: an
exponential gap between determinism randomization. In
PODC ’87: Proceedings of the sixth annual ACM
Symposium on Principles of distributed computing, pages
98–108, New York, NY, USA, 1987. ACM.
[3] D. Bruschi and M. Del Pinto. Lower bounds for the
broadcast problem in mobile radio networks. Distrib.
Comput., 10(3):129–135, 1997.
[4] K.-W. Chin, J. Judge, A. Williams, and R. Kermode.
Implementation Experience with MANET Routing
Protocols. SIGCOMM Computer Communication Review,
32(5):49–59, 2002.
[5] B. S. Chlebus, L. Gasieniec, A. Gibbons, A. Pelc, and
W. Rytter. Deterministic broadcasting in unknown radio
networks. In Symposium on Discrete Algorithms, pages
861–870, 2000.
[6] M. Chlebus, L. Gasieniec, A. Ostlin, and J. Robson.
Deterministic broadcasting in radio networks. In the
International Colloquium on Automata, Languages and
Programming (ICALP), 2000.
[7] M. Chrobak, L. Gasieniec, and W. Rytter. Fast broadcasting
and gossiping in radio networks. Journal of Algorithms,
43:177–189, 2002.
[8] A. Clementi, A. Monti, and R. Silvestri. Selective families,
superimposed codes, and broadcasting on unknown radio
networks. In the annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 709–718, Philadelphia, PA, USA,
2001. Society for Industrial and Applied Mathematics.
[9] A. Clementi, A. Monti, and R. Silvestri. Round robin is
optimal for fault-tolerant broadcasting on wireless networks.
Journal of Parallel Distributed Computing, 64:89–96, 2004.
[10] A. E. F. Clementi, A. Monti, F. Pasquale, and R. Silvestri.
Broadcasting in dynamic radio networks. J. Comput. Syst.
Sci., 75(4):213–230, 2009.
[11] A. E. F. Clementi, A. Monti, and R. Silvestri. Round robin is
optimal for fault-tolerant broadcasting on wireless networks.
J. Parallel Distrib. Comput., 64(1):89–96, 2004.
[12] A. Czumaj and W. Rytter. Broadcasting algorithms in radio
networks with unknown topology. J. Algorithms,
60(2):115–143, 2006.
[13] D. S. J. De Couto, D. Aguayo, J. Bicket, and R. Morris. A
High-Throughput Path Metric for Multi-Hop Wireless
Routing. Wireless Networks, 11(4):419–434, 2005.
[14] P. Erdos, P. Frankl, and Z. Furedi. Familes of finite sets in
which no set is covered by the union of 𝑟 others. Israel
Journal of Mathematics, 51:79–89, 1985.

345

Brief Announcement: Towards Robust
Medium Access in Multi-Hop Networks
Andrea Richa∗, Jin Zhang

Christian Scheideler†

Stefan Schmid

Dept. Computer Science
Arizona State University
Tempe, AZ, USA

Dept. Computer Science
University of Paderborn
Paderborn, Germany

TU Berlin / T-Labs
Berlin, Germany

{aricha,jzhang82}@asu.edu

scheideler@upb.de

ABSTRACT

that other nodes experience low throughput by not being
able to access the channel. In this paper we focus on jamming attacks on the physical layer, that is, the interference
caused by the jammer will not allow the nodes to receive
messages. The fundamental question that we are investigating is: Is there a MAC protocol so that for any physical-layer
jamming strategy, the protocol will still be able to achieve an
asymptotically optimal throughput for the non-jammed time
steps? Such a protocol would force the jammer to jam all
the time in order to prevent any successful message transmissions. Finding such a MAC protocol is not a trivial problem.
In fact, the widely used IEEE 802.11 MAC protocol already
fails to deliver any messages for very simple oblivious jammers that jam only a small fraction of the time steps [2]. On
the positive side, Awerbuch et al. [1] have demonstrated that
there are MAC protocols which are provably robust against
even powerful, adaptive jamming, but their results only hold
for single-hop wireless networks with a single jammer, that
is, all nodes experience the same jamming pattern.
In this paper, we significantly extend the results in [1].
We present a MAC protocol called Jade (a short form of
“jamming defense”) that can achieve a constant fraction of
the best possible throughput for a large class of jamming
strategies in a large class of multi-hop networks where transmissions and interference can be modeled using unit-disk
graphs.

This paper introduces the distributed MAC protocol Jade.
We consider a multi-hop wireless network with a single communication channel in which a powerful adversary is able to
jam (groups of) nodes individually and during a (1 − )fraction of the entire time, where  > 0 is an arbitrarily
small constant. Despite this harsh environment, Jade exploits the few non-jammed slots effectively and guarantees
a high throughput.

Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and Wide-Area Networks—Access Schemes; F.2.2
[Analysis of Algorithms and Problem Complexity]:
Nonnumerical Algorithms and Problems—Sequencing and
Scheduling

General Terms
Algorithms, Reliability, Theory

Keywords
Wireless Ad-hoc Networks, MAC Protocols, Jamming

1.

INTRODUCTION

Coordinating the access to a shared medium is a central
challenge in wireless networks. Ideally, a Medium Access
Control (MAC) protocol should not only be able to use
the wireless medium as effectively as possible, but it should
also be robust against attacks. Unfortunately, most of today’s MAC protocols can be easily attacked. A particularly
critical class of attacks are jamming attacks (i.e., denial-ofservice attacks on the broadcast medium). Jamming attacks
are typically easy to implement as the attacker does not need
any special hardware. Attacks of this kind usually aim at
the physical layer and are realized by means of a high transmission power signal that corrupts a communication link or
an area, but they may also occur at the MAC layer, where
an adversary may either corrupt control packets or reserve
the channel for the maximum allowable number of slots so
∗
†

stefan@net.t-labs.tuberlin.de

2.

MODEL

We consider the problem of designing a robust MAC protocol for multi-hop wireless networks with a single wireless
channel. The wireless network is modeled as a unit disk
graph (UDG) G = (V, E) where V represents a set of n = |V |
honest and reliable nodes and two nodes u, v ∈ V are within
each other’s transmission range, i.e., {u, v} ∈ E, if and only
if their (normalized) distance is at most 1. We assume that
time proceeds in synchronous time steps called rounds. In
each round, a node may either transmit a message or sense
the channel, but it cannot do both. A node which is sensing
the channel may either (i) sense an idle channel (if no other
node in its transmission range is transmitting at that round
and its channel is not jammed), (ii) sense a busy channel (if
two or more nodes in its transmission range transmit at that
round or its channel is jammed), or (iii) receive a packet (if
exactly one node in its transmission range transmits at that
round and its channel is not jammed).
In addition to these nodes there is an adversary (who may
control any number of jamming devices). We allow the adversary to know the protocol and its entire history and to use

Supported by NSF Award number CCF-0830704.
Supported by DFG grant SCHE 1592/1-1.

Copyright is held by the author/owner(s).
PODC’10, July 25–28, 2010, Zurich, Switzerland.
ACM 978-1-60558-888-9/10/07.

114

this knowledge in order to jam the wireless channel at will
at any round (i.e, the adversary is adaptive). However, like
in [1], the adversary has to make a jamming decision before
it knows the actions of the nodes at the current round. The
adversary can jam the nodes individually at will, as long as
for every node v, at most a (1 − )-fraction of its rounds is
jammed, where  > 0 can be an arbitrarily small constant.
That is, v has the chance to receive a message in at least
an -fraction of the rounds. More formally, an adversary is
called (T, 1 − )-bounded for some T ∈ N and 0 <  < 1, if
for any time window of size w ≥ T and at any node v, the
adversary can jam at most (1 − )w of the w rounds at v.
In order to investigate the issue of multiple jammers in
more detail, we also introduce the notion of a k-uniform
adversary. An adversary is k-uniform if the node set V can
be partitioned into k subsets so that the jamming pattern is
the same within each subset.
Given a node v and a time interval I, we define fv (I) as
the number of time steps in I that are non-jammed at v and
sv (I) as the number of time steps in I in which v successfully
receives a message. A MAC protocol is called c-competitive
against some (T, 1 − )-bounded adversary if, for any time
interval I with |I| ≥ K
P large K (that may
P for a sufficiently
depend on T and n), v∈V sv (I) ≥ c · v∈V fv (I). In other
words, a c-competitive MAC protocol can achieve at least a
c-fraction of the best possible throughput.
Our goal is to design a symmetric local-control MAC protocol (i.e., there is no central authority controlling the nodes,
and all the nodes are executing the same protocol) that has
a constant-competitive throughput against any (T, 1 − )bounded adversary in any multi-hop network that can be
modeled as a UDG.
In this paper, we will say that a claim holds with high
probability (w.h.p.) iff it holds with probability at least 1 −
1/nc for any constant c ≥ 1

the past Tv rounds in which v sensed a successful message
transmission or an idle channel, then pv := (1 + γ)−1 pv and
Tv := Tv + 1.
The concept of using a multiplicative-increasemultiplicative-decrease mechanism for pv and an additiveincrease-additive-decrease mechanism for Tv , as well as the
slight addition to the protocol in [1], marked in italic above,
are crucial for Jade to work.

4.

RESULTS

Jade has the following properties.
Theorem 4.1. When considering a time interval of
Ω((T log n)/ + poly(log n, 1/)) length, Jade has a constant
competitive throughput for any (T, 1 − )-bounded adversary
and any UDG w.h.p. as long as γ = O(1/(log T + log log n))
and (a) the adversary is 1-uniform and the UDG is connected, or (b) there are at least 2/ nodes within the transmission range of every node.
Note that in reality, log T and log log n should be rather
small so that our condition on γ is not too restrictive.
Interestingly, the analysis of Theorem 4.1 can be reduced
to the study of a slightly weaker form of adversary. Let us
call a round t open for a node v if v and at least one other
node in its neighborhood are non-jammed (which implies
that v’s neighborhood is non-empty). An adversary is weakly
(T, 1 − )-bounded for some T ∈ N and 0 <  < 1 if the
adversary is (T, 1 − )-bounded and in addition to this, at
least a constant fraction of the non-jammed rounds at each
node are open in every time interval of size T . We can prove
the following theorem.
Theorem 4.2. When running Jade for Ω([T +
(log3 n)/(γ 2 )] · (log n)/) rounds it holds w.h.p. that Jade
is constant competitive for any weakly (T, 1 − )-bounded
adversary.

3.

THE JADE PROTOCOL
Jade is a fairly simple protocol: it is based on a very small
set of rules and has a minimal storage overhead. We believe
that these properties make our protocol interesting for a real
deployment. In contrast to the algorithm, our stochastic
analysis is rather involved as it requires to shed light onto
the complex interplay between the nodes all following their
randomized protocol in a highly dependent manner and an
adversary.
In Jade, each node v maintains a probability value pv ,
a threshold Tv and a counter cv . The parameter γ < 1 is
the same for every node and is set to some sufficiently small
value (that will be specified below). Let p̂ be any constant
so that 0 < p̂ ≤ 1/24.

We can also show that Theorem 4.1 essentially captures
all the scenarios (within our notation) under which Jade
can have a constant competitive throughput.
Theorem 4.3. If (a) the UDG is not connected, or (b)
the adversary is allowed to be 2-uniform and there are nodes
with o(1/) nodes within their transmission range, then there
are cases in which Jade is not constant competitive for a
constant c independent of .
Certainly, no MAC protocol can guarantee a constant
competitive throughput if the UDG is not connected. However, it is still an open question whether there are simple
MAC protocols that are constant competitive under nonuniform jamming strategies even if there are o(1/) nodes
within the transmission range of a node.

Initially, every node v sets Tv := 1, cv := 1 and pv := p̂.
Afterwards, the protocol works in synchronized rounds. In
every round, each node v decides with probability pv to send
a message. If it decides not to send a message, it checks the
following two conditions:

5.

REFERENCES

[1] B. Awerbuch, A. Richa, and C. Scheideler. A
jamming-resistant MAC protocol for single-hop wireless
networks. In Proc. of PODC ’08, pages 45–54, 2008.
[2] E. Bayraktaroglu, C. King, X. Liu, G. Noubir,
R. Rajaraman, and B. Thapa. On the performance of
IEEE 802.11 under jamming. In Proc. of IEEE Infocom
’08, pages 1265–1273, 2008.

• If v senses an idle channel, then pv := min{(1 +
γ)pv , p̂}.
• If v successfully receives a message, then pv := (1 +
γ)−1 pv and Tv := max{Tv − 1, 1}.
Afterwards, v sets cv := cv + 1. If cv > Tv then it does the
following: v sets cv := 1, and if there was no round among

115

SKIP+ : A Self-Stabilizing Skip Graph
RIKO JACOB, Eidgenössische Technische Hochschule Zürich
ANDREA RICHA, Arizona State University
CHRISTIAN SCHEIDELER, Universität Paderborn
STEFAN SCHMID, T-Labs and Technische Universität Berlin
HANJO TÄUBIG, Technische Universität München

Peer-to-peer systems rely on a scalable overlay network that enables efficient routing between its members.
Hypercubic topologies facilitate such operations while each node only needs to connect to a small number
of other nodes. In contrast to static communication networks, peer-to-peer networks allow nodes to adapt
their neighbor set over time in order to react to join and leave events and failures. This article shows how
to maintain such networks in a robust manner. Concretely, we present a distributed and self-stabilizing
algorithm that constructs a (slightly extended) skip graph, SKIP+ , in polylogarithmic time from any given
initial state in which the overlay network is still weakly connected. This is an exponential improvement
compared to previously known self-stabilizing algorithms for overlay networks. In addition, our algorithm
handles individual joins and leaves locally and efficiently.
Categories and Subject Descriptors: C.2.2 [Computer-Communication Networks]: Network Protocols
General Terms: Design, Algorithms, Performance
Additional Key Words and Phrases: Distributed algorithms, distributed systems, robustness, churn, peer-topeer systems, self-stabilization, graph theory, overlay networks, performance
ACM Reference Format:
Riko Jacob, Andrea Richa, Christian Scheideler, Stefan Schmid, and Hanjo Täubig. 2014. SKIP+ : A selfstabilizing skip graph. J. ACM 61, 6, Article 36 (November 2014), 26 pages.
DOI: http://dx.doi.org/10.1145/2629695

1. INTRODUCTION

Peer-to-peer computing is one of the most intriguing networking paradigms of the last
decade. Numerous Internet applications make use of peer-to-peer technology, including
file sharing, streaming and gaming tools. A distinguishing feature of these networks
is that they typically have an open clientele, allowing machines to join and leave at
any time and concurrently. If no countermeasures are taken, the dynamic membership changes can degenerate the network, rendering key operations such as routing
A preliminary version of this work was presented at the 28th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing (PODC).
This work is partly supported by the National Science Foundation (NSF grant CCF-0830704), the German
Research Foundation (DFG project SCHE 1592/1-1 and SFB 901: On-the-Fly Computing), and GIF Research
Grant No. I-1245-407.6/2014.
Authors’ addresses: R. Jacob, Department of Computer Science, ETH Zürich, Universitätstr. 6, 8092
Zürich, Switzerland; email: rjacob@inf.ethz.ch; A. Richa, Department of Computer Science and Engineering,
Arizona State University, Box 878809, Tempe, AZ 85287-8809; email: aricha@asu.edu; C. Scheideler, Department of Computer Science, Universität Paderborn, Fürstenallee 11, 33102 Paderborn, Germany; email:
scheideler@upb.de; S. Schmid, T-Labs, TU Berlin, Ernst-Reuter-Platz 7, 10587 Berlin, Germany; email:
stefan@net.t-labs.tu-berlin.de; H. Täubig, Department of Computer Science, TU München, Boltzmannstr. 3,
85748 Garching, Germany; email: taeubig@in.tum.de.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by
others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
c 2014 ACM 0004-5411/2014/11-ART36 $15.00

DOI: http://dx.doi.org/10.1145/2629695
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36

36:2

R. Jacob et al.

inefficient. In an effort to gain deeper insights into (and deal with) these dynamics,
researchers have studied different approaches. In theoretical studies, the dominating
approach has been to make sure that an overlay network never leaves a certain set of
legal states so that at any time, information can be efficiently exchanged between its
members. This is mostly achieved through redundancy in the overlay network topology,
which can significantly increase its maintenance overhead. The rate of changes such
networks can sustain is usually rather limited. However, a high change rate can happen due to heavy churn or join-leave attacks. Also, partitions of the underlying physical
network or DoS attacks may push the overlay network into some illegal state. In this
case, the overlay network may get vulnerable to further changes, and proper recovery
mechanisms are needed to get it back into a legal state as quickly as possible. Some
results are known in this direction, but most of the proposed protocols only manage to
recover the network from a restricted class of illegal states (e.g., Angluin et al. [2005],
Aspnes and Wu [2007], and Stoica et al. [2001]). Those few results known for truly
self-stabilizing networks either just show eventual self-stabilization without any time
bounds (e.g., Clouser et al. [2008]) or only provide linear or superlinear bounds on the
convergence time (e.g., Gall et al. [2010] and Onus et al. [2007]).
Our work is the first that demonstrates that sublinear, in fact, polylogarithmic recovery time is possible. More precisely, we present a fast self-stabilizing algorithm for
a proper extension of the skip graph [Aspnes and Shah 2007], as the topological consistency of the original skip graph is not locally checkable. Skip graphs are very useful for
scalable overlay networks. They have logarithmic diameter and degree (with respect
to the number of nodes in the network) and a constant expansion with high probability
(w.h.p.) [Aspnes and Wieder 2005]. Also, like in hypercubic networks, no extra routing
tables have to be maintained for fast, low congestion routing. Before we delve into the
details of our solution, we discuss related work and present our model.
1.1. Related Work

There is a large body of literature on how to maintain peer-to-peer networks efficiently,
for example, Awerbuch and Scheideler [2004], Bhargava et al. [2004], Druschel and
Rowstron [2001], Harvey et al. [2003], Kuhn et al. [2005], Malkhi et al. [2002], Naor
and Wieder [2003], Ratnasamy et al. [2001], Scheideler and Schmid [2009], and Stoica
et al. [2001]. Structured overlay networks have also found their way into the practical
world; for instance, the Kademlia [Maymounkov and Mazières 2002] overlay network is
used in the popular Kad network which can be accessed with eMule clients, and also for
BitTorrent similar extensions exist. An interesting and flexible overlay structure are
skip graphs [Awerbuch and Scheideler 2003; Goodrich et al. 2006; Harvey and Munro
2004; Harvey et al. 2003]. These networks are based on the classical skip list data
structure and allow for efficient, low-congestion routing while requiring only a small
node degree. Due to the typically very dynamic nature of peer-to-peer systems, there
is a need to maintain the overlay topology or, in case of catastrophic events, recover it
from arbitrary connected states. While many results are already known on how to keep
an overlay network in a legal state, not much is known about self-stabilizing overlay
networks.
In the field of self-stabilization, researchers are interested in algorithms that are
guaranteed to eventually converge to a desirable system state from any initial configuration. The idea of self-stabilization in distributed computing first appeared in a
classical paper by E. W. Dijkstra in 1974 [Dijkstra 1974] who studied the problem of
self-stabilization in a token ring. Since then, self-stabilization has been considered in
many contexts, including communication protocols, graph theory problems, termination detection, clock synchronization, and fault containment. For an overview, see, for
example, Brzezinski and Szychowiak [2000], Dolev [2000], and Herman [2002].
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:3

Also general techniques for self-stabilization have been considered. Awerbuch and
Varghese [1991] showed that every local algorithm can be made self-stabilizing if all
nodes keep a log of the state transitions until the current state. The convergence time
equals the execution time of the local algorithm: any deterministic, synchronous local algorithm whose running time is T synchronous communication rounds can be
converted into a self-stabilizing algorithm that stabilizes in time T ; see Lenzen et al.
[2009] for an example. Since then several other methods have emerged including various local and global checking and correction techniques [Awerbuch et al. 1991, 1994;
Costello and Varghese 1996; Katz and Perry 1993; Varghese 1992, 1994]. Also so-called
time-adaptive techniques [Kutten and Patt-Shamir 1997; Kutten and Peleg 1995] as
well as local stabilizers [Afek and Dolev 2002] have been presented which can recover
any distributed algorithm in O( f ) time depending only on the number f of faults. This,
however, does not hold any more if faults include changes in the topology. In this case,
a single fault may require the involvement of all nodes in the system and is therefore
quite expensive to fix. Thus, researchers have also looked at so-called superstabilizing protocols, which are protocols that can handle a single topology change as well
as arbitrary state faults (e.g., corrupted variables) with low overhead (e.g., Dolev and
Herman 1997).
Interestingly, though self-stabilizing distributed computing has been in the limelight
for many years, even in the context of dynamic networks, the problem of designing
self-stabilizing networks has attracted much less attention. The general techniques
mentioned previously are not applicable here as they have not been designed to actively perform local topology changes (network changes are only considered as faults
or dynamics not under the control of the algorithm). Even though logging techniques
such as Awerbuch and Varghese [1991] to convert non-self-stabilizing algorithms into
self-stabilizing algorithms can also be applied to self-stabilizing networks, they usually need some non-local knowledge of the network (such as its size) to bound the state
space which can make self-stabilization very expensive. Our goal instead was to find
dedicated, much more lightweight algorithms for self-stabilizing networks.
Some preliminary work in this direction has already been done. In the technical
report of the Chord network [Stoica et al. 2001], protocols are described which allow
the topology to recover from certain degenerate states. It is also known how to repair skip graphs from certain degenerate states [Aspnes and Shah 2003, 2007] but
the problem of recovering them from an arbitrary connected state has remained open.
This is not surprising as the neighborhood information alone is not sufficient for the
Chord network as well as skip graphs to locally verify the correctness of the topology.
Hence, additional information would be needed, which significantly complicates the
self-stabilization process. Finally, in this context, we also want to mention the interesting work by Aspnes and Wu [2007] who present a self-stabilizing algorithm to construct
a balanced search tree for the case that nodes initially have out-degree 1.
In order to recover scalable overlays from any initial graph, researchers have started
with simple line, ring, or prefix-tree networks [Caron et al. 2008]. The Iterative Successor Pointer Rewiring Protocol [Cramer and Fuhrmann 2005] and the Ring Network [Shaker and Reeves 2005] organize the nodes in a sorted ring. The runtime of
both protocols is rather large. Angluin et al. [2005] describe an asynchronous protocol which turns an initially weakly connected graph into a sorted list. Unfortunately,
their algorithm is not self-stabilizing. Onus et al. [2007] present a local-control strategy
called linearization for converting an arbitrary connected graph into a sorted list. However, the algorithm may need a linear number of communication rounds. Clouser et al.
[2008] formulated a variant of the linearization technique for asynchronous systems
in order to design a self-stabilizing skip list. Gall et al. [2010] combined the ideas from
Clouser et al. [2008] and Onus et al. [2007] and introduced a model that captures the
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:4

R. Jacob et al.

parallel time complexity of a distributed execution that avoids bottlenecks and contention. Two algorithms are presented together with an analysis of their distributed
runtime in different settings. No sublinear time bounds are shown there either. Recently, Jacob et al. [2009b] generalized insights gained from graph linearization to
two dimensions, and presented a self-stabilizing O(n3 )-time construction for Delaunay
graphs.
To the best of our knowledge, this is the first article to describe a self-stabilizing
algorithm for a scalable overlay network (in our case, a variant of skip graphs) in
sublinear time. In fact, the skip graph construction terminates after a polylogarithmic
number of communication rounds. In addition to the quick recovery from an arbitrary
connected state, we show that if the network forms the desired topology, our algorithm
efficiently supports join and leave events, which only entail a polylogarithmic amount
of work. This means (in contrast to considering a completely new starting situation
and recovering the structure in polylogarithmic time) that only a small, “local” part of
the nodes are involved in repairing the overlay topology.
The original conference publication of this work [Jacob et al. 2009a] has led to interesting follow-up papers. The closest work to ours is Re-Chord [Kniesburges et al.
2011] which also describes local self-stabilizing algorithms for hypercubic graphs (see
also Berns et al. [2011]), namely for the Chord topology. The Chord topology turns
out to be more complicated to maintain than SKIP+ , and indeed, the algorithm has a
polynomial complexity.
1.2. Model

An overlay network is represented as a directed graph G = (V, E) with n = |V | nodes
and directed links E. The set E does not contain any parallel edges, but we make
frequent use of anti-parallel (or bidirected) edges. The semantics of an edge (u, v) is
topological, that is, information can only be transmitted in the direction of the edge.
In the context of peer-to-peer computing, a directed edge (u, v) ∈ E means that peer u
knows the IP address of peer v. Given an edge (u, v), we will call u an in-neighbor of v
and similarly, v an out-neighbor of u. Moreover, we will sometimes refer to the node u
of an edge (u, v) as the source of that edge. If a link is bidirected, that is, both directed
links (u, v) and (v, u) exist, we may simply write {u, v}.
For ease of presentation, in the following, we will often refer to the out-neighbors
simply as the neighbors. In particular, we define the neighborhood N(u) of a node u as
the set of out-neighbors of u; |N(u)| is the (out-)degree of u.
Each node is assumed to have a unique identifier (abbreviated: ID) v.id ∈ U that is
immutable (i.e., fixed), where U is an ordered universe of all identifiers. At any time,
each node knows its own state, which consists of its own ID, and the value of all its
variables, including the IDs of all its out-neighbors. Beyond that, a node does not know
anything, not even the current size n of the overlay network.
However, as we will see, each node u in our algorithm will always send its own state
along each link (u, v), and hence an out-neighbor v will also know about its in-neighbors
and their states. Upon learning about an in-neighbor u, a node v always adds the link
(v, u), essentially making the link bidirected, and sends its state to u.
The view and the influence of a node are inherently local: A node can only perform local topological changes, that is, a node may decide to cut a link to a neighbor or ask two
of its neighbors to establish a link between themselves. The decisions to cut or establish
links are controlled through rules (also referred to as actions in the literature). A rule
is called enabled if and only if its Boolean predicate (the precondition) is fulfilled. This
predicate is computed over the state of a given node v, that is, the variables of v and
v’s out-neighbors. A rule specifies a set of commands which are executed if the predicate is true. Such a command may change the state (i.e., variables) of the executing
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:5

node v, request the insertion of a new edge between two nodes u and w in the neighborhood of v (insert(u, w), where w may be v), or request the deletion of an edge in v’s
neighborhood.
The algorithm presented in this article will never delete any edges without replacements. Rather, our algorithm is based on the concept of edge forwarding (essentially
a combination of an insertion and a deletion): an edge (v, w) can be forwarded by
node v to some out-neighbor u of v (forward(u, w)), and is consequently replaced by the
edge (u, w). If such an edge (u, w) already exists, to avoid parallel edges, only one of
the two edges is kept. This approach of avoiding deletions ensures that connectivity is
maintained, despite the local view of the nodes.
We will describe and analyze our self-stabilizing algorithm in the standard synchronous LOCAL model [Peleg 2000] of distributed computing. In the LOCAL model,
time is divided into synchronous communication rounds. In each round, each node u
can send a message to each neighbor v (reachable via a directed link (u, v)). All messages or requests generated in round i are reliably delivered at the beginning of round
i + 1. The message size is not restricted.
Our goal is to minimize the number of communication rounds needed in the worst
case until the overlay network has reached its desired structure. We need the following
minimal requirement on the initial network state at time t0 : Consider the (directed)
network G0 = (V0 , E0 ) at t0 , where E0 consists of the valid directed links between two
nodes in V0 (i.e., the links given by the neighbor IDs locally stored at the nodes V0 ), as
well as the links (u, v) implied by pending insertion requests (u, v). (As we will see, our
algorithm will never send any deletion requests.) We require that the resulting graph
G0 is weakly connected. From t0 onwards, no more external topological changes, such
as peer joins and leaves or variable corruptions, are allowed: the topology changes only
according to the self-stabilizing algorithm.
Let us now make the notion of a (topological) self-stabilizing algorithm more formal.
When using the synchronous message-passing model, the global state of the system
at the beginning of each round is well-defined. A computation is a sequence of states
such that for each state si at the beginning of round i, the next state si+1 is obtained
after executing all rules that were enabled in round i. (Rules which are enabled as a
consequence of the execution of some rules in round i are not executed in that round;
similarly, enabled rules that would be disabled by some rule in round i are executed
nevertheless.)
A distributed algorithm is called self-stabilizing if and only if (1) from any given
initial state, the algorithm eventually reaches a legal state: This is known as the
convergence criterion of the self-stabilizing algorithm. Moreover, (2) provided that no
external changes occur (i.e., neither topological changes nor unreliable communications), the system stays in a legal state in the future; this property is called the closure
criterion.
In the context of topological self-stabilization, a legal state defines a desired topology;
in our case, a variant of a skip graph. In fact, given an initial state, there is only one
legal state in our case: the state describes the unique desirable skip graph. The selfstabilizing algorithm needs to reach this state from any given topology in the absence
of external changes (in our case: additions and deletions of nodes or links). In the
remainder of this article, we will use the terms legal and desirable as synonyms.
As we will see, we will prove correctness of our algorithm for any weakly connected initial topology. Note that a weakly connected initial topology is the minimal requirement
from which a distributed algorithm can reach a legal state, as otherwise the topology
is disconnected. In case of multiple disconnected graph components, our correctness
definition implies that a distributed self-stabilizing algorithm needs to converge to the
desired topology for each (weakly connected) component.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:6

R. Jacob et al.

As a final remark, we emphasize that the term round is used in two different ways in
this article. First, as described previously, a round usually refers to a (communication)
round where all nodes receive requests from the last round, locally process the requests,
and send out new requests accordingly. Second, we will often refer to the simultaneous
execution of all currently enabled rules as an (execution) round as well. However, since
the commands in the rules of our algorithm ALG+ always consist of a constant sequence
of communication rounds, the two notions are tightly related and hence the ambiguity
is intended. Where necessary, we will explicitly mention whether a communication
round and when an execution round is meant.
1.3. Our Contributions

We introduce a slightly extended variant of the skip graph, called SKIP+ , that can be
checked locally for the correct structure. For this graph class, we present a distributed
self-stabilizing algorithm ALG+ which converges in O(log2 n) rounds with high probability (w.h.p.: with probability at least 1 − 1/nc for a constant c), for any given initial
state in which the nodes are weakly connected. This is an exponential improvement
over all previous results on the number of communication rounds needed to arrive at
a scalable overlay network.
We also show that a single join event (i.e., a new node connects to an arbitrary node in
the system) or leave event (i.e., a node just leaves without prior notice) can be handled
efficiently by our algorithm, namely, with polylogarithmic work: only a polylogarithmic
number of edges are replaced. This demonstrates that our algorithm is not just useful
for the worst case where SKIP+ needs to be rebuilt from scratch, but also for repairing
only a small and local part of the “almost perfect” overlay topology.
1.4. Organization

This article is organized as follows. Section 2 presents the SKIP+ graph together with
a self-stabilizing maintenance algorithm. The correctness and performance proofs
are given in Section 3. The complexities of individual joins and leaves are studied in
Section 4. In Section 5, the contribution is summarized and we discuss open problems.
2. ALGORITHM

We first introduce the extended skip graph SKIP+ and then present our self-stabilizing
algorithm ALG+ .
2.1. The Skip Graph

We start with the definition of a skip graph. In skip graphs, in addition to the basic
model introduced in Section 1.2, the identity of a node v consists of two constants:
(1) v.id, a unique but otherwise arbitrarily chosen identifier from a totally ordered
universe U , and (2) v.rs, a random bit string of sufficient length1 ; the bit string is
independent of the identifier and the bits are chosen independent and identically distributed (i.i.d.) with probability one half. (We will sometimes refer to the bit of v.rs at
a given index or level i as a color, and accordingly, will depict the bits black and white
in our illustrations.) Both constants are assumed to be immutable.
In the following, an edge (u, v) is said to be to the right (respectively, left) if u.id < v.id
(respectively, u.id > v.id). For a node v and a subset W ⊆ V of nodes, define the
predecessor and the successor of v in W as the next node in W to the left and to the
1 In practice, instead of a constant bit string and if the required length is unknown, an append-only string
could be used which is extended lazily bit by bit when needed.

Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:7

Fig. 1. The lowest levels of a correct skip graph. Node identifiers are represented by single letters
{a, c, e, m, . . .}, and the ordering is alphabetical. The bottom row shows the nodes on level 0 together with
their 2-bit prefix. Depending on the first bit, a node either belongs to the connected list of nodes with prefix 0
or to the list of nodes with prefix 1 on level 1.

right with respect to the order of the ids.
pred(v, W) = arg max {w.id < v.id},
w∈W ∪{⊥}

succ(v, W) = arg min {w.id > v.id},
w∈W ∪{}

where ⊥ and  are virtual nodes having the artificially smallest and largest identifiers
⊥.id = −∞ and .id = ∞, which are not allowed as identifiers of real nodes. By the
assumption that no two nodes share the same id, the predecessor and the successor
are well defined.
For any i ≥ 0, let pfxi (v) denote the first i bits of v.rs (the random bit string of v),
that is, the prefix of v.rs of length i. Here, pfx0 (v) is the empty bit string. Moreover, for
i ≥ 1, let v.rs[i] represent the ith bit of v.rs. To distinguish the definitions for the ideal
skip graph from analogous definitions used in the algorithm (based on the current local
views of the nodes), a superscript ∗ is used for the ideal graphs. We define the level-i
predecessor of v by
predi∗ (v) = pred(v, {w ∈ V | pfxi (w) = pfxi (v)})
and the level-i successor by
succi∗ (v) = succ(v, {w ∈ V | pfxi (w) = pfxi (v)}).
In other words, the level-i predecessor is the predecessor with respect to the nodes that
share the prefix of length i, and analogously, the level-i successor is the successor with
respect to the nodes that share the prefix of length i.
Definition 2.1 (Skip Graph). Assume we are given a set of nodes together with associated IDs and random strings. In the corresponding skip graph, every node v is
connected exactly to predi∗ (v) and succi∗ (v) for every i ≥ 0 (except for the empty cases ⊥
and , see Figure 1).
Given unique node identifiers and the values of the random bit strings, the skip
graph is uniquely defined. (Alternatively one could also define a cyclic variant.) It is
not difficult to see that with high probability (w.h.p.), the skip graph has logarithmic diameter and maximum degree, and it allows us to perform “hypercubic routing” between
any pair of nodes in O(log n) time, w.h.p. Aspnes and Shah [2003, 2007] and Harvey
et al. [2003]. However, the nodes cannot locally verify the correctness of the skip graph
topology (see Figure 2). For this reason and in order to achieve a polylogarithmic time
bound, we propose a slight extension of the skip graph that we call SKIP+ . We introduce
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:8

R. Jacob et al.

Fig. 2. A faulty skip graph: in this setting, the nodes cannot locally verify the correctness of the topology.
Node a and both its only neighbors c and u do not recognize the existence of node n, which is closer to a
than u. This inconsistency will never be discovered by any node if the distance between the node pairs {a, n},
{n, u}, and {n, z} is at least 4, since the local view is always correct.

Fig. 3. The figure depicting the lowest two levels indicates additional edges that render the network locally
checkable: node c will realize that n is not a neighbor of a yet and will notify a accordingly.

two new types of edges accordingly, one for local checkability (see Figure 3) and one for
efficiency. (Note that the latter edges could be omitted if the goal is to only eventually
reach the desired topology, and the polylogarithmic runtime is not important.)
2.2. The SKIP+ Graph

The definition of SKIP+ requires us to also define (extended) predecessors and successors on level i with a specific value x in the next bit. For any i ≥ 0 and x ∈ {0, 1},
define
predi∗ (v, x) = pred(v, {w ∈ V | pfxi+1 (w) = pfxi (v) ◦ x})
succi∗ (v, x) = succ(v, {w ∈ V | pfxi+1 (w) = pfxi (v) ◦ x}),
where operator ◦ means concatenation of bit strings. That is, we require that pfxi (w) =
pfxi (v) and w.rs[i + 1] = x. Let
lowi∗ (v) = min{predi∗ (v, 0).id, predi∗ (v, 1).id}
highi∗ (v) = max{succi∗ (v, 0).id, succi∗ (v, 1).id}
rangei∗ (v) = [lowi∗ (v), highi∗ (v)].
With this definition, the SKIP+ graph shall have the following neighbor set Ni∗ (v) of v
at level i: the set of all nodes w with pfxi (w) = pfxi (v) and w.id ∈ rangei∗ (v).
Definition 2.2 (SKIP+ Graph). Assume we are given a set of nodes together with
associated IDs and random strings. In the corresponding SKIP+ graph
 every node v is
connected to exactly the nodes in Ni∗ (v) for all i ≥ 0, that is, N ∗ (v) = i≥0 Ni∗ (v).
Note that although i is not bounded here, the number of neighbors of a node is
bounded. Figure 4 illustrates the connections in SKIP+ . The white (respectively, black)
nodes in the figure illustrate the nodes v at level i for which v.rs[i + 1] = 0 (respectively,
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:9

Fig. 4. Visualization of SKIP+ connections.

v.rs[i +1] = 1). The total sorted order of the nodes according to their identifiers is shown
at the bottom; the SKIP+ structure at level 0 is depicted in the middle, whereas the top
part of the figure (top two connected components of black and white nodes) correspond
to level 1. Note that skip graph edges of level i + 1 appear in the SKIP+ graph already
on level i.
Since for each set of nodes, IDs and random strings, the corresponding skip graph
is a subgraph of SKIP+ , SKIP+ has a logarithmic diameter and constant expansion.
Another important characteristic of the SKIP+ topology is its height.
Definition 2.3 (Height H). The height HG of a SKIP+ graph G is defined as the
longest prefix length of a nontrivial level: that is, a level which consists of more than
one node.
Since we expect O(1) nodes to share a prefix of length (log n), it follows from Chernoff
bounds that the height of the skip graph is logarithmic: HSKIP+ ∈ O(log n) w.h.p.
Aspnes and Shah [2003, 2007] and Harvey et al. [2003].
Also the maximum node degree, that is, the cardinality of the out-neighbor set,
remains logarithmic with high probability.
LEMMA 2.4. For any node v in SKIP+ , N ∗ (v) ∈ O(log n) w.h.p.
PROOF. Recall the definition of a SKIP+ graph and consider any node v. For any i,
let Ri be the set of all nodes w with w.id > v.id and pfxi (w) = pfxi (v). Certainly, Ri
represents the set of all nodes larger than v that v may potentially connect to in level i.
Suppose that Ri = {w1 , . . . , wk} ⊂ Ri is the set of v’s right neighbors (on this level).
Then it follows from the definition of the SKIP+ graph that w j .rs[i + 1] = w1 .rs[i + 1]
for all 1 < j < k and wk.rs[i + 1] 
= w1 .rs[i + 1]. Since the rs-bits are chosen uniformly
and independently at random for each node, the probability for this is equal to 1/2k−1 .
Let the random variable Xi denote the number of right (out-)neighbors of v in level i.
Then, for any Ri with |Ri | ≥ 2, it holds that Pr[Xi = k] = 1/2k−1 for all 1 < k < |Ri | and
H
Pr[Xi = |Ri |] = 1/2|Ri |−2 , and for |Ri | < 2, Xi = |Ri |. Let X = i=0
Xi be the number of
all right neighbors of v up to level H. Since the rs-bit of a given level is independent of
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:10

R. Jacob et al.

other levels, it follows from these probabilities and a union bound that


H−1

 1
d+ H −1
1
Pr[X = d] ≤
≤
.
ki −2
d−2H
2
2
H
−
1
 H−1
k0 ,...,kH−1 ≥0:

i=0

ki =d i=0

If d = c · (H − 1), we get


d+ H −1
1
[(c + 1)d] H−1
[(c + 1)d] H−1 · 24(H−1)
1
≤
≤
≤ c
d−2H
c(H−1)−2H
H−1
2
n
2
2c(H−1)
for some constant c that can be made arbitrarily large if H = c log n for some constant
c that is sufficiently large. Since the maximum level of a node is bounded by O(log n)
w.h.p., the number of v’s right neighbors is at most O(log n) w.h.p. A similar argument
applies to the left neighbors of v, which implies that the neighborhood of v has a size of
at most O(log n), w.h.p. Taking the union bound over all nodes implies the lemma.
2.3. Concepts and Intuitions

The algorithm ALG+ is based on the following concepts. It consists of four simple rules
only, and each rule specifies a short sequence of commands. Topologically, a command
may (1) insert a new edge (u, w) in the neighborhood of a node v, or (2) forward an
edge (v, u) to an out-neighbor w of v to form an edge (w, u) (note that the source v of a
directed edge (v, u) is in charge of the forwarding). Besides the forwarding of edges and
the merging of parallel edges, edges are never actually deleted. This is vital to ensure
connectivity.
For the presentation and analysis of ALG+ , we will need the following definitions.
Definition 2.5 (Vρ , Eρ , Gρ , Connected ρ-Component). Given any bit string ρ and any
directed graph G = (V, E) currently formed by the nodes, we define Gρ = (Vρ , Eρ ) to be
the subgraph induced by the nodes with prefix ρ, formally:
Vρ = {v ∈ V | pfx|ρ| (v) = ρ}

(the ρ-nodes)

Eρ = {(u, v) ∈ E : u, v ∈ Vρ }

(the ρ-edges),

where |ρ| denotes the (bit-)length of a prefix ρ. Given a prefix ρ, we will refer to a weakly
connected component of nodes in Gρ as a connected ρ-component. A pair of nodes in
such a component is called ρ-connected.
For any node v, let N(v) denote v’s current outgoing neighborhood, and let rangei (v)
be its current range at level i, which is based on its current view of predi (v, x) and
succi (v, x), that is,
predi (v, x)
succi (v, x)
lowi (v)
highi (v)
rangei (v)

=
=
=
=
=

pred(v, {w ∈ N(v) | pfxi+1 (w) = pfxi (v) ◦ x})
succ(v, {w ∈ N(v) | pfxi+1 (w) = pfxi (v) ◦ x})
min{predi (v, 0).id, predi (v, 1).id}
max{succi (v, 0).id, succi (v, 1).id}
[lowi (v), highi (v)].

Note that in contrast to the definitions of predi∗ (v, x) and predi∗ (v, x), the predecessors
and successors are chosen from the neighborhood N(v), rather than from all nodes V ,
as indicated in bold.
For each level i, rangei (v) ⊇ rangei∗ (v), that is, the current range will always include
the desired range in the target topology (as defined in SKIP+ ). We will see that as long
as no additional external changes happen during the self-stabilization process, ALG+
monotonically converges to the desired ranges for every i.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:11

ALG+ distinguishes between stable edges and temporary edges. Node v considers an
edge (v, w) to be temporary if v knows that (v, w) does not belong to SKIP+ and so
according to ALG+ v will try to “forward” it to one of its neighbors (Rule 2): This means
that v informs its neighbor v  which shares the longest common prefix with w.rs about
w, introducing an edge (v  , w). Otherwise, v considers (v, w) to be a stable edge and will
make sure that the connection is bidirected, that is, it will request w to establish the
edge (w, v). There is a binary flag v.F(w) for each neighbor w that states whether the
edge to it is stable. The flag turns out to be important when a stable edge destabilizes
(i.e., converts into a temporary edge) because this triggers the introduction of several
temporary edges that are needed for the construction. The new edge resulting from
forwarding a temporary edge is then stable if it connects a pair of nodes which consider
each other adjacent in SKIP+ . More details will be given later.
The intuition behind the stabilization process of ALG+ is as follows. The execution
of the algorithm can be thought of as being divided into two main phases: The first
phase proceeds in a bottom-up (i.e., from level 0 upwards) fashion, forming connected
ρ-components for every prefix ρ. This will be accomplished by letting each node v find
another node w of the opposite “color” (i.e., bit), that is, such that pfxi (w) = pfxi (v) and
v.rs[i + 1] 
= w.rs[i + 1] for all levels i ≥ 0 (on each side, we call the closest neighbor w
with this property a buddy of v). We can show that once all nodes in Vρ have formed a
single connected component and have found a buddy on level |ρ| (Lemma 3.13), then
connecting all nodes which are at most three hops away in the ρ-component results in
a single connected ρ0-component and a single connected ρ1-component (Lemma 3.9).
This will be accomplished by Rule 1 (where new nodes in the range of a node are
discovered and where ranges may be refined) and Rule 3 (where a local variant of a
transitive closure is computed).
Once the connected ρ-components are formed, the second phase of the execution will
form a sorted list out of each ρ-component. This is occurs in a top-down fashion, as two
already sorted ρ0- and ρ1-components are merged into a sorted ρ-component, until all
nodes in the bottom level form a single sorted list.
However, this “division into phases”-intuition oversimplifies what is really happening
during the execution of the algorithm: Many rules in our algorithm may be enabled at
any time, causing the phases to be intertwined in their executions. Hence, the main
challenge in this article is to show that, nevertheless, the rules transform any initially
weakly connected graph into the SKIP+ graph in O(log2 n) rounds.
2.4. The ALG+ Algorithm

With these intuitions in mind, we are ready to present ALG+ in more formal detail.
The local state of a node v consists of its identifier v.id and random string v.rs, the
out-neighborhood N(v) (i.e., the edges and implied ranges of v, cf also Lemma 2.4), as
well as the edge stability flag v.F(u) for each neighbor u. The algorithm is structured
into a preprocessing (or “transition”) phase and four regular rules.
The execution of each rule consists of a constant number of commands and communication rounds. Therefore, in the following, we will often not distinguish between
execution rounds and communication rounds (see Section 1.2).
At the beginning of each execution round, the preprocessing phase is executed (before
any enabled rule is applied): the precondition of this phase is true by definition, and
we hence also separate the preprocessing from the other rules in the description. In
particular, throughout this article and if not stated otherwise, when we write about a
time step t, we usually refer to the moment right before the next preprocessing phase.
Essentially, the preprocessing ensures a deterministic state transition in the sense
that all pending requests sent for the previously executed rule are processed. It also
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:12

R. Jacob et al.

Fig. 5. The nearest neighbor edge: v is in the range of u, but u is not in the range of v. The edge (u, v) is
considered stable as a nearest neighbor edge (because v is the current nearest black node of u to the left)
until u discovers one of the black nodes in-between.

serves to learn about in-neighbors and to exchange state information. Since the exchanged state information includes neighborhoods, after the preprocessing phase, each
node knows the identifiers of the neighbors of its neighbors [Naor and Wieder 2004].
Preprocessing/Transition. First, each node u sends a ping (or “hello”) message to all
its outgoing neighbors; the message includes u.id and u.rs. In the next communication
round, if node u receives a ping message from a so-far unknown neighbor v, it locally
adds the edge (u, v) and puts neighbor v in the set of (out-)neighbors N(u); nodes from
which no ping message was obtained are removed from N(u). In order to create antiparallel edges between neighbors, the nodes exchange their state information (including
their flags and out-neighborhoods): for every stable edge (u, v), u sets u.F(v) = 1 and
initiates an insert(v, u) request (if it has not already done so). Finally, u removes parallel edges, determines, for every level i, its current predecessors predi (u, 0), predi (u, 1)
and its current successors succi (u, 0), succi (u, 1) within N(u), and updates its range
information accordingly.
For the other rules, we need to formalize the concept of stable and temporary edges.
Definition 2.6 (Stable and Temporary Edges). An edge (u, v) is considered stable, if
v is within u’s range at some level i with pfxi (u) = pfxi (v), that is, v.id ∈ rangei (u) for
some i (so-called in-range edges).
All edges that are not stable are called temporary. We will sometimes refer to an
edge as stable independently of its direction. In this case, we mean that the edge is
bidirected (i.e., there exists an edge in both directions (u, v) and (v, u)), and that it is
stable in both directions.
Note that we call an edge (u, v) stable if v is in u’s range on some level. Nevertheless,
it is sometimes useful to refer to a specific level on which an edge is stable: We say that
an (u, v) is stable on level i if v.id ∈ rangei (u). Note that an edge which is stable on level
i is also stable on all levels j > i, as long as u and v have a shared prefix of length j,
that is, pfx j (u) = pfx j (v).
Given a stable edge (u, v), we will sometimes refer to v as a stable neighbor of u.
Nearest neighbor edges are a special instance of stable edges: (u, v) is a nearest
neighbor edge if v = predi (u, x) or v = succi (u, x), for some level i and bit x ∈ {0, 1}.
Note that a nearest neighbor edge (u, v) for level i implies that v is in the i-range of u;
(u, v) is stable at least on level i. Figure 5 illustrates the definition of nearest neighbor
edges.
This second kind of stable edges is needed to stay in touch with a buddy, in order to
forward temporary edges. Our algorithm guarantees that once a node has a buddy to
the left (or to the right), it will always have such a buddy in the future.
Definition 2.7 (Level of Temporary Edge). We define the level of a temporary edge
(u, v) as the length of the longest common prefix of u and v.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:13

Rule 1. Range Reduction. For every node u with a stable neighbor v, for every i ≥ 0
and every (not necessarily stable) neighbor w ∈ N(u), w 
= v with pfxi (v) = pfxi (w) and
w.id ∈ rangei (v), node u requests insert(v, w) to build the stable edge. In addition, if
v.id ∈ rangei (w), v also inserts the edge (w, v).
Note that Rule 1 only inserts stable edges that reduce the range of the corresponding
node. Moreover, it only applies if at least one of the two nodes u and v is a stable
neighbor of the other node.
Rule 2. Forward Edges. Node u forwards every temporary edge (u, v) to the stable
neighbor w of u (forward(w, v)) which has the largest common prefix with v.rs.
Note that such a neighbor w exists because otherwise (u, v) would be a stable edge;
ties can be broken arbitrarily.
Rule 3. Local Closure. Each node u ∈ V initiates insert(v, w) for all its neighbors v, w ∈ N(u), if and only if its stable neighborhood changes, in the following sense:
In contrast to the previous round, either at least one stable edge e = (u, x), for some
x ∈ V , became instable, or the lowest level on which e is stable changed, or u is incident
to a new stable edge.
In particular, if an edge destabilizes, both incident nodes will introduce their neighbors.
Rule 4. Linearize. For every prefix length i, u identifies the stable neighbors v1 , . . . , vk
with v1 .id < v2 .id < · · · < vk.id having the prefix pfxi (u) ◦ 0 and initiates insert(v1 , v2 ),
insert(v2 , v3 ), . . . , insert(vk−1 , vk) for them. The same is applied to the stable neighbors w1 , . . . , w with w1 .id < w2 .id < · · · < w .id having the prefix pfxi (u) ◦ 1, where
insert(w1 , w2 ), insert(w2 , w3 ), . . . , insert(w−1 , w ) is requested.
Intuitively, Rule 1 follows a “pointer doubling” principle and ensure that nodes
quickly learn about their vicinity and neighbors’ neighbors, and in particular about
a node of the opposite color (a so-called “buddy”), on any level. Rule 2 ensures that
edges are either forwarded to a location where they are needed and can become stable,
or are removed without losing connectivity (by being merged with another, parallel
edge). Rule 3 performs a local closure to quickly propagate edges in the neighborhood,
and to keep already connected subsets of nodes also connected in the future. Finally,
Rule 4 sorts the nodes according to their identifiers, establishing the desired search
structure.
3. ANALYSIS

We first analyze the bottom-up phase and then tackle the top-down phase, according
to the intuition provided in Section 2.3. The analysis starts with the first (execution)
round after t0 , the time where no more external topological changes occur.
3.1. Bottom-Up Phase

Let us start with some basic observations.
LEMMA 3.1. Consider any bit string ρ ∈ {0, 1}∗ . Suppose that nodes a, b ∈ Vρ are
ρ-connected at time t0 . Then, a and b are also ρ-connected at any time t ≥ t0 .
PROOF. We prove the lemma by induction over execution rounds. Consider any edge
e = (u, v) (temporary or stable) at time t ≥ t0 with u, v ∈ Vρ . Only Rule 2 can remove
this edge, because all other rules only create edges. If the edge e is forwarded by Rule 2
to a node w, node w must have a shared prefix with u that extends ρ, and all three
nodes u, v, w remain connected in Gρ at the next time step.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:14

R. Jacob et al.

Fig. 6. Left: Nodes u and v are σ -V-linked via node w. Right: Nodes a and b are connected by a (σ, k)-bridge.

LEMMA 3.2. Assume a node u has a stable ρ-edge on level i to the right (left) at time t0 .
Then at any time t > t0 , node u will have a stable ρ-edge to the right (left) on level i.
Moreover, if a node has a nearest neighbor edge to a node v of the same or opposite color
on that level (i.e., u.rs[i] = v.rs[i], respectively, u.rs[i] 
= v.rs[i]), it will have a nearest
neighbor edge for that color at any time in the future.
PROOF. The only reason for a node v to fall out of the ρ-range of u is the appearance
of another node w of the respective color which is closer to u and shares the prefix ρ.
Then, the edge (u, w) becomes the new stable edge.
A ρ-buddy of a node u is a stable neighbor v with pfx|ρ| (u) = pfx|ρ| (v) = ρ and
u.rs[|ρ| + 1] 
= v.rs[|ρ| + 1], where |ρ| denotes the length of the prefix; that is, u and v
have the common prefix ρ but differ in the next bit of the random string. Lemma 3.2
implies that once a node has a left (or right) ρ-buddy at time t, then it will also have a
left (or right) ρ-buddy at all times t ≥ t.
For our analysis, the concept of σ -V-linked nodes is helpful to describe a situation
where two nodes are indirectly connected via a third node of the opposite bit/color. (Note
that the V in V-linked does not refer to the node set V but to the V-shaped situation.)
Henceforth, let x̄ denote the negation of bit (color) x.
Definition 3.3 (σ -V-link). Consider any bitstring (i.e., node prefix) ρ ∈ {0, 1}∗ and a
bit/color x ∈ {0, 1}. Assume there are two nodes u, v with prefix ρx = σ and one node w
with prefix ρ x̄. If u, v ∈ N(w), we say that u and v are σ -V-linked via w.
Figure 6 (left) depicts the situation.
LEMMA 3.4. Assume that u and v are σ -V-linked via w at time t. Then, at time t + 1
the nodes u and v are σ -connected, that is, in the same connected σ -component.
PROOF. Assume that u, v have prefix σ = ρx, and w has prefix ρ x̄. By Rule 4, all
stable ρx-neighbors of w are in the same, single ρx-component C at time t + 1. If (w, u)
is a temporary edge, then there must be other neighbors of w having prefix ρx, and
(w, u) is forwarded to one of the stable ρx neighbors of w (by Rule 2); hence u is also in
the ρx-component C. By the same reasoning, v is also in C at time t + 1, which proves
the lemma.
Note that the σ in “σ -V-linked” refers to the nodes that are linked, not to the middle
node providing the link.
LEMMA 3.5. Consider any bitstring (i.e., node prefix) ρ ∈ {0, 1}∗ and bit/color x ∈
{0, 1}. Let node a with prefix ρx be a (left or right) ρ-buddy of node u at time t and let
node b with the same prefix ρx be a (left or right) ρ-buddy of u at time t ≥ t. Then, a
and b are in the same ρx-component from time t + 1 on.
PROOF. The proof is by induction over t − t. The base case t = t follows from
Lemma 3.4.
Induction Step. Let a = at , at+1 , . . . , at = b be ρ-buddies of u at the respective times.
With Lemma 3.1, it suffices to show that ai−1 and ai are in the same ρx-component at
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:15

Fig. 7. The shaded nodes belong to the same (ρ0, k)-pre-component. In this figure, |τ | = k − 1, temporary
edges are dashed and stable edges are solid.

time i + 1. Because no rule deletes or forwards stable edges, the edge (u, ai−1 ) exists at
time i and Lemma 3.4 can be applied.
Definition 3.6 (Temporary/Stable (σ, k)-Bridge). Consider two nodes a, b with prefix σ = ρx that are in different connected components of Gσ . Then, for k ≥ 1, a and b
are connected by a (σ, k)-bridge if there is a node c with stable edge (c, a) and a node d
with stable edge (d, b), where c, d have prefix ρ x̄ and have the edge (c, d) at level at
least |ρ| + k (i.e., pfx|ρ|+k(c) = pfx|ρ|+k(d)).
If (c, d) is stable, this is a stable bridge; if (c, d) is temporary, it is a temporary bridge.
Similar to the notion of σ -V-linked nodes, we define connectivity via a bridge by the
prefix of the nodes that are connected by this bridge (and not by the nodes that provide
the bridge).
It is central to the bottom-up proof that eventually for all prefixes ρ we have that Gρ
consists of only one connected component. In the process where the connectedness of
Gρ0 follows from Gρ being connected, the bridges play an important role. First, given
that the nodes have found their buddies using Rule 1 (Lemma 3.13), the levels (and
hence k) of the temporary bridges connecting two components increase in every step:
the shared prefix of the two endpoints of a temporary bridge edge becomes larger and
larger (Lemma 3.12); this is mainly due to Rule 2. Accordingly, the bridge stabilizes
fast, and can then serve to forward edges (Rule 2), yielding new stable bridges on lower
levels. Once two nodes are indirectly connected via a path of bridges, in a constant
number of rounds (e.g., using Rule 1 and Rule 3), they are also connected directly
(Lemma 3.14).
Definition 3.7 ((σ, k)-pre-component). Two nodes a and b, both with prefix σ = ρx
for some x ∈ {0, 1}, are directly (σ, k)-preconnected
(1) if Gρ is (weakly) connected,
(2) if every node in Gρx knows at least one neighbor in Gρ x̄ and vice-versa (“buddy” is
there), and
(3) if there is an edge (a, b) or (b, a), or if a and b are σ -V-linked (taking into account
the pending requests as well!), or if there exists a stable (σ, k )-bridge with k ≤ k
between a and b.
The transitive closure of this (symmetric) relation defines the (σ, k)-precomponent.
Figure 7 illustrates the situation. We will sometimes use the expression (σ, 0)-precomponent (i.e., where k = 0 and without bridges), to refer to the transitive hull of the
direct and σ -V-links.
LEMMA 3.8. For a prefix σ and k ≥ 1, assume that nodes a, b are in the same (σ, k)precomponent at time t0 . Then, a and b are in the same (σ, k)-precomponent at any
time t > t0 .
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:16

R. Jacob et al.

Fig. 8. Illustration of shortest bridges path.

PROOF. Proof by induction over time t. If a, b are directly linked, we can employ
Lemma 3.1, and the case that they are σ -V-linked is valid due to Lemma 3.4. In both
cases, a and b stay directly (σ ,k)-preconnected at time t + 1.
If a, b are not directly connected nor σ -V-linked, this implies that there is a connecting
(σ, k)-bridge via stable edges e1 = (u, v), e2 = (u, a), and e3 = (v, b) at time t. See Figure 6
(right) for an illustration. Obviously, if e1 , e2 , e3 remain stable at t, the claim holds
trivially also at time t + 1.
Otherwise, a subset of the edges e1 , e2 , e3 destabilizes. Let us first consider what
happens if e1 = (u, v) destabilizes. In this case, u found a new neighbor w, and executes Rule 3, which introduces new bidirected edges, in particular {a, w}, {w, v}, {a, v}.
However, the edges (v, a) and (v, b) constitute a σ -V-linked situation, and hence a, b are
again in the same σ -component at time t + 1 by Lemma 3.4.
An instable e2 = (u, a) edge triggers Rule 3 at u. Node u then requests an edge (v, a)
such that a and b remain σ -V-linked as well. Finally, if e3 = (v, b) becomes unstable,
there must exist a stable σ -neighbor b of v. Since Rule 3 introduces direct links between
b and b , a and b remain in the (σ, k)-precomponent as well: either via the stable bridge
between u and v (if e1 remains stable), or via a direct link {b, b } plus a σ -V-link between
a and b (otherwise).
The following lemma establishes the main connectivity result of the bottom-up phase:
bridges of a precomponent are quickly replaced by direct connectivity in Gσ .
LEMMA 3.9. Assume that nodes a, b are in the same (σ, k)-precomponent at time t,
k > 1. Then, a and b are σ -connected at time t + 4.
PROOF. According to Definition 3.7, if a and b are in the same (σ, k)-precomponent
(assume again that σ = ρx for some x ∈ {0, 1}), there must exist a path a = p1 
p2  · · ·  p = b (all with prefix σ ) where p j is connected to p j+1 either directly, via
a σ -V-link, or via a stable (σ, k)-bridge. We only allow a bridge between p j and p j+1 if
these two nodes are not in the same (σ, 0)-precomponent, that is, if they are neither
directly connected nor σ -V-linked.
In the upcoming analysis, we will consider a path with “shortest bridges”. Intuitively,
shortest bridges are needed to prove that the neighboring nodes p j and p j+1 discover
each other. Shortest bridges are defined as follows (see also Figure 8). Given an arbitrary path a = p1  · · ·  p = b as defined above, assume two consecutive nodes p j
(in the figure: p3 ) and p j+1 (in the figure: p4 ) are connected by a bridge over an edge
e = (u, v) of length λ = |u.id − v.id| (i.e., with respect to node identifiers), u and v having
prefixes of the form ρ x̄τ y where |τ | = k − 1 and y ∈ {0, 1}). Then, in order to construct
a path of “shortest bridges”, whenever possible, we replace e by two stable edges via
an intermediate node w with prefix ρ1 if for the lengths it holds that |(u, w)| < λ and
|(w, v)| < λ (i.e., u.id < w.id < v.id and w is connected to u and v). Thus, in our path,
a new node is inserted: a buddy of w with prefix σ , henceforth denoted by p∗ , and we
replace the stable bridge via (u, v) by two stable bridges. (The edge (w, p∗ ) is also stable
as w is a buddy.)
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:17

Fig. 9. Situation for i + 1 > |σ | in proof of Lemma 3.9 at time t.

Fig. 10. The two situations for i + 1 = |σ | in proof of Lemma 3.9.

Hence, it is sufficient to consider two nodes a and b that are connected via the (stable)
bridge edge (u, v) on this path with shortest bridges. First, assume u is not in v’s range
at some level. Then, it holds that v must know two closer nodes (one for each bit/color)
than u, at least one of which will be proposed to u in the next round according to Rule 1,
such that both closer nodes are connected to u after this round. Hence, (u, v) will no
longer be stable on this level (but maybe still on higher levels), triggering Rule 3 at
t + 2. Thus, p j and p j+1 become σ -V-linked, due to the new edge (v, p j ). According to
Lemma 3.4, p j and p j+1 subsequently become σ -connected.
From now on, it remains to consider the case where u and v mutually include each
other in their ranges on some level. First assume that (u, v) is stable due to a level > |σ |;
the other case where (u, v) is a nearest-neighbor edge and is stable due to level |σ |, is
treated later. Let us refer to the lowest level where (u, v) is stable as i + 1, that is, where
pfxi+1 (u) = pfxi+1 (v) and v.id ∈ rangei+1 (u), but v.id ∈
/ rangei (u), so |σ | + k ≥ i + 1 > |σ |.
Without loss of generality, assume that u.id ∈
/ rangei (v). Due to the definition of the
ranges, v must have two stable predecessors (relatively to the current neighborhood)
that lie between u and v: w0 = predi (v, 0) and w1 = predi (v, 1). Let w ∈ {w0 , w1 } such
that pfxi+1 (u) = pfxi+1 (w) = pfxi+1 (v). See Figure 9.
However, (w, u) ∈
/ E at time t, due to our selection of the shortest bridge path: (u, w)
and (w, v) would imply a path with shorter bridges. Thus, at time t + 1, (u, w) is created
by Rule 1. Now, we will show that this triggers u to fire Rule 3 at t + 2, either because
(u, w) becomes stable or because (u, v) becomes unstable. (Edge (u, v) must be stable at
time t, as it forms a bridge.) Observe that it is not possible that (u, v) remains stable
and (u, w) is unstable, since both are within u’s range on level i + 1: u and w have a
common prefix ρ = pfxi+1 (w). Thus, at time t + 1, Rule 3 is triggered at u, and thus, at
t + 2, a and b are σ -V-linked and connected in round t + 3.
Observe that if w has a buddy w closer to u, see Figure 9, the same arguments apply,
as w  will be introduced to v, which triggers Rule 3 at v.
It remains to study the case where (u, v) is stable due to level i = |σ | for prefix ρ x̄ (i.e.,
v is a nearest neighbor, see Figure 10). Note that u is stably connected to p j and v is
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:18

R. Jacob et al.

stably connected to p j+1 (u, v ∈ Gρ x̄ ). We distinguish two cases: Either there is a stable
neighbor c ∈ Gρx of u or v on level |ρ| between them (Figure 10 left) or not (Figure 10
right ). First, assume there is such a node c, and without loss of generality assume c is
stably connected to v on level |ρ|. There cannot be both edges (u, c) and (c, v), otherwise
the bridge would not be necessary, as the nodes p j and p j+1 are connected by two σ -Vlinks (u, p j ), (u, c) and (v, c), (v, p j+1 ). We will show that in this case, a new connection
(u, c) is proposed. Either (u, v) destabilizes at time t + 1, triggering Rule 3 at t + 2,
or (u, c) must be proposed according to Rule 1. By the same reasoning as previously
stated, the claim follows for time t + 4 in this case.
If there is no such node c, consider the ρ-buddy of u or v closest to position μ =
(u.id + v.id)/2. That is, let B denote the set of all |ρ|-buddies of u and v on level i, that
is, B is set of stable neighbors in (N(u) ∪ N(v)) ∩ Gρx . Let w ∈ B be the buddy which
minimizes |μ − w.id|. Without loss of generality, assume w is a buddy of u. Now let w
be the buddy of v which is located on the same side of μ as w; the case that such a
w  does not exist is treated later. Observe that rangei (v) is defined by w  , due to our
assumption that c does not exist and since w is further away from μ (and hence also
from v). Therefore, w ∈ rangei (v), u proposes a new stable edge (v, w), and this triggers
Rule 3 at v. On the other hand, if no such w exists, then rangei (v) is not bounded, and
the same reasoning applies. Thus, Rule 3 is triggered at u or v, leading to a and b being
σ -V-linked in round t + 2. Therefore, the claim also follows for the case i + 1 = |σ |,
which concludes the proof.
LEMMA 3.10. A temporary edge (u, v) of level  at time t is either transformed into a
stable edge, or forwarded and changed into a temporary edge of level at least  + 1, at
time t + 1.
PROOF. Let ρ be the common prefix of u and v with |ρ| = , and assume without loss
of generality that u has label ρ0 and v has label ρ1. If (u, v) is temporary, it either
becomes stable now (during Preprocessing/Transition) or there must be a stable ρ1node w ∈ N(u) between u and v. In this case, Rule 2 replaces (u, v) with (w, v) for such
a w, which is a temporary edge of level at least  + 1.
The following lemma follows from the previous one because no temporary edge has
a level higher than the height H (see Definition 2.3).
LEMMA 3.11. Every temporary edge becomes stable after H time steps.
Next, we show that the length of the shared prefix (and hence the level) of the temporary bridges grows quickly, and that temporary bridges with long common prefixes
disappear quickly. Thus, components eventually become connected by stable bridges,
that is, are preconnected.
LEMMA 3.12. Assume two ρx-nodes a, b for some x ∈ {0, 1} are directly connected by
a temporary ρx-bridge at time t, that is, there is a temporary edge (u, v) and stable edges
(u, a) and (v, b). Then, a and b are (ρx, H − |ρ|)-preconnected at time t + (H − |ρ|).
PROOF. Define k = H − |ρ|, and, without loss of generality, let x = 1. We consider
the situation where at time t, we have two stable edges (u, v) and (v, b), while (u, v) is
temporary. Let us first consider what happens when (v, b) becomes temporary. In this
case, a better edge has been discovered within the range of v, reducing the range in
the future, and yielding a new stable edge. Concretely, let bi be a ρ-buddy of v, that
is, the stable edge (v, bi ) exists at time t + i, b0 = b. At time t + i + 1, there is at
least the temporary edge (v, bi ), and bi and bi+1 are ρ1-V-linked via v, and hence, by
Lemma 3.1 and Lemma 3.4, bi is directly connected with b also at time t + k. Moreover,
by Lemma 3.8, once (ρ1, k)-pre-connectivity is established, it is not lost anymore.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:19

Let us now consider what happens when edge (u, v) is forwarded (Rule 2), that is, a
new edge (ui , v) results at time t + i. If (ui , v) is stable, and if a is in the range of ui , the
edge (ui , a) will be established by Rule 1, and a and b are preconnected at level k ≤ k. If
a is not in the range of ui , let x be the node to which ui has a stable edge (ui , x) instead.
In this case, a second bridge is established, that is, a is stably preconnected via the ui s
to x, and x is preconnected via v to the bi s.
On the other hand, if (ui , v) is temporary, by Rule 2, ui and v must share a strictly
longer common prefix. However, this means that after at most H − (|ρ| + 1) steps, the
forwarded edge must become stable. Since the resulting edge is stable, a and b are
(ρx, H − |ρ|)-preconnected: by definition, at times t + i for all i, also the stable edges
(ui , ui+1 ) exist.
The case where (u, a) becomes instable is analogous to the case where (v, b) become
instable.
The next lemma establishes the missing link: it shows that buddies will always be
found quickly by “pointer doubling” (Rule 1).
LEMMA 3.13. Consider any bit string ρ ∈ {0, 1}∗ . Suppose that Gρ is weakly connected
at time t0 . Then, every node u ∈ Vρ will have a neighbor in Vρ0 and Vρ1 (if it exists) at
time t0 + O(log n).
PROOF. Consider any node u that does not have a neighbor in Vρ0 or Vρ1 . In this
case, rangei (u) = U for i = |ρ|, which implies together with Rule 1 that every node
v ∈ N(u) ∩ Vρ with missing ρ-buddy will introduce every node w ∈ N(v) ∩ Vρ to u.
Suppose, without loss of generality, that u is still missing a node in Vρ0 . Then, the
neighbor introduction of Rule 1 implies that the distance of u in Gρ to the closest node
in Vρ0 is cut in half in each round. Thus, it takes at most O(log n) rounds until a node
in Vρ0 is a direct neighbor of u, which implies the lemma.
Lemma 3.13 can also be proved by observing that in every round, due to the “pointer
doubling” operations, the diameter of the connected component formed by nodes without a buddy is cut in half.
LEMMA 3.14. Assume Gρ is connected at time t. Then, at time t + (H − |ρ|) + O(log n),
the graph Gρ0 is connected and so is Gρ1 .
PROOF. Define k = H − |ρ|. At time t = t + O(log n), by Lemma 3.13, every ρ0 node
has a stable connection to a ρ1 node and vice-versa. Let a and b be two ρ1 nodes.
Since Gρ is connected at time t, it is still connected at time t , and there is a path
a = u1 , u2 , . . . , um = b in Gρ at time t . If ui is a ρ0 node, define vi to be one of its
ρ1 buddies, otherwise set vi = ui . Then, by definition, at time t the nodes vi and vi+1
are either directly connected, or connected by a temporary (ρ1, k)-bridge. Hence, by
Lemma 3.12 (or Lemma 3.1), at time t + k the nodes vi and vi+1 are in the same (ρ1, k)pre-component, and by Lemma 3.9, Lemma 3.8, and Lemma 3.1, vi and vi+1 are in the
same connected component of Gρ1 at time t + k + O(1). With this and the symmetric
argument for Gρ0 , the claim of the lemma follows.
3.2. Top-Down Phase

Summing up Lemma 3.14 over all levels, we obtain that in O(log2 n) rounds, all nodes
sharing a given prefix ρ have discovered each other and are connected (via other nodes
with prefix ρ). In addition, each node is connected—on each level—to at least one
“buddy” having the opposite last bit of the corresponding prefix. We now prove that
after these properties have been achieved, the final SKIP+ topology is established in
only O(log n) rounds.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:20

R. Jacob et al.

The analysis of the top-down phase is done by induction overloading. For our induction step, we need the concept of finished levels.
Definition 3.15 (i-finished). We say that a graph is i-finished if and only if the
following have.
(1) ∀ρ with |ρ| = i. It holds that Gρ contains all edges of the SKIP+ -graph (as stable
edges).
(2) Let ρ = pfxi (u) for any given u ∈ V . if v is a level j < i right-buddy of u, Then for
all w ∈ Gρ with w.id ∈ [u.id, v.id], it holds: u and w are connected by a stable edge.
If there is no such buddy v, then u is connected to all nodes to the right of u in Gρ .
The same definition is used for left-buddies.
Intuitively, Definition 3.15 means that when an i-finished state is reached, a node
knows all its SKIP+ -graph edges at levels higher than |ρ| = i. For the levels i and lower,
the ranges may still be too large as not all neighbors have been found yet. However,
we require that a node at least already knows all its correct SKIP+ neighbors on these
levels, restricted to the nodes in Gρ (i.e., as if only nodes in Gρ existed).
Observe that after the bottom-up phase, the “top labels” are finished trivially: These
labels form graphs Gρ with more than one node, whereas Gρ1 and Gρ0 are trivial, that
is, consist of a single node. Clearly, for a top label ρ the graph Gρ consists of precisely
two nodes. In addition, once the graphs Gρ for all top labels are connected, they contain
all edges of the SKIP+ -graph as stable edges.
The following reasoning shows that the levels will finish one after the other starting
from the highest level: the graphs Gρ0 and Gρ1 connected by at least one edge “zip” to
graph Gρ . Each level takes constant time only.
LEMMA 3.16. For all i > 0, assume that at time t, the graph is i-finished. Then, at
time t + 3, the graph is (i − 1)-finished.
PROOF. We consider a node u and show that at time t + 1, u knows its closest level-i
neighbor w in the direction of the old buddy (that must exist due to Lemma 3.14 of the
bottom-up phase). Having established this, it follows directly that at time t + 2, w will
inform u about all other neighbors in the desired region (Rule 1). At time t + 3, node u
will be informed about its neighbors on the opposite side of the level i − 1 interval by
the corresponding buddy, establishing our induction invariant.
In order to prove that u knows its closest node w at time t + 1, we distinguish three
cases (cf Figure 11). From the bottom-up phase, we know that u already has a buddy
on one side. Without loss of generality, assume this buddy is on the right of u. Let this
buddy node be denoted by v.
Case I. If v is already the closest node to the right, the claim holds trivially (v = w).
We know that w also has a buddy, which can either be on the right (Case II) or on
the left (Case III) of w (and hence also u). Let w’s buddy (take any if w has two) be
denoted by w .
Case II. Assume w  is also on the right of w. We distinguish two cases: Either v is
left of w  or right of w  . If v is left of w  , by our induction hypothesis, w must have a
stable edge to node v (and vice-versa). By Rule 1, v will introduce w to u at t + 1 (edge
(w, u)), and the claim follows. The case where v is right of w is analogous: the roles of
v and w  are simply switched.

Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:21

Fig. 11. Proof of Lemma 3.16: top illustrates Case I, middle Case II, bottom Case III.

Case III. Now assume w has a buddy w  on the left. We distinguish two cases. If u
has another buddy u on the left as well, the same arguments as in Case II show that
either u or w  will introduce the corresponding neighbors at time t + 1. If, on the other
hand, u does not have a buddy on the left, then by our induction hypothesis, w must
have a stable edge to u, over which u is introduced to w in the next round as well.
Finally, observe that after all levels are finished, all non-SKIP+ edges must be temporary. They will be forwarded towards the highest level and disappear in H rounds
(i.e., in logarithmic time). Therefore, we conclude that the top-down phase takes only
logarithmic time.
3.3. Combining Bottom-Up and Top-Down

From Lemma 3.14, by summing up over all levels, it follows that the bottom-up phase
lasts for at most O(log2 n) rounds w.h.p. The subsequent top-down phase takes time
O(log n) (cf. Lemma 3.16) w.h.p. Thus, we have derived the following theorem.
THEOREM 3.17. Given an arbitrary weakly connected graph, the self-stabilizing algorithm ALG+ constructs SKIP+ in O(log2 n) rounds, w.h.p.
4. NODE JOIN/LEAVE

Our self-stabilizing algorithm ALG+ also supports efficient and local join and leave
events. In the following, we show that the work required to handle individual node
departures (leaves), or to include a new node (join), is at most polylogarithmic.
THEOREM 4.1. When a node v leaves the system, it takes O(log n) rounds and O(log4 n)
total work w.h.p. for the graph to converge back to the desired SKIP+ structure.
PROOF. Certainly, only the nodes that were directly connected to node v will need to
adapt their current set of neighbors upon the departure of v: the departure of v cannot
possibly alter the neighborhoods or ranges of other nodes, given that v is directly
connected to all nodes in its range for all levels. By Lemma 2.4, the size of the entire
neighborhood of node v (across all levels) is O(log n) w.h.p., so only O(log n) nodes need
to change their neighborhood.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:22

R. Jacob et al.

In order to show that these O(log n) nodes can quickly adapt their neighborhoods, we distinguish between several cases for every level i. In these cases, let
Vl (respectively, Vr ) be the set of all left (respectively, right) neighbors w ∈ Ni∗ (v)
with w.rs[i + 1] = v.rs[i + 1] and let Wl (respectively, Wr ) be the set of all left
(respectively, right) neighbors w ∈ Ni∗ (v) with w.rs[i + 1] 
= v.rs[i + 1]. Certainly,
Vl ∪ Vr ∪ Wl ∪ Wr = Ni∗ (v). Let vl , vr , wl and wr be the closest neighbors in Vl , Vr , Wl
and Wr to v. Let us assume for now that vl , vr , wl and wr exist.
Case 1. wl .id < vl .id < vr .id < wr .id. In this case, all neighborhoods are correct once
v has been removed, so no further edges are needed by the nodes.
Case 2. vl .id < wl .id < vr .id < wr .id. In this case, all nodes in {vl } ∪ Wl \{wl } have to
learn about vr and vice-versa to update the neighborhoods. The other nodes just have
to remove v from their neighborhood. Since wl has edges to all nodes in {vl , vr } ∪ Wl , a
single round of applying Rule 3 suffices to update all neighborhoods correctly.
Case 3. wl .id < vl .id < wr .id < vr .id. This case is just the reverse of Case 2.
Case 4. vl .id < wl .id < wr .id < vr .id. In this case, all nodes in {vl } ∪ Wl have to
learn about Wr ∪ {vr } and vice versa. Since wl knows {vl } ∪ Wl ∪ {wr } and wr knows
{wl } ∪ Wr ∪ {vr }, after one round of applying Rule 3, all nodes in {vl } ∪ Wl learn about
wr and all nodes in Wr ∪ {vr } learn about wl . Since the stable neighborhoods of wl and
wr just got updated, wl and wr will trigger another local closure by Rule 3, so nodes in
{vl } ∪ Wl ∪ Wr ∪ {vr } will have updated their neighborhoods by the second round.
The other cases when some of the nodes vl , vr , wl and wr do not exist are similar
and omitted here. Hence, after at most two rounds, all (stable) neighborhoods have
been updated. Since only O(log n) nodes need to change their neighborhood, and each
of these nodes inserts at most O(log2 n) edges due to Rule 3 in each round, at most
O(log3 n) edges are inserted in total. These either merge with stable edges, become a
new stable edge or become a temporary edge. Each of the temporary edges will need at
most O(log n) applications of Rule 2 until it merges with a stable edge. Hence, altogether
the time is bounded by O(log n) and the work is bounded by O(log4 n).
THEOREM 4.2. Assume a new node v joins the system by establishing an edge to a
node u which is currently in SKIP+ . It will take O(log n) rounds of the algorithm and
O(log4 n) total work w.h.p. for the graph to converge back to a valid SKIP+ structure.
PROOF. Upon learning about node u, node v immediately considers edge (v, u) as
stable: u is currently the only predecessor or successor node v knows. That will prompt
the insertion of edge (u, v) (Preprocessing/Transition). If u considers (u, v) a temporary
edge, then it forwards the edge via Rule 2 to a node u with a longer prefix match with
v than u. This leads, after at most H steps, to a stable edge (w, v). Till then, v keeps
inserting the edge (u, v) in each round (as it considers u to be a stable neighbor), so
there will be a string of temporary edges moving upwards from u. Besides Rule 2, no
other rule will be applied at this point by the old nodes in SKIP+ .
Suppose that w is the first node that considers the edge (w, v) to be stable. Let i be
the maximum level such that pfxi (v) = pfxi (w). Let Vl (respectively, Vr ) be the set of
all nodes to the left (respectively, right) of v in SKIP+ of maximum cardinality so that
for all w  ∈ Vl (respectively, w  ∈ Vr ), pfxi+1 (w  ) = pfxi+1 (v) and there is no node w  in
between the nodes of Vl (respectively, Vr ) with maximum common prefix equal to i with
v. Moreover, let Wl (respectively, Wr ) be the set of all nodes to the left (respectively,
right) of v of maximum cardinality so that for all w ∈ Wl (respectively, w  ∈ Wr ), the
maximum common prefix with v is of size i, and there is no node w in-between the
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:23

nodes of Wl (respectively, Wr ) with maximum common prefix with v larger than i. Let
vl , vr , wl and wr be the closest neighbors in Vl , Vr , Wl and Wr to v. Let us assume
for now that vl , vr , wl and wr exist. Recall that w considers v to be a stable neighbor.
Suppose, without loss of generality, that v is to the right of w. We distinguish between
the following cases.
Case 1. wl .id < vl .id < vr .id < wr .id. In this case, all nodes in {wl } ∪ Vl ∪ Vr ∪ {wr }
have to connect to v and vice versa, and besides these, no other connections are needed
to fully integrate v into level i. Since w = wl and w therefore knows all nodes in
Vl ∪ Vr ∪ {wr } by the SKIP+ definition, one round of applying Rule 3 (which is caused
by (w, v)) suffices to fully integrate node v into level i.
Case 2. vl .id < wl .id < vr .id < wr .id. In this case, all nodes in {vl } ∪ Wl ∪ Vr ∪ {wr }
have to learn about v and vice-versa to fully integrate v into level i. Since w is a node
in Wl , w has links to all nodes in {vl } ∪ Wl ∪ Vr ∪ {wr }, so again one round of applying
Rule 3 suffices to fully integrate node v into level i.
Case 3. wl .id < vl .id < wr .id < vr .id. In this case, all nodes in {wl } ∪ Vl ∪ Wr ∪ {vr } have
to learn about v and vice-versa to fully integrate v into level i. Since w = wl , w has
links to all nodes in Vl ∪ {wr }. Hence, one round of Rule 3 introduces v to the nodes in
Vl ∪ {wr } and vice-versa. Afterwards, wr will apply Rule 3 since its stable neighborhood
changed due to v, so wr will introduce v to Wr ∪ {vr } and vice-versa, which completes
the integration of v into level i.
Case 4. vl .id < wl .id < wr .id < vr .id. In this case, all nodes in {vl } ∪ Wl ∪ Wr ∪ {vr }
have to learn about v and vice-versa to fully integrate v into level i. As w is any node
in Wl , w knows about {vl } ∪ Wl ∪ Wr ∪ {vr }, so one round of applying Rule 3 suffices to
fully integrate node v into level i.
The remaining cases in which vl , vr or wr do not exist are similar and omitted here.
Hence, it takes at most two rounds to fully integrate v into level i.
Once v is fully integrated into a level i, it knows the closest predecessor and successor
w in SKIP+ with maximum prefix match at least i + 1 (if it exists). Since each of these
nodes will consider v to be a stable neighbor in level i + 1, we can use similar case
distinctions as previously described to show that v will be fully integrated into level
i + 1 in at most two further rounds. Node v also knows its closest predecessor and
successor w in SKIP+ with maximum prefix match at least i. Since each of these nodes
will consider v to be a stable neighbor in level i − 1, we can also use similar case
distinctions as previously described to show that v will be fully integrated into level
i − 1 in at most two further rounds. Using these arguments inductively implies that v
will be fully integrated into the SKIP+ graph in O(log n) time.
It remains to bound the work. Let us first consider the first phase where v contacts
u, until a node is met that considers the edge to v to be stable. The sequence of nodes
involved here will continuously forward temporary edges upwards until v ceases to
insert the edge (u, v). This happens once v is not a nearest neighbor of u anymore for
some level. This first phase consumes O(log2 n) work.
Each time a node destabilizes, O(log2 n) new edges are created. Certainly, only nodes
that will consider v to be their stable neighbor (and vice-versa) will destabilize, and
we know from Lemma 2.4 that the degree of v in SKIP+ will be O(log n) in the end
with high probability. Hence, altogether at most O(log3 n) new edges are created. These
either merge with stable edges, become new stable edges or become temporary edges.
Each of the temporary edges will need at most O(log n) applications of Rule 2 until it
merges with a stable edge. This yields the claim.
Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:24

R. Jacob et al.

5. CONCLUSION

This article describes the first self-stabilizing algorithm that quickly establishes a
scalable peer-to-peer topology out of any state in which this is possible in principle. We
were able to show a probabilistic O(log2 n)-time convergence bound.
We would like to suggest two open problems. The first problem regards the proof of
optimality of the derived convergence time, respectively, a better upper bound (for our
algorithm and in general). The second problem is to derive a bound on the number of
enabled rules (per node and round) during convergence.
ACKNOWLEDGMENTS
We would like to thank Dan Levin from TU Berlin for proofreading this article.

REFERENCES
Yehuda Afek and Shlomi Dolev. 2002. Local stabilizer. J. Paral. Distrib. Comput. 62, 5, 745–765. DOI:
http://dx.doi.org/10.1006/jpdc.2001.1823
Dana Angluin, James Aspnes, Jiang Chen, Yinghua Wu, and Yitong Yin. 2005. Fast construction of overlay
networks. In Proceedings of the 17th ACM Symposium on Parallelism in Algorithms and Architectures
(SPAA). 145–154. DOI:http://dx.doi.org/10.1145/1073970.1073991
James Aspnes and Gauri Shah. 2003. Skip graphs. In Proceedings of the 14th ACM-SIAM Symposium on
Discrete Algorithms (SODA). 384–393.
James Aspnes and Gauri Shah. 2007. Skip graphs. ACM Trans. Algor. 3, 4, Article 37. DOI:http://dx.doi.org/
10.1145/1290672.1290674
James Aspnes and Udi Wieder. 2005. The expansion and mixing time of skip graphs with applications. In
Proceedings of the 17th ACM Symposium on Parallel Algorithms and Architectures (SPAA). 126–134.
DOI:http://dx.doi.org/10.1145/1073970.1073989
James Aspnes and Yinghua Wu. 2007. O(log n)-time overlay network construction from graphs with outdegree 1. In Proceedings of the International Conference on Principles of Distributed Systems (OPODIS).
Lecture Notes in Computer Science, vol. 4878, Springer-Verlag, 286–300. DOI:http://dx.doi.org/10.1007/
978-3-540-77096-1 21
Baruch Awerbuch, Boaz Patt-Shamir, and George Varghese. 1991. Self-stabilization by local checking and
correction. In Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science (FOCS).
268–277. DOI:http://dx.doi.org/10.1109/SFCS.1991.185378
Baruch Awerbuch, Boaz Patt-Shamir, George Varghese, and Shlomi Dolev. 1994. Self-stabilization by local checking and global reset. In Proceedings of the 8th International Workshop on Distributed Algorithms (WDAG). Lecture Notes in Computer Science, vol. 857, 326–339. DOI:http://dx.doi.org/10.1007/
BFb0020443
Baruch Awerbuch and Christian Scheideler. 2003. Peer-to-peer systems for prefix search. In Proceedings
of the 22nd ACM Symposium on Principles of Distributed Computing (PODC). ACM, New York, DOI:
http://dx.doi.org/10.1145/872035.872053
Baruch Awerbuch and Christian Scheideler. 2004. The hyperring: A low-congestion deterministic data structure for distributed environments. In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA). 318–327.
Baruch Awerbuch and George Varghese. 1991. Distributed program checking: A paradigm for building selfstabilizing distributed protocols. In Proceedings of the 32nd Annual IEEE Symposium on Foundations
of Computer Science (FOCS). 258–267.
Andrew Berns, Sukumar Ghosh, and Sriram Pemmaraju. 2011. Building self-stabilizing overlay networks
with the transitive closure framework. In Proceedings of the 13th International Symposium on Stabilization, Safety, and Security of Distributed Systems (SSS).
Ankur Bhargava, Kishore Kothapalli, Chris Riley, Christian Scheideler, and Mark Thober. 2004. Pagoda: A
dynamic overlay network for routing, data management, and multicasting. In Proceedings of the 16th
Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA). 170–179.
J. Brzezinski and M. Szychowiak. 2000. Self-stabilization in distributed systems - A short survey. Found.
Comput. Dec. Sci. 25, 1.
E. Caron, A. K. Datta, F. Petit, and C. Tedeschi. 2008. Self-stabilization in tree-structured peer-to-peer service
discovery systems. In Proceedings of the 27th International Symposium on Reliable Distributed Systems
(SRDS).

Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

SKIP+ : A Self-Stabilizing Skip Graph

36:25

Thomas Clouser, Mikhail Nesterenko, and Christian Scheideler. 2008. Tiara: A self-stabilizing deterministic
skip list. In Proceedings of the 10th International Symposium on Stabilization, Safety, and Security of
Distributed Systems (SSS).
A. Costello and G. Varghese. 1996. Self-stabilization by window washing. In Proceedings of the 15th ACM
Symposium on Principles of Distributed Computing (PODC). 35–44.
Curt Cramer and Thomas Fuhrmann. 2005. Self-stabilizing ring networks on connected graphs. Tech. Rep.
2005-5. System Architecture Group, University of Karlsruhe.
E. W. Dijkstra. 1974. Self-stabilization in spite of distributed control. Commun. ACM 17, 643–644.
S. Dolev. 2000. Self-Stabilization. MIT Press.
S. Dolev and T. Herman. 1997. Superstabilizing protocols for dynamic distributed systems. Chicago J. Theoret.
Comput. Sci. 4, 1–40.
Peter Druschel and Antony Rowstron. 2001. Pastry: Scalable, distributed object location and routing for largescale peer-to-peer systems. In Proceedings of the 18th IFIP/ACM International Conference on Distributed
Systems Platforms (Middleware). 329–350. See also http://research.microsoft.com/∼antr/Pastry.
Dominik Gall, Riko Jacob, Andra Richa, Christian Scheideler, Stefan Schmid, and Hanjo Täubig. 2010. Time
complexity of distributed topological self-stabilization: The case of graph linearization. In Proceedings
of the 9th Latin American Theoretical Informatics Symposium (LATIN).
Michael T. Goodrich, Michael J. Nelson, and Jonathan Z. Sun. 2006. The rainbow skip graph: A fault-tolerant
constant-degree distributed data structure. In Proceedings of the 17th Annual ACM-SIAM Symposium
on Discrete Algorithm (SODA). 384–393.
N. J. A. Harvey and J. I. Munro. 2004. Deterministic SkipNet. Inf. Process. Lett. 90, 4, 205–208.
Nicholas J. A. Harvey, Michael B. Jones, Stefan Saroiu, Marvin Theimer, and Alec Wolman. 2003. SkipNet: A
scalable overlay network with practical locality properties. In Proceedings of the 4th USENIX Symposium
on Internet Technologies and Systems (USITS). 113–126.
T. Herman. 2002. Self-stabilization bibliography: Access guide. University of Iowa. (ftp://ftp.cs.uiowa.edu/
pub/selfstab/bibliography/.)
Riko Jacob, Andrea Richa, Christian Scheideler, Stefan Schmid, and Hanjo Täubig. 2009a. A distributed
polylogarithmic time algorithm for self-stabilizing skip graphs. In Proceedings of the ACM Symposium
on Principles of Distributed Computing (PODC).
Riko Jacob, Stephan Ritscher, Christian Scheideler, and Stefan Schmid. 2009b. A self-stabilizing and local
delaunay graph construction. In Proceedings of the 20th International Symposium on Algorithms and
Computation (ISAAC).
S. Katz and K. Perry. 1993. Self-stabilizing extensions for message-passing systems. Distrib. Comput. 7, 1,
17–26.
Sebastian Kniesburges, Andreas Koutsopoulos, and Christian Scheideler. 2011. Re-Chord: A self-stabilizing
chord overlay network. In Proceedings of the 23rd ACM Symposium on Parallelism in Algorithms and
Architectures (SPAA).
F. Kuhn, S. Schmid, and R. Wattenhofer. 2005. A self-repairing peer-to-peer system resilient to dynamic
adversarial churn. In Proceedings of the 4th International Workshop on Peer-To-Peer Systems (IPTPS).
S. Kutten and B. Patt-Shamir. 1997. Time-adaptive self stabilization. In Proceedings of the 16th ACM
Symposium on Principles of Distributed Computing (PODC). 149–158.
S. Kutten and D. Peleg. 1995. Fault-local distributed mending. In Proceedings of the 14th ACM Symposium on
Principles of Distributed Computing (PODC). 20–27.
Christoph Lenzen, Jukka Suomela, and Roger Wattenhofer. 2009. Local algorithms: Self-stabilization on
speed. In Proceedings of the 11th International Symposium on Stabilization, Safety, and Security of
Distributed Systems (SSS).
Dahlia Malkhi, Moni Naor, and David Ratajczak. 2002. Viceroy: A scalable and dynamic emulation of the
butterfly. In Proceedings of the 21st Annual Symposium on Principles of Distributed Computing (PODC).
183–192.
Petar Maymounkov and David Mazières. 2002. Kademlia: A peer-to-peer information system based on the
XOR metric. In Proceedings of the 1st International Workshop on Peer-to-Peer Systems (IPTPS). 53–65.
M. Naor and U. Wieder. 2003. Novel Architectures for P2P Applications: The continuous-discrete approach.
In Proceedings of the 15th Annual ACM Symposium on Parallelism in Algorithms and Architectures
(SPAA).
Moni Naor and Udi Wieder. 2004. Know thy neighbor’s neighbor: Better routing for skip-graphs and small
worlds. In Proceedings of the 3rd International Conference on Peer-to-Peer Systems (IPTPS). SpringerVerlag, 269–277. DOI:http://dx.doi.org/10.1007/978-3-540-30183-7 26

Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

36:26

R. Jacob et al.

Melih Onus, Andrea Richa, and Christian Scheideler. 2007. Linearization: Locally self-stabilizing sorting
in graphs. In Proceedings of the 9th Workshop on Algorithm Engineering and Experiments (ALENEX).
SIAM.
David Peleg. 2000. Distributed computing: A Locality-Sensitive Approach. SIAM.
Sylvia Ratnasamy, Paul Francis, Mark Handley, Richard Karp, and Scott Schenker. 2001. A scalable contentaddressable network. In Proceedings of the ACM SIGCOMM Conference on Applications, Technologies,
Architectures, and Protocols for Computer Communications. 161–172.
Christian Scheideler and Stefan Schmid. 2009. A distributed and oblivious heap. In Proceedings of
the 36th International Colloquium on Automata, Languages and Programming (ICALP). Lecture
Notes in Computer Science (LNCS), vol. 5556, Springer, Berlin/Heidelberg, Germany, 571–582. DOI:
http://dx.doi.org/10.1007/978-3-642-02930-1-47
Ayman Shaker and Douglas S. Reeves. 2005. Self-stabilizing structured ring topology P2P systems. In
Proceedings of the 5th IEEE International Conference on Peer-to-Peer Computing. 39–46.
Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan. 2001. Chord: A
Scalable Peer-to-Peer Lookup Service for Internet Applications. Tech. Rep. MIT-LCS-TR-819. MIT.
G. Varghese. 1992. Self-stabilization by local checking and correction. Ph.D. Dissertation, MIT.
G. Varghese. 1994. Self stabilization by counter flushing. In Proceedings of the 13th ACM Symposium on
Principles of Distributed Computing (PODC).
Received June 2012; revised January 2013 and July 2013; accepted May 2014

Journal of the ACM, Vol. 61, No. 6, Article 36, Publication date: November 2014.

Theory Comput Syst (2014) 55:110–135
DOI 10.1007/s00224-013-9504-x

A Note on the Parallel Runtime of Self-Stabilizing
Graph Linearization
Dominik Gall · Riko Jacob · Andrea Richa ·
Christian Scheideler · Stefan Schmid ·
Hanjo Täubig

Published online: 22 September 2013
© Springer Science+Business Media New York 2013

A preliminary version of this work without all proofs and simulations has been presented at the 9th
Latin American Theoretical Informatics Symposium (LATIN) 2010. This work is partly supported by
the National Science Foundation (NSF grant CCF-0830704) and by the German Research
Foundation (DFG project SCHE 1592/1-1 and SFB 901: On-the-Fly Computing)
D. Gall · R. Jacob · H. Täubig
Department of Computer Science, Institut für Informatik, Technische Universität München,
Boltzmannstr. 3, 85748 Garching, Germany
D. Gall
e-mail: dominik.gall@mytum.de
R. Jacob
e-mail: rjacob@inf.ethz.ch
H. Täubig
e-mail: taeubig@in.tum.de
Present address:
R. Jacob
Eidgenössische Technische Hochschule (ETH) Zürich, Universitätstr. 6, 8092 Zürich, Switzerland
A. Richa
Department of Computer Science and Engineering, Arizona State University, Box 878809, Tempe,
AZ 85287-8809, USA
e-mail: aricha@asu.edu
C. Scheideler
Department of Computer Science, Institut für Informatik, Universität Paderborn, Fürstenallee 11,
33102 Paderborn, Germany
e-mail: scheideler@upb.de

B

S. Schmid ( )
Telekom Innovation Labs & Technische Universität Berlin, Ernst-Reuter-Platz 7, 10587 Berlin,
Germany
e-mail: stefan@net.t-labs.tu-berlin.de

Theory Comput Syst (2014) 55:110–135

111

Abstract Topological self-stabilization is an important concept to build robust open
distributed systems (such as peer-to-peer systems) where nodes can organize themselves into meaningful network topologies. The goal is to devise distributed algorithms where nodes forward, insert, and delete links to neighboring nodes, and that
converge quickly to such a desirable topology, independently of the initial network
configuration. This article proposes a new model to study the parallel convergence
time. Our model sheds light on the achievable parallelism by avoiding bottlenecks of
existing models that can yield a distorted picture. As a case study, we consider local
graph linearization—i.e., how to build a sorted list of the nodes of a connected graph
in a distributed and self-stabilizing manner. In order to study the main structure and
properties of our model, we propose two variants of a most simple local linearization
algorithm. For each of these variants, we present analyses of the worst-case and bestcase parallel time complexities, as well as the performance under a greedy selection
of the actions to be executed. It turns out that the analysis is non-trivial despite the
simple setting, and to complement our formal insights we report on our experiments
which indicate that the runtimes may be better in the average case.
Keywords Distributed algorithms · Distributed systems · Peer-to-peer systems ·
Self-stabilization · Overlay networks · Performance

1 Introduction
Open distributed systems such as peer-to-peer systems are often highly transient in
the sense that nodes join and leave at a fast pace. In addition to this natural churn,
parts of the network can be under attack, causing nodes to leave involuntarily. Thus,
robustness is a prime concern in the design of such a system. Over the last years,
researchers have proposed many interesting approaches to build robust overlay networks. A particularly powerful concept in this context is (distributed) topological
self-stabilization. A topological self-stabilizing mechanism guarantees that:
1. By local neighborhood changes (i.e., by creating, forwarding and deleting links
with neighboring nodes), the nodes will eventually form an overlay topology with
desirable properties from any initial (and in our case: connected) topology; this is
known as the convergence property. (The assumption that the initial topology is
connected is the fundamental minimal requirement to manage any topology in a
distributed manner, as no communication is possible between disconnected components. In order to re-establish connectivity, we assume an external mechanism
such as trackers or well-known bootstrap peers.)
2. The system will also stay in a correct configuration provided that no external topological changes occur; this property is called the closure property.
In this article, we address one of the first and foremost questions in distributed
topological self-stabilization: How to measure the parallel time complexity? We consider a very strong adversary who presents our algorithm with an arbitrary connected
network. We want to investigate how long it takes until the topology reaches a (to
be specified) desirable configuration. While several solutions have been proposed in

112

Theory Comput Syst (2014) 55:110–135

the literature over the last years, these known models are inappropriate to adequately
model parallel efficiency: either they are overly pessimistic in the sense that they
can force the algorithm to work serially, or they are too optimistic in the sense that
contention or congestion issues are neglected.
Our model is aware of bottlenecks in the sense that nodes cannot perform too much
work per time unit. Thus, we consider our new model as a further step to explore the
right level of abstraction to measure parallel execution times.
As a case study, we investigate the problem of graph linearization where nodes—
initially in a set of arbitrary connected graph components—are required to sort themselves with respect to their identifiers. As the most simple form of topological selfstabilization, linearization allows to study the main properties of our model. As we
will see in our analysis, graph linearization under our model is already non-trivial and
reveals an interesting structure.
This article focuses on two natural linearization algorithms, such that the influence
of the modeling becomes clear. For our analysis, we will assume the existence of
some hypothetical schedulers. In particular, we consider schedulers that, for each
round, make one of the following selections for the actions/rules (or synonymously:
parallel (independent) steps) to execute: one scheduler always makes a best possible,
one a random, and one a “greedy” selection. Since the schedulers are only used for
the complexity analysis of the protocols proposed, for ease of explanation, we treat
the schedulers as global entities and we make no attempt to devise distributed, local
mechanisms to implement them.1
1.1 Related Work
The first article to study self-stabilization in the context of distributed computing was
[11] by E. W. Dijkstra. After Dijkstra’s seminal work on the token ring, researchers
have investigated self-stabilization in many other domains such as clock synchronization or fault containment. In 1991, Awerbuch and Varghese [4] proved that every local
algorithm can be made self-stabilizing if all nodes keep a log of the state transitions
until the current state. For a general overview of the field, the reader is referred to [7,
13, 18].
Our article focuses on topological self-stabilization. The construction and maintenance of a given network structure is of prime importance in many distributed systems, for example in peer-to-peer computing [12, 14, 24]. In the technical report of
the distributed hash table Chord [25], stabilization protocols are described which allow the topology to recover from certain degenerate situations. Unfortunately, however, no algorithms are given to recover from arbitrary states. Similarly, also skip
graphs [2] can be repaired from certain states, namely states which resulted from
node faults and inconsistencies due to churn.
1 In fact, most likely no such local mechanism exists for implementing the worst-case and best-case sched-

ulers, while we believe that local distributed implementations that closely approximate—within a constant
factor of the parallel complexity—the randomized and greedy schedulers presented here would not be hard
to devise.

Theory Comput Syst (2014) 55:110–135

113

In order to gain insights into how to construct or self-stabilize more complex
topologies such as hypercubic networks, in the last years, researchers started to analyze line and ring networks. The Iterative Successor Pointer Rewiring Protocol [10]
and the Ring Network [24] organize the nodes in a sorted ring. Unfortunately, both
protocols have a large runtime. In [1], Angluin et al. present an efficient asynchronous
algorithm which takes an initially weakly connected pointer graph and constructs a
linked list with low contention. However, their algorithm is not self-stabilizing. In a
follow-up paper [3], a self-stabilizing algorithm is given which assumes that nodes
initially have out-degree 1.
The question of how to efficiently build a certain network structure is also related
to resource discovery [17], leader election [8], and parallel sorting [16] problems.
For instance, [17] analyzes how processes in a initial weakly connected knowledge
graph can learn the identities of all other processes, [8] gives a deterministic algorithm
for leader election in an initially connected knowledge graph, and [16] proposes a
sorting algorithm for a parallel pointer machine that builds a binary tree.
The works closest to ours are by Onus et al. [23] and by Clouser et al. [9]. In [23],
a local-control strategy called linearization is presented for converting an arbitrary
connected graph into a sorted list. However, the strategy allows a node to communicate with an arbitrary number of its neighbors, which can be as high as Θ(n) for
n nodes and is not scalable. Clouser et al. [9] formulated a variant of the linearization technique for arbitrary asynchronous systems in which edges are represented
as Boolean shared variables. Any node may establish an undirected edge to one of
its neighbors by setting the corresponding shared variable to true, and in each time
unit, a node can manipulate at most one shared variable. If these manipulations never
happen concurrently, it would be possible to emulate the shared variable concept in
a message passing system in an efficient way. However, concurrent manipulations
of shared variables can cause scalability problems because even if every node only
modifies one shared variable at a time, the fact that the other endpoint of that shared
variable has to get involved when emulating that action in a message passing system
implies that a single node may get involved in up to Θ(n) many of these variables in
a time unit.
Recently, Jacob et al. [20] generalized insights gained from graph linearization [15] to two dimensions, and presented a self-stabilizing O(n3 )-time construction
for Delaunay graphs. Moreover, for a local-checkable variant of a skip graph, a polylogarithmic maintenance algorithm has been described in [22], and a self-stabilizing
variant of a Chord graph appears in [21]. These works study a simpler model for the
parallel runtime complexity that ignores congestion.
1.2 Our Contributions
The contributions of this article are two-fold. First, we present an alternative approach to modeling scalability of distributed, self-stabilizing algorithms that does not
require synchronous executions like in [23] and also gets rid of the scalability problems in [9, 23] therefore allowing us to study the parallel time complexity of the
proposed linearization approaches. Concretely, in our model, an independent set of
nodes participates in the parallel execution of the different actions of the given round.

114

Theory Comput Syst (2014) 55:110–135

Second, we propose two variants of a simple, local linearization algorithm. For
each of these variants, we present extensive formal analyses of their worst-case and
best-case parallel time complexities, i.e., the number of (parallel) steps until the nodes
converge to a desired fixpoint topology, and study their performance under a random
and a greedy selection of the actions to be executed. We also validate the behavior of
these algorithms by experiments which complement our formal findings, and indicate
that the runtimes may in fact be better in practice. Finally, this article discusses a
particular situation that illustrates how the new model compares to others proposed
in the literature.
1.3 Organization
The remainder of this article is organized as follows. In Sect. 2, we describe our
setting and the graph linearization problem, and introduce our model for the parallel
time complexity. Section 3 presents a self-stabilizing algorithm together with a formal
analysis. We report on our simulation results in Sect. 4. After discussing our approach
and comparing our model to alternative frameworks in Sect. 5, we conclude the article
in Sect. 6.
2 Model
We are given a system consisting of a fixed set V of n nodes. Every node has a unique
(but otherwise arbitrary) and constant integer identifier. In the following, if we compare two nodes u and v using the notation u < v or u > v, we mean that the identifier
of u is smaller than v or vice versa. For any node v, pred(v) denotes the predecessor
of v (i.e., the node u ∈ V of largest identifier with u < v) and succ(v) denotes the
successor of v according to “<”. Two nodes u and v are called consecutive if and
only if u = succ(v) or v = succ(u).
Connections between nodes are modeled as shared variables. Each pair (u, v) of
nodes shares a Boolean variable e(u, v) which specifies an undirected adjacency relation: u and v are called neighbors if and only if this shared variable is true.
The set of neighbor relations defines an undirected graph or network G = (V , E)
among the nodes. A variable e(u, v) ∈ E, a link between u and v (in the following,
sometimes simply referred to as an undirected link {u, v}), can only be changed by
u and v, and both u and v have to be involved in order to change e(u, v). (More
details on this will be given below.) For any node u ∈ V , let u.L denote the set of left
neighbors of u—the neighbors which have smaller identifiers than u—and u.R the
set of right neighbors of u.
In this article, deg(u) = |u.L ∪ u.R| will denote the degree of a node u. Moreover,
the distance between two nodes dist(u, v) is defined as dist(u, v) = |{w : u < w ≤ v}|
if u < v and dist(u, v) = |{w : v < w ≤ u}| otherwise. The length of an edge e =
{u, v} ∈ E is defined as len(e) = dist(u, v).
We consider distributed algorithms which are run by each node in the network.
The algorithm or program executed by each node consists of a set of variables and
actions (often also referred to as rules). An action has the form
< name > : < guard > →

< commands >

Theory Comput Syst (2014) 55:110–135

115

where < name > is an action label, < guard > is a Boolean predicate over the
(local and shared) variables of the executing node and < commands > is a sequence
of commands that may involve any local or shared variables of the node itself or
its neighbors. Given an action A, the set of all nodes involved in the commands is
denoted by V (A). Every node that either owns a local variable or is part of a shared
variable e(u, v) accessed by one of the commands in A is part of V (A). Two actions
A and B are said to be independent if V (A) ∩ V (B) = ∅. For an action execution to
be scalable we require that the number of operations involving interactions between
the nodes (and therefore |V (A)|) is independent of n.
An action is called enabled if and only if its guard is true. Every enabled action is
passed to some underlying scheduling layer (to be specified below). The scheduling
layer decides whether to accept or reject an enabled action. If it is accepted, then the
action is executed by the nodes involved in its commands.
We model distributed computation as follows. The assignments of all local and
shared variables define a system configuration. Since our algorithms consider only
variables that directly effect the topology, a configuration represents a graph. Hence,
in the following, we will often treat the terms graph, topology, and configuration, as
synonyms.
A computation or execution is a sequence of configurations, such that for each
configuration ci (a graph) at the beginning of (computation) step i, the next configuration ci+1 (the next graph) is obtained after executing an action that was selected by
the scheduling layer in step i.
The concepts of sequences of configurations and of steps are useful to reason
about the correctness of the self-stabilizing algorithm. In order to study the parallel
time complexity, we additionally define the concept of a (parallel time) round: In
each round, the scheduling layer may select any set of independent, enabled actions
to be executed by the nodes, that is, a round consists of a set of parallel steps. Indeed,
for the runtime analysis, we may think of the independent steps executed in parallel
in a round as simultaneous.
Finally, the work performed (e.g., per round) is defined to the number of actions
selected by the scheduling layer (in that round).
The following definition summarizes these concepts.
Definition 1 (Step, Round, Work) An enabled rule which is chosen and executed by
the scheduler is called a (computation) step. The set of independent rules selected
and executed in parallel by the scheduler constitutes a (parallel time) round (a set of
parallel steps). The work performed in a round is defined to the number of actions
selected by the scheduling layer (e.g., in that round).
In this paper, we will typically think of the execution as a sequence of configurations (i.e., topologies) following each as (computation) steps. In other words, the
configuration ci is obtained from ci−1 by the execution of step si−1 .
The following definition summarized the self-stabilization requirements (see also
Chap. 2.2 in [13]).
Definition 2 (Self-Stabilizing Algorithm) A self-stabilizing distributed algorithm
can be started in any arbitrary configuration (topology) and will eventually exhibit

116

Theory Comput Syst (2014) 55:110–135

a desired legal (or safe) behavior. We define the desired legal behavior as a set of
legal executions LE (for a particular system and a task). Every system execution
should have the suffix that appears in LE. A configuration c is safe with regard to a
task LE and an algorithm if every fair execution of the algorithm that starts from c
belongs to LE; an algorithm is self-stabilizing for a task LE if every fair execution
of the algorithm reaches a safe configuration with relation to LE (convergence) and
stays there (closure).
Notice that our model can cover arbitrary asynchronous systems in which the actions are implemented so that the sequential consistency model applies (i.e., the outcome of the executions of the actions is equivalent to a sequential execution of them)
as well as parallel executions in synchronous systems. In a round, the set of enabled
actions selected by the scheduler must be independent as otherwise a configuration
transition from one round to another would, in general, not be unique, and further
rules would be necessary to handle dependent actions that we want to abstract from
in this article.
2.1 Linearization
In this article, we are interested in designing distributed algorithms that can transform any connected component of an initial graph G0 = (V , E0 ) into a sorted list
(according to the node identifiers) using only local interactions between the nodes.
A distributed algorithm is called (topologically) self-stabilizing, if for any initial
configuration or topology G0 = (V , E0 ), it eventually arrives at a configuration GL =
(V , EL ) in which the nodes are each connected component form a sorted list. In the
following, for ease of presentation, we will typically assume that G0 forms a single
connected component: if a graph consists of multiple connected components, G0 can
be a placeholder for any of these connected components. The components are treated
completely independently by our algorithms.
Definition 3 (Linear/Chain Graph GL ) Given a set of nodes V , the linear/chain
graph GL is defined as GL = (V , EL ) such that {u, v} ∈ EL if and only if
e(u, v) = 1

⇔

u = succ(v) ∨ v = succ(u)

Once the self-stabilizing algorithm arrives at this configuration (the legal configuration), it should stay there, i.e., the configuration is a fixpoint of the algorithm. In the
distributed algorithms studied in this article, each node u ∈ V repeatedly performs
simple linearization steps in order to arrive at that fixpoint.
Note that for a given connected initial graph G0 = (V , E0 ), its linearized graph is
unique.
Lemma 1 GL = (V , EL ) is uniquely defined for a given node set V .
Proof Definition 3 requires that e(u, v) = 1 if and only if u = succ(v) ∨ v = succ(u).
Since nodes V have unique identifiers, for any given node v ∈ V , its successor is
uniquely defined.


Theory Comput Syst (2014) 55:110–135

117

Fig. 1 Left and right linearization: node u forwards the link to its neighbor without violating connectivity

Definition 4 (Linearization) A linearization algorithm is a distributed self-stabilizing
algorithm (according to Definition 2) where
1. an initial configuration c1 ⊆ C forms any (undirected) connected graph G0 =
(V , E0 ),
2. the only legal configuration L = {cl } ⊆ C is the linear topology GL = (V , EL ) on
the nodes V (i.e., EL connects consecutive nodes, see Definition 3), and
3. actions only update the neighborhoods of the nodes (in our case, left and right
linearization steps).
Our linearization algorithms will be based on simple linearization rules. A linearization involves three nodes u, v, and w with the property that u is directly connected to v and w and either u < v < w or w < v < u. In both cases, u may command
the nodes to move the edge {u, w} to {v, w}. If u < v < w, this is called a right linearization and otherwise a left linearization step (see also Fig. 1). Since only three
nodes are involved in such a linearization step, this can be formulated by a scalable
action. In the following, we will also call u, v, and w a linearization triple or simply
a triple.
2.2 Schedulers
Our goal is to find linearization algorithms that spend as little time and work as possible in order to arrive at a sorted list. In order to investigate their worst, average,
and best performance under concurrent executions of actions, we consider different schedulers. Essentially, the scheduler chooses a set of enabled actions from the
fire-table and executes the corresponding steps in parallel, thus defining on how configuration (or topology) ci becomes configuration ci+1 .
1. Worst-case scheduler Swc : This scheduler must select a maximal independent set
of enabled actions in each round, but it may do so to enforce a runtime (or work)
that is as large as possible.
2. Randomized scheduler Srand : This scheduler considers the set of enabled actions
in a random order and selects, in one round, every action that is independent of
the previously selected actions in that order.
3. Greedy scheduler Sgreedy : This scheduler orders the nodes according to their degrees, from maximum to minimum. For each node that still has enabled actions
left that are independent of previously selected actions, the scheduler picks one of
them in a way specified in more detail later in this article when our self-stabilizing
algorithm has been introduced. (Note, that ‘greedy’ refers to a greedy behavior
w.r.t. the degree of the nodes; large degrees are preferred. Another meaningful
‘greedy’ scheduler could favor triples with largest gain w.r.t. the potential function that sums up all link lengths.)

118

Theory Comput Syst (2014) 55:110–135

4. Best-case scheduler Sopt : The enabled actions are selected in order to minimize
the runtime (or work) of the algorithm. (Note that ‘best’ in this case requires maximal independent sets although there might be a better solution without this restriction.)
The worst-case and best-case schedulers are of theoretical interest and allow us to
explore the parallel time complexity of the linearization approach. The greedy scheduler is a concrete algorithmic selection rule that we mainly use in the analysis as a
lower bound on the performance under a best-case scheduler.
The randomized scheduler allows us to investigate the average case performance
when a local-control randomized symmetry breaking approach is pursued in order to
ensure sequential consistency while selecting and executing enabled actions.
As noted in the introduction, for ease of explanation, we treat the schedulers as
global entities and we make no attempt to formally devise distributed, local mechanisms to implement them (that would in fact be an interesting, orthogonal line for
future work). The schedulers are used simply to explore the parallel time complexity limitations (e.g., worst-case, average-case, best-case behavior) of the linearization
algorithms proposed. In practice the algorithms LINall and LINmax to be presented
below may rely on any local-control rule (scheduler) to decide on a set of locally independent actions—which trivially leads to global independence—to perform at any
given time.

3 Algorithms and Analysis
We now introduce our distributed and self-stabilizing linearization algorithms LINall
and LINmax . Section 3.1 specifies our algorithms formally and gives correctness
proofs. Subsequently, we study the algorithms’ runtime.
3.1 LINall and LINmax
Both algorithms LINall and LINmax are based on two simple linearization rules:
linearize right and linearize left. (Fig. 1 visualizes the corresponding commands
of these rules.) However, the two algorithms differ in the preconditions (i.e., the
guards) when the rules are enabled.
In LINall , each node v ∈ V enables the linearization rules for all possible triples
that are incident to v. More formally, for every node u, we have the following rules
for every pair of neighbors v and w:
linearize left(v, w): (v, w ∈ u.L ∧ w < v < u) → e(u, w) := 0, e(v, w) := 1
linearize right(v, w): (v, w ∈ u.R ∧ u < v < w) → e(u, w) := 0, e(v, w) := 1
LINmax is similar to LINall , but instead of proposing all possible triples on each
side to the scheduler, LINmax only proposes the triple which is the furthest on the
corresponding side.
In LINmax , every node u ∈ V uses the following rules for every pair of neighbors
v and w:

Theory Comput Syst (2014) 55:110–135

119

linearize left(v, w): (v, w ∈ u.L) ∧ (w < v < u) ∧ (x ∈ u.L \ {w} : x < v) →
e(u, w) := 0, e(v, w) := 1
linearize right(v, w): (v, w ∈ u.R) ∧ (u < v < w) ∧ (x ∈ u.R \ {w} : x > v) →
e(u, w) := 0, e(v, w) := 1
We first show a basic property of LINall and LINmax .
Lemma 2 Let G0 = (V , E0 ) be an initial configuration (or graph), and let Gt =
(V , Et ) be the graph computed by LINall or LINmax , in step t. Then, it holds that if
G0 is connected, also Gt is connected.
Proof We will prove the lemma by induction over the execution, i.e., over the sequence of configurations and steps. Concretely, for the induction, we will prove that
if the configuration Gt describes a connected topology, then after step t the configuration Gt+1 will again be connected.
Also note that while the rules of LINall and LINmax differ in their preconditions
(i.e., guards), their commands are the same and consist only of left and right linearization steps. Thus, in order to study the system’s configuration transitions, we
can focus on the left and right linearization steps, and do not have to differentiate
between LINall and LINmax .
Initially, before step t = t0 , the network is connected by our assumption (connected configuration). We show that a linearization step of LINall and LINmax at
any time t > t0 will not disconnect the graph. Without loss of generality, consider
a triple u, v, w ∈ V with u < v < w, {u, v} ∈ E, and {u, w} ∈ E (cf Fig. 1), which
is right-linearized. (The proof for left-linearizations follows from symmetry arguments.) Clearly, the addition of a new edge cannot disconnect the network, and hence,
it suffices to study the effect of removing the edge e := {u, w} from E. Consider two
arbitrary distinct nodes x, y ∈ V that were connected before the linearization step. If
there is a path between x and y that does not use e, then this path also exists after
the linearization step, and connectivity is preserved. On the other hand, if all paths
between x and y use e, then x and y must still be connected as well, as e can be emulated by the edges {u, v} and {v, w}. Thus, the resulting configuration (i.e., topology)
is connected again, and the claim follows.

We can now prove that LINall and LINmax are correct in the sense that eventually,
a linearized graph will be reached.
Theorem 1 LINall and LINmax are self-stabilizing linearization algorithms.
Proof According to Definition 2, a self-stabilizing algorithm must guarantee that
starting from any configuration, (1) the system will eventually reach a correct configuration (convergence), and (2) the system will also stay in a correct configuration
provided that no fault occurs (closure). In particular, a linearization algorithm (Definition 4) converts any initially connected network into a sorted chain.
Closure: We know from Lemma 1 that linearization specifies a single legal configuration: the linear graph GL where consecutive nodes are connected (Definition 4).

120

Theory Comput Syst (2014) 55:110–135

To show the closure property, we must prove that the linear graph constitutes a fixpoint of LINall and LINmax : in the legal configuration, all actions are disabled and
hence neighborhoods remain unchanged. Recall that there only exist two types of
rules for LINall and LINmax : linearize left and linearize right. For the sake of contradiction, assume that at least one such rule is still enabled in the unique legal configuration. Let us examine the two actions in turn: If a linearize left rule is still enabled,
there must exist a node u having two neighbors v and w with v, w ∈ u.L where
w < v < u. However, this is a contradiction to the property that in a legal configuration, node u can have at most one predecessor (|u.L| ≤ 1). Similarly, for a linearize
right rule, a node u needs two neighbors v and w with v, w ∈ u.R and u < v < w.
This contradicts the assumption of a linearized topology where u can have at most
one successor (|u.R| ≤ 1). All actions must hence be disabled, and the linearized
topology will remain unchanged.
Convergence: Let us now examine the convergence property. First note that if the
network is in a configuration where it is connected but it does not constitute the linear
chain graph yet, then there must exist a node having at least two left neighbors or at
least two right neighbors. Accordingly, the linearize left or linearize right rule is
enabled and will continue to change the topology in this step.
To show eventual convergence to the unique legal configuration (the linear graph),
we will prove that after any execution of the linearize left or linearize right rule, the
topology will come “closer” to the linearized configuration, in the following sense:
we can define a potential function whose value is monotonically decreased with each
executed rule (and hence an arbitrary sequence of interleaved executions), and which
is minimized for the linearized network topology.
We consider the potential function Ψ that sums up the lengths (hop distances)
of
 all existing links with respect to the linear ordering of the nodes, i.e., Ψ =
e∈E len(e). Due to our assumptions, in any initial configuration, it holds that the
network G0 is connected, and hence Ψ ≥ n − 1: a connected graph consists of at least
n − 1 edges.
Whenever an action is executed (in our case: a linearization step is performed), the
potential Ψ is reduced by at least the length of the shorter edge in the linearization
triple, i.e., by len({u, v}) ≥ 1 (see Fig. 1): edge {u, w} is removed and potentially
an edge {v, w} inserted (if it does not exist already). Thus, LINall and LINmax will
eventually reach a topology of minimal potential Ψ , where all actions are disabled.
We know from Lemma 2 that LINall or LINmax will never disconnect an initially
connected graph again. However, the only connected topology with minimum Ψ (i.e.,
minimum edge lengths) is the desired legal configuration (the line topology).
Thus, there is always an action enabled unless the graph reached the target topology. Therefore, the network converges to a line in a finite number of steps, and the
claim follows.

3.2 Runtime
We first study the worst case scheduler Swc for both LINall and LINmax .
Theorem 2 Under a worst-case scheduler Swc , LINmax terminates after O(n2 ) work
(single linearization steps), where n is the total number of nodes in the system. This

Theory Comput Syst (2014) 55:110–135

121

is tight in the sense that there are situations where under a worst-case scheduler Swc ,
LINmax requires Ω(n2 ) rounds.
Proof Upper Bound: In order to study the evolution of the configurations (i.e.,
topologies) over time (i.e., over the execution), we define a potential function over
the topology. Let ζl (v) denote the length of the longest edge out of node v ∈ V to
the left and let ζr (v) denote the length of the longest edge out of node v to the right.
If node v does not have any edge to the left, we set ζl (v) = 12 , and similarly for the
right. We consider the potential function Φ which is defined as

Φ=
ζl (v) + ζr (v).
v∈V

Let Φi denote the value of Φ after i time steps. Observe that initially, Φ0 ≤ n2 , as
ζl (v) + ζr (v) ≤ n for each node v. We show that after i linearization steps, the potential is at most Φi ≤ n2 − 2i . Since LINmax terminates (cf also Theorem 1) with
a potential Φj > 0 for some j (the term of each node is positive), the claim follows. In order to see why the potential is reduced by at least 12 in every step, consider a triple u, v, w which is right-linearized and where u < v < w, {u, v} ∈ E,
and {u, w} ∈ E. (Left-linearizations are similar and not discussed further here.) During the linearization step, {u, w} is removed from E and the edge {v, w} is added
if it did not already exist. We are interested in the change of the value, of Φ, i.e.,
Φ = ζr (u)+ζr (v)+ζl (w). Since the rightmost neighbor of u changes from w
to v, we know ζr (u) = dist(u, v) − dist(u, w) = −1. Further, we know that ζr (v)
changes its value only if there was no edge {v, w} before the linearization step. Thus,
we have ζr (v) ≤ dist(v, w) − 12 = 1 − 12 = 12 . Since w had a left neighbor (u) before
the linearization, the value of ζl (w) cannot increase, i.e., ζl (w) ≤ 0. This implies
Φ = ζr (u) + ζr (v) + ζl (w) ≤ −1 + 12 + 0 = − 12 . Since at least one triple can
be linearized in every round, this concludes the proof.
Lower Bound: We consider a simple network over a set of nodes V = {1, . . . , n},
and show that there is a scheduling strategy for this network that creates a large number of blocked nodes in each round, ending up with only constant work per round
and a quadratic number of rounds. Our sample network resembles a complete bipartite graph where the first half of all nodes is completely connected to the second half
(see Fig. 2). In addition, all nodes are adjacent to their predecessors and successors,
i.e., all links of the desired linearized topology are already present. (During linearization, one link will disappear in each step.)
Now consider a node having an incident edge which is a longest link for some
other node. Note that initially, only the leftmost and the rightmost node (if nodes
are ordered with respect to their IDs) fulfill this property (the longest edges of the
nodes on the right all end at the leftmost node, and vice versa). In the following,
we will count the number of longest left and right links incident at a node v ∈ V
and will denote such a link a (left-link or right-link) pebble. For instance, in Fig. 2,
node 1 has the longest left-link pebbles of nodes n/2 + 1, . . . , n, whereas node n has
the longest right-link pebbles of nodes 1, . . . , n/2. In the first round, the scheduler
decides to (left-)linearize node n/2 + 1 (which automatically involves nodes 1 and 2

122

Theory Comput Syst (2014) 55:110–135

Fig. 2 Bad case for linearizing a complete bipartite network. See the proof of Theorem 2 for explanations

Theory Comput Syst (2014) 55:110–135

123

according to LINmax ) and to right-linearize node n/2 (which automatically involves
nodes n − 1 and n). Observe that these two actions block all other linearization steps
since any other triple would involve some non-blocked node having a pebble, but
nodes 1 and n are the only nodes with pebbles and are blocked. Therefore, in the
first round, the edges {n/2, n} and {1, n/2 + 1} are removed, and the longest left-link
pebble of node n/2 + 1 is moved from node 1 to node 2 and the longest right-link
pebble of node n/2 is moved from node n to node n − 1.
In the next round, the scheduler decides to left-linearize node n/2 + 2 and to rightlinearize node n/2 − 1. Again, this involves nodes 1 and 2, as well as nodes n − 1
and n. Therefore all nodes with pebbles are blocked which prevents any further action. Besides removing the respective edges the effect of the round is that the longest
left-link pebble of node n/2 + 2 moves from node 1 to node 2 and the longest rightlink pebble of node n/2 − 1 moves from node n to node (n − 1). This procedure
is repeated until all longest left-link pebbles (except the one of node n) have moved
from node 1 to node 2 and all longest right-link pebbles (except the one of node 1)
have moved from node n to node n − 1. The length of this first phase is n/2 rounds.
Note that there are always exactly two linearization triples in each round (except for
the last two rounds, where only one triple is linearized). At the end of this first phase,
there is one link left from node 1 to node n, which is later linearized in parallel to
the next phase. At this point, the scheduler has created again a complete bipartite
network, which is smaller by one node on both sides.
In applying the same method recursively, the scheduler implements a series of
phases where in each phase all longest left-link pebbles (except one) move one node
to the right and all longest right-link pebbles (except one) move one node to the
left (one left-pebble and one right-pebble per round). At the end of the phase, only
one triple of the inner part can be linearized. At this time, the single outer edge is
also linearized (in all rounds before, both of the outmost nodes of the inner part are
blocked, therefore this large edge persists until then). Such a Phase i takes n/2 + 1 − i
rounds. The total number of rounds is thus at least
n/2 

n
i=1

2


+1−i =

n/2


 
i ∈ Ω n2 .

i=1



For the LINall algorithm, we obtain a slightly higher upper bound, as we will show
next. In the analysis, we need the following helper lemma.
Lemma 3 Let Ψ be any positive potential function, where Ψ0 is the initial potential
value and Ψi is the potential after the ith round of a given algorithm ALG. Assume
that Ψi ≤ Ψi−1 · (1 − 1/f ) and that ALG terminates if Ψj ≤ Ψstop for some j ∈ N.
Then, the runtime of ALG is at most O(f · log (Ψ0 /Ψstop )) rounds.
Proof From Ψi ≤ Ψi−1 ·(1−1/f ), it follows that Ψj ≤ Ψ0 ·(1−1/f )j . Now consider
0
, which leads to (using ln(1 + x) ≤ x for all x > −1)
j = f · ln ΨΨstop
Ψ

0
f ·ln Ψstop

Ψj ≤ Ψ0 · (1 − 1/f )

= Ψ0 e

f ·(ln

Ψstop
Ψ0 )·ln (1−1/f )

124

Theory Comput Syst (2014) 55:110–135

≤ Ψ0 e

Ψ

0 )·(−1/f )
f ·(ln Ψstop

= Ψ0 e

Ψ

0
− ln Ψstop

= Ψstop .



Theorem 3 LINall terminates after O(n2 log n) many rounds under a worst-case
scheduler Swc , where n is the network size.
Proof We consider the potential function
Ψ=


e∈E

len(e)

with Ψ0 ≤

 
n
(n − 1) < n3 .
2

We show that in each round, this potential is multiplied by a factor of at most 1 −
Ω(1/n2 ).
Consider an arbitrary triple u, v, w ∈ V with u < v < w which is right-linearized
by node u. (The case of left-linearizations is similar and not discussed further here.)
During a linearization step, the sum of the edge lengths is reduced by at least one.
So what is the amount of blocked potential in a round due to the linearization of the
triple (u, v, w) (cf also the proof of Theorem 4)? Nodes u, v, and w have at most
deg(u) + deg(v) + deg(w) < n many independent neighbors, and hence, in the worst
case, when the triple’s incident edges are removed (the blocked potential is at most
O(n2 )), these neighbors fall into different disconnected components which cannot
be linearized further in this round; in other words, the remaining components form
sorted lines. The blocked potential amounts to at most Θ(n2 ). Thus, together with

Lemma 3 (using Ψ0 < n3 , Ψstop ∈ O(n), f = n2 ), the claim follows.
Note that Theorem 3 suggests that allowing to linearize any neighbor (like LINall ) in
the independent sets may yield higher runtimes than restricting the selection to the
maximal neighbor.
Besides Swc , we are interested in the following type of greedy scheduler. In each
round, both for LINall and LINmax , Sgreedy orders the nodes with respect to their
remaining (total, i.e., left plus right) degrees: after a triple has been fired, the three
nodes’ incident edges are removed. For each node v ∈ V selected by the scheduler
according to this order (which still has enabled actions left which are independent
of previously selected actions), the scheduler greedily picks the enabled action of v
which involves the two most distant neighbors on the side with the larger remaining
degree. (If the number of remaining left neighbors equals the number of remaining
neighbors on the right side, then an arbitrary side can be chosen.) The intuition behind
Sgreedy is that neighborhood sizes are reduced quickly in the linearization process.
Under this greedy scheduler, we get the following improved bound on the time
complexity of LINall .
Theorem 4 Under a greedy scheduler Sgreedy , LINall terminates in O(n log n)
rounds, where n is the total number of nodes in the system.

Proof Again, we consider the potential function Ψ = e∈E len(e). As before, Ψ0 ≤
n(n − 1)2 /2. At the end we have Ψstop = n − 1. We will prove that in each round,
the potential is multiplied by a factor of at most 1 − 1/(24 · n), i.e., f (n) ≤ 24n.

Theory Comput Syst (2014) 55:110–135

125

Given this factor bound and Ψ0 /Ψstop < n2 , Lemma 3 implies that the total number
of rounds is in O(n log n).
It remains to prove that the potential is indeed reduced by a factor of 1 − Θ(1/n)
in each round. First, observe that firing a triple reduces the potential Ψ , but prevents other triples from being fired in the same round. For our analysis, we want to
bound this blocked potential. Recall our definition of the greedy scheduler Sgreedy
which always chooses the node with the largest remaining degree and selects for
the linearization operation the two neighbors which are furthest away from this
node on the side of larger degree. Consider any triple v1 , v2 , v3 ∈ V of nodes with
v1 < v2 < v3 and {v1 , v3 }, {v1 , v2 } ∈ E which is right-linearized (left-linearizations
are similar and not described further here). As we will see, removing the edge
 1 )/4,
{v1 , v3 } and adding (if necessary) edge {v2 , v3 } reduces Ψ by at least deg(v
 1 ) is the number of neighbors of v1 if the edges incident to the alwhere deg(v
ready processed nodes in this round by the greedy scheduler are removed: Note
that by removing {v1 , v3 } and possibly adding {v2 , v3 }, the potential is reduced by
at least dist(v1 , v3 ) − dist(v2 , v3 ) = dist(v1 , v2 ). Since the potential decreases by at
 1 ) = 2 and deg(v
 1 ) = 3 that Ψ deleast 1, we know for the special cases of deg(v
 1 )/4. Furthermore, according to Sgreedy , v1 has at least
creases by more than deg(v
as many remaining neighbors on the right as it has on the left, i.e., we have that
 1 )/2 − 1 (since v2 and v3 are the two most distant neighbors on
dist(v1 , v2 ) ≥ deg(v
 1 ) ≥ 4 that Ψ
the side having at least half of the neighbors). This implies for deg(v


decreases by at least deg(v1 )/2 − 1 ≥ deg(v1 )/4.
By firing the triple, we may lose the option to linearize other nodes. In order to
bound the blocked potential by this linearization step, we consider the components
that remain after nodes v1 , v2 and v3 (plus incident edges) have been removed. Let
w be an arbitrary neighbor of vi , for i ∈ {1, 2, 3}. Consider the connected component
after vi has been removed which includes w. We distinguish two different cases.
Case 1: If this connected component forms a line where nodes are ordered, the
nodes in the component cannot be linearized or scheduled further in this step. Thus,
the component blocks the potential contained in this line, which is however at most n.
Moreover, we lose the edge {vi , w} which also has a potential of at most n, yielding
a total potential of at most 2n.
Case 2: If the component has any other form, there must exist triples in it that
can still be fired later in this round, and hence, the blocked potential is accounted for
similarly during the linearization of another triple. Thus, we only have to take into
account the blocked potential due to the lost edge incident to the triple which is at
most n.
 1 ) · n: As
The total amount of blocked potential is therefore at most 6 · deg(v
 1) ≥
Sgreedy chooses the node with largest remaining degree, it holds that deg(v


max{deg(v2 ), deg(v3 )}. Since we have at most a blocked potential of 2n per neighbor
 1 ) · 2n.
of vi , for i ∈ {1, 2, 3}, the blocked potential is at most 3 · deg(v
 1 )/2 − 1 ≥ deg(v
 1 )/4, we have that Ψi ≤ (1 − 1/(24 · n))Ψi−1 = (1 −
Since deg(v

Θ(1/n))Ψi−1 , and the claim follows.
Finally, we have also investigated an optimal scheduler Sopt .

126

Theory Comput Syst (2014) 55:110–135

Theorem 5 Even under an optimal scheduler Sopt , both LINall and LINmax require
at least Ω(n) rounds in certain situations.
Proof Let v1 , v2 , . . . , vn ∈ V denote the nodes in sorted order, i.e., v1 < v2 < · · · <
vn . Consider the following initial topology G0 = (V , E0 ) where ∀i such that 0 <
i < n − 1: {vi , vi+1 } ∈ E0 . Additionally, E0 contains a long edge e := {v1 , vn } ∈ E0 .
In the beginning, edge e has length of len(e) = n − 1. Observe that in each round,
both for LINall and LINmax , the length of e is reduced by one. Thus, by induction,
it takes at least a linear number of rounds to sort G0 , as the execution is inherently
sequential.

3.3 Degree Cap
It is desirable that the nodes’ neighborhoods or degrees do not increase much during
the sorting process. We investigate the performance of LINall and LINmax under the
following degree cap model. Observe that during a linearization step, only the degree
of the node in the middle of the triple can increase (see Fig. 1). We do not schedule
triples if the middle node’s degree would increase, with one exception: during leftlinearizations, we allow a degree increase if the middle node has at most one left
neighbor, and during right-linearizations we allow a degree increase to the right if the
middle node has at most one right neighbor. In other words, we study a degree cap of
two.
We find that both our algorithms LINall and LINmax still terminate with a correct
solution under this restrictive model.
Theorem 6 With degree cap, LINmax terminates in at most O(n2 ) many rounds under a worst-case scheduler Swc , where n is the total number of nodes in the system.
Under the same conditions, LINall requires at most O(n3 ) rounds.
Proof Bound for LINmax : The claim follows from the same arguments as used in
Theorem 2. It only remains to prove that in each round there exists a triple which can
be right or left linearized. In order to see that at least one triple can be linearized,
consider the node u of largest order which has two neighbors to the right. (If there
does not exist any node with two neighbors that can be right-linearized, we apply
the same argument to the left. If there is no node with two left neighbors that can be
left-linearized, this implies that the graph is already sorted.)
Let v and w be u’s two neighbors to the right, where v < w. The triple consisting
of the three nodes u, v and w can definitely be right-linearized without violating the
degree cap constraint: v is the only node whose degree increases during the linearization step. However, v’s degree to the right cannot be more than two after linearization,
otherwise we have a contradiction to our assumption that u is the largest node with
two neighbors to the right.

Bound for LINall : We consider again the potential function Ψ = e∈E len(e) summing up all edge lengths in the graph. Note that initially, Ψ0 < n3 , and each linearization step reduces Ψ by at least one. When the graph is sorted, Ψ < n. Therefore, for
the O(n3 ) bound, it remains to prove that the system cannot deadlock and there is

Theory Comput Syst (2014) 55:110–135

127

progress in every round. However, this holds for the same reasons as discussed above

for the LINmax bound.
Interestingly, as we will see in the experimental section (Sect. 4), the runtime of
LINall and LINmax is typically better than shown in Theorem 6. Moreover, it turns
out that even without imposing a degree cap, LINall and LINmax do not increase the
maximal degrees during their computations.

4 Experiments
In order to improve our understanding of the parallel complexity and the behavior
of our algorithms, we have implemented a simulation framework which allows us to
study and compare different algorithms, topologies and schedulers. In this section,
some of our findings will be described in more detail.
We will consider the following graphs. We chose these graphs as they appeared to
be good representatives for easy, average, and difficult problem instances.
1. Random graph: Any pair of nodes is connected with probability p, i.e., if V =
{v1 , . . . , vn }, then P[{vi , vj } ∈ E] = p for all i, j ∈ {1, . . . , n}. If necessary, edges
are added to ensure connectivity.
2. Bipartite backbone graph (k-BBG): For n = 3k for some positive integer k define the following k-bipartite backbone graph on the node set V = {v1 , . . . , vn }.
All n nodes are connected to their respective successors and predecessors (except for the first and the last node). This structure is called the graph’s backbone.
Additionally, there are all (n/3)2 edges from nodes in {v1 , . . . , vk } to nodes in
{v2k+1 , . . . , vn }.
3. Spiral graph: The spiral graph G = (V , E) is a sparse graph forming a spiral, i.e., V = {v1 , . . . , vn } where v1 < v2 < · · · < vn and E = {{v1 , vn }, {vn , v2 },
{v2 , vn−1 }, {vn−1 , v3 }, . . . , {vn/2 , vn/2+1 }}.
4. k-local graph: This graph avoids long-range links. Let V = {v1 , . . . , vn } where
vi = i for i ∈ {1, . . . , n}. Then, {vi , vj } ∈ E if and only if |i − j | ≤ k.
We will constrain ourselves to two schedulers here: the greedy scheduler Sgreedy
which we have already considered in the previous sections, and a randomized scheduler Srand which among all possible enabled actions chooses one uniformly at random
at a time, deletes all conflicting actions, and repeats until a maximal non-conflicting
set of actions is chosen.
Many experiments have been conducted to shed light onto the parallel runtime of
LINall and LINmax in different networks. Figure 3 (top) depicts some of our results for
LINall . As expected, in the k-local graphs, the execution is highly parallel and yields
a “constant” runtime—independent of n. The sparse spiral graphs appear to entail
an almost linear time complexity, and also the random graphs perform better than
our analytical upper bounds suggest. Among the graphs we tested, the BBG network
yielded the highest execution times. Figure 3 (bottom) gives the corresponding results
for LINmax .

128

Theory Comput Syst (2014) 55:110–135

Fig. 3 Top: Parallel runtime of LINall for different graphs under Srand : two k-local graphs with k = 5,
k = 10 and k = 20, two random graphs with p = 0.1 and p = 0.2, a spiral graph and a n/3-BBG. Bottom:
Same experiments with LINmax (Due to high execution times, BBG is only shown up to a network size of
roughly 200 nodes)

Theory Comput Syst (2014) 55:110–135

129

A natural yardstick to measure the quality of a linearization algorithm—besides
the parallel runtime—is the node degree. For instance, it is desirable that an initially sparse graph will remain sparse during the entire linearization process. It turns
out that LINall and LINmax indeed maintain a low degree. Figure 4 shows how the
maximal and average degrees evolve over time both for LINall and LINmax on two
different random graphs. Note that the average degree cannot increase because the
rules only move or remove edges. The random graphs studied in Fig. 4 have a high
initial degree, and it is interesting to analyze what happens in case of sparse initial
graphs. Figure 5 plots the maximal node degree over time for the spiral graph. While
there is an increase in the beginning, the degree is moderate at any time and declines
again quickly.
Finally, we have studied the behavior of LINall and LINmax under a degree cap
constraint, where triples can only be linearized if the center node’s degree does not
grow to more than a certain threshold in the corresponding direction. Figure 6 (top)
indicates that all the runtime remains roughly linear even for a degree cap of two.
For degree caps larger than two, the performance is better. However, interestingly, it
seems that the number of rounds does not decrease monotonously with larger caps—
rather, a lower degree cap might help to speed-up the linearization process under
certain circumstances. Figure 6 (bottom) shows the runtimes under a BBG graph;
here, the greedy scheduler requires much more (and even super-linear) time compared
to the other settings, which indicates that this configuration together with the BBG
graph is a particularly challenging one.

5 Discussion and Model Comparison
This section provides a short discussion and also compares our approach to the alternative models, e.g., to the so-called critical path model introduced in [5, 6].
One may wonder whether our actions (left and right linearization) really have
to be executed in an independent way in order to maintain sequential consistency.
Here, it would be sufficient if each node initiates a new linearization only after its
previously initiated linearization has been completed (or canceled), however, for a
model for concurrent executions of actions to be scalable, only a bounded number of
executions of actions should be allowed to overlap at any node at any time. In order
to come up with a simple and general model taking this into account, we decided to
constrain the scheduling layer to independent sets of actions in each round.
An alternative model to study parallel complexity is the worst-case critical path
[5, 6] (i.e., the longest possible sequence of action executions that depend on each
other) of a distributed execution of our linearization approach. However, it turns out
that one can identify critical paths of length up to Θ(n3 ) for our linearization approach, which is so far away from its real performance that the critical path notion
seems to be too conservative and not meaningful in our context.
The critical path model can be defined in our framework in the following way.
Consider a worst case scheduler that schedules one action (triple) per round. These
triples form the nodes of a directed acyclic graph (DAG). An edge from an action A
to a later action B is present, if and only if a connection that B requires to be present
(or absent) was created (deleted) by A.

130

Theory Comput Syst (2014) 55:110–135

Fig. 4 Top: Maximum and average degree during a run of LINall and LINmax on a random graph with
edge probability p = 0.1. Bottom: The same experiment on a random graph with p = 0.2

Theory Comput Syst (2014) 55:110–135

131

Fig. 5 Evolution of maximal degree on spiral graphs under a randomized scheduler Srand

A simple graph family where the differences of the models become clear are the
k-BBG graphs (cf Sect. 4). In the critical path model, LINall needs Θ(n3 ) rounds to
linearize the n/3-BBG, while in our model, LINall needs at most O(n2 log n) rounds
in the worst case (cf Theorem 3). In the following, we will show a lower bound for
LINall on the n/3-BBG.
Theorem 7 There is a graph, where a worst case scheduler Swc for LINall needs
time Ω(n2 ) to finish.
Proof Consider the following graph: the (even) nodes v2 , v4 , . . . , vk−2 , vk have all
edges to the nodes v2k+1 , . . . , v3k . A worst case scheduler can transform this graph in
k rounds of LINall into the graph where the (odd) nodes v3 , v5 , . . . , vk−1 , vk+1 have
all edges to the nodes v2k+1 , . . . , v3k : In the first round, node triple (v2 , v3 , v2k+1 )
“moves” the “first” edge of node v2 to node v3 , simultaneously with (v4 , v5 , v2k+2 )
and so on, the ith even node “moving” its ith edge. More generally, in the j th round,
the ith even node “moves” its (i + j mod k)th edge to its right odd neighbor. After
k such rounds the above described second graph is reached. The actions of one round
form a maximal independent set because all long edges end at positions v2 , . . . , vk+1 ,
and are hence blocked by one of the described triples.
In total, a worst case scheduler can perform the above k rounds k times by exchanging odd for even and shifting the left side further to the right. Additionally it
uses a left shifted version of the above k rounds to transform the n/3 bipartite backbone graph into the described initial graph.


132

Theory Comput Syst (2014) 55:110–135

Fig. 6 Top: Parallel runtime (plus standard deviations) of LINmax for different cap constraints (cap = 2,
3, 5, and none) and under a random scheduler Srand . The initial topologies are random graphs with edge
probability 0.2. Bottom: Parallel time complexities of LINall and LINmax on the n/3-BBG for different
network sizes under the Sgreedy and the Srand scheduler

Theory Comput Syst (2014) 55:110–135

133

Figure 6 (bottom) plots the performance of LINall and LINmax on the BBG under
different schedulers. Unfortunately, as some simulations require much computing resources, we have only generated experimental data up to certain network sizes. However, we can already see that while LINall is slow under Sgreedy , the other times are
comparable and roughly linear.
For the critical path model, the picture looks quite different.
Theorem 8 Under the critical path model, LINall needs time Θ(n3 ) for the n/3BBG.
Proof Note that a single long edge {v1 , vn } will take n − 1 linearization steps using
backbone edges—edges between consecutive (w.r.t. IDs) nodes—before it is deleted.
There are many such reduction sequences, one of them has as a last edge {v1 , v3 },
another one has {vn−2 , vn }. The gist of the construction is to force all long edges to
be deleted in this way at least between vk and v2k+1 , and to make all these sequences
depend on each other to form a long critical path.
More precisely, consider the long edges in order of increasing length (and for
example increasing left endpoint). In this order, the edges get alternating colors red
and blue. The semantics of the colors is that red edges are reduced to {vk , vk+2 },
whereas blue edges are reduced to {v2k−1 , v2k+1 } before they get deleted in one step.
Every edge is first changed to {vk , v2k+1 } using the backbone (these actions will
not be part of the critical path). Because there are no shorter long edges, the edge does
not become parallel to another long edge (which would mean it gets deleted). Then
a red edge is reduced to {vk , v2k−1 } using the previous blue edge, and then this blue
edge is deleted. Similarly, a blue edge is reduced to {vk+2 , v2k+1 } using the previous
red edge, and then this red edge is deleted. Then, in k − 3 steps, a red edge is reduced
to {vk , vk+2 }, a blue edges to {v2k−1 , v2k+1 }.
In the critical path model, the shrinking of one edge along the middle part of the
backbone depends on the previous edge already being reduced to an edge of length
two. In total, this yields a sub-path of length k − 3 on the critical path for every long
edge, i.e., a critical path of length k 2 (k − 3) ∈ Θ(n3 ).

Finally, it remains to mention that in the model studied in [23], an adapted version
of LINall would reduce the number of links in the n/3-BBG network from Θ(n2 )
to O(n) in only three rounds—performing a linear work per node and round, and
thus ignoring the large contention. Subsequently, the linearization process requires a
linear number of rounds until the graph is completely linearized. We believe that this
behavior is not intuitive and that the insights that can be obtained with this model are
limited.

6 Conclusion
This article has investigated the parallel complexity of self-stabilizing graph linearization. We have proposed a new model which we believe is more appropriate
and intuitive than existing frameworks, and we provided a first analysis of the parallel time complexity of two most simple and archetypical self-stabilizing algorithms.

134

Theory Comput Syst (2014) 55:110–135

We also conducted simulations of the algorithms proposed to complement our formal insights, and our experimental results indicate that our upper bounds may be too
pessimistic.
We consider this work as a first step, and hope that our model will spark discussions and future research in the community. Indeed, we have started ourselves to consider 2-dimensional linearization problems [20] as well as scalable skip graphs [19].
However, both results are based on a simpler execution model that ignores node congestions. Moreover, it turns out that the 2-dimensional constructions require geometric reasoning that renders the analysis more complex, and it remains an open question
how to apply our parallel runtime model in these more difficult settings.

References
1. Angluin, D., Aspnes, J., Chen, J., Wu, Y., Yin, Y.: Fast construction of overlay networks. In: Proc.
of the 17th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA), pp. 145–154
(2005). doi:10.1145/1073970.1073991
2. Aspnes, J., Shah, G.: Skip graphs. In: Proc. of the 14th ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 384–393 (2003)
3. Aspnes, J., Wu, Y.: O(log n)-time overlay network construction from graphs with out-degree 1. In:
Proc. of the 11th Int. Conference on Principles of Distributed Systems (OPODIS). LNCS, vol. 4878,
pp. 286–300. Springer, Berlin (2007). doi:10.1007/978-3-540-77096-1_21
4. Awerbuch, B., Varghese, G.: Distributed program checking: a paradigm for building self-stabilizing
distributed protocols. In: Proc. of the 32nd IEEE Symposium on Foundations of Computer Science
(FOCS), pp. 258–267 (1991)
5. Blumofe, R.D., Leiserson, C.E.: Space-efficient scheduling of multithreaded computations. SIAM J.
Comput. 27(1), 202–229 (1998). doi:10.1137/S0097539793259471
6. Blumofe, R.D., Leiserson, C.E.: Scheduling multithreaded computations by work stealing. J. ACM
46(5), 720–748 (1999). doi:10.1145/324133.324234
7. Brzeziński, J., Szychowiak, M.: Self-stabilization in distributed systems—a short survey. Found.
Comput. Decision Sci. 25(1), 3–22 (2000)
8. Cidon, I., Gopal, I., Kutten, S.: New models and algorithms for future networks. IEEE Trans. Inf.
Theory 41(3), 769–780 (1995). doi:10.1109/18.382023
9. Clouser, T., Nesterenko, M., Scheideler, C.: Tiara: a self-stabilizing deterministic skip list and skip
graph. Theor. Comput. Sci. 428, 18–35 (2012). doi:10.1016/j.tcs.2011.12.079
10. Cramer, C., Fuhrmann, T.: Self-stabilizing ring networks on connected graphs. Tech. Rep. 2005-5,
System Architecture Group, University of Karlsruhe (2005)
11. Dijkstra, E.W.: Self-stabilization in spite of distributed control. Commun. ACM 17(11), 643–644
(1974). doi:10.1145/361179.361202
12. Dolev, D., Hoch, E.N., van Renesse, R.: Self-stabilizing and Byzantine-tolerant overlay network. In:
Proc. of the 11th Int. Conference on Principles of Distributed Systems (OPODIS). LNCS, vol. 4878,
pp. 343–357. Springer, Berlin (2007). doi:10.1007/978-3-540-77096-1_25
13. Dolev, S.: Self-Stabilization. MIT Press, Cambridge (2000)
14. Dolev, S., Kat, R.I.: HyperTree for self-stabilizing peer-to-peer systems. Distrib. Comput. 20(5), 375–
388 (2008). doi:10.1007/s00446-007-0038-9
15. Gall, D., Jacob, R., Richa, A., Scheideler, C., Schmid, S., Täubig, H.: Time complexity of distributed
topological self-stabilization: the case of graph linearization. In: Proc. of the 9th Latin American Theoretical Informatics Symposium (LATIN). LNCS, vol. 6034, pp. 294–305. Springer, Berlin (2010).
doi:10.1007/978-3-642-12200-2_27
16. Goodrich, M.T., Kosaraju, S.R.: Sorting on a parallel pointer machine with applications to set expression evaluation. J. ACM 43(2), 331–361 (1996). doi:10.1145/226643.226670
17. Harchol-Balter, M., Leighton, T., Lewin, D.: Resource discovery in distributed networks. In: Proc.
of the 18th ACM Symposium on Principles of Distributed Computing (PODC), pp. 229–237 (1999).
doi:10.1145/301308.301362

Theory Comput Syst (2014) 55:110–135

135

18. Herman, T.: In: Self-Stabilization Bibliography: Access Guide (2002). See ftp://ftp.cs.uiowa.edu/
pub/selfstab/bibliography/
19. Jacob, R., Richa, A., Scheideler, C., Schmid, S., Täubig, H.: A distributed polylogarithmic time algorithm for self-stabilizing skip graphs. In: Proc. of the 28th ACM Symposium on Principles of Distributed Computing (PODC), pp. 131–140 (2009). doi:10.1145/1582716.1582741
20. Jacob, R., Ritscher, S., Scheideler, C., Schmid, S.: Towards higher-dimensional topological selfstabilization: a distributed algorithm for Delaunay graphs. Theor. Comput. Sci. 457, 137–148 (2012).
doi:10.1016/j.tcs.2012.07.029
21. Kniesburges, S., Koutsopoulos, A., Scheideler, C.: Re-Chord: a self-stabilizing Chord overlay network. In: Proc. of the 23rd ACM Symposium on Parallelism in Algorithms and Architectures (SPAA),
pp. 235–244 (2011). doi:10.1145/1989493.1989527
22. Moscibroda, T., Schmid, S., Wattenhofer, R.: On the topologies formed by selfish peers. In: Proc.
of the 25th ACM Symposium on Principles of Distributed Computing (PODC), pp. 133–142 (2006).
doi:10.1145/1146381.1146403
23. Onus, M., Richa, A., Scheideler, C.: Linearization: locally self-stabilizing sorting in graphs. In: Proc.
of the 9th Workshop on Algorithm Engineering and Experiments (ALENEX), pp. 99–108. SIAM,
Philadelphia (2007)
24. Shaker, A., Reeves, D.S.: Self-stabilizing structured ring topology P2P systems. In: Proc. of the 5th
IEEE Int. Conference on Peer-to-Peer Computing (P2P), pp. 39–46 (2005). doi:10.1109/P2P.2005.34
25. Stoica, I., Morris, R., Karger, D., Kaashoek, M.F., Balakrishnan, H.: Chord: a scalable peer-to-peer
lookup service for internet applications. Tech. Rep. MIT-LCS-TR-819, MIT (2001)

Efﬁcient Broadcasting and Gathering in Wireless Ad-Hoc Networks
Melih Onus and Andréa Richa ∗
Computer Science and Engineering Department
Arizona State University
PO Box 875406
Tempe, AZ 85287, USA
{Melih.Onus,aricha}@asu.edu
Abstract
This paper considers the problem of broadcasting and
information gathering in wireless ad-hoc networks, i.e. in
wireless networks without any infrastructure in addition to
the mobile hosts. Broadcasting is the problem of sending
a packet from a source node in the network to all other
nodes in the network. Information gathering is the problem
of sending one packet each from a subset of the nodes to a
single sink node in the network. Most of the proposed theoretical wireless network models oversimplify wireless communication properties. We will use a model that takes into
account that nodes have different transmission and interference ranges, and we propose algorithms in this model
that achieve a high time and work-efﬁciency. We present algorithms for broadcasting a single or multiple message(s),
and for information gathering. Our algorithms have the advantage that they are very simple and self-stabilizing, and
would therefore even work in a dynamic environment. Also,
our algorithms require only a constant amount of storage at
any host. Thus, our algorithms can be used in wireless systems with very simple devices, such as sensors.

1. Introduction
In this paper we consider the problem of broadcasting and gathering messages in wireless ad-hoc networks.
Broadcasting is a basic communication primitive for wireless networks, and it has therefore been heavily studied both
in the systems and in the theory community. Though broadcasting itself appears to be an easy problem, it is actually
quite hard to realize in an efﬁcient and reliable way in a
mobile ad-hoc network. The main problem concerning theoretical investigations is that mobile ad-hoc networks have
∗
†

Supported by NSF CAREER grant CCR-9985284.
Supported by NSF grants CCR-0311121 and CCR-0311795.

Kishore Kothapalli and Christian Scheideler †
Department of Computer Science
Johns Hopkins University
3400 N. Charles Street
Baltimore, MD 21218, USA
{kishore, scheideler}@cs.jhu.edu
many features that are hard to model in a clean way. Major
challenges are how to model wireless communication and
how to model mobility. Here, theoretical work is still rare.
So far, people in the theory area have mostly looked at static
wireless systems (i.e. the wireless units are always available
and do not move). Wireless communication is usually modeled using the packet radio network model. In this model,
the wireless units, or nodes, are represented by a graph, and
two nodes are connected by an edge if they are within transmission range of each other. Transmissions of messages interfere at a node if at least two of its neighbors transmit a
message at the same time. A node can only receive a message if it does not interfere with any other message(s).
The packet radio network model is a simple and clean
model that allows one to design and analyze broadcast algorithms with a reasonable amount of effort. However, since
it is a high-level model, it does have some serious problems with certain scenarios in practice. For example, in reality it is not true that the transmission range of a node is
the same as its interference range. Instead, the interference
range of a node is usually at least twice as large as its transmission range. Not taking this into account may result in
broadcasting algorithms that cannot handle certain scenarios well although efﬁcient on paper. In fact, it is not difﬁcult
to construct examples (see [16]), where most existing protocols for broadcasting require Ω(n) rounds even in expectation when we consider the situation that the interference
range is bigger than the transmission range.
Thus it is necessary that algorithms for broadcasting in
wireless networks consider problems due to interference.
There is a limited number of papers that use a model that
differentiates between the transmission range and interference range [1, 7, 8], but they assume that nodes are distributed in an ideal space so that the transmission range and
interference range of every node can be speciﬁed in terms
of Euclidean distance.
Another serious limitation in most of the existing algorithms is the assumption that the size of the network, or at

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

least a linear estimate of the size of the network, is available to all of the nodes in the network. Without an estimate
of the size of the network it was shown in [11] that in an
n node network, Ω(n) time units are required in expectation for a single message to be sent successfully, if physical
carrier sensing is not available.
We will use a much more general wireless communication model that recently appeared in [17]. In this work we
present self-stabilizing algorithms for broadcasting and information gathering in wireless overlay networks. To keep
this paper at a reasonable length, we do not give a detailed
motivation for the model adopted but instead refer the interested reader to [17].

1.1. Wireless communication model
We assume that we are given a set V of mobile stations,
or nodes, that are distributed in an arbitrary way in a 2dimensional Euclidean space. For any two nodes v, w ∈ V
let d(v, w) be the Euclidean distance between v and w. Furthermore, consider any cost function c with the property that
there is a ﬁxed constant δ ∈ [0, 1) so that for all v, w ∈ V ,
• c(v, w) ∈ [(1 − δ) · d(v, w), (1 + δ) · d(v, w)] and
• c(v, w) = c(w, v), i.e. c is symmetric.
c determines the transmission and interference behavior of
the nodes and δ bounds the non-uniformity of the environment. Notice that we do not require c to be monotonic in
the distance or to satisfy the triangle inequality. This makes
sure that our model even applies to highly irregular environments.
We assume that the nodes use some ﬁxed-rate powercontrolled communication mechanism over a single frequency band. When using a transmission power of P , there
is a transmission range rt (P ) and an interference range
ri (P ) > rt (P ) that grow monotonically with P . The interference range has the property that every node v ∈ V can
only cause interference at nodes w with c(v, w) ≤ ri (P ),
and the transmission range has the property that for every
two nodes v, w ∈ V with c(v, w) ≤ rt (P ), v is guaranteed to receive a message from w sent out with a power of
P (with high probability) as long as there is no other node
v  ∈ V with c(v, v  ) ≤ ri (P  ) that transmits a message at
the same time with a power of P  .
For simplicity, we assume that the ratio ri (P )/rt (P ) is
a ﬁxed constant greater than 1 for all relevant values of P .
This is not a restriction because we do not assume anything
about what happens if a message is sent from a node v to
a node w within v’s transmission range but another node u
is transmitting a message at the same time with w in its interference range. In this case, w may or may not be able to
receive the message from v, so any worst case may be assumed in the analysis. The only restriction we need, which

is important for any overlay network algorithm to eventually stabilize, is that the transmission range is a sharp threshold. That is, beyond the transmission range a message cannot be received any more (with high probability).
Nodes can not only send and receive messages but also
perform physical carrier sensing, which has not been considered before in models proposed in the algorithms community. Given some sensing threshold T (that can be ﬂexibly set by a node) and a transmission power P , there is
a carrier sense transmission (CST) range rst (T, P ) and a
carrier sense interference (CSI) range rsi (T, P ) that grow
monotonically with T and P . The range rst (T, P ) has the
property that if a node v transmits a message with power P
and a node w with c(v, w) ≤ rst (T, P ) is currently sensing
the carrier with threshold T , then w senses a message transmission (with high probability). The range rsi (T, P ) has
the property that if a node v senses a message transmission
with threshold T , then there was at least one node w with
c(v, w) ≤ rsi (T, P ) that transmitted a message with power
P (with high probability). More precisely, we assume that
the monotonicity property holds. That is, if transmissions
from a set U of nodes within the rsi (T, P ) range cause v to
sense a transmission, then any superset of U will also do so.
For simplicity, we will assume in the following that for the
carrier sense ranges, rsi (T, P )/rst (T, P ) = ri (P )/rt (P )
for all relevant values of T .

1.2. Related work
Broadcasting in wireless ad-hoc networks has been extensively studied in the literature, especially in the more applied ad-hoc networking community. See [18] for a survey.
To the best of our knowledge, our paper is the ﬁrst work that
formally develops and analyzes broadcast algorithms under
a model with separate transmission and interference ranges.
All of the work on the broadcast problem cited below assume a static network scenario where the transmission and interference ranges of a node are the same. In an
early work, Chlamtac and Weinstein [3] presented a deterministic centralized broadcast protocol which assumes
complete knowledge of the network topology and which
runs in O(D log2 n) time, where n is the number of nodes
and D is diameter of the network. Bar-Yehuda et al. [2]
were the ﬁrst to present a distributed algorithm for the
broadcasting problem in ad-hoc wireless networks without assuming any topological knowledge, except immediate neighborhood, of the network. Their algorithm had expected O(D log n + log2 n) time. In [14] a lower bound
of Ω(D log (n/D)) is shown for any randomized broadcast protocol. In [13, 4] randomized protocols with expected runtime of O(D log (n/D) + log2 n) are presented
— in [13] the underlying topology is assumed to be symmetric, while in [4] this assumption is dropped.
Adler and Scheideler [1] present approximation algorithms for the unicast problem in wireless ad-hoc networks

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

under the assumption that the transmission and interference
ranges are not the same. However they still assume a simpliﬁed disk model based on Euclidean distances. Moreover,
their unicast algorithm does not translate directly into an efﬁcient broadcasting algorithm.
The problem of information gathering in wireless networks is studied mostly in the context of wireless sensor
networks. The authors of [10] construct a tree on which
gathering and aggregation can be performed. However, they
do not deal with inherent problems such as channel contention and interference and also do not provide theoretical
bounds on time and work. Information gathering and aggregation have also been studied in [5, 12, 15, 19, 9] but a rigorous formal analysis for wireless ad hoc networks has not
been performed prior to this work.

1.3. Our results
We consider two important communication problems in
wireless ad hoc networks, namely, broadcasting and information gathering.
The problem of broadcasting can be described as follows. Given a static connected wireless network of n nodes,
minimize the total time and work to send m ≥ 1 broadcast messages originating from a source node s to all nodes
in the network. In Section 3, we consider the simple case
where a single node s is the source of a single broadcast
message, i.e., m = 1. In Section 4, we extend our algorithm to handle the case that node s is the source of multiple broadcast messages.
Information gathering is another important communication primitive in wireless networks. The problem has applications in many scenarios in sensor networks [10, 19, 6],
and maintaining connectivity with base stations in a multihop wireless network. The problem of information gathering can be described as follows. Given a static connected
wireless network of n nodes among which m packets are arbitrarily distributed and a sink node s in the network, minimize the total time and work required for sending the m
packets to the sink node. In Section 5, we present and analyze a simple strategy for information gathering.
Our algorithms are self-stabilizing (i.e., can start in an arbitrary state) and can therefore adapt to changes in a wireless ad-hoc network. Our algorithms do not require any
knowledge of the size of the network. For our algorithms
to work correctly, it sufﬁces that the nodes in the network
have identiﬁers that are locally different. We only require
that the nodes synchronize locally into rounds up to some
reasonably small time difference, which can be easily accomplished using GPS signals or any form of beacons. Another important feature of our algorithms is that a constant
amount of storage at any node sufﬁces even in the case of
gathering. The above properties make our algorithms applicable to sensor networks without any modiﬁcations.

The proofs are omitted in this version but can be found
in [16]. Before proceeding further, in Section 2 we present
some preliminary deﬁnitions and assumptions used in the
paper.

2. Preliminaries
Our results build on top of a distributed algorithm for organizing the wireless nodes into a constant density spanner,
proposed recently in [17]. A constant density spanner is deﬁned as follows. Given an undirected graph G = (V, E), a
subset U ⊆ V is called a dominating set if all nodes v ∈ V
are either in U or have an edge to a node in U . A dominating set U is called connected if U forms a connected component in G. The density of a dominating set is the maximum over all nodes v ∈ U of the number of neighbors that
v has in U . In our context, constant density spanner is a connected dominating set U of constant density with the property that for any two nodes v, w ∈ V there are two nodes
v  , w ∈ U with {v, v  } ∈ E, {w, w } ∈ E, and a path p
from v  to w along nodes in U so that the length of p is
at most a constant factor larger than the distance between v
and w in G.
Let V be the set of nodes in the network. For any transmission range r, let the graph Gr = (V, E) denote the graph
containing all edges {v, w} with c(v, w) ≤ r. Throughout
this paper, rt denotes the transmission range and d(u, v) denotes the shortest distance between u and v in Grt . Furthermore, let D(s) = maxv∈V d(s, v).
The spanner protocol for Grt presented in [17] consists
of three phases that are continuously repeated in rounds as
shown in Figure 1. The task of Phase I is to obtain a set
U ⊆ V of active nodes so that U forms a constant density dominating set in Grt . As U may not be connected, additional phases are required to arrive at the constant density spanner. The task of Phase II is to arrange nodes in U
into color classes that keep nodes with the same color sufﬁciently far apart from each other. Only a constant number of different colors is needed for this, where the constant depends on δ as deﬁned in Section 1.1. Every node organizes its rounds into time frames consisting of as many
rounds as there are colors, and a node in U only becomes
active in Phase III in the round corresponding to its color,
also referred to as the round owned by that active node. The
task of Phase III is to interconnect nodes in u, v ∈ U such
that d(u, v) ≤ 3 via a set G of gateway nodes. (See Figure
2). Each phase has a constant number of time slots associated with it, where each time slot represents a communication step as shown in Figure 1. To achieve interference free
communication among the active nodes, the coloring obtained in Phase II is used. Nodes in V \ (U ∪ G) are referred
to as inactive nodes and the constant dˆ refers to the number of active nodes that are within the interference range of
any node.

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

Phase II

Phase II

Phase III Phase I

Phase I

Phase III

Round

Figure 1. Timeline of the spanner protocol.

Legend:

Active Node
Inactive node
Gateway node
Gateway
Other edges

Figure 2. Constant density spanner
In [17], it was shown that such a spanner can be constructed in O(∆ log ∆ log n + log4 n) time steps, with high
probability, where ∆ is the maximum number of nodes that
are within the transmission range of a node.

3. Isolated Broadcasting
In this section we consider the problem of broadcasting
a single message. Let node s be the source of the broadcast message. Since s has a maximum distance of D(s) to
any node in Grt , D(s) is a lower bound on the time an optimal ofﬂine algorithm needs to broadcast a message from s
to all nodes. Our goal is to come up with a broadcast scheme
so that the time needed by the broadcast message to reach
all nodes is as close to D(s) as possible. We use the constant density spanner construction of [17] as the basis. If s
is not an active node, i.e., s ∈ U , then let  be some active node that is within the transmission range of s. Then s
ﬁrst sends the message to . The broadcast scheme then proceeds in rounds that are synchronized among the nodes. In
the broadcast scheme below,  refers to the ID of an active
node that owns the current slot. Every item below is a separate time step.
1. If  received the broadcast message in the previous
round and it is the ﬁrst time it received the broadcast
message,  sends out the broadcast message.
2. If v is a gateway node and has already received the
broadcast message, then v sends out an RTS (RequestTo-Send) signal with probability p.
3. If v is a gateway node and decided not to send out an
RTS signal or v is an active node, then v checks if it
correctly received an RTS signal. If so, and v has not
received the broadcast message yet, v sends out a CTS
(Clear-To-Send) signal.
4. If v is a gateway node and sent out an RTS signal, then
v checks if it sensed a CTS signal. If so, v sends out
the broadcast message.

Notice that inactive nodes just need to listen to the wireless channel in order to receive the broadcast message eventually. This is because our spanner algorithm [17] makes
sure that message transmissions of active nodes in step 1
above never interfere at an inactive node. The following
theorems demonstrate that the above protocol has a high
time and work efﬁciency. We neglect the cost for sending
and sensing the RTS/CTS signals in arriving at the work
bound.
Theorem 3.1 Given the constant density spanner of Grt
as in [17], the broadcast algorithm with p = 1/d̂ needs
O(D(s) + log n) rounds, with high probability, to deliver
the broadcast message to all nodes.
Theorem 3.2 Given the constant density spanner of Grt
as in [17], the broadcast algorithm needs O(W (s)) work,
where W (s) is the optimal work required to send a broadcast message from s to all nodes.
The broadcast algorithm can also be made to selfstabilize by making simple changes to the algorithm above
as shown in [16].

4. Broadcasting Multiple Messages
Next we look at the case that the source s wants to send
out multiple broadcast messages instead of just one. Then
s attaches continuous sequence numbers to the messages,
starting with 1.
The broadcast scheme proceeds in rounds that are synchronized among the nodes. Each active or gateway node
v keeps track of two numbers, iv and jv . Number iv denotes the minimum message number v has not received so
far and number jv denotes the minimum message number (v
knows about since its last successful transmission attempt)
a node of distance at most rt from v has not received so far.
In the broadcast scheme below,  refers to the ID of an active node that owns the current slot. Initially, for each gateway and active node v, iv =jv =1. In each round, every node
v = s does the following. Each item below represents a separate time step.
1. If  received the broadcast message with sequence
number i = i in the previous round, then it sets
i = i + 1 and sends out the broadcast message with
sequence number i .
If v is a gateway node and received a broadcast message with sequence number i = iv , then it sets iv =
iv + 1.
2. If v is an active or gateway node, then it sends out
an (RTR, iv ) message (RTR means “ready-to-receive”)
with probability p. If v decides not to send out an
RTR message, it checks whether it is able to receive
an (RTR, i ) message. If so, it sets jv = min{jv , i }.

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

3. If v is a gateway node and iv > jv , then it sends out an
(RTS, jv ) message with probability p.
If v is a gateway node and decided not to send out an
RTS message or v is an active node, then v checks if it
correctly received an (RTS, j  ) message with j  = iv .
If so, v sends out a CTS signal.
4. If v is a gateway node and sent out an (RTS, jv ) message, then v checks if it sensed a CTS signal. If so, v
sends out the broadcast message with sequence number jv . Afterwards, v sets jv = min{jv + 1, iv − 1}.
If v is a gateway node and did not send a message but
received a broadcast message with sequence number
i = iv , then it sets iv = iv + 1.
The source node s uses the same protocol as above with
the only difference that it only executes the ﬁrst step. The inactive nodes just need to listen to the wireless channel in order to receive the broadcast messages eventually. The following theorems demonstrate that this protocol has a high
time and work efﬁciency.
Theorem 4.1 Given the constant density spanner of Grt as
in [17], the concurrent broadcast algorithm with p = 1/2 d̂
needs O(D(s) + m + log n) rounds, with high probability,
to deliver m broadcast messages to all nodes.
Theorem 4.2 Given the constant density spanner of Grt as
in [17], the broadcast algorithm needs O(W (s, m)) work,
where W (s, m) is the optimal work required to send m
broadcast messages from s to all nodes.
The above protocol also can be made to self-stabilize and
the details can be found in [16].

5. Information Gathering
We now consider the situation where a total of m packets
distributed in an arbitrary way among the nodes in the wireless network are to be delivered to a sink node s in the network. Firstly, note that Ω(m + D(s)) is a lower bound on
any solution for the information gathering problem. In the
following, we describe a 2-stage protocol to perform information gathering efﬁciently. Each stage has a constant number of reserved time slots, 4 slots for stage 1 and 4 slots for
stage 2.

5.1. Stage 1: Building Gathering Tree T (s)
We ﬁrst show how to build the gathering tree rooted at
s. All internal nodes in this tree will belong to U ∪ G. The
sink node s, if it is not in U ∪ G, selects an active node 
such that d(, s) = 1 and sends a route packet to  with sequence number of 0 of the form ROUTE, s, 0. The rest of
the nodes do the following. Initially, d (s, v) = ∞, π(v) =
NULL ∀v ∈ U ∪ G. Let d (s, s) = 0 and π(s) = s.
1. If u ∈ U ∪ G receives a message ROUTE, v, d (s, v)
from v with a sequence number of d (s, v) and if

d (s, v) + 1 < d (s, u), then u sets π(u) = v and
d (s, u) = d (s, v) + 1. Node u also sets ﬂag(u) = 1
in this case, indicating that u has to send a route message since u updated its predecessor. If u ∈ U ∪ G
and d (s, v) + 1 < d (s, u) and v ∈ U , then u updates π(u) = v and d (s, u) = d (s, v) + 1.
2. If u ∈ U ∪ G and d (s, u) = ∞ and ﬂag(u) = 1 then
u sends an RTS signal with probability p to be determined later.
3. If u ∈ U ∪G and u received an RTS signal then u sends
a CTS signal.
4. If v ∈ U ∪ G and v sent an RTS signal and receives a CTS signal then v sends a route packet
ROUTE, v, d (s, v) and sets ﬂag(v) = 0 signifying that the update has been notiﬁed.
Set T (s)
=
(VT , ET ) with VT
=
V and
ET = {(v, π(v))|v ∈ VT } where π(v) is set as in
step (1) of the above protocol. Due to the properties of the spanner [17], for the above construction it holds
that maxv∈V dT (s) (s, v) ≤ 5 maxv∈V d(s, v). The following lemma can be shown [16].
Lemma 5.1 Given the constant density spanner of Grt as
in [17], to construct T (s) the protocol given above with
ˆ takes O(D(s) + log n) time steps w.h.p.
p = 1/d,

5.2. Stage 2: Gathering on T (s)
In the gathering tree, T (s), constructed in stage 1, each
node has an unique path to the sink node s via the predecessor pointers π. Nodes use this path system to eventually
deliver packets to s.
The active node uses the ﬁrst time slot to deliver packets and the second and third time slots are used to coordinate the actions of the inactive nodes. Nodes  ∈ U ∪G have
a queue, Q , which can hold a constant number of packets.
This queue works as a ﬁrst-in-ﬁrst-out list and supports operations enqueue and dequeue which add a packet and return a packet respectively to Q .
In the following when we refer to inactive nodes, it is implicit that we are referring to those inactive nodes that have a
packet to send. Inactive nodes have a state among {awake,
asleep}. Initially all inactive nodes are in the asleep
state.
1. If  is active and has a non-empty queue, then  sends
the packet dequeue(Q) during the time slot owned by
. This packet has a destination π() and nodes other
than π() discard the packet and π() stores the packet
by calling enqueue on Qπ() . In the second time slot,
the active nodes listen to the channel.
If g is a gateway node and has a non-empty queue then
g sends an RTS message containing the id of π(g) with
probability p, where p is to be determined later.

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

2. If u ∈ U ∪ G and Qu is not full and u receives an RTS
message containing the id of u then u sends a CTS
message.
If u is inactive and has a packet to send and u is awake
then u sends an I-RTS (for Inactive-RTS) signal to
π(u) with a probability 1/2.
3. If g is a gateway node and sent an RTS signal in the
previous time step and receives a CTS signal from π(g)
then g sends the packet dequeue(Qg ) to π(g). This
packet has a destination π(g) and other nodes that receive the packet ignore it.
If  is active and  receives an I-RTS signal from an
inactive node u then  sends an I-CTS signal. If 
senses a busy channel but does not receive any I-RTS
signal, then  sends a collision message of the form
, COLLIDE. Otherwise if  senses a free channel
then  sends a free message of the form , FREE.
4. If u is inactive and asleep and receives a free message then u becomes awake. If u is inactive and decided not to send an I-RTS message in the previous
time step and u is awake and receives a collision message then u decides to go to asleep state with probability 1/2. If u is inactive and sent an I-RTS in the earlier step and gets an I-CTS then u sends the packet to
π(u).
Let ∆m denote the density of nodes that have a packet to
send (note that ∆m ≤ m). The following theorems demonstrate that the gathering protocol described above is efﬁcient
in terms of the time and work. The work performed while
sending the RTS/CTS signals and the I-RTS/I-CTS signals
is ignored while arriving at the work bound.
Theorem 5.2 Given the constant density spanner of Grt
as in [17], and a gathering tree T (s) with sink node s,
the information gathering algorithm presented above with
p = 1/dˆ needs O(m + ∆m log n log ∆m + D(s) + log n)
time steps w.h.p so that all the m packets reach the sink s.
Theorem 5.3 Once a stable gathering tree has been constructed, the gathering protocol described above needs
O(W  (s)) work, where W  (s) is the optimal work required to send all the m messages to the sink node
s.
Further, the algorithms for both the stages can be made
to self-stabilize by making necessary changes as shown in
[16].

6. Conclusions
In this paper, we use the realistic model for wireless communication of [17] to design algorithms for broadcasting
and information gathering in wireless ad-hoc networks. The
natural next steps would be to directly address node mobility and node faults, and to study more complex communication tasks such as anycasting and multicasting.

References
[1] M. Adler and C. Scheideler. Efﬁcient communication strategies for ad hoc wireless networks. Theory of Computing Systems, 33:337–391, 2000.
[2] R. Bar-Yehuda, O. Goldreich, and A. Itai. On the timecomplexity of broadcast in multi-hop radio networks: An exponential gap between determinism and randomization. J. of
Comp. and System Sci., 45:104–126, 1992.
[3] I. Chlamtac and O. Weinstein. The wave expansion approach
to broadcasting in multihop radio networks. IEEE trans. on
commun., COM-39, 3:426–433, 1991.
[4] A. Czumaj and W. Rytter. Broadcasting algorithms in radio
networks with unknown topology. In IEEE FOCS, 2003.
[5] E. J. Duarte-Melo and M. Liu. Data-gathering wireless sensor networks: organization and capacity. Computer Networks, 43(4):519–537, 2003.
[6] M. Enachescu, A. Goel, R. Govindan, and R. Motwani. Scale
free aggregation in sensor networks. In First Intl. W. on Algorithmic Aspects of Wireless Sensor Networks, 2004.
[7] P. Gupta and P.R. Kumar. The capacity of wireless networks.
IEEE Trans. Information Theory, IT-46(2):388–404, 2000.
[8] P. Gupta and P.R. Kumar. Internets in the sky: The capacity
of three dimensional wireless networks. Communications in
Information and Systems, 1(1):33–50, 2001.
[9] Q. Han, S. Mehrotra, and N. Venkatasubramanian. Energy
efﬁcient data collection in distributed sensor environments.
In ICDCS, pages 590–597, 2004.
[10] C. Intanagonwiwat, R. Govindan, and D. Estrin. Directed
diffusion: a scalable and robust communication paradigm
for sensor networks. In Mobile Computing and Networking, pages 56–67, 2000.
[11] T. Jurdzinski and G. Stachowiak. Probabilistic algorithms for
the wakeup problem in single-hop radio networks. In Proc.
of ISSAC, pages 535–549, 2002.
[12] K. Kothapalli and C. Scheideler. Information gathering in
adversarial systems: Lines and cycles. In ACM SPAA, 2003.
[13] D. Kowalski and A. Pelc. Broadcasting in undirected ad hoc
radio networks. In ACM PODC, pages 73–82, 2003.
[14] E. Kushilevitz and Y. Mansour. An Ω(D log(N/D)) lower
bound for broadcast in radio networks. SICOMP: SIAM
Journal on Computing, 27, 1998.
[15] H. Luo and G. Pottie. Routing explicit side information for
data compression in wireless sensor networks. In DCOSS,
2005.
[16] M. Onus, A. Richa, K. Kothapalli, and C. Scheideler. Efﬁcient broadcasting and gathering in wireless ad-hoc networks. Available at http://www.cs.jhu.edu/∼kishore.
[17] M. Onus, A. Richa, K. Kothapalli, and C. Scheideler. Constant density spanners for wireless ad-hoc networks. In ACM
SPAA, 2005.
[18] Brad Williams and Tracy Camp. Comparison of broadcasting techniques for mobile ad hoc networks. In ACM MOBIHOC, pages 194–205. ACM Press, 2002.
[19] Y. Yu, B. Krishnamachari, and V. Prasanna. Energy-latency
tradeoffs for data gathering in wireless sensor networks. In
IEEE Infocom, 2004.

Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN’05)
1087-4089/05 $20.00 © 2005

IEEE

Leader Election and Shape Formation with
Self-Organizing Programmable Matter
Joshua J. Daymude1 , Zahra Derakhshandeh1 , Robert Gmyr2 Thim
Strothmann2 , Rida Bazzi1 , Andréa W. Richa1 , and Christian Scheideler2

arXiv:1503.07991v2 [cs.ET] 31 Mar 2016

1

Department of Computer Science and Engineering,
Arizona State University, USA,
{jdaymude,zderakhs,bazzi,aricha}@asu.edu
2
Department of Computer Science,
University of Paderborn, Germany,
{gmyr,thim}@mail.upb.de, scheideler@upb.de

Abstract. We consider programmable matter consisting of simple computational elements, called particles, that can establish and release bonds
and can actively move in a self-organized way, and we investigate the
feasibility of solving fundamental problems relevant for programmable
matter. As a suitable model for such self-organizing particle systems, we
will use a generalization of the geometric amoebot model first proposed
in SPAA 2014. Based on the geometric model, we present efficient localcontrol algorithms for leader election and line formation requiring only
particles with constant size memory, and we also discuss the limitations
of solving these problems within the general amoebot model.

1

Introduction

A central problem for programmable matter is shape formation, and various solutions have already been discovered for that problem using different approaches
like DNA tiles [35], moteins [15], or nubots [41]. We are studying shape formation using the amoebot model that was first proposed in [22]. In order to
determine how decentralized shape formation can be handled, we are particularly interested in the connection between leader election and shape formation.
In the leader election problem we are given a set of particles, and the problem
is to select one of the particles as the leader. Many problems like the consensus
problem (all particles have to agree on some output value) can easily be solved
once the leader election problem can be solved. The same has also been observed
for shape formation, as most shape formation algorithms depend on some seed
element. However, the question is whether shape formation can even be solved in
circumstances where leader election is not possible. The aim of this paper is to
shed some light on the dependency between leader election and shape formation:
On one hand, we present an efficient decentralized algorithm for solving leader
election in a geometric variant of the amoebot model and show how having a
leader leads to an efficient solution for the basic line formation formation problem; on the other hand, we show that both problems cannot be solved efficiently

2

Daymude et al.

under the general version of our model . Before we present our results in more
detail, we first give a formal definition of the model and the problems we study
in this paper.
1.1

Models

We use two models throughout this work. Firstly, we consider a generalization
of the amoebot model [22] which abstracts from any geometry information. We
call this model the general amoebot model. Secondly, we consider a model that
is essentially equivalent to the original amoebot model presented in [22] but is
defined based on the general amoebot model. We refer to this second model as
the geometric amoebot model.
In the general amoebot model, programmable matter consists of a uniform set
of simple computational units called particles that can move and bond to other
particles and use these bonds to exchange information. The particles act asynchronously and they achieve locomotion by expanding and contracting, which
resembles the behavior of amoeba.
As a base of this model, we assume that we have a set of particles that aim
at maintaining a connected structure at all times. This is needed to prevent the
particles from drifting apart in an uncontrolled manner like in fluids and because
in our case particles communicate only via bonds. The shape and positions of
the bonds of the particles mandate that they can only assume discrete positions
in the particle structure. This justifies the use of a possibly infinite, undirected
graph G = (V, E), where V represents all possible positions of a particle (relative
to the other particles in their structure) and E represents all possible transitions
between positions.
Each particle occupies either a single node or a pair of adjacent nodes in G,
i.e., it can be in two different shapes, and every node can be occupied by at most
one particle. Two particles occupying adjacent nodes are connected, and we refer
to such particles as neighbors. Particles are anonymous but the bonds of each
particle have unique labels, which implies that a particle can uniquely identify
each of its outgoing edges. Each particle has a local memory, and any pair of
connected particles has a shared memory that can be read and written by both
particles.
Particles move by expansions and contractions: If a particle occupies one
node (i.e., it is contracted ), it can expand to an unoccupied adjacent node to
occupy two nodes. If a particle occupies two nodes (i.e., it is expanded ), it can
contract to one of these nodes to occupy only a single node. Performing movements via expansions and contractions has various advantages. For example, it
would easily allow a particle to abort a movement if its movement is in conflict with other movements. A particle always knows whether it is contracted
or expanded and this information will be available to neighboring particles. In
a handover, two scenarios are possible: a) a contracted particle p can "push"
a neighboring expanded particle q and expand into the neighboring node previously occupied by q, forcing q to contract, or b) an expanded particle p can

Leader Election and Shape Formation

3

"pull" a neighboring contracted particle q to a cell occupied by it thereby expanding that particle to that cell, which allows p to contract to its other cell.
The ability to use a handover allows the system to stay connected while particles
move (e.g., for particles moving in a worm-like fashion). Note that while expansions and contractions may represent the way particles physically move in space,
they can also be interpreted as a particle "looking ahead" and establishing new
logical connections (by expanding) before it fully moves to a new position and
severs the old connections it had (by contracting).
Summing up over all assumptions above, the state of a particle is uniquely
determined by its shape, the contents of its local memory, the edges it has to
neighboring particles, the contents of their shared memory (which may allow a
particle to obtain further information about the neighboring particles beyond
their shape), and finally the shape of the neighboring particles. The state of
the particle system (or short, system state) is defined as the combination of all
particle states. We say a particle system in a system state in which the particle
occupy a set of nodes A ⊆ V is connected if the graph G|A induced by A
is connected. We assume the standard asynchronous computation model, i.e.,
only one particle can be active at a time. Whenever a particle is active, it can
perform an action (governed by some fixed, finite size program controlling it)
consisting of a finite amount of computation (involving its local memory, the
shared memories with its neighboring particles, and random bits) followed by no
or a single movement. Hence, a computation of a particle system is a potentially
infinite sequence of actions A1 , A2 , . . . based on some initial system state s0 ,
where action Ai transforms system state si−1 into system state si . The (parallel)
time complexity of a computation is usually measured in rounds, where a round
is over once every particle has been given the chance to perform at least one
action.
Let S be the set of all system states in which the particle system is connected.
In general, a computational problem P for the particle system is specified by a
set S 0 ⊆ S of permitted initial system states and a mapping F : S 0 → 2S ,
where F (s) ⊆ S determines the set of permitted final states for any initial state
s ∈ S 0 . A particle system solves problem P = (S 0 , F ) if for any initial system
state s ∈ S 0 , all computations of the particle system eventually reach a system
state in F (s) without losing connectivity, and whenever such a system state is
reached for the first time, the system stays in F (s). If for all computation a
final state is reached in which all particles decided to halt (i.e., they decided not
to perform any further actions, irrespective of future events), then the particle
system is also said to decide problem P . Note that being in a final state does not
necessarily mean that all particles decided to halt. If S 0 = S, so any initial state
is permitted (including arbitrary faulty states, as long as the particle system is
connected), then a particle system solving P is also said to be self-stabilizing. It
is well-known that in general a distributed system solving a problem P cannot
decide it and also be self-stabilizing because if so, it would often be possible to
come up with an initial state s where a member of the system decides to halt
prematurely, disallowing the system to eventually reach a state in F (s).

4

Daymude et al.

Besides the general amoebot model, we will also consider the geometric amoebot model. The geometric amoebot model is a specific variant of the general
amoebot model in which the underlying graph G is defined to be the equilateral
triangular graph Geqt (see Figure 1), and the bonds of the particles are labeled
in a consecutive way in clockwise orientation around a particle so that every
particle has the same sense of clockwise orientation. However, we do not assume
that the labeling is uniform, so the particles do not necessarily share a common
sense of direction in the grid.

Fig. 1. The left part shows an example of a particle structure in the geometric amoebot
model. A contracted particle is depicted as a black dot, and an expanded particle is
depicted as two black dots connected by an edge. The right part shows a particle
structure with 3 boundaries. The outer boundary is shown as a solid line and the two
inner boundaries are shown as dashed lines.

1.2

Problems

In this paper we consider the following two problems. For both problems we
define the set of initial system states as the set of all states such that the particle
system is connected and all memories are empty.
For the leader election problem the set of final system states contains any state
in which the particles form a connected structure and exactly one particle is a
leader (i.e., only this particle is in a leader state while the remaining particles are
in a non-leader state). Our goal will be to come up with a distributed algorithm
that allows a particle system to decide the leader election problem. Note that
the leader election problem is well defined for both the general amoebot model
and the geometric amoebot model.
In a shape formation problem, the set of final states consists of those system
states where the particle structure forms the desired shape. As a specific example
of a shape formation problem, we consider the line formation problem. In the
geometric amoebot model, the shape the particles have to form is a straight line
in the equilateral triangular grid and all particles have to be contracted in a
final system state. Of course, in the general amoebot model a straight line is not
well-defined. Hence, for this model the set of final states for the line formation
problem is defined to consist of all system states in which the particles form a
simple path in G.

Leader Election and Shape Formation

5

Throughout the paper, we assume for the sake of simplicity that in an initial state all particles are contracted. Our algorithms can easily be extended to
dispose of this assumption.
1.3

Our Contributions

We focus on the problem of solving leader election and shape formation for
particles with constant size memory. For shape formation, we just focus on the
already mentioned line formation problem.
For the geometric amoebot model, we show that there is a distributed algorithm that can decide the leader election problem (Section 3), i.e., at the end we
have exactly one leader and the leader knows that it is the only leader left. Moreover, the runtime for our leader election algorithm is worst-case optimal in the
sense that it needs O(Lmax ) rounds with high probability (w.h.p.)3 , where Lmax is
the maximum length of a boundary between the particle structure and an empty
region (inside or outside of it) in Geqt . Based on the leader election algorithm,
we present a distributed algorithm that solves the line formation problem with
worst-case optimal work in Section 4.
On the other hand, for the general amoebot model, we show that neither
leader election nor shape formation can be decided by any distributed algorithm
in Section 2. More concretely, we show that there cannot exist a randomized
algorithm for solving either problem with any bounded probability of success in
the general model.
The algorithms presented for leader election and line formation under the
geometric amoebot model both assume that the system is in a well-initialized
state. It would certainly be desirable to have algorithms that can tolerate any
initial state, but at the end of the paper, in Section 5, we show that there
are certain limitations to solving leader election and line formation in a selfstabilizing fashion.
1.4

Related Work

Many approaches related to programmable matter have recently been proposed.
One can distinguish between active and passive systems. In passive systems the
particles either do not have any intelligence at all (but just move and bond
based on their structural properties or due to chemical interactions with the
environment), or they have limited computational capabilities but cannot control
their movements. Examples of research on passive systems are DNA computing
[1, 9, 15, 21, 39], tile self-assembly systems in general (e.g., see the surveys in [23,
35, 40]), population protocols [4], and slime molds [10, 33]. We will not describe
these models in detail as they are only of little relevance for our approach. On
the other hand in active systems, computational particles can control the way
3

By with high probability, we mean with probability at least 1 − 1/nc , where n is the
number of particles in the system and c > 0 is a constant.

6

Daymude et al.

they act and move in order to solve a specific task. Robotic swarms and modular
robotic systems are some examples of active programmable matter systems.
In the area of swarm robotics it is usually assumed that there is a collection
of autonomous robots that have limited sensing, often including vision, and communication ranges, and that can freely move in a given area. They follow a broad
variety of goals: for example, graph exploration (e.g., [24]), gathering problems
(e.g., [2, 17]), shape formation problems (e.g., [25, 37]), and to understand the
global effects of local behavior in natural swarms like social insects, birds, or fish
(e.g., [8, 12]). Surveys of recent results in swarm robotics can be found in [31,
34]; other samples of representative work can be found in [5, 7, 18–20, 28, 32].
While the analytical techniques developed in the area of swarm robotics and
natural swarms are of some relevance for this work, the individual units in those
systems have more powerful communication and processing capabilities than in
the systems we consider.
The field of modular self-reconfigurable robotic systems focuses on intrarobotic aspects such as the design, fabrication, motion planning, and control
of autonomous kinematic machines with variable morphology (see e.g., [26, 42]).
Metamorphic robots form a subclass of self-reconfigurable robots that share some
of the characteristics of our geometric model [16]. The hardware development in
the field of self-reconfigurable robotics has been complemented by a number of
algorithmic advances (e.g., [11, 37, 38]), but so far mechanisms that automatically scale from a few to hundreds or thousands of individual units are still under
investigation, and no rigorous theoretical foundation is available yet.
The nubot model [13, 14, 41] by Woods et al. aims at providing the theoretical framework that would allow for a more rigorous algorithmic study of
biomolecular-inspired systems, more specifically of self-assembly systems with
active molecular components. Although biomolecular-inspired systems share similarities with our self-organizing particle systems, there are many differences that
do not allow us to translate the algorithms and other results under the nubot
model to our systems — e.g., there is always an arbitrarily large supply of "extra" particles that can be added to the system as needed, and the system allows
for an additional (non-local) notion of rigid-body movement.
Our developed leader election algorithm for the geometric amoebot model
shares some similarities with the algorithm of [?] for cellular automata. However,
since cellular automata work in a synchronous fashion and have access to a global
compass, the approach of [?] is vastly different from our leader election algorithm.

2

Impossibility Results in the General Amoebot Model

In this section, we show that both leader election and line formation are impossible to solve in the general amoebot model. Suppose that there is a distributed
algorithm solving the line formation problem in the general amoebot model
(when starting in a well-initialized state). Since in this case it is possible to decide when G|A0 forms a line, it is also possible to design a protocol that solves
the leader election problem: once the line has been formed, its two endpoints

Leader Election and Shape Formation

7

contend for leadership using tokens with random bits sent back and forth until
one of them wins. On the other hand, one can deduce from [29] that in the
general amoebot model there is no distributed algorithm that can decide when
a leader has been elected (with any reasonable success probability).
More concretely, in [29] the authors show that for a ring of anonymous nodes
there is no algorithm that can correctly decide the leader election problem4
with any probability α > 0, i.e., for any algorithm in which the particles are
guaranteed to halt, the error probability is unbounded. Since in the general
amoebot model G can be any graph, we can set G to be a ring whose size is the
number of particles and the result of [29] is directly applicable.
Hence, there is no a distributed algorithm deciding the line formation problem
(with any reasonable success probability) in the general amoebot model, and
therefore not even an algorithm for solving it since a protocol solving the problem
could easily be transformed into a protocol deciding it.

3

Leader Election in the Geometric Amoebot Model

In this section we show how the leader election problem can be decided in the
geometric amoebot model. Our approach organizes the particle system into a set
of cycles and executes an algorithm on each cycle independently (Section 3.1).
For simplicity and ease of presentation we first state the protocol in a simple
model in which particles have a global view of the cycle they are part of, act
synchronously, and have unbounded local memory (Section 3.2). We prove its
correctness in Section 3.3. In Section 3.5, we present the corresponding localcontrol protocol that works without these assumptions in the geometric amoebot model. However, since the local realization combines many different token
passing schemes and techniques, a formal analysis would be beyond the scope of
this paper. Instead, we show that our approach has expected linear runtime in a
variant of the simple model which is motivated by the token passing schemes of
the local-control protocol, i.e., it takes into account that interaction between two
particles is dependent on the distance between those particles (Section 3.4). We
conclude our analysis of the leader election problem by presenting an extension
of our protocol (in the simpler model) in Section 3.6 with a linear runtime with
high probability.
3.1

Organization into Cycles

Let A ⊆ V be any initial distribution of contracted particles such that Geqt |A
is connected. Consider the graph Geqt |V \A induced by the unoccupied nodes in
Geqt . We call a connected component of Geqt |V \A an empty region. Let N (R)
be the neighborhood of an empty region R in Geqt . Then all nodes in N (R) are
occupied and we call the graph Geqt |N (R) a boundary. Since Geqt |A is a connected
4

Or, in their words, that can solve the leader election problem with distributive
termination.

8

Daymude et al.

finite graph, exactly one empty region has infinite size while the remaining empty
regions have finite size. We define the boundary corresponding to the infinite
empty region to be the unique outer boundary and refer to a boundary that
corresponds to a finite empty region as an inner boundary, see Figure 1.
The particles occupying a boundary can instantly (i.e., without communication) organize themselves into a cycle using only local information: Consider a
boundary corresponding to an empty region R. Let p be a particle occupying a
node v of the boundary. By definition there exists a non-occupied node w ∈ R
that is a adjacent to v in the graph Geqt . The particle p iterates over the neighboring nodes of v in clockwise orientation around v starting at w. Consider the
first occupied node it encounters; the particle occupying that node is the successor of p in the cycle corresponding to that boundary. Analogously, p finds its
predecessor in the cycle by traversing the neighborhood of v in counter-clockwise
orientation.
Note that a single particle can belong to up to three boundaries at once.
Furthermore, a particle cannot locally decide whether two empty regions it sees
(i.e., maximal connected components of non-occupied nodes in the neighborhood
of v) are distinct. We circumvent these problems by letting a particle treat
each empty region in its local view as distinct. For each such empty region, a
particle executes an independent instance of the same algorithm. Hence, we say
a particle acts as a number of (at most three) distinct agents. For each of its
agents a particle determines the predecessor and successor as described above.
This effectively connects the set of all agents into disjoint cycles as depicted in
Figure 2. Observe that from a global perspective the cycle of the outer boundary
is oriented clockwise while a cycle of an inner boundary is oriented counterclockwise. This is a direct consequence of the way the predecessors and successors
of an agent are defined.

Fig. 2. The depicted particle system is the same as in the right part of Figure 1. In this
figure particles are depicted as gray circles. The black dots inside of a particle represent
its agents. As in Figure 1 the outer boundary is solid and the two inner boundaries are
dashed.

Leader Election and Shape Formation

3.2

9

Algorithm

The leader election algorithm operates independently on each cycle. At any given
time, some subset of agents on a cycle will consider themselves candidates, i.e.
potential leaders of the system. Initially, every agent considers itself a candidate.
Between any two candidates on a cycle there is a (possibly empty) sequence of
non-candidate agents. We call such a sequence a segment. For a candidate c
we refer to the segment coming after c in the direction of the cycle as seg(c)
and refer to its length by |seg(c)|. We refer to the candidate coming after c as
the succeeding candidate (succ(c)) and to the candidate coming before c as the
preceding candidate (pred(c)) (see Figure 3). We drop the c in parentheses if it
is clear from the context. We define the distance d(c1 , c2 ) between candidates
c1 and c2 as the number of agents between c1 and c2 when going from c1 to
c2 in direction of the cycle. We say a candidate c1 covers a candidate c2 (or
c2 is covered by c1 ) if |seg(c1 )| > d(c2 , c1 ) (see Figure 3). The leader election

Fig. 3. The figure depicts a part of a cycle that is oriented to the right. Non-candidate
agents are small black dots, candidates are bigger dots. The candidate c covers pred(c)
since |seg(c)| > d(pred(c), c).

progresses in phases. In each phase, each candidate executes Algorithm 1. A
phase consists of three synchronized subphases, i.e., agents can only progress to
the next subphase once all agents have finished the current subphase.

Algorithm 1 Leader Election for a Candidate c
Subphase 1:
pos ← position of succ(c)
if covered by any candidate or |seg(c)| < |seg(pred(c))| then
return not leader
Subphase 2:
if coin flip results in heads then
transfer candidacy to agent at pos
Subphase 3:
if only candidate on boundary then
if outside boundary then
return leader
else
return not leader

10

Daymude et al.

Consider the execution of Algorithm 1 by a candidate c. If the algorithm
returns "not leader" then c revokes its candidacy and becomes part of a segment. If the algorithm returns "leader", c will become the leader of the particle
system. The transferal of candidacy in subphase 2 means that c withdraws its
own candidacy but at the same time promotes the agent at position pos (i.e.,
succ(c) in subphase 1) to be a candidate. Once a candidate becomes a leader it
broadcasts this information such that all particles can halt.
3.3

Correctness

In order to show the correctness of our algorithm, we show that it satisfies the
following conditions, that relate to the entire particle system (not just a single
cycle):
1. Safety: There always exists at least one candidate.
2. Liveness: In each phase if there is more than one candidate, at least one
candidate withdraws leadership with a probability that is bounded below by
a positive constant.
Lemma 1. Algorithm 1 satisfies the safety condition.
Proof. We will show by induction that on the cycle associated with the outer
boundary there will always be at least one candidate. Initially, this holds trivially.
So assume that it holds before a phase. Let c be the candidate with the longest
segment. Then there is no candidate covering c and also |seg(c)| < |seg(pred(c))|
cannot be true. Hence, c will not withdraw candidacy in subphase 1. In subphase
2, the candidacy of c might be transferred but will not vanish. Let c0 be the
agent that received the candidacy if it was transferred and c0 = c otherwise. In
subphase 3, c0 will not withdraw candidacy because it lies on the outer boundary.
Hence, there is still a candidate after the phase.
t
u
Lemma 2. Algorithm 1 satisfies the liveness condition.
Proof. Assume that there are two or more candidates in the system. First we
consider the case that there is a cycle with two or more candidates. If there are
segments of different lengths on that cycle, we have |seg| < |seg(pred)| for at
least one candidate which will therefore withdraw its candidacy in subphase 1.
If all segments are of equal length, we have that in subphase 2 with probability
at least 41 there is a candidate c that transfers candidacy while succ(c) does not.
Hence, the number of candidates is reduced with probability at least 41 . Now
consider the case that all cycles have at most one candidate. Then there is a
cycle corresponding to an inner boundary that has exactly one candidate. That
candidate will withdraw candidacy in subphase 3 and thereby reduce the number
of candidates in the system.
t
u
The following Theorem is a direct consequence of Lemmas 1 and 2.
Theorem 1. Algorithm 1 successfully decides the leader election problem.

Leader Election and Shape Formation

3.4

11

Runtime Analysis

For a cycle of agents let L be the length of the cycle and let li be the longest
segment length before phase i of the execution of Algorithm 1. We define li = L
if there is no candidate on the cycle. It is easy to see that if li ≥ L/2 then in
phase i + 1 either the leader is elected (outer boundary) or all candidates on the
cycle vanish (inner boundary). For the case li < L/2, Lemma 3 provides the key
insight of our analysis.
Lemma 3. For any phase i such that li < L/2 it holds li+1 ≥ li in any case
and li+1 ≥ 2li with probability at least 1/4.
Proof. Consider a candidate c such that |seg(c)| = li . Subphase 1 can only
increase segment lengths and c will not withdraw leadership. So after Subphase
1 we have |seg(c)| ≥ li . Also, we have |seg(pred(c))| ≥ li because c covers any
candidate c0 such that d(c0 , c) < li . Here, we use pred(c) to refer to the cadidate
preceeding c after the execution of Subphase 1. For Subphase 2 we have to
distinguish between two cases based on the outcome of the coin flip of c. If
c does not transfer candidacy, we still have |seg(c)| ≥ li . For the case that c
does transfer candidacy, we have to further distinguish two cases based on the
outcome of the coin flip of pred(c). If pred(c) also transfers its candidacy, c will
receive that candidacy while c itself transfers its candidacy forward by a distance
of li . Therefore, we still have |seg(c)| ≥ li . If pred(c) does not transfer candidacy,
after Subphase 2 we have |seg(pred(c))| ≥ 2li because the segment of pred(c)
now spans both the segment of c after Subphase 1 and the segment of pred(c)
after Subphase 1, which is at least li . The probability that c transfers candidacy
while pred(c) does not is 1/4.
t
u
Let Lmax be the length of the longest cycle in the particle system. Based
on Lemma 3 it is easy to see that under complete synchronization of subphases
and with the agents having a global view of the cycle, our algorithm requires
on expectation O(log(Lmax )) phases to elect a leader. For the following analysis
assume that phases still progress in lockstep for all agents; however, the duration
of a phase is dependent on the longest segment length, i.e., phase i requires
O(li ) rounds. Theorem 2 gives a bound on the number of rounds required by the
algorithm based on this assumption.
Theorem 2. Algorithm 1 requires O(Lmax ) rounds on expectation.
Proof. Let the random variable Xi describe the number of rounds during the
execution of Algorithm 1 such that li ∈ [2i−1 , 2i ). Then, under the assumption
that phase i requires O(li ) rounds, the total runtime of our algorithm is
dlog(Lmax )e

T =

X
i=1

Xi · O(2i ).

12

Daymude et al.

Since E(Xi ) ≤ 4 due to Lemma 3, the expected runtime is
dlog(Lmax )e

E(T ) ≤

X

E(Xi ) · O(2i ) = O(Lmax ).

i=1

t
u
Note that subphase 1 of the algorithm is not important in terms of correctness.
However, it is crucial to achieve a linear runtime in expectation. If agents would
only execute subphases 2 and 3, the runtime would degrade to O(Lmax log Lmax ).
3.5

Asynchronous Local-Control Protocol

In this section we present a realization of Algorithm 1 as an asynchronous localcontrol protocol. The protocol heavily relies on token passing. All tokens used
by the protocol are messages of constant size and at any time an agent has
to hold at most a constant number of tokens. The tokens of each subphase of
Algorithm 1 are independent of each other, so an agent has to handle distinct
tokens for each phase at the same time. If not otherwise specified, we assume
that tokens of a single subphase move through the agents in a pipelined fashion
(i.e., a token does not surpass another token in front of it but waits until the
agent is free to hold it).
Candidate Elimination via Segment Comparison In Subphase 1 of Algorithm 1, a candidate c can become demoted if either the length of its front
segment is strictly less than that of its back segment, or some other candidate
covers c while performing its own comparison. Since c is unable to measure its
segments’ lengths directly in a local-control setting, it will instead use a token
passing scheme to match agents in its front segment with agents in its back segment. Conceptually, this matching proceeds as follows: each agent in the front
segment generates a cover token which travels into the back segment, matching one by one with the agents therein. If the front segment is longer than the
back segment, then some cover token will match with the preceding candidate;
a candidate which matches with a cover token is said to be covered. If c detects
that it was unable to cover any preceding candidates in this process and that
the segments lengths were not equal, it concludes that its front segment is too
short and revokes candidacy.
In detail, consider c as it begins the segment comparison subphase. Since this
subphase requires tokens to be passed in both its front and back segments, we
introduce the notion of tokens being either active or passive to avoid potential
collisions with tokens from other candidates. This can be imagined as two different “lanes” of tokens which can pass by one another unhindered. Candidate c
must first measure the length of its front segment; this is achieved by generating
a passive starting token and forwarding it along its front segment. Each noncandidate that receives this starting token forwards it and generates a passive

Leader Election and Shape Formation

13

cover token which is forwarded back towards c. The starting token is ultimately
forwarded to the succeeding candidate which consumes it and generates a final
cover token.
These cover tokens are forwarded back to c, which then converts them to
active cover tokens when forwarding them to its back segment where they begin
to match with the agents therein. Non-candidates in the back segment consume
the first cover token that is passed to them and continue to forward the others. If
another candidate, say c0 , receives an active cover token, then c0 has been covered
by c and revokes candidacy, henceforth behaving as a non-candidate. However, c0
may have been executing a number of now irrelevant leader election operations in
its front segment; thus, it generates a passive cleaning token which is forwarded
along its front segment, deleting any tokens it encounters with the exception
of those from c, which are active. This cleaning token is consumed by c. Additionally, it is possible that c0 performed work in its back segment, e.g., segment
comparison. Therefore, it is possible that there are upcoming non-candidates in
its back segment which are now holding incorrect information (e.g. the consumption of a cover token of c0 ). As this incorrect information could interfere with
the active cover tokens of c, also an active cleaning token is generated to reset
all non-candidates in the back segment of c0 , which is consumed by the first live
candidate it encounters. Lastly, this active cleaning token is also responsible for
destroying any other cleaning tokens it encounters, as the segment comparison
operations of c are the only ones still relevant to this segment.
Eventually, all active cover tokens will be consumed, completing the matching
originally described. Candidate c must now gather whether or not it has covered
another candidate, which is achieved as follows: instead of matching with the
final cover token, the final agent encountered, say a, consumes it and generates
a final cleaning token which travels in the direction of the cycle towards c. This
final cleaning token records whether or not a was a candidate. At every other
agent it visits, it resets the agent’s matched state and checks if the agent is
a covered candidate. Thus, when c receives this final cleaning token, one of
three cases occurs: (1) if the final cleaning token indicates that c covered some
other candidate, c remains a candidate; otherwise (2) if the final cleaning token
indicates that a was a candidate, c remains a candidate since its segments are
of equal length; otherwise (3) c’s front segment was too short, and it revokes
candidacy.
Coin Flipping and Candidate Transferral In order to realize the second
subphase as a local-control protocol, we need a token passing scheme for the
candidacy transferral, since a candidate is not able to transfer its candidacy in
an instant. Moreover, since candidates do not have a global view of the system,
they cannot know the position of their successor. The local-control protocol
consists of two different token passing schemes and candidates use one of the
two schemes dependent on the result of their coin toss.
A candidate c that flips a coin and receives heads sends a candidacy token along seg(c). However, c itself does not give up its candidacy immediately,

14

Daymude et al.

but (virtually) stays a candidate. This token is forwarded by non-candidates
in seg(c). A candidate c0 that receives a candidacy token sends a confirmation
token back along the segment of its predecessor. This confirmation token is forwarded back to c such that the virtual copy of c can finally give up its candidacy.
Moreover, there can be three different scenarios for a candidate c0 that receives a
candidacy token: (i) if c0 is not in subphase 2 of the protocol, it continues with its
desired behavior, (ii) if it received tails in the coin flip, c0 proceeds to the solitude
verification (with some caveats that will be explained in the next paragraph), or
(iii) if it received heads in the coin flip (i.e. c0 is a virtual candidate copy and
therefore also aims at transferring leadership), c0 will not give up its candidacy
(i.e., it will progress to solitude verification once it receives its own confirmation
token). Consequently, it is possible that even though a candidate receives heads,
it will not give up candidacy, because its predecessor also received heads.
In addition to the above mentioned token passing scheme for candidates
that receive heads, there is also an additional simple scheme for candidates that
receive tails. Before progressing to solitude verification a candidate c that receives
tails sends a token to pred(c) which is simply sent back by pred(c) and therefore
traverses seg(pred(c)) twice.
The first token passing scheme makes sure that candidacy tokens are not
forwarded infinitely often, but are eventually received by some candidate. This
behavior is due to the fact that candidacy is not revoked immediately after a
coin flip, i.e., virtual candidates are able to receive candidacy tokens.
The second scheme guarantees that a candidate c∗ with tails synchronizes
with its preceding candidate, i.e., c∗ waits for the return of its own token and a
preceding candidate that flipped heads has the chance to send its own candidacy
token to c∗ before c∗ progresses to solitude verification. This busy-waiting-like
behavior is needed due to the asynchronicity of the amoebot model which could,
in a worst-case scenario, prevent progress in the leader election process. To be
more specific consider a scenario with only two remaining candidates s, t with
equal segment lengths on a boundary. Leader election will not make any progress
if both candidates continuously get the same result from tossing a coin. In fact,
without the second scheme it is possible to enforce that both candidates always
get the same result. Imagine that candidate s gets heads, while t gets tails in
some round r. Since particles act in an asynchronous fashion we can assume that
t does not perform any action for any amount of time while s does. Without the
second scheme s can progress to solitude verification (which will fail), then start
segment comparison (which will not make any progress) and will flip a coin in
again. We continue this process of going through all the phases until s eventually
gets tails. Note that since t does not perform any action and due to the definition
of an asynchronous round, the round counter does not progress (i.e. the particle
system is still in round r) and we enforced that both candidates get the same coin
toss result. With the second scheme, s cannot immediately progress to solitude
verification, but waits for its token. Thereby, t is able to send its candidacy token
to s before s can flip a coin for the next time.

Leader Election and Shape Formation

15

Solitude Verification The local-control protocol for solitude verification is
based on the following simple observation: A candidate c is the only candidate
left on a cycle if and only if succ(c) = c. To allow the candidates to check this,
let each particle assign a unique identifier from {1, 2, 3} to each of its agents. It
is easy to see that succ(c) = c if and only if
1. c and succ(c) occupy the same node in Geqt , and
2. c and succ(c) have the same identifier.
Note that since a particle can hold multiple agents, Condition 2 is in fact necessary. It can be checked using a trivial token passing scheme. Checking Condition
1 requires a bit more effort: Intuitively, c enforces its own orientation on all
agents in its segment to establish a common coordinate system. For each agent
a in the segment consider the vector pointing from a to succ(a). The sum of
these vectors is (0, 0) if and only if c and succ(c) occupy the same node in Geqt ,
see Figure 4.

Fig. 4. An example of solitude verification: The candidate (shown slightly bigger) enforces a coordinate system (x and y arrows) on all agents. The agents determine the
vectors pointing to the succeeding agent in direction of the cycle (arrows and tuples at
nodes). Since the candidate is the only candidate on the cycle, the vectors add up to
(0, 0).

This algorithmic idea can be implemented as a local-control protocol using
token passing in the following way. We use two different types of tokens: matching
tokens and a unique activation token. The activation token is created by the
candidate c and traverses seg(c) four times. In its first pass, the activation token
moves forward (i.e., in direction of the cycle) from c to succ(c) and establishes
the common coordinate system among the agents. Also, whenever the activation
token is passed forward in the first pass by an agent a, that agent will create a
matching token which stores the vector pointing from a to succ(a) in the common
coordinate system. Once the activation token reaches succ(c), it initiates its
second pass in which it simply moves unhidered backwards (i.e., opposite to the
direction of the cycle) along seg(c) from succ(c) to c. In its third and fourth pass,
the activation token again moves forward from c to succ(c) and back. However,
in these two final passes it is not allowed to surpass any matching token.
The matching tokens move from their initial position forwards to succ(c) and
then backwards towards c. Every agent is only allowed to store one matching

16

Daymude et al.

token moving forward and one matching token moving backward (the direction
of movement is stored as part of a token). Furthermore, a matching token is never
allowed to surpass the activation token or another matching token. Whenever
an agent that holds two matching tokens (one for each direction) is activated,
that agent will try to match these tokens. In this matching, the agent compares
the vectors stored in the tokens coordinate-wise. If one vector has a 1 in a
coordinate while the other vector has a −1 in that coordinate, the agent will
change both these values to 0. Should a token be left with the vector (0, 0)
because of this matching process, it is deleted. Finally, the activated agent passes
on any remaining tokens if possible.
It is not hard to see that Condition 1 holds if and only if all matching tokens
are deleted in this process. Furthermore, c can easily distinguish whether all
matching tokens have been deleted: First, note that c will eventually be able
to delete or forward the matching token it created itself, so assume that c does
not hold that token anymore. If some matching token remains, that token will
reach c before the activation token returns from its fourth pass because the
activation token cannot surpass the matching token. On the other hand, if no
matching token remains, no such token can reach c before the activation token
returns from its fourth pass. Finally, note that the described process only stores
a constant amount of information in a token and every agent holds at most a
constant number of tokens at any time.
Inner Outer Boundary Test The last candidate of a cycle can decide whether
its cycle corresponds to an inner or the outer boundary as follows. A cycle
corresponding to an inner boundary has counter-clockwise rotation while a cycle
corresponding to the outer boundary has clockwise rotation, see Figure 2. The
candidate sends a token along the cycle that sums the angles of the turns the
cycle takes, see Figure 5. When the token returns to the candidate, its value

Fig. 5. The angle between the directions a token enters and exits an agent.

represents the external angle of the polygon corresponding to the cycle while
respecting the rotation of the cycle. So it is −360◦ for an inner boundary and
360◦ for the outer boundary. The token can represent the angle as an integer k

Leader Election and Shape Formation

17

such that the angle is k · 60◦ . Furthermore, to distinguish the two possible final
values of k it is sufficient to store the k modulo 5, so that the token only requires
3 bits of memory.
3.6

Linear Runtime with High Probability

The algorithm presented in Section 3.2 guarantees that a leader will be elected in
an expected linear number of rounds. A small modification of the algorithm can
lead to linear runtime with high probability (w.h.p.), without compromising its
correctness. For this we only need a slight modification of Algorithm 1: Subphase
1 needs to be executed twice, and in Subphase 2 a candidate c generates a
sequence of random bits b(c) = (b(c)1 , b(c)2 , . . .) that are compared with the
random bits of neighboring candidates instead of just flipping a single coin by
themselves. For two candidates c and c0 , b(c) < b(c0 ) if there is an i ≥ 0 so that
b(c)j = b(c0 )j for all j ≤ i and b(c)i+1 < b(c0 )i+1 .

Algorithm 2 Modified Leader Election
Execute Subphase 1 of Algorithm 1 twice
Subphase 2:
generate random bits for b(c) until b(c) 6= b(pred(c)) and b(c) 6= b(succ(c))
if b(c) < b(pred(c)) or b(c) < b(succ(c)) then
return not leader
Execute Subphase 3 of Algorithm 1

In a low-level implementation of Subphase 2, each candidate c continues to
produce random bits for b(c) and sends them in both directions in a pipelined
fashion until it learns that the competition on both sides is over, i.e., b(c) 6=
b(pred(c)) and b(c) 6= b(succ(c)). The competition is realized by pairing off the
bits from consecutive candidates at the agent of the segment between them where
they meet until a bit pair is different (if both bits are equal, they are deleted at
the agent), which is then reported back to the candidates.
As before, let li be the longest segment length before phase i, and let c be a
candidate such that |seg(c)| = li . Subphase 1 can only increase segment lengths
and hence after one execution of Subphase 1, c will not withdraw its leadership
candidacy. So after one application of Subphase 1 we have |seg(c)| ≥ li , and
also, |seg(pred(c))| ≥ li . After another application of subphase 1, various cases
can happen:
– Case 1: c is not a leader candidate any more. Then the segments of c and
pred(c) now belong to one candidate c0 , which means that |seg(c0 )| ≥ 2li .
– Case 2: c is still a leader candidate, but pred(c) is not a candidate any more.
Then the segments of pred(c) and pred(pred(c)) (by which we mean the

18

Daymude et al.

predecessor of pred(c) after the second application of Subphase 1) now belong to pred(pred(c)), and since |seg(pred(pred(c)))| ≥ |seg(pred(c))| before
pred(c) gave up its candidacy, after Subphase 1, |seg(pred(pred(c)))| ≥ 2li .
– Case 3: Both c and pred(c) are still candidates. Note that then, after the
second application of Subphase 1, also |seg(pred(pred(c)))| ≥ li . Since Subphase 2 ensures that of any two consecutive candidates, one of them will give
up its candidacy, either c or pred(c) will give up its candidacy in Subphase
2, which means that there will be a candidate c0 with |seg(c0 )| ≥ 2li .
Hence, in each phase the longest segment length is guaranteed to at least double, which means that the modified leader election algorithm is guaranteed to
terminate after at most log n phases. It is easy to show via Chernoff bounds
that in each phase and for each candidate c, O(log n) bits suffice w.h.p. so that
b(c) 6= b(pred(c)) and b(c) 6= b(succ(c)), which means that phase i takes at most
O(li+1 + log n) rounds w.h.p. Summing up these bounds over all phases results
in a runtime of O(Lmax ) w.h.p.
Note that while we are confident that the asynchronous local-control algorithm presented in the previous section performs close to the given simplified
analytical bound, it might require considerable effort to implement the modifications presented in this section as an asynchronous local-control protocol in such
a way that the given bound holds. The main issue is that our analysis requires
that the executions of Subphase 1 by Algorithm 2 are completely synchronized
among agents, which appears to be quite tricky to realize.

4

Line Formation in the Geometric Amoebot Model

In this section, we consider the line formation problem in the geometric amoebot
model. We assume that initially we have an arbitrary connected structure of
contracted particles with a unique leader. The leader is used as the starting
point for forming the line of particles and specifies the direction along which
this line will grow. As the line grows, every particle touching the line that is
already in a valid line position becomes part of the line. Any other particle
adjacent to the line becomes the root of a tree of particles. Every root aims at
traveling around the line in a clockwise manner until it joins the line. As a root
particle moves, the other particles in its tree follow in a worm-like fashion (i.e.,
via a series of handover operations)5 .
Before we give a detailed description of the algorithm, we provide some preliminaries. We distinguish for the state of a particle between idle, follower, root,
and retired (or halted). Initially, all particles are idle, except for the leader particle, which is always in a retired state. In addition to its state, each particle p
may maintain a constant number of flags in its shared memory. For an expanded
particle, we denote the node the particle last expanded into as the head of the
5

For a simulation video
http://sops.cs.upb.de .

of

the

Line

Formation

Algorithm

please

see

Leader Election and Shape Formation

19

particle and call the other occupied node its tail : In our algorithm, we assume
that every time a particle contracts, it contracts out of its tail.
The spanning forest algorithm (see Algorithm 3) is a basic building block
we use for shape formation problems. This algorithm aims at organizing the
particles as a spanning forest, where the particles that represent the roots of the
trees determine the direction of movement, whom the remaining particles follow.
Each particle p continuously runs Algorithm 3 until it retires. If particle p is a
follower, it stores a flag p.parent in its shared memory corresponding to the edge
adjacent to its parent p0 in the spanning forest (any particle q can then easily
check if p is a child of q). If p is the leader particle, then it sets the flag p.linedir
in the shared memories corresponding to two of its edges in opposite directions
(i.e., an edge i and the edge i + 3 (mod 6) in clockwise order), denoting that it
would like to extend the line through the directions given by these edges.

Algorithm 3 Line Formation Algorithm
SpanningForest (p):
Particle p acts as follows, depending on its current state:
idle:
If p is connected to a retired particle, then p becomes a root particle. Otherwise, if an adjacent particle p0 is a root or a follower, p sets the flag p.parent
on the shared memory corresponding to the edge to p0 and becomes a follower. If none of the above applies, it remains idle.
follower: If p is contracted and connected to a retired particle, then p becomes a
root particle. Otherwise, it considers the following three cases: (i) if p is
contracted and p’s parent p0 (given by the flag p.parent) is expanded, then
p expands in a handover with p0 , adjusting p.parent to still point to p0 after
the handover; (ii) if p is expanded and has a contracted child particle p0 ,
then p executes a handover with p0 ; (iii) if p is expanded, has no children,
and p has no idle neighbor, then p contracts.
root:
Particle p may become retired following CheckRetire (p). Otherwise, it
considers the following three cases: (i) if p is contracted, it tries to expand
in the direction given by LineDir(p); (ii) If p is expanded and has a child
p0 , then p executes a handover contraction with p0 ; (iii) if p is expanded and
has no children, and no idle neighbor, then p contracts.
retired: p performs no further action.
CheckRetire (p):
if p is a contracted root then
if p has an adjacent edge i to p0 with a flag p0 .linedir, where p0 is retired then
Let i0 be the edge opposite to i in clockwise order
p sets the flag p.linedir in the shared memory of edge i0
p becomes retired.
LineDir(p):
Let i be the label of an edge connected to a retired particle.
while edge i points to a retired particle do
i ← label of next edge in clockwise direction
return i

20

Daymude et al.

We have the following theorem, where work is defined as the number of
expansions and contractions executed by all particles in the system:
Theorem 3. Algorithm 3 correctly decides the line formation problem in worstcase optimal O(n2 ) work.
In order to prove Theorem 3, we first prove that the algorithm will eventually
correctly converge to a line in Geqt in Sections 4.1 and 4.2, and then show that
the algorithm terminates within worst-case optimal O(n2 ) work in Section 4.3.
4.1

Spanning Forest Formation

The first three lemmas demonstrate some properties that hold during the execution of the spanning forest algorithm and will be used in Section 4.2 to analyze
the complete algorithm, when we incorporate the check for retirement of particles according to the line formation problem, and the propagation of the line
direction.
The configuration of the system of particles at time t consists, for every
particle p, of the current state of p, including whether the particle is expanded
or contracted, any flags in p’s shared memory, the node(s) p occupies in G (given
by the relative position of p according to the other particles) at time t, as well
as the labeling of the bonds of p. Following the standard asynchronous model,
the system of particles progresses by performing atomic actions, each of which
affects the configuration of one or two particles. We say that followers and roots
particles are active. As specified in Algorithm 3, only followers can set the flag
p.parent.
Lemma 4. For every follower p, the node indicated by the flag p.parent is occupied by a non-idle particle.
Proof. Consider a follower p in any configuration during the execution of Algorithm 3. Note that p can only become a follower from an idle state, and once it
leaves the follower state it will not switch back to that state again. Consider the
first configuration c1 in which p is a follower. In the configuration c0 immediately
before c1 , p must be idle and it becomes a follower because of an active particle
p0 occupying the position indicated by p.parent in c0 . The particle p0 is still
adjacent to the edge flagged by p.parent in c1 . Now assume that p.parent points
to an active particle p0 in a configuration ci , and that p is still a follower in the
next configuration ci+1 that results from executing an action a. If a affects p and
p0 , the action must be a handover in which p updates its flag p.parent such that
p.parent may be moved to the edge that now connects p to p0 in ci+1 . If a affects
p but not p0 , it must be a contraction in which p.parent does not change and
still points to p0 . If a affects p0 but not p, there are multiple possibilities. The
particle p0 might switch from follower to root state, or from root to retired state,
or it might expand, none of which violates the lemma. Furthermore, p0 might
contract. If p.parent points to the head of p0 , p0 is still adjacent to the edge
flagged by p.parent in ci+1 . Otherwise, p is a child adjacent to the tail of p0 in ci

Leader Election and Shape Formation

21

and therefore the contraction must be part of a handover. As p is not involved
in the action, the handover must be between p0 and a third active particle p00 .
After the handover, p00 will occupy the position originally occupied by the tail of
p0 and hence p.parent points to p00 . Finally, if a affects neither p nor p0 , p.parent
will still point to p0 in ci+1 .
t
u
Based on Lemma 4, A(c) contains the same nodes as the nodes occupied in
Geqt by the set of active particles in c. For every expanded particle p in c, A(c)
contains a directed edge from the tail to the head of p, and for every follower p0
in c, A(c) contains a directed edge from the head of p0 to p0 .parent, if p0 .parent
is occupied by an active particle.
Lemma 5. The graph A(c) is a forest, and every connected component of idle
particles is connected to a non-idle particle.
Proof. Since in an initial configuration c0 all particles are idle, except for the
leader particle which is retired, and the particle system is connected, the lemma
holds trivially for c0 . Now assume that the lemma holds up to a cerain configuration ci and consider a connected component of idle particles. If an idle particle
p in the component is activated, it may stay idle or become an active particle.
The former case does not affect the configuration. So consider the latter case.
If p has changed into a follower state, it joins an existing tree, and in case p
has become a root, it forms a new tree in A(ci+1 ). In either case, A(ci+1 ) is a
forest and the connected component of idle particles that p belongs to in ci is
either non-existent or connected to p in ci+1 . If a follower or a root particle p
that is connected to the idle component is activated, it cannot contract unless
by a handover with another active particle, which implies the nodes occupied
by p will remain occupied by the active particles. While such a handover can
change the parent relation among the nodes, it cannot violate the lemma. If an
active particle p is activated and has no child p0 such that p0 .parent is the tail of
p and p is not connected to the idle component, it contracts and its contraction
does not disconnect any particle in its tree in A(ci+1 ). Moreover, an expansion
of an active particle or changing its state to a root, if it is a follower, or to a
retired particle, if it is a root particle, does not violate the lemma too. Finally,
if a retired particle is activated, it does not move. Therefore, the lemma holds
at configuration ci+1 .
t
u
The following lemma shows that the spanning forest always makes progress,
by showing that as long as the roots keep moving, the remaining particles will
eventually follow.
Lemma 6. An expanded particle eventually contracts.
Proof. Consider an expanded particle p in a configuration c. Note that p must be
active. If there is an enabled action that includes the contraction of p, that action
will remain enabled until p eventually contracts when it is selected within the
current or next round. So assume that there is no enabled action that includes the

22

Daymude et al.

contraction of p. According to Lemma 5 and the transition rule from idle to some
active state, at some point in time there will be no idle particles in the system. If
the contraction of p becomes part of an enabled action before this happens, p will
eventually contract. So assume that all particles are non-idle but still p cannot
contract. If p has no children, the isolated contraction of p is an enabled action
which contradicts our assumption. Therefore, p must have children. Furthermore,
at least one child p0 of p must have a p0 .parent flag on the edge connecting to
the tail of p and all children of p must be expanded, as otherwise p could again
contract as part of a handover. If p0 would contract, a handover between p0 and
p would become an enabled action and p would eventually contract. Hence p0
also cannot contract. Applying this argument recursively, we identify to p0 a set
of expanded particles forming a branch of a tree in A(c), until we reach a leaf
q of A(c) (by Lemma 5). Obviously q will contract next time it is activated.
Therefore, we found a sequence of expanded particles that starts with p0 and
ends with a particle that eventually contracts, implying that it in the sequence
to contract and so on. the contraction of p will become part of an enabled action
and therefore p will eventually contract.
t
u
4.2

Line Formation

Now, we can show that the algorithmic primitives as developed in the Section 4
solve the line formation problem.
Theorem 4. Algorithm 3 correctly decides the line formation problem.
Proof. We need to show that the algorithm terminates and that when it does,
the formed shape is a straight line in Geqt . As a result of Lemma 5 and due to the
transition rules from idle to active states, every idle particle p eventually switches
out of idle state. According to the algorithm proposed for the line formation
problem, if p is adjacent to a retired particle, it becomes a root and moves in
clockwise order around the current line structure, until it eventually reaches one
of the valid positions that can extend the line and becomes retired (halted). By
contradiction, assume p never becomes retired. Since the number of particles
is bounded (and therefore the size of the current line structure is bounded), p
cannot move around the line structure indefinitely (since in this case p must
occupy a valid extension position and would have become retired). Thus there
must be an infinite number of configurations ci where p has a particle blocking
its desired path around the current line structure. Let p0 be the last particle
p sees as its clockwise neighbor over the line structure. Since p0 is touching a
retired particle, p0 will become a root particle within at most two rounds, and will
stay connected to the line structure and always attempt to move in a clockwise
manner, p0 is well-defined. Applying the same argument we had for p inductively
to p0 results an infinite sequence of roots adjacent to the line structure that never
touch a valid spot pointed by one of p.linedir flags of an already retired particle
belong to the line. This is a contradiction, since the current line structure (the
current number of retired particles) is bounded. Therefore, every root eventually
changes into a retired state.

Leader Election and Shape Formation

23

According to spanning forest construction, as long as the roots keep moving
the followers eventually follow. From Algorithm 3, every follower in the neighborhood of a retired particle becomes a root. For every root q with at least one
follower child, let c be the first configuration when q becomes retired. If q still
has any child in c then all of its children p become roots. Applying this argument
recursively together with the already proven fact that every root eventually becomes retired we will reach to a configuration such that there exists no root with
a follower child which proves that eventually every follower becomes a root.
Putting it all together, eventually all particles become retired and the algorithm terminates. Note that it also follows from the argument above that the set
of retired particles at the end of the algorithm forms a connected structure (since
the particles start from a connected configuration and never get disconnected
through the process). The connected structure must form a line, since a root
particle may only become retired once it is contracted and occupies a valid spot
extending the current line.
t
u
4.3

Performed Work

Finally, we evaluate the performance of our algorithm in terms of the number
of movements (expansions and contractions) of the particles, i.e., the total work
performed by the algorithm.
Lemma 7. The worst-case work required by any algorithm to solve the line formation problem is Ω(n2 ).
Proof. Assume the initial configuration
of the set of particles forms a connected
√
structure of diameter at most 2 n + 2 (e.g., if it forms a hexagonal or square
shape in Geqt ). Since the line has diameter equal to n − 1, there must
√ exist
1
some particle that will need to traverse a distance of
at
least
n − 3, a
n
−
2
2
√
second particle that will traverse a distance of n − 2 n − 4, etc., irrespective of
the algorithm
used. The number of particle movements incurred will be at least
P 12 n−2√n−1 1
√
2
(
t
u
i=1
2 n − 2 n − i − 2) = Θ(n ).
In the following, we will show a matching upper bound:
Theorem 5. Algorithm 3 terminates in O(n2 ) work.
Proof. To prove the upper bound, we simply show that every particle executes
O(n) movements. The theorem then follows. Consider a particle p. While p is in
an idle or a retired state, it does not move. Let c be the first configuration when
p becomes a follower. Consider the directed path in A(c) from the head of p to
its root p0 . There always is a such a path since every follower belongs to a tree in
A(c) by Lemma 5. Let P = (a0 , a1 , . . . , am ) be that path in A(c) where a0 is the
head of p and am is a child of p0 . According to Algorithm 3, p will follow P by
sequentially expanding into the nodes a0 , a1 , . . . , am . The length of this path is
bounded by 2n and, therefore, the number of movements p executes while being
a follower is O(n). Once p becomes a root, it only performs expansions and

24

Daymude et al.

contractions around the currently constructed line structure, until it reaches one
of the valid positions on the line. Since the total number of retired particles is at
most n, this leads to an additional O(n) movements by p. Therefore, the number
of movements a particle p executes is O(n), which concludes the theorem.
t
u

5

Self-stabilizing Leader Election and Shape Formation

Consider the variant of the geometric amoebot model in which faults can occur
that arbitrarily corrupt the local memory of a particle. Recall that for an algorithm to solve the leader election problem in a self-stabilizing manner, it has to
satisfy the following requirements: First, from any initial system state (in which
the particle structure is connected) the particle system eventually reaches a final
system state while preserving connectivity, i.e., eventually a unique leader will
be established. Second, once a final system state is reached, the system has to
remain in that state as long as no faults occur. Analogous requirements have to
be satisfied for self-stabilizing shape formation.
Our leader election algorithm can be extended to a self-stabilizing leader
election algorithm with O(log∗ n) memory using the results of [6, 30] (i.e., we
use their self-stabilizing reset algorithm on every cycle in order to recover from
failure states). However, it is not possible to design a self-stabilizing algorithm for
the line formation. The reason for this is that even a much simpler problem called
movement problem cannot be solved in a self-stabilizing manner. It is easy to see
that if the movement problem cannot be solved in a self-stabilizing manner, then
also the line formation problem cannot be solved in a self-stabilizing manner.
In the movement problem we are given an initial distribution A of particles
that can be in a contracted as well as expanded state, and the goal is to change
the set of nodes occupied by the particles without causing disconnectivity. For
the ring of expanded particles it holds that for any protocol P there is an initial
state so that P does not solve the movement problem. To show this we consider
two cases: suppose that there is any state s for some particle in the ring that
would cause that particle to contract. In this case set two particles on opposite
sides of the ring to that state, and the ring will break due to their contractions.
Otherwise, P would not move any particle of the ring, so also in this case it
would not solve the movement problem in a self-stabilizing manner.

6

Conclusion

The algorithms presented for the geometric amoebot model can be extended for
the case that G is a different regular grid graph embedded in the two-dimensional
Euclidean plane. As future work, we would like to identify the minimum set of
key geometric properties that G must have in order for the proposed algorithms
to work.

Leader Election and Shape Formation

25

Acknowledgment
We would like to thank John Reif for the helpful discussions that led to the
realization of our algorithm with high probability guarantees.

References
1. L. M. Adleman. Molecular computation of solutions to combinatorial problems.
Science, 266(11):1021–1024, 1994.
2. Chrysovalandis Agathangelou, Chryssis Georgiou, and Marios Mavronicolas. A
distributed algorithm for gathering many fat mobile robots in the plane. In Proceedings of the 2013 ACM symposium on Principles of distributed computing, pages
250–259. ACM, 2013.
3. R. Ananthakrishnan and A. Ehrlicher. The forces behind cell movement. International Journal of Biological Sciences, 3(5):303–317, 2007.
4. D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in
networks of passively mobile finite-state sensors. Distributed Computing, 18(4):235–
253, 2006.
5. D. Arbuckle and A. Requicha. Self-assembly and self-repair of arbitrary shapes
by a swarm of reactive robots: algorithms and simulations. Autonomous Robots,
28(2):197–211, 2010.
6. Baruch Awerbuch and Rafail Ostrovsky. Memory-efficient and self-stabilizing network {RESET} (extended abstract). In Proceedings of the Thirteenth Annual ACM
Symposium on Principles of Distributed Computing, Los Angeles, California, USA,
August 14-17, 1994, pages 254–263, 1994.
7. L. Barriere, P. Flocchini, E. Mesa-Barrameda, and N. Santoro. Uniform scattering
of autonomous mobile robots in a grid. Int. Journal of Foundations of Computer
Science, 22(3):679–697, 2011.
8. A. Bhattacharyya, M. Braverman, B. Chazelle, and H.L. Nguyen. On the convergence of the hegselmann-krause system. CoRR, abs/1211.1909, 2012.
9. D. Boneh, C. Dunworth, R. J. Lipton, and J. Sgall. On the computational power
of DNA. Discrete Applied Mathematics, 71:79–94, 1996.
10. V. Bonifaci, K. Mehlhorn, and G. Varma. Physarum can compute shortest paths.
In Proceedings of SODA ’12, pages 233–240, 2012.
11. Z. J. Butler, K. Kotay, D. Rus, and K. Tomita. Generic decentralized control for
lattice-based self-reconfigurable robots. International Journal of Robotics Research,
23(9):919–937, 2004.
12. B. Chazelle. Natural algorithms. In Proc. of ACM-SIAM SODA, pages 422–431,
2009.
13. Ho-Lin Chen, David Doty, Dhiraj Holden, Chris Thachuk, Damien Woods, and
Chun-Tao Yang. Fast algorithmic self-assembly of simple shapes using random
agitation. In DNA Computing and Molecular Programming, pages 20–36. Springer,
2014.
14. Moya Chen, Doris Xin, and Damien Woods. Parallel computation using active selfassembly. In DNA Computing and Molecular Programming, pages 16–30. Springer,
2013.
15. K. C. Cheung, E. D. Demaine, J. R. Bachrach, and S. Griffith. Programmable assembly with universally foldable strings (moteins). IEEE Transactions on Robotics,
27(4):718–729, 2011.

26

Daymude et al.

16. G. Chirikjian. Kinematics of a metamorphic robotic system. In Proceedings of
ICRA ’94, volume 1, pages 449–455, 1994.
17. Mark Cieliebak, Paola Flocchini, Giuseppe Prencipe, and Nicola Santoro. Distributed computing by mobile robots: Gathering. SIAM Journal on Computing,
41(4):829–879, 2012.
18. R. Cohen and D. Peleg. Local spreading algorithms for autonomous robot systems.
Theoretical Computer Science, 399(1-2):71–82, 2008.
19. S. Das, P. Flocchini, N. Santoro, and M. Yamashita. On the computational power
of oblivious robots: forming a series of geometric patterns. In Proceedings of 29th
ACM Symposium on Principles of Distributed Computing (PODC), 2010.
20. X. Defago and S. Souissi. Non-uniform circle formation algorithm for oblivious
mobile robots with convergence toward uniformity. Theoretical Computer Science,
396(1-3):97–112, 2008.
21. E. D. Demaine, M. J. Patitz, R. T. Schweller, and S. M. Summers. Self-assembly of
arbitrary shapes using rnase enzymes: Meeting the kolmogorov bound with small
scale factor (extended abstract). In Proceedings of STACS ’11, pages 201–212,
2011.
22. Zahra Derakhshandeh, Shlomi Dolev, Robert Gmyr, Andréa W. Richa, Christian
Scheideler, and Thim Strothmann. Brief announcement: Amoebot — a new model
for programmable matter. In SPAA, 2014.
23. David Doty. Theory of algorithmic self-assembly. Communications of the ACM,
55(12):78–88, 2012.
24. Paola Flocchini, David Ilcinkas, Andrzej Pelc, and Nicola Santoro. Computing
without communicating: Ring exploration by asynchronous oblivious robots. Algorithmica, 65(3):562–583, 2013.
25. Paola Flocchini, Giuseppe Prencipe, Nicola Santoro, and Peter Widmayer. Arbitrary pattern formation by asynchronous, anonymous, oblivious robots. Theoretical
Computer Science, 407(1):412–447, 2008.
26. T. Fukuda, S. Nakagawa, Y. Kawauchi, and M. Buss. Self organizing robots based
on cell structures - cebot. In Proceedings of IROS ’88, pages 145–150, 1988.
27. Jacob Hendricks, Matthew J Patitz, and Trent A Rogers. Replication of arbitrary hole-free shapes via self-assembly with signal-passing tiles. arXiv preprint
arXiv:1503.01244, 2015.
28. T.-R. Hsiang, E. Arkin, M. Bender, S. Fekete, and J. Mitchell. Algorithms for
rapidly dispersing robot swarms in unknown environments. In Proceedings of the
5th Workshop on Algorithmic Foundations of Robotics (WAFR), pages 77–94, 2002.
29. Alon Itai and Michael Rodeh. Symmetry breaking in distributive networks. In 22nd
Annual Symposium on Foundations of Computer Science, Nashville, Tennessee,
USA, 28-30 October 1981, pages 150–158, 1981.
30. Gene Itkis and Leonid A. Levin. Fast and lean self-stabilizing asynchronous protocols. In 35th Annual Symposium on Foundations of Computer Science, Santa Fe,
New Mexico, USA, 20-22 November 1994, pages 226–239, 1994.
31. S. Kernbach, editor. Handbook of Collective Robotics – Fundamentals and Challanges. Pan Stanford Publishing, 2012.
32. P. Kling and F. Meyer auf der Heide. Convergence of local communication chain
strategies via linear transformations. In Proceedings of the 23rd ACM Symposium
on Parallelism in Algorithms and Architectures, pages 159–166, 2011.
33. K. Li, K. Thomas, C. Torres, L. Rossi, and C.-C. Shen. Slime mold inspired path
formation protocol for wireless sensor networks. In Proceedings of ANTS ’10, pages
299–311, 2010.

Leader Election and Shape Formation

27

34. J. McLurkin. Analysis and Implementation of Distributed Algorithms for MultiRobot Systems. PhD thesis, Massachusetts Institute of Technology, 2008.
35. Matthew J Patitz. An introduction to tile-based self-assembly and a survey of
recent results. Natural Computing, 13(2):195–224, 2014.
36. M. Rubenstein and W. Shen. Automatic scalable size selection for the shape of a
distributed robotic collective. In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent
Robots and Systems (IROS), 2010.
37. Michael Rubenstein, Alejandro Cornejo, and Radhika Nagpal. Programmable selfassembly in a thousand-robot swarm. Science, 345(6198):795–799, 2014.
38. J. E. Walter, J. L. Welch, and N. M. Amato. Distributed reconfiguration of metamorphic robot chains. Distributed Computing, 17(2):171–189, 2004.
39. E. Winfree, F. Liu, L. A. Wenzler, and N. C. Seeman. Design and self-assembly of
two-dimensional dna crystals. Nature, 394(6693):539–544, 1998.
40. Damien Woods. Intrinsic universality and the computational power of selfassembly. In Turlough Neary and Matthew Cook, editors, Proceedings Machines,
Computations and Universality 2013, MCU 2013, Zürich, Switzerland, September
9-11, 2013., volume 128 of EPTCS, pages 16–22, 2013.
41. Damien Woods, Ho-Lin Chen, Scott Goodfriend, Nadine Dabby, Erik Winfree, and
Peng Yin. Active self-assembly of algorithmic shapes and patterns in polylogarithmic time. In ITCS, pages 353–354, 2013.
42. M. Yim, W.-M. Shen, B. Salemi, D. Rus, M. Moll, H. Lipson, E. Klavins, and G. S.
Chirikjian. Modular self-reconfigurable robot systems. IEEE Robotics Automation
Magazine, 14(1):43–52, 2007.

