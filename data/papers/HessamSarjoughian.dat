Superdense Time Trajectories for DEVS Simulation Models
Hessam S. Sarjoughian and Savitha Sundaramoorthi
Arizona Center for Integrative Modeling & Simulation
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA
sarjoughian@asu.edu and savitha@asu.edu
a set and ‫ ط‬is an ordering relation on the elements of ܶ.
Given ܶ and a set of arbitrary values ܸ with the constraints
that for any ‫ ܶ א ݐ‬there exists only one value ‫ܸ א ݒ‬,
trajectory charts with linear time can be easily plotted. For
modeling approaches that can support multiple, contiguous
instantaneous state changes the concept of linear time needs
to be augmented with superdense time where two
simultaneous time instances are defined to be contiguous
with time instances‫ݐ‬ሾ݊ଵ ሿand‫ݐ‬ሾ݊ଶ ሿ.
A discrete-time and discrete-event models can be defined in
terms of input and output sequences. In these systemtheoretic modeling formalisms [10,18,19], time is
mathematically defined to be either discrete or continuous.
These models may allow multiple inputs (or outputs) to
occur at an instance of time due to a model being in one or
multiple states for a zero time duration (e.g., parallel DEVS
[2]). The functions responsible for processing these inputs,
changing states, and producing outputs can be
mathematically specified in terms of superdense time [11].
However, to visually render trajectory charts for a finite
number of contiguous state changes at an instance of time
(i.e., instantaneous state transitions) and thus multiple inputs
and outputs occurring an instance of time, visual
representations accommodating superdense time need to be
conceptualized, formulated, and implemented. They are
particularly important for understanding transient state
transitions (state changes in zero time period). State (with
input/output trajectories) are very useful for understanding
time-based dynamics of simulation models and especially
identifying subtle timing issues which may be due to design
flaws and/or implementation errors.
A modeling formalism that supports sequential and parallel
model execution and communication is known as Discrete
Event System Specification (DEVS) [2]. The theory of
DEVS modeling is well-suited for conceptualizing
superdense time. Trajectory charts with superdense time can
be systematically defined and developed for tools such as
DEVS-Suite simulator [5,8].
The DEVS formalism, grounded in system-theory [18], is
based on a strong notion of I/O modularity where data
belonging to any model component can only be shared with
another model component via their coupled input and output
ports. Complex, large-scale coupled models can be
composed via tree-structure composition of atomic and
coupled models subject to satisfying the closure under
coupling principle. Inner dynamics of any atomic model

ABSTRACT

Many scientific and engineering applications generate data
that are well-suited to be studied using time series charts.
Two types of time series that define input, output, and state
dynamics of DEVS models are piecewise constant and event
charts. In this paper, time series capable of displaying both
linear and superdense time segments and trajectories are
conceptualized and formulated. These lend themselves for
visualizing behavior of parallel atomic and coupled DEVS
models. The concept of superdense time segments is realized
as plug-ins as part of the Eclipse BIRT (Business Intelligence
and Reporting Tool) framework. They can receive timebased alphanumerical data sets from external static and
dynamic sources, including the DEVS-Suite simulator. As
standalone plug-ins, time series can be used to create static
plots and used in BIRT reports. These plug-ins are also
integrated into the DEVS-Suite simulator where each model
component’s behavior can be customized and dynamically
plotted. Time series charts simplify and complement tabular
logging of data sets for developing simulation models that
exhibit zero-time transitory state transitions.
Author Keywords

BIRT, Data Trajectories; DEVS-Suite Simulator; Dynamic
Visualization, Event Chart, Linear Time; Piecewise Constant
Chart, Superdense Time.
ACM Classification Keywords

D.2.2 [Software Engineering]: Design Tools and
Techniques; D.2.5 [Testing and Debugging]: Debugging
Aids; D.2.6 [Programming Environments] Graphical
Environments; I.6.1 [Simulation Theory]; I.6.4 [Model
Validation and Analysis]; I.6.5 [Model Development]; I.6.6
[Simulation Output Analysis]; I.6.8 [Types of Simulation]:
Visual.
INTRODUCTION

In many component-based modeling approaches, dynamics
of the simulated components and their interactions are
defined in terms of time. Computations and communications
between any two components must occur either sequentially
or concurrently with respect to a clock. The measure of time
between any two consecutive time instances can have
unbounded accuracy (as in continuous time) or bounded
accuracy (as in discrete-time). The notion of linear time is
concisely defined as an algebraic structure ‫ܶۃ‬ǡ ‫ ۄط‬where ܶ is
6SULQJ6LP706'(96$SULO$OH[DQGULD9$86$
6ociety for Modeling & Simulation International (SCS)

249

(defined in terms of time and state) can be observed via I/O
variables. Operations in all atomic and coupled model
components can be defined (directly or indirectly) in terms
of a logical time base. As complexity and scale of atomic and
coupled model increases, it is useful to conveniently choose
and observe their dynamics of interests in alternative, and
complementary textual, tabular, and visual forms. Plotting
trajectory charts for models that undergo contiguous,
instantaneous state transitions is desirable although nontrivial to conceptualize and formulate given the restrictions
imposed for creating input, output, and state trajectories in
finite spatial spaces.
Every model can have input and output variables with
corresponding input and output ports. The data variable types
for inputs, outputs, and states can be simple (e.g., int and
enum primitive data types) or complex (e.g., Integer and
Pair class types). Variables such as Pair have complex
structure (e.g., (key,value), key=(phase,sigma),
value=String) from the standpoint of displaying them
visually. Consequently, crude simplifications are often
employed to display such complex data types. To visually
view these data trajectories requires defining representations
that can capture simultaneity for input, outputs, and states.
The time base used for visual data trajectories can be
restricted to be linear even though the model can undergo
internal or external transition in zero logical time. Limiting
the time base of visual data trajectories to be strictly linear
prevents visualizing transient state changes and generation
of simultaneous events. The dynamics of a model
instantaneously responding to input events or generate
output events cannot be visually created and thus
inaccessible (visualized) while the model being executed.
Another useful capability for visualizing the dynamics of any
atomic model is to display variables belonging to a
component in a “synchronized” fashion. Although an
arbitrary model’s behavior is asynchronous (i.e., inputs,
outputs, and states may occur at distinct time steps during a
simulation cycle), it is useful for the time axes of all selected
data trajectories to be aligned. Such a visualization allows
precise visual observation (evaluation) of the behavior of any
atomic model including its I/O and state changes in response
to internal, external, and confluent transition functions.
Observation of the I/O and state variables can be extended to
include the atomic simulator’s state variables such as time of
last event and time to next event. Alignment of the model’s
and simulator’s visual data trajectories offers the modeler the
ability to examine and understand simple and complex
relationships that exist among external, internal, output, and
time advance functions.
In the DEVS modeling formalism, two key concepts for the
atomic model are allowing time to the next event to be
defined as infinity or zero for the external or internal
transition functions. The former allows future input events to
arrive at arbitrary time instances. This requires visualizing
time to next event data trajectories having the infinity value.
With the latter, transitory state transitions can be allowed.

250

The consequence is that (input, output, and state) trajectory
charts for all legitimate DEVS models needs to be supported
– i.e., dynamics of any atomic model that has a finite number
of state transitions with zero time durations – can be plotted.
Supporting visualization of superdense time results in
creating trajectory charts for atomic and coupled models that
can receive multiple inputs or outputs at the same time
instance.
TRAJECTORIES WITH LINEAR AND SUPERDENSE TIME
BASES

Trajectories States, inputs, and outputs of any dynamical
model can be defined in terms of linear [19] and superdense
time [11]. Time can be represented by a variable ࢚. We can
specify every time-dependent input, state, or output of a
model represented in terms of a variable ࢜ and variable ࢚.
Definition 1. ‫ ݐ‬ሾ‫׋‬ሿ ൌ ‫ ݐ‬ൈ ሾ‫׋‬ሿǡ ‫  א ݐ‬Թା
଴ǡஶǡ ‫ א׋‬Գ̳λ represents
a collection of time instances that are monotonically ordered
with respect to time ‫ݐ‬. Time is a linear variable – i.e., given
time duration ߜ‫ݐ‬௜ ؐ ‫ݐ‬௜ାఓ െ ‫ݐ‬௜ ൌ ߤ and shifted by ߦ ‫  א‬Թା ,
then ߜ‫ݐ‬௝ ؐ ‫ݐ‬௝ାఓ െ ‫ݐ‬௝ ൌ ߤǡ ݆ ൌ ݅ ൅ ߦ. The reference value
for ‫ ݐ‬is defined to be zero; it can be shifted by any positive
or negative finite value. For any time instances ‫ݐ‬௜ and ‫ݐ‬௝ that
are equal (i.e., ‫ݐ‬௜ െ ‫ݐ‬௝ ൌ Ͳ), they are distinguished using the
index term ‫ – ׋‬e.g., given ‫ ݐ‬ሾ଴ሿ ǡ ‫ ݐ‬ሾଵሿ ǡ ‫ ݐ‬ሾଶሿ , time instant ‫ ݐ‬ሾଵሿ
occurs after ‫ ݐ‬ሾ଴ሿ and before ‫ ݐ‬ሾଶሿ denoted as ‫ ݐ‬ሾ଴ሿ ‫ ݐ  ع‬ሾଵሿ ‫ع‬
‫ ݐ‬ሾଶሿ . The operator ‫ ع‬defines weak simultaneity of time
which means two consecutive time instances ‫ ݐ‬ሾ௡ሿ and ‫ ݐ‬ሾ௡ାଵሿ
are unequal. The index term ݊ is required if at least there are
two equal time instances. For totally ordered time instances
belonging to ‫ݐ‬, they degenerate to ‫ ڮ‬ǡ ‫ݐ‬௚ ǡ ‫ݐ‬௝ ǡ ‫ݐ‬௣ ǡ ‫ڮ‬ǤTherefore,
ሾ଴ሿ ሾଵሿ

ሾ௠ሿ

the time base can be ‫ ڮ‬ǡ ‫ݐ‬௝ ǡ ‫ݐ‬௞ ‫ݐ‬௞ ǡ ‫ ڮ‬ǡ ‫ݐ‬௞ ǡ ‫ݐ‬κ ǡ ‫ ݉ ڮ‬൏
λ. Totally and partially ordered time instances are illustrated
in Figure 1.
Definition 2. ݃ ൌ  ‫ ݐ‬ሾ‫׋‬ሿ ՜ ‫ ݒ‬is an onto, but not a one-to-one
function. ‫  ؐ ݒ‬ሼܽǡ ‫ ڮ‬ǡ ‫ݖ‬ሽ ‫ ׫‬ሼ‫ܣ‬ǡ ‫ ڮ‬ǡ ܼሽ ‫ ׫‬Ժ ‫ ׫‬Թ ‫ ׫‬േλ can
represent alphanumeric values for state, input, and output
variables. Every alphanumeric value has a finite length.
Values at any time instance for input, output, and state
trajectories are determined using ݃൫‫ ݐ‬ሾ‫׋‬ሿ ൯ ൌ ‫ݒ‬. Variables
with numeric values are linear. Alphanumeric values have an
arbitrary sequence. The negative and positive infinity values
are the smallest and largest values. When a variable with
alphanumeric values is assigned infinity, the values form a
linear relationship with respect to the negative and positive
infinity values as being the only smallest and the only largest
values. A piecewise constant trajectory with a time that has
both linear and superdense time is illustrated in Figure 2.
A trajectory is defined to be a concatenation of one or a finite
number of segments. Two distinct time instance values are
required to define the duration of a segment. The duration of
a segment can be defined to range from zero to infinity
inclusively. A segment can have a duration of zero, in which
case its start and end time instances are partially ordered
ሾ௞ሿ
ሾ௞ାଵሿ
(see Definitions 1 and 2). A segment with
‫ݐ‬௜ ‫ݐ  ع‬௜

‫ܺۃ‬ǡ ܻǡ ‫ܦ‬ǡ ሼ‫ܯ‬ௗ ሽǡ ‫ܥܫ‬ǡ ‫ܥܫܧ‬ǡ ‫ ۄܥܱܧ‬are strictly defined as event
segments and trajectories. The state of any atomic model are
also strictly defined as piecewise constant segments and
trajectories. Event segments and trajectories may be
transformed to piecewise, continuous, or other forms using
appropriate mapping functions.
The values for trajectories can range from negative to
positive infinity. Therefore, it is necessary for charts to
support visualizing the full range of values a variable may
have. An example of a variable that can have positive infinity
value is time to next event in DEVS atomic model. Clearly,
there may also exist some variable with negative infinity
value. Therefore, input, output, and state piecewise constant
and event trajectories with infinity values are important to be
visually rendered.

ሾ௤ሿ

infinity duration is linearly ordered – i.e., ‫ݐ‬௜  ൏  ‫ݐ‬௝  or ‫ݐ‬௜ ൏
ሾ௤ሿ
λǡ ‫ݐ‬௜

് λǡ ‫ݐ‬௝ ൌ λ and ‫ ݍ‬൐ Ͳ. A segment
‫ݐ‬௝ where‫ݐ‬௜ ്
with infinity duration is a mathematical artifact which cannot
be exactly displayed.

Figure 2. Piecewise constant data trajectory with
superdense time base.
Input trajectories

An input is defined as either endogenous or exogenous.
Inputs received from other atomic models are endogenous.
Time-based input variables are defined as ݃ ൌ  ‫ ݐ‬ሾ‫׋‬ሿ ՜ ‫ݒ‬.
Every input value corresponds to a unique time instance (see
Definition 1). Input trajectories generated by sources other
than atomic or coupled model (communicated via
‫ܥܫ‬ǡ ‫ܥܫܧ‬ǡ ‫ ܥܱܧ‬couplings) are exogenous. An example of an
exogenous input trajectory is a data file having all its entries
satisfying Definitions 1 and 2. These input trajectories can
become endogenous via “autonomous” atomic models that
read such data files and generate output events which in turn
become input events.

Figure 1. (a) Time segment satisfying the ൏ linear time
ordering relationship, (b) contiguous time segments each
having zero time duration, (c) contiguous time segments
transformed to superdense time.

A segment can be piecewise constant (i.e., variable ‫ ݒ‬has a
constant value for the duration of the segment). We also have
event segments where there exists a single between ‫ݐ‬௜ and ‫ݐ‬௝

(‫ݐ‬௜ ǡ ‫ݐ‬௝ ȁ‫ݐ‬௜  ൏  ‫ݐ‬௝ ) or ‫ݐ‬௜௞ሾκሿ and ‫ݐ‬௜௞ሾκାଵሿ (see Definitions 1 and
2). Values for a segment can be arbitrary (i.e., values for
segments that have durations greater than zero and less than
infinity can be defined using any function or relation). A
trajectory can be created by concatenating segments using
ሾǡ ሻ rule. Therefore, trajectory ߱଴ ‫߱  ל‬ଵ for consecutive
piecewise constant ߱଴ ൌ ‫ݒ‬଴ and ߱ଵ ൌ ‫ݒ‬ଵ segments shown in
Figure 2, ߱଴ has value ‫ݒ‬଴ from ‫ݐ‬଴ until ‫ݐ‬ଵ and ߱ଵ has value ‫ݒ‬ଵ
from ‫ݐ‬ଵ until ‫ݐ‬ଶ . Event trajectories follow the same
concatenation rule. Specifically, every event segment in an
event trajectory has its values at the start of the segment with
no values defined for the rest of its time duration (e.g., ߱଴
has value ‫ݒ‬଴ at ‫ݐ‬଴ with no values until ‫ݐ‬ଵ ).

Output trajectories

All outputs are endogenous and are generated by atomic
models or other non-simulatable components [15] that
belong to atomic models. Strictly speaking, coupled models
cannot generate outputs on their own due to the closure under
coupling principle.
State trajectories

Any state trajectory can become an output trajectory. The
state ܵ of any atomic models is defined as a set of variables.
States of a model can be specified as ‫ ݁ݏ݄ܽ݌‬ൈ ߪ ൈ ‫ݒ‬ଵ ൈ ‫ ڮ‬ൈ
‫ݒ‬௤ ȁߪ ‫ א‬Ըାஶ
଴ . The values for ߪ are computed using ‫ܽݐ‬ሺ‫ݏ‬ሻ.
Together ‫ ݁ݏ݄ܽ݌‬and ߪ define the minimum state set for a
model. They are referred to as the primary states. Phase must

DEVS I/O AND STATE TRAJECTORIES

The inputs and outputs ܺǡ ܻ of any DEVS atomic model ‫ ܯ‬ൌ
‫ܺۃ‬ǡ ܵǡ ܻǡ ߜ௘௫௧ ǡ ߜ௜௡௧ ǡ ߜ௖௢௡௙ ǡ ߣǡ ‫ ۄܽݐ‬and coupled model ܰ ൌ

251

have at least two values in order to define state-based
behavior (i.e., a model must at least be in two distinct states)
[18]. Other variables (i.e., ‫ݒ‬ଵ ൈ ‫ ڮ‬ൈ ‫ݒ‬௤ ) are called secondary
states. There are no restrictions on what secondary variables
may represent, except any variable that is related to time
must be consistent with ߪ as defined in time advance
function ‫ܽݐ‬ሺ‫ݏ‬ሻ. The state variables are commonly analyzed
for run-time simulation verification and model validation.
Every simulation scenario for an atomic model with a finite
number of state transitions must have the same number of
segments as there are state transitions. The simulation
scenario for an input trajectory can have at most the same
number of input segments as there are state transitions. The
same holds for output trajectory. A state transition can be
transitory or non-transitory. Zero logical-time refers to a
simulation cycle (due to either an internal or external
transition function) with ‫ܽݐ‬ሺ‫ݏ‬ሻ ൌ Ͳ. Such segments are
called zero-time segments and conform to Definition 2. In
the context of a logical-time atomic DEVS model, a zero
logical-time segment corresponds to a transitory state
(‫ܽݐ‬ሺ‫ݏ‬ሻ ൌ Ͳ). For any legitimate DEVS model, there may
only be a finite number of consecutive zero logical-time
segments in any trajectory. Furthermore, only a finite
number of non-transient and transient state transitions may
exist for a simulation scenario having a finite duration. It is
also assumed there is no ordering among simultaneous input
and output events that occur at the same time. The order of
contiguous transient states, however, is determined in the
external transition function which can be deterministic or
not. There is ordering among simultaneous output events.
Given simulation cycle periods specified by ‫ܽݐ‬ሺ‫ݏ‬ሻ in a
model's external or internal transition function, a simulation
scenario can be categorized to have either non-transient state
transitions: Every simulation cycle has a duration greater
than zero and less than infinity or mixed non-transient and
transient state transitions: Every simulation cycle can have
zero, finite, or infinite duration.

DEVS-Suite simulator variables

It is sometimes useful to plot time of last event ‫ݐ‬௅ and time
to next event ‫ݐ‬ே . These variables belong to the simulation
protocol used in the DEVS-Suite simulator. These variables
specify time duration for the input, output, and state
segments. For an atomic model, ‫ݐ‬ே is the time for the internal
transition firing and can also be the time instance at which
outputs are computed and sent out. For a coupled model, ‫ݐ‬ே is
the shortest time duration from ‫ݐ‬௅ for one or more atomic
models which have either an internal transition, an input
scheduled to arrive, or both. Time instances belonging to ‫ݐ‬ே
and ‫ݐ‬௅ can have linear or superdense relationships.
MIXED
LINEAR
TRAJECTORIES

AND

SUPERDENSE

TIME

The time axis and spatial axis are distinct. The numerical
time axis has to be mapped to string values, each of which is
spatially displayed using pixels. One approach to display
both linear and superdense time is shown in Figure 3. In this
setting, time axis is composed of linear and superdense time
segments where time instances are labeled as defined above.
Mapping numeric time and value axes to their spatial
counterparts

The duration for linear time segments are defined to be
greater than zero and less than infinity. Time labels for these
segments are unique for any single trajectory. These
numerical numbers map one-to-one to pixels. In contrast, the
duration for every superdense time segment is zero.
Therefore, their time labels are indexed as defined above. A
segment with zero time duration (measured in terms of the
model and simulation time) is defined to have a finite spatial
length which can be in pixels. For example, given the
segment between ‫ݐ‬ଵ ሾͲሿ and ‫ݐ‬ଵ ሾͳሿ with ο‫ ݐ‬ൌ Ͳ (i.e., ‫ݐ‬ଵ ሾͲሿ ‫ع‬
‫ݐ‬ଵ ሾͳሿ) is mapped to ‫ݔ‬ଶ and ‫ݔ‬ଷ with ο‫ ݔ‬ൌ ߯, which must be a
finite number of pixels (see Figure 3). The ‫ݔ‬ଶ and‫ݔ‬ଷ spatial
valuesare strictly ordered (i.e., ‫ݔ‬ଶ ‫ݔ ط‬ଷ ) even though their
corresponding time values are weakly simultaneous. The
spatial length for all superdense time segments for all input,
output, and state variables is assumed to be the same.
Composing linear and superdense time segments as just
described has few shortcomings. Adding or removing
superdense time segments causes charts to expand or shrink.
This is undesirable when the charts are being plotted
dynamically. An alternative approach is shown in Figure 4.
In this setting, the linear and superdense time axes are
separated. This is important as it simplifies visualization of
the charts even though more display space is required.
Another benefit of this approach is that charts can scale.
When there are several series of superdense time segments,
they are separated and therefore simpler to view and
evaluate.

Restrictions

To plot trajectories for input, output, or state variables, they
must be a primitive type. The variables that can be plotted
over a time period can be numbers and strings. Numbers can
be integer, real, or others (e.g., bytes). Variable type String
can also be plotted. In principle, symbols (e.g., ‫ ڃ‬and ۨ) can
be used instead of numbers or strings. If a variable has a
complex type, it must be decomposed to its primitive parts
(numbers and strings) in order to be plotted. If complex data
types can be mapped to symbols, they can be displayed too.

252

Figure 3. Combined linear and superdense time segments with their spatial counterparts.

number in the set and the assigned value to infinity) can
change dramatically. The data value set including the
assigned finite value to infinity can be mapped to spatial axis
rendered linearly in pixels.
CREATING MIXED LINEAR AND SUPERDENSE CHARTS

Several professional grade plotting tools exist for creating a
different kinds of charts including line charts. Among them
are the Business Intelligent and Reporting Tool [1,17],
JFreeChart [6], and d3.js [3]. The Business Intelligence and
Reporting Tool (BIRT) offers a flexible framework for
extending or creating new kinds of charts. It supports
reporting and business intelligence capabilities for rich client
and web applications, especially those based on Java and
J2EE. Main components for BIRT are its design, report,
chart, and script engines. BIRT has report designer for
creating BIRT reports within the Eclipse IDE [4] , Chart
Builder, and Report Viewer. These can be deployed in any
Java application. BIRT supports different kinds of charts
such as Bar, Pie, Line, Scatter, and Gantt charts among
others. One of its major capabilities is its flexible, rich chart
customization wizard. As BIRT is built using the Eclipse
plugin framework, one can extend the BIRT API using
extension points to develop new chart types.

Figure 4. Stacked linear and superdense time segments.

Considering the positive and negative values, they too must
be mapped to spatial representations. Since infinity needs to
be plotted relative to a finite value set, it must be mapped to
a number larger than the largest number in the set. If the
value set is known a-priori, it is relatively simple to assign a
finite value to infinity, which can be visually rendered well
relative to the numbers (or strings) in the set. However, in a
dynamic setting, the largest number cannot be known.
Therefore, the (positive or negative) infinity value can be
defined to be a percentage larger than the largest (positive or
negative) value in the value set. If the range of values is very
large (e.g., ͳି଻ to ͳାଵଽ and ͳ to ͳାଷ ), then the numerical
infinity band (defined as the difference between the largest

Piecewise constant and event charts

The approach taken for implementing the piecewise constant
(shown in Figure 4) and event charts is to extend several of
the BIRT plug-ins [16]. For example, the “data point
definition” extension point from the Design engine is
extended to process data sets and their mapping to pixels and
creating superdense segments. The package for the piecewise
constant is shown in Figure 5. Definitions for infinity bands
and transitory state transitions among many others are

253

input, and output to have positive and negative infinity
values.
The DEVS-Suite Modeling and Simulation tool, supporting
Parallel DEVS modeling formalism, offers basic support for
generating linear time Piecewise constant and Event chart
types at run-time [8]. This simulator was built using the
Model-Façade-View-Control (MFVC) architecture pattern
[14]. The View component receives notification from the
Model and can generate input, output, and state trajectories
in tabular, piecewise constant and event charts. The
piecewise constant (or event) chart consists of piecewise
constant (event) segments. Similar to other classical and
contemporary simulator tools such as Simulink [12] and
Ptolemy II [13], superdense time segments and charts as
defined in this paper are not supported. Given the separation
of concerns in the MFVC, the Timeview of the DEVS-Suite
simulator 2.1.0 is extended to support plotting piecewise
constant and event charts with superdense time segments and
positive and negative infinity bands at run-time. The top
level design of the Timeview is illustrated in Figure 7 [16].
The Piecewise constant and event chart plugins that are
developed in BIRT and BIRT’s chart plug-in are used. The
BIRT scripting is used in the Timeview. It affords
configuring the attributes of the charts (e.g., label font and
color, line type and thickness, tick marks, etc.). The scripting
complements customization using BIRT’s chart wizard.

specified in the PiecewiseConstantImpl class. Other
important capabilities (e.g., switching on and off display of
superdense time segment) for visualization of complex
content are also supported. Considering the rendering of the
charts, Figure 6 illustrates use of the BIRT plug-ins. An
important focus of these plug-ins is to display charts from
arbitrary data sets that are generated dynamically. These and
other classes and packages developed for Piecewise Constant
charts are used for Event charts with appropriate
modifications.

Figure 5. Classes specifying Piecewise Constant chart.

Figure 7. Illustration of integration of BIRT Piecewise
Constant and Event Charts with DEVS-Suite’s Timeview.
Figure 6. Classes for rendering Piecewise Constant chart.
Exemplar linear and superdense time charts

Integration with the DEVS-Suite simulator

As noted above, charts can have both linear with superdense
time segments and infinity bands as shown in Figure 8. The
Phase piecewise constant chart is generated from a model
that undergoes transitory state transitions at time instances
ͳͲǤͲ and ʹͲǤͲ. In the linear time trajectory part, only the
phase ‫ ݕ̴݀݊݁ݏ‬is displayed. In contrast, in the superdense
time trajectory, the phase ‫ ݁ݒ݅ݏݏܽ݌‬can also be viewed. At
time instance ʹͲǤͲ, there are two contiguous transitory state

The DEVS-Suite simulator’s charting engine has major
limitations common to many classical and contemporary
tools. Such simulation tools do not directly support the
concept for superdense time, which is necessary for
segments that have zero time duration.
The DEVS-Suite simulator supports generating run-time
plots that have zero-time segments while allowing state,

254

transitions followed by a non-transitory state transition
(phase ‫ ݐݑ݋̴݀݊݁ݏ‬with ߪ ൌ ͳͲǤͲሻ. The sigma piecewise
constant chart shows the positive infinity band and at time
instance ͵ͲǤͲ, phase is ‫ ݁ݒ݅ݏݏܽ݌‬with ߪ ൌ λ. In Figure 9,
event charts with simple and complex data types are shown.
All the elements in the piecewise constant (and event) chart
can be customized. The switch for plotting superdense time
segments is set to off and therefore events generated at time
instances ͳͲሾͲሿ, ʹͲሾͲሿ and ʹͲሾʹሿ corresponding to the charts
in Figure 8 are not shown. To avoid cluttering of the charts,
the ʹͲሾͲሿ, ʹͲሾͳሿ, and ʹͲሾʹሿ time instance labels are
displayed as ʹͲሾͲሿ, ሾͳሿ, and ሾʹሿ.
The axes in the plots have default scales, but can be
initialized before the simulator starts, or resized when the
simulator is paused. The inputs, states, and outputs of the
model components can be individually selected and tracked
dynamically (see Figure 10). Furthermore, all plots for a
component can be stacked as shown. This is useful to
understand complex timings for state transitions, I/O events,
separately in linear and superdense time to be aligned.
Rudimentary linear time trajectories was implemented in
Java AWT in DEVSJAVA [20], the predecessor to the
DEVS-Suite simulator.

Figure 9. Event charts with linear time segments.

Figure 10. DEVS-Suite’s Component tracking for Tracking
Log and TimeView.
RELATED WORKS

Data visualization is crucial in many science and engineering
areas. Techniques including data normalization, filtering,
zooming, etc. are important for creating useful visual data
representations such as 2D histograms [1]. Tools supporting

Figure 8. Piecewise Constant Charts with linear and
superdense time segments.

255

REFERENCES

time series that are commonly used in analysis applications
do not support creating the kinds of charts developed in this
research. They are intended for understanding patterns,
trends, etc. In contrast to these kinds of charts, for designing
and experimenting with dynamical simulation models for
concurrent systems, such charts are insufficient. For this
reason tools such as D3 [3]. Other tools such as Graphite [7]
are developed for real-time data storage with support for data
visualization. Tools such as BIRT support 3D line charts
where one axis can be used for time variable, one for
continuous variable and one for discrete variable.
Alternatively, two axes can be used for linear and superdense
time with the remaining axis for a variable. One crucial
difference is that 2D charts, unlike 3D charts, can be stacked
and evaluated together. The concept of expanding a portion
of the linear time axis with high resolution relates to our
work. Users can zoom on any finite time segment of a 2D
time series with high precision [9]. The spatial representation
of a plot’s time axis can be scaled uniformly unlike the
superdense time axis with variable values that may have
positive and negative infinity bands.

1.
2.
3.
4.
5.
6.
7.
8.

9.
10.

CONCLUSIONS

In this paper, we have described superdense time which is
central for simulation models such as Parallel DEVS that
exhibit concurrency. It is important to create trajectories that
have superdense time segments. In this paper, we have
detailed this concept with a formulation of it where visual
representation of superdense time is defined based on its
mathematical specification. We developed a formulation for
developing complementary linear and superdense time
piecewise constant and event chart types with positive and
negative infinity bands. These can be used for visualizing
input, output, and state trajectories for DEVS atomic models.
They are implemented using the BIRT framework. As BIRT
plug-ins, they can be used in RCP and Java applications. As
part of the BIRT tool, the piecewise constant and event charts
defined as ‫ ݒ‬ൈ ‫݁݉݅ݐ‬data sets (for example stored as CSV
files) can be plotted as static charts in the BIRT Design
Report. These plug-ins are used in the Timeview module of
the DEVS-Suite simulator. Time series are dynamically
plotted alongside I/O animation and tabular log files. These
piecewise constant and event trajectories can be customized
which is crucial when simulation data to be visualized is
complex, numerous, and may not necessarily be known apriori. Future work includes supporting other kinds of chart
types such as continuous piecewise and ൛‫ݒ‬ଵ ǡ ‫ ڮ‬ǡ ‫ݒ‬௤ ൟ ൈ ‫݁݉݅ݐ‬
and integrating the BIRT Design Report with the DEVSSuite simulator. Another area of research is to offer greater
degree of customization resulting in greater data modality
and fewer data trajectories.

11.
12.
13.
14.

15.

16.

17.
18.
19.
20.

ACKNOWLEGEMENT

We are grateful to the anonymous referees for their helpful
reviews.

256

BIRT 4.4, (2014), http://www.eclipse.org/birt/.
Chow, A.C., Zeigler, B.P., (1994), “Parallel DEVS: a
parallel, hierarchical, modular modeling formalism,” In
Proceedings of Winter Simulation Conference, 1994.
Data Driven Documents, (2013), https://ds3js.org.
D'Anjou, J., Fairbrother, S., Kehn, D., Kellerman, J., &
McCarthy, P., (2005), The Java Developer's Guide to
ECLIPSE (2nd ed.), Addison Wesley.
DEVS-Suite simulator 2.1.0, (2009), http://devssuitesim.sourceforge.net.
JFreeChart, (2013), http://www.jfree.org/index.html.
Graphite, (2011), http://graphite.readthedocs.org.
Kim, S., Sarjoughian, H. S., Elamvazuthi, V., (2009),
“DEVS-suite: A Simulator Supporting Visual
Experimentation Design and Behavior Monitoring,” In
Proceedings of 2009 Spring Simulation Multiconference, San Diego, CA.
Kincaid, R., (2010), “SignalLens: Focus+Context
Applied to Electronic Time Series,” IEEE Transactions
on Visualization and Computer Graphics, vol.16, no.6.
Lee, E.A., et al., (2014), System Design, Modeling, and
Simulation using Ptolemy II, http://ptolemy.org.
Manna, Z., Pnueli, A., (1993), The Temporal Logic of
Reactive and Concurrent Systems, Springer, Berlin.
Mathworks,
Simulink,
(2011),
http://www.mathworks.com/products/simulink.
PtolemyII, (2010), http://ptolemy.eecs.berkeley.edu/.
Sarjoughian, H.S., Singh, R., (2003), “Building
simulation modeling environments using systems theory
and software architecture principles,” In Proceedings of
Advanced Simulation Technology Conference, SCS,
Washington DC.
H. S. Sarjoughian and V. Elamvazhuthi, “CoSMoS: a
visual environment for component-based modeling,
experimental design, and simulation,” In Proceedings of
the 2nd international conference on simulation tools and
techniques, 2009.
Sundaramoorthi, S., (2015), “Eclipse BIRT Plug-ins for
Dynamic Piecewise Constant and Event Time Series,”
Master’s Thesis, School of Computing, Informatics and
Decision System Engineering, Arizona State University,
Tempe, AZ.
Weatherby, J., Bondur, T., & Chatalbasheva, I., (2011),
Integrating and Extending BIRT (3rd ed.).
Wymore, A.W., (1993), Model-Based Systems
Engineering, CRC Press, Boca Raton.
Zeigler, B.P., (1976), Theory of Modelling and
Simulation, Wiley Interscience, New York.
Zeigler, B.P., Sarjoughian, H.S., Au, V., (1997),
“Object-oriented DEVS: object behavior specification,”
In Proceedings of Enabling Technology for Simulation
Science, Orlando, FL.

DEVS MODELING AND SIMULATION: A NEW LAYER OF MIDDLEWARE’
Bernard P. Zeigler
Hessam S. Sarjoughian
Sunwoo Park
James J. Nutaro*
Jong S. Lee
Young K. Cho
Arizona Center for Integrative Modeling and Simulation
Electrical & Computer Engineering Department
University of Arizona, Tucson, AZ 85721-0104, USA
www.acims.arizona.edu

* and Raytheon Missile Systems, Tucson, AZ
services and functions that benefit many applications in a
networked environment Middleware represents an
expansion of the networking infrastructure to subsume
functions needed by many applications, improve targeted
characteristics
of
the
applications,
enhance
interoperability among applications and reduce the
complexity encountered by application developers and
end users. Middleware typically includes a set of
components (such as resources and services) that can be
utilized by applications either individually or in various
subsets. Middleware gets its name as software that lies
above the transport layer (e.g., TCP), but below the
application environment. It may be embedded within
operating systems, or may be separate and the boundary
may change with time. Indeed, as networking software
infrastructure expands and specializes, the term
middleware may begin to cover a number of distinct
layers of services, and distinctions such upper
middleware and lower middleware may become
informative.
This paper introduces a concept of a particular
advanced middleware that adheres to the definition just
given. As illustrated in Figure 1, DEVS middleware
would provide a set of services for constructing discrete
event models and executing them in simulation or in realtime. Based on the DEVS (Discrete Event System
Specification) formalism, [ 2 ] it would provide a reusable,
expandable set of services and functions that benefit
distributed simulation in a networked environment. Thus,
DEVS Middleware represents an expansion of the

Abstract
This paper discusses a concept of DEVS middleware
that would provide a set of services for constructing
discrete event models and executing them in simulation
or in real-time. Although generic middleware such as
CORBA and MPI supports simulation and simulationoriented middleware such as High Level Architecture
Runtime Infrastructure exists, there are many issues that
either not addressed at all or have been inadequately
addressed in existing systems. DEVS Middleware would
improve modeling and simulations due to the beneficial
formal properties of DEVS, enhance interoperability
among components adhering to the DEVS protocol, and
reduce programming complexity by hiding lower level
simulation and execution details. The DEVS Middleware
concept includes components for model construction and
for mapping models onto simulators or real-time
execution engines to support the novel concept of
“distributed programming by modeling.” This paper
discusses several sub-layers that have been identified
within the overall DEVS middleware concept, existing
work that can be brought to bear on developing these
layers and the integration needed to create a widely
accepted standard.

1.

INTRODUCTION

As defined by a National Science Foundation
committee, [ I ] middleware is reusable, expandable set of

’

This research has been supported by “A Simulation Platform for Experimentation and Evaluation of
Distributed-Computing Systems,” a NSF Next Generation Software Initiative Grant, and “DEVS as a Formal
Modeling Framework for Scalable Enterprise Design - Case Study: Model-Driven Data Management,” a Phase I
grant of the NSF Scalable Enterprise Initiative.

0-7695-1528-2102 $17.00 0 2002 IEEE

22

Generic services for modeling and sirnulation in a
distributed networked setting have been in existence for
several years. The IEEE standard for distributed
interactive simulation (DIS) supports connection of
simulators using specific formats for message exchanges
and enables training exercises involving geographically
dispersed and multi-service real and simulated assets. The
success of DIS led to the establishment of the High Level
Architecture (HLA) standard mandated by the United
States Department of Defense for all its contractors and
agencies [3]. HLA, through its code implementation, the
RTI (runtime infrastructure) provides a variety of services
and a generic object-based message exchange format to
make it easier to interoperate existing simulators using
distributed simulation. However, HLA oriented as it is
toward interoperating existing assets, does not provide a
generic means to develop new models and simulations.
Moreover, non-DoD applications of distributed modeling
and simulation, such as in distributed business enterprises
and e-commerce, are becoming increasingly important as
system complexity increases and lead-times diminish.
Developers of such applications may find the constraints
placed by HLA-- however critical to DoD simulations -overly burdensome in their own transaction-based
environments. Moreover, some of the difficulties faced in
creating
distributed
modeling
and
simulation
environments are not addressed by the HLA prescriptions
and run-time infrastructure. Thus, there is a need for
further middleware development for modeling and
simulation [4]. Besides construction of models for
distributed simulation there is a need to address
fundamental issues such as model credibility (e.g.,
validation, verification and model family consistency)
and interoperation (e.g., repositories, reuse of
components, and resolution matching) have received a lot
less attention.

infrastructure to subsume functions needed to easily
construct distributed simulations. It improves simulations
due to the beneficial formal properties of DEVS,
enhances interoperability among components adhering to
the DEVS protocol, and reduces programming
complexity by hiding lower level middleware details.
Providing the right level of abstraction for modeling and
simulation applications, it improves the usability to end
users. DEVS Middleware includes components for
model construction and for mapping models onto
simulators or real-time execution engines. Such mappings
required object management services such as container
classes that work in both distributed and non-distributed
incarnations. In this way, DEVS Middleware would
support the novel concept of “distributed programming by
modeling,” i.e., constructing distributed systems by first
constructing models, testing them via simulation, and
then changing the underlying environment from a
simulation to a distributed real-time execution
environment.

applications (domain model collections)
model construction services

connection middleware services (e.g. CORBA)
~

~~~

network, processing,
and storage infrastructure

Figure 1 DEVS Upper Middleware

2.

3.

DEVS AS A BASIS FOR M&S MIDDLEWARE

Development of DEVS is intended to meet the need
for advanced middleware to support modeling and
simulation. DEVS is an increasingly accepted paradigm
for understanding and supporting the activities of
modeling and simulation. DEVS is a sound formal
modeling and simulation framework based on concepts
derived from dynamic systems theory.
It is a
mathematical formalism with well-defined concepts of
coupling of components, hierarchical,
modular
construction, support for discrete event approximation of
continuous systems and an object-oriented substrate
supporting repository reuse. Perhaps the most basic
concept is that of mathematical systems theory. DEVS
theory provides a fundamental, rigorous mathematical

THE NEED FOR MODELING AND
SIMULATION MIDDLEWARE

As suggested by Figure 1, DEVS is ‘‘upper
middleware” in that it sits upon lower layers of network
middleware that provide connection and messaging
services. On example of such a layer is CORBA
(Common Object Request Broker Architecture), a
generic, widely adopted standard which has significant
vendor support for e-commerce and real-time
applications. MPI (Message Passing Interface) is a
common framework employed in high performance
parallel and distributed systems for science applications.

23

models consist of components, which are atomic models
or previously defined coupled models. Thus DEVS
supports hierarchical
modular construction
and
composition methodology
[81. This
bottom-up
methodology keeps incremental complexity bounded and
permits stage-wise verification since each coupled model
“build” can be independently tested.
In addition to their components, coupled models
specify the coupling between output ports and input ports.
As illustrated in Figure 3, such coupling specifications
provide paths for objects generated by output events to
appear at the input ports of other models as external
events.
DEVS is capable of expressing any discrete event
dynamic system and of representing a wide class of other
dynamic systems. Universality for discrete event systems
is defined as the ability to represent the behavior of any
discrete event model where “represent” and “behavior”
are appropriately defined. Concerning other dynamic
system classes, DEVS can exactly simulate discrete time
systems such as cellular automata and approximate, as
closely as desired, differential equation systems. This
theory is presented in [ 2 ] .
As part of the NSF Next Generation Software
initiative, a DEVS-based platform is under development
that aims to provide generic support for developing
models of embedded software systems, evaluating their
performance via simulation and easing the transition from
simulation to actual execution. Since real-time
considerations enter into many embedded systems, a
central component of this research is the design of a RealTime Distributed Execution Environment (RTDE). Based
on results of previous research, the major objective is to
establish a framework in which distributed real-time
systems can be designed through DEVS-based modeling
and simulation and then be migrated with minimal
additional effort to be executed in the RTDE. The design
of the RTDE was described in [9]. T o date, a prototype
has been developed which supports distributed simulation
of DEVS models over Visibroker CORBA middleware as
well as execution of real-time DEVS models over
Visibroker with calls to ACEKAO real-time CORBA
services, such as time servers and event-channel. [IO].
Figure 4 illustrates the mapping of a DEVS coupled
model into a distributed simulation and execution
environment as abstracted from the implementation of
DEVSKORBA RDTE. Components of the coupled
model are assigned to agents called simulators, while the
coupled model itself is mapped to a coordinator agent.
The coordinator downloads relevant coupling information
to the simulators enabling peer-to-peer data exchange.
Simulation proceeds via exchange of control and
synchronization messages between the coordinator and

formalism for representing dynamical systems. There are
two main, and orthogonal, aspects to the theory:

levels of system specification - these are the levels at
which we can describe how systems behave and the
mechanisms that make them work the way they do.
systems specification formalisms - these are the types
of modeling styles, such continuous or discrete, that
modelers can use to build system models.
Although the theory is quite intuitive, it does present
an abstract way of thinking about the world with
independence of the simulation mechanisms, underlying
hardware and middleware.
Discrete event models can be distinguished along at
least two dimensions from traditional dynamic system
models - how they treat passage of time and how they
treat coordination of component processes. In digital
simulation, traditional models such as differential
equations and cellular automata march through their
states with fixed time steps while discrete event models
move from event to event. Furthermore, event-based
approaches allow more realistic representation of loosely
coordinated semi-autonomous processes, while traditional
approaches tend to impose strict global coordination on
such components. Event-based simulation is inherently
efficient since it concentrates processing attention on
events - significant changes in states that are relatively
few in space and time - rather than continually processing
every component at every time step. Discrete event
concepts are also the basis for advanced distributed
simulation environments that employ multiple computers
exchanging data and synchronization signals through
efficient interest-based message passing [ 5 , 61.

3.1

DEVS Modeling, Simulation and Execution

The DEVS formalism [7] provides a way of
expressing discrete event models and a basis for an open
distributed simulation environment[2]. DEVS is formally
defined as a mathematical structure with sets and
functions whose operation is illustrated in Figure 2.
Inputs events arriving in time are handled somewhat in
the manner of interrupts by the modeler-specified external
transition function and result in an immediate change in
state This function determines the state transition and
how long to stay in the new state. At the end of this
duration, the model outputs an event determined by the
output function and transits to a new state determined by
the internal transition function.
DEVS developers define two kinds of models, atomic
and coupled. Atomic models define the characteristic sets
and functions directly using the host language. Coupled

24

the simulators as dictated by the DEVS simulation
protocol. More details on this protocol are available in

infrastructure, e.g., replacing a single server with a highperformance server farm. Current and foreseeable trends
are to employ object-oriented technology to enable such
scalability attributes. In these terms, the scalability
problem can be stated as designing a system with the
appropriate interface definitions that allow the servant
implementations behind the interfaces to be upgraded
from say single objects, to multiple coordinated objects or
to objects of more capable classes. Abstraction,
modularity, and layering underlie such interface design
concepts [ 121.

VI.
4.

DESIGN OF D E W MIDDLEWARE

The layering architecture required to realize a robust
and scalable DEVS middleware is specified in Figure 5.
Difficulties in dealing with large-scale software systems
are well documented in a recent NRC report. [ l l ] .
Techniques that work for small software systems fail
markedly when the scale is increased by one million fold.
Scalability is an attribute of the design of a system's
architecture that pertains to the behavior and performance
of the system as the size, complexity, and
To be
interdependence of its elements increases.
upwardly scalable, a system must be able to assure
invariance in its both the functionality and the quality of
the services it provides as the number of its users
increases indefinitely. To scale by a million, an
application's storage and processing capacity would have
to be able to grow by a factor of one million just by
adding more resources [l 13. This implies that as system
expands or performance demands increase, the underlying
architecture supports the ability to reimplement the same
functionality
with more powerful
or capable

4.1 Distributed Container Classes Layer
The DEVS distributed simulation protocol requires the
services of the Distributed Container Classes layer
illustrated in Figure 5. More information on the
distributed containers concept is available in. [ 131. Our
attempts to implement distributed heterogeneous
container classes to support DEVSKORBA ran into
limitations in the basic CORBA design [ 141. Learning
from these attempts, we plan to extend the concept of
interface definition by developing an Object Interface
Description Language (OIDL).

Y
b

Figure 2 Logic and Dynamics of a Basic DEW Model

25

Coupling

4;/-3

-.-

R

.

I
B

I

Figure 3 Coupled DEW Model

I

I

Simulator

Genr

I

dara messages

"

Simulator

lo,

Pmc

""1

-

1

Simulator

miwd

+

Transd

l'iniskt

Coordinator

Figure 4 Mapping of D E W models onto distributed simulators
This language will be an extension of the OMG
(Object Management Group) IDL for CORBA that
provides advanced object oriented design (OOD) features.

The need for such an extension is explained by the fact
that the OMG IDL was designed to support the most
widely used procedural languages such as C and COBOL

26

the meaningfulness of what federates “say” to each other
[ 1.51. Technical
interoperability is focused at
interoperation among different simulators, live systems,
and simulation support tools.
This interoperability
among federates and federations is dependent upon the
Run Time Infrastructure (RTI). Unfortunately, the
definition of the RTI is not part of the HLA specifications
([ 161. Furthermore, while HLA standardizes FOM for
single
federation
execution,
inter-federation
interoperability (i.e., how two or more federations can be
composed to form a “super-federation”) is unspecified.
Substantive interoperability implies interdependency
among the internals of federates (models) and not merely
their data types and “exchange contracts” as captured in
HLA FOM and SOM [4].
The philosophy underlying the DEVS Middleware
concept differs radically from that underlying HLA. The
latter’s support for interoperability only at the technical
level can be traced to its goal of enabling interoperability
of simulators not of models. The distinction is critical
because, if viewed only as simulators, federates must be
able to interoperate independently of their underlying
modeling paradigms no matter how incongruent these
might be. While interoperability a the modeling level can
be conceptually simple to describe, a fundamental theory
of modeling and simulation is needed to cast it into a
formal setting and to realize it in well-founded software.
DEVS Middleware is aimed at this problem.

and therefore could not take advantage of such objectoriented languages as C++ and Java. Our design of the
OIDL will focus on enabling standardized and scalable
implementations of DEVS models and their simulators
developed in a fully-capable object-oriented languages. In
greater detail, OIDL will need to:
Support f o r both interface and implementation
inheritance. This will allow users to build 00
implementations of hierarchical information that is
defined in OIDL files. Specifically, a derived class only
needs to implement its own interfaces. In contrast, in
CORBA, developers must implement interfaces defined
in all super classes.
Support Inter-IDL Operability Mechanism, OIDL
will provide an Inter-IDL Operability Mechanism
(IIOM). This will allow building “multiple IDL-aware”
implementations (e.g., IDL gateway, IDL converter and
IDL wrapper) to support interoperability of OIDL
applications with (legacy) programs based on the existing
IDL.
Support operation overloading. Since IDL is designed
for both procedural languages, it cannot support operation
overloading. OIDL will be designed for sole use with true
00 languages.
Support unified conversion. OIDL will support
unified class conversion, which allows easily performing
data conversion between primitive data type (e.g., int,
float, and double in C++ and Java) and class data type
(e.g., Object in Java.) This unified class conversion
provides very powerful capabilities for OOD, because
with i t a method signature can accept all primitive data
types as well as all class data types.
Application of agent technology to automate the setup and management of distributed containers for loadbalancing and fault-tolerance.

4.2.2 An Outline of the DEVS Simulation and
Execution Protocol Layer
DEVS can be used to characterize fundamental
properties of systems that participate in distributed
simulations. This provides a strong theoretical basis for
constructing a toolkit that is both sufficient and minimal
for the purposes of supporting small and large-scale
distributed simulations. A minimal toolset is desirable for
portability. Ideally, the toolkit should be specified in
such a way that simulations can operate in any type of
distributed environment using exactly the same API. This
promotes reuse by reducing migration costs (e.g. from
DIS to HLA). It promotes interoperability by giving the
federate access to the fundamental distributed services
provided by, say, the HLA. Direct access to native
middleware services is a key aspect of interoperability.
Advanced middleware tools such as the HLA provide
distributed algorithms that have federation wide impact
but whose mechanism is invisible to the individual
federates. From the HLA world, the time management
services and synchronization points are examples of such
a service. In both cases, an individual federate is
responsible for providing partial information needed to do
a global computation (in this case, to determine if some
global predicate holds). The result of the computation is

4.2 DEVS Simulation and Execution Protocol Layer
This layer implements the mapping of coupled models
into distributed simulation and execution discussed earlier
(see Figure 1). The HLA service categories outlined in
Table 1 provide a good basis for discussing the DEVS
model mapping.

4.2.1 If HLA Exists, Why is DEVS Middleware Still
Needed?
“The High Level Architecture is an architecture for
reuse and interoperation of simulations” [ 3 ] .However, it
should be noted that there are major limitations to the
interoperability concept underlying the HLA standard.
Very briefly, HLA supports “technical” interoperability,
i.e., interoperability at the “syntactic” level enabling
federates to converse with a common grammar. It does
not
support “substantive” interoperability,
i.e.,
constructing simulations at the “semantic” level (assuring

27

delivered to individual federates, but the mechanism is
hidden. This implies that middleware gateways (e.g.
DIS/HLA gates and HLA/HLA bridges) can not
participate in global computations that are computed
within the middleware.

Table 1 HLA Management Services

1 Distributed Modeling

HLNOMT

DEVS

SES

SOM (modeling is
partially supported)

Atomic model

NIA

FOM (modeling is
partially supported)

Coupled model
(execution management
is partially supported)
Supports interaction

Model decomposition
is supported

Object class
instantiation, messaging
through port and
coupling is supported
Can support
ownership in its usual
connotations

N/A

Predictive Contracts
(partially supported)
Centralized time
management; messaging
follows outpudinput port
couplings
not supported

NIA

Services

operations
distributed
models
Declaration management

r
Object management

PublishfSubscribe
UDdates and interaction
Object class instantiation,
sendheceive interactions
are supported

NIA

I
Ownership management

Data distribution management
Time management

One or more federates
may be declared to be
responsible for computing
a set of SOM attributes
(partially supported)
Routing Space
(partially supported)
Centralized time
management; timestamped order and receive
order message passing
not supported

r
Design Spaces

I

NIA

NIA

Alternative
decompositions,
specialization
+ Pruning and
Synthesis

Table 2: HLMOMT & DEVS/SES Modeling and Simulation in Spotlight

28

However, one comment can be made. The commit
local time service provides a Lamport-like clock for
ordering sequences of zero time advance interactions and
provides a means for meeting both functional and local
causality constraints required for correct distributed
simulation of DEVS models. This corrects a major flaw
in the HLA specification that was identified in [18, 191.
The field of parallel discrete event simulation has
explored a large variety of conservative and optimistic
algorithms [20]. The toolkit associated with the DEVS
simulation protocol layer will support experimenting with
alternatives for a given DEVS simulation and selecting
the most efficient [21].

As discussed above, HLA does not support
hierarchical federations, and in particular, HLA
synchronization points and time management services
cannot operate across an HLNHLA bridge.
Consequently, when these services are required, they
must be implemented directly by the federates. Given the
complexity of the algorithms and the global extent of
failure of a single node, this is a daunting task.
The DEVS middleware kernel presents a standard API
(application program interface) that is layered over
several modules that provide basic services. The modules
are independent of one another in so far as is possible.
This allows the developer to select subsets of services to
meet specific needs. A full DEVS kernel will implement
six sets of services. These are listed in Table 3, along
with the specific services in each set. Space does not
permit detailed discussion of these services and their
relation to those of HLA.

Service
group
Federate
management
Messaging

UID
Interest and
data
distribution
management

Time

Synchronizati
on

Specific services
join -Join a distributed federation
resign - Resign from a previously joined federation
send - Send a typed, parameterized message. Messages may be associated with a timestamp and/or
ID. Sends are broadcasted to all federates.
receive - Receive a typed, parameterized message and its associated timestamp and ID, if defined.
request - Request a typed, parameterized message with an associated ID. The reply will come via
the receive service.
reply -This method is invoked in response to a request by a remote federate, indicating that a
message has been requested.
create UID - Create a globally unique identifier.
destroy UID - Destroy a globally unique identifier (i.e. free is for reuse)
subscribe -Express interest in a messages with a particular type and parameter set.
publish - Indicate that the federate will be sending messages with a particular type & parameter set.
unpublish - Revoke a publication
unsubscribe - Revoke a subscription
assign quantum size - for DEVS predictive integrator
change quantum size - also reset integrators for new quantum [ 171
enable time management - Participate in a time management scheme. Supported types are scaled
real time and algorithmic (i.e. logical time). The latter is suitable for simulation of arbitrary DEVS
models.
disable time management - Stop participating in a time management scheme.
request local time - Request that the local clock for this federate be advanced to a particular time.
commit local time - Commit this federate to a particular local time.
set barrier - Create a labeled barrier and its synchronization set. The synchronization set includes
all federates joined to the federation.
reach barrier - Block until all federates in the synchronization set have reached the labeled barrier.
Table 3 Services required to implement the DEVS Simulation Protocol
relationship between the modeling layer and the
simulation layer below. A complete set of interfaces for
characterizing the correct simulation and real time
execution of DEVS mode]s was defined in and validated
in Java implementations [14]. They allow transitioning a
DEVS model from logical simulation to distributed and

4.3 DEVS Modeline
- Laver
"
Several major issues need to be addressed at this layer,
standardization
programming language

in

Of

format

and

a
the

29

real-time execution with minimal modification in support
of “distributed programming by modeling.”
Much
remains to be done before a standard of this type is
developed and accepted broadly among the simulation
community.
The current state-of-practice in modeling and
simulation is rife with problems of data heterogeneity due
to lack of accepted standards and multiplicity of divergent
approaches to tools [22]. Therefore, a practical approach
to
scalability
also requires consideration of
interoperability as well. Scalability - invariance of
and
function under increased user loading
interoperability, the ability to inter-operate disparate
applications, are related but distinct concepts. For
example, the High Level Architecture (HLA) is intended
to support interoperable simulations by providing a
common data exchange format to which they must
adhere. As a second example, The Extensible Markup
Language (XML) is the universal format for structured
documents
and
data
on
the
Web
(http:Nwww.w3.org/XML). While interoperability and
scalability are both supported by common interface
definitions, the ability to make applications work together
is necessary, but not sufficient, to make an indefinitely
large number work together. Due to the co-existence of
models at multiple levels of abstraction in the layered
architecture, the problem of interoperability is not solved
merely with common data formats.

study how various varying software components can be
mapped to competing heterogeneous hardware nodes and
links [231.
Distributed simulation involves simulated entities that
could generate enormous amounts of exchanged data as
the number of simulated entities grows with potential for
all-to-all interactions. A wide variety of mechanisms are
available for filtering and routing the flow of data from
producers to interested consumers. This enhances
scalability by reducing the bandwidth requirements
placed by distributed simulation on the underlying
network. Recently studies have addressed the scalability
of schemes in terms of the growth of the local
computation overhead with the entire model size and the
tradeoff between message traffic reduction and associated
local computation overhead [5, 24, 251. Quantizationbased filtering to enhance scalability in distributed
simulation has demonstrated very significant reductions
in network data loading in association with multiplexing
methods [ 17, 251.

-

I

I

1-1
1

5.

The development of DEVS middleware would provide
a set of services for constructing discrete event models
and executing them in simulation or in real-time. DEVS
Middleware improves modeling and simulations due to
the beneficial formal properties of DEVS, enhances
interoperability among components adhering to the
DEVS protocol, and reduces programming complexity by
hiding lower level simulation and execution details. The
DEVS Middleware concept includes components for
model construction and for mapping models onto

1

I

D E W Models

simulators o r real-time execution engines. In this way, it

would support the novel concept of “distributed
programming by modeling,” i.e., constructing distributed
systems by first constructing models, testing them via
simulation, and then changing the underlying
environment from a simulation to a distributed real-time
execution environment. Several sub-layers have been
identified within the overall DEVS middleware concept.
We are developing distributed container classes to
provide object messaging and distribution services. At the
next level, a tool kit to support correct and efficient
distributed discrete event simulation algorithms
implementing the DEVS protocol is under development.
Scalability and interoperability, especially among models
posed at different levels of resolution, are prime issues
that need to be addressed in order for large scale
simulations to be supported. Research has yielded
efficient mechanisms to reduce data load based on
quantization concepts, and these approaches need to be.
integrated into the modeling layer. Efforts to develop a
standard for expressing DEVS models and the DEVS
simulation
protocol
are
underway

Simulation Protocol

Distributed Container Classes

1

CONCLUSIONS

Connection Middleware

1

Network, Processing and Storage Infrastructure
Figure 5 Layering for DEW Middleware
Simulation models are often deliberately constructed
at various resolution levels to enable reasonable execution
times, but also to reduce network data loading. In this
situation, interoperability requires uniform application of
abstraction mappings to preserve the semantic coherence
among components of a system. Intuitive, yet powerful,
concepts of abstraction and homomorphism 121 are
available
to
address
such
abstraction-induced
interoperability requirements
It is also necessary to

30

(DEVSTD@yahoogroups.com). However, much remains
to be done before a standard of this type is accepted
broadlv among the simulation community.

[ 121Messerschmitt,

D.G., Understanding Networked
Applications: A First Course. 2000, San Fransisco,
CA: Morgan-Kaufmann.
[ 131Zeigler, B.P., Objects and Systems:
Principled
Design
with
C++/Java
Implementation.
Undergraduate Texts in Computer Science, ed. D.
Gries. 1997, New York, NY: Springer-Verlag.
[ 141Park, S.W., B.P. Zeigler, and H. Sarjoughian.
Interfaces for Scalable DEVS and Distributed
Container
Object Behavior Specifications. in
Systems, Man and Cybernetics. 2001. Tucson, AZ.
[15]Dahmann, J. and C. Terrel. HLA and Beyond:
Interoperability. in
Simulation Interoperability
Workshop. 1999. Orlando, FL.
[ 161Myjak, M.D. RTI Interoperability Study Group Final
Report. in Simulation Interoperability Workshop.
1999. Orlando, FL: IEEE.
[ 171Lee,
J.S. and B.P. Zeigler, Space-based
Communication Data Management. Jnl. Par. Dist.
Comp., 200 1.
[ 181Zeigler, B.P., et al. Implementation of the DEVS
Formalism over the HLA/RTI: Problems and
Solutions. in SIW. 1999. Orlando, FL.
[19]Lake, T., et al. DEVS Simulation and HLA
Lookahead. in SIW. 2000. Orlando, FL.
[20] Fujimoto, R.M., Parallel and Distributed Simulation
Systems. 1999: Wiley-Interscience.
[21] Nutaro, J., Time Management and Interoperability in
Distributed Discrete Event Simulation, in ECE. 2000,
University of Arizona.
[22] Neal, R., Modeling & Simulation Roadmap 24 July :
Section 4: Enterprise Modeling & Simulation, . 2000:
Integrated Manufacturing Technology Initiative:
http://www.imti21 .org.
[23] Hild, D.R., Discrete Event System Specification
(DEVS) Distributed Object Computing (DOC)
Modeling And Simulation, in ECE. 2000, University
of Arizona: Tucson.
[24] Zeigler, B.P., et al. Predictive Contract Methodology
and Federation Performance. in SIW. 1999. Orlando,

Y

REFERENCES
NSF, CISE Advisory Committee, Subcommittee on
the Middleware InfrastructureWhite paper on an NSF
ANIR Middleware Initiative, . 2001, National
Science Foundation.
Zeigler, B.P., T.G. Kim, and H. Praehofer, Theory of
Modeling and Simulation. 2 ed. 2000, New York,
NY: Academic Press. 510.
Dahmann, J.S., F. Kuhl, and R. Weatherly, Standards
for Simulation: As Simple As Possible But Not
Simpler The High Level Architecture For Simulation.
Simulation, 1990.7(6): p. 378-387.
Sarjoughian, H. and B.P. Zeigler, DEVS and HLA:
Complimentary Paradigms for M&S? Transactions
of the SCS, 2000.
Zeigler, B.P., et al. Bandwidth Utilization/Fidelity
Tradeoffs in Predictive Filtering. in SIW. 1999.
Orlando, FL.
Zeigler, B.P., et al., The DEVS/HLA Distributed
Simulation Environment And Its Support for
Predictive Filtering, . 1998, DARPA Contract
N6133997K-0007: ECE Dept., UA, Tucson, AZ.
Zeigler, B.P., Theory of Modelling and Sirnulation.
1976, New York: John Wiley (Under Revision for
2nd Edition 1998).
Zeigler, B.P. and H. Sarjoughian. Support for
Hierarchical Modular Component-based Model
Construction in DEVS/HLA. in SIW. 1999. Orlando,
FL.
Cho, Y.K., et al. Design Considerations for
Distributed Real-Time DEVS. in AIS 2000. 2000.
. _
'I'ucson, AL.
[10]Cho, Y.K., B.P. Zeigler, and H. Sarjoughian. Design
and Implementation of
Distributed Real-Time
DEVSKORBA. in Systems, Man and Cybernetics.
200 I . Tucson, AZ.
[ 1 I ] NRC, Making IT Better. 2000, Washington, DC:
National Academy Press.

FL.
[25] Zeigler, B.P., et al., Quantization-based Filtering in
Distributed Discrete Event Simulation. Jnl. Par. Dist.
Comp., 2001.

31

A Multi-Formalism Modeling Composability Framework: Agent and
Discrete-Event Models
Hessam Sarjoughian
Dongping Huang
Arizona Center for Integrative Modeling & Simulation
Department of Computer Science & Engineering
Fulton School of Engineering
Arizona State University, Tempe, Arizona, 85281-8809
Email: {sarjoughian|dongping}@asu.edu
Abstract
It is common practice to build complex systems from
disparate sub-systems. Model composability is concerned
with techniques for developing a whole model of a system from the models of its sub-systems. In this paper we
present a new kind of Multi-Formalism Modeling Composability Framework which introduces the concept of Knowledge Interchange Broker for composing disparate modeling
formalisms. The approach offers separation of concerns between model specifications and execution protocols across
multiple modeling formalisms. The framework is exemplified via vehicle and agent models described in the DiscreteEvent System Specification and Reactive Action Planning
formalisms. A high-level software specification that illustrates an implementation of this framework is described.
Ongoing and future research directions are also briefly presented.

1. Introduction
Systems built from sub-systems have interesting behavior and characteristics stemming from diverse capabilities
of individual sub-systems as well as interactions among
them. Some well-known examples of these types of systems
are transportation [6] and supply-chain networks [15]. One
of two recurring relationships are typically observed among
sub-systems of such systems: either one sub-system is controlled or managed by one or more sub-systems or, alternatively, one sub-system controls several other sub-system
processes. A simple example illustrating a process and its
control is a vehicle traveling from one location to another
under the guidance of an agent. A model of such a system can consist of a vehicle model and an agent model.

The vehicle movement represents a process and the agent
decisions represent the vehicle’s control. We can consider
the vehicle model as representing a physical entity and the
agent model as representing a logical entity. Each entity
has its own unique characteristics. A vehicle model specification describes how inputs are processed and outputs are
produced to simulate its movement. An agent specification
describes the rules dictating feasible roadway paths a vehicle can travel given some constraints.
The physical and logical sub-systems have a symbiotic
relationship where the former sends its state to the latter in
order to receive appropriate commands to satisfy the needs
of the overall system—for example, the agent model can direct the vehicle to follow a path (e.g., a set of x and y coordinates). In such a scenario, the vehicle model can have state
variables for speed, fuel consumption, and distance traveled
with respect to a reference location. Given the dynamics
of a traveling vehicle under control of an agent, different
approaches may be used to model their combined behavior.
To describe complex systems, we can use a monolithic
modeling specification. This choice, however, can result in
a model that can be difficult to specify and develop because
of the confounding disparities in formulating vehicle and
agent dynamics. For example, vehicle dynamics and decision policies can be described using either discrete-event
[27] or mathematical logic [10]. Relatively complex models for vehicle movement with simple routing schemes can
be developed using a discrete-event modeling and simulation framework. Similarly, mathematical logic can be used
to model complex plans and simple vehicle dynamics. Neither, however, lends itself for describing both procedural
and declarative behavior of vehicles and agents in their respective modeling formalisms.
A common approach to composability is to integrate
“models” as software modules or components. For exam-

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

ple, vehicle and agent models can be combined by writing
customized software to handle data and control interactions.
A model of a vehicle navigated via an agent model is developed by adding “software hooks and wires” to accommodate sending and receiving information between these
models [12]. Alternatively, middleware technologies may
be used to combine different execution engines, especially
when there is a requirement for distributed or parallel execution. These approaches offer higher-level programming
concepts and interfaces (i.e., a set of generalized services)
to integrate models as software components. Indeed, numerous studies in many domains illustrate that while data
and control can be successfully exchanged between models described in distinct formalisms, their integration often
degenerates to software engineering or computer programming.
The above approaches, however, suffer from a major
shortcoming—the resultant models rely on arbitrary modeling syntax and semantics, which not only make composition of disparate models difficult, but also adversely affect
the degree to which composed models can be formalized.
Thus, instead of integrating models, we propose composing modeling formalisms. This concept has been used for
closely related modeling formalisms (e.g., [19]), but not for
inherently different formalisms such as discrete-event and
those for agent modeling (e.g., RAP). To help with model
composability, we have developed a novel concept called
Knowledge Interchange Broker (KIB). It enables composition of modeling formalisms. This kind of composability offers a common, generic basis for describing models
that conform to their respective formalisms, yet the resultant
composed models have well-defined structure and behavior.

2. Background
Research in model composability has been underway
from many disciplines including systems engineering, software engineering, and artificial intelligence. The basic (informal and formal) notion of composability and its challenges are well recognized from science and engineering
points of view. In particular, composability concepts, theories, and techniques can be said to consist of abstraction, modularity, hierarchy (aggregation, disaggregation),
and encapsulation. These provide a set of primitives for
component (de-)composition and reuse and therefore model
composability. Numerous studies have described characteristics of composability, some detailing technical and
non-technical challenges in achieving composability (see
[2, 7, 17, 21, 25]).
Systems theory offers a sound framework for composition of models. It has traditionally focused on creation
of theories for design of systems and, in recent years,
their implementation aspects. The system-theoretical con-

cepts, principles, and formal treatment of time provide a
sound foundation for continuous and discrete modeling formalisms. For continuous and discrete-event models, time
base is continuous (i.e., real) and for discrete-time models,
time base is discrete (i.e., integer). Formal specifications
for input, output, and state trajectories, and their transition
functions support rigorous model development. Two wellknown formalisms are discrete-time specification [26] and
discrete-event system specification [27].
Systems theory offers two fundamental concepts. First,
it enables characterization of a system at different levels of specification (e.g., input/output functions and statespace systems). Second, different types of formal specifications—continuous, discrete-time, and discrete-event dynamics—are supported within a framework such that they
can be composed under well-defined conditions.
A system-theoretic specification is founded on the basic
characterization of a system in terms of its structure and
behavior. Behavior refers to the system’s outwardly (visible) time-based manifestation as governed by its structure.
Structure describes a system’s architecture in terms of each
component’s structure and behavior and components’ interactions. For example, the internal structure of a component
can specify state set, state transitions, and pair-wise timed
input/output data sets. Complex structures can be specified
through hierarchical decomposition.
Another research direction which offers important concepts and methods for separation and composition of
knowledge comes from the artificial intelligence (AI) community. An example of an AI approach for modeling complex systems is INTERRAP [18] which is based on Reactive
Action Planner (RAP) [4]. It models a system’s processes,
control, and planning via a layered architecture in order
to enable separation of concerns and systematic interaction
and cooperation among agents.

2.1. Related Work
The software engineering paradigm largely leans toward
specifying a system in terms of semi-formal modeling techniques. This paradigm often results in composing systems
where the specifications of sub-systems have to be represented within a single modeling framework such as UML.
Since a generalized framework must account for a variety
of specifications (e.g., Activity and Statechart Diagrams)
which do not have a common semantics, it becomes necessary to depend on the lowest common denominator to develop models and rely on defining concepts and abstractions
using programming language constructs.
Other approaches focus on component-based modeling
where a computational framework is used to execute finite
state machines and, discrete-event, and continuous models
(e.g., [16]). The theoretical basis of this approach, known as

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

Ptolemy II, relies on token-based dataflow to combine execution of models of mixed signal and hybrid systems. This
approach formalizes input/output interactions among actors
(model components) under the control of directors using interface automata [3]. This formalization is specialized for
different computational domains including Communicating
Sequential Processes and Process Networks. This approach
can also be viewed as multi-formalism but with a software
engineering focus. Another approach to composability is
based on mapping some related formalisms (differential and
algebraic equations, Petri nets, and finite state machines)
into the DEVS formalism—this is a realization of a metamodeling concept [14].
Synthesis of simulation and physical components can
also be carried out using computational composability and
in particular interoperability. Mature technologies exist
for enabling interoperability with simulation engines and
other applications (see [8]). Interoperability enables different software programs to exchange information using
service-oriented framework—i.e., offering different suites
of generic services which can be brought together to satisfy
specific needs.
Interoperability by itself, however, cannot ensure that
the integrated models perform in a semantically consistent
manner. Instead, if models are composed in a manner that
is syntactically and semantically well-defined, then interoperability can provide a precise basis (methods and technology) for realization using appropriate software environments.
In addition, platform-independent business logic model
development can be used to derive a platform-specific software model and subsequently automatic generation of target application code (known as Model Driven Architecture
(MDA)). Two of the MDA specifications are Meta-Object
Facility and XML Metadata Interchange, which are important for ensuring different syntactic specifications work together.
Related to this work is the integration of systemtheoretic modeling with a non-monotonic logical reasoning
to support inductive modeling. This approach enables
reasoning of time-based input/output model dynamics
in a synchronous setting [20]. More closely related
to the work presented here is the composability of (a)
discrete-event (DEVS) and (b) linear programming (LP)
[11] for the semiconductor supply-chain domain, in which
we describe a knowledge interchange broker to support i)
detailed process and decision models for semiconductor
supply-chain networks and ii) configurable data aggregation/disaggregation mapping. Another closely related work
addresses composition of the DEVS and Model Predictive
Control model specifications [22].

3. Modeling Composability Framework
A useful concept underlying the success of modeling approaches is the separation of a model’s specification from its
execution protocol. For example, separation of a simulation
model description (specification) from its execution (simulator) allows composed models, expressed within one modeling formalism, to be executed using alternative simulation protocols in both single or multi-processors [24]. This
separation is important when applied to formalism composability. In particular, it provides a basis to address the
consensus that approaches and technologies based on the
concept of interoperability 1 by themselves cannot support
component-based (modeling & simulation) composability
(e.g., see [2]).
We consider a modeling formalism to consist of a model
specification and an execution protocol with a unique syntactic and semantic characterization 2 . We use “the separation of model specification and execution protocol” as our
conceptual basis for the multi-formalism modeling composability framework. As stated earlier, the main idea is
to “compose modeling formalisms” instead of “composing models”. The difference is that without composability at the level of modeling formalisms, composability can
only be achieved via interoperation ranging from low-level
programming constructs to high-level middleware services.
Consider two modeling formalisms suitable for specifying
and executing a vehicle guided to a destination as an example. One modeling formalism can be discrete-event specification and its associated simulation protocol. Another can
be of an agent specification and its interpreter. Formalism
composability, therefore, is defined as a pair consisting of
two specifications (e.g., DEVS and RAP specifications) and
the interaction of their executors (e.g., DEVS simulator and
RAP interpreter) [23].
As shown in Figure 1 the DEVS formalism is suitable
for representing the vehicle dynamics and the RAP formalism for the agent decisions. Here, we refer to process
flow (movement of vehicles) and decision making (paths
to follow) as distinct layers to emphasize that the agent
model and vehicle simulation model represent two levels
of abstraction of the overall system-i.e., the agent model
describes higher-level knowledge (decisions) compared to
the vehicle simulation model (operations). An important
benefit of this approach is that the general-purpose multiformalism model composition can support different do1 Interoperability refers to how two or more simulation/execution applications can work together either using middleware (e.g., message exchange
and time management protocols) or inter-process services. As mentioned
above, several differences between composability and interoperability have
been described.
2 A formalism may have several implementations. While each can be
consistent with the formalism specification, the implementations may differ due to lower level expressiveness used in their designs.

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

mains (see Figure 1). Furthermore, this approach naturally
applies to distributed systems where physical process flows
and logical decision making can be executed separately.
Therefore, with this framework, we have a basis to
characterize model heterogeneity—i.e., composition of a
larger model composed from smaller model components
described in different modeling formalisms. For example,
this approach supports composability of DEVS and RAP
formalisms upon which a separate layer of domain-specific
network systems can be specified such that the structure and
behavior of the composed models are consistent within the
DEVS/RAP multi-formalism framework using the Composition Specification and its corresponding Executor (see
Figure 1). The KIB formalism, therefore, allows specification of all models to include proper syntactic structure and
behavioral semantics with generalized support for data and
control between the DEVS and RAP formalisms.

Figure 1. Multi-formalism modeling composability framework.

It should be noted that no modeling formalism can ensure validity of models universally; rather, models can be
shown to be i) syntactically correct and ii) semantically
valid for a well-defined set of requirements. The key (twofold) capability is to support development of DEVS and
RAP models separately and also to allow their composition.
That is, the DEVS and RAP modeling formalisms are composed to allow for (i) exchange of appropriately mapped
outputs to inputs data (messages) and (ii) appropriate data
transformation under a well-defined interaction protocol (at
various levels of homomorphism).
Therefore, the composition of two formalisms is defined
in terms of the Composition Specification, and its corresponding Executor as depicted in Figure 1. In contrast,
”pseudo composability” can be supported by using lowlevel generalized services (e.g., [13]) that are specialized

for high-level modeling formalisms. Next, we consider
the KIB’s Composition Specification and Executor for the
DEVS and RAP formalisms.

3.1. Composition of DEVS and RAP Formalisms
We begin with brief descriptions of the Discrete-Event
System Specification (DEVS) [27] and Reactive Action
Planning (RAP) [4, 5] formalisms. The DEVS atomic
model
specification—X, S, Y, δ int , δout , δconf , λ, ta
—allows representing state transitions using internal
transition function (δ int ) in the absence of external events
which can be received on input ports. Modeling of arbitrary
arrival of input events is characterized using the external
transition function (δ out ). Concurrent internal and event
transitions can be specified with the confluent transition function (δ conf ). The output function (λ) specifies
mapping of states (S) to outputs (Y ). The time advance
function (ta) determines the holding times for states (S).
Input and output events are associated with ports (i.e.,
.
X = (port, value)). The atomic models can be coupled
hierarchically using output to input, input to input, and
output to output port couplings.
The Reactive Action Planning system consists of an Interpreter, Task Agenda, Monitor Agenda, and a library of
Reactive Action Packages (RAP) where hierarchical, sequential, or parallel tasks can be executed given the state
of the world and actions to be taken to satisfy some logical conditions. Each RAP specification is described using
a set of well-defined formulas in mathematical logic—i.e.,
A1 . . . Ak  W where Ai ’s for i = 1, . . . , n are facts and
W represents all possible logical deductions given a set of
axioms. Therefore, as shown in Figure 2, the RAP library
consists of a collection of rules expressed as logical clauses
where index is a unique identifier, succeed, fail, precondition, and constraints specify conditions in which the RAP
can be used, and retries limits the number of retries (see below). The on-start, query, etc are task nets (i.e., method) to
be executed when the RAP task begins and finishes.
(define-RAP
index
(succeed
query)(fail
query)(precondition query)(constraints query)(retries
query)(on-start |query |failure |success |finish task-net
forms)(method plan-for-carrying-out tasks))
Figure 2. Syntax for Reactive Action Packages.

Each RAP defines a group of possible ways a task may
be carried out given different world situations. RAP events
are asynchronous messages which may be generated internally or externally. The elementary constructs of RAP are

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

query and action (command) events. These events change
the memory of RAP. Each of these is specified using (eventname. args)-e.g., (position A x y) specifies vehicle A’s x
and y coordinates. The expression used in the events can
have numeric and logical operators.
The Task Agenda consists of a set of tasks which may be
generated externally or internally. These tasks may come
from a planner supporting a particular set of goals for a
specific domain (e.g., guide a vehicle to a destination from
a starting location). To support asynchronous handling of
tasks, the RAP Monitor is defined similar to RAPs where
start, active, trigger, and reset constructs are included.
The primitives wait-for-time and wait-for-event time constructs are defined for the RAP Monitor. These can be
used to manage situations within which the RAPs can be
processed since RAPs are not specified in terms of time.
The Monitor Agenda, although not strictly necessary, can
be used to monitor situations when specific events in the
world require placing tasks in the Task Agenda (e.g., upon
detecting low fuel level, the agent guides the vehicle to a
location for refueling). The Interpreter is a type of resolution theorem-prover that can correctly execute the logical
formulae in the memory and the RAPs.

3.2. KIB Specification
The composition of two modeling formalisms is defined
via a generalized Knowledge Interchange Broker 3 (KIB),
which specifies event mapping, synchronization, concurrency, and timing properties. These properties can be described in terms of structure and behavior of the composition specification and executor as exemplified using the
DEVS and RAP formalisms [23]. The proposed approach to
multi-formalism composition, therefore, is based on characterizing how two formalisms can interact with one another.
Input and output mappings are required to support composition of models based on message types and interactions
which are syntactically and semantically well-defined for
the modeling formalism as shown in Figure 3.
For example, the vehicle needs to receive and process input events such as adjust-speed (object, v) from the KIB.
The agent command must be mapped into the input event
of the vehicle model (speed, v) where speed is the input
port name and v is the agent’s demand velocity. In contrast,
since the RAP events do not have ports, the KIB needs only
to send the values of the DEVS output messages. These
mappings or translation (I KIB → OKIB ) are carried out in
the KIB instead of being handled inside the DEVS and RAP
models (refer to Figure 3).
3 The term KIB is coined in reference to the Knowledge Interchange
Format (KIF) [9]. KIF is useful for information interchange between disparate computer languages. Its level of abstraction (programming languages), however, does not lend it to describing model specifications and
therefore model composability as presented here.

The messages (events) exchanged between DEVS and
RAP can be arbitrarily complex—e.g., an input message
sent from DEVS to RAP can be moveToPosition(vehicle,
x, y). Obviously, the KIB is tasked with bi-directional message translation and mapping and therefore it accounts for
the structural aspect of model composability—it must handle the disparity between the DEVS and RAP inputs and
outputs. Message mappings can be specified such that two
messages have identity, isomorphic, and homomorphic relationships. The messages expressed in two distinct modeling formalisms can have simple structures (e.g., a string) or
complex structures (e.g., a list of objects). Thus, a suite
of data translation schemes based on the two participating modeling formalisms and domain knowledge mappings
is needed. The data translation can be arbitrarily complex
since the generalized DEVS and RAP message structures
can be specialized to meet domain specific needs.

IKIB = IDEV S→KIB ∨ IKIB←RAP and
OKIB = ODEV S←KIB ∨ OKIB→RAP where
.
.
IDEV S = ODEV S←KIB , ODEV S = IDEV S→KIB ,
.
.
IRAP = OKIB→RAP , ORAP = IKIB←RAP
Figure 3. DEVS/RAP KIB I/O mapping and
transformation.
The message mapping M → N mentioned above is
based on the well-known concepts of knowledge reduction
and augmentation. Knowledge reduction is simpler, in comparison with knowledge augmentation, since the KIB needs
to discard information in the process of translating one message type to another. For example, given any DEVS model
message (port, value), the port information needs to be discarded. In contrast, for knowledge augmentation, consider
a RAP message. Since the RAP messages do not have ports
associated with them, the KIB must assign ports to these
messages before they can be sent out as DEVS messages.
To handle this, the DEVS formalism can be extended to
have a pair of unique single input and output ports dedicated to receiving and sending messages to and from the
KIB. The KIB therefore needs to support not only message
decoding but encoding as well. Further more, to deal with
the complexity of information mapping, the KIB can provide a basis for handling different types of data abstraction
and concretization (i.e., aggregation or disaggregation).

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

In addition to the structural specification of the KIB, we
also need to specify its behavioral aspect. The specification
must account for ordering of messages exchanged. The control of messages among DEVS, KIB, and RAP can be described in terms of synchronization, concurrency, and timing concepts and their specifications. A DEVS model may
send an output message and receive an input message from
the RAP model. Consider a situation where RAP is guiding the vehicle based on its amount of available fuel. Since
the vehicle may send this information while also receiving a
query from RAP about its speed, the KIB specification must
support synchronization of multiple messages to be sent and
received.
Related to synchronization is concurrency of messages.
The parallel DEVS formalism supports concurrent execution of model components and multiple input and output
events. It supports sending and receiving multiple events
through multiple ports. RAP, unlike DEVS, can send its
command and query messages sequentially. The capability
to handle concurrent events is important. Consider the situation where the vehicle sends the distance left to reach its
destination and its current location at the same time. The
KIB needs to be able to receive these concurrent messages
from the vehicle model and send them to the agent model
in some order. Since these DEVS can send messages simultaneously, the KIB must order the DEVS messages before
sending them to the RAP. This is appropriate since concurrent DEVS messages are not causally dependent, and the
messages sent to the RAP need to be ordered based on the
domain-specific choices to be supported in the RAPs.
The KIB may be defined to execute while consuming
(logical) time. Since the DEVS formalism explicitly accounts for passage of time, and the RAP formalism does
not, the use of time in the KIB is not strictly required. This
is because the RAP can respond to queries in zero logical
time. Similarly, it can generate commands while consuming no logical time w.r.t. the DEVS simulation protocol.
As mentioned earlier, the RAP tasks may account for passage of time via it’s the RAP Monitor. In this case, the
RAP tasks can be executed using wait-for-time and waitfor-event time constructs defined within the RAP Monitor.
The presence of logical time in both DEVS and RAP can require the KIB to manage the ordering of its messages. This
may be achieved with a KIB that uses a reference time assuming its mappings do not consume logical time. Alternatively, the processing of messages may be defined using
time-based (causal) ordering.
The combination of synchronization and concurrency
is used to define the DEVS+RAP Composition Specification control protocol. The ordering of messages in the
KIB—messages sent to and received from the DEVS and
RAP models—is based on DEVS sending a message to
RAP followed by receiving a response (message) from

the RAP. In this scheme the interaction between RAP and
DEVS is circumscribed based on the DEVS abstract simulator protocol. Once the output messages of the DEVS
are sent to the KIB, the DEVS simulator is stopped until
a response is received from the KIB (and therefore RAP).
The KIB’s executor, therefore, is responsible for synchronous message interactions between DEVS and RAP while
no logical time is consumed with respect to the DEVS simulator. That is, each cycle of interaction between DEVS and
RAP (messages sent and received via the KIB) occurs during one DEVS simulation cycle—i.e., the simulation clock
remains unchanged and therefore no clock is necessary in
the specification of the KIB executor. Under this interaction regime, local control in the process and planning models can be specified independently of the KIB control protocol. This type of synchronization is implemented in the
DEVS/RAP environment which is briefly described in the
next section.
Aside from the just described synchronous executor control protocol, DEVS and RAP may also interact via an asynchronous executor. In general, we can define two types of
asynchronous interactions depending on whether or not one
or both of the formalisms are time based. For example, the
RAP system may use a Monitor to manage creation of RAP
tasks in response to unexpected events. Without the RAP
Monitor, the synchronous KIB specification is simpler and
sufficient if the RAP responses can be considered to happen
instantaneously. In contrast, the KIB can be asynchronous.
The KIB asynchronicity can be defined based on the causality of the DEVS and RAP messages generated according
to the DEVS simulation protocol and the RAP interpreter.
With asynchronous process flow, the vehicle can continue
to travel along its current route while the agent is devising
new (updated) routes. The asynchronous interaction with
explicit accounting for time can be defined in terms of (logical or real-time) clocks. That is, with a time-based KIB
executor, the DEVS simulator and the RAP interpreter can
execute concurrently while handling message mapping and
interaction.

4. DEVS/RAP Environment
The multi-formalism modeling framework presented
in the previous section can be developed using different software design techniques and programming languages. Here we describe a high-level software design for
the DEVS/RAP KIB environment. We highlight its basic capabilities—message mapping and synchronization of
events—and how they are supported. Two main elements
of the DEVS/RAP KIB environment are shown in Figure 4
[23].
The KIB was developed in Java to simplify its interaction with DEVSJAVA [1]. Similarly, the interaction be-

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

tween the KIB and the RAP was developed in C++ since
the RAP interacts with its outside world via C++ wrapper
around MZScheme [5]. The software design specification
of the KIB was developed in accordance with Figure 3—the
structure and behavior of the KIB message mappings and
synchronization are consistent with those that are defined
above and those of the DEVS and RAP formalisms. The
executor of the DEVS+RAP Composition Specification is
responsible for message mapping and controlling the interactions between DEVSJAVA and RAP. In this design,
ordering and mapping of DEVSJAVA and RAP messages
are assigned respectively to the JSimMessageManager
and CRAPMessageManager. The ordering of messages
to and from the KIB was designed into the RapBridge
and SimulatorBridge packages and their interfaces, respectively. We used JNI as our primitive connectivity between
the Java and C++ programming languages.

ing formalisms are to be composed [11, 22].

6. Acknowledgements
This research was initially supported by Advanced Simulation Center, Lockheed Martin in Sunnyvale, California
and subsequently by the Intel Research Council, Chandler
Arizona. Steven Hall from the Advanced Simulation Center provided key insights on difficulties for integrating the
RAP and the DEVS-C++ models. The first author acknowledges Gary Godding of the Arizona State University and
Karl Kempf of the Intel Corporation for stimulating discussions about data transformation in complex domains. The
DEVS/RAP environment has been developed jointly with
Jeff Plummer of General Dynamics and with the help from
Preston Cox of Lockheed Martin. Thanks to Elizabeth Pulcini for her help in preparing this manuscript.

References

Figure 4. DEVS/RAP KIB implementation.
The control of the composed models is initiated by DEVSJAVA and is maintained by the KIB. This choice of initialization and control was in part due to the need for the
Java Virtual Machine to create the RAP system and to load
the RAPs or agent models. As mentioned above, the KIB
synchronization between the DEVSJAVA and RAP system
is referenced with respect to the DEVSJAVA simulator cycle.

5. Conclusions
We have proposed a new framework where combined
process dynamics and decision making models—in particular, discrete-event processes and agent-based planning—can be developed within a model-theoretic composability approach. This framework is useful for characterizing the structure and behavior of a system’s model whose
overall dynamics are derived from its sub-systems’ dynamics. This work offers a generalized basis and a methodology
toward model validation and execution verification for heterogeneous systems. Our ongoing research is focusing on
time-based asynchronous composability and a hierarchical
knowledge interchange broker where three or more model-

[1] ACIMS.
Arizona Center for Integrative Modeling
and Simulation, 2001 [cited 2005].
Available from:
http://www.acims.arizona.edu/SOFTWARE.
[2] P. Davis and R. Anderson. Improving the Composability of
Department of Defense Models and Simulations. RAND,
Santa Monica, CA, 2004.
[3] L. de Alfaro and T. Henzinger. Interface automata. In Proceedings of the 8th European Software Engineering Conference, Vienna, Austria, 2001.
[4] R. Firby. Adaptive Execution in Complex Dynamic Worlds.
PhD thesis, Computer Science Department, Yale University,
New Heaven, CT, 1989.
[5] R. Firby and W. Fitzgerald. The RAP System Language
Manual, Version 2.0. Neodesic Corporation, Evanston, IL,
1999.
[6] K. Fisher, J. Müller, and M.Pischel. Cooperative transporttion scheduling: An application domain for dai. Applied
Artificial Intelligence, Special Issue on Intelligent Agents,
1(10):1–33, 1996.
[7] P. Fishwick. Next generation modeling: A grand challenge.
In International Conference on Grand Challenges for Modeling and Simulation, San Antonio, Texas, USA, 2002. SCS.
[8] R. Fujimoto. Parallel and Distributed Simulation Systems.
John Wiley and Sons Inc., 2000.
[9] M. Genesereth and R. Fikes. Knowledge Interchange Format Reference Manual, Version 3. Computer Science Department, Stanford University, 1992.
[10] M. Genesereth and N. Nilsson. Logical Foundation of Artificial Intelligence. Morgan Kufmann, San Mateo, CA, 1987.
[11] G. Godding, H. Sarjoughian, and K. Kempf. Multiformalism modeling approach for semiconductor supply/demand networks. In Proceedings of Winter Simulation
Conference, Washington DC, USA, 2004.
[12] M. Hocaoglu, H. Sarjoughian, and C. Firat. DEVS/RAP:
Agent-based simulation. In AI, Simulation and Planning in
High-Autonomy Systems, Lisbon, Portual, 2002. SCS.

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

[13] IEEE. HLA Framework and Rules. IEEE, 2000. IEEE 15162000.
[14] J. Jaramillo, H. Vangheluwe, and M. Moreno. Using metamodelling and graph grammar to create modelling environments. Electronic Notes in Theoretical Computer Science,
3(72):1–15, 2002.
[15] K. Kempf. Control-oriented approaches to supply chain
management in semiconductor manufacturing. In Proceedings of IEEE American Control Conference, Boston, MA,
USA, 2004.
[16] E. Lee. What’s ahead for embedded software. IEEE Computer, 9(33):18–26, 2000.
[17] P. Mosterman and H. Vangheluwe. Guest editorial: Special issues on computer automated multi-paradigm modeling. ACM Transaction on Modeling and Computer Simulation, 4(12):249–255, 2002.
[18] J. Müller. The design of intelligent agents: A layered approach. In Lecture Notes in Artificial Intelligence. Springer,
1996.
[19] H. Praehofer. System Theoretic Foundations for Combined
Discrete Continuous System Simulation. PhD thesis, Institute of Systems Science, Department of Systems Theory and
Information Engineering, Johannes Kelper University, 1991.
[20] H. Sarjoughian. Inductive Modeling of Discrete-event Systems: A TMS-based Non-monotonic Reasoning Approach.
PhD thesis, Electrical and Computer Engineering, The University of Arizona, Turson, AZ, USA, 1995.
[21] H. Sarjoughian and F. Cellier, editors. Discrete Event Modeling and Simulation Technologies: A Tapestry of Systems
and AI-Based Theories and Methodologies. Springer Verlag., 2001.
[22] H. Sarjoughian, D. Huang, and et al. Hybrid discrete event
simulation with model predictive control for semiconductor
supply-chain manufacturing. In Proceedings of Winter Simulation Conference, Orlando, FL, USA, 2005.
[23] H. Sarjoughian and J. Plummer. Design and implementation of a bridge between RAP and DEVS. Computer Science and Engineering, Arizona State University, Tempe, AZ,
2002.
[24] H. Sarjoughian and B. Zeigler. DEVS and HLA: Complementary paradigms for modeling and simulation? Transaction of the Society for Modeling and Simulation International, 4(17):187–197, 2000.
[25] E. Weisel, M. Petty, and R. Mielke. Validity of models and
classes in semantic composability. In Simulation Inteoperability Workshop, Orlando, FL, USA, 2003.
[26] A. Wymore. Model-based Systems Engineering: an Introduction to the Mathematical Theory of Discrete Systems and
to the Tricotyledon Theory of System Design. CRC, Boca
Raton, 1993.
[27] B. Zeigler, H. Praehofer, and T. G. Kim. Theory of Modeling
and Simulation: Integrating Discrete Event and Continuous
Complex Dynamic Systems. Academic Press, 2nd edition,
2000.

Proceedings of the 2005 Ninth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’05)
0-7695-2462-1/05 $20.00 © 2005

IEEE

Proceedings of the 2013 Winter Simulation Conference
R. Pasupathy, S.-H. Kim, A. Tolk, R. Hill, and M. E. Kuhl, eds.

INTERACTING REAL-TIME SIMULATION MODELS AND REACTIVE
COMPUTATIONAL-PHYSICAL SYSTEMS
Hessam S. Sarjoughian
Soroosh Gholami

Thomas Jackson

Arizona Center for Integrative Modeling & Simulation
School of Comp., Info., and Decision Systems Eng.
Arizona State University, Tempe, AZ 85281, USA

9855 Scranton Road, Bldg. 5
San Diego, CA, 92121, USA

ABSTRACT
For certain class of problems notably in cyber-physical systems it is necessary for simulations to be indistinguishable from computational-physical systems with which they interact. Hard real-time simulation
offers controlled timing which lends it to be composed with systems operating in physical-time. Accurate
real-time simulation equipped with distinct input/output modularity for simulation and software systems
is proposed. To demonstrate this approach, a new model for composing ALRT-DEVS (a hard real-time
simulation platform) with computational-physical systems is proposed. It provides an abstract communication model for hard real-time simulation and software systems to interact. Experiments are developed
where a simulated control switch model (with single and multiple inputs and outputs) operates independent
mechanical relays. In another experiment, the control switch commands are communicated with software
capable of operating simulated and mechanical relays in real-time. In this setting, simulated software,
physical, or computational-physical systems may be interchanged with their actual counterparts.
1

INTRODUCTION

Systems of systems have been experiencing rapid rise in their complexity, particularly from the standpoint
of timing (Douglass 2004; Jamshidi 2010). Conceptualization, development, operation, and even retirement
of these systems are commonly supported with logical-time simulations. However, integral to some of
these systems is the necessity to operate in physical-time. For example, a modern vehicle’s operations are
controlled via software executing under (soft and/or hard) real-time constraint.
In cyber-physical systems (Derler et al. 2012), some interactions taking place among software and
physical systems require both physical-time and logical-time. Two abstract representations of physical-time
are logical-time and real-time. The former is independent of computational platform in which time is handled
whereas the accuracy of latter is not. A simplified representation of the logical-time is event sequence
in which passage of time is represented as a total-ordered set of steps. From theoretical perspective,
logical-time (or real-time) may be represented as discrete or real values with infinite accuracy. From
implementation perspective, both logical-time and real-time have finite accuracy. More generally, Realtime is a representation of physical-time with finite precision as supported in a computational platform
(Sarjoughian and Gholami 2013).
Common simulations of cyber-physical systems are specified using logical-time and/or event sequence
abstractions. A variety of theories and techniques can be used to model and simulate these systems. Both
logical-time and event sequence representations of the physical-time are useful. Theoretical models may
be specified and manipulated with arbitrary accuracy for inputs, outputs, and states. Concrete simulations
can only approach the theoretical accuracy defined in such model abstractions. In general, computational
precision afforded in host software/hardware system is assumed to have insignificant side effect on the

978-1-4799-3950-3/13/$31.00 ©2013 IEEE

1120

Sarjoughian, Gholami, and Jackson
simulation results. Indeed, abstraction of physical-time as either logical-time or event sequence in defining
abstract models and simulators are ubiquitous in numerous scientific and engineering application domains.
For some systems the abstractions for the physical-time pose major limitations. Execution of a simulation
in terms of logical-time or event sequence may not be a part of systems of systems operating in physicaltime. Consider a simulated cruise controller for a real vehicle. The simulation during execution must
receive sensor signals just-in-time from actual sensors placed on wheels. Sensor data must be detected and
transmitted to the cruise controller neither too fast nor too slow given the physical characteristics of the
vehicle. The simulator’s execution speed must be within some acceptable lower and upper time bounds
given a wheel’s angular speed.
Physical operations of a system may be replicated using real-time simulation. This is possible assuming
a real-time model can be executed in a simulator having a real-time clock. The simulator must be able to
correctly execute the model (i.e., consume inputs and produce outputs in real-time). Unlike physical-time
which has infinite precision, real-time used in the model and in the simulator can have finite accuracy
(resolution). The hardware can guarantee timing precision up-to a finite threshold.
Real-time simulations although not as precise as logical-time simulation can serve purposes that are not
achievable otherwise. Accuracy of a real-time simulation clock may range from 10−3 to 10−9 seconds which
is much less than logical-time accuracy, for example 2−32 seconds. A major use of real-time simulation
of a sub-system is that it can be synthesized with a physical sub-system. The system as a whole runs
in mixed real-time and physical-time. The input and output interactions taking place between the virtual
and physical entities generally occur synchronously. This, of course, assumes exactly the same amount
of time passed in the real-world has also passed in the virtual world. This requires formulating a concise
synchronization across physical-time and real-time. This requirement can be satisfied if the hardware and
software operations needed to execute the simulation can sustain the speed at which the physical and
software sub-systems are operating at. For example, simulated cruise controller execution and the wheel’s
speed remain synchronized in time throughout a designated time period.
A sub-system can also be a computational entity such as the cruise controller mentioned above. Virtual
and software sub-systems as well as software and physical entities have their own timing relations. This
view holds under the assumption that time is explicitly accounted for in simulation but not in software (i.e.,
software executes as fast as possible by its hardware). Thus, simulators, software, and physical entities can
be considered as three types of sub-systems. A physical system can be computationally-based or not. A
pure physical system can be a vehicle wheel. A computational-physical system can be a sensor with A/D
and D/A converters.
In this paper, we focus our attention to the synthesis of real-time simulations and computational-physical
systems. We use Action-Level Real-Time DEVS simulator (Gholami and Sarjoughian 2012) which supports
hard real-time execution of DEVS models. We use this simulation platform to show its capabilities for
interacting with computational-physical systems. The chosen computational-system is a 4-relay Phidget
(PhidgetInterfaceKit-0/0/4 2013). It is a hardware/software system with four electrically-controlled relays
where each can be independently turned on or off using firmware and software. The result is a simple
testbed for formulating interactions taking place between virtual and actual systems. We propose a novel
proof-of-concept approach for integrated hard real-time simulations and computational-physical systems.
Example scenarios are developed to evaluate timing properties of interacting independent action-level
simulation operations with software/hardware systems.
2

BACKGROUND

Cybernetics —as the foundation for control of mechanical and electrical systems— has played a key role
in building many of past and existing systems. In recent years, computing has grown to be central to
operations of many time-critical systems. Real-time operation of a vehicle cruise controller is but one of
numerous systems that must operate in mixed physical-time and real-time. To analyze and design these

1121

Sarjoughian, Gholami, and Jackson
kinds of systems real-time simulation is in use. Real-time design requirements for hardware and software
parts of a cyber-physical system can be determined at reduced cost and risk.
An example is the Simscape model where real-time simulation is used as a virtual counterpart of
a physical system (Miller and Wendlandt 2010). Real-time means driving and observing the simulation
input/output in the physical world. Design details such as device actuation and sensor data collection
frequencies can be analyzed. Formulation of a design is enabled by Simulink. Solvers with fixed-cost
execution and fixed-step size solvers can be used to obtain an acceptable tradeoff between accuracy and
speed. Real-time simulation (a sub-system) may then be synthesized with other physical sub-systems.
2.1 Software/Hardware Real-time Control
In the area of multi-processor systems, real-time scheduling has been well studied (Davis and Burns 2011).
These algorithms are expected to guarantee a set of tasks representing a running application meet their
individual and collective time constrictions. Tasks can have fixed priority. Scheduling algorithms for
homogeneous computing systems are divided into periodic and sporadic task models. In the former, tasks
arrive periodically in strict order. Tasks may be synchronous or asynchronous. In the latter tasks may
arrive independently at any time once a minimum time interval has elapsed since the arrival of the last task.
Each task is characterized by its relative deadline, worst-case execution time, and minimum inter-arrival
time. Given these, three levels of constraints on task deadlines are implicit, constrained, and arbitrary.
Other key concepts are processor load and processor demand which are used to determine whether or
not an application’s tasks are feasible. There and many formulations can be used to define performance
metrics. Real-time scheduling algorithms can be analyzed algorithmically and empirically given resource
availability.
The ideas and methods developed for multi-processor architectures are applied to co-design of real-time
control systems (Cervin and Eker 2005). Controllers may be designed for optimal performance given limited
CPU resources. In this work and other similar to it algorithms are implemented as real-time software
executing on hardware with high clock resolution. A hard real-time simulator as described below offers
an alternative platform for development of controllers.
2.2 Action-Level Real-Time DEVS
Classical and parallel DEVS formalisms specify simulation models using abstract time (Zeigler, Kim,
and Praehofer 2000). ALRT-DEVS (Sarjoughian and Gholami 2013, Gholami and Sarjoughian 2012)
was introduced to support the specification of models with real-time constraints using independent, timeconstrained, and action-based external and internal transition functions. The definition of atomic ALRTDEVS models —inspired from RT-DEVS (Hong et al. 1997) and Real-time Statecharts (Giese and Burmester
2003)— is as follows: hX,Y, S, A, Γ, Ω, ψ, λ ,tai. Elements X, Y , and S are input, output, and state sets
respectively. The action set (A) with use of time-window is defined in RT-DEVS. Internal transition (Ω),
external transition (Γ), and activity mapping (ψ) functions are defined using the concept of locations in
real-time Statecharts as well as parallel DEVS formalism. Statechart locations for ALRT-DEVS models are
uniquely identifiable by the current phase and the remaining time to the deadline. Actions are mapped to
each location with a predefined sequence of execution and guarded statechart transitions between locations
are incorporated to provide dynamic decision making capability. Making use of locations and guarded
transitions in real-time statechart enables the model to adaptively decide on the actions to execute and
the ordering among them. Coupled models in ALRT-DEVS do not specify time as in atomic models.
The input/output couplings of coupled models are timeless and therefore messages traversing couplings
consume zero time with respect to the simulation protocol. Zero time operations or instantaneous message
transfers may become problematic in real-time simulation.
ALRT-DEVS lends itself for developing temporally constrained models for dynamical systems. These
time-constrained models are specified using actions, time windows, locations, and the priorities defined for

1122

Sarjoughian, Gholami, and Jackson

Figure 1: Relay Phidget as a computational-physical system
all atomic models. Timing for every action has a continuous time-base with infinite accuracy. However,
having a real-time model does not lead to real-time simulation. The actual implementation of the model
introduces limitations on timing subject to the underlying platform which executes the simulation models.
The constraints introduced by the executing platform define the maximum accuracy of the simulation. This
applies to real-time guarantees as well as time granularity. In other words, the capacity of the executing
platform for the load it can manage to execute legitimately (no deadline loss) is bounded. Similar to time
resolution, real-time guarantees are not specified in the model and only come into play when the models
are realized on a specific platform.
Also, as mentioned in the previous section, in real-time simulation —with regards to the level of time
granularity the executing platform supports— instantaneous communications introduce time inconsistency.
This inconsistency is usually handled in atomic models by accounting for the real time it takes for messages
to reach their destination after departing from the source model. Similar to time granularity and realtime guarantees, time inconsistency–as a result of instantaneous message transfers–only appears during
simulation execution.
2.3 Relay Phidget System
For the proof of concept experiments, the PhidgetInterfaceKit 0/0/4 (PhidgetInterfaceKit-0/0/4 2013) (4
digital output ports) is selected. This is a computational-physical system with mechanical relays, firmware,
and USB cable being the physical part and the Hardware Driver API being the computational part. This is
very simple cyber-physical system where its computational part is un-timed and the physical part executes
in physical-time. Although the 4-relay Phidget only contains four digital outputs, it is possible to simulate
bi-directional communication between the cyber and the physical sides by leveraging the callback signal
received from the hardware. These callback signals simply inform the driver (cyber side) of the change in
the value of each output port. Therefore, the resulting cyber-physical system is capable of bi-directional
interaction where operations in the physical part can be executed under hard real-time constraints.
A representation of the Phidget as a computational physical system is depicted in Figure 1. As shown
in the figure, the Phidget contains both hardware and software sides. In the hardware side, the board
contains the communication chip and the relays. The software side consists of an API (Phidget driver)
which enables software/simulation to interact with the Phidget relays.
The operation of the physical side of the Phidget is based on electrical current and electromagnetic
fields created by the flow of current in the relay coil. The mechanical process of switching in each relay
limits the operational frequency of the system. Despite the limitations on operational frequency, the relays
are considered as pure physical systems, which perceive time as continuous (infinite time resolution). On
the other hand, the software side is untimed (sequenced) or has an abstract concept of time (logical or
real) with a certain resolution based on the underlying platform. Therefore, the Phidget in its entirety is
considered a discrete time system based on the platform its software side is executed on. The underlying
platform–illustrated in Figure 1 as hardware and operating system layers–is shared among all tasks running
on the software platform. Usually, the impact of the hardware on the API is neglected and the interaction
with the board is considered platform independent or instantaneous.

1123

Sarjoughian, Gholami, and Jackson
On the other hand, real-time simulation is heavily influenced by the executing platform. Since all
interactions between the cyber and physical components occur in physical-time, experimentations (data
collection and analysis) should also occur in real-time. This adds to computational load on the underlying
platform (software and hardware), which may ultimately result in loss of accuracy (both in experimentation
and simulation). This is extensively discussed in (Gholami and Sarjoughian 2013) in which this point
is clarified that unlike logical-time, real-time experimentation cannot be treated trivially and requires
system/platform dependent design.
3

COMPOSING SIMULATION AND COMPUTATIONAL-PHYSICAL SYSTEMS

The systems of systems discussed above may be categorized to three types. Physical systems are operated
without the aid of software systems. A mechanical relay used in the relay Phidget is a very simple example
of physical systems. As a Single Pole, Double Throw switch, the relay can be controlled with AC and DC
power sources. Another physical system is an engine of a vehicle built from mechanical and pneumatic
sub-systems. A software system has both software and hardware. In the Phidget the mechanical relay
activation (turning on or off) is operated via a software system which consists of an interface API executing
on a personal computer. Combining a physical system and a software system results in a computationalphysical system (e.g., see the Relay Phidget shown in Figure 1). The term computational is restricted
to software-based computations even though some biological, mechanical, electrical circuits or others
may offer capabilities similar to those of software systems. Physical, software, or computational-physical
systems are real-world entities. Computational-physical systems are cyber-physical systems if the former
can offer the capabilities designated for the latter.
Simulation systems are a special kind of software systems where their dynamics are governed in artificial
settings. In this work, simulation systems are considered to be strictly virtual entities where there are no
real world entities. This kind of simulation is akin to constructive simulation where all parts of a system of
systems are simulated (DoD 1995). Simulations involving real people operating real systems (called live
simulation (DoD 1995)) are not considered.
A class of systems of systems has simulation and computational-physical systems. These systems are
akin to virtual simulations (DoD 1995) where simulated systems are operated by real people. Interactions
between simulation and computational-physical systems can be viewed as those within computationalphysical systems. However, as noted earlier there are important distinctions between software and simulation
systems. Abstractions in simulations, grounded in abstract time, offer flexibilities that are not usually
considered in software systems unless time critical operations are required. Inherent to simulation is the
ability to arbitrarily accelerate or slow down its execution which is to say the virtual dynamics of a system
is artificially structured and governed.
A major distinction between simulation and software systems is worth noting here. In approaches such
as DEVS a simulation model is defined to have a specification and an abstract protocol which defines the
order in which the parts of a model are to be executed. Software system operations and executions, however,
are generally not separated. Most software systems execute based on best-effort principle where there is
an underlying assumption that sufficient resources are available for implemented system. Thus integrating
hard real-time simulation with best-effort software systems needs special attention. Hard real-time used in
simulation can be used for software integration.
3.1 A Model for Composing ALRT-DEVS Simulation with Physical Systems
Hard real-time simulations and computational-physical systems can be synthesized in different ways.
Simulation systems can interact with software systems. Simulations can also interact with physical systems
if simulation and software systems are indistinguishable. Since simulation and software systems are distinct
types of systems, simulation systems can interact with physical systems via software systems. In the case
of the Phidget system, a simulation model can operate the mechanical relay via the Phidget’s software.

1124

Sarjoughian, Gholami, and Jackson

Figure 2: Composing ALRT-DEVS Simulation models with the computational-physical system

Figure 3: Interaction types between real-time simulation and software system
A logical, structural model for composition of simulation and computational-physical systems is
illustrated in Figure 2. In the simulation system, components interact via simulation I/O interfaces (ports).
Messages sent and received among model components are communicated via these I/O ports. The simulation
interfaces shown in Figure 2 sanction simulation component to/from simulation component communications
according to a modeling formalism and its associated simulator protocol.
Modularity for I/O simulation and software interfaces is proposed. Communications between simulation
components and software components are sanctioned via software interfaces. A typical method is for the
simulation components to interact directly with software components (Hu and Zeigler 2005). For example,
direct call can be made inside external, internal, output, or helper functions of a DEVS simulation model
running as a standalone simulation. Direct calls can pass data (output) to a software component. Direct
calls can also retrieve data (input) from software for use in simulation. In distributed setting, I/O operations
can be synchronous and asynchronous.
Simulation components may communicate with software components in logical-time and real-time.
The execution speed of the computational-physical systems has no impact on logical-time simulation. The
correctness of hard real-time simulation is crucial for computational-physical systems. Outputs of the
simulation component must be produced and delivered to the software components within some specified
hard time limits. Similarly, simulation components must receive inputs from the software components within
designated hard time windows. Resource availability, therefore, affects real-time simulation dynamics in
relation to the computational-physical system with which it interacts.
Inaccuracy in dynamics of hard real-time simulations can be due to two reasons. First the timings
defined for operations in simulation model components are inaccurate. This is not considered in this
work. Second the simulator is unable to execute the models’ operations before their deadlines. Separating
simulation and software communication helps in reducing execution inaccuracy. Simulation models can
be developed and evaluated more accurately since communication with software components is excluded.
This can be significant for simulations requiring frequent and high volume communications with software
components. A less visible benefit is that I/O modularity in the simulation is strongly preserved.
As shown in Figure 3, simulation input/output ports are exclusive to simulation model components.
Software input/output ports (UML 2.0 (OMG 2004)) are defined for communication with software system.
Outputs generated by atomic model components can only be sent to software components via the highest-level
coupled model. The same requirement holds for software outputs destined for atomic model components.
Each simulation model can have multiple simulation and software input and output ports. The basic concept
of strict modularity as defined in parallel DEVS is enforced. Hierarchical external software input coupled

1125

Sarjoughian, Gholami, and Jackson

Figure 4: Interaction between simulated switch control and the Phidget software system
to the software input ports of atomic model component is defined. Similarly, atomic simulation model
component outputs coupled to the external software output ports is defined.
3.2 Hybrid DEVS-Suite/Phidget
DEVS-Suite simulator is extended to support the class of hybrid simulation and software systems modeling
and execution as described above. Atomic and coupled ALRT-DEVS model specifications enable bidirectional simulation to software communication. The DEVS Simulation Hardware Abstraction Layer
(DSHAL) is responsible for input/output data conversion and handling for software systems. It acts as
a generic proxy between the DEVS-Suite simulation engine and software systems. Its operations are
writing to and reading from the simulation component output and input ports. Interaction with specific
computational-physical systems is mediated through designated hardware-specific software (Jackson 2012).
Hardware Driver (i.e., PhidgetInterfaceKit kernel library) is an example of such hardware-specific software.
Functions such as memSet() belonging to the kernel library executes invocation calls initiated in the
simulation model components. For activating the mechanical relays as well as querying their states as
stored in the Hardware Driver, DEVS-Phidget software has been developed. The DEVS-Phidget functions
are listening for input and output events. Operations related to the Hardware Driver include reading/writing
to/from memory and attaching/detaching relays.
A sample event trajectory between simulation and the relay is illustrated in Figure 4. The trajectory
is shown from the perspective of the physical/simulated relay. The events are either generated within
simulation or the relay. The phase trajectory shows handling of events with turnOn/turnOff actions. Time
duration to execute operations in the DSHAL and DEVS-Phidget can be considered negligible. Real-time
simulation granularity is in milliseconds whereas execution of a function in these non-real time software
is much smaller by comparison. On the other hand, some operations in the Hardware Driver can consume
as much real-time as the simulator. This is due to the mechanical operations taking place in the relay
(minimally 30ms) and I/O with the host computer’s OS. Depending on the computational-physical system,
ALRT-DEVS may not be able to complete all of its actions within a time period.
The separation of the software system from DEVS-Suite simulator allows collecting and time stamping
software events and simulation events. In the centralized setting, DEVS-Suite Simulator, DSHAL, DEVSPhidget, and Hardware Driver are executed in the Java VM. This is advantageous since JVM clock can be
used for all.
4

PROOF-OF-CONCEPT EXPERIMENTS

In this section, several experiments are designed and carried out to serve as proof of concepts for the
discussions made above. In these experiments, the data of interest is the time it takes for the relay to
switch from on to off or vice versa when triggered by the simulated generator. Since there are no means to
measure the time when the relay is switched either on or off, the timestamp is recorded when the callback
signal is received in the cyber side from the physical relay. Therefore, the switch-on/off times are in reality
the aggregate time of the control signal leaving the cyber side up to receiving the callback signal from the
1126

Sarjoughian, Gholami, and Jackson

Figure 5: Mixed simulated/hardware Relay Phidget scheme
hardware (signal turnaround time). Since turning off and on are identical from the point of view of the
simulation and timing, in these set of experiments, we did not distinguish between these two operations.
Also, in all experiments with multiple options, one is chosen randomly such as choosing two relays to
toggle.
All experiments are done on a Windows 7 computer with Intel Core 2 Due 1.86GHz processor and
2GB of memory. The models are executed on DEVS-Suite 2.1.0 and Java 7. Three design experiments
are developed and briefly described below. The first two experiments cover single I/O and multiple I/O
component-based interactions. The third experiment explores interaction methods where data types, timings,
conversions span simulated and cyber-physical entities.
Single vs. Fan-out Signaling – In this experiment, the difference between single hardware port
manipulation and fan-out signaling (multiport manipulation) is under inspection. Phidget hardware is
known to support parallel logic. This experiment is aimed at recognizing whether our real-time simulation,
hardware adapters (DSHAL), and Phidget driver —overall, the cyber side— can leverage this parallelism
and manipulate relays concurrently.
Single and Multi I/O Systems – SISO and MIMO atomic models are designed to support hardware
interaction using primitive data types to eliminate the need for data transformation from complex object
to physical signals, which in this case is electrical current. SISO/MIMO schemes suppose to simplify the
interface between cyber and physical sub-systems. For this experiments two new base classes are developed
which support interaction using double data type.
Mixed Physical and Simulated Relay Execution – In this experiment, the Phidget is transformed
from pure physical system to a mixed simulated/physical system. For this purpose, 3 hardware relays of
the physical Phidget are coupled with a single simulated relay inside the cyber sub-system. In order to
imitate the behavior of the physical relays, the timing data gathered from the hardware ports are fed into
the simulated relay as its timing configuration.
Figure 5 depicts the outline of the mixed physical/simulated relay along with the adapter (DSHAL).
As illustrated in the figure, signals produced by the generator (aimed at simulated relay or physical relay),
all travel the same path to the I/O interface (DSHAL). However, from there, simulated relay signals are fed
back to the outer coupled model to reach the simulated relay while hardware relay signals are forwarded
to the hardware driver. The cardinality of each port is specified in the figure. In order to further mimic the
operation of physical relays, the simulated relay also sends a callback signal to the DSHAL informing the
receipt of signal and state change. Therefore, both generator and simulated relay are simulation models
which communicate indirectly through DSHAL. Also, timing data (both simulated and hardware relays) is
still gathered in DSHAL.

1127

Sarjoughian, Gholami, and Jackson
1. public message out() {
2.
message m = new message();
3.
m.add(makeContent("outport",
4.
new hwBooleanEntity(this.state, this, "outport")));
5.
return m;
6. }
7.
8. public void deltext(double e, message x){
9.
Continue(e);
10.
for(int i = 0 ; i < x.getLength() ; i++){
11.
hwBooleanEntity m = (hwBooleanEntity)x.getValOnPort("inport", i);
12.
if(m.getv()){
13.
turnOn.setExactExecutionTime(switchingSpeed(true));
14.
phase = "turnOn";
15.
}
16.
else{
17.
turnOff.setExactExecutionTime(switchingSpeed(false));
18.
phase = "turnOff";
19. }}}

Figure 6: External transition and output functions for the simulated relay in mixed simulation scheme
Figure 6 is a part of the simulated relay realization in DEVS-Suite. In the external transition function
in line 8, on/off signals are received from the input port and change the phase of the relay. The simulated
relay has two actions: one for switching off and one for switching on (not included in the code snippet).
As mentioned above, the time durations for executing these two actions are read from a configuration file
and stored in the switchingDelays variable. These timings are gathered from real hardware switches (signal
turnaround time) in multiple SISO/MIMO experiments. The data is retrieved by switchingSpeed method and
used for specifying the duration of switching on and switching off actions in lines 13 and 17, respectively.
The output function in line 1 is responsible for sending callback signals to the DSHAL as described above.
Communication between DSHAL and simulated/physical relays is done by hwBooleanEntity which stores
a boolean value, the origin of the signal, and the port with which the signal is transmitted. Alternatively,
the function setExactExecutionTime in lines 13 and 17 can also be tied to a high resolution clock source
in the hardware such as a high resolution timers. The high resolution timers solution can achieve high
resolution microseconds timing data at a stable rate for even greater accuracy. Working directly with raw
system ticks could be problematic since there could be the problem of time drift or jitter.
4.1 Results
For the first experiment (Single vs. Fan-out Signaling) a multiport generator is modeled and simulated
based on ALRT-DEVS with a single output port. This generator is capable of sending complex objects
(port numbers and boolean values) to the adapter. The adapter then discretizes these complex objects into
basic hardware signals and transmits them to the hardware via the Phidget driver. Experiments are done for
1 up to 4-port manipulation, to see the level of parallelism supported by the cyber side. Each experiment
consists of 50 signals and repeated 5 times. As shown in the results in Figure 7-a, there is a millisecond
time difference between one, two, three, and four relay manipulation. Therefore, this proves that although
the hardware side supports parallel port manipulation, the software side is incapable of such task since
the callback signals are received one by one and serviced sequentially. However, this shortcoming is at
the level of software instead of the real-time simulation. Although the simulation side sends signals as
bundle, it receives the callback signals sequentially which shows sequential processing in the software layer

1128

Sarjoughian, Gholami, and Jackson

Figure 7: Experimentation results
(Phidget driver). The diagram shows a steady rise in the numbers and decrease in the difference between
the highest and the lowest turnaround time.
As for the second experiment (Single and Multi I/O Systems) other than the difference in data
transformation which was mentioned above, SISO and MIMO operate quite similar to regular single and
fan-out signaling models. The results retrieved from the SISO experiments were similar to those of single
port manipulation in the previous experiment. In MIMO, at every cycle, a random number between 0 and
4 (both inclusive) is generated which specifies the number of relays to be manipulated and then the state
of the selected relays are altered. As expected, the behavior of all hardware ports are similar. The stock
chart in Figure 7-b demonstrates the results retrieved from this experiment. The differences observed in
the behavior of various hardware ports are the result of unpredictable behavior of the software and the
mechanical part of the relays.
In the third experiment (Mixed Physical/Simulated Relay Execution), A total number of 13 experiments
are carried out on the mixed relay system in which each experiment consists of 50 send/receive signals.
Figure 7-c is a comparison between the average timing behavior of physical relay and the behavior of
simulated relay. The timing differences are small and similar to the timing differences between physical
ports. Figure 7-d visualizes this data in the form of stock chart with maximum, minimum, and median
data for each relay. The simulated port shows similar behavior (similar median and lower/upper bounds)
to the other hardware ports.
5

DISCUSSION

Simulation has demonstrated its use in many areas including support for embedded (software/hardware)
system design. An example is the use of logical-time DEVS simulation for autonomous, intelligent cruise
controller (Schulz et al. 1998). Software/hardware co-design is approached as a process where appropriate
tradeoffs can be studied in a virtual setting to implement the cruise controller. Soft real-time DEVS simulation
is used for analyzing and designing cooperative simulated and actual robots (Hu and Zeigler 2005). Using
this approach DEVS atomic model I/O messages are communicated with the Relay Phidget through a
non-real-time DSHAL (Jackson 2012). Furthermore, in this work and other DEVS-based approaches (e.g.,

1129

Sarjoughian, Gholami, and Jackson
(Cellier and Kofman 2006; Bergero and Kofman 2011)), there is no support for action-level modeling,
simulation is not subject to hard real-time execution, and simulation-software interactions are not separated
as defined in this work.
Researchers in the domain of cyber-physical systems have also developed a variety of model-based
design and simulation approaches and tools. Some challenges (e.g., timing among interacting parts of
CPS, scalability, and reuse) that face development of cyber-physical systems are described. An example is
design contracts where integration of systems can be evaluated in (logical-time and soft real-time) simulated
settings using Ptolemy II (Derler et al. 2013). In another effort, SystemC with timing annotation is used
for virtual prototyping of the network needed for software systems to control physical systems (Zhang
et al. 2013). Simulation is proposed as an attractive alternative to hardware-in-the-loop simulation.
Another trend is to utilize platforms supporting real-time software execution. Use of dedicated hardware
and platform-specific programming languages afford virtualizing realistic dynamics of physical systems.
Simulation models developed in high-level programming languages may be transformed to programming
languages that can execute using real-time operating systems. For example, Ptolemy II models developed
in Java language can be converted to hardware-specific C code. This requires representing the model syntax
and semantics to match a target programming language (Giotto) and OS (FreeRTOS). Other works transform
DEVS models to VHDL (e.g., (Pifer 2012; Molter et al. 2009)). Embedded system simulation has also
been proposed (e.g., (Yu and Wainer 2007)). Furthermore, the importance of hard real-time simulation
of physical systems also attracted researchers to use special-purpose platforms. An example is the use of
FPGA instead of general-purpose platforms for simulating rigid body systems (Bishop et al. 2000). There
exist research in distributed, real-time simulation which fall outside the scope of this paper.
6

CONCLUSION

This paper describes a kind of systems of systems where hard real-time simulations and computationalphysical systems are mirror images of one another. Interactions within simulations are separated from those
with computational-physical systems. A simple scenario is developed where a control switch executes in
the ALRT-DEVS simulation platform while interacting with a Relay Phidget. This role of simulation is
complementary to the logical-time systems of systems simulations and different types of hardware-in-theloop simulations used in cyber-physical systems research. With new advances in computation and hardware
platforms, accuracy and scale of real-time simulation will increase and thus leading to seamless interactions
between simulated and real-world systems. In particular, use of a real-time operating system (RTOS) such
as Wind River VxWorks can increase timing accuracy and offer higher granularity for tasks scheduling.
Another area of interest is for heterogenous simulation models to interact with computational-physical
systems. Distributed real-time simulation with high fidelity (i.e., action level timing) is another research
direction.
REFERENCES
Bergero, F., and E. Kofman. 2011. “PowerDEVS: a tool for hybrid system modeling and real-time simulation”.
Simulation 87 (1-2): 113–132.
Bishop, B., T. P. Kelliher, and M. J. Irwin. 2000. “Hardware/software co-design for real-time physical
modeling”. In ICME, Volume 3, 1363–1366. IEEE.
Cellier, F. E., and E. Kofman. 2006. Continuous System Simulation. Springer.
Cervin, A., and J. Eker. 2005. “Control-scheduling codesign of real-time systems: The control server
approach”. Journal of Embedded Computing 1 (2): 209–224.
Davis, R. I., and A. Burns. 2011, October. “A survey of hard real-time scheduling for multiprocessor
systems”. ACM Comput. Surv. 43 (4): 35:1–35:44.
Derler, P., E. A. Lee, M. Törngren, and S. Tripakis. 2013. “Cyber-Physical System Design Contracts”. In
ACM/IEEE 4th International Conference on Cyber-Physical Systems. Philadelphia, PA, USA.

1130

Sarjoughian, Gholami, and Jackson
Derler, P., E. A. Lee, and A. S. Vincentelli. 2012. “Modeling Cyber–Physical Systems”. Proceedings of
the IEEE 100 (1): 13–28.
DoD 1995. Modeling and Simulation (M & S) Master Plan. Department of Defense.
Douglass, B. P. 2004. Real Time UML: Advances in the UML for Real-Time Systems. 3rd ed. Redwood
City, CA, USA: Addison Wesley Longman Publishing Co., Inc.
Gholami, S., and H. S. Sarjoughian. 2012. “Real-time Network-on-Chip Simulation Modeling”. In SIMUTools, 103–112. Desenzano, Italy: ICST.
Gholami, S., and H. S. Sarjoughian. 2013. “Observations on Real-time Simulation Design and Experimentation.”. In SpringSim Multi-Conference, 1–8. San Diego, CA, USA: SCS.
Giese, H., and S. Burmester. 2003. “Real-time Statechart Semantics”. TR-RI-03-239, University of Paderborn.
Hong, J., H. Song, T. Kim, and K. Park. 1997. “A real-time discrete event system specification formalism
for seamless real-time software development”. Discrete Event Dynamic Systems 7 (4): 355–375.
Hu, X., and B. P. Zeigler. 2005. “A simulation-based virtual environment to study cooperative robotic
systems”. Integrated Computer-Aided Engineering 12 (4): 353–367.
Jackson, T. 2012. “DEVS Hardware In The Loop (HIL) Mixed Mode Simulation with Bi-Directional Support:
Modeling and Simulation and Application”. Applied Project, Master’s of Engineering, Arizona State
University, http://acims.asu.edu.
Jamshidi, M. 2010. Systems of Systems Engineering: Principles and Applications. CRC Press.
Miller, S., and J. Wendlandt. 2010. “Real-Time Simulation of Physical Systems Using Simscape”. MATLAB
News and Notes.
Molter, H. G., A. Seffrin, and S. A. Huss. 2009. “DEVS2VHDL: Automatic transformation of XMLspecified DEVS Model of Computation into synthesizable VHDL code”. In Forum on Specification &
Design Languages, 1–6. IEEE.
OMG 2004. “2.0 Superstructure Specification”. OMG, Needham.
PhidgetInterfaceKit-0/0/4 2013. “Products for USB Sensing and Control”. http://www.phidgets.com/.
Pifer, T. J. 2012. “DEVS-Based Hardware Design, Synthesis, and Power Optimization Using Explicit Time
Specifications and Deterministic Path-Based Latency”. Master’s thesis, University of Arizona.
Sarjoughian, H. S., and S. Gholami. 2013. “Action-level Real-time DEVS Modeling and Simulation.
Submitted”. ACM Transactions on Modeling and Computer Simulation submitted.
Schulz, S., J. W. Rozenblit, M. Mrva, and K. Buchenriede. 1998. “Model-based codesign”. Computer 31
(8): 60–67.
Yu, Y. H., and G. Wainer. 2007. “eCD++: an engine for executing DEVS models in embedded platforms”.
In Proceedings of the Summer Computer Simulation Conference, 323–330. SCS.
Zeigler, B. P., T. G. Kim, and H. Praehofer. 2000. Theory of Modeling and Simulation. 2nd ed. Orlando,
FL, USA: Academic Press, Inc.
Zhang, Z., E. Eyisi, X. Koutsoukos, J. Porter, G. Karsai, and J. Sztipanovits. 2013. “Co-Simulation Framework
for Design of Time-Triggered Cyber Physical Systems”. In Proceedings of the 4th International
Conference on Cyber Physical Systems. Philadelphia, PA, USA.
AUTHOR BIOGRAPHIES
HESSAM S. SARJOUGHIAN is Associate Professor of Computer Science at ASU and Co-Director of
ACIMS. He can be contacted at hsarjoughian@asu.edui.
SOROOSH GHOLAMI is a PhD student in Computer Science at ASU. He can be contacted at hsoroosh.
gholami@asu.edui.
THOMAS JACKSON holds a Masters of Engineering from ASU. He is a senior Member of Technical
Staff at Wind River Systems Inc., a wholly owned subsidiary of Intel Corporation. He can be contacted at
htom.jackson@windriver.comi.

1131

J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Quantization-based ﬁltering in distributed
discrete event simulation$
Bernard P. Zeigler*, Hyup J. Cho, Jeong G. Kim,
Hessam S. Sarjoughian, and Jong. S. Lee
Department of Electrical and Computer Engineering, Arizona Center for Integrative Modeling and
Simulation, University of Arizona, Tucson, AZ 85721, USA
Received 14 July 2000; received in revised form 2 February 2001; accepted 11 March 2001

Abstract
Quantization is an approach to distributed logical simulation in which the value space is
quantized and trajectories are represented by the crossings of a set of thresholds. This is an
alternative to the common approach which discretizes the time base of a continuous trajectory
to obtain a ﬁnite number of equally spaced sampled values over time. In distributed
simulation, a quantizer checks for threshold crossings whenever an output event occurs and
sends this value across to a receiver thereby reducing the number of messages exchanged
among federates in a federation. This may increase performance in various ways such as
decreasing overall execution time or allowing a larger number of entities to be simulated.
Predictive quantization is a more advanced approach that sends just one bit of information
instead of the actual real value size with the consequence that not only the number of
messages, but also the message size, can be signiﬁcantly reduced in this approach. In this
paper, we present an approach to packaging individual bits into a large message packet, called
multiplexed predictive quantization. We demonstrate that this approach can save signiﬁcant
overhead (thereby maximizing data transmission) and can reach close to 100% efﬁciency in the
limit of large numbers of simultaneous message sources encapsulated within individual
federates. We also discuss the tradeoff between message bandwidth utilization and the error
incurred in the quantization. The results relate bandwidth utilization and error to quantum
size for federations executing in the HLA-compliant discrete event distributed simulation
environment, DEVS/HLA. The theoretical and empirical results indicate that quantization
can be very scaleable due to reduced local computation demands as well as having extremely
favorable network load reduction/simulation ﬁdelity tradeoffs.
r 2002 Elsevier Science (USA). All rights reserved.

$

This work was supported in part by the Advance Simulation Technology Thrust (ASTT) DARPA
Contract N6133997K-0007 and partially supported by National Science Foundation Grant: ‘‘DEVS as a
Formal Modeling Framework for Scaleable Enterprise Design.’’
*Corresponding author.
E-mail addresses: zeigler@ece.arizona.edu (B.P. Zeigler), hjcho@ece.arizona.edu (H.J. Cho),
jkkim@ece.arizona.edu (J.G. Kim), hessam@ece.arizona.edu (H.S. Sarjoughian), jslee@ece.arizona.edu
(J.S. Lee).
0743-7315/02/$ - see front matter r 2002 Elsevier Science (USA). All rights reserved.
PII: S 0 7 4 3 - 7 3 1 5 ( 0 2 ) 0 0 0 0 2 - 3

1630

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1. Introduction
A quantizer is a form of signiﬁcant event detector. It monitors its input using a
base value and a relative threshold quantum so that whenever the input crosses its
upper or lower threshold boundary, it generates the appropriate output message.
The quantum size is a measure of how big a change must be to be considered
signiﬁcant. Rather than represent a continuous curve by points sampled at regular
time intervals, the curve shown in Fig. 1 is represented by the crossings of an equally
spaced set of boundaries, separated by a quantum size, D: The quantization of an
n-dimensional real valued space can be done in many ways. One way is through the
independent quantization of each dimension.
The baseline mechanism for quantization, called non-predictive quantization, is
illustrated Fig. 2. We assume a sender federate1 is updating a receiver federate on a
numerical, real-valued, state variable (dynamically changing attribute), V : In the
non-predictive approach, a quantizer is applied to the sender’s output which checks
for threshold (boundary) crossings whenever a change in V occurs. Only when such
a crossing occurs, is a new value of V sent across the network to the receiver. This
approach is relatively easy to apply and requires minimal restructuring of the
federates’ state computation processes. However, it can incur loss of accuracy due to
the receiver’s diminished state updates and this may propagate in a global error due
to feedback between sender and receiver.2
Our focus in this work is on the use of quantization to ﬁlter messages and thereby
reduce trafﬁc over the network supporting a distributed simulation. The intended
application is to discrete event simulation working in logical time in which trafﬁc
load on the network shows up as increased execution time due to collisions, queuing
and retransmission attempts. This focus on network throughput contrasts with the
situation in real time distributed interactive simulation in which latency caused by
trafﬁc congestion as well as physical delays in the underlying medium is the major
concern. Moreover, we assume fully reliable message delivery as commonly assumed
in distributed discrete event simulation studies and as provided by the HLA RTI
time-management services for logical time simulation. The basic idea in quantization
is simple—the larger the quantum size, the fewer the messages sent over the network.
However, a larger quantum size may cause larger errors because receiving federates
are employing saved values that are not up-to-date. Therefore, the size of the
quantum must be determined through an error analysis of related models. In recent
work we have developed a theory of quantization that takes into account the global
error due to error propagation [2,7,8,11]. Previous analyses of predictive contracts’
accuracy/performance tradeoffs have assumed that open loop analysis carries over to
the closed loop case (e.g., [1]). Unfortunately, experience in numerical analysis
suggests that the dynamics of feedback interaction may cause errors generated to
grow without bound. The theory of quantized systems we developed provides
conditions, based on a formulation of input sensitivity, under which homomorphic
(error-free) quantization-based ﬁltering is possible. It shows how error can be
generated if the conditions are violated and formulates a suitable concept of
1
We use high level architecture (HLA) terminology where a federate is a participating element in a
distributed simulation called a federation.
2
In classical dead reckoning for distributed simulation [3], quantization is applied to the errors in the
positions, velocities and accelerations continually output by an approximate representation of a federate’s
high-ﬁdelity model of motion. Quantization as discussed here does not employ such approximation models
as in dead reckoning so that a direct comparison is not possible. For a framework in which dead
reckoning, quantization and other schemes are compared please see ‘‘Predictive Contract Methodology
and Federation Performance,’’ SIW, September, 1999.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1631

5D

Variable, V

4D
3D
2D
D

T

2T

3T
Time

4T

5T

6T

Fig. 1. Two approaches to digital computer simulation: discretizing time and quantizing variables.

Sender

Receiver

Quantizer

Fig. 2. Non-predictive quantization.

approximate homomorphism. We have veriﬁed the theory when applied to
quantization of arbitrary ordinary differential equation models [8,9].
In this paper, we take a stochastic process approach to analysis of message
reduction and error tradeoff in quantization-based ﬁltering. It turns out that
stochastic renewal theory can throw some light on this tradeoff that supplements the
previous results based on deterministic analysis [8,9]. In addition, we develop more
advanced forms of quantization, called predictive quantization and discuss an
implementation which applies to simulations with large numbers of entities
exchanging state-update messages. Finally, we present the results of experimentation
with actual distributed simulations and show that they conﬁrm the theoretical
predictions.

2. Multiplexed predictive quantization
A more efﬁcient form of quantization is predictive quantization, as illustrated in
Fig. 3. Here the sender employs a model to predict the next boundary crossing and

1632

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Sender

(t1,1)

t1

t2

(t2,1) (t3,1)

Receiver

t3

Fig. 3. Predictive quantization.

time; this crossing will occur given its current state. This approach is inherently event
based since the sender waits until the predicted next event (boundary crossing) time
before updating its state. Since the federate need not be computing state changes
during this waiting period, this approach has a computational advantage over nonpredictive quantization. Further, since the next boundary crossing is either one
above or one below the last recorded boundary, the sender need not send the full
ﬂoating point (double word) value to the receiver. Indeed, assume that the receiver
keeps track of the last boundary and knows the quantum size. Then only one bit of
information is required—for example, þ1 indicates adding the quantum to the last
boundary, while 1 indicates a similar subtraction. Thus, not only the number of
messages but also the message size—hence, total number of bits transmitted—can be
signiﬁcantly reduced in this approach. Compared to quantized time-stepped
simulations, there is an additional overhead in predictive quantization since
messages must convey the time of boundary crossing in addition to incremental
values. However, messages in most discrete event logical time simulations are time
stamped by default anyway. Moreover, if simple predictive models are used,
predictive quantization can also greatly reduce the execution time of the sender’s
state transition computation. For example, a linear ﬁrst-order approximation to an
integrator is discussed in Ref. [11]. It enables any differential equation system to be
simulated in distributed form using predictive quantization with error of
approximation determined by the quantum size.
To exploit the reduction in the size of an update afforded by predictive
quantization (from say 64 bits to 1 bit) is not straightforward due to the
accompanying information that is also required. For example, the HLA runtime
infrastructure (measured using RTI utilities) uses 20 bytes for each message to carry
information such as the object ID, attribute ID, and time stamp. Moreover, in the
RTI, 1 byte ð8 bitsÞ is the minimum size for the update value in a message. Since our

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Federate A

value

Federate B

2bits for
active/+1 /-1

model A1
PQ

model B1
model M’

model M

model A2
value

PQ

±D

value

±D

model B2

value

network

model An-1

value

1633

±D

PQ

model Bn-1
1

value

±D

2bits for
active/+1 /-1

model An

value

PQ

model Bn

value

Fig. 4. Efﬁcient implementation of the multiplexed predictive ﬁltering scheme.

predictive quantization scheme was implemented over HLA, we developed an
approach to take into account the HLA overhead while exploiting the reduced data
size. However, the approach and analysis to be given below are generally applicable
since they assume only that a message contains both overhead bits and data bits.
Such a scheme can take advantage of the fact that, in simulations with a large
number of entities, there will be many entities assigned to each federate. As shown in
Fig. 4, consider two federates, sender and receiver, that encapsulate a large number
of similar component models, e.g., A1 ; y; An ; in Federate A: In a pursuer–evader
example to be discussed later, the components are moving entities such as tanks.
Assume that each of these components has a predictive quantizer to produce a 1-bit
output of a variable (e.g., its position). Then at each event time, several components
will be crossing their boundaries. We call a component active at a given event time if
it has a boundary crossing at that time. Then we can encode the joint output of the
federate into a single message with 2 bits of information for each component; 1 bit
for active/inactive status and for active components, the bit that encodes the nature
of the incremental boundary crossing. At the receiver side, this multiplexed packet is
decoded in inverse fashion using a set of ghost components in one-to-one
correspondence with the sending components. Each pair of bits is examined. If the
ﬁrst bit of the source indicates active, then the receiver updates the appropriate
variable of the counterpart (ghost) with the predeﬁned quantum size, incrementing
the saved value of the tracked variable by the quantum in the direction indicated by
the second bit. Of course, sending and receiving federates must know the shared
value of the quantum size and be informed of the new value, should it be changed (in
a related article we consider dynamic quantization).
Provided a federate has a large enough population of active components, close to
100% efﬁciency can be achieved for multiplexing relative to non-multiplexed
quantization.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1634

Table 1
Specialization of quantization schemes
Scheme

Predictive dimension
Non-predictive
(send real value: 64 bit)

Multiplexed Non-multiplexed
Non-predictive
dimension
(1 message per output
at a time instant)
Multiplexed
Multiplexed non-predictive
(1 message for all
(send real value: 64 bit)
component outputs at a time instant)

Predictive
(send a change: 1 bit)
Predictive

Multiplexed predictive
(send a change+
identity/output: 2 bit

Table 1 outlines the quantization schemes as we shall discuss them.3 We consider
the ratio of the message size needed for multiplexed predictive quantization to the
number of bits needed for non-multiplexed quantization with the same number of
components. Let SOH be the number of overhead bits for a packet, SD ; the nonquantized data bit size, and Ncomp ; the number of components of which a fraction, a
are active. While at each global event, the multiplexed cases always send the same
number of bits, the non-multiplexed cases send a fraction a of (larger) messages.
Then, for updating a single real-valued attribute, the ratio of multiplexed to nonmultiplexed bits required is
ðSOH þ 2Ncomp Þ=ðSOH þ SD ÞaNcomp
which approaches
2=aðSOH þ SD Þ

for Ncomp cSOH :

For example, for a ¼ 1 (all components are active) the ratio becomes less than 1%
with SOH ¼ 160 bits ð20 bytesÞ; SD ¼ 64 bits ð8 bytes double precision real number) and Ncomp ¼ 1000: If a ¼ 0:1 (10% active), the bit ratio is still very favorable
under 10%. A more complete analysis is given in Table 2 where we consider all four
combinations of quantization and multiplexing. As expected, due to the overhead,
predictive quantization without multiplexing can afford only limited advantage
(approx., 25% reduction in network load relative to non-predictive quantization). If
we multiplex the non-predictive quantization scheme (i.e., combine the actual
outputs of components into one message) we obtain greater reduction (approx.
70%=a) due to the spreading of overhead over large numbers of data elements.
However, the greatest advantage, by far, is obtained from the multiplexed predictive
quantization which fully exploits the reduction in data requirements to 2 bits per
component. To reduce the bit sending ratio below 10% however, requires at least
10% active components. In our simulations, we employ a time interval, called a time
granule, which enables boundary crossings within the time granule to be considered
simultaneous [2]. The time granule can be set to increase the active percentage a. In
our experiments this granule was set to 0.001 so as to represent a precision of 3
decimal places in the time stamp.4
3

Multiplexing as considered here is related to, but differs from, the concept of ‘‘bundling’’ as
implemented in the HLA/RTI. In bundling, the RTI is given a period during which to accumulate
messages to be sent together. In multiplexing, we encode all messages from a federate emitted at one time
into a newly created message. The effect of improving performance by reducing overhead is similar, but
multiplexing actually reduces data trafﬁc while bundling does not.
4
The time granule is typically much smaller than the quantum size so that error introduced due to time
equivalencing can be taken to be a second-order effect relative to that introduced by quantization.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1635

Table 2
Network load (bandwidth) requirements for quantization schemes
Scheme

# bits required
for Ncomp

Ratio to non-predictive
quantization for large Ncomp

Ratio for Ncomp ¼ 1000
SOH ¼ 160 bits
SD ¼ 64 bits

Non-predictive
quantization

aNcomp ðSOH þ SD Þ

1

1

Predictive quantization
(non-multiplexed)

aNcomp ðSOH þ 1Þ

ðSOH þ 1Þ=ðSOH þ SD Þ

0.74

Multiplexed nonpredictive quantization

ðSOH þ SD Ncomp Þ

SD =a ðSOH þ SD Þ

0:28=a

Multiplexed predictive
quantization

ðSOH þ 2Ncomp Þ

2=a ðSOH þ SD Þ

0:0096=a

3. Pursuer–evader model quantization tradeoff analysis
Fig. 5 depicts the pursuer/evader coupled model, expressed in the DEVS formalism
[8], that was used to evaluate the behavior of predictive and non-predictive
quantization. Each pursuer–evader pair has as components a pursuer and an evader
that in turn are coupled models of a tank and a driver with simple rules for pursuit
and evasion.
Each component of a pursuer/evader pair maintains a proxy (ghost) of the other’s
tank. The ghost receives quantized updates of the other side’s actual tank position.
The driver of the pursuer computes the distance between position of its own tank
and that of the evader (via the ghost). It uses rules based on this value (discussed in a
moment), to decide whether or not to give chase to the evader. Position variables are
declared as attributes that belong to Tank object (BlueTank or RedTank). The state
quantizer synchronizes the values of position attributes between tanks and their
remote ghosts.
Fig. 6 illustrates the motion trajectories of the pursuer and evader in a
single dimension while their state transition rules are depicted in Fig. 7. Initially,
the pursuer is placed at a point at a distance given by the parameter, Far, behind
the evader. The evader waits until the pursuer approaches to within a distance
given by Near before running away from the pursuer. It stops upon putting distance Far between it and the pursuer. Likewise, the pursuer starts to
chase when the distance exceeds Far and stops when the distance has closed to
Near.
To enable experimental study of quantization in a distributed simulation setting
we developed a DEVS/HLA [2,7] distributed simulation with two federates each
capable of holding an arbitrary number of pursuer/evader pairs. A stochastic
formulation enables independent sampling for initial states and boundary crossings
based on quantum size. The stochastic model, to be described in a moment, is
consistent with a deterministic predictive quantization model perturbed by small
uniform noise. An experiment consisted of a number of randomly initialized
identical pairs simulated for the same simulation time with the same quantum size.
In successive experiments, the number of pairs was increased so as to approach
network saturation conditions.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1636

Purser

Evader

otherX

Red Tank
Ghost
position

State
quantizer

Red Tank
myX

position
runAway

driver
chase

myX

Blue Tank
Ghost
position

BlueTank
position

driver

otherX

State
quantizer
Fig. 5. Pursuer–evader model and pairs.

time
Pursuer

Near
Evader
Far

distance
Fig. 6. The trajectories of the pursuer and evader.

distance = Evader_Position -Pursuer_Position

start

distance ≤ Near
Pursuer_Chase,
Evader_Stop

Pursuer_Stop,
Evader_RunAway
distance ≤ Far

distance > Near

Evader_Position ≤ Observ_distance

distance < Far

Pursuer_Stop,
Evader_Stop
Fig. 7. State transition diagram for pursuer–evader model.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

x
1

x
2

x
3

1637

xn

SN(t)

Error
(Y(t))
y

N(t)

×

×

×

×

N(1)

N(2)

N(3)

N(n)

t

×

observer
xi: random number with mean µ and variance
N(t) : the number of events by time t
S():time of the nth event

σ2

Fig. 8. Stochastic formulation of pursuer/evader model.

The stochastic representation of the pursuer/evader simulation model is illustrated
in Fig. 8. Incremental steps of a pursuer or evader are represented by a random
variable, X ; following the normal distribution. Consider the evader moving from
Near to Far relative to the pursuer; the evader triggers the pursuer after having
passed the Far threshold of the latter.
To analyze this example, consider motion with mean speed v > 0 and updated with
a ﬁxed time step of size D: The resulting summation of incremental advances in one
direction yields the total distance traveled at any time. This summation can be
represented by a renewal process [5] as deﬁned in the appendix. The incremental
distribution underlying the renewal process has mean increment, m ¼ vD and we
assume that its variance is small enough to ensure that increments are never negative.
To go from one quantum threshold to the next, the mean number of steps taken is
that required to traverse a distance D; i.e., D=m: During this number of steps there
are no messages transmitted from one agent to the other and this is followed by one
message when the next quantum boundary is crossed. Thus the ratio of reduction in
messages sent is m=D: Note that to be meaningful, mpD; since otherwise, a single
step can cross multiple quantum boundaries and there is no reduction in messages
sent. To assure this condition, we must choose the time step small enough, namely,
DpD=v: The smaller the time step, D (below the lower limit), the smaller the
increment, m; and the larger the message reduction for a given quantum size D: This
is illustrated by the descending message curve in Fig. 9.
One way to measure error is the delay in notiﬁcation to the pursuer, when an
evader crosses the far distance threshold (Fig. 8). The same error holds for late
notiﬁcation of the pursuer crossing the near distance threshold to the evader. From
renewal theory, the mean overshoot of any increment over any given threshold level
is approx. m=2 for small enough variance. So measured in this way, the error is
approx. m=2v ¼ D=2:
To simplify our experiments we normalized v ¼ 1 set m ¼ D ¼ D: In this case, the
expected message reduction ratio is 1=D while the temporal error due to late
notiﬁcation is approximately D=2 as in Fig. 9.5

5
Note that this does not contradict the general independence of overshoot error on quantum size. Here,
the notiﬁcation delay dependence on D is a result of the constraint, m ¼ D:

1638

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

# Messages
error

1
D
Error ≈ 0.5 * D

1

D
Fig. 9. Message reduction/error tradeoff.

The resulting tradeoff between message reduction and error increase is illustrated
in Fig. 9. The number of messages exchanged decreases inversely with quantum size
while the error grows linearly with quantum size. For example, by doubling the
quantum size, half the messages can be ﬁltered while the error doubles. However,
since the extent of the motion is at least 100, a quantum size D ¼ 2 introduces a 1%
error, which may be well within the acceptable level.
The above error characterization holds for both forms of quantization. However,
the lateness error in predictive quantization may actually be reduced to zero under
special circumstances, namely, when the quantum boundaries of the sender line up
exactly with the sensor boundaries of the receiver. In the present example, this occurs
when ðFar  NearÞ=D is an integer. In this case, the pursuer is triggered by an evader
update exactly at its decision threshold (and conversely).
The integration of motion is a special case of a quantized integrator where
the output is ﬁltered through a quantizer with quantum size D: Late notiﬁcation,
as the error criterion, relies on the special structure of the pursuer/evader
example where agents are insensitive to inputs except when thresholds are
crossed. Rather than assume this special structure, the error in a general quantized integrator is better represented as a measure of the average discrepancy
between the last value transmitted (a quantum level) and the current value of the
actual output. In this case, the error turns out to be approximately D=2: We provide
an informal proof that has been shown formally using the renewal theory results in
[5].
As before the number of steps in going from one quantum level to the next is
N ¼ D=m: On step i; the difference between the actual output and the last threshold
is im: This difference persists for the duration D: Accumulating the resulting
increments, imD; from i ¼ 0 to N; we have the total error of approx. N 2 mD=2:
Dividing by the duration of the traversal, ND; we have
Erroravg DNm=2 ¼ D=2:

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1639

Federate 2

Federate 1
Pursuer n

Evader n

Pursuer 1
Red Tank Endo

State
quantizer

Evader 1
Red Tank

myX

otherX

position

position
runAway

driver
chase

myX

Blue Tank

Blue Tank Endo

position

position

perceiver

otherX

State
quantizer

DEVS/HLA

(Network)

Fig. 10. Multiple pursuer–evader model.

4. Experimental results
As already indicated, to test the predictions just discussed, we developed a DEVS/
HLA model with two federates each capable of holding an arbitrary number of
matched pursuer–evader pairs (Fig. 10). An experiment consisted of a number of
randomly initialized identical pairs simulated for the same observation time with the
same quantum size. As discussed before, we set the speed of motion in each
encounter to unity so that time and space are measured in the same way and each
step represents the crossing of a quantum boundary with ﬁxed quantum size, D and
speed normally distributed NðD; sÞ; with mean, D (quantum size), and standard
deviation, s ¼ 0:3; where D was parameterized in the range ½0; 100: The Far and
Near values are 100 and 10, respectively. Pursuers are all placed on one federate
using a DEVS coupled model. Similarly, evaders are on a second federate. Various
platforms (NTs and UNIXs) were used on local and wide area networks. In
successive experiments, the number of pairs was increased so as to approach network
saturation conditions.
The test was usually executed during night (7 p:m:–2 a:m:) to avoid abrupt heavy
load which may happen during day time. However, 9–11 a:m: and 4–9 p:m: were
used for the WAN test between Sunnyvale, CA and Tucson, AZ.
The number of messages and execution time (Fig. 11a,b) are reduced inversely
proportional to the increase of the quantum size as predicted earlier.
As shown in Fig. 11c, the late notiﬁcation error also increases linearly as predicted
earlier. However, the observed error differs slightly from the approximated value of
D=2: For example, with the quantum size 20 the observed error is 10.65, as opposed
to D=2 ¼ 10: To explain this discrepancy, we ﬁrst tried to take into account the nonzero variance which was actually employed. The error derived from renewal theory
in the appendix can be expressed as
Late notification error ¼ 0:5ðD þ s2 =DÞ:
For example, with D ¼ 20 and s ¼ 0:3; the predicted error is 10.0023 which is still
less than the observed value. We conjecture that the remaining difference is due to

1640

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Fig. 11. Number of messages, execution time and error for pursuer/evader pairs (non-predictive case): (a)
number of messages vs. quantum size (for position); (b) execution time vs. quantum size; (c) error vs.
quantum size; (d) WAN, LAN(workstation–workstation), and LAN(PC–PC) cases.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1641

the fact that the number of events is ﬁnite in the experiment, but is assumed to
become inﬁnite in the analysis.
Fig. 11d shows that execution time through wide area network (WAN) between
University of Arizona (Tucson, AZ) and Lockheed-Martin (Sunnyvale, CA) is
almost 3 times greater than that through LANs between sites within the University
of Arizona—100 MB (dedicated) and 10 MB (shared).
Fig. 12 illustrates heavy load test with 1000–5000 pursuer/evader pairs with nonpredictive quantizers. The execution time increases exponentially as the number of
messages increases, while error is approximately half of the quantum size. The
number of pairs of the pursuer–evader is proportional to the number of messages
since each pair generates approximately the same number of messages.
Fig. 12a shows that the number of messages generated for the same simulation run
decreases sharply as the quantum size is increased, while the error grows linearly.
The increase in execution time with decreasing quantum size is non-linear
demonstrating the expected approach to saturation (Fig. 12b). Meanwhile, the
message rate increases linearly with the numbers of pairs as in Fig. 12c.
Fig. 12d illustrates the utility of the quantization approach to executing
large simulations with many entities. As the number of pairs (or message rate)
increases the execution time increases non-linearly for each quantum size. Given a
bound on the execution time available, the number of pairs that can be
accommodated within that time, increases more than linearly as the quantum size
is increased. This is especially true for the increase from 1 to 5 when operating at
network saturation levels. When there are numerous messages or long distance data
transfer on the network, the delay time for data transfer is seriously prolonged. Thus
quantizers can reduce the number of messages signiﬁcantly within the allowable
error tolerance.
As mentioned, predictive quantization makes it possible to reduce the information
sent about a boundary crossing to 1 bit—indicating whether the boundary crossed is
one above or one below the current threshold. But, given the overhead bits that must
be included in a packet, reducing the payload from 64 bits to 1 may not produce a
signiﬁcant overall reduction. However, when large numbers of entities reside on each
federate, it is possible to package their reduced outputs together into a single
packet and exploit the message size beneﬁts of predictive quantization as suggested
earlier.
In the implementation in DEVS/HLA, the boundary crossing data are encoded
into bit sequences and sent across the network using HLA interactions. For
convenience, integer sized blocks ð4 bytesÞ were used. Thus, a one-integer (32 bits)
payload can accommodate 16 components (2 bits per component), a two-integer
payload can accommodate 32 components, and so on. We experimented with
payloads ranging from 1 to 10,000 integers, representing from 16 to 160,000 pairs of
components, respectively. The experiments executed 100 DEVS simulator cycles. In
the baseline case, this represents transmission of 100Ncomp messages, each with an
8 byte payload (double real), where Ncomp is the number of components employed in
the experiment. In the comparable multiplexed case every run requires 100 messages
(independent of Ncomp ), where each message has a payload size which is linear in
Ncomp :
Fig. 13 shows the results obtained using Unix workstations connected via an
Ethernet network. Clearly, as Ncomp increases, the execution time of the nonpredictive quantization baseline increases in an exponential manner (due to
saturation of the network). In contrast, the execution time of the multiplexed case
is ﬂat until 16,000 components and then starts to increase. Moreover, the execution
time of the latter is uniformly less than the baseline, indicating an extremely positive

1642

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Fig. 12. Heavy load test for LAN between departments (non-predictive case): (a) execution time vs.
number of messages; (b) number of messages vs. quantum size; (c) number of messages vs. number of
pairs; (d) execution time vs. number of pairs.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1643

Fig. 13. Pursuer–evader results (non-predictive vs. multiplexed predictive).

beneﬁt/cost ratio for the additional local computation incurred.6 It should be noted
that there was a high level of activity in the experiments—the best case scenario for
multiplexing, but the large improvement in execution time that we observed indicates
a robust advantage for the multiplexer. As before, let a be the average percent
activity (relative number of active components) of a block over a run. Then for Ncomp
components, the multiplexer must work with Ncomp inputs. However, the effective
number of components for the baseline case is aNcomp in terms of execution time.
The ﬂat nature of the multiplexer indicates that, even though the baseline execution
time is reduced, it still exceeds that of the multiplexer for a as small as 0:1%:
Although not shown, we also measured the performance of the multiplexed nonpredictive case and found that it showed behavior much closer to the non-predictive
case than to the multiplexed case. This is in agreement with the analysis given earlier
where the activity fraction is taken to be less than 0.3.

5. Discussion and conclusions
Formulation of quantization in terms of stochastic processes allowed us to use
renewal theory to obtain simple expressions for the dependence of message reduction
and error on quantum size. In the pursuer/evader case, the output message rate is
inversely related to the quantum size and the average error is half the quantum size.
This represents a highly favorable performance/accuracy tradeoff relation for
trajectories that range over 100 times the quantum size or greater. Experimental
observations conﬁrmed these results. However, the error characterized in these
studies is local and does not directly inform us of the global error. In the
6
In our initial implementation, the local computation required for multiplexing added signiﬁcant
overhead to the overall simulation execution time causing the multiplexing scheme to be inferior to nonpredictive quantization. Subsequently, an efﬁciently encoding and decoding approach was devised and
implemented, leading to the results reported here.

1644

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

pursuer/evader example, the error, taken as the difference between actual positions
and true positions, accumulates linearly with each pursue/evade cycle. However, in
this example, the absolute locations of the pursuer and evader may not be important,
and one may be concerned only that the relative distances between the actors is
preserved. It can be shown that the average distance between pursuer and evader
over a cycle remains close to the expected value ðFar  NearÞ=2 independently of
quantum size. Thus using an error that relates to average separation distance, the
tradeoff relation is extremely favorable since one can reduce message rate without
introducing error. It should be noted that in our experiments, pursuer/evader pairs
were independent. We are currently experimenting with simulations in which pursuers
and evaders interact in a many-to-many manner. Results from other experimental
studies suggest that global error may increase more rapidly than linearly with quantum
size and that the message rate may decrease slower than inversely with quantum size,
but savings for a given error tolerance can nevertheless be very signiﬁcant [6].
Predictive quantization has been shown to reduce the size of messages as well as
their rate. It sends just a 1 bit transmission between federates, while non-predictive
quantization transmits the actual value size. Thus, not only the number of messages
but also the message size can be signiﬁcantly reduced. Furthermore, we presented an
approach to multiplexing individual bits into a single message packet. We
demonstrated that this approach can save signiﬁcant overhead and can reach close
to 100% efﬁciency in the limit of large numbers of components within federates.
Experimental measurements conﬁrmed the predictions, with special signiﬁcance
emerging for the heavy load conditions. Under such saturation conditions, which
were created with large numbers of communicating entities, execution time rose
sharply with increasing message rates. Increasing the quantum size can bring
operation back to the unsaturated region and signiﬁcantly reduce execution time.
However, if the error incurred with such a quantum sizes is unacceptable, the
predictive quantization multiplexing scheme can be invoked. The experiments
showed that the latter approach can greatly reduce the network data load and delay
the onset of saturation where execution rises rapidly with increasing load. Thus,
predictive multiplexing allows either signiﬁcantly larger numbers of entities or smaller
quantum sizes (hence reduced error) than does the non-predictive quantization.
A limitation of the experiment reported here is that it was limited to onedimensional spatial interaction. In a recent work, to be presented in another forum
[4,10], we generalized the single-dimensional approaches discussed here to threedimensional representations. Pursuer/evader pairs were cast as interceptor and threat
missiles, respectively, with realistic three-dimensional dynamics based on prototypes
supplied by Raytheon Missile Company, Tucson. Predictive integrators were used to
exchange position updates between missiles which fed into internally encapsulated
acceleration and velocity integrators. In predictive quantization of each position
coordinate, there are only three possibilities to send f1; 0; 1g where 0 indicates no
change and f1; 1g denote as before the crossing of the next quantum threshold
below and above the current one, respectively. Whenever there is a quantum change
event in at least one of the three spatial coordinates, the full position vector is
updated. However, since there are only 33 ¼ 27 combinations of the base value set,
f1; 0; 1g; only 5 bits are needed to encode these combinations.7 Using such
encoding and decoding for single pursuer/evader pair missiles, as well as
multiplexing for multiple pursuer/evader pairs, we obtained results fully in line with
7

In general, using predictive quantization, one can multiplex any number of attributes which might
change together, each of which is described by the base value set f1; 0; 1g; using a binary coding scheme
which employs far fewer bits than the original double-valued representations.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1645

those presented here. For example, total data bit transmission could be reduced up
to 70 times in single pair. Multiplexing the predictive integrators in large federates
with 80 missile pairs reduced total execution time by over 10,000 compared to nonmultiplexed, non-predictive ﬁltering. This performance gain came at a cost of error
accumulation in the range of 2–3% which was adequate for the problem studied.
Another limitation of this article’s experiment is that it deals with only two
federates each of which contain a number of components of the same type. In
general, there can be a large number of different model components distributed
among a number of federates. This more general case is addressed in [4] where a
generic scheme for multiplexing and distributing messages from one federate to
others is presented. It employs the capabilities of DEVS coupling to direct the
messages from senders to receivers (in DEVS/HLA this coupling information is
automatically mapped into appropriate services of the HLA-RTI) [7–9,12].
A second conﬁrmation of the applicability of the quantization technique was
recently reported in [3]. Here the pursuer/evader quantization framework was
applied to realistic a three-dimensional spatial scenario georeferenced in a
geographical information system (GIS) representation in which a helicopter outﬁtted
with radar pursues a tank. Quantization is applied to the radar emissions exchanged
between the two components, both of which have realistic radar behavioral
characterizations. The experiments produced network trafﬁc/simulation ﬁdelity
tradeoffs similar to those reported here. Other applications being studied include
supply chain modeling, data updating for weather forecasting, real-time stock
information reporting and image video transmission.
The theoretical and empirical results indicate that quantization, especially in
multiplexed predictive form, can be very scaleable due to reduced local computation
demands as well as having extremely favorable network data load reduction/
simulation ﬁdelity tradeoff. In our experience, however, care must be taken to provide
efﬁcient implementation of the local computation algorithms needed to achieve
message reduction, lest the overall performance be degraded rather than improved.
Further the effect of ﬁltering on simulation ﬁdelity is not easily predicted in complex
interactive simulations. Therefore the need is apparent for continued research and
development of predictive contract and other performance enhancing approaches.

Appendix. A brief review of some results of renewal theory
A renewal process is a natural generalization of the Poisson process, in which the
exponential distribution governing the interarrival times of successive events is
generalized to an arbitrary distribution. Let fXn ; n ¼ 1; 2; yg be a sequence of nonnegative independent random variables, called increments, with a common
distribution F such that F ð0Þ ¼ PfXn ¼ 0go1: We interpret Xn as the time between
the ðn  1Þth and nth event. The mean F is m and the variance is s2 : Formally,
Z N
x dF ðxÞ;
m ¼ E½Xn  ¼
0

S0 ¼ 0;

Sn ¼

n
X

Xi ;

nX1;

i¼1

Sn
-m as n-N;
n
NðtÞ ¼ maxfn : Sn ptg;
where NðtÞ is the number of events that have occurred by time t:

1646

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

Fig. 14. Error analysis of the pursuer/evader example.

Then the mean of NðtÞ; mðtÞ ¼ E½NðtÞ ¼ t=m; as t goes to inﬁnity (Fig. 14).
Further let Y ðtÞ ¼ t  SNðtÞ ; i.e., the overshoot of the ﬁrst event after t: Then the key
renewal theorem states that for E½x2 oN;


1
s2
mþ
lim E½Y ðtÞ ¼
:
t-N
2
m
When m > s2 ; limt-N E½Y ðtÞE12m:
An intuitive explanation of this result is as follows: with s small, each step is close
to the constant m; and the range of possible overshoot is approx. ½0; m: The theorem
says, in effect, that the distribution with this interval is uniform with the resulting
mean of m=2 [5].

References
[1] M.A. Bassiouni, et al., Performance and reliability analysis of relevance ﬁltering for scalable distributed interactive simulation, ACM Trans. Model. Comp. Sim. (TOMACS) 7 (3) (1997)
293–331.
[2] H.J. Cho, Discrete event system homomorphisms: design and implementation of quantization-based
distributed simulation environment, Dissertation, University of Arizona, May 1999.
[3] S. Hall, S.M. Venkatesan, B.P. Zeigler, H.S. Sarjoughian, Using joint measure to study
tradeoffs between network trafﬁc reduction and ﬁdelity of HLA compliant pursuer/evader
simulations, Summer Computer Simulation Conference, Vancouver, Canada, July 2000,
pp. 287–294.
[4] J.S. Lee, Space-based data management for high performance distributed simulation, Doctoral
Dissertation, University of Arizona, June 2001; A portion this work is described in ‘‘Space-based
Communication Data Management in Scalable Distributed Simulation’’ J. Parallel Distributed
Comput. 62 (3) 336–365.
[5] S.M. Ross, Stochastic Processes, 3rd Edition, Academic Press, New York, NY, 1996.
[6] G. Wainer, B.P. Zeigler, Experimental results of timed cell-DEVS quantization, AI and Simulation,
AIS 2000, Tucson, AZ, 2000.
[7] B.P. Zeigler, DEVS Theory of Quantization, DARPA Contract N6133997K-0007, ECE Department,
UA, Tucson, AZ, 1998.
[8] B.P. Zeigler, G. Ball, H. Cho, J.S. Lee, H. Sarjoughian, The DEVS/HLA distributed simulation
environment and its support for predictive ﬁltering, DARPA Contract N6133997K-0007, ECE
Department, UA, Tucson, AZ, 1998.
[9] B.P. Zeigler, G. Ball, H. Cho, J.S. Lee, H. Sarjoughian, Bandwidth utilization/ﬁdelity tradeoffs in
predictive ﬁltering, Simulation Interoperation Workshop(SIW), June 1999, Orlando, FL.

B.P. Zeigler et al. / J. Parallel Distrib. Comput. 62 (2002) 1629–1647

1647

[10] B.P. Zeigler, H. Cho, J.S. Lee, Y.K. Cho, H. Sarjoughian, Predictive contract methodology and
federation performance, Simulation Interoperation Workshop (SIW), September 1999, Orlando, FL.
[11] B.P. Zeigler, T.G. Kim, H. Praehofer, Theory of Modeling and Simulation, 2nd Edition, Academic
Press, New York, NY, 2000.
[12] B.P. Zeigler, H. Sarjoughian, H. Praehofer, Theory of quantized systems: DEVS simulation of
perceiving agents, J. Sys. Cyber 31 (6) (2000) 611–648.

SimSaaS: Simulation Software-as-a-Service
Wei-Tek Tsai†*, Wu Li†, Hessam Sarjoughian†, Qihong Shao†
†School of Computing, Informatics and Decision Systems Engineering, Arizona State University, Tempe Arizona
85281 USA
*Department of Computer Science and Technology, Tsinghua University, Beijing, China
{wtsai, wu.li, sarjoughian, qihong.shao}@asu.edu
same software base. Furthermore, a cloud offers a
computing platform where an organization does not need
to acquire its own servers and infrastructure, and thus
may save a lot of resources to support simulations.
Early exploration of simulation in a cloud has been
done by Lanner group [10] and Thomas Paviot[11].
Lanner group develops a simulator for business process
management systems in a cloud. Thomas offers
simulation CAD services in a cloud. However, they have
not explored the configuration, MTA, and scalability
features of SaaS.
This paper explores the potential of using these SaaS
features for simulation software.
To support
configurability at multi-layers, both simulation
supporting framework and simulation models need to be
included. To support MTA, the system should have the
abilities to add/modify/delete the tenants, address the
tenants’ accessibility controls, distinguish tenant’s
simulation interaction message during executions and
isolate tenant’s own specific data. To support scalability,
both scale up and scale down need to be considered[12].
To fully explore these features, one can assume the
Platform as a Service (PaaS) such as EC2[1] or GAE [2]
can provide relevant features such as scheduling,
automated workload detection, web data storage,
automated
redundancy
management,
scalability
(including scaling up and down, out and in) and recovery,
which is indeed supported.
As a summary, the contribution of this paper is as
follows:
 Propose SimSaaS framework that incorporates key
elements of SaaS features for simulation;
 Design a platform-independent configuration model
to support MTA;
 Present a simulation runtime infrastructure that
supports MTA;
 Investigate various run-time analysis techniques that
can monitor, analyze and calibrate the simulation
continuously.
This paper is organized as follows: Section 2 discusses
the SimSaaS framework; Section 3 presents a SimSaaS
MTA configuration model; Section 4 talks about a MTA
simulation runtime; Section 5 presents analysis and
calibration of the simulation model; Section 6 presents an
example, Section 7 concludes this paper.

Abstract
Simulation can benefit from cloud computing that often
comes with thousands of processors and its software is
structured as Software-as-a-Service (SaaS) with its multitenancy architecture (MTA). This paper proposes
Simulation Software-as-a-Service (SimSaaS) with a MTA
configuration model and a cloud-based runtime to
support rapid simulation development to be run in an
elastic cloud environment.

1. Introduction
Cloud computing has received significant attention
recently as it is a new computing infrastructure to enable
rapid delivery of computing resources as a utility in a
dynamic, scalable, and virtualized manner. Public clouds
are available from Amazon[1], Google[2], Microsoft[3],
Salesforce[4]. Private clouds, in which the cloud software
is loaded locally, are available from VMware[5],
Eucalyptus[6], Citrix[7], and thousands of vendors offer
cloud solutions.
SaaS has become a new delivery model for cloud
computing, and it has a four-level maturity model [8]
depending on their customization, multi-tenancy
architecture (MTA), and scalability capabilities.
Researchers have started to investiate the simulated
cloud, such as Cloudsim[9]. A simulated cloud enables
applications to run. Note that this paper will not address
simulation of a cloud, but provide a new model for
simulation as a service.
Different from simulated cloud, this paper presents
how a cloud can support simulations with unique features
such as configuration and MTA. Simulation in a cloud
can take advantage of various resources including
physical resources such as processors, cache and logical
resources such as in-memory data stores, distributed file
systems, and SaaS infrastructure in the cloud. In fact,
simulation software can be structured in a SaaS manner
in which every resource/software is treated as service,
and thus SimSaaS (Simulation Software as a Service).
Traditional simulation often requires intensive software
developments, but SimSaaS can provide significant
advantages as it can potentially provide configuration,
MTA, and scalability, and it can save notable time and
effort in developing simulation software based on the

77

Supporting
Environment

Modeling
Participant/ Environment
Modeling Services

Service Modeling
Services

Configuration Services

Code Generation and Deployment

Workflow
Repositories
UI
Repositories

Ontology System

Service
Repositories

Intelligent Recommendation Services

Participant
Repositories

Code Generation Services

Deployment Services
Simulation Runtime

RTI Services
Control Services

Event Spaces
Services

Configuration
Services

Global Status
Services

Logging Services

Monitoring
Services

Provenance Services

RTE Services

Load Balancing

Filter Server

Resource
Allocation

Simulation
Execution

Addressing Services
RTS Services
Tenant Data
Tenant Metadata
Storage
Storage
Logging Data Storage

Data
Repositories

Analysis Engine
Profiling Services

5
Visualization Services

Figure 1 Elements of SimSaaS Architecture

Modeling Services: These provide modeling and
configuration for simulation using PSML[16], simulate
service modeling for
both atomic and composite
services[16]. Configuration services are offered here as
well to support the MTA configuration. Tenant
configuration will be discussed in section 3.
Code Generation/Deployment Services: These
include services that generate and deploy the code to
simulation engine. Similar to DDSOS [15], simulation
deployment in SimSaaS can support both the on demand
and automated way.
Simulation Runtime: This offers the vital
infrastructure services, stores necessary information and
executes simulation models in cloud. Three major
components include Run Time Infrastructure (RTI)
services, Run Time Execution (RTE) services and Run
Time Storage (RTS) services. More details of Simulation
Runtime and MTA support will be discussed in section 4.
Analysis Engine: This tracks the runtime information,
analyzes the simulation and displays through a graphic
interface. Profiling services analyze the runtime
execution information and make predictions to aid the
dynamic calibration of simulation. Visualization services
display the data from logging, provenance and profiling
services. Both timing analysis [13] and event analysis
[14] will be supported in SimSaaS.
The whole process is a continuous cycle, in which the
analysis results are sent back to the modeling phase for
continuous calibration until it meets the desirable goals
of the simulation

2. Simulation Architecture
The SimSaaS architecture design is discussed in this
section, As shown in Figure 1 shows the SimSaaS
architecture; and Figure 2 shows the relations of the
major components in the framework through the lifecycle
of
SimSaaS,
including
a
modeling,
code
generation/deployment, simulation, and analysis cyclical
process.
As one may find out, the design is similar to the SOA
simulation framework[13-15] but with several distinct
differences. In modeling phase, configuration services
are offered to configure the existing simulation services
with MTA abilities. In simulation runtime phase, a new
infrastructure is introduced to handle the tenant add,
delete, modify operations and the MTA simulation
execution including resource allocation, addressing,
isolation, accessibility control et al.
Supporting Environment: This contains repositories
and services that facilitate the reuse of simulation
models. Service repositories store simulation services,
workflows, participants and data; ontology systems
classify the models, store their relations and support
reasoning
function
for
models;
intelligent
recommendation services serve as the interfaces to query
metadata.
Model information is then passed to the modeling
services through intelligent query services. To speed up
the searching, continuous indexing and caching system
can be used.

78

Participant
Repository

Participant/
Environment
Modeling
Services

Service
Modeling
Services

Code
Generation
Services

Configuration Services

Ontology Secure
Sytem Check in/
out

Intelligent Query Services

Workflow
Repositories

Simulation

Analysis

RTE Services

UI
Repository
Service
Repository

Codegen and
Deployment

Modeling

Simulation
Execution

Visualization
Services

Resource
Allocation

Deployment
Services

Data
Repositories

RTS
Services

RTI
Services

Profiling
Services

Figure 2 SimSaaS Main Flow

Considering the space constraint, this paper will focus
on MTA configuration, execution runtime, and the
continuous analysis.

tenants.
Physical and logical address for the simulation
models.

Capability controls of different simulation models.

Sharing strategies among tenants.
This paper proposes TC (T-Filter Configuration), a
configuration model for SimSaas, to address the above
MTA requirements.


3. Simulation Multi-tenancy Configuration
This section designs a configuration model for the
MTA support of existing PSML simulation model.
The simulation models used in SimSaaS are described
by PSML, which can be executed in cloud platform such
as GAE through the code generation abilities. However,
PSML does no support of MTA inherently. The desirable
features for supporting MTA can actually be configured
using a model describing essential requirements for
MTA. Then, simulation runtime can read the tenant
configuration model, furthermore handle resource
allocation, and other related problems in execution
runtime. Thus this paper needs to figure out a MTA
configuration model first.
Take a smart home simulation example. Suppose a
HomeSimulation model is developed using PSML for a
room. Before deploying the simulation to cloud, the
director changes her mind and wants to replicate the
simulation for 100 different families and each room has
its unique requirements. One straightforward solution is
to make modification to existing model and create 100
instances. In cloud, the 100 different simulations are
essentially 100 tenants. And thus this is a MTA problem.
However, several issues are raised: Do we need to
change the existing model for MTA?
How we
differentiate the requirements of tenants? Will hardware,
software and storage be shared among the simulations?
How does the system manage these simulations with
respect to resource allocation, addressing, accessibility
control and execution? In the following discussion, our
model will solve these issues in turn.
Thus the following requirements are required for
support MTA in an existing simulation model:

Identities and requirements for different simulation

3.1 T-Filter Configuration: The MTA Model for
SimSaaS
TC is used to support MTA in PSML simulation
model. The goal of TC is to let tenants focus on the
simulation modeling without worrying about the tedious
MTA concerns at code level.
TC is composed of four parts: S-Model (Simulation
Model), T-Filter (Tenant Filter), TE (Tenant
Expressions) and hardware requirements. S-Model
represents the PSML simulation model. T-Filter stands
for tenant filter, which is used as a unique id for filtering
different tenants at simulation runtime. TE is tenant
expression which defines which T-Filter will be applied
to which S-Model as well as the capability controls
towards the S-Model.
Formally, corresponds to the four MTA configuration
requirements, this paper categorized the definition into
three groups: tenant model definitions and addressing,
capability controls and sharing strategies.
3.1. 1 Tenant Model Definitions and Addressing:
The configuration model first needs to define the
configuration (TC), the tenant (T-Filter, T-Space), the
simulation model(S-Model, Sub-Model), and how to
apply the configuration to the simulation model (TE).
Then, the end of this section explains using these models
for physical and logical addressing.

79

Definition 1, Given a tenant set T, T  {T , T2 ,...Tn }

RoomA, RoomB can be two different T-Filters.

there exists a tenant configure set TC, which maps tenant
Ti to TCi, denotes as Ti  TCi (i = 1,..n). Each TCi is

Definition 5 T-Filter Space (T-Space): T-Space stores
and guards the uniqueness of the T-Filters in a simulation
environment. It is a TRIE [17] thus it eliminates the
naming conflicts in simulation runtime. TRIE is a prefix
tree in which the value of the node is stored on the edge.
Every leaf of the prefix tree represents a T-Filter. A
lemma of prefix tree is that every leaf will be different
since the paths from root to the leaves are different.
Example 4:

composed four parts, denotes as
 Fi , M i , Ei , Ri  , in which






Fi is the configures file for TCi,
Mi is the simulation model set for TCi,
M i  {M i ,1 , M i , 2 ,..., M i ,k } , |k| is the size of
Mi.
Ei is a set of regular expression. Given a finite
alphabet  , with operations . (concatenation), |
(alternation), * (start set union) are defined.
Ri is the hardware requirements for a specific
resource, Ri  {Ri ,1 , Ri , 2 ,..., Ri ,q } , in which

a

k
A

In the following discussion, this paper will define each
components in details.

BankA

, M , M , M
M
( j)
M  M

as

( 2)

,...M

( p)

such

S

m
A

B

B

BankB RoomA RoomB

Figure 3 T-Space Implementation using TRIE

Definition 2 Simulation Model (S-Model):
A simulation M is composed of several sub model, denote
(1)

o
o

n

Ri,1 is request for a recourse q, e.g. cache.

( j)

S

B R

In Figure 3, the sample T-Space has four different TFilters and each of them is a leaf in the TRIE, thus no
naming conflicts will occur.

that

Let S-Model denotes the set of simulation model names.
si is in S-Model. si= {sub1, sub2..., subn } and subn SubModel.
Example 1: BankSimulation and HomeSimulation are
two S-Models which contain Sub-Models including
workflows, services, participants and data.

Definition 6 T-Filter EMPTY Set (∅): Defines the case
that no T-Filter is applied to an S-Model. It is represented
as ∅. Empty set implies no tenant will use a specific SModel.
Definition 7 Tenant Expression (TE): TE is the tenant
expression to describe how to apply T-Filters to SModels. It is composed by a number of T-Filters, SModel and T-Filter operators including “Capability”,
“Negation”, “Union”, “Intersection” as defined below.
Capability and Negation tells whether and how the SModel is accessed. Union and Intersection tells how the
S-Model is shared.
Let TE be the set of tenant expressions. Each tei in TE is
a 4-tuple <T-Filters, S-Model, Operators, Capabilities>
where T-Filters denote the set of tenant filters to tei, SModel the simulation model to tei, Operators the
operation from T-Filter to S-Model, and Capabilities the
read, write, execute capabilities to tei.
Figure 3 shows a TE example.

Definition 3 (Sub-Model): Sub model of PSML
simulation model. Sub-Models are identified by a unique
id under each S-Model. For convenience, the later part of
the paper uses names instead of ids.
Let Sub-Model denotes the set of sub simulation model
names. subi is in Sub-Model and subi participant, UI,
data, service, workflow.
Sub-Model can be accessed using “.” from S-Model.
Example 2:
HomeSimulation.UI1
HomeSimulation.Workflow1
Definition 4 Tenant Filter (T-Filter): T-Filter is unique
name that is used for supporting multi-tenancy in cloud
for SimSaaS. T-Filter and tenant names are
interchangeable.

Capabilities
T-Filter
TE

Let T-Filter be the set of tenant filters, let Tenant be the
set of tenants. tenanti in Tenant,
t-filteri in T-Filter.
tenanti string.
Example 3:

S-Model
rw

RoomA→ HomeSimulation
Capability Operator

Figure 4 A Sample Tenant Expression (TE)

80

However, this might result in some validity problems
since not every Sub-Model in S-Model can be simply
assigned with a capability. A way to go around is to list
out Sub-Models that can be configured. And thus it will
be safe to configure the S-Model with the capabilities.
3.1. 3 Model Sharing Strategies
Operation 3 Union Operator ( ): Union operator
defines the set of T-Filters that can be applied to the
same S-Model. It can concatenate T-Filters. Often, it is
used for abbreviation purpose.
Let tn be the tenant filter, s be the simulation model. (t1
t2 … tn) s denotes t1, t2… tn all share the same
logical model s.
For instance, the aforementioned example 5 can be
expressed as
Example 8:
(RoomA RoomB)
HomeSimulation
Operation 4 Intersection Operator ( ): Intersection
operator defines the set of tenants that share the same SModel. The difference between Intersection and Union is
that Union keeps separate execution instances for
different tenant while Intersection shares the same
execution instance.
Let tn be the tenant filter, s be the simulation model. (t1
t2 … tn) s denotes t1, t2… tn all share the same
s in execution.
Example 9:
(RoomA RoomB)
HomeSimulation
The example means the same HomeSimulation model
is shared between tenant RoomA and tenant RoomB. This
is different to the meaning of example 5. In example 5,
HomeSimulation has two copies of models at runtime; in
this example, only one copy of model is shared between
tenants at runtime.
Whether the intersection operator can be applied to
simulation is depend on the cloud platform
implementation.
Next chapter of the paper will discuss applying these
configurations for MTA simulation.

The six definitions are also used in addressing the
resource in execution runtime. More specifically, TFilter, Req in TC are used later in physical addressing. TFilter, TE, Sub-Models and S-Models are used in logical
addressing.
3.1. 2 Capability Controls
Capability-based systems [18] has been proposed in
hardware system to apply access control. This paper uses
capability to solve tenant access control problems in a
MTA simulation runtime. Meanwhile, capability system
is also used in the logical addressing in runtime.
Operation 1 Capability Operator ( ): Defines the
capability operation from a T-Filter to a specific SModel. By default, capability operator implies to have all
capabilities unless addressed specifically.
Let C be a tuple <T-Filter, S-Model> where T-Filter
denotes tenant filter, and S-Model denotes simulation
model. ci is in C. t is a T-Filter and s is a S-Model.
Operation t s means that T-Filter t will be applied to SModel s in ci with default capabilities.
Example 5:
RoomA HomeSimulation
RoomB HomeSimulation
Indicate two tenants which logically share the same SModel HomeSimulation but physically separate in
implementation. Two T-Filters RoomA and RoomB are
used to isolate tenant information between the two model
implementations.
Example 6:
RoomA HomeSimulation
RoomA TemperatureControl
Indicate one tenant with a T-Filter RoomA contains two
different S-Models.
Operation 2 Negation Operator (¬):
Negation
operator defines the T-Filter that will not be applied to an
S-Model.
Example 7:

¬RoomA HomeSimulation
The expression means that T-Filter RoomA cannot be
applied to HomeSimulation model.

3.2 Discussions
As discussed earlier the goal of TC is to support MTA
in PSML simulation model in a cloud platform such as
GAE. With TC, GAE can 1) integrate the physical
addressing from the cloud and the logical addressing
from PSML; 2) GAE will know how to apply the
resource sharing strategies to simulation; 3) apply the
fine grained capability controls to the PSML simulation
model.

Definition 8 Tenant Capability (Read, Write, and
Execute):
Defines the Read, Write and Execute
capability for the capability operator.
Let a be a tenant filter and b be a simulation model.
denotes that T-Filter a will be applied to SModel b with Read(r), Write (w) and Execute (e)
capability. Thus this expression will be identical to a
b.

4. SimSaaS Execution Runtime

denotes that T-Filter a will be applied to
S-Model b without Read, Write and Execute capability.
This will be identical to the
.
Capability control to the Sub-Model of the simulation
will give flexibility for the accessibility of S-Models.

This section designs a simulation runtime that be able
to handle MTA simulation in cloud. MTA support is
enabled in simulation runtime through TC. Other than the
runtime infrastructure, three main issues exist for

81

After modeled S-Model and defined the TC for the
model, the next step is how to apply the configuration to
the simulation runtime. This section will discuss three
basic operations: add, delete and modify.
S-Model is stored in RTS as tenant metadata thus when
creating a new tenant for an existing model, there is no
need to upload the S-Model for multiple times. The
simulation developers just need to operate upon the TCs.
The following algorithm explains how to do tenant
configuration. The inputs are the TC and the type of the
operation. Different outputs will be yielded depending on
the operations.

supporting MTA. 1) To configure the simulation tenants
in real-time; 2) To allocate and address the tenant
resource among the tenants; 3) To execute the model,
share and isolate resources, and apply the capability
controls to the model.
4.1 Simulation Runtime Infrastructure
T-Filter

RTE
Filter Server

Load Balancing

…

Filter Server

Tenant Addressing

RTI
Configuration
Services

RTS
Tennant Data

Resource
Allocation

Monitoring Services
Global Status Services

Tennant MetaData
Control Services
Index
Logging Services

Logging Data

Provenance Services

Profiling Service

Figure 5 Simulation Runtime

Simulation
runtime
includes
three
major
responsibilities: storing simulation model and related
information for the simulation; executing the model;
offering fundamental services for the execution of the
model. In this paper, Runtime Storage Services (RTS),
Runtime Execution Services (RTE) and Runtime
Infrastructure Services (RTI) are introduced to solve the
three responsibilities. Figure 5 shows an architecture
view of simulation runtime.
RTS store the necessary information for the simulation
engine in cloud. The data include tenant data, tenant
metadata, tenant index and simulation logging data.
RTE allocate resource according to individual
simulation model and tenant configuration and then
execute it from server side. They support adding,
modifying, deleting tenants; addressing tenants; and
executing the simulation for tenants.
RTI manage the simulation models and support the
communication within each individual tenant. A list of
services are offered for execution of simulation including
control services for the synchronization among the
simulation parties, event space services for categorizing
the events, global status services for sharing the current
statuses of the variables, runtime monitoring services for
watching the current execution status, logging services
for recording the simulation procedures, configuration
services for runtime and provenance services track the
origin of simulation data from logging services.
Simulation models and configurations are delivered to
the RTS first. RTE read data from RTS and then execute
the simulation model. To handle events, resources,
timing and synchronization issues of individual
simulation, RTI is then required to offer infrastructure
level support.
4.2 Real-Time Tenant Configuration

Add Operation: This adds a new tenant by offering a
new tenant configuration. In the algorithm, it involves
the new allocation of computing and storage resource,
and the recording of the current TC.
Delete Operation: This deletes an existing tenant from
simulation runtime. In the algorithm, it involves the
removal of computing resource, storage resource and TC
from the existing system.
Modify Operation: modifying an existing tenant
configuration in runtime. In the algorithm, it is related to
the TC modification rules.
For each capability of the S-Model (Sub-Model), it is
marked with a boolean flag as stated in S-Model/SubModel capability table. Two rules that will change the
Real-Time behavior when modifying the simulation:
Restart Rule: If the model has been accessed when
being modified, a restart of simulation is required and the
runtime will have to clean/differentiate the existing data
associated with the current simulation execution.
Precede Rule: If the model has not been accessed
being modified, the modification will be marked as safe.
The simulation can proceed without restarting.
Using the above algorithm, people can add tenant,

82

delete tenant and modify an existing tenant at real time.
4.3 Tenant Physical and Logical Addressing
B
Cache
T-Filter

…

o
o

n
Physical Address
(Node & Storage)

k
A

…

S

R

a

S

m
A

B

S-Model Capability Table
B

T-Filter

S-Model
HomeSimulation

RWE

T

RoomB

HomeSimulation

RWE

F

…

…

T-Filter:Model

Model
Capability

RoomA

…

…

…

Resource Requirement Table
T-Filter

CPU

Memory

Storage

RoomA

1

2G

100G

RoomB

1.5

3G

150G

Caches are also used for speeding up the lookup in a
relational table. LRU, MRU and other algorithms can all
be applied for the cache services.
4.4 Resource Isolation and Capability Control

Sub-Model Capability Tables
HomeSimulation
T-Filter
Service1
Data1
…
T-Filter
Service1
Data1
…
RoomA
RWE
F
RW
T
…
RoomA
RWE
F
RW
T
…
RoomB
RWE
F
RWE
F
…
RoomB
RWE
F
RWE
F
…
…
…
…
…
…
…
…
…
…
…
…
…

Jobid
J001
J002

Nodes
Node1(100%, 2G)

Node2(100%, 2G),
Node3(50%, 1G)

Address
RoomTable
RoomTable

T-Filter b

TC

TC

TC

Execution

Execution

Execution

Storage

Storage

Storage

Infrastructure

Infrastructure

Infrastructure

Virtual Space

T-Filter c

Virtual Space

Virtual Space

Cloud Virtualization Layer(Supported by cloud infrastructure)

…

Resource Allocation Table

T-Filter
RoomA
RoomB

T-Filter a

Limit
100G
…

…

…

Figure 7 Virtual Space for Simulation Execution

MTA introduces two new issues in the simulation
runtime. 1) Share and isolate the resources; 2) Apply the
different capability controls to the models deployed in
runtime.
Logical isolation of the resource is done by T-Filter.
Each tenant creates a virtual space by T-Filters.
Logically, these virtual spaces are independent since
none of them will duplicate.
In fact physically, the execution environment can share
models and resources. As stated in section 3.1.3 the
sharing happens at two levels.
The first is at resource level. Sharing is confined to SModels, hardware and storage. Sharing S-Model is
obvious, since through TC, it is easy to create two tenants
out of the same PSML simulation model. Sharing
hardware can actually be handled by the PaaS in a
synchronized or sequential way. Sharing storage can be
done through proper data storage schema design. In
Tsai’s book[19] 6 different schema designs for
supporting MTA in cloud are provided. All of them can
be used in SimSaaS design. Other than that in the case
study part, the paper listed a T-Filter based design for
storing the tenant data in Bigtable.
The other sharing is at the execution level, in which
two or more tenants share the same execution instance.
This will require a redesign of the simulation model and
it will be PaaS dependent. For instance, GAE allows
sharing the same execution instance from different
tenants. S-Model of SimSaaS can also implement
simulation in this way; however, this requires a redesign
of the simulation model since namespace switch will be
extensively used in GAE. To support this, a code
generator needs be introduced to insert pieces of code
into the model. In the resource allocation table in figure
6, physical addressing will point to the same set of
resource, while it will have a controller to budget the
resource usage according to the requirements. Instance

Figure 6 Resource Addressing Example

Another important issue is to find the resources for
different simulations in the same simulation runtime. The
addressing includes both physical and logical addressing.
Resource physical addressing is related to resource
allocation since when allocating the physical resource to
different tenants, it will be specified with the details of
the hardware resource.
Resource allocation is done through the Resource
Allocator in RTE as shown in Figure 5. When a new
tenant configuration arrives, the defined physical
resource requirement (Figure 6) will be stored to the
resource requirement. Physical resource for each tenant
such as CPU, memory and storage requirements will be
specified. The resource allocator will then interact with
PaaS such as GAE for the required physical resources.
Each T-Filter will point to an entry in resource allocation
table, in which the physical address for the allocated
resource including JobId, node names, storage table
address and limits for the storages are specified. The
arrows in the left part of Figure 6 show how to address
the physical resources.
Logical addressing is related to TE. As stated earlier,
each simulation model has a TE. It defines the capability
control from T-Filters to the S-Models and Sub-Models.
That in fact is also used for addressing the logical
resource. The addressing is divided in two steps. First, SModel level: it can be addressed by T-Filter since each of
them is different in tenant space. Second, Sub-Model
level: it can be addressed by the ids of the Sub-Model.
The arrows in the right hand side of Figure 6 explain how
to address the logical resource.
Note that each Sub-Model is part of S-Model, thus the
addressing for a Sub-Model will involve the addressing
for the S-Model first and then for the Sub-Model. For
each type of S-Model, it will have its own Sub-Model
capability table since they contain different Sub-Models.

83

the simulation analyzing services, a simulation engineer
can change the model.
Profiling services in the simulation analyzing services
are also useful to make the calibration process dynamic.
Based on the information from the provenances services
and logging services in RTI, profiling services can
control the selection of services by following the
predefined algorithms in the services.
Simulation model can evolve by calibrations. In Jin’s
paper[20] he addresses some patterns for service
evolvement such as add, replace. Since these operations
can be strictly described and proved, it is easy to derive
that the whole process can in fact be automated. For
instance, the calibration process can automatically use
the intelligent query service and then use the query
results to replace the current model with the other
candidates.
A report is generated at the end and thus it is easy to
compare the simulation models by viewing them
visually.
T-Filter still applies in analysis engine since it isolates
data collection from the Logging Services and
Provenance Services. Analysis and reports are generated
by different T-Filters and presented to end users.

sharing may also include resource sharing including SModels, computing and storage resources.
Sharing of the models tells the common parts of
different simulation, while capability controls specify the
differences among the different tenants. With capability
control, one set of S-Model can behave differently since
they are configured with different accessibilities.
Problems are raised in checking the capability in a MTA
since it allows the capabilities to be considered in a MTA
environment.
Various designs can be applied to solve the capability
control problems. Considering MTA, we summarized
two ways below with their own pros and cons.
• Inject: Require every Sub-Model maintain a
capability attribute along with the model. The
capabilities of the model are injected to the model
when the virtual model is initiated. This works
pretty efficiently since the capability is with the
model. However, it has potential problem in MTA
since the model needs to be redesigned to record the
different T-Filters and its capabilities.
• Force Look Back: Update the capability in TC, force
look back the TC whenever accesses the model. It
will cost a bit more since an external look up is
required. However, it requires no change towards the
existing simulation model and the MTA problem is
solved by offering a capability lookup table as
shown in figure 6. This solution is taken in SimSaaS.

5. Real-Time Analysis
Calibration
Simulation
Engineers

and

6. Example
Figure 9 shows a smart home DeviceControl
simulation which requires controlling the devices in a
smart home. The whole process is divided in five steps.
The goal of this case is to create two tenants RoomA and
RoomB which use the same S-Model. In execution time,
the two tenants will create two execution instances with
their data stored in one Bigtable.
Step1: An S-Model template is created using PSML
model. The S-Model contains a workflow and a table
which contains the device lists.
Step 2: An S-Model is created through customization.
In customization, the simulation developers will design
the ontology system for the desired UIs, services,
workflows, data and participants.
Step 3: Two tenants are created by specifying the
tenant configuration defined follows:
RoomA DeviceControl
RoomB DeviceControl
RoomB DeviceControl.TurnOnDevice
This TC configured two tenants RoomA and RoomB
which shared one simulation S-Model while executes
separately.
Step 4: The TC will be added to simulation runtime.
Simulation runtime invoke the resource allocator and
allocate necessary computing resource through GFS
according to the hardware resource requirements. Two
execution instances are named after RoomA and RoomB.

Continuous

Simulation Framework
Simulation Analyzing Services
Profiling
Visualization
Services
Services

Simulation Modeling Services
Participant/
Service/
Environment
Workflow

Simulation
Execution
Services

..
.

Figure 8 Simulation Process

This section talks about how to do real-time analysis
and continuous calibration in SimSaaS.
Continuous calibration of simulation includes the
interferences of simulation users, simulation analyzing
services, simulation modeling services and simulation
runtime. And it is done through the dynamic cyclical
process as shown in the Figure 8.
While static analysis provides useful data, many
analyses can be done only via dynamic simulation.
Simulation modeling services are useful to make the
process dynamic. By monitoring the execution data from

84

isolating the data from tenant RoomA and RoomB. For
instance, the id of DeviceControl for RoomA will have a
column name RoomA:DeviceControl:ID. Thus it is easy
to store and look up data for different tenants.

Step 5: a Bigtable is created for the storage of the data
according to the schema. In implementation, it used
Bigtable to store the data, in which each column name
contains the logical addressing of the data. This helps

Simulation
Developer
User

1

Start new
tenant.

User

User

…

DeviceControl Modeling
Execution

User

…

Storage

Login

RoomA → DeviceControl

No
Authoriz
ed
Yes
Search
Device

id

name

1

Light

2

…

…

…

TC

RoomB → DeviceControl
¬ RoomB →
DeviceControl.TurnOnDevice

No

…

No

Search
Device

…

Turn On
Device

TC

User
Authentic
ation
Yes
Author
ized

3

name
Light
…
…

Room1

id
1
2
…

Infrastructure

2
Ontology Based Model Customization

GFS

5

Create schema
for different
tenants.

User
Authentic
ation
Yes
Author
ized
Search
Device

Turn On
Device
id
1
2
…

4

name
Light
…
…

Bigtable

Room2

Infrastructure

Simulation Runtime

Tenant Configuration,
Generate Code

RoomA:DeviceControl:id

RoomA:DeviceControl:name

“001”

G0001

Light

“009”

G0003

BinarySwitch

…

RoomB:DeviceControl:id

…

RoomB:DeviceControl:name

Figure 9 Creating DeviceControl Simulation to Support Multi-tenancy

simply deployed their current simulator from a local
server to a remote cloud by using the web services
protocols. Obviously, there are more challenges for
doing simulation in cloud. How to model simulation,
how to reuse simulation, how to execute simulation and
how to do the analysis and calibration have become ever
important to solve to run simulation on cloud and makes
fully use of cloud.
Different cloud platform has different ways to support
MTA. For instance, saleforce.com uses a database
approach, IBM uses Corent to solve MTA [21], and
Google enforces namespaces in Google App Engine to
support MTA [2].
Configuration is widely used in software [22] and
hardware system to add flexibility to existing models.
With proper configurations, one can modify the existing
model easily and satisfy diverse requirements. SimSaaS
uses tenants’ configuration to address the MTA
requirements and then applies the configurations to the
simulation runtime.

7. Related Work
CloudSim is a simulation project that simulates a
cloud platform. It has two features: 1) to allow the cloud
developers to test the provisioning and service delivery
policies before the actual deployment of the application;
2) to support tuning up the performance bottlenecks
before deploying on real clouds. CloudSim’s focus of the
cloud simulation focused on the backend infrastructure,
they simulate data centers, virtual machine availabilities,
and ability for picking up the virtual processing cores for
virtualized services.
Early explorations of simulation in cloud has been
done by Lanner group [10], an business process
improvement company. Lanner’s simulator simulates the
business process management systems. L-SIM 2.0 is
configured as a RESTful service in the cloud and thus the
end user can do the business process simulation in the
cloud. In his presentation, Thomas[11] used open source
project and open standard and built a CAD simulation on
cloud to reduce the need for expensive hardware, costly
licensing scheme and to solve the interoperable problems
of using the CAD software. However, most of these early
explorations are still at a very primitive state. For
instance, instead of considering customization, MTA, and
scalability, both L-SIM 2.0 and Thomas CAD simulation

8. Conclusion
This paper proposed a MTA simulation framework
SimSaaS by following the SaaS architecture in cloud
computing. Furthermore, this paper uses a filter system to

85

[14]

perform access control similar to the capability systems
proposed earlier, but this feature is used in MTA in cloud
platform. The design of SimSaaS on GAE will be
presented in the near future.

[15]

9. ACKNOWLEDGMENT
This project is sponsored by U.S. Department of
Education FIPSE project P116B060433, U.S. National
Science Foundation project DUE 0942453, and
Department of Defense, Joint Interoperability Test
Command. The research is also partially sponsored by
the European Regional Development Fund and the
Government of Romania under the grant no. 181 of
18.06.2010.

[16]

[17]

10. References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]

[9]

[10]

[11]

[12]
[13]

[18]

M. Palankar, et al., "Amazon S3 for science
grids: a viable solution?," 2008, pp. 55-64.
(2010, 12/30). Google App Engine. Available:
http://code.google.com/appengine/
Microsoft. (2010, 12/30). Azure. Available:
http://www.microsoft.com/windowsazure/
(2010,
12/30).
Salesforce.
Available:
www.Salesforce.com
VMWare.
(2010,
12/30).
Available:
http://www.vmware.com/
Eucalyptus.
(2010,
12/30).
Available:
http://eucalyptus.cs.ucsb.edu/
Citrix.
(2010,
12/30).
Available:
http://www.citrix.com
F. Chong and G. Carraro, "Architecture
strategies for catching the long tail," MSDN
Library, Microsoft Corporation, 2006.
R. Buyya, et al., "Modeling and simulation of
scalable cloud computing environments and the
cloudsim toolkit: Challenges and opportunities,"
2009.
L. Group. (2010, 11/27). Simulation as a service
to business process management (BPM).
Available:
http://www.lanner.com/comms/090924/LSIM_September.pdf
J. F. Thomas Paviot, "Implementation of a SaaS
Based Simulation Platform Using Open
Standards and Open Source Software,"
presented at the 12th NASA-ESA Workshop on
Product Data Exchange (PDE2010), 2010.
Wikipedia. (2010, 12/30). Scalablity. Available:
http://en.wikipedia.org/wiki/Scalability
W. T. Tsai, et al., "Timing Specification and
Analysis for Service-Oriented Simulation," in
the 42nd Annual Simulation Symposium (ANSS),
San Diego, CA, 2009.

[19]

[20]

[21]

[22]

86

W. T. Tsai, et al., "Event-driven serviceoriented simulation framework," in the 43st
Annual Simulation Symposium (ANSS), 2010, p.
176.
W. T. Tsai, et al., "DDSOS: A Dynamic
Distributed
Service-Oriented
Simulation
Framework," 2006, pp. 160-167.
W. T. Tsai, et al., "PSML-S: A Process
Specification and Modeling Language for
Service-Oriented Computing," in The 9th
IASTED international conference on software
engineering and applications (SEA), Phoenix,
2005, pp. 160-167.
D. E. Willard, "New trie data structures which
support very fast search operations," Journal of
Computer and System Sciences, vol. 28, pp.
379-394, 1984.
H. Levy, Capability-based computer systems
vol. 1984: Digital Press, 1984.
Y. Chen and W. T. Tsai, Service-Oriented
Computing and Web Data Management:
Kendall/Hunt Publishing, 2010.
Z. Jin, et al., "LiveMig: An Approach to Live
Instance Migration in Composite Service
Evolution," in Web Services, 2009. ICWS 2009.
IEEE International Conference on, 2009, pp.
679-686.
S. Chate. (2010, 12/14). Convert your web
application to a multi-tenant SaaS solution.
Available:
http://www.ibm.com/developerworks/cloud/libr
ary/cl-multitenantsaas/
L. Cons and P. Poznanski, "Pan: A high-level
configuration language," in LISA '02: Sixteenth
Systems Administration Conference, 2002, pp.
83-98.

Proceedings of the 2006 Winter Simulation Conference
L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto, eds.

FLEXIBLE EXPERIMENTATION AND ANALYSIS FOR HYBRID DEVS AND MPC MODELS

Daniel E. Rivera

Dongping Huang
Hessam S. Sarjoughain
Gary W. Godding

Control Systems Engineering Laboratory
Chemical Engineering Department
Arizona State University, Tempe, Arizona

Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Department
Arizona State University, Tempe, Arizona

Karl G. Kempf
Decision Technologies
Intel Corporation, Chandler, Arizona

ABSTRACT

has been attracting researchers and practitioners (Gjerdrum
et al. 2001, Godding et al. 2004, Venkateswaran and Jones
2004, Sarjoughian et al. 2005, Xu and Sen 2005, Zeigler
2006).
Formalizing the composition is desirable to achieve
systematic synthesis of disparate models. Advances in
simulation interoperability and software engineering—e.g.,
HLA (HLA 2000), agent-based modeling (Gjerdrum et al.
2001) , and software service-oriented architecture (Tsai et al.
2006)—support integrating different simulation models as
independent components and enable interactions between
them. These approaches and their supporting technologies
employ low-level programming languages or high-level interoperability concepts to allow simulators to interact. Such
approaches, however, are not based on model composability.
Moreover, lack of model composability concepts and methods adversely affects capturing domain-specific knowledge.
A modeling composition approach referred to as Knowledge Interchange Broker (KIB)—was developed to formalize
integration of disparate models at the level of modeling formalisms (Sarjoughian and Plummer 2002). The conceptual
basis of KIB is that disparity between different syntax and
semantics should be accounted with a distinct model and thus
enabling independent data and control interactions required
for simulation models to interact. In particular, rather than
relying on middleware concepts and techniques, interaction
among models is specified as a pair: (i) model composability at the level of modeling formalisms and (ii) simulation
interoperability at the level of simulation execution. The
separation of the disparate models and their composition
enables two key activities of simulation model development
—i.e., model validation and simulation verification.

Discrete-event simulation and control-theoretic approaches
lend themselves to studying semiconductor manufacturing
supply-chain systems. In this work, we detail a modeling approach for semiconductor manufacturing supplychain systems in a hybrid DEVS/MPC testbed that supports experimentations for DEVS and MPC models using
KIBDEV S/M P C . This testbed supports detailed analysis
and design of interactions between discrete processes and
tactical controller. A set of experiments have been devised to
illustrate the role of modeling interactions between Discrete
Event System Specification and Model Predictive Control
models. The testbed offers novel features to methodically
identify and analyze complex model interactions and thus
support alternative designs based on tradeoffs between model
resolutions and execution times.
1

INTRODUCTION

A variety of simulation and optimization approaches have
been proposed for semiconductor manufacturing supplychain systems for many years (e.g., Shapiro 2001 and Kempf
2004). Each approach aims at tackling specific aspects of
semiconductor enterprise systems. For example, DiscreteEvent Simulation (DES) is beneficial for capturing realistic
dynamic behaviors. Linear Optimization (LP) and Model
Predictive Control (MPC), on the other hand, are suitable
for modeling strategic planning and tactical control. More
recently, due to the increasing complexity in semiconductor
enterprise-level operations (e.g., on-demand manufacturing,
short product shelf-life, distributed operations, and lower
cost), synthesis of the complementary modeling approaches

1-4244-0501-7/06/$20.00 ©2006 IEEE

1863

Huang, Sarjoughian, Rivera, Godding, and Kempf
Hybrid discrete-event simulation with model predictive
control (Sarjoughian et al. 2005) and linear optimization
(Godding et al. 2004) have been developed using the KIB
approach. The realization of these approaches use DEVSJAVA (ACIMS 2001) as the discrete-event simulation tool,
SIMULINK/MATLAB as the MPC tool (Mathworks 2005),
and OPL Studio as the optimization tool (ILOG 2005).

can offer clearer boundaries between distinct modeling formalisms, which are beneficial for composite system model
verification and validation.
2

SEMICONDUCTOR MANUFACTURING
SUPPLY-CHAIN PROCESS MODELING

Discrete-event simulation has been generally considered
suitable for modeling and simulating physical manufacturing process in semiconductor supply-chain systems. Some
of its key advantages are handling unexpected events and
capturing stochasticity in time and state. Component-based
discrete-event modeling approaches can represent the complex manufacturing dynamics at arbitrary levels of detail.
For example, a relatively high-level factory model specification can be replaced with by a set of machine models
connected together to capture important details such as
availability, downtime, and efficiency.
Discrete EVent System specification (DEVS) (Zeigler
et al. 2000) is a mathematical modeling formalism for
describing (discrete and continuous) dynamical systems as
discrete event models—atomic models and/or coupled models.
For semiconductor manufacturing supply-chain systems, there exist a set of primitive process nodes (atomic
models)—e.g., factory, warehouse, transportation, and customer (Godding et al. 2004, Singh et al. 2004). These nodes
can be combined to form complex nodes (coupled models)
such as Inventory-Factory (Sarjoughian et al. 2005). The
primitive and complex nodes can be connected to constitute
a variety of supply chain topologies.
All the supply-chain manufacturing process nodes
present common dynamics: (a) receive materials (products) from upstream process nodes and release commands
from decision models, (b) store and/or process materials
and process control commands or demands, and (c) send
materials to downstream process nodes and send status
information to the decision model. Given the data and
control flows among process models and also with other
models such as KIB, it is possible to specify common interface structures for these nodes—i.e., generic input/output
ports Data ([Data In, Data Out]) and Control ([Control In,
Control Out]) are used to represent physical material flow
and logical information flow, respectively.
Each type of process node offers specialties as well.
A warehouse node can hold material and allow material
to leave according to release commands. A key piece of
information for a warehouse node is its inventory level—i.e.,
beginning-on-hand, BOH. A factory model can represent
daily manufacturing operations such as building, assembling,
testing, and splitting products, or some combinations thereof.
A factory model can have capacity as well as stochastic
yield and throughput time (TPT). The state information that
needs to be considered includes WIP (work-in-progress),

1.1 Related Work
As noted in the previous section, multi-modeling approaches
are becoming increasingly popular for simulating integrated
supply-chain management modeling. For example, systemtheoretic simulation models are being used to evaluate specific supply-chain decision control models specified using
linear optimization (Xu and Sen 2005). This distributed
computing architecture supports interoperability between
discrete-event simulation and linear programming optimization. In this approach, the LP model is wrapped as an atomic
DEVS model which allows its execution to be reduced to
input and output events. DEVS coupling and simulation
protocol are used to ensure correct input and output data
exchanges between process and decision models. Input and
output transformations have to be carried out inside DEVS
and/or optimization models. This approach for process and
decision models to interact is limited since the interactions
must be divided into simulation and optimization models at
a low level of abstraction; thus, timing and synchronization
are restricted and ad-hoc. Another key consequence of this
approach is partial support for reusability and scalability.
The manufacturing simulation model and decision control model can be specified within a homogeneous environment. For example, a set of manufacturing simulation
models and one MPC model were developed in terms of
user-defined system blocks and executed as discrete-time
models in the SIMULINK/MATLAB environment (Wang
et al. 2005). The environment supports synchronous interaction between the simulation models and the MPC model
through customized couplings. It is difficult to capture
discrete-event behaviors —e.g., modeling unexpected events
using discrete-time modeling approaches. Ad-hoc support
for modeling discrete event dynamics adversely affects the
interactions between process and model predictive control
models.
There exist some other combined approaches for
simulation-based supply-chain modeling and decision control assessment (Gjerdrum et al. 2001, Venkateswaran and
Jones 2004). The interactions between the disparate models
are usually held within the model behavioral specification
using simulation interoperation and/or software engineering techniques. Very little research has concentrated on
studying formal specification of the interactions between
system-theoretic simulation modeling and advanced control modeling, although a separate formal interaction model

1864

Huang, Sarjoughian, Rivera, Godding, and Kempf
// input ports and values
X = inport × invalues
inport = {Data In, Control In}
invalues = {Lot, Command}

mal yield, or yield distribution, 2) commands, if there are any
from the decision control model, arrive as external messages
through Control In port in phase WaitForDecision, 3) at the
end of phase StartMaterial, the lots in Qoutput are sent out
to the downstream process models through Data Out port;
at the same time, the lots from upstream process models
may arrive as external messages through Data In port; they
are initially held in Qinput , 4) during the phase Process,
lots in Qinput are first moved into Qstorage and then the
lots in Qstorage are processed (e.g., lots’ processed-time
is increased and/or product names are changed); those lots
which have been completely processed are then transferred
to Qoutput , and 5) At the end of phase UpdateStatus, status
messages (e.g., WIP) are sent out to the decision control
model through Control Out port; meanwhile, the factory’s
local capacity is sent out to its immediate upstream inventory. A daily operation cycle is completed. The model goes
back to step 2 for a new operation cycle.

// output ports and values
Y = outport × outvalues
outport = {Data Out, Control Out}
outvalues = {Lot, Status}
// state sets
S = phase
× σ × Q

Initialize, W aitF orDecision,
phase =
StartM aterial, P rocess, U pdateStatus
σ : ℜ+
0,∞
Q : Qinput × Qstorage × Qoutput
Figure 1: Partial Factory Model Specification

Table 1: TPT-Load Model
Cases

3 levels of
distribution

Figure 2: Factory Model State Chart

5 levels of
distribution

AO (actual-out), average TPT and yield. A transportation
node represents transportation delays; it can be treated as
a simplified factory model that cannot change products. A
customer node can send demands to both decision model
and process model and receive product orders from upstream
warehouses. The detailed DEVS specification of a factory
model is described as follows.
The structural specification of the atomic factory model
is defined in Figure 1 (Godding et al. 2004, Singh et al.
2004). The state set includes two generic state variables
Phase and sigma (σ) and domain-specific state variables
for capturing behavior of the factory. For the factory model
described here, σ0 = 0, σ1 = σ2 = σ4 = 0.1, and σ3 = 0.7
(see Figure 2).
A partial statechart depicting external and internal events
of the factory model is shown in Figure 2. External events for
a factory model can be from other manufacturing nodes (e.g.,
inventory) or external models. External events from a source
other than manufacturing nodes (e.g., KIB) are supported
with special Control ports. These Control and Data ports
play a key role in separating a DEVS manufacturing model
interaction with a non-DEVS model (e.g., computational
entity).
The processing procedure of each phase can be informally described in the following: 1) in the phase Initialize, a
set of model parameters are configured—e.g., capacity, nor-

Load
TPT (time unit)
(%)
MIN AVE MAX
(0 − 70]
30
32
34
(70 − 90]
32
35
38
(90 − 100]
35
40
45
(0 − 70]
30
32
34
(70 − 80]
31
34
36
(80 − 90]
32
35
38
(90 − 95]
34
37
42
(95 − 100]
36
40
45

The lot processing needs to represent realistic manufacturing processes. It must account for not only various
processing operations but also stochastic characteristics. In
a more complicated circumstance, the uncertain behavior
can depend on one or more temporal properties. In particular, the TPT may vary depending on the run-time load of the
factory—i.e., heavier load results in longer throughput time.
The TPT-load relationship may be specified in discrete or
continuous forms given the source of data or assumptions
made on non-linearity and stochastic duration (time) for
each factory operation. Modeling of the TPT-load computational relationship play a significant role in representing
practical manufacturing processes, since linear or exponential relationships can lead to vastly different supply-chain
dynamics. One simplified approach is that TPT can be
divided into two or more ranges given different percentages
for factory loads. Within specific load percentages, the
TPT can be uniformly or triangularly distributed between
specified values (see Table 1).
3

PREDICTIVE OPTIMIZATION-BASED
PROCESS CONTROL MODELING

Model Predictive Control (MPC) is aimed at controlling
dynamical systems using a combination of control theoretic

1865

Huang, Sarjoughian, Rivera, Godding, and Kempf
and optimization techniques (Qin and Badgwell 2003). It has
been shown that MPC can be used as a tactical controller for
high volume semiconductor supply-chain systems to handle
nonlinear, stochastic dynamics and unpredictable demand
changes while enforcing constraints on desired inventory
levels and production and transportation capacities (Wang
et al. 2005). In the MPC model, a system prediction model,
which is a simplified system as compared with the real system
or a detailed simulation model as described in the previous
section, estimates future inventory levels by considering
historical information such as warehouse inventories, actual
customer demands and factory release starts, reference points
(i.e., inventory targets), and forecasting customer demands.
In the optimizer, a sequence of factory starts is calculated by
solving the optimization whose objective functions include
keeping inventory levels close to targets, minimizing changes
in the manipulated factory starts, and maintaining the factory
starts at strategic planning targets.
To meet the requirements of the semiconductor supplychain tactical control, the MPC model offers different tuning
parameters defined for filters or controllers (e.g., Kalman
filter)(Wang et al. 2005). These parameter values are key
in achieving desired performance requirements for forecasted demands, inventory targets tracking, and unforecasted customer demands. For example, a tuning parameter
αj ∈ [0, 1), j = 1, 2, ..., n represents each inventory target
tracking speed—smaller can result in a faster response that
an inventory can achieve to track the target. A tunable parameter filter gain (fa ) is used to deal with prediction errors
generated by the stochasticity and uncertainty in the system.
As fa approaches zero, the controller ignores most of the
prediction errors and the solution is mainly determined by
the deterministic information (e.g., normal system delays
and measured anticipations). In contrast, the controller will
compensate all of the prediction errors from the stochasticity
and uncertainty if fa equals one.
4

properties, which account for both structural and behavioral compositions.
The structural composition is to specify message transformation between two distinct interface structures. For
example, in composing models expressed in DEVS and
MPC (Sarjoughian et al. 2005) or LP (Godding et al.
2004), the interface structure for a DEVS (atomic and coupled) model consists of a set of input and output ports with
message bags to be exchanged with other model components. The interface structure for an MPC model, on the
other hand, is a set of variables which can have primitive
values (e.g., integers). The modeling language (or environment) may also have an impact on interface structure. In
particular, DEVS model specification has complex message
types whereas MPC model specification does not. To ensure correct transformation, domain-specific knowledge also
needs to be considered, since an application domain (such
as semiconductor manufacturing supply-chain) contains its
own characteristics such as timing constraints, value ranges,
and frequency of information exchanges.
The process simulation model and MPC tactical control
models describe different aspects of a system. The process
simulation model can provide state information (e.g., WIP
and BOH) for each node at the end of an operation cycle
(e.g., number of products processed in a factory based
on hourly or daily time period). The tactical controller
may receive accumulated state information and provide
control commands for simulation nodes based on single or
multiple time periods. In this circumstance, the message
transformation must account for data abstraction consistency
(Godding et al. 2004).
In a homogeneous modeling framework, the correct interaction between model components are generally ensured
by the individual model behavioral specifications (e.g., time
advanced function and internal/external transition function
in an atomic DEVS model) and its well-defined simulation
protocol. In a heterogeneous modeling environment, each
participating modeling formalism relies upon its own execution protocol to handle timing and synchronization needs.
Therefore, it is necessary to synchronize execution of the
DEVS simulation protocol and the MPC execution.
To guarantee correctness of execution of models described in DEVS/MPC, the KIB specification is defined
as being strictly synchronized (Sarjoughian et al. 2005).
The KIB specifies the logical time instance at which the
send (or receive) messages must occur between disparate
models. In this approach, the wall-clock time consumed
by the MPC is abstracted to be instantaneous with respect
to the DEVS model and the KIB also does not consume
(logical or wall-clock) time.
The time synchronization specification must conform
to the synchronization discipline that is associated with
the coordination execution control which in turn relies on
correct execution of the disparate models throughout the

SIMULATION/OPTIMIZATION
ENVIRONMENT

In this work, the DEVS model captures complex dynamics of semiconductor manufacturing processes whereas the
MPC model is responsible for tactical control. In industrial
settings, these are used complementary to each other for analyzing and solving real-world semiconductor manufacturing
supply-chain problems.
The Knowledge Interchange Broker (KIB) for a hybrid
DEVS/MPC modeling approach ensures the interactions
between two disparate models are modeled and executed
correctly (Sarjoughian et al. 2005). The KIB composition
specification is an independent model between the process
simulation model and the tactical control model. It explicitly describes model interactions in terms of message
transformation, concurrency, synchronization, and timing

1866

Huang, Sarjoughian, Rivera, Godding, and Kempf
to inventory Raw Resource, Die/Package, Semi-finished and
Customer Warehouse to observe stochastic yield and TPT in
each factory and validate mass balance among the processing
models. The values of upstream factory-starts are set larger
than the values at downstream factory-starts given expected
yields. Similarly, factory-starts delays are chosen in a such
a way to represent realistic dynamics in factory or inventory
models. These experiments in this category are important
since the manufacturing process model specification will be
used in the hybrid DEVS/MPC model.
In the second category, only customer demand is sent
to the hybrid system. Sarjoughian et al. 2005 has verified
the model composition with KIBDEV S/M P C , by assuming
relatively simple manufacturing process dynamics. In our
experiment, the customer demand uses a square input regime:
the average customer demand is set at 951/day with a small
variance starting from day 61; the demand increases by about
500 (about 53% percent more than 951) from day 201 to day
400 and then it remains at the average customer demand with
a small variance after day 400 until the end of simulation.
The aim of this profile is to analyze the robustness of
the MPC with respect to sharp increases and decreases in
customer demands and thus to determine how well the MPC
controller can be tuned to handle unanticipated changes in
customer demand, DEVS simulation and KIBDEV S/M P C
models.

Figure 3: Hybrid DEVS/MPC Model
combined DEVS/MPC simulation (i.e., it accounts for causal
ordering of messages and time management). The execution of the KIB model generates dynamic behavior of the
disparate model interactions and thus allows tracking the
data and control exchanges which is essential in validating
interactions between the models in a systematic fashion.
5

EXPERIMENTAL RESULTS

The manufacturing process network is limited to contain only
one pipeline process which consists of 3 factories (Fab/Test1,
Assembly/Test2, and Finish), 4 warehouses (Raw Resource,
Die/Package, Semi-finished and Customer Warehouse), one
Shipment and one Customer (see Figure 3). These manufacturing model components are described as atomic and
coupled DEVS models developed in DEVSJAVA, while the
MPC model is the tactical controller implemented in MATLAB. The interaction of DEVS model and MPC model
is modeled using the KIBDEV S/M P C . The experimental
testbed also includes some other auxiliary models (e.g., a
transducer model is developed for collecting messages).
We have devised a set of experiments to investigate
the detailed dynamics of manufacturing process network
and to verify the correctness of the composite DEVS/MPC
model with KIBDEV S/M P C . These experiments help to
analyze alternative designs for a prototypical manufacturing
process, tactical controller, and their interactions.
Given the above goals, we have devised two categories
of experiments. One is to validate the Manufacturing Process
Network model with data sets for factory starts and customer
demand. The other is to study the MPC model robustness
given the detailed simulation of the Manufacturing Process
Network model.
In the first category, the factory-starts (i.e., commands
to inventories) and customer demand are given to the DEVS
simulation model for autonomous process simulation—this
enables the simulation model to interact with an idealistic
MPC and KIB models. Daily controller commands/demands
(defined as standard steps and sinusoidal regime) are sent

5.1 Autonomous Process Simulation Analysis
Each node in the Manufacturing Process Network model
has detailed dynamics (see Section 2). The factory models
can be configured with or without stochastic behavior. Deterministic configuration is used to verify TPT and yield at
each factory and also ensure that mass balance across the
entire manufacturing process network is maintained (i.e.,
the total number of lots entering and exiting the Manufacturing Process Network remains constant). Stochasticity
in the factory nodes are modeled by assigning (triangular
or uniform) distribution functions to each lot for obtaining
actual yield and TPT. Factory-starts with sinusoidal and
square input regimes are sent to each inventory model. The
observed information in this experiment includes WIP and
AO for each factory model, BOH for each inventory model,
and run-time yield and TPT for each factory model.
It has been shown that, Lot size, which is defined as
minimal processing unit in every process node, plays an
important role since it directly affects a model’s stochastic behavior. For example, when the Lot Size is set to a
maximum value, the stochastic TPT values generated from
the distribution function are assigned to only one lot which
includes the amount of all materials flowing in a factory.
In this circumstance, the AO from a factory may vary significantly (e.g., from 0 in one day to maximum value in
the next day). Such sharp changes causing major change

1867

Huang, Sarjoughian, Rivera, Godding, and Kempf
start on Fab/Test1, which is determined by the MPC model,
is increased accordingly to satisfy the customer demand.
Consequently, the load in the factory simulation model is
also increased. Since the run-time TPT is calculated based
on the load, a heavier load can result in longer delay in the
Fab/Test1 model. Long delays in turn impact the inventory
level of the downstream Die/Package model. Similarly, noticeable transients occur when customer demand decreases
sharply.
In contrast to the DEVS simulation model, the nominal
TPT configured in the predictive model is deterministic.
If the difference between the nominal TPT in the predictive model and the average run-time TPT in the process
simulation model is significant, the MPC cannot calculate
acceptable factory starts and consequently the DEVS simulation dynamics are not robust. To demonstrate this, another
experiment in which a 5-level TPT-load computation relationship is configured in Fab/Test1 model shows better
system dynamics (see Figure 4) since the 5-level TPT-load
model has smoother change in TPT in comparison to the
3-level TPT-load model.
The experiments help us analyze and evaluate tactical
control policies specified in the MPC model. For example,
based on the analysis of the experimental results, one of the
future research areas is to develop an adaptive MPC model
in which it can support dynamic nominal TPT on the basis
of certain criteria.
Given the experiments described above, apparently
small changes in either the process simulation model or the
MPC model can cause significant changes in the manufacturing supply-chain system dynamics. The hybrid discreteevent simulation with optimization control makes it convenient for us to detail and extend manufacturing process
simulation and tactical control models separately. The separation gives us a better understanding of both of the models and their interactions. The component-based modeling
and simulation environment supports model reusability and
configuration flexibility, which also simplifies setting up
different experimentation scenarios.

Figure 4: Fab/Test1 Starts & Load and Die/Package Inventory with Different TPT-Load Granularity
in factory load and thus TPT do not reflect realistic behavior of a factory in semiconductor supply-chain system.
Appropriate choice of lot size, therefore, can significantly
impact the dynamics of the manufacturing process model.
As expected, the results from a series of experiments with
different lot sizes (e.g., maximum, 100, 50, 20 and 10)
show that smaller lot sizes give smoother factory behavior as expected. However, a small lot size requires more
computation time, which adversely affects the performance
of the process and therefore combined DEVS/MPC simulation (see Section 5.4). These experimental results show
open-loop DEVS manufacturing process simulations to be
logically correct and consistent with prototypical realistic
manufacturing systems (Rose 1999).
5.2 Process/OPT Simulation Analysis
The parameter settings for both the process simulation model
and MPC model are defined in Table 2. The more-than 50%
increase in customer demand can cause significant non-linear
dynamics in Fab/Test1 due to the TPT-load model. It is
helpful to know to what extent MPC can handle process
nonlinearity and uncertainty.
As shown in Figure 4, more transient states—i.e., inventory level—occur in the Die/Package inventory models. This
was due to the large TPT changes in the upstream Fab/Test1
factory model. Ideally, when Fab/Test1 maintains its load
within a specific range (e.g., load ∈ [72%, 76%] ), the average TPT can be kept at 35 days in the process simulation
model. Accordingly, such average TPT value is consistent
with the corresponding nominal TPT parameter configured
in the predictive model inside the MPC model. However,
due to the significant increase in customer demand, the

5.3 Execution Time vs. Accuracy Analysis
Execution time for simulation studies depend on a variety of
a factors including details of models, efficiency of individual
components of the DEVS/MPC environment (DEVSJAVA,
SIMULINK/MATLAB and KIB) and the underlying computing environment(Java Runtime Environment), and computer operating system and hardware configuration. For
example, in the above experiments, we have chosen lot size
= 50 since it provides a suitable trade-off between accuracy
and performance. For example, the DEVSJAVA simulation
time can be reduced by about 30% when changing lot size
from 10 to 50 (see Figure 5) while maintaining acceptable
accuracy of the combined DEVS/MPC models. With this

1868

Huang, Sarjoughian, Rivera, Godding, and Kempf
Table 2: Parameter Configurations in Manufacturing Process Network Model and MPC Model

FAB/Test1
Assembly/Test2
Finish
Shipping

Factory

Inventory

Die/Package
Semi-Finished
Customer Warehouse
Others

Process Model
MPC Model
TPT distribution
Yield distribution (%)
Ave
Ave
Load (%) MIN AVE MAX MIN AVE MAX Capacity TPT Yield (%)
See Table 1
93
95
97
70,000
35
95
[0,100]
5
6
7
98 98.5
99
10,000
6
98.5
[0,100]
1
2
3
98.5 99
99.5
5,000
2
99
[0,100]
1
1
1
100 100
100
2,500
1
100
Maximum Capacity
Target Point
20,000
5,712
10,000
2,856
10,000
1,787
Lot Size
Simulation time
fa
50
638 days
0.01
0.05

As shown in the experimental analysis, the separation
offers flexibility and convenience to explicitly observe and
analyze how specific factors on each side can affect the
holistic system dynamics. It enforces systematic interactions
between disparate models.
ACKNOWLEDGMENTS
This research is supported by NSF Grant No. DMI-0432439
and grants from Intel Research Council. We would like to
acknowledge the contributions of Wenlin Wang and Hans
Mittelmann of ASU. We also thank Dieter Armbruster of
ASU and Kirk Smith of Intel Corporation for fruitful discussions.

Figure 5: Average DEVS and DEVS/MPC Execution Times
testbed, we can measure and compare wallclock execution
time for DEVSJAVA and SIMULINK/MATLAB.
Measurement of the execution times helps to understand relative computational resources committed to each
component (e.g., DEVSJAVA) and to identify bottlenecks
and ways to make them more efficient while ensuring desirable accuracy in simulation results. For example, in the
DEVS/MPC testbed, we have carried out a series of experiments. The results of these experiments are average
execution times for a single execution cycle averaged over
5 simulation runs, each of which has 638 cycles—the execution times are shown in Figure 5. One complete execution
cycle is measured starting from DEVS to KIB to MPC and
back to DEVS. These experiments were conducted on a single computer configured with a 3.2 GHz Intelr Pentiumr
4 CPU, 1G RAM and Microsoftr Windowsr XP Professional OS, DEVSJAVA 2.7, SIMULINKr /MATLABr 7.0,
and JAVATM Sun Microsystems JRE 1.5.0.
6

REFERENCES
ACIMS 2001. Arizona Center for Integrative Modeling and
Simulation. Available from <http://www.acims.
arizona.edu/> [cited 2006].
Gjerdrum, J., N. Shah, and L. Papageorgiou. 2001. A combined optimization and agent-based approach to supply
chain modelling and performance assessment. Production Planning & Control 12 (1): 81–88.
Godding, G., H. Sarjoughian, and K. Kempf. 2004. Multiformalism modeling approach for semiconductor supply/demand networks. In Proceedings of Winter Simulation Conference, 232–239. Washington DC, USA.
HLA 2000. IEEE standard for modeling and simulation
(m&s) High Level Architecture (HLA)—Federate Interface Specification. IEEE.
ILOG 2005. OPL Studio. Available from <http:
//www.ilog.com/products/oplstudio/>
[cited 2006].
Kempf, K. 2004. Control-oriented approaches to supply
chain management in semiconductor manufacturing. In
Proceedings of IEEE American Control Conference,
4563–4576. Boston, MA, USA.
Mathworks 2005. MATLAB/Simulink. Available from
<http://www.mathworks.com> [cited 2006].

CONCLUSIONS

The experimental results described and discussed in Section
5 show in a quantitative setting the qualitative expectation
that higher precision TPT-load models are important in capturing the detailed dynamics between manufacturing processes and decision making and thus help in the modeling
of realistic supply-chain systems.

1869

Huang, Sarjoughian, Rivera, Godding, and Kempf
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science and Engineering at ASU. He can be
contacted at <sarjoughian@asu.edu>.

Qin, S., and T. Badgwell. 2003. A survey of industrial
model predictive control technology. Control Engineering Practice 11 (7): 733–764.
Rose, O. 1999. CONLOAD—a new lot release rule for
semiconductor wafer fabs. In Proceedings of the Winter
Simulation Conference, 850–855. Phoenix, AZ, USA.
Sarjoughian, H., D. Huang, W. Wang, D. Rivera, K. Kempf,
G. Godding, and H. Mittelmann. 2005. Hybrid discrete
event simulation with model predictive control for semiconductor supply-chain manufacturing. In Proceedings
of Winter Simulation Conference, 256–266. Orlando,
FL, USA.
Sarjoughian, H., and J. Plummer. 2002. Design and implementation of a bridge between RAP and DEVS.
Computer Science and Engineering, Arizona State University, Tempe, AZ.
Shapiro, J. 2001. Modeling the supply chain. Duxbury.
Singh, R., H. Sarjoughian, and G. Godding. 2004. Design
of scalable simulation models for semiconductor manufacturing processes. In Proceedings of the Summer
Computer Simulation Conference, 235–240. San Jose,
CA, USA.
Tsai, W., C. Fan, and Y. Chen. 2006. DDSOS: a dynamic distributed service-oriented simulation framework. In 39th
Annual Simulation Symposium, 160–167. Huntsville,
AL, USA.
Venkateswaran, J., and A. Jones. 2004. Hierarchical production planning using a hybrid system dynamic-discrete
event simulation architecture. In Proceedings of the
Winter Simulation Conference, 1094 – 1102. Washington D.C., USA.
Wang, W., D. Rivera, and K. Kempf. 2005. A novel model
predictive control algorithm for supply chain management in semiconductor manufacturing. In American
Control Conference, 208–213. Portland, OR, USA.
Xu, Y., and S. Sen. 2005. A distributed computing architecture for simulation and optimization. In Proceedings of
the Winter Simulation Conference, 365–373. Orlando,
FL, USA.
Zeigler, B. 2006. Embedding DEVS&DESS in DEVS. In
DEVS Integrative Modeling & Simulation Symposium,
125–132. Huntsville, AL, USA.
Zeigler, B., H. Praehofer, and T. Kim. 2000. Theory of
modeling and simulation: Integrating discrete event and
continuous complex dynamic systems. 2nd ed. Academic
Press.

DANIEL E. RIVERA is Associate Professor Chemical
and Materials Engineering, ASU. He can be contacted at
<daniel.rivera@asu.edu>.
GARY W. GODDING is a Technologist at Intel Corporation and a PhD candidate in the Computer Science and
Engineering department at ASU. He can be contacted at
<gary.godding@intel.com>.
KARL G. KEMPF is Director of Decision Technologies
at Intel Corporation and Adjunct Professor at ASU. He can
be contacted at <karl.g.kempf@intel.com>.

AUTHOR BIOGRAPHIES
DONPING HUANG is a PhD candidate in the Computer
Science and Engineering department at ASU. She can be
contacted at <dongping.huang@asu.edu>.

1870

IEEE TRANSACTIONS ON SERVICES COMPUTING,

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

247

Toward Development of Adaptive
Service-Based Software Systems
Stephen S. Yau, Fellow, IEEE, Nong Ye, Senior Member, IEEE,
Hessam S. Sarjoughian, Member, IEEE, Dazhi Huang, Auttawut Roontiva,
Mustafa Gökçe Baydogan, and Mohammed A. Muqsith
Abstract—The rapid adoption of service-oriented architecture (SOA) in many large-scale distributed applications requires the
development of adaptive service-based software systems (ASBS) with the capability of monitoring the changing system status,
analyzing, and controlling tradeoffs among various quality-of-service (QoS) aspects, and adapting service configurations to satisfy
multiple QoS requirements simultaneously. In this paper, our results toward the development of adaptive service-based software
systems are presented. The formulation of Activity-State-QoS (ASQ) models and how to use the data from controlled experiments to
establish ASQ models for capturing the cause-effect dynamics among service activities, system resource states, and QoS in servicebased systems are presented. Then, QoS monitoring modules based on ASQ models and SOA-compliant simulation models are
developed to support the validation of the ASBS design. The main idea for developing QoS adaptation modules based on ASQ models
is discussed. An experiment based on a voice communication service is used to illustrate our results.
Index Terms—Design concepts, distributed/Internet-based software engineering tools and techniques, methodologies, modeling
methodologies, quality of services, services systems.

Ç
1

INTRODUCTION

R

ECENT development of service-oriented computing and
grid computing has led to rapid adoption of serviceoriented architecture (SOA) in distributed computing
systems, such as enterprise computing infrastructures,
grid-enabled applications, and global information systems.
Software systems based on SOA are called service-based
software systems (SBS). Unique characteristics of SOA, such
as loosely coupling and late binding, provide the capabilities
that enable the rapid composition of the needed services
from various service providers for distributed applications
and the runtime adaptation of SBS. However, fundamental
changes to existing software engineering techniques are
needed for the design of high-quality SBS due to the unique
characteristics of SOA. A major problem to achieve this goal
is how to manage the satisfaction of multiple quality-ofservice (QoS) requirements simultaneously, such as timeliness, throughput, accuracy, security, dependability, survivability, and availability. Because the satisfaction of the
requirement of a QoS aspect often affect the satisfaction of
other QoS aspects, tradeoffs of requirements among multiple QoS aspects must be taken into consideration in the
design of SBS.
Existing software engineering techniques do not support
the analysis and adaptive control of such tradeoffs due to

. The authors are with the Information Assurance Center and the School of
Computing, Informatics, and Decision Systems Engineering, Arizona State
University, PO Box 878809, Tempe, AZ 85287-8809.
E-mail: {yau, nongye, hessam.sarjoughian, dazhi.huang, auttawut.roontiva,
mustafa.baydogan, mmuqsith}@asu.edu.
Manuscript received 15 Jan. 2009; revised 2 Apr. 2009; accepted 18 June 2009;
published online 29 June 2009.
For information on obtaining reprints of this article, please send e-mail to:
tsc@computer.org, and reference IEEECS Log Number TSCSI-2009-01-0006.
Digital Object Identifier no. 10.1109/TSC.2009.17.
1939-1374/09/$25.00 ß 2009 IEEE

lack of comprehensive understanding of such tradeoffs and
their relations to service activities in networked computing
systems. Furthermore, an SBS often comprises services
owned by various providers. Each service may support
multiple service configurations, each of which defines the
runtime properties of the service, such as authentication
mechanism, priority, and maximum bandwidth reserved
for the service, and may affect the runtime performance of
the service. The services in SBS often operate in highly
dynamic environments, where the services may become
temporarily unavailable due to various system and network
failures, overloads or other causes. Hence, high-quality SBS
need to have the capability to monitor the changing system
status, analyze and control tradeoffs among multiple QoS
aspects, and adapt service configurations accordingly. Such
an SBS is referred as an Adaptive SBS (ASBS).
In this paper, we will present our results toward the
development of ASBS. After discussing the related work in
Section 2 and the overview of our ideas toward developing
ASBS in Section 3, we will present in Section 4 our
Activity-State-QoS (ASQ) models for capturing the causeeffect dynamics among service activities, system resource
states, and QoS in service-based systems, and how the
ASQ models are established using data from experiments.
Three critical QoS aspects, timeliness, throughput, and
accuracy, are considered in our ASQ models due to their
importance in many applications. In Section 5, we will
show how QoS monitoring modules are developed based
on our ASQ models. In Section 6, we will present how
SOA-compliant simulation models are developed to support the validation of ASBS design. The main idea for
developing QoS adaptation modules based on ASQ models
will also be discussed.
Published by the IEEE Computer Society

248

2

IEEE TRANSACTIONS ON SERVICES COMPUTING,

RELATED WORK

Currently, software design is usually based on logic-based
operational models. However, performance models linking
service operations to resource states and QoS performance
are needed for QoS monitoring and adaptation (M/A) in
ASBS. Individual QoS aspects related to various activities
and resource states in computing and communication
systems have been studied both empirically and theoretically [1], [2], [3]. However, how system activities and
resource states affect the tradeoffs among various QoS
aspects has not been sufficiently investigated [4], [5], and
the lack of such knowledge makes the design of ASBS
difficult.
Recently, more research has focused on the design and
adaptation of systems to satisfy various QoS requirements
[6], [7], [8], [9], [10]. QoS-aware service composition [6], [7],
[8] aims at finding optimal or suboptimal service composition to satisfy various QoS constraints, such as cost and
deadline, within a reasonable amount of time. Various
techniques have been presented for QoS-aware service
composition, such as service routing [6] and genetic
algorithms [7], [8]. However, the QoS models considered
in existing QoS-aware service composition methods are
usually simple, and runtime adaptation of service composition cannot be efficiently handled by most QoS-aware
service composition methods. Self-tuning techniques and
autonomic middleware providing support for service
adaptation and configuration management have been
developed in autonomic computing [9], [10]. In [9], an
online control approach was presented to automatically
minimizing power consumption of CPU while providing
satisfactory response time. In [10], an architecture was
presented for an autonomic computing environment
(AUTONOMIA) with various middleware services for fault
detection, fault handling, and performance monitoring.
However, these techniques and middleware are not designed for SBS, which is more loosely coupled and more
difficult to control.
In order to provide a cost-effective way for validating
whether the design of an ASBS with QoS M/A capabilities
will meet the QoS requirements for the ASBS, it is necessary
to have proper modeling and simulation support for ASBS,
which cannot be provided by existing simulation approaches [11], [12], [13], [14], [15]. The simulation environment Discrete Event System Specification (DEVS)/DOC
[11], which supports distributed object computing concepts,
may be used to design distributed software systems, but it
is not suitable for ASBS due to lack of modeling constructs
which are needed for describing QoS aspects in SBS. Other
simulation approaches and tools, such as HLA [12] and
OPNET [13], are not suitable for developing ASBS unless
they are extended with new abstractions that can describe
key characteristics of service-oriented computing. The UML
2.0 has been suggested to support simulating software
systems, but its concept of time and execution protocol is
limited [14]. The process specification and modeling
language (PSML) [15] is useful in designing, implementing,
and testing services with simulation support, but it lacks
direct support for simulating time-based services, which is
important for describing dynamics of ASBS. Our simulation

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

models are based on service-oriented computing concepts
and will play a central role in validating the proposed
design approach for ASBS with multiple QoS.

3

OVERVIEW OF DEVELOPMENT OF ASBS

The unique characteristics of SOA, including loosely
coupled, late binding, dynamic service availability, and
the potentially large number of services available to be used
in SBS, create the following significant differences between
conventional software systems and SBS:
Services are autonomous entities that can be used by
many users and usually operate in environments not
fully known and controlled by users and developers.
Hence, the performance of SBS is more difficult to
predict than conventional software systems.
. SBS often span across the boundaries of multiple
organizations, which makes centralized management difficult.
. SBS often have many services providing similar
functionality, which gives users the opportunity to
select services more suitable for their needs.
Furthermore, service discovery and late binding
make SBS more adaptable than conventional software systems.
Due to these differences, new techniques for developing
high-quality SBS are needed. Fig. 1 shows a conceptual view
of our ASBS, in which functional services in the ASBS and
the modules for QoS M/A form a closed control loop [16].
The QoS monitoring modules collect the measurements of
various QoS aspects as well as relevant system status, which
are used by the QoS adaptation modules to adapt service
compositions and/or service configurations in ASBS to
satisfy various QoS requirements simultaneously. Our ideas
toward the development of ASBS mainly focus on the
following three aspects:
.

Methodology for service performance modeling to
capture the system dynamics affecting various QoS
aspects and their tradeoffs by analyzing data
collected in controlled experiments.
2. Approaches to developing QoS M/A modules in
ASBS based on service performance models and
users’ QoS requirements in ASBS.
3. SOA-compliant simulation models for validating
ASBS with QoS M/A modules.
Since ASBS in various application domains usually have
different QoS requirements, resource requirements, and
workload patterns, we have selected a group of example
application scenarios utilizing various multimedia services
for the development and evaluation of our results.
The scalability of our ASBS development can be viewed
from the following three aspects: 1) our service performance
modeling methodology, 2) the developed ASBS, especially
the QoS M/A modules, and 3) SOA-compliant simulations.
For 1), it is difficult to perform large-scale experiments with
many experimental conditions to cover many possible
situations in actual systems due to resource constraints.
Hence, we use SOA-compliant simulations to validate the
ASBS design under conditions that cannot be examined in
1.

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

249

Fig. 1. A conceptual view of ASBS.

small-scale experiments with actual service implementations. In our SOA-compliant simulations, basic SOA
elements, such as services, service registry and messages,
as well as service QoS can be simulated to allow the
construction of a simulated ASBS, which provides I/O and
state data of simulated services to QoS monitoring modules
[17]. Dynamic structures are supported in our SOAcompliant simulations, and enable runtime adaptation to
system behavior by QoS adaptation modules. With these
features, our SOA-compliant simulations support rapid
construction and validation of simulated ASBS with
simulated services and actual QoS M/A modules. For
2) and 3), we are currently evaluating the runtime
performance with respect to the size of our QoS monitoring
modules and the simulations developed using our new
DEVS-Suite Simulator [18].

4

ACTIVITY-STATE-QoS MODELS FOR THE
CAUSE-EFFECT DYNAMICS IN ASBS

and tradeoffs among various QoS aspects. Fig. 2 depicts the
cause-effect chain of user activities, system resource states
and QoS performance, consisting of a service request from a
user call for a system process that will utilize certain
resources and change the states of the resources in the
system environment. The changes of the resource state in
turn affect the QoS of the process. The performance models
revealing such cause-effect dynamics in SBS are called the
Activity-State-QoS (ASQ) models. Constructing ASQ models
is challenging because the differences in the types of services,
the ways of service composition, and even service implementations, will affect different resource consumptions
which in turn produce different characteristics of various
aspects of QoS and the tradeoffs among the QoS aspects. In
Sections 4.1-4.3, we will present the formulation of ASQ
models, the design of experiments for collecting data related
to user activities, system resource states, and service QoS,
and our data analysis methodology for establishing ASQ
models. We will also use an experiment with a voice

The QoS M/A modules shown in Fig. 1 are the core of ASBS
because they provide the capabilities for measuring the QoS
aspects and system status, and making decisions for adapting
service configurations in ASBS. However, these modules
cannot be developed without knowing the QoS-related
cause-effect dynamics in ASBS, especially the following:
QoS composition: How is the overall QoS of an ASBS
determined by the QoS of individual services and
the composition patterns used to compose the ASBS?
2. QoS tradeoff: How does one aspect of QoS of an ASBS
affect other aspects of QoS?
3. Environment and configuration impact: How does the
QoS of a service vary when the configuration of
the service or the status of the execution environment changes?
Hence, it is necessary to have a systematic way to identify the
underlying cause-effect dynamics driving the performance
1.

Fig. 2. The cause-effect chain of activity state QoS in SBS.

250

IEEE TRANSACTIONS ON SERVICES COMPUTING,

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

TABLE 1
Metrics for Accuracy, Timeliness, and Throughput

communication service as an example, and analytical results
of the experimental data to illustrate these results.

4.1 ASQ Models
An ASQ model is a 7-tuple <A; S0 ; S; Q; RAS ; RAQ ; RSQ >,
where A is a set of possible user activities that can be
performed on a particular service, S0 is a set of initial
system resource states, S is the set of all possible system
resource states, Q is the set of possible value for a particular
aspect of QoS, RAS is a relation defined on A  S0 ! S
representing how user activities and initial system resource
states affect future system resource states, RAQ is a relation
defined on A  S0 ! Q representing how user activities
and initial system resource states affect Q, and RSQ is a
relation defined on S  S ! Q representing how changes of
system resource states affect Q.
For each service in SBS, an ASQ model needs to be
constructed for each QoS aspect of interest to the users and
designers. All the ASQ models for various QoS aspects of all
services in SBS collectively show the cause-effect dynamics
in SBS. In SBS, user activities performed on a particular
service are carried out through service invocations with
various service parameters that will lead to different
resource consumptions and thus different QoS performance. Since these parameters reflect various levels of user
activities leading to different QoS performance, these
parameters form the set of A variables in ASQ models.
A system resource state is represented by a set of state
variables, where a state variable is a property of a certain
system resource that can be directly acquired or derived
from system monitoring, such as committed memory and
CPU utilization.
To evaluate a QoS aspect, proper metrics for the QoS
aspect need to be defined. There are well-defined metrics
for timeliness, accuracy, and throughput [4] based on which
analytical models can be developed to evaluate these three
QoS aspects based on user activities and current system

resource states of the system. Table 1 shows the metrics
used for timeliness, accuracy, and throughput in ASQ
models. These metrics are selected based on the given
multimedia services and applications. For other services
and applications, different metrics may be used. For
instance, the accuracy of a web search service will be
measured by the precision and recall of the search results,
rather than the loss rate and error rate in Table 1, where
recall is the percentage of web documents relevant to a
query retrieved by the search service.
In Section 4.3, we will discuss how RAS , RAQ , and RSQ
are determined.

4.2

Our Experiments for Collecting Data to Build
ASQ Models
To design an experiment for collecting data to build ASQ
models, we need to construct an application scenario that
uses one or more services, develop workload generation
components to generate service requests and create different operational environments for the experiment, and then
develop a data collection component to collect necessary
data for constructing ASQ models. In the following, we will
briefly describe our experiments, and use an experiment on
the voice communication service as an example.
4.2.1 Selection of Services and Constructions of
Application Scenarios for Experiments
To construct application scenarios for our experiments, we
have selected a set of three multimedia services shown in
Table 2 because these services have well-understood QoS
requirements [4] and different resource consumption
characteristics. Using these three services, the following
four application scenarios are constructed for our experiments: online radio broadcasting, video sharing, motion
analysis, and real-time surveillance camera. These four
scenarios cover different service composition patters to
construct ASQ models for the three selected services

TABLE 2
Services Selected for Our Experiments

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

251

TABLE 3
Service Parameter Settings of Our Experiments

individually as well as the compositions of these services.
For example, the scenario online radio broadcasting only
uses the voice communication service. The scenario realtime surveillance camera uses a sequential composition of
the services video streaming and motion detection.
Besides different service composition patterns, we also
considered various QoS requirements and tradeoffs covered
by these four application scenarios. For example, according
to [4], online radio broadcasting requires the delay (timeliness) to be less than 150 ms, error rate (accuracy) less than
0.1 percent, and data rates (throughput) larger than 56 Kbps.
In the online radio broadcasting scenario, there may be
many users with different minimum data quality requirements. Given limited system resources, it is necessary to
adapt the configuration of online radio broadcasting to
provide audio data with different quality to ensure that
users’ QoS requirements are satisfied while serving more
users with improved audio quality.

4.2.2 Workload Generation and Identification of
Experimental Control Variables
To construct an ASQ model <A; S0 ; S; Q; RAS ; RAQ ; RSQ >,
the experiments under various conditions representing
different user activities (A) and system resource states
(S0 ; S) need to be conducted to collect the necessary data for
determining RAS , RAQ , and RSQ . These experimental
conditions are specified by a set of experimental control
variables, which determine the system resource states, such
as the current CPU workload and network bandwidth
utilization, and certain characteristics of user activities,
such as the number of concurrent users for a service and the
quality of audio or video streams requested by users. These
control variables are identified from the parameters of the
selected services and the workload generation component
to be used in the experiments. To generate workload in our
experiments, we have developed an ExperimentLauncher
to generate a specified number of service requests within a
specified period of time to the selected services in our
experiments. We have also developed a tool to generate
controllable network traffic to simulate various network
conditions.
4.2.3 Data Collection in Our Experiments
Data to be collected in our experiments reflect the system
resource states and the three QoS aspects. In our experiments, we used IIS on Windows XP as our service platform,
and developed a system monitoring tool using Windows
Performance Objects to collect data of system status. We
also added instrumentation code in the services used in our
experiments to collect data for measuring the three QoS
aspects of the services. Data were collected periodically and
aggregated in two log files: one for the data collected from

Windows Performance Objects, and the other for the data
collected from the services.

4.2.4 Design of an Example Experiment on the Voice
Communication Service
The example experiment is based on the online radio
broadcasting scenario, in which multiple subscribers could
simultaneously use the voice communication service to
receive real-time voice streams of various qualities specified by the subscribers. We developed the voice communication service by converting an open-source Video
Conferencing software package (http://69.10.233.10/KB/
IP/Video_Voice_Conferencing.aspx) into Web Services
using C# in .NET environment. The experimental data
were collected from the service provider and subscriber(s).
In this experiment, 232 counters from nine performance
objects, including Process, Memory, Physical Disk, System,
IP, UDP, TCP, Server, and Web Services, were collected.
Three service parameters were used in the experiment:
the sampling rate for recording the voice stream, the
number of subscribers, and the size of buffer for storing
the voice stream before network transmission. Table 3
shows various settings for these service parameters. Under
each experimental condition, 60 data observations were
recorded during a period of one minute. In addition, the
same data were also collected for the no-service condition,
in which the system was monitored when the service was
not running.
Some counters from Windows performance objects
record accumulated values of system state or performance
that increase over time and/or depend on the order of
running experimental conditions. For example, Working Set
Peak from the Process object records the peak value of the
size of the working set of a process. The value of this counter
both increases over time and varies based on the past
experimental conditions. We need data screening to identify
and remove such performance counters since they cannot
accurately reflect the impact of different experimental
conditions on the system resource state and service
performance. To collect data for data screening, we
conducted two small-scale runs, each with three experimental conditions and two no-service conditions, as follows:
.

Run 1:

.

1. no-service condition,
2. [(S, C, B): (44.1 KHz, 1, 16 KBytes)],
3. (132.3 KHz, 3, 32 KBytes),
4. (220.5 KHz, 5, 48 KBytes), and
5. no-service condition;
Run 2 with the reversed order of that in Run 1.

252

IEEE TRANSACTIONS ON SERVICES COMPUTING,

Then, we conducted a complete run of the experiment
under all 125 conditions to collect the data used for
determining the effects of the conditions and constructing
ASQ models.

4.3

Data Screening and Construction of
ASQ Models
The following data analyses were carried out to screen the
data and construct ASQ models:
D1. Data screening using Mann-Whitney test [19] on the
data collected from the two small-scale runs to
identify and remove the counters whose values
depend on the time and order of running experimental conditions.
D2. Data analysis and modeling using data from the
complete run of the experiment with the following
statistical data analyses:
D2.1. ASQ relationship discovery and categorization through analysis of variance (ANOVA)
and the Tukey’s Honest Significant Difference
(HSD) test [20].
. D2.2. Generating the ASQ relationship map
capturing A-S (service activity and system
resource state) and S-Q (system resource state
and QoS) relationships.
. D2.3. ASQ modeling using Multivariate Adaptive Regression Splines (MARS) technique [21].
The results from D2.3 constitute the ASQ models, and are
used in the development of QoS M/A modules in ASBS. In
the following, we will discuss each step of the above
construction process and use the voice communication
experiment as an example to illustrate our methodology.
.

4.3.1 Data Screening
Mann-Whitney test [19] is used to identify the counters,
whose values vary based on the time and/or the order of
running experimental conditions. Mann-Whitney test is
chosen because it uses nonparametric statistics based on
ranks of the data, and thus depends little on the probability
density distribution of data [20]. For each small-scale run
and each counter from the Windows performance objects,
we conducted a Mann-Whitney test to compare each of the
three service conditions with each of the two no-service
conditions. If a counter has inconsistent behavior under the
same condition in the two runs, the counter will be removed
because the effect of the same condition on the counter
should be consistent if the counter is not affected by the
time and/or order of service conditions.
In our voice communication experiment, 106 of 232 counters remained after data screening and are considered for
further data analysis.
4.3.2 Data Analysis and Modeling
In D2, we consider the activity parameters (number of
subscribers C, sampling rate S, and buffer size B in our
voice communication experiment) as activity variables (A
variables), and the remaining performance counters after
D1 as state and QoS variables (S and Q variables).
In D2.1, ANOVA [20] is first used to analyze the effects of
the activity variables in various experimental conditions
on each remaining counter. If the effects are statistically

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

significant based on the ANOVA results, the Tukey’s HSD
test is performed to determine how different levels of the
activity variables affect the state and QoS variables. Based on
the Tukey’s HSD test results, the qualitative A-S (between an
A variable and an S variable) and S-Q (between an S variable
and a Q variable) relationships are revealed and further
categorized into a certain type of ASQ relationships. In D2.2,
an ASQ relationship map is constructed by representing
each A, S, or Q variable as a node and an A-S or S-Q
relationship as a link between the nodes. In D2.3, each
qualitative A-S and S-Q relationship represented in the ASQ
relationship map constructed in D2.2 are refined into a
quantitative prediction model of the corresponding
S variables or Q variables. Since A-S and S-Q relationships
are mostly nonlinear, the MARS technique is used to build a
nonlinear regression model for each relationship.
In our voice communication experiment, ANOVA revealed 28 performance counters that at least one of the three
activity variables has significant effects on. The Tukey’s
HSD test further revealed how these 28 counters were
affected by the activity variables. Based on the Tukey’s HSD
test results, the 28 performance counters were grouped into
five categories of A-SQ relationships as shown in Table 4.
Among these five categories, the inconsistent change
patterns of the seven variables in category 5 indicated
that these variables were not only affected by the voice
communication service, but also significantly affected by
other system activities. Hence, only the 21 variables in
categories 1-4 were further analyzed. Among the 21 variables
in categories 1-4, some variables provide similar information. For example, the five variables in categories 1 and 4
reflect the memory consumption, only from different angles
(consumed and remaining) and with different units (byte,
KByte, and MByte). After removing such redundancy, we
identified five state variables and one QoS variable from the
21 variables for further analysis. Fig. 3 shows the ASQ
relationship map constructed using these six variables. For
each state or QoS variable in the ASQ relationship map, we
build a regression model to capture the quantitative A-S or
S-Q relationship. As an example, we show the regression
model for the QoS variable (Fragments Created/sec_IP) here.
In this regression model, QF C denotes the value of Fragments
Created/sec_IP. SIO , ST C , and SCB denote the value of IO
Other Operations/sec_Process, Thread Count_Process, and Committed Bytes_Memory.
ð ÞQF C ¼ 969:7692  0:1815  maxð0; SIO  1125:777Þ
 1:3904  maxð0; 1125:777  SIO Þ  87:4388
 maxð0; ST C  30Þ  9:8643  maxð0; 30  ST C Þ
þ 93:5347  maxð0; ST C  37Þ þ 1:5414e  05


maxð0; SCB  490;037; 200Þ þ 3:7299e  05
maxð0; SCB  518;844;400Þ  7:0875e  06

maxð0; 518;844;400  SCB Þ:


The R square value for this regression model is 0.9787,
which shows that this regression model accurately captures
the S-Q relationships observed in our experiment.

5

DEVELOPMENT OF QOS M/A MODULES

In various application domains, QoS M/A in SBS may have
various objectives, such as optimizing resource utilization,

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

253

TABLE 4
Five Categories of A-SQ Relationships

Fig. 3. The ASQ relationship map for our voice communication experiment.

maximizing service throughput, and improving service
availability. Our research on the development of QoS M/A
modules mainly focused on improving the overall QoS of
applications involving long-running workflows (WFs) to
satisfy users QoS requirements as much as possible. In this
section, we will first present an overview of a model for
distributed workflow execution, monitoring, and adaptation
in SBS, and discuss how various software entities in this
model interact among them to support QoS M/A in ASBS.
Then, we will discuss how QoS monitoring modules in
ASBS are developed based on the ASQ models generated in
Section 4. Finally, we will also discuss some research ideas
on the development of QoS adaptation modules in ASBS.

A Model for Distributed Workflow Execution,
Monitoring, and Adaptation in SBS
Based on our work in [22], [23], we establish a model shown
in Fig. 4 for distributed workflow execution, monitoring,
and adaptation in SBS. In this model, functional services in
SBS are provided by distributed hosts, which also provide
various system services, including service discovery,
performance monitoring, and resource management. Each
host also provides a persistent data repository, in which
service performance models (ASQ models) and data
collected from system monitoring are stored. A workflow
in SBS is coordinately executed by distributed WF agents,
each of which is responsible for invoking one or more

functional services. The status of each functional service
used in the workflow is observed by a WF monitor, which
checks whether the service QoS satisfies the user requirements. If the QoS of a functional service no longer satisfies
the user requirements, the WF adaptors responsible for the
functional service and the subsequent services to be
invoked in the workflow adjust the configuration of these
services or find alternative services to provide satisfactory
QoS. The WF agents, monitors, and adaptors are deployed
on distributed hosts, and communicate through named
input/output channels supporting the publish/subscribe
communication model.

5.1

Fig. 4. A model for distributed workflow execution, monitoring, and
adaptation in SBS.

254

IEEE TRANSACTIONS ON SERVICES COMPUTING,

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

invoked), and highlight the changes of the control flows
during the execution phase.
Preexecution Phase. Before S is invoked, M, A, and W
perform the following activities:
.

.

Fig. 5. Internal control flow of M during the preexecution phase.

5.2

Interactions Among WF Agents, Monitors, and
Adaptors
Consider a functional service S deployed on a host H in a
network N, and the WF agent W , monitor M, and adaptor A
responsible for invoking, monitoring, and controlling S. To
show how W ; M, and A interact with each other to support
QoS M/A, we will describe the internal control flows of W ,
M, and A during the preexecution phase (i.e., before S is

Fig. 6. Internal control flow of A during the preexecution phase.

.

As shown in Fig. 5, M first collects data for the status
of S, H, and N. If the service parameters of S are
available, M retrieves the appropriate service performance model for S, estimates the QoS of S based on
the service parameters and the collected data, and
checks whether the estimated QoS satisfies the
QoS requirements for S. If not, M notifies A about
the SERVICE_REPLACEMENT message from A, it
changes the monitoring target to the new service
replacing S. If M receives a SERVICE_RECONFIGURATION message from A, M retrieves the new
service performance model from the persistent data
repository of H and continues to monitor S. M repeats
the above activities periodically until S is invoked or a
TERMINATION message is received.
As shown in Fig. 6, A waits for messages from M
and other WF adaptors. If there are unsatisfied QoS
requirements for S or unexpected delay in executing
a service invoked before S in the workflow, A finds a
new service configuration for S that can satisfy the
QoS requirements or reduce the expected delay for
executing S. If a new service configuration is found,
A sends a SERVICE_RECONFIGURATION message
to M and W . If not, A finds a replacement for S that
can satisfy QR or reduce the expected execution
delay. If such a replacement is found, A sends a
SERVICE_REPLACEMENT message to M and W .
Otherwise, A sends out a TERMINATION message
and terminates. A repeats the above activities until S
is invoked or A needs to terminate.
As shown in Fig. 7, W waits for the service
parameters of S, and messages from M and A. If
the service parameters of S are ready, W sends M the
service parameters. If a CHECKPOINT_CLEAR message is received from M, W checks whether all the
services, which must finish before S starts. If yes, W

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

255

Fig. 7. Internal control flow of W during the preexecution phase.

invokes S immediately. After invoking S; W notifies
M and A to enter the execution phase. If a
SERVICE_REPLACEMENT or a SERVICE_RECONFIGURATION message is received from A, it is selfreconfigured to use the new service or the new
service configuration.
Execution Phase. During the execution of S, M, A, and W
perform similar activities as the preexecution phase, except
the following:
.

.

.

M measures the actual QoS of S instead of
estimating the possible QoS. In addition, M checks
whether the current service delay is greater than the
maximum acceptable delay of S. If yes, M notifies
the WF adaptors responsible for the services to be
invoked after S in the workflow about the unexpected delay in the execution of S.
A notifies W to suspend the execution of S before
trying to find a new service configuration or a
replacement for S.
W suspends the execution of S if A notifies W to do
so. The execution of S will restart or resume when
W receives a SERVICE_REPLACEMENT or a SERVICE_RECONFIGURATION message.

5.3 Development of QoS Monitoring Modules
We have developed an approach to automatically generating QoS monitoring modules for a workflow based on the
workflow specification, including the services used in the
workflow, the control flow structure of the workflow, and
the QoS requirements for the workflow, and the ASQ
models of the services to be used in the workflow. We
assume that the supported configurations for each service
used in the workflow are also known a priori. Our approach
consists of the following three steps: S1) Decomposing the
QoS requirements for the workflow into requirements for
individual services used in the workflow. S2) Synthesizing
-Calculus [22] descriptions of the QoS monitoring modules. S3) Compiling the synthesized -Calculus descriptions
into executable code.

In this paper, we will mainly discuss S2. For S3, we
modified the compiler we developed in [22]. For S1, we
represent the QoS requirements as a 6-tuple,
<max loss; max error; max service delay;
max network delay; min data rate; min bandwidth>;
based on the six metrics shown in Table 1. Since the ASQ
models generated in Section 4 are formulas similar to ( ),
the range of possible QoS of a service can be calculated
based on the ASQ models of the service given the supported
service configurations, which specify the range of possible
values of activity variables used in the ASQ models. Then,
the decomposition of QoS requirements can be easily done
based on the control flow structure of the workflow and the
range of possible QoS of the services used in the workflow.
After the decomposition of QoS requirements is done,
the QoS monitoring modules for the workflow will be
generated using the following two templates to support the
interactions between monitoring modules and WF agents as
well as QoS adaptation modules during the preexecution
phase (Lines 1-34), and the execution phase (Lines 35-61):
Template 1:
1 fix $MONIT OR$ P reExecðtuple s info; int t;
tuple counters; tuple qrÞ¼
2
ððtime t:let tuple s datað$S COUNT ER1 $; . . . ;
$S COUNT ERm $Þ¼
3
SystemService:performanceMonitoring
ðtuple counters->s counters; null;
tuple s infoÞinstantiate zeroÞ
4
þ
5
ðtuple $SERV ICE$ paramðtuple paramsÞ.
6
let int handle¼ASQ:loadModelðtuple params;
tuple s infoÞinstantiate zeroÞ
7
þ
8
boolean $SERV ICE$ invokedðboolean invokedÞ
9
þ
10
tuple $SERV ICE$ adaptðtuple adaptÞ
11
þ
12
tuple $W F $ controlðtuple controlÞÞ.

256

13
14
15
16
17
18
19
20
21
22
23
24
25

26
27
28
29
30
31
32
33
34

IEEE TRANSACTIONS ON SERVICES COMPUTING,

if ðinvoked !¼ null and invoked ¼¼ trueÞthen
$MONIT OR$ Execðs info; t; params;
counters; qrÞ
else if ðadapt !¼ nullÞ then
if ðadapt->cmd ¼¼
‘‘SERV ICE REP LACEMENT ’’Þthen
$MONIT OR$ P reExecðadapt->s info;
t; adapt->counters; adapt->qrÞ
else if ðadapt->cmd ¼¼
‘‘SERV ICE RECONF IGURAT ION’’Þthen
let int handle¼ASQ:loadModelðtuple
params; tuple adapt->s infoÞinstantiate
$MONIT OR$ P reExecðadapt->s info;
t; counters; adapt->qrÞ
else zero
else if ðcontrol !¼ null and control->cmd ¼¼
‘‘T ERMINAT ION’’Þ then
zero
else if ðparams !¼ nullÞ then
let tuple eqðfloat s delay; . . . ; float bwÞ ¼
ASQ:qosEstimationðint handle; tuples dataÞ
instantiate
let boolean b ¼ ASQ:qosCompareðtuple
eq; tuple qrÞinstantiate
if ðb ¼¼ trueÞthen
tuple $SERV ICE$ control<
‘‘CHECKP OINT CLEAR’’; int t>.
$MONIT OR$ P reExecðs info; t;
counters; qrÞ
else
tuple $SERV ICE$ mon<‘‘QoS NOT
SAT ISF IED’’; s info; params; qr>.
$MONIT OR$ P reExecðs info; t;
counters; qrÞ
else
$MONIT OR$ P reExecðs info; t;
counters; qrÞ

Template 2:
35 fix $MONIT OR$ Execðtuples info; intt;
tuple params; tuple counters; tuple qrÞ ¼
36
ððtime t:let tuple datað$COUNT ER1$; . . . ;
$COUNT ERn$Þ ¼
37
SystemService:performanceMonitoring
ðtuple counters->s counters; tuple counters->
q counters;
38
tuple s infoÞinstantiate zeroÞ
39
þ
40
tuple $SERV ICE$ adaptðtuple adaptÞ
41
þ
42
tuple $W F $ controlðtuple controlÞÞ.
43
if ðadapt !¼ nullÞ then
...
==Similar to lines 16-23
51
else
52
let tuple cqðfloat s delay; . . . ; float bwÞ ¼
ASQ:currentQoSðtuple dataÞ instantiate
53
let boolean b ¼ ASQ : qosCompareðtuple cq;
tuple qrÞ instantiate

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

if ðb ¼¼ trueÞ then
$MONIT OR$ Execðs info; t;
params; counters; qrÞ
else
tuple $SERV ICE$ mon<
‘‘QoS NOT SAT ISF IED’’; s info;
params; qr>.
if ðqr->MAX DELAY <cq->
s delayÞ then
tuple $W F $ control
<‘‘UNEXP ECT ED DELAY ’’;
s info; cq->s delay; qr->
MAX DELAY >.
$MONIT OR$ Execðs info; t;
params; counters; qrÞ
else $MONIT OR$ Execðs info; t;
params; counters; qrÞ

54
55
56
57

58
59

60
61

The above templates are described in -Calculus [22], which
is a process calculus capable of describing service invocations, control flow structures of processes, and communications among processes. $MONITOR$, $S COUNTER$,
$WF$, and $SERVICE$ are placeholders in the templates.
During the synthesis of monitoring modules, these placeholders will be replaced by the identifiers of monitoring
modules, performance counters, workflows, and services.
The following five services are used in the above
templates:
1.

2.

3.

4.

5.

SystemService:performanceMonitoring takes the identifiers of state counters (e.g., s counters in line 2) and
QoS counters (e.g., q counters in line 38) and the
description of the service to be monitored as inputs
(e.g., s info in line 3), and returns a tuple containing
the values of the state and QoS counters as the result.
The description of the service to be monitored
includes the identifiers of the service, the host
providing the service, and the network in which
the host resides, and the supported configurations of
the service.
ASQ:loadModel takes the parameters (see params in
line 6) and description of the service to be monitored
as inputs, loads the corresponding ASQ models
from the persistent data repository (see Fig. 4) to
dynamically instantiate an ASQ model interpreter,
which will be used to estimate the possible QoS of
the service to be monitored, and returns the handle
(see handle in line 6) to the ASQ model interpreter.
ASQ:qosEstimation takes the collected data of the
state counters (see s data in line 25) and the handle
to the ASQ model interpreter for the service to be
monitored as inputs, and calculates the estimated
service QoS as the result (see eq in line 25).
ASQ:qosCompare takes the estimated service QoS and
the QoS requirements (see qr in line 26) as inputs,
checks whether the QoS requirements can be
satisfied, and returns true or false.
ASQ:currentQoS takes the collected data of the state
and QoS counters (see data in line 52) as the input,
and calculates the current service QoS as the result
based on the metrics in Table 1.

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

Notice that the above services are only interfaces for the
actual service implementations because we want to keep the
synthesized -calculus descriptions platform-independent.
The compiler we developed for the QoS monitoring
modules can translate the invocations of the above services
to the platform-dependent code. The actual service implementations have been done on Windows XP. In particular,
ASQ:loadModel, ASQ:qosEstimation, ASQ:qosCompare, and
ASQ:currentQoS are implemented according to our ASQ
models and QoS metrics. SystemService:performanceMonitoring is implemented using Windows Performance Objects.
Line 1 declares a process named $MONIT OR$ P reExec,
which takes four inputs: information of the service to be
monitored, interval of data collection, identifiers of the
counters to be monitored, and the QoS requirements of the
service. The process $MONIT OR$ P reExec is the sequential composition (denoted by “.”) of two subprocesses,
defined in lines 2-12 and lines 13-34, respectively.
The subprocess in Lines 2-12 is the composition of five
subprocesses using nondeterministic choice (the “þ” in
lines 4, 7, 9, and 11), which means that at least one of the
five subprocesses will be executed in runtime before
executing the subprocess in lines 13-34. The five subprocesses are defined as follows:
Lines 2-3. time t in line 2 is a time-out process, which
puts the monitoring module into sleep for t seconds.
The remaining part of lines 2-3 is an invocation of
SystemService:performanceMonitoring and assigns the
result to a tuple s data. Tuple is a structured data
type allowing multiple variables with various data
types to be grouped together. Access to a variable in
the tuple is represented by TupleName->VariableName, such as counters->s counters in line 3.
. Lines 5-6. tuple $SERV ICE$ paramðtupleparamsÞ
in line 5 defines an input action for accepting the
parameters of $SERV ICE$ through an input
channel named $SERV ICE$ param. After receiving
the service parameters, ASQ:loadModel will be
invoked in line 6 to load the corresponding ASQ
models for $SERV ICE$.
. Line 8 defines an input action for receiving the
notification of the invocation of $SERV ICE$.
. Line 10 defines an input action for receiving
adaptation commands.
. Line 12 defines an input action for receiving workflow control message.
The subprocess in lines 13-34 checks for various conditions
following the process described previously (see Fig. 5) for
the interaction between QoS monitoring modules and WF
agents as well as QoS adaptation modules. Lines 35-61
define a process to be executed by a monitoring module
during execution phase, which is quite similar to lines 1-34.
.

5.4 Development of QoS Adaptation Module
Our main idea for the development of QoS adaptation
module is to first formulate QoS adaptation in ASBS as a
multiobjective optimization problem [24]. We will define
QoS expectation functions to quantify gains and losses for
satisfying and violating users’ QoS requirements, respectively. The ASQ models and QoS expectation functions will
be used as the constraints and objective functions, respectively. To solve this optimization problem, we will develop

257

optimization and control synthesis algorithms for generating appropriate adaptive control commands that adapt the
system and service configurations to achieve the desired
QoS tradeoffs. Then, the QoS adaptation modules in ASBS
will be developed based on the optimization and control
synthesis algorithms.

6

VALIDATION OF ASBS DESIGN THROUGH
SOA-BASED SIMULATIONS

Simulation modeling offers important and unique capabilities for analysis and design of service-oriented computing
systems that must satisfy multiple, competing QoS requirements. In order to aid design of SBS, it is important to
employ a suitable modeling framework that can account
for the SOA concepts. Based on the DEVS framework [25]
and SOA concepts, a novel set of generic SOA-compliant
simulation models are developed.

6.1 SOA-Compliant DEVS Modeling
The DEVS is a system-theoretic approach to specifying
dynamical systems. It provides two canonical model types,
called atomic and coupled models. An atomic model of a
system can be defined in terms of input, output, state, and
time sets with functions that determine next states and
outputs, given current states and inputs at arbitrary time
instances. Together, external, internal, confluent, output,
and time advance functions define a component’s behavior
over time. A coupled model is defined in terms of atomic
and/or coupled models. As in an atomic model, a coupled
model contains a set of inputs, a set of outputs, a set of
component names, a set of components, and a set of
coupling relationships among the input and output ports
of the composed model components. Atomic and coupled
models interact with one another using messages (called
entities) that are exchanged via input and output ports.
The SOA framework has a higher level of abstraction
compared to the DEVS framework. The basic SOA elements
include services, service registry, service discovery, and
messages. We have defined a set of SOA-compliant DEVS
(SOAD) elements [17] that correspond to the SOA (see
Table 5). The SOAD models communicate with messages
that represent service description, look up, and service
messages. Runtime service registry and discovery are
defined via an executive model. Given a set of services, it
can handle other services to be dynamically added or
conversely one or more existing services to be removed. The
SOAD approach supports the composition of publisher,
subscriber, and broker services to create flat and hierarchical
compo site services. Furthermore, the SOAD modeling
framework can be extended to support runtime composition
of services using the DEVS dynamic structure modeling.
Services communicate with one another via messages that
contain service description or other content consistent with
a chosen messaging framework. For example, a message
from the broker to the subscriber is a service description
which contains an abstract definition (an interface for the
operation names and their input and output messages) and
a concrete definition (consisting of the binding to physical
transport protocol, address or endpoint, and service).
Another message could be from a publisher to a subscriber.
The implementation of these messages can be based on
SOAP. To support the essential capability for simulating

258

IEEE TRANSACTIONS ON SERVICES COMPUTING,

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

TABLE 5
SOA Model Elements to the DEVS Model Elements Mapping

SBS, the SOAD modeling approach supports simulation of
hierarchical service compositions. A hierarchical service
composition has a publisher or subscriber service which
itself is composed of other (flat or hierarchical) services.
Since broker service is required for both primitive (flat) and
composite (hierarchical) service compositions, either a
single broker service or multiple broker services may be
used. The DEVS model couplings support a single broker
for primitive and composite service compositions. The
common concept of DEVS and SOA modularity allows
creating composite service composition without restrictions.
Generic SOAD simulation models are designed and
introduced into the object-oriented DEVS-Suite simulator, a
new extension of the object-oriented DEVSJAVA Tracking
Environment [18]. The DEVS-Suite simulator supports
parallel, efficient execution of the above SOAD models.
The simulator affords animation of simulation execution
and visualization of simulation results as time trajectories.
With this simulator, common variations of SBS can be
systematically modeled using SOA concepts. The simulator
can address the difficulties associated with creating a real
testbed and running extensive experiments (e.g., a system
having many hundreds of services). The fundamental
architecture and high-level design of software-based systems can be simulated and validated before low-level
design, implementation, and testing.

6.2

Service-Based Software System Modeling
and Simulation Results
To show the use of service-based software system modeling, a voice communication service described in Section 4.2
is considered. The voice communication application service,

Fig. 8. Instantaneous throughputs of the simulated and real publisher.

user services, and the broker service are modeled based on
the actual services implemented in .Net. A model of a
simple network with configurable delay and throughput is
also developed. The network model is an abstraction of the
communication links and routers/switches that allow the
users to subscribe to the voice communication service via
the broker. Each publisher, subscriber, broker, and network
model has a corresponding transducer model. These
transducer models are developed for systematic collection
and analysis of simulation data (e.g., the amount of data
that is transmitted among services as well as data
transmission delays). These data sets are important for
evaluating the throughput and timeliness of a system.
After constructing the simulated voice communication
service, we conducted simulated experiments similar to the
example experiment shown in Section 4. Fig. 8 shows that the
instantaneous throughput of the simulated and the actual
voice communication services where similar stochastic
behavior is observed. Fig. 9 shows that the average
throughput of the simulated service is similar to that of the
actual service, when sampling rates are varied. The
dynamics of the actual service in terms of its individual
parts and their different compositions can be studied at a
desirable level of detail using SOA-compliant simulated
voice communication. Fig. 10 shows a simulation scenario,
where two different sampling rates are requested by the five
subscribers. The resultant dynamics show the impact of
background traffic on system delay QoS for subscribers due
to the decrease in network bandwidth with increasing
background traffic. Therefore, the dynamics of ASBS designs

YAU ET AL.: TOWARD DEVELOPMENT OF ADAPTIVE SERVICE-BASED SOFTWARE SYSTEMS

259

Fig. 9. Average throughputs of the simulated and real publisher for different sampling rates.

can be developed with direct support for measuring their
QoS attributes like timeliness, throughput, and accuracy.

6.3 Validation Using Simulation
The SOAD modeling framework with the DEVS-Suite
simulator offers a foundation for creating and simulating
SBS. These simulation models help validating design of
ASBS. With simulated services, we are able to 1) design and
verify the structure and behavior of SOA-compliant service
application models in terms of their time-based QoS
attributes, 2) conduct simulation scenarios and validate
design models against real experimental settings, and 3) use
validated simulation models with real M/A modules to
support rapid design and validation of ASBS. Simulation of
the composite services is necessary for the design and
development of the M/A modules.

7

CONCLUSIONS AND FUTURE RESEARCH

In this paper, we have presented our ideas toward the
development of ASBS. We first presented our methodology for constructing ASQ models, and the summary of the
results from our experiment based on a voice communication service to illustrate this methodology. We have

also presented how QoS monitoring modules in ASBS can
be developed based on the ASQ models, and the main
idea on the development of QoS adaptation modules in
ASBS. Finally, we have discussed our SOA-compliant
DEVS modeling technique, which can support rapid
design and validation of ASBS. Our approach will be
extended to fully address various issues in developing
ASBS with multiple QoS requirements, such as incorporating the impact of external events on various QoS aspects
(especially security) in our ASQ models, and developing a
performance index to provide a unified metric for multiple
QoS performance of ASBS. In addition, we will improve
our data analysis methodology for constructing ASQ
models, develop algorithms for tradeoffs among multiple
QoS aspects and methods for solving the optimization
problem for QoS adaptation, and evaluate and improve
the scalability of our approach.

ACKNOWLEDGMENTS
This work is supported by the US National Science
Foundation under grant number CCF-0725340.

REFERENCES
[1]
[2]
[3]
[4]
[5]

[6]
Fig. 10. Average system delay of the simulated and real subscriber.

M. Reisslein, K.W. Ross, and S. Rajagopal, “A Framework for
Guaranteeing Statistical QoS,” IEEE/ACM Trans. Networking,
vol. 10, no. 1, pp. 27-42, Feb. 2002.
X. Xiao, T. Telkamp, V. Fineberg, C. Chen, and L.M. Ni, “A
Practical Approach for Providing QoS in the Internet Backbone,”
IEEE Comm., vol. 40, no. 12, pp. 56-62, Dec. 2002.
Z. Yang, N. Ye, and Y.-C. Lai, “QoS Model of a Router with
Feedback Control,” Quality and Reliability Eng. Int’l, vol. 22, no. 4,
pp. 429-444, June 2006.
Y. Chen, T. Farley, and N. Ye, “QoS Requirements of Network
Applications on the Internet,” Information-Knowledge-Systems
Management, vol. 4, no. 1, pp. 55-76, 2004.
J. Zhou, K. Cooper, I. Yen, and R. Paul, “Rule-Base Technique for
Component Adaptation to Support QoS-Based Reconfiguration,”
Proc. Ninth IEEE Int’l Symp. Object-Oriented Real-Time Distributed
Computing, pp. 426-433, May 2005.
J. Jin and K. Nahrstedt, “On Exploring Performance Optimization
in Web Service Composition,” Proc. ACM/IFIP/USENIX Int’l
Middleware Conf., pp. 115-134, Oct. 2004.

260

[7]

[8]

[9]

[10]

[11]

[12]
[13]
[14]

[15]

[16]

[17]
[18]
[19]
[20]
[21]
[22]

[23]

[24]
[25]

IEEE TRANSACTIONS ON SERVICES COMPUTING,

G. Canfora, M. Di Penta, R. Esposito, and M.L. Villani, “An
Approach for QoS-Aware Service Composition Based on Genetic
Algorithms,” Proc. Conf. Genetic and Evolutionary Computation,
pp. 1069-1075, June 2005.
C. Guo, M. Cai, and H. Chen, “QoS-Aware Service Composition
Based on Tree-Coded Genetic Algorithm,” Proc. 31st Ann. Int’l
Computer Software and Applications Conf. (COMPSAC ’07), pp. 361367, July 2007.
N. Kandasamy, S. Abdelwahed, and J.P. Hayes, “Self-Optimization in Computer Systems via On-Line Control: Application to
Power Management,” Proc. First Int’l Conf. Autonomic Computing,
pp. 54-61, May 2004.
X. Dong, S. Hariri, L. Xue, H. Chen, M. Zhang, S. Pavuluri, and S.
Rao, “AUTONOMIA: An Autonomic Computing Environment,”
Proc. IEEE Int’l Conf. Performance, Computing, and Comm., pp. 61-68,
Apr. 2003.
D.R. Hild, H.S. Sarjoughian, and B.P. Zeigler, “DEVS-DOC: A
Modeling and Simulation Environment Enabling Distributed
Codesign,” IEEE Trans. Systems, Man, and Cybernetics, Part A,
vol. 32, no. 1, pp. 78-92, Jan. 2002.
J.S. Dahmann, “High Level Architecture for Simulation,” Proc.
First Int’l Workshop Distributed Interactive Simulation and Real-Time
Applications (DIS-RT ’97), pp. 9-14, Jan. 1997.
OPNET, “OPNET Modeler,” http://opnet.com, Dec. 2005.
D. Huang and H. Sarjoughian, “Software and Simulation
Modeling for Real-Time Software-Intensive Systems,” Proc. Eighth
IEEE Int’l Symp. Distributed Simulation and Real-Time Applications,
pp. 196-203, Oct. 2004.
W.T. Tsai, Y. Chen, R. Paul, X. Zhou, and C. Fan, “Simulation
Verification and Validation by Dynamic Policy Specification and
Enforcement,” SIMULATION, vol. 82, no. 5, pp. 295-310, May
2006.
S.S. Yau, N. Ye, H. Sarjoughian, and D. Huang, “Developing
Service-Based Software Systems with QoS Monitoring and
Adaptation,” Proc. 12th IEEE Int’l Workshop Future Trends of
Distributed Computing Systems (FTDCS ’08), pp. 74-80, Oct. 2008.
H.S. Sarjoughian, S. Kim, M. Ramaswamy, and S.S. Yau, “A
Simulation Framework for Service-Oriented Computing,” Proc.
Winter Simulation Conf., pp. 845-853, Dec. 2008.
DEVS-Suite Simulator, http://sourceforge.net/projects/devssuitesim, May 2009.
H.B. Mann and D.R. Whitney, “On a Test of Whether One of Two
Random Variables is Stochastically Larger than the Other,” Annals
of Math. Statistics, vol. 18, pp. 50-60, 1947.
D.C. Montgomery, G.C. Runger, and N.F. Hubele, Engineering
Statistic, fourth ed. John Wiley, 2006.
J.H. Friedman, “Multivariate Adaptive Regression Splines (with
Discussion),” Annals of Statistics, vol. 19, pp. 1-141, Mar. 1991.
S.S. Yau, H. Davulcu, S. Mukhopadhyay, H. Gong, D. Huang, P.
Singh, and F. Gelgi, “Automated Situation-Aware Service Composition in Service-Oriented Computing,” Int’l J. Web Services
Research (IJWSR ’07), vol. 4, no. 4, pp. 59-82, Oct.-Dec. 2007.
S.S. Yau, D. Huang, and L. Zhu, “An Approach to Adaptive
Distributed Execution Monitoring for Workflows in Service-Based
Systems,” Proc. 31st Ann. Int’l Computer Software and Application
Conf., vol. 2, pp. 211-216, July 2007.
R.L. Rardin, Optimization in Operations Research. Prentice Hall,
1998.
B. Zeigler, T.G. Kim, and H. Praehofer, Theory of Modeling and
Simulation, second ed. Academic Press, 2000.

Stephen S. Yau received the PhD degree in
electrical engineering from the University of
Illinois, Urbana. He is the director of the
Information Assurance Center and a professor
of computer science and engineering at Arizona State University, Tempe. He was previously
with the University of Florida, Gainesville, and
Northwestern University, Evanston, Illinois. He
served as the president of the IEEE Computer
Society and the editor-in-chief of Computer. His
current research is in distributed and service-oriented computing,
adaptive middleware, software engineering, trustworthy computing,
and data privacy. He is a fellow of the IEEE and a fellow of the
American Association for the Advancement of Science.

VOL. 2,

NO. 3,

JULY-SEPTEMBER 2009

Nong Ye received the PhD degree in industrial
engineering from Purdue University. She also
received the MS degree from the Chinese
Academy of Sciences and the BS degree from
Peking University, in computer science. She is
a senior member of the IEEE and a professor
of industrial engineering at Arizona State
University (ASU), Tempe. Before joining ASU,
she was an assistant professor of industrial
engineering at the University of Illinois, Chicago. Her research interests are in information assurance, quality of
service, and data mining.
Hessam S. Sarjoughian received the PhD
degree in electrical and computer engineering
from the University of Arizona, Tucson. He is an
assistant professor of computer science and
engineering at Arizona State University, Tempe.
He is a codirector of the Arizona Center for
Integrative Modeling & Simulation (ACIMS). His
research focuses on modeling and simulation
methodologies, agent-based simulation, visual
simulation modeling, model composability, and
codesign modeling. He is a member of the IEEE.
Dazhi Huang received the BS degree in computer science from Tsinghua University, China.
He is a PhD student in computer science at
Arizona State University, Tempe. His research
interests include middleware, mobile and ubiquitous computing, and workflow systems in service-oriented computing environments.

Auttawut Roontiva received the BS degree
from Chulalongkorn University in 2000 and the
MS degree in mechanical engineering from
Arizona State University (ASU), Tempe, in
2004. He is presently a PhD student in industrial
engineering at ASU. His research interests
include information system security and assurance, adaptive service-based software systems,
and data mining.

Mustafa Gökçe Baydogan received the MS
degree in industrial engineering from Middle
East Technical University, Turkey, in 2008. He
is a PhD student in industrial engineering at
Arizona State University, Tempe. His research
interests are data mining, heuristic search,
and simulation.

Mohammed A. Muqsith is a PhD student in
computer science at Arizona State University
(ASU). He is a member of the Arizona Center for
Integrative Modeling & Simulation at ASU. His
research interests include simulation-based design, service-oriented software/hardware codesign, system performance modeling, software
engineering, and networked embedded systems.

Proceedings of the 2004 Winter Simulation Conference
R .G. Ingalls, M. D. Rossetti, J. S. Smith, and B. A. Peters, eds.

MULTI-FORMALISM MODELING APPROACH FOR
SEMICONDUCTOR SUPPLY/DEMAND NETWORKS
Gary W. Godding

Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Dept.
Arizona State University
Tempe, AZ 85281-8809, U.S.A.

Component Automation Systems
Intel Corporation
5000 W. Chandler Blvd – MS CH3-68
Chandler, AZ 85226, U.S.A.

Karl G. Kempf
Decision Technologies
Intel Corporation
5000 W. Chandler Blvd – MS CH3-10
Chandler, AZ 85226, U.S.A.

Although the algorithms, models, and data usually
overlap to some degree, decision making and decision execution are generally very different computationally. There
are at least two extreme options that can be employed to
address these differences.
On one hand, a system design can force the required
functionality into a single monolithic model. However the
result is often very difficult to develop, validate, verify,
use, and maintain because of the confounding of the decision making and decision execution processes.
On the other hand, the models and algorithms for decision making and decision execution can be designed and
implemented separately and then used together in an integrated fashion. One major difficulty with this approach lies
in the details of integrating the operation of the decision
making module and the decision execution module. While
there exists a rich and extensive literature on computational
models for both decision making and decision execution,
most examples where these models are used in combination rely on ad hoc approaches to integration of the specific
implementations of interest. From a more general and formal perspective, the computational issues with robust integration are model composibility (our particular focus here)
and module interoperability (see Figure 2). Composability
is concerned with how to create different models that are
semantically consistent. Interoperability is focused on the
software used to support communication and synchronization at runtime.
In addition to composibility and interoperability, another domain-independent issue inherent in this approach
is the concept of “netting”. Execution of the plan will

ABSTRACT
Building computational models of real world systems usually requires the interaction of decision modules and simulation modules. Given different models and algorithms, the
major hurdles in building a principled and robust system
are model composibility and algorithm interoperability.
We describe an approach to the composibility problem including initial results. This exposition is given in the context of Linear Programming as the decision technique and
Discrete Event Simulation as the simulation technique,
both applied to the design and operation of semiconductor
supply/demand networks.
1

INTRODUCTION

A recurring theme in developing computational models of
the real world is the necessity of including both physical
and logical processes. The system being studied presumably has some physical behavior that develops over time,
but requires direction from some logical process to select
among possible actions to achieve a system performance
goal (see Figure 1).
Decision Algorithm
Past and Current
World State

Current and Future
Action Plan
Projection Algorithm

Figure 1: The Abstract Computation Problem
232

Godding, Sarjoughian, and Kempf
To ground our exposition of composability, interoperability, and netting, we have selected, without loss of
generality, specific decision and execution approaches for
experimentation. From the range of possible mathematical
and heuristic decision algorithms, Linear Programming
(LP) has been selected. Discrete Event Simulation (DES)
has been selected from the range of possible continuous
and discrete real world execution projection approaches.
To reiterate, the focus of our work is not LPs and DESs for
SSDN, but rather the general and specific aspects of the
composability of their models in this application domain.
We have described our work in the domains of LP and
DES in other publications (Kempf et al. 2001, Kempf
2004) and the references they contain.
The practical use of the test-bed resulting from robust
model composibility (and attendant module interoperability and netting) is the improved design and operation of the
SSDN example. This type of economic system can generate, in the case of Intel Corporation, 30B$ in annual revenue. Given the complexity of operating such a network, the
test bed partially described here is invaluable. The ability
to refine decision processes as well as the number, location, topology, and properties of the physical entities without the overhead of continuously reconfiguring interfaces
is a major improvement over existing methods.

Decision
Model

Decision
Engine

Projection
Model

Projection
Engine

Module
Interoperability

Model
Composability

Decision Module

Projection Module

Figure 2: Composibility and Interoperability
rarely (if ever) produce the state of the stochastic world
expected by the planning module. This “netting” will have
to reconcile the expected and actual states of the world for
the decision module, and while this problem is universal,
its solution is highly dependent on the details of the domain. Composibility is similar in that it is universal in this
class of problems, but different in that it has both domain
independent and dependant facets as we will show here.
To explore these issues without loss of generality, we
have selected as an application domain that of managing a
semiconductor supply/demand network (SSDN) (Figure 3).
The decision module is charged with building a plan for
current and future activities including starting materials
into factories, shipping materials between locations, and
moving materials into and out of warehouses based on a
profitability goal. The execution module is intended to
manage the (virtual) current and future application of the
plan to the factories, warehouses, and transportation links
of the real system including as much of the stochasticity of
the real world as is possible and appropriate. The decision
module needs as input the state of the world resulting from
the past application of the previous plan, and produces the
plan for the future. The execution module needs the plan of
action to project forward in time to produce the future
states of the world. Clearly these are quite different algorithms requiring different models.

2

2.1 Model Composability
With model composability, a large problem can be partitioned and appropriate modeling approaches used to model
the overall system. An approach to model composability,
therefore, must ensure semantic integrity of the parts and
their combinations. Composite models with appropriate
semantic underpinning can then be ensured to correctly
execute and interoperate. The lack of an approach and its
associated methodology to support simulation model composability is a well recognized and documented concern
(e.g. Davis and Anderson 2003; Fishwick 1995). For
SSDN, it is possible to integrate LP and DES models using
custom-built bridges through a combination of programming languages and communication protocols (e.g., Godding and Kempf 2001). Similarly, Agent and DES models
can be composed by ensuring correct ordering of events
and valid types of message exchanges (Sarjoughian and
Plummer 2002). These kinds of approaches result in
pseudo composability – achieving model integration
through interoperability between a simulation engine and
an optimization engine or an interpreter.

Assembly-1

Fabrication-n

Assembly-m

STATE

PLAN

how much to hold
how much to release
how much to ship and the destination
how much to hold
how much to release
Fabrication-1

stochastic duration
stochastic yield
stochastic duration
stochastic yield
stochastic duration
Factory

Warehouse

RELATED WORK

2.2 Multi-Formalism Modeling
Constructing models using two or more modeling approaches requires a multi-formalism approach where each
part of a composite model is described in a formalism that

Transportation

Figure 3: An Example from Manufacturing in a SSDN
233

Godding, Sarjoughian, and Kempf
is most suitable. Within a system theoretic framework,
much progress has been made on formal theories and
methodologies of how to construct discrete and continuous
simulation models that have well-defined properties such
as conversion of continuous data to discrete data and correctness of synchronization between continuous and discrete simulation protocols (Zeigler, Praehofer, and Kim
2000). Other approaches such as Ptolemy II (Plotomy
2004) offer capabilities to compose continuous models of
physical systems with discrete models of controllers that
are responsible for managing behavior of the former. Unfortunately, neither of these approaches or other similar
ones are suitable for the SSDN. For example, an agent
based approach such as (Swaminathan, Smith, and Sadeh
1998) does not address how to compose complex decision
models with large stochastic data flows. For these formalisms to support development of complex decision models
and thus SSDN, their underlying frameworks need to be
extended with new concepts and methods.
Due to the unavailability of a suitable framework for
modeling SSDN, decision and process modules are generally combined using ad-hoc low-level programming techniques. Optimization modules have been integrated with
simulation modules for specific cases (e.g. Hung and
Leachman 1996) resulting in ad-hoc composability. Customized combination of models, however, are not based on
sound principles that can support well-defined relationships among the components of a composed model.
3

models can be hierarchically constructed from atomic and
coupled models that communicate with other models.
An atomic model specifies input variables and ports,
output variables and ports, state variables, internal and external state transitions, confluent transition function, output
and time advance functions (Zeigler, Praehofer, and Kim,
2000). An atomic model is stand-alone component capable
of autonomous and reactive behavior with well-defined
concepts of causality, timing, handling multiple inputs and
generating multiple outputs.
A coupled model description specifies its constituents
(atomic and coupled models) and their interactions via ports
and couplings. A coupled model can be composed from a
finite number of atomic and other coupled models hierarchically. Due to its inherent component-based support for
model composition, this framework lends itself to simple,
efficient software environments such as DEVSJAVA
(ACIMS 2003). Coupled models, similar to atomic models,
have sound causality, concurrency, and timing properties
that are supported by various simulation protocols in either
distributed or stand-alone computational settings.
While this formalism may be used for optimization, its
conceptual framework is not well suited to support linear
programming or other optimization modeling approaches
where general mathematical equations specify constraints
among decision variables (Godding, Sarjoughian, and
Kempf, 2003). Instead, DEVS and more generally systems
theory is concerned with describing the structure and behavior of a system and simulating it for some period of time.

BACKGROUND

3.3 SSDN Modeling Using LP and DEVS

3.1 Linear Programming Formalism
The purpose of an LP is to find the best answer when many
exist. Linear programming can be classified as a selection algorithm where an answer is selected from a set of many different possibilities. Models described in linear programming
consist of an objective function, a set of constraints, a set of
cost variables, a set of decision variables, and a set of constants (Wu and Coppins 1981). The LP relationships including constraints and objective functions must all be linear.
LPs have been applied successfully to a wide range of
planning applications (Hopp and Spearman 1996, Chopra
and Meindl 2001) when each problem can be described as a
set of linear constraints and a cost function. Linear programming, however, is not suitable for describing dynamic
(time-driven) behavior of systems. Instead, constraints and
objective function are useful for formulating the inputs to an
optimization problem – i.e., a problem that searches for the
best assignment of values assigned to decision variables to
achieve the maximum (minimum) objective function value.

Composing multi-formalism models described in linear
programming and discrete-event formalisms requires a set
of well-defined concepts that account distinctly for both
composition and interoperability. Many benefits can be
foreseen with such an approach. For example, it can enable
development of a methodology for describing models
composed of decision models and process models without
requiring modelers to delve into the details of a given
simulation engine and a given optimization solver. Evaluation of decision policies against physical process flows, or
evaluation of the impact on decision policies when the
physical system is changed would be possible without requiring custom software development. In addition, if a
methodology is available, it would pave the way to enable
the development of tools that allow modelers of different
expertise (such as mathematicians and simulation modelers) to create different, yet semantically consistent model
components of the composite model that could then seamlessly execute and interoperate.

3.2 DEVS Formalism

4

The DEVS (Discrete Event System Specification) is a
modeling formalism for describing (discrete and continuous) dynamical systems as discrete-event models. Complex

Two significant issues must be addressed to support composition and execution of SSDN multi-formalism models.
One is how to support the complex data transformations
234

APPROACH

Godding, Sarjoughian, and Kempf
required between the models. The second is how to enforce
semantically correct composition of the different models.
Our approach to manage the data transformations is a
broker between the models. The broker is referred to as a
Knowledge Interchange Broker (KIB). This broker must
provide capabilities beyond those offered by the simulation/execution (e.g., ILOG CPLEX and DEVS simulation
protocol) and middleware (e.g., CORBA) layers.
Our approach to enable the enforcement of the composition of semantically correct models is to introduce the
concept of a common model. This model would provide a
common vocabulary to impose constraints on the decision
and physical models.

Second, given the decision and process behavior of the
SSDN, it is necessary to describe how the SSDN application domain affects or constrains the KIB. For example, the
decision and process models illustrated in Figure 4 need to
use the same names for the messages and have a common
model of what data is shared, exchanged, and the transformations needed for a composed model.
Third, the specification of the KIB must account for
timing between the LP and DEVS models. This includes
enforcement of (time-based) causal relationships supporting their concurrent execution, and their synchronized
communications.
Figure 5 illustrates some simple primitive capabilities
of the KIB. Part (a) is a scenario showing the generic aggregation and disaggregation of the port ID for messages
between LP and DEVS. LP models do not have ports while
DEVS models do. Part (b) is a domain specific scenario
exemplifying the passing of messages and data transformation between the SSDN decision policy and the associated
manufacturing process flow it is controlling.

The proposed multi-formalism composability approach for
SSDN focuses on combining two classes of models – optimization and simulation models expressed in LP and
DEVS formalisms, respectively. This approach is based on
a three-layer worldview where the modeling, simulation/execution, and middleware layers are separated from
one another (Sarjoughian and Cellier 2001). Within the
modeling layer, decision policies can be modeled as linear
programs and discrete event simulation can be used for
modeling process flows. A conceptual view of the modeling layer is depicted in Figure 4.

decision
policies

LP

LP to DEVS, port information added

…

goals

constraints

DEVS
atomic

DEVS to LP, port information discarded

coupled

(a) general KIB capability

decision module

external control

Release Schedules
product
release

external data

Knowledge
Interchange
Broker

semifinished
goods

Knowledge Interchange Broker (KIB)

…

4.1 KIB

release
quantity

inventory

manufacturing
processes

Netting and disaggregation

finish line

Process State

finished
goods
warehouse

Data Aggregation

(b) domain specific

Figure 5: Capabilities of KIB

WIP (work
in process)

finish
line

inventory

Netting is a special type of transformation needed to
correct discrepancies between the plan and what actually
happened. One simple strategy is to divide the plan into
smaller time periods and add or subtract the differences
from the current time to the next period. For example, the
netting algorithm could divide a weekly plan by seven and
at the end of each day, add or subtract the daily discrepancies to the plan for the next day.

finished
goods

local control

Figure 4: Three Layer World View with KIB

4.1.1 LP and DEVS I/O Composability via KIB

The KIB must provide its own modeling constructs
and execution capabilities to support composition of decision models and process models. The KIB can be viewed
from three complementary perspectives. First, it needs to
provide primitive capabilities for translating (or mapping)
of data from LP to DEVS and vice versa. This requires not
only syntactical (structural) translation between LP and
DEVS but also imposing semantically sound behavior of
the composed model and its constituents.

The mathematical representations for LP and DEVS modeling formalisms are shown in Table 1. Each formalism
provides a set of general modeling constructs for describing a class of behaviors – e.g., dynamical discrete process
models with DEVS. To form a composable modeling approach, it is necessary to formulate appropriate logic for
the KIB to support the transformation of data/control be235

Godding, Sarjoughian, and Kempf
tween LP and DEVS, the addition of missing data/control,
and the deletion of superfluous data/control that is irrelevant. Furthermore, it is central for the KIB to support causality, concurrency, and synchronization as the keys to correct behavior specification and combined execution of LP
and DEVS models.

4.2 Problem Domain Common Model
To formulate decision models and process models that are
semantically correct, they need to share some common
data of the problem they are representing. A methodology
is necessary to characterize ways in which various pieces
of the common model can be mapped onto the optimization and process models. Additionally, the common model
is important in the development of the KIB – defining
translations (interactions) between the optimization and
simulation models. For example, KIB needs to have a well
defined knowledge representation in order to convert a release command from the decision layer into an instruction
for the correct entity in the process model. As mentioned
earlier, these translations may involve complex calculations such as netting.
Figure 6 shows a conceptual representation of how the
common model relates to the other models. The common
model constrains the valid names, configurations, and
translations that can be modeled in the LP, KIB, and
DEVS. The common model needs to specify the structure
of the problem, such as the SSDN topology, and also the
valid relations between the models. For example, the
common model may specify that some specific data values
need to be aggregated from a daily resolution to a weekly
resolution. The KIB transformation model would use this
to specify the detailed aggregation algorithm to use on the
data variables given by the common model.

Table 1: Standard Forms for LP and DEVS
LP Formalism
DEVS Formalism
min{cx: Ax=b, x≥0)
where:
x ∈ ℜn
c ∈ ℜn
b ∈ ℜm
A ∈ ℜm×n
• c is a vector of cost
variables
• x is a vector of decision variables (unknowns)
• b is a vector of constants
• A is the constraint
matrix

<X,S,Y,δint,δext ,δconf, λ, ta>
where:
• X is the set of input values
• S is a set of states
• Y is the set of output
values
• δint is the internal state
function
• δext is the external state
function
• δconf is the confluent
function
• λ is the output function
• ta is the time advance
function

Since these two formalisms are distinct, the KIB needs
to translate the inputs and outputs from one formalism to
the outputs and inputs of the other, respectively. This is
appropriate since each formalism can be viewed as a system with well-defined input and output interface. The possible inputs to the LP formalism from DEVS would be the
cost vectors, the constraint matrix, and the constraint values. (i.e. c, A, and b shown in Table 1). Some or all of
these could be DEVS inputs depending on the problem being modeled. The outputs of the LP formalism to DEVS
would be x, the decision variables. For the DEVS formalism, the inputs from the LP would be a subset of X – i.e.,
the set of values available on the input ports of atomic and
coupled models. Similarly, the outputs of the DEVS to the
LP would be a subset of Y – i.e., the set of output values
from the atomic and coupled models.
LP and DEVS models may have inputs and outputs
that are independent of one another. For example, a process model can generate outputs that are of no interest to the
decision model, yet of importance to observe the process
flows. The process model may receive inputs that are not
sent by the decision model. These types of interactions between LP and DEVS models provide a basis for formulating an appropriate logic for the KIB such that it can support transforming data structures and ensuring causality,
concurrency, and synchronization between the models.

SSDN
Common Model
imposes constraints on ...

LP
Optimization
Model

KIB
Transformation
models

DEVS
Simulation
Model

Figure 6: Common Model Relationships
A small example of the type of common structural
data needed to represent the topology and product flow of a
SSDN is shown in Figure 7. This common model is a mapping of the BOM (Bill of Materials) to a process flow topology. It specifies facility names, product names, and the
relation of product routes to facility topologies. Material
can be shipped between facilities via transportation, can be
held within a manufacturing line and considered as work in
progress (WIP), or can be sitting in a warehouse.
This model corresponds to a segment of the topology
shown in Figure 3. The possible routes through the network have been modeled for products P1-P7. For example,
raw silicon (P1) can be sent into plant 1 or 2 and fabricated
into one of the P4-P7 products. If the material is made into
236

Godding, Sarjoughian, and Kempf
Aside from synchronization of processes, there is
also the potential of “logical concurrency”. That is both
linear and process models consume logical time. Since
both models can be expressed in terms of time, they can
be executing concurrently in logical or physical time. For
example, assuming it takes two days (logical time) to
generate a plan, the process model may execute concurrently with the decision model for two days (logical time)
before receiving a plan.

Raw Silicon P1
Fabrication1

Fabrication2
P3

P2

P4

P5

P7

P5
P6
P7
Warehouse2

P4
P5
Warehouse1
Factory

P6

Warehouse

WIP

Transportation

4.3.3 Synchronization

Figure 7: SSDN Common Model

In addition to causality and concurrency, the KIB also
needs to support synchronization. A basic form of synchronization is to ensure that both input/output exchanges
between the process and decision models are logically correct – i.e., the order of events produced and consumed by
process and decision models are maintained. More advanced forms of synchronization can account for timing
and therefore support causality using both ordering and
timing of events given concurrent processes.

P5, it can be shipped to either warehouse 1 or 2. The knowledge of which products can be made in what facilities and
where it can be routed must be the same in both models.
4.3 Behavioral Composability between
LP and DEVS Models
The approach described in Sections 4.1 and 4.2 must support the passage of time since simulation models are time
driven. The necessity of modeling time has important consequences as alluded to earlier. In particular, causality,
concurrency and synchronization need to be accounted for
with respect to time. For example, causality not only needs
to be consistent in the decision and process model specifications, but also the concurrent execution of the models
and synchronized data exchange between them need to be
consistent with the causality specification of the KIB.
Some of the details of the causality, concurrency, and synchronization are presented next.

5

PROTOTYPE KIB DESIGN

The design and implementation of the KIB was based on
OPL Studio (ILOG 2004) and DEVSJAVA (ACIMS
2003). The KIB was developed using a combination of
software objects specified in DEVSJAVA and JAVATM.
The prototype implementation has been created and tested
with the simple models shown in Figure 8. The DEVS
model consists of an inventory, a finish line, and a finished
goods inventory. The LP calculates release schedules using
Inventory, WIP, and Demand models. The inventory and
WIP data is supplied from the DEVS model. The demand
model is used only by the LP.

4.3.1 Causality
The KIB will need to enforce the correct causality for message passing between the decision and process models. For
example, in the scenarios we are modeling, it is not possible
in the real world for the planning organization to have
knowledge of the future state of the world, so it would not be
correct for the decision algorithm to have any future projected state from the process models. Causality is already
accounted for within the DEVS framework. However, LP
models are not time dependent and their behaviors do not
evolve over time. Therefore, support for causality of the
composed models may be placed in the KIB. That is, the
KIB will support causality between LP and DEVS models
while preserving the causality of the DEVS models.

LP
Inventory and
WIP model

Decision
Variables
Release
Schedule

Inventory
Values

Demand
Model

WIP
Values

KIB
Release Inventory
Command Message

WIP
Message

4.3.2 Concurrency
Inventory

The KIB will need to manage concurrency since the execution and simulation modules must be under the control of
two different processes. One of the most straightforward
methods to support this type of concurrency is via the use
of synchronization.

Finish Line

Inventory
Message

Finished
Goods
Inventory

DEVS

Figure 8: SSDN First Prototype
237

Godding, Sarjoughian, and Kempf
The SSDN model execution sequence starts with the
DEVS model initializing the KIB. When the KIB receives
the initialization call, it initiates the simulation and sends
the appropriate data to the LP followed by an LP solve request. The results of the initial solve are sent to the simulation prior to the current time period being completed. This
sequence continues until a desired number of iterations
have been executed.
The implementation of the KIB requires components
for a data model, synchronization, and mappings of outputs
to inputs between LP and DEVS and vice versa. The data
model is necessary for maintaining a representation of the
SSDN common model and the relationships of the LP and
DEVSJAVA models to the SSDN. The synchronization
logic ensures correct ordering of the DEVS messages sent
and received to KIB and also used to request an LP solve at
the proper time instances during the SSDN execution. The
mapping logic performs the necessary data transformations
between the LP and DEVS models.
The data model consists of three different sets of data.
The first set of data is the SSDN common model representation. The second set of data is the DEVSJAVA specific
data such as port names to use for each of the atomic models. The third set of data is the OPL specific interface data.
The prototype implementation used a Hashtable to hold
this data. Each entry of the Hashtable represents a part of
the SSDN model such as Finish Line. For each of the
SSDN entries, additional entries were linked for DEVS and
LP specific data. The input and output specifications for
DEVS and LP were included so the KIB logic could do the
required data translation. No complex translations of the
data such as disaggregation were required for this implementation. In the future, when these types transformations
are added, a fourth set of data tied to the KIB will need to
be added. The DEVS portion of the Hashtable is populated
automatically. The DEVS model reports its structure in a
set of initialization messages. The LP portion is populated
manually, although it is important to support automatic
configuration of the data model.
The synchronization between the models is accomplished using a clock in the KIB. A clock event occurs at
the end of each time period and specifies what the value of
the next time is. The clock is implemented as an atomic
DEVSJAVA model and is synchronized with the SSDN
process models. The KIB uses the clock to timestamp messages to and from the LP and to synchronize the execution
of the LP and DEVS models.
The mapping logic translates data representations from
DEVSJAVA event messages to OPL data structures. This
includes the addition and deletion of port information for
DEVS messages, and the translation of DEVS messages
sets into OPL arrays and variables.
6

four categories – develop a specification for the SSDN
Common Model, support additional capabilities for the
Knowledge Interchange Broker, apply the composability approach in real-world settings, and devise a methodology to
assist modelers to develop composable models. Research in
each of these categories is expected to extend the basic capabilities of the proposed model composability approach.
For example, since the common model is a meta-model imposing constraints on the constituent models, its specification needs to support representation of the product routing
relations and data transformations between the models.
Similarly, the kinds of transformations within the KIB need
to be extended. In particular, it is important for the KIB to
support a set of generic as well as domain-specific modeling
constructs for input and output transformations – e.g., supporting netting. Furthermore, it is important to show how the
Common Model and the KIB can be used systematically for
developing decision and process models. It is also useful to
provide a roadmap and guidelines to help modelers develop
composable SSDN models.
7

CONCLUSIONS

A multi-formalism modeling approach enabling composition of models described as decision and projection algorithms is presented. The proposed approach is based on the
separation of decision and physical process models with
the aid of a Common Model and Knowledge Interchange
Broker. The roles of the KIB for LP and DEVS applied to
a SSDN problem were described and demonstrated using a
prototype to offer a sound basis for model composability in
conjunction with simulation/execution interoperability.
One of the key offerings of the multi-formalism modeling approach is its support for describing models in
widely used LP and DES modeling paradigms. A complementary advantage is the separation between model composability and module interoperability. The ability to use
multiple modeling formalisms and execute each separately
can afford important benefits such as relying on soundness
of the models and the correctness of their distributed execution. Moreover, modelers can focus their attention on
model specifications since execution (i.e., simulation and
solver) interoperability would already be supported as part
of the multi-formalism approach.
ACKNOWLEDGMENTS
This research is supported by the Intel Research Council,
Chandler, Arizona. The authors would like to express their
appreciation to Bin Xie of the Computer Science & Engineering Department at Arizona State University for assistance in the development of the SSDN model.

FUTURE WORK

The research described in this paper provides a basis for
composing LP and DES models. Follow up work falls into
238

Godding, Sarjoughian, and Kempf
8

Department, Arizona State University, Tempe, AZ,
pp. 1-38.
Swaminathan, J. M., S. F. Smith, and N. M. Sadeh. 1998.
Modeling Supply Chain Dynamics: A Multiagent Approach, Decision Sciences, 29(3) : 607-632.
Wu, N. and R. Coppins. 1981. Linear Programming and
Extensions, McGraw-Hill.
Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory
of Modeling and Simulation, Second Edition, Academic Press.

REFERENCES

ACIMS. 2003. DEVSJAVA Software. Available online via
<http://www.acims.arizona.edu/SOFTWA
RE> [accessed August 19, 2004].
Chopra, S. and P. Meindl. 2001. Supply Chain Management: Strategy, Planning, and Operation, Upper
Saddle River, NJ: Prentice-Hall, part 2 (Planning
Demand and Supply in a Supply Chain).
Davis, P. K. and R. H. Anderson. 2003. Improving the
Composability of Department of Defense Models and
Simulations, RAND.
Fishwick, P. A. 1995. Simulation Model Design and Execution: Building Digital Worlds, Prentice Hall.
Godding, G. W. and K. G. Kempf, 2001. A modular, scalable approach to modeling and analysis of semiconductor manufacturing supply chains, In Proceedings
IV SIMPOI/POMS, pp. 1000-1007, Sao Paulo.
Godding, G. W., H. S. Sarjoughian, and K.G. Kempf.
2003. Semiconductor Supply Network Simulation, In
Proceedings of 2003 the Winter Simulation Conference, New Orleans, LA, pp. 1593-1601.
Hopp, W. J. and M. L. Spearman. 1996. Aggregate and
Workforce Planning. In Factory Physics: Foundations of Manufacturing Management, 535-581. New
York: McGraw Hill.
Hung, Y.F. and R. C. Leachman. 1996. A Production
Planning Methodology for Semiconductor Manufacturing Based on Iterative Simulation and Linear Programming Calculations, IEEE Transactions on Semiconductor Manufacturing, 9(2) : 257-269.
Kempf, K. G., K. Knutson, J. Fowler, D. Armbruster, P.
Babu, and B. Duarte. 2001. Fast Accurate Simulation of
Physical Flows in Demand Networks, In Proceedings of
Semiconductor Manufacturing Operational Modeling
and Simulation Symposium, Tempe, AZ, pp. 111-116.
Kempf, K. G. 2004. Control-Oriented Approaches to Supply Chain Management in Semiconductor Manufacturing, In Proceedings IEEE American Control Conference, Boston, MA, to appear.
ILOG. 2004. ILOG OPL Studio. Available online via
http://www.ilog.com/products/oplstud
io/ [accessed July 14, 2004].
Ptolemy. 2004. Ptolemy project; Heterogeneous Modeling
and Design. Available online via <http://ptolemy.
eecs.berkeley.edu> [accessed July 14, 2004].
Sarjoughian, H. S. and F. E. Cellier. 2001. Toward a Unified Framework for Simulation-Based Acquisition, in
Discrete Event Modeling and Simulation Technologies: A Tapestry of Systems and AI-Based Theories
and Methodologies, ed. H.S. Sarjoughian and F.E.
Cellier, pp. 1-14, Springer-Verlag.
Sarjoughian, H. S. and J. Plummer. 2002. Design and Implementation of a Bridge between RAP and DEVS,
Internal Report, Computer Science and Engineering

AUTHOR BIOGRAPHIES
GARY W. GODDING is a Software Engineer at Intel
Corporation and a PhD candidate in the Computer Science
and Engineering department at Arizona State University.
His research includes modeling and simulation of supply
networks, software architecture, and artificial intelligence.
He can be contacted by e-mail at <gary.godding@
intel.com>.
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science and Engineering at Arizona State University, Tempe. His research includes hybrid simulation
modeling and intelligent agents, collaborative modeling,
distributed co-design, and software architecture. His
industrial experience has been with Honeywell and IBM.
For more information visit <http://www.eas.asu.
edu/~hsarjou/index.htm >.
KARL G. KEMPF is Director of Decision Technologies
at Intel Corporation and an Adjunct Professor at Arizona
State University. His research interests span the optimization of manufacturing and logistics planning and execution
in semiconductor supply chains including various forms of
supply chain simulation. He can be contacted by e-mail at
<karl.g.kempf@intel.com>.

239

Composable Cellular Automata
Gary R. Mayer
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling and Simulation,
Department of Computer Science and Engineering,
School of Computing and Informatics,
Arizona State University,
Tempe, AZ 85281, USA
gary.mayer@asu.edu, sarjoughian@asu.edu

Cellular automata (CA) provide a convenient approach to modeling a system comprised of homogeneous entities that, generally, have a spatial relationship with one another. CA are used to model
systems that can be appropriately represented as a collection of interconnected automata. These
networked automata may act as either a model representation of the entire system, or used to model
a sub-system within a hybrid system. As the sub-systems within a hybrid system are disparate, so
too can the models representing them be disparate using a multi-model approach. However, to take
advantage of multi-modeling, CA and other models used to represent the sub-systems must be
founded on system-theoretical principles. Furthermore, each model’s formalism must account for
input and output data exchange with other modeling formalisms. Therefore, to support modular synthesis of distinct CA models with non-CA models, a composable cellular automata (CCA) formalism
is proposed. This formalism is provided as a domain-neutral, mathematical specification. The CCA
is then exemplified as part of a multi-model, and the GRASS development environment is used to
describe one possible implementation approach.
Keywords: cellular automata, geographic information system, model composability, multi-modeling

1. Introduction
Cellular automata (CA) have been applied across many interesting problem domains and, within a domain, for many
different purposes. A majority of these model the entire
system [1–3] based on the CA concept. However, some
system models are better represented using a multi-model
approach. An example model is one representing the activities and processes of disaster relief efforts. A simulation for such a system may consist of models for people,
resource availability and land. These entities can be described using different modeling approaches. For example, a rules-based discrete-event model can describe people’s actions, a CA model can describe the disaster effects across an environment, and a continuous or discretetime model can describe the dynamics of the resources

SIMULATION, Vol. 85, Issue 11/12, Nov./Dec. 2009 735–749
c 2009 The Society for Modeling and Simulation International
1

DOI: 10.1177/0037549709106341
Figures 1–4 appear in color online: http://sim.sagepub.com

available to personnel. All three of these heterogeneous
sub-system models must be composable in order to use a
multi-model approach.
Synthesis of different kinds of sub-systems results in a
hybrid system. The term hybrid can be applied to both
an original system and to a model of a system. A hybrid system model is a model of a system, comprising
two or more distinct sub-system models. The term ‘hybrid’ is used since each of the sub-system models can be
best developed with its own appropriate structural and behavioral details. The role of hybrid modeling in modeling complex systems is well recognized (e.g. [4–6]) with
concrete implementations in engineering domains such as
automotive and manufacturing (e.g. [7, 8]). A key advantage of hybrid modeling is overcoming the unsuitability
or impracticality of a monolithic modeling approach for
describing disparate model dynamics (e.g. human decision making and land erosion) and their interactions. In
this regard, CA are useful for their ability to implicitly
define spatial relationships and demonstrate complex behaviors using simple rules. Some approaches and tools for
simulating cellular-based models have already been de-

Volume 85, Number 11/12 SIMULATION

735

Mayer and Sarjoughian

veloped. Computational frameworks such as Tarsier [9],
ODD [10] and Repast [11], as well as the cellular-discrete
event system specification (DEVS) modeling framework
(e.g. [12, 13]), have been used for social and ecological
simulations. These approaches differ in whether the models are formulated primarily in terms of formal modeling
languages, simulation interoperability concepts, or software concepts.
Socio-ecological modeling is one problem domain that
strongly lends itself to the use of combined CA and decision models. The Mediterranean Landscape Dynamics
(MEDLAND) project [14] is an example of such human–
landscape research. One of the project’s goals is to develop a simulation tool in which to study humans, the environment and their relationship. In the simulation tool,
humans are represented by agents and the environment is
represented by a landscape model. The landscape model
is used as the candidate sub-system to demonstrate a realization of the composable cellular automata (CCA) formalism. Within the context of this paper, the term realization refers to the software architecture, design and implementation of a modeling formalism which provides a
domain-neutral model specification language with an abstract execution algorithm for characterizing and executing a general class of systems (e.g. ecological processes).
The rest of the paper includes our motivation for developing the CCA, a brief overview of CA and the Geographic Resources Analysis Support System (GRASS) development environment [15, 16], and our approach to developing the CCA itself. The paper uses the author’s polyformalism modeling approach [17] in an ongoing project
[18] to show details of the CCA.
1.1 Motivation
Multi-modeling approaches are useful for studying systems that implement cellular and non-cellular model
types. For example, consider a hybrid system model that
is capable of representing the details of human farmer activities and the landscape process dynamics. Simulating
complex interaction between these disparate sub-system
model types depends as much on the behavior of the landscape as the effects that the human actions have on the
landscape. Instances where the two sub-system models are
very distinct in structure and behavior may lead the modeler to choose disparate specifications to define each subsystem model. A human farmer–landscape environment
[14] is one such exemplar system. To model and simulate such a system, a discrete-event specification may be
the best choice for an agent-based model that defines the
human farmers. In addition, the landscape model may be
best represented using a CA in order to tessellate the landscape into discrete, uniform areas, and also to capture the
spatial relationship between those areas. These two disparate model types must then be composed to provide
a complete representation of the entire modeled system
736 SIMULATION

Volume 85, Number 11/12

[19]. However, for the model composition to be theoretically grounded, both of the sub-system formalisms must
possess formal descriptions of the input and output from
their respective systems. This capability is lacking from
existing CA specifications, which either do not formally
treat external input and output (I/O) or specify the CA and
other models within the same class of models [20, 12, 21].
Therefore, it was necessary to devise a CA specification
that did explicitly describe these I/O formally, and in a
manner that allows composition with classes of models
outside of its own.
In our research, the landscape model is a stand-alone
entity whose existence has meaning beyond acting just as
a support structure and environment for the agents. As
such, the landscape is developed to be much larger in
size and with much greater detail than what is required
for the human agents’ immediate needs. Furthermore, it
is beneficial to build the landscape in a development environment that can support multi-faceted attributes over millions of cells and perform operations on these cells very
efficiently. This is in comparison to the approximately 200
agents that may only look at a small percentage of the total
landscape at any given time instance. Therefore, the exemplar landscape model demonstrates both the functionality
of the CCA and its ability to be implemented using a geographic information system (GIS) software environment.
Specifically, the GRASS [15] development environment
is utilized to build a spatially based landscape model.
A benefit of the GRASS environment is the efficiency
of its operations for handling large cellular spaces. Using the GRASS environment to realize a CCA requires a
mapping from the composable CA modeling formalism
to the GRASS implementation constructs. Conversely, a
mapping from the GRASS implementation constructs to
the CCA models can enable formalizing the implementation of the CCA. Mappings are well-formulated constructs
derived from domain knowledge that relate data or a construct in one model, to the same within another model (e.g.
[22, 18]). The benefit of mapping CCA and GRASS to one
another is that the CCA formalism more closely resembles
the CA concepts and provides less ambiguity of model behavior in its descriptions than the constructs that are provided for GRASS. At the same time, the benefits of the
GRASS development environment, such as the capability
to manipulate large data sets efficiently, the use of a wellrecognized interface, and the availability of various data
visualization tools, is retained. Therefore, the broad range
of scientists who are familiar with GRASS and want to
devise a formal model using its environment, should find
this approach beneficial and more accessible.
While this CCA approach is demonstrated for a human
farmer–landscape environment research domain, the CCA
formalism is domain-neutral. Therefore, other modelers
should be able to apply the same principles in other kinds
of modeling efforts that benefit from CA modeling. Furthermore, the domain neutrality and general mathematical
specification allow the realization of the model to be de-

COMPOSABLE CELLULAR AUTOMATA

veloped using an environment other than GRASS or, more
broadly, other than tools that are based on the GIS concept.
2. Background
In [23], CA are described as a ‘collection of . . . cells on
a grid of specified shape that evolves through a number
of discrete time steps according to a set of rules based
on the states of neighboring cells. The rules are then applied iteratively for as many time steps as desired’. CA
are described from a slightly different viewpoint in [2] as
‘a class of spatially and temporally discrete, deterministic
mathematical systems characterized by local interaction
and an inherently parallel form of evolution’. What both
are saying is that CA are a networked system of individual automaton (most often referred to as a ‘cell’). A cell
interacts with other nearby cells and all cells in the network undergo state change at the same (discrete) time and
in accordance with the same functions (or rules). Note that
not all of the ways to simulate CA employ discrete-time
dynamics [21].
The set of cells that affect the state of a particular cell
is referred to as that cell’s neighborhood. Borrowing notation from graph theory, the ith neighborhood of a cell,
c, is the set of all cells that lie at a radius i from c [24,
2]. The neighborhood is often considered the cell itself
and all immediately adjacent cells [23, 25], implying a radius of one. This is also called ‘nearest neighbor’ [23].
Considering the system structure from the perspective of
a network, it is possible to conceive many different neighborhood variations: hexagonal, triangular and octahedron
networks, for example. A tetragon is the most common
configuration [2].
CA offer a few distinct benefits to modelers modeling natural system phenomena in which the system is
comprised of many homogeneous sub-systems. First, the
modeler is able to abstract the behavior of a single subsystem into a single cell. Second, the interaction between
the sub-systems can be distinctly modeled through choice
of network topology and specification of how neighboring sub-systems impact one another. From relatively simple behavior specifications and network topologies, complex system behaviors can be achieved. Finally, the way
in which CA are tesselated lend themselves well to visualization [3].
Most current CA approaches handle I/O from the cellular network in a manner that is not conducive to formal composition. Many of the publications for these approaches discuss neither the specific CA specification
used, nor do they implement I/O from the cellular network. The CA represents the entire system. The interest
generated by the work is in the rule or rules applied and
the resultant patterns. The reader is encouraged to examine any number of the publications found in [26, 27] as examples. In Cell-DEVS, the specific modeling formalism
is specified. However, the cellular network is abstracted

as a DEVS coupled model component, and all input to the
coupled model are sent simultaneously to all of the cells
within the network [13]. A tool called JCASim uses a Java
implementation of CA that proposes to provide ‘coupling
of different CA’ that ‘can have different state sets and different spatial or temporal resolutions’. However, it provides no theoretical grounds and all discussions regarding I/O occur at the application level, using Java remote
method invocation, and without any proof of correctness
of the individual models which are made to interact [28].
A formal approach called complex automata (CxA) is
proposed for coupling CA that have different scales [29,
30]. The aim of CxA is on composition of two or more
CA with potentially differing spatial and temporal resolutions. There is no explicit input and output as the external
input to one CA is the state of the other being coupled to it.
The input effects the current cell through boundary conditions and collision operators. In addition, time is treated
implicitly in the update function. The intent of CxA is
to create a system of CA that mimic the original, more
course-grained CA1 not to provide the capability to allow
an external system (both CA and non-CA) to provide input to a CA [29]. CxA appears to be the closest approach
to a desired composable CA, but it still does not provide
the needed capabilities as considered in this paper.
There appear to be no clearly defined specifications for
input and output from a CA that allow the CA to interact with non-CA using system-theoretical principles. Nor
does there appear to be a specification that provides the
flexibility to clearly represent the domain within the interaction, whether that be input to a subset of the network
cells, temporal disparities between the two composed systems, or differences in spatial resolution and representation. For example, if an agent model is composed with
a CA, the agent should be capable of effecting a subset of the cells in the CA directly, without applying input to a boundary and having that propagate to the desired cell(s). From a domain perspective in which the CA
represents a physical landscape, this propagation would
make little sense. Furthermore, if a climatology model
is composed with a CA, the climatology model should
not have detailed knowledge of the cellular network. Using a domain-specific mapping protocol such as {“North”,
“Northeast”, “East”, “Southeast”,. . . } provides flexibility
in design and reconfiguration. The CA resolution could be
changed or the network configuration could even change
from a tetragon to a hexagon with no change to the climatology model itself. The mapping, obviously, would have
to be modified but we submit this is often a much easier and less error-prone endeavor than revising the code
within another system model.
A CA, as a general, implementation-independent
specification, can be realized in a number of different
software development environments. The proposed CCA
specification is no different. This paper describes a CCA
implementation within the GRASS software development
environment due to its many benefits to the authors’ reVolume 85, Number 11/12 SIMULATION

737

Mayer and Sarjoughian

search. The GRASS environment, as a GIS, is a georeferenced software development environment that uses map
algebra operations to modify its data. The data is stored
in a file called a map in either a raster or vector format.
GRASS is comprised of independent modules that can act
upon the maps. The modeler is able to define complex dynamics by executing one or more modules against one or
more maps either manually or via the use of scripts which
contain a specified execution sequence [16].
Map algebra is a way of relating two or more map attributes that represent the same geographic space1 a concept that was first defined in [31]. The maps are tessellated
into equivalent numbers of rows and columns and operators are used to generate new maps based upon the values
in the originals. Included in the class of basic operators
are arithmetic, relational, Boolean, combinatorial, logical,
accumulative, and assignment operators. Some GIS implementations may also have high-order operators such
as functions, verbal statements, programs, and iterations.
Operators such as addition are similar to matrix algebra:
the same 1r o23 column4 of each matrix is added to produce the result. However, map algebra maintains this oneto-one locational correspondence even for operators such
as multiplication, which matrix algebra does not [32]. As
an example, consider a map that contains slope values for
terrain. A second map contains values for the amount of
loose soil in the same geographical area. Map algebra can
create a revised soil map dependent upon the relationship
of the existing maps under a heavy rainfall by using the
slope and soil amount for each region in the tessellation.
From a formalism perspective, map algebra constructs
by themselves are inadequate to enforce a strict formal
representation of the landscape dynamics to be modeled.
However, the composable CA specification can be rigorously implemented within the GRASS environment by being particular about which GRASS modules are employed
and how they are applied against the maps. This implementation (or realization) may then be used in formal hybrid model compositions, such as poly-formalism.
The term poly-formalism is drawn from a previously proposed taxonomy of modeling approaches which
may be used to model and simulate complex application domains [7, 18, 17]. Poly-formalism uses a separate model to describe the interactions between the composed sub-system models (see Figure 1). Unlike superformalism [20, 21] and meta-modeling [33] composability
approaches, poly-formalism requires a formulation of the
models’ interactions. This approach specifies the composition of two disparate modeling formalisms via a third
modeling formalism. A concise delineation between the
sub-system models and their interaction via an interaction model is gained through the use of the formalisms
and poly-formalism composition. The poly-formalism approach is grounded in the knowledge interchange broker
theory [34] which defines interactions between disparate
models in terms of data transformation, synchronization,
concurrency and timing. With this multi-modeling ap738 SIMULATION

Volume 85, Number 11/12

Figure 1. Poly-formalism modeling concept (using CA).

proach, the formulation of the interaction model (IM)
structure and behavior is consistent with the formalisms
that are composed and also accounts for the formalisms’
disparities.
Using a poly-formalism modeling approach, the agent
and landscape models discussed above do not interact directly. They will be composed using an IM that explicitly models their interactions. It is within the realization of
the IM that the interaction between the sub-system models occur. The IM is, in essence, a model of the interaction
between the two sub-systems models. It has its own formalism and realization separate from the two composed
sub-system models. Interaction visibility and, therefore,
management of interaction complexity, is gained through
this explicit modeling of the sub-system models’ interaction [18].
3. CCA Formalism
A formalism defines its system using a domain-neutral
specification and execution. The specification is a mathematical description of the system defining structure and
behavior. The execution portion defines machinery which
can execute model descriptions (i.e. an abstract simulator)
[21].
3.1 Approach
A CA formalism that supports the specification of its external I/O with another modeling formalism was initially
introduced in [35]. This proposed composable CA formalism is capable of interacting with model formalisms which
are distinct from its own class of models. The CCA is
a network representation built upon the two-dimensional
multi-component discrete-time system specification provided in [21]. The specification described in [35] has
been revised to include internal mapping functions within
the network specification. The additional details better
demonstrate the formulation of the CCA concept.
The term ‘influencer’ will be used instead of ‘neighbor’ in keeping with the language derived from the CCA
specification’s system-theoretic foundation [21]. A clear
distinction can be drawn between the terms if the context
in which they are applied is considered. ‘Neighbor’ is often used to define the cells which have an impact on a

COMPOSABLE CELLULAR AUTOMATA

cell in a general sense, and without specific indication of
how the CA might be developed [23, 25]. Using the term
‘influencer’ allows the CCA specification to specify the
cells that explicitly provide input into the cell. In light of
the fact that cells are components for the purposes of the
specification provided in Section 3.2, this has direct implications in the software design and implementation of the
specification. If a cell is declared to be its own influencer,
then a direct feedback loop of output to input must be provided. Alternatively, since the cell’s state is used in specifying the transition function, 5 (see (2)), and is available to
the component, then the state variable data could also be
used within the transition function to determine the next
state of the cell itself.
The CCA formalism in Section 3.2 is presented with
specific attributes for clarity and application to the authors’ ongoing research. However, many aspects are
adaptable. The CCA is described as a three-dimensional
network, and may easily be reduced to represent either a
one-dimensional or two-dimensional network system. The
formalism could be extended to another regular tessellation of cells [23], regardless of the ‘shape’ that defines
the tessellation. However, for simplicity in describing the
formalism, only a square grid or cube structure is explicitly considered. Furthermore, the connectivity of the network (i.e. set of influencers of each cell) is restricted to
immediate neighbors. This is important for physical continuous processes that are mapped to CA as the dynamics
of cells are dependent on their immediate neighbors (i.e. a
cell’s dynamics cannot be influenced from inputs received
from cells not immediately adjacent to the cell). Finally,
the specification only concerns itself with discrete-time
automata. It may be possible to extend the neighborhood
to a radius greater than one, and to use timing other than
discrete time. However, the authors caution against using
the current CCA specification in this way until these two
concepts are examined fully to ensure correctness of the
models and their executions. In addition, disparities which
arise during composition of a CCA model with a heterogeneous model type, such as timing and synchronization,
can be managed through the use of poly-formalism composition [18, 17].

T 2 4h m 80  m  n5 is the time-ordered, finite set of
time intervals, h m , (i.e. 4h 0 3 h 1 3 h 2 3 6 6 6 3 h n 5) such
that m, n 9 10 , 10 
 11  4054  45, h m 9 1 3
1 
 1  403 5 and h m , h m occurs before h m1
(i.e. h 0 occurs before h 1 , h 1 occurs before h 2 , etc.),
F 2 4 f out 3 f in 5 is the set of I/O mapping functions
1 be2
tween the network and its cell components, Mi jk ,
3
Y7i jk  Y N and f in : X N 
where f out :
1i3 j3k49D
3
X7 i jk ,
1i3 j3k49D

D 2 41i3 j3 k48a  i  b3 c  j  d3 e  k  f 5 is
the index set where a, b, c, 4d,
f 9 21 the
1 e, 24
total number of components, 4 Mi jk 4, assuming
a regular, contiguous network, is 11b  a4  14 
11d  c4  14  11 f  e4  141 and 1i3 j3 k4, the
component Mi jk is specified as
5
6
Mi jk 2 X i jk 3 Yi jk 3 Q i jk 3 Ii jk 3 5i jk 3 7i jk 3 T 3 (2)
where
X i jk 2 X7i jk8 X7 i jk is the set of input to Mi jk , X i jk 2
3
Y8 is the input into Mi jk originating from the
89Ii j k
1
2
set of output of its influencers, 4M8 5 88 9 Ii jk ,
and X7 i jk  X N is the input originating from outside the network, N , which is mapped to Mi jk . Note
that X7 i jk may be ,
Yi jk 2 Yi jk  Y7i jk is the set of output from Mi jk , where
Yi jk is the output from Mi jk that acts as input to the
cells that Mi jk influences and Y7i jk  Y N is the output from Mi jk that contributes to the network output, Y N . Note that Y7i jk may be ,
Q i jk is the set of states of Mi jk , Ii jk  D is the set of
indices of the influencers of Mi jk where, given a cell
M with index 1i3 j3 k4,
Ii jk 2

i1
3

j1
3

k1
3

1a3 b3 c4 for a three-dimensional

a2i1 b2 j1 c2k1

3.2 Specification
A three-dimensional, composable CA, represented by a
network, N , can be specified by a sextuple:
N 2 3X N 3 Y N 3 D3 4Mi jk 53 T3 F63

(1)

where
1
2
X N 2 X7 i jk 8 1i3 j3 k4 9 D 
  is the set of external
input mapped to the network, N ,
1
2
Y N 2 Y7i jk 811i3 j3 2k4 9 D 
  is the set of external output for Mi jk from the network, N ,

network. Note that this is a general case that includes Mi jk as its own influencer and that 1i3 j3 k4 9
9
Ii jk , implying Mi jk is not its own influencer, is permissible. This general case is also applicable to onedimensional and two-dimensional networks.
5 i jk : Q i jk  X i jk  Q i jk is the state transition function
of Mi jk that is dependent upon the current state, q 9
Q i jk at time tr , to map the set of component input,
X i jk at time tr , to the new cell state, q  9 Q i jk at
time tr1 ,
7i jk : Q i jk  Yi jk is the output function of Mi jk that
maps the cell state, q 9 Q i jk at time tr , to the component output, Yi jk at time tr , and
Volume 85, Number 11/12 SIMULATION

739

Mayer and Sarjoughian

T is the network time-ordered set of finite time intervals
1 (defined in (1)) such that r, 0 2r  8T 8  1,
5 i jk 2 
t 
 1tr1  tr 4 2 h r 9 T 1 r , t 9 10 1
h r 9 1 1 tr is the time at the start of the current
discrete-time segment1 tr1 is the time at the start
of the next discrete-time segment1 h r is the time interval between tr and tr1 1 and the time when the
network, N , is in its initial state, q  9 Q i jk , is represented by t0 .
There are two levels of abstraction defined within the
specification above. The network level (1) defines the
cellular network structure, or how the automata interact.
Also, the network-level abstraction is what is exposed to
the external systems that are composed with the CCA.
The automaton-level abstraction (2) defines each individual automaton. It is at this level where dynamics occur due
to input and from which output is generated. Ultimately,
the automata just process input and generate output without regards for what other systems surround them. A key
facet of this approach is that the cells are treated as components and do not openly expose their state. The effect
of this approach is that the resultant dynamics due to input, whether it is generated internally to the network (i.e.
from influencers) or input from an external system, is left
up to the automaton. The role which the network plays in
behavior is restricted to the definition of automata interaction, both via structure and external mapping.
The component Mi jk defined in (2), which represents
a cell within the CCA, requires that the output function, 7i jk , occur immediately before the transition function, 5 i jk , takes place. Also, while the time interval between discrete-time segments is not necessarily constant,
the definition of T in (2) implies that the difference between any two specific segments, 
t, must be the same for
all components, 4Mi jk 5, within the network at any given
time, tr . In other words, all components within the network execute their output function and undergo state transition at the same scheduled time. This should be apparent from the fact that the network time-ordered, finite set
of time intervals, T , is a part of the septuple that defines
every component cell within the network.
Boundary conditions are one of the open topics of CA
when they are simulated. A description of several options is provided from the perspective of the CCA. In
the most simple of cases, the CCA may be treated as a
torus. For this case, there are no edges. However, while
this eases the need to specifically address edges, it does
not allow the CCA to faithfully represent a realistic physical system such as a landscape. Another approach is to
use the cells’ own state as a substitute for the ‘missing’
influencer cells. This approach does not work well with
the specification above. First, it requires that the edge cells
have state transition functions that differ from the other
cells. Second, it means that cells must have knowledge of
the network structure. This represents knowledge outside
of that which is provided by the tuple elements and, con740 SIMULATION

Volume 85, Number 11/12

sequently, ignores the specification. A third approach uses
a fixed value in place of the missing influencers. However, this approach suffers the same problems as those of
the second insofar as the need for a different transition
function and knowledge of the network structure [36]. It
is best that no function explicitly anticipates the number
of influencer input values. Instead, it should use the cardinality of the cell’s influencer set, 8Ii jk 8, which per the
definition of Ii jk in (2), for all 8, 8 9 Ii jk  8 9 D.
Here D is defined as an element of the network in (1). The
specification given in (1) and (2) is also valid for Ii jk that is
a subset of all of its immediate neighbors (e.g. a von Neumann neighborhood [2]). Furthermore, the above formalism provides the capability to explicitly manage values
going into and being output from edge cells. For example,
if the CCA is used to model watershed across a landscape,
the external input, X N , could map the specific values entering the edge cells while the external output from each
cell, Y N , could provide the data for material departing the
cell (i.e. off of the edge).
As an example, assume that there is a two-dimensional
CCA of dimension 3  3. Using (1), this model’s network may be specified by 1 for X N , 403 15 for Y N , D as
4103 043 103 143 103 243 113 043 113 143 113 243 123 043 123 143
123 245, T as 413 33 13 13 23 13 25, and the f out and f in mapping function set as


9
3
Y i j 4  Y N 3 1X N 4modulus 2  X 12324 3
median 1
1i3 j49D

respectively. Each example cell, Mi3 j , specified in accordance with (2), has 403 15 for X i j , 403 15 for Yi j , 403 15 for
Q i j , 41i  13 j43 1i  13 j43 1i3 j  143 1i3 j  145 for Ii j ,
and T as 413 33 13 13 23 13 25. Here 5 i j of Mi j is given as
follows: if q 2 0 and median1 x3
 x4 2 1, then q 2 11
otherwise q 2 0. Finally, 7i j is y 2 y 2 q. Here, and in
the previous equations, x3
 x 9 X i j (where 4x 2 y8 8 y8 9
Y8 3 8 9 Ii j 54, y 3 y 9 Yi j , and q3 q 9 Q i j . Furthermore, q
represents the state of Mi j at time tr while q represents
the state at time tr1 .
The exemplar CCA described above maps all external
input to a single cell, 123 24, and ensures that the input values received by the component, Mi j , are contained within
the set of valid input values that are specified for the component. Given this and the state transition function 5 i j , cell
123 24 may undergo a state change that would not have
been achieved if the input was dependent solely upon its
influencers. This of course may have an influence on the
states of its influencees in later time segments. Figure 2
demonstrates this.
In Figure 2, t0 is the time when the CCA is in its initial
state, q  , and we let that time be zero. Each subsequent
time is calculated by adding the next element from T (i.e.
413 33 13 6 6 6 5) to the current time. Note that for the way
that Ii j is defined in this example, Mi j is not one of its
own influencers. It is only the cells directly above, below
and to the sides of the cell that influence its next state.

COMPOSABLE CELLULAR AUTOMATA

Figure 2. Exemplar 3  3 CCA cell states at different time instances, tr .

For instance, the state of M10314 (first row, second column)
is zero at time t0 . This cell is influenced by cells M10304 ,
M10324 and M11314 , which output their current states at the
end of the first discrete-time segment1 one, zero and one,
respectively. There is no external input to this cell. When
the state transition function executes at the start of time t1 ,
the median of these three input values are taken and the
state of M10314 is set to one at time t1 .
A pattern is seen in the cell states from times t0 to t3
that repeats every two time steps. This pattern would repeat until the end of the simulation without any external
intervention. However, at time t3 , external input with value
one is mapped to cell M12324 . This external input breaks the
pattern by changing the state of M12324 , as seen in Figure 2,
time t4 . This state change is similar to that which was described above for M10314 at time t1 . The state of M12324 at
time t4 is dependent on the median of the input received
from M11324 and M12314 , and the external input, x, received
at the end of the t3 discrete-time segment. The external input also, by chance, begins a new pattern that can be seen
in times t5 –t7 , and would have been repeated if there were
additional time elements within T . As the output from the
network is the median of the output from each cell (which
is simply the current state in the example), the network
output Y N would be zero for all times except t6 . At this
time, Y N 2 1. Note that this change in external output
would not have occurred had the external input not been
received.
3.3 Execution
A cellular automaton, regardless of the number of dimensions, is a system comprising homogeneous components.
Furthermore, during each time segment, all of the cells
within the network update their current state in accordance with a transition rule that takes into account the
states of each cell’s influencers [2]. In the context of the
specification provided in Section 3.2, the transition rule,

5 i jk , takes into account the input, X i jk , which includes
both X i jk , the input received from its influencers, and
X i jk , the input originating from an external system (i.e.
state data of the cell’s influencers and of the external system). Since external input may be mapped to an arbitrary set of cells within the network, 5 i jk must allow for
X i jk 2 . Thus, input must be guaranteed to come from
the cell’s influencers. As the cell components, Mi jk , do
not have direct visibility of influencer state data, this mandates that a cell’s output to the cells it influences, Yi jk ,
(and, therefore, the influnecees’ input, X i jk ) must not be
.
Proper execution of the formalism requires more than
the state transition and output mechanisms provided in the
specification. As a cellular automaton is a network of individual automaton (referred to throughout this paper as
cells and, sometimes, components), there exists the possibility of an infinite, delayless feedback loop within the
network. To understand this, keep in mind that: (a) a signal input at time t persists and is constant until time t  1,
when a new signal is received1 and (b) within the network,
it is assumed that the communication between cells and
the execution of their output functions occur in zero time
(i.e. they are delayless). This means that, at a given time,
t, the output of one component may effect the output of
another which, in turn, may effect the output of the first,
which then immediately effects the output of the second,
etc. An indirect association between the input and output
is required in order to create a ‘delay’ and prevent this
infinite loop condition.
By specifying that the output function, 7i jk , for each
component, Mi jk , is solely dependent upon the current
state, Q i jk , to produce the output, Yi jk (thus, defining it
as a Moore model), a delay is introduced. The input is
only considered in the state transition function, 5 i jk , which
maps the input to a new state. It is this new state which
will then modify the output signal, but at time t  1. Had
7i jk considered the current input, X i jk (which would make
it a Mealy model), the input received from its influencers
could immediately effect its output signal. The use of discrete time (which does not allow zero-time state transitions and ensures that t 2 t  
t, 
t  0) enforces that
there will not be a delayless feedback loop once state transitions do occur as a result of the execution of 5 i jk . Thus,
by construction, it can be seen that the networked components will interact correctly as a system.
What is needed next is a way of formally getting data
to and from the CCA network system. Mapping functions
suit this purpose. While the network is the system of cells1
as stated above, it can be considered one abstraction of the
CA system. For the purposes of understanding the mapping, it may be easier to at first think of the network, N ,
as a shell around the set of cells, {Mi jk }, which comprise
the system. Then, one can think of a mapping function
which aggregates the individual cellular output, Y i jk , to
the network output, Y N . Similarly, another mapping funcVolume 85, Number 11/12 SIMULATION

741

Mayer and Sarjoughian

tion could provide the opposite, disaggregation of data
from the network input, X N , to the respective external input for each cell, X i jk . The functions f out and f in from (1),
are the aggregation and disaggregation functions that provide external output from and input to the cells, respectively. Thus, 4 f out 3 f in 5 (i.e. F from (1)) is the interface
that the CCA exposes to other components.
With the specification and the input and output dynamics provided, a general algorithm for executing the CCA
model is given as Algorithm 1.
Algorithm 1 General CCA execution.
Require: T 2  {T is the set of time intervals (see (1))}
BEGIN:
r  0 {current discrete segment}
tr  t0 {current time equals initial time}
while r  8T 8 do
Retrieve h r {the time interval, where h r 9 T }
tr1  tr  h r {increment current time to end of
segment}
for all 1i3 j3 k4 9 D do
Execute 7i jk {the output function of each component, Mi jk }
end for
Execute f out {generate the external output from the
network}
Execute f in {generate external input to each component, Mi jk }
for all 1i3 j3 k4 9 D do
Execute 5 i jk {the state transition function of each
component, Mi jk }
end for
r  r  1 {next segment}
end while
:END

3.4 Composition
Consider that an external system will likely have a different structure than the CA, may not provide external input
to the entire CA structure, may not have external input
every execution cycle, and may not receive output from
every cell in the network. It would be inefficient and, from
a component visibility perspective, inappropriate, for the
external system to map to every cell in the network. Thus,
two other mapping functions are required. Let 
 represent the external modeled system and G 2 4gout 3 gin 5 be
the set of mapping functions that map external data out
of N and into N , respectively. Then, it may be stated
that the output from the CA network, N , mapped to 
may be described by gout : Y N  
input . Also, the input from the external system mapped to the network may
be described by gin : 
output  X N . Function composition may then be used to define the entire mapping from
742 SIMULATION

Volume 85, Number 11/12

the external system, 
, to the cells, {Mi jk }, and back.
3
In other words, gout  f out : 1i3 j3k49D Y i jk  
input and
3
f in  gin : 
output  1i3 j3k49D X i jk . As gout and gin operate externally to the CCA network N , G is not defined
within the CCA specification. Here G would be defined in
a component that is external to the CCA. In a direct interface, within 
 itself. If poly-formalism is used, G would
be defined within an interaction model (see Figure 1).
It may at first appear to be more efficient to employ
two functions which map the external system and cells directly. However, the reader is reminded that the purpose of
this work is to present a ‘composable CA’ which is part of
a larger system. To draw away from a monolithic system
and to provide flexibility in design, interaction between
systems and reuse, the concept behind the four mapping
equations should be employed. These draw a distinct delineation between what occurs within the network and outside it. Furthermore, it allows the external system to use its
own unique identifiers for cells without specific knowledge of the network’s internal indexing structure.
Note that the mapping functions are unidirectional and
may be arbitrarily adapted to manage disparate model
structures. Thus, if the mapping functions G are between
two composable CA models, it is not required that the
two CA have the same structure. For example, a twodimensional CA of size 3  3 may be mapped to a threedimensional CA of size 6  6  3. The output of the
3  3 network, Y N 1334 , may be mapped to the input of
the 6  6  3 network, X N 16634 , by applying the output
matrix six times as three quadrants to two of the three z
layers of the larger network (see Figure 3(a)). Note that,
in this example, one quadrant of layers 0 and 2 and all of
layer 1 of the 6  6  3 network will not receive external input (i.e. X i jk 2 ). Similarly, the output from the
6  6  3 network may be mapped to the smaller network
as shown in Figure 3(b). For this example, only the output
of the second layer will be mapped to the 3  3 network
by partitioning the layer of the 6  6  3 structure into
nine 2  2 sub-networks. For the other layers, the external
output, Y i jk , is . The corresponding output from the cell
within each sub-network is then aggregated and provided
as input to the respective cell in the smaller network. The
input and output in this example is made distinctly different to stress the point that the mapping may be arbitrarily
devised as necessitated by the problem domain. This also
highlights a key difference between a multi-dimensional
CA and multiple composable CA which are mapped to
one another.
In contrast to the homogeneous nature of a single CCA,
two CCA mapped to one another may differentiate in
many ways: structure, possible states, etc. How they relate
to one another is defined via the mapping. Furthermore,
the external output from one cellular automaton, Y N (and,
therefore, the external network input, X N , to the other and
the external input for each of the other’s cells, X i jk ) may
be . Referring back to Figure 3(a) as an example, if the

COMPOSABLE CELLULAR AUTOMATA

for instance, may be a (CA-index, value) pair. Then f in
would have the specific knowledge of the network structure to provide the data to the cells specified by the index
set. With this approach, the external mapping functions,
gin and gout , only require knowledge of how the network
references its individual cells and not the structure of the
network itself.
4. Realization
As stated in Section 1, a CCA has potential benefits as
a model for many different systems. While this formalism could be used for stand-alone CA systems, its major benefits are gained in a hybrid system. With the formalism defined, this section provides guidance for implementation of the CCA specification through an exemplar
model. The intent is to concretize the CCA concept for
researchers of varied background. Models details are provided here only to the extent that they help to further explain the CCA concept and implementation. Additional
model details may be found in [37, 18, 38].
The exemplar landscape model is a representation of
a large watershed area in the early Bronze Age on which
farmers tend fields. It is an interconnected set of two models: land cover and soil erosion. Detailed discussions on
the landscape modeling may be found in [39, 37, 40, 41].
The landscape model is itself a sub-system that is part of
a hybrid system model (see Figure 4). This hybrid system model is a composition of the landscape sub-system
model and the farmer sub-system model. The sub-systems
are independent in terms of structure and behavior. As
can be seen in Figure 4, the farmer and landscape models are not connected directly. Instead, they communicate
through the IM that manages the structural and behavioral
disparities.
While the landscape model may be separate from the
farmer model, the land cover and soil erosion models
within the landscape model in Figure 4 are interdepen-

Figure 3. CA mapping. (a) Mapping two-dimensional external output to three-dimensional external input. Cells without letters indicate no external input is received. (b) Mapping three-dimensional
external output to two-dimensional external input. Cells without
letters indicate no external output is generated. (Top layer outlined
for clarity.)

two-dimensional system is considered to be just an arbitrary external system, then the set of output, 
output , could
be considered to be the set of nine values, 4a3 b3 6 6 6 3 i5,
and may be indexed and structured in any manner that is
appropriate within 
. Here gin would map these nine values to the specific 54 cells in the three-dimensional cellular automaton using the network index set, D. The output,

Figure 4. Exemplar hybrid model.
Volume 85, Number 11/12 SIMULATION

743

Mayer and Sarjoughian

dent. Soil erosion uses land cover, elevation and rainfall
values to determine how much soil is removed and or deposited in the landscape region. The land cover prevents
soil erosion. The heavier the land cover foliage, the lower
the erosion in that area. In turn, healthy soil values support
larger land cover. If soil is washed away, a smaller amount
of foliage is supported. The rainfall values provide the
underlying cause of the landscape dynamics. Heavy rain
helps the land cover to grow but also washes away more
soil. For simplicity, if the system is modeled over a shorter
term, one can assume that the rainfall is constant. Overall,
the system operates independently of the farmer model.
However, when they are composed using the interaction
model, each creates impacts on the other. The farmer, in
an effort to create land that is suitable for agriculture, removes some of the land cover. This, in turn, increases the
erosion in the area. The reduced soil, combined with the
continual farming in the area does not allow the foliage to
grow back and soil conditions worsen until farming can
no longer be sustained. So, the farmer moves to different locations in an effort to grow enough food to survive.
Land cover may grow back but may not be capable of returning to the condition it once was due to changes in soil
quality.
GRASS has been selected as a development environment for the CCA within the exemplar domain for two
reasons. The first is for its efficiency in modeling the very
large number of cells that the CCA requires in order to adequately represent the landscape. The second is to demonstrate how a software development tool that lacks strong
theoretical underpinnings may be employed to create a
model built to the CCA specification. GRASS modules,
which are essentially map algebra constructs, are typically
ill-suited for formal specifications such as CA. The more
general map algebra specification that GRASS is built
upon allows manipulation of the maps in ways that would
be detrimental to ensuring the correctness of the dynamics
specified in the CCA formalism. However, the modules
available in GRASS can be judiciously used to realize the
CCA formalism. The approach is similar to how a modeler may use a programming language (e.g. Java and C)
to implement a theoretical specification, for example, by
limiting the use of the environment’s tools in an effort to
strictly comply with the formalism.
To realize a CCA within the GRASS development environment, it is important to understand how the two relate to each another. There is a close resemblance to the
regular grid-like tessellation of cells within a GRASS
raster map and those within a square-grid cellular automaton. A GRASS raster map may be formatted for either
two-dimensional or three-dimensional data storage. Each
raster map represents one data value (e.g. soil depth). If
another value is desired to describe that same geographical
area, such as a numeric value for land cover, it would be
stored in a different map. The benefit of using a georeferenced environment such as GRASS is that multiple maps
can be assigned to represent the same geographical region.
744 SIMULATION

Volume 85, Number 11/12

Then, a cell 1x3 y4 referenced in one map would have a
geographic representation in each of the others. GRASS
modules are devised such they can perform map algebra
calculations across maps given these geospatial relationships [16]. In contrast, a cell within a CA has a state. That
state can be comprised of many sub-states (e.g. soil depth
and land cover). Thus, if a CA is realized within GRASS,
the relationship between a cellular automaton cell and
GRASS maps is one-to-many. Also, while a cell may have
dimensions as part of its state, these values may have little impact on the CA as a system. So, to ease the burden
of association between the two, it would be beneficial to
choose a GRASS region and resolution setting that creates
a one-to-one cell count ratio between the GRASS map and
the CA index set. Otherwise, a more complex mapping
is required (the devising of such a mapping is left to the
reader’s own discretion).

4.1 Architecture
Figure 4 also provides a conceptual representation of the
software design for the exemplar hybrid model. The two
CCA models, the land cover and the soil erosion, should
be treated as separate CCA. They should only interact
through their respective mapping functions. In this case,
gout  f out of one model becomes the f in  gin of the other.
Although these components may directly manipulate the
data from both models despite differences such as dimension and scale, the modeler is strongly cautioned against
this approach. To do such leads to modeling choices that
leave a model’s correctness suspect. As is the case with
the hybrid model in Figure 4, both the land cover CCA
and the soil erosion CCA are capable of interacting with
another model (in this case, the IM), which does not share
a CCA formalism or realization environment. Allowing
direct manipulation of data across the CCA systems, in
general, can significantly reduce interaction visibility and
increase the complexity of model verification and simulation output validation. For example, direct manipulation
of state data between the CCA may invalidate the data being shared with the IM. By maintaining the inviolability
of each CCA through use of the mapping functions, the
modeler maintains interaction visibility1 not only between
the composable CA, but between each CCA and the interaction model.
An interaction model facilitates the composition between the landscape CCA models with the agent model.
An IM is not needed between the CCAs as the two CCAs
use the same formalism and implementation mechanisms.
The CCA and agent models, however, are disparate. Their
underlying specifications, execution engines and architectures are all different. Therefore, the KIB concept is applied to handle data exchange and control while ensuring
simulation execution verification and supporting model
validation.

COMPOSABLE CELLULAR AUTOMATA

4.2 CCA implementation in GRASS
Two CCA models are implemented in GRASS: a land
cover CCA model and a soil erosion CCA model. The
state data for each cell in each CCA is stored in one or
more GRASS maps. The dynamics of the formalism are
expressed through scripts that specify specific GRASS
modules to be executed for each model. The earlier point
about being particular about which GRASS modules are
implemented and how they are applied against the maps
needs further explanation.
Timing is necessary to give meaning to the created
GRASS maps as a progression of the CCA cell states
over time and in order to synchronize the land cover and
soil erosion models with the agent model. However, the
GRASS environment lacks an innate sense of timing. To
compensate, the authors have chosen to employ a timed
state-based timer model. This timer will be realized using
a parallel atomic DEVS model [21]. The DEVS model is
devised to undergo internal transitions that are synchronized with the set of time intervals, T , and execute the
GRASS scripts as appropriate to provide functionality for
the CCA system functions and mappings. Each CCA is
provided with its own model timer and set of scripts. In
order to avoid confusion, it is important to highlight the
fact that the CCA are not being made into DEVS models1 the DEVS timing models are simply providing timing
for the CCA dynamics since GRASS does not provide a
formal specification of time with an execution protocol. A
modeler could, if one so desired, create a more elaborate
script with a loop and a variable representing time to provide similar functionality. All of the CCA state changes, in
accordance with the formalism given in Section 3, remain
within GRASS (including the associated scripts). As both
the IM and the farmer models are realized in DEVSJAVA
[42], it is a matter of convenience for the authors to also
use DEVS for the timer model.
If all of the DEVS models are executed as DEVSJAVA models under a single, coupled model, then they
may be coupled per the DEVS specification and share the
same simulator. The DEVS timer models that provide timing to the GRASS scripts may interface with the scripts
via Java’s runtime.exec command and capture return
data parsed from an output buffer stream [18]. Just as the
DEVS models may all be incorporated into a single coupled model, the GRASS maps may all be enclosed within
a single GRASS mapset. As GRASS is a general development environment, and it is difficult to provide validation
techniques using scripts, the modeler must ensure that the
scripts limit their activities to the appropriate maps, using
appropriate GRASS modules, in order to maintain correctness and consistency according to the composable CA formalism.
A combination of GRASS modules (organized and applied via scripts), GRASS raster maps and DEVS timer
atomic models are used to realize the CCA formalism.
The GRASS module r.mapcalc performs the major-

ity of the GRASS map algebra operations. It may be applied to provide internal and external input, to emulate the
transition functions and to generate the output. It accepts
one or more existing maps as input, performs a set of operations on those maps, and produces one map from the
result. Modules such as r.stats and r.info are map
data query functions and can be employed to obtain and
format data as external output. They simply query all or a
portion of a map and provide the result as output. Following Algorithm 1, the execution within GRASS and DEVS
can be specified. To clarify the relationship between the
state-based timer model and the CCA, the timer model
will be given two states: input and output. Each state handles the scripts represented by its name. Note that, in each
simulation cycle, the CCA executes both its output function and transition function (see (2) in Section 3.2). Also,
in order for a timer model to change state, it must undergo
either an internal or external transition (caused by a timebased, state-specified internal change or input from an external entity, respectively). As both the input and output
states are necessary regardless of whether or not there is
external input, this means that the timer model will undergo two internal transitions for every CCA simulation
cycle. In the list that follows, the timer model execution is
described. For simplicity, the description is limited to the
Land Cover CCA’s timer model dynamics with respect to
the calls made to the GRASS mapping scripts and module functions and the external input and output messages
it receives from the IM. Note that a similar exchange occurs between two mapped CCA (such as the land cover
CCA and soil erosion CCA in Figure 4). In this case, it
should again be noted that the gin and gout mapping functions from the perspective of the first CCA, are the f out
and f in mapping functions of the second, respectively.
Let r be the current segment where 0  r  8T 8 and
let the timer model be initialized to the output state at the
start of a discrete-time segment at time tr . Then we have
the following algorithm.
1. The timer model undergoes an internal transition to
an input state at time tr  h r . Before entering the
input state, the DEVS timer model output function
is called. The output function calls the output function script, which is the CCA cells’ 7i jk in (2). This
script directs the r.mapcalc GRASS module to
query all cells within each GRASS map that represents the composable cellular automaton and generates an output map based on the state of each cell as
represented by the map values.
2. Also, before entering the input state, the output
mapping script is called which, in turn, implements
the GRASS module r.stats to collect the values
from the output map and format it as representative
output from the mapping function, f out . This data is
then sent to the IM. The IM then executes the gout
mapping function to enable the data to be processed
Volume 85, Number 11/12 SIMULATION

745

Mayer and Sarjoughian

Table 1. Parallel atomic model for timing and I/O relationship to CCA/GRASS.
Step

DEVS timer model

Script

GRASS module

result

1
2
3, 4

Output function script, 7i jk
Output mapping script, f out
Input mapping script, f in

r.mapcalc
r.stats, r.info
r.mapcalc

CCA cell output map
CCA output, f out
CCA external input
map, f in  gin

5
6

Output function, (output state)
Output function, (output state)
External transition function,
(input state)
Internal transition, (input state)
Output function, (input state)

State transition function script, 5 i jk

r.mapcalc

7

Internal transition, (output state)

Revised map values
(i.e. cell state)
New discrete-time segment

by the farmer agent model. Once the f out output data
is sent to the IM, the timer model enters the input
state.
3. If the IM receives input for the land cover CCA
from the external sub-system model (i.e. farmer
agent model), the IM executes gin to map the data
to X N . The data is then sent to the timer model.
As part of the external transition, the timer model
checks its current state.
4. If, and only if, the timer model is in the input state
when external input arrives, the timer model calls
the input mapping script, which is representative of
f in . This script uses r.mapcalc to convert the received data into an input map with the same regional
specifications as the maps which comprise the land
cover CCA model. If the timer model is in the output state when the external input arrives, the input
is ignored.
5. The timer model remains in the input state for zerotime (simulation clock) and immediately schedules
an internal transition back to the output state with
the time to remain in that output state equal to the
next scheduled time interval, h r1 . If external input
is received (as described in the previous item), then
it is handled before the internal transition is scheduled.
6. Before the timer model undergoes its internal transition to the output state, its output function is called.
The timer model output function calls the state transition function script, which is the CCA cells’ 5 i jk
in (2). This script employs r.mapcalc to evaluate
both the current CCA maps and the map of external
input (if it exists) to create revised values for one or
more cells in one or more maps, thus changing the
state of the CCA.
7. The timer model enters the output state and, therefore, the next discrete-time segment.
The above algorithm represents one possible approach
to handling the external input. This method enforces a
746 SIMULATION

Volume 85, Number 11/12

Figure 5. Timer model statechart for the exemplar CCA realization.

discrete-time handling of input at the start of a timing segment. The dynamics and the pieces which play a role in
the algorithm described above can also be seen in Table 1
and Figure 5. Note that Figure 5 is presented from the
DEVS timing model’s perspective. Therefore, there are
two states: input and output. The CCA cells have at least
two states and are not restricted to ‘input’ and ‘output’. In
addition, there are two DEVS internal transitions specified
(before which, the output function of the DEVS model is
called). However, the output from the CCA only occurs
once, in step 2 of the algorithm above (the DEVS output
function from the output state, in Figure 5). In Figure 5, it
should be noted that there are two DEVS transition functions from the input state to the output state. The function
used is dependent upon whether or not external input is received. If external input is received, the DEVS confluent
transition function is used. For this model, the external
events (the input) is handled first. Then, there is an internal transition to the output state. As before, immediately before the internal transition, the DEVS model output function is called. For the transition from input state
to output state, the DEVS model output function executes
the CCA state transition function, 5 i jk . If there is no input,
then just the internal transition function is executed and, as
before, the output function is called, which executes 5 i jk
in the CCA. Table 1 provides a step-by-step relationship

COMPOSABLE CELLULAR AUTOMATA

between the algorithm in Section 4.2 and Figure 5. This
separation of mechanics from domain dynamics representation should help enforce to the reader the concept that
the DEVS atomic model mechanics are being manipulated
to provide timing to a CCA that is realized in GRASS.
The validity of the CCA model is dependent upon following the above steps and, more importantly, working
within the constraints they represent. Instead of state variables, GRASS employs the use of maps. While many software environments might use a temporary variable to store
the output from a cell to those cells that it influences,
GRASS saves the output values in a map form. The state
transition function is able to read this map and construct a
new map that represents the states of each cell in the CCA
at the start of next discrete-time segment.
It is important that the modeler does not undermine
the purpose of the constraints using one of the development environments simply for ‘ease of coding’. For example, assume that a simulation begins at time t0 . The timer
model does not begin its transition from output state to
input state until what can be considered the end of the
segment, whose length is defined by t0  h 0 . As part of
the transition, the timer model output function is executed,
enabling the CCA to provide output via the output function, 7i jk , from each of its cells. This output is representative of the initial state of the CCA. Note that the output,
as would be expected for a discrete-time system, does not
change until the start of the next segment. Next, the timer
enters the input state. The transition occurs in zero-time
and, therefore, is still considered to occur as part of the
current segment. As stated above, external input is managed during the input state through the use of the timer
model’s external transition function. Note that external input does not immediately change the state of the CCA1 it
is stored for later application. The time at which the CCA
system transitions to the output state is considered to be
the end of current segment (i.e. tr ) and the start of the next
(i.e. tr1 ), for r  0. As part of the timer model transition,
its output function is again executed. The output function
calls the state transition script, implementing 5 i jk for each
cell in the CCA, which accounts for the input from its
influencers and the external input all at once. Note that,
similar to the output, the state transition only occurs once
for each CCA simulation cycle. Also, note that what the
modeler chooses to do with the input is arbitrary. For example, external input may cause influencer input to be ignored or input from all sources may be aggregated. What
is important is that this is the only period during the segment during which the cells and, therefore, the CCA as a
system, may change state.
The modeler must verify the state transition function
by ensuring that, for all acceptable external input and
influencer input, the cell, Mi jk reaches a state that is an
element of the set of possible states, Q. Note that acceptable input implies current input (i.e. from this time segment). The modeler should not, for example, draw upon
GRASS maps which represent input values from previ-

ous time segments. If this data is required for the system representation, then the cells themselves should all be
defined to store this historical data as part of their state and
then provide the data as part of their output function (i.e.
current influencer input). This maintains a basis that conforms to the formalism and that may more systematically
be traced during validation exercises. Once the CCA cells
enter their new states, they remain there for the duration of
the segment. The timer model stays in the output state until the cycle begins anew. If it is assumed that there exists a
state transition function and an output function which are
appropriately bounded by their input and output sets, then
the implementation steps described above in this section
provide a proof by construction of the validity of the use
of the DEVS timer model, and GRASS scripts and maps
to simulate the CCA.
5. Conclusion
In this paper we have described a CCA specification that
is capable of receiving external input and sending output1
a necessity for interacting with other CCA and/or non-CA
models. This approach establishes a clear distinction between the automata themselves, and the network to which
they belong. Furthermore, the automata only interact via
input and output1 no direct visibility or manipulation of
state by other automata is allowed. Similarly, the network
exposes an interface to its encapsulated set of automata
through the use of an input and output set, and mapping
functions. A key benefit to this approach is the ability to
formally incorporate the CCA into a hybrid model1 allowing the CCA and another disparate model formalism to
represent heterogeneous sub-systems within the modeled
system. Using this approach, the modeler also gains the
ability to devise a specific implementation of input and
output from a CA in general, domain-neutral terms. This
is in contrast to program environment and domain-specific
descriptions required by other approaches and techniques.
In addition, we show how two CCA can be composed with
one another and with an IM, a non-CA model. The formal and explicit modeling of the interactions within the
IM supports defining data transformations between CA
and non-cellular models. Therefore, management and visibility of interaction complexity is supported using formal
modeling methods. Furthermore, the explicit modeling of
the interactions assists in validating the simulation models.
Any CCA simulator must be capable of executing Algorithm 1, which was presented in Section 3.3. Of course,
the specifics of how a simulator executes the algorithm
provided is dependent upon the software environment in
which the model and the simulator are implemented. The
implementation in Section 4.2 is just one possible approach in a specific software development environment.
The capability to efficiently represent a very large cellular space was the major momentum for the authors to
Volume 85, Number 11/12 SIMULATION

747

Mayer and Sarjoughian

use the GRASS environment. Other factors such as more
direct timing mechanisms and state space representation
may drive other modelers to employ other approaches.
At a time when some scientists are using informal
model specification or prose to describe their simulation
models [10], the proposed approach enables a broader
community to benefit from a rigorous approach to model
development and simulation experimentation. While this
paper has focused on a square grid, GRASS representation
of a CCA, other dimensions, networks and development
environments are possible using the CCA specification
provided herein. Many of the discussions about GRASS
may be applicable to other GIS implementations. Furthermore, the CCA may be implemented within non-GIS
environments. One such possible environment is Repast,
where integration of agent and CA implementations can
be elevated to a composition of agent and CA models.
6. Acknowledgements
This research is supported by NSF grant #BCS-0140269.
We would like to thank the MEDLAND team for their
help and partnership1 particularly M. Barton and I. Ullah.
7. References
[1] Anderson, J.A. 2006. Automata Theory with Modern Applications.
Cambridge University Press, New York.
[2] Ilachinski, A. 2001. Cellular Automata: A Discrete Universe. World
Scientific, Hackensack, NJ.
[3] Wolfram, S. 2002. A New Kind of Science. Wolfram Media, Inc.,
Champaign, IL.
[4] Davis, P.K. and R.H. Anderson. 2004. Improving the Composability
of Department of Defense Models and Simulations. RAND, Santa
Monica, CA, 2004.
[5] Fishwick, P. 1995. Simulation Model Design and Execution: Building
Digital Worlds. Prentice-Hall, Upper Saddle River, NJ.
[6] Mosterman, P. and H. Vangheluwe. 2004. Computer automated
multi-paradigm modeling: an introduction. Simulation Transactions, 80: 433–450.
[7] Huang, D., H.S. Sarjoughian, W. Wang, G.W. Godding, D.E. Rivera,
K.G. Kempf and H.D. Mittelmann. 2009. Simulation of Semiconductor Manufacturing Supply-Chain Systems with DEVS,
MPC, and KIB. IEEE Transactions on Semiconductor Manufacturing, 22(1): 165–174.
[8] Mosterman, P. and H. Vangheluwe. 2002. A hybrid modeling and
simulation methodology for dynamic physical systems. Simulation Transactions, 78: 5–17.
[9] Watson, F. and J. Rahman. 2004. Tarsier: a practical software framework for model development, testing and deployment. Environmental Modelling and Software, 19(3): 245–260.
[10] Grimm, V., U. Berger, F. Bastiansen, S. Eliassen, V. Ginot, J. Giske,
J. Goss-Custard, T. Grand, S. K. Heinz, G. Huse, A. Huth, J. U.
Jepsen, C. Jørgensen, W. M. Mooij, B. Müller, G. Pe’er, C. Piou,
S. F. Railsback, A. M. Robbins, M. M. Robbins, E. Rossmanith,
N. Rüger, E. Strand, S. Souissi, R. A. Stillman, R. Vabø, U. Visser
and D. L. DeAngelis. 2006. A standard protocol for describing
individual-based and agent-based models. Ecological Modelling,
198(1–2): 115–126.
[11] North, M., T. Collier, and J. Vos. 2006. Experiences creating three
implementations of the Repast agent modeling toolkit. ACM
Transactions on Modeling and Computer Simulation, 16(1): 1–
25.
748 SIMULATION

Volume 85, Number 11/12

[12] Ntaimo, L., B. Zeigler, M. Vasconcelos and B. Khargharia. 2004.
Forest fire spread and suppression in DEVS. Simulation Transactions, 80(10): 479–500.
[13] Wainer, G. 2006. Applying Cell-DEVS methodology for modeling
the environment. Simulation Transactions, 82(10): 635–660.
[14] MEDLAND. 2004. Landuse and landscape socioecology in
the Mediterranean Basin: A natural laboratory for the study
of the long-term interaction of human and natural systems.
http://www.asu.edu/clas/shesc/projects/medland/, accessed September 2008.
[15] GRASS. 2008. Geographic Resources Analysis Support System.
http://grass.itc.it/, accessed September 2008.
[16] Neteler, M. and H. Mitasova. 2004. Open Source GIS: A GRASS
GIS Approach, 2nd edn, Springer, New York.
[17] Sarjoughian, H.S. 2006. Model composability. In L.F. Perrone, F.P.
Wieland, J. Liu, G. Lawson, M. Nicol and R.M. Fujimoto (eds),
Proceedings of the 2006 Winter Simulation Conference (WinterSim’06), Monterey, CA, December 2006, pp. 149–158, IEEE
Press, Piscataway, NJ.
[18] Mayer, G.R. and H.S. Sarjoughian. 2007. Complexities of simulating a hybrid agent-landscape model using multi-formalism composability. In Proceedings of the 2007 Spring Simulation Conference (SpringSim ’07), Norfolk, VA, March 2007, pp. 161–168,
IEEE Press, Piscataway, NJ.
[19] Mayer, G.R., H.S. Sarjoughian, E.K. Allen, S.E. Falconer and C.M.
Barton. 2006. Simulation modeling for human community and
agricultural landuse. In Proceedings of the 2006 Spring Simulation Conference (SpringSim ’06), Huntsville, AL, April 2006,
pp. 65–72, IEEE Press, Piscataway, NJ.
[20] Muzy, A., E. Innocenti, D. R. C. Hill, A. Aiello, J. F. Santucci and
P. A. Santoni. 2004. Dynamic structure cellular automata in a fire
spreading application. In First International Conference on Informatics in Control, Automation and Robotics (ICINCO), Setubal,
Portugal, 2004, pp. 143–151.
[21] Zeigler, B., H. Praehofer and T. Kim. 2000. Theory of Modeling and
Simulation: Integrated Discrete Event and Continuous Complex
Dynamic Systems, 2nd edn, Academic Press, San Diego, CA.
[22] Godding, G., H.S. Sarjoughian and K. Kempf. Application of combined discrete-event simulation and optimization in semiconductor enterprise manufacturing systems. In Proceedings of the
2007 Winter Simulation Conference, Washington, DC, December 2007, pp. 1729–1736.
[23] Weisstein, E.W. 2008. Cellular automaton. From Mathworld—
a Wolfram web resource. http://mathworld.wolfram.com/
CellularAutomaton.html, accessed September 2008.
[24] Barile, M. and E.W. Weisstein. 2008. Neighborhood. http://
mathworld.wolfram.com/Neighborhood.html, accessed September 2008.
[25] Wolfram, S. 1983. Statistical mechanics of cellular automata. Reviews of Modern Physics, 55: 601–644.
[26] El Yacoubi, S., B. Chopard and S. Bandini (eds). 2006. Proceedings 7th International Conference on Cellular Automata for Research and Industry, ACRI 2006, Perpignan, France, September
2006 (Lecture Notes in Computer Science, Vol. 4173), Springer,
Berlin.
[27] Sloot, P.M.A., B. Chopard and A.G. Hoekstra (eds). Proceedings
6th International Conference on Cellular Automata for Research
and Industry, ACRI 2004, Amsterdam, The Netherlands, October
2004 (Lecture Notes in Computer Science, Vol. 3305), Springer,
Berlin.
[28] Briesen, M. and J.R. Weimar. 2001. Distributed simulation environment for coupled cellular automata in Java. Parallel Computing,
pp. 66–74.
[29] Hoekstra, A.G., J.-L. Falcone, A. Caiazzo and B. Chopard. 2008.
Multi-scale modeling with cellular automata: the complex automata approach. Proceedings 8th International Conference on
Cellular Automata for Research and Industry, ACRI 2008 (Lecture Notes in Computer Science, Vol. 5191), pp. 192–199,
Springer, Berlin.

COMPOSABLE CELLULAR AUTOMATA

[30] Hoekstra, A.G., E. Lorenz, J.-L. Falcone and B. Chopard. Towards
a complex automata framework for multi-scale modeling: formalism and the scale separation map. Proceedings International
Conference on Computational Science 2007 (Lecture Notes in
Computer Science, Vol. 4487), pp. 922–930, Springer, Berlin.
[31] Tomlin, C.D. 1990. Geographic Information Systems and Cartographic Modeling. Prentice-Hall, Englewood Cliffs, NJ.
[32] DeMers, M.N. 2002. GIS Modeling in Raster. John Wiley & Sons,
Inc., New York.
[33] de Lara, J., H. Vangheluwe and M. Alfonseca. 2004. Metamodelling and graph grammars for multi-paradigm modelling in
AToM3. Software and Systems Modeling, 3(3): 194–209.
[34] Sarjoughian, H.S. and D. Huang. 2005. A multi-formalism composability framework: agent and discrete-event models. The
9th IEEE International Symposium on Distributed Simulation
and Real Time Applications, Montreal, Canada, October 2005,
pp. 249–256.
[35] Mayer, G.R. and H.S. Sarjoughian. 2008. A composable discretetime cellular automaton formalism. In H. Liu, J. Salerno and
M. Young (Eds), Proceedings of the 1st International Workshop on Social Computing, Behavioral Modeling, and Prediction, Phoenix, AZ, March 2008, pp. 187–196, Springer, Berlin.
[36] Schiff, J.L. 2008. Cellular Automata: A Discrete View of the World.
John Wiley & Sons, Inc., Hoboken, NJ.
[37] Barton, C.M., I. Ullah and H. Mitasova. 2009. Computational modeling and socioecological dynamics in the Neolithic of the Near
East. American Antiquity, in review.
[38] Ullah, I., C.M. Mayer, G.R. Barton and H.S. Sarjoughian.
2008. Coupled agent-based and geospatial models: a new approach to socioecological simulation. In Invited Symposium
Paper Presented at the 73rd Annual Meeting of the Society for American Archaeology, Vancouver, BC, March 2008,
http://www.asu.edu/clas/shesc/projects/medland/files/Ullah_etal
_SAA2008.pdf, accessed January 2009.
[39] Arrowsmith, J.R., E.N. DiMaggio, C.M. Barton, H.S. Sarjoughian,
P. Fall, S.E. Falconer and I. Ullah. 2006. Geomorphic mapping and paleoterrain generation for use in modeling Holocene
(8,000—1,500 yr) agropastoral landuse and landscape interactions in southeast Spain. American Geophysical Union, Fall
Meeting 2006, San Francisco, CA, December 2006.

[40] Soto, M., P. Fall, M. Barton, S. Falconer, H. Sarjoughian and R. Arrowsmith. Land cover change in the Southern Levant: 1973 to
2003. Presented at ASPRS Southwest Technical Conference. Arizona State University, March 2007.
[41] Ullah, I. and C.M. Barton. 2007. Alternative futures of the past:
modeling neolithic landuse and its consequences in the ancient Mediterranean. In Proceedings of 72nd Annual Meeting
of the Society for American Archaeology, Austin, Texas, April
2007.
[42] Arizona Center for Integrative Modeling and Simulation (ACIMS).
2001. DEVSJAVA, available from http://www.acims.arizona.edu,
accesseed January 2009.

Gary R. Mayer is a PhD candidate in the department of Computer Science and Engineering at Arizona State University. His
research focuses on composition of heterogeneous model types.
He has worked for the past four years as a research associate
on the Mediterranean Landscape Dynamics (MedLand) project,
which is modeling human–environmental activities in the Early
Neolithic era, and is supported by NSF.
Hessam S. Sarjoughian received the PhD degree in Electrical
and Computer Engineering from the University of Arizona, Tucson, in 1995. He is Assistant Professor of Computer Science and
Engineering at Arizona State University in Tempe, Arizona, and
Co-Director of the Arizona Center for Integrative Modeling and
Simulation (ACIMS). His research focuses on model composability, visual modeling, service-oriented simulation, and distributed co-design modeling. He led the development of the Online Masters of Engineering in Modeling and Simulation in the
Fulton School of Engineering at ASU in 2004. He was among
the pioneers who established the Modeling and Simulation Professional Certification Commission in 2001. His research has
been supported by NSF, Boeing, DISA, Intel, Lockheed Martin,
Northrop Grumman, and US Air Force.

Volume 85, Number 11/12 SIMULATION

749

Simulation
Methodology

Action-level real-time DEVS modeling
and simulation

Simulation: Transactions of the Society for
Modeling and Simulation International
2015, Vol. 91(10) 869–887
Ó 2015 The Author(s)
DOI: 10.1177/0037549715604720
sim.sagepub.com

Hessam S Sarjoughian* and Soroosh Gholami

Abstract
For some classes of systems, it is advantageous to develop real-time models instead of step-wise or logical-time models.
Toward this goal, the action-level real-time (ALRT) discrete-event system specification (DEVS) modeling and simulation
approach is proposed. Modeling of actions is introduced into the parallel DEVS formalism using time invariants defined
for real-time statecharts. Actions are specified in terms of time-windows that are to be executed in real-time. An
abstract simulator protocol is devised for executing the ALRT-DEVS models under constrained computational resources.
The approach is implemented in the parallel DEVS-Suite simulator. These models can be simulated on unitary computing
platforms. A simple example switch model is detailed and tested to show the kinds of real-time modeling and simulation
studies that can be supported.

Keywords
action-level modeling, real-time simulation, real-time statecharts, real-time DEVS

1. Introduction
A simulation platform refers to a simulator (a kind of software tool) executing on a given computing platform which
typically consists of hardware, an operating system, and
runtime libraries. If the operations of a model are defined
and executed in terms of logical-time, then a computing
platform can affect the physical time it takes to simulate
the model (i.e., simulation execution can slow down or
speed up) but not what the model does. For example a
simulation execution lasting a very short physical time
(e.g., a few seconds or a few hours) can reveal how
throughput of a supply-chain process model can change
over a period of one year. However, it is possible for
the computing platform to adversely affect a simulation
model’s behavior. For some simulation studies including
embedded systems (e.g., automobile cruise control and
network-on-chips) and cyber-physical systems (e.g., selfdriving cars and smart manufacturing), it is useful to
develop simulation models that can execute as closely as
possible in real-time and thus can interact with physical
systems.1 To achieve this goal, we may develop logicaltime models and then execute them using a simulator that
maps a model’s logical-time to the simulator’s real-time
clock. This approach can lead to inaccurate or incorrect
behaviors since time cannot be exactly represented and
manipulated in computing platforms.2
It is useful to note that developing models that can be
guaranteed to execute correctly and accurately in real-time

remains among the challenging modeling and simulation
research topics.3 Hybrid (software and hardware) systems
are more demanding to design if they are to be executed
both in real-time and in the presence of finite resources.4
Hardware systems naturally lend themselves to independent, concurrent model abstractions. In contrast, software
systems are generally abstracted to sequential models.
Design methods have been proposed to develop real-time
models of software using bounded execution times. We
can use the concept of a time-window to account for the
variability of the processing cycles and memory of a computing platform on available real-time for executing a system’s operations. A time-window defines an upper bound
for an operation to be correctly executed. Worst case execution time analysis and static scheduling methods have
been proposed for calculating time-windows.5
Returning to simulation modeling, execution of the
operations in a model can be classified to occur in logicaltime, soft real-time, or hard real-time. For some simulation
Arizona Center for Integrative Modeling and Simulation, School of
Computing, Informatics, and Decision Systems Engineering, Arizona State
University, Tempe, AZ, USA
*

SCS member.

Corresponding author:
Hessam Sarjoughian, Ira A Fulton School of Engineering, Brickyard Suite
501, 699 South Mill Avenue, Tempe, AZ 85281-8809, USA.
Email: hss@asu.edu

870

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

applications, such as autonomous, intelligent cruise controller6 and flit-level Network-on-Chip design,7 operations
must be completed within hard real-time deadlines, otherwise the simulation may not be useful or may even be
incorrect. For some other kinds of simulation applications,
such as human-in-the-loop training exercises, executing
operations in soft real-time (also known as best-effort)
with some delay can be acceptable. Each of these modeling approaches must support specifications for time-based
operations within individual models and across compositions of models. Furthermore, a model specified using one
of these modeling approaches should be executed using a
simulator having a logical-time, soft real-time, or hard
real-time clock.
The classic and parallel atomic discrete-event system
specification (DEVS) formalisms are defined in terms of
state transitions with inputs and outputs. A model’s external transition function maps a state and an input to another
state at an instance of time. Similarly, internal transition
function maps a state to another state at an instance of
time. Operations can be associated with both of these functions, but they do not have specifications in these atomic
models. Furthermore, state transition events are defined to
occur at instances of time. However, in real-time simulation, state changes and operations may occur during a time
interval instead of an instance of time. To allow an event
to occur during some period of time, time-window was
introduced to classic DEVS formalism.8 Time-window is a
multi-value function as compared with time advance function which is a single-value function. We note that the concept of time-window is different from probabilistic or
other methods which compute an instance of time from a
range of values. Therefore, neither classic nor parallel
atomic DEVS models can represent operations and enforce
their executions to be completed within assigned timewindows.
To represent operations that can be performed within
designated time-windows, real-time DEVS (RT-DEVS)
modeling approach with a simulator has been developed.9
The concepts of activity set with constraints is introduced
for representing operations. Also an activity mapping is
introduced to map state to activities. The time-window
concept is used to restrict the actual real-time that can be
used to execute activities on a given simulation platform.
Activities are not explicitly included in the external and
internal transition functions. Individual activities with individual time-windows are not accounted for. Therefore, a
set of activities cannot be directly assigned to external transition or internal transition functions. Without the ability to
represent individual activities with their designated time
constraints, they cannot be systematically prioritized. In
addition, due to real-time constraints, it is necessary to
recover from actions that cannot be completed within their
assigned deadlines. We note that RT-DEVS is extended
from classic DEVS which allows handling single events in

atomic models and requires selecting one out of multiple
atomic models that are ready to be executed.
Therefore, to allow representing actions with strict time
constraints, we have developed the action-level real-time
(ALRT) DEVS modeling approach where state-based
actions are specified and sanctioned to be simulated within
hard real-time time-windows. With this approach, timings
are assigned to actions associated with state changes
occurring within internal and external transition functions.
Unlike logical-time or soft real-time DEVS models,
the ALRT-DEVS atomic model is constrained to have
time-windows greater than zero and at least as large as the
minimum real-time clock resolution provided by the simulation platform. A real-time simulator capable of defining
and executing the ALRT-DEVS model has been developed. A real-time model of a switch network is developed
and simulated. The purpose of this example is to highlight
what can be expected from real-time models that are targeted to be executed on real-time simulation platforms.
The contributions of this paper are the ALRT-DEVS
model with its abstract simulator protocol and an exemplar
switch model.
The rest of this paper is organized as follows. In Section
2, the building blocks upon which this work is developed
are presented. In Section 3, the ALRT-DEVS modeling
approach is introduced with the focus on atomic model
specification. Section 4 presents a simulation protocol for
real-time simulation of ALRT-DEVS models. In Section 5,
we describe a simple network switch with sample simulation results to show what can be expected using the ALRTDEVS modeling and simulation approach. Works that are
directly related to the proposed real-time modeling and
real-time simulation are discussed in Section 6 and finally,
Section 7 summarizes the paper and suggests some ideas
on future research.

2. Background
The contribution of this research is inspired and built upon
works in the modeling and simulation, and the software
engineering communities. Specifically, DEVS,10 a systemtheoretic modeling approach, deals with how to specify
discrete-event models and how these models can be executed and statecharts,11 a component-based design
approach, deals with how to build software systems. Few
simulators have been developed using classical and parallel DEVS formalisms to support real-time simulations with
varying degrees of timing accuracy.9,12–14 Many DEVS
simulators supporting logical-time modeling and logicaltime and soft real-time simulations have been developed
dating back to around the 1990s. Among these we use
DEVS-Suite simulator15 for convenience and use it to
develop the ALRT-DEVS simulator. A variety of methods
and tools have developed for designing software systems

Sarjoughian and Gholami
that operate in real-time.16–18 In this section, works that
serve as the basis for ALRT-DEVS modeling and simulation are described.

2.1 Discrete-event system specification
In this paper, formulation of real-time modeling and realtime execution is based on the parallel DEVS framework.10,19 In our work, real-time simulation modeling is
aimed at message-based input and output events and independently handling state transitions subject to hard realtime constraint specification and execution. Events may
arrive at arbitrary time instances and actions associated
with state transitions may consume arbitrary non-discrete
time durations. We note that although the computation
cost of a input/output messaging scheme is high, it does
not outweigh model modularity.
2.1.1 Parallel DEVS models. The parallel DEVS formalism
is based on a real-valued time-based abstraction. It can
handle multiple input events, multiple output events, and
allows simultaneous occurrences of input and output
events subject to not violating its legitimacy principle.
Simultaneity in external and internal state transitions is
generally handled using a state transition that has a time
duration of zero. For a coupled model, any of its components can receive multiple input events at the same time or
send multiple output events at the same time. Sending and
receiving events are independent and are causal. That is,
although inputs and outputs can occur at the same time,
output events must be first generated and then received as
input events. Neither atomic nor coupled models can send
output events to itself. Every hierarchical coupled model
must have a tree-structure and no coupled model can contain itself. The time in each atomic model is an abstract,
real-valued entity. A coupled model does not have its own
time. This is because given a coupled model, it takes zero
time for an output event of a model to arrive as an input
event to another model. Couplings in all coupled models
are timeless and define output to input causality. When
delivery of a model’s output as input to another model
takes time, then an atomic model representing a delay can
be added to the coupled model.
These atomic and coupled models have a corresponding
abstract atomic simulator and coupled coordinator protocols. These protocols must be implemented, which means
they can have different software designs. Each atomic and
coupled model is assigned its own simulator and coordinator. A root coordinator, assigned to the highest level
coupled model, is responsible for executing together all
atomic and coupled models that are contained in the highest level coupled model. One such design and implementation is developed for the DEVS-Suite simulator.15,20 This
simulator intrinsically supports simulation in abstract time

871
since state transition with a period of zero time is allowed.
Every simulator has an independent abstract clock. An
abstract global clock is used for coordinators. These clocks
are real-valued. This global clock is used to coordinate
message exchanges among atomic models of a coupled
model. It is implemented using a the clock provided by
the host computing platform and in particular, Java Virtual
Machine (JVM) clock. Thus, the abstract global clock can
run faster than, equal to, or slower than the JVM clock.
Under the assumption that the pace of the abstract clock is
the same as the JVM clock and that there is an absence of
zero-time state transitions, there still can be no guarantee
for the simulation steps to be completed in concert with a
physical clock. In such scenarios, the simulator can at best
execute as-fast-as-possible in relation to the physical clock
with best-effort synchronization between the JVM and
physical clocks. Timing for external and internal transition
functions are holistic, i.e., time is allocated for all operations that belong to external (or internal) functions. The
DEVS atomic simulator protocol is not defined to handle
time periods for individual operations, priority among
actions, or recovery from terminated actions. We note that
time advance function can be defined as a probability
function, but it still must have a single value.
2.1.2 RT-DEVS. An extension of the classic DEVS formalism10 called RT-DEVS has been developed.9,21 A real-time
atomic model is specified as RTAM = hX, S, Y, dext, dint, l,
ti, c, Ai. A set representing actions A and a mapping function c are introduced to the formalism. Every action is
defined to be atomic. The time interval function (ti(s)) is
defined as a time-window, ti(s) : s ! <0,+‘ 3 <0,+‘ .
Actions are defined to be completed within a duration that
is bounded by a pair of minimum and maximum time
instances (i.e., a time-window). Therefore, the external
transition function is defined as dext: Q 3 X!S, where
Q = {(s, e)js 2 S, 0 4 e 4 ti(s)jmax}. A state can be
associated with a set of actions, c: S ! A. Actions associated with states can have their own timings. An external
event for a given state can be associated with action and
must be completed within its designated time-window.
Input (X), state (S), and output (Y) sets as well as the internal transition (dint) and the output (l) functions are the
same as those defined for classic DEVS. Receiving and
sending events are considered to be instantaneous. Realtime atomic models are coupled in the same way as classic
coupled models without the ‘‘select’’ function. The tiebreaking is delegated to the simulator protocol. The selection is said to reflect randomness inherent in real-time
systems.
A real-time simulator protocol has been proposed for
RT-DEVS.9,21 A real-time clock (provided by the operating system (OS)) employed to execute actions is defined in
atomic models. The simulator allows executing one action

872

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

due to an external event. This action is completed within
its allotted time-window unless it is interrupted by another
external event, in which case the current action is aborted
and another action associated with the new event is started.
If an external event is received outside the time-window,
then an error is generated. A real-time coordinator supporting two threads is developed. A real-time operating system
is assumed for concurrently monitoring and executing the
atomic models. One thread monitors for external events
and another for executing activities. When multiple atomic
models need to be executed, one of them must be chosen.
This allows suspending or terminating active external and
internal events. A simulator confined to non-zero, finite
discrete time-steps (N) is developed using interrupt handlers and priority scheduling provided by a real-time operating system.

same time, one is (randomly) fired and the others are
delayed. Some temporal inconsistencies (e.g., time invariant in a state is contradicted with a transition’s time
guard) can be avoided using static analysis. Other inconsistencies may be detected by restricting real-time statecharts. Assuming periodic execution, WCET for entry, do,
and exit operations can be computed to avoid temporal
inconsistencies.16
The above time invariant and time guard specifications
are well-suited for defining real-time DEVS external and
internal transition functions that are specified to have
actions. External and internal transition functions can be
specified in terms of actions, each with its own timewindow assignment. Thus, an atomic model can have
action-level specifications within and across internal and
external transition functions as detailed in the following
section.

2.2 Real-time statecharts
Statecharts offer concepts and constructs to model behaviors of components in terms of states, events, and
actions.11 Transitions can be defined to change the state of
one component due to both events that are invoked by
itself or other components. The after (Dt) and when
(t = ta) constructs allow representing time. However these
constructs both syntactically and semantically are not
well-suited for handling hard real-time modeling. Instead,
a real-time statecharts approach22 is developed using statecharts and Timed Automata.23 Hard real-time is introduced into statecharts using clocks from Timed Automata.
Clocks allow specifying time intervals in the forms of time
invariants associated with states, timed guards for transitions, and worst case execution times (WCET) for actions.
Each clock can be reset when a transition fires. These
allow placing timing constraints (deadlines) explicitly and
independently on both states and state transitions. A state
can be active (i.e., the entry, do, and exit operations of the
states in statecharts can be restricted to execute within
time-windows) for a specified time period as long as its
time invariant is true. Time-windows can be specified for
triggering transitions, executing their actions, and setting
deadlines (relative to the triggering of a transition or a
clock). States that are time dependent are referred to as
locations in order to separate them from those which are
untimed. Several locations can be mapped to one state in
order to specify the priority and type of actions to be executed. Figure 1 shows visual notation for a simple realtime statechart (examples of complex, hierarchical realtime statecharts can be seen in the literature22).
For real-time statecharts, the time invariants are restricted to ^ti 2C (ti 4 N [ f‘g). Time guards are restricted to
^ti 2C (ai 4 ti 4 bi ), where ai 2 N, bi 2 N [ f‘g and C is a
set of clocks. Real-time statecharts support hierarchical
statecharts as defined for extended hierarchical timed automata.22 In case more than one transition is triggered at the

3. ALRT-DEVS modeling
The examination of the parallel DEVS, RT-DEVS, and
real-time statecharts shows that they are not well-suited
for real-time simulation where individual actions in a
model can be assigned deadlines and their executions are
subject to finite computational resources. To achieve this
goal, the action-level, real-time DEVS (ALRT-DEVS)
simulation modeling approach with hard real-time simulation capability is developed as described below. The realtime statecharts modeling syntax and semantics are used
for introducing action-level, real-time specification into
RT-DEVS. Next, appropriate concepts and modeling constructs are developed.

3.1 Revisiting time
The notion of time has been described and formalized from
different points of view of computing.2,24,25 In this work,
time is accounted for in terms of simulation modeling and
in particular, from a unitary computing platform with a single processor supporting multiple threads. Time is considered as a totally ordered set of discrete or real numbers for
representing passage of time in models.10 Time used in
execution engines (such as simulators) for these models is
also considered as a totally ordered set of discrete or real
numbers.
A physical clock measures time as a scalar value. The
rate of change for physical-time is linear and monotonic.
The rate of change for physical-time is constant and equal
to one. Time in a simulator clock is defined relative to a
physical clock. The physical and simulator clocks can be
related to one another using a numerical scale factor
sf 2 N . 0. Time as a scalar quantity in a simulator can
be elongated or shortened with respect to time in another
clock which may be a physical clock. This is useful for
allowing the physical-time defined in a model to execute

Sarjoughian and Gholami

873

Figure 1. Real-time statecharts.

faster or slower relative to a simulator’s clock (e.g., JVM
clock or a processor’s central processing unit [CPU]
clock). A simulation is said to be executing in ideal ‘‘realtime’’ if and only if its clock’s rate of change is identical
to a physical clock. In this paper real-time refers to the
simulator’s passage of time to be as accurate and precise
as the time in a physical system not withstanding that a
physical clock and a real-time clock cannot be identical to
each other.2
Time is also classified as physical-time, simulation
time, or wallclock time.26 Physical-time refers to time in
physical systems, simulation time refers to an abstract
notion of time for simulation execution, and wallclock
time refers to the physical-time while the simulation executes. A simulation can execute in as-fast-as-possible (i.e.,
logical-time), scaled real-time (faster or slower than wallclock time by a scale factor), or real-time in relation to
wallclock time. Time in a simulation is defined as a totally
ordered set of values where each value represents an
instance of time in some model of a physical or fictitious
system. In the context of virtual simulations and training
exercises, as much as possible time monotonically
increases at the same rate as the wallclock time. The wallclock time is converted using a scalar quantity to define
simulation time. To support real-time simulation execution, the rate at which simulated time is increased must be
slowed down assuming the simulation executes faster than
the wallclock time.27
Both theoretical and pragmatic perspectives on the role
of time in models, simulators, and computing platforms
are important for specifying real-time models and executing them in real-time. In the former, the physical clock is
the basis for concretizing time in models and the simulation protocol. In the latter, wallclock time is the basis for
executing simulations. In both perspectives, the notion of
the global simulator clock (logical-time and real-time) is
used for synchronizing time across multiple, integrated
simulation models. In the rest of this paper, the real-time
clock used in a simulator is assumed to be provided by the
clock of some target computing platform with finite accuracy and precision. The real-time clock is assumed to represent the physical-time as accurately and precisely as
possible. Physical-time refers to the time it takes for some

operation to be completed in some physical system. The
basic notions are that real-time is used for measuring simulation executions and physical-time represents passage of
time in some operating actual (physical) systems.
1.

2.

3.

Physical-time refers to the time in the physical
world. Ideally, it can be measured with infinite
accuracy and precision.
Real-time is an approximation of physical-time. As
an abstract quantity (i.e., virtual time) its rate of
change is assumed to be one. Theoretically, realtime defined in a model or for a simulator cannot
be equal to physical-time; uncontrollable factors in
computing platforms affect its accuracy and
precision.
Logical-time is an abstract computable quantity,
ideally having the properties of physical-time.

In order to define timing with respect to real-time models
and real-time simulators, we provide the following definitions. A simulation model refers to the implementation of a
model in a computer programming language that can be executed using a simulator. The simulator is kind of a real-time
software system. The time in a simulation platform should
be a scaled value of the time in a physical system. However,
during a simulation execution period, the time instances of a
simulation model in either logical-time or real-time cannot
necessarily be a scaled factor of the time instances of a physical system as described above. A segment of a real-time
simulation execution may run faster than (slower than or
equal to) its physical system counterpart. We include the
physical-time as a theoretic value and use it to show that
real-time in a simulation platform can approach it, but not
equal it. This is useful when real-time simulation is part of a
physical system and the executions of the simulation and
non-simulation parts need to be synchronized.
1.
2.

Physical system refers to an actual system that
operates in physical-time.
Real-time modeling and simulation platform refers
to a computational environment in which real-time
models can be implemented and also executed in
real-time.

874
3.

4.

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)
Real-time simulator protocol refers to an algorithm
that can execute an abstract real-time model in
real-time.
Abstract real-time model refers to a specification
for a physical system where time in the model is a
totally ordered set of real values.

Two consecutive time instances tm,a \ tm,b for a model
m and ts,a \ ts,b for a simulator s can be linearly scaled
relative to each other and tp,a \ tp,b for a physical system
p. Every \ tm,a, tm,b . (or \ ts,a, ts,b .) represents either
logical-time or real-time instances, but not both. Time
durations dtm = \ tm,a, tm,b ., dts = \ ts,a, ts,b ., and
0
0
dtp = \ tp,
a , tp, b . must be consistent with each other.
Consistency means given dt = tb 2 ta, dt0 = tb0  ta0 ,
ta = ta0 , and a scale factor sf, then dt = sf * dt#. For sf = 1,
real-time used in a model with its real-time simulator is
theoretically equal to the physical-time of its actual system
counterpart. Real-time is assumed to increase at the same
rate as the physical-time. Differences in real-time and
physical-time instances are assumed to be negligible, i.e.,
computability indistinguishable. We note that the passage
of time in any computational modeling and simulation
platform can be at most synchronized at a finite number of
time intervals with respect to the physical-time. The passage of logical-time in a simulated model can be faster (or
slower) compared with its physical-time. For a given modeling and simulation platform, the scale factor between
dtm and dts can be N . 0 or a real number with finiteprecision approximation. Assuming that time for a model
m increases as in physical-time, then dts b sf * dtm (or
dtm b sf * dts). Given Dtm = Shi= 1 dtm, i and Dts = Skj= 1
dts, i , then Dts = sf * Dtm (or Dtm = sf * Dts). It is important
to note that theoretical accuracy for time in a model cannot be realized in simulators which invariably represent
model time with finite accuracy. This mismatch with some
techniques including use of Q instead of R are described
for DEVS models.28

3.2 Atomic model
The above concepts and characterizations of time are used
in developing the action-level real-time atomic model.
The specifications for the building blocks of the ALRTDEVS atomic model are exemplified using a simple example shown at the end of this subsection
ALRT DEVS = hX , Y , S, A, G, O, c, l, tii

It is useful to note that the model specification is at the
input/output system (IOS) abstraction hierarchy.10 The
input (X), output (Y) and state (S) sets are the same as those
defined for a parallel atomic model.10,19 The external (dext)
and internal (dint) transition functions in parallel DEVS are
defined as G and O sets of functions, respectively. The

activity mapping c and time-window ti functions defined
in the real-time DEVS model specification are reformulated as part of the G and O. The output function (l) is also
reformulated. The action set (A) is defined in the atomic
real-time DEVS specification. In the following sections,
we will detail the specifications of the ALRT-IOS atomic
model.
3.2.1 Primary and secondary state variables. Next states with
hold-in times for parallel DEVS models are generally
defined using the phase and sigma (s) state variables.
These primary state variables are necessary for defining
how a model responds to external and internal events.
Additional state variables are often needed as the
dynamics of a model grows in complexity and scale. Thus,
the state of an atomic model is specified using a tuple
(S = phase 3 s 3 v1 3    3 vn). Variables (i.e., v1
3    3 vn) are secondary states. These state variables
play a fundamental role in specifying real-time DEVS
models. State-to-action mappings (c: S!A) can be specified systematically. They are used in formulating state-toaction mappings such that multiple actions can be defined
according to specific ordering of actions while satisfying
real-time simulation execution constraints. The secondary
state concept also aids not only in state-to-action mappings
within each of the internal and external transition functions but also for moving between these state transition
functions.
3.2.2 State-to-action mapping with time constraints. A
finite number of actions can be specified for a location
\ phase, Dt .. The time-window for any action is
+
defined as dtaction : A!TWl 3 TWu, where TWl = <½0,
‘) ,
+
TWu = <(0, ‘ , 0 \ twu 2 twl 4 N, twl 2 TWl and
twu 2 TWu. The lower and upper time bounds for an
action are twl and twu, respectively. A finite number of
actions can be assigned to a location (see Figure 2). The
maximum total amount time for action1,., actionr is
defined as Dt = Sqj= 1 dtj , q 4 r. Each dti can be its
WCET (i.e., twu). These actions are sequentially executed to completion subject to the amount of real-time
that is available to the simulator. The total real-time
expended on executing these actions by the simulator is
expected to be consistent with the physical-time an
actual system takes to perform the same actions (see
Section 3.1). This requires having time specifications for
every action. In the event that the available real-time in
the simulation platform is less than Dt, one or more
actions cannot be executed.
As described in Section 2.2 real-time statecharts provide time constructs (i.e., time invariants, timed-guards,
and deadlines) for specifying multiple actions with designated time-windows for external and internal transition
functions. Actions are allocated to primary states with

Sarjoughian and Gholami

875

Figure 2. A switch model with two different routing methods.

assigned time-windows. Each transition may invoke one
or more actions belonging to a location. A transition from
one location to another is triggered by evaluating secondary state variables. One or more secondary state variables
act as guard conditions for the actions assigned to a location. An action is completed only if its time-window constraint (WCET) is satisfied, i.e., its execution can be
completed in its designated real-time as sanctioned by the
simulation platform. The clocks in real-time statecharts
can be used to define elapsed times. Therefore, execution
of some actions in ALRT-DEVS internal or external transition functions are terminated when sufficient time is not
available (i.e., WCET is not satisfied). Simulation execution can benefit from efficient data structures for state
transitions and actions.
As an example shown in Figure 2, a list of actions for a
switch are defined.7 The time intervals are associated with
actions (e.g., the Route_Complex action is specified to have
minimum and maximum times 3.2 and 7.7). Distinct sets
of actions having their own total durations (T \ 6 and
T \ 10) can be defined for the Active phases using secondary state queue size (QSize) as a(n) (un-timed) guard
condition. That is, Qsize = MAX and QSize = 34 MAX
guards serve to change what sequence of actions is to be
executed. Thus, a transition becomes necessary when some
actions are to be reordered or dropped. Receiving, processing, and generating events are treated as individual actions.
This is necessary since receiving and sending events should
not be considered to occur instantaneously. Note that there
are two Active phases in the external transition function,
each having its own time invariant. Transition between
hActive, 10i and hActive, 6i is instantaneous. Finally, note
that in ALRT-DEVS actions in external and internal transition functions are associated with states. Although timedguards and deadlines are not assigned to transitions as in
statecharts, this poses no restrictions since an atomic model
cannot directly invoke actions of another model. In ALRTDEVS, as in DEVS, actions in a model may be executed
by receiving messages from another model.
3.2.3 External transition function. The external transition
function handles the events received via input ports as in
parallel DEVS. Processing an event may cause the system

to change state and/or invoke a set of actions. In parallel
DEVS, an external transition function changes state and
executes a set of actions holistically (i.e., a single sigma is
assigned regardless of the number of actions). In contrast,
in ALRT-DEVS, G, c, and ta are the elements that support
defining multiple actions due to external events. A state
change can cause a sequence of actions, each of which has
its own state and time invariant called location ‘. The
external transition function G can be divided such that
there are as many locations as there are actions. Location
is specified as q 4 s since s is a total state. The external
transition function must allow receiving input events while
any of the actions are executing. The elapse time e acts as
a clock in real-time statecharts with the requirement that it
is reset upon invocation of any action.
Activity Mapping Function:
c : L ! A, L = f‘g g, ‘g = (qg , dtg ), ‘g ! a 2 A,
qg  sg , sg 2 S, Sng = 1 dtg 4 Dt,
dtg 2 ti(s)

Example: c(h‘‘Active’’, 6i) = {Route_Simple}
The function maps the location h‘‘Active’’, 6i to action
Route_Simple.
Time Window Function:
+
ti(s): S ! twl 3 twu , twl = <(0,
‘) ,
+
twu = <(0,
‘ , twl \ twu

Example: ti (‘‘Active’’, s, Qsize) = [3.2, 10)
The shortest time required to finish the routing process
action is 3.2 (simple routing) and the maximum time is 7.7
(complex routing).
External Transition Function:
G=fdext, g g, g 2 N, dext, g : (L, e, xb ) ! L0 , 0\e 4 ‘, xb 2 X

Example: dext,1(h‘‘idle’’,Ni, e, flit_receipt) = h‘‘Active’’, 10i
This example shows the external transition for when a
flit is received in the switch model. There is a transition
from phase idle to phase routing if the queue is empty.

876

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

As in parallel DEVS, ALRT-DEVS supports receiving
a bag of input events. The maximum time allocated to execute all actions is Dt. Note that elapse time e is measured
in real-time and it must be greater than zero. This prevents
simulation steps with zero time duration. The lower bound,
twl, for time-window of every action must be greater than
zero and less than infinity. The upper bound, twu, for timewindow of every action can be infinity (see Section 3.2.2).
The lower bound value must be strictly less than the upper
bound value. Unlike soft real-time DEVS where only an
external event can interrupt an action, in ALRT-DEVS an
action can be terminated once its assigned time-window
expires. Time expiration is handled by the real-time atomic
simulator (see Section 4). The real-time atomic model does
not support the confluent transition function (dconf) defined
for parallel DEVS since receiving and sending events cannot be handled simultaneously (see Section 3.2.2).
3.2.4 Internal transition function. In ALRT-DEVS internal
transition function, state can change solely based on the
current state. However, unlike parallel DEVS, there can be
multiple actions, each with its own time-window. As in
the external transition function, states with time invariants
(i.e., locations L) are used. These actions, each consuming
a positive time duration, can be sequentially executed. The
internal transition function O can complete all of its individual actions assuming there exists sufficient physicaltime available at the time they are to be executed.
Therefore, unlike parallel DEVS, there can be no guarantee that all actions in the internal transition function can be
completed. Furthermore there can be as many output
events as there are actions. These actions are generated at different time instances. This is in contrast to parallel DEVS
where all output events are defined with respect to the state
of the model (i.e., due to cumulative state changes and
actions). The lower and upper bound time instances for every
action is handled as in the external transition function.
Internal Transition Function:
O = fdint, h g, h 2 N, dint, h : L ! L0 , L = f‘h g,
‘h = (qh , dth ), ‘h ! a 2 A, sh 2 S,
qh  sh , Snh = 1 dth 4 Dt, dth 2 ti(s)

Example: dint,1(h‘‘Active’’, 10i) = h‘‘Active’’, 6i if
Qsize = 34 MAX
In the case where three-quarters of the queue is full, the
switch transitions from the complex routing location
(which requires more time) to the simple routing location.
3.2.5 Output function. The remaining element of the realtime atomic model is the output function. Outputs are
defined in terms of actions. In parallel DEVS, outputs are
associated with change of states which implicitly account

for any operations that may be called in either external or
internal transition functions. Thus, in ALRT-DEVS, output
function is a mapping from actions to outputs. However,
since there is no guarantee for all internal and external
transition functions in G and O to be completed, some outputs may be missed. If a state change remains within its
time-window (i.e., a state change is not interrupted due to
lack of sufficient physical-time, but only because either
dext,g or dint,h is completed successfully), then ALRTDEVS output function degenerates to that of the parallel
DEVS output function. Multiple output events may be
associated with an action. Output function, unlike external
and internal transition functions, is defined to consume no
physical-time. This does not strictly limit allocating time
for generating output events; instead it requires accounting
for time in external and internal transition functions (i.e.,
increasing twu of the location that is responsible for output
generation). It is useful to note that in real-time statecharts,
outputs are conceptualized as generating actions which
can be constrained to complete within designated time
periods.
Output Function:
l(‘) : A!Yb
Example:
l(Route Simple) = (indexoutput port )
where indexoutput port 2 N

In the simple switch example, the output for routing
action is a number which specifies the output port from
which the flit is to be sent out.
The ALRT-DEVS modeling approach allows dividing
generic external and internal transition functions defined
in parallel DEVS into individual actions and producing
output for each of them individually. This is depicted in
Figure 3. A delay model is defined to handle external
events without internal transition function. The operations
of the model are: receive an external event; stores the data
(received event); fetch the data; and send an output. The
external event invokes two actions: Action1 and Action2.
For these two actions along with the idle phase, this model
requires three locations and at an instance of time it can be
in one of them. The right-hand side of Figure 3 shows a
sample execution of the model with external event In1 and
the two actions. The external event is encountered at time
instance 5 and then the model enters the location L1. In
this location, Action1 is executed and Out1 is produced at
time instance 10. Then the model enters L2, Action2 is executed and output Out2 is produced at time instance 17.
Both of these actions were executed in their respective
time intervals (Action1: [426] and Action2: [528]).
Here, we provide the ALRT-DEVS specification for the
delay model illustrated in Figure 3. As described earlier
and shown in equation (1), we consider phase and sigma

Sarjoughian and Gholami

877

Figure 3. A delay model with multiple actions associated with one external input event.

as primary state variables which together make a location.
Locations of the delay model are specified in equations (5)
to (8). The remaining equations (states, input/output/action
sets, external/internal transition functions, output/activity
mapping functions, and time intervals) directly correspond
to Figure 3.
location

zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
phase

dint, 1 ðLn , dataÞ =


(L2 , data)
½n equals 1
(L0 , data = null) ½n equals 2

ð13Þ

l(L1 , data) = ðoutport, ‘‘CONFIRM’’Þ

ð14Þ

l(L2 , data) = ðoutport, dataÞ

ð15Þ

c(L1 , data, index) = fAction1 g

ð16Þ

c(L2 , data, index) = fAction2 g

ð17Þ

ti(L1 , data) = ½4, 6

ð18Þ

ti(L2 , data) = ½5, 8

ð19Þ

data

sigma
zﬄﬄﬄ}|ﬄﬄﬄ{
zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
z}|{
S = fActive, Idleg 3 s 3 f0, 1g

ð1Þ

X = fðinport, f0, 1g Þg

ð2Þ

Y = fðoutport, f0, 1g Þg

ð3Þ

A = fAction1 , Action2 g

ð4Þ

L = fL0 , L1 , L2 g

ð5Þ

L0 = f(‘‘Idle’’, ‘), fgg

ð6Þ

L1 = f(‘‘Active’’, 6), fAction1 gg

ð7Þ

L2 = f(‘‘Active’’, 8), fAction2 gg

ð8Þ

G = fdext, 1 , dext, 2 g

ð9Þ

dext, 1 ðL0 , data, e, (inport, X )Þ = (L1 , data = X )

ð10Þ

dext, 2 ð(‘‘Active’’, s), data, e, (inport, X )Þ =
((‘‘Active’’, d  e), data)

ð11Þ

O = fdint, 1 g

ð12Þ

What is shown in this figure and in the specification (equations (1) to (19)) cannot be replicated in P-DEVS or in RTDEVS. In these modeling approaches, an external event
handling and state transition is started with one or more
event and then one (RT-DEVS) or more outputs (P-DEVS)
produced. By enabling the modeler to specify actions
inside external and internal transition functions, time-critical, reactive systems can be more accurately modeled and
simulated by imposing strict time constraints to individual
actions.

3.3 Coupled model
ALRT-DEVS atomic models can be composed hierarchically to create larger models. Such composed models are
at the coupled model abstraction hierarchy.10 Similar to
other timed and un-timed modeling formalisms, a coupled

878

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

model does not ascribe time to input/output communication. Instead, there is ordering from outputs and inputs. A
component’s output must occur prior to being used as an
input by another component. In parallel DEVS, there exist
three types of communications using input and output
ports via couplings. External input coupling allows a
coupled model’s input to be transmitted as input to its
components. External output coupling allows the components of a coupled model to transmit their outputs to the
coupled model. Internal coupling allows any component
of a coupled model to send its outputs to any other component of a coupled model as inputs. Couplings are instantaneous; they only specify causality (i.e., outputs that are
generated in one simulation cycle become inputs at the
beginning of the next simulation cycle). Couplings that
must have time can be specified as atomic models. The
passage of time in a coupled model can only be due to its
atomic components. It is straightforward to add a delay
from the time output departs a sender to the time it arrives
at a receiver. The coupling is replaced with an atomic
component with appropriate couplings added.

4. Abstract ALRT-DEVS simulator
Models specified in ALRT-DEVS can be simulated in
real-time provided time-windows are strictly enforced
using real-time clocks. This requires developing abstract
atomic simulator and coupled model coordinator protocols
that can ensure correct executions of atomic and coupled
models. The parallel DEVS protocols supporting logicaltime (and soft real-time) are adapted to execute actions
defined in the internal and external transition functions G
and O in real-time. Implementations of these abstract
atomic simulators and coupled coordinators are available
in the literature.15

4.1 Atomic simulator protocol
A real-time atomic simulator protocol is devised such that
an action is allowed to execute to completion as long as
there is no time violation (i.e., an action can be completed
within the time-window that is available in the simulation
platform). The protocol manages execution of a sequence
of actions.7 The simulation platform does not guarantee to
provide the same real-time duration for an action when it
is executed multiple times throughout one or more simulations. Furthermore, the simulator does not ensure the execution of action’s time-window takes the exact amount of
physical-time as its (actual or hypothetical) system does.
As in most atomic or parallel DEVS, every ALRT-DEVS
atomic model is assigned its own atomic simulator. The
abstract simulator protocol handles all the actions defined
in both the external and internal transition functions.
Actions to be executed, their ordering, and their timewindows are defined in G and O. The simulation protocol

has Tmin and Tmax variables which get their values from
real-time clock provided by, for example, Java Virtual
Machine. These variables represent the simulator’s realtime clock. After every simulation cycle, the values of
these variables are updated. An action belongs to a set of
actions that are defined either in the external or internal
transition function. Every action set can be interrupted
when one or more external events are received at an
instance of time. Therefore, an action can be preempted or
terminated, but not due to real-time constraints. The execution of the actions belonging to the G and O functions
cannot be interleaved.
In order to explain in detail how the atomic simulator
protocol is devised, we provide Algorithm 1 and the flowchart diagram shown in Figure 4. The algorithm’s Delta
Function procedure has two arguments called Message
Bag and absolute time t. A simulation cycle is a single
path starting and ending with the conditional statement of
‘‘Message bag is empty?’’ corresponding to the first node
in Figure 4 and the first statement in Algorithm 1 at line 2.
Next, we describe the details of the algorithm.
In Algorithm 1, when the conditional statement at line
2 is TRUE then an action-related event has occurred. The
conditional statement at line 3 checks for time violation of
the current action (Action). If time violation has not
occurred, the output of the Action is generated (at line 4).
In the event of a maximum time violation, at line 6, the
action is terminated and no output is generated. After this,
the protocol checks whether the previous action (Action)
was the final action in the current Action Set (checked at
line 7). If yes, an internal transition is necessary; therefore,
the internal transition function (dint) is called at line 8 and
the Action Set is updated based on a new location. Finally,
common to all internal events (whether a time violation,
internal transition, or action substitution) is setting the current Action (the first action in the Action Set), the Tmax and
Tmin values for this new Action (line 10).
If the conditional statement at line 2 returns FALSE
(the Message Bag is not empty), an external event has
occurred. External events are handled by the external transition function (dext) at line 12. The execution of dext may
have two outcomes: 1) change of the location and a new
Action Set or 2) continuing with the same location, Action,
and Action Set. This condition is checked at line 13. The
first case is handled at line 14 which is setting the new
Action and updating the Tmax and Tmin values. Lines 16-18
correspond to the second case. In this statement, the
elapsed time of the action is updated and its execution is
continued from where it was left off.
Figure 4 shows the algorithm. A description of the protocol with respect to the elements of the ALRT-DEVS
model is provided below. Also, in the end, we discuss the
time-inconsistencies caused by real-time execution of the
simulator and how it is handled in the atomic simulator
protocol.

Sarjoughian and Gholami

879

Figure 4. Abstract ALRT-DEVS atomic simulator protocol.

Initialization
 At initialization, the simulator’s Tmin and Tmax are
set. These default values are set to zero which
means there are no actions to be executed. If there
is at least one action, its lower and upper timewindow are used to set Tmin and Tmax.
External transition function
 All actions are completed without any interruption
as long as their time-window constraints are satisfied. The actions are executed according to the
order specified in G. Using the ‘‘Last Action?’’
conditional, O is invoked.
 Some actions are not completed. An action will be
executed to completion unless it cannot be completed within its allotted time. An action may miss
its upper limit deadline (i.e., dt . Tmax) which
results in its termination and scheduling of the next
action if there is one. For every successful

execution of an action, an output can be generated
and Tmin and Tmax are updated. Execution of an
action can also be terminated due to arrival of
external events; an active action can be preempted
with another action.
External transition function to internal transition
function
 After the last action in G is completed, the simulator
starts to execute actions that belong to O.
Internal transition function
 All actions are completed. This requires every
action to be executed to completion.
 Some actions are not executed due to missing their
deadlines or are terminated due to receiving external events.
 When an internal transition action is scheduled to
end at the same time that an external transition
action is to begin, then priority is given to the

880

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

Algorithm 1. ALRT-DEVS atomic simulator protocol.
1: procedure DELTAFUNCTION (Message Bag,t)
2:
if Message Bag is empty then
3:
if t < Tmax ^ t > Tmin then
4:
Call λAction
5:
else
6:
Terminate Action
7:
if ActionSet is Empty then
8:
Call δint
9:
Set ActionSet
10:
Set Action, Tmax, and Tmin
11:
else
12:
Call δext
13:
if Location is Changed then
14:
Set ActionSet, Action, Tmax, and Tmin
15:
else
16:
Elapsed delta – tL
17:
tL t
18:
return
19:
tL t
20:
tN tL + Dist(Tmax, Tmin)

former action. Generating output events has priority
(i.e., external events are lost). This scheme follows
the default scheme provided for the confluent transition function defined for parallel DEVS. It should
be noted that in ALRT-DEVS, outputs are generated per actions, not in the holistic manner allowed
for the external and internal transition functions of
parallel atomic DEVS models. It is possible to give
priority to accepting external events that are
received at the same time an internal transition
action is scheduled to end.
Remaining functions
 The time interval, state-to-action mapping, and output functions are timeless. The abstract simulator,
therefore, does not allocate consuming time for
their executions. Obviously, the simulation platform consumes physical-time even though the passage of time is not represented in the simulation.
The real-time required for executing a complete simulation
cycle is for an action either in the external or the internal
transition function. The time duration specified for every
action must be greater than zero. Thus, the physical-time it
takes for executing actions is at least as long as the realtime that the simulator is keeping track of. The real-time
atomic model does not include timing for extraneous operations such as reading data from or writing data to an
external file or console. Thus, from the physical-time perspective, some amount of time is needed for operations
due to all operations in the atomic model except the actions
in G and O and thus, there is discrepancy between the
amount of time it takes in the real-time simulation platform
to execute a model when measured against the physicaltime consumed in the CPU’s clock. Furthermore, the

timing defined for the atomic model does account for the
simulation protocol itself. Physical-time is consumed for
operations such as setting Tmin and Tmax. Thus, we need to
include Dtrest for these kinds of computations that must be
completed in running simulation experiments. As noted in
Section 3.1, the accuracy of time assigned to actions in an
atomic model is imperfect.28 This can cause undefined
concurrent events or cause actual concurrent event to be
lost for coupled models (see Section 5).
Execution time for all the operations in an atomic
model is subject to the physical-time that can be accommodated in the hardware, hosting some implementation of
the simulator protocol. The total time for an atomic simulation cycle Dta . 0 is equal to Dtext + Dtint + Dtrest.
Assuming Dtext + Dtint  Dtrest, the real-time simulation
of ALRT-DEVS atomic model can be accepted as executing in physical-time. The impact of timing and actions as
defined in ALRT-DEVS is demonstrated with some
experiments (see Section 5). The experiments show the
degree to which real-time simulation can be achieved.29

4.2 Coupled coordinator protocol
The protocol for ALRT-DEVS coupled model is the same
as parallel DEVS. In a coupled model, message routing
from one model to another occurs instantaneously.
Couplings in coupled models are timeless. They can be
modeled as atomic models but inherently the coupled
model specification requires input and output exchanges
to be ordered, but untimed. We note that the coordinator
protocol itself consumes physical-time even though it does
consume real-time with respect to the simulation platform.
As alluded earlier, execution time of the coordinator protocol is subject to uncontrollable factors as viewed in physical-time. DtI/O . 0 is defined to represent the time
consumed for all message handling and transmission
across external input, internal, and external output couplings. For a flat coupled model with one atomic model,
the physical-time for its execution is defined as
Dtfc = Dta + DtI/O, where Dta is the physical-time allocated for execution of atomic models and DtI/O for input
and external output messaging. As in the atomic model,
there exists extraneous operations that also consume physical-time, but not real-time while being executed in the
simulation platform. For a flat coupled model with n
atomic P
models, time for a flat coupled cycle is defined as
Dtfc = ni= 1 Dta, i + DtI=O . In this case, DtI/O accounts
for all three kinds of coupling. The physical-time execution for a hierarchical coupled cycle with n atomic models,
p flat coupled models,Pand q hierarchial
P coupled models
P is
defined as Dthc = ni= 1 Dta, i + pj= 1 Dtfc, j + qk = 1
Dthc, k + DtI=O . It is obvious that the physical-time spent
executing real-time atomic and coupled models exceeds
the
P consumed in the simulation platform. If
P real-time
Dta  DtI=O for every flat coupled model and for

Sarjoughian and Gholami
every hierarchial coupled model, then real-time execution
of ALRT-DEVS coupled models can be accepted as
executing in physical-time.

5. Example model
The network switch system7 shown in Figure 5 is developed based on the following scheme. The network-on-chip
(NoC) switch consists of input ports for receiving flits,
output ports for sending the flits on the link, and a crossbar
which connects the input ports to output ports. The switch
depicted in Figure 5 has two virtual channels and operates
using a 5-stage pipeline (routing, virtual channel (VC)
allocation, switch (SW) allocation, SW transfer, link transfer). The input ports possess a number of (equal to the
number of VCs) input queues and status arrays for the
newly received flits and perform routing and allocation
operations on them. Now, the complete model in Figure 5
contains a generator, a sink, two switches and a bus. The
couplings between switches and the Bus/Gen/Sink are untimed, as opposed to the bus which has four links, each of
which consumes time for transmitting flits. The switch is
for a mesh-network with four inputs and outputs, each of
which can be configured to have multiple virtual channels.
The remaining parts of the switch are crossbar, router, virtual channel allocator, and switch allocator.15,30 Select
actions for these components are time-bounded. The flow
control policy is on/off, in which the downstream switch
(Switch 2) sends an ‘‘off’’ signal to the upstream switch
(Switch 1) when its input queue is full so that Switch 1
stops sending it flits. The generator on the other hand, continuously injects new packets into the network without
considering the current state of the switches. Without delving into further details, it is useful to note that the model
is stochastic in terms of its actions (switches operate in
parallel and flits may arrive at the same time and in arbitrary order). Execution is also stochastic due to the unpredictably of threads in the host simulation platform. During
simulation some actions may be terminated if they cannot
be completed given their designated time-windows.

5.1 Discussion
The configuration parameters of the models are listed in
Table 1. The timing parameters are in real-time. Flits traversing from the generator to the switches and sink are
assumed to consume no time, but they can be assigned to
consume real-time. For this experiment, the injection rate
is increased from 5 to 2000 flits per second and a 10-second sample is taken from the network. Based on the configuration provided in Table 1, the modeled system will be
saturated quickly at high flit injection rates and consequently lose a high number of flits. The switch model is
also developed in Booksim31 and used to validate the one
described here.7

881
For the purpose of comparison, this experiment is performed in both real-time and logical-time settings. The
logical-time simulation illustrates the impact of the executing platform for real-time simulation. The models are executed using the DEVS-Suite 2.1.0 simulator15 on a 32-bit
Windows 7 machine with 2.93 GHz, Intel Core 2 Dual
processor and 2 GB of DDR2 memory from which only
1.4 GB is used because of the 32-bit JVM. Due to stochasticity in the model, each experiment is repeated five times
and the averages of all five experiments are shown in
Figures 6 and 7. Multiple experiments are simulated to
show the impact of variability in the simulation platform.
If the switch model were to be used as a blueprint for
implementing a time-critical system, as mentioned in
Section 1, worst case execution time analysis and design
of experiments among others are needed.
The reason we stopped at five runs for each experiment
was the convergence between the results. Repeating the
experiments beyond five data points did not add more to
our data analysis and showed little variations. Although
real-time analysis is mostly done via worst case analysis,
for our analysis it was useful to consider averages. These
experiments are mainly designed and carried out to show
the capabilities of ALRT-DEVS for real-time modeling
and simulation. These proof of concept experiments are
not purposed for model verification and simulation validation using worse case time of the model or worst case execution time.
Figure 6 shows the drop rate in the system based on the
generator’s injection rate. Packet loss ratio for the first few
injection rates is magnified to illustrate real-time and
logical-time simulation executions. Although the data
extracted from these simulations represent similar behaviors, a small difference can be observed. This difference
is due to the impact of the executing environment on the
simulation protocol. Each deadline miss leads to a flit loss;
flit losses (although few) cause the simulation results to
diverge from those obtained from the logical-time simulation. This impact may become significant if the executing
environment is incapable of guaranteeing actions to be
completed as required due to high model complexity and
granularity or inadequate computational resources. A realtime simulation platform can exhibit significant unpredictability when model requirements cannot be met. This is
not the case for logical-time simulation since time is not a
limiting factor, time can be stretched or shrunk without
any side effect on the simulation results.
Results from a real-time simulator can be categorized
to fall into three regions. First, the simulation execution
scenarios can be handled by the executing platform; simulation results are not impacted by limited resource availability. Second, the execution platform cannot meet some
ideal timing or complexity but the observed system
dynamics are within an acceptable error margin. Third,
use of the execution platform leads to unreliable results.

882

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

Figure 5. A switch network system.

These categories are also observed in non real-time simulation where observed results in the third category could
be due to models that are under specified or are subjected
to incorrect experimental settings.32
The above three regions are observed in the simulation
experiments shown in Figure 7. However, in Figure 6,
these regions are not visible since only flit loss ratio is
being observed and visualized. In Figure 7(a), average flit
latency for the first region is demonstrated (injection rate
between 5 to 40 packets per second). In this region the
results are ideal since no flit loss has occurred due to realtime execution and constraints in the computing platform.
The difference between various injection rates is reflected
in high, low, and average flit latency though they all
belong to the same region. The results extracted from the
third region (injection rates higher than 1000 packets per
second) are unreliable. This is evident in Figures 7(b) to (e).

In Figure 7(b), the number of deadline misses are
recorded for various injection rates. The number of
deadline misses experiences a significant increase at
high injection rates. One interesting phenomenon in this
diagram is the very small difference in deadline losses
between 1500 and 2000 (flit per second) injection rates.
The reason is that the underlying platform is so overwhelmed with the number of actions that some of them
do not even begin to execute and therefore never lose
their deadline. However, for the number of actions executed, the system shows the same behavior as shown in
Figure 7(c) for the flits injected/lost ratio and in Figure
7(d) for the actual number of injected flits and those of
them that are lost (overlapping lines). Figure 7(e) shows
the chaotic nature of the simulation in the third region.
An approximate number of 15,000 packets are expected
to be injected into the system for a period of 10 seconds

Sarjoughian and Gholami

883

Table 1. Network switch system configuration.
Timing specification (10−3 s)

Topology and physical specification
Variable

Value

Variable

Value

Virtual channels per link
Number of flits per packet
Buffer size
Number of input/output ports

1
1
8
4

Routing
Link traversal
Status traversal
SW traversal
VC allocation
SW allocation

[9–11]
[0.9–1.1]
[9–11]
[9–11]
[9–11]
[9–11]

Figure 6. Packet loss ratio based on injection rate.

with an injection rate of 1500 packets per second.
However, the number of injected packets demonstrate
significant variations (from 5000 to 15,000) across five simulation runs. This variability in the number of injected packets
coupled with the number of deadline misses demonstrates the
unreliability of the simulation. Finally, Figure 7(f) represents
the second region of the real-time simulation in which the
results are still reliable but show a small divergence from the
logical-time results. Here the difference between the total
number of packet losses of logical and real-time simulator is
depicted and fashioned in a stock diagram.
As discussed in previous sections, time can be categorized into physical-time, real-time, and logical-time. The
third region in simulation occurs when the real-time clock
in the simulation cannot stay synchronized with the physical-time. This may happen when executing the model
exceeds the limits of hardware and/or software resources.
Also, one of the other limiting factors in real-time simulation is handling the concurrency. P-DEVS is based on synchronized parallel simulator protocol. However, true
concurrency cannot be achieved in the implementation of
real-time simulators such as DEVS-Suite due to slicing of

time periods for multiple actions. Therefore, ALRT-DEVS
simulator cannot guarantee concurrency as in the parallel
DEVS logical-time simulation. This can be observed in
the second and third regions where real-time simulation
measure diverges from its logical-time counterpart.

6. Related work
Model specifications and execution algorithms have been
developed toward real-time modeling and simulation. We
do not attempt to compare our work with these; a few
from the point of view modeling are discussed in the literature.7,29 We also refrain from discussing real-time distributed modeling and simulation approaches, an ongoing
research area. Instead, we begin with some early works
that were aimed at adopting logical-time modeling for
real-time simulation through the 1990s. Starting in the late
1990s, there was a surge in developing DEVS-based tools
based on classic and parallel DEVS formalisms and their
extensions, such as fuzzy logic, probabilistic, and dynamic
structure. Furthermore, our focus is not on simulation tools
and applications, which are significant research areas in

884

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

Figure 7. Simulation results for the model shown in Figure 5: (a) a stock diagram manifesting high, low, and average flit latency for
the first region with zero packet loss (exponential growth; (b) significant increase in deadline losses for the third region; (c) ratio of
the lost flits versus the injected ones for the third region at 99% ratio); (d) actual number of flits injected and lost in the third region
(two overlapping lines); (e) variations in injection rate for the third region; and (f) depicting high, low, and average differences in the
total number of packet loss between logical and real-time simulations for the second region of the network (small variations).

their own right. We use the DEVS-Suite simulator to support developing ALRT-DEVS models, simulate the example switch network model, and evaluate the simulation
results to show the extent in which action-level real-time
modeling and real-time simulation are supported.
Classical DEVS modeling implemented in the DEVSScheme simulator was developed to execute under the host
computer clock.33 The atomic model does not allow specifying actions. The simulator, which was developed in the
Scheme language, uses an interface channel written in C
programming language for handling analog signals converted to/from discrete data. Time instances for incoming
and outgoing events in atomic and coupled models are
coordinated using the host computer clock. In Section 2 we

described parallel DEVS and RT-DEVS approaches. We
noted that the parallel DEVS modeling formalism does not
support representing actions. For example, although models developed for DEVS-on-a-chip simulator12 built on the
DEVSJAVA simulator34 uses real-time Java, soft real-time
execution is supported. RT-DEVS is based on classic
DEVS, which precludes assigning sets of contiguous
actions separately to external and internal transition functions. In particular, RT-DEVS requires use of the Select
function for coupled models and does not support receiving multiple input and sending multiple output events at
the same time instance. To handle timing for coupled models, a clock matrix is developed.21 RT-DEVS supports
non-blocking and synchronization loss. The clock matrix is

Sarjoughian and Gholami
introduced as an analytic capability to predict, as precisely
as possible, the upper and lower bounds for the next state
transition times. The differences between RT-DEVS and
ALRT-DEVS resulted in developing an abstract simulator
protocol for ALRT-DEVS model.
Another approach to real-time classic DEVS modeling
is proposed in eCD++.35 This work expands on the
DEVS-on-a-chip by flattening hierarchical models with
soft real-time execution. To handle limited resource availability in a given computing platform, interface DEVS is
defined.13 It introduces a function for setting the execution
of a state to be either optional or mandatory. Therefore, a
state transition (corresponding to a simulation cycle) may
not be executed depending on the simulator having enough
time or not. The abstract simulator protocol is mapped to a
real-time task list for executing atomic external and internal transition functions. A coupled model prioritizes which
atomic models must be executed and which ones may be
executed depending on the modes assigned to the state
transitions. This approach delegates the choice of which
state transitions are to be executed to the coupled model.
Fewer resources are needed as the ratio of the required to
the optional state transitions decreases. Thus, the main
objective is reducing the number of state transitions to
achieve hard real-time simulation. Real-time execution is
achieved by embedding the simulator in network processors with time resolution in nanosecond range executing
on RT Linux.36 Unlike this eCD++, in ALRT-DEVS,
actions can be defined with real-time time-windows with
its abstract simulator enforcing hard real-time execution.
Another DEVS approach to support real-time simulation is
PowerDEVS.37 It is implemented in C++ and uses a
real-time clock provided by Linux RTAI. Generation and
communication of the output events are synchronized
using the aforementioned real-time clock. As in RT-DEVS
and eCD++, this approach focuses on using simulator
clock with interrupt handling whereas ALRT-DEVS
focuses on defining the external and internal transition
functions with actions each of which is bounded by a realtime clock.
Real-time modeling and simulation has also been the
subject of study in the Ptolemy II project.38 As in some
DEVS-based tools, Ptolemy II supports soft real-time
simulation.29 For hard real-time execution, a Giotto model
of computation has been developed in Ptolemy.39 The
Giotto programming model40 with periodic timing is
developed where tasks defined in a Giotto Director coordinate the execution of the actors based on their assigned
frequencies or default values. Ptolemy’s code generation
capability is used to generate C code. The FreeRTOS,41
which is portable to common target microcontroller platforms, executes the generated code. The program code
executes correctly provided there exists a scheduling policy that can meet the requested deadlines given the
resources and time constraints of the target platform. As in

885
ALRT-DEVS, it may not be feasible to identify worst-case
execution time. In such cases, the execution of tasks are
not terminated; instead, the execution continues generating
outputs with delays. In ALRT-DEVS, the actions that cannot be completed within the available resources are terminated or ignored. Another difference between these
approaches is that the Giotto model requires time-periodic
cycles which determine the scheduling policy that can be
supported in FreeRTOS. In ALRT-DEVS, timings for
actions are event-based and subject to the clock resolution
of the computing platform. The allocation of time for the
actions is determined by the number of threads (i.e., the
number of atomic models) that are not scheduled to execute in any particular order even though the sequence of
actions (based on the remaining time) is defined in the
external and internal transition functions.

7. Conclusions
We have proposed an approach for developing DEVS
models where actions, in addition to state changes, can be
specified. Timings in the form of time-windows are
assigned to actions. These actions are associated with individual state changes using location and time-invariant concepts developed for real-time statecharts. ALRT-DEVS
offers a structured approach for decomposing external and
internal transition functions to sets of real-time time-constrained and schedulable actions. To simulate these models, a real-time abstract simulator protocol is proposed
based on common virtual multi-threading computing
platform and implemented as part of the DEVS-Suite
simulator. The simulation protocol can enforce certain
hard real-time execution to the degree supported by the
computing platform and implementation of the models
and the simulator protocol. The action-level real-time
modeling is independent of specific hardware and software
technologies. Thus, ALRT-DEVS can lend itself to the
development of hard real-time computing platforms using
real-time operating systems to achieve guaranteed clock
accuracy and precision. This is because specification of
actions with constrained real-time can be mapped one-toone to hard real-time simulation cycles. It is necessary to
develop analytical techniques to determine (near-)optimal
hard real-time timings for actions given worst-case execution times available. ALRT-DEVS approach stands to benefit from code generation targeted for network processors
with high precision clocks, specialized hardware, and realtime operating systems. Developing scheduling policies
for computing platforms is another important area of
research where simulating safety critical systems can be
performed as close as possible to physical-time. In terms
of use, action-level, real-time modeling has the potential
to be used in building simulations that can interact seamlessly with physical systems.

886

Simulation: Transactions of the Society for Modeling and Simulation International 91(10)

Funding
This research received no specific grant from any funding agency
in the public, commercial, or not-for-profit sectors.

References
1. Eidson JC, Lee EA, Matic S, et al. Distributed real-time software for cyber–physical systems. Proc IEEE 2012; 100: 45–59.
2. Lamport L. Time, clocks, and the ordering of events in a distributed system. Commun ACM 1978; 21(7): 558–565.
3. Lee EA. Computing needs time. Commun ACM 2009; 52(5):
70–79.
4. Sastry S, Sztipanovits J, Bajcsy R, et al. Scanning the issue:
Special issue on modeling and design of embedded software.
Proc IEEE 2003; 91: 3–10.
5. Cervin A and Eker J. Control-scheduling codesign of realtime systems: The control server approach. J Embedded
Comp 2005; 1: 209–224.
6. Cunning SJ, II Shultz S and Rozenblit JW. An embedded
system’s design verification using object-oriented simulation. Simulation 1999; 72: 238–249.
7. Gholami S and Sarjoughian HS. Real-time network-on-chip
simulation modeling. In: SIMUTools. Desenzano: ICST,
2012, pp. 103–112.
8. Wang Q and Cellier FE. Time windows: An approach to
automated abstraction of continuous-time models into
discrete-event models. Int J Gen Syst 1991; 19: 241–262.
9. Hong J, Song H, Kim TG, et al. A real-time discrete event
system specification formalism for seamless real-time software development. Discrete Event Dyn Syst 1997; 7: 355–
375.
10. Zeigler BP, Kim TG and Praehofer HS. Theory of modeling
and simulation. 2nd ed. Orlando, FL: Academic Press, 2000.
11. Harel D and Politi M. Modeling reactive systems with statecharts: The STATEMATE approach. New York: McGrawHill, 1998.
12. Hu X, Zeigler BP and Couretas J. DEVS-on-a-chip:
Implementing DEVS in real-time Java on a tiny internet
interface for scalable factory automation. In: IEEE international conference on systems, man, and cybernetics, volume
5. IEEE, 2001, pp. 3051–3056.
13. Moallemi M and Wainer GA. Designing an interface for
real-time and embedded DEVS. In: Proceedings of the 2010
Spring Simulation Multiconference, New York: ACM Press,
2010, pp. 137:1–137:8.
14. Ptolemaeus C. System design, modeling, and simulation:
Using Ptolemy II. ptolemy.org, http://ptolemy.org/systems,
2014.
15. ACIMS, DEVS-Suite Simulator, http://devs-suitesim.sourceforge.net/ (2015).
16. Burmester S, Giese H and Schäfer W. Code generation for
hard real-time systems from real-time statecharts. Technical
Report TR-RI-03-244, University of Paderborn, Germany,
2003.
17. Henzinger TA and Kirsch CM. The embedded machine:
Predictable, portable real-time code. In: Proceedings of ACM
SIGPLAN Conference on Programming Language Design
and Implementation (PLDI). New York: ACM Press, 2002,
pp. 315–326.

18. MathWorks. Simulink coder, http://www.mathworks.com/
products/rtw/ (2012).
19. Chow A and Zeigler BP. Parallel DEVS: A parallel, hierarchical, modular, modeling formalism. In: Proceedings of
the 26th conference on Winter simulation. Society for
Computer Simulation International, 1994, pp. 716–722.
20. Kim S, Sarjoughian HS and Elamvazhuthi V. DEVS-suite:
A simulator supporting visual experimentation design and
behavior monitoring. In: Proceedings of the 2009 Spring
Simulation Conference. Society for Computer Simulation
International, 2009, pp. 29–36.
21. Song HS and Kim TG. Application of real-time DEVS to
analysis of safety-critical embedded control systems:
Railroad crossing control example. Simulation 2005; 81:
119–136.
22. Giese H and Burmester S. Real-time statechart semantics.
Technical Report TR-RI-03-239, University of Paderborn,
Germany, 2003.
23. Alur R and Dill D. A theory of timed automata. Theor
Comput Sci 1994; 126: 183–235.
24. Abadi M and Lamport L. An old-fashioned recipe for real
time. ACM Trans Program Lang Syst 1994; 16: 1543–1571.
25. Lee I, Wolfe V and Davidson S. Motivating time as a first
class entity. Technical Report MS-CIS-87-54, University of
Pennsylvania, 1987.
26. Fujimoto R. Parallel and distributed simulation systems.
New York: John Wiley & Sons, 2000.
27. McLean T and Fujimoto R. Repeatability in real-time distributed simulation executions. In: Proceedings of the fourteenth workshop on Parallel and distributed simulation. Los
Alamitos, CA: IEEE Computer Society Press, pp. 23–32.
28. Vicino D, Dalle O and Wainer GA. A data type for discretized time representation in devs. In: Proceedings of the 7th
International ICST Conference on Simulation Tools and
Techniques (SIMUTools’14), Brussels, Belgium. Belgium:
ICST.
29. Gholami S and Sarjoughian HS. Observations on real-time
simulation design and experimentation. In: Symposium
on Theory of Modeling and Simulation, San Diego, CA,
pp. 1–8. New York: ACM Press.
30. Gholami S and Sarjoughian HS. Network-on-Chip (NoC)
simulation with ALRT-DEVS: Supplementary material and
experiments, 2013. URL http://acims.asu.edu/wp-content/
uploads/2014/02/ASUCIDSE-CSE-2013-001.pdf.
31. Dally W and Towles B. Principles and practices of interconnection networks. San Mateo, CA: Morgan Kaufmann, 2004.
32. Sarjoughian HS, Hild DR, Hu X, et al. Simulation-based
SW/HW architectural design configurations for distributed
mission training systems. Simulation 2001; 77: 23–38.
33. Zeigler BP and Kim J. Extending the DEVS-scheme
knowledge-based simulation environment for real-time
event-based control. IEEE Trans Robotic Autom 1993; 9(3):
351–356.
34. Zeigler BP, Sarjoughian HS and Au V. Object-oriented
DEVS. In: AeroSense’97. International Society for Optics
and Photonics, 1997, pp. 100–111.
35. Yu YH and Wainer GA. eCD++ : An engine for executing
DEVS models in embedded platforms. In: Proceedings of the
2007 Summer Computer Simulation Conference, SCSC 2007,

Sarjoughian and Gholami

36.

37.

38.
39.

40.

41.

San Diego, CA, 16–19 July 2007, pp. 323–330. San Diego,
CA: Simulation Councils.
Castro R, Ramello I and Wainer GA. Modeling and simulation based design of real-time controllers embedded on network processors. In: Proceedings of Spring Simulation
Conference (SpringSim12), pp. 1–9. Society for Modeling
and Simulation International (SCS), ACM.
Bergero F and Kofman E. PowerDEVS: A tool for hybrid
system modeling and real-time simulation. Simulation 2011;
87: 113–132.
Lee EA, et al. The Ptolemy project, http://ptolemy.eecs.berkeley.edu/ (2013)
Forbes S. Real-time C Code Generation in Ptolemy II for the
Giotto Model of Computation, http://books.google.com/
books?id=2KxMtwAACAAJ (2009).
Henzinger TA, Horowitz B and Kirsch CM. Giotto: A timetriggered language for embedded programming. In:
Embedded Software. New York: Springer, pp. 166–184.
Openrtos. The FreeRTOS project, http://www.freertos.org
(2013).

887
Author biographies
Hessam S Sarjoughian received a PhD in electrical and
computer engineering from the University of Arizona,
Tucson, in 1995. He is an associate professor of computer
science and engineering at Arizona State University in
Tempe, Arizona. Sarjoughian is co-director of the Arizona
Center for Integrative Modeling and Simulation (ACIMS).
His research focuses on model composability, visual modeling, service-oriented simulation, and distributed codesign modeling. He led the development of the Online
Masters of Engineering in Modeling and Simulation in the
Fulton School of Engineering at ASU in 2004. He was
among the pioneers who established the Modeling and
Simulation Professional Certification Commission in
2001. His research has been supported by NSF, Boeing,
DISA, Intel, Lockheed Martin, Northrop Grumman, and
the US Air Force.
Soroosh Gholami received his BSc in 2010 and his
MSc in 2012, both from the University of Tehran, Iran.
He is currently a PhD student at Arizona State University.
As a member of ACIMS, he has been involved in various
modeling and simulation research projects on hardware
chips, supply chain, and environmental phenomenon supported by Intel and NSF.

78

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

DEVS-DOC: A Modeling and Simulation
Environment Enabling Distributed Codesign
Daryl R. Hild, Member, IEEE, Hessam S. Sarjoughian, Member, IEEE, and Bernard P. Zeigler, Fellow, IEEE

Abstract—An approach to modeling and simulating distributed
object computing (DOC) systems as a set of software components
mapped onto a set of networked processing nodes is presented.
The modeling approach has clearly separated hardware and
software components enabling systems level, distributed codesign
engineering. The distributed codesign engineering refers to a
system-theoretic approach to concurrent hardware and software
systems engineering that provides a tractable method for analyzing
the inherent complexities that arise in distributed computing
systems. A software abstraction forms a distributed cooperative
object (DCO) model to represent interacting software objects.
A hardware abstraction forms a loosely coupled network (LCN)
model of processing nodes, network gates and interconnecting
communication links. The distribution of DCO software across
LCN processors forms an object system mapping (OSM). This
OSM provides a sufficient specification to allow simulation
investigations. In simulation, the behavioral dynamics of the
interacting DCO software components load and compete for
LCN processing and networking resources. The LCN resource
constraints thus impose performance constraints on the interactions
of the DCO software objects. Class models of the DCO, LCN, and
OSM component structures and behavior dynamics were formally
characterized using the discrete event system specification (DEVS)
formalism. These class model specifications were implemented
in DEVSJAVA, a Java implementation of DEVS. Class models of
experimental frame components were developed and implemented
to facilitate analysis of the interdependent distributed system
behaviors during simulations. Our DEVS-DOC M&S environment
enables distributed systems architects, integration engineers and
system designers to analyze performance and examine engineering
trades of system structures, topologies and technologies. A case
study demonstrates the ability to model and simulate a real world
system and the complex interactions that arise in distributed
computing systems.
Index Terms—Codesign, collaboration, discrete event system
specification (DEVS), distributed object computing (DOC),
modeling and simulation (M&S).

Manuscript received October 18, 1999; revised December 13, 2001. This
work was supported in part by the Advance Simulation Technology Thrust
(ASTT) under DARPA Contract N6133997K-0007 and in part by National
Science Foundation Grant “DEVS as a Formal Modeling Framework for
Scaleable Enterprise Design.” The views expressed in this article are the
authors’ and are not intended to represent in any way The MITRE Corporation
or its opinions on this subject matter. This paper was recommended by
Associate Editor J. Rozenblit.
D. R. Hild is with the MITRE Corporation, Colorado Springs, CO 80910 USA
(e-mail: d.hild@ ieee.org).
H. S. Sarjoughian was with the University of Arizona, Tucson, AZ 85721
USA. He is now with the Computer Science and Engineering Department,
Arizona State University, Tempe, AZ 85287 USA (e-mail: Hessam.Sarjoughian@asu.edu).
B. P. Zeigler is with the Electrical and Computer Engineering Department,
University of Arizona, Tucson, AZ 85721 USA (e-mail: zeigler@ece.arizona.edu; http://www.acims.arizona.edu).
Publisher Item Identifier S 1083-4427(02)02695-4.

I. INTRODUCTION

D

RAMATIC increases in both networking speeds and processing power has shifted the computing paradigm from
centralized to distributed processing. The economics of an increasing performance-to-cost ratio is continuing to drive this
shift with more and more emphasis on building networked processing capabilities. Concurrently, a shift is occurring in the
software industry with a move toward object oriented technologies. This object orientation trend is creating more and more
interacting software components. Distributed object computing
represents the convergence of these two trends.
This convergence results in a highly complex interaction of
hardware and software components that form distributed object
computing systems. Two sets of challenges associated with developing distributed computing systems have been identified as
inherent and accidental complexity [1]. The inherent complexities are a result of building systems that are distributed. The
developer of a distributed system needs to resolve issues such
as how to detect and recover from network and host failures;
how to minimize communications latency and its impacts; and,
determine the partitioning of software service components to
optimally balance computational loads on hosts and messaging
loads on networks. The accidental complexities are a result of
the tools and techniques used to construct distributed systems.
Examples of accidental complexity are the use of functional
design methodologies, which result in nonextensible and nonreusable software components; use of platform specific coding
tools that limit portability and interoperability; and ineffective
application of concurrency and multithreading techniques in
constructing software components.
To attain a level of tractability in developing distributed
object computing systems, we advocate the use of modeling
and simulation tools to explore design alternatives. We take
a codesign and synthesis approach in considering software
and hardware implications in conjunction with network design
issues. In modeling a distributed system, our constructs deal
with partitioning functionality into software objects, defining
software object interactions, distributing software objects
across processing nodes, selecting processing components
and networking topologies and structuring communication
protocols supporting the software object interactions. The work
described in this article clearly does not address all the necessary issues required for the building of complex distributed
systems. It is primarily focused on architectural analysis and
design of software components executing on geographically
dispersed multiple processors communicating with one another
using hardware components such as Ethernet and switches.

1083–4427/02$17.00 © 2002 IEEE

HILD et al.: DEVS-DOC

79

Our conceptual approach has been inspired with the Distributed Object Computing (DOC) modeling framework
proposed by Butler [2]. It is defined as two layers—distributed
cooperative objects (DCO) and loosely coupled networks
(LCN)—and a mapping [object system mapping (OSM)]
between the two layers. We’ve characterized and implemented
key DOC framework objects in DEVSJAVA [3] and have
extended them to enable both quantum simulation—modeling
bulk behavior with random variables—and directed simulation—modeling specific underlying system technologies and
behaviors.
II. MOTIVATION
The motivation of this research is to support the design and
analysis of distributed computing systems not only from a software point of view, but also from the hardware point of view.
These systems may be targeted at supporting a variety of business endeavors to include process control and manufacturing,
transportation management, military command and control, finance, banking and medical records and imaging. Often, conflicting technical requirements arise in analyzing the user requirements and constraints, e.g., military command and control
systems needing to exchange large data loads over bandwidth
limited networks; or, a process control system needing guaranteed job throughputs on a remote multitasked processor. The
complex interactions of distributed object computing systems
make static analysis of system behavior intractable.
The behavioral complexity associated with distributed object
computing systems arises from both the dynamics of individual
components and the structural relationships between components. Design decisions affecting the dynamics of individual
components and in coupling and structuring these components
often have significant impacts on the overall behavior and
performance of the system under development or modification.
For example, processor speed and memory selections impact
job throughputs. Buffer size and network bandwidth drive
the queuing and dropping of traffic. Choices in networking
technologies constrain communication protocol options and
impact channel error rates. Communication failure and recovery schemes and mechanisms control network behavior
under stress and load. Distribution of software objects across
processing nodes affects processing workloads and network
traffic loads. The level of multithreading implemented within
software objects determines behavior in handling multiple,
simultaneous invocation requests. Exploring these design
decisions and alternatives is the focus of our interest.
We intend to enable the exploration of these design decisions
through a modeling and simulation framework. This framework
will enable system designers to model the software objects, the
hardware components and the distribution of these software objects across the hardware components.
A. Java—A Technology for Coding Enterprise Computing
Solutions
Java has generated a lot of excitement in the programming
community with its promise of portable applications and

applets. In fact, Java provides three distinct types of portability: source code portability, CPU architecture portability,
and OS/GUI portability [4]. These three types of portability
result from the packaging of several technologies—the Java
programming language, the Java virtual machine (JVM) and
the class libraries associated with the language. The Java
programming language provides the most familiar form of
portability—source code portability. A given Java program
should produce identical results regardless of the underlying
CPU, operating system, or Java compiler. The JVM provides
CPU architecture portability. Java compilers produce object
code (J-code) for the JVM and the JVM on a given CPU
interprets the J-code for that CPU. For OS/GUI portability,
Java provides a set of library functions (awt, util, and lang) for
an imaginary OS and GUI.
The Java programming language embraces the object-oriented paradigm and provides a means for developing distributed
software objects and applications. DEVS-DOC is providing
a means to model the dynamics of such distributed objects
and simulate resulting models to support design decisions on
structuring and distributing the software objects being coded.
B. Unified Modeling Language—A Language for Modeling
Software Objects
The software systems being developed today are much more
complex than the human mind can generally comprehend. To
simplify this complexity, software developers often model target
systems. Typically, no one model is sufficient; so, several small,
nearly independent models are developed. The unified modeling language (UML) [5] is a graphics based language for specifying, visualizing, constructing, and documenting such software models. The development of UML has incorporated ideas
from numerous methodologies, concepts and constructs. The
common syntactic notation, semantic models and diagrams of
UML facilitate comprehension.
UML collaboration diagrams and sequence diagrams provide
a static illustration of behavior. UML deployment diagrams
provide a mapping of software to hardware configurations.
UML component diagrams illustrate software structures.
These UML diagrams provide a graphical complement to the
DEVS-DOC specification of the DCO and OSM abstractions.
Extending these diagrams with details on object size, message
size, and object method computational workload estimates can
provide a complete set of parameters needed for a DEVS-DOC
model and simulation.
C. Rational Unified Process—A Process for Constructing
Models of a Software System
The UML is a generic modeling language that can be used to
produce blueprints for a software system. As a language, however, UML does not prescribe any process or method that results
in such blueprints. To complement the UML, Rational Software has developed the Rational Unified Process (RUP) [6] as a
generic set of steps to develop such software blueprints using the
UML. The idea behind RUP is to define who does what, when
and how during the development of a software system.
The RUP has steps for an architect to develop an architectural
view of the target software system, early on, during the analysis

80

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

and design workflow. In particular, the architect develops the
software architectural design, flushes out software concurrency
issues and determines software object distribution. The details
of these steps complement a DEVS-DOC development of the
DCO and OSM abstractions.
D. Codesign—Engineering Processes to Simultaneously
Consider Hardware and Software Constraints
Existing codesign efforts are focused on enabling better communications between hardware and software developers in the
design of embedded systems, such as portable devices (refer
to Section VII for a brief discussion). Traditional codesign assumes single or multiple software components (possibly multithreaded) being executed on a single machine having one or
more processors. Distributed codesign, however, can be viewed
from a higher level of abstraction—i.e., a network of distributed
machines servicing parallel and/or distributed software components. Our intent is to expand such capabilities into the arena of
distributed codesign of networked systems.
To model and simulate dynamics of distributed networked
software components executing on hardware components, we
define Distributed Codesign as activities to concurrently design hardware and software layers of a distributed networked
system. It is important to realize that our use of the term “distributed” is distinct from its common use in the embedded systems codesign literature (e.g., [7]) where emphasis is, for example, at low-level specification of FPGA. That is, our use of
the term refers to software objects—spread across a network
of computers—communicating with another using communication links and routers/switches. In comparison, the elements
of distributed codesign for embedded systems lies at a much
smaller level of specification (e.g., modeling instructions used
in application-specific integrated circuits). Characterization of
distributed networked systems, consequently, entails modeling
and simulation for high-level specification 1 of hardware and
software layers as well as distinct mappings from the former to
the latter.
In the remainder of this paper, we begin by describing the
DEVS-DOC framework (Section III). In Section IV, we provide an account of a distributed object computing (DOC) model,
as formulated by Butler [2], while providing some discussions
on issues that are important in how DOC can be represented
in a DEVS modeling and simulation framework. In Section V,
we present some additional concepts beyond those discussed in
Section IV, DEVS-DOC modeling constructs and realization of
the DEVS-DOC framework in the DEVSJAVA modeling and
simulation environment. In Section VI, we discuss a case example followed by related work, future research directions and
conclusions in the remaining three sections.
III. DEVS-DOC MODELING AND SIMULATION FRAMEWORK
Engineered systems continue to offer more and more
advanced functionality, improved ease of use and enhanced
1Such high-level specifications, for example, do not include modeling of operating system level mechanisms or instructions. In contrast, low-levels specification for embedded systems includes such instructions in application-specific
integrated circuits.

computational capabilities generally at the expense of substantial increases in size and complexity. A framework provides a
conceptual structural basis to conceive, design, analyze, and
eventually implement systems under consideration. Another
important and essential benefit of a framework is its accompanying methodology, where the methodology can assist end
users to achieve their objectives. Next, we briefly describe the
DEVS-DOC conceptual framework.
The DEVS-DOC modeling and simulation framework is
founded on 1) the theoretical foundation of the discrete event
system specification (DEVS) formalism, 2) object-orientation
concepts, and 3) distributed object computing principles [8],
[9]. The DEVS formalism is based in system-theory and, using
a set-theoretic notation, enables the expression of hierarchical,
modular discrete event models, and execution of such models
within a simulator. Aside from discrete-event models, it has
recently been shown that discrete-time and continuous systems
can also be represented as discrete event approximations [10].
DEVS is founded on well-defined concepts of system modularity and component coupling to easily synthesize hierarchical
composite models while maintaining a separation of modeling
and simulation concerns. Furthermore, DEVS implementations
support distributed simulation execution, model repository
and reuse mechanisms, and hierarchical testing of model
components and structures [11]. As detailed in the following,
DEVS-DOC models are represented as discrete-event models
since their behavior of interest is piecewise constant over
variable periods of time. However, the DEVS-DOC framework
is still desirable even if specific component models are more
readily represented using a discrete-time or continuous system
notation.
Aside from the DEVS capability to formally characterize
many types of systems [12], DEVS complements the use
of object-oriented concepts—encapsulation, abstraction and
inheritance—to realize the DEVS-DOC environment. And,
leveraging distributed, network enabled object computing
technologies (e.g., Java and Object Request Brokers) sets
the stage for distributed object computing realizations of the
DEVS-DOC modeling and simulation (M&S) environment.
Consequently, the proposed DEVS-DOC framework offers a
methodology for model development while providing the basis
to use powerful software technologies to develop an extensible
and robust M&S environment.
A. Architecture
In this section, we discuss DEVS-DOC with emphasis on
how it fits into a distributed DEVS modeling and simulation
environment [13]. Fig. 1 depicts a component-based architecture comprised of two layers: Modeling and Simulation. The
simulation layer provides distributed and centralized simulation capabilities (e.g., DEVS/HLA 2 and DEVSJAVA). For a
distributed capability, DEVS/HLA uses the HLA/run time infrastructure (RTI) and persistent object store. The core of the
modeling layer provides domain neutral modeling capabilities
2High level architecture (HLA), an IEEE Standard, is a middleware entity
supporting simulation reuse and interoperability [14]. The run time infrastructure (RTI), a realization of HLA, provides a set of common services (e.g., time
management) for model composition and distributed simulation execution.

HILD et al.: DEVS-DOC

Fig. 1.

81

Distributed M&S architecture.

with DEVS as the basic modeling formalism enabling development of hierarchical, heterogeneous models. The distributed object computing and DEVS modeling (DEVS-DOC) component,
based on DEVS modeling constructs, enables construction of
software objects, hardware components, and their associations
(as detailed in the following section). The well-defined modeling constructs offered by the DEVS-DOC component provide
the basis for domain-specific model development. The auxiliary
component allows adding specialized capabilities (e.g., fuzzy
logic, genetic algorithms and neural networks). The architecture
depicted in Fig. 1 can facilitate extending DEVS-DOC capabilities for use in collaborative settings with distributed simulation;
see Section VIII-C
B. Dicrete Event System Specification
The DEVS modeling approach supports capturing a system’s
structure from both functional and physical points of view.
A DEVS model can be either an atomic model or a coupled
model [15]. Given atomic models, DEVS coupled models can
be formed in a straightforward manner. Atomic and coupled
models can be simulated using sequential computation and/or
various forms of parallelism [12]. A DEVS parallel atomic
model is defined as
ta
where
set of internal input events;
set of sequential states;
set of external output events;
external transition function specifying state transitions due to external events;
internal transition function specifying state transitions due to internal events;

confluent transition function specifying handling of
multiple input events and tie breaking rule between
and
;
output function generating external events as output;
ta
time advance function.
The sequential and parallel views play a central role in
modeling and simulation of coupled models since each coupled
model is essentially comprised of multiple atomic models. Two
different formalisms have been introduced. The sequential formalism [15] treats components’ simultaneous transitions (
and
) sequentially, while a more recent formulation [16]
treats them concurrently. Parallel DEVS supports 1) processing
of multiple input events and 2) local control on the ordering of
simultaneous internal and external transition functions.
A DEVS coupled model is defined as

where
set of component names;
set of basic components for each in ;
set of influences for each in ;
to output translation for each in and each in
.
While a part of a system can be represented as an atomic
model with well-defined interfaces, a coupled model represents
systems composed from subsystems in a hierarchical fashion.
Given (atomic or coupled) components of a coupled model, the
couplings among them can be systematically captured using
). Using three different types
output to input mappings (i.e.,
of coupling (internal coupling, external input coupling, and
external output coupling), we can ensure semantically identical
input/output interfaces for atomic and coupled models. Internal
coupling interconnects components of a coupled model. External input coupling interconnects input ports of a coupled

82

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

model to input ports of its components. Similarly, external
output coupling interconnects component output ports of a
coupled model to the output ports of the coupled model itself.
With coupled models, increasingly more complex models can
be constructed using simpler models in a well-defined setting
supporting stepwise model development, modular model
verification and validation and distributed execution.
C. DEVJAVA
Using the Java programming language, the basic DEVS constructs have been implemented in an environment called DEVSJAVA [3], [17]. This environment provides the foundation
upon which higher constructs of DEVS (e.g., endomorphism
and variable structure) can be created using the basic, as well as
the Internet-based features, of the Java environment. The DEVS
atomic and coupled models can be developed and simulated
both as standalone applications and applets. As DEVSJAVA is
a multithreaded implementation, each simulation model is assigned a unique thread. This allows not only simultaneous execution of several models, but also execution of internal and
external transition functions within each atomic model. These
multithreading features are critical to the handling of multiple
events concurrently within the component models of distributed
object computing systems.
In developing the DEVS-DOC environment, we use a DEVSJAVA implementation of the DEVS formalism along with
the experimental frame concept described below. Implementation of DEVS-DOC on top of DEVSJAVA enables object-oriented modeling, concurrent simulations, concurrency among interacting simulation objects and web-enabled simulations.
D. Experimental Frame
An experimental frame (EF) is an artifact of a modeling and
simulation enterprise [15]. It is a specification of the conditions
under which a system is observed and experimented. The experimental frame is the operational formulation of the objectives
motivating a modeling and simulation project. A typical experimental frame consists of a generator, an acceptor, and a transducer. The generator stimulates the system under investigation
in a known, desired fashion. The acceptor monitors an experiment to see that desired conditions are met. The transducer observes and analyzes the system outputs. The experimental frame
concept provides a structure to specifying the simulation conditions to be observed for analysis. DEVS-DOC specific experimental frame components are discussed later in the paper.
IV. DISTRIBUTED OBJECT COMPUTING
Using set theory, Butler introduced an abstract mathematical
basis for specifying a static, structural model of a distributed
object computing system [2]. However, while Butler’s formulation identified key attributes associated with distributed objects,
it did not provide a dynamic (time-driven) characterization and
realization of the objects in any modeling and simulation environment. We discuss, in Section V, how dynamic aspects of
distributed objects may be characterized and simulated in the
DEVS-DOC environment.

To bring a level of tractability into the analysis of a distributed object computing system, we decompose the system
modeling effort into three steps: defining the processing nodes
and interconnecting network; defining the software objects
and their interactions; and mapping the software objects onto
processing nodes. Following the terminology and conventions
of Butler, these three steps result in the definition of a loosely
coupled network (LCN), a set of distributed cooperative objects
(DCO) and an object system mapping (OSM). This framework
is graphically depicted in Fig. 2. A fourth modeling step
involves defining an experiment to stimulate the model during
simulation and to collect data needed for performance and
behavior analysis. Following the terminology and conventions
of Zeigler [15], this fourth step results in an experimental
frame.
This framework enables modeling the abstract behavior
of the software components independent of the computing
and networking hardware components. The resulting software
and hardware components are then coupled together to form
a dynamic model of a targeted distributed object computing
system. Software applications are modeled in the DCO following a distributed object paradigm. Hardware for networked
computing components is modeled in the LCN.
A. Hardware: Loosely Coupled Networks (LCN)
The LCN abstraction of networked computer components results in the specification of processors, network gates and links
[2]. The processors serve as nodes on which software objects of
the DCO abstraction may be loaded and executed. The two critical parameters for these processing nodes are processor storage
size and processor speed. The storage size of the processor constrains software objects in their competition for memory resources. The processor speed constrains the rate at which software jobs are processed. The network gates in the LCN represent hubs and routers in a computer network. The critical parameters constraining the performance of the gates are operating mode (hub or router), buffer size and bandwidth. The links
model the communication media between processors and gates.
Critical parameters for links include number of channels, bandwidth per channel, and error rates. The LCN network topology
is defined by mapping the processors and gates onto the links.
B. Software: Distributed Cooperative Objects (DCO)
The DCO abstraction of software components results in
the specification of computational domains, software objects,
methods, and object interaction arcs [2]. A set of software
objects form a computational domain and any software object
may belong to more than one computational domain. A computational domain represents an executable environment or
program.
A software object contains both attributes (data members)
and methods (functions) that operate on the attributes. The collective memory requirements of these attributes and methods
characterize the object’s size. When the software object is invoked, the size parameter loads the supporting LCN processor
memory. The object may be invoked autonomously or on the receipt of an object interaction arc. Software objects may be set to

HILD et al.: DEVS-DOC

Fig. 2. Butler’s distributed object computing framework.

invoke autonomously to simulate the initial execution order of a
domain. The software object has a thread mode parameter that
defines the granularity of its multithreaded behavior: method,
object, or none. This thread mode determines the level of execution concurrency the software object supports. At the method
level, all requests to the object may execute concurrently. At
the object level, only one request per method may execute concurrently; additional requests against an executing method get
queued. At the none level, only one request per object may execute at a given time; each additional request to the object gets
queued.
Software object methods are characterized by a computational work load factor and an invocation probability. The
work load factor represents the amount of computational work
required to fully execute the method. The invocation probability
is an artifact of quantum modeling [2]. Taking the quantum
perspective, when a software object is invoked, it is irrelevant
which method is actually selected as long as all the methods of
the object are invoked in correct proportion.
Two types of software object interactions are identified, messaging arcs and invocation arcs. A messaging arc represents
peer-to-peer message passing between objects. When a source
object sends a message, it may target several destination objects.
The frequency of firing a messaging arc is based on the computational progress of the source software object. The message size
parameter characterizes the amount of data transmitted. An invocation arc represents client/server type interactions between
software objects. When an object, the client, fires an invocation
arc, the message size parameter specifies the amount of data
sent. The destination object, the server, invokes a method and
sends a response once that method completes its execution. Invocation arcs have one of two types of blocking modes: synchronous or asynchronous. The synchronous setting causes the
source software object to be blocked from continuing its method
computation while the asynchronous setting allows method execution to continue at the source software object.
Software object methods and interactions drive the dynamic
behavior of distributed object computing systems. In cases
where software objects are executed on dispersed processing
units, then the invocation and messaging arcs are routed
through the LCN processors, gates, and links. When invoked by

83

an incoming arc, a software object is “loaded” (added) into the
memory of its assigned processor. Arc reception also triggers
the software object to select a method for execution. The
selected method is set up as a unique job thread and then either
fired for execution or queued based on the multithreading mode
and the current state of the software object. Fired jobs go to the
assigned processor for execution. Completed jobs are received
back from the processor and mark computational progress
for the software object. Based on computational progress,
additional interaction arcs are selected for exchanges with
other DCO software objects. Selected arcs are fired/transmitted
across the LCN. Job threads that trigger arc firings continue
execution unless blocked by the firing of a synchronous arc. In
this case, the job thread continues execution after the associated
return arc is received. When all job threads complete execution;
and all communicating arcs expecting return arcs are received;
the object unloads processor memory and deactivates.
C. Distribution: Object System Mapping
Individually, neither the LCN nor the DCO models are of any
great value in modeling the behavior or performance of a distributed object computing system. The LCN provides a model
of the target hardware architecture that imposes time and space
constraints. The LCN, however, lacks a specification of dynamic
behavior. Dynamic behavior is specified in the DCO model with
the definition of computational loads and object interactions.
These DCO definitions provide a model of the target software
architecture. However, this DCO behavior representation is independent of time and space. By mapping the DCO software objects onto LCN processing nodes, the abstract dynamics of the
software architecture are coupled and constrained by the capabilities and topology of the hardware architecture. These mappings, as specified by the user, form the OSM abstraction.
The performance and behavior of the resulting distributed object computing system can be analyzed with simulation. During
simulation, the invocation of a software object competes for processor resources in terms of memory to load the object and processor computational cycles to execute its methods. These dynamics drive the performance of the processor as it serves assigned (mapped) software objects. The performance of the processor also determines the performance of the software object in
terms of completing computational tasks and communications
exchanges with other software objects. These communication
exchanges also drive the dynamics of the network performance
as well as the performance of the computational domain (application) as a software object exchange denotes computational
progress.
In addition to mapping DCO software objects to LCN processors, Butler used the OSM model to define a set of communication modes and mapped the DCO interaction arcs onto these
communication modes. These communications modes specify
how DCO interaction arcs are processed into packets that will
transit the LCN. The communication mode constrains packet
size, which drives arc segmentation into packets, as well as
sets packet overhead size, packet acknowledgment size and acknowledgment timeout values.

84

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

V. REPRESENTATIONS OF DOC COMPONENTS IN DEVS
In this section, we describe our DEVSJAVA implementation
of the DCO, LCN, OSM, and the associated experimental frame
components. While DEVS-DOC can be implemented using alternative programming languages and simulation environments,
we believe that our approach has been successful in using the object-oriented paradigm to maintain a separation of concerns and
to encapsulate abstractions of behaviors and data 3 . For example,
considering our primary separation of concerns is between the
LCN and DCO abstractions, in the DEVS-DOC implementation, messages exchanged between DCO software objects and
LCN processors contain one of two object class types: job or
msg. A DCO software object sends a job to an LCN processor
to signify the execution of a method and the LCN processor returns the job to the DCO software object to signify execution
completion (refer to Section V-C for further details). Similarly,
a DCO software object sends a msg (message) to an LCN processor to interact with other DCO software objects and the LCN
processor routes the msg through the appropriate LCN nodes to
the destination DCO software object.
A. Loosely Coupled Networks
1) LCN Links: LCN links describe the communication
medium that interconnects two or more LCN nodes. Butler’s
specification for a link is an entity with an assigned error
coefficient and a set of one or more channels, where the
bandwidth on each link is a random variable to account for
servicing actions external to the scope of the simulation space.
Our implementation extends this concept with the specification
of technology specific links. In [19] and [20], we developed a
model of an ethernet link. This ethernet link model allows us
to simulate and evaluate DOC systems that employ ethernet
technologies within the networking topology.
2) LCN Gates—Hubs and Routers: LCN gates are entities
that interconnect two or more network links. Behavioral attributes for these gates are mode (hub or router), bandwidth for
processing traffic and a buffer size for queuing traffic. The key
distinction between router and hub mode is routing decision
logic. As a hub, packets from one link are broadcast out on
every other link. As a router, packet address information is
used to make routing decisions and only send a packet down a
specific link as necessary for end-to-end communication.
In supporting this routing decision logic, we opted to not
burden the DOC modeler with the development of routing
tables. Populating routing tables within LCN routers requires
knowledge of the LCN topology and the mapping of DCO software objects to LCN processors. To avoid burdening the DOC
modeler with developing this information and maintaining
consistency with alternative LCN topologies and alternative
DCO to LCN mappings, route discovery logic is encoded into
the DEVS atomic router model and into the DCO software
object model. When a DEVS-DOC simulation starts, the DCO
software objects announce themselves, via DEVS messages, to
3We note that our treatment of hardware and software components in DEVSJAVA does not address how the (static and dynamic) structures of such models
may be determined. We refer the interested reader to [18] for characterization of
DEVS-DOC model components in the domain of distributed mission training.

their assigned processors. The processors then broadcast this
information on their assigned LCN links. LCN routers receive
these broadcasts, load their routing tables and forward the
broadcast. Obviously, alternative routing algorithms (routers or
gates) can be modeled as DEVS models and serve as candidate
components for the LCN layer.
Our desire to support technology specific link models has
also required the development of technology specific models of
media access units (MAUs) or network interface cards (NICs).
To separate these two concerns—routing and technology specific NICs—we choose to implement LCN gates as separate object classes. For hubs, a technology specific NIC model class
is implemented for each corresponding link technology model.
For a router, a single router class model is implemented with
the required routing logic. To couple a router to a technology
specific link, a DEVS coupled model is used to define the LCN
gate. To model a router interconnecting two ethernet links and
a T1 carrier link, the DEVS coupled model of the gate consists
of two ethernet hub models and a T1 hub model as depicted in
Fig. 3.
3) LCN Processors: The LCN processor object is capable
of performing computational work for DCO software objects
and it enables these software objects to interact via the LCN.
Butler identifies only two specification parameters for a
processor—a computational speed and a storage capacity. If a
processor is connected to multiple LCN links, routing decision
logic is necessary and we have elected to implement it within
the processor. Butler also identified a need to define communication modes—segmenting interaction arcs into packets,
dealing with packet overhead, packet acknowledgment, and
timeouts—in the OSM abstraction. Again, we have opted to
implement these mechanisms within the LCN processor model.
The resulting LCN processor is implemented as a DEVS coupled model of a central processing unit (CPU), a router, and a
transport component. The router component is simply a reuse
of the router class model just described in LCN Gates—Hubs
and Routers. Technology specific NIC models are coupled to the
processor as needed for the LCN link technologies and topology.
The couplings for this DEVS coupled processor model are depicted in Fig. 4.
The CPU is modeled as a DEVS atomic model with two input
ports, one output port, six state variables, and three parameters. The inJobs input port accepts requests from DCO software
objects to execute jobs. The inSW input port accepts requests
from DCO software objects to load and unload software into
and out of, memory and disk space. The outJobs output port
emits jobs that have completed execution. A cpuSpeed parameter determines how quickly data processing operations associated with accepted jobs are executed. When processing multiple
jobs, the effective cpuSpeed is divided equally across each job.
Thus, jobs with equal loading factors are completed on a first-in,
first-out basis. And, in the case of jobs with unequal loading
factors arriving at the same time, the smaller jobs will complete
first. A memSize parameter defines the memory usage constraint
and triggers job swapping dynamics as the CPU processes jobs.
A swapTimePenalty parameter specifies a job processing time
loading factor when memory usage becomes constrained during
simulation runs.

HILD et al.: DEVS-DOC

Fig. 3. DEVS coupled model of a gate.

Fig. 4. DEVS coupled processor model.

The transport component is modeled as a DEVS atomic
model and segments a DCO software object interaction msg
into packets for transport across the LCN. The transport
component at a destination node receives and collects packets.
When all packets for an interaction msg are received, the
destination transport component delivers the interaction msg to
the destination DCO software object. Future versions of this
transport implementation can support alternative schemes such
as packet acknowledgment and timeouts.
B. Distributed Cooperative Objects
1) Computational Domains: Computational domains
can be defined for two purposes: the grouping of software
processes and program scheduling [2]. Grouping software
processes facilitates the extraction of simulation results across
a set of software processes that form a program or collection
of programs. Program scheduling facilitates identifying and
characterizing software objects that represent the main program
components. The former is an experimental frame issue and
the latter is a DCO behavior issue [8]. To maintain a separation
of these two concerns in the DEVS-DOC implementation,
we have opted to identify computational domains within
the confines of the experimental frame and address program
scheduling within the confines of the DCO software objects.

85

2) Software Objects: DCO software objects represent the
interacting processes of an executable program. Within Butler’s
structure, software objects have four parameters: a thread mode,
a memory storage size, a set of method computational workloads, and a set of method invocation probabilities. The thread
mode determines the multithreading granularity of the object:
none, object, or method. The memory storage size represents
the total (data and methods) memory requirement of the object when loaded into processor memory for execution of its
methods. Each method is represented by a computational workload factor (e.g., processor cycles) and an invocation probability.
The invocation probability is an artifact of the quantum modeling approach and represents the probability that an invocation
method will call (invoke) that method.
Our software object (swObject) is implemented as a DEVS
atomic model. Fig. 5 highlights the inputs, outputs, state variables and parameters of this DEVS atomic model. The swObject
model uses thread mode, object memory size and a methods set
as the base suite of parameters. We extended this model implementation to also allow defining initializer objects, interaction
arcs source mappings and sequencing of methods and arcs for
a directed modeling capability.
To implement program scheduling within software objects,
as alluded to in Section V-B1 (Computational Domains), each
software object is declared with a duty-cycle parameter. The
duty-cycle supports triggering multiple executions of the software object during a simulation run by defining the time lag
between executions. Any software object with a duty-cycle set
to less than infinity is considered an initializer object. So, program scheduling is supported with the designation of initializer
objects that represent the main program. If the DOC modeler
is detailing specific method and arc sequences in modeling the
DCO, an initMsg (initialization message) is also defined to start
each program execution with the invocation of a target method.
The implementation of an interaction arc is described in the Invocation and Messaging Arcs section.
The structure used in defining a set of methods for a software
object depends on the DOC modeler’s intent and desired level
of detail in modeling the selection and sequencing of methods
and interaction arcs. Following the quantum (probabilistic) approach, methods are defined as a set of computational loads
paired with a set of invocation probabilities. In a quantum sense,
it is irrelevant which method is invoked through an interaction
arc, only that a method of a given behavior is invoked in correct
proportion to the aggregate invocations of all methods of the object. We have extended the level of detail associated with modeling software object method and interaction arc sequencing. Interaction arcs can be defined to call (invoke) a specific method
on a targeted software object. Furthermore, a software object
method can declare a specific computational workload and interaction arc firing sequence. These extensions offer a DOC
modeler the ability to do directed simulations of specific DCO
interactions [8].
3) Invocation and Messaging Arcs: Invocation and message
arcs were defined independent of computational domains and
software objects in [2]. In defining the DCO representation,

86

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

Fig. 5. DEVS atomic model for swObject.

C. Object System Mapping
The object system mapping component handles the assignment of DCO software objects to LCN processors. For coupling
software objects to processors, we’ve extended the DEVS coupled model with specific procedures for coupling a software object or set of software objects to a given processor. For example,
when a DEVS set container called swObjects contains swObject1, swObject2, and swObject3, the procedure

Fig. 6. DCO software objects and LCN processor couplings.

mapping functions are used to map one source (calling) software object to each arc and to map one or more target software
objects to each arc. Arcs are further defined with a firing frequency parameter and a message size parameter. The firing frequency is expressed as the amount of computational progress
to be made by the source software object between arc firings
(software object interactions). The message size parameter represents the number of bytes sent across the LCN. Invocation type
arcs are defined with two additional parameters: a return message size and a blocking mode (synchronous or asynchronous).
As invocation arcs represent a client-server interaction, the return message size represents the number of bytes returned by
the target software object at the completion of its method execution.
To facilitate the declaration of invocation and message arcs
in our DEVS-DOC implementation, we defined a dcoArc object class as an extension of the DEVS entity class. Using the
quantum modeling approach, an arc is declared by specifying an
arc name, a target set of software objects, a message size, a return size, a message type (synchronous, asynchronous, or message), and a firing frequency. To enable a more directed modeling capability, we also allow the DOC modeler to declare arcs
by replacing the firing frequency parameter with the name of a
method on the target software object. The source software object for an arc is defined by including that arc in the declaration
of the software object.

will add the necessary couplings as depicted in Fig. 6. In particular, this procedure couples the outMsgs port of each component
in swObjects to the inMsgs port of the processor. Similar couplings are made for outJobs to inJobs and for outSW to inSW.
The procedure also couples the outMsgs port of the processor
to the inMsgs port for each component in swObjects. Similar
couplings are made, as well, for outJobs to doneJobs. Such procedures significantly simplify the coding of the OSM.
These couplings facilitate the following interactions during
simulations. When a software object is invoked, it is loaded into
processor memory by sending a “load software” message to the
processor inSW port. Furthermore, a method of the software object is selected for execution and a computational job is sent to
the processor model inJobs port. Once the processor completes
executing the job, the job is returned to the software object via
the outJobs port of the processor. Each software object receives
the job completed by the processor and must check whether the
job on the doneJobs port originated from itself. If so, the software object selects an arc, creates a message, and sends it to
the processor port inMsgs. The processor can receive a message
destined for one of its associated software objects on either the
inMsgs port (for local software interactions) or the inLink port
(for nonlocal software interactions). The message received by
the processor is sent on the outMsgs port. Each of the local software objects receives the message and must verify whether it is
the targeted (destination) software object prior to servicing the
message.

HILD et al.: DEVS-DOC

87

TABLE I
MAJOR DOC SYSTEM SIMULATION METRICS

Fig. 7.

Layered experimental frame.

At this level of abstraction, the DCO to LCN mapping may
be more appropriately characterized as a software to firmware
mapping in that the processor components represent an aggregation of hardware and software functional resources. We recall
that the DEVS-DOC abstraction is at a higher level in comparison with typical embedded systems representations. The cpu
actually represents a combination of processor subsystems: central processing unit, disk and system memory, system bus, and
input/output ports. Likewise, the gate actually represents a combination of a network interface and an intra-processor communication channel for cohosted interacting software objects.
D. DEVS-DOC Experimental Frame
For distributed object computing systems, Table I depicts a
set of metrics to observe and assess while conducting an experiment through simulation of a DOC model. Table I is revised
from [2] and lists some of the major sets of information metrics that may be derived from simulation of the DCO and LCN

components. Metrics marked with an asterisk (*) are applicable
for that component class. Metrics marked with a “ ” indicate
that an aggregation of the metric is applicable over the set of
components in that class. The objectives, for a distributed computing system modeling and simulation experiment, determine
which metrics are applicable and dictate the functional components needed for the experimental frame.
We have developed a set of transducers for the processor, link,
domain, object and the interaction arc classes listed in Table I.
Our current link transducer is specifically for ethernet links. Hub
and router gate transducers and a generic link transducer can be
developed following the experimental frame principles.
Given the number of random variables in a DOC model,
we have also developed a “tuples” transducer to collect results
across multiple simulation runs. The tuples transducer collects
the results of each transducer from a single simulation run
and computes the mean, variance, lower and upper bounds
for the metrics of Table I. This approach can be viewed as an

88

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

experimental frame containing two operational layers: the first
layer to support individual simulation runs and a second layer
to drive multiple simulation runs and aggregate the results into
statistical quantities. A traditional experimental frame (EF-1) is
constructed with a generator to stimulate the model, an acceptor
to control the simulation, and a transducer to collect simulation
data and summarize the results. The second operational layer
experimental frame (EF-2) stimulates, controls, collects, and
summarizes the results of the first experimental frame layer.
Fig. 7 depicts this layered experimental frame concept.
1) Domain Transducer : As pointed out in the DCO Computational Domains section, a purpose of defining computational
domains is to group coupled software processes for extraction of
simulation results. So, we implement computational domains in
DEVSJAVA using a set from the heterogeneous container class
library (HCCL) [21]. A HCCL set is declared for each computational domain being represented. Software objects associated
with the domain are added to the HCCL set. This set of software objects is then used in declaring the domain transducer
that monitors the assigned software objects during simulation.
VI. CASE EXAMPLE
In this section, we discuss a simple network management
protocol (SNMP) DEVS-DOC model, an associated SNMP
software object model and present results obtained from a
DEVS-DOC simulation of the model and its real system
counterpart.
A. SNMP Monitoring
As an example application of the DEVS-DOC framework
we have modeled and simulated an Internet Engineering Task
Force compliant SNMP management system [22]. Such a
system has four essential elements: management stations;
management agents; management information bases (MIBs);
and a management protocol. Quite simply, management
stations request agents using the management protocol to
perform operations on MIB objects; and agents respond to such
requests. The MIB objects represent manageable attributes. For
our scenario, the manager requests agents to provide status on
selected MIB objects via a SNMP get command.
For this example, the LCN consists of six host processors
interconnected with 10 Mbps ethernet links through a central
hub to form a network with a star topology. For the DCO, on
each of these processors is an SNMP management agent. One of
the six processors also has an SNMP manager software object
and a “loop” controller software object. For the experimental
frame, one software computational domain is defined, which encompasses all the SNMP management agents, plus the manager
and loop software objects. This case example is diagrammed in
Fig. 8.
The “loop” controller drives the system dynamics for this case
example. The “loop” object fires a snmpwalk arc to invoke the
manager object to “walk” the MIB of one of the six host processors. When invoked, the manager fires a series of snmpget
commands. These commands translate into snmpget jobs that
load the processor and snmpget arcs that query (invoke) the targeted SNMP agents. Each SNMP agent fires a job to process

the request and then fires a return message representing the response to the query.
Within DEVS-DOC the SNMP agents are declared as DCO
software components based on the following code fragment.
// snmp (snmp management agent)

=

int snmp_Size 32 000 8; //bytes 8 bits/byte
intEnt snmpget_response_WL
new intEnt(2 470 600);
intEnt snmptrap_WL
new intEnt(1 560 000);
// snmp management agent methods:

=

=

//methodName, queue_of(pairs_of(workLoad,arc))
queue snmptrap
new queue();
snmpget.add(new pair(snmpget_response_WL,noArc));
queue snmptrap
new queue();

=
=

snmptrap.add(new pair(snmptrap_WL,
new dcoArc(“snmptrap”,“mgr”, 48 8,“snmptrap”)));
methods
new queue();
methods.add(new method(“snmpget”, snmpget));

=

methods.add(new method(“snmptrap”, snmptrap));
//snmp swObject: name, size(bits), threadMode,
// dutyCycle, initMethod, methods
snmp 4
new swObject(“snmp4”,snmp_Size,“method”,

=

INFINITY,“snmptrap”,methods);
snmp 6
new swObject(“snmp6”,snmp_Size,“method”,
INFINITY,“snmptrap”,methods);

=

=

snmp 15
new swObject(“snmp15”,snmp_Size,“method”,
INFINITY,“snmptrap”,methods);
snmp 17
new swObject(“snmp17”,snmp_Size,“method”,
INFINITY,“snmptrap”,methods);

=
=

snmp 19
new swObject(“snmp19”,snmp_Size,“method”,
INFINITY,“snmptrap”,methods);
add(snmp4);
add(snmp6);
add(snmp15);
add(snmp17);
add(snmp19);

Within our DEVS-DOC framework, the SNMP Monitoring
OSM of Fig. 8 is implemented with the following code fragment.
public void Object_System_Mapping() {
Add_coupling_swObject_to_processor(loop, asc6);
Add_coupling_swObject_to_processor(mgr, asc6);
Add_coupling_swObject_to_processor(snmp4, asc4);
Add_coupling_swObject_to_processor(snmp6, asc6);
Add_coupling_swObject_to_processor(snmp15, asc15);
Add_coupling_swObject_to_processor(snmp17, asc17);
Add_coupling_swObject_to_processor(snmp19, asc19);
}

We ran three series of simulations for this DOC system. For
each series, we set the manager software object to none, object and method level multithreading on its thread mode. Within
each of these series, we executed five simulations with the loop
software object set to fire the snmpwalk arc the same number

HILD et al.: DEVS-DOC

89

Fig. 8. SNMP monitoring.

system (real) for each thread mode configuration: none, object
and method.
From these results we see that the execution times begin to
rapidly increase for the real system when the manager is set for
a method level of granularity. We explored this behavior further using the system activity reporter (sar) utility on a Unix
system running the manager software component. From the sar
report, we found that after ten iterations significant amounts of
time accumulated for block transfers in and out of memory. Our
DEVS-DOC model of the manager software component is such
that it models the start of a new process for each snmpget request. So with a method level granularity setting, on the third
simulation we have ten loop iterations and 35 snmpgets per iteration, which equates to 350 concurrent execution requests. The
block transfers end up swapping out the processes executing
these requests. Our simulation results for this case example correspond well with most runs. For the method level configuration, however, our poor simulation results are attributable to
a weak representation for memory swapping dynamics in the
cpu model. Developing an improved cpu memory representation
is one example for modeling information technologies as discussed in the Future Research Directions section of this paper.
VII. RELATED WORK

Fig. 9.

SNMP monitoring execution times.

of times as the simulation iteration. In other words, fire snmpwalk once for the first simulation; fire snmpwalk five times
for the second simulation; ten times for the third simulation; 15
times in the fourth simulation, and 20 times in the fifth simulation. We also realized this DOC system in a lab environment
and collected data on the execution times. Fig. 9 depicts plots of
the results for the simulation runs (simulation) against the real

Research in codesign is closest to our work when viewed
from an abstract view of concurrent hardware and software
analysis and design within a framework. Given research
activities in Codesign since the 1980s, many of the developed
methodologies are primarily focused on real-time embedded
systems (e.g., [7], [23], [24]). Unlike the work described
in this paper, Codesign methodologies and their supporting
environments generally emphasize concurrent engineering of
hardware and software constraints (e.g., size, timing, weight,
cost, reliability, etc.) for a single piece of hardware. Aside from
research on system-on-a-chip analysis, design, and testing, recent codesign approaches are targeted at low-level (e.g., FPGA)
analysis and design for distributed hardware components on a
chip. For example, the types of work discussed in [7] focuses on
embedded (locally distributed) systems executing on a multiple
number of processors residing on a single piece of hardware.
Unlike these approaches, our distributed object computing
approach is aimed at system representation at a high-level

90

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

of abstraction (e.g., processors, switches and links). That is,
DEVS-DOC advances the basic ideas of codesign system
development from the execution of one or more processes on a
single device to a unified framework for networked hardware
components and physically distributed cooperating software
components.
Examples of research activities in codesign include Codesign
And Synthesis Tool Environment (CASTEL) [25], POLIS at
the University of California, Berkeley [26], and Model-based
Codesign at the University of Arizona [27] (refer to [7] for a
comprehensive survey of existing work on codesign). CASTLE
is a set of tools to support synthesis of embedded systems.
CASTLE’s tool set can be used to convert a system’s software
and hardware specifications via System Intermediate Representation using input and output processors from one to the other.
For example, CASTLE’s cosynthesis environment can generate
a processor’s VHDL description and a compiler can translate
any C/C++ program from a given application domain into the
operational code of the intended processor. Similarly, POLIS
provides a Codesign Finite State Machine representation
which can be used to characterize both hardware and software
components of a system using tools such as VHDL or graphical
FSMs. POLIS provides a comprehensive design flow:
1)
2)
3)
4)
5)
6)
7)

high level language translation;
formal verification;
system co-simulation;
design partitioning;
hardware synthesis;
software synthesis;
interfacing implementation domains

for embedded systems hardware/software codesign. The modelbased codesign methodology advocates a five-step process:
1)
2)
3)
4)
5)

specification (requirements and constraints);
modeling;
simulation/verification;
model mapping to hardware/software components;
prototyping and implementation.

Approaches such as CASTLE, POLIS, and model-based codesign are not intended, however, to support concurrent distributed
hardware/software analysis and design at a high-level of abstraction as does DEVS-DOC.
An environment known as DiSect is proposed to aid the development, execution monitoring and post execution analysis of
distributed simulations [28]. The Distributed Simulation Exercise Construction Toolset (DiSect) is comprised of three software tools. The Exercise Generation (ExGen) tool aids developers in composing simulations from a web-based simulation
object repository. The Distributed Exercise Manager (DEM) is
the simulation execution monitoring tool. DEM provides capabilities to control simulations and monitor the distributed simulation infrastructure: workstations loads, network loads and the
high level architecture (HLA) run time infrastructure (RTI) [14].
The Modular After Action Review System (MAARS) provides
tools for visualization and analysis of data collected during a
DEM simulation. DiSect is claimed to have supported evaluation and analysis during and after, execution of an Army training
simulation exercise. Nevertheless, while such an environment

can aid in the redesign of systems of interest, it is not a modeling and simulation environment.
VIII. FUTURE RESEARCH DIRECTIONS
From a basic inquiry into the limitations of the proposed
framework, support for system architectures with distributed,
(virtual) shared-memory, object computing seems inevitable.
The basis of this work is on the assumption that each processor
will have its own unique memory. Therefore, supporting the development of these (virtual, shared memory) kinds of systems
remains open for future research. Furthermore, a substantial part
of today’s computational needs are based on agent-oriented systems where such a system inherently has a dynamic structure
(e.g., mobile networked tele-/video-conferences not only depend on varying network topologies, but also varying software
computational characteristics such as load and response time).
Therefore, representation of variable-structure systems is believed to be of importance in future years. The work described
here is an ongoing research activity with several distinct directions to follow. Aside from further development of DEVS-DOC
in terms of a richer set of LCN and DCO components, several
interesting and valuable applications are believed to be suitable
candidates for verifying and validating concepts, modeling constructs and their realization.
A. Mobile Software Agents
Mobile software agents are autonomous, intelligent programs
that move through a network, searching for and interacting with
services on the user’s behalf. These systems use specialized
servers to interpret the agent’s behavior and communicate
with other servers. A mobile agent has inherent navigational
autonomy and can ask to be sent to other nodes. Mobile agents
should be able to execute on every machine in a network and
the agent code should not have to be installed on every machine
the agent could visit. Therefore mobile agents use mobile code
systems like Java with Java classes being loaded at runtime over
the network. DEVS-DOC provides a potential means to model
systems with mobile agents. Extensions would be needed in
the distributed software objects (DCO) to allow for an object
to wrap and unwrap itself for firing across the LCN. The LCN
would also need extensions to support dynamic routing of
traffic to an agent that is, or has moved.
B. Real Time Distributed Object Computing
Increasingly, applications with stringent timing requirements
are being implemented as distributed systems. These timing
requirements mean that the underlying networks, operating
systems and middleware components forming the distributed
system must be capable of providing quality of service (QoS)
guarantees to the supported applications. Implementation
mechanisms and technologies for such real time systems is an
active area of research. DEVS-DOC provides a potential means
to model and simulate proposed mechanisms and technologies.
To enable modeling of real time systems, DEVS-DOC will
need to be extended with a means for distributed software
objects to specify QoS requirements in methods and arcs

HILD et al.: DEVS-DOC

91

and for LCN/DCO components to respond to such QoS
requests. Current research in real-time DEVS modeling and
simulation (DEVS/CORBA [29]) can provide a basis to extend
DEVS-DOC modeling constructs to represent QoS requirements in mixed simulation and physical (real-system) settings.
C. Collaborative Modeling and Simulation
Finally, to enable anytime/anyplace concurrent, distributed
hardware/software studies, DEVS-DOC may be used inside a
collaborative environment. For example, Collaborative DEVS
Modeler [30] can be used as a basis to enable DEVS-DOC to
be used in a collaborative and distributed setting (see Fig. 1).
At the University of Arizona, several projects are underway to
extend collaboration support to a full scale modeling and simulation capability. These projects seek to build on experience
with GroupSystems [31] in combination with advances in modeling and simulation methodology and high performance, distributed simulation support environments [32]. Research concepts evolving from these projects form the basis of a conceptual collaborative M&S architecture. Furthermore, DEVS-DOC
can be used to explore and study alternative implementation designs for various collaboration architectures.
IX. CONCLUSIONS
The concepts and implementation of the DEVS-DOC modeling and simulation framework are overviewed and a case
study of using DEVS-DOC to model and simulate a real-world,
distributed computing system is presented. The case study
demonstrates some key capabilities as well as one limitation,
the cpu memory model, of the DEVS-DOC framework and its
ability to support distributed codesign modeling and simulation. The paper concludes with a review of related work and
highlights several future directions associated with extending
the DEVS-DOC framework.
REFERENCES
[1] D. Schmidt, “Guest editorial: Distributed object computing,” IEEE
Commun. Mag., vol. 35, no. 2, pp. 42–44, 1997.
[2] J. M. Butler, “Quantum modeling of distributed object computing,”
Simul. Dig., vol. 24, no. 2, pp. 20–39, 1995.
[3] (2000) DEVSJAVA. Electr. Comput. Eng. Dept., Univ. Arizona,
Tucson. [Online]. Available: http://www.acims.arizona.edu/SOFTWARE/software.shtml
[4] M. Roulo. Java’s three types of portability. presented at Java
World.
[Online].
Available:
http://www.javaworld.com/javaworld/jw-05-1997/jw-05-portability.html
[5] (1999) What is OMG-UML and Why is it Important?. Object Management Group. [Online]. Available: http://www.omg.org/news/pr97/umlprimer.html
[6] (1999) Rational Unified Process: Best Practices for Software
Development Teams. Rational Software Corporation. [Online].
Available: http://www.rational.com/sitewide/support/whitepapers/dynamic.jtmpl?doc_key=100 420#1
[7] T.-Y. Yen and W. H. Wolf, Hardware-Software Co-Synthesis of Distributed Embedded Systems. Boston, MA: Kluwer, 1997.
[8] D. Hild, “Discrete event system specification (devs) distributed object
computing (doc) modeling and simulation,” Ph.D. dissertation, Dept.
Electr. Comput. Eng., Univ. Arizona, Tucson, 2000.
[9] H. S. Sarjoughian, D. R. Hild, and B. P. Zeigler, “DEVS-DOC: A co-design modeling and simulation environment,” IEEE Computer, vol. 33,
no. 3, pp. 110–113, 2000.

[10] B. P. Zeigler et al., “The DEVS environment for high-performance modeling and simulation,” IEEE Comput. Syst. Eng., vol. 4, no. 3, 1997.
[11] (1999) DEVS/HLA Publications. Electr. Comput. Eng. Dept., Univ. Arizona, Tucson. [Online]. Available: http://www.acims.arizona.edu/PUBLICATIONS/publications.shtml
[12] B. P. Zeigler, T. G. Kim, and H. Praehofer, Theory of Modeling and
Simulation, 2nd ed. New York: Academic, 2000.
[13] B. P. Zeigler, H. S. Sarjoughian, and S. Vahie, “An architecture for collaborative modeling and simulation,” in 11th Eur. Simulation Multiconf., Istanbul, Turkey, 1997.
[14] (1999) High Level Architecture Homepage. DMSO, U.S. Dept. Defense. [Online]. Available: http://www.dmso.mil/hla/
[15] B. P. Zeigler, Multi-Facetted Modeling and Discrete Event Simulation. New York: Academic, 1984.
[16] A. Chow, “Parallel DEVS: A parallel, hierarchical, modular modeling
formalism and its distributed simulator,” SCS Trans. Simul., vol. 13, no.
2, pp. 55–102, 1996.
[17] H. S. Sarjoughian and B. P. Zeigler, “DEVSJAVA: Basis for a devsbased collaborative M&S environment,” in SCS Western Multi-Conf.,
San Diego, CA, 1997.
[18] H. S. Sarjoughian et al., “Scalability considerations and evaluation for
distributed mission training systems,” in Simulation Interoperability
Workshop, Orlando, FL, 2001.
[19] D. Hild, H. S. Sarjoughian, and B. P. Zeigler, “Distributed object computing: DEVS-based modeling and simulation,” in Proc. SPIE, Orlando,
FL, 1999.
[20] D. Hild and H. S. Sarjoughian, “Ethernet: DEVS-based ethernet modeling & simulation,” in SCS Western Multi-Conf., San Diego, CA, 1998.
[21] B. P. Zeigler, “Objects and systems: Principled design with C++/Java implementation,” in Undergraduate Texts in Computer Science, D. Gries,
Ed. New York: Springer-Verlag, 1997.
[22] (1990) Simple Network Management Protocol (SNMP). Internet Engineering Task Force. [Online]. Available: http://www.ietf.org/rfc.htm
[23] R. Rajsuman, System-on-a-Chip: Design and Test. Boston, MA:
Artech House, 2000.
[24] J. Rozenblit and K. Buchenrider, Computer-Aided Software/Hardware
Engineering, J. Rozenblit and K. Buchenrider, Eds. New York: IEEE
Press, 1994.
[25]
(1999)
Codesign
And
Synthesis
Tool
Environment
(CASTEL) Homepage. German National Research Center
for Information Technology (GMD). [Online]. Available:
http://borneo.gmd.de/EDS/SYDIS/castle/start.html
[26] M. Chioda et al., “Hardware-software codesign of embedded systems,”
IEEE Micro, vol. 14, no. 4, pp. 26–36, 1994.
[27] S. Schulz et al., “Model-based codesign,” IEEE Computer, vol. 31, no.
8, pp. 60–67, 1998.
[28]
(1999)
Distributed
Simulation
Exercise
Construction
Toolset (DiSECT). U.S. Army Simulation, Training and
Instrumentation Command (STRICOM). [Online]. Available:
http://www.stricom.army.mil/STRICOM/E-DIR/ES/DISECT/
[29] H. Cho et al., “Design considerations for distributed real-time DEVS,”
in AI, Simulation & Planning In High Autonomy Systems. Tucson, AZ:
SCS, 2000.
[30] H. S. Sarjoughian, J. Nutaro, and B. P. Zeigler, “Collaborative DEVS
modeler,” in Int. Conf. Web-Based Modeling and Simulation, San Francisco, CA, 1999.
[31] J. F. Nunamaker, R. O. B. Jr., and D. D. Mittleman, “Groupware user
experience: ten years of lessons with GroupSystem,” in Groupware:
Technology and Applications, D. Khanna and Editor, Eds. Englewood
Cliffs, NJ: Prentice-Hall, 1995.
[32] (1999) Any-time/Any-place Concurrent, Collaborative Support
for M&S Life-Cycle. Electr. Comput. Eng. Dept., Univ. Arizona,
Tucson. [Online]. Available: http://www.acims.arizona.edu/PUBLICATIONS/publications.shtml

Daryl R. Hild (M’84) received the Ph.D. degree in electrical and computer engineering from the University of Arizona, Tucson.
He is a Lead Engineer with the MITRE Corporation, Colorado Springs, CO.
His research interests include modeling and simulation, distributed systems integration, distributed systems management, and security engineering.

92

IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART A: SYSTEMS AND HUMANS, VOL. 32, NO. 1, JANUARY 2002

Hessam S. Sarjoughian (M’83) received the Ph.D. degree from the University
of Arizona, Tucson, in 1995.
He is Assistant Professor of computer science and engineering at Arizona
State University (ASU), Tempe. Since 1996, his research activities have focused
on theory, methodology and development of distributed/collaborative modeling,
and simulation including distributed codesign with applications to areas such as
distributed mission training and information technology. His other ongoing research and development efforts are concentrated in hybrid agent and simulation
modeling, software engineering, and artificial intelligence. Before joining ASU,
he was Assistant Research Professor of electrical and computer engineering at
the University of Arizona. Prior to receiving his Ph.D., he was employed at Allied Signal Aerospace Corporation and IBM.

Bernard P. Zeigler (F’95) is Professor of electrical and computer engineering
at the University of Arizona, Tucson, and Director of the Arizona Center for
Integrative Modeling and Simulation. From 1993 to 1996, he headed a multidisciplinary team to demonstrate an innovative approach, based on DEVS, to
massively parallel simulation supported by NSF’s HPCC Grand Challenge initiative. He was also sponsored by Rome Labs to research the use of such high
performance simulation technology in support of optimization and model abstraction. He was the PI on a DARPA Advanced Simulation Technology Thrust
project to develop the DEVS framework for the DOD High Level Architecture (HLA) distributed simulation standard and its application to message reduction through predictive filtering. This research received Honorable Mention in the 1999 DMSO (U.S. Defense Modeling and Simulation Organization) Awards—the only university-based work to be so recognized. A book on
the modeling and simulation, organized by the Air Force Academy, Colorado
Springs, has adopted the framework originated in his classic text Theory of Modeling and Simulation (New York: Academic, 2000), which has been revised for
a second edition.
Dr. Zeigler served on two National Research Council committees to recommend directions for information technology and simulation modeling in the 21st
Century. He has been appointed to a third NRC committee on simulation enhancements to manufacturing. Serving from 1996 to 2000 as editor-in-chief of
the Transactions of the Society for Computer Simulation, he is currently Vice
President in charge of Publications (web and print). He has given numerous
keynote talks, tutorials and short courses and organized symposia and conferences that were the first to promote modeling and simulation fundamentals and
theory. He received awards for his books and articles in the foundations of simulation and was named Fellow of the IEEE for his theory of discrete event simulation based on the Discrete Event System Specification (DEVS) formalism
in 1995. In 2000, he received the McLeod Founder’s Award by the Society for
Computer Simulation, its highest recognition, for his contributions to discrete
event simulation.

Proceedings of the 2007 Winter Simulation Conference
S. G. Henderson, B. Biller, M.-H. Hsieh, J. Shortle, J. D. Tew, and R. R. Barton, eds.

A CO-DESIGN MODELING APPROACH FOR COMPUTER NETWORK SYSTEMS
Weilong Hu
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85281-8809, U.S.A.

Networked systems are important to be described in
terms of software and hardware layers. Model abstractions
are defined in terms of software applications executing on
hardware nodes. Modeling and simulation approaches that
are used for networked system modeling are generally
monolithic – hardware and software components are not
separately modeled and combined systematically. For example, to model and simulate a network system, NS-2 (ns2 2004) can be used to describe the system’s communication protocol and hardware resources. With this framework
and its toolset, modelers use abstractions that have combined software and hardware aspects of computing nodes.
This and others such as OPNET (OPNET 2004), however,
do not support separate specification of software and
hardware with the capability to synthesize them.
A framework that explicitly separates software and
hardware layers of networked systems is known as Distributed Object Computing (Butler 1995). An abstract specification is provided to describe a system in terms of its software and hardware components and the mapping of the
former to the latter. The Discrete Event System Specification (DEVS) modeling and simulation framework has been
used to realize the Distributed Object Computing concept
and abstractions (Hild 2000; Sarjoughian, Hild, and Zeigler
2000). The DEVS/DOC approach extends the generic
DEVS modeling concepts and models to support co-design
specification of network systems. It enables modelers, for
example, to model a computer network in terms of separate
sets of hardware and software components and software to
hardware mappings. With this approach, the hardware and
software layers can be independently specified and the
software to hardware mapping be used to configure system
designs. This level of flexibility is useful for evaluating alternative designs – i.e., hardware designs and topologies,
software application capabilities, and how they are synthesized (Hild, Sarjoughian, and Zeigler 2002; Hu and Sarjoughian 2005).
Aside from logical specification of network systems
using DEVS/DOC, it is also important to support visual

ABSTRACT
Co-design modeling is considered key toward handling the
complexity and scale of network systems. The ability to
separately specify the software and hardware aspects of
computer network systems offers new capabilities beyond
what is supported in modeling frameworks and tools such
as NS-2 and OPNET. The DEVS/DOC simulation environment supports logical co-design specification based on
the Distributed Object Computing (DOC) abstract model.
To overcome DEVS/DOC’s lack of support for visual and
persistent modeling, this paper presents SESM/DOC, a
novel approach, which is based on the Scaleable Entity
Structure Modeler (SESM), a component-based modeling
framework. This approach supports logical, visual and persistent modeling. Modelers can develop software and
hardware models separately and systematically integrate
them to specify a family of computer network system designs. This paper details the SESM/DOC co-design modeling approach with the help of a search engine system example, and presents a discussion for future research
directions.
1

INTRODUCTION

Simulation is commonly used for analysis and design of
systems ranging from embedded devices to distributed
network systems. In recent years, the modeling and simulating of combined discrete/continuous systems has witnessed major advances. Frameworks such as DEVS
(Zeigler, Praehofer, and Kim 2000) and Ptolemy II (Eker et
al. 2003) offer capabilities to describe discrete and continuous aspects of a system and their integration. The underlying concept of these frameworks is to support codesign where a system is divided into software and hardware layers and their integration is specified through A/D
and D/A conversions. However, the underlying abstractions of these frameworks are not well suited for modeling
and simulating computer network systems.

1-4244-1306-0/07/$25.00 ©2007 IEEE

685

Hu and Sarjoughian
and persistent modeling (Burmester and Henkler 2005).
Visual and persistent modeling need to have their own concepts and elements. A component-based modeling framework called Scaleable Entity Structure Modeling (SESM)
offers a unified logical, visual, and persistent foundation
for
developing
hierarchical
simulation
models
(Sarjoughian 2001; Fu 2002; Sarjoughian 2005). A realization of this framework is developed which supports the
semi-automatic translation of SESM logical models into
the simulation code for execution in DEVSJAVA. However, the SESM framework does not support co-design
modeling as described above. In this work, we have introduced the DOC co-design modeling into the SESM. This
has resulted in the SESM/DOC approach which supports
logical, visual, and persistent co-design modeling with
support for generating partial simulation models that once
completed can be executed using DEVS/DOC simulation
environment.
2

requirements of these attributes and methods. The attributes include size, thread mode, and initial method. The
Loosely Coupled Network is for modeling the hardware
components including the CPU, the transport, the link, the
router (network router and routing unit), and the network
interface. Another hardware component is processor which
consists of the CPU, the transport, and the routing unit
components. The Object System Mapping defines a set of
axioms for assigning software components to hardware
components.
The hardware provides computation and memory resource for software. It also presents the network topology
(Hild, Sarjoughian, and Zeigler 2002). When a software
object is invoked, its size loads into the memory of its assigned processor. Besides memory, another resource
needed to run a software object is the processing mode
which handles the workload of the software. Software objects are defined to be executed in one of three modes:
none, object, method. In the none mode, the software can
only process one job at a time, all other jobs need to be
queued. In the object mode, a software object can have one
job per defined method concurrently active. In the method
mode, all the incoming jobs of a software object can be
processed concurrently.
DEVS/DOC provides a mechanism for managing the
workload of software objects. Each software object can
generate, send/receive, and finish jobs. The software object
workload is managed by classifying jobs and handling in
different stages (i.e., need to be ‘done’, is in ‘processing,’
and ‘finished’). The software object records its number of
done jobs which is used to determine when the software
has finished its tasks. Once all the software objects have
completed their work, the simulation is completed and will
be stopped. The control of the simulation execution is defined in terms of pre- or user-defined experimental frame
models.
The Object System Mapping defines the assignment of
the components in the DCO layer to the components in the
LCN layer. The OSM mapping concept is formulated in
terms of couplings in the DEVS/DOC. The software object
loads itself into its assigned processor memory by sending
a load software message to the processor. The selected
method in the software object will be executed by the
processor’s CPU. When all the jobs are done, the software
object will unload itself from the hardware and release the
processor’s memory.
The Distributed Object Computing abstract model
components and their relationships were formalized in
DEVS and implemented in the DEVSJAVA simulation
environment. The DEVS/DOC environment supports modeling the components of the DCO and LCN layers with
their OSM component. The structure and behavior of these
components are specified as atomic and coupled DEVS
models (see Table 1 for the partial Routing Unit model
specification). Details of the transition and time advance

BACKGROUND

2.1 DEVS/DOC
Distributed object computing (DOC) offers concepts and
an abstract model for describing a network system in terms
of a software layer mapped onto a hardware layer (see Figure 1). The software layer describes software components
of the network system in terms of a Distributed Cooperative Object (DCO) model. The hardware layer describes
the hardware components of the network system in terms
of a loosely coupled network (LCN) model. The mapping
of DCO onto LCN is described in terms of an Object System Mapping (OSM).

Figure 1: DOC conceptual view.
The Distributed Cooperative Object is for modeling
software components. A software object is defined to have
attributes (data members) and methods (function). The size
of a software object is defined by the collective memory

686

Hu and Sarjoughian
models and complete coupled models (Bendre and Sarjoughian 2005). Similarly, it supports specifying XML
schemas (Sarjoughian and Flasher 2007).
Each primitive and composite model type is defined in
terms of Template, Instance Template, and Instance model
types. A primitive Template Model can have input/output
with ports and states. The collection of input and output
ports for each component is defined as its interface. Input
and output ports may be used to receive or send simple
data or complex objects. A composite model can have input/outputs with ports, states, and a set of links connecting
the components that are contained in it. Any two components can send and receive information using links. Every
composite component for a Template Model or Instance
Template Model has a unique name and tree structure. The
allowed relationships among composite components are
whole-part. Given a component, a sub-component and super-component composition relationship may exist only
when no sub-component can be the same as its (immediate
or higher) super-component. The sub-component is referred to as part and the component and super-component
are referred to as whole. Composite components can be
used in multiple composite Instance Template Models. The
hierarchy depth of a composite component is equal or
greater than two. All instances of a composite component
(corresponding to the Instance Model) are distinguishable
from one another using their assigned (or given) names.
The primitive and composite Instance Models are instances
of their respective Instance Template Models. The models
satisfy the uniformity constraint which states two components having the same name must have identical structures.
The is-a relationship is also defined for primitive and composite components. It can be used to specify a model (specialize) to be specialized to one or more other models (specialized). The specializee and specialized are defined such
that the former is replaced with the latter when instance
models are generated. Instance models can be generated
from Instance Template models. An Instance model can be
total or partial — i.e., a model hierarchy can be of any hierarchical depth depending on the model that is being
transformed.
Two types of logical models are defined – simulatable
ad non-simulatable (Sarjoughian and Flasher 2007). This
separation is introduced to differentiate between models
that can be simulated in time from models whose behavior
is not defined in terms of time. For example, models of a
processor and queue have dynamical behaviors. However,
a processor model, for example specified in DEVS, can
process a job given a finite period of time. In contrast, a
queue model, for example specified in UML, can return a
job that is queued without taking any time. Given the
specifications of the simulatable and non-simulatable models, the simulatable models are defined to contain nonsimulatable models, but not vice versa. For example, the
processor model can have a queue model.

function specifications for all models can be found in (Hild
2000). Compare to other environments such as DEVS, NS2, and OPNET, DEVS/DOC provides direct support for
discrete-event co-design specification of network systems.
Table 1: A partial routing unit DEVS/DOC model.
Routing
Unit
<X, Y, S, δext, δint, δconf, λ, ta>
InPorts

{inLoop, inLink}

OutPorts

{outLoop, outLink}

X

{(import, pdu)}

pdu

(clientID, searching key word, size, request format)

Y

{(outPort, pdu)}

S

Phase × σ × OutLoopBuffer × OutLinkBuffer ×
OutLoopDelay × OutLinkDelay × SearchingIndex
Phase ∈{passive, busy}, σ ∈ R+0,∞

2.2 Scalable System Entity Structure Modeler
The Scalable System Entity Structure Modeler (SESM)
(Sarjoughian 2001; Fu 2002; Bendre and Sarjoughian
2005; Sarjoughian and Flasher 2007) is a framework aimed
at hierarchical component-based modeling. Its specification capabilities are derived from Entity-Relation (ER),
System Entity Structure (SES) (Zeigler and Hammonds
2007) , and Object-Oriented (OO) modeling approaches. It
introduces visual modeling and transforming logical models into simulation models. Its realization is a modeling engine for developing specifications that have formal logical
syntax and semantics (see Figure 2).

Visual
Modeling

Logical
Modeling

Persistent
Modeling

Model
Translator

Simulation
Code

Standardized
Models

Scalable Entity Structure Modeler

Figure 2: Logical, visual, and persistent model types.
In SESM, the logical models are defined to consist of
primitive and composite model types. A composite model
consists of one to many primitive and/or composite components. The composite model and its components have
the same model type. The primitive and composite model
types can be used to define different kinds of models. For
example, SESM supports partial specification of atomic

687

Hu and Sarjoughian
SESM is visual modeling environment and therefore
supports visual modeling of logical model. Visual models
are represented as hierarchical blocks and tree structures.
These provide complementary visual models such that the
block models depict coupling of ports and the tree structure
shows multi-level model hierarchy. Coupling is presented
as a line with an arrow showing the direction of information flow. A component in a composite model can be either
a primitive model or a composite model. Three different
kinds of couplings are supported. They are internal, external input and external output couplings. These couplings
are defined according to the DEVS coupled specifications,
but they may be changed to support other kinds of models
(e.g., block models in Simulink).
Another key aspect of the SESM framework is its support for model persistence. Logical models are stored according to a set of relational schemas. Model persistence in
a relational database supports retrieving information about
any model component efficiently. This capability is particularly useful for large-scale and complex models.
3

3.1 Example Model
Before detailing the SESM/DOC, a server-client network
system which includes two servers and two clients is considered (see Figure 3). The two servers can be used by clients to search for information. One server supports text file
search and the other supports video file search. A client
can search for mixed text and video information. From a
co-design perspective, the network system shown in Figure
3 can be designed to consist of software and hardware
components and their integration. The Text File Server and
Video File Server are two software components. The Link
1, Router 1, and Hub_ethernet_1 are examples of LCN
components. In the following sections, this system network
will be studied based on the separation of its software applications and hardware facilities.

COMBINED SOFTWARE/HARDWARE MODEL
SPECIFICATION

The software and hardware parts of a system can be modeled based on the SESM’s modeling framework. Users
may develop models according to the distributed object
computing framework. The modelers must apply the DOC
concepts and methods manually. The use of SESM without
direct support for logical co-design modeling concepts and
constructs is ad-hoc and undesirable. Furthermore, the
SESM’s framework does not support visual and persistent
co-design modeling. To overcome these limitations, it is
desirable to introduce the DOC co-design concepts and
methods to the SESM framework. The resulting
SESM/DOC can support co-design from logical aspect as
well as the visual, persistence, and model transformation
aspects. For example, the logical models stored in the
SESM database cannot distinguish among software and
hardware model components. Similarly visual models cannot be differentiated to represent DCO and LCN models.
Additionally, the concept of software to hardware mapping
(OSM) is supported.
The main capability of the SESM/DOC is independent
specifications for software and hardware model components. This separation is to support by database schemas
that conform to the DOC abstract specification. Furthermore, visual model design is to support modeling of software, hardware, and their integration. SESM/DOC must
also support integrating software and hardware layers by
mapping the former to the latter based on the OSM specification. Therefore, to handle the separation and integration
of software and hardware layers, new model types and
constraints are introduced to the SESM framework.

Figure 3: Network system example.
3.2 Co-Design Model Types
In order to support co-design model specification, the
SESM/DOC approach defines primitive and composite
model types for software and hardware model components.
These models extend the syntax and semantics of the
SESM models. Each of the software and hardware models
can have whole/part and coupling relationships. Model
may have specialization relationships to one another. For
example, a model may not contain other models as components or a composite model may have specific whole/part
relationships (see LCN models). The SESM/DOC models
are defined as follows:
•
•

688

Software layer (SESM/DCO): alternative software
model specifications and configurations are supported using a predefined software model.
Hardware layer (SESM/LCN): alternative hardware specifications and configurations are sup-

Hu and Sarjoughian

•

ported using a predefined collection of hardware
models.
Object System Mapping Layer (SESM/OSM): alternative assignments of software models to
hardware models to define combined software/hardware models.

3.2.2 Hardware Layer Model Types
The hardware layer model in SESM/DOC specification
corresponds to the Loosely Coupled Network (LCN) layer.
It is defined to consist of Hardware Layer Model (HLM)
and a set of primitive and composite hardware model
types. The hardware model types are Processor Model
(PM), Network Interface Model (NIM), Link Model (LM),
and Router Model (RM). The composite models are Processor Group Model (PGM), Network Interface Group
Model (NIGM), Link Group Model (LGM), Router Group
Model (RGM), Processor and Network Interface Unit
Model (PNM), and Processor and Network Interface Unit
Group Model (PNGM).

3.2.1 Software Layer Model Types
The software layer model in SESM/DOC specification corresponds to the Distributed Object Computing (DCO)
layer. It is defined to have two model types. They are
Software Layer Model (SLM) and Software Application
Model (SAM). The SLM and SAM correspond to the Distributed Object Computing layer and the software object
defined in DCO. The SLM is a composite model which is
specified in terms of one or more SAMs. Every SAM is a
primitive model which corresponds to a software application that is to be modeled. The SESM/DOC supports the
following modeling constraints in the software layer:
1.
2.
3.
4.
5.

SLM is a composite model that can only contain a
finite number of SAMs;
SLM has to contain at least one SAM;
SAM is primitive model;
SAM can only be contained in a SLM;
SLM and SAM can only be coupled with one another.
Figure 5: Model selection in hardware model working section.
Given the kinds of hardware models that can be defined based on the constraints that are defined for LCN, the
following modeling constraints are supported in
SESM/DOC.
1.
2.
3.
4.
5.
6.
7.
8.

Figure 4: Server-client software specification.
In the example shown in Figure 3, the server-client
network architecture has two server applications and two
client applications in the software layer. These can be
specified in the software modeling working section as
shown in Figure 4. The software layer model has four
software application models. The SoftwareApplication01
and SoftwareApplication02 are the server software applications; (these are called “Text File Server” and “Video File
Server” in Figure 3). The SoftwareApplication03 and SoftwareApplication04 are the client software applications.
The ports defined for SAM and SLM are bi-directional and
therefore couplings between the software layer model and
these software application models are bi-directional.

HLM can only contain group models (PGM,
NIGM, LGM, RGM);
PGM can only contain PM;
NIGM can only contain NIM;
LGM can only contain LM;
RGM can only contain RM;
PNGM can only contain PNM;
PNM can only contain one PM and one NIM;
RM, NIM, LM, RM are primitive models.

These model types and the composition relationships
among them allow specifying different hardware network
topologies. The coupling constraints between these different model types are defined as follows.
1.
2.

689

HLM can only be coupled with group models
RM can only be coupled with RGM;

Hu and Sarjoughian
3.
4.
5.
6.
7.
8.
9.

NIM can only be coupled with NIGM;
LM can only be coupled with LGM;
PNM can only be coupled with PNGM;
RGM can only be coupled with RM and NIGM;
NIGM can only be coupled with NIM, RGM,
LGM, PM, PNGM and RGM;
LGM can only be coupled with NIGM or PNGM;
RGM can only be coupled with NIGM.

allows specifying three model types: SLM and HLM models and how they may be synthesized. Similar to SLM and
HLM, the OSM Layer model satisfies some constraints.
These composition and coupling constraints are:
1.
2.

Given the distinctions between the software and hardware models a modeler must choose the kind of a model
that is to be developed. The “DOC Model” allows the
modeler to choose either the Software Layer Model or
Hardware Layer Model. For modeling the hardware layer
of a network system, the selection of hardware provides a
list of model components as listed above and shown in
Figure 5.
Given the above model types and constraints, a hardware layer model such as HardwareLayer01 can be specified as shown in Figure 6. Furthermore, SESM/DOC supports specifying alternative model specifications. For
example, an alternative hardware network specification for
the HardwareLayer01 can be defined using PNGM. These
different models are specified in terms of the Instance
Template Model with additional information such as the
number of processors, the number of links, and routers (see
Figure 7). The topology of the hardware layer can be
changed which results in different computer network system models.

the OSM model contains only one SLM and one
HLM;
the SLM to HLM can only be coupled with another.

In SESM/DOC, the choices of the SLM and HLM
models that can be used in the OSM layer are specified in
the simulatable software and hardware layers.

Figure 7: Instance template model for HardwareLayer01.
3.3 Multiple Working Sections for Software/Hardware
Layer Model Design
As a co-design modeling approach, SESM/DOC offers
three modeling working sections: software modeling,
hardware modeling, and system modeling. Figure 4 depicts
the user-interface of the SESM/DOC. This environment
extends the software architecture of SESM and is implemented using JavaTM and MS ACCESS database technologies. In each working section, the SESM concepts and
functionalities that separates simulatable and nonsimulatable modeling are used. The Template Model,
Template Instance Model, and Instance Model are supported and thus modelers to create (or delete/modify)
DCO, LCN, and OSM models. For software modeling,
Software Template Model (STM), Software Instance Template Model (SITM), and Software Instance Model (SIM)
are defined (see Figure 8). Similarly, Hardware Template
Model (HTM), Hardware Instance Template Model
(HITM), and Hardware Instance Model (HIM) and OSM
Template Model (OTM), OSM Instance Template Model
(OITM), and OSM Instance Model (OIM) are defined. All
software and hardware component models (e.g., software
template model) are specified as simulatable models that
can have non-simulatable models as state variables or input
and output values (see Section 2).

Figure 6: Server-client network system architecture hardware design.
3.2.3 Object System Mapping Layer
Unlike the DOC and LCN, the OSM specifies mapping
software to hardware components. The OSM model layer

690

Hu and Sarjoughian
Each of the working sections has its own unique tree
structure and block models and support model operations
that are unique to DCO, LCN, and OSM. For example,
Figure 8 provides the view of a software model working
section which has “Template_SoftwareModel_Tree” (a
tree structure only for software models) and also a view for
software models (see the “softwareApplication02” model
in the right panel). Every simulatable model component in
the software modeling working section can be a software
application or a software layer (similarly, every simulatable model component in the hardware modeling working
section can be a hardware application or a hardware layer).
Alternative system models can be synthesized from the
DCO and LCN layers.

3.4 Model Visualization and Persistent
As shown above, SESM/DOC supports logical model
specifications. It also needs to support visual modeling.
Furthermore, all logical models need to be stored in a relational database. The visual and persistent co-design modeling defined simplifies model development, reuse of models, separation of software and hardware, and alternative
software/hardware specifications. The visual and persistent
modeling capabilities are particularly important for largescale, complex model systems. This because visual modeling using tree structure and block models reduces model
development effort and supports maintaining consistency
among a family of alternative models.
3.4.1 Model Visualization
The visual modeling separates software and hardware from
one another by extending the SESM visuals for co-design.
The distinction among model types defined for software
and hardware components, software/hardware layers, and
multiple mappings of software layers to hardware layers is
important in the modeling of computer network systems.
As the scale of a model grows, it is important to reduce the number of components and their relationships.
This is because visual models of large-scale systems is
known to be NP hard problem (Young, Cook, and Mahabadi 2003). Hierarchical modeling combined with a diagonal layout of model components of a composite model reduces significantly the difficulty of visualizing model
couplings. However, since couplings between components
(or layers) are bi-directional, it is important to use visual
notations that distinguish between uni- and bi-directional
visual notations. A uni-directional coupling is defined for
coupling and shown as a dashed line with a single direction. A bi-directional coupling is defined for mapping and
is shown as a directionless dashed line.

Figure 8: Software model working section.
The models in the software and hardware working sections are separated. The software model (or hardware
model) can only be created, edited and viewed in its designated working section. In the OSM section, the modeling
of software components mapped to hardware components
is supported. Therefore, in OSM, there are three kinds of
models – software, hardware and object system mapping.
When an OSM model is created, a software layer model
needs to be selected from the simulatable software model
and a hardware layer model needs to be selected from the
simulatable hardware model. Figure 9 shows a software
layer model chosen for an OSM model in the OSM working section.

Figure 10: SoftwareLayer01 to HardwareLayer01 mapping.
The use of bi-directional coupling reduces visual clutter that can result even with small-scale models. For exam-

Figure 9: Model integration in OSM working section.

691

Hu and Sarjoughian
can only be connected to a network interface group model
(NIGM); if a RGM is connected to a processor group
model (PGM), SESM/DOC displays the “A Router Group
Model Can Not be Connected to a Processor Group
Model!”. Besides ensuring the above constraints, model
types are important for object system mapping modeling.
In the OSM working section, when a system model is created, only one software layer model and one hardware
layer model can be added to the system model. The software layer model and the hardware layer model can be
used to define system mapping assignment.

ple, since software components have bi-directional couplings with the software layer, the couplings in
SESM/DOC are shown as a directionless dashed line (see
Figure 4). This reduces significantly the number of couplings. For hardware components, they have bi-directional
coupling and thus directionless links are also used (see
Figure 6). Considering the OSM layer, the mapping is
shown as a dotted line with an arrow (see Figure 10). This
visual notation helps to separate compositions defined for
software and hardware layers of a system from the assignments of the software layer model to hardware layer
model.

4

DISCUSSION

3.4.2 Model Persistent
Since SESM/DOC supports independent software and
hardware modeling with the capability to synthesize them,
it can be extended with new model types for software and
hardware components that are not defined in DOC. Given
the existence of the models in a database, simulation models can be partially transformed to simulation code. For example, given DEVS/DOC, the automatically generated
primitive and composite software and hardware simulation
models have full structural specifications. Specification of
primitive models are partial. For example, transition and
time advance functions of a primitive model cannot be defined visually or stored in the database.
The separation of concern afforded by SESM/DOC
co-design is important toward model Validation, Verification, and Accreditation (VV&A). As the scale and complexity of models increase, verification and validation becomes more difficult. To reduce the immense effort
required for developing correct models, the SESM/DOC
provides a basis for separately carrying out VV&A for
software and hardware as well as their combination. The
separation affords systematic verification and validation
using the combined logical, visual, and persistent models –
i.e., every model’s structure is guaranteed to be consistent
with the DOC abstract specification. To ensure compliance, the structural model specifications are examined for
correctness given the logical software, hardware, and object system mapping model specifications. However, as
with all other modeling approaches and tools, a modeler
may specify models that are consistent with the DOC abstract model but unsuitable given some desired aspect
and/or resolution of a network system structure and behavior. In terms of simulation modeling (and thus validation),
even though SESM/DOC can generate partial simulation
models, the amount of effort it takes could be significantly
less, especially for large-scale and complex models.

DCO, LCN, and OSM models are stored in a set of relational database tables. The SESM/DOC database schemas
play a major role since it allows storing software and hardware co-design models systematically. The logical DCO,
LCN, and OSM models specified in SESM/DOC are stored
in three sets of tables which are extensions of those defined
for SESM. The Entity-Relationship schemas are defined in
accordance with the software/hardware co-design logical
specification presented above. The different schemas for
the DCO, LCN, and OSM conform to the DEVS/DOC
logical model abstractions. The “ModelType” schema
(i.e., a table in the SESM/DOC database) is introduced to
distinguish among DCO, LCN, and OSM models. The
software layer and hardware layer, and object system
model types and their constraints are also defined as schemas.
3.4.3 Model Consistency
SESM/DOC is devised to ensure consistency among logical, visual, and persistent models. First, it uses the SESM’s
concepts and constructs to guarantee that all visual modeling operations are consistent with the logical model. Second, the consistency is extended based on the DEVS/DOC
models. The SESM/DOC data schemas ensure that the visual models are developed and stored in accordance to their
logical specifications. For example, the LCN link group
model (LGM) can only contain link models (LM). When a
component is added to the LGM, the type of the container
model and the model to be added are checked to have
proper model types. If the added component is a link
model (LM), then the SQL query succeeds. Otherwise, the
query fails and an error message is displayed. As another
example, when a modeler wants to add a router model
(RM) to a link group model, SESM/DOC displays the “A
Link Group Model Can Not Contain a Router Model!”
since this is an invalid operation. In order to allow only
well-defined composite models, the SESM/DOC examines
every coupling and only allows those that satisfy the DOC
specification. For example, a router group model (RGM)

5

CONCLUSION

A co-design modeling approach has been developed for
describing computer network systems that are defined in
terms of combined software and hardware models. It sup-

692

Hu and Sarjoughian
Mathworks. MATLAB/SIMULINK. Available via
<http://www.mathworks.com/>
[accessed March 2007].
ns-2. 2004. The Network Simulator - ns-2. Available via
<http://www.isi.edu/nsnam/ns> [accessed March 2006].
OPNET. 2004. OPNET Modeler. Available via
<http://www.opnet.com> [accessed December 2006].
Sarjoughian, H. 2001. An approach for scaleable model
representation and management. Internal report,
1-11, Arizona State University, Tempe, AZ.
Sarjoughian, H. 2005. A scalable component-based modeling environment supporting model validation. In
Proceeding of 2005 Interserive/Industry Training,
Simulation, and Education Conference, 1-11, Orlando, FL.
Sarjoughian, H., D. Hild and B. Zeigler 2000. DEVSDOC: A co-design modeling and simulation environment. IEEE Computer 3(33): 110-113.
Sarjoughian, H. and R. Flasher 2007. System modeling
with mixed object and data models. In Proceeding
of the 2007 DEVS Symposium, Spring Simulation
Multi-conference, 199-206, Norfolk, VA, USA.
Young, M., J. Cook and L. Mahabadi. 2003. Stochastic local search algorithms for minimizing edge crossings in complete rectilinear graphs. Available via
<http://www.cs.unm.edu/~young/fin
al_report.pdf> [accessed June 2006].
Zeigler, B., P. E. Hammonds 2007. Modeling and simulation-based data engineering: introducing pragmatics into ontologies for net-centric information exchange, in-press.
Zeigler, B., H. Praehofer and T. Kim 2000. Theory of
Modeling & Simulation, New York: Academic.

ports visual modeling according to the distributed object
computing abstract model. With SESM/DOC, computer
network system can be specified in terms of a set of software model components (software model layer) mapped
onto a set of hardware model components (hardware model
layer). The co-design modeling approach is aimed at specifying families of models and also supports generating partial simulation code for DEVS/DOC. This modeling approach is attractive for systems that are being developed to
execute using the Global Information Grid and Service
Oriented Computing frameworks and technologies.
REFERENCES
ACIMS, Arizona Center for Integrative Modeling and
Simulation, Available via <http://www.
acims.arizona.edu> [accessed June 2007].
Bendre, S. and H. Sarjoughian 2005. Discrete-event behavioral modeling in SESM: software design and implementation. In Proceeding of the 2005 Advanced Simulation Technology Conference, 2328, San Diego, CA.
Burmester, H. and S. Henkler 2005. Visual model-driven
development of software intensive systems: a survey of available techniques and tools. In Proceeding of the 2005 IEEE Symposium on Visual
Languages and Human-Centric Computing, Dallas, Texas, USA.
Butler, J. 1995. Quantum modeling of distributed object
computing. Simulation Digest 24(2): 20-39.
Eker, J., W. Janneck, E. Lee, J. Liu, X. Liu, J. Ludvig, S.
Neuendorffer, S. Sachs and Y. Xiong 2003. Taming heterogeneity--the Ptolemy approach. In Proceedings of the IEEE 91(2): 127-144.
Fu, T. 2002. Hierarchical Modeling of Large-Scale Systems Using Relational Databases. Master thesis,
Department of Electrical & Computer Engineering, University of Arizona, Tucson, AZ, USA.
Hild, D. 2000. DEVS-Based Co-Design Modeling and
Simulation Framework and Its Supporting Environment. Ph.D. thesis, Electrical & Computer Engineering, University of Arizona, Tucson, AZ,
USA.
Hild, D., H. Sarjoughian and B. Zeigler 2002. DEVSDOC: a modeling and simulation environment
enabling distributed codesign. IEEE SMC Transactions 32(1): 78-92.
Hu, W. and H. Sarjoughian 2005. Discrete-event simulation of network systems using distributed object
computing hybrid discrete. In Proceeding of 2005
Symposium on Performance Evaluation of Computer and Telecommunication Systems, 884-893,
Philadelphia, PA.

AUTHOR BIOGRAPHIES
WEILONG HU is a PhD candidate in the Computer Science and Engineering department at ASU. His research is
in simulation-based co-design and software engineering.
He can be contacted at <weilong.hu@asu.edu>.
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science & Engineering at ASU and Co-Director
of the Arizona Center for Integrative Modeling and Simulation. His research focuses on multi-formalism modeling,
collaborative modeling, distributed simulation, and software architecture. He can be contacted at
<sarjoughian@asu.edu>.

693

A Component-based Simulator for MIPS32
Processors
Yu Chen
Indigo Digital Press R&D,
Hewlett Packard,
Boise, ID 83714, USA
yu.chen8@hp.com
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling and Simulation,
Department of Computer Science and Engineering,
School of Computing, Informatics and Decision Systems Engineering,
Arizona State University,
Tempe, AZ 85281, USA
hss@asu.edu
Processor concepts, implementation details, and performance analysis are fundamental in computer
architecture education, and MIPS (microprocessor without interlocked pipeline stages) processor
designs are used by many universities in teaching the subject. In this paper we present a MIPS32
processor simulator, which enriches students’ learning and instructors’ teaching experiences. A family of single-cycle, multi-cycle, and pipeline processor models for the MIPS32 architecture are developed according to the parallel Discrete Event System Specification (DEVS) modeling formalism.
A collection of elementary sequential and combinational model components along with the processor models are implemented in DEVS-Suite. The simulator supports multi-level model abstractions,
register-transfer level animation, performance data collection, and time-based trajectory observation. These features, which are partially supported by a few existing simulators, enable important
structural and behavioral details of computer architectures to be described and understood. The
MIPS processor models can be reused and systematically extended for modeling and simulating
other MIPS processors.
Keywords: computer architecture, computer organization, DEVS, MIPS32 processor, modeling
methodologies, simulation

1. Introduction
In a recent report, simulation science was identified as
a complementary paradigm to experimental science [1].
The report identifies simulation as necessary for conceptualization, specification, and the development of complex systems. Aside from research and development, simulation also plays a significant role in teaching. For example, in many computer science and engineering de-

SIMULATION, Vol. 86, Issue 5-6, May-June 2010 271–290
c 2010 The Society for Modeling and Simulation International
1

DOI: 10.1177/0037549709346279
Figures 6, 7, 13–18 appear in color online: http://sim.sagepub.com

partments, simulators have been used to teach the fundamentals of computer architectures. Compared with experiments using hardware, simulation is flexible (easy to
set up, use, and modify) and less expensive. Considering traditional teaching materials, such as schematic diagrams and text descriptions, simulation can significantly
improve students’ learning and assist instructors in teaching, owing to its ability to visualize system dynamics.
When using simulation as a method for understanding,
analyzing, or developing a system, it is important to have
a suitable modeling and simulation framework. With such
a framework, a system’s structures and behaviors can be
precisely formulated as a collection of simulation models,
which can be executed using a provably correct simulator. It is helpful to have an environment that implements
the modeling and simulation framework, and can support
Volume 86, Numbers 5-6 SIMULATION

271

Chen and Sarjoughian

model specification, simulation execution, and user interaction. It is also desirable for the environment to support
describing models in both logical-time and real-time fashion and allow models to be executed on a single or multiple machines. With such an environment, modelers can
concentrate on creating model specifications and simulation experiments that support model verification and validation, instead of spending a lot of time implementing
simulation routines. In the context of education, it is extremely useful for such an environment to provide good
visualization as well. Animation of individual components
and the signals exchanged among them is invaluable for
revealing and understanding complex dynamics of hardware architectures.
The main goal of this work is to develop a MIPS (microprocessor without interlocked pipeline stages) processor simulator that can help students learn and instructors teach the processor designs described in [2] at the
RT (register-transfer) level. Existing computer architecture simulators are developed at different levels of abstraction: hardware components (e.g. [3]), instruction set
architectures (ISAs1 e.g. [4, 5]), and computer systems including processors, memories, I/O, etc. (e.g. [6, 7]). The
scope of this research is within hardware components,
specifically, MIPS32 processors. Existing MIPS processor simulators either do not model processors at the RT
level [3, 8], or lack details and accuracy for studying
processor performance [9, 10]. In this paper, the concepts,
approach, features, and classroom usage of a MIPS32
processor simulator are presented. The simulator provides
RT level animation, collects performance data, supports
multiple levels of model abstraction, and can be reused
or extended to model other MIPS processors. The simulator provides models for the single-cycle, multi-cycle, and
pipeline processor described in [2], and those models are
implemented in DEVS-Suite [11, 12], a JavaTM realization of the Discrete Event System Specification (DEVS)
[13] framework. The DEVS-Suite environment supports
modeling and simulation of component-based systems,
visualizing model structures, and monitoring system
dynamics.
Various modeling and simulation approaches have
been used in existing computer architecture simulators.
Some of them are implemented with basic programming language utilities (e.g. WinMIPS64 [3]), Hierarchical computer Architecture design and Simulation Environment (HASE) [14] and SimJava [15] (e.g. Simple MIPS
Pipeline [16]), and DEVS environments (e.g. ALFA-1
[17]). Hardware Description Languages and their tools,
such as Verilog and Verilog-XL [18], can also be used in
modeling hardware systems, and they are widely used in
simulation-based engineering of application-specific integrated circuits (ASICs). This paper briefly compares
relevant modeling and simulation techniques, and proposes that DEVS-Suite is well suited for modeling and
simulating computer architecture systems, such as MIPS
processors, and provides many desirable features includ272

SIMULATION

Volume 86, Numbers 5-6

ing component-based model development, multiple layers
of model visualization, animation, and reusability.
2. Background and Related Work
The MIPS ISA is widely used in embedded systems today.
It belongs to the reduced instruction set computer (RISC)
family and has many versions, from the original MIPS
I, through MIPS V, to the current MIPS32 and MIPS64.
MIPS I, MIPS II, and MIPS32 support 32-bit registers
and datapaths while the others are 64-bit implementations.
Owing to its simple and elegant design, it has been used
in teaching undergraduate computer architecture courses
at many universities.
In teaching computer architecture courses to undergraduate students, we have found that many students have
difficulties in understanding different processor implementations and their performances. Although there are existing computer architecture simulators, they do not address our teaching objectives sufficiently. The purpose
of this work is to create a simulator that helps students
to study MIPS processor implementations. The simulator
should meet the following requirements.
(R1) It should model the MIPS32 single-cycle, multicycle, and pipeline processor implementations described in [2] at the RT level.
(R2) It should provide statistical data (including execution time, cycles per instruction (CPI), and cycle
count) for performance comparison.
(R3) It should provide animation that demonstrates how
the signals are passed among components during instruction execution.
(R4) It should be platform independent and can be used
on alternative hardware platforms.
Before embarking on creating the DEVS-Suite MIPS32
simulator [12], we first examine the existing tools.
WinMIPS64 [3], EduMIPS64 [8], and Simple MIPS
Pipeline [16] are simulators for pipeline processors. They
focus on modeling functional aspects of pipeline stages instead of real RT level logic components inside processors.
EduMIPS64 is actually a re-design and re-implementation
of WinMIPS64 in Java.
MiniMIPS [9] has a similar goal to this work. However,
it models control units at a higher level instead of treating
them as the composition of RT level components, such as
shifters, arithmetic logic unit (ALU) controls, and multiplexors. These lower-level logic components are important for understanding processor implementations. Also,
their attributes, such as delay, are needed to model processor performances. From the limited resources that we obtained from the authors, it seems that MiniMIPS does not

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Table 1. Comparison of MIPS processor simulators
Simulator

R1

R2

R3

R4

Implementation approach

WinMIPS64
EduMIPS64
SimpleMIPSPipeline
MiniMIPS
WebMIPS
ProcessorSim

N
N
N
Y
N
N

Y
Y
N
N
N
N

N
N
N
N
N
Y

N
Y
N
N
Y
Y

Programming language (C++)
Programming language (Java)
Discrete event modeling (HASE)
Programming language (C)
Programming language (ASP, html)
Programming language (Java, XML)

provide animation, only provides a cycle count as performance data, is implemented in C, and requires Unix machines.
WebMIPS [19] only models a pipeline processor. It
models all of the components inside the processor, and
users can view each component’s input and output data at
a certain time by clicking on the component. However, the
simulator does not provide animation that shows how the
signals are sent and received among components during
instruction execution. In terms of performance data, the
total clock cycles are provided.
ProcessorSim [10] can be configured to model several datapath configurations for the MIPS32 single-cycle
processor implementation [2], and provides animation that
shows how instructions are executed inside processors. It
provides good visualization but has some shortcomings. In
ProcessorSim, only one component can send out messages
at a time. However, components inside a real processor
always work concurrently. ProcessorSim only shows effective execution paths for an instruction execution. That
makes it easier for students to understand the processor
implementations but hides some important details. ProcessorSim does not model component delays and thus can
only support limited performance data. ProcessorSim is
not developed on any modeling and simulation framework, and therefore lacks a rigorous basis for defining
the structures and behaviors of the MIPS components and
their compositions. Thus, extending ProcessorSim to support other processor designs (e.g. MIPS64) is difficult.
Table 1 compares the existing MIPS processor simulators against the four criteria listed above as well as their
implementation techniques. Given the available literature,
the list is not complete: for some simulators no code or
executables were available and for others no articles or
documentation could be found. In the table, the letter N
(for ‘no’) means that the criterion is not met and the letter
Y (for ‘yes’) means the criterion is met. The table shows
that none of the simulators meets all four criteria.
3. Processor Simulator Implementation and
Discrete Event Modeling Approaches
All of the simulators listed in Table 1 are implemented
within some sequential programming language environments (e.g. Java, C, C++, and ASP), except Simple MIPS

Pipeline [16]. Implementing a RT level simulator in a
sequential programming language environment is not a
good choice because a simulator must map the microarchitecture, which is inherently hierarchical and concurrent. This mapping process is laborious and error-prone,
and can often lead to low reusability and extensibility.
Hardware description languages, such as VHDL and Verilog, can also be used to model and simulate MIPS processors. However, as far as we know, their existing simulators lack RT level animation support. Some discrete
event modeling approaches and environments [14, 17]
have been used in processor simulation. These environments have built-in facilities that support modeling and
simulation, receiving and dispatching messages, concurrency and synchronization, and visual animation. With
this kind of environment, productivity can be improved
because modelers can concentrate on modeling the target
systems instead of dealing with simulation routines. Also,
the developed simulation models can be easily reused, extended, and verified, due to their underlying formal theory basis. In the following, discrete event modeling approaches and some related environments are briefly discussed.
3.1 DEVS Modeling Approach and DEVS-Suite
Simulator
A number of discrete event modeling approaches have
been developed since the 1960s. Among them, DEVS
[13] provides a formal theory for describing concurrent
processing and event-driven nature of discrete systems,
such as computer architectures. In this framework, analog (continuous) systems with proven stability properties may also be described and efficiently simulated using the concept of quantized state systems [20]. Furthermore, simulation models can be combined with hardware
[21], thus supporting mixed simulation of a processor with
user applications. Figure 1 provides an overview of the
DEVS simulator framework. DEVS models are separated
from their simulators. DEVS models are developed using
atomic or coupled model specifications, and are executed
by simulators via abstract simulation protocols. Simulators can be executed in logical-time or real-time on a
single processor or multiple (distributed) processors. The
DEVS framework can be implemented in different lanVolume 86, Numbers 5-6 SIMULATION

273

Chen and Sarjoughian

Figure 1. DEVS models and simulators

guages, including C++ and Java. When using a DEVS environment to simulate a target system, modelers only need
to worry about how to create the models, and the simulation part is taken care of by the DEVS environment.
The DEVS modeling approach supports hierarchical
modular model construction and distributed execution. It
uses atomic and coupled model types to describe complex
time-based dynamic systems. Atomic models characterize the structure and behavior of individual components
via input events (X), output events (Y ), states (S), the
external transition function (1 ext ), the internal transition
function (1 int ), the confluent function (1 conf ), the output
function (2), and the time advance function (ta). These
functions define a component’s behavior over time. Internal and external transition functions describe autonomous
behaviors and responses to external stimuli, respectively.
The passage of time is defined with the time advance function, which specifies for what length of time (t 2 13
0 ) the
model will be in a state due to either the internal or external transition function. The model dynamics can be preempted at arbitrary time instances. The output function is
used to generate outputs. Atomic models can be coupled
together in a strict hierarchy to form more complex models. Parallel DEVS (4X3 Y3 S3 1 ext 3 1 int 3 1 conf 3 23 ta5) (see
[22]), which extends the classical DEVS, is capable of
processing multiple input events, multiple output events,
and concurrent occurrences of internal and external transition functions. The parallel DEVS confluent function provides local control by handling simultaneous internal and
external transition functions.
A DEVS coupled model can be constructed by composing models into hierarchical tree structures, and
is defined in terms of its constituent (atomic and/or
coupled) models (i.e. N 6 4X3 Y3 D3 Md 7 d 2
D3 E I C3 E OC3 I C5 (see [22]) [13]). The input and output sets X and Y have the same specification as those of
the atomic model. Here D is a set of component names
and Md is set of atomic and/or coupled components. We
use E I C, E OC, and I C to denote external input, exter274

SIMULATION

Volume 86, Numbers 5-6

nal output, and internal couplings, respectively. The ‘closure under coupling’ feature allows a coupled model to
be used as an atomic model when constructing other coupled models. Coupled models can be constructed systematically using the concepts of ports and couplings between
them. When a component sends messages, the (external
input, external output, and internal) couplings between
input and output ports immediately relay the messages
from the sender to receiver components. Upon receipt of
messages by atomic models, the messages are processed,
which may result in new states and generation of new outputs for other models.
DEVS atomic and coupled models have corresponding abstract simulators. These simulators dictate how the
atomic/coupled models are to be executed according to
the semantics of the Parallel DEVS formalism. A model
receives messages through its input ports and sends out
messages via its output ports. A message consists of a collection of port and value pairs, where port denotes
the destination port and value encapsulates the data object. Ports and messages are the only means by which a
model communicates with its external world.
Computational realizations of the DEVS formalism
and its associated simulation protocols are executed using
simulation engines such as DEVS-Suite [11, 12], which is
built upon DEVS Tracking Environment [23] and DEVSJAVA [24, 25]. DEVSJAVA is an object-oriented realization of parallel DEVS using the JavaTM programming language. DEVS-Suite supports selecting input/output ports
and state variables to be visualized as time-based trajectories and tables. The trajectories and tables can also be
exported as CSV files for post simulation analysis. DEVSSuite can show a model in different views. In the tree view,
a model and its sub-models are displayed as a hierarchial
tree where each tree node represents a model. The input,
output, and states of every atomic model (i.e. a leaf node
in the model hierarchy) can be monitored. Similarly, the
input and output of every coupled model can also be monitored. Simulation animation can be seen from the layout
view, where a model is shown as a rectangle with input
ports on the left and output ports on the right. If the model
is a coupled model, its sub-models can be shown or hidden, depending on its setting set as a white or black box.
This capability allows a modeler to selectively view each
coupled model at the desired abstraction level.
3.2 HASE
Simple MIPS Pipeline has been implemented and simulated with HASE [14]. HASE models must be described
using Entity Description Language (EDL), Entity Layout (EL), and Hase++, and then can be simulated using the Hase++ discrete-event simulator. The EDL file
defines model components, their ports, and links between
the components. The EL file contains information relevant
to the display of the components. The Hase++ files specify

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

the entities’ behaviors. Hase++ is a superset of C++, containing simulation methods that allow entities to change
state, update parameter values, send and receive packets,
hold at certain state for a period of time, etc.
Although DEVS-Suite and HASE are both discreteevent-based simulation and visualization environments
and support parallel execution, they differ in some ways.
HASE uses EDL to model structures and C++ to model
behaviors, while DEVS-Suite uses set-theoretic constructs
and Java to model both structures and behaviors. HASE
provides post-animation. It first generates a trace file from
the simulation, then the trace file is used to provide visualization. However, DEVS-Suite provides run-time animation. It explicitly models external transition, internal
transition, confluent, time advance, and output functions,
while HASE does not. In HASE, packets may be sent
along specific links between entities or between entities
that do not have specific links connecting them. While in
DEVS-Suite, packets can only be sent along links. Another important difference is the ‘closure under coupling’
property of DEVS, which guarantees that every coupled
model has an exact atomic model representation. This
property, which is not supported in HASE, is essential in
developing hierarchial models and distributed execution
of DEVS-Suite models. Finally, HASE, unlike DEVSSuite, is platform dependent, although some of its descendants, such as HASE-III [14] and SimJava [15], are platform independent.

3.3 CD++
A simulator for the ALFA-1 processor has been developed
based on a Cellular DEVS [26]. The simulator provides a
SPARC processor, memory, bus, and input/output models implemented in CD++ [17]. The ALFA-1 and DEVSSuite MIPS32 processor simulators are developed in accordance to the classic and parallel DEVS formalisms, respectively. The DEVS ALFA-1 model specification can
be reformulated and implemented in terms of the parallel DEVS formalism and thus overcome the necessity of
defining the ‘select’ function which requires sequential
execution of the components of a coupled model. These
simulators are targeted for different kinds of computer architectures. Another distinction between them is that the
MIPS32 introduces combinational and sequential components to help with better understanding of the basic concepts of computer organization.
In terms of CD++ and DEVS-Suite, they are different
implementations of DEVS. They both support reusability,
extensibility, and multiple levels of abstraction. As CD++
is implemented in C++, it should be more efficient than
DEVS-Suite in simulating models. As DEVS-Suite is implemented in Java, it is more portable than CD++. For
the class of simulations that are being considered here,
efficiency is less important than portability.

3.4 Summary
In terms of processor simulator implementation, we want
to use an environment that is based on a sound simulation
framework, facilitates rapid model development, handles
multiple levels of model abstraction, and supports simulation model visualization. Also, it is desirable that the
developed models can be systematically reused and extended. Compared with the existing simulators, DEVSSuite is a better choice.
4. Models of the MIPS32 Processors
The single-cycle, multi-cycle, and pipeline MIPS32
processors [2] are modeled using DEVS [13] modeling
approach and implemented in DEVS-Suite environment
[11, 12]. The processors are modeled according to their
physical and functional structures via a bottom-up approach. First, every RT level component inside a processor
is modeled as an atomic model. Additional atomic models are used to model special situations, such as when a
bus is split into several buses and several buses are combined into one. Then, some models are grouped into coupled models according to their functionality to provide
higher level of abstraction. Finally, each processor is developed as a coupled model by composing and coupling
sub-components together.
One major goal of the simulator is to support learning processor implementations discussed in [2]. Thus, the
same assumptions made in [2] are followed by the design
of the simulator, and they are listed below.
8 The single-cycle and multi-cycle processor support the following instructions: lw (load word), sw
(store word), add, sub, and, or, slt (set on less
than), beq (branch on equal), and j (jump).
8 The pipeline processor supports all of the above instructions except j.
8 The delayed branch is not modeled for beq.
8 The multi-cycle processor handles two types of exceptions: arithmetic overflow and invalid instruction.
8 The pipeline processor only handles arithmetic
overflow exception and some data and control hazards.
8 Edge triggered timing is used.
In the following, some related models and classes are
described.
Volume 86, Numbers 5-6 SIMULATION

275

Chen and Sarjoughian

4.1 Basic Components and Helper Classes
A computer processor consists of many basic logic
components, which are either combinational or
sequential. A combinational logic component does
not have memory, and its outputs depend solely on its current inputs. Some examples of combinational logic components are adders, shifters, and multiplexors. A sequential logic component has memory, and its outputs depend
on the current inputs and states. Registers, memories, and
multi-cycle processor control are examples of sequential
logic components. Three abstract models were developed
to model the generic, combinational, and sequential logic
components. Given the Object-oriented realization of the
DEVS formalism [27] and the relationship between DEVS
and Unified Modeling Language (UML) [28], in the following, UML class diagrams and statecharts are used to
show the structures and behaviors of simulation models,
respectively.
The class diagrams for the AtomicCombLogicComp
and AtomicSeqLogicComp DEVS atomic model
types are shown in Figure 2. These combinational and
sequential model components are the specializations from
the AtomicLogicComp model, which provides generic
structure and behavior for every basic element inside a
MIPS processor. The specifications can be done using
set theory, i.e. defining input set, output set, state sets,
external transition functions, internal transition functions,
confluent functions, output function, and time advance
function.
4.1.1 Basic Combinational Logic Components
The AtomicCombLogicComp shown in Figure 2(a)
is an atomic model for the combinational logic component. The input and output for this model are defined as
X 6 94in 1 3 5 5 5 3 in m 6 
 String, Y 6 94out1 3 5 5 5 3 outn 6 

String6, where n and m are finite. This model has two
standard state variables S 6 9phase3 7 , where phase
can have values of passi8e or acti8e and 7 can have
values between zero and prop or 3. Here prop is a
model parameter that indicates propagation delay. It can
be zero, and in that case the model actually describes a
component (such as Fork) that produces outputs instantaneously upon receiving inputs. Initially, the model stays
in phase passi8e for 7 6 3. Upon receiving inputs,
the model’s 1 ext function is invoked to process received
inputs. The external transition function stores the inputs
and enters into the transitory phase acti8e when required
inputs have been received. After staying for a period of
7 6 prop in phase acti8e, the model generates its output using its 2 function and enters the phase passi8e for
7 6 3 due to the execution of 1 int function. The role
of the output function is to send the generated outputs
via one or more output ports. The phase and sigma states
in combination with the 1 ext 3 1 int 3 2 functions as defined
276

SIMULATION

Volume 86, Numbers 5-6

Figure 2. Basic models for the combinational and sequential logic
components

above ensure correct cause and effect ordering (i.e. receipt of input followed by generation of output) for any
of the combinational components. The statechart for the
AtomicCombLogicComp model is shown in Figure 3.
Given the generality of the above AtomicCombLogicComp model, it is used as the skeleton model for
a comprehensive family of combinational logic model
components shown in Figure 2(a). The Control_SCP,
Control_PP, and Control_PPEH components characterize the control components used in the singlecycle processor, pipeline processor, and pipeline processor with exception handling, respectively. The BranchDetectionUnit, ForwardingUnit, and HazardDetectionUnit components are used in the pipeline
processor to handle data and control hazards. The
AbstractALU component describes a generic ALU with
two operand inputs, one operation input, and three outputs: Zero, Overflow, and Results. The Adder_1 component is a special ALU with one fixed operand input (4),
one fixed operation input (addition), and one output (Results). The Adder_2 component is similar to Adder_1,
except it does not have the fixed operand input. The Fork
models the situation where a bus splits into several buses.
Fork1, Fork2, Fork3, and Fork4 model the special cases used in the processor implementations [2]. The
Join models the case where several buses are combined
into one bus.

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Figure 3. Statechart for the AtomicCombLogicComp model

4.1.2 Basic Sequential Logic Components
The AtomicSeqLogicComp is another type of
AtomicLogicComp model. It characterizes a sequential logic component. Different from a combinational logic component, a sequential logic component has memory elements for storing its states. Thus,
AtomicSeqLogicComp uses extra state variables besides standard phase and 7 . Generally, a sequential logic
component has a clock input for writing data. Figure 2(b)
shows the models that are developed for modeling and
simulating sequential logic components. Control_MCP
models the control component used in the multi-cycle
processor implementation. RegisterFile models a
set of 32 general registers while RegisterFile_PP
models the register set used in the pipeline processor implementation. AbstractRegister models a
register that updates its outputs upon the active clock
edge, and has four sub-classes. PipelineRegister
models a register used in the pipeline processor implementation1 Register models a general register1
Register_WC models a register with additional write
control1 and InstructionRegister models the register used for storing the current instruction. Memory describes the memory shared by both data and instructions.
DataMemory and InstrMemory model data memory and instruction memory used by the single-cycle and
pipeline processor implementations.
In the following, the Register component is used
to exemplify an atomic model implementation detail. A
Register has a clock input port, a data input port, and a
data output port. It stores data input when the clock input
is asserted, and updates data output according to the stored
data value on every active clock edge.
For this model, the input and output are defined as
X 6 94C3 I n6 
 String and Y 6 94Out6 
 String. The
state variables are S 6 9phase3 7 3 stateV al Map, where
phase can be either passi8e or acti8e, 7 can have val-

ues between zero and prop (which can be zero) or 3, and
stateV al Map contains stored input prot and val pairs.
Initially, the model stays in phase passi8e for 7 6 3,
and its stateV al Map is empty. Upon receiving inputs, the
model’s 1 ext function is invoked. In the passi8e phase, if
the clock is asserted, the model updates its stateV al Map
with inputs. At an active clock edge in passi8e phase, the
model goes to acti8e phase for a period of 7 6 prop.
Just before leaving the acti8e phase, the model generates
data output using its 2 function. It then enters the passi8e
phase for 7 6 3 owing to the execution of 1 int function.
Figure 4 illustrates these state transitions.
4.1.3 Helper Classes
Some helper classes were developed to implement the
simulator. Assembler translates an assembly file into
machine code. Instruction encapsulates a machine
instruction. Signal encrypts and decrypts messages, and
translates numbers between different formats. In the simulation, all messages are displayed in binary, decimal, or
text strings. A binary string is preceded by a ‘B:’.
4.2 Single-cycle Processor
The single-cycle processor implementation [2] executes each instruction within one clock cycle, and its
schema diagram is shown in Figure 5. In the diagram,
PC, Instruction Memory, Registers, and Data
Memory are sequential logic components, which need a
clock edge for writing but not for reading. The rest of the
components are combinational logic components.
Figure 6 shows a simulation model of the singlecycle processor implementation. In the figure, a rectangle represents both an atomic or coupled DEVS model
with input ports placed at the left-hand side and output ports placed at the right-hand side. An atomic model
Volume 86, Numbers 5-6 SIMULATION

277

Chen and Sarjoughian

Figure 4. Statechart for the Register model

Figure 5. Single-cycle processor control and data path adapted from [2].

278

SIMULATION

Volume 86, Numbers 5-6

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Figure 6. Single-cycle MIPS processor model

Figure 7. Single-cycle MIPS processor model at higher abstraction level

is shown with filled color. Couplings are represented
by lines connecting ports. There are two coupled models, SingleCycleProcessor and InstrFetcher,
which are shown as white boxes with its inner components visible. InstrFetcher is used to group
InstrucMemory and Fork1 together, for easy view-

ing. Each component in the hardware schema is mapped
to an atomic model in the simulation model. Some extra
models, such as Fork and Join, are added to the simulation model to model bus splitting and joining.
Figure 7 shows another simulation model of the
same single-cycle processor at a higher level of abstracVolume 86, Numbers 5-6 SIMULATION

279

Chen and Sarjoughian

Figure 8. Elements of the single-cycle MIPS processor model

tion. According to their functionalities, some components are grouped into three coupled models, InstrFetcher, AddressCalculator, and ControlUnit. The SCP_InstrFetcher model handles instruction fetch. The SCP_AddrCalc model computes
the address for the next instruction. The SCP_CtrlUnit
model provides control signals. These coupled models are
shown as black boxes, so viewers can only see their external not internal behaviors.
Figure 8 is the UML representation of the simulation model in Figure 7. The SingleCycleProcessor
contains seven atomic models: three Multiplexor instances, one DataMemory, one RegisterFile, one
SignExtend, and one ALU. It also has three coupled
models. The SCP_InstrFetcher model has a Fork
and an InstructMemory. The SCP_AddrCalc
model contains ten atomic models. SCP_CtrlUnit consists of a Control_SCP and an ALUControl.
The coupled DEVS specification of the simulation
models in Figure 7 are given in Figure 9.
4.3 Multi-cycle Processor
The MultiCycleProcessor model describes the
multi-cycle processor implementation [2] that executes
an instruction between three and five clock cycles. Figure 10 shows its sub-models. The multi-cycle processor
shares many components with the single-cycle processor, such as ALU, ALUControl, SignExtend, and
ShiftLeft. However, by design, the two processors
have some differences. To model PC, the multi-cycle
processor uses Register_WC that has a write control
signal, while the single-cycle processor uses Register.
The multi-cycle processor contains four Register instances and one InstructionRegister instance to
model the four temporary registers (DMR, A, B, and
ALUOut) and the instruction register, which do not exist
in the single-cycle processor. The multi-cycle processor
280

SIMULATION

Volume 86, Numbers 5-6

uses Control_MCP as the control component, which is a
sequential logic element (see Figure 2(a))1 the single-cycle
processor uses Control_SCP as the control component,
which is a combinational logic element. One Memory
instance is used by the multi-cycle processor, while
one InstructionMemory and one DataMemory instance are used by the single-cycle processor. Also, the
multi-cycle processor uses one more Multiplexor element and a new Or element, and does not use Adder_1
and Adder_2.
The MultiCycleProcessor_EH model describes
the multi-cycle processor implementation with exception
handling [2]. It handles two types of exceptions, arithmetic overflow and invalid instruction. The simulator
only models the steps involved in storing defending instruction address, recording exception cause, and loading
PC with the predefined exceptional handling routine address. The MultiCycleProcessor_EH model component is implemented by adding some components (e.g.
two Register_WC instances are used to model EPC
and Cause register) into the MultiCycleProcessor
model component with some additional input and output
ports.
4.4 Pipeline Processors
The PipelineProcessor model describes the
five-stage pipeline processor implementation [2]. As
shown in Figure 11, three new model components
BranchDetectionUnit, ForwardingUnit, and
HazardDetectionUnit are developed to handle
data and control hazards. Also, four pipeline registers,
Register_IF_ID, Register_ID_EX,
Register_EX_MEM and Register_MEM_WB are
developed for use between the five pipeline stages.
The PipelineProcessor_EH describes the pipeline
processor that can handle arithmetic overflow exception
[2] and is implemented by adding component models (including Register_WC, Multiplexor, and Or) into
the PipelineProcessor model and modifying some
input and output ports.
4.5 Experimental Setup
Experiments can be performed at several levels: the RT
component level (atomic model), the function unit level
(coupled model), and the processor level (coupled model).
In this section, we mainly focus on processor level experiments, which can be conducted through various PEF models, as shown in Figure 12. PEF (Processor Experimental Frame) consists of an ExperimentalFrame and an
AbstractProcessor. The ExperimentalFrame
model supports configuring different experiments. It reads
simulation settings from a property file, which indicates
the assembly file path, output file path, initial register values, and component timing parameters. An example property file can be found in Appendix A.2.

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Figure 9. The coupled DEVS specification of the simulation models in Figure 7.

Figure 10. Elements of the multi-cycle MIPS processor model
Figure 12. A family of experimental frame models for single-cycle,
multi-cycle, and pipeline MIPS processor types

gram execution time, clock cycle time, CPI, cycle count,
and instruction count. Five coupled models, SCPEF,
MCPEF, MCPEHEF, PPEF, and PPEHEF are developed
for conducting experiments with the single-cycle processor, multi-cycle processor, multi-cycle processor with exception handling, pipeline processor, and pipeline processor with exception handling, respectively.
To set up and run a simulation, the following steps can
be followed.
8 Create an assembly file and put its path in the property file.
Figure 11. Elements of the pipeline MIPS processor model

8 Set initial register values, output file paths, and
component timing parameters in the property file.
An ExperimentalFrame consists of the Clock and
Transducer models. The Clock model generates
clock signals. The Transducer collects simulation
data and produces performance statistics, including pro-

8 Optionally, specify when a component should send
outputs in the property file, upon receiving any effective inputs or when the new outputs are different
from the old ones.
Volume 86, Numbers 5-6

SIMULATION

281

Chen and Sarjoughian

Figure 13. Black box single-cycle processor with experimental frame model

8 Start the simulator, choose a model (SCPEF,
MCPEF, MCPEHEF, PPEF, or PPEHEF) to run, observe the component interactions, and adjust simulation speed if needed.
8 Check the results in the specified output file when
the simulation ends.
The single-cycle processor simulation model called
SCPEF is depicted in Figure 13. Every coupled
model can be configured as either a white or black
box. If a model view is set as a white box, its
sub-models are visible. How each sub-model is displayed in turn depends on the sub-model’s view setting. The SCPEF shown in Figure 13 is set as a
white box, so its sub-models, ExperimentalFrame
and SingleCycleProcessor, are visible. However,
these two sub-components are themselves set as black
boxes and thus their sub-models are not displayed.
For another example see Figure 7, where the same
SingleCycleProcessor model is shown as a white
box.
5. Discussion and Results
In the previous section, the structures and behaviors
for the single-cycle, multi-cycle, and pipeline MIPS32
processor models were discussed in terms of the parallel
282

SIMULATION

Volume 86, Numbers 5-6

DEVS atomic and coupled models. Those models can be
executed using the simulator provided by the DEVS-Suite
environment. They may also be simulated in other parallel DEVS implementations. The DEVS-Suite MIPS32
processor simulator [12] provides visualization, performance data, and reusability which have not been fully
supported by the existing MIPS32 processor simulators.
Also, the simulator is available via Java WebStart technology [29], which automatically installs the latest version
of the simulator. In the following, the simulator features,
activities that can improve teaching and learning, and the
classroom usage are briefly discussed.
5.1 Visualization
The DEVS-Suite MIPS32 processor simulator allows visualization at different levels of abstraction. Each basic
RT level component, such as ALU shown in Figure 14,
can be simulated and visualized. The DEVS-Suite GUI
consists of four panels. The top panel has a menu and tool
bar. In the middle of the tool bar, the five icons are shortcuts for Step, Step(n), Run, Pause, and Reset commands,
respectively. The left panel has the tree view of the currently loaded model component, the state of the currently
selected component, simulation control, and the simulator’s state and time. The right middle panel provides the
layout view of the loaded component, where the animation can be observed. In the right bottom panel, various

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Figure 14. Atomic ALU model

information, including console output, tracking log, and
time trajectories, can be viewed depending on the simulation settings. ALU is an atomic model with input ports on
the left and output ports on the right. The binary strings
represent the output from the model, and the value follows. ‘Time of last event’ label indicates the current simulation time, 200 picoseconds (ps). A user can click on the
input ports to inject one or more predefined inputs into
the model, and click the ‘step’ button or icon to observe
the model’s state change (i.e. phase and 7 ) and outputs
generated by the output function. By momentarily placing the mouse on the component, a tooltip will display all
user-defined model states.
A user can also view a model’s inputs and outputs
as well as its states in time-based trend charts. For example, the dynamics of the above ALU components can
be shown as in Figure 15, which displays ALU’s inputs
(operand inputs a and b and operation input ALUOp),
output (result), and state (phase) during a period of
900 ps. For the ease of explanation, all of the numbers
in Figure 15 and the following description are in decimal
format. When the ALU component receives a new operation (set on less than) at t=800 ps, it changes
from phase passive to active. Then data values (10

and 9) are received at t=850 ps, after 100 ps delay,
the output (1) is produced at t=950 ps before the ALU
returns to phase passive. These trajectories are useful
in examining time-based operations of the MIPS components.
Inside a processor, some components that perform a
particular functionality can be grouped together to create
higher-level components, and the simulator supports visualizing composed components as well. For instance, in the
multi-cycle processor implementation, four components
(Control_MCP, ALUControl, And, and Or) are used
to compose MCP_CtrlUnit, which is a coupled model
and shown in Figure 16 with the name ControlUnit.
The couplings among components are shown as lines.
Lines connecting input and output ports, input and input
ports, and output and output ports represent internal coupling, external input coupling, and external output coupling, respectively.
To view the entire processor implementation, a user
can choose the view settings (black or white box) for each
coupled model (see Section 4.2) inside the processor. Figure 17 shows the MCPEF model as it is stepping through
a simulation cycle. Owing to the limited space, we only
show the view inside the layout panel. Two coupled modVolume 86, Numbers 5-6 SIMULATION

283

Chen and Sarjoughian

Figure 15. Input, output, and state trajectories for the ALU model

els MCP_CtrlUnit and MCP_InstrIssuer) are displayed as black boxes. The simulator can be executed in
soft real-time or logical time for many hundreds of millions of simulation steps. The physical time the simulator takes to execute a model is determined by its realtime factor setting and the host computer’s hardware and
software configurations. Running the MCPEF model with
the assembly file and property file provided in the appendix takes about 45 seconds on a computer with the
following software/hardware configuration: Mobile Intel
CPU 2.0 GHz, 1.0 GB RAM, Windows XP OS, JDK 1.5,
and Eclipse SDK 3.2.0. In this simulation, there were
2,403 execution steps with animation of messages turned
off and no trajectories or tabular data visualized. As expected, when all visualizations are turned off including
showing state updates, the simulator’s performance can be
284

SIMULATION

Volume 86, Numbers 5-6

improved significantly. In a recent study [30], it was observed that the execution time is reduced by half when the
simulator’s visualization was completely turned off.
WinMIPS64 [3], EduMIPS64 [8], and Simple MIPS
Pipeline [16] provide visualization for functional aspects
of pipeline stages, not the interactions between RT level
logic components as shown in this section. From the limited resources that we obtained from the authors, MiniMIPS does not support animation. WebMIPS [19] does
not provide animation that shows how the signals are sent
and received among components during instruction execution. Among the related work, only ProcessorSim [10]
provides visualization comparable to this work. However, the visualization provided by ProcessorSim does not
reflect the concurrency accurately, as only one component
can send messages at a time.

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

Figure 16. Coupled MCP_CtrlUnit model

5.2 Performance Data
A processor’s performance is determined by many factors, including processor implementations, component delays, clock cycle time, and applications. The MIPS processor simulator can be used in comparing processor performances. In the following, some simple examples are considered.
In the first experiment, we compare the single-cycle,
multi-cycle, and pipeline processor implementations, with
the same timing parameters (except for clock cycle time)
as shown in Appendix A.2. Three assembly files were executed on each processor, and the simulation results are
shown in Table 2. It is not surprising that the pipeline
processor implementation has the best performance, as it
uses the shortest time to execute all three files. The singlecycle processor implementation takes shorter time than
the multi-cycle processor to execute all three files, which
is counter-intuitive to the general belief that multi-cycle
processors always outperforms single-cycle processors. In
the experiment, the clock cycle time is a dominant performance factor. For the single-cycle processor, each instruction takes one clock cycle to finish, which is equal

to 600 ps. For the multi-cycle processor implementation,
an instruction takes between three and five clock cycles
to finish, which are equal to between 600 and 1,000 ps.
Hence, by using the same experiment settings and varying applications, the multi-cycle processor implementation will always take no less processing time to execute
user-specified applications than the single-cycle processor
implementation.
In the next experiment, we compare the performance
between single-cycle and multi-cycle processors. The
same timing parameters as above are used, except the
memory delay time is reduced from 200 to 100 ns. Thus,
the clock cycle time can be reduced to 401 ns for the
single-cycle processor and 101 ns for the multi-cycle
processor. Two input files are used: File2 from the first
experiment and File4, which has more lw instructions. Table 3 provides the simulation results. The results show the
multi-cycle processor takes shorter time to run File2, but
longer time to execute File4. Here, for an instruction, the
single-cycle takes 401 ps to execute while the multi-cycle
processor takes between 303 and 505 ps to execute. When
the number of instructions are the same, CPI is the dominant performance factor.
Volume 86, Numbers 5-6 SIMULATION

285

Chen and Sarjoughian

Figure 17. Multi-cycle processor model with animation of its input and output messages

Table 2. Simulation Results 1

Execution Time (ns)
Cycle Time (ps)
CPI
Cycle Count
Instruction Count

Single-cycle processor
File1
File2
File3

Multi-cycle processor
File1
File2
File3

Pipeline processor
File1
File2
File3

120.6
600
1
201
201

153
200
3.806
765
201

48.8
200
1.214
244
201

60.6
600
1
101
101

2.4
600
1
4
4

Table 3. Simulation Results 2
Single-cycle
processor
File2
File4
Execution Time (ns)
Cycle Time (ps)
CPI
Cycle Count
Instruction Count

40.501
401
1
101
101

24.461
401
1
61
61

Multi-cycle
processor
File2
File4
38.885
101
3.812
385
101

25.654
101
4.164
254
61

None of the related simulators allow users to study
processor performance by varying processor implementations, component delays, clock cycle time, and applications as we did above. Also, related MIPS processor simulators provide limited performance data. WinMIPS64 [3]
286

SIMULATION

Volume 86, Numbers 5-6

77
200
3.812
385
101

3
200
3.75
15
4

24.8
200
1.223
124
101

1.8
200
2.25
9
4

and EduMIPS64 [8] provide CPI, cycle count, and instruction count, Simple MIPS Pipeline [16], MiniMIPS [9],
and WebMIPS [19] provide cycle count, and ProcessorSim [10] does not provide any performance data.
5.3 Reusability
In our implementation, each RT level physical component inside the CPU is mapped to a DEVS model. Implementing a processor is simply developing the basic models and composing them together. Thus, creating a DEVS
processor model is similar to building a real processor using hardware. As real MIPS processors share many common components (e.g. ALU, multiplexor, and register),
their corresponding DEVS models can also be reused in
DEVS processor models. These models are loosely coupled owing to the DEVS component-based framework,

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

which makes it easier to modify or extend the existing
models to model other MIPS processor implementations.
All of the simulators examined in Section 2, except the Simple MIPS Pipeline [16], lack a componentbased infrastructure. Consequently, in comparison, they
are difficult to reuse or extend.

varying input assembly code, component timing parameters, clock cycle time, etc. Students can then compare the
results from different settings and explain the differences
in the results. Fifth, students interested in research projects
may extend the current processor implementation to support other instructions or functions.

5.4 Accessibility

5.6 Classroom Usage

One design goal of the simulator is to make it easy to access and be platform independent. The source code and
executables for the MIPS models described in the previous section are freely available upon request, and users
can download the code and run on any platform with the
right Java Virtual Machine installed. Also, DEVS-SuiteWS ,
which uses JavaTM Web Start [29], allows users to access the simulator through web browsers without manually installing the simulator [31]. Currently, SCPEF,
MCPEF, and PPEF models can be easily simulated using
simulator at [12].
the DEVS-SuiteMIPS32
WS
Referring to Table 1, ProcessorSim [10] and EduMIPS64 [8] are platform independent. Simple MIPS
Pipeline [16] has a Java Applet interface and thus can lend
itself for online execution. However, the Java Applet did
not run when we tried it. Furthermore, the performance of
Applets is poor for simulation purposes. WebMIPS [19]
can be accessed and executed online. WinMIPS64 [3] and
MiniMIPS [9] simulators are platform dependent since
they are implemented using C++ and C, respectively.

The DEVSJAVA MIPS32 processor simulator was introduced in an undergraduate computer architecture course
(CSE/EEE 230) at Arizona State University in Spring
2008 [32]. The students were required to use the simulator in two exercises. The first exercise was to test the
students’ understanding of the control unit signals for various instructions. The second exercise was based on the
multi-cycle processor design. At the end of the semester,
34 out of 43 students completed the following survey. The
first seven questions ask students to provide responses on
a five-point Likert scale plus ‘Not applicable’, and the last
question is a request for written comments.

5.5 Education Activities
The DEVS-Suite MIPS32 processor simulator can facilitate many kinds of learning. In the following, we briefly
describe some of them. First, to study a basic logic component, such as ALU, students can inject inputs and observe
simulated outputs. They can also use the tracking feature
to view the trajectories of the target component over the
time and examine timing relationships among inputs and
outputs. Then, students can run the simulator for a specific
processor implementation, and observe how that particular logic component interacts with others. Second, to study
how instructions are executed inside a processor, students
can start the simulator with the assembly instructions as
input, step through the simulation, and observe how and
when the signals are generated and which components are
involved in the instruction executions. Third, to help students understand performance differences between different processors, an instructor can ask students to provide
performance data for the same assembly code on different
processors without using the simulator, and then run the
simulator and compare their initial results with the data
provided by the simulator. Fourth, to help students understand performance factors, the instructor can ask students
to conduct several experiments on the same processor by

1. Did you find that the simulator was easy to start and
run?
2. Did the simulation of the single-cycle control unit
increase your understanding of the settings of the
control signals by the control unit?
3. Did you understand the relationship between the
components in the simulator display of the multicycle datapath and the multi-cycle datapath diagram
we studied in the textbook?
4. Were you able to easily identify the state elements
of the multi-cycle design by looking at the simulator
window?
5. When you clicked Step to execute the next step
in the simulation, did you find that the simulation
speed was appropriate?
6. Did the multi-cycle processor simulator increase
your understanding of the multi-cycle control unit
and why the control signals are set to the values they
are?
7. Were the assignment instructions explaining how to
load and use the simulator clear and understandable?
8. Do you have any comments or suggestions which
would be helpful to improve the DEVSJAVA MIPS
simulator?
The responses to the first seven questions are shown
in Table 4, where the numbers in the table represent percentages. Question 1 asks for students’ impression of the
Volume 86, Numbers 5-6

SIMULATION

287

Chen and Sarjoughian

Figure 18. Responses to Questions 1, 2, and 6

Table 4. Student survey results
Likert Scale Ranking

Q1

Q2

Q3

Q4

Q5

Q6

Q7

Strongly agree
Agree
Neutral
Disagree
Strongly disagree
Not applicable

38
56
6
—
—
—

9
59
18
6
6
3

9
53
26
6
3
3

9
53
18
20
—
—

17
56
15
12
—
—

6
47
32
15
—
—

38
50
6
6
—
—

ease of use of the simulator (see Figure 18-Q1). The survey results show 94% of the students thought the simulator was easy to use. Question 2 uses single-cycle processor control unit as a basic component example, and asks
students if the simulator helps their understanding of it.
We found that 67% of the students thought the simulator
helped them, 18% of the students stayed neutral, and 15%
of the students did not find the simulator helpful (see Figure 18-Q2). Question 6 asks students whether the multicycle processor simulator could help them understand a
specific component, the control unit, inside the processor
better. The survey indicates 53% of the students thought
the simulator helped them with 32% of the students mostly
being neutral and 15% of the students did not think the
simulator was helpful (see Figure 18-Q6). The results of
Question 6 is not surprising since the multi-cycle processor architecture has a large number of components and
interactions that inevitably increases complexity of model
visualization.
In their responses on how to improve the simulator,
some students commented that the usability of the simulator decreases when there are many components in the
simulator layout window with too many signals generated
288

SIMULATION

Volume 86, Numbers 5-6

at the same time and links among components crossing
each other. This is consistent with students’ responses to
Questions 2 and 6. When a model is simple, more students found the simulator helpful. When a model is more
complex, some students are overwhelmed with the amount
of information provided by the simulator. Visualization
of complex structures such as the multi-cycle processor
is difficult, and all simulation tools that we are aware
of have similar limitations, although some (e.g. DEVSSuite) use white and black box concepts to reveal details of a complex model at multiple layers of abstraction. When there are many components in a model, some
related components can be grouped into coupled models
and shown as black boxes. If a user is interested in the
sub-component interactions inside a coupled model, they
can observe them in a different context. For instance, to
reduce the visual complexity of MCPEF model, four components (Control_MCP, ALUControl, And, and Or)
are grouped together to build the MCP_CtrlUnit coupled model, which is shown as a black box in Figure 17.
To view the activities happening inside the coupled model,
a user can view the model as a white box, as shown in Figure 16. Also, a user can use the simulator to generate time
graphs (see Figure 15) for the components that they are
interested in.
6. Conclusion
Understanding computer processor concepts is integral to
the education of computer science and engineering students. The MIPS processor designs are used by many instructors to teach fundamental computer architecture principles. Processor simulators complement textbooks and
can significantly improve the learning and teaching experiences of the students and instructors. We have devel-

A COMPONENT-BASED SIMULATOR FOR MIPS32 PROCESSORS

oped a set of combinational and sequential models for the
class of MIPS32 processors using the DEVS modeling
approach. We then implemented these models in DEVSSuite. The resulting MIPS32 processor simulator supports
the single-cycle, multi-cycle, and pipeline processors described in [2]. This simulator, aside from having a theoretical underpinning, provides important features such
as visualization, performance data, reusability, and platform independency, which are either lacking or not well
supported by existing MIPS processor simulators. The results show that the processor simulator executes programs
efficiently and DEVS-Suite is well suited for modeling
and simulating processor architectures for education purpose. Students have found the simulator very useful in
terms of animation of components individually and collectively, generation of alternative simulation data (in tabular form and time-based trajectories), control of simulation, and efficient execution. The simulator demonstrates
that DEVS-Suite provides another alternative for modeling and simulating computer architecture.
The simulator is freely available upon request
and online via Java WebStart technology, which supports automatic simulator installation and updates. The
has two versions. The simple version
DEVS-SuiteMIPS32
WS
supports use of pre-built models and the advanced version
allows users to run their own models. The DEVS-Suite
simulator lends itself to distributed execution, but adding
this capability is unnecessary given the intended use of
the MIPS processor simulations. Future work include obtaining more feedback from students and instructors, improving user interfaces for configuring user-defined inputs (e.g. the assembly and property files), extending the
current models to support more MIPS32 instructions and
modeling of the MIPS64 processor family, and enhancing
the visualization for complex systems through zooming.
Also, we plan to have the simulator evaluated by more
students.
7. Acknowledgments
An earlier version of this paper was reviewed by Peter
Hughes and Lasse Natvig at the Norwegian University
of Science and Technology (NTNU) in Trondheim, Norway and Gabriel Wainer at Carleton University in Ottawa,
Canada, and anonymous referees. We are grateful for their
comments and suggestions which has helped improve the
paper. We also extend our sincere thanks to Kevin Burger
for his use of the simulator in his class and conducting the
survey.
8. Appendix
A.1 Example Assembly File
main:

add $s0, $s0, $t2
slt $s1, $s0, $t3

#increase number value
#check if reached the maximum

exit:

add $t0, $t0, $t1
sw $s0, 0($t0)
beq $s1, $t4, main
lw $s2, 0($t0)

#increase address
#store number at the new address
#if is less than max
#load the new number into $s2

A.2 Example Property File
############ component timing parameters ############
# For components not listed, their delay time is zero
Memory = 200
RegisterFile = 50
ALU = 100
Clock = 600
############ Others ############
alwaysUpdateOuput = false
############ assembly file path ############
asmFilePath = .\\in\\example.asm
############ initial register values ############
$t0 = 512
$t1 = 4
$t2 = 1
$t3 = 10
$t4 = 1
$s0 = -10
############ output file path ############
outFilePath = .\\out\\out.csv

9. References
[1] NSF Blue Ribbon Panel. 2006. Simulation-Based Engineering
Science—Revolutionizing Engineering Science Through Simulation, retrieved August 15, 2006, from http://www.ices.
utexas.edu/events/SBES_Final_Report.pdf
[2] Patterson, D.A. and J.L. Hennessy. 2004. Computer Organization
and Design: The Hardware/Software Interface, 3rd edn, Morgan
Kaufmann, San Fransisco, CA.
[3] Scott, M. 2006. WinMips64, http://www.computing.dcu.ie/mike/
winmips64.html.
[4] Larus, J. 2007. SPIM—A MIPS32 simulator, http://pages.cs.wisc.
edu/larus/spim.html.
[5] Vollmar, K. and P. Sanderson. 2006. MARS: an education-oriented
MIPS assembly language simulator. SIGCSE ’06, ACM Press,
New York, pp. 239–243
[6] Martin, M.M.K., D.J. Sorin, B.M. Beckmann, M.R. Marty, M. Xu,
A.R. Alameldeen, K.E. Moore, M.D. Hill and D.A. Wood. 2005.
Multifacet’s general execution-driven multiprocessor simulator
(gems) toolset. SIGARCH Computer Architecture News, 33(4):
92–99. http://www.cs.wisc.edu/gems/.
[7] SimpleScalar LLC. 2004. SimpleScalar tools, http://www.
simplescalar.com/.
[8] The EduMIPS64 Team. 2007. EduMIPS64, http://www.edumips.org/.
[9] Bem, E.Z. and L. Petelczyc. 2003. MiniMIPS: a simulation project
for the computer architecture laboratory. SIGCSE ’03, ACM
Press, New York, pp. 64–68.
[10] Garton, J. 2005. ProcessorSim—a visual MIPS R2000 processor
simulator, http://jamesgart.com/procsim/.
[11] Kim, S., H.S. Sarjoughian and V. Elamvazhuthi. 2009. DEVS-Suite:
a component-based simulation tool for rapid experimentation and
evaluation. High Performance Computing & Simulation Symposium, Spring Simulation Conference, accepted.
[12] Arizona Center for Integrative Modeling and Simulation. 2008.
DEVS-Suite, http://devs-suitesim.sourceforge.net/.
[13] Zeigler, B.P., H. Praehofer and T.G. Kim. 2000. Theory of Modeling
and Simulation, Academic Press, New York.
Volume 86, Numbers 5-6 SIMULATION

289

Chen and Sarjoughian

[14] Coe, P.S., F.W. Howell, R.N. Ibbett and L.M. Williams. 1998. Technical note: a hierarchical computer architecture design and simulation environment. ACM Transactions on Modeling and Computer Simulation, 8(4): 431–446.
[15] Institute for Computing Systems Architecture. 2007. SimJava,
http://www.dcs.ed.ac.uk/home/hase/simjava/.
[16] Dolman, D. 2007. Computer architecture: aisualization of a
simple MIPS pipeline, http://www.icsa.inf.ed.ac.uk/research/
groups/hase/models/mips/.
[17] Wainer, G. 2002. CD++: a toolkit to define discrete-event models.
Software, Practice and Experience, 32(3): 1261–1306.
[18] Cadence Design Systems, Inc. 2006. Verilog-XL. Retrieved October
25, 2007, from http://www.cadence.com/.
[19] Branovic, I., R. Giorgi and E. Martinelli. 2004. WebMIPS:
a new web-based MIPS simulation environment for computer architecture education. Workshop on Computer Architecture Education, 31st International Symposium on Computer Architecture, Munich, Germany, June 2004, pp. 93–98.
http://www.dii.unisi.it/giorgi/WEBMIPS/.
[20] Cellier, F.E. and E. Kofman. 2007. Continuous System Simulation,
Springer, New York.
[21] Hu, X. and B.P. Zeigler. 2004. Model continuity to support software
development for distributed robotic systems: a team formation
example. Journal of Intelligent and Robotic Systems, Theory and
Application, 39(1): 71–87.
[22] Chow, A.C. 1996. Parallel DEVS: a parallel, hierarchical, modular
modeling formalism and its distributed simulator. Transactions of
the Society for Computer Simulation International, 13(2): 55–67.
[23] Sarjoughian, H.S. and R.K. Singh. 2004. Building simulation modeling environments using systems theory and software architecture principles. Advanced Simulation Technology Conference,
SCS, Washington, DC, pp. 99–104.
[24] Arizona Center for Integrative Modeling and Simulation. 2003. DEVSJAVA, http://www.acims.arizona.edu.
[25] Zeigler, B.P. and H.S. Sarjoughian. 2003. Introduction to DEVS
modeling and simulation with JAVA: developing componentbased simulation models. Retrieved July 21, 2006, from
http://www.acims.arizona.edu/.
[26] Wainer, G., S. Sacicz and A. Troccoli. 2001. Experiences in modeling and simulation of computer architectures in DEVS. Simulation Transactions, 18(4): 179–202.
[27] Zeigler, B.P., H.S. Sarjoughian and V. Au. 1997. Object-oriented
DEVS: object behavior specification. Proceedings of Enabling
Technology for Simulation Science, Orlando, FL, pp. 100–111.
[28] Sarjoughian, H.S. and B.P. Zeigler. 2000. DEVS and HLA: complementary paradigms for modeling & simulation? Transactions

290

SIMULATION

Volume 86, Numbers 5-6

of the Society for Computer Simulation International, 17(4):
187–197.
[29] Sun Microsystems, Inc. 2005. JAVA Web Start Technology.
Retrieved January 21, 2008, from http://java.sun.com/javase/
technologies/desktop/webstart/.
[30] Elamvazhuthi, V. 2008. Visual Component-based System Modeling
with Automated Simulation Data Collection and Observation.
Master’s thesis, Computer Science and Engineering Department,
School of Computing and Informatics, Arizona State University,
Tempe, AZ.
[31] Dairman, C. and H.S. Sarjoughian. 2006. DEVSJAVA WebStart. Retrieved June 15, 2007, from http://acims1.eas.asu.edu/
DEVSWebStart/.
[32] Sarjoughian, H.S., Y. Chen and K. Burger. A component-based visual simulator for MIPS32 processors. Frontiers in Education,
Saratago, Spring, 2008, IEEE, New York, pp. F3B–9–F3B–14.

Yu Chen is a software engineer at Indigo Digital Press R&D,
Hewlett Packard. She received her PhD degree in Computer Science from Arizona State University in 2006. She worked in many
industry fields including telecommunications, online learning,
embedded system, and digital press web applications. Her current research interests are in software product lines, modeling and simulation, software process management, and software
testing.
Hessam S. Sarjoughian received the PhD degree in electrical and computer engineering from University of Arizona, Tucson, in 1995. He is Associate Professor of Computer Science
and Engineering at Arizona State University in Tempe, Arizona.
Sarjoughian is Co-Director of the Arizona Center for Integrative Modeling and Simulation (ACIMS). His research focuses
on model composability, visual modeling, service-oriented simulation, and distributed co-design modeling. He led the development of the Online Masters of Engineering in Modeling and
Simulation in the Fulton School of Engineering at ASU in 2004.
He was among the pioneers who established the Modeling and
Simulation Professional Certification Commission in 2001. His
research has been supported by NSF, Boeing, DISA, Intel, Lockheed Martin, Northrop Grumman, and US Air Force.

Proceedings of the 2014 Winter Simulation Conference
A. Tolk, S. Y. Diallo, I. O. Ryzhov, L. Yilmaz, S. Buckley, and J. A. Miller, eds.

DEVELOPING COMPOSED SIMULATION AND OPTIMIZATION MODELS USING ACTUAL
SUPPLY-DEMAND NETWORK DATASETS
Gary W. Godding
Daniel R. Peters
Victor Chang

Soroosh Gholami
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
Computing, Informatics & Decision Systems Engr.
Arizona State University
Tempe, AZ 85281, USA

Supply Chain Intelligence & Analytics Group
Intel Corporation
Chandler, AZ, 85226, USA

ABSTRACT
Large, fine-grain data collected from an actual semiconductor supply-demand system can help automated
generation of its integrated simulation and optimization models. We describe how instances of Parallel
DEVS and Linear Programming (LP) models can be semi-automatically generated from industry-scale
relational databases. Despite requiring the atomic simulation models and the objective functions/constraints
in the LP model to be available, it is advantageous to generate system-wide supply-demand models from
actual data. Since the network changes over time, it is important for the data contained in the LP model to
be automatically updated at execution intervals. Furthermore, as changes occur in the models, the
interactions in the Knowledge Interchange Broker (KIB) model, which composes simulation and
optimization models, are adjusted at run-time.
1

INTRODUCTION

Simulation methods are used for modeling and simulation of supply-demand networks for predicting
operations schedules and costs. For example, optimizing large-scale semiconductor supply-chain systems
is very complex and the number of configurations of their processes and parameters are many. Therefore,
there has been interest in using optimization modules for optimizing the operation of simulation. In this
paper, we consider an actual semiconductor manufacturing supply-demand system (aka supply-chain
system). Efficiency in the management of such systems can reduce costs by many millions of dollars each
year (Wu, Erkoc and Karabuk 2005, Kempf 2004).
Semiconductor supply-demand networks are a collection of suppliers, inventories, processes,
transportations, and customers (Kempf 2006). Raw material is processed in sequential and parallel stages
to produce many different kinds of products to customers. This enterprise can be divided to manufacturing
processes and decision planning parts. The key variables of interest for discrete processes and logistics
include stochastic processing times in building products in multiple stages and in different geographies.
Another important variable is inventory holdings across manufacturing plants and logistics stages. Both the
scale and complexity in manufacturing requires complex decision making, often supported with linear
programming where reduced cost and just-in-time (raw material, semi-finished goods, packaging, etc.)
delivery across the supply-demand is highly sought after. To simulate the operation of such dynamic
enterprises, we can use discrete event modeling specifications such as Discrete Event System Specification
(DEVS) and optimization modeling such as Linear Programming (LP) methods. Two important factors in
creating realistic models of such enterprise systems are scale and model integration. Manufacturing has
many tens of processes and inventories with alternative source to target routes and shipping modes.
Similarly, logistics has many inventories and hubs for transportation and delivery to customers in different
geographies. Simplifying generation of these simulation models is attractive. Considering the optimization
978-1-4799-7486-3/14/$31.00 ©2014 IEEE

2510

Gholami, Sarjoughian, Godding, Peters, and Chang
model, it uses Bill-of-Materials (BOMs) and the manufacturing/logistics state information to forecast
desired inventory holdings and shipping routes for materials and products. The optimization model requires
an abstract model of the manufacturing/logistics supply-chain. The LP model needs to find an optimal
network configuration of the supply-chain having hundreds of thousands of possible flow paths. Therefore,
automatic generation of these possible networks is useful. Furthermore, a Knowledge Interchange Broker
(KIB) model composing DEVS and LP model has a few hundred data transformations that must be executed
at time intervals the simulation and optimization models interact (Huang, et al. 2009). The KIB concept and
its use in supply-demand networks are described in previous work (Sarjoughian 2006, Huang, et al. 2009).
In this paper, we describe our research for creating (instantiation, parameterization, and initialization)
of DEVS and LP from actual data, supporting the integration of these models with the Knowledge
Interchange Broker (KIB) model which interacts with the DEVS and LP models, and performing
experiments using these models. A database containing actual Intel’s supply-demand network dataset is
integrated into the Optimization, Simulation, and Forecasting (OSF) platform (Sarjoughian, Smith, et al.
2013). We note that forecast modeling is excluded in this work. In model creation we take on the problem
of automatically generating manufacturing/logistics processes and formulating decision plans using the
database and the generated simulation model. The work in this paper was delivered as an Industrial Case
Study presentation at the Winter Simulation Conference (Gholami, et al. 2013).
2

RELATED WORK

The significance of automatically generating simulation and optimization models for the entire supplydemand network for the production of semiconductor is well documented (Missbauer and Uzsoy 2011,
Fordyce, et al. 2011). Automatic model generation is invaluable for large, complex supply-demand
networks where many hundreds to thousands of products are manufactured in dynamical settings and stages
under varying demand forecasts. One of the early works is IMPreSS (Leachman, et al. 1996), an automated
production planning and delivery quotation system. A more recent work uses mixed-integer programming
where optimization for a supply-chain process is driven by heuristics (Denton, Forrest and Milne 2006).
In another work, discrete-event simulation is used to model manufacturing process and linear programming
to optimize the operation of the supply-demand network using heuristics forecast modeling (Sarjoughian,
Smith, et al. 2013).
From the standpoint of automatic simulation and optimization model generation, it is useful to generate
them automatically. This need has been recognized from the early days of simulation (Oldfather, Ginsberg
and Markowitz 1966). Efforts have been made to increase portability, maintenance, and modifiability of
models via automatic model generation from databases (Taylor and Taha 1991). If we restrict autonomous
instantiation to simulation models (as opposed to application instantiation (Stauber, Ambrose and Rothwein
2003)), there are two common ways to automatically generate simulation models. One is to use a descriptive
model of a system with pre-built components and then generate simulation models. Second is to generate
simulation models by parameterized and compose simulation models from pre-built components using data
collected from an actual system. As an example of first way, automatic simulation model generation is
proposed and exemplified for a shop floor resource model (Son, Wysk and Jones 2003). The source of
models for autonomous model generation may be a database or documents supporting ASCII and XML
formats.
Automatic model instantiation, parameterization, and initialization which we have developed follows
the second way. This is similar to the work in (Jeong and Allan 2004) where simulation models are
automatically generated from a dataset belonging to a discrete manufacturing system. In our case, data
gathered from the real supply-demand network along with existing models are contained in a set of database
tables which are then used to generate coupled simulation models. Actual data is used to parameterize
atomic and coupled model components. From the standpoint of automatic simulation model generation, the
difference is in the scale and the complexity of the models (i.e., databases). In our work, we use an

2511

Gholami, Sarjoughian, Godding, Peters, and Chang
optimization model which requires generating network flow graphs for the optimization model. This is
achieved using the same database that is used for generating simulation models.
3

SUPPLY-DEMAND SIMULATION AND OPTIMIZATION

In this section, we summarize the effort made to model an actual supply-demand network in DEVS and
DEVS-Suite. The modeled network consists of four Fabrications (FAB), two of each: Die Warehouses
(DW), Assembly Test Model (ATM), and Customer Warehouse (CW). The network also possesses 9
Vendor Managed Inventories (VMI) and more than 200 customers. A high-level view of the network is
depicted in Figure 1.

Figure 1: A high-level view of the simulated supply-demand system.
Each facility in the supply chain is modeled as either an atomic or coupled DEVS model. A coupled
model may have several atomic/coupled sub-models. Fabrication sites are specified as atomic models which
produce Fab products. These products are then delivered to DWs (which similar to ATM contain three
inventories and processes inside them) in which they are split in order to be processed for different targets
(high performance processors, energy efficient, etc.). ATM sites further split the products and then process
them into FG (Finished Good), ready to be handed to the customers. These finished goods are stored in
CWs in big entities and then shipped to VMIs and customers in smaller proportions.
Each of the DW and ATM facilities (containing inventories and processes) are coupled models
containing several internal atomic models for themselves. The internal view of the both facilities are
presented as coupled models with circles indicating inventories and rectangles indicating processes. All
inventory and coupled models have control-in port with which receives commands for releasing products.
These commands may come from a database (historic data) or be generated by an strategic controller as is
the case in this research. A strategic controller manages the operation of the system by considering the state
of the network, the future demands/supplies, and penalty costs. It controls the operation of the network by
sending commands to each individual facility. Notice that commands are for inventories. Inventories store
products until receiving a command to release them. This is not the case for processes as they immediately
release the products that are processed and ready.
Although the high-level view of this network does not seem to be large, however, DEVS adaptation of
such a system contains 620 atomic and coupled models and more than 1000 couplings between them.
Building such a system statically is not scalable and impractical. Therefore, we leveraged automatic model
generation to handle instantiation, initialization, parameterization, and coupling of all these models. This is
further explained in the rest of the paper. The overall view of the simulation/optimization system is depicted
in Figure 2. The reader can identify 6 major components in this simulation which are further discussed in
Section 0.
The Supply-Demand Network contains all simulation models and their interactions among one
another. Each component has an outgoing port (status-out port) via which state information of the
component is sent out. Decision Connector is in charge of receiving state information from the simulation
models and send them over to the Knowledge Interchange Broker (KIB). Also, it distributes the commands

2512

Gholami, Sarjoughian, Godding, Peters, and Chang
it receives from the KIB and LP module among simulation components. Dataset stores various sorts of
information for instantiation, parameterization, initialization, and testing the network. This component is
thoroughly introduced in Section 4. Network Generator is a data structure which provides structured data
to the LP module. The static structure of the network is formed into a graph and sent to the LP along with
dynamic information such as the current stock, demand (in the horizon of one several months), etc.
LP/CPLEX Optimization Module is designed and implemented using CPLEX, this module receives the
input graph (network generator) and optimizes it (find a solution) which maximizes the profit and minimizes
the penalty of missing customer demands. The Knowledge Interchange Broker works as a mediator as
mentioned earlier. The interaction between different parts of the system is modeled and implemented in the
KIB. These models, in addition to the type conversion, provides time synchronization, aggregation,
disaggregation, broadcasting, etc.

Figure 2: An illustration of the integrated simulation, optimization, and interaction models.
3.1

Issues and considerations in supply-demand modeling and optimization

Although Figure 1 may not show the actual size of the simulation model, as described in Section 6 it is
quite large. Instantiation of a model with this size is tricky especially when scalability and modifiability are
of importance. Here, we need a scalable method for instantiating, parameterizing, and initializing the
network which reads the structure and parameters from a database or a configuration file and construct the
network in run time. For company level supply-demand problems with several hundred components and
changing topology (from one experiment to the other) we incorporated a similar method. In our case, the
network is constructed from a dataset and is dynamically parameterized and initialized after instantiating
its components (see Section 6).
Second, is the formulation of the optimization problem from network state variables. In order to
optimize the operation of the network based on network parameters and state variables, one has to find a
suitable formulation which can be automatically created and passed on to the LP side. On the reverse
direction, the optimized operation should be translated back to the supply-demand domain as control
commands. For this purpose, we need a data structure which is created at run-time, stores all structure and
state information of the network, and passes them on to the LP module (see Section 0).

2513

Gholami, Sarjoughian, Godding, Peters, and Chang
As real processes have stochasticity in their operation, the DEVS models must present the same level
of stochasticity to accurately imitate the behavior of the physical system. Since we aimed at realistic
modeling of the network, accurate behavior of each site/process/inventory was investigated from real
historic data and embedded into the model. The stochasticity is formulated as mathematical distribution
functions and incorporated as processing time or shipping time for related components.
Finally, validation of the simulation models should be taken very seriously. If the simulation diverges
from the plan (expected amounts) it could be because of the errors in the simulation module or the incorrect
commands that come from the control module. Moreover, when such a divergence occurs the gap gets
greater over time and it becomes impractical to find the source of the error. Therefore, each simulation
model must be extensively validated before being used in the system.
4

SEMI-AUTOMATIC MODEL INSTANTIATION, PARAMETERIZATION, AND
INITIALIZATION

Automatic model construction is defined according to the dataset (see Figure 2). The dataset stores the
structural configuration and parameterization of the network in several tables which are used by the highestlevel coupled model for instantiation and coupling. After that, each model has to be parameterized in terms
of processing/shipping times (in terms of distribution functions), penalty costs, etc. The most important
parameter is BOM (bill of material) which specifies how products are converted to one another in split and
assign processes. In the initialization phase, each component is initialized with products of different types.
Among all tables in the database, we focus on tables Site, Intransit_tpt, Process_tpt, Products, and BOM.
Each record in the Site table specifies one facility of the network. So, instantiation of simulation components
is done with the assistance of this table. The coupling of these facilities are determined by the Intransit_tpt
table. This table specifies a shipping (which is a component in the simulation model) by recognizing its
source/destination facilities and a random distribution function (specified by a string representing the type
of the distribution and at most five number parameters) for the shipping time. Based on the type of the
distribution, any number of parameters may be used; for example, a triangular distribution uses 3
parameters. The facilities and their connections fall into the category of static structure of the network. For
parameterization, in addition to Intransit_tpt (which parameterizes shipping components), we have
Process_tpt and BOM which specify the processing times and bill of material (product split and assign
processes), respectively. Both of these tables need references to the Products table which stores all product
names and their stage in the chain. Each record in Site is associated with several records in Process_tpt
which specify the processing times (distribution functions) for each type of product.
For instantiating, several methods have been developed and used. Coupled models such as ATM and
DW are constructed using makeATM and makeDW methods which not only instantiate all inner elements
of ATM and DW in the coupled model but also couple them together and connect the control_in and
control_out ports to those of the coupled model. The novelty of this kind of model generation is the level
of flexibility it provides. For creating a network with a completely different topology or parameters, one
needs only to deal with a database (provided by the company owning the supply chain) which is much more
convenient than a flat file or changing the code. In addition, the stages inside coupled models can also be
determined from the dataset. In other words, there is no need to define configuration of the internal
components of DW and ATM (i.e., composition of model components)as these can be identified from the
dataset. It is assumed the dynamics of the model components are well-defined in terms of creating coupled
models. As for parameterization of the models, a method goes over all parameter records in the dataset,
find its associated model (by name), and set the parameter inside the simulation model.
One benefit of our approach for dynamic model instantiation, parameterization, and initialization is that
the topology or parameters can be changed over time. These topology/parameter changes are then
dynamically applied to DEVS models (which can be modified at runtime). Therefore, the simulation can
change its most fundamental foundations in runtime. Although we are not using such a capability for

2514

Gholami, Sarjoughian, Godding, Peters, and Chang
supply-demand networks, one can certainly find suitable applications for it. The dataset holds various
product types, processing configurations, historic data, and structural information.
5

OPTIMIZATION PROBLEM FORMULATION

In order to transform structure and state information into LP format (for optimization) we require an
efficient and dynamic data structure which can be manipulated at every cycle and sent to the LP. The static
structure encapsulates all the static characteristics of the network such as topology, BOM, product types,
etc. A graph is designed with weights on its arcs to represent the static structure. Later the state information
will be added to this graph. In order to provide the structure (static) to the LP, several methods are
implemented which encode the network into the graph. Since each facility (FAB, DW, ATM, CW, and
Shipping) has its own characteristics and formulation each one of them has its own structure encoder
method. The pseudo code presented in Figure 3 are for adding the static structure of those facilities.

Figure 3: Adding static structure for Shipping, ATM, DW, FAB, and CW.
For all components in the network except FAB, we need two nodes as in and out. These nodes are
connected to upstream and downstream nodes via Shipping. In ATM, DW, and CW the two nodes are
connected to a middle node. Furthermore, ATM and DW specify the percentage of product conversion
on the arc. Figure 4 provides a visual example on how the structure information of a DW is transformed
into the graph. This process receives wafers as input products (W1 or W2 ) and outputs six types of dies.
For splitting the wafers it has n different BOMs (or recipes). Therefore, a graph is generated with two
starting nodes (W1 and W2 ), each of the connected to n different BOMs. The BOMs are then connected
to the output products based on their split configuration. For example, consider BOM1 which converts

2515

Gholami, Sarjoughian, Godding, Peters, and Chang
W1 to DP1 , DP3 , and DP4 in 40%, 40%, and 20% proportions, respectively. So, BOM1 node is connected
to DP1 , DP3 , and DP4 with their split percentage set as the weight of the outgoing arc (the thick lines).
The dataset we are currently using contains 460 product types, 400 BOMs, 800 various processing
times, 250 sites, 340 shipping elements, and more than 50000 demands for a full year. Translating this
into a optimization problem (using the approach described above) results in extremely large graphs, each
containing few hundred thousand nodes. The entire graph is handed to the CPLEX/LP module for
optimization.
The state information (current stock, currently under process, supply, and demand for the next 30
days) are added to the graph as weighted arcs between nodes. Supply information are added to the
outgoing arc from FAB to DW. Similarly, current stock in every inventory is added as weighted arc
between the inventory and the following process/inventory. Therefore, the addition of state information
to the graph does not change the structure of the graph; instead it adds new arcs to it.

Figure 4: Transformation of a process with input and output products with BOMs into a flow graph.
6

SIMULATION MODEL AND OPTIMIZATION MODULE INTEGRATION

Our ultimate product in this research is an integrated simulation/optimization platform in which the
optimization module manages the operation of the network. For this purpose, automatic simulation model
instantiation and parameterization was designed and developed. Also, formulating the LP problem from the
parameters and state variables of the simulation was another problem that was addressed in this research.
This optimization module receives general information from simulation components and optimizes their
operation by sending control commands to each facility via their control-in port. In addition to the
simulation and optimization modules, a mediator must model and manage the interaction between the two.
This module is called Knowledge Interchange Broker (KIB).
The discrete event simulation contains supply-demand models (e.g. Fabrication, Assembly Test,
Customer, Warehouse, etc.). The optimization module (e.g. control strategy) receives information such as
the supply, customer demand, current stock of each inventory, penalty costs, etc. as input and optimizes the
operation of the supply-demand network by running a CPLEX optimization program on the state
information it received. Commands are generated and sent back to the simulator to control the operation.
All these interactions have to go through the KIB. The KIB receives state and structure information from

2516

Gholami, Sarjoughian, Godding, Peters, and Chang
the simulation module and formulates it as an LP problem. In Figure 2, we can identify six major
components for this integrated environment. The supply-demand network simulation, automatic model
construction via the dataset, and the optimization module (along with the network generator) are described
in Sections 3, 4, and 5. In addition to these, the Decision Connector component is in charge of receiving
state information from the simulation models and sending them to the KIB. Also, it distributes the
commands it receives from the KIB among simulation components. It has logic to aggregate or disaggregate
state information and commands when needed. The concept of the KIB comes from poly-formalism in
which the KIB (as a separate formalism) defines and manages interactions between two other formalisms.
Each interaction specifies how to transform data from one model type to another model type. Some or all
interactions are executed with respect to time and under a control scheme instructing the order in which any
two distinct model types can execute with respect to one another. With all these components implemented
and integrated with each other, our platform is ready. In the following section selected experiments
conducted using this platform are described.
7

EXPERIMENTATION

We conducted 3 sets of validating experiments on this platform: Single-chain, historic data, and systemwide. In the single-chain experiment, we intend to test facilities under the most simplistic scenarios to make
sure their logic is implemented correctly. In historic data experiment, we validate the operation of the
simulation models by testing and comparing them with historic data. We expect to see rough consistency
between the output of our simulation and the historic data. Finally, in the system-wide experiment, all
components of the system are included and we carry out end-to-end testing. The results are provided below.
Please notice that due to the sensitivity of the data, details are taken out (product names, site names,
quantities, and timestamps).
7.1

Single-chain validation

To simplify the simulation as much as possible, we manually modified the dataset to instantiate a single
chain of facilities (one of each Fab, Process, Assembly, CW, and VMI) with shipping between them. Also,
we made all processing and shipping times deterministic. In addition to all these, this experiment is done
in a simulation standalone mode; meaning there is no optimization, KIB, or network generator. All
commands are read from the database with stochasticity removed.
Then, the experiment is based on a single demand forecast and supplying enough raw material for
meeting the demand. We make sure that the demand is supplied on time and the flow of semi-finished and
finished goods in the chain occurs at right timestamps and with correct amounts. In the single chain supply
network there is only one customer and one of each other facility. Also, a clock module synchronizes the
time between all simulation models and an experimental frame (EF) collects data from all models. We
validated the basics of our simulation model with this experiment and observed that all events happen at
the right time, quantities are as expected, and the product split and conversions are done without error.
7.2

Historic data validation

In this experiment, we include stochasticity in the simulation model but without integrating it with the LP
and the KIB models. Instead, even the release commands are read from the dataset (regardless of the current
state). Also, for this experiment, the complete network is used instead of a single chain as in the previous
example. The results are then gathered using a EFG and reported in by-product by-site basis. These results
are compared with the expected outcome (the data gathered from the real world again in the dataset) and
can be reported in terms of plots as is done in Figure 5.
We expected to see small difference in the plots (as it can be seen in the left plot) because of the
stochasticity in shipping and processing times. We gathered the data for all products and sites for a year of

2517

Gholami, Sarjoughian, Godding, Peters, and Chang
simulation (with historic data records for one year) and compared the results. Similar to the examples shown
in Figure 5.

Figure 5: Validating simulated outputs measured against historical data by product and site .

7.3

System-wide validation

In this set of experiments, we used the system shown in Figure 2. Instead of receiving release commands
from the dataset, state information is sent to the LP side (via the KIB) and then release commands are
generated and sent to the simulation models. For such system, several validation scenarios were necessary.
In one set of experiments, the overall operation of the system is test by comparing LP expectations and
simulation outcomes. Results for all of these experiments are provided below.
KIB validation: the KIB is validated by conducting several experiments with and without the KIB and
comparing their results. Validation of the KIB serves to show that the data transformations, timing and
scheduling are correct. In a simulation without the KIB, the transformation of data and time is done
manually (not scalable and static). Among those conducted we include one of them (specific to one product
and one site) here (see Figure 6). The cumulative view is presented in the top right corner.

Figure 6: Per period and cumulative release quantities for product x at site y with and without the KIB.
Overall operation of the platform: in this set of experiments, we compare the actual output with the
excepted output of the LP. This would give us a rough estimate of how the system components work
together and whether the harmony required to be established by the KIB exists in the system or not. Again,
the simulation was executed for a full year and extensive amount of data was gathered. The data is then

2518

Gholami, Sarjoughian, Godding, Peters, and Chang
analyzed both with R and Excel to compare LP expectations and simulation outcomes. Figure 7 is the plot
generated from our data.
Figure 7 shows the expected and actual release of product x at site y and their absolute difference (top
right corner). This experiment, along with many more, demonstrates that the overall
simulation/optimization platform has the required accuracy to be used in practice.
We developed this simulation environment depicted in Figure 2 in Java, using DEVS-Suite 2.1.0 simulator,
IBM ILOG CPLEX Optimization Studio 12.5, and Microsoft SQL Server 2012. All the above experiments
are carried out on 64bit OS using Java 7. The platform uses Windows 7 with a 2.9GHz Intel Core 2 Duo
processor and 8GB of DDR3 physical memory. The minimum, average, and maximum execution times for
simulating one week are 8.61, 62.29, and 28.30 seconds. The total execution time for 59 weeks is 131.8
minutes.

Figure 7: Simulation output (actual) vs. optimized expectation (expected); Absolute difference between
simulated output vs. optimization expectation.
8

CONCLUSION

Developing realistic models for supply-demand systems is time consuming. This is particularly true when
the goal is to produce high-fidelity simulation studies based on fine-grain characteristics and dynamics of
discrete processes and logistics. Equally important to such studies are use of actual decision models that
can optimize dynamics of manufacturing over long time horizons. A well-known, common challenge for
both of these modeling efforts is scale of the system. This severity of this problem can be reduced if the
models can be generated with help from data collected from an actual enterprise. Assuming we have access
to models developed for the atomic parts of the manufacturing supply-demand system from which data is
available, then very large system-wide models can be automatically generated. We extended the DEVSSuite simulator with algorithms and database connectivity to generate system-wide simulation models from
actual data. Although such models are created prior to simulation, this is not necessary provided changes
in the topology of the supply-demand is available in the database. For the optimization model, algorithms
were developed to generate product flow networks. This capability is important as it allows the optimization
model to account for changes in Bill-of-Materials that can occur, for example, in weekly intervals.
Regularly updated LP models may reduce computational time for finding optimal inventory holdings and
shipping routes. Finally, bi-directional interactions between DEVS and LP models via the KIB model is
also updated which can improve efficiency of the simulation experiments. Current research includes
extending our simulation/optimization platform to support customer demand forecasting (Sarjoughian,
Smith, et al. 2013) with diagnostics capability.

2519

Gholami, Sarjoughian, Godding, Peters, and Chang
ACKNOWLEGEMENT
We express our thanks to the anonymous referees including Dr. Ken Fordyce. Their comments and
suggestions helped improve the focus of this paper. This research is supported by Intel Research Council.
REFERENCES
Denton, B. T., J. Forrest, and R. J. Milne. 2006. "IBM solves a mixed-integer program to optimize its
semiconductor supply chain." Interfaces 36 (5): 386-399.
Fordyce, K., C. T. Wang, C.-H. Chang, A. Degbotse, B. Denton, P. Lyon, R. J. Milne, R. Orzell, R. Rice,
and J. Waite. 2011. "The ongoing challenge: creating an enterprise-wide detailed supply chain plan
for semiconductor and package operations." In Planning Production and Inventories in the
Extended Enterprise, by K. G. Kempf, P. Keskinocak and R. Uzsoy, 313-387. New York: Springer.
Gholami, S., H. S. Sarjoughian, G. W. Godding, V. Chang, and D. Peters. 2013. "Independent Verification
& Validation of Integrated Supply-Chain network Simulation and Optimization Models." Winter
Simulation Conference-Industrial Case Presentation. edited by R. Pasupathy, S.-H. Kim, A. Tolk,
R. Hill, and M. E. Kuhl. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers,
Inc.
Huang, D., H. S. Sarjoughian, W. Wang, G. W. Godding, D. E. Rivera, K. G. Kempf, and H. Mittelmann.
2009. "Simulation of semiconductor manufacturing supply-chain systems with DEVS, MPC, and
KIB." Semiconductor Manufacturing, IEEE Transactions on 22 (1): 164-174.
Jeong, K. Y., and D. Allan. 2004. "Integrated system design, analysis and database-driven simulation model
generation." Proceedings 37th Annual Simulation Symposium. IEEE. 80-85.
Kempf, K. G. 2004. "Control-oriented approaches to supply chain management in semiconductor
manufacturing." Proceedings of the American Control Conference. IEEE. 4563-4576.
Kempf, K. G. 2006. "Complexity and the Enterprise." International Conference on Potentials of Complexity
Science. Collegium Budapest. 248-432.
Leachman, R. C., R. F. Benson, C. Liu, and D. J. Raar. 1996. "IMPReSS: An automated productionplanning and delivery-quotation system at Harris Corporation—Semiconductor Sector." Interfaces
26 (1): 6-37.
Missbauer, H., and R. Uzsoy. 2011. "Optimization Models of Production Planning Problems." In Planning
Production and Inventories in the Extended Enterprise, by Karl G Kempf, P Keskinocak and R
Uzsoy, 437-508. New York: Springer.
Oldfather, P. M., A. S. Ginsberg, and H. M. Markowitz. 1966. Programming by questionnaire: How to
construct a program generator. RAND Report RM-5129-PR, Santa Monica.
Sarjoughian, H. S. 2006. "Model composability." Proceedings of the 38th conference on Winter Simulation
Conference. edited by L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M.
Fujimoto. Monterey, CA. 149-158. Piscataway, New Jersey: Institute of Electrical and Electronics
Engineers, Inc.
Sarjoughian, H. S, J. Smith, G. W. Godding, and M. Muqsith. 2013. "Model composability and execution
across simulation, optimization, and forecast models." Proceedings of the Symposium on Theory of
Modeling & Simulation-DEVS Integrative M&S Symposium. Society for Computer Simulation
International.
Son, Y. J., R. A. Wysk, and A. T. Jones. 2003. "Simulation-based shop floor control: formal model, model
generation and control interface." IIE Transactions 35 (1): 29-48.
Stauber, C., J. Ambrose, and T. M. Rothwein. 2003. Application instantiation based upon attributes and
values stored in a meta data repository, including tiering of application layers objects and
components. USA Patent US Patent 6,574,635. June 3.
Taylor, R. B., and H. A. Taha. 1991. "Automatic generation of a class of simulation models from databases."
Proceedings of the 23rd conference on Winter simulation. Washington, DC, 1209-1217. edited by

2520

Gholami, Sarjoughian, Godding, Peters, and Chang
Barry L. Nelson, W. David Kelton, Gordon M. Clark. Piscataway, New Jersey: Institute of
Electrical and Electronics Engineers, Inc.
Wu, D., M. Erkoc, and S. Karabuk. 2005. "Managing capacity in the high-tech industry: A review of
literature." The Engineering Economist 50 (2): 125-158.
AUTHOR BIOGRAPHIES
SOROSOH GHOLAMI is a Computer Science PhD student at ASU. He can be contacted
<soroosh.gholami@asu.edu>.
HESSAM S. SARJOUGHIAN is an Associate Professor of Computer Science at ASU. He can
contacted at <sarjoughian@asu.edu>. H. S. Sarjoughian is the point of contact.
GARY W. GODDING is an Engineering Manager at Intel Corporation. He can be contacted
<gary.godding@intel.com>.
DANIEL R. PETERS is a Technologist at Intel Corporation. He can be contacted
<daniel.r.peters@intel.com>.
VICTOR CHANG is a Senior Software Engineer at Intel Corporation. He can be contacted
<victor.chang@intel.com>.

2521

at
be
at
at
at

EMF-DEVS Modeling
Hessam S. Sarjoughiana,b
sarjoughian@asu.edu
Abbas Mahmoodi Markidb
a.mahmoodi@ece.ut.ac.ir
a

School of Computing, Informatics, and Decision Systems Engineering
Arizona State University,Tempe, Arizona, U.S.A.
b

School of Electrical and Computer Engineering
University of Tehran,Tehran, Iran

Keywords: EMF, DEVS, DEVS-Suite Simulator, MetaModeling, Model Verification, Simulation Validation

Abstract
This paper introduces EMF-DEVS, a meta-modeling approach based on the Eclipse Modeling Framework (EMF)
and system-theoretic Discrete Event System Specification
(DEVS). Generic atomic and coupled EMF-DEVS metamodels are conceptualized and developed. These aid automating basic input/output consistency checking among model
components. They provide the basis upon which to develop
meta-models for systems that can be expressed in the DEVS
formalism. Structural syntax and semantics for generic and
domain-specific meta-models are enforced. The Eclipse Java
EMF tool transforms the meta-models to their implemented
counterparts. Behaviors for a system under consideration can
be added to the implementations of the meta-models so that
they can be executed in DEVS-Suite, a target simulator that is
used in the EMF-DEVS modeling engine. Some observations
and future work including how EMF-DEVS meta-modeling
can strengthen model verification and simulation validation
activities are briefly described.

1.

INTRODUCTION

Simulation models that are simple to conceptualize and can
lend themselves for (semi-)automatic Verification and Validation (V&V) pose challenges to the current state-of-the-art
M&S concepts, methods, and tools. This should not be surprising since developing rigorous simulation models is akin
to engineering software systems. Many steps are required in
creating, simulating, and evaluating models (e.g., [6]).
A variety of concepts, methodologies, efficient, and userfriendly simulation modeling tools have been developed ranging from those that are aimed at specific applications domains
to those that can be used for different kinds of systems. Significant advances, however, are still needed toward automation for creating rigorous and useful simulation models and

experimentations. For some application domains (e.g., manufacturing processes) existing approaches and tools can be
used with relative ease by domain experts. The same cannot
be said for generic approaches and tools that are intended to
build many different kinds of simulation models.
A basic approach for developing simulation models is to
evolve some informal conceptual understanding to abstract
mathematical specifications to programming code. Considering component-based simulation models, typically a modeler develops a sketch of the model in terms of components
and their relations. Next, assuming the use of a formal approach, the model is specified using the abstractions supported by a modeling formalism. In the next step, the formal
specifications are mapped to software components that are
programmed in the language of a target simulation engine.
For example, an informal model can be formalized using the
DEVS modeling approach [18]. The basic (atomic/coupled)
model components then are translated to parallel DEVS models that can be executed in the DEVS-Suite simulator [8].
In this work, we introduce EMF-DEVS, a modeling approach based on the concept of meta-modeling as defined in
the Eclipse Modeling Framework (EMF) [17]. This framework lends itself to capture a meta-model of a software system using, for example, Annotated Java or UML. Simulation models are developed as if one were to develop software tools. Meta-modeling in EMF-DEVS is aimed at developing generic atomic and coupled simulation model components which can be used to develop domain-specific models. (This idea is analogous to how UML can be used to develop models for a specific software system.) Once atomic
and coupled meta-models are developed, their concrete counterparts (implementations of the meta-models) can be generated. The generic and domain-specific properties of the
DEVS model implementations can be automatically verified
based on the DEVS meta-models that are built atop EMF
framework. Upon adding behavioral details (e.g., concrete
implementation of the operations defined for an external tran-

sition function), the simulation models can be executed. The
use of EMF-DEVS with its target DEVS-Suite simulator improves simulation validation in ways that are not feasible in
similar simulation tools.
The main contributions of this research, therefore, are the
EMF-DEVS meta-modeling and a realization of it in Eclipse
EMF. In Sections 2 and 3, related work and background are
described. In Section 4, the EMF-DEVS meta-modeling approach is detailed. A simple example is developed in a prototype EMF-DEVS tool in Section 5. In Section 6, conclusions
and future work are briefly described.

2.

RELATED WORK

An important quality measure of simulation models is the
extent in which they can be verified and validated. Verification (i.e., the simulation model is constructed correctly with
respect to a sound specification language) and validation (i.e.,
the simulation model generates dynamics consistent with its
intended goal) have been the subject of research for many
years (e.g., [1, 3, 10, 13, 16, 19]). Important concepts and processes have been developed, particularly from the perspective
of M&S lifecycle [6] which are mainly adapted from software
engineering.
Research has been underway on developing systematic and
automatic capabilities based on “meta-modeling” ideas to enrich developing simulation models. Some efforts, for example, have employed XML and UML with model transformation techniques [7] to aid users develop simulation models
using high-level programming and modeling languages. We
refrain from offering a classification for such approaches; instead a selection of some representative works are briefly described below.
XML with a subset of DEVS known as XFD-DEVS is proposed to bridge the gap between the UML graphical modeling elements and DEVS specification [12]. A meta-model
called eUDEVS allows mapping UML models to DEVS models. This allows a modeler to develop UML meta-models and
transforming them to DEVS meta-models. Once behavior is
added to these models, they can be executed using designated
simulators. Executable DEVS models can also be developed
using DEVS-UML [9]. The structure and behavior of DEVScompliant UML models are defined in terms of class and statechart diagrams and DEVS-Suite, a target simulator.
In another effort, Open Simulation Architecture (OSA) Instrumentation Framework is proposed to support instrumentation, validation, and analysis of simulations [11]. Aspect
Oriented Programming, COSMOS, and Fractal are used. This
framework provides separation of concerns among modeling,
instrumentation and optimizing data processing which may
aid verification and validation.
Models can also be developed using visual languages
which are at higher level of abstraction as compared with

programming languages. Once models are developed, they
must be translated to the programming language of a target simulator and behavior added to the models. An example of this is Component-based System Modeler and Simulator (CoSMoS) [14] which supports generating partial source
code for DEVS-Suite, its designated simulator. In another
work, a DEVS meta-model is developed using Meta Object
Facility [7]. The MOF-compatible syntax for DEVS model is
then transformed to Simulation Model Definition Language
which also has a MOF-based meta-model. They are used to
develop Query/View/Transformation (QVT) engine and thus
the ability to develop DEVS simulation models that conform
to SMDL which is used for Simulation Model Portability
standard 2.

3. BACKGROUND
3.1. Eclipse Modeling Framework
The Eclipse Modeling Framework is founded on the Model
Driven Architecture (MDA) approach [17]. It is built as part
of Eclipse, a multi-language software development environment. Eclipse consists of an Integrated Development Environment (IDE) which is bootstrapped on an extensible plug-in
system [4]. As a tool, Eclipse supports code generation facility for building tools and other applications based on a structured data model.
EMF has a common, high-level model that unifies UML,
XML Schema, and Java languages. As a framework and
code generation facility, it supports developing meta-models
for software systems. The vehicle to represent a model in
any of these languages is Ecore, a base meta-model upon
which meta-models for some software tools may be developed. The Ecore meta-model has a central role in that it is a
language at a higher-level of abstraction that those of UML
and Java, for example. Ecore meta-model has the key responsibility of ensuring referential integrity among alternative EMF models. A subset of the main elements of Eclipse
EMF Ecore meta-model is shown in Figure 1. The EClass,
EAttribute, EReference, and EDataType are used
to represent classes, attributes, associations between classes,
and attribute data types, respectively. The Ecore meta-model
has 20 and 32 complex and primitive data types [17]. The
generic Ecore elements are devised to represent structure of
software, not its behavior. The Ecore meta-model enforces
structural syntax and semantics of a modeled system, not its
behavioral syntax and semantics.
Models can be specified using annotated Java, UML, XML
documents and then imported into EMF. XMI model, the serialized form of these models can be generated in EMF Eclipse
tool. XMI is void of information that are specific to annotated Java, UML, and XML Schema. Ecore meta-model and
its XMI serialization form the kernel of EMF and what is used
to generate Java implementation code.

abstract atomic and coupled models can be realized as simulation models (i.e., executable models) using a set of simulation model components that are implemented in the Java programming language. Simulation models are concrete implementations of the abstract models and use software modeling
concepts and methods such as class inheritance and instantiation. More specifically, simulators such as DEVS-Suite have
so-called simulatable and non-simulatable components [15].
The former incorporates capabilities that are mainly defined
in DEVS and the latter are those components that are defined
according to the object-oriented models, languages, and runtime environments.

Figure 1. Basic EMF Ecore meta-model [17]

EMF Eclipse provides tools and runtime support to produce a set of Java classes for the Ecore meta-model of a
given system, a set of adapter classes that enable viewing
and command-based editing of the model, and a basic editor given XMI models. To generate these classes for given a
model called name, the name.ecore must be first developed. From this, the name.genmodel is generated. EMF
Eclipse run-time environment supports generating Model
Code (supports generating Java code) and plugins Edit
Code (supports building viewers and editors for the model)
and Editor Code (supports powerful model editing (e.g.,
copy, drag-and-drop, undo, and redo)). Additionally, test
files which provide basic support for testing functions that
are defined in the Model Code model are also generated.
The design, implementation, and code generation capabilities
of EMF Eclipse are described in [17].

3.2.

Parallel DEVS Models and DEVS-Suite
Simulator

DEVS is a modeling approach formalized in systemtheoretic terms for specifying hierarchical, modular models
[18]. Parallel atomic model (AM) has time (T ), input (X),
output (Y ), and state (S) sets as well as external (δext ), internal (δint ), confluent (δcon ), output (λ), and time advance
(ta) functions. The coupled model (CM) has time (T ), input
(X), output (Y ), component names ({Md|d∈D }), external input
coupling (EIC), external output coupling (EOC), and internal coupling (IC) sets where d can be any atomic or coupled
model subject to no self containment and no direct couplings
between its output and input ports. These models can be used
to characterize physical (or logical) properties and dynamics
of systems and correctly simulated using simulators that support the DEVS formalism.
DEVS-Suite [8] is a simulation environment for developing and simulating hierarchical, parallel DEVS models. The

4.

EMF-DEVS MODELING

A way to describe EMF-DEVS is to consider formal DEVS
models on the one hand and their simulation models that are
developed for given simulator on the other. It is straightforward to see developing simulation models as one develops
software design and implementing it in a programming language of a target simulator. For example, given the DEVSSuite simulator and the generic atomic and coupled UML
specifications, they can be extended to create domain-specific
models and then mapped to Java code. Here, it is easy to see
similarities between symbolic model abstractions and their
concrete executable code. One has to navigate among conceptual representations, formal models, software specifications,
and concrete implementations. In the absence of systematic
framework, the use of these elements to develop rigorous,
sound simulation models is challenging. In this work, we propose EMF-DEVS meta-modeling. The approach is depicted
in Figure 2. The representation highlights that generic EMFDEVS meta-model is developed using the Ecore.ecore
provided in the Eclipse EMF and Annotated Java derived
from DEVS-Suite simulator. These are shown as solid arrows.
The striped arrow illustrates that the EMF-DEVS meta-model
is defined for the DEVS formalism. The devs.ecore and
generator.genmodel support the generic DEVS metamodeling. They can be used to develop specific EMF-DEVS
models (see Section 5.). The domain knowledge is required
to develop EGP.ecore; the striped arrow is used to show
structural modeling for a specific system requires domain
knowledge. In the following, some of the basics for the EMFDEVS meta-models developed using DEVS-Suite simulator
are shown.

4.1.

DEVS Meta-model Characterization

The basis for EMF-DEVS meta-model is to divide a model
to its structural and behavioral parts. We define these for
atomic and coupled models, but only include some parts for
the atomic model in this paper. Each part of the atomic has its
own syntax and semantics and are the same as those that are

Figure 2. Generic and specific EMF-DEVS meta-models

defined in parallel DEVS formalism. The separation offers a
boundary that is useful for defining EMF-DEVS models.
Atomic Model: The inputs, outputs, and states are the
structural part. They are abstract and their syntax is defined as xAM = (portin , eventin ) and yAM = (portout , eventout )
– input and output port names and events. The syntax for
portin and portout are alphanumeric symbol or syntax. There
is no operational semantics for the port names. The syntax
for the events is arbitrary. The operational semantics for the
events is that they can be received or sent on input and output ports. They may be accepted or generated at present or
future time instances with respect to a reference time frame.
The syntax of the state S is arbitrary as the input and output
events. The semantics of the states is that they can change
their values with respect to a reference time frame given the
functions in the atomic model.
The behavioral syntax for inputs, outputs, and state are the
same as the structural part except that it is used to define concrete inputs, outputs, and states. The semantics for these are
determined for the simulation modeling purpose at hand. For
example, given a state variable v for a CPU, the acceptable
values can range from −40 to + 85◦ C. This range of values
can be used in a transition function to allow certain operations
to be carried out.
The remaining structural part of an atomic model are
δext , δint , δcon , λ, and ta. These are abstract functions since
they are not particular to any system to be modeled or a part
thereof. Therefore, they can be considered to also have structural and behavioral parts. The syntax for δext (s, e, x) defines
states s, elapsed time e, and inputs x. The structural syntax is
void of the specification for how the states may be changed
for input events of a given specific system. The semantics for
the structural part of the external transition function is to re-

ceive a collection of legitimate input events. These events X
may be processed at any time given the elapsed time e and
if the model is in a designated state s. The semantics for
the structural part is also dependent on the semantics of the
other functions defined for atomic model. The atomic model
structural semantics is defined in terms of the ordering among
δext , δint , δcon , λ,ta.
The behavioral syntax for the transition, output, and time
advance functions uses the syntax that is used for concrete functions specific to a system’s behavior. For example δext (s, e, x) can have a function that sets a warning signal
when temperature of a CPU is within 10◦ C of its minimum or
maximum value. The syntax used for the concrete functions
used in the δext , δint , δcon , λ, and ta define the behavioral syntax of atomic model. The operational semantics for the behavioral part is given by the system’s dynamics.
Observation: Clearly, simple meta-models are simpler to
verify and validate. However, avoiding strong restrictions
(e.g., requiring input events to be integers) significantly reduce the expressiveness of the DEVS formalism and its use.
To illustrate this concept, consider the following partial specification for a model of a processing component in a service
software system:
δext ((phase, σ, job), e, x) =
(“busy”,time p , job) if phase = “passive”
(phase, σ − e, x) if phase 6= “passive”
where job = (id,timec ) represent a task (event) with attributes id (e.g., 7) and “time-to-complete” (e.g., 19 units of
time). For CM, the specification for XCM = (id,timec , quality)
where the “quality” (e.g., high) is also given as an attribute of
the “job”. The external transition function for the Processing component is defined to use timec to set the value of
time p and discard “quality” attribute of the job. It is important
to note the behavior specifications of this model defined in
terms of setting values for the phase, σ, and job variables. As
defined above the structural specification for the processing
model has {X,Y, S} and {δext , δint , δcon , λ,ta}. These are the
elements that are defined to be the abstract EMF-DEVS metamodel. The behavioral specification of the processing model
are functions such as phase = “busy” and σ = time p that are
defined in the external transition function. These are the elements that define the concrete EMF-DEVS meta-model. Implementations generated from these meta-models using the
EMF-DEVS prototype adhere to the structural parts of the
atomic and coupled DEVS models (see Section 5.).

4.2.

Atomic and Coupled EMF-DEVS models

In relation to the theoretical DEVS models, simulation
models have typing that are defined and implemented according to the software design and implementation a designated

simulation tool. In this simulation model developed in DEVSSuite simulator, the job input event is of type entity
which contains the necessary requirements for execution in
the simulator (see Listing 1). The deltext(double e,
message x) handles a bag of messages. The out( )
function has a return type of message which has a structure containing one or more pairs of port names and values
which must be of type entity. Codes snippet lines 30-34
and 37-38 are the structural and lines 35-36 are the behavioral parts of δext . Code snippet lines 60-61 and 65-66 are the
structural part and the remaining ones are the behavioral part
of λ.

Listing 1. Processor Model Code Snippet in DEVS-Suite
Simulator

17 protected entity job;
30 public void deltext(double e, message x){
31 Continue(e);
32 if (phaseIs("passive"))
33
for (int i=0; i< x.getLength();i++)
34
if (messageOnPort(x,"in",i)){
35
job = x.getValOnPort("in",i);
36
holdIn("busy",job.proc_time);
37
}
38 }
60 public message out( ) {
61 message m = new message();
62 if (phaseIs("busy")){
63
m.add(makeContent("out",job));
64 }
65 return m;
66 }

It is desirable for a modeling engine to support verifying data
type consistency among individual and composed simulation
models. The data types in a coupled model must be consistent
such that a component that receives some data is able to use
it as-is or has the ability to transform the data to a type that
it can process. This means having the ability to verify that all
atomic models can handle their input and output data. Toward
this objective, we can view simulation models to evolve from
meta-models to concrete models. At the meta-model level,
generic atomic and coupled models consistent with their abstract specifications can be defined. At the concrete level, constraints such as input variable names and values can be accounted for. Given the DEVS atomic model specification, its
meta-model can be conceptualized as follows:

• Inputs – X: {(port, var)}. Data type for port
names are strings and data type for input variables
are primitive data or complex objects. Each variable var has a name (string) and a value (primitive
data or complex object). E.g., Input("in", new
entity("job1")).
• Outputs – Y: {(port, var)}. Data type for port
names are strings and data type for output variables are
primitive data or complex objects.
• States – S: (name, value). Data types for names
are strings and data types for values are primitive data or
complex objects. E.g., protected entity job.
• External transition function – external(S,
X){· · · }. The operations for the external transition
function are arbitrary. The input port name and
inputs must be acceptable to the external transition function. E.g., if (Port("in")) {job =
x.getValOnPort("in")}. This partial example
requires the value received on input port "in" to be of
data type job.
• Internal transition function – internal(S){· · · }. The
operations for the internal transition function are arbitrary. The states are the same as those that are defined in external transition function. E.g., job = new
entity("none"). This partial example requires the
data type for the state variable to be job.
• Output function – output(S){· · · }. The operations
for the output function are arbitrary. The states are the
same as those that are defined in external transition function. E.g., ("out",job). This partial example must
generate an output value that the receiving components
can process.
• Time advance function – time(S){· · · }. The time advance function can have arbitrary operations.

The atomic meta-model is distinct from its formal specification in that the latter represents inputs, outputs, states, and
functions as mathematical abstractions. In contrast, the artifacts of the meta-model are defined in terms of the data types
and function signatures. The idea for the meta-model is that
definitions for the input/output port names, the input, output,
and state values, and the functions are collectively consistent.
For example, the external transition function must be able to
handle all input values it receives. Coupling consistency between every pair of sending and receiving components of a
coupled model satisfies the following constraint. The output
generated by the sending component can be processed by the
receiving component.

Figure 4. EMF-DEVS eCoupled meta-model
Figure 3. EMF-DEVS eAtomic meta-model
The eAtomic and eCoupled meta-models shown in
Figures 3 and 4. These meta-models are defined using
Ecore.ecore and the atomic and coupled classes defined in
the DEVS-Suite simulator. The elements of the eAtomic
and eCoupled meta-models are designed based on primitive and complex classifiers defined for the EMF Ecore metamodel (see Figure 2).

5.

DEVELOPING MODELS IN EMF-DEVS
ENVIRONMENT

To illustrate the EMF-DEVS meta-modeling, a simple producer/consumer example is considered. The producer generates jobs and the consumer has the responsibility to process the received jobs. The producer waits until the processing time of previous job has been reached before generating
the next job. The egenr and eproc models are combined
to create egp (see Figure 5). I/O coupling (e.g., input port
ingen receiving input from the input port egp) between the
egenr and ingp models are defined using devs.ecore. Arbitrary IO data values can be added to these meta-models. In
this example ejob and etask are complex data types. Annotated interfaces for egner is shown in Figures 6.
For all models, getter and setter functions are automatically

Figure 5. Consumer/Producer meta-model

Figure 8. I/O datatype mismtaches
level instead of having an error stack generated from simulation code. EMF-DEVS environment is developed using
Eclipse version 3.7 and EMF version 2.7.

5.1.

Figure 6. Generator (egenr) meta-model

Figure 7. Code snippet for egp model
generated. For example, given egp and egenr models, an
external input coupling for them is generated as shown in Figure 7). These methods use notifications to send information to
their observers and provide a proper structure for validators
to check their constraints are satisfied. For the getIn2in
method, if an object is a proxy, it must be resolved (i.e., replaced with its target object) and a notification invoked. These
provide stable and flexible support for modeling and validating models given their meta-models as well as their implementations.
The implementation codes are generated using basic structures defined for the egenr, eproc, and egp. After adding
behaviors to the atomic models, they can be simulated using DEVS-Suite simulator. An example of the EMF-DEVS
modeling engine support for I/O data consistency is provided. An IO type mismatch between outgen and inproc
ports is created. Outgen sends ejob to inproc port that
expects etask. This data compatibility mismatch between
connected ports is identified using the EMF-DEVS automatically generated plugins. They support validating constraints
and thus I/O data type consistency checking at the modeling

Model Verification and Simulation Validation

Verified models are key in validating their simulated dynamics for their intended purposes. Inputs of
simulation models can be defined as X = {(p, v)|p ∈
{name1 , · · · , namek }, wherenamei is a unique alphanumeric
input port name for i = 1, · · · , k; v ∈ {Z ∪ R ∪ Q ∪
{value1 , · · · , valuen }} where each value j is a simple data
or complex object for j = 1, · · · , n. The same specification can be used for outputs. Simulators (e.g., DEVSSuite) or modelers (e.g., CoSMoS) cannot readily or systematically verify I/O matching between two simulation
models. Commonly problems are detected at the code
level. Given output y and input x for the above generator
and processor models, input values can be modeled as
v = (portname , (varname , vartype , varvaluerange )) where port and
variable are unique, finite alphanumeric names, variable can
have a primitive or a complex type (as in strongly typed programming languages). Each data type at the lowest level has
either a numeric or alphanumeric value. At this level of specification, the equality type checking between y output specification for the generator and the x input specification for the
generator model can be guaranteed. This can be extended for
the I/O well-formedness of data space {(ω, ρ)|ω ∈ ΩI , ρ ∈
(Y, T ), dom(ω) = dom(ρ)} [18] for any two models that are
coupled together. As noted above, with meta-models, I/O
matching can be detected at the modeling level. Developing
meta-models and evaluating their potential I/O incompatibilities are supported at the level of generic eAtomic and eCoupled models with application domain constraints captured by
the modeler. This is useful when viewed in the context of developing models while making changes to them. Changes or
incompatibilities at the I/O level as well as their relations to
the behaviors of the models’ operations can be detected. The
implementations of the models can be automatically generated and thus achieve stronger model verification and simulation validation using the EMF-DEVS modeling approach.

6.

CONCLUSION

Meta-modeling is attractive for developing simulation
models that can be verified and validated with automation
support in early stages of M&S lifecycle. In this work, the

Eclipse Modeling Framework is proposed as a basis for M&S
meta-modeling. EMF is used to represent the mathematical
DEVS models in their meta-model counterparts. The metamodels reduce the gap between formal specifications and
their concrete implementations. The meta-modeling concepts
introduced in EMF-DEVS offer a new kind of DEVS modeling approach and tool. Modelers may use alternative metamodeling languages (e.g., UML and Annotated Java) with automatic transformation support among them. Modelers can
create domain-specific meta-models using generic DEVS
meta-models with support for I/O consistency checking at
the level of data types and their use in input, state, and output transition function specifications. Future work includes
consistency checking for concrete model behaviors. This suggests the need for new ideas and methods for adding domainspecific behaviors to the abstract (generic) EMF-DEVS models and in particular the core atomic model functions (i.e.,
δext , δint , δcon , λ, and ta) as well as other kinds of functions
that are needed, for example, in dynamic structure [2] and
constrained real-time modeling [5]. This entails extending
EMF-DEVS to handle behavioral modeling and thus offering
new levels of support for increasingly demanding simulation
model V&V capabilities.
Acknowledgement: This research is supported in part by
Intel Corporation under research grant #4875173.

REFERENCES
[1] O. Balci. Verification, validation, and accreditation. In
Proceedings of the 30th Winter Simulation Conference,
pages 41–48, 1998.
[2] F.J. Barros. Modeling formalisms for dynamic structure
systems. ACM Transaction on Modeling and Computer
Simulation, 7:501–515, October 1997.
[3] P.K. Davis and R.H. Anderson. Improving the Composability of Department of Defense Models and Simulations. RAND, Santa Monica, CA, 2004.
[4] E. Gamma and K. Beck. Contributing to Eclipse principles, patterns, and plug-ins. Addison-Wesely, 2004.
[5] S. Gholami and H.S. Sarjoughian. Real-time networkon-chip simulation modeling. In SIMUTools, Desenzano, Italy, pages 1–10. ICST, 2012.
[6] IEEE.
IEEE recommended practice for high
level architecture (HLA) federation development and
execution process (FEDEP), version 1516.3-2003.
http://www.ieee.org/Standards, May 2003.
[7] Y. Lei, W. Wang, Q. Li, and Y. Zhu. A transformation
model from DEVS to SMP2 based on MDA. Simula-

tion Modelling Practice and Theory, pages 1690–1709,
2009.
[8] DEVS-Suite Simulator.
February 2009.

http://devs-suitesim.sf.net,

[9] J. Mooney and H.S. Sarjoughian. A framework for executable uml models. In High Performance Computing &
Simulation Symposium, Spring Simulation Conference,
pages 1–8, 2009.
[10] T. Ören. Concepts and criteria to assess acceptability of
simulation studies: a frame of reference. Communications of ACM, 24(4):180–189, 1981.
[11] J. Ribault, O. Dalle, D. Conan, and S. Leriche. OSIF:
a framework to instrument, validate, and analyze simulations. In Proceedings of the International Conference
on Simulation Tools and Techniques, SIMUTools, pages
1–9, Torremolinos, Malaga, Spain, 2010.
[12] J.L. Risco-Martin, S. Mittal, B.P. Zeigler, and J.M.
Cruz. eUDEVS: Executable UML using DEVS theory
of modeling and simulation. Transactions of the Society for Computer Simulation International, 85:750–777,
2009.
[13] R. Sargent. Verification and validation of simulation
models. In Proceedings of the Winter Simulation Conference, pages 41–48, 2009.
[14] H.S. Sarjoughian and V. Elamvazhuthi. CoSMoS: a visual environment for component-based modeling, experimental design, and simulation. In Proceedings of
the International Conference on Simulation Tools and
Techniques, SIMUTools, pages 1–9, Rome, Italy, 2009.
[15] H.S. Sarjoughian and R. Flasher. System modeling
with mixed object and data model. In DEVS Symposium, Spring Simulation Multi-conference, pages 199–
206, 2007.
[16] L. Schruben. Establishing the credibility of simulation
models. Transactions of the Society for Computer Simulation International, 34(3):101–105, 1980.
[17] D. Steinberg, F. Budinsky, M. Paternostro, and
E. Merks.
EMF Eclipse Modeling Framework.
Addison-Wesely, 2008.
[18] B.P. Zeigler, H. Praehofer, and T.G. Kim. Theory of
modeling and simulation. Academic press, New York,
second edition, 2000.
[19] B.P. Zeigler and H.S. Sarjoughian. Implications of
M&S foundations for the V&V of large scale complex
simulation models. pages 1–51, 2002.

Complexities of Simulating a Hybrid Agent-Landscape Model
Using Multi-Formalism Composability

Gary R. Mayer
Gary.Mayer@asu.edu

Hessam S. Sarjoughian
Sarjoughian@asu.edu

Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Department
School of Computing and Informatics
Arizona State University, Tempe, Arizona

combined. Their interaction is bi-directional as the human
and environment models act as both producer and consumer
in the hybrid model. Yet, by maintaining a clear delineation
of each sub-system, researchers are able to choose a
formalism that best represents the sub-system to be
modeled, complex or not, thus enabling a more intuitive
mapping from domain knowledge to formalism (e.g., using
an agent-based model to represent the humans and a cellular
automaton for the environment). Furthermore, by
maintaining independent models researchers are able to
study the human population and the environment separately,
as well as examine the interactions between the two
methodically.
It is the intent of this paper to discuss an approach that
facilitates these studies and presents some of the challenges
associated with this approach. The underlying concept is to
compose modeling formalisms instead of integrating
models’ inputs and outputs via interoperability concepts.
The Mediterranean Landscape Dynamics (MEDLAND)
project [1] is an on-going international and multidisciplinary effort. One of its goals is to develop a
laboratory in which to study humans, the environment, and
their dynamics. In MEDLAND, humans are represented by
agents and the environment is represented by a landscape
model. The two models will be composed using a third
model, the interaction model (see Figure 1). It is from the
MEDLAND domain that examples are provided and
previous research summaries are drawn.
Figure 1 shows a discrete-event, rules-based agent
model composed with a discrete-time, cellular automaton
landscape model. Note that there is no direct communication
between the two composed models. Their communication is
managed by an interaction model which handles the data
transformation and control. This interaction model is
complete with its own formalism and realization separate
from the two composed models.

Keywords: agents, cellular automata, multi-modeling,
multi-formalism, poly-formalism.
Abstract
Hybrid agent-landscape models are used as an
environment in which to study humans, the environment,
and their dynamics. To provide flexibility in model design,
expressiveness, and modification, the environment models
and human agent models should be developed
independently. While retaining each model’s individuality,
the models can be composed to create a model of a
complex, hybrid agent-landscape system. This should allow
for a much more in-depth analysis of each model
independently, as well as a study of their interactions. To
create such a modeling environment requires a look beyond
a simple interface between two models. It may require that
the models’ formalisms be composed, their execution be
synchronized, their architectures be integrated, and a
common visualization be created to provide a whole-system
data view during simulation. This paper discusses the
complexities of such an undertaking.

1

INTRODUCTION

Hybrid agent-landscape models are typically used to
better understand the effects of human beings interacting
with their environment. The term “hybrid” is used to imply
that both the human and environmental sub-system models
are developed to some higher resolution that enables a clear
delineation between the two to be drawn, if not developed to
the point that they might be separated and executed
independently. However, the incorporation of a large
amount of detail in a model often results in large numbers of
interacting pieces – modeled systems whose interacting
pieces bring about different kinds of complexities.
Moreover, a new dimension of complexity is introduced
when the human and environmental sub-systems are

SpringSim Vol. 1

161

ISBN 1-56555-313-6

external input does not arbitrarily modify the state of that
model. All state changes must be in accordance with the
rules of that model’s formalism else correctness and validity
of that model is suspect.

agent

interactions
Discrete-event,
Rule-based Models

Specification

Specification

Execution

Execution

Software
Architecture

Software
Architecture

Implementation
Specific Details

Implementation
Specific Details

landscape
Data Transformation
+ Control Model

Discrete-time, Cellular
Automata Models

Figure 1. Proposed Agent-Landscape Model Architecture

2

Model A

BACKGROUND

formalism
realization

2.1

Composability Problem
Using disparate modeling formalisms to describe
different sub-systems has important benefits ranging from
acquiring requirements to simulation experiments (see [2]
for details). To achieve model composability, the main
challenge centers on having appropriate concepts and
methods to compose different model types such that both
the disparate models and their interactions have well defined
syntax and semantics. The composed models must be
correct and valid – i.e., model specifications must be
consistent as (i) correctness is ensured according to the
domain-neutral modeling formalisms and (ii) validation is
ensured according to the domain-specific model
descriptions [3].
To achieve model correctness and enable model
validation, a variety of issues must be considered. The
authors group these to formalism and realization aspects
(see Figure 2). The formalism focuses on model
specification and execution – i.e., the former is for
mathematical descriptions of the system and the latter is for
the machinery that can execute model descriptions. The
formalism modeling and execution layers are not specific to
any one model instantiation; they’re generalized for a class
of models (e.g., discrete-time).
Important considerations for composability are the
implications of what it means to inject data and control from
an external source that may not have the same approach to
model specification and execution. For instance, one model
may have an innate concept of time, while the other does
not. Consider, also, what it means for a discrete-event model
to inject data into a discrete-time model not at predefined
time steps. As a final example, take one model that uses a
state-based approach. The modeler must ensure that an

ISBN 1-56555-313-6

Model B
order of
composition

Figure 2. Model Layer Composition
The realization aspect deals with software architecture,
design, and implementation. The software architecture and
design are conceptual and detailed software specifications
(e.g., described in the Unified Modeling Language (UML))
that can be forward engineered to specific programming
language constructs (e.g., Java, C, Lisp). Examples of
design considerations can be as simple as converting an
integer from one model into a double for another. For
distributed model systems, it includes such things as quality
of service and handling synchronous versus asynchronous
input and output.
The last layer under realization is titled
“Implementation Specific Details”. This refers to those
elements of a model implementation that are not
architecture related; they are more closely tied with the data.
Specifically, how things like the scale and resolution of the
model interact with other models. For example, an
environmental model may employ millions of data
elements; each at a 100 meter scale, while an agent model
uses only a couple of hundred agents working at a 10 meter
resolution. The modeler must consider the significance to an
agent’s movement possibly existing entirely within one cell
of the environment model.
2.2

Multi-Modeling Approaches
In their books, Fishwick [4] and Zeigler and others [5],
describe multi-modeling as the creation of a model
composed of smaller models. Each sub-system model
captures only a part of the whole system. The composition

162

SpringSim Vol. 1

of these sub-system models enables the modeler to create a
much more complex representation of the whole system. For
example, the Joint-MEASURE simulation environment
integrates DEVS-C++ and GRASS models to simulate
movement of vehicles on geo-referenced terrain [6, 7].
A taxonomy for multi-modeling using the same or
different types of formalisms has been proposed [3]. This
nomenclature offers four approaches to multi-modeling.
A mono-formalism refers to one in which there is a
hierarchical composition of models into/from parts
described within one syntax and semantics. For instance, a
system model composed of all discrete-time models.
Super-formalism is a single formalism that supports
describing two or more different types of models. It requires
that the models be of the same family (e.g. system
specifications). Note that the super-formalism approach
forces a uniform execution approach and syntax on both
models. For example, the Discrete Event & Differential
Equation System Specification (DEV&DESS) [5, 8] can
describe both a continuous model and a discrete-time model.
A meta-formalism describes mapping two disparate
formalisms to a third, common formalism. It does not have
the same-family model restrictions that are levied upon the
super-formalism. However, expressiveness of the model
formalisms must often be restricted according to the metaformalism to ensure proper, multiple mappings. High-Level
Architecture (HLA) [9] and Repast [10, 11] are examples of
this.
Poly-formalism is the approach in which disparate
formalisms interact via a third formalism while retaining
their original formalisms. An example of this approach is
the use of a Knowledge Interchange Broker (KIB) to
compose Discrete Event System Specification (DEVS) and
Linear Programming (LP) models [12].
The choice of which multi-modeling approach to use is
situation dependant. It will likely be the modeler’s opinion,
motivated by the domain and requirements levied upon the
system model (e.g., ability to described the model in one
formalism or withstand the loss of expressiveness and/or
domain specific details during mapping).

3

design. While Repast offers more features, including
support for the Geographic Information System (GIS), both
have some visualization capability. Both systems use a
meta-formalism approach that requires the modeler to
convert models into an object-oriented construct that
complies with their simulator’s specifications. As there is no
formalism for either system, proof of correctness of the
entire model is left up to the modeler. Further, neither
system provides a formal method for conversion of any
particular type of modeling formalism into their system. The
use of the toolkit simulator, which uses a simplified
definition of discrete event that allows multiple state
transitions to result from a single event [15], is prohibitive
to implementing a formalism within one of the toolkit
environments. Without the ability to properly implement a
formalism within these environments, correctness is a
concern and composition of formalisms and models
described within them are difficult to validate. Thus, the
poly-formalism composition approach is not suitable in
these environments.
3.2

Ptolemy
Ptolemy II [16] is a computational framework for
embedded systems that focuses on concurrent systems.
Ptolemy composes model domains through interactions and
domain polymorphism. It does this by structuring the model
domains as components and treating each as an actor under
the control of directors using interface automata. It is superformalism modeling with a strong software engineering
emphasis [17]. While Ptolemy addresses multi-formalism
modeling, it does so while focusing on interactions between
embedded systems (each a unique domain). In the case of
this project, the focus is on a single domain in which pieces
of the domain are best described using a different modeling
formalism. Furthermore, the discrete time domain within
Ptolemy II is still experimental. The current model has strict
requirements such as static scheduling and the requirement
to know what will execute on the ports before the simulation
begins. A restriction that is impossible to meet if the two
composed models are to truly remain independent.
3.3

KIB
There have also been successful attempts at polyformalism composability. In three other research projects,
the third model that facilitates the composition is referred to
as a Knowledge Interchange Broker (KIB) [18]. The KIB
has been used to compose DEVS with Linear Programming
in semiconductor supply/demand networks [12]; to compose
DEVS with the Reactive Action Planner (RAP), an agentbased planner [19]; and to compose DEVS with Model
Predictive Control (MPC) in semiconductor supply-chain
manufacturing [20]. These three projects demonstrate two
things. First, it is possible to compose models using the
poly-formalism modeling approach. Second, the fact that
three different projects exist and each composes DEVS with

RELATED WORKS

Building a system model from multiple sub-system
models is not a new concept. As such, there are a number of
existing toolkits that perform some combination of agentlandscape modeling [13]. These toolkits have been
examined during the course of this research and the most
applicable are discussed here.
3.1

Swarm and Repast
Two well known toolkits are Swarm [14] and the
Recursive Porous Agent Simulation Toolkit (Repast) [10,
11]. Both are discrete-event, multi-agent modeling systems
using object-oriented software programming concepts and

SpringSim Vol. 1

163

ISBN 1-56555-313-6

a distinctly different formalism implies that the KIB is not
generic to all formalisms. A KIB is a unique composition
between two distinct formalisms. The research being done
here is unique from the previous works in that it composes a
discrete-event agent model with continuous processes
represented by cellular automata.
3.4

well as a geographical database management system such as
GRASS. Furthermore, while Cellular-DEVS offers some
visualization tools, the GRASS visualization tools offer a
richer set of predefined features.
3.4.2 GRASS
GRASS is an implementation of GIS [23] that manages
georeferenced information. As a geographical database
management system, it is specifically tailored to efficiently
examine and modify large geographical data sets. GRASS
data (points, lines, polygons, or pixels) is stored in files
referred to as mapsets. Each mapset may be either a vector
or raster data model. Vector data models are entity models
in which the data model represents a specific entity and the
topology (relationship) between the data is either implicit or
explicit depending upon what is represented. For example, a
polygon stored as a vector would have details about each
point in the polygon and the lines connecting those points,
thus providing an explicit relationship between the data. The
raster data model is a square, regular tessellation of
continuous space. The topology in this discretization is
implicit [24].
GIS implementations are capable of logical and
numerical data analysis methods. These methods are not
commutative (sequence matters). Thus, more complex
command sets are stored in structured command files
(scripts) and are referred to as ‘models’. The scripts specify
the order of function execution and to which data the
functions apply [24]. These models have no predetermined
specification for behavior, structure, or execution. These are
left to the modeler’s discretion when devising the scripts.

MEDLAND

3.4.1 Agent-Landscape Model: Phase I
The initial agent-landscape model developed for the
MEDLAND project uses a super-formalism approach
created in DEVS. In that version, the agent is a discreteevent model and the landscape is a Cellular-DEVS, discretetime model [5]. This approach was taken to enable a study
of the agent model concept while reducing the complexity
associated with interfacing models in different formalisms.
The model events are cyclic with each cycle
representing a calendar year. The agents in this model
represent households. Each household has a population that
it uses to derive a need for food to survive and a labor force
with which to manage land. The basic management actions
are “cultivate” land, “fallow” land, and “release” land. The
landscape cells each have their own soil value indicative of
soil quality that ranges from 0 to a maximum of 5. When a
landscape cell is cultivated, its soil quality reduces by one
each cycle (to a minimum of 0). Each cycle that a landscape
cell is fallowed, its soil value increases by one, up to the
maximum value for that cell.
Each cycle, the agent assesses its current food
requirement and compares it to an expected yield (derived
from last cycle’s yield and current population) to create a
management plan. Conflict resolution is handled on a firstcome, first-served basis and plans are revised as necessary
until all food requirements are met or no additional land
exists for cultivation. Any excess land held beyond what is
need for cultivation is held in fallow.
The agents are given two goals. The first is simply
survival. With only this goal active, the agents were able to
reach a steady-state population quickly and demonstrated
that cultivated and fallowed lands were swapped each cycle.
The second goal, growth, allowed the agents to use large
excesses of fallowed land and cultivate it. Since population
growth is tied to the difference between yield and food need,
this created large growth spurts in agent population. This
growth continued until the agent population reached
simulation boundaries [21].
While this approach worked, it also became obvious
that the DEVSJAVA environment would be inappropriate
for the landscape as the number of data elements increased.
While efficiencies could be imparted to the Cellular-DEVS
landscape model, it would be unlikely that the model could
be made as robust as a continuous or discrete-time model
devised using the Geographic Resources Analysis Support
System (GRASS) [22, 23]. Cellular-DEVS does not scale as

ISBN 1-56555-313-6

3.4.3 Agent-Landscape Model: Phase II
The next version of the model progressed towards a
poly-formalism approach. The behaviors from Phase I were
maintained. However, instead of using Cellular-DEVS for
the landscape model, a GRASS landscape model was
created and an interface was implemented within a
component of the agent model. When agents interacted with
the landscape, they sent DEVS message objects to the
interface component. This component converted the
message into GRASS scripts. The scripts were then
executed and the resultant output was captured and returned
to the requesting agent. Further, the DEVS simulator was
used to execute the GRASS landscape model by
implementing landscape updates as internal events within
the interface component. This approach allowed DEVS to
provide a schema to the execution of the landscape models
in GRASS.
It is purposefully stated that this is a progression
towards a poly-formalism approach rather than the approach
itself. The reason for this is that there is a direct interface
between the agent and landscape models. Also, there is
currently no third model supporting this interaction.
Furthermore, the interface is solely at the implementation-

164

SpringSim Vol. 1

level; the formalism-level is not yet realized. This phase
provided a better understanding of the difficulties of
exchanging data and control between the two systems while
examining approaches for describing GRASS models using
more formal methods (e.g., cellular automata).
Simulation experiments were conducted on the second
agent-landscape model to ensure that all of the functionality
enabled in Phase I still worked through the interface. A
simulation visualization was created using GRASS
visualization tools to show the current status of key data.
The GRASS data visualization had three panels. The first
showed the current land use – cultivated, fallowed, or wild.
The next panel displayed soil quality. Colors changed to
represent soil values, whose dynamics are related to
cultivation. The last panel displayed the agent that is
currently managing each landscape cell. Each agent was
assigned its own color.

aggregation/disaggregation, and control passing. Note that
the IM has an additional component, a visualization layer,
attached to its software architecture layer.
Visualization is an important tool for supporting
simulation experimentation. The evaluation of executing
models, particularly for complex large-scale domains, is
invaluable for researching hybrid agent/landscape dynamics.
The visualization layer is attached to the software
architecture because appropriate ways to probe the
composed models, while still maintaining their
independence, must be considered in order to retrieve data
dynamically. Despite the connection to the software
architecture, the visualization layer is unlikely to be derived
from the formalism.
The remainder of this section discusses the individual
layers of the IM (as shown in Figure 3) and the challenges
associated with the design and implementation of that layer.

4

4.1

INTERACTION MODEL
DERBA

IM

DTCA

Specification

Specification

Specification

Execution

Execution

Execution

Software
Architecture

Software
Architecture

Visualization

Software
Architecture

Impl. Specific
Details

Impl. Specific
Details

Impl. Specific
Details

Domain Specific
Knowledge

Domain Specific
Knowledge

4.1.1 Specification Interaction
The interaction model must respond to input from both
composed models and, in the case of the discrete-event
agent model, may not know the exact timing of such events.
Therefore, it makes sense to consider a discrete-event
modeling formalism for the IM. However, it should be
noted that the discrete-event specification for the agent is
rich enough to also specify discrete-time agents and the
discrete-time cellular automata may also represent a
discretized continuous model. Thus, when designing the IM,
it should be kept in mind that by using a discrete-event
interaction model to compose any of these, a problem arises.
The poly-formalism modeling approach moves the
details of the domain and formalism of each model and;
therefore, the ability to interact as well, into the interaction
model. This removes any domain-specific knowledge of the
composed models from each other. However, it implies that
the discrete-event interaction model will then inject any data
into either model as an event. Since the agent and landscape
models may have different time deltas, the time at which an
event is injected may not align with a regularly scheduled
discrete-time event. This poses a problem if any of the
functions within a discrete-time model are time-delta
dependant. For example, a landscape soil erosion model
assumes that its values are updated once every 10 cycles.
The value of 10 is explicitly used as a time delta to revise
data values each time the function is run. The agent model
updates every 1 cycle and, somewhere between the soil
erosion model’s update, the agent makes a modification that
impacts that soil erosion model and requires that soil values
be updated immediately. Running the soil erosion model
would erroneously cause the model to update its time by 10
and cause it to be out of synch with the rest of the modeled
system.

Domain Specific
Knowledge

formalism

derivation

realization

interaction

Figure 3. Proposed Hybrid Model Architecture Using a
Poly-Formalism Approach
The on-going effort involves research and development
of a discrete-event interaction model (IM) that composes a
DEVS discrete-event, rule-based agent (DERBA) model
with a GRASS discrete-time cellular automata (DTCA)
landscape model (see Figure 3). This version completely
segregates the agent and landscape models. Figure 3 shows
that the formalisms of the two composed models (DERBA
and DTCA) will be used to derive the formalism of the IM.
Next, the realization of all three models is derived from their
formalisms. It is within the realization of each model that
the interactions between them occur through data mapping,

SpringSim Vol. 1

Formalism

165

ISBN 1-56555-313-6

time to automate it, the centralized control scheme seems
the logical choice with minimal additional overhead.

There are three approaches to managing this situation.
The first requires that all composed models meet the
discrete-event specification. However, this creates a
problem for continuous functions within the models. By
representing a continuous function within a discrete-event
model, the modeler must handle discontinuity within the
continuous function. This is a problem whose solution is
still being studied using approximation mappings of
continuous functions to discrete-events (quantizations) [5].
A second alternative is to allow the interaction model to
inject the event during a model’s next, regularly scheduled
event time. The problem with this approach is that one
model may execute much more frequently than the other
and is likely to be dependant upon the data in the other
model being current. This leads to further complications as
the modeler must not only specify how concurrent actions
injected into a model at the same time are managed, but how
actions injected across multiple time frames between the
model’s update are controlled. The third approach levies a
caveat on all discrete-time models that interface with the
interaction model. The caveat states that any model in which
an external model may have an impact must not contain a
function that explicitly anticipates the time delta between
function executions. This last approach allows the most
flexibility throughout the system with the least complexity.

4.2

4.2.1 Software Architecture
The software architecture must account for two main
obstacles – disparate software languages and constructs, and
hardware resource needs. The DEVS models are being
created in DEVSJAVA, a Java language implementation of
DEVS in which all models are object-oriented constructs.
GRASS modules are written in C. Scripts (and functions)
may be written in any scripting language such as Bash or
Python depending on the system functionality that the
modeler requires (e.g., file management, use of regular
expressions, etc.). To run a GRASS script, a DEVSJAVA
component uses the Java Runtime.exec() command to
execute it.
Each GRASS module is independent and, therefore, has
its own interface but the modules do not continuously run.
They accept input, return output, and terminate. The output
from the modules is only provided to the standard output
stream (and, sometimes, standard error stream). Thus, to get
return data, a program must capture and parse the data from
the standard output buffer and then insert that data into a
DEVS message object on the appropriate port. This
approach is complicated by the fact that GRASS, being an
open source project initially developed during the early
1970’s during the time of command-line interfaces, has
output that is typically preformatted for ASCII viewing and
each module outputs a different format. The GRASS
community is working on standardizing such variations.
The second issue that the software architecture must
prepare for is hardware resources. This is because it is
unknown how the two models will run together on a single
machine, even if multi-processor enabled. Further, as the
number of landscape cells grows and the number of agents
grows, run-time memory may become a limitation. So, the
architecture needs to provide an efficient approach to run
both models and prepare for a possible distributed
architecture. In this case, each model, the agent and the
landscape, will reside on a separate computer. It must then
be decided where the interaction model will reside.

4.1.2 Execution Control
DEVS decouples models from their execution. There is
an explicit model specification and a simulator
specification. This enables a DEVS model to be run on
different DEVS simulators, all of which have an innate
sense of time. A GRASS model, on the other hand, is
closely tied to its execution through the scripts and
functions. Timing is provided by manually injecting the
time as a variable or through external programs. This raises
the question of how a modeler composes a model with a
simulator and a sense of time with one that has no formal
execution schema or timing. A solution under evaluation is
to use a DEVS model to provide the GRASS model with
timing. The time delta for each landscape model can be
provided to the DEVS model upon initialization and the
DEVS model could act simply as a clock that calls an
execution script for each GRASS model.
The execution of the models as a system can be handled
in two ways. The first is using a centralized control scheme.
With this approach, the system has a single control scheme
that exercises all three models. The second approach is a
decentralized control scheme. This means that the
interaction model simulator would send control messages
and data to individual model simulator/executor, which
would then exercise their respective models. Given that the
agent and interaction models are both in DEVS1 and the
landscape model requires a simulator with an innate sense of
1

Realization

4.2.2 Visualization
Visualization also plays a role in the IM software
architecture. The intent is to provide a unified, synchronized
data visualization with key data elements from both models
displayed in a comprehensive manner. It can not be assumed
that the only data that the researchers will wish to see are
those that are being passed between the two composed
models. Therefore, some method by which the interaction
model can dynamically retrieve data during simulations
must be devised. A Model-View-Controller (MVC) design
pattern is being considered as the foundation for the

And, therefore, decoupled from any specific simulator.

ISBN 1-56555-313-6

166

SpringSim Vol. 1

visualization architecture. This pattern separates the (data)
model from the visualization component and the controller
which manages both. The data model in this case is the data
resulting from formatting, aggregation, and/or mapping of
composed model data. The use of this pattern will allow the
implementation of a controller that compliments the hybrid
model framework while enabling flexibility for visualization
tools that support the data under study.

up based upon ethnographic data. Once examination of the
data is complete, the group will make decisions on where to
abstract the data in the development of the new model. This
redesign provides an opportunity to also revise the agent
model’s structure.
The current agents in the agent model are single
components. Rules, needs, capabilities, etc. are all contained
within a single model. Thus, the only capability that can be
provided to the researchers while being certain that
correctness is not compromised is the ability to modify
parameters. One approach being considered is to reduce the
agent model to component pieces, each with minimal
distinct behaviors. These pieces can then be configured at
simulation initialization. Each piece would then maintain its
specific, correct behavior with modifiable parameters.
DEVS component ports could be keyed such that specific
outputs could only be coupled to specific input ports, etc. to
improve usability. The overall agent behavior would be
modified based upon which components were coupled.

4.2.3 Implementation Specific Details
The implementation specific layer focuses mainly on
scalability from an execution (performance) perspective. To
reiterate the example given above, an environmental model
may employ millions of data elements; each at a 100 meter
scale, while an agent model uses only a couple of hundred
agents working at a 10 meter resolution. To be resolved is
the relation between the agent and the landscape. Is it a 1-to1 relationship where an agent can individually impart a
unique action on each landscape cell or 1-to-many, where an
agent’s actions are applied to a group of landscape cells?
How does the difference in number of elements within each
model effect things like wall clock time to execute the
model through a specific cycle? As posed above, what does
it mean for an agent to move 1/5th of the way through a
landscape cell?
Given that each of the composed models has no domain
knowledge of the other, the resolution for all of these
questions must be handled in the interaction model. The
resolution issue may be handled using a data mapping that is
configurable at initialization. An approach to the scale issue
is to allow the interaction model to maintain its own map
such that it keeps track of agent locations using a finer
granularity than the landscape model. The timing issue is
managed through synchronous/asynchronous event handling
and defining in what order the models are executed. This too
may be a configurable parameter at initialization.

5

4.2.4 Usability
There is one additional problem that arises as a result of
the research domain and the models’ intended users. The
MEDLAND project is targeting social scientists with little
or no formal programming skills. By using DEVSJAVA, we
are introducing an unfamiliar programming language with
unfamiliar, object-oriented constructs as compared with
scripting languages. Since this system is meant to become
an operational laboratory environment, it is necessary to
minimize the difficulty of revising agents in order to
maintain the flexibility to research more broad topics. The
challenge truly comes in not only providing this flexibility,
but ensuring the model behaviors are not changed to the
point that the model itself is no longer correct.
The agent model is currently under revision to
incorporate more details than were originally provided
under the top-down models created for the first and second
phases. This version is building the agent from the bottom

SpringSim Vol. 1

CONCLUSIONS

Composing two disparate modeling types is not an easy
task. The modeler must first decide upon a multi-modeling
approach, taking into consideration the system requirements
and domain. Next, the impacts to the specification and
execution of the model formalism should be considered.
Within the realization of the model, the modeler should
consider the software architecture, visualization, and
implementation specific layers that come into play as well.
Each of these has its own constraints.
Many research issues can be studied using each of the
multi-formalism approaches discussed in Section 2.2,
above. It will likely be a mixture of the modeler’s
preference and the requirements levied on the model that
determine which approach to use. However, if it is
important to the modeler that each sub-system retains its
formalism expressiveness to provide the best description of
the sub-system model and be loosely coupled with its
composed model to ensure the flexibility to make changes
with minimal impact to the other model, then the polyformalism approach is suitable for achieving correctness of
the modeled system. In essence, the poly-formalism
modeling approach affords the modeler two levels of
generality. First, at the formalism level, creating an IM
allows any system containing the same class of models
(DERBA and DTCA) to be composed. Note that realization
details may dictate some IM changes. Second, at the
realization level, the IM can compose any DEVSJAVA
DERBA model with any GRASS DTCA model. At this
level, only the implementation specific details within the IM
may have to be adjusted.
While there are many issues presented within this
paper, none are insurmountable as described above. The
information provided for possible solutions reflects our

167

ISBN 1-56555-313-6

research into this problem as viewed from agent-landscape
modeling and simulation. The reader may find different
solutions based upon their own requirements and domain
specifics.

[12] Godding, G., H.S. Sarjoughian, and K. Kempf. 2004.
Modeling
Approach
for
“Multi-Formalism
Semiconductor
Supply/Demand
Networks”.
In
Proceedings of Winter Simulation Conference, pp. 232239, Washington, D.C., USA.
[13] Parker, D.C., S.M. Manson, M.A. Janssen, M.J.
Hoffmann, and P. Deadman. 2003. "Multi-Agent
Systems for the Simulation of Land-Use and LandCover Change: A Review". Annals of the Association of
American Geographers. 93(2), 314 – 337.
[14] Swarm. 1996. Swarm Simulation System. Swarm
Development
Group.
Available
from
(accessed
http://www.santafe.edu/projects/swarm/
December 2006).
[15] Minson, R. and G.K. Theodoropoulos. 2000.
Distributing RePast agent-based simulations with HLA.
Concurrency and Computation: Practice and
Experience. John Wiley & Sons, Ltd., pp. 1 – 22.
[16] Ptolemy Project. 2006. Ptolemy II. Available from
http://ptolemy.berkeley.edu/ptolemyII/main.htm
(accessed December 2006)
[17] de Alfaro, L. and T. Henzinger. 2001. “Interface
automata”. In Proceedings of the 8th European
Software Engineering Conference. Vienna, Austria.
[18] Sarjoughian, H.S. and J. Plummer. 2002. Design and
Implementation of a Bridge between RAP and DEVS.
Internal Report, Computer Science and Engineering,
Arizona State University. pp. 1 – 26.
[19] Sarjoughian, H.S. and D. Huang. 2005. A MultiFormalism Modeling Composability Framework: Agent
and Discrete-Event Models. In The 9th IEEE
International Symposium on Distributed Simulation and
Real Time Applications, pp. 249 – 256. Montreal,
Canada.
[20] Sarjoughian, H.S., et al. 2005. Hybrid Discrete Event
Simulation with Model Predictive Control for
Semiconductor Supply-Chain Manufacturing. In
Proceedings of the 2005 Winter Simulation Conference,
pp. 255-266, Orlando, FL, USA.
[21] Mayer, G.R., H.S. Sarjoughian, E.K. Allen, S. Falconer,
and M. Barton. 2006. Simulation Modeling for Human
Community and Agricultural Landuse. In Proceedings
of the 2006 Spring Simulation Conference, pp. 65-72,
SCS. Huntsville, AL, USA.
[22] GRASS. 2004. Geographic Resources Analysis Support
System. http://grass.itc.it/ (accessed December 2006).
[23] Neteler, M. and H. Mitasova. 2004. Open Source GIS:
A GRASS GIS Approach, 2nd ed. Springer Science +
Business Media, Inc. New York, NY.
[24] Burrough, P.A. and R.A. McDonnell. 1998. Principles
of Geographic Information Systems. Oxford University
Press Inc. New York, NY.

Acknowledgements
This research is supported by National Science Foundation
Grant #BCS-0140269. We thank the MEDLAND team
members including Eowyn Allen, Ramón Arrowsmith,
Steven Falconer, Patricia Fall, Helena Mitasova and, in
particular, Michael Barton and Isaac Ullah for their support
with GRASS.

References
[1] MEDLAND. 2005. “Landuse and Landscape
Socioecology in the Mediterranean Basin: A Natural
Laboratory for the Study of the Long-Term Interaction
of
Human
and
Natural
Systems”.
http://www.asu.edu/clas/shesc/projects/medland/
(accessed December 2006).
[2] Davis, P.K. and R.H. Anderson. 2004. “Improving the
Composability of DoD Models and Simulations”. The
Journal of Defense Modeling and Simulation. 1(5), pp.
5 – 17.
[3] Sarjoughian, H.S. 2006. “Model Composability”. In
Proceedings of the 2006 Winter Simulation Conference,
pp. 149-158, Monterey, CA, USA.
[4] Fishwick, P.A. 1995. Simulation Model Design and
Execution: Building Digital Worlds. Prentice-Hall, Inc.
Englewood Cliffs, NJ.
[5] Zeigler, B.P., H. Praehofer, and T.G. Kim. 2000.
Theory of Modeling and Simulation: Integrated
Discrete Event and Continuous Complex Dynamic
Systems, 2nd ed. Academic Press. San Diego, CA.
[6] Hall, S. 1998. Personal Communication.
[7] Hall, S. 2005. “Learning in a Complex Adaptive
System for ISR Resource Management”. Spring
Simulation Conference.
[8] Zeigler, B.P. 2006. “DEVS&DESS in DEVS”. DEVS
Integrative Modeling & Simulation Symposium.
Huntsville, Alabama.
[9] IEEE. 2000. HLA Framework and Rules. IEEE 15162000.
[10] North, M., T. Howe, N. Collier, and J. Vos. 2005. “The
Repast Symphony Development Environment”. In
Proceedings of the Agent 2005 Conference on
Generative Social Processes, Models, and Mechanisms.
Chicago, IL, USA.
[11] North, M., T. Howe, N. Collier, and J. Vos. 2005. “The
Repast Symphony Runtime System”. In Proceedings of
the Agent 2005 Conference on Generative Social
Processes, Models, and Mechanisms. Chicago, IL,
USA.

ISBN 1-56555-313-6

168

SpringSim Vol. 1

An Approach for Activity-based DEVS Model Specification
Abdurrahman Alshareef
Arizona Center for Integrative
Modeling & Simulation
School of Computing, Informatics
and Decision Systems Engineering
Arizona State University
699 S. Mill Avenue
Tempe, AZ, 85281, USA
alshareef@asu.edu

Hessam S. Sarjoughian
Arizona Center for Integrative
Modeling & Simulation
School of Computing, Informatics
and Decision Systems Engineering
Arizona State University
699 S. Mill Avenue
Tempe, AZ, 85281, USA
sarjoughian@asu.edu

ABSTRACT

Creation of DEVS models has been advanced through
Model Driven Architecture and its frameworks. The
overarching role of the frameworks has been to help
develop model specifications in a disciplined fashion.
Frameworks can provide intermediary layers between the
higher level mathematical models and their corresponding
software specifications from both structural and behavioral
aspects. Unlike structural modeling, developing models to
specify behavior of systems is known to be harder and more
complex, particularly when operations with non-trivial
control schemes are required. In this paper, we propose
specifying activity-based behavior modeling of parallel
DEVS atomic models. We consider UML activities and
actions as fundamental units of behavior modeling,
especially in the presence of recent advances in the UML
2.5 specifications. We describe in detail how to approach
activity modeling with a set of elemental behavioral
constructs for atomic DEVS model. We show how Activity
models correspond to the atomic DEVS model using an
exemplar. We also highlight the complementary roles of
Activity and Statecharts models.
Author Keywords

Activity modeling; Behavior Modeling; Parallel DEVS,
UML.
ACM Classification Keywords

I.6.5 [Model Development]: Modeling methodologies; I.6.8
[Types of Simulation]: Discrete event, Parallel;
1.

INTRODUCTION

The Unified Modeling Language and the MDA (Model
Driven Architecture) framework are commonly used to
specify models of systems. They offer modeling constructs
(e.g., activity node and control flow) capable of specifying
DEVS atomic model behavior. Furthermore, it is possible
for some UML behavioral specifications, in conjunction
with the MDA framework, to be shown to conform to the
DEVS formalism. Tools supporting MDA offer built-in
capabilities to validate user-defined DEVS models in a
disciplined manner. Enabling early validation of
SpringSim-TMS/DEVS 2016 April 3-6 Pasadena, CA, USA
© 2016 Society for Modeling & Simulation International (SCS)

Bahram Zarrin
DTU Compute
Technical University
of Denmark
Kgs Lyngby 2800
Denmark
baza@dtu.dk

simulatable models has significant benefits, especially as
systems continue to rapidly grow in complexity and scale.
These benefits can be achieved once the model
development environment is enriched with some formalism
that has a well-defined syntax (modeling constructs) and a
sound semantic (execution protocol). The DEVS formalism
and its abstract simulator provide a suitable means to satisfy
these needs for developing and simulating system-theoretic
models [20].
There are many approaches that use well-known languages
(such as Statecharts) for specifying the behavior of discreteevent models. Some efforts focus on restricted variants of
the DEVS formalism such as FD-DEVS [8], supporting
model verification. More recently validation of DEVS
behavior, grounded in UML metamodeling and the Eclipse
Modeling Framework (EMF), a realization of the MDA, has
been proposed [16]. Auto-generated simulation models has
also received attention with few tools supporting some
basic behavior modeling.
To specify behavior of atomic DEVS models using UML
specification methods such as Statecharts, it is necessary for
the methods to conform to the DEVS formalism. This
implies that both the syntax and semantic of the atomic
model (structure and behavior) must be captured in UML
models (such as Activity models) and their variants.
Furthermore, it is important for the atomic model
specification and its simulators to be loosely coupled. This
is useful for the models to be not specific to some target
simulator.
Availability of software system modeling frameworks with
their increasing automation capabilities is invaluable for
reducing the gap between DEVS and UML abstractions
[15,16,19]. Advanced architectures of frameworks such as
EMF offer important functionalities across model
development
lifecycle.
Some
capabilities
are
straightforward to employ following guidelines and
standards, but others require rigorous analyses and further
development in order to be applied for simulation purposes.

To define behaviors of any arbitrary atomic DEVS models,
we customize the UML activities according to the atomic
DEVS formal specification. We will describe the
metamodel of activities in the context of the atomic
modeling. Due to the nature of the Activity modeling
language as a subset of UML, the usage of the activity
metamodel significantly varies with different views and
aspects in modeling process. Therefore, we consider
different views that can be taken when employing activities
to describe the behaviors of DEVS atomic models. The
activity explicitly defines a set of modeling capabilities
such as sequencing which leads to many possibilities
corresponding to different ordering and partitioning of the
behavior being modeled.
In this work, we briefly give a background about some
candidate languages for developing behavioral atomic
DEVS models with emphasize on the UML activities as
defined in the UML 2.5 specification [11]. After the related
work, we present our approach by going through different
views for specifying the behavior followed by a presenting
an activity specification for atomic DEVS model. Then, we
illustrate the usage of the actions in the atomic model
followed by an example. After that, we discuss further the
relationship with the DEVS Statecharts [3]. Finally, we
conclude and briefly discuss ongoing future research.
2.

BACKGROUND

There are many modeling languages that can be used to
specify the behavior of an atomic DEVS model. Statecharts
are used due to its power and expressiveness in representing
complex behavior of a system [7]. Statecharts defines
mainly hierarchical states and state transitions. A
Statecharts can be used to specify discrete behavior of a
system and its components. Many other languages and
metamodels also exist for modeling the behavior such as
behavior diagrams in UML [11]. They provide unique
notations (diagrams) where behavior can be captured in
different forms. Each diagram generally has some
advantages relative to some other diagrams. The diagrams
vary in their syntax as well as semantics in order to satisfy
different needs. Modelers can use state machines,
interaction, sequence, or activities for describing behaviors
within and across model components. Actions are the only
kind of the executable nodes in UML 2.5. They are
necessary to be used in order to take advantage of more
capabilities provided by the activities.
The UML activities [11] allows for behaviors to be
specified using a set of elements along with their
sequencing defined as control and object flows. The
elements are defined as activity nodes. The node can be
control, object, or executable node. Control nodes define
the flow of tokens between other activity nodes. A control
node can be either initial to define the starting point of an
activity execution, final to define when activity stops, fork
for splitting a flow into multiple flows, join for
synchronizing multiple flows, merge to act similar to join

but without synchronizing, or decision node to select
between its outgoing flows. Each control can be used to
define certain behavioral properties in the atomic model.
Object nodes are used to handle data. We will use activity
parameter node to define inputs. Finally, executable nodes
are the core elements of activities. Actions are defined as
executable nodes and therefore they can be created within
an activity. On the other hand, actions in UML 2.5 can be
only defined in the context of an activity. Thus, together
they provide a means for modeling behavior to define
processing routines that include control structures.
3.

RELATED WORK

There are many works on the concepts, methods, tools, and
technologies to make DEVS more accessible to users. In
this paper, we focus on some of the ongoing works that use
certain UML behavioral diagrams for DEVS model
specifications. Some works address the need to create
models that are ready for simulation via transformation and
translation techniques. They take into account some
common approaches for modeling the behavior of systems
such as Statecharts (state-based approach) and activities
(flow and event-based approach). They target specific
simulators where high-level model specifications can be
partially translated to code that can be executed using a
specific simulator. Other works consider model-driven
techniques in addressing the problem of specifying
behavior at different levels of abstractions. They consider
metamodeling and model-driven architecture as a means for
defining modeling languages that take into account
behavioral specifications of the system. Formal verification
is being addressed as well in some of these approaches and
frameworks using variety of software engineering methods
and tools. Our work is focused on behavioral atomic DEVS
model specification. We neither focus on transforming
models nor translating models to simulatable code. Our aim
is to enrich developing behavioral model specification.
There have been some efforts in using Statecharts for
modeling atomic DEVS behavior. In an early work, the use
of Statecharts for defining behavior of DEVS models was
proposed [17]. A mapping from DEVS models to UML
Statecharts is proposed in order to support graphical DEVS
model development [21]. In another work, an executable
framework based on UML Statecharts is developed [9].
This work shows a subset of UML Statecharts models that
conforms to certain properties making them executable. In a
recent work, a Statecharts metamodel specialized for DEVS
is proposed [3]. Other works focus on defining UML state
machines for behavioral definition along with use case,
sequence, and timing diagrams [14].
There is a significant distinction between viewing the
behavior of atomic models in activities as opposed to
Statecharts. We also think that activities as a language for
specifying system behavior has not received sufficient
attention, especially after the release of UML 2.5 [11].
There have been few works that consider activities as a way

to approach the simulation for models specified at higher
levels of abstraction. They focus on transforming the higher
level models into an executable form. In [6], the goal is to
provide a simulation for activity diagram conforming to
OMG SysML specifications. The solution was not
developed for DEVS models [13]. Instead, it proposes
transforming sequence diagrams to FD-DEVS models. It
also generates Java code for atomic models that have
simple behaviors. In the SysML profile for classic DEVS
[10], the activity diagram is used to facilitate the definition
of the external transition function. There are also other three
sub-diagrams for describing the behavioral specification of
the atomic model, those diagrams are SysML constraint
diagram for defining states, parametric diagram for defining
states association, and state machine diagram for defining
internal transition, output, and time advance functions. In a
recent work, UML activity modeling is used to define the
external, internal, output, and time advance functions for
atomic models of a health care system [12]. These models,
as compared with mathematical specification or code, are
more attractive to domain experts. However, the use of
activities in this work is incorporated with the simulation
routine resulting in an agent-based models. Behavior
modeling is achieved by representing the functions of the
atomic and coupled DEVS model component of a system as
UML activities and defining their relationships in terms of
the DEVS simulator protocol.
Ptera [4] is also another way for specifying an eventoriented model. It contains actions, final and initial
parameters that can be attributed to an event which is
represented as a vertex in the model. Activity has different
notations for each one of those attributes. It lacks the
rigorous definition of time as opposed to Ptera model of
computation.
In contrast to the existing works, our focus is to employ
activities and the activity metamodel itself and how to
enrich it to arrive at activity-based DEVS model
specifications. We will view activity as a major diagram in
the object-oriented paradigm to support the specification of
an atomic model. However, rather than using UML activity
modeling as-is, we are interested in specializing it to
capture the atomic DEVS model syntax and conform to the
DEVS execution semantics. This is useful since the UML
activity syntax and semantics is aimed at satisfying a wide
range of needs and thus lacks the necessary constructs to
specify atomic models that can conform to the DEVS
formalism.
4.

APPROACH

We begin by considering DEVS metamodels (i.e., DEVS to
SMP2 [18], MDD4MS [2], and EMF-DEVS [16]) that have
been proposed based on MDA. Based on such metamodels,
we focus on the behavioral specifications using a modeldriven approach in order to be consistent with the other
existing approaches.

UML State Machines [10] provides a specification based on
an object-oriented variant of the original Statecharts
formalism [7]. However, the metamodel of the UML State
Machines associates the Behavior element with the State
and Transition elements. Behavior defined as an effect of
Transition may also have actions assigned to it. Likewise,
Behavior can be defined for State as an entry, exit, and
doActivity. The action is placed in an activity as an
executable node. A subset of UML 2.5 metamodel is shown
in Figure 1.
Behavior

0..1
0..1
*

Activity

0..1

0..1

0..1

*

1..*

ActivityEdge

State
1

Region
0..1

*

ActivityNode

ControlNode

Transition

StateMachine

ObjectNode

*

Vertex

ExecutableNode
Action

Figure 1. A subset of behavior elements and their relationships in
UML 2.5 metamodel.

Here, we examine how activities can be employed in the
context of atomic DEVS modeling. Activities may be used
from different point of views in modeling behavior of an
atomic model. A view may also depend on using other
kinds of models. We consider three views for specifying the
behavior using activities. The first view is to create a
separate activity diagram for each routine belonging to a
function. For instance, the activity captures the behavior,
including actions and their order, for using an input to set
the state of an atomic model. For each control state, there
are a set of activities defined to handle one or more inputs.
The second view is to create an activity for each function.
For an atomic model, there are five activity diagrams
corresponding to each of the external transition, internal
transition, confluent, output, and time advance functions.
The third view defines one activity diagram that
corresponds to the behavior of an atomic model as specified
by all of its functions.
4.1. Three views for specifying atomic model behavior

In the first view, an activity is defined to specify the
behavior for each event-routine as a subordinate unit of the
corresponding DEVS atomic model component. We refer to
them as sub-activities. It takes advantage of the existing
definitions provided in the activity diagram to complement

sub-activity
Input received

Set phase

Set sigma

(a) A sub-activity to describe a simple routine.

atomic
<<iterative>>

x

Read phase

Initialize

output

Set sigma

Send output

deltInt

Set phase

Set phase

Set sigma

(b) One activity for the atomic model.
Figure 2. Different views of activities DEVS modeling.

Statecharts by ordering actions. It does not involve complex
behavior handling with respect to other DEVS components
nor the simulator. However, it can represent some basic
patterns intrinsic to modeling, for example, the external
transition function as shown in Figure 2(a). Other
procedures are assumed to be handled externally in a
separate activity or even in a different model that can
communicate with the activity model. An example of this
behavior would be handling decision points based on the
control state for the external transition function. The
selected activity is executed directly once being called.
Thus, it is a suitable option when those activities are being
sub-models due to their simplicity by removing most of the
complexity encountered when handling the total state of an
atomic model in state transitions.
The notion of activities provide some means for handling
more complex scenarios among atomic model parts in
addition to the capability of handling flows. We devised the
second view to capture the concept of control in DEVS
functions. An activity is created for the external and internal
transition, confluent, output, and time advance functions. In
this view, we use additional constructs for representing
behavioral patterns that can be captured in the DEVS
functions. For example, decision node can be used to check
for the current phase. Decision nodes can also be used to
check ports and input values. Arbitrariness is presumed for
any ordering that is not explicitly defined in the model; this
conforms to the parallel DEVS formalism. For instance, the
expansion region construct can be used to iterate a
collection of received inputs in an arbitrary order. The
execution of such a scenario is given by the simulator
protocol and its implementation in some target simulator.
These activity models, as compared with the first view,
have more artifacts for handling more complex patterns.
However, the modeler has more capabilities at hand in

modeling and constraining the behaviors of atomic DEVS
models.
Considering the functions of an atomic model to be viewed
in one activity is also possible; this is the third view.
However, this holistic view necessitates having definitions
and artifacts involving relationships between the atomic
model functions. Unlike the second view, implicit
relationships, for example, between internal and external
transition functions may be modeled. In this view, we may
also take into account the simulation protocol. For example,
in DEVS-Suite simulator, atomic model’s components are
specified and also simulated independently of each another.
In this view, the behavior of the model with respect to the
simulator has to be defined explicitly in some structures
that conform to the simulator interfaces using some activity
artifacts such as accept event actions. A major drawback for
this view is that model specification is tightly dependent on
the abstract simulator. The behavior of a simple processor
model [20] (it processes inputs it receives and sends out
processed inputs) is shown in the first view Figure 2 (a) and
the third view (Figure 2 (b)). Next, the second view is
detailed along with the processor model shown in Figures 3
and 4.
4.2. Activity specifications for atomic DEVS model

Considering all the UML activity constructs [11], we can
model even more complex behavior by using fork, join,
decision, merge nodes, and expansion regions. We also use
call behavior actions. The nodes are to specify the behavior
of the atomic model components where the call behavior
actions are to depict the communication points with other
models. Each node is used to represent some specific
concepts in the atomic models. We describe these artifacts
and their corresponding concepts in the atomic DEVS
model. Then, we present how these artifacts (see Table 1)

can be used to define the behavior of the exemplar
processor model with multiple inputs.
The fork node splits the flow into concurrent flows, each
having multiple input events. Expansion region can be also
used for the same purpose. Unlike the fork node, expansion
region is a specialized action which is a structured activity
node. It defines concurrent flows of its included elements
for the number of input events in the collection of received
input events. However, the execution might be performed
sequentially based on the simulation engine design and its
implementation. Iterative expansion region processes the
collection of received inputs sequentially. The order of
processing these inputs is arbitrarily determined. In a
Parallel atomic DEVS model, multiple inputs can be
received simultaneously. Therefore, the activity has a
collection of inputs processed in an iterative expansion
region. Further, these inputs can be examined inside the
region via some activity nodes nested in the same region.
The activity can be terminated inside the region.
Decision node provides a means for controlling the flow by
having multiple outgoing edges with guards assigned to all

edges. For example, the decision node can be used to
control the flow based on the phase. The phase is read in a
preceding action and then evaluated by some guard
conditions associated with some outgoing edges from the
decision node. An “else” can be also defined as a guard for
one of the outgoing edges.
We can also use a special kind of action called
CallBehaviorAction. It allows invocation of some behavior
in a different model. We can use this action to call the
behavior specified in the sub-activities discussed in the first
view. For each input arriving in some control state, the subactivity is called synchronously meaning that the main
activity will not proceed until the called behavior is
completed.
We now create multiple activities for atomic model
behavior as shown in Table 1. These activities are created
for modeling the behavior of an atomic DEVS model with
multiple inputs. We model the external, internal transition
and output functions, each in a separate activity. Activity
models for the time advance and confluent transition
functions can be specified in a similar manner. We use the

deltExt
<<iterative>>

A template for an activity diagram for the external transition
function receiving a collection of inputs and processing them in
iterative expansion region

x

output
<<iterative>>

deltInt
<<iterative>>

y

A template for an activity diagram for the output function with an
output parameter node

A template for an activity
diagram for the internal
transition function

An action to call behavior
specified in another activity

Read structural feature action
to read the input and assign it
to the output pin

Receive a value and assign it
to the phase

Read structural feature action
to read the phase and assign it
to the output pin

Set value and assign it to the
output pin

Read value of the received
feature via the input pin

Decision, merge, join and fork
nodes to control the flow

Table 1. Activity specifications for atomic DEVS model.

artifacts as described in Table 1 to provide the required
modeling capabilities for capturing the behaviors of atomic
DEVS models. The semantics of these artifacts as defined
in the UML specification have been aligned with the
concepts defined for DEVS.
4.3. Action specifications for atomic DEVS model

Actions are used as executable nodes in the activity
diagram. They are the fundamental units of behavior
specification in UML. Some actions change the state of the
system. This kind of actions in our approach satisfies how
states can be changed in the DEVS atomic internal and
external transition functions. For example, a phase change
is modeled using add structural feature value action named
“Set phase” and the value of the target phase is modeled
using value specification action with the value of the target
state, which is then sent out through the output pin using an
object flow connected to the value input pin of “Set phase”
action. The actions can be placed in the activities
corresponding to the external and internal transition
functions. Similar procedure is followed for setting sigma
or other state variables.
Other kinds of action support handling objects. Read self
action is used to obtain the current object context and place
it in its output pin. Value specification action is also used to
provide certain value and place it in its output pin as
mentioned in giving the target phase value for setting the
phase. The structural feature actions are used for either
assigning or retrieving a structural feature of an object.
Both are used for the phase as a structural feature. They can
be also used for other features or more complex objects.
Event actions can be also used. An accept event action can
be used to model the waiting for an event to occur in some
other entity to proceed in the activity flow. In the context of
UML actions, this event can be caused by the simulation
protocol to trigger some other parts of an atomic model, or
by other models that decompose the behavior into multiple
models. There can be also an accept time event action
which is basically used to model wait time. We also use a
send signal action which can be used to model invocation
for some other components. It is also used to explicitly
enforce some order when used with accept event. For
example, to enforce the order of executing the output and
internal transition function, a sending event signal must be
completed to enable the accept event action although this
sort of scenarios could be viewed as part of the simulation
protocol.
We also use invocation actions. An example of these
actions is send signal action. A call behavior action is used
to call either a behavior. The action can be synchronous if it
has to wait for the called behavior to be completed.
Otherwise, it can be asynchronous and then immediately
proceed after calling the associated behavior or operation.
We use synchronous call behavior action in the external
transition activity in order to enforce the activity to wait for
the completion of sub-activities before proceeding to other

activities. This is necessary since the behavior of atomic
model is executed sequentially (i.e., the behavior in any of
the functions of an atomic model is sequential).
Using the described activity diagram artifacts, we can
create activities for the output and internal transition
function for the processor model shown in Figure 3. The
activity for the external transition function is shown in the
following section.
output
Read phase

[busy]

Make
content

y

deltInt
Set phase

Set sigma

Figure 3. Activity models for the processor (simplified).
5.

STATECHARTS AND ACTIVITIES

The actions belonging to activities can be used with states
and transitions in Statecharts. These actions can be viewed
as the sub-activities in activity models detailed in the
previous section. This approach follows the syntax of UML
2.5 where the effect of the transition is described using the
Behavior element which is a generalization for the Activity
as well as the StateMachine (see Figure 1). We consider the
DEVS metamodel [16] as a higher level abstraction to
couple behaviors of atomic metamodel [15] through
coupling their input and output ports as formalized in the
parallel coupled DEVS model.
Together Statecharts and Activity models support richer
behavioral modeling for atomic DEVS models. Figure 4
illustrates this approach and highlights basic relationships
between Activity and Statecharts models. The DEVS
structural metamodel for the atomic and coupled DEVS
models is not the focus except the input and output ports for
atomic models. The top left-hand side shows the coupled
model GP (GeneratorProcessor) composed of generator
(Generator) and processor (Processor) models. The top
right-hand side shows the external transition function of the
exemplar processor in the activity model while the bottom
left-hand side shows the external function of the same
exemplar processor as a Statecharts. Finally, the bottom
right-hand side shows one of the sub-activities. The subactivity can be decomposed such that it includes other
activity to develop behavior models that are simpler (i.e.,

avoid developing unnecessarily complex behavioral model
specifications). The decomposition process must conform
to the relationships with the Behavior element as defined in
the UML 2.5 metamodel.
The Activity and Statecharts models shown in Figure 4
provide complementary behavioral specifications for the
processor model. Both can be used to describe DEVS
model specifications; none of these alone is known to be
sufficient to have a complete specification of an arbitrary
atomic model. Given the modularity in DEVS (models can
only communicate with one another through couplings), the
Activity and Statecharts models can only represent
encapsulated behaviors of atomic models. These behavioral
models capture different aspects of atomic models using
distinct modeling syntaxes and semantics. We can use these
behavioral models to concretely represent the abstract
mathematical specifications of the atomic model functions.
In the processor Activity model, the inputs for the atomic
model processor is defined as an input parameter for the
activity. The input, however, is defined as an event for its
corresponding transition in the Statecharts model. Both
models define phase state transition from passive to active.
In the activity model, we use multiple actions and control
nodes to describe the implementation specified in the state
transition. We use the read phase action, decision nodes,
and set phase action. This sequence of nodes in the activity

model is equivalent to state transition and actions defined in
the Statecharts model.
Using Activity modeling overcomes some limitations in the
other Object-Oriented modeling methods given its unique
capabilities such as sequencing, control and data flow. Use
of DEVS Activity modeling is promising for generating
models that can be executed in simulators. This is
especially useful when considering the ongoing efforts to
have tool support for defining model specifications and
automated code generation. Overall, as Figure 4 shows,
Statecharts and Activity models complement each other and
support creating richer specification for atomic models.
The activity diagrams shown in Figure 4 may be developed
in tools such as Papyrus [5]. A modeling engine specialized
for developing activity models for atomic DEVS models
can also be developed. The coupled GP model and the
external transition function belonging to the Statecharts of
the Processor model are developed in CoSMoSim [1].
6.

CONCLUSION AND FUTURE WORK

Few approaches are proposed to utilize the potential
benefits of employing MDA concepts for DEVS. These
approaches follow guidelines to enrich development of
simulation models. Including behavioral specifications in a
model development lifecycle demands careful usage and
guided by MDA framework. Building models in stages can
Processor External Transition Activity

x: Message
<<iterative>>

Read phase

[phase is
passive]
[port is in]

Call sub-act for
passive in

Set
phase

[else]

Call sub-act for
passive else

Set
phase

Processor External Transition State Machines
Job Passive Sub-Activity

Processing time

Set sigma

Figure 4. The overall view of the approach and relationships between different models of processor behavior.

help modelers move across model abstractions. We can
start developing Activity models and supplement them with
Statecharts models, for example.

9.

In this work, we described using modeling artifacts such as
activity nodes and activity edges to specify behavior
patterns such as sequencing and synchronization defined in
Activities Behavior metamodel. We proposed customizing
activity modeling to specify behaviors for atomic models.
The activity metamodel can be used in various ways. We
discussed creating activity models considering different
views and exemplified behavioral activity modeling for an
atomic processor model by a set of DEVS-based activity
constructs. For future work, we plan to extend this work to
support domain-specific activity-based behavioral DEVS
modeling. We also plan to extend the CoSMoS to support
Activity modeling for DEVS atomic models. This requires
Statecharts and Activity models not only conform to the
DEVS formalism, but also consistent with respect to one
another. Achieving this kind of capability may lead to
improved model verification and validation.

10. Nikolaidou, M., Dalakas, V., Mitsi, L., Kapos, G.D.,
and Anagnostopoulos, D. A SysML Profile for
Classical DEVS Simulators. Proceedings of the 2008
The Third International Conference on Software
Engineering Advances, (2008), 445–450.

REFERENCES

1.

ACIMS.
CoSMoSim.
https://sourceforge.net/projects/cosmosim/.

2015.

2.

Cetinkaya, D., Verbraeck, A., and Seck, M.D.
MDD4MS: a model driven development framework for
modeling and simulation. Proceedings of the 2011
Summer Computer Simulation Conference, (2011),
113–121.

3.

Fard, M.D. and Sarjoughian, H.S. Visual and
Persistence Behavior Modeling for DEVS in CoSMoS.
Proceedings of the Symposium on Theory of Modeling
& Simulation-DEVS Integrative M&S Symposium,
(2015), 227–234.

4.

Feng, T.H., Lee, E.A., and Shruben, L.W. Ptera: an
event-oriented model of computation for heterogeneous
systems. Proceedings of the tenth ACM international
conference on Embedded software, (2010), 219–228.

5.

Eclipse Foundation. Papyrus Mars release (1.1.3).
2016. https://eclipse.org/papyrus/.

6.

Foures, D., Albert, V., Pascal, J.C., and Nketsa, A.
Automation of SysML Activity Diagram Simulation
with
Model-driven
Engineering
Approach.
Proceedings of the 2012 Symposium on Theory of
Modeling and Simulation-DEVS Integrative M&S
Symposium, (2012), 1–6.

7.

Harel, D. and Politi, M. Modeling reactive systems with
statecharts: the STATEMATE approach. McGraw-Hill,
Inc., 1998.

8.

Hwang, M.H. and Zeigler, B.P. Reachability graph of
finite and deterministic devs networks. IEEE
Transactions on Automation Science and Engineering
6, 3 (2009), 468–478.

Mooney, J. and Sarjoughian, H. A framework for
executable UML models. Proceedings of the 2009
Spring Simulation Multiconference, (2009), 1–8.

11. OMG. Unified Modeling Language version 2.5. 2012.
12. Ozmen, O. and Nutaro, J. Activity Diagrams for DEVS
Models : A Case Study Modeling Health Care
Behavior (WIP). Proceedings of the Symposium on
Theory of Modeling & Simulation-DEVS Integrative
M&S Symposium, (2015), 1006–1011.
13. Pasqua, R., Foures, D., Albert, V., and Nketsa, A.
From Sequence Diagrams UML 2.x to FD-DEVS by
Model Transformation. European Simulation and
Modelling Conference, (2012), 37–43.
14. Risco-Martin, J.L., de la Cruz, J.M., Mittal, S., and
Zeigler, B.P. eUDEVS: Executable UML with DEVS
Theory of Modeling and Simulation. Simulation 85,
11-12 (2009), 750–777.
15. Sarjoughian, H.S., Alshareef, A., and Lei, Y.
Behavioral DEVS Metamodeling. Proceedings of the
2015 Winter Simulation Conference, IEEE Press
(2015), 2788–2799.
16. Sarjoughian, H.S. and Markid, A.M. EMF-DEVS
modeling. Proceedings of the 2012 Symposium on
Theory of Modeling and Simulation-DEVS Integrative
M&S Symposium, (2012), 1–8.
17. Schulz, S., Ewing, T.C., and Rozenblit, J.W. Discrete
event system specification (DEVS) and StateMate
StateCharts equivalence for embedded systems
modeling. Proceedings Seventh IEEE International
Conference and Workshop on the Engineering of
Computer Based Systems, 2000. (ECBS 2000), (2000),
308–316.
18. Yonglin, L., Weiping, W., Qun, L., and Yifan, Z. A
transformation model from DEVS to SMP2 based on
MDA. Simulation Modelling Practice and Theory 17,
10 (2009), 1690–1709.
19. Zeigler, B. and Sarjoughian, H.S. Guide to modeling
and simulation of systems of systems. Springer Science
& Business Media, 2012.
20. Zeigler, B.P., Praehofer, H., and Kim, T.G. Theory of
modeling and simulation: integrating discrete event
and continuous complex dynamic systems. Academic
press, 2000.
21. Zinoviev, D. Mapping DEVS Models onto UML
Models. DEVS Symposium, Spring Simulation
Multiconference, (2005).

Model Composability and Execution across Simulation, Optimization, and
Forecast Models
Hessam Sarjoughian1, James Smith1, Gary Godding2, Mohammed Muqsith1
1

Arizona Center for Integrative M&S
School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University, Tempe, Arizona, USA
sarjoughian@asu.edu, james.m.smith@asu.edu

Keywords:
DEVS,
Demand
Forecasting,
KIB,
Optimization, Semiconductor Supply-Chain, Planning,
Simulation.
Abstract
We present a novel simulation platform called Optimization,
Simulation, and Forecasting (OSF) for the domain of
manufacturing and logistics supply-chain systems. It
supports composition of DEVS, Linear Program (LP), and
forecast models using an extended Knowledge Interchange
Broker (KIB). Models developed in DEVS-Suite simulator,
OPL-Studio optimization, and a heuristic Inventory Strategy
Forecaster (ISF) can be composed using a new set of
scalable XML Schemas developed for DEVS, LP, and ISF
models. The addition of forecast modeling offers new kinds
of supply-chain system simulation. In particular, alternative
customer demand forecast “bias correction” methods can be
evaluated towards optimized operation of supply-chain
processes. The OSF platform affords modeling interactions
among process, optimization, and forecast models. The KIB
coordinates simulation (execution) of the DEVS, LP, and
ISF models in a sequential fashion. Composition of each
pair of DEVS, LP, and ISF models leads to scalability for
specifying model interactions. Independent execution of
each model allows flexible computation platforms. They
simplify defining a large number of different data
transformations. The concept, basic architectural design, and
implementation of this composable simulation platform are
highlighted using example single-echelon and multi-echelon
semiconductor manufacturing, logistics systems.

1

Introduction

For a supply-chain system to be efficient across time and
space, customer orders have to be placed weeks in advance,
factories should be used at their maximum capacity with
minimal resources, and sufficient inventories must be
available to be shipped in advance of expected delivery
dates [3]. This requires a supply-chain system where
uncertainty in supplies, process operations, and customer
demands can be collectively accounted for. To this end,
many people have recognized the importance of both cost
effectiveness and high customer satisfaction. To achieve this

2

Intel Corporation
Chandler, AZ, USA
gary.godding@intel.com

goal, models for the manufacturing processes and decision
making have been previously developed and experimented
within an industrial setting. Among existing approaches,
discrete-event process simulation using Parallel Discrete
Event System Specification (DEVS) and optimized decision
making using Linear Programming (LP) have been proposed
[2,5]. A DEVS/LP Knowledge Interchange Broker (KIB)
has been developed and employed in realistic semiconductor
supply-chain application domain [2]. Discrete-event process
models simulate physical movement of products, for
example, across a semiconductor manufacturing supplychain consisting of fabrication, testing, inventories, and
shipping. Decision plans can project future customer
demand using the knowledge of the manufacturing state.
Decisions that can lead to (near-) optimal operation of the
supply-chain can be generated. The optimization has the
task of handling the uncertainty in long-term future
customer demand which is inherently inaccurate, sometimes
by orders of magnitude. Decisions such as how many
products to be moved from one node (e.g., factory) of the
supply-chain to another node (e.g., inventory) play a central
role – it can increase customer demand satisfaction while
lowering cost of operating supply-chain processes.
However, for optimization to generate better decisions,
it needs to have reliable customer demand forecasts.
Customers project what they need weeks or months ahead of
the actual dates the products may actually be needed. This is
beneficial for customers as they can overestimate their
needs since they often have the option of reducing or
canceling their orders without penalty or at greater cost.
This results in a “biased customer demand”. In order to
handle biased customer demand, forecast modeling has been
proposed [3]. A successful bias correction can significantly
reduce inaccuracy in customer demand projection. Accurate
customer forecast demand modeling such as Inventory
Strategy Forecast (ISF) may lend to better optimized
decisions, which in turn can increase manufacturing and
logistics efficiencies – i.e., better use of manufacturing
resources (less cost) while improving customer satisfaction
(increased service level).
In the remainder of this paper we will describe a novel
simulation platform which integrates three subsystems. This
platform is referred to as OSF (Optimization, Simulation,

and Forecasting). The DEVS-Suite simulator, OPL-Studio
optimization, and their accompanying DEVS/LP
Knowledge Interchange Broker already exist [2]. The
KIBDEVS/LP supports modeling and executing the interactions
between discrete process and optimization models. The third
subsystem, an Inventory Strategy Forecaster (ISF), is aimed
at countering bias in future customer demand. The addition
of this subsystem has resulted in the OSF simulation
platform, enabling optimized operations of the supply-chain
processes based on handling the uncertainty in future
customer demand. The ISF can employ Exponential
Smoothing (ES) and Kernel Smoothing (KS) strategies to
reduce bias in future customer demand given historical
forecast and actual demands. In order to support inclusion of
the forecasting subsystems, the KIBDEVS/LP XML Schema is
reformulated. The new XML Schema affords scalability in
terms of simplifying data exchange definitions as well as
composing ISF with both DEVS and LP models. After a
brief description of supply-chain process and optimization
models, we will discuss some key aspects of ISF. Then, we
highlight the KIB reformulation. Thereafter, we present
sample results to show the kinds of evaluations that can be
supported using OSF platform. In the rest of the paper, we
briefly compare this platform with closely related ones and
provide a summary of this research and future work.

2

Background

A multi-echelon supply-chain system is a chain of singleechelon (factory or logistic) stages starting from product
producers and ending at product consumers. Each echelon is
defined to consist of process, shipping, and inventory parts.
An example is shown in Figure 1. Echelon1 consists of
finished goods warehouse (CW), shipping (Ship), and hub
(Hub). These single echelons form a multi-echelon
manufacturing, logistics supply-chain where an echelon may
have adjoining up-stream and/or down-stream echelons.
Echeloni+1 is upstream of Echeloni. The most down-stream
echelon in a supply-chain of any length is Echelon1. Each
element in an echelon can receive inputs, operate on the
inputs, and produce outputs. Echelon1 is connected to
customers. The customer for any up-stream echelon
(1< i  n) is its immediate down-stream echelon. In this
formulation, CW is a process for Echelon1 and a hub is a
process for Echelon2. For a supply-chain system spanning
multiple geographies, a customer at the end of a supplychain (i.e., a geographic location for product delivery) is
referred to as a Geo-Customer (GC). The products available
at the hub can be delivered to GC immediately.
Each part in an echelon can have its own physical
characteristics. For example, a finished goods warehouse
may have infinite capacity while shipping, hub, and geocustomer may have finite capacity. These parts have other
characteristics; in particular, their operations can be
controlled externally. For example, flow of products from

CW to hub via shipping can be managed using an
optimization model that can satisfy a set of constraints
defined across an echelon or a chain of echelons.
Optimization can determine the release command for CW to
Hub – i.e., the number of products that are to be shipped
given a desired safety stock at a future time for the Hub. In
the multi-echelon case, the optimization model can compute
multiple release commands. Release commands can be
computed for any number of echelons. Alternatively, in the
manufacturing/logistics process, the state of a segment can
be used to control the flow of produces within one echelon
or multiple echelons. For example, given the states of the
finished goods warehouse and hub depicted in Figure 1, the
former can compute how many products it can send to the
latter [5]. A combination of release commands and state
information such as processing time at factory can be used
to calculate the number of products that can be sent to the
inventory.
Numerous efforts have been undertaken to assist in
cost-effective operations of supply-chain systems. A class of
approaches and tools use simulation modeling in
combination with decision making tools. In particular,
simulation can represent past and current state of physical
processes (e.g., Echelon1 in Figure 1) whereas algorithms
such as linear programming produce plans (i.e., directives)
for the manufacturing or logistics process.
The illustration shown in Figure 1 can be concretely
characterized from four perspectives. First is physical
product engineering and movement (manufacturing/logistics
processes). Second is computing optimized plans to operate
the physical echelons (optimized processes). Third is
removing bias from future customer demand and predicting
inventory safety stock (unbiased inventory holdings). Forth
is synthesizing interactions among process, optimization,
and forecast models using KIB. Details of
manufacturing/logistics and optimization models have
already been detailed elsewhere [2,5]. In the following
section, the ISF from the perspective of OSF will be
described and the KIB will be detailed.

Figure 1. A two-echelon supply-chain system with four kinds of
models

3

OSF Platform

The OSF platform consists of optimization, simulation,
forecasting, and KIB. Optimization provides a plan for the
simulation over a finite time horizon. Simulation captures
current state, operations, and events of the semi-conductor
manufacturing and logistics processes. Forecasting removes
bias from future customer demand. KIB handles data
transformations (aggregation and disaggregation) as well as
exchange frequency among disparate DEVS, ISF, and LP
models. In the following, we describe the novel
contributions behind the OSF architecture and two of its
constituents (i.e., ISF and KIB).

3.1

Approach

A conceptual architecture for OSF is shown in Figure 2.
From a high-level abstraction view, the parts of the diagram
show DEVS, ISF, KIB, and LP. Two copies of a database
containing Actual Customer Demand (ACD), Historical
Forecast Customer Demand (HFC), and Forecast Customer
Demand (FCCD) are used with the Simulation and
Forecasting parts. Simulation also uses Lot Generator (LG)
data contained in the database. Two copies of the database is
necessary because ISF is a logical, standalone subsystem
both from implementation and execution perspectives (see
Section 3.4). ISF can handle multiple echelons based on
Base Stock and Multiple Echelon Inventory Optimization
(MEIO) approaches [3].
The KIB has three pairs of interfaces (INT), two for
each of DEVS, ISF, and LP subsystems. The simulation
state is sent to optimization and optimization plan is sent to
simulation. The DEVS and ISF interfaces together can
transform simulation state (e.g., hub inventory) and forecast
data (e.g., hub safety stock) that are needed for optimization.
Similarly, optimization plan can be transformed to meet the
needs of the simulation. Basic examples are Beginning OnHand (BOH) inventories that can be aggregated across
seven days, assuming each optimization ‘solve’ is for a
period of one week and simulation execution cycle
represents one day. Other exchange configurations can be
devised (e.g., optimization, forecasting, and simulation may
execute on a daily period or optimization may use the sum
of BOHs across multiple inventories). As shown in
Figure 2, forecaster has a uni-directional data exchange with
optimization. Similarly, simulation and forecasting have a
uni-directional relationship where simulation sends its
current week number (Wk#) to forecaster. This is needed to
guarantee the forecaster is synchronized with simulation and
thus creates a well-formed synchronization among DEVS,
ISF, and LP model executions. This approach provides
simplicity and flexibility as compared, for example, with a
design where the forecaster is wrapped inside a DEVS
atomic model.

Figure 2. Conceptual OSF architecture

3.2

Inventory Strategy Forecaster

The intent of this section is to highlight the challenge of
forecasting inventory holding under demand uncertainty
[3,4]. It serves an important role in optimizing the operation
of stochastic manufacturing/logistics processes. An ISF,
also known as Inventory Strategy Module (ISM) in other
works, can employ different bias correction strategies in
computing future forecast customer demand and safety
stock given historical forecast and actual demands. Excess
products can handle unexpected increase in customer
demand, but at a cost.
An inventory strategy such as ISF can be used to
compute safety stocks to safeguard against inevitable
variability in both supply and demand forecasts. Inventory
can be characterized in terms of order size, time to order,
and holding status. These combined with variables such as
future time horizon for which safety target is to be
computed can be used to develop different smoothing
algorithms. These heuristics algorithms remove bias in
future customer demand data given historical and actual
data. For this work, an ISF model has been developed using
exponential and kernel smoothing techniques [9]. The
model can use exponential, kernel, or no smoothing in order
to determine safety stock for a future time horizon (e.g.,
several weeks). Such heuristic models compute minimum
safety stocks. It may be possible to minimize
manufacturing, delivery, and other costs across supply-chain
while maximizing customer satisfaction (i.e., Service
Level). In general, a range of safety stock values are
computed for a given set of desired service levels (e.g.,
60%, …, 100%). The ISF has been developed to
demonstrate the OSF platform support for single and twoechelon supply-chain systems. It can handle, in a scalable
fashion, multiple hubs and products which are needed for
Sequential Base Stock [4] or Multiple Echelon Inventory
Optimization (MEIO).

There are well known models for characterizing
inventories [4]. An inventory can be characterized to have
Bin1 and Bin2. The size of Bin2 specifies the safety stock and
the size of Bin1 specifies the order-up-to. One formulation is
called Order-point, Order-Quantity system (s, Q). This
model is easy to understand. It can be used when production
requirements for supplier can be predicted and no more than
one replenishment order is outstanding at any given time.
For large transactions, the replenishment may be too small
to raise it above the re-order point. Another formulation is
called Order-point, Order-Up-to-Level system (s, S). In this
model, replenishment quantity is variable. The ISF is
defined based on the (s, S) model. In Figure 3, it is shown
that the inventory position can be above or below the reorder-point for a given discrete period (e.g., one week). For
some on-order and on-hand stocks, inventory position can
fall below re-order-point which then requires placing order
given order-up-to over some future time horizon (S - IP) and
safety stock.

on-hand stock (stock available in the
inventory; cannot be less than zero)

OH

order-up-to

S

re-order-point

s

order-quantity

S-IP

order-quantity (fixed replenishment)

Q

committed (unavailable stock)

C

safety stock (average net stock
before replenishment arrival)

SS

Figure 4. ISF forecast customer demand prediction

3.3

Figure 3. Inventory model
ISF can compute FCCD given HFC, ACD, and the
current week (see Figure 4). The kernel and exponential
smoothing algorithms reduce (or ideally remove) bias from
future customer demand. ISF can then determine safety
stocks for the inventories belonging to the
manufacturing/logistic system. The optimization model
optimizes release commands given safety stock and actual
forecast customer demand.
Table 1. Inventory model variables and parameters
Terms
inventory position
net stock (can be negative)
on-order stock (replenishment)

Symbols
IP = NS + OO – C
NS = OH - BO
OO

Knowledge Interchange Broker

The abstract KIB model is motivated by multi-formalism
modeling [7]. Concrete models, methods, and domainspecific models for semi-conductor supply-chain systems
(DEVS/LP [2] and DEVS/MPC [5]) are developed. The
basic KIB model is grounded in systems theory where it has
inputs, states, outputs, and time-based functions. As an
executable model, it has syntax and operational semantics.
XML-Schema is used to define syntax of interactions
among different kinds of models. For OSF data
transformation and execution control, transformation and
control schemes are specified as XML constructs that
comply with XML-Schemas, defined for DEVS, LP, and
ISF (see Figure 5). The use and execution of the data
transformations (i.e., XML schema instance models) are
achieved using components developed through the Java
programming language.
Data transformations are defined and executed
according to a clock and time indexes. From the perspective
of the simulation, these transformations occur
instantaneously. The aggregation, disaggregation, and other
input/output mappings occur with respect to the simulation
clock and time-indexes contained in data from the models
that predict multiple time-steps given the current state of the
simulation. For example, LP model computes optimized

inventory release commands and holdings for future timeindexes (e.g., several weeks) given the current state of
manufacturing and logistics processes. It should be noted
that timing aspect of the KIB which is key for time-based
data transformations is not shown in Figure 5. The
execution control uses clock (current time) and future timeindexes included in the data provided by the ISF and LP
models (more details are provided in Section 3.2.2).

held at a hub. The data outputs are strongly typed. The
echelon index is for multiple echelons (e.g., echelon 1 in
Figure 1).

(a) with its constituent modules

Figure 5. Illustration of the tri-model KIB specification
3.3.1
Interaction Schemas
The original XML formulation for data and their
transformations were based on a singular view of
interactions between two disparate models. An XML
instance contained all the elements needed with respect to
its counterpart XML instance. Furthermore, elements would
refer to the names of the interfaces. This requires redefining
XML Schemas where new interfaces are needed. As a
consequence, these XML schemas and their instances are
weak in terms of generality and scalability.
In order to make the XML Schema more flexible and
scalable, a new XML Schema has been developed (see
Figure 6) [9]. The new design allows defining data
transformations based on the concept of models. A Model
XML Schema (see Figure 6(a)) has modules, each of which
having input and output data sets. Aside from the Model
XML Schema, Relationship (See Figure 6(b)) and Control
XML Schemas have also been defined. One is for mapping
and transformations. An example partial XML code for
ISF/LP is shown in Figure 7. This partial code defines
FCCD and hub safety stock outputs for a single-echelon
supply-chain system (see Figure 1). In general a single
echelon can have multiple hubs with each hub holding
different products, and different amount of products may be

(b) Relationships between pairs of source and target models
Figure 6. KIB XML schema specifications
<Model Interface="ISF" Name="SupplyChainISF"> 
  <Module Name="ISF_TARGET"> 
    <DataOutput Name="FC_CD"> 
      <DataVariable Name="echelon_index" Type="Int" IsKey="true"/> 
    <DataVariable Name="hub" Type="String" IsKey="true"/> 
    <DataVariable Name="product" Type="String" IsKey="true"/> 
    <DataVariable Name="quantity" Type="Int" IsKey="false" 
ArraySize="Variable"/> 
    </DataOutput> 
    <DataOutput Name="HUB_SS"> 
      <DataVariable Name="echelon_index" Type="Int" IsKey="true"/> 
      <DataVariable Name="hub" Type="String" IsKey="true"/> 
      <DataVariable Name="product" Type="String" IsKey="true"/> 
      <DataVariable Name="quantity" Type="Int" IsKey="false 
ArraySize="Variable"/> 
    </DataOutput> 
… 
</Module> 
 

Figure 7. Snippet XML model for ISF model

3.3.2
Execution regime
The executions of DEVS, OPL-Studio, and ISF are
independent from one another. Different instances of OSF
system models can be developed using the generality of the
KIB. An important aspect of this composition is the
execution control. Their interactions are controlled in a
sequential manner. For example data from ISF is passed
through
the
KIB
to
LP.
Data
from
the
manufacturing/logistics model can be aggregated at the
DEVS KIB interface and then delivered to LP. The order of
executions is DEVS  KIB  ISF  KIB  LP  KIB
[9]. In terms of timing, ISF, KIB, and LP do not consume
simulation clock time. This is meaningful in the context of
some manufacturing/logistics systems where forecasting,
optimization and data transformation can be assumed to be
instantaneous.
It can be observed in Figure 8 that ISF is concerned with
future whereas the current state of the system as captured in
simulation uses cumulative actual customer demand. For a
given time t, forecast and optimization models predict over
n simulation timer periods as compared with simulation
which predicts over time period t (see Figure 8) For
example, simulation is on a daily cycle whereas forecast and
optimization are on a weekly basis. When the simulation,
forecast, and optimization periods are different, the
frequency for data transformation is also specified as part of
the KIB execution control.

Figure 8. Simulation, forecast and optimization timing with
respect to past, current and future time periods

Figure 8. Evolution of forecast demand as a function of
current time period

3.4

Implementation

The Optimization, Simulation, and Forecasting (OSF)
platform is an extension of the simulation/optimization
platform built using DEVS-Suite simulator, OPL-Studio,
and KIBDEVS/LP. In OSF, the latest CPLEX Optimization
Studio (version 12.4) is used. The OSF has a Forecaster
implemented in Java with RMI support. The KIB is
extended to support DEVS, LP, and Forecaster. As briefly
described above, the KIB has a new model specification
(see Section 3.2) and accompanying execution engine. The
simulator, forecaster, and KIB are developed in Java. The
optimization IDE tool is also developed in Java with its
solver developed in C/C++. The platform supports multiple
instances of Inventory Forecasters. This is useful because
there are emerging approaches for handling bias in
inventory control.

4

Experimentations

The data used for the ISF and more generally for
manufacturing/logistics processes and optimizations are for
testing the correctness of the OSF design and
implementation. The simulation period has its own nonperiodic and periodic timing. The forecast and optimization
models have their own planning horizon duration and
periods for which safety stock and release commands are
generated.
Examples from many simulation experiments are
shown in Figures 9-11 [9]. The result for single echelon
shows the ideal case – forecast model has perfect knowledge
of future customer demand. This provides perfect safety
stock data for a number of future periods (see Figure 8). The
optimization model is able to make perfect release
commands given information such as beginning on hand at
the finished goods warehouse and having deterministic
shipping time. For the first 3 time periods, the customer
demand is not met due to initialization – it takes 3 time
periods for products from warehouse to reach the geocustomer. The impact of imperfect knowledge of customer
demand for the forecast model and stochastic shipping is
shown in Figure 9. The importance of exponential, kernel
and no smoothing algorithms in calculating safety stocks
can be seen in Figure 10. The figure shows the average
inventory holdings that are needed to satisfy different
customer service levels. The length of a simulation run
determines the duration of historical data (ACD, FCCD),
planning horizon for forecast and optimization models, and
shipping period for a given hub (H1) and product (P4). It is
not necessarily true that increasing safety stock holding
results better customer service level. For example,
calculating safety stock without using any smoothing
algorithm (NS) on the future customer demand yields better
result than adjusting for demand bias using either
exponential or kernel smoothing algorithms.

Experiments with or without bias correction are carried out
for the two echelon supply chain system as shown in
Figure 1. In this example, it can be seen that inventory
holdings for finished goods warehouse and hub are
optimized separately. The optimization model can receive
safety stocks for these two inventories and compute release
commands accordingly. From this configuration, it can be
seen that OSF can support each echelon having its own
forecast model. This means, for example, no smoothing
forecasting can be used for one echelon and exponential
smoothing for another echelon. The example average
inventory holdings for the finished goods warehouse and
hub suggest kernel smoothing may be desired as shown in
Figures 11(a) and 11(b).
(a) Average inventory at H1 (down-stream) echelon

Figure 9. Simulated single-echelon inventory at hub using
perfect forecast customer demand
(b) Average Inventory at CW (up-steam) echelon
Figure 11. Inventories across a two-echelon supply-chain
system

5

Figure 10. Single echelon with log-normal shipping, 10 day
mean, 8 day minimum

Related work

As described above, this research extends the KIB-based
integrated LP optimization and DEVS simulation platform.
It introduces scalability for modeling KIB as well as
supporting inventory modeling that can counter bias in
customer demand forecast.
In another research, an “inner and outer loop
optimization” is developed for long-term planning and
short-term control [10]. It uses Linear and Quadratic
Programming (LP and QP) and Model Predictive Control to
handle stochasticity due to customer demand in addition to
those belonging to manufacturing processes. Optimization
works well for coarse grain planning (e.g., days to weeks to
months), whereas MPC works well for fine grain control
e.g., hourly or daily) of manufacturing processes. To
account for variability in customer demand forecast,
inventory planning is used. The data from inventory

planning is used as external input for LP, QP, and MPC.
This approach is implemented in Simulink/Matlab toolkit.
The MPC controller is formulated for a discretized
manufacturing process model. Unlike this work, OSF
supports explicit modeling interactions between ISF (model
inventory) and optimization models.
In another work, control-theoretic techniques
commonly used for continuous processes are adapted to
discrete manufacturing processes [8]. A control model is
developed for a supply-chain system built from productioninventory discrete models. Feedback-feedforward control
policy (akin to Proportional–Integral–Derivative controller
control) is developed according to the MPC model. This
approach has the advantage of responding effectively to
short-term changes in inventory targets and changes in
customer demand forecast. However, similar to the inner
and outer loop optimization approach, the combined process
and control cannot provide the kind of model composability
flexibility and scalability afforded with the Knowledge
Interchange Broker and OSF. Handling complex
transformations by dividing data mappings among DEVS,
ISF, and LP models provides strong modularity. This
contrasts relying on programming and interoperability
techniques (e.g., HLA standard) that inherently offers little
support for model composability [2,5,6]. These and other
approaches also adversely affect flexible design of
experiments.
The OSF platform is similar to the previously
developed multi-KIB [5] where DEVS, MPC, and LP
models can be composed and executed asynchronously. The
OSF', a uni-KIB, is defined for the interfaces encompassing
DEVS, LP, and ISF models (see Figure 2). The optimization
and forecast models execute at an instance of time. This
contrasts the multi-KIB approach where DEVS and MPC
models execute concurrently. Therefore, the uni-KIB
compensability approach has a simpler operational
semantics as compared with the multi-KIB.

6

Conclusions

To support study of multi-echelon supply chain systems, a
novel Optimization, Simulation, and Forecasting platform
has been developed. This platform proposes a Knowledge
Interchange Broker for composing DEVS, LP, and ISF
models. The KIB has a new, scalable KIB XML schema
model. A sequential scheme controls the executions of the
DEVS-Suite simulator, CPLEX optimizer, and ISM engine.
The platform also introduces forecast modeling which
provides basic support for computing safety stocks over
multiple hubs and products. Basic example models for
single and two echelon supply chain systems have been
developed and analyzed.
Experiments using perfect data were conducted to show
the correctness of the OSF uni-KIB design and
implementation. The platform demonstrates new kinds of

interactions among different complex models and can be
specified in a scalable and systematic manner. Large input
and output data can be transformed in accordance with the
modularity of the composed models and the KIB itself.
Future work includes developing experimentation concepts
and methods that can benefit from interaction modeling.
This is expected to be particularly important toward
exploring system scenarios that are principally driven by the
interactions taking place among simulation, optimization,
and forecast models.

Acknowledgement
We would like to acknowledge the contributions of Asima
Mishra, David Bayba, and Charles Arnold of the Intel
Corporation. A. Mishra formulated the Single-Echelon ISF
model. David Bayba and Charles Arnold supported
experimentation and data analysis. This project has been
funded by Intel Corporation, Chandler, Arizona, USA.

References
[1] DEVS-Suite Simulator, (2009), Simulator, http://devssuitesim.sourceforge.net/.
[2] Godding, G., (2008), “A Multi-Modeling Approach Using
Simulation and Optimization for Supply-Chain Network
Systems”, PhD Dissertation, Computer Science and
Engineering, Arizona State University, Tempe, AZ, June.
[3] Graves, S.C. and S.P. Willems, (2008), Strategic Inventory
Placement in Supply Chains: Non-Stationary Demand,”
Manufacturing & Service Operations Management, Vol. 10,
No. 2, pp. 278-287.
[4] Hopp, W., M. Spearman, (2011), Factory physics:
foundations of manufacturing management, Second Edition,
McGraw-Hill.
[5] Huang, D., H.S. Sarjoughian, W. Wang, G. Godding, D.
Rivera, K. Kempf, H. Mittelmann, (2009), “Simulation of
Semiconductor Manufacturing Supply-Chain Systems with
DEVS, MPC, and KIB”, IEEE Transactions on
Semiconductor Manufacturing, Vol. 22, No. 1, 165-174.
[6] Kempf, K. G, (2004), “Control-Oriented Approaches to
Supply
Chain
Management
in
Semiconductor
Manufacturing”, Proceedings of American Automatic
Control, Boston, MA.
[7] Sarjoughian, H.S., (2006), “Model Composability”, Winter
Simulation Conference, Methodology Track, pp. 149-158,
December, Monterey, CA.
[8] Schwartz, J. and D. Rivera, (2010), “A Process Control
Approach to Tactical Inventory Management in ProductionInventory Systems”, Int. J. Production Economics, Vol. 125,
pp. 111–124.
[9] Smith, J., (2012), “Scalable Knowledge Interchange Broker:
Design and Implementation for Semiconductor Supply Chain
Systems”, MS Thesis, School of Computing, Informatics, and
Decision Systems Engineering, Arizona State University,
Tempe, AZ, December.
[10] Wang, W., D. Rivera, (2008), Model Predictive Control for
Tactical Decision-Making in Semiconductor Manufacturing

NoC Simulation Modeling in DEVS-Suite
Hoda Ahmadinejad1, Fatemeh Refan2, Hessam S. Sarjoughian3
1

Computer Architecture Research Group, ECE Department, University of Tehran, Tehran 14399, Iran
h.ahmadinejad@ece.ut.ac.ir,
2
CAD Research Group, ECE Department, University of Tehran, Tehran 14399, Iran
refan@cad.ece.ut.ac.ir,
3
School of Computing, Informatics, and Decision Systems Engineering, Computer Science and Engineering Faculty,
Arizona State University, Tempe, Arizona, USA
ECE Department, University of Tehran, Tehran 14399, Iran
sarjoughian@asu.edu,
Recently, a few NoC simulators have been developed.
Among formal models, Petri Net is used to model NoC
performance
characteristics
under
alternative
communication schemes [2] [3]. These models are based on
Colored Petri Net and Deterministic Stochastic Petri Net.
Noxim [4] use concepts and constructs provided by
SystemC [5] and programming languages.
This paper presents an NoC model developed based on
NoC first-principles and the general-purpose Discrete Event
System Specification (DEVS) modeling formalism [6]. The
NoC model types are Processing Element (PE), Switch
(SW), and Network Interface (NI). The Network Interface is
a composite model as it has packetizing and de-packetizing
model components. The logical structure and behavior of
these models are derived from the NoC description. The
components of the NoC model are implemented in DEVSSuite [7]. To experiment with the NoC models, an
experimentation model is devised. Its responsibility is to
define the conditions for conducting experiments. The NoC
model can be externally controlled and observed via start
and stop signals provided by the experimentation model.
The rest of paper is organized as follows: In Section 2
the basic NoC concepts and characteristics are presented. In
Section 3, related work is briefly described. In Section 4, the
approach and simulation models for NoC are developed.
The experimentation testbed, results, and a comparison with
Noxim simulator are presented in Section 5. Finally Section
6 concludes the paper and highlights some future work.

Keywords: DEVS, DEVS-Suite, NoC, NoC-DEVS, Noxim,
SystemC.
Abstract
Study of Network-on-Chip (NoC) systems requires
simulators capable of handling their unique characteristics.
Toward this objective, a set of simulation models are
developed based on NoC first principles and the DEVS
modeling formalism. The models constitute the NoC-DEVS
simulator, a domain-specific DEVS-Suite simulator. A
representative prototypical NoC mesh-based system is
modeled using processing elements, network interfaces,
switches, and links. The same NoC example system is also
modeled in Noxim simulator which is developed using
SystemC. The NoC-DEVS simulator is evaluated against
the Noxim simulator in terms of delay and throughput
performance metrics. Some basic capabilities of these two
simulators are briefly compared. A sketch of future research
directions is also provided.

1

Introduction

Developing design specifications for System-on-Chip (SoC)
is known to be challenging. Similar to other complex
networked systems, design solutions must account for
operations and communications among different
components. SoCs and in particular Network-on-Chip
(NoC) systems [1] have some unique characteristics – e.g.,
their physical structures and interactions are constrained as
compared with common computer network systems. Such
distinct features have prompted development of new
concepts, methods, and tools (e.g., simulators) to aid in the
design and experimentation of Network-on-Chips.
Given the importance of early exploration of NoC
design solutions, specialized modeling and simulation
approaches and tools have been proposed. General-purpose
counterparts are difficult to use. Indeed, there are numerous
cases (e.g., computer network simulators) where specialized
modeling concepts, theories, and methods have been shown
to be very useful in the modeling and simulation life-cycle.
It is also worth noting that real-world experimentation for
complex systems, especially in the early stages of system
development, remain limited and often impractical [2].

2

Background

Processing Elements, Network Interfaces, Switches, and
Communication channels can be synthesized to create
arbitrary NoCs. Processing elements (or cores) are the
computational building blocks. They are the sources and
destinations of data. In mesh based NoCs, processing
elements are uniformly connected to one another through a
collection of network interfaces, switches, and
communication links.
A network interface, as the name suggests, is the
gateway between a PE and a Switch. I.e. its main
responsibility is to translate the PE’s data into network data
and vice versa using packetization, and depacketization

134

schemes [8]. A switch is responsible for routing data among
PEs usually using other intermediary switches. It is
comprised of input ports and buffers, router state, routing
logic, allocators, and a crossbar. The communication
channels can be categorized in two types. Conventional type
supports primitive communication – e.g., packets are sent
and received between a PE and a switch. Buffering type
uses ports for some NoC elements – e.g., switch-to-switch
communication uses queuing. Other elements and aspects of
NoCs including topology, routing, and flow control are also
needed to be modeled. The physical layout and connections
between nodes (PEs, Switches, and NIs) and channels
(communication links) in the network are determined by
network topology. NoC can have one of several kinds of
topologies (direct, indirect, regular, and irregular [8]). Given
the NoC’s topology, the routing algorithm determines the
paths through the network that a message can traverse to
reach its destination. It affects the latency, power
consumption, and traffic distribution. A routing algorithm,
in addition to being deadlock free, should support features
like contention minimization and hot spot avoidance, and at
the same time does not require much hardware overhead in
switches or additional fields in packets. Among routing
algorithm categories, minimal vs. non-minimal and
oblivious vs. adaptive routing can be named [8].
Flow control determines how resources are allocated to
messages as they pass through the system. It allocates (and
de-allocates) buffers and channel bandwidth to packets.
Furthermore, it needs to determine the granularity of data to
be traversed. Given time and area constraints, flow control
is handled at the flit level [8]. Flit is a Flow Control unit, a
part of a packet. Flits are usually defined in constant size for
NoC and are commonly categorized into head, normal, and
tail types. Head flit includes addressing information needed
for routing a packet and typically constructed from the
header part of the packet. Tail flits have an indicator in
which show they are the last flit of packets. Normal flits are
data flits which are sandwiched between head and tail flits.
Metrics such as network latency and accepted traffic are
defined for evaluating NoC designs. The lower bound on
average message latency is measured as zero-load latency
which is the latency experienced by a packet when there are
no other packets in the network. Another important metric is
throughput which is calculated from latency, but provides
more detailed information.

Petri Nets are used for NoC performance evaluations such
as CPU memory throughput and average clock cycles
required for establishing connections between a data source
node and data sink source [2]. A basic model consisting of
two processors, local RAM and dedicated serial ports
compete for a share memory using a common
communication structure, has been developed. This
Deterministic and Stochastic Petri Net (DSPN) model has
19 places and 20 transitions. It has been validated using an
emulated FPGA-based testbed. To more accurately model
this system, the basic model is extended with an additional
26 places and 28 transitions. Another model that has a
regular mesh structure is also developed. The model which
has 500 places and 600 transitions represents a system
having a regular mesh-structure with eight clients and four
nodes. The results reported are incomplete and hierarchical
modeling is proposed as future work.
A more recent study uses Colored Petri Nets for
modeling mesh-based and k-ary NoC [3]. The models
capture detailed structure and dynamics of NoCs. The
models similar to the DSPN models require very large
number of places, arcs, and transitions and exhibit high
visual complexity. In this study a 44 mesh is studied under
different scenarios. Using a variety of supporting tools and
considering diverse operational settings, it is observed that
the model is deadlock free. Performance metrics (percent
switch loading) are discussed. Study of alternative network
communication and role of switch fabric types are proposed.
Among NoC simulators, the Noxim simulator [4] has
attracted interest. It is studied and examined in detail as part
of this research. This mesh-based simulator is developed
using the SystemC discrete event simulator [5]. The
processing element and switch are SystemC modules with
send and receive methods. A reset signal and positive edge
of clock are used by each method. The switch has a buffer
monitor method to manage the information exchange with
neighboring switches. All functionalities in Noxim occur at
the positive edge of a clock signal and a reset signal for the
start of simulation. Every NoC model can be configured in
terms of network size, switch channel depth, routing
algorithm, traffic spatial distribution, and packet injection
rate using a command line interface. The total number of
received packets/flits, global average delay, throughput,
min/max global delay, total energy, and per-communication
delay/throughput/energy can be measured.

3

4

Related work

A number of NoC simulators have been proposed in
literature. An earlier work uses SystemC and the ns-2
network simulator to efficiently simulate embedded systems
[9]. A design is developed to accurately represent hardware
and network interactions. Use of ns-2 makes the use of this
kind of simulation modeling difficult for NoC. Most
recently, Generalized Stochastic Petri Nets and Colored

Model development

In order to model NoC, a bottom-up approach is chosen; i.e.
we use the real physical NoC architecture and the coupled
DEVS IO system level of abstractions [6]. Structural and
behavioral
abstractions
representing
the
core
functionalities of the NoC are developed. The operations for
every model component can have their own independent

135

timing as conceptualized with clocks. In the following, each
of these models is detailed with emphasis on their dynamics
with timing specification.

4.1

model phase to “packetize”. Otherwise the queue size
changes with no change in phase.
In the “packetize” phase, a packet is converted to flits.
In this state external events occurrence only changes the
“outQStatus” and the queue size state variables and not the
phase. After packetingTime, an internal transition will
occur. Depending on “outQStatus” value, two cases may
occur. First, by receiving a “nok” value the phase of the
model changes to “wait4OK” and it remains there for
infinity unless it receives an “ok” on the “statusFromSW”
port. Second, by receiving an “ok” the model's phase
changes into “sendOutFlit”.

Processing element

The proposed model for PE does not include computation
capability, i.e. it is only responsible for generating and
consuming data. It is a simple atomic model which injects
data to the network and receives data from its counterparts.
It does not consider application-specific operations. The
work presented in [10] also uses the PE as traffic generator.
In fact it collects execution traces of all PEs and then uses
the results to model their behavior with a reduced set of
instructions.
The PE model has two basic phases (passive and busy).
In the busy state the model generates and sends out data
every sendOutInterval. Three patterns are used to generate
sendOutInterval value. The time distribution of the
generated data is Poisson, Gaussian, or Random. The size of
the generated data can be set.

4.2

Network interface

A Network Interface (NI) does both packetizing and depacketizing. These two atomic models are defined as shown
in Figure 1. NI receives data from PE and network via
“fromPE” and “fromSW”, respectively. The dynamics of the
packetize model is controlled through “statusFromSW”
port; the de-packetize model is uncontrolled (i.e., data can
be sent to PE without any limitation). The packetize model
can send data only by receiving notification of free slots in
NoC switch incoming buffer.
start

Figure 2: Packetize atomic model

During the packetingTime the model splits a packet into
flits, which means filling an array of flits by the packet
information. The source id of the flits is set according to the
corresponding PE id which is defined in the model. The
destination id of the packet is set randomly, which means
the traffic spatial distribution is random.
In the “sendOutFlit” phase, by receiving data on
“statusFromSW” port, depending on its content the model's
phase may remain unchanged or changed to “wait4OK”.
After senidngTime units of time four internal transitions are
possible. In the first case, the number of sent flits is smaller
than the maximum number of flits within a packet and the
“outQStatus” is “ok”. So the model remains in the
“sendOutFlit” phase. The second case is the same as the
previous scenario except for the “outQStatus” which is
“nok” here and the model enters the “wait4OK” phase. In
the third situation the number of the sent flits is greater than
the maximum number of flits in a packet and the size of the
queue holding the incoming data is greater than the size of a
packet so the phase changes to “packetize”. The fourth
scenario is the same as the previous one; however the size of

PE_NI
start
stop

stop

PE

recData

out

Packetize
toSW

NI

toPE

De‐Packetize
fromSW

fromSW

fromPE

toSW

statusFromSW

statusFromSW

Figure 1: PE_NI and NI coupled model structures

4.2.1
Packetize
The basic functionality of this atomic model is to receive
data from PE and convert the data to flits. The model states
and transitions are shown in Figure 2. The model starts in
“passive” phase. In this phase, the data received on port
“fromPE”, is split into 8-bits (1 byte) data and then inserted
in a queue. Now if the number of elements in the queue is
greater than or equal to the packet size in bytes (e.g. when
the packet size is 64 bit the size of the queue must be greater
than or equals to 8), the external transition changes the

136

the queue is smaller than the size of a packet in bytes. So the
phase of the model will change to “passive”.
If the model receives data on the “statusFromSW” port
while it is in “wait4OK” and the content is “ok”, it will go
to the “sendOutFlit” phase and starts sending out flits.

The phases and transitions for this atomic model are
shown in Figure 3. The model starts in the “check” phase in
which each input queue is checked for incoming flits
periodically (through internal transition with period of
checkTime to “check” phase). The “curIndex” holds the
index of currently examined input queue and “inputQi” for i
in [0, “portNum”-1] represents the input queue associated to
each input port (“INi”). If “inputQcurIndex” is not empty, the
phase of the switch changes to “route” phase using its
internal transition at the end of checkTime time.
In the “route” phase, the first flit of the “inputQcurIndex”
is read and “dstID” field is extracted. Then the index of
destination port is obtained from the route table, and is
stored in “dstPort”. Finally, the “extStatusdstPort” is checked
and the phase changes to “sendOut” if it is true; otherwise
the next phase is set to “check”. Both of the state changes
occur through internal transitions and after routeTime time.
The model remains in “sendOut” phase, using internal
transitions with the delay of outTime until the tail flit is read
from the “inputQcurIndex” and is sent to the “dstPort”, or the
“extStatusdstPort” becomes false (i.e., the receiving switch
buffer becomes full). In the former and latter cases, the
model changes phase using internal transitions to “check”
(in addition to increment of “curIndex” variable) and
“wait4Ok” with the delay of checkTime and infinity,
respectively. The Boolean “sendTail” variable is set when
the tail flit is reached; it determines that the next phase is
“check”. The “wait4OK” phase has no internal transition.
This phase may change due to external transition function
(i.e., an “ok” is received on the “extSTdstPort” input port).

4.2.2
De-Packetize
This atomic model receives data from switch, collects them
in packets and partitions them to data size units before
sending them to the designated PE. We assumed that the PE
only receives data without processing them. Accordingly the
PE doesn't need input queues and there is no need for
interrupt. The model description is excluded due to lack of
space; however the model states and the transition, output,
and time advance functions are similar to the Packetize
atomic model.

4.3

Switch

All communications in NoC are handled with this model. It
receives incoming flits which are then routed according to a
predefined network topology and a routing strategy. In order
to avoid losing incoming flits, each input port has a
dedicated queue. The flits stored in these queues are
checked based on a round robin scheduling policy; i.e.
starting from an initial input port, each input port is checked
(this is conceptualized in Figure 3). When un-routed flits
exist, a complete set of flits starting from head to tail flit is
chosen. As the tail flit is routed (sent), the queue of next
input port is checked for un-routed flits, and continues in the
same way. After reading the first flit (the head flit), the ID
of destination PE is obtained from flit. The ID is used as the
input for the routing algorithm which determines to which
destination port this set of flits is to be sent. Finally, the
head flit, and its consequent normal flits are sent to the
determined destination port. This process terminates, when
the tail flit is sent.
Since the available buffer for each input queue is
limited, before sending a flit to the next switch, the sender
switch should ensure that the next switch has enough
capacity at least for one flit. Therefore, each switch informs
its neighboring switches, whenever one of its input queues
are full. On the other hand, as one flit of the full queue is
routed, it again informs its neighbors about the freed buffer.
This scheme guarantees that no flits will be lost. However,
deadlock between switches can occur when the buffer size is
small or the network is highly congested. Distributed
simulation techniques [11] may be used to avoid deadlock.
The NoC switch atomic model has two sets of input and
output ports (“INi” and “OUTi” given the n neighboring
switches defined as 0  i < n). The “extSTi” and “intSTi”
input and output ports are responsible for getting available
buffer information of neighboring switches and announcing
the available buffer information of the input queues, using
“ok”/“nok” messages.

Figure 3: NoC switch atomic model

Finally, the “setStatus” phase is responsible for setting
the values of “intST” output ports if there is any change in
the status of “inputQs”. This phase disturbs all other phases

137

when a status set is needed by keeping the state info and
making a zero time internal transition to “setStatus” phase.
After setting the new status, the model transits back to its
previous phase and sigma. This phase change is necessary
for producing status outputs on “intST” output ports in two
cases: (i) when the “inputQ” becomes full after receiving a
flit, and (ii) when a flit is en-queued from “inputQ” which
had been full. In the first case, where the phase change
occurs due to an external transition, may happen in each
phase except for the “setStatus”. The latter case occurs in
the “sendOut” phase when a flit from a full “inputQ” is dequeued and sent to “dstPort”.
Only two phases of “setStatus” and “sendOut” can
produce output. The “setStatus” produces “ok” or “nok”
message for the “intSTcurAddedQ” output port while in the
“sendOut” phase a flit is removed from the top of
inputQcurrentIndex and send on the “OUTdstPort” output port.

4.4

Figure 4: A segment of an NoC model with Data Collector

5.1

Unidirectional Link

This model is responsible for transferring flits between
switches. It represents the communication channel with
delay as in a real digital system. So it requires a period of
time (as defined by user) for flits to be transmitted between
switches. Two switches can communicate with another via a
pair of unidirectional links (a bidirectional coupled link
model).

4.5

Simulation setup: simulation timing and data size
information are determined as follows

Data collector

This model collects and analyzes the data generated by PEs
and consumed by other components. It has input ports
“toNIi” and “fromNIi” which gather data entering and
leaving every NI. The data is used to track network load.
The model also has two output ports: “stop” and “start. The
number of the generated packets is tracked and average
global, minimum, maximum, and maximum global delays
as well as global throughput are computed; where global
average delay is computed as the average transmission delay
of each packet. The inputs of the model are inserted in an
array of PacketInfo which is a complex data type. This type
has the following fields: sequence number, source id,
destination id, start time, and stop time. Start time holds the
time a packet leaves its source PE and stop time holds the
time a packet enters its destination PE. It also checks the
correct delivery of flits implicitly. The remaining details are
left out due to lack of space.

5

Simulation model setup

The execution process of an NoC model can be divided into
initialization, simulation, and result report parts. Since the
simulation and report result are already described in Section
4, below a detailed description of the initialization part is
provided. In the initialization part, the configuration
parameters are read from an input file and the NoC model is
constructed. The input file includes simulation setup, NoC
size, routing table, and interconnection pattern; each starting
by a special character. Once all the parameters from the
input file are read, the corresponding NoC structure is
created. Thereafter, the simulation starts and data are
collected until all the packets sent to the NoC are delivered
to their destination (i.e., PEs). The order and syntax for
determining each part is as follows:

 simulation time (simTime): The time between the start and
stop commands,
 data size information, including size of packet (packSize),
size of flit (flitSize), and the capacity of input queue buffers
(inputQ) of switches,
 NI timing information, including packetizing time
(pPackTime), depacketizing time (dPackTime), flit send time
from de-packetizing model (dSendTime), flit send time in the
packetizing model (pSendTime), and
 Switch timing parameters including the delay between
checking two consequent input queues (sCheckTime), the
switch routing table access time (sRouteTime), and send time
for each flit (sSendTime).

NoC size: the input file determines the number of PEs
(PENum) and switches (SWNum) in an NoC model.
Routing table: the routing table uses SWNum×PENum
triples, each determining one entry of the routing table for
each switch. The triples are of the form (Switch ID (curID),
PE destination ID (dstID), Destination port (dstPort)). If a
packet arrives in curID intending to go to dstID, it will go to
the dstPort. This enables definition of arbitrary deterministic
routing tables; for every algorithmic routing strategy; it can
be changed to a method.

Experimentation

A 33 NoC model is developed and simulated in the DEVSSuite simulator [7]. Figure 4 shows a segment of the NoC
spanning from Data Collector to Switch 2 in addition to the
coupling details between Switch 2 and Switch 3.
Experimental results are collected, evaluated, and compared
to the results obtained for the NoC model developed in
Noxim simulator.

138

Interconnection pattern: there are SWNum entries each of
which has a switch id (swID), swID input/output ports
(pNum), and to the names of the element (unconnected, PE
through NI, or another switch) to be connected to.

5.2

Due to lack of space, we only mention a couple of key
differences between these two simulators. NoC-DEVS
models have a formal basis; Noxim simulator is more
efficient. NoC-DEVS capabilities such as animation and
multiple modes of control are useful given the importance of
detailed study of model behaviors and in particular timing.

Results and discussion

The simulation experiment results are used as a means of
validating the NoC-DEVS model abstractions and their
dynamics. The intent has not been to show Noxim and NoCDEVS produce results that are very close to each other.
Instead, the goal is to show the advantage of using the
DEVS formal modeling approach. Table 1 shows the
parameters used for running simulations. To the extent
feasible, the same values are configured in Noxim. Here, the
comparative results of global average delay are reported (the
trend is shown in Figure 5). Other data is excluded due to
lack of space. By increasing the ratio of packet to flit size,
congestion in NoC increases and therefore global average
delay can also increase. Both simulations show the same
type of behavior. The difference between the results can be
attributed to several factors. Two basic factors are timing
and different levels of model abstraction. NoC-DEVS
components more closely represent timing in NoC. Every
transition function is explicitly defined in terms of time.
Noxim uses execution cycle. The implementation of NoC
switch in Noxim uses reservation table for output ports; i.e.,
if an output port is not used for sending a packet and there is
a packet to be sent through this port, the process of routing
flits will be started even though another output port is busy
sending another set of flits. This decreases the global
average delay in Noxim. Random distributions and the rate
of feeding packets to the system are not identical.

6

Conclusion

In this paper a discrete event model for Network-on-Chip is
proposed and developed. The NoC-DEVS model is
implemented in the DEVS-Suite simulator and was inspired
from simulation modeling of MIPS processors [12]. The
NoC-DEVS model is configurable and directly accounts for
timing for the NoC Processing Element, Network Interface,
Switch, and Link components. Some of the common metrics
for evaluating digital systems are included in the model; the
other metrics can also be incorporated in a straightforward
manner. Important configuration parameters (e.g., adaptive
and localized routing algorithms) and performance metrics
such as power and energy consumptions are proposed as
future work. With these capabilities, the simulator can
support formal, yet detailed and flexible NoC simulation.

References
[1] L. Benini and G. De Micheli, “Networks on Chips: A New
SoC Paradigm,” Computer, vol. 35, no. 1, pp.70-78, 2002.
[2] H. Blume, T. von Sydow, D. Becker, and T. G. Noll,
“Application of deterministic and stochastic Petri-Nets for
performance modeling of NoC architectures,” Journal of
System Architecture, vol 53, no. 8, pp. 466–476, 2007.
[3] H. Bazzaz, M. Sirjani, R. Khosravi, S. Taheri, “Modeling
networking issues of network-on-chip: a coloured petri nets
approach”, Proceedings of the 2nd International Conference
on Simulation Tools and Techniques, March 2009.
[4] Noxim – NoC Simulator, 2007, http://noxim.sourceforge.net/
[5] Open SystemC Initiative, SystemC Version 2.0, User’s Guide,
www.systemc.org, 2001.
[6] B. Zeigler, H. Praehofer, and T.G. Kim, Theory of Modeling
and Simulation, 2nd ed. New York: Academic Press, 2000.
[7] DEVS-Suite, 2009, “DEVS-Suite Simulator”, Arizona Center
for Integrative M&S, http://devs-suitesim.sourceforge.net/
[8] N. Enright-Jerger and L-S Peh, On-Chip Networks, Synthesis
Lecture in Computer Architecture, Editor: Mark Hill, Morgan
Claypool, 2009.
[9] F. Fummi, P. Gallo, S. Martini, G. Perbellini, M. Poncino,
and F. Ricciato, “A timing-accurate modeling and simulation
environment for networked embedded systems,” Proceedings
of the 42nd Design Automation Conference, 2003, pp. 42–47.
[10] S. Mahadevan et al. “A network traffic generator model for
fast network on-chip simulation,” Proc. of the Design,
Automation and Test in Europe Conference and Exhibition
(DATE’05), vol. II, March 2005, pp. 780–785.
[11] R. Fujimoto, Parallel and Distributed Simulation Systems,
New York: Wiley Interscience, 2000.
[12] Chen, Y., H. Sarjoughian, (2010), “A Component-based
Simulator for MIPS32 Processors”, Simulation Transactions,
86(5-6), 271–290.

Table 1: Configuration parameters of the model
sendOutInterval
random route Time
10
packetingTime
10
check time
3
sendingTime (packetize)
5
link delay
2
depacketingTime
12
buffer size
1000
sendingTime (depacketize)
5
Topology Mesh 3×3
sendingTime (switch)
5
Routing
X-Y
execution time
10000

Figure 5: Global average delay

139

Proceedings of the 2006 Winter Simulation Conference
L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto, eds.

APPLICATION OF THE DEVS FRAMEWORK IN CONSTRUCTION SIMULATION
Sivakumar Palaniappan
Anil Sawhney

Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
Dept. of Computer Science and Engineering
Ira A. Fulton School of Engineering
Arizona State University
Tempe, AZ 85281-8809, U.S.A.

Del E. Webb School of Construction
Ira A. Fulton School of Engineering
Arizona State University
Tempe, AZ 85287-0204, U.S.A.

consists of five basic modeling elements such as ‘normal’,
‘combi’, ‘queue’, ‘function’ and ‘counter’. The functionality
of each modeling element is documented in the literature
(Halpin 1977, Abourizk et al. 1992). This was followed by
the development of a software tool using micro computer
called MicroCYCLONE (Lluch and Halpin 1982).
The application of CYCLONE to model and analyze a
number of construction scenarios is described in detail by
Halpin and Riggs (1992). CYCLONE modeling framework
provided the foundation for many construction researchers
to develop a number of construction simulation tools in the
past 20 years. This includes the development of (i)
INSIGHT system based on CYCLONE with interactive
user interface (Paulson 1987); (ii) CYCLONE with advanced resource handling capabilities called RESQUE
(Chang and Carr 1986); and (iii) UM-CYCLONE with
menu driven user interface (Ioannou 1990).
A discrete event simulation system with object oriented design concepts called ‘COOPS’ (Construction Object-oriented Process Simulation System) was developed
(Liu and Ioannou 1992). Further, CIPROS, an objectoriented system for simulating construction plans was developed (Odeh et al. 1992). A simulation programming
language, called STROBOSCOPE, (State and Resource
based Simulation of Construction Operations), was designed and developed to specifically model and simulate
construction operations (Martinez and Ioannou 1994).
STROBOSCOPE is based on three phase activity scanning
approach. A detailed discussion of the three simulation
strategies namely process interaction, activity scanning and
event scheduling is provided by Martinez and Ioannou
(1999).
The application of hierarchical simulation modeling
(HSM) method in construction simulation was demonstrated through a bridge project (Sawhney and Abourizk
1995). Though a number of construction simulation tools
were developed following the CYCLONE methodology,
their use was mostly limited to academic and research
community (Abourizk and Hajjar 1998). This is primarily

ABSTRACT
The Discrete Event Systems Specification (DEVS) framework is based on systems engineering principles and is
used for modeling and simulation in many application domains. This framework supports a number of important
features such as component based hierarchical simulation
model development, scalability, reusability and distributed
simulation. This paper demonstrates the application of the
DEVS framework for construction simulation through a
simple, but representative real world application. The application focuses on analyzing the work flow between
various trade contractors in production home building. The
software tool ‘DEVSJAVA’ is used for modeling and
simulation of the construction application. DEVSJAVA is
object oriented; it implements the DEVS framework using
Java programming language. This paper consists of three
main components: an overview of the state-of-the-art in
construction simulation methods and tools, an introduction
to the DEVS framework, and a detailed construction example to highlight the application of DEVS framework.
1

CONSTRUCTION SIMULATION

Construction operations involve a number of repetitive tasks
similar to manufacturing operations, for example, earth hauling, tunneling, road construction, and glass installation on
tall buildings (Abourizk et al. 1992). This demonstrates the
potential to apply discrete event simulation techniques (especially process and resource based simulation) for modeling and analysis of construction operations. The history of
construction simulation dates back to 1960’s with the development of simple networking concepts (Abourizk et al.
1992). These networking concepts were used as a modeling
framework to study construction operations. As a first attempt, “link-node” model was developed and used to assist
selection of construction equipment (Teicholz 1963). After
this, the CYCLONE (Cyclic Operations Network) modeling
framework was developed by Halpin (1977). CYCLONE

1-4244-0501-7/06/$20.00 ©2006 IEEE

2077

Palaniappan, Sawhney, and Sarjoughian
attributed to the significant differences between the simulation model representation and the real world construction
system. This made the process of developing and understanding the simulation model more tedious for construction experts who have limited amount of time to apply
simulation in construction. To overcome this difficulty, the
concept of special purpose simulation (SPS) was introduced as an application framework for developing construction simulation tools (Hajjar and Abourizk 2000). SPS
is based on object-oriented application framework and the
constructs of SPS tools are similar to the objects of real
world system. Three independent customized simulation
tools were developed based on the SPS concept (Hajjar and
Abourizk 2002). They are (i) ‘AP2-Earth’ for the analysis
of large earth moving projects (ii) ‘CRUISER’ for modeling aggregate production plants and (iii) ‘CSD’ for construction site dewatering. The use of the above tools by
large construction companies demonstrated the success of
SPS based simulation tools.
Based on the experience gained through the development of SPS based simulation tools, a unified modeling
methodology for construction simulation was proposed
(Hajjar and Abourizk 2002). This approach resulted in the
development of a simulation tool development and utilization environment called ‘Simphony’. Simphony is an integrated environment for construction simulation. The development time for new SPS tools was reduced
significantly due to the construction simulation object library provided within the Simphony framework. To understand the important features of Simphony, the reader can
refer to Abourizk and Mohamed (2000).
In an effort to extend the field of construction simulation, the authors undertook this experimentation with
DEVSJAVA. The objective of this work is to demonstrate
the application of DEVSJAVA in construction simulation
through an example problem in production home building.
The remaining content in the paper is organized as follows:
The next section presents an introduction to the DEVS
framework. The third section presents background information about production home building and the details of
the example problem. The simulation model development
using DEVSJAVA is described in the fourth section. A
discussion on the simulation results is presented in the fifth
section. Finally the sixth section presents conclusions
based on the simulation results and the contribution of this
work.
2

framework was implemented as a simulation environment
called ‘DEVSJAVA’ using the Java programming language (ACIMS 2005).
The DEVS framework and DEVSJAVA simulation
environment have a number of important features for modeling and simulation: They are summarized as follows:
•

•

•
•
•

•

DEVS framework is founded on system-theoretic
principles including component based hierarchical
modeling using input/output ports and couplings.
The framework defines two types of models –
atomic model and coupled model; atomic models
are the basic modeling constructs whereas coupled model represents a group of atomic and/or
coupled models. It supports feedback loops; the
process logic of a component can be customized
as function of the property of an incoming entity.
This helps to develop simple and effective simulation model without any repetitions as in case of
‘feed forward’ model.
The framework supports scalability and reusability through the use of one coupled model as a basic component in another coupled model. It supports hierarchical simulation model development.
This enables to build and test large complex simulation models in an incremental fashion.
DEVS framework supports distributed simulation
of models thereby allowing development and deployment of very large-scale complex models.
Since the DEVS framework is generic, its application is not limited to process and resource based
simulation.
DEVSJAVA offers comprehensive flexibility to
customize the simulation model development
based on the specific requirements of each problem.
The modeler can visually monitor the simulation
of entities across various processes through the
‘STEP’ option. This helps to iterate through the
simulation event by event and to check the correctness of the model through traces.

The above features of DEVS framework can be effectively applied to construction simulation.
2.1

DEVS FRAMEWORK

Modeling and Simulation Concepts and Artifacts

A general framework of modeling and simulation is defined
to consist of three basic elements: real system, model and
simulator (Zeigler et al. 2000). The real system is considered
as the fundamental source of data or input/output pairs. It
can be either in existence or proposed. The model is a set of
instructions which define how inputs and/or states may be
processed and outputs generated. Every model has a corresponding simulator which is responsible for executing the

The Discrete Event Systems Specification (DEVS)
framework was introduced in 1972 by Zeigler (Zeigler et
al. 2000). This framework was later extended to support
parallel model specification and execution (Chow 1996),
hierarchical model development and modularity, and object orientation (Zeigler and Sarjoughian 2003). DEVS

2078

Palaniappan, Sawhney, and Sarjoughian
model instructions according to a well-defined abstract protocol. Figure 1 shows these elements and their key relationships.

state. Sigma can also be stated as the time to next state
transition which is determined by the time advance function (iv) internal transition function that specifies the state
to which the system will change after the time specified by
the time advance function has elapsed (v) external transition function that specifies the system state changes when
an external input is received (vi) confluent transition function that is applied when the arrival of an external input
and an internal transition happens at the same time and
(vii) output function that generates external output.
A coupled model is a model that consists of several
atomic and/or coupled models connected together internally through coupling. Experimental frame is one example
for coupled model since it consists of two components
namely generator and transducer. Figure 2 shows an example of a coupled model.

Figure 1: Basic Elements and Relations for Modeling
and Simulation
The modeling relation links the real system and the
model. It defines how well the model represents the real
system that is being modeled (scope of the model). The
simulation relation links the model and the simulator. It
represents how faithfully the simulator can execute the instructions of the model (validity or correctness of the
model). Experimental frame specifies the conditions under
which the system is experimented. It systematically handles the existence of many models for a given system and
thus formalizes the relationship between the system and its
models. It supports specifying the scope of the model by
defining the objectives of a simulation project in terms of
observable inputs and outputs. Experimental frame consists
of the generator and the transducer. The function of a generator is to create entities at specific time intervals and release them into the system. The transducer collects simulation output data specified by the modeler.

Figure 2: Example for Coupled Model
A coupled model consists of all information that an
atomic model consists (closure under coupling). In addition, they contain the coupling relationship namely external
input coupling, internal coupling and external output coupling. External input coupling connects the input ports of
the coupled model to one or more input ports of the coupled model’s components. Internal coupling connects the
output port of one component to the input port of another
component within the coupled model. External output coupling connects output ports of one or more components to
output port of the coupled model.
The coupling details for the example shown in Figure
2 are given below:

2.2 Types of Models in DEVS

External Input Coupling (EIC):
EIC = { (N, “in”), (proc1, “in”) }
Internal Coupling (IC):
IC = { (proc1, “out”), (proc2, “in”) }
External Output Coupling (EOC):
EOC = { (proc2, “out”), (N, “out”) }

There are two types of modeling elements in DEVS
framework namely atomic models and coupled models
[Zeigler et al 2000]. Atomic models are the basic building
blocks of the DEVS framework from which larger models
are built. For example, each of generator and transducer
can be an atomic model.
An atomic model contains the following information
[Zeigler and Sarjoughian 2003]: (i) set of input ports to receive external inputs (ii) set of output ports to send output
(iii) set of states and parameters; examples of state include
‘active’ or ‘passive’; parameter refers to ‘sigma’; values
for ‘sigma’ are positive values including zero and infinity.
Sigma specifies the time that a model will stay in a given

Further details about atomic and coupled models, their
formal specification including simulation protocols and
several examples can be found in (Zeigler et al. 2000) and
(Zeigler and Sarjoughian 2003).

2079

Palaniappan, Sawhney, and Sarjoughian
3

75%, and 33% respectively (City of Peoria 2004). From
this, it can be noted that the first time inspection pass rate
for critical inspections varies from 15% to 75%. The poor
inspection pass rate of one trade can significantly influence
the work flow of a succeeding trade, which is true in case
of framing.
The negative influence of workflow variability on the
succeeding trade is demonstrated through a parade game in
construction (Tommelein et al. 1999). This game consists
of a sequence of five commercial construction trades. The
variability in the number of jobs released from one trade to
another trade was controlled by number written on the rolling dice. The impact of workflow variability and methods
to improve the workflow is presented in construction literature (Ballard 1999; Ballard et al. 1998).
Few studies have been reported on the application of
discrete event simulation to analyze production home
building scenarios (Sawhney et al. 2001; Bashford et al.
2003; Sawhney et al. 2005). However, the scope of these
studies does not include modeling the workflow in production home building. The current study focuses on the following: (i) analyze the work flow variability in residential
construction and (ii) demonstrate the application of
DEVSJAVA for construction simulation.

CONSTRUCTION EXAMPLE

This section presents the details of a production home building example problem that is used to demonstrate the features
of DEVSJAVA.
3.1 Problem Background
Production home building consists of mass construction of
100 or so similar homes on a tract called subdivision. These
homes (sometimes called lots) are similar in nature (in terms
of their basic configuration) but have minor variations
(based on home buyer option selections) to satisfy individual
customer preferences (Bashford et al. 2003). The construction of each lot consists of 10 to 12 major phases and a total
of 90 to 100 activities or work packages. These activities are
completed by the coordination of 25 to 35 specialized trade
contractors.
Successful transfer of a lot from one trade contractor
to another trade contractor is critical for the timely completion of a residential construction project. Many factors
cause delay during this transfer and they include: (i) preceding trade does not complete the work on time (ii) preceding trade completed the work but defects in construction are found by a building inspector or a quality control
personnel (iii) the work completed by preceding trade was
damaged and (iv) communication failure occurred between
the two trade contractors. Among these, failure in code
compliance inspections or quality control inspections are
the prime factors that causes delay during a control transfer
most of the times. This was obvious from the data collected from City of Peoria for the period October 2003 to
September 2004. The inspection pass rate of critical inspections was computed and it was found that the first time
inspection pass rate of critical inspections – UG/Soils, pre
slab, framing, drywall and final were 59%, 72%, 15%,

3.2 Problem Details
The objective of the test problem is to model the workflow
in production home building. The preceding trade is considered as rough framing trade and the succeeding trade is
drywall trade. The inspection pass rate of the framing
phase is varied from 15% to 100% and its impact on the
work flow to the drywall trade is measured in terms of
number of jobs released per week from the framing trade.
The process of rough framing trade is illustrated below in
Figure 3.

Figure 3: Process Map for the Rough Frame Trade

2080

Palaniappan, Sawhney, and Sarjoughian
The process details of rough framing trade is described
through the following steps: (1) home builder notifies the
framing trade (2) check for framing crew availability, start
the construction work if crew available else wait for crew
(3) trade notifies the home builder after the completion of
work and then builder notifies city inspector (4) city inspector completes the inspection (5) check inspection result, if pass then the lot is released to the succeeding trade,
if fail then the following group of steps happen till the lot
passes the inspection (6) home builder notifies framing
trade about fail in inspection (7) check for framing crew, if
crew available, crew starts working on rework else wait for
crew (8) framing trade informs builder about completion of
rework (9) builder notifies city inspector for inspection, the
process flow repeats from step-5 again.
The following input data regarding the framing trade is
needed to conduct the simulation: (1) start pattern (2) inspection pass rate (3) crew size (4) duration for construction and rework. Start pattern refers to the number of jobs
per week that arrives to the framing trade from his predecessor. Figure 4 shows the start pattern of the framing trade
that was used in this study. This start pattern was obtained
based on a complete lot simulation.

the 6th inspection, there can be a maximum of 5 reworks.
The duration of rework-1, rework-2, rework-3, rework-4
and rework-5 were assumed as 2 days, 2 days, 1 day, 1 day
and 1 day respectively. Actually, it was found that the reworks follow exponential distribution based on inspection
data analysis. Constant durations for rework are used in
this study; the objective behind this is to minimize the
variability due to statistical sampling and to observe the
effect on the system performance due to the variation in the
input data to the simulation model.
4

The simulation model was implemented using DEVSJAVA.
This model consists of 5 coupled models and they are: (i)
‘WORK_FLOW_SIMULATION’ representing the complete simulation model; it consists of the remaining four coupled models as basic components (ii) ‘EXPERIMENTAL_
FRAME’ (iii) ‘SIM_PROCESSOR’ representing the construction, inspection and rework (iv) ‘FRAMING_
TRADE_CONTRACTOR’, a coupled model representing
the framing construction and reworks process and (v)
‘CITY_BUILDING_INSPECTOR’, a coupled model representing the inspection process by city building inspector.
The last two coupled models are two basic components
within the coupled model ‘SIM_PROCESSOR’. The complete simulation model is shown in Figure 5.

Start Pattern for the Rough Frame Trade
10
Number of Lots

SIMULATION MODEL DEVELOPMENT

8
6

4.1 Coupled Model ‘EXPERIMENTAL_FRAME’

4
2

Experimental frame is a coupled model that specifies the
conditions under which the simulation model is experimented or simulated. It consists of two types of atomic
models namely ‘generator’ and ‘transducer’. The component
‘generator’ is represented by the atomic model
‘GENERATE_STARTS’. It generates a specific number of
lots each week and releases them to the framing trade. The
‘out’ port of the generator is connected to the ‘out’ port of
the ‘EXPERIMENTAL_FRAME’ which in turn connects to
‘in’ port of the ‘SIM_PROCESSOR’.
The transducer collects data on simulation output variables. The experimental frame consists of three transducers. They are ‘TRANSD_CYCLE_TIME’, ‘TRANSD_
WAIT_TIME’ and ‘TRANSD_RESOURCE_UT’. These
three transducers compute the cycle time, average waiting
time of lots for resources and resource utilization. The output ports of ‘SIM_PROCESSOR’ are connected to the input ports of these transducers through the input ports of
experimental frame. The transducer ‘TRANSD_CYCLE
_TIME’ computes the cycle time of each lot. It uses the
port ‘startCon’ to note the time at which each entity is created; further it uses the port ‘passed’ to note the time at
which each entity passes the inspection; cycle time is computed as the difference between these two times.

0
1

4

7

10 13 16 19 22 25 28 31 34 37 40 43 46
Week

Figure 4: Start Pattern for the Rough Framing Trade
The framing inspection pass rate was computed using
the inspection data collected for one complete year - October 2003 to September 2004 (City of Peoria 2004). From a
total of 35631 framing inspections that belong to 932 lots,
the framing inspection pass rate was computed as 15%,
43%, 55%, 62%, 71%, and 100% for the first, second,
third, fourth, fifth, and sixth inspections respectively.
Two scenarios were analyzed based on inspection pass
rate. In scenario-1, the pass rate is based on the actual inspection data analysis which is 15%, 43%, 55%, 62%,
71%, and 100%. In scenario-2, the pass rate is 100% in the
first inspection.
The number of framing crews in a subdivision normally varies from two to four. In this study, it is assumed
that the framing crew size is two. The duration for construction was computed using the schedule data obtained
from one of the leading home builders; it was found to be 7
days. Since the current study assumes that all lots passes in

2081

Palaniappan, Sawhney, and Sarjoughian

Figure 5: Snapshot of the Complete Simulation Model
The finish time (inspection pass time) of all entities is
noted and using this, the number of lots per week released
by the framing trade is computed. The number of lots per
week received by drywall trade is same as the number of
lots per week released by the framing trade. The transducer
‘TRANSD_WAIT_TIME’ computes the average waiting
time of lots for framing crew. It marks the time at which
entities join/leave the queue by using the ports ‘qJoin’ and
‘qLeave’ respectively. The waiting time is computed as the
difference between the queue leaving time and the queue
joining time. The transducer ‘TRANSD_RESOURCE_UT’
computes the resource utilization; it uses the port ‘res_use’
to mark the time at which a job is allocated to a resource.

is used to model a scenario where there is a process that is
served by several servers or resources (Zeigler and Sarjoughian 2003). The number of servers can be specified as
an input variable while defining the architecture.
The
coupled
model
‘FRAMING_TRADE_
CONTRACTOR’ represents the construction and rework
processes performed by the framing trade contractor. It is
based on the multi-server architecture and consists of two
types of atomic models namely multi-server coordinator
and a simple processor. There are two processors (resources) defined within this coupled model and they represent the two framing crews namely ‘crew1’ and ‘crew2’.
The multi-server coordinator is represented by the atomic
model ‘MultiScoCon’. The responsibilities of the coordinator are (i) to queue the incoming entities in a FIFO based
queue if all resources are busy; (ii) to send or assign jobs to
one or more processors that are available (iii) to receive the
completed jobs from one or more processors and send
them through the output port. The multi-server allows
sending jobs simultaneously to multiple crews (e.g., crew1
and crew2) and thus all crews can process their work concurrently. It also supports receiving processed jobs that are
completed at the same time.

4.2 Coupled Model ‘SIM_PROCESSOR’
This coupled model represents the simulation model specification. It consists of two coupled models namely
‘FRAMING_TRADE_CONTRACTOR’
and
‘CITY_
BUILDING_INSPECTOR’, which represent the framing
construction process and the framing inspection process respectively. These two coupled models are based on multiserver architecture. Multi-server architecture is a reusable
modeling template defined within the DEVS framework; it

2082

Palaniappan, Sawhney, and Sarjoughian
The coordinator ‘MultiScoCon’ receives incoming entities through three input ports namely ‘in’, ‘rework’ and
‘x’. Entities that come to the framing crew for construction
work are received through the input port ‘in’; entities that
come for rework after a ‘fail’ in the first or succeeding inspections are received through the port ‘rework’; entities
that complete service with a framing crew are received
through the port ‘x’. The coordinator uses four output ports
namely ‘con_out’, ‘rw_out’, ‘y’ and ‘job_alloc’. The entities that have completed construction work are sent
through ‘con_out’; entities that have completed rework are
sent through ‘rw_out; ‘y’ output port is used to send entities to processors for service; the output port ‘job_alloc’ is
used to send data to the transducer (calculates resource
utilization) whenever a job is allocated to a processor.
The
coupled
model
‘CITY_BUILDING_
INSPECTOR’ represents the inspection process performed
by the city building inspector. Inspection happens normally
after the completion of each construction or rework process
by a framing crew. This coupled model consists of 5
atomic models out of which 4 are simple processors and 1
is a multi-server coordinator. Each processor denotes an
inspector resource and they are inspector:1, inspector:2,
inspector:3 and inspector:4. The functionality of the coordinator (called ‘MultiScoInsp’) is similar to the coordinator
‘MultiScoCon’ defined in the coupled model ‘FRAMING_
TRADE_CONTRACTOR’.
The coordinator uses two input ports to receive incoming entities: (i) port ‘in’ to receive entities that come for
inspection after construction or rework and (ii) port ‘x’ to
receive entities that have completed inspection process by
processors inspector:1 through inspector:4. Four output
ports namely ‘pass’, ‘fail’, ‘y’ and ‘frw’ are used by the
coordinator. Entities are sent to processor for inspection
through port ‘y’; entities that passes the inspection are sent
through the port ‘pass’; entities that fail in the inspection
are sent through the port ‘fail’; the port ‘frw’ is used to
send entities to transducer in order to note the time at
which an entity goes for first rework.
5

wall trade per week for Scenario-1 and Scenario-2 are
shown in Figure 6 and Figure 7 respectively.
Job arrival rate to the Drywall Trade [Scenario-1]

Number of jobs arrived

10
8
6
4
2
0
1

5

9

13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77
Week

Figure 6: Job arrival rate to drywall trade: Scenario-1
Job arrival rate to the Drywall Trade [Scenario-2]

Number of jobs arrived

10
8
6
4
2
0
1

5

9

13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77
Week

Figure 7: Job arrival rate to drywall trade: Scenario-2
The authors have run the same scenarios using the
construction simulation software ‘Simphony’ and verified
that the results of DEVSJAVA are same as the results of
Simphony. This confirmed the correctness of the DEVS
model implementation for the test problem.
The following are the inferences based on the simulation results:
•

SIMULATION RESULTS AND DISCUSSION

•

The simulation model was run for two scenarios: (i) Scenario-1: the inspection pass rate is based on the actual data
and it is 15%, 43%, 55%, 62%, 71%, and 100% in the first,
second, third, fourth, fifth, and sixth framing inspections respectively and (ii) Scenario-2: the inspection pass rate is
100% in the first inspection.
Each scenario was run for 10 runs; the mean and variance of number of starts plot for each run was computed;
and then the average mean and average variance of all 10
runs was computed. A representative number of starts plot
was selected for each scenario in which the mean and variance of the selected run is close to the average mean and
average variance. The number of jobs released to the dry-

When the first inspection pass rate is 15% (Scenario-1) and there is limited crew (2 framing
crews) on the upstream side, there is high variability in the workflow (number of lots per week) released to the downstream (drywall) trade.
On the other hand, when there is 100% (Scenario2) first time inspection pass rate and limited crew
(2 framing crews) on the upstream side, the job
arrival pattern to the drywall follows an even flow
pattern. The two plots shown in Figure 6 and Figure 7 are compared quantitatively in Table 1.

From the results shown in Table 1, it can be noted that
the inspection pass rate and the crew size of the upstream
trade influences the workflow variability of the downstream trade. Poor inspection pass rate on the upstream increases the workflow variability and the spread of the
number of starts on the downstream. Hence, improving the
inspection pass rate on the upstream is critical to ensure
better work flow to the downstream trade.

2083

Palaniappan, Sawhney, and Sarjoughian
Table 1: Comparison of Scenario-1 and Scenario-2
Factor
Scenario-1
Scenario-2
Minimum number of
1
1
starts per week
Maximum number
8
2
of starts per week
Week 9 to
Week 9 to
Spread of
week 74
week 56
number of starts
(66 weeks)
(47 weeks)
Mean
2.20
1.96
Sample Variance
2.93
0.04
6

modeling constructs through user interface to enable the
modeler to drag and drop the modeling elements and to
build simulation model and (ii) effective user interface for
each modeling element to specify input to the simulation
and read output after the simulation.
In general, when sophisticated user-interface features
are provided to the modeler, the modeling freedom is often
limited and it depends on the basic framework adopted in
the simulation tool. On the other hand, where there is comprehensive modeling freedom available, user-interface features available to the modeler are often limited or none.
Development of simulation models in DEVSJAVA
requires not only knowledge of sound modeling and simulation principles, but also varying degrees of Java programming language depending on the complexity of models being developed. That is, while the atomic and coupled
models in DEVSJAVA have common (reusable) syntax,
modelers can use the full power of the Java programming
language to devise any level of complex logic as needed.
To support modeling by domain experts (e.g., managers),
the Scalable Entity Structure Modeler (SESM) framework
has been developed to support visual and persistent model
development (Sarjoughian 2005). The SESM environment
automatically generates complete simulation code for coupled models and partial simulation code for atomic models.
Therefore, the combination of SESM and DEVSJAVA allows domain experts including construction experts to develop models while allowing researchers to carry out advanced research in the field of modeling and simulation,
e.g., develop integrated simulation models where a combination of detailed processes, control, and financial planning is desired (Sarjoughian et al. 2005).

CONCLUSIONS

This work demonstrates the application of DEVSJAVA in
construction simulation through a detailed residential construction example. DEVS is a formal framework for discrete
event modeling and simulation. It has a number of important
features such as component-based hierarchical model development, support for modularity and reusability, parallel and
distributed execution, and support to the modeler to develop
custom components during simulation model development.
User can perform the simulation in DEVSJAVA either using
the STEP option (iterate through simulation clock event by
event) or using the RUN option (fast mode).
The application of DEVSJAVA is not restricted to
process and resource based simulation but it can be widely
applied in many domains to conduct various types of simulation, for example agent-based simulation. Further
DEVSJAVA can be integrated with other software
tools/components such as MATLAB/Simulink to bring in
additional functionality into the simulation. This affords
extending construction simulation with optimization capability to help manage large-scale, complex enterprises (Sarjoughian et al. 2005).
DEVSJAVA enables the modeler to develop an effective simulation model representation without any repetitions. The example problem discussed in this paper consists of one construction cycle and five rework cycles as
shown in Figure 3. When this is implemented using a conventional discrete event simulation tool, the simulation
model representation will be similar to Figure 3. This
means that the same set of modeling elements will be repeated for each rework cycle. From the equivalent
DEVSJAVA simulation model shown in Figure 5, it can be
noted that the model does not have any repetitions of process flow for the five rework cycles. The logic is internally
captured within the multi-server architecture used for the
construction and inspection process.
A simulation environment/tool can be used for teaching, research or industry practice. Simulation tools for
teaching or research usually do not provide sophisticated
user interfaces. Use of simulation in industry practice
needs the support for rapid model development and simulation in a short time. This requires (i) provision of basic

ACKNOWLEDGMENTS
The study was supported in part by the National Science
Foundation (NSF) through Grant Number 0333724. The
authors wish to express their appreciation to City of Peoria
for providing the data presented in this paper. The opinions, conclusions, and interpretations expressed in this paper are those of the authors, and not necessarily of NSF.
REFERENCES
Abourizk, S. M., D. W. Halpin, and J. D. Lutz. 1992. State
of the Art in Construction Simulation. In Proceedings
of the 1992 Winter Simulation Conference, ed. J. J.
Swain, D. Goldsman, R. C. Crain, and J. R. Wilson,
1271-1277. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers.
AbouRizk, S. M., and D. Hajjar. 1998. A Framework for
Applying Simulation in the Construction Industry.
Canadian Journal of Civil Engineering, 25(3), 604617.

2084

Palaniappan, Sawhney, and Sarjoughian
puting in Civil Engineering, ASCE, Dallas, Texas,
1139-1146.
Lluch, J. F. and D. W. Halpin. 1982. Construction Operation and Microcomputers. Journal of the Construction
Division, ASCE, 108(1), 129-145.
Martinez, J.C. and P.G. Ioannou. 1994. General Purpose
Simulation using Stroboscope. Proceedings of the
1994 Winter Simulation Conference, ed. J. D. Tew, S.
Manivannan, D.A. Sadowski, and A.F. Seila, 11591166. Piscataway, New Jersey: Institute of Electrical
and Electronics Engineers.
Martinez, J. C. and P. G. Ioannou. 1999. General Purpose
Systems for Effective Construction Simulation. Journal of Construction Engineering and Management,
ASCE, 125(4), 265-276.
Odeh, A. M., I. D. Tommelein, and R. I. Carr. 1992.
Knowledge-Based Simulation of Construction Plans.
Proceedings of the Eighth Conference on Computing
in Civil Engineering, ASCE, Dallas, Texas, 10421049.
Paulson, B. C. 1978. Interactive Graphics for Simulating
Construction Operations. Journal of the Construction
Division, ASCE, 104(1), 69-76.
Sarjoughian, H. S. 2005. A Scalable Component-based
Modeling Environment Supporting Model Validation.
Interserive/Industry Training, Simulation, and Education Conference, 1-11, Orlando, FL.
Sarjoughian, H. S., D. Huang, W. Wang, D. E. Rivera, K.
G. Kempf, G. W. Godding, H. D. Mittelmann. 2005.
Hybrid Discrete Event Simulation with Model Predictive Control for Semiconductor Supply-chain Manufacturing. Proceedings of the 2005 Winter Simulation
Conference, 256-266, Orlando, FL. Piscataway, New
Jersey: Institute of Electrical and Electronics Engineers.
Sawhney, A. and S. M. AbouRizk. 1995. HSM - Simulation-based Project Planning Method for Construction
Projects. Journal of Construction Engineering and
Management, ASCE, 121(3), 297-303.
Sawhney, A., H. H. Bashford, K. D. Walsh, A. Mund.
2001. Simulation of Production Home Building using
Simphony. Proceedings of the 2001 Winter Simulation
Conference, ed. B. A. Peters, J. S. Smith, D. J.
Medeiros, and M. W. Rohrer, 1521-1527. Piscataway,
New Jersey: Institute of Electrical and Electronics Engineers.
Sawhney, A., H.H. Bashford, S. Palaniappan, K. D. Walsh,
and J. Thompson. 2005. A Discrete Event Simulation
Model to Analyze the Residential Construction Inspection Process. Proceedings of the 2005 ASCE International Conference on Computing in Civil Engineering, ed. L. Soibelman and F. Peña-Mora, July 12–
15, 2005, Cancun, Mexico.

AbouRizk, S. M. and Y. Mohammad. 2000. Simphony-An
Integrated Environment for Construction Simulation.
In Proceedings of the 2000 Winter Simulation Conference, ed. J. A. Joines, R. R. Barton, K. Kang, and P.
A. Fishwick, 1907-1914. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers.
ACIMS. 2005. DEVSJAVA [online]. Available via
<www.acims.arizona.edu/SOFTWARE/soft
ware.shtml> [accessed March 31, 2006].
Ballard, G. and G. Howell. 1998. Shielding Production:
Essential Step in Production Control. Journal of Construction Engineering and Management, ASCE,
124(1), 11-17.
Ballard, G. 1999. Improving Work Flow Reliability. 7th
Annual Conference of the International Group for
Lean Construction (IGLC-7), Berkeley, California,
USA, 26-28 July 1999.
Bashford, H. H., A. Sawhney, K. D. Walsh, and K. Kot.
2003. Implications of Even Flow Production Methodology for the U.S. Housing Industry. Journal of Construction Engineering and Management, ASCE,
129(3), 330–337.
Chang, D. Y. and R. I. Carr. 1987. RESQUE: A Resource
Oriented Simulation System for Multiple Resource
Constrained Processes. Proceedings of the PMI Seminar/Symposium, Milwaukee, Wisconsin, 4-19.
Chow, A. 1996. Parallel DEVS: A Parallel, Hierarchical,
Modular Modeling Formalism and Its Distributed
Simulator. SCS Transactions on Simulation, 13(2), 55102.
City of Peoria. 2004. Record of Inspections Completed
from October 2003 to September 2004. Peoria, Arizona.
Hajjar, D. and S. M. AbouRizk. 2000. Application Framework for Development of Simulation Tools. Journal of
Computing in Civil Engineering, ASCE, 14(3), 160167.
Hajjar, D. and S. M. AbouRizk. 2002. Unified Modeling
Methodology for Construction Simulation. Journal of
Construction Engineering and Management, ASCE,
128(2), 174-185.
Halpin, D. W. 1977. CYCLONE: Method for Modeling of
Job Site Processes. Journal of the Construction Division, ASCE, 103(3), 489-499.
Halpin, D. W. and L. S. Riggs. 1992. Planning and Analysis of Construction Operations. Wiley Inter Science,
New York, N.Y.
Ioannou, P. G. 1990. UM-CYCLONE Discrete Event
Simulation System User’s Guide. Technical Report,
UMCE-89-12, Dept. of Civil and Environmental Engineering, University of Michigan, Ann Arbor, Michigan.
Liu, L.Y. and P. G. Ioannou. 1992. Graphical ObjectOriented Simulation System for Construction Process
Modeling. Proceedings of the 8th Conference on Com-

2085

Palaniappan, Sawhney, and Sarjoughian
Teicholz, P. 1963. A Simulation Approach to the selection
of Construction Equipment. Technical Report No. 26,
The Construction Institute, Stanford University.
Tommelein, I. D., D. Riley, and G. A. Howell. 1999. Parade Game: Impact of Work Flow Variability on Trade
Performance, Journal of Construction Engineering and
Management, ASCE, 125(5), 304–310.
Zeigler, B. P., T. G. Kim, and H. Praehofer. 2000. Theory
of Modeling and Simulation. Second Edition, Academic Press, New York, NY.
Zeigler, B. P. and H. S. Sarjoughian. 2003. Introduction to
DEVS Modeling and Simulation with JAVA.
DEVSJAVA Manual, [online]. Available via
<www.acims.arizona.edu/PUBLICATIONS/
publications.shtml> [accessed March 31,
2006].

tion at ASU. For more information visit <http://
www.acims.arizona.edu>
and
<http://
www.eas.asu.edu/%7Ehsarjou/index.htm>

AUTHOR BIOGRAPHIES
SIVAKUMAR PALANIAPPAN is Graduate Research
Associate in the Del E Webb School of Construction at
ASU. He received his B.E. degree in Civil Engineering
from Madurai Kamraj University, Tamil Nadu, India in
1997, and a M.S. degree in Building Technology and Construction Management from I.I.T. Madras, Chennai, India
in 2002. He is a doctoral student in the Department of Civil
and Environmental Engineering at ASU since January
2004. His research interests are production home building,
construction simulation, and automation in construction.
His e-mail address is <plsiva@asu.edu>.
ANIL SAWHNEY is Associate Professor in the Del E
Webb School of Construction at Arizona State University
(ASU). His research and teaching are in the areas of integration of information technology in construction and construction process modeling and simulation. He teaches
courses in Construction Planning and Scheduling, Information Technology in Construction, Construction Productivity, and Design and Analysis of Construction Operations.
Dr. Sawhney is the Co-Director of the Housing Research
Institute (HRI) at ASU. He has published over 50 technical
papers in journals and conferences. His e-mail address is
<Anil.Sawhney@asu.edu> and his web address is
<http://construction.asu.edu/faculty/sa
whney>.
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science and Engineering at Arizona State University, Tempe and Co-Director of the Arizona Center for
Integrative Modeling and Simulation. His research includes simulation modeling theories and methodologies
with emphasis on multi-formalism modeling, agent-based
and collaborative modeling, simulation-based design, and
software architecture. His educational emphasis has lead in
the Online Master of Engineering in Modeling and Simula-

2086

Proceedings of the 2000 Winter Simulation Conference
J. A . Joines. R. R. Burton, K. Kung, and P. A. Fishwick, eds.

MODELS AND REPRESENTATION OF THEIR OWNERSHIP

Hessam S. Sajoughian
Bernard P. Zeigler
Arizona Center for Integrative Modeling and Simulation
Electrical and Computer Engineering Department
University of Arizona
Tucson, AZ 85721-0104, U.S.A.

ABSTRACT

(HLA) (Dahmann, Kuhl et al. 1998, DoD 1998, DoD 1998,
DoD 1999) and Common Object Request Broker
Architecture (COMA) (Orfali, Harkey et al. 1997; OMG
1998; ORyan, Levine et al. 1999). These have been
employed to enable some degree of interoperability among
distributed simulations (Fujimoto 1990; Fujimoto 1998).
Similarly, advanced techniques have been proposed and
implemented to reduce amount of data transmission among
simulation nodes by a few orders of magnitude (Zeigler,
Ball et al. 1998, Zeigler, Hall et al. 1999). While
considerable research has been .conducted in the areas of
simulation interoperability and scalability, M&S research
has yet to pursue research in the area of model ownership.
Conceptual basis and modeling paradigms for model
ownership is an essential need for distributed simulation,
especially in multi-organizational settings.
Accountability for model ownership is integral to the
existence and firther advances in collaborative modeling
as well as distributed simulation. There are numerous
instances where ownership becomes the initial, and often
the main, impediment to collaborative model development
across multiple, sometime competing, organizations.
Consider the case where two teams of modelers from two
different organizations must collaborate with one another
to develop a complex model comprised of two sets of
models. One group of modelers is to devise a model of a
communication system including its host hardware with
many hundreds of components. The other group of
modelers is to embark on building an E-commerce
business application. The key observation is that while
these two organizations are developing their own models,
they must eventually interoperate with one another. This is
to due to fact that only combined simulation of these two
sets of models can reveal intertwined, complex concealed
behavioral characteristic of the overall model. However,
modeling of such large-scale, multi-organizational systems
are plagued with numerous pitfalls. For example, due to
lack of systematic model ownership and consequently lack
of availability of other’s models, the modeling teams must

Models, similar to other intellectual properties, are
increasingly being treated as commodities worthy of
protection. Providing ownership for models is key for
promoting model reusability, composability, and
distributed .simulation. However, to date, it appears no
principled approach has been developed to support
ownership of models. Instead, individuals such as
modelers and legal personnel employ ad hoc means to
obtain and (re)use models developed and owned by others.
In this article, we briefly describe access control
capabilities offered by computer languages, operating
systems, and HLA ownership management services. The
examinations of such methods suggest the need for formal
ownership specification. The article discusses, in an
informal setting, requirements for model ownership from
the point of view of increasing demand and necessity for
model reuse, distributed simulation, and future trends for
collaborative model development. We develop concepts for
model ownership suitable for collaborative model
development and distributed execution. Based on the
developed concepts, we present an approach, within the
DEVS modeling & simulation framework, for specifying
model ownership. The article closes with the consideration
of the proposed approach for the Collaborative DEVS
Modeling environment and a brief discussion of HLA
services relevant to model ownership.

1

INTRODUCTION

For many years, research inquires and emphasis in
distributed computing, and distributed simulation in
particular, has been on advancing computational,
communication, time management, and load-balancing
techniques. Recently, other basic research inquiries in
distributed simulation have focused on in interoperability
and scalability issues. Examples of such inquires are
middleware technologies such as High Level Architecture
440

Sarjoughian and Zeigler

make obscure assumptions. The undesirable consequence
is that these independently developed sets of models are
unlikely to interoperate. However, with well-defined
model ownership, teams can use each other’s models at
varying levels of details throughout model development
and execution phases and therefore minimize model
interoperability related uncertainties. Furthermore, model
ownership can provide orderly handling of proprietary
issues.
As of this writing, HLA offers some limited
capabilities to designate which set of simulation models
can make available their attributes to other simulation
models.
For example, HLA supports data
publishinghubscription and data management - i.e., it
provides ownership in the sense of who may publish data
as opposed to, for example, who the actual owner of a
model is and what authorities the owner may posses.
Similarly, in CORBA, the Electronic Commerce taskforce
Object Management Group (OMG), has been proposed an
specification to support altemative negotiation styles (e.g.,
bilateral and multilateral) and legal related issues (e.g.,
jurisdiction). The capabilities offered by HLA and C O M A
indicates two distinct aspects of ownership: micro level
(attributes of a model are considered) and macro level
(entire organizations are considered).
Therefore, in this work we will provide a basis for
model ownership concepts such as “modeler rights and
privileges” in a collaborative setting. To devise an
approach for formally representing model ownership, we
will employ the Discrete-event System Specification
(DEW) modeling framework.
2

efforts are directed toward a process whereby models (1)
can be developed and stored according to standards, (2)
identified, and (3) can be assigned authority (Le.,
credibility) level. This and the HLA initiative are
important in building a repository of “reusable” models.
However, we believe collaborative, distributed modeling
paradigm is needed based on the SCO principles as the
underlying foundation to guide objectives such as those
advocated by the ADS project. Within such a formal
collaborative, distributed modeling paradigm, not only the
data credibility, composability, and cost objectives are
more likely be achieved, the broader needs (e.g.,
distributed simulation) of the M&S community can be
supported as well.
2.1 Scalability

An examination of existing modeling and simulation
environments reveals that ever more larger models are
needed to represent an array of systems - environmental
models (e.g., climatology), enterprise resource planning
(e.g., supply chain), computer networks (e.g., FAA), and
system of systems (e.g., C4I).
Models capable of
representing dynamics of such systems can be
characterized along two dimensions: (1) number of submodels and (2) data size and exchange frequency. From
the modeling perspective, these dimensions, are
interdependent since data size and exchange frequency are
in part directly due to the number of sub-models and the
number of state variables.
The other key factors
responsible for increased interaction among submodels can
be attributed to model resolution, accuracy, cost, and
execution speed.

SCALEABLITY, INTEROPERABLITY,
AND OWNERSHIP

2.2 Interoperability

Increasingly contemporary software systems are expected
to operate in a distributed setting due to, in part, extensive
interdependencies among various worlds’ economies,
short-lived collaborations, and their expected and
accidental emerging complexities. Distributed computing
technologies are expected to support scalability and
interoperability requirements for systems exhibiting
homogeneity and heterogeneity characteristics. While
scaleabilty and interoperability are attracting much needed
attention, in contrast, the necessity and role of
distributed/multi-organization ownership is hardly
recognized and reckoned with. This is unfortunate since the
trio of scalability, interoperability and ownership must be
collectively supported to enable distributed modeling and
simulation.
The Authoritative Data Source (ADS) project, under
the Defense Modeling and Simulation Office (DMSO)
Master Plan 5000.59-P directive, has undertaken steps
toward supporting data credibility and composability, and
reduction in cost (Sheehan, McGlynn et a!. 1999). Such

Simulation models of a variety of systems have been
developed over the span on many decades, many of which
in isolation and therefore without the goal of satisfying
model interoperability. Consequently, interoperability,
generally, is considered at the simulation model execution
level. Absence of formal interoperability concepts at the
modeling level can be attributed to model development
across literally all scientific disciplines without adhering to
any formal, comprehensive modeling framework. This may
explain why DoD’s High Level Architecture (HLA) and its
Object Model Template (OMT) proposed standards are
primarily specified at the simulation level. HLNOMT can
support “interoperability” among simulations using the
object orientation concepts. For example, it appears that
interoperability across DoD’s training, analysis, and
acquisition mandates can be quite hard to achieve without
relying on the old ad hoc means. In software engineering,
CORBA has been instrumental in supporting
heterogeneous software components to interoperate. The

441

Sarjoughian and Zeigler

choices are public and abstract. The access control of a
clasdinterface (object) with an inheritance relationship is
determined by its inherited parent field declaration.
In Java, for example, public, protected, private, and static
field modifiers can be assigned to attributes. Other, less
relevant athibute field modifiers are final, transient, and
volatile. The final modifier can be used for providing one
incarnation of the field, transient modifier provides non-persistence fields, and volatile modifier makes private copies for
threads. The field declarations for methods are public, protected, private, abstract, static, final, synchronized, and native.
Field declarations and their abilities to control read,
write, and execution can be seen in two settings: nondistributed and distributed. The field declarations have
well-defined characteristics in non-distributed setting - that
is a class hierarchy of objects contained in a single memory
workspace. In contrast, field declarations in a distributed
setting are primarily through packages. While attributes,
methods, and inheritance field declaration provides some
level of mutual exclusion, they cannot support ownership
since ownership can be enforced through file ownership
offered by operating system.
Moreover, the field
declarations are too weak for control access control for
individual use, especially outside of a given hierarchy of
classes or a package. For example, consider an attribute for
a class clsA. By declaring this attribute to have no field
modifier, the attribute is available within its own package
and to any other child class (e.g., clsB e x t e n d s
c 1SA) that resides in the same package.

CORBA specification (CORBA 1995) provides a host of
services such as events, persistence, naming, time,
concurrency, and licensing. These services, however, do
not have their foundation in dynamical system modeling
and therefore cannot in their present form enable “model
interoperability.” Nevertheless, the underlying concepts of
CORBA provide a sound basis for modeling and
simulation interoperability.

2.3 Ownership
We will compare model ownership with access control to
show the required additions. Data and specifically
component access control, from the object oriented worldview, is through field declarations. For example,
“ownership” of simple and higher-order objects (derived
from inheritance and composition relationships) can be
enforced through field declarations. However, relying
solely on such accessibility mechanisms for providing
access control offer limited ownership capabilities for
model development, model (re)use, and execution.
Furthermore, in a distributed setting, such access controls
are weak since they cannot provide a systematic approach
to assigning, maintaining, and controlling ownership at
alternative levels of granularities (e.g., attribute vs. class
ownership).
Other means for access control can be enforced
through security check and more generally private
networks and user authentication. These approaches have
been employed for users and their applicability and
adaptation for model ownership is an open research
inquiry. Next we discuss access control from the 00
perspective as well as publisldsubscribe in terms of data
exchange among distributed nodes and model bases.
Other capabilities related to ownership (e.g., access
control) have been available for many years. For example,
SCMS Software Configuration Management Systems
(SCMS) (Bersoff, Henderson et al. 1980) and configuration
programming (Shaw, DeLine et al. 1995, Bishop and Faria
1996) support some types of access to data primarily based
on the operating systems’ services.

2.3.2 Publish/Subscribe in Distributed Computing

2.3.1 Access Control in Object Orientation
Object orientation provides a rich set of access control
fields for each of classes, interfaces, attributes, and
methods. A clasdobject is typically has attributes,
methods, as well as inheritance and composition
relationships. Attributes generally can be first-class
objects. Therefore, an object’s attributes can be thought of
as component having its own attributes, methods, and
inheritance relationship. Object orientation provides a
uniform access control policy via field declarations for
objects, attributes, and methods. Class field modifier
choices are public, abstract, and final. Interface modifier

-

442

In the world of distributed computing, the basic concepts
of object-orientation are insufficient due to the fact that
there does not exist a single-address space, but instead
make it possible to communicate across a network.
Accordingly, “distributed object”, an extend form of
object, make it possible for it to be used in a seamless
fashion. Such distributed objects not ‘only have their
encapsulated knowledge, but also can be manipulated
across a network or send their data to others.
The concepts of publish and subscribe allow a
component to publish its own data or subscribe to data
from other components. The components capabilities to
interchange data, makes it possible for them to be
heterogeneous and therefore support portability and
scalability. Of course, components should be able to
interoperate with inhomogeneous data types under the
condition that they are able to transform receiving data to
their expected types.
Data exchange can be either through ports or not.
Components can have unidirectional input and output ports
through which various kinds of data types can be sent or
received. Alternatively data can be sent or received
through “virtual” bi-directional ports.

Sarjoughian and Zeigler

1998; Zeigler, Praehofer et al. 2000). In addition to DEVS
Modeling constructs, the System Entity Structure (SES) in
conjunction with rule-based system have been proposed
and employed to deal with design choices.
The
combination of DEVS and SES can provide a suite of
modeling capabilities for representing model spaces and
dynamics containing intelligent features (e.g., rule-based
model synthesis, fuzzy logic, and neural networks based
dynamics.)

2.3.3 Distributed Model Bases
Our underlying assumption is that persistence models exist
in an independent fashion in a distributed setting.
Moreover, our treatment of the ownership is independent
of form of storage -that is models located in a node can be
either in a database or not. The basic requirement that
must be satisfied is that the model can be treated as an
object and independent of any specific programming
language.

3.1 Discrete Event System Specification

3

INFUSING OWNERSHIP INTO
D E W MODELING

The Discrete Event System Specification (DEVS) (Zeigler,
Praehofer et al. 2000)modeling approach supports
capturing a system’s structure in terms of atomic and/or
coupled models where coupled models are hierarchical
satisfying closure under coupling (Zeigler 1984). While a
part of a system can be represented as an atomic model
with well-defined inputloutput interfaces, a system
represented as a DEVS coupled model designates how
systems can be coupled together to form system of
systems. Such coupled models also have the same
input/output interfaces. Given atomic models, DEVS
coupled models can be formed in a straightforward
manner. Both atomic and coupled models can be
simulated using sequential andor various forms of parallel,
distributed computational techniques (Zeigler and
Sarjoughian 1997).

A rather extensive variety of models, methodologies, and
applications have been introduced. For example, models
are represented using laws of physics, chemistry, statistics
using a variety of mathematical formalisms such as a
systems theory, neural networks, fuzzy logic, situation
calculus. Aside from these types of models, many of
today’s contemporary computer systems (e.g., commerce)
are modeled using UML (UML 2000) and other computer
language-based schemas such as System Entity Structure
(SES) (Zeigler 1984; Rozenblit 1992).
Model design and simulation of distributed systems, in
comparison to non-distributed (singular) systems, is much
more complex. From the standpoint of collaborative model
development and distributed simulation, scalability,
interoperabiiity, and ownership considerations are of
fundamental importance. In particular, a viable modeling
paradigm must enable its users to represent and formalize
interoperability and ownership concepts as well as lending
itself as blueprint for alternative forms of simulation
execution strategies.
Given the existence of numerous modeling
approaches, we focus our attention on systems theory as
the basis to formalize interoperability and ownership
concepts discuss above. From the three alternative system
theoretic formalisms, discrete event system specification
has been shown to lend itself quite well in characterizing
many kinds of systems exhibiting causal, time-invariant
(varying), and (non) deterministic behaviors. More
importantly, distributed systems, generally, are eventoriented and are generally best represented in discrete
event form.
Given modeling paradigms founded on the principles
of System Theory, the Discrete-event System Specification
(DEVS) is believed to be the most appropriate candidate
for incorporating distributed model ownership. The DEVS
modeling formalism enables characterization of systems in
terms of hierarchical modules with well-defined interfaces.
Due to its system-theoretic foundations, DEVS modeling
paradigm naturally maps into object-orientation
implementation and consequently has been implemented in
sequential, parallel, and distributed environments (AIS

3.2 Atomic Model with Ownership

Aside from 00 declaration fields such as public, we
employ the broader concepts of publish and subscribe.
These concepts are used with distributed systems (e.g.,
clientlserver applications and distributed simulations). We
define DEVS atomic model with ownership as a
mathematical structure:

AM = (X.Y, S, S. A, tu, ons) where
X

set of input events,
set of sequential states,
Y set of output events,
6 state transitions due to internal changes and
external stimuli
L output function generating external events as
outputs,
tu time advance function.

S

3.2.1 Ons Ownership

Sets S and Y represent publishable and non-publishable
data of an atomic model. Similarly, set X represents
subscribable data.
The element ons is used to
assigdidentify the model owner. The model ownership is
w.r.t. the data contained in the model and specifically what

443

Sarjoughian and Zeigler

If multiple owners are permitted or desired,
authorities
(e.g.,
appropriate
levels of
mastedslave) mechanisms are needed in addition
to support for resolving conflicts for inconsistent
concurrent assignments of (un)publishable states.

can be made available to other models. The remaining
elements of the atomic model structure represent its
dynamics; In terms of modular, hierarchical distributed
modeling, internal functionality of an atomic model is of
interest only to the extent of its states and inputloutput
events (data) associated with their inpdoutput ports.
Therefore, we can characterize an atomic model’s
ownership as follows.
r

The existence of inputloutput ports can result in
separate ownership of statedparameters, thus allowing for
distinct ownership for ports and stateslparameters. Under
the supposition that both ports and statedparameters of an
atomic model can be assigned ownership, ownership can
be of three types: (a) single owner for ports and
statedparameters, (b) a single owner for each port and the
data (i.e., statedparameters) that is made available via it,
(c) multiple owners for ports and a single owner for
statedparameters. Input and output ports have opposite
scope w.r.t. statedparameters. An output port can sanction
what subset of publishable states can be made available to
other models. That is, output port ownership is superior to
data ownership. The date owner, however, would be able
to sanction what input values (states published by other
models) it may choose to use, thus asserting control
(superiority) over input port ownership.
These
characterizations, along with the concept of modularity and
object orientation, point to case (a) as being the most ideal.
In this case, state/parameters and output ports .ownership
are the same. Similarly, ownership of input ports would be
the same as acceptinghejecting other model’s published
statedparameters. Cases (b) and (c) provide no
fundamental advantage since supporting such finer grain
ownership will complicate needlessly ownership
characterization of coupled models. In the next section, we
consider ownership of ports for the coupled model case in
view of our present discussion.

1

Atomic Model
@_., _v*)
~

sublcnbc

Spubpublkhable states

s:

non-publishoble states

I

OnS. owner of S,, and S,

{(p,, v,) I p, E P,, input port names, vx E V,,
input.values, Ssub + Vx, SSub = Spub v Ppub
where Spub and Ppub are other DEVS models
publishable states and parameters, respectively.}
Y = {(pY ’vY) I pY E Py’ arbitrary inputport names,
vy E Vy. arbitrary input values, Spub + Vy,

X

’

=

spub 5 s.}
spub v SN where Spub and SN are the set of
states that are publishable and non-publishable
states, respectively.
ons is an arbitrary name with an associated
authentication (passwd.) The model owner is able
to specify publishable vs. non-publishable states
and parameters. Owner can be a singular or not.
S

=

Coupled Model

spub:
publishable smtcs

subsmbe

ons:uwncr of S,,

EIC

ons:owner of S,,

and S,

. ................
.

*

and S ,

...... ...............
*

CM: owner of
IC,

EIC,
EOC

EOC

444

Sarjoughian and Zeigler
Since DEVS, Differential Equations System
Specification (DESS), and Discrete-Time System
Specification (DTSS) are founded on the basis of systems
theory concept [ 191, the ownership characterization of the
DEVS atomic model is equally applicable to continuous
systems that can be represented as differential equations.
(Treatment for the discrete-time systems is the same as
DEVS.) That is, the specification of DESS is AMcont = (X

Smpub such that S k u b f SMd for i = 1. ..., m for d

0.1

Y = /byv$

I

py

E

OutPorts. v,,

E

.

Vy, Spub

E

U

.

X . . . X Snpub such that
Ppub + vy Spub _C
g p u b = SMd f o r j = I , ..., n for d E D.)

Each component is a DEVS model. That is, for
each d E D.
Md = (X Y, S, S, A, ta, OM ) is a DEVS as
defined above.

Q, Y, f: A, ons) where all of its elements are the same as
those of DEVS atomic model except f which is the rate of
change function.

3.3 Coupled Model with Ownership
Given a coupled model (CM), its representation is concise
and reusable since any coupled model has a corresponding
basic DEVS model due to the closure-under-coupling
property. The couplings among components can be
systematically captured using output to input mappings. In
particular, there exist three different types of coupling:
internal coupling, external input coupling, and external
Internal coupling interconnects
output coupling.
components of a coupled model. External input coupling
interconnects input ports of a coupled model to input ports
of its components. Similarly, external output coupling
interconnect component output ports of a coupled model to
the output ports of the coupled model itself. Note that the
ports provide a generic pipe through which a variety of
messages (datdobjects) can be transferred from one
component to another

CM

Interpretation: model owner can (1) ascertain
components couplings and (2) restrict child component
a output values (Vy,J that can be made available as
input values (Vx$ to sibling component b.)

( X , Y, D, {Md 1 d E D ) , IC, EIC, EOC, ons)
where
X set of input port and value pairs;
Y set of outputport and valuepairs;
D set of the component names:
IC set of internal coupling connecting component
outputs to component inputs;
EIC set of External input couplings connecting
external inputs to component inputs;
EOC set of external output coupling connecting
component outputs to external outputs;
ons an arbitrary name with an associated
authentication (passwd.) Publishable vs. nonpublishable states and parameters are controlled
by MO via the internal, external input, and
external output couplings ownership.
=

Interpretation: model owner can (1) ascertain EIC
couplings and (2) restrict component CM input values
( V x , c ~ that
)
can be made available as input values
(Vx,d) to child component d.)

Subject to:
Interpretation: model owner can (1) own EOC
couplings and (2) restrict child component d output
values (VX,J that can be made available as output
values (Vx,CM) to component CM.

445

Sarjoughian and Zeigler

No direct feedback loops between the output and
input ports of any DEVS model is allowed. That is, no
output port of a component may be connected to an
,
input port of the. same component i.e., ((a,
vy,a)). (b, (Px,b , vx,b))) E IC implies a f b. For
simplicity, without -loss of generality, our formulation
does not explicitly account for parameters.

The collaborative DEVS modeler is based on the
integration of two disciplines: Modeling and Distributed
Object Computing (Sarjoughian, Nutaro et al. 1999). The
Collaborative DEVS Modeler approach to modeling is
based on the concept of a session which is a “loosely
bounded workspace” within which a group of knowledge
modelers develop a model as a team. This conceptual view
of CDM illustrates two basic issues: distribution of
modelers (knowledge workers) and their resources across
time and space. It supports synchronous and asynchronous
synthesis of models. Synchronous collaboration can be
supported by complementary capabilities such as textbased and video-teleconferencing tools in order to support
rich collaboration among modelers. These capabilities can
be tailored for modeling by providing representations of
common modeling primitives and enforcing correct
modeling constructs. Asynchronous collaboration also
benefits from the availability of modeling primitives and
semantics enforcement.
We can distinguish two types of modeling activities:
model construction and model synthesis (Zeigler,
Sarjoughian et al. 1997). Model construction mostly deals
with identifying dynamics of models while model synthesis
concerned with synthesizing coupled models given existing
atomic and/or coupled models.
Realizing that the
boundary between construction and synthesis can be
imprecise due to the iterative nature of modeling, the
development of ownership concepts, methods, and
methodologies demand caution.

Coupled models are defined to own couplings alone.
As discussed in Section 3.2, the ports of any atomic model
can have ownership associated with them. A simple
approach is to let a coupled model own its inputloutput
ports with the implication that the EIC (subscription) and
EOC (publishing) ownership are also applicable to input
and output ports. This approach lends itself to modular
ownership of ports for any DEVS model in a simple
manner. Of course, it is possible to grant multiple
ownership in a coupled model - let each of IC, EIC, and
EOC be owned by separate owners. Or indeed, designate
ownership at a finer grain level - have multiple owners for
multiple couplings for each set of declared IC, EIC, and
EOC coupling. These finer grain ownership strategies
require complex ownership strategies and are not discussed
here.

3.4 Role of Ownership in System
Entity Structure
A System Entity Structure (SES) provides the means to
represent a family of models as a labeled tree (Rozenblit
1992). Two of its key features are support for
decomposition and specialization. The former allows
representing a large system in terms of smaller systems.
The latter supports representation of alternative choices.
Specialization enables representing a generic model (e.g., a
computer display model) and its specialized variations (a
flat panel display or a CRT display.) Based on SES
axiomatic specifications, a family of models can be
represented and pruned to study and experiment with
design choices (alternatives.) An important, salient feature
of SES is its ability to represent models not only in terms
of their decomposition and specialization, but also aspects
(SES represents alternative decompositions via aspects.)
4

5

RELATEDWORK

5.1 High Level Architecture
The Department of Defense has instituted the HLA
standard (HLA 1999)across its branches to support
reusable, plug and play heterogeneous distributed
simulation.
Its overarching objective is to enable
simulation interoperability and reuse, thus enabling various
simulations to interoperate with one another in logical
andor real-time. HLNOMT is founded on the Federation
Development Process following software engineering
principles. A s such, HLNOMT offers some support for
distributed “model design” by following the software
engineering principles, but intentionally does not
incorporate any specific modeling paradigm and
methodology (Sarjoughian and Zeigler 1999; Sarjoughian
and Zeigler 1999).
The HLA standard is a modeling and simulation
interface specification. The HLNOMT specification, one
of the three pieces of the HLA standard, defines primarily
HLA object models as opposed to model dynamics. Users
are able to develop different classes of simulations across
alternative domains using HLA Interface Specification (IS)
and Object Model Template (OMT) for defining data

SUPPORTING ENVIRONMENTS

To demonstrate the applicability and usefulness of
ownership concepts, we propose the Collaborative DEVS
Modeler (CDM) (Sarjoughian, Nutaro et al. 1999). Such an
environment is attractive since it can support dispersed
modelers to develop models in a collaborative setting. In
collaborative settings, ownership related concerns are
always present if each modeler owns her mode! (i.e.,
models are stored, maintained, accessed from the modeler’s
physical location).

446

Sarjoughian and Zeigler

exchange interfaces among federates (simulation
components) and rules for constructing federations
(composite simulations). HLNIS provides federation,
time, data distribution, declaration, object, and ownership
management services. Alternative combinations of the
latter three services can be used to enable (a) object class
registration and discovery, instance attribute updating and
reflection, (b) parameter sending or receiving belonging to
interaction classes. HLNOMT supports object and
interaction class hierarchies - these class hierarchies are
interchangeable. Publishhubscribe are used with object
class attributes and sendreceive are used with interaction
class parameters. Of significant relevance in terms of
ownership are the declaration, object, and ownership
management services provided by the Run Time
Infrastructure (RTI) to federates. A great majority of these
services are in support of simulation execution while others
are for simulation model (i.e., federate) declaration and
initialization.
Declaration Management services are used by
federates to declare their intention to generate or receive
information. Object and interaction classes are the focus of
declaration management since they must be available prior
to, for example, registering object instances, updating of
attributes and sending of interactions. The Declaration
Management services are start(stop) registration of object
classes, turning on(off) interaction classes, (un)publishing
objecthteraction classes and (un)subscribing object class
attributes, and (un)subscribing interaction classes. Such
services rely on some form of ownership of the object and
interaction classes and their content (attributes and
parameters).
Object Management services such as object instances
registration and delete object instance allow the federates
of a federation to transfer ownership of object instance
attribute with on another. The RTI and federate “update
attribute values” and “reflect attribute values” services
together provide the basic mechanism for data exchange.
Other Object Management services support sending and
receiving of interactions. The send and receive interaction
services enable sending of interactions to federation and
receipt of interactions by a federate.
Ownership Management services support to grant and
transfer ownership of one or more attributes of an object
instance. A federate is an owner of an attribute and not the
object instance. This allows various attributes of an object
instance to be “owned” by different federates. Using these
services, a federate can invoke services such as “update
attribute value.” Some of the main RTl’s Ownership
Management services are “unconditionahegotiated
attribute ownership divestiture”, “attribute ownership
acquisition,” “attribute ownership release response,” and
“is attribute owned by federate”. The complementary
services provided by a federate are “inform attribute
ownership” and “attribute ownership unavailable.”

Attributes may or may not have ownership. Owner of
an instance attribute is the federate responsible for
publishing it. Likewise, if a federate seizes to publish an
attribute the attribute will have no owner. That is, there
exist interdependencies between ownership and
publication. We observe that this work presented allows
ownership assignment using well-defined comprehensive
mechanisms. It hrther supports ownership persistence,
thus
supporting
dynamic,
run-time
ownership
modifications and run time execution.
6

CONCLUSIONS

We have described the ownership for models and the
necessity of models having distinct owners. We discussed
a variety of existing means by which models may be
controlled. In particular, the concepts of access control and
HLA publisldsubscribe were discussed in relation to model
ownership. After analyzing the constituents of ownership
from a formal point of view, we extended the DEVS
modeling framework to support ownership assignment.
Ownership was defined for atomic and coupled models.
We hrther described how the system entity structure
knowledge representation scheme can be extended to
support ownership as well.
ACKNOWLEDGMENTS

This research has been supported in part by NSF Next
Generation Software (NGS) grant #EIA-9975050 and
DARPA Advanced Simulation Technology Thrust (ASTT)
Contract #N61339971(-0007.
REFERENCES

AIS

(1998). AI & Simulation Research
<http://www.acims.arizona.edu>.

Group.

Bersoff, E., V. Henderson, et al. 1980. Software
Configuration Management, Prentice-Hall.
Bishop, J. and R. Faria 1996. Connectors in configuration
programming languages: are they necessary? 3rd
International Conference on Configurable Distributed
Systems, Annapolis? MD.
CORBA 1995. COMA: Architecture and Specification,
OMG.
Dahmann, J. S., F. Kuhl, et al. 1998. Standards for
simulation: as simple as possible but not simpler the
high level architecture for simulation. Simulation
71(6): 378-387.
DoD, U. S. 1998. High-Level Architecture Interface
Specification (Version 1.3).
DoD, U. S. 1998. High-Level Architecture Rules (Version
1.3).
DoD, U. S. 1999. High-Level Architecture Object Model
Specification (Version 1.4).

447

Sarjoughian and Zeigler

Zeigler, B. P. and H. S. Sarjoughian 1997. Object-oriented
DEVS. I I th SPIE, Orlando, Florida.

Fujimoto,
R.
1990.
Distributed
simulation.
Communications of the ACM 33( IO): 30-53.
Fujimoto, R. 1998. Time management in the high-level
architecture. Simulation 71(6): 388-400.
HLA 1999. High Level Architecture, Defense Modeling
and Simulation Office.
OMG
1998.
CORBA/IIOP
2.2
Specification,
http://www.omg.org/corba/corbaiiop.html.
Orfali, R., D. Harkey, et al. 1997. The Essential
ClientBerver Survival Guide, John Wiley & Sons.
Orfali, R., D. Harkey, et al. 1995. The Essential
Distributed Objects Survival Guide, John Wiley &
Sons.
O'Ryan, C., D. L. Levine, et al. 1999. Applying a scaleable
corba events service to large-scale distributed
interactive simulations. 5th Workshop on Objectoriented Real-time Dependable Systems.
Rozenblit, J. R., J.F. Hu 1992. Integrated knowledge
representation and management in simulation based
design generation. IMACS Journal of Mathematics
and Computers in Simulation 34(3-4): 262-282.
Sarjoughian, H. S., J. Nutaro, et al. 1999. Collaborative
DEVS modeler. International Conference on WebBased Modeling and Simulation, San Francisco, SCS.
Sarjoughian, H. S. and B. P. Zeigler 1999. Collaborative
modeling: the missing piece of distributed simulation.
enabling technology for simulation science, 13th SPIE,
Orlando, FL.
Sarjoughian, H. S. and B. P. Zeigler 1999. The role of
collaborative DEVS modeler
in
federation
development. Simulation Interoperability Workshop,
Orlando, FL.
Shaw, M., R. DeLine, et al. 1995. Abstractions for
software architecture and tools to support them. IEEE
Transactions on Software Engineering 21(4): 3 14-335.
Sheehan, J., L. McGlynn, et al. 1999. Authoritative data
sources: how do i eficiently find the knowledge i
require? PHALANX32(1): 13-16.
UML
2000.
-Unified
Modeling
Language.
<http://www.rational.com/uml/index.j
tmpl>.
Zeigler, B. P. 1984. Multi-Facetted Modeling and Discrete
Event Simulation. New York, Academic Press.
Zeigler, B. P., G . Ball, H. S. Sarjoughian 1998. The
DEVS/HLA Distributed Simulation Environment And
Its Support for Predictive Filtering, ECE, The
University of Arizona.
Zeigler, B. P., S. B. Hall, et al. 1999. Exploiting HLA and
DEVS to promote interoperability and reuse in
lockheed's corporate environment. Simulation 73(5):
288-295.
Zeigler, B. P., H. Praehofer, et al. 2000. Theory of
Modeling and Simulation, 2nd Edition, Academic
Press.

AUTHOR BIOGRAPHIES
HESSAM S. SARJOUGHIAN is Assistant Research
Professor of Electrical and Computer Engineering at the
University of Arizona. His current research interests are in
theory, methodology, and practice of distributed
collaborative modeling & simulation. His other research
interests are in AI and Software Engineering. His email
and web addresses are <hessam@ece. arizona.
edu> and<www.acims.arizona.edu >.
BERNARD P. ZEIGLER is Professor of Electrical and
Computer Engineering at the University of Arizona,
Tucson. He has written several foundational books on
modeling and simulation theory and methodology. He is
currently leading a DARPA sponsored project on DEVS
framework for HLA and predictive contracts. He is a
Fellow of the IEEE. His email and web addresses are
<zeigler@ece.arizona.edu>
and
<www.
acims.arizona.edu >.

448

TECHNICAL ARTICLE

5, 288-2

Exploiting HLA and DEVS
To Promote Interoperability and Reuse in
Lockheed’s Corporate Environment
1
Bernard P.

Zeigler

AI & Simulation Research

The

Group
University of Arizona

Tucson, AZ 86721
E-mail: zeigler@ece.arizona.edu
URL: www.acims.arizona.edu

Steve B. Hall

Hessam S.

Lockheed Martin Missiles and Space
Sunnyvale, CA
E-mail: steve_hall@lmco.com

Joint MEASURE&trade; (Mission Effectiveness
Analysis Simulator for Utility, Research and
Evaluation) demonstrates the effective applica-

of HLA-compliant simulation outside the
traditional realm of training to analytic studies
of mission effectiveness of systems within larger
tion

systems of systems configurations. This paper
discusses how Joint MEASURE effectively ex-

ploits both the DEVS modeling and HLA simulation frameworks to support high performance
distributed simulation and thereby, to overcome
impediments to interoperability and reuse of
Lockheed’s models arising from the reluctance of
groups to share their code at the source level
with others within the corporation.

Keywords: Mission effectiveness simulation,
formalism, High Level Architecture,
interoperablility, site-proprietary impedi-

Sarjoughian

Simulation Research Group
The University of Arizona
Tucson, AZ 86721
E-mail: hessam@ece.arizona.edu
URL: www.acims.arizona.edu

AI &

1.

Introduction

Lockheed Martin, one of the few large surviving aerospace conglomerates, is an aggregate of perhaps a hundred smaller entities, many of which were originally
independent companies. This vestigial variety provides
a treasury of modeling and simulation assets that offers
a rich potential for reuse in new projects, especially
systems-of-systems studies that require models of earlier developed components. However, interfacing simulations originally developed for disparate purposes,
in variegated languages and platforms, represents a
major challenge-one, of course, that the HLA standard is intended to address. Nevertheless, the developers of HLA, intent on solving the interoperability
problem within the government, probably did not
foresee a second impediment to interoperability that
arises in an evolved corporate environment such as
Lockheed’s. This is the reluctance of subsidiaries to
share their code at the source level with others within

DEVS

ments, data distribution management

1

This work was supported by Advance Simulation Technology Thrust (ASTT) DARPA Contract N6133997K-0007.

distinguished by the enormous volume of
analysis it requires. Since the study is in the initial
stages, there are a large number of alternatives to
examine. (For example, how many space-based lasers, what orbits to use, what orbiting frequencies,
whether to use relay mirrors, etc. In addition, there
are combinations of space-based and ground-based
sensors which significantly multiply the alternatives that need to be considered. Finally, there may

the corporation-which we will refer to as the &dquo;siteproprietary&dquo; problem and elucidate further on. In this

that is

paper, we show how HLA and distributed simulation
enable interoperability and reuse while eliminating
the need to share source code. While critical to the success of Lockheed in marshalling its resources to meet
the challenges of the new era of DoD budget allocations-and as one of the defense contractor giants, to
the national interest, lessons learned in overcoming
site-proprietary constraints are applicable to the larger
commercial world where independent companies may
wish to safeguard their intellectual property while
interoperating their models without going the full distance of elaborate licensing arrangements.

be several alternative missions, besides the main
one of detecting ballistic missiles, such as space
control, ground-based surveillance, etc.) This large
number of architecture-level alternatives, when
multiplied by the number of experiments required
to investigate each to an acceptable level of confidence, yields an enormous number of simulation
runs. Such a workload demands that Joint MEASURE supplies the speed and high-performance
possible with parallel/distributed simulation.

Joint MEASURETM Applications and

2.

Distributed Simulation Issues

DEVS-compliant simulation system, Pleiades,
developed by Lockheed Martin, Sunnyvale to enable analytic simulation studies of mission effectiveness of systems within larger systems of systems configurations. Recently (early 1999), Pleiades was renamed
Joint MEASURE (Mission Effectiveness Analytic
Simulator Utility Research Environment) after having
been ported to execute on DEVS/HLA, an HLA-compliant distributed simulation environment supporting
high-level development of DEVS models in C++ and
Java. Joint MEASURE has been used to perform analysis on advanced surface ships, underwater vehicles
and various sensor systems-underwater, terrestrial,
airborne and space-based. The objectives in making
Joint MEASURE HLA-compliant were to improve runtime performance by exploiting the parallel/distributed simulation supported by HLA. Enhanced perforIn 1995

a

was

is needed so that executing Monte Carlo runs
of complex scenarios involving large numbers of entities could be accommodated. Further, the future could
be seen to require support of the phases of the system
development life cycle, as envisioned by the Simulation-Based Acquisition initiative. In particular, the migration of initially developed design models to realtime operation for system testing could be supported
by exploiting the time management services of HLA
in support of linking together federates operating on
physical and logical time. Moreover, as mentioned and
will be explained, site-proprietary issues limiting access to models owned by remote Lockheed sites could
be overcome by the interoperability and information
hiding afforded by HLA-distributed simulation. In
sum, the goal of reducing simulation development
costs could be furthered by HLA through its support
of reuse of corporate models through interoperable
distributed simulation.
We’ll place these objectives in context by describing
some current and planned applications.

.

jective is to analyze the utility of measurementbased fusion of sensor data obtained in real time
from a variety of sources, including satellite-borne
IR, reconnaissance aircraft, and Thadd. The questions address fundamental feasibility issues, such
as: can composing such multiple sources be done
fast enough and with sufficient accuracy and quality to be useful for a missile interception? Developing credible answers requires access to remote
Lockheed Martin models and authenticated data,
such as from the Aegis, SBIRS, and Thadd. The
HLA-compliant distributed simulation of Joint
MEASURE enables federating such models together into a usable composite.

mance

.

SBL (Space-Based Laser)-This is a study of alternative high-energy laser constellation architectures

JCTN (Joint Composite Tracking Network)-The ob-

We will

now

present a brief review of DEVS/HLA

provide
background for the discussion of the
role of HLA and Joint MEASURE in overcoming barriers to model reuse and interoperability in Lockheed’s
corporate environment.
to

3.

some

Review of DEVS/HLA

an HLA-compliant modeling and
simulation environment formed by mapping the
DEVS-C++ system [2] to the C++ version of the DMSO
RTI. DEVS is a mathematical formalism for expressing
discrete event models which supports discrete event
approximation of continuous systems and has an object-oriented substrate supporting model implementation and repository reuse. Advantages of the DEVS
methodology for model development include welldefined separation of concerns supporting distinct
modeling and simulation layers that can be independently verified and reused in later combinations with
minimal re-verification. The resulting divide-and-conquer approach can greatly simplify and accelerate
model development, leading to greater credibility at
reduced effort. DEVS has a well-defined concept of

DEVS/HLA [1] is

289

development and simulation is shown in Figure 1.
Models developed in a DEVS/C++ or DEVSJAVA
(C++ and Java-based DEVS implementations, respectively) can be directly simulated in the DEVS/HLA
environment over any TCP/IP, ATM, or other network of hosts executing an HLA C++ RTI. Based on
model-supplied information, DEVS/HLA takes care
of the declarations and initializations needed to create
federations, joining and resigning of federates, communication among federates and time management.

Figure 1. DEVS/HLA layered architecture and
its

system modularity and component coupling to form
composite models. It enjoys the property of closure
under coupling which justifies treating coupled models as components and enables hierarchical model

composition constructs.
Such formal properties of the DEVS methodology
enable DEVS/HLA to support high-level federation
development and execution. Modularity, in which
component models are coupled together through input/
output ports, allows messages to be sent from one
federate to another
tion messages. In

using the underlying HLA interac-

addition, DEVS/HLA supports

at-

updating and quantization-based message filtering [3]. For this, the modeler declares HLA objects
and attributes desired for reflecting states of DEVS
federates. By attaching quantizer objects supplied by
DEVS/HLA to such attributes, the appropriate publish/subscribe mechanisms are automatically set up.
The resulting environment for high-level federation
tribute

Joint MEASURE
Joint MEASURE was developed atop DEVS/HLA by
Lockheed Martin, Sunnyvale. Simulation-based acqui3.1

support of supersets such as Joint MEASURE

sition requires that early in the development of a new
defense system there be an assessment of its expected
mission effectiveness. The benefit aspect of this cost/
benefit analysis examines how well the proposed system would contribute to the nation’s overall ability to
counter an enemy threat. This contribution is a result
of the system’s ability to gather and share information,
survive, and/or service hostile targets. Unfortunately,
making this assessment can be difficult. Subtle design
decisions can result in significant impacts on effectiveness. Similarly, modestalterations inthe way the
system is used can have a marked impact on effectiveness, as can relatively minor modifications of the scenario that the system is immersed within and is react-

ing to.
Joint MEASURE (JM) is intended to assist in evalu-

ating a proposed defense system’s mission effectivein the context of a specified set of possible design

ness

Figure 2. Joint MEASURE screen shot with GIS-based map displayed along with dynamic data output
290

could include pseudo-random number seeds). Moreover, the architecture, when mapped into DEVS/
HLA, can improve performance without risk of

variations, operational utilization patterns and engagement scenarios.

Examples of such defense system proposals will be given later in the paper. Joint MEASURE
contains a library of sensor, weapon, and command/
control communication platform models that can be
composed to model military systems of systems operating in physically realistic environments. It is intended
to support Monte Carlo and/or optimization simulation runs involving large numbers of runs with different random number seeds and parameter settings. JM
contains Event Distribution and Predictive Encounter
controllers to efficiently manage interactions among
mobile, communicating platforms. As shown in Figure 2, JM contains a visualization interface to display
moving platforms that are spatially referenced within
a Geographical Information System (GIS). It also contains a Data Modeling and Analysis Toolkit to support
statistical analysis of simulation output. In JM, experiments can be specified interactively, to include attributes and behaviors of system components. Such scenarios can be stored, retrieved and concatenated to
form new composites. JM has been in development
for five years and includes approximately 200K lines
of internally developed C++ code in addition to its
substantial utilization of available libraries.
3.2 DEVS

Compliance of Joint MEASURE

Lockheed Martin made an early decision to comply
with the DEVS formalism due to its formal precision
and event-based simulation efficiency. This suggested
its capability to support speed, repeatability, code
maintainability/reusability, and high-level portability.
We discuss these requirements individually:
~

~

Speed: It is easy to generate a requirement to simulate a multi-hour analysis scenario thousands of
times. For example, if we assess three alternatives
to each of three platforms’ design parameters (e.g.,
speed, signature and vulnerability) and three alternatives to three utilization patterns (e.g., distance
from enemy, friendly deployment configuration
and friendly reactivity) and three alternatives to
three scenario characteristics (geographical location,
enemy deployment configuration and enemy reactivity) and repeat each of these configurations 20
times (due to the probabilistic nature of the simulation), then we will need to run nearly 400,000 simulations of this multi-hour scenario. If we can run
our simulator 1,000 times faster than real time, it
will still take a month to finish the analysis. The
developed architecture makes it possible to step
trough time when necessary and selectively jump
over intervals when useful on a selective (platformby-platform) basis.
Repeatability: Since it is based on DEVS, a logical
model formalism with a well-defined state concept,
Joint MEASURE has the ability to exactly replicate
simulation runs given the same initial state (which

losing repeatability or inducing discrepancies since
the temporal ordering of events is guaranteed by
the Parallel DEVS protocol used in DEVS/HLA
when running over a distributed network.
Maintainability/Reusability: The modular aspect
of DEVS models enforces a software engineering
practice that simplifies code reuse and simplifies
the construction of new federations from existing

~

components.
The first DEVS simulaDevSim++ [4], as a
single-processor implementation of the DEVS formalism. The high level of abstraction provided by
the DEVS formalism subsequently facilitated porting the system to DEVS/C++ and later to DEVS/

Portability at a High Level:

~

tion environment used

was

HLA.

3.3

Component Models

Based on DEVS concepts, Joint MEASURE defines
four basic kinds of coupled2 models: Generators and
Transducers, (forming the Experimental Frame Module) and Platforms and Space Model. This architecture
is designed to exploit the fact that often nothing interesting is happening during some interval of a simulated scenario. By exploiting this fact we can produce
significant performance gains beyond those achieved
by the standard use of a discrete event simulator. Each
of these classes of models is described briefly below.
The Space Model is then discussed in somewhat more
detail.
Generator

The Generators (one of the components of the Experimental Frame) are relatively simple models in
Joint MEASURE. Only one Generator can be instantiated in a given simulation. The Generator is responsible for instantiating platform models and interfacing
with models outside the scope of Joint MEASURE.
Transducer
The Transducer model

(the other component of the
is
Frame)
Experimental
responsible for gathering data,

performing simple analyses, generating dynamically
updating data displays, and creating and updating
analysis databases for later detailed statistical and
mathematical examination.
The runtime data display sets are graphically updated in 3D at the end of every N simulation runs.
This capability has proven to be especially useful for
early detection of scenarios that have been specified
incorrectly or have not been specified so as to produce
&dquo;interesting&dquo; results, e.g., when none of the Platforms
2

A coupled model in DEVS is
other models.

a

model that

encapsulates

291

detects a Platform with an opposing affiliation.
The databases are generated in a generic format
that is readable by multiple tools such as PROPHET, a
statistical visualization module developed for life science research which has useful tools for examining
characteristics such as &dquo;survivability&dquo; on top a robust
set of standard statistical and modeling tools.
ever

Platforms
Platforms represent the systems that are to be
evaluated. Platforms have a location in the world;
they have extent; they can be collided with; and they
are potentially detectable. Platforms may also actively
or passively sense the world around them; move their
location; generate emissions of various sorts; launch

sub-platforms (e.g., missiles); participate in physical
transfer operations; and fail for a variety of reasons.
Multiple Platforms can be instantiated in a given simulation. The standard Platform model is a coupled
model containing three singly instantiated (coupled)
models: the Hull model, the Global Status model and
the Subsystems model.
The Hull coupled model models the structure of the
platform. It knows about the relative locations of the
subsystems onboard the platform, and routes the transfer of maLeiial and data to them and between them
with the appropriate temporal delays. It also models
the impact of collisions and detonations on the viability of the platform itself and on the conduits that connect the various subsystems.
The Global Status coupled model generates and
maintains a model of the aggregated appearance (response to other platforms’ emissions, e.g., IR signature) and capability of the platform for the consumption of the world beyond the platform. That is, what
does this platform look like and sound like at the current time, and what are its current capabilities (e.g.,
could it detect a radar burst from another platform)?
The Space Model, discussed below, is the primary
consumer of this data.
The Subsystem coupled model models the various
subsystems on board the Platform. The standard
Subsystem model is composed of the following six
(coupled) models: a Sensor-Transceivers model; a
Weapon-Launchers model; a C3I model; a Stores model;
a Mechanical model; and the Subsystem Router model.
The Sensor-Transceivers component includes all
the sensors and active emitters on board the platform.
The Weapon-Launchers component includes all the
weapon systems on board the platform. The C3I model
models the decision processes on board the Platform.
The Stores model models the renewable and consumable resources of the platform. The Mechanical model
models the propulsion, maneuvering and effector capabilities of the Platform. Finally the Subsystem Router
model is a software device to facilitate the interchangeability of various Subsystems models.

292

Figure 3. Joint MEASURE Architecture

Space Modeling Infrastructure
JM includes an intelligent model of the &dquo;space&dquo; enclosing the platforms. This Space Model performs two
important functions. First, it infers when interesting
encounters will occur among any number of moving
platforms in a pair-wise manner and provides such
spatial encounter prediction services to the Platforms,
resulting in significant simulation performance improvement. Second, it determines the propagation
losses and delays of various emissions and how much
noise the environment will add to a signal. It then
propagates the emissions of a Platform to other &dquo;withinrange&dquo; Platforms capable of receiving them. These two
functions are localized in components of the Space
Model as illustrated in Figure 3. Additionally, to maintain knowledge of Platform whereabouts, the Space
Model includes a Logger component.
Spatial Encounter Prediction
The Space Model predicts interesting spatial relationship events and informs the various Platforms so
that they are not unnecessarily attempting to interact.
In order to make these predictions, the &dquo;trajectory&dquo; of
a platform must be known by the Space Model. The
Spatial Encounter Prediction (SEP) component currently knows about the following kinds of trajectories:
Rhumb line (constant heading) with constant elevation
changes; Great Circle (shortest distance) with constant
or ballistic elevation changes and orbital trajectories
(without atmospheric drag). The Platform is responsible for updating the Logger anytime the Platform’s
trajectory parameters change.
The SEP can produce a number of spatial encounter
predictions with what it knows about any two Platform trajectories. These predictions currently include:
the time at which they will be at their closest point of
approach (potentially a collision) and the first and last
time they will be: &dquo;over the horizon;&dquo; within a given

distance; within a given azimuth angle; or within a

given elevation angle. It can also make optimistic estimates of the probability of detection and inform a
sensor when that probability is exceeded. The Space
Model also predicts at what point in time, if at all, a
Platform will &dquo;run aground.&dquo; Platforms subscribe to
encounters of interest and

are

informed of them at the

appropriate point in time.
When Platforms exploit this information effectively,
significant simulation efficiencies are achieved as a
consequence of jumping over dead time intervals
(when no Platforms are interacting) and by limiting
which Platforms are time stepping to those that are
potentially interacting.

(integrated with a GIS system). Emissions
received by the Space Model are passed on (when
transmission is not blocked by terrain3), at the appropriate point in time (with the signal strength diminished and embedded in noise4), to all the &dquo;eligible&dquo;
Platforms. Eligible Platforms are those that have functioning receivers (of the right sort) that conservatively
might detect the signal.
ronment

4.

Overcoming Site-Proprietary Impediments
Interoperability with Joint MEASURE
As already mentioned, Lockheed Martin must be able
to

to marshal the models and simulations of its onceindependent groups to synthesize composites that

can

emerging system-of-system development issues such as outlined earlier. Although now part of a
larger corporation, such groups are often reluctant to
share their source code with peers.5 In the absence of
a distributed simulation solution, this &dquo;site-proprietary&dquo;
disposition can severely impede the integration of
codes from different areas of the corporation. In the following we examine a case study to illustrate this point.
answer

Emission Propagation
The Emissions Propagator component of the Space
Model propagates emissions with their appropriate
propagation losses, delays and associated background
noises. Platforms are limited to two forms of interaction : physical transfers (e.g., supplies or fuel) and
emissions. Emissions cover pressure waves from explosions, acoustic noise (self-generated and reflected),
and electromagnetic emissions (self-generated and
reflected). The model allows emissions to carry semantic content as appropriate, for example, in radio communications.

Propagation loss (i.e., how much of the source signal strength remains when the signal arrives at the
destination’s location) is a function of both the distance
traveled and the nature of the environment. Both factors (distance and environment) similarly influence
propagation delay (i.e., how long it takes for the signal to transit from the source to the destination). And
finally noise (in which the signal is embedded) is a
function not only of the environment, but also of the
other Platforms in the region. These delays (especially
in the acoustic realm) and losses can be significant.
Furthermore, the environmental influences themselves
can be dynamic. For example, wind, rain and fire can
significantly change the nature of the medium that is
being transited through. For these reasons the Space
Model includes as a component, a model of the envi3

4

We do line-of-sight obstruction checking, which includes
both the curvature of the Earth and the influence of the
terrain. The terrain check uses either 30-second data (from
the GLOBE (Global Land One-km Base Elevation) database
from the National Geophysical Data Center or the ETOPO5
five-minute data. If the 30-second data is missing for a
given region, we automatically use the five-minute data.
We assume a spherical Earth, but could approximate a
spheroid without much additional work.
On water, radar noise is calculated as a function of Sea
State. On land, it is calculated as a function of vegetation
type, urbanization and land hilliness. Wind effects are a
planned enhancement. Laser energy propagation is diminished not only by the distance traveled, but by the atmosphere absorption (via the FASCODE model). Thermal

4.1 Lockheed’s

DD21/Deepwater Project

The major Navy and Coast Guard ship design programs, DD21 and Deepwater, respectively, require
extensive analysis of system-by-system options.
Originally, the Medusa simulator was developed by
the GES company to study the Aegis warship’s ability
to detect and intercept missile threats. However, the
model considered only a single missile attack on a
motionless ship. Subsequently, the need arose to embed the ship’s sensor in a more realistic environment
of multiple missiles, ship’s motion, and air and sea
emission propagation effects. The shortest route to
achieving this embedding was to couple Medusa with
Pleiades, the non-HLA compliant predecessor to Joint
MEASURE. However, reluctance to share source code
(even though GES was now incorporated within Lockheed Martin) made this coupling difficult to achieve
in a completely satisfactory manner. One approach
that was taken was to replace Medusa with a table
lookup to yield the probability of missile kill under

blooming will be added to the equation in the near future.
Currently, we are in the process of integrating acoustic
propagation models as well as the SSGM (Synthetic Scene
Generation Model) database to
and noise models.
5

support infrared signature

Reluctance to share model source code may derive from
several reasons. The customer, whose system is modeled,
may rightly be concerned about the potential for indiscriminate dissemination of the code within the corporation,
unauthorized use, and claims made based upon model
application outside its domain of validity. The model developer may be rightly concerned about the loss of ownership control of the source in the event that the group finds
itself sold into another corporation.

293

Figure 4. DEVS /HLA supporting integration
of heterogeneous distributed components
the

given situation arising in the combined model. To

generate a table attempting to cover all situations that
might arise in the coupled model, a thousand cases

employed (missiles starting from some 10 disship, approaching at some 10 azimuth
angles, and 10 loading conditions (numbers of other
were

tances from the

missiles)). Each case was executed 100 times in
Medusa to obtam an acceptable level ot confidence in
the probability estimate. This effort foundered due to
the time-consuming nature of the table generation
and the fact that at this resolution level, it could not
accurately represent the actual result of interaction
between Medusa and Pleiades.
HLA and distributed simulation provides a means
to bypass such site-proprietary problems. To develop
the appropriate FOM and SOM interfaces for the
model components requires the collaboration of their
developers but can be done without access to source
code. Models can be compiled locally with appropriately programmed HLA interfaces so that model source
and object code can remain at the owners’ sites. Federations can’t be established and executed without the
consent of each federate developer and the authorization of the customer. So models remain fully under
the control of their developers on the one hand and
customers on the other.
Developing and executing federations, however, is
not necessarily a routine process. Indeed, a methodology, expressed in the FEDEP (Federation Development
and Execution Process) is being developed to facilitate
it. To interoperate properly, developers must understand both the data exchange requirements and the
dynamics of the components to be federated. The DEVS
formalism provides a means to express dynamics in a
standardized manner. In particular, the DEVS Bus concept provides a basis for wrapping non-DEVS models
within DEVS components, enabling DEVS/HLA to
support integrating legacy or other simulation software. In application to DD21 and Deepwater development, Joint MEASURE has been coupled to an OPNET
active

294

model of an inter-satellite communication network.
To complete the federation, these federates will be
coupled to a warship model that extends the Medusa
legacy simulation code.6 Figure 5 illustrates our approach to such integration of heterogeneous model
components-where each is represented by a DEVS
federate in DEVS/HLA, that is, each is either a DEVS
model or wrapped within a basic DEVS interface.
In such federations, owners of components can retain complete control of their models since they remain
at their own sites. Further, provided that the appropriately specified SOM, FOM and DEVS interfaces enable proper coupling of components, the latter can be
inter-operated at the object code level without requiring source code integration. This obviates the thorny
issue of site-proprietary concerns of participating
Lockheed groups.
An issue to be addressed in developing distributed
simulations with federate components located in various parts of the country or the world is how to reduce
significantly enormous numbers of updates and messages to a manageable quantity given limited network
bandwidth. Predictive contract and other message filtering methods are intended to overcome the bandwidth and propagation latencies introduced by the
connecting wide-area network, e.g., the Internet [3].

5.

Conclusions

Joint MEASURE exploits the advantages of DEVS and
HLA to support the high performance and accuracy
demands of analytic mission effectiveness studies for
complex systems. Its distributed simulation capability
provides an end run around site-proprietary issues of
concern to participating groups in an evolved corporate environment such as Lockheed’s. JM is being extended to address migration of simulation models to
real-time operation and other life cycle support functions needed for Simulation-Based Design. One key to
meeting speed performance requirements is the computation efficiency afforded by spatial encounter prediction and its ability to reduce communication events
among mobile, interacting platforms without impacting simulation fidelity. Also required for overcoming
the network bottleneck are the predictive contract and
other message reduction mechanisms afforded by the
HLA and DEVS/HLA layers within JM. Unfortunately,
because of the possibility of unpredictably jumping
over intervals of time, hardware and man-in-the-loop
simulation are not currently possible with Joint MEASURE. However, the emergence of commercial realtime RTIs together with the ability to execute models
6

The current status of these projects is as follows: the
Deepwater work is real and operational. It deals with
scenarios that have thousands of objects. The simulation
supports the Coast Guard in evaluating alternative
strategies for monitoring, coordinating and interdicting
suspicious sea traffic. The DD21 federate is currently
under development.

[5, 6] could provide a development path
for real-time execution of DEVS models. Eventually,
this would enable Joint MEASURE to support all life
cycle phases, especially design and test, of SimulationBased Acquisition. For additional information, please
see the Website at www.acims.arizona.edu.

Bernard P. Zeigler is Professor of
Electrical and Computer Engineering
at the University of Arizona, Tucson.
He has written several foundational
books on modeling and simulation
theory and methodology. He is cur-

6. References
[1] Zeigler, B.P., et al. "Implementation of the DEVS Formalism

low of the IEEE.

in real time

the HLA/RTI: Problems and Solutions." Fall
lando, FL, 1999.
over

rently leading a DARPA-sponsored
project on DEVS framework for HLA
and predictive contracts. He is a Fel-

SIW, Or-

[2] Zeigler, B.P., et al.
mance

"The DEVS Environment for High-PerforModeling and Simulation." IEEE C S & E, Vol. 4,

No. 3, pp 61-71, 1997.
[3] Zeigler, B.P., et al. "Bandwidth Utilization/Fidelity Tradeoffs
in Predictive Filtering." Fall SIW, Orlando, FL, 1999.
[4] Hong, G.P. and Kim, T.G. "A Framework for Verifying Dis-

Steven B. Hall is a Senior Technical
Manager at Lockheed Martin Missiles
and Space Corporation, Sunnyvale,
California.

crete Event Models

[5]
[6]

Using a Dual Specification Approach."
TRANSACTIONS of the Society for Computer Simulation International, Vol. 13, No. 4, pp 19-34, 1996.
Zeigler, B.P., et al. "Distributed/Real Time Simulation: Integrating Legacy and Modern Codes." Int’l Conf. on Env. Sys.,
Soc. Aut. Engr, Danvers, MA, 1998.
Hong, J.S. and Kim, T.G. "Real-time Discrete Event System
Specification Formalism for Seamless Real-time Software
Development." Discrete Event Dynamic Systems: Theory and
Applications, Vol. 7, No. 4, pp 355-375, 1997.

Hessam S. Sarjoughian is Assistant
Research Professor of Electrical and
Computer Engineering at the University of Arizona. His current research
interests are in theory, methodology,
and practice of distributed/collaborative modeling and simulation.

295

A Novel Visual CA Modeling Approach and its Realization in CoSMoS
Sajjan Sarkar

Gary R. Mayer

PeopleNet Inc.

Department of Computer Science

7000 Central Parkway Ave NE Suite 650
Atlanta,GA-30328
sajjan.sarkar@peoplenet.com

Southern Illinois University
Edwardsville, IL 62026-1656
gamayer@siue.edu

ap
er

Hessam S. Sarjoughian
School of Computing, Informatics and
Decision Systems Engineering
Arizona State University
Tempe, AZ, 85281-8809
sarjoughian@asu.edu

Keywords: Cellular Automata, Component-based modeling,
CoSMoS, DEVS, visual modeling

2.

BACKGROUND
Visual modeling tools can significantly reduce
development effort including design specifications,
simulation model verification and validation, and
documentation. In particular, the ability to synthesize
hierarchical system models from alternative sub-system
models without resorting to textual representations or direct
use of computer program languages is highly desirable.
The concept of cellular automata introduced by Von
Neumann [1] affords a simple, yet powerful approach for
modeling a variety of complex systems [12]. These models
may be specified in terms of general-purpose or specialized
programming languages and modeling formalisms (e.g., see
[4,7,10,12,13]). An attractive approach for describing CAs
is to consider each cell as a component [13]. The cells, a
topological network pattern, and rules – as basic elements –
form a spatial network of components. The components
define the structure and behavior of a CA – i.e., the basic
model elements together support constructing families of
hierarchical models. For example, NetLogo is an easy to use
tool that supports visual modeling of agents and their
environment specified as cellular automata. This tool is
advantageous in its accessibility to users with limited
programming skills. However, its implementation is
inflexible given the way in which the agents and CA are
closely tied and the inability to reuse CA constructs. Our
goal in this research is to show that visual (or graphical)
modeling of cellular automata has its own characteristics
and thus tools such as CoSMoS [2,8] with their underlying
concepts are best suited for non-cellular component-based
models.

INTRODUCTION
Cellular Automata (CA) is defined as a collection of
homogeneous cells with some specific connectivity pattern.
One or more cells may change their dynamics according to a
set of state-based rules. Every cell is identical to every other
cell in terms of its spatial dimension, shape, attributes and
behavior. A variety of problems and solutions such as
landscape erosion dynamics are commonly studied using
Cellular Automata (CA) models [4,12].
Users with different degrees of expertise use
component-based modeling and simulation tools across
numerous application domains. A tool such as
Matlab®/Simulink® [6] with its underling modeling
framework has been shown to be highly effective in
simplifying model development and use. However, existing
approaches and tools that can lend themselves to CA model
development are restrictive. Tools such as NetLogo [11] and
Mathematica [5] use the concept of generic pre-built models
that can be specialized and synthesized to specify userdefined models, often within special application domains.
DEVS-based approaches and tools such as Cellular DEVS

Po
s

1.

te

rP

Abstract
Many important scientific and engineering problems are
studied using Cellular Automata (CA) models.
Mathematical formulae with component-based modeling
concepts are used to specify CA models. Computer
simulation tools support viewing the dynamics of CA
simulation models. Existing approaches with their tools are
restrictive from visual model development perspective since
models must be specified in programming languages or use
pre-built models. In this paper, the novel concept of spatialspecialization CA modeling is developed and introduced
into CoSMoS (Component-based System Modeler and
Simulator) – a logical, visual, and persistence modeling and
simulation framework. Modelers can visually create and
manipulate structures of a family of hierarchical componentbased CA models. The tool supports persistent models and
semi-automatic parallel DEVS-based simulation code.

[10,13] use ordinary component-based concepts and
methods for visual CA model development.
In this paper, we propose a novel approach aimed at
visual development of CA models. The approach uses a new
relationship between a CA and its cells called spatialspecialization. A realization of the concept is implemented
by extending the Component-based System Modeler and
Simulator (CoSMoS) framework [2,8]. The tool supports
storage of a family of models in relational databases and
automated generation of parallel DEVS CA [13].

© 2010 SCS. All rights reserved. Reprinted here with permission.

Page 1

concept of components-within-components visual model
development for CA. They do not offer useful concepts that
are directed for visual cellular automata model
development. Instead, the concepts and techniques
developed for non-CA component are used. As a result,
visual creation of cellular models is restricted to rendering
models that are already implemented or use non-CA
component-based modeling concepts (e.g., little support for
visually modeling a hierarchical component-based CA
model much less a family of CA models).
The concept of spatial-specialization for cellular
automata specification is proposed and introduced into the
CoSMoS framework. The concept enables the creation and
manipulation of logical cellular automata models with
support for database persistence and automatic creation of
partial simulation models. CA models with different
connectivity patterns can be created, deleted, and modified.
Cells and CAs are the primitive and composite model
components similar to non-cellular hierarchical componentbased models. Data persistence provides for both reuse and
incremental building of the hierarchical CA structures.
The spatial-specialization concept is based on every CA
using a cell that is treated as the base model component – it
is the fundamental, atomic unit of the cellular automaton.
Every cell has states, behavior, shape, dimension, and
inducers. Every state has a name with an associated data
type. The state value can be of any primitive or complex
data type. The behavior of a cell can be defined by a
specification such as the parallel DEVS atomic – e.g., next
state value(s) are computed based on previous state value(s),
the rules, and input from neighboring cells. We use the term
nearest neighbor to describe cells that are immediately
connected to one another via input and output ports. This is
to differentiate between these cells and those that have ith
neighbor relationship [4] – i.e., interacting via one or more
intermediary cells. Every cell and therefore CA must have a
finite number of dimensions. Most common dimensions are
2D and 3D. The inducers of a cell (called influencees) are
its neighbors (called influencers). The interactions among
cells (i.e., sending and receiving messages via output and
input ports) are specified in the CA model. Thus, every cell
only has only knowledge of itself – i.e., a cell does not
know of the cells to which it is connected. An input port of
an influencee is used to receive data from an influencer,
enabling the influencee to potentially change its state based
on the input and rules. An output port of an influencer is
used to inform the cell’s that it influences about the
influencer’s state change.

ap
er

2.1. Cellular DEVS
In Cellular DEVS, the cells of a CA are defined as basic
DEVS atomic models that are coupled to form multidimensional component models [10,13]. Thus, the topology
and rules that account for individual cells and their
combination are defined within atomic and coupled DEVS
model components. A CA model is a kind of coupled DEVS
model with a special connectivity pattern (e.g., Von
Neumann and Moore neighborhood) among its cells. Cells
are atomic components with structured inputs/outputs, timebased state transitions, and I/O connectivity pattern. The
difference between one cell and another is its state at a given
time instance and its location with respect to the CA’s
topology.

Po
s

te

rP

2.2. CoSMoS
CoSMoS (Component-based System Modeling and
Simulation) is a framework offering a unified logical,
visual, persistent component-based simulation model
specification [8]. Its underlying premise is to support both
simulatable (e.g., DEVS) and non-simulatable (XML
Schema) models. Logical model specifications are defined
for primitive and composite DEVS and XML schema
models. A set of axioms is provided to ensure consistency
of a family of alternative hierarchical model specifications.
Three different levels of abstraction called Template
Models (TM), Instance Template Models (ITM) and
Instance Models (IM) are supported in CoSMoS [2,8]. A
Template Model specifies primitive and composite models
with hierarchy depth of two. Every primitive and composite
component can be specialized and can have a finite number
of input and output ports. Primitive or complex data types
for input, output, and state variables can be specified. An
Instance Template Model is defined to have a finite
hierarchy of depth greater than two. An Instance Model is
an instantiation of an Instance Template Model where the
multiplicity of model instances within the hierarchy is
specified.
Logical models are completely specified visually and
stored in one or more relational databases. The models in
the databases can be transformed to target simulation
models for simulation environments such as DEVS-Suite
[3] which is integrated into CoSMoS. Structural and
behavioral metrics can be queried for any model component.
The model transformation provides basic structural
implementation and behavioral details must be provided
before the models can be simulated.
3.

APPROACH
The approaches available for specifying cellular
automata models use common techniques to render CA
models visually as interconnected components with either
explicit or implicit connections (e.g., Cellular DEVS and
NetLogo). These approaches and tools lack the basic

3.1. CA Model Specification in CoSMoS
For 2D CA, we define the Instance Template Model
(ITM) and the Instance Model (IM). The Template Model
that is defined for non-cellular component-based CoSMoS
models is not used. This is due to the homogeneity of

Page 2

The ITM can be used to define its associated IM.
Instance models of cells and CAs are based on their distinct
attributes, dimensions, and frames of reference. Primitive
cell can only be instantiated as elements of one or more
CAs. The IM is defined to have only composition
relationship among its cells. Thus, given an instance
template model, its instance models define the specialized
cell values and the frames of reference for specialized cells
and CAs. The network pattern (von Neumann or Moore) is
defined in the ITM. Any cell that is not specialized also
retains default attribute values and frame of reference from
those defined in the ITM. Every change in the primitive cell
cascades through all specialized cells and the CAs that use
it. Details for the design and implementation in CoSMo with
an example can be found in [9].

ap
er

cellular automata models and lack of need to specify
multiplicity for cells. The ITM specification includes
hierarchy depth, frame of reference, spatial dimension, and a
network connectivity pattern. The elements of the CA are
the base (primitive) cell and any collection of cells that have
the nearest neighbor relationship that are part of one or more
CAs. Each cell is defined to have a name, scale (also
referred to as dimension), and a frame of reference. Scale
for (x, y) is defined to be 1×1 and the frame of reference is
defined as (x c , y c , orientation) where x c and y c are x and y
coordinates with finite lengths and each of the x and y has
an enumerated orientation of either North, South, West, or
East. The orientation can be eliminated given a fixed (x, y)
orientation. The ITM for cellular automata is specified as
follows:

Properties

Cell

Specializee
CA

Specialized
CA

CA

Attributes

Y

Y

Y

Y

(x, y)
Dimensions

1×1

> 1×1

> 1×1

>
1×1

Coordinates

Y

N

Y

Y

rP

• Uses a single base cell with (x, y) scale equal to 1×1.
• All cells are connected to one another using one type of
I/O connectivity scheme (Moore or Von Neumann).
• CA is homogeneous, has at least two cells witha set of
rules.
• Every cell and CA can have specializee/specialization
relationship (see Table 1).
• Every cell and CA has its own frame of reference
subject to spatial-specialization relationship.

te

Tables 2 and 3 define composition and spatialspecialization relationships for the ITM. In all three tables,
the usage of ‘Y’ indicates that, Yes, a relationship or
property is valid; while ‘N’ indicates that it is not. The
composition refers to a CA containing cells or other CAs
subject to the ITM specification defined above. The spatialspecialization relationship is based on the specializee and
specialized non-CA component models – specializee model
refers to a model that can be specialized [8]. The spatialspecialization relationship is defined as:

Po
s

• The specializee cell attribute values are not initialized
and the specialized cell attribute values are initialized.
• Two specializee CA differ in their assigned attributes’
values and/or scales.
• Each specialized cell or CA, unlike its specializee
counterpart, has a frame of reference.
• There exists only one specializee cell from which all
specialized cells can be defined. The specialize cell can
be specialized once or as many times as desired subject
to the maximum number of its appearances in an
instance model. Every specialized cell’s attribute values
are unique with respect to all other specialized cells.
• The x and y scales of any specialized CA is greater than
1×1and less than or equal to the x and y scales of the
CA of which it is a part.

Table 1: Elements of CA Models

Composition
Relationship
(Column to Row)

Cell

Specializee
CA

Specialized
CA

CA

Cell

N

Y

Y

Y

Specializee CA

N

N

N

N

Specialized CA

N

N

N

Y

CA

N

N

N

Y

Table 2: CA Composition Relationships: Cell, Specializee
CA, and Specialized CA Elements
SpatialSpecialization
Relationship
(Column to Row)

Cell

Specializee
CA

Specialized
CA

CA

Cell

N

Y

Y

Y

Specializee CA

N

N

N

N

Specialized CA

N

Y

N

N

CA

N

N

N

N

Table 3: CA Spatial-Specialization Relationships: Cell,
Specializee CA, and Specialized CA Elements

Page 3

Menu

x

(0,0)

5.

CONLUSIONS
The main idea of this research has been to develop the
spatial-specialization in order to significantly simplify
component-based CA modeling. Toward this goal, the
CoSMoS framework supports specifying structures of
arbitrary 2D cellular automata models. The implementation
does not support modeling cell behavior, but it can be
extended to support visual specification of rules and
increasing support for code generation. Other desirable
capabilities include support for multi-dimensional CAs [4]
and composable CAs [7]. Further development is also
needed for supporting viewing of CA dynamics (i.e.,
simulation results) in CoSMoS.
Acknowledgement
This research is partially by NSF Grant #BCS-0140269.

Cell Operations

References

y

Source Code
Pane

Model Pane

te

(a)

(b)

rP

Model Tree

(c)

Po
s

Figure 1. UI for developing hierarchical CA models
4.

Modeler [10], hierarchical models can be developed using
textual programming code or non-CA visual model
components. Mathematica7 offers a document-centered UI,
but with no support for visual component models. None of
these tools support automation for developing and managing
a group of alternative models. Also models can be stored as
flat files, thus weakening model development lifecycle.

ap
er

3.2. User Interface
The special characteristics of cellular automata require
visualization that is distinct from those provided for noncellular component models (see Figure 1). The UI design is
shown in Figure 1(a). It extends the non-cellular
component-based visual modeling in CoSMoS. Figures 1(b)
and 1(c) are example ITM and IM for a landscape model 9].
The SLOPE, REDSOILEROSION, and COPSE elements
shown in Figure 1(b) can be specified using Model Tree and
Cell Operations panes. Any number of instance models such
as SLOPE_1 and SLOPE_2 can be generated by iteratively
allowing the user to assign x and y coordinates as well as
initial values to the specializee elements as shown in Figure
1(c) (for example, the x and y coordinates (7×6) for COPES
is notated as @(7,6)). The Model and Source Code panes
allow viewing 2D CAs and partial generated source code.
Utility operations such as launching source code generator
are supported using the Menu pane.

RELATED WORK
Many tools have been developed to aid with modeling
and simulation of CAs. Due to the focus of the paper and
lack of space, we consider main approaches that support
visual modeling, not those that support visualization of
simulation results. A comparison of CoSMoS with
NetLogo, Cellular DEVS, and Mathematica7 is briefly
described. The comparison is focused on visual model
development for a family of CA models with model
persistence and simulation code generation. None of the
above offers the concept of spatial-specialization and
therefore the kind of visual model development outlined in
Section3. NetLogo allows model development from prebuilt templates with no support for hierarchy. Also cells are
not first class objects with I/O ports. In DEVS-Suite and CD

[1] Burks, A.W., 1970, Essays on Cellular Automata, Editor,
University of Illinois Press, Urbana, Illinois, USA.
[2] CoSMoS, 2009, http://cosmosim.sourceforge.net.
[3] DEVS-Suite, 2009, http://devs-suitesim.sourceforge.net.
[4] Ilachinski, A., 2001, Cellular Automata: A Discrete Universe.
World Scientific, New Jersey.
[5] Mathematica, 2009, http://www.wolfram.com/products.
[6] Mathworks, 2007, http://www.mathworks.com.
[7] Mayer, G. R., H. S. Sarjoughian, 2009, “Composable Cellular
Automata,” Simulation Transactions, Vol. 85, No. 11-12,
735-749.
[8] Sarjoughian, H. S. and V. Elamvazhuthi, 2009, “CoSMoS: A
Visual Environment for Component-based Modeling,
Experimental Design, and Simulation,” International
Conference on Simulation Tools & Techniques, Rome, Italy.
[9] Sarkar, S., 2009, “An Approach to Visual Modeling of
Cellular Automata,” Master's thesis, Computer Science and
Engineering Department, Arizona State University, August,
Tempe, AZ, USA.
[10] Wainer, G., 2009, Discrete-Event Modeling and Simulation:
A Practitioner’s Approach, CRC Press.
[11] Tisue, S., and U. Wilensky, 2004, “NetLogo: Design and
implementation of a multi-agent modeling environment,”
Agent Conference on Social Dynamics: Interaction,
Reflexivity and Emergence.
[12] Wolfram, S., 2002, A New Kind of Science. Wolfram Media
Incorporation.
[13] Zeigler, B. P., H. Praehofer, and T. G. Kim, 2000, Theory of
Modeling and Simulation: Integrating Discrete Event and
Continuous Complex Dynamic Systems, 2nd Edition,
Academic Press.

Page 4

A Group-Based Approach For Distributed Model Construction
James Lee a, Hessam Sarjoughianb, Frank Simcoxc, Sankait Vahie b, Bernard Zeigler b
a

Center for the Management of Information, MIS, University of Arizona, jlee@bpa.arizona.edu
AI & Simulation Group, ECE, University of Arizona, (hessam, sankait, zeigler)@ece.arizona.edu
c
US Air Force Armstrong Laboratory, Logistics Research Division, fsimcox@alhrg.wpafb.af.mil

b

Abstract
This paper describes a research and development
(R&D) effort entitled Depot Operation Modeling
Environment (DOME). The project is sponsored by the
Armstrong Laboratory, Logistics Research Division
(AL/HRG) located at Wright-Patterson AFB, Ohio. The
effort is focused on enabling of groupware technology
to support the extension of IDEF modeling techniques
to accomplish Discrete Event System Specification
(DEVS) model construction and associated simulations
in a distributed environment. The methodology
envisioned enables team participants to enter process
data and then be queried for additional data that
supports DEVS system decomposition, assigning the
entities to components and adding in relevant
dynamics.

1. Introduction
The growth of distributed computing in recent time
has been substantial and will continue to mature. It is
estimated that Internet connections are doubling each
year. Recent survey data shows the number of Internet
connections at 16,146,000 worldwide as of January
1997, up from 9,472,000 in January 1996. Systems are
being designed and built based on the inherent
characteristics of the Internet. Emerging architectures
such as the Defense Information Infrastructure Common
Operating Environment (DII COE) are providing
guidance on software/hardware technologies that
support distributed computing. Furthermore, since many
future systems will be distributed in nature, there is a
need for modeling and simulation (M&S) environments
that support the design of these systems, and that take
advantage of these systems.
As more organizations become physically distributed
and their employees work in teams that are not colocated, group-enabled technologies begin to play a
central role in the teams ability to be successful. Such
technologies provide useful tools to support modeling

and analysis of processes using group-based technology
in support of collaboration of dispersed teams. Groupbased technology is of particular importance when
designing processes that are distributed in nature across
multiple organizations such as supply, transportation, or
manufacturing processes.
Most current modeling and simulation tools are
limited due to their lack of support for team work
among their users, whether co-located or not. Groupenabled environments are important since they can
support knowledge acquisition from various people who
are each primarily knowledgeable in a particular part of
a complex system. By working in teams and integrating
knowledge between groups, information about a system
can be gathered that is not only complete and accurate,
but also recognized and owned by the people involved
in the system design and implementation. The use of
group-enabled technology will have profound
implications for modeling and simulating of distributed
large-scale and complex systems.
IDEF (Integrated Computer-Aided Manufacturing
[ICAM] DEFinition) methods are used to perform
modeling activities in support of enterprise integration.
The original IDEF Methods were developed for the
U.S. Air Force to support communication between the
USAF and its suppliers. IDEFØ (Function Modeling
Method) was designed to allow an expansion of the
description of a system’s functions through the process
of function decomposition and categorization of the
relations between functions (i.e., in terms of the Input,
Output, Control, and Mechanism classification). IDEF1
(Information Modeling Method) was designed to allow
the description of the information that an organization
deems important to manage the accomplishment of its
objectives. IDEF3 (Process Flow and Object State
Description Capture Method) has been developed to
support the structuring of descriptions of the user view
of a system; and, IDEF5 (Ontology Description Capture
Method) serves as a method for fact collection and
knowledge acquisition [2].

1060-3425/98 $10.00 (C) 1998 IEEE

To obtain simulation models that can be used for
systems
from
conceptual
design
to
their
implementations, it is necessary to use a general M&S
framework such as DEVS [9, 10]. The DEVS M&S
environment supports capturing a system's behavior
(functional structure) and topology (physical structure)
from a wide range of perspectives while supporting
their behavior generation without the requirement for a
specific simulation engine.
A group-enabled M&S methodology, targeted toward
the development of simulation models of Air Logistics
Command (ALC) industrial based processes, can be
built by incorporating group-enabled technology into
existing (as well as future) M&S environments. In
particular, a suitable group-enabled technology can
provide a layer supporting M&S techniques. It is the
intent of this paper to lay out what is necessary to
create, using group-enabled technologies, DEVS
models given IDEF model techniques such as IDEF3.

2. IDEF3 Process Description
The IDEF3 Process Description Capture Method
provides a mechanism for collecting and documenting
processes. IDEF3 captures precedence and causality
relations between situations and events by providing a
structured method for expressing how a system or
process works.
There are two IDEF3 description methodologies,
process flow and object state transition network. A
process flow description captures knowledge of "how
things work" in an organization. The object state
transition network description summarizes the
transitions an object may undergo throughout the
process at hand.
The basic elements of IDEF3 Process Description
language are Unit of Behavior (UOB), Junction Boxes,
Links, and Referents [3, 4]. The processes that make up
an IDEF3 description appear as labeled boxes. The
IDEF3 term for elements represented by boxes is a Unit
Of Behavior. A UOB is made up of a name, number,
objects involved in the UOB, facts, constraints, and a
definition. Smaller boxes define junctions that provide a
mechanism for introducing logic to the flows. The
junctions are asynchronous AND synchronous AND
asynchronous OR synchronous OR and exclusive OR
with their symbolic representations as |&, |&|, |O, |O|,
and X, respectively.
Links (depicted as arrows in diagrams) connect the
boxes (processes) together and represent logical,
temporal, and/or causal flow. Within the Process
Description Diagram, referents are employed to
enhance understanding and provide additional meaning

to the Process Flow Diagrams. They capture anything
else that cannot be captured by UOB, junction box, and
links. In particular, role of an object or a relation in a
UOB is represented as a referent. Specific constraints
on junction boxes and references/links between process
description and OSTN diagrams are also delegated to
referents. Referents are also used for referencing
previously defined UOB and accounting for specific
examples or referenced data or objects.
Each UOB can have associated with it a hierarchical
decomposition. A decomposition is a more detailed
examination of a UOB. It decreases the complexity of
the diagram by enabling the capture of descriptions at
varying levels of abstraction.
Object state transition network (OSTN) diagrams
capture object-centered views of processes which cut
across the process diagrams and summarize the
allowable transitions. Object states and state transition
arcs are the key elements of an OSTN diagram. In
OSTN diagrams, object circles represent states and the
lines connecting the circles represent state transition
arcs. An object state is defined in terms of the facts and
constraints that need to be true for the continued
existence of the object in that state and is characterized
by entry and exit conditions. State transition arcs
represent the allowable transitions between the object
states. Attaching a UOB reference to the transition arc
between the two object states normally represents
participation of a process in a state transition.

3. DEVS Modeling and Simulation
The DEVS approach to modeling and simulation is a
discrete-event formalism that takes advantage of the
object-oriented paradigm. A formalism is a
mathematical language for describing the structure and
behavior of a model. The DEVS formalism is founded
on a mathematical structure of definitions and theorems
based on set theory, thus enabling manipulation of
system parameters that is similar to the role of
differential equations in continuous systems [11]. The
DEVS formalism includes a time base, inputs, states,
outputs, and functions for determining next states and
outputs, given current states and inputs. In the DEVS
formalism, one must specify 1) basic models (atomic)
from which larger ones are built (coupled), and 2) how
these models are connected together in hierarchical
fashion [9, 10].
In the DEVS formalism, an atomic model is defined
by the structure M = <X, Y, S, δint, δext, λ,ta> where
X: set of external input events;
Y: set of external event types generated as output;

1060-3425/98 $10.00 (C) 1998 IEEE

S: sequential state set;
δ ext:external transition function dictating state
transitions due to external events;
δ int: internal transition function dictating state
transitions due to internal events;
λ: output function generating external events as
output;
ta: time advance function.
When an input value X is received at the input port of
an atomic model M, the external transition function δext
determines the new state S based on the current state
and the amount of elapsed time associated with the
current state. The internal transition function δ int
specifies the next state of the model after the model has
been in the current state s for a specific amount of time
e, such that 0 ≤ e ≤ ta(s). The output function λ
determines the output Y that is produced when the state
change occurs.
An important aspect of the DEVS formalism is that
the time intervals between event occurrences are
variable in contrast to discrete time where the time step
is a fixed number. Independence from fixed time steps
affords important advantages for modeling and
simulation. Multiprocess models contain many
processes operating on different time scales. Such
models are difficult to describe when a common time
granule must be chosen to represent them all. Also,

simulation is inherently inefficient since the states of all
processes must be updated in step with the smallest time
increment--such rapid updating is wasteful when
applied to the slower processes. In contrast, in a discrete
event model every component has control over the time
of its next internal event. Thus, components demand
processing resources only to the extent dictated by their
own intrinsic speeds or their response to external events
[12].
DEVS supports hierarchical, modular, object-oriented
models [9]. For a set of component models (objects), a
coupled-model can be created by specifying how the
input and output ports are connected to each other, and
to external ports. The new coupled-model is itself a
modular model and thus can be used as a component in
a yet larger, hierarchically higher level model.
Hierarchical, modular construction is important since it
supports stagewise validation and verification leading to
highly reliable simulation models built in reasonable
time.
By combining flexible model expression with efficient
simulation, DEVS provides a sound, general framework
within which to address design and analysis issues for
business systems and other systems containing both
physical and decision-making components [13, 14].

coupled model

X1

atomic model δext1

s

e

δint1
λ1
ta1

Y1=X2

atomic model δext2

s

e

δint2
λ2
ta2

Y2

Figure 1. DEVS Coupled Model Example

3.1. Strengths and Limitations Of IDEF3 Vs
DEVS
In its core, IDEF3 supports capturing and structuring
of incomplete and possibly inaccurate descriptions
of how a system may operate. The strength of IDEF3 is
its capability to acquire semi-formal knowledge for a
given process from domain experts who are not
modelers by discipline. In this respect, IDEF3 is
primarily conceived and developed to support high-

level analysis rather than simulation entailing detailed
analysis and prediction.
For detailed analysis and prediction, especially
where dynamic behaviors must be captured, the syntax
and semantics of DEVS are much more capable than
those of IDEF3. This is not surprising, since the IDEF3
methods (i.e., process-centered and object-centered)
syntax and semantics derive from its objectives of semiformal knowledge capture. In particular, beyond
identifying UOBs and their interconnections through

1060-3425/98 $10.00 (C) 1998 IEEE

Junction Boxes and Links, the Process Description
method can only loosely and incompletely characterize
the functionality of a collection of UOBs using
Referents. In the complementary object-centered view,
OSTN diagram is limited since each OSTN can only
capture states associated with a single object. Therefore
IDEF3 affords weak support for representing and
manipulating a collection of objects and their joint
state-space.
DEVS, however, emphasizes the functionality of each
UOB, as well as sets of interacting UOBs, with precise
syntax and semantics. The modeling of multiple objects
and their states is also supported by DEVS using
coupled models. In comparison to IDEF3, DEVS is a
generic formalism, and does not have primitives for
specific model behaviors. Thus, while Junction Boxes
are not available directly, they can be represented
completely and precisely as atomic models.
Another important limitation of IDEF3 is its weak
support for representing and manipulating objects
flowing through UOBs, the information and/or material
items that are being processed. In IDEF3 you can signal
that something is flowing along a link, but you cannot
say exactly what it is. Moreover, even though the OSTN
provides an object-centered view, it is intrinsically
limited to representing a single object and its states.
Flow of multiple objects, such as part and an associated
textual record, cannot be represented along the single
link that connects a pair of UOBs. Hence, despite the
co-existence of Process Description and OSTN
methods, the IDEF3 framework cannot supersede its
intended functionality and role as a semi-

TIME
PLACE
Same
Different

formal knowledge capture approach. WitnessTM is a
simulation engine used with ProSimTM, a modeling tool
built supporting some of the principles of IDEF3
method. While WitnessTM is a capable simulation
language, the mapping of IDEF3 into it is not
straightforward and requires considerable simulation
expertise and manual intervention.
This extra
information will be clarified in Section 5, where we
examine mapping of IDEF3 into DEVS.

4. Group Based Technology
In the global environment of the 1990s and beyond,
informational networks are predicted to help distributed
companies and organizations link up and work together
from start to finish. They will operate as "virtual
organizations", formed for a specific project and then
dissolve. The computer systems to support these often
temporary and rapidly changing, geographically
distributed task forces and partnerships will combine
characteristics of computer-mediated communication
systems and group support systems to create distributed
group support systems (DGSS) [7]. Such computer
facilities will need to support the full range of tasks
involved in projects, including planning, budgeting,
information gathering, system design, and decision
support through simulation.
Group-enabled technology or groupware refers to
technology that supports groups of people as opposed to
single users. A group support system (GSS) is a
computer-based "social technology," the basic purpose
of which is to increase the effectiveness of decision
groups by facilitating the interactive sharing and use of
information among group members and also between
the group and the computer [1]. Table 1 depicts various
modes of groupware, such as email and electronic
meeting facilitation.

Same

Different

meeting facilitation

work shifts
team rooms
electronic mail
electronic bulletin boards

video/desktop conferencing
electronic whiteboards
interactive multicast seminars
Table 1. Modes of Groupware

According to Valacich, Dennis, and Nunamaker [8],
"a group support system (GSS), is described as an
environment that contains a series of networked
computer workstations that enable groups to meet faceto-face, with a computer-supported electronic

communication channel used to support or replace
verbal communication." When a GSS is applied to
group decision making: (1) ideas can be exchanged
between group members and organized into distinct
categories, (2) the categories can be analyzed by group

1060-3425/98 $10.00 (C) 1998 IEEE

members exchanging information through electronic file
folders, (3) consensus can be developed between group
members, (4) data can be used and reviewed in future
meetings, and (5) data can be exported to a superior or
expert for critique or approval. An example of a GSS is
GroupSystemsTM (GS) from Ventana Corporation,
which has been effectively used to support strategic
planning and group document authoring.
During most instances where a GSS is employed, a
meeting facilitator is used. A facilitator has multiple
roles during a meeting [5]. For example, the facilitator
may be the group leader, a group member, or an
individual that is separate from the group and neutral by
decree. The role of the facilitator is to provide technical
support, plan an agenda, maintain an agenda, and set
ongoing standards for how the GSS is used in an
organization [5].
Most instances of GSS usage seen today occurs in the
same-time/same-place mode of groupware. However,
several efforts at the Logistics Research Division,
Armstrong Laboratory, Wright-Patterson AFB OH, are
underway to extend group-enabled technology along the
line of time and place using video/desktop conferencing
and distributed databases.

the repair could be scheduled and carried out. It is
important to note that this process does not primarily
focus on one organization, but must consider all
organizations while taking into account their
interaction. Moreover, the wing would have specific
knowledge pertaining to the part of the process that it
owns. Likewise, the ALC is most knowledgeable about
its own processes. From this we conclude that each
organization must be considered as equal to all others
when modeling such a process so that the entire process
can be captured from end to end. Hence, from the
standpoint of the repair process, the objectives of the
associated organizations are intertwined (e.g., repair the
aircraft), it would be of great value if a model
encompassing all organizations and their associated
interactions could be developed.
Using expertise from both the customer and supplier,
an initial IDEF3 model of the process can be captured.
The part of the repair request model presented here is
used for discussing what is necessary to extend an
IDEF3 process description to its DEVS counterpart
model.

6. IDEF3 Representation
5. Real-world Example
Consider the Air Force depot field repair process
where Air Force Material Command (AFMC) with the
help of the ALCs provides maintenance assistance to
Air Force wings and deployed sites. This process can be
considered in terms of a supplier (i.e., AFMC and the
ALCs) and a customer (i.e., the warfighter). A typical
wing owns a fleet of aircraft that must met flying hour
goals as well as real world deployment requirements. In
addition, other organization (e.g., Major Commands or
MAJCOMs) have their objectives and would also be
involved.
The wing has a number of objectives such as
satisfying its flying goals, safety, and meeting training
objectives. We can see that the ALCs are the suppliers
of unique repair actions on aircraft owned by the
operational wings. Similarly, the ALC organization
would have its own objectives (providing quality
service for its customers, meeting budget constraints,
etc.).
For the purpose of this paper, we look at a process
(i.e., maintenance request) involving customers and
suppliers (Figure 2). This is a process in which the
operational wing would submit requests for aircraft
repairs to the associated ALC. The repair request would
need to be processed and approved by both
organizations and their associated MAJCOMs before

Each of the UOBs in Figure 3 can be viewed as
processes that make up the overall process flow diagram
representation of the request for depot support. The
IDEF3 diagram in Figure 3 represents a processcentered view of the request for depot support. This
view focuses on the assertions about the processes that
occur and their order of completion. At times it is
convenient to organize the process description in an
object-centered view. For this example, the "request
document" could be considered an object that changes
state during the processes described at the base and
MAJCOM level (see Figure 4). Remember, IDEF3
facilitates object-centered views through OSTN
diagrams.
Wing

Air Logistics Center

•Ops Group
•Log Group

•System Program Offices
•Production Directorates

MAJCOMs
(ACC, AMC, AFMC)

Figure 2. High Level View of ALC, Wing, and MAJCOM

1060-3425/98 $10.00 (C) 1998 IEEE

•

In a typical setting such as the example above, the
following types of objects are sent from one process
activity to another. The objects types could include:
• Hard documents, forms, reports, etc. (e.g.,
repair request document)
• Soft documents, forms, reports, etc. (e.g.,
preliminary email repair request)

Research
maintenance
problem

1.1

O

Hardware/Software (e.g., parts, scheduling
application)
Audio/Visual (e.g., telephone, video
conference)

•

Every IDEF3 description has associated with it one
or more scenarios and one or more objects. These
scenarios and objects bound the context of the entire
description. The scenarios and objects are considered
part of the IDEF3 description and provide for the
organizing and scoping of the description.
Each OSTN diagram focuses on one object. One of
the first tasks at hand when developing an OSTN
diagram is to define all possible states of the object at
hand. For each of these states, the OSTN supports
definition of: 1) the conditions which characterize the
state, 2) the conditions that will permit a transition into
the state (entry conditions), and 3) the conditions that
must hold for the object to transition out of the state
(exit conditions) as shown in Figure 5 [3].

Accomplish
local fix

J1
2.1

Object/
request

MAJCOM
disapproves
request

7.1

Send request
for depot
support

MAJCOM
receives
request

3.1

4.1

MAJCOM
researches
prob/budget

O
J4

5.1

MAJCOM
approves
request

6.1

Figure 3. IDEF3 Process Flow Diagram

7. Extending Process Models To DEVS
Simulation Models
Despite an IDEF3 model capability to capture
considerably more dynamics of a given system in
comparison to IDEFØ, it cannot capture all that is
required to create models in accordance to the DEVS
formalism. Here, we present a transitioning from the
IDEF3 description to the DEVS model. This mapping is
similar to the way IDEFØ models are converted to
DEVS [6]. In order to develop a DEVS model that
could be used to simulate the depot field repair process,
it would be necessary to specify inputs, states, outputs,
and functions as well as any coupling to other models.
Given the IDEF3 descriptions of the request for depot

support shown in Figures 3 and 4, the following is
required in order to construct its DEVS model.
1. inputs/outputs
2. state sets
3. functions associated with the completion of an
event or task in terms of states and/or inputs
4. conditions associated with completion or noncompletion of a function
5. time to complete each function
6. whether functions are performed sequentially or
concurrently, and how coordination is
accomplished
7. how a state change results in output

1060-3425/98 $10.00 (C) 1998 IEEE

Process/
send request
to depot

draft
request
form

Process/
MAJCOM
receives req

place
form in
mail

Process/
Research
prob/budget

request
form
arrives

Process/
MAJCOM
approves

annotate
approval

process
request

Process/
MAJCOM
disapproves

annotate
disapproval

Figure 4. OSTN Diagram

Consider the request for depot support example, the
set of inputs associated with UOBs 5.1 and 6.1,
although not clearly defined by the IDEF3 descriptions,
would include such items as:

Transition Arcs

Object State

X = {repair request, maintenance information
associated with problem}

Entry
State
Exit
Conditions Description Condistions

Figure 5. OSTN Diagram Concepts

Given IDEF3, it is clear the lowest level decomposition
of UOBs can be considered atomic models. The state
sets (item 2) are captured through the associated IDEF3
OSTN diagrams while the sequencing and coordination
information (item 6) is derived from the IDEF3 process
flow diagrams.
The constraints associated with a UOB define the
conditions under which a UOB may start, continue, or
terminate. The facts associated with a UOB define the
object characteristics and relations between objects
associated with the particular UOB. The facts and
constraints provide the required information to define
conditions associated with the completion of a function
(item 4) and some of the information required to define
the functions associated with an event (item 3). The
notion of how a state change results in output (item 7) is
not clear, as inputs and outputs are not adequately
defined.
Note that inputs and outputs in DEVS can represent
objects that flow into, and out of, a model. This
information is critical for characterizing the state of the
model and how it is changed by inputs and generates
outputs. Clearly what is lacking in the IDEF3
methodology is a firm definition of UOB inputs and
outputs (item 1), as well as timing information that goes
beyond UOB sequencing (item 5).

The set of outputs associated with 5.1 and 6.1 would
include simply the decision to approve or disapprove
the request for depot support.
Y = {approval notification, disapproval
notification}
The state set for the document object associated with
UOBs 5.1 and 6.1 and derived from the OSTN diagram
would include:
S = {request arrives, process request, annotate
approval, annotate disapproval}
An external transition occurs when an external input
event, such as those listed in X is received by the model
causing a state transition. Given two input events, the
following external transitions result:
•
•

Input event: repair request -> Initiates state:
request arrives -> Output: none
Input event: maintenance information ->
Initiates state: process request -> Output: none

An internal transition indicates the initialization of a
state as a result of the completion of a preceding state.
Given the state set S above, the following internal
transitions result:
•

Initial state: request arrives -> Initiates state:
process request -> Output: none

1060-3425/98 $10.00 (C) 1998 IEEE

•
•

Initial state: process request -> Initiates state:
annotate approval -> Output: approval
notification
Initial state: process request -> Initiates state:
annotate disapproval -> Output: disapproval
notification

Discrete event models are time based and as a result,
rely heavily on timing information in order to function
properly when executing a simulation. This information
is normally captured as UOB constraint information
during UOB definition. A process walk-through of the

IDEF3 process flow diagram could validate the timing
information required.

8. A Collaborative Modeling and
Simulation Environment
Figure 6 shows a conceptual architecture that provides
a framework for implementing a collaborative
environment for M&S. The layers consist of a network
layer, simulation layer, modeling layer, search layer,
decision layer, and collaboration layer.

C o llab o ra tio n L a ye r
S a m e/D iff T im e /P la c e S m all/L arg e G rou p

D e cis io n L ay e r
E xp lo ra tion

E v a lua tion

O ptim iz a tio n

S e arc h L ay er
C la s s ic al
AI
E vo lutio n (G A )
M o d elin g L a y e r
F orm alis m s D is cre te C o n tinu ou s D E V S ID E F
S im u latio n L a ye r
B e ha vio r G en e ration D B M S G U I A nim atio n V isu aliz a tio n

W ork sta tio n

N e tw o rk L a ye r
D is tribu te d W e b/Inte rn et

P ara llel(H P C )

Figure 6. Collaborative Architecture for Modeling and Simulation

The network layer consists of the actual computers
(including workstations and high performance
systems) and the connecting networks (both LAN and
WAN, their hardware and software) that do the work
of supporting all aspects of the M&S lifecycle.
The simulation layer is the software that executes
the models to generate their behavior. Included in this
layer are the protocols that provides the basis for
distributed interactive simulation (which are
standardized for DoD in the DMSO-sponsored high
level architecture (HLA)).
Also included are
database management systems, software systems to
support
control
of
simulation
executions,
visualization and animation of the generated
behaviors.
The modeling layer supports the development of
models in formalisms that are independent of a given
simulation layer implementation. HLA provides an
object-oriented templates for model description
aimed at supporting confederations of globally
dispersed models. However, formalisms for model
dynamics, whether continuous, discrete or discrete
event in nature are also included. Model construction
and especially, the key processes of abstraction are
also included.

The search layer supports the investigation of large
sets of alternative models, whether in the form of
spaces set up by parameters or more powerful means
of specifying alternative model structures. The
decision layer applies the capability to search and
simulate large model sets to real problem domains to
support explorations, “what-if “ investigations, and
optimizations.
The collaboration layer enables people with partial
knowledge about a system, whether based on
discipline, location, task, or responsibility
specialization, to bring to bear individual perspectives
and contributions to achieve an overall M&S goal.
This layer would take advantage of groupware based
tools.
The vision for this architecture would include
Windows NT/Windows 95 computing platforms
networked together using TCP/IP as the primary
network
protocol.
Simulations
would
be
accomplished using generic simulation engines that
are capable of understanding discrete event models
such as those provided using the DEVS formalism.
Model construction is accomplished through an
IDEF3-like interface utilizing groupware concepts to
gather information in order to build atomic and

1060-3425/98 $10.00 (C) 1998 IEEE

coupled DEVS models. A search engine based on
genetic algorithms is provided to gather existing
models as possible alternatives. The decision support
capability is provided through access to simulations
to support exploration of alternative process designs.
Collaboration would support personnel with partial
knowledge of the system to contribute to all
appropriate aspects of the M&S effort.

Information Integration for Concurrent Engineering (IICE)
IDEF3 Process Description Capture Method Report,
Armstrong Laboratory Technical Report, AL-TR-19920057.

9. Conclusions

[5] Nunamaker, J.F., Dennis, A. R., Valacich, J. S., Vogel,
D. R., and George, J.F., 1991, Electronic Meetings to
Support Group Work, Communications of ACM, Vol. 34
(7), pp. 40-61.

This paper discussed the concepts associated with
groupware and the need for group-enabled M&S
environments. In particular, a brief discussion of
group-enabled technologies was given within the
context of a distributed-view of modeling. The
transition from the IDEF3 process description to the
DEVS modeling formalism to support discrete event
model construction in a distributed environment was
discussed. Finally, a candidate architecture to support
a collaborative M&S environment was presented. The
architecture presented is evolving. Hence, current and
future efforts will be directed toward elaboration and
detailed definition of the associated architecture.
Specifically, it would be important to know to what
extent a group-enabled modeling framework can be
independent of any specific technology. Furthermore,
in terms of group-enabled modeling applicability, it is
important to determine how distributed object
management might be handled.
The IDEF3 approach is inappropriate for
mathematical modeling and simulation since it does
not support rigorous modeling principles that underlie
the development of complex models amenable to
dynamic simulation. However, an IDEF3-like method
supporting semi-formal knowledge acquisition of
processes may be used as a pre-step toward
developing DEVS models. That is, the combination
of IDEF3-like and DEVS methods can provide a
continuum to develop models based on informal,
semi-formal, and formal knowledge captured from
ordinary users, subject matter experts, and modelers
respectively.

References

[4] Mayer, R.J., Menzel, C.P., Painter, M.K., deWitte, P.S.,
Blinn, T., Perajath, B., 1995, Information Integration for
Concurrent Engineering (IICE) IDEF3 Process
Description Capture Method Report, Armstrong
Laboratory Technical Report, AL-TR-1995-XXXX.

[6] Sarjoughian, H., Vahie, S., Lee, J., 1997, GroupEnabled DEVS Model Construction Methodology for
Distributed Organizations, in proceedings of Enabling
Technology for Simulation Science, SPIE AeoroSense 97,
Orlando, Fl.
[7] Turoff, M., Hiltz, S.R., Bahgat, A.N.F., Rana, A.R.,
1993, Distributed Group Support Systems, MIS
Quarterly/December pp. 399-417.
[8] Valacich, J.S., Dennis, A.R., Nunamaker, Jr., J.F.,
1992, Group size and anonymity effects on computer
mediated idea generation, Small Group Research, 2(1).
[9] Zeigler, B.P., 1990, Object-Oriented Simulation with
Hierarchical, Modular Models: Intelligent Agents and
Endomorphic Systems, San Diego, CA: Academic Press.
[10] Zeigler, B.P., 1984, Multifacetted Modelling and
Discrete Event Simulation, Academic Press, London &
Orlando FL.
[11] Zeigler, B.P., 1976, Theory of Modelling and
Simulation, (Under Revision for 2nd Edition), New York:
John Wiley.
[12] Zeigler, B.P., Song, H.S., Kim, T.G., Praehofer, H.
1996, The DEVS Formalism for Modelling, Analysis and
Design, Hybrid Systems II, P. Antsaklis et. al. (eds.), LCNS
999, pp. 529-552, Springer Verlag.
[13] Zeigler, B.P., Kim, J.W., 1996, A High Performance
Modelling and Simulation Environment for Intelligent
Systems Design, International Journal of Intelligent Control
and Systems, vol. 1, no. 1.

[1] Huber, G.P., 1984, Issues in the Design of Group
Decision Support Systems, MIS Quarterly/September, pp.
195-204.
[2]
Knowledge
Based
Systems,
www.kbsi.com/idef/idef_ovr.html.

Inc.,

1997,

[3] Mayer, R.J., Cullinane, T.P., deWitte, P.S.,
Knappenberger, W.B., Perakath, B., Wells, M.S., 1992,

1060-3425/98 $10.00 (C) 1998 IEEE

[14] Zeigler, B.P., Sanders, W.H., 1993, Preface to special
issue on environments for discrete event dynamic systems,
Discrete Event Dynamic Systems: Theory and Application,
pp. 110-119.

1060-3425/98 $10.00 (C) 1998 IEEE

Speedup of a Sparse System Simulation
James Nutaro
Hessam Sarjoughian

Arizona Center for Integrative Modeling and Simulation
University of Arizona
{nutaro,hes.ram}@ece.arizona. edu
www. acims. arizona. edu

2. Sparse DEVS

Abstract

To state the hypothesis precisely, a formal
representation for discrete event systems is needed. While
the logical process model is most common in distributed
discrete event simulation, this model fails to capture key
elements of the system under study. In particular it does
not capture, in explicit terms, the system state space,
transition function, or output function. Representation of
the system in these terms is necessary in order to
distinguish between output producing and non-output
producing states, a distinction that is at the heart of the
lcfinition of a sparse system.
We selected DEVS as a formal system representation
for its uniqueness property [ 9 ] . That is, DEVS can
represent the entire class of discrete systems. This
includes the traditional event stepped and process oriented
views of discrete event simulation. Hence, it captures the
logical process model as well as more esoteric instances of
discrete event systems (e.g. quantized continuous
systems).
DEVS describes systems in terms of atomic and
coupled models. Atomic models are of interest for our
purposes here. A parallel DEVS atomic model is a
structure

Optimistic and conservative simulation algorithms
have been effective for speeding up the execution of many
simulation programs. Event stepped techniques have also
been shown to be effective for certain types of problems.
This paper presents a conjecture that sparse systems are
effectively simulated by conservative and optimistic
techniques, where sparseness is characterized by the ratio
of output generating states to non-oirtput generating
states. The speedup results for a representative sparse
simulation are shown to support this conjecture.

1. Introduction
Research into distributed discrete event simulation has
focused, primarily, on optimistic and conservative
techniques. It is intuitively obvious that some degree of
parallelism must be present in the simulation if these
techniques are to be effective. However, the source of
parallelism in discrete event models has yet to be
described in concrete terms. By developing a model that
describes the parallelism inherent in a discrete event
system, we move closer to being able to reason about the
types of algorithms, conservative and optimistic or event
stepped, that will be effective for a given simulation
problem.
This paper presents a conjecture, which is intuitively
nice, that sparse systems are effectively simulated by
conservative and optimistic techniques, where sparseness
is characterized by the ratio of output generating states to
non-output generating states. The speedup exhibited by
sparse system simulations is related to Amdahl’s Law.
Finally, a particular sparse system is simulated using DSE,
the Distributed Simulation Environment [ 6 ] , and the
speedup results are shown to conform to these
expectations.

X is the set of system inputs
Y is the set of system outputs
S is the set of system states
T is the system time base
6,,,,: S + S is the internal transition function
6,: S x T x X + S is the external transition
hnction
6collf:S x X + S is the confluent transition
function
h: S -+ Y is the output function, and
ta: S -+ T is the time advance function.

193
0-7695-1104-WO1$10.00 0 2001 IEEE

The input set describes the objects (events, values) that
are admissible as input to the DEVS. The output set
describes the objects (event, values) that are produced by
the DEVS.
The state set describes the internal
representation of the system (i.e. the state space of the
system). The time base is the set of values that can be
assumed by the system clock (usually the real numbers or
integers).
The internal transition function describes the
autonomous (input-free) behavior of the system. The
external transition function describes the input response of
the system. The confluent transition function describes
the response of the system when internal event and
external event transitions coincide. The output function
describes the system output as a function of the state of the
system. Finally, the time advance function indicates the
time until the next autonomous state change.
When interpreting a DEVS, system outputs may be
assumed to coincide with autonomous state changes. That
is, a system output is produced just prior to an input free
change of state. More generally, system outputs are due to
state changes via the internal or external transition
functions. T o model outputs that are an immediate
response to an input event, a transitory state whose time
advance is 0 is used.
To define a sparse DEVS, let So G S be the set of
output generating states (i.e. s E So if and only if h(s)f 0,
where 0 is the non-event). A sparse system has lSol << IS/
[ 5 ] . Given a set of sparse systems, the probability of any
two interacting during a given simulation cycle is small.
Consequently, it may be possible to process large numbers
of events concurrently, without concern for the
timestamps of those events’.
From the point of view of a distributed algorithm, the
event timestamps are consequential only if those events
could affect the state trajectory of some other system. By
definition, such an event is a system output. Since outputs
are infrequent, the distributed algorithm rarely needs to
intervene in order to correct and/or prevent a causality
violation. On the other hand, if output producing states
occur frequently, it would behoove us to minimize the cost
of detecting and either correcting or preventing causality
violations.
In general, algorithms geared towards sparse systems
are expected to have a high overhead cost associated with
system outputs. An algorithm that assumes every state is
likely to produce output can minimize the overhead
associated with an output, but will likely do so at the
expense of a larger overhead when non-output generating
states occur.

3. Predicting speedup for sparse system
simulations
Suppose each state in S occurs with equal probability
during a simulation. Then p = IS01 / IS1 is the probability
that any given state will result in an output. Since outputs
can require the simulators to interact, those simulators
involved in the interaction must have their execution
serialized (i.e. if simulator A sends output to simulator B,
the output producing transition for A must be computed
before the resulting input can be processed by B). For the
sake of simplicity, we will assume that the entire simulator
is serialized for that transaction. Under this assumption,
sparseness probability p may be used as the measure of the
serial workload of the simulation. Similarly, (1 - p)
represents the potentially parallel workload. Both the
serial and parallel workloads have a direct relation to p.
Using Amdahl’s Law, we can anticipate the simulator’s
potential speedup as follows [ 3 ] . Given k processors and
sparseness probability p, the simulation speedup Sp(k, p)
isSp(k,p)= 1 /(p+(l-p)/k).
Although it is unlikely that Amdahl’s Law will allow
us to predict actual speedup metrics, increasing speedup
should be observable as p decreases. Similarly, as p
increases, we expect to see reduced speedup.

4. Approximating p by experimental methods
For systems with a large, or infinite, state set, it will
not be feasible to determine p directly from the model
description. In these cases, IS01 can be approximated by
looking at the number of outputs produced by the DEVS
during a simulation run. To approximate IS/, we count the
number of internal transitions that occur. This reflects the
portion of the state space explored during the simulation
run. Only the internal transition count is considered since
model outputs only occur in conjunction with internal
events.

5. Speedup of a multi-protocol simulation of a
sparse system
The objective of this experiment is to rank simulation
algorithms in terms of their ability to exploit parallelism
inherent in a sparse system. Towards this end, we can
make a few observations that should be easily
demonstrated by experimentation. In all cases, we are
concerned with sparse systems.
Event stepped algorithms’ extract the smallest amount
of the potential parallelism. In this case, only events with
identical timestamps can be processed in parallel. As the
model becomes increasingly sparse, event stepped
algorithms should exhibit decreasing speedup. This will

’

The DEVS abstract simulator is responsible for
implicitly assigning time stamps to events. Similarly,
logical process simulators assign timestamps to events that
are produced by their respective models.

’Event stepped algorithms are described in [ 6 ] .
194

It

l

ComDuter~

I
arrive
solved

Computer 1

n

I

lm

transd
stop

Computer 8

Figure 1: The model partitioned for simulation on 9 computers.
in this case would still exceed the speedup obtained by a
purely event stepped or conservative simulation.

result from the number of concurrent events decreasing
relative to the total number of events processed during a
simulation run.
Conservative algorithms exploit windows of
parallelism that are described by the lookahead of the
system. These windows have an upper bound on their size
that is independent of the particular output trajectory
exhibited by the system during a simulation run.
Consequently, a conservative algorithm is likely to miss
opportunities for parallelism that are unique to a particular
simulation run. As the sparseness of the system increases,
these missed opportunities are likely to become more
common, thereby degrading the effectiveness of the
conservative algorithm.
Optimistic algorithms provide automatic parallelism
detection with a potentially large overhead cost. It seems
reasonable to expect that optimistic algorithms will scale
effectively as the problem size increases. However, if the
overhead incurred by check pointing and rollbacks
becomes overly large, as would be expected for a nonsparse system, the optimistic algorithm may exhibit
severely degraded performance relative to a conservative
or event stepped algorithm.
It is much more difficult to make generalized
statements concerning mixed mode simulators (e.g.
interoperating event stepped and conservative algorithms).
It would seem reasonable to assume that, in general,
mixed mode simulators can provide some speedup. In
cases where some component of a system can not be
simulated using, say, an optimistic algorithm, it may be
beneficial to simulate those components with an event
stepped or conservative protocol. The remainder of the
system could then take advantage of a potentially more
effective optimistic algorithm. Presumably, the speedup

5.1. Methods
The model for this experiment consists of a multiple
processor server attached to a generator and transducer.
The generator produces jobs that need to be processed by
the multiprocessor. The transducer collects finished jobs
from the multiprocessor and compiles throughput and
turnaround time statistics. The multiprocessor consists of
eight processors that maintain a job queue and assign time
to each job in a round robin fashion. The load balancing
process is responsible for equalizing the lengths of the
processor queues. Figure 1 shows the partitioning of the
model for distributed simulation on 9 computers.
A job is a structure with the following fields:
0

0
0

processing time - the time required to finish the
job.
This is a random variable uniformly
distributed in [0,40]
job ID -the job’s unique ID number.
processor ID - the ID of the processor assigned to
this job by the load balancer.

The generator (genr) produces a job every r units of
time, where r is a random variable uniformly distributed
in [0,10]. When input is received on the stop port, the
generator stops producing jobs. In the case of optimistic
processing, the state of the random number generator is
saved along with the generator’s state. Consequently, the
random number stream is fixed relative to the initial seed.
The load balancer (lb) tries to do load balancing for
eight processors. The load balancer maintains a load

195

measurement for each processor. The load measurements
are initially set to zero. When the load balancer receives a
job on its in port, it looks for the least loaded processor,
adds one to its load measurement, and then sends that job
to the target processor. When a completed job is received
on the load port, the load measure for the processor that
sent the job is decreased by one. The load balancer
requires zero units of time to process a job.
The transducer (transd) computes average turnaround
time and throughput over an observation interval. At the
end of the observation interval, it produces an output on
its stop port.
A processor (proc) maintains a queue of jobs, and
processes the jobs in a round robin fashion, giving 0.01
units of time to each job in turn. When a job is completed,
it is removed from the queue and sent to the out port.
When a job is received on the in port, the job is added to
the back of the queue. If the queue was empty, the
processor immediately starts processing the job.
Otherwise, the processor finishes up the time slice for the
current job. The processor’s lookahead value is the
smallest of the processing times of the jobs in the queue.
The model was simulated with six different simulation
algorithm configurations, each over four different
observation intervals, and the execution time for each
configuration was observed. The sparseness of the system
as a whole was measured by tallying the number of
internal transitions and outputs produced by the
component models (i.e. the generator, transducer, load
balancer, and processors) and then computing p as
described in sections 3 and 4. Each run is begun with the
same seed for the random number generator, so the state,
input, and output trajectories will be fixed relative to the
observation interval.
The configuration and observation intervals for each
experiment were as follows. Each configuration was run
with four different observation intervals: 500, 3500, 6500,
and 9500. The simulation algorithm used on each
computer is described in table 1.
The optimistic
algorithms are risk-free optimistic in all cases.

5.2. Results
The experiments were performed using nine Sun
SPARC Ultra 5/10 Workstations connected by a IOOMbps
local area network. Each workstation has 64MB of
memory and is running SunOS 5.7. To generate a baseline
for determining speedup and an approximation of p, the
model was simulated using a single computer DEVS
simulation engine and the execution time and number of
internal transitions and outputs observed. The one-sided,
95% confidence interval for the single computer execution
was less than 0.8 seconds in all cases, with means of 17,
121.6, 226, and 326.6 seconds for the respective
observation intervals.
Table 2 shows the measured experimental values of p-’
for the system as a whole. It can be seen that p-’ remains
reasonably constant, decreasing by about 6.4 percent
overall. Using Amdahl’s Law, we could expect a change
in speedup of about 3.06 x IO”.
Table 2: p-’ versus the observation interval

I Observation
500
3500
6500
9500

I p-’

I

1614.07
1532.79
1523.01
1509.38

Figures 2 and 3 show execution times and speedup for
each simulation experiment configuration. The execution
times shown are mean values, with the one sided 95%
confidence interval being less than 15 seconds in all cases.
The format for the legend is <even computer type>.<odd
computer type>.<computer zero type’
(simulation
configuration). Conservative, risk-free optimistic and
event stepped simulators are denoted by con, rfo, and es
respectively.

5.3. Discussion

Table 1: Assignment of simulation algorithms to
computers
ComDuters 0

interval

I Odd comnuters 1 Even comnuters I
Conservative

Conservative

196

The purely optimistic case appears to be the most
scalable configuration. This matches our intuition, where
the optimistic algorithm was expected to discover
parallelism automatically. It seems safe to assume that, in
the purely optimistic case, the state management overhead
was small compared to the cost of doing useful simulation.
The mixed mode simulators exhibit erratic speedup,
but are generally faster than the single processor case.
The degraded performance of the partially optimistic
configurations can likely be attributed to excessive
optimism (and, consequently, excessive overhead) on the
part of the optimistic simulators. On the other hand, the
conservative/event stepped mixed mode configuration
appears to exhibit fairly constant performance. Since the

Simulator execution time

overhead in this case is fixed, the consistent performance
is not surprising.
Consider the partially optimistic case. As the length of
the simulation run increases, the non-optimistic portions
of the simulation will tend to fall increasingly hrther
behind the optimistic portions. As a result, the optimistic
portions will either run out of memory or require lengthy
rollbacks. In the first case, the optimistic simulator is
dependent on the increasing global clock to allow it to free
up memory needed to do more processing. Consequently,
the optimistic simulator can only advance its local clock at
the same rate as the global clock. In the second case,
every saved state beyond the input timestamp must be
deleted before optimistic processing continues. For the
implementation used in this experiment, this is an O(N)
operation. Hence, the cost of a rollback grows linearly
with the distance of the local clock from the global time of
next event.
The worst of the four partially optimistic cases
involves the generator, transducer, and load balancer
models being simulated optimistically, while the processor
models are attached to conservative simulators. Given the
large amount of memory required to save the state of the
three models, it is not surprising that the overhead in this
case grows faster than when just the processor models are
involved in optimistic processing. To get a sense of the
difference in memory requirements, the processor requires
a job and a Standard Template Library (STL, see [ 11) list
to represent the state of the model. The generator,
transducer, and load balancer models require, collectively,
a 624 byte random number generator, two STL maps, and
an STL list.
It is likely that the performance of the partially
optimistic cases could be substantially improved by
employing a more intelligent state saving strategy.
Soliman studies tradeoffs between periodic state saving
and incremental state saving techniques [7]. Steinman
considers the use of an event horizon to limit the amount
of optimistic processing performed between global time
calculations [SI. In both cases, the speedup of an
optimistic simulator was improved by selecting an
appropriate state management strategy.

350
300
250
U)

U
C

200

0

al
m

150

+

N 6 o n . con. e s '
100

,_, 'rfo.Pfo.eS'

0

*

' r f o . con. eg'
50

2000

4000

6000

8000

1 0 0 0 0 12000

Observation interval

Figure 2: Simulator execution time
Simulator speedup
2

A--,-"-'
U

61

'

'

---!-A.------!-.-

--_----*-/----

7

'con.con.es'
'rfo.rfo.es*
' r f o . con. es'

1.8

1.6
3
U

K

+
0

x

'r f 0 . r t o . r f 0'
' r f o . con. r f o '

-A--

'con.con.rfo'

-8.-

-*

N.

D

al
1.4

-

U

ul
K

x

2000

0

4000

6000

8000

10000 12000

observation interual

Figure 3: Simulator speedup

6.1. Methods
The multiprocessor model was partitioned as before
and simulated using a risk free optimistic algorithm on
each computer. The model parameters were set as before,
with the exception of the time slice length and observation
interval. The observation interval was fixed at 9500 units
of time. The time slice was varied from 0.01 to 0.0001.
The speedup for each case was observed.
In order to relate speedup to p, the model was executed
once on a single computer and the number of internal
transitions, number of outputs, and the execution time was
recorded. Since the state trajectory is fixed relative to the
system's initial state, a single observation for p is
sufficient.

6. Varying parallelism and the speedup of a
sparse system simulation
This experiment seeks to verify the claim concerning
the relationship between speedup and p with respect to a
risk free optimistic algorithm and sparse system. As p
increases, the speedup achieved by the optimistic
algorithm should increase. Similarly, as p decreases, the
optimistic algorithm should exhibit less speedup. A plot
of p versus speedup is expected to show a scaled version
of the plot predicted by Amdahl's Law.

6.2.Results
The experiment was performed using eight 400 MHz
processors installed in an SGI ORIGIN 2000 running the

197

IRIX 6.5 operating system. Each processor has 1024 MB
of memory. One 400 MHz processor is used for the single
computer case.
Table 3 shows the value of p-' for the system as a
whole for each time slice. As the time slice decreases, an
increasing number of internal transitions are required by
the processors to simulate the round robin scheduling of
jobs. Consequently, the sparseness of the system is seen
to increase as the time slice decreases.

25000

I

I

I

I

solo
Risk-free optimistic

,-.

+

VI

it

20000
0
U
VI
e

" 15000
e
E

+
10000
0

+
3

;

Table 3: Processor time slice vs. p-'

5000

X

+ +

e

-I

Time slice

0.00 1
0.0005
0.000 1

I

0

I

0.002

I

15074.17
130146.14
I 150722.00

0.004

I

I

0.006

0.008

0.01

time slice

Figure 4: Simulator execution times

Figures 4 and 5 show execution times and speedup for
each simulation experiment configuration. The one-sided
95% confidence interval for the simulation execution time
is less than 1000 seconds in all cases. The solo line
indicates the execution time and speedup of the single
computer case. Risk-free optimistic line indicates the
execution time and speedup of the risk free optimistic
simulation.

solo
Risk-free optimistic

-

3.5

~

o

3 -

2.5 -

$
U
e

e

2 -

CL

0

1.5

6.3. Discussion

-

1
0

The experimentally determined speedup vs. p and the
speedup vs. p predicted by Amdahl's Law are shown in
figure 6. The observed data was scaled to match the range
of values exhibited by Amdahl's Law. The results of the
experiment show that as p decreases the observed speedup
increases at approximately the predicted rate.
Amdahl's Law does not predict actual speedup figures.
This is not surprising, since Amdahl's Law is a very
simple speedup model. Examples of effects not accounted
for by the law include the cost of communication,
overhead imposed by synchronization, and the effects of
unbalanced computation. The effect of these overheads
can be seen in the speedup data for the experiment. In the
large p cases, the unaccounted for overhead is significant
enough to actually slow down the simulation (speedup <
l)! However, it is significant that, based solely on
properties of the model, it is possible to anticipate a
speedup trend. Specifically, it has been demonstrated that
the performance of a risk free optimistic algorithm
improves as p decreases and worsens as p increases.

I

I

I

0.002

0.004

0.006

0.5

I

0.01

0.008

time slice

Figure 5: Simulator speedup
8
7

I

Q

I

I

I

I

I

Observedcp x 10"3, speedup x 2) --+Expected l/(x+(l-x)/8)

6
$ 5
U

e

G 4

-

3

2
1

I

I

I

I

I

l

o

7. Conclusions
The evidence in not strong enough to draw a general
conclusion.
However, intuition suggests that the
conjecture will hold in the general case.

The speedup figures obtained for the queuing system
simulation provide evidence that conservative and
optimistic algorithms effectively simulate sparse systems.

198

If Amdahl’s Law is a reasonable speedup model for
distributed simulations, it suggests that for conservative
and optimistic algorithms to be effective, the model needs
to be partitioned in such a way that the sub-model residing
on each computer is sparse. If this is not the case, then p
will begin to approach 1 and speedup will be diminished.
It may be reasonable to expect that non-sparse systems
would be effectively simulated by event stepped
algorithms (replacing p with s, where s = 1 - p). This
would match the observations made by Kim and Fujimoto
where a time granule was used to increase the concurrency
of a non-sparse system [4][2].

8. References
[I] Leen Ammeraal, STL f o r C++ Programmers, John
Wiley & Sons, New York, 1997.

[ 2 ] R. M. Fujimoto, “Exploiting Temporal Uncertainty in
Parallel and Distributed Simulations”, 1999 Workshop on
Parallel and Distributed Simulations, May 1999.
[3] Kai Hwang, Advanced Computer Architecture:
Parallelism, Scalabili@, Programmability, McGraw-Hill
Inc., New York, 1993.
[4] Doohwan Kim and Bernard P. Zeigler, “Orders of
Magnitude Speed Up with DEVS Representation and
High Performance Simulation”, Enabling Technology for
Simulation Science, SPIE Aerosense 97, Orlando, Florida,
1997.
[5] C. Liao, A. Motaabbed, D. Kim, B.P. Zeigler,
“Distributed Simulation of Sparse Output DEVS”, in 4th
AI, Simulation and Planning in High Autonomy Systems,
Conference, pp. 171- 177, Gainesville, FL, 1993.
[6] James Nutaro, “Time Management and Interoperability
in Distributed Discrete Event Simulation”, Master’s
Thesis, University of Arizona, Department Electrical and
Compute Engineering, 2000.
[7] Hussam M. Soliman, “On the Selection of the State
Saving Strategy in Time Warp Parallel Simulations”,
Transactions of the Society for Computer Simulation
International, Vol. 16, No. I , pp. 32-36, 1999.
[SI Jeff S. Steinman, “Breathing Time Warp”, Proceedings
of the 7th Workshop on Parallel and Distributed
Simulation, Vol. 23, No. 1 , pp 109-1 18, 1993.
[9] Bernard P. Zeigler, Herbert Praehofer, Tag Con Kim,
Theory of Modeling and Simulation, 2”dedition, Academic
Press, New York, 2000.

199

Proceedings of the 2008 Winter Simulation Conference
S. J. Mason, R. R. Hill, L. Mönch, O. Rose, T. Jefferson, J. W. Fowler eds.

PANEL DISCUSSION: WHAT MAKES GOOD RESEARCH IN MODELING AND SIMULATION:
SUSTAINING THE GROWTH AND VITALITY OF THE M&S DISCIPLINE
Levent Yilmaz
(Panel Chair)
Computer Science and Software Engineering
3116 Shelby Center
Auburn University
Auburn, AL 36849, USA
Paul Davis

Paul A. Fishwick

RAND
1776 Main Street
Santa Monica, CA 90407, USA

Computer & Information Science and Engineering
332 Bldg. CSE
University of Florida
Gainesville, FL 32611, USA

Xiaolin Hu

John A. Miller, Maria Hybinette

Department of Computer Science
34 Peachtree Street, Suite 1438
Georgia State University
Atlanta, GA 30302, USA

Department of Computer Science
415 Boyd Graduate Studies Research Center
University of Georgia
Athens, GA 30602, USA

Tuncer I. Ören

Paul Reynolds

School of Information Technology and Engineering
3116 Shelby Center
University of Ottawa
Ottawa, ON, K1N 6N5 Canada

Department of Computer Science
1214 Olsson Hall
University of Virginia
Charlottesville, Virginia 22904, USA

Hessam Sarjoughian

Andreas Tolk

Computer Science and Engineering
Arizona State University
Tempe, AZ 85281, USA

Engineering Management and Systems Engineering
241 Kaufman Hall, Room
Old Dominion University
Norfolk, VA 23529, USA
main. In light of these emergent needs how can M&S stay
relevant as new critical fields such as global climate
change mitigation, energy restructuring, genetic engineering impacts on society and universal health care emerge
and come into prominence? Surely the systems point of
view and the tools that M&S brings to the table are key to
these new directions. So, what are the critical issues and
challenges facing M&S community in the face of change
and need for rapid discovery?

ABSTRACT
The aim of this panel session is to promote discussion on
emergent challenges and the need for advancements in the
theory, methodology, applications, education in M&S.
The changing landscape in science and engineering (e.g.,
industrial and defense application, medicine, predictive
homeland security, energy and environment) introduces
new types of problems and challenges into the M&S do-

978-1-4244-2708-6/08/$25.00 ©2008 IEEE

677

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
1

INTRODUCTION

able activities and research to sustain the growth, relevance, and vitality of the M&S profession and its discipline.

The members of the panel (organized by Levent Yilmaz)
are: Paul Davis, Paul Fishwick, Xiaolin Hu, John Miller,
Tuncer I. Ören, Paul Reynolds, Hessam Sarjoughian, and
Andreas Tolk. The position statements of the panel members are given separately. However, it is noteworthy that
the visions underlying the statements provided by panel
members share specific common perceptions regarding the
research directions to meet the challenges posed by the
emergent landscape in simulation-based science and engineering. Levent Yilmaz and Paul Reynolds point out the
significance of addressing ambiguity, deep uncertainty,
and unpredictability revolving around the analysis of complex adaptive systems, including but not limited to policy
analysis for disease spread, crisis and disaster management, and conflict resolution. Paul Davis acknowledges
the same problem and suggests exploratory analysis as a
strategy to address various forms of uncertainty. Levent
Yilmaz suggests advancing the theory and methodology of
M&S to tackle the problem by promoting autonomic simulation systems concept as a coherent framework that has
the potential to address ambiguity and uncertainty in creative problem solving.. Another issue raised by Paul Reynolds involves improvement of efficiency of model development, which has been also considered indirectly in the
position statements of Paul Davis and Hessam Sarjoughian
via explicit discussion on the need for the improvement of
composability, as well as transparency, understandability,
and modularity of simulation models.
Both Paul Fishwick and Paul Davis recognize the significance of model variety and the utility of integrated and
compatible families of models that capture different aspects of the same phenomena while enabling alternative
modalities of interaction for different stakeholders.
John Miller considers the interdisciplinary nature of
M&S to examine the pros and cons of providing institutional support to sustain the M&S discipline. He also observes and analyzes various M&S educational programs to
draw a conclusion on the characteristics of successful initiatives. Andreas Tolk's position statement emphasizes the
importance of explicit characterization and documentation
of the core competencies of M&S to sustain it as a discipline. He also points out the importance of avoiding the pitfalls of focusing exclusively on M&S applications, which
may have undesirable consequences of stagnation and decline in relevance of M&S to emergent problems, for
which conventional M&S methodologies may fail to respond effectively. By recognizing the role of education in
supporting the vitality of the M\&S profession and its discipline, Xiaolin Hu, suggest proper enculturation of apprentices or students via strong comprehensive educational
programs based on core foundations to facilitate growth
and continuity of the discipline. Finally, Tuncer I. Ören
lays out a comprehensive framework that delineates desir-

2 COMPUTATIONAL CREATIVITY: TOWARD
AUTONOMIC INTROSPECTIVE SIMULATION
SYSTEMS (LEVENT YILMAZ)
While M&S has been widely used in engineering and
computational sciences to facilitate empirical insight, optimization, prediction, and experimentation, the role of
simulation in supporting creativity received less attention.
The position advocated in this statement is based on the
observation that supporting creative problem solving requires new novel M&S methodologies that take the characteristics of creative problem solving into account. Normative uses of simulation focuses mostly on convergent thinking, while creativity requires what-if analysis that incorporates not only convergent but also divergent thinking by
addressing ambiguity in problem formulation and model
finding, as well as uncertainty and variability. The challenge is to find out means to advance the theory and methodology of M&S to provide computational aids to encourage scientific and engineering creativity. One possible
direction is to understand the role of ambiguity and uncertainty in creative problem solving, while courting creative
success through judicious tradeoffs between constraints
and flexibility.
Figure 1 presents the dimensions depicting possible
problem characteristics in creative problem solving. While
the variability in parameters of interest in simulation exercises is well captured by probability distributions and experiment design, uncertainty that pertains to lack of knowledge in a given problem representation is difficult to
represent using tools of statistical modeling and analysis.
Methods such as Exploratory Modeling (Bankes 1993) and
Exploratory Analysis (Davis 1988) are useful in dealing
with deep uncertainty.
Ambiguity, which is more difficult to address, involves lack of knowledge pertaining to function, structure,
behavior, and use of problem solving algorithms. In this
case, characteristics of a situation in which the set of relevant variables, as well as their functional relationships are
in need of determination.

678

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
tory and transformative modeling and analysis should coexist at this important stage of problem solving (see Figure
2). Ambiguity often occurs due to shifting, ill-defined, and
competing goals. Model variety (see Paul Fishwick's position statement) and exploratory analysis (see Paul Davis'
statement) with such varieties suggest directions to resolve
uncertainties, once ambiguity is reduced to manageable
levels. Here, I would like to focus on ambiguity and suggest that proper advances in M&S has the potential to provide a basis for computational discovery aids to encourage
scientific and engineering creativity, which manifests itself
in various situations, including the following:
•
Figure 1: Problem Characteristics in Problem Solving
•
•

The nature of the problem may change as the simulation unfolds. Initial parameters, as well as
models can be irrelevant under emergent conditions. Relevant models need to be identified and
instantiated to continue exploration.
Our knowledge about the problem being studied
may not be captured by any single model or experiment.
Dealing with ambiguity is paramount to analyzing
complex evolving phenomena. Adaptivity in simulations and scenarios is necessary to deal with
emergent conditions for evolving systems in a
flexible manner.

The lack of knowledge pertaining to the environment,
as well as the problem may invalidate the current model, as
it may not have been designed a priori to cope of with unexpected and unforeseen changes. For such problems, a
simulation system with the following characteristics will
prove useful.
•
•

Figure 2: Transformative Analysis with Autonomic
Introspective Simulation

•

During foresight and problem finding phases of the
broader creative complex problem solving process, ambiguity pervades. During these phases, creative leap is more
likely to ensue through not only searching the problem
space, but also opportunistic alteration of problem representation as new data and observations are gathered. Creative processes often involve a broad idea generation phase.
followed by a selection/evaluation stage. Since creativity
requires novel yet useful solutions, appropriate tradeoffs
between constraints and flexibility is needed over the problem representation to make creative leaps. Hence, explora-

Be aware of not only the status of its own components, but also the resources it uses and the context in which it is used.
Be able to sense, perceive, and understand the environmental conditions to notice change and/or interactive user feedback so that its structure/behavior can be tailored and fine tuned to respond in ways that improve its functions (e.g., novelty).
Be able to plan and facilitate change by altering
its own configuration and status via dynamic update and composition so that (re)selected or emergent goals can be satisfied.

By leveraging the principles and techniques from the
autonomic computing paradigm in conjunction with computational intelligence mechanisms provided by agents,
M&S community may start developing next generation
simulation theories, methodologies, and proof-of-concepts
under the umbrella of Autonomic Simulation Systems to
cope with ambiguity, as well as uncertainty.

679

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
In summary, for problems with inherent ambiguity, the
use of predictive authoritative, as the lack of knowledge or
uncertainty underlying the basic assumptions of these
models makes them useless when they contradict the
knowledge gleaned later in the process. Simulation systems
that are capable of identifying strategies that exhibit robust
behavior across alternative assumptions are likely to provide more confidence in the recommendations inferred
from simulation studies. The effectiveness of such simulation systems will rely heavily on their ability to start behaving robustly across large number of plausible hypotheses, constraints, and propositions, followed by optimization
for a narrow and limited range of conditions constrained by
the knowledge gleaned during the simulation process. Applications for which Autonomic Introspective Simulation
Systems will prove useful include in developing creativity
support tools scientific problem solving, online symbiotic
simulation, real-time decision support in asymmetric and
irregular environments, adaptive experience management
for training, crisis and disaster management, and dynamic
data-driven application systems.

creative simulation research begins with the identification
of impactful issues, the generation of promising ideas to
address them and the rigorous, open exploration of their
validity and utility. My research group has identified model uncertainty and developer efficiency as most important
issues. Results of modern disease spread studies reveal the
effects of uncertainty. Epidemiologists have addressed the
question of government level actions and reactions regarding the spread of smallpox and bird flu. Should a comprehensive vaccination program be initiated? How and to what
degree should infected individuals be isolated, and for how
long? The range of simulationists’ answers to these questions is broad and full of conflict. Elderd (2006) has
shown analytically that just four among hundreds of critical independent variables in these studies induce extreme
sensitivity in model predictions, leading to serious conflict
regarding remedial approaches involving billions of dollars
and millions of people. Our answer? Design and implement the first simulation language to support expression of
a scientist’s understanding of model uncertainty, quantitatively.
Developer efficiency is an equally challenging topic.
The time simulationist's spend adapting their simulations
for reuse, composition and ultimate validation is huge!
Anyone researcher in simulation technology knows many
fellow researchers who spend years of their lives tinkering
with simulations to gain insight and to meet requirements
the modeled phenomena dictate. Can we make this process
any more efficient? My research group has been pursuing
methods for increasing the efficiency of insight acquisition,
but there are many other possible avenues: better simulation languages, better methods of testing simulations, better methods for gathering reliable statistics on model output, and others.
Good and creative research? Opportunities abound in
simulation, partly because the field is so broad and partly
because there are so many unanswered questions! In a jar
full of stones, the best way to empty it is to choose the biggest stones first; then the smaller stones can be removed by
the fistful. Find the big issues, find promising ways to attack them, and charge ahead!

3 UNCERTAINTY AND RISK ANALYSIS IN M&S
(PAUL REYNOLDS)
Simulation has joined theory and experimentation as a
primary avenue of scientific pursuit. Its use is ubiquitous.
It is used to gather insight, validate models and experiments, train humans, support statistical analyses, drive
computer animations, control real time processes, predict
potential outcomes, test and evaluate new systems, and
support what-if analyses, among others. Simulation is
transforming the way researchers think and explore and the
way all of us live. Armed with a broader range of studies,
students, analysts and decision makers gain insight, become better informed and reduce risk in their analyses and
decisions.
Given its breadth of application, widespread impact,
and broad set of implementation issues, simulation shares
many of the research challenges faced in general purpose
software research. There are software engineering questions related to simulation reuse, composition, validity and
interaction with humans. There are systems questions related to its use on various computing platforms. There are
graphics and HCI questions related to its interface with
humans. Beyond mainstream computer science issues,
simulation research is touched by every application field
(many!) in which it is applied. Questions related to uncertainty and risk analysis must be addressed because simulations are used so widely to support decision analysis and
policymaking. There are philosophical questions relating
to meaning and causation that remain unexplored. The set
of questions modern use of simulation engenders is broad
and deep.
As with most quality research paradigms, good and

4 MODULAR AND COMPOSABLE SIMULATION
(PAUL DAVIS)
Among the characteristics of good and creative research in
M&S are:
1. Addressing seriously troublesome problems (rather than
just finding new clever ways to do old things). Such problems include:
• Improving transparency and comprehensibility
(some refer to explanation)
• Improving the ability to conceive, publish, and
transform modularly developed models in a varie-

680

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk

•
•

•

ty of context--without requiring commonality of
environment or even language.
Composability
Improving the ability to construct, test, and use
multiresolution models (in the sense of my definition of MRM), recognizing that such MRM systems will almost always be mere approximations
of reality and that finding the appropriate approximations is fundamental to the effort.
Improving the ability to conduct broad, synoptic,
"exploratory analysis" (in the sense that I use that
term in my own writing routinely in applications.

model their interactions in a separate formalism. Without
accounting for the syntactic and semantic differences of
two disparate modeling formalisms, the mapping of domain knowledge into models is ad-hoc and generally results in simulations that are difficult to develop and use.
Use of multiple modeling and simulation approaches
and tools where each has its own theoretical underpinning
and pragmatic utilization require research in visual modeling, domain-specific model libraries, and persistence model storage. Overcoming these challenges can result in hybrid modeling and simulation environments that are robust,
semantically expressive, computationally scalable, and
accessible to relatively large classes of users. We contend
that no framework or tool exists that offers visual development of generic formal models, customizable domainspecific modeling, automatic simulation code generation,
flexible experimentation, run-time visualization of simulations, and maintenance of consistency in model repositories. With the realizations of such end-to-end capabilities
for mono- and super-formalisms, researchers should be
better positioned to tackle these kinds of problems that are
radically more challenging for meta- and poly-formalism
modeling and simulation frameworks.

2. Moving toward more universal sets of representations
and automated translations, and doing so in a way that ties
into developments by people who may think of themselves
as software engineers or design engineers, rather than
M&S practitioners.
3. Moving toward foundational discussions and related non
proprietary tools that can be understood and used by multiple disciplines.
5
COMPOSABILITY and MULTI-FORMALISM
(HESSAM SARJOUGHIAN)

6 MODEL VARIETY AND HUMAN INTERACTION (PAUL FISHWICK)

Simulation tools are commonly used for engineering and
operations of numerous systems. Most successful simulations in use can be considered to be grounded in monolithic
frameworks where structures and behaviors of a system are
formulated using a single theory with various extensions.
In many other instances, some combination of simulation
interoperability with partial model composability is used.
Yet, in other situations, concepts and methods from computer science, and in particular software engineering, have
resulted in what might be called pseudo modeling and simulation notions and tools.
The above difficulties have led researchers in pursuit
of developing frameworks that can characterize complexity
and scale of systems using disparate modeling formalisms
(Davis and Anderson 2004; Mosterman and Vangheluwe
2002). Toward this goal, hybrid frameworks classified as
super-, meta-, and poly- formalism have been proposed
(Sarjoughian 2006). This is because practitioners, engineers, and scientists find use of a mono-formalism framework unacceptable. A major reason is that a single formalism severely limits conceptualization of model abstractions
that are best represented in different formalisms. Lack of
composable model semantics across disparate formalisms
is a root cause for the difficulties that are experienced
across modeling and simulation lifecycle phases including
conceptual modeling, formal model specification, simulation model implementation, simulation execution, and experimentation. For example, for simulating a realistic
supply-chain system, it is crucial to not only use discrete
event simulation and optimization models, but also to

People build models to better understand the environment
around them. Our discipline of modeling and simulation
(M&S) is centered on the design, implementation, and
analysis of these models. Some of the models may be of
the physical scale variety, but most of them are digital and
require implementation in the form of a computer program.
We have a wide variety of model types, shapes, forms, and
formalisms. Two problems that arise as part of our research
are:
(1) our discipline is not as widely noticed and as integrated as we would prefer, and
(2) our efforts are somewhat splintered across different groups who prefer their own models and methods. Even though we may be experts in modeling as a discipline, many groups do not look to us
for guidance; they have their own domain experts
and modeling techniques.
If one considers the role of modeling in software engineering, for instance, there are significant movements in standardizing models (e.g., Unified Modeling Language). Is
there a role for M&S in this activity? Who decides which
model types are to be used in an implementation - model
developers or participants who engage the models?
We may respond to these queries by using the timetested approach of specifying requirements and then building a model to meet those requirements. This approach is
still valid, but we must be wary of building a set of indi-

681

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
vidual models that have narrow audiences. If one considers
how we interact with a web browser, we rapidly obtain
information at a variety of different levels for a topic--all
the way from a sentence down to photos, videos, and possibly equations and flowcharts. To take a type of model:
hurricane catastrophe risk assessment, there are many
people who are interested in these models. The general
public may wish to see a few sentences in natural language, or perhaps an artifact that grabs their attention as
might be found in a science or art museum. A risk analyst
may wish to view a high level flowchart, and the scientist
wants to see the equations and FORTRAN code. The goal
should be to link all of these models together not only at
the formal level, but at the level of human interaction.
Teaching a sixth grader how a hurricane causes damage
may involve a different look-and-feel to the model than
teaching a university student, or a politician. Different media may be appropriate -- videos, games, or multi-user virtual environments depending in which space the consumers
are located.
It may be tempting at first to imagine building separate
environments for separate people, but at some point we
need to connect these different ways together to empower
the user to choose the model type that they want. This issue
of "who chooses" has been active in the education community for decades--is it best to learn through top-down
lecturing or through bottom-up constructive exploration?
The same types of questions need to be asked within an
M&S framework: can we build models that connect together to support both types of learners without enforcing
one modeling paradigm?

various engineering disciplines are good examples. The
history of Computer Science (CS) as a discipline is different; it emerged as an interdisciplinary field at the intersection of Mathematics and Engineering. Its birth was accelerated by the Second World War, during which significant
advances were made outside universities (e.g., Britain’s
work to crack the Enigma led by Alan Turing). After the
war, universities recognized the importance of this new
area, but struggled to find a place for it. At some institutions, CS was initially a component of Mathematics; at
others it was a branch of Engineering (consider, for instance the departments of EECS at the University of California, Berkeley and the Massachusetts Institute of Technology).
It is worthwhile to consider the evolution of CS at
Carnegie Mellon University (CMU) in particular. Their
CS Department was established in 1965 as a part of the
College of Science. The Department was eventually elevated to the status of “School” in 1988, which enabled it to
contain its own component departments. Today, CMU’s
School of Computer Science houses several departments
and PhD programs including: Computer Science, Robotics,
HCI, Machine Learning, Language Technologies, and
Software Engineering. The CMU model has been followed
also at the Georgia Institute of Technology, which established a College of Computing in 1990. They now have
three departments: CS, Interactive Computing, and Computational Science and Engineering. The point is that the
case can be made that CS is spawning new disciplines with
staying power.
In a manner similar to CS, Modeling and Simulation
(M&S) has been highly interdisciplinary from its formation
in the 1940's. The primary fields that support the development of new modeling and analysis techniques and methodologies are Computer Science, Industrial Engineering,
Management Science, Statistics, Mathematics and Physics.
Of course, many scientific and engineering disciplines, that
are too numerous to list, utilize, customize and extend the
techniques and methodologies developed by the primary
disciplines. Given this highly interdisciplinary nature,
there are likely to be advantages and disadvantages for
turning M&S into its own academic discipline. Perhaps it
should remain principally interdisciplinary and supported
by multiple traditional academic departments. One could
argue that too much of a home department mentality might
reduce the vitality of the field, since without the influx of
new challenges and problems to address the field may
stagnate.
Once thought of as a method of last resort, simulation
models are playing an increasing role due to the fact that
problems are often too complex to solve by other means
and that the capabilities of today's computer hardware
along with advances in software tools and techniques for
modeling and simulation have increased dramatically. The
new experts are likely to be computer models (e.g., weath-

7 SHOULD M&S BECOME ITS OWN DISCIPLINE (JOHN MILLER, MARIA HYBINETTE)
What is necessary for a field to become a discipline? All
academic disciplines of academic study arise from the
study of philosophy established 2500 years ago by Socrates, his pupil Plato, and the father of metaphysics Aristotle. At that time one person could feasibly master all
scientific knowledge. But as science grew, the corpus of
knowledge expanded, and new disciplines were recognized. The rate at which new disciplines emerge corresponds roughly to the rate at which the body of scientific
knowledge grows. And the breadth and depth of each discipline corresponds roughly to what one very intelligent
person might master and contribute to in a career. Another
way to view it is that a definition of “discipline” serves to
classify the many possible directions of intellectual pursuit
into reasonably sized chunks. Today we recognize many
disciplines including physics, biology, psychology, and
chemistry, all with their origins in Greek philosophy.
Most new disciplines established over the last few
hundred years have been “incubated” by another discipline,
from which they split upon reaching critical mass. The

682

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
er models, climate change models, economic models, network traffic models, air traffic control models, systems
biology models, etc.).
Since M&S is growing in importance (Swain, 2005),
there will be an increasing need for students with better
training in M&S. In the past, students would get a degree
in, for example, Computer Science with maybe one or two
courses in M&S. They should have more than two, but
more importantly, they should have the flexibility in their
program to take courses from other departments to broaden
their knowledge of application domains. At this point in
the evolution of M&S, it may be most beneficial to establish degree programs (Crosbie 2000; Szcerbicka et al.
2000; Sarjoughian et al. 2004) as well as research groups
and centers within one or more traditional academic departments. Indeed, this is the recent trend that has been
taken place. The list below indicates universities that offer
such programs.
•
•
•
•
•

•

which of these dozen courses are offered by the six graduate programs listed above. In addition to M&S courses,
students should take courses from application areas within
science, engineering or business.
Increasing the number of graduate programs is critical
in establishing M&S at least as its own sub-discipline, yet
much more is necessary. At the same time, more research
centers and institutes should be established to link together
researchers in multiple departments interested in M&S, as
well as, create a presence that university administrations
and funding agencies will become aware of. There are
several examples of such centers listed in (Miller and Hybinette 2008) that could serve as models for other universities to follow. These research centers would be in a position to seek funding in two ways. They could seek funding
for basic research in M&S and as components of larger
grants in science and engineering. As these centers gain
greater visibility, direct funding of M&S research will likely increase. Many of these centers have also created graduate programs in M&S. The two go hand in hand.
Beyond increasing the number and quality of graduate
programs in M&S and research centers focusing on M&S,
there are several additional factors of importance. The
discrete event and continuous systems simulation communities should become more integrated. These communities
have evolved largely independently, but at some point
should combine forces. As has been mentioned by other
authors, M&S needs more textbooks to be written. Of
course, this is slowed by the chicken and egg problem.
However, as more graduate programs are added, this situation should resolve itself. Modeling should be elevated.
Model sharing should occur at an implementation independent, high level, much like algorithms are shared. Models
should be described semantically to enhance understanding, ability to discover and compare models, improve interoperability and composability (Miller and Baramidze
2005). Finally, M&S should continue to broaden its scope
and keep looking for new approaches (e.g., Web-Based
Simulation, Agent-Based Simulation, Multi-Scale Modeling) as well as new application areas.
In conclusion, we believe that M&S is beginning to
make the transition from a field to a discipline and that the
trends of introducing new graduate programs and research
centers should continue. In the not too distant future, the
question of establishing mainstream academic departments
of Modeling and Simulation should be seriously considered. Even then, its highly interdisciplinary nature should
not be sacrificed.

Modeling and Simulation Graduate Program, Old
Dominion University.
Interdisciplinary Graduate Programs in Modeling and
Simulation, University of Central Florida.
Master of Engineering in Modeling & Simulation
Degree, Arizona State University.
Graduate Certificate Program in Modeling and
Simulation, University of Alabama, Huntsville.
Graduate Education Program in Defense Modeling
and Simulation, Modeling, Virtual Environments,
and Simulation (MOVES) Institute, Naval
Postgraduate School (NPS).
Simulation Science, California State University,
Chico (CSUC).

There are also programs in Computational Science that
have a heavy emphasis on Modeling and Simulation, such
as the Computational Science and Engineering program at
the Georgia Institute of Technology.
Looking at the course offerings and core requirements
in these programs, the following are common to most of
them: an introductory course in Modeling and Simulation,
a course in Discrete Event Simulation and a course in Continuous Systems Simulation. Since some of the entering
graduate students may not have the prerequisites needed,
many programs offer a migration class to quickly boost
their background knowledge of Mathematics and Statistics
as well as, in some instances, Programming. Several more
advanced courses are also offered covering topics such as
Interactive Simulation & Training, Stochastic Systems
Modeling, Analysis of Simulation Experiments,
HCI/Visualization in M&S, Computer Animation, Virtual
Environments and Gaming, Parallel and Distributed Simulation and a project course. A course topical area cross
graduate program matrix is given on the following Web
page (Miller and Hybinette 2008). The matrix indicates

8

M&S EDUCATION (XIAOLIN HU)

An equally important issue to modeling and simulation
(M&S) research is the educational aspect of M&S. Successful education of M&S will not only produce the work
forces needed for M&S research, but also help consolidate

683

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
the state of knowledge in M&S and thus benefit the M&S
community in general. In the following discussion, instead
of responding to the question of “what makes good and
creative M&S research”, I consider a different, but certainly related topic: what makes good and effective M&S education.
It is believed that good M&S education should promote the general theory of M&S while in the meantime
provide hands on experience on M&S activities. This contrasts to the misled view of many that M&S learning is
simply to learn how to use fancy M&S tools (unfortunately
this view is shared by many professionals too, who view
M&S as a tool instead of a discipline by itself). In my opinion, such a “tool-driven” approach of teaching/learning
put M&S education in a wrong focus and downplays the
role of M&S as a discipline. Most simulation tools are domains specific, designed for solving problems outside the
M&S field. For good reasons, they hide details about core
M&S concepts such as model abstraction and simulation
protocol. Thus while these tools are certainly useful for
specific domains, they are ineffective, if not act against, for
learning the general theory of M&S itself. It is argued that
any operational environment that supports learning of the
M&S concepts needs to build on top of a general modeling
formalism and simulation framework, such as the Discrete
Event System Specification (DEVS) (Zeigler et al. 2000).
A framework like that can embody the general theory of
M&S as well as leverage students to focus on the core activities of M&S using its operational environment. This is
analogous to the role that UML (and its supporting environment) plays in learning of software engineering concepts. The two aspects of M&S research and education,
are closely related and depend on each other. It is hoped
that in this panel discussion, opinions about M&S research
will inspire ideas on M&S education, and the other way
around.

2000). Ören is and has been actively supported by many
researchers and educators in collecting contributions to a
Body of Knowledge (BoK) of M&S (Ören 2005a; 2005b).
Banks summarized a series of panel discussions organized
as Academic Nights during the Simulation Interoperability
Workshops (Banks 2006a; 2006b). This list of references is
neither complete nor exclusive, but it gives an overview
what activities are currently conducted in the M&S community.
What are the implications for M&S research, and what
defines “good” M&S research? Good M&S Research must
be rooted in and contribute to the BoK for M&S. If this is
not the case, it is not M&S research but the application of
M&S means in support of the research conducted in the
applying domain, such as biology, sociology, military operations research, etc. The situation is comparable with
computer science several years ago. With the rise of affordable work stations and personal computers, computation on large scale suddenly became an option for all disciplines. But the applications were not limited to the academic community. Computers were applied everywhere.
This drove the need for computer experts, engineers, programmers, etc. But academia was not prepared to support
the market. Many “professionals” of these days were not
academically prepared, but “self-declared” experts, often
only “one step beyond” the customers. One of the results
was the “software crisis” of computer science. Only after
the software crisis, the problem was publicly recognized,
the BoK for computer science in general and software engineering in particular was adapted, and formal education
became more or less a requirement for “serious” computer
science applications. Today, nobody would call someone
who programmed a handful of Java applications a computer expert or computer scientist.
However, we are facing the same challenges in M&S
again. The formal education in the core competencies of
M&S is not required to be accepted as an “M&S expert.”
The danger is that we will run into an “M&S crisis” comparable to the software crisis soon. Such as stable software
requires software engineering, stable and reliable M&S
applications require in-depth knowledge in modeling – the
purposeful abstraction of reality resulting in a formal specification of this conceptualization –, simulation – the execution of a model over time, knowing the computational constraints in particular of distributed execution –, analyses –
including the means of operations research, statistics, experiment planning –, and visualization – including but not
limited to human-machine interfaces, etc. This knowledge
must be enriched by theory and methods for validation and
verification, knowledge representation, composability, and
enabling mathematics.
Only these core competencies of M&S can deliver the
answer to the “grand challenges” of M&S that are not in
the focus of applied M&S research. The challenge is comparable to statistics: while everyone applies probability and

9 UNDERSTANDING THE CORE COMPETENCE
IN M&S (ANDREAS TOLK)
This position paper makes the case that good research in
M&S cannot be limited to application domains. A recognized and accepted Body of Knowledge is needed as the
common ground for research efforts. The areas of modeling, simulation, analysis, visualization, and application
domains have been identified as a potential hub for curriculum development as well as a basis for research. Without
being rooted in such application specific core competencies of M&S, good research is not possible, as the foundation is missing.
M&S is ubiquitous. M&S is applied in a variety of
domains and disciplines that can only be compared with
the use of computers or statistics. Research in all scientific
fields and domains utilize M&S, but what is M&S as a
discipline on its own? Szcerbicka and colleagues started
the formal discussions nearly a decade ago (Szczerbicka

684

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
Table 1: Consolidation of M&S Knowledge

accepted tests, it is the discipline of statistics that needs to
produce the general means. Without formally capturing
and teaching respective core knowledge in M&S, good
M&S research is not possible. Good M&S research must
build the core foundation; otherwise it would just remain
M&S application and the term M&S discipline was not
justified.

• Comprehensive view (Big picture) of all aspects
• M&S Body of Knowledge (M&SBOK):
Systematization of the index and
preparation of a Guideline
• M&S Dictionaries
- As inventory of M&S concepts
- As systematic inventory of M&S concepts
(ontology-based M&S dictionary)

10
A FRAMEWORK FOR DESIRABLE ACTIVITIES AND RESEARCH IN M&S (TUNCER I. ÖREN)
I would like to start with the last sentence of another article
Ören (2002): "Progress in any area is not possible by keeping the status quo –no matter how advanced it can be."
Indeed M&S is already very advanced and is also an important–sometimes vital– enabling technology for many
areas. However, we ought to continue advancing it for several reasons: (1) Consolidate and disseminate pertinent
knowledge about M&S. (2) Assure requirements of professionalism. (3) Advance M&S science, methodology, and
technology to continue solving problems in hundreds of
traditional application areas and challenging new areas.
Position statements in this panel session cast light on
several important issues. The framework offered here
(which can be elaborated on and refined) may be useful in
consolidating and refining the views expressed in this Panel and elsewhere to advance M&S, and also systematically
monitoring the progress. Some activities would require
R&D and some others would be other types of professional
and dedicated activities.
My (over 100) publications, presentations, and other
activities on advanced methodologies and normative views
for advancements of M&S are listed at (Ören-normative
views).
In the sequel, Table 1 summarizes the activities for the
consolidation of M&S knowledge, Table 2 outlines dissemination of M&S knowledge, Table 3 is on the requirements of professionalism, Table 4 elaborates on
science, engineering, and technology of M&S, Table 5 is
on trustworthiness, reliability, and quality, and finally,
Table 6 is on more challenging applications.

• Curriculum development and international
standardization for:
- Degree programs (graduate, undergraduate)
- Service programs for other disciplines
- Professional development courses

Table 3: Requirements of Professionalism
• Ethics: widespread adoption & practice of Code
of professional ethics (voluntary and/or require
ment for individuals and/or M&S companies)
• Certification: (voluntary and/or requirement for
individuals and/or M&S companies)
• Maturity levels of individuals and/or companies
Recognition of the M&S Discipline and Profession
• US House resolution 487 (widespread
dissemination: nationally, internationally)
• Simulation Systems Engineering needs to be
promoted
• Consider analogies with history of dentistry &
professional engineering and current status where
non-simulationists doing simulation studies.
• No need to be too humble (consider the use of
the term “model” in art and in M&S as well as
simulation-based engineering)

Table 2: Dissemination of M&S Knowledge
• (National, Regional, International)
e-clearinghouse(s) of:
- Resource libraries
- Funding agencies, funding sources
- Documents of funded research
- Dissertations/theses (as sources of specialists
& specialized knowledge)
- Centralized dissemination of professional
information (events, job market)
- e-encyclopedia, e-books, M&S portals
Table 4: Science, Engineering, and Technology of M&S
Proper system theory based modeling and symbolic

685

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
health responses to bioterrorism and emerging diseases. Proceedings of the National Academy of Science
103, 42, 15693–15697.
Miller A.J. and M. Hybinette 2008. Graduate Programs in
Modeling and Simulation,
http://www.cs.uga.edu/~jam/jsim/DeMO
/wsc08/modsim_programs.html.
Miller A. J. and G. Baramidze 2005. Simulating and the
Semantic Web. Proceedings of the 2005 Winter Simulation Conference, ed. M. E. Kuhl, N. M. Steiger, F.
B. Armstrong, and J. A. Joines, 2371-2377. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Mosterman, P., and H. Vangheluwe 2002. Guest Editorial: Special Issue on Computer Automated Multiparadigm Modeling, ACM Transactions on Modeling
and Computer Simulation 12 (4): 249-255.
Ören,T.I. (normative-views). List of Publications, Presentations and Other Activities on Normative
Views for Advancements of M&S.
http://www.site.uottawa.ca/~oren/pub
sList/MS-advanced.pdf .
Ören, T.I. 2002. SCS and Simulation: Fifty Years of
Progress, 50th Anniversary Issue, Modeling and
Simulation, 1(3) (July-September), 32-33.
Ören, T.I. 2005a. Maturing Phase of the Modeling and
Simulation Discipline. Proceedings Asian Simulation
Conference 2005, International Academic Publishers World Publishing Corporation, Beijing, P.R. China,
pp. 72-85
Ören, T.I. 2005b. Toward the Body of Knowledge of
Modeling and Simulation (M&SBOK). Proceedings
Interservice/Industry Training, Simulation Conference,
Paper 2025, pp. 1-19.
Sarjoughian, H.S. 2006. Model Composability. In Proceedings of the 2006 Winter Simulation Conference,
ed. L. F. Perrone, F. P. Wieland, L. Liu, B. G. Lawson,
D. M. Nicol, and R. M. Fujimoto, 149-158, Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Sarjoughian H., J. Cochran, J. Collofello, J. Goss, B. Zeigler 2004. Proceedings of the Summer Simulation Conference, San Jose, California (July 2004)
Szczerbicka, H., Banks, J., Rogers, R.V., Oren, T.I., Sarjoughian, H.S.; Zeigler, B.P. 2000. Conceptions of
curriculum for simulation education. In Proceedings of
the 2000 Winter Simulation Conference, ed. J. A.
Joines, R. R. Barton, K. Kang, and P. A. Fishwick,
1635 – 1644. Piscataway, New Jersey: Institute of
Electrical and Electronics Engineers, Inc.
Swain J. 2005. Gaming Reality: Biennial Survey of Discrete-Event Simulation Software Tools. OR/MS Today, 32(6): 27-32.

model processing based for simulation of complex
systems
Proper simulation paradigms and practices
• Model-based simulation
• Mixed formalism simulation
• Multisimulation
• Concurrent simulation
• Holonic agent simulation for
- Simulation of coopetition (cooperative
competition)
• Specification languages / environments for
interoperability
• Computer-Aided Problem Solving Environments
with M&S abilities
Advanced modeling formalisms & technologies
• with abstraction & descriptive power
• Theory-based modeling formalisms for:
- Variable structure models
- Multimodels
- Multiaspect, multistage, multiperspective,
multiresolution, multiparadigm, and
evolutionary modeling
Advanced experimentation / scenario generation
• Automation of design & execution of
Experiments as well as analyses of results
REFERENCES
Banks, C. 2006a. Is Modeling and Simulation a Discipline,
Academic Night at the Spring Simulation Interoperability Workshop, Simulation Technology Magazine
9(1),
http://www.sisostds.org/webletter/siso/iss_107/art_670
.htm
Banks, C. 2006b. Implementing a Multi-Disciplinary Approach to Modeling & Simulation in Education and
Research, Academic Night at the Spring Simulation
Interoperability Workshop, Simulation Technology
Magazine 9(2),
http://www.sisostds.org/webletter/siso/iss_108/art_698
.htm
Crosbie, R. 2000. A Model Curriculum in Modeling and
Simulation: Do We Need It? Can We Do It?, In Proceedings of the 2000 Winter Simulation Conference,
ed. J. A. Joines, R. R. Barton, K. Kang, and P. A.
Fishwick, 1666-1668. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Davis, P. K. and R. H. Anderson 2004. Improving the
Composability of DoD Models and Simulations.
JDMS: Journal of Defense Modeling and Simulation:
Applications, Methodology, Technology 1(1): 5-17.
Elderd, B.D., Dukic, V.M., and Dwyer, G. 2006. Uncertainty in predictions of disease spread and public
686

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
Zeigler P. B., T. G. Kim, and H. Preahofer. 2000. Theory
of Modeling and Simulation, New York, NY, Academic Press.

bers of the Society for Modeling Simulation International
to its board of directors for the 2006-2008 biennium. He is
an advisor to Workforce Development Committee of the
Alabama Modeling and Simulation Council. His email and
web addresses are yilmaz@auburn.edu and
http://www.eng.auburn.edu/~yilmaz

Table 5: Trustworthiness, reliability, and quality
• Built-in reliability assurance
prior to traditional validation & verification

PAUL K. DAVIS is a Principal Researcher at RAND and
Professor of Policy Analysis in the Pardee RAND Graduate School. His research involves strategic planning, decision-making, defense planning, counterterrorism, and
advanced methods for analysis and modeling--most notably "exploratory analysis under uncertainty" and "multiresolution modeling". Before joining RAND in 1981, he had
served as a senior executive in the Office of the Secretary
of Defense. During his time at RAND, Dr. Davis has published well over a hundred peer-reviewed papers and
books; he has also served on numerous panels of the National Research Council and Defense Science Board. Dr.
Davis received a B.S. from the U. of Michigan (Ann Arbor) and a Ph.D. in Chemical Physics from M.I.T.

• Proper computer-aided & computer processable
documentation of simulation studies
(including assumptions)
• Taming, monitoring, and assuring software
agents in order for agents to behave in a trustworthy way
Table 6: Challenging M&S Applications
Reduce time of simulation (from conception to generation of alternatives for decision makers)
Formulate new success metrics
Applications
• Use of simulation for machine learning
• Use of switchable understanding in simulation
(to avoid dogmatic thinking & to assure emotional
intelligence)
• Proactive system simulation
• Introspective system simulation
• Simulation of emergent phenomena
• Conflict management simulation
• Security training simulation
• Personality, emotions, and cultural
backgrounds in simulation

PAUL A. FISHWICK is Professor of Computer and Information Science & Engineering at the University of Florida. He is also Director of the Digital Arts and Sciences
Curricula. Fishwick is actively involved with research in
model design and methodology for computer simulation,
and served as General Chair of WSC 2000. His currently
funded research is constructing a simulation environment
for supporting Chinese cultural immersion in the Second
Life virtual environment, and a visualization study of fluid
flow for scalar temperature fields over two airframes. Recently published books include the comprehensive CRC
Handbook on Dynamic System Modeling and Aesthetic
Computing (MIT Press). Fishwick is also active in setting
and maintaining software engineering standards for the
Florida Hurricane Commission on Hurricane Loss Projection Methodology.

AUTHOR BIOGRAPHIES
LEVENT YILMAZ is Associate Professor of Computer
Science and Software Engineering at Auburn University.
He received the B.S. degree in Computer Engineering and
Information Sciences from Bilkent University and the M.S.
and PhD. degrees from Virginia Tech. His research focuses on Agent-directed Simulation and its applications in (1)
advancing the theory and methodology of modeling and
simulation via novel formalisms (e.g., autonomic introspective simulation, symbiotic adaptive multisimulation)
and their use in decision support systems engineering and
(2) socio-technical/cognitive/cultural systems (e.g., sociology of software engineering/science, systems models of
creativity and innovation, software project and process
simulation, conflict and peace studies). Dr. Yilmaz is a
member of ACM, IEEE Computer Society, Society for
Computer Simulation International (SCS), and Upsilon Pi
Epsilon. Dr. Yilmaz has held industrial positions at Trident Systems Incorporated. He has served as the secretary
of SCS during 2004-2006, and he is elected by the mem-

XIAOLIN HU is an Assistant Professor in the Computer
Science Department at Georgia State University, Atlanta,
Georgia. He received his Ph.D. degree from the University
of Arizona, M.S. degree from Chinese Academy of
Sciences, and B.S. degree from Beijing Institute of Technology in 2004, 1999, and 1996 respectively. His research
interests include modeling and simulation, and their applications to complex system design, multi-agent/multi-robot
systems, and ecological and biological problems. He has
served as program chairs for four international conferences/symposiums in the field of modeling and simulation, and guest editor for Simulation: Transaction of The
Society for Modeling and Simulation International. He has
edited two conference proceedings books, and published
over thirty journal/conference papers. His research is cur-

687

Yilmaz, Davis, Fishwick, Hu, Hybinette, Miller, Ören, Reynolds, Sarjoughian, and Tolk
rently supported by National Science Foundation (NSF)
and Georgia State University internal grants.

AFCEA, and NATO; and is recognized by IBM Canada as
a Pioneer of Computing in Canada (2005).
http://www.site.uottawa.ca/~oren/

JOHN A. MILLER is a Professor of Computer Science at
the University of Georgia and has also been the Graduate
Coordinator for the department for 9 years. His research
interests include database systems, simulation, bioinformatics and Web services. Dr. Miller received the B.S. degree in Applied Mathematics from Northwestern University in 1980 and the M.S. and Ph.D. in Information and
Computer Science from the Georgia Institute of Technology in 1982 and 1986, respectively. During his undergraduate education, he worked as a programmer at the Princeton Plasma Physics Laboratory. Dr. Miller is the author of
over 125 technical papers in the areas of database, simulation, bioinformatics and Web services. He has been active
in the organizational structures of research conferences in
all these areas. He has served in positions from Track
Coordinator to Publications Chair to General Chair of the
following conferences: Annual Simulation Symposium
(ANSS), Winter Simulation Conference (WSC), Workshop
on Research Issues in Data Engineering (RIDE), NSF
Workshop on Workflow and Process Automation in Information Systems, and Conference on Industrial & Engineering Applications of Artificial Intelligence and Expert
Systems (IEA/AIE). He is an Associate Editor for ACM
Transactions on Modeling and Computer Simulation and
IEEE Transactions on Systems, Man and Cybernetics as
well as an Editorial Board Member for Journal of Simulation and International Journal of Simulation and Process
Modeling. In addition, he has been a Guest Editor for the
International Journal in Computer Simulation and IEEE
Potentials.

PAUL F. REYNOLDS, Jr. joined the Computer Science
faculty at the University of Virginia in 1980. His primary
research activities are in simulation technology, parallel
and distributed computing and computing technologies for
the visually impaired. He has been awarded over 60 grants
while at the University of Virginia, including a recent NSF
ITR (funding rate < 4%). He has conducted research sponsored by DARPA, the National Science Foundation, DUSA (OR), the National Institute for Science and Technology, the Defense Modeling and Simulation Office, Virginia
Center for Innovative Technology and numerous industries. His current research includes simulation issues -simulation coercion, multi-resolution modeling and time
management-- concurrency control in ordered networks, in
particular Isotach Timing, and inexpensive technologies
for the blind. He leads the interdisciplinary Modeling and
Simulation Technology Research Initiative (MaSTRI) at
UVa. He has served in numerous leadership roles in modeling and simulation within the Department of Defense,
including initial setup and later oversight roles for the Joint
National Test Facility in Colorado Springs, and the conceptualization and design of the DoD High Level Architecture for Modeling and Simulation (standard: IEEE 1516).
He is a staff expert in Modeling and Simulation for the
National Ground Intelligence Center in Charlottesville. He
is the Program Chair for PADS 2009.
HESSAM SARJOUGHIAN is Assistant Professor of
School of Computing and Informatics at ASU and CoDirector of the Arizona Center for Integrative Modeling
and Simulation. His research focuses on multi-formalism
modeling, collaborative and visual modeling, distributed
simulation, and software architecture. He can be contacted
at <sarjoughian@asu.edu>.

TUNCER ÖREN is a professor emeritus of Computer
Science at the University of Ottawa. His current research
activities include (1) advanced M&S methodologies such
as: multimodels, multisimulation (to allow simultaneous
simulation of several aspects of systems), and emergence;
(2) agent-directed simulation; (3) cognitive simulation (including simulation of human behavior by fuzzy agents,
agents with dynamic personality and emotions, agents with
perception, anticipation, and understanding abilities); and
(4) reliability and quality assurance in M&S and user/system interfaces. He has also contributed in Ethics in
simulation as the lead author of the Code of Professional
Ethics for Simulationists, M&S Body of Knowledge, and
multilingual M&S dictionaries. He is the founding director
of the M&SNet of SCS. He has over 350 publications
(some translated in Chinese, German and Turkish) and has
been active in over 370 conferences and seminars held in
30 countries. He received "Information Age Award” from
the Turkish Ministry of Culture (1991), Distinguished Service Award from SCS (2006) and plaques and certificates
of appreciation from organizations including ACM, AECL,

ANDREAS TOLK is Associate Professor for Engineering
Management and Systems Engineering of Old Dominion
University's Modeling, Simulation, and Visualization Faculty. He is also a Senior Research Scientist at the Virginia
Modeling Analysis and Simulation Center (VMASC). He
holds a M.S. in Computer Science (1988) and a Ph.D. in
Computer Science and Applied Operations Research
(1995), both from the University of the Federal Armed
Forces of Germany in Munich. He is a member of FAS,
SCS
and
SISO.
His
e-mail
address
is
<atolk@odu.edu>.

688

DEVS-Suite: A Simulator Supporting Visual Experimentation Design and
Behavior Monitoring
Sungung Kim
Hessam S. Sarjoughian
Vignesh Elamvazhuthi
Arizona Center for Integrative Modeling & Simulation
Dept. of Computer Science and Engineering
Arizona State University
Tempe, AZ 85281, USA

Keywords: DEVSJAVA, DEVS-Suite, experimental design, simulation monitoring, visual complexity
Abstract
Complexity associated with the design of experiments
for simulation models can be reduced through visualization.
DEVS-Suite, a new generation of the DEVS Tracking Environment which itself was extended from DEVSJAVA, supports visual design of experiments and introduces simulation data visualization. Data generated by the selected
models can be collected dynamically and displayed as timebased trajectories. These capabilities complement animation
of DEVS model components and their interactions. A service-oriented software system is modeled to illustrate the
novel modeling features for DEVS simulations. Another example is developed in Ptolemy II and SimEvents to show
the reduced visual complexity afforded by DEVS-Suite.
1

INTRODUCTION

Modeling is commonly used for understanding, engineering, and operations of systems. Simulation tools such as
DEVSJAVA [1], SimEvent [8], and Ptolemy [6] can be
used for modeling complex discrete systems. Each provides
its own unique approach for model specification and simulation visualization. A key capability for simulation studies is
automated design of experiments such that models can be
easily chosen and their simulated dynamics monitored at
run-time [2].
Considering the DEVS modeling framework [16], a variety of simulators have been implemented for it in different
programming languages. They are used for simulating discrete event models, and in some cases continuous and discrete, across various application domains. Tools such as
CD++ [14], DEVSJAVA, and DEVS Tracking Environment
(DTE) [13] conform either to parallel or classic DEVS formalisms and support visualization of pre-built models. For
example, DEVSJAVA, a Java-based implementation of Parallel DEVS, supports visualization of hierarchical model

components and animation of message exchanges among
atomic and coupled model components. However, it does
not support on-the-fly selection and monitoring of model
components and displaying data trajectories at run-time.
Similar to many other tools, models are embellished with
code to gather data of interest and are made available to
console or written to external files for post processing.
A key advantage of simulators such as SimEvents and
Ptolemy II is support for viewing simulation data as time
trajectories. SimEvents, which is an extension of Simulink,
[8] supports hierarchical activity-based models from prebuilt components including queues, servers, switches, gates,
timers, and generators for entities, events, and signals.
These components can be visually composed to develop
bigger models. Model parameters such as congestion, resource contention and processing delays can be monitored at
run-time. The pre-built, strongly typed monitoring components such as Signal Scope can be used to plot events or the
states of the models along time axes. Monitoring a model’s
input/output ports may require several plotters.
Ptolemy II supports different types of models (e.g., continuous and discrete) using a graphical user interface called
Vergil. Pre-existing code with information about their
couplings allows automatic code generation in XML format.
Users can visually synthesize hierarchical models from prebuilt components which have symbolic representations. The
animation feature displays the models that are active at different instances of time. The simulation results can be monitored with plotters which are part of the model layout. Similar to SimEvents, plotters in Ptolemy II are strongly typed.
Given the importance of selecting components of a hierarchical model with support for monitoring and visualizing
inputs and outputs of any component, we developed the
DEVS-Suite simulator. We begin in Section 2 with a review
of the DEVSJAVA and DEVS Tracking Environment simulators and the TimeView, a module that supports plotting
input, output, and state trajectories. In Section 3, we describe the DEVS-Suite simulator and use it to model a ser-

vice-oriented computing system. In Section 4, we consider a
simple assembly line system. We model this system in
DEVS-Suite, Ptolemy II, and SimEvents and then compare
them to show their differences for design of experiments
and monitoring of simulation results with respect to a visual
complexity metric. In Section 5, we summarize and discuss
future research.
2

BACKGROUND

As we mentioned in the previous section, a number of
simulators have been built for simulating modular, hierarchical DEVS models. Among these, we consider
DEVSJAVA and DEVS Tracking Environment (DTE) since
they are used for developing the DEVS-Suite simulator [1].
Here, we review the basic concepts for the DEVSJAVA and
DTE. The DEVS-Suite simulator offers new and integrated
features that can help users devise experiments and conduct
simulations more easily. There is no need to delve into the
details of the simulator protocol design and implementation
since the simulation engine used in DEVS-Suite is the same
as the one used in DEVSJAVA and DTE.
2.1

DEVSJAVA

A commonly used tool for simulating parallel DEVS
models is DEVSJAVA. It displays a view of the entire hierarchy of the simulation model using components-withincomponents style. For any coupled model, one or more
messages simultaneously travel along coupling paths that
connect atomic to atomic, atomic to coupled, and coupled to
atomic model components [7]. The design of the
DEVSJAVA separates execution control from the tightly
integrated simulator kernel and view. That is, visualization
of models and their animations are supported by simView, a
module that supports user interactions and control of simulation execution. The control supports logical- and soft realtime simulation execution. The states for every model components can be individually examined (viewed) when the
simulator is stopped or paused (i.e., at the end of a simulation cycle). The visualization of the messages is allowed for
the entire model. Furthermore, animation of the messages is
not synchronized against the simulation’s execution speed.
2.2

DEVS Tracking Environment

To support automated design of experiments through
observing inputs and outputs and as applicable common
state variables phase and sigma, the DEVSJAVA Tracking
Environment was developed. Its design is based on the
classical Model-View-Control (MVC) architecture [9]. In
addition to the Model, View, and Control modules, another
module called Façade layer is also used. The data available

in the Façade module can be accessed by the Controller and
the View. With the MFVC architecture, the simulation data
sets can be displayed with one or more views. The data generated by the Model module are obtained during simulation
execution and made available to the View module. The separation between the Model and the View modules and the
presence of the Façade module, model components can be
arbitrarily selected for monitoring and their dynamics displayed at run-time or exported for external use. The encapsulation and modularization through the Façade module significantly reduce dependencies between the Model and the
Control and View modules.
DTE’s key capability is simplifying design of experiments for simulation models. Its graphical user interface allows a user to select model components to be monitored and
thus design experiments in terms of components’ inputs/outputs and state variables. Simulation model data sets,
which include states such as Time of Next Event, Time of
Last Event, and user selected input/output ports, can be dynamically tracked. The user, therefore, is able to observe
simulation data for any number of atomic and coupled models without any code development. The data can also be displayed in a tabular format using a Tracking Logger or exported to CSV files.
2.3

TimeView

The TimeView is a module developed for run-time display of data sets as two dimensional plots (every plot has x
and y coordinates). Its operation is similar to an oscilloscope. The TimeView is a passive module. It can display sets
of (x, y) values where x (or y) values are plotted with respect to y (or x). In order to use it for plotting time-based
simulation data, the x-coordinate for all plots is defined to
represent time. The allowed values for the x-coordinate are
integers (e.g., natural numbers) and the unit can be alphanumeric (e.g., seconds). The y-coordinate can be numbers
(integer and real) and string. The values for the ycoordinates are generated inputs and outputs that are generated by atomic or coupled models. As an example, size of a
queue can be plotted at time instances 0, 1, 2, ..., 100. The
time increment duration and the units for time and variable
to be plotted can be set by user.
For every atomic and coupled model component, one or
all of its input and output ports can be viewed independently. The time trajectories for all variables of a model that are
selected to be tracked are combined into a single view.
There is no support for overlaying multiple variables into a
single plot and multiple time trajectories from different
models cannot be combined into a single trajectory (e.g.,
given model output port outA from model A and output port
outB from model B, a single trajectory cannot be created to
display data from both outA and outB ports).

2

3

DEVS-SUITE SIMULATOR
devs

The architecture of the DEVS-Suite simulator is the
same as that of the DTE. The architecture separates simulation models (i.e., the source of data) from how they are controlled and viewed. As shown in Figures 1 and 2, the
DEVS-suite package diagram consists of Model, Façade,
Controller, and View packages and their sub-packages [5].
The design and implementation of the DTE’s Façade layer
is extended. Mainly, its connection to the Model is altered in
order to include DEVSJAVA’s simView to the View.
Therefore, the View module has the simView, TimeView,
and TrackingLog packages. Given the presence of the
Façade, the View allows combined animation and tracking
(i.e., viewing at run-time trajectories and tabulation of simulated data). Users, therefore, may choose simView and/or
Tracking. Similarly, the Controller design is extended to
handle simulation animation speed (i.e., the speed at which
messages are exchanged among atomic and coupled model
components).

(from modeling)

RTCentralCoord
(from realTime)

atomic

digraph

(from modeling)

(from modeling)

TunableCoordinator
(from realTime)

ViewableAtomic

ViewableDigraph

(from modeling)

(from modeling)

SimViewCoupledCoordinator
(from simulation)

Controller
(from controller)
... )

FModel

FCoupledSimulator

(from modeling)

(from simulation)

Model
Modeling

Simulation

ViewableComponent
ViewableComponent

FCoupledModel

FSimulator

(from modeling)

(from modeling)

(from simulation)
... )

FModelView

FSimulatorView

(from view)

(from view)

ViewableAtomicSimulator
ViewableAtomicSimulator

Facade
Modeling

FAtomicModel

Simulation

FModel
FModel

FSimulator
FSimulator

View

(from simulation)

(from view)

View
Controller
Controller
Controller

View
TrackingControl

FAtomicSimulator

simView
SimView
SimView

TrackingControl

Tracker

(from view)

(from view)

timeView
ModelTrackingComponent
[Tracking Log]

TimeView
TimeView

TimeView

ModelTrackingComponent

SimView

(from timeView)

(from view)

(from simView)

Figure 1: DEVS-Suite MFVC Package Diagram
Figure 2. DEVS-Suite Class Diagram
From the scalability perspective, a representative set of
simulation models having 20 to 7000 model components
were devised and simulated for a service-oriented software
system called voice communication system (VCS)
[5][12][15]. We used a desktop machine with Core 2 Duo
2.66 GHz CPU and 4GB RAM. Obviously, the execution
speed afforded by the simulator depends on the number of
components that are chosen to be tracked and whether or not
the model is animated. The simView was turned off and no
data was tracked. Initially, the wall-clock simulation time
increases proportionally and then slowly becomes exponential.

3

3.1

Interface and Monitoring of Simulation Data

DEVS-Suite user interface consists of four parts: (1)
Model Viewer at the top left corner, (2) Simulator Control
at the bottom left corner, (3) simView at the top right hand
corner, and (4) TimeView at the bottom right hand corner
(see Figure 3). In order to make better use of available display space, the Model Viewer and Simulator Control are
combined to form a part which we call MVSC. A user,
therefore, can choose to view any one of the TimeView,
SimView, or MVSC parts within the DEVS-Suite interface
since any two of the three parts can be hidden. A user may
also view MVSC with either TimeView or SimView. Alternatively, the user can hide the MVSC part and only view the
TimeView and SimView while executing the model using
the execution buttons provided in the menu bar.

components as time trajectories. The tree view is used for
choosing model components and deciding which input and
output ports to monitor. For atomic models, pre-defined
state variables and basic simulator variables can also be
chosen and tracked. The block model is used for animation.
The dynamics of every atomic and coupled model can
be individually displayed with TimeView. The semantics of
the data generated by the Model module in DEVS-Suite is
applied to the TimeView. Therefore, to display time-based
state and input/output data, simulation time is used to synchronize generation of the time trajectories. Users have the
flexibility to select animation and tracking view options for
any number of atomic/coupled models. They can set the unit
for data that is to be monitored as well as the time axis. The
time increment, units, and the selection of data to be observed can be set as shown in Figure 4.

Both block and tree views of hierarchical model components are available. It provides flexibility in that a user
can select animation and/or tracking of simulation model

Figure 3: DEVS-Suite UI with Model Viewer, Simulator Control, simView, and TimeView

4

If the Tracking option is selected and one or more model components are also chosen to be viewed via TimeView
and TrackingLog, a tracker collects simulation data sets for
each selected model. For example, we consider an SOAcomplaint DEVS model shown in Figure 3. This model
represents Voice Communication Service (VCS) and Travel
Agency Service (TAS). Voice communication service called
VoiceComm is an atomic service that broadcasts voice data
streams in response to Subscriber1 and Subscriber2 atomic
services. The system is devised to support audio data that
can be sampled at any of the following rates – 44.1, 88.2,
136.4, 176.4, 220.5 KHz. For the VoiceComm service, its
input and output ports can be tracked. Figure 4 shows the
VoiceComm requested and provided data rates that are selected to be tracked.

4

SIMULATION MODEL AND COMPLEXITY
EVALUATION

To evaluate the tracking and viewing supported by
DEVS-Suite, we compared it with Ptolemy II and SimEvents simulators. The Assembly Line model [4] shown in
Figure 5 is included in Ptolemy II. In this model, jobs are
generated by a Generator model at predefined intervals and
are serviced by three processors P1, P2, and P3 in a cascade
fashion. The service time for each job is specified by a Processor. We developed the same model in SimEvents and
DEVS-Suite simulation tools (see Figure 6) [3].

Figure 5: Conceptual Assembly Line Model

Figure 4: Tracking VoiceComm Model
3.2

Simulation Control Logics

The DEVS-Suite simulator supports the control logics
that are provided by DEVSJAVA and DTE. The simulation
execution can be controlled using Run, Step, Step(n), Request Pause, and Reset (see Figure 3). The other controls are
Show Coupling, real-time factor, and a new control called
Animation speed. The control logic manages the simulator
execution and the data that is provided to SimView. DEVSSuite handles the simView animation and TimeView trajectories independently. This is advantageous since the speed
of animation for messages may not be the same as the speed
at which time trajectories can be displayed. The Controller
supports compilation of models at run-time. This is important to allow users to configure the path to packages of
model classes and source files as well as model package
names as in DEVSJAVA. After the configuration, a user selects a package name and then the list of available models in
the selected package can be displayed.

We define a simple visual complexity metric as the
numbers of components that are displayed for a given model. The total number is equal to the sum of the number of
block components (i.e., models that represent dynamics of
the modeled system) and their ports and couplings. The
block components are categorized into logical and monitoring components. The number of logical component models
among SimEvents, Ptolemy II, and DEV-Suite vary due to
their underlying modeling approaches. A consequence of the
modeling differences is that the number of ports and couplings for SimEvents and Ptolemy II are higher as compared
with DEVS-Suite.
A key difference among these tools is the presence or
absence of monitoring components. As shown in Table 1,
the number of the displayed components for the Assembly
Line model varies significantly even for such a small model.
The visual complexity metrics for SimEvents is 58 vs. 23
for DEVS-Suite. The visual complexity for Ptolemy II is
better as compared with SimEvents, but not with respect to
DEVS-Suite. The complexity metric also depends on flat vs.
hierarchical models. In DEV-Suite, the hierarchy of a model
does not impact the visual complexity metric. In comparison, the visual complexity metric for hierarchical models
developed in Ptolemy II and SimEvents may be affected
given the necessity of couplings monitoring components to
logical components.

5

Figure 6: Models of the Assembly Line System in DEVS-Suite, Ptolemy II, and SimEvents Simulators

The main cause for the different visual complexity metrics is the use of monitoring components. They increase the
number of ports and couplings for SimEvents and Ptolemy
II. It can be easily seen that TimedPlotter for Ptolemy II and
SignalScopes for SimEvents are required for monitoring inputs and outputs of components. They, therefore, add to the
visual complexity of model display. In contrast, in DEVSsuite, the user selects model components that are to be monitored using dialogue boxes. It is noted that the monitoring
components in SimEvents and Ptolemy II collect data, but in
order to observe this data, separate windows must be
created.
Thus, it is straightforward to observe that the DEVSSuite visual model complexity is not affected by the number
of models to be monitored. However, the DEVS-Suite’s
block model component view suffers from the overlapping
of the couplings. This problem also can be seen to a lesser
degree in the other tools. Some research aimed at overcoming the crossing of couplings has results in the visual modeling environment CoSMoS [11], which supports simulating
models using DEV-Suite.
Some factors that are related to monitoring components
are shown in Table 2. All three tools support plotting numbers with DEVS-Suite also supporting strings. In contrast,

SimEvents and Ptolemy II plotters have user-friendly features such as zoom in and zoom out. Ptolemy II supports
combing multiple variables which could be from multiple
logical components into a single plot (i.e., overlaying of trajectories). DEVS-Suite allows viewing together independent
plots for a single model, but has no support for overlaying
time trajectories. The plotters for Ptolemy II and SimEvents
are specialized and efficient. In contrast, while the TimeView for DEVS-Suite is generic, it lacks flexibility for the
resizing of plots. Finally, as noted above, the monitoring
components for SimEvents and Ptolemy must be coupled
with logical components.
Table 1: Visual Complexity Metric

Logical components
Ports
Couplings
Monitoring
components
Total number
of components

SimEvents

Ptolemy II

DEVSSuite

11

9

5

29
14

15
11

10
4

4

2

0

58

37

23

Table 2: Comparison of Component-based Simulators
Plotters
Data
Numbers
Types
String
Consolidate into a
single plotter
Specialized
Generic
Require couplings
or an output port

5

SimEvents

Ptolemy II

DEVSSuite

Yes
No
No (single
plotter per
port)
Yes
No
Yes

Yes
No
Yes

Yes
Yes
Yes (per
model)

[5]

Yes
No
Yes

No
Yes
No

[6]

[4]

[7]

CONCLUSION AND FUTURE WORK

In this paper, we presented the DEVS-Suite simulator
and described its capabilities for designing experiments and
visualization of simulation executions both as time trajectories and animation. The simulator simplifies designing simulation experimentations and observing their dynamics
without unnecessarily complicating the display of models.
Systems such as service-oriented software systems can be
more easily configured for colleting simulation data and
run-time examination of the model behavior via time trajectories. The simulator can also be used for education [10].
This is because DEVS-Suite simplifies creation of simulation scenarios (i.e., experimental designs) which in turn aids
verification and validation of systems. In terms of future
work, it is useful for the DEVS-Suite to support real-time
visualization when useful. It is desirable for animation and
visualization of time trajectories to be synchronized. Better
support for manipulating views of trajectories at varying levels of details is also important.

[8]
[9]

[10]

[11]

[12]

[13]
Acknowledgement
This research was partially supported by NSF Grant numbers #BCS-0140269 and #CCF-0725340. The TimeView
module was developed by Robert Flasher as part of his undergraduate senior project in the Computer Science and Engineering department at Arizona State University.

[14]

[15]

References
[1]

[2]

[3]

Arizona Center for Integrative Modeling and Simulation.
2007.
http://www.acims.arizona.edu/
SOFTWARE.
Dalle, O. 2007. An Instrumentation Framework for
Component-Based Simulations based on the Separation of Concerns Paradigm. 6th EUROSIM Congress,
Ljubljana, Slovenia.
Elamvazhuthi, V. 2008. Visual Component-Based
System Modeling with Automated Simulation Data
Collection and Observation, MS Thesis, Department

[16]

of Computer Science and Engineering, Arizona State
University, Tempe, AZ, USA.
Jayadev, M. 1986. Distributed discrete-event simulation. ACM Computing Surveys, 18(1): 39-65.
Kim, S. 2008. Simulator for Service-based Software
Systems: Design and Implementation with DEVSSuite. MS Thesis. Computer Science and Engineering. Arizona State University, Tempe, AZ, USA.
Lee, E. A. 2003. Overview of the Ptolemy Project.
University of California, Berkeley, CA, USA.
Mather, J. 2003. The DEVSJAVA Simulation Viewer: A Modular GUI that Visualizes the Structure and
Behavior of Hierarchical DEVS Models, MS Thesis,
Electrical and Computer Engineering. University of
Arizona, Tucson, AZ, USA.
Mathworks. 2007. http://www.mathworks.com.
Reenskaug, T. M. H. 1978. The Model-ViewController (MVC). http://heim.ifi.uio.no/~trygver/
themes/mvc/mvc-index.html.
Sarjoughian, H.S., Y. Chen, K. Burger. 2008. A
Component-based Visual Simulator for MIPS32 Processors, Frontiers in Education, TIA1-TIA6, October,
Saratoga Spring, NY, USA.
Sarjoughian, H. S. and V. Elamvazhuthi. 2009. CoSMoS: A Visual Environment for Component-based
Modeling, Experimental Design, and Simulation,
Proceedings of International Conference on Simulation Tools and Techniques, Rome, Italy.
Sarjoughian, H., S. Kim, M. Ramaswamy, S. Yau.
2008. An SOA-DEVS Modeling Framework for Service-Oriented Software System Simulation, Proceedings of the Winter Simulation Conference, 845-853,
December, Miami, FL, USA.
Sarjoughian, H. S. and R. Singh. 2004. Building Simulation Modeling Environments Using Systems
Theory and Soft-ware Architecture Principles. Proceedings of the Advanced Simulation Technology
Conference, 235-240, Washington DC, USA.
Wainer, G. 2002. CD++: A Toolkit to Develop DEVS
Models, Software - Practice and Experience, 32:
1261-1306.
Yau, S. S., N. Ye, H. S. Sarjoughian and D. Huang,
2008. Developing Service-based Software Systems
with QoS Monitoring and Adaptation. Proceedings of
the 12th IEEE Int'l Workshop on Future Trends of
Distributed Computing Systems, 74-80, October,
Kunming, China.
Zeigler, B. P., H. Praehofer, T. G. Kim. 2000. Theory
of Modeling and Simulation: Integrating Discrete
Event and Continuous Complex Dynamic Systems,
Academic Press.

7

Software and Simulation Modeling for Real-time Software-intensive System
Hessam Sarjoughian1
Computer Science & Engr. Dept.
Arizona State University, Tempe, AZ
Sarjoughian@ASU.EDU

Dongping Huang
Computer Science & Engr. Dept.
Arizona State University, Tempe, AZ
Dongping.Huang@ASU.EDU

Abstract1
Successful development of large-scale complex and
distributed real-time systems commonly relies on models
developed separately for simulation studies and software
implementation. Systems theory provides sound modeling
principles to characterize structural and behavioral
aspects of systems across time and space. The behavior of
these models can be observed using simulation protocols
that can correctly interpret time-based logical dynamics.
Similarly, object-orientation theories and software
architecture principles enable modeling static and
dynamic behavior of systems. While models described
either in system-theoretic or object-orientated languages
may be used for both software design and simulation
modeling, each has its own strengths and weaknesses. For
example, a class of system-theoretic modeling approach
called Discrete-event System Specification (DEVS)
provides an appropriate basis to develop simulation
models exhibiting concurrent and distributed behavior.
Similarly, the Unified Modeling Language with real-time
(UML-RT) constructs can be used to develop software
design models that can be implemented and executed.
Since software models are not suitable to be used as
simulation models and simulation models may not
adequately lend themselves to serve as software design
blueprints, it is important to examine these approaches.
We show some of the key shortcomings of these simulation
and software design modeling approaches by developing
some detailed specifications and implementation of a
coffee machine with a focus on their treatment of logical
and physical time.

1

Introduction

The existing body of theories, methods, and technologies
that have been developed over the last two decades
support analysis, design, and development of non-realtime software systems. Unfortunately, the same cannot be
said about real-time software-intensive systems since their
designs remain chiefly ad-hoc. Real-time softwareintensive (embedded) system design must account for new
kinds of complexity due to timing constraints.
1

Corresponding author.

Specifications for such systems need to support structural
and behavioral modeling of components and their
interactions while rigorously accounting for time – i.e., it
is not sufficient to model time informally or in abstract
terms. For example, while constructs such as “always”
and “next-time” [1] can be used to specify time-based
behavior of systems, they are not sufficient for precise
specification of time constraints using Object-Orientation
frameworks such as UML [2]).
Systems theory offers fundamental concepts and
theories of sub-systems (components), their compositions,
and interactions with one another through well-defined
interfaces or ports (e.g., see [3]) which are governed by
time. However, system-theoretic specifications do not
account for the computational basis required for software
design and implementation.
Given the capabilities and limitations of objectorientation and systems theory, it is useful to investigate
their relationships and in particular (i) the applicability of
each for the analysis and design of software-intensive
systems and (ii) their complementary roles in handling
modeling and computational complexity of embedded
systems. To achieve our objective, this paper focuses on
the capabilities of the UML-Real-time – an object-oriented
software specification with real-time object-oriented
modeling framework – and DEVS (Discrete-Event System
Specification) [4] – a system-theoretic modeling and
simulation framework.

2

Background

Systems theory [3-5] defines a system in terms of its
structure and behavior. A system has a hierarchical
structure and can have continuous and discrete behaviors.
An important feature of systems theory is explicit support
for time – i.e., the semantics of models are directly related
to the passage of time and causality of outputs with respect
to states and inputs. Components (representing subsystems) can interact with each others via input and output
ports. Models which are composed using these
components have sound syntax and semantics and lend
themselves to well-defined computation. For example,
Discrete-event System Specification (DEVS) [4] supports
modeling a set of hierarchical interacting components that
are able to respond to external stimuli and exhibit

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

autonomous behavior. Behavior and structure of the
models are described as atomic and coupled models in
environments such as DEVSJAVA [6]. Parallel atomic
models can have multiple ports and can process bags of
inputs and produce bags of output events. Parallel
hierarchical coupled models can be composed of atomic
and coupled models with three constraints – (i) atomic
models can be at the lowest level in any coupled
hierarchical model, (ii) no coupled model can contain
itself as a component, and (iii) no direct output to input
coupling is allowed for atomic or coupled models.
Modeling and simulation approaches are suitable and
widely used for modeling real-time systems. For example,
DEVS and its extension Real-time DEVS [7-9] can be
used to model time as either abstract or physical entity.
These modeling and simulation frameworks have been
proposed and developed to support specification and
simulation of atomic and coupled models under real-time
constraints. We note that recent extensions to DEVS [10]
support modeling the class of ordinary differential
equations and discrete-time models can be automatically
mapped onto discrete-event models.
System theoretic based modeling and simulation
framework, however, are not inherently intended for
software design and development [11]. Software design
blueprints must specify details that are beyond the scope
of simulation modeling. Simulation models are extensively
used for requirement analysis and evaluation of alternative
system designs at relatively high levels of abstractions.
Compared to system-theoretic modeling, software
modeling is concerned with software specification and
implementation. For example, object-oriented modeling
supports
characterizing
components
and
their
compositions. Object-orientation approaches such as UML
offers constructs to model relationships among
components that are not possible in system-theoretic
approaches. Using UML we can design logical, non-realtime dynamics of software using inheritance, aggregation,
and realization relationships that are appropriately defined
among classes, interfaces, sub-systems.
For real-time applications, formal timing and
concurrency modeling constructs are crucial and various
efforts are underway to extend OO principles and methods
in order to formally account for time. For example, UMLRT [12] extends UML by incorporating the Real-time
Object-Oriented Modeling (ROOM) [13]. In UML-RT
Profile [14], the time, schedule and performance related
properties that have already been captured are
quantitatively codified using standard UML stereotypes,
tagged values, and constraints. Nonetheless, there does not
exist first-class concepts and constructs for time. The
UML-RT – which is gaining popularity as a commercial
tool in developing real-time software specifications –
offers the system-theoretic concepts of ports and
hierarchical composition. Other ongoing efforts are

focusing on combining UML with formal languages such
as Object-Z (e.g., [15]) or Specification Description
Language (SDL) (e.g., [16]). In addition to these, the
Model Driven Architecture [17] proposes a framework to
bring together Platform Independent Models of business
logic to Platform Specific Models of the underlying
software architecture which can automatically be mapped
into application code. Another research effort is
developing a precise and multi-level meta-modeling
framework to visually capture the transformations by
using graph transformation [18]. Other approaches,
primarily driven by the DEVS framework, offer some
capabilities to map simulation models to executable code
(e.g., [9, 19]).

2.1

Modeling time

The specifications of real-time systems or simulation
models must account for concurrency, synchronization,
and causality among a set of interacting components. A
unifying concept for concurrency, synchronization, and
causality is time. However, different concepts are used to
specify simulation models and real-world (physical)
systems. For example, the “happened before” relation in
logical time can be used to define the causality relation on
the set of events in which an event is only influenced by
the past and current events; it cannot be influenced by
future events [20]. Events in the same process are causally
related, but events in distributed processes could be
causally dependent or not. Causally independent events
can be thought as concurrent events. The reverse is not
true as concurrent events can also be causally dependent.
In real-time distributed systems, all the events among the
processes may be ordered by the causality relationship.
For the causally independent events with identical logical
clocks, they can be ordered by taking into account the
identifier of processes.
From the discrete event simulation worldview, three
notions of time have been defined (see [21] for a detailed
treatment). These are physical, simulation, and wallclock
times. Physical time refers to time in the physical (realworld) system. Simulation time is introduced as an
abstraction of physical time. Wallclock time is used to
mark the beginning and the end of a simulation execution.
The wallclock time and simulation time have a linear
relationship – appropriate correspondence between the
physical and the simulation times is ensured. From
system-theoretic point of view, the dependencies between
components are allowed only through the input/output
couplings. Therefore, the model’s notion of time, by itself,
may not be used directly to deduce causality. In logical
processes worldview of parallel discrete events simulation,
causally dependent events are identified using event time
stamping or ordering.

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

3

Related work

Real-time system development requires the concepts and
techniques offered by systems theory and object
orientation. To make use of capabilities afforded by these
worldviews, researchers have proposed and developed
approaches to support both simulation and software
development of software-intensive systems. For example,
several improvements and extensions of the Unified
Process by using the system theoretical methodology for
object oriented software analysis [22] have been proposed.
I-Logix [23] has developed a framework that integrates
systems engineering and software engineering together.
The first part of the process focuses on requirement
analysis, system analysis and design. Software design and
implementation are the focus of the second part of the
process. To support the process activities, two software
tools have been developed. First, the Statemate
MAGNUM is a visual modeling and simulation tool for
system engineers. It focuses on system model analysis and
design and it allows capturing graphical description of the
system prototype using use-case models and scenarios,
activity chart, control block diagram and statechart. The
environment provides support for maintaining consistency
among these models. Second, the Rhapsody implements
UML 2.0 and provides a model-driven development
environment for software engineering. It supports
modeling software design and implementation. The formal
languages of activity charts and Statecharts in the
Statemate enable the models’ execution and verification
using mapping rules [24].
In contrast to I-Logix, Ptolemy II [25] provides a
unified development environment for both system
engineering and software engineering. Ptolemy project
[25] specifically is targeted for real-time embedded
systems modeling, simulation and design by coupling
system designs with control model designs. It supports a
hierarchical heterogeneous software environment for
model refinement and system implementation. Ptolemy’s
actor-oriented approach is quite similar to the concepts
developed in ROOM. It separates functional models
(actors) from component interaction models (frameworks).
A framework defines a set of constraints on how actors
can interact with another [25]. As a computation model, a
framework must maintain a set of states and their
transitions. In theory, frameworks may be combined to
support heterogeneous models of computations. For an
actor transition, the framework state needs to be
considered as guard conditions; for a framework
transition, actor states also need to be considered.
Therefore, it is difficult to maintain consistency between
the framework states and the actor states as a model’s
complexity grows.
Alternatively the DEVS framework can be extended
with capabilities for real-time execution (e.g., [9]). While

this approach offers important features, its support for
software design is limited. In particular, the supported
modeling constructs are limited compared with those
available from UML (e.g., in software design and
implementation it is beneficial to use interfaces, something
that is not part of the DEVS framework).

4

Model development in UML-RT and
DEVS

As we have noted above, object-oriented and systemtheoretic approaches allow modeling of real-time
embedded systems. In this section, we develop two sets of
models using UML-RT and DEVS. Specifications of these
models reveal key aspects of each approach. In particular,
model specifications serve to highlight strengths and
weaknesses of each approach from developing simulation
models and software models.
A Coffee Machine is a simple, yet illustrative system
for examining the modeling and simulation/execution
capabilities of the DEVS and the UML-RT approaches.
The Coffee Machine hardware consist of a water pot, a
filter container, a coffee filter, a warming plate, a boiler, a
brew button, and an indicator light. The Coffee Machine’s
software controls its operations. A typical scenario for
brewing coffee starts with placing a filter in the filter
holder, filling it with coffee grounds, and sliding the filter
holder into its receptacle. After pouring water into the
water strainer, the brew button can be pressed to start
brewing coffee. Once the water reaches boiling, it is
sprayed over the coffee grounds and coffee starts to drip
through the filter and into the coffee pot. The coffee pot is
kept warm for extended periods by a warmer plate, which
only turns on if there is coffee in the pot. If the coffee pot
is removed from the warmer plate while coffee is brewing,
the flow of water is stopped in order to avoid spilling
coffee on the warmer plate.
A set of sensors monitor the warmer plate, the boiler,
the brew and the indicator buttons. Similarly, a set of
actuators control the warmer plate heating element, the
boiler, and the brew button, and indicator lights.
Text/graphic user input and output is used to simulate the
behavior of the Coffee Machine. We model the software
aspect of the Coffee machine controller as well as the
sensors and actuators. The controller consists of four parts:
front panel (controls the buttons and display light
indicator), sprayer controller (controls the boiler spraying),
warmer controller (controls pot warmer), and brewer
controller. The brewer controller coordinates the front,
sprayer, and warmer controllers.

4.1

UML-RT modeling approach

The UML Real-Time, which is an extension of the UML
offers additional modeling constructs based on Real-time

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

Object-Oriented Modeling [26] and is targeted for
modeling complex, event-driven, and distributed real-time
systems [26, 27]. It supports specifying both logical and
physical aspects of real-time systems. UML-RT can be
used to model individual structural and behavioral aspects
of systems and their relationships. In the following subsections, we use Rational Rose RealTime software [12] to
model the Coffee Machine.
4.1.1
Structural modeling. Capsule, Port, and
Connector are important concepts for specifying structure
models. Capsule is one of the basic UML-RT modeling
elements that represent an active object within a system
communicating with other capsules exclusively through
one or more interface objects that are called ports. For
example, CM_Container (Coffee Machine Container) and
CM_Brewer (Coffee Machine Brewer) represent the
highest level software model of the Coffee Machine and
one of its components, respectively. No public attributes
or methods can be defined in capsules. Every port (e.g.,
ctr_Warmer) is “owned” by a capsule (e.g., CM_Brewer)
in the sense that the capsule and its port have the same
lifetime – i.e., ports cannot be defined independently of
their capsules. The signal messages that are communicated
between the ports are defined in Protocol, which is the
main concept when specifying the behavior. Capsules and
protocols (e.g., CUControl_Warmer) can be shown using
standard UML graphical notations (class diagrams).
Capsule structural diagram can be defined graphically as
shown in Figure 1.
/ cmBrewer : CM_Brewer

sub-capsule

+ / ctr_Sprayer
: CUControl_Sprayer~

+ / ctr_Warmer
: CUControl_Warmer~

+ / coffeeControl
: CUControl_Sprayer

+ / coffeeControl
: CUControl_Warmer

/ cuSprayer
: CU_Sprayer

end

relay

/ cuWarmer
: CU_Warmer

+ / boiler

+ / brewButton

+ / warmer

: Boiler~

: BrewButton~

: Warmer~

+ / boiler
: Boiler~

connector

+ / warmer
: Warmer~

port

Figure 1: Structure diagram with capsule
composition, ports and protocol specification.
Sub-capsules can be hierarchically combined into
capsules (see Figure 2). Connector is used to represent
interaction channel among parent and its immediate subcapsules as well as among sub-capsules. Two types of
ports are defined: relay and end ports. A relay port
transmits signals between a capsule and its sub-capsule.
An end port is responsible for processing the signals in the
capsule’s statechart. Capsule supports both static and
dynamic component constructions. The behavior

specification inside the capsule implementation is defined
as a statechart (see Section 4.2.2). It should be noted that
UML modeling capabilities such as inheritance can be
used in UML-RT to facilitate structural specification. For
example, protocol can take advantage of inheritance to
implement message reuse.
Capsule

<<Capsule>>
CM_Brewer

<<Capsule>>
CM_Container

/ cmBrewer
isDeviceReady()

+ / boiler : Boiler~
+ / brewButton : BrewButton~
+ / indicator : IndicatorLight~
+ / warmer : Warmer~

Ports

Capsule
Role

+ / ctr_FrontPanel : CUControl_FrontPanel~
+ / ctr_Sprayer : CUControl_Sprayer~
+ / ctr_Warmer : CUControl_Warmer~
# / timing : Timing

<<Port>>
+ / ctr_Warmer~
<<Protocol>>
CUControl_Warmer

Protocol

(from ControlUnits)

Figure 2: UML-RT Hierarchical
Capsules, Ports and Connectors.

Model

with

4.1.2
Behavioral modeling. Protocol and Statechart
are two essential concepts necessary for specifying
behavior in UML-RT. A Protocol is a specification of
contractual agreement among the participating capsules, in
which the allowable input/output messages are defined
based on their roles. Only ports that have compatible
protocol roles can communicate with one another. A
protocol consists of participants where each has a specific
role in the protocol. A protocol role has a unique name and
a set of signals that it can receive and send. Protocol has
its own statechart to specify the sequence of signal
communication. The most common type of protocol is a
binary protocol which is defined to have two participants.
A binary protocol must have one role (base role) and it
may also have a second role (conjugate role). The
conjugate role has its incoming message set and the
outgoing message set opposite of the base role. In
addition, a protocol may be specified to have a restrict set
of communication sequences given its state machine.
However, the quality of service specification (e.g.,
message priority) and finer grain specification of all valid
message sequences allowed by the protocol (sequences) is
not supported.
Each capsule or sub-capsule behavior is specified with
a statechart. In UML-RT the trigger events in a capsule’s
statechart are defined with a port and signal pair. The
action is specified by a specific programming code, which
is then used for execution. A statechart diagram for the
CM_Brewer controller is shown in Figure 3.
As UML-RT provides run-to-completion mechanism
for a statechart to process a single event at a time, the
concurrency can be due to multiple capsules executing
simultaneously. That is, UML-RT supports concurrency to

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

the same extent as defined by UML. An important
shortcoming for modeling concurrency is the inability to
guarantee processing of events using priority settings.
As discussed earlier, developing real-time systems
requires modeling of time and therefore the necessity of
having time as an artifact. UML-RT provides timing
service through a standard port called SAP (Service
Access Point). The timing service converts time (time
provided by the target operating system) into events that
can then be handled in the same way as other signal-based
events. The resolution (granularity) of time is dependent
on the target platform or operating system. Since time is a
service provided by the target platform, it cannot be
readily manipulated – e.g., making the clock run slower.
Initial
BrewRequested
Wait_to_Start

Not_Brewing
BrewDenied

Brew

BrewComplete

Brewing

brewButton.
brewButtonPushed /
coffeeControl.
BrewRequested.send

IgnoreXtraButtonPush

Figure 3: Statechart diagram for brewer controller.
To access the timing service of UML-RT, a timing port
with a pre-defined Timing protocol can be added to each
capsule. Service requests are activated by operation calls
to this port (referenced by its name) while replies from the
timing service are sent as messages that arrive through the
same port. Two types of time-based situations can be
modeled in UML-RT: the expiration of a specific time
interval and the occurrence of a set instance in time. If a
timeout occurs, the capsule instance that requested the
timeout receives a message with the pre-defined message
signal 'timeout'. Then a transition with a trigger event for
the timeout signal must be defined in the behavior in order
to receive the timeout message.
A timing service request results in the creation of a
dedicated “timer” for the requesting capsule. Therefore
each request has its own timer so that concurrent timing
requests can be supported. However, since these timers are
independent of one another, it is the modeler’s
responsibility to define suitable timing-constraint among
protocols and capsules.
Models can be forward engineered and converted to
executable code. The real-time execution is through the
timing service of the UML-RT software environment.
However, while UML-RT offers important capabilities to
model real-time software, it does not provide suitable
semantics suitable for simulated time – i.e., without the
concepts of physical, simulation, and wallclock times,
UML-RT prohibits carrying out simulation studies.

4.2

DEVS modeling approach

The Discrete Event System Specification is a
mathematical formalism targeted for developing
simulation models. The formalism, based on general
system theoretic concepts, supports developing models
with well-defined behavior [4]. It uses the mathematical
set theory and offers a framework for system modeling
and simulation. In this framework, every model
component is either an atomic model or a coupled model.
Atomic model defines a set of inputs with ports and
messages, a set of outputs with ports and messages, a set
of states, internal transition function mapping from states
and time to states, external transition function mapping
from states, inputs, and time to states, output functional
mapping from states to outputs. In addition, in order to
model time, each atomic model has a time advanced
function which maps a state to real numbers.
4.2.1
Structural modeling. Port is the key concept for
modeling the structure of atomic models. Coupling is the
key concept important for when modeling the structure of
a coupled model. Instead of providing duplex ports as in
UML-RT, DEVS distinguishes ports by its communication
direction. Input ports only receive messages while output
ports only send messages. DEVS formalism doesn’t
specify any constraints on the message types coming in or
going out of a specific port. Therefore, couplings do not
specify what kind of messages may be sent or received.
Instead, it is assumed the consumer of the message is able
to determine whether or not a message has an appropriate
type. This implies that producer of the message and its
consumers must be consistent with one another. The
Brewer Controller atomic model specification is
implemented in DEVSJAVA [6]. The CM-Brewer
external transition function (delt_ext) is shown in Figure
4. This function specifies that when the CM-Brewer
receives an external event Msg.On on input port
fromSprayer, it creates two messages, sets the state of the
model to Checking and schedules an internal transition
with time advance 0. Before the execution of the internal
transition function, the simulator sends the output
messages job_sprayer and job_warmer on output ports
toSprayer and toWarmer based on the CM_Brewer
output function (see Figure 5).
DEVS supports communication among multiple ports
so one input port can receive messages from multiple
output ports. Due to fan-in and fan-out couplings, the same
message sent from two or more atomic models need to be
identified based on their senders. In UML-RT, this is
unnecessary since each protocol is associated with only
one pair of ports. The coupling is similar with the
connectors specified in UML-RT except that there is no
constraint on port communication as long as input

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

messages go to input ports while output messages go to
output ports.
public void delt_ext (double e, message x) {
if ( phaseIs("Passive") ) {
for (int i = 0; i < x.getLength(); i + +){
if (messageOnPort(x, "fromSprayer")){
job = x.getValOnPort("fromSprayer", i);
if (job.getName().equals(Msg.ON)){
job_sprayer = new entity(Msg.CHECK_READY);
job_warmer = new entity(Msg.CHECK_READY);
state = "Checking";
holdIn("SendOut", 0);
}}}}

Figure 4: Snippet of the DEVS CU_Brewer model
Output Port

Input Port

CU_Brewer
fromSrayer
toBrewer

fromButton
toSrayer

fromWarmer

toWarmer

fromBrewer

toBrewer

fromBrewer

CU_Sprayer

CU_Warmer

toBoiler

toBoiler

toWarmer

fromButton

toWarmer

Figure 5: DEVS CM_Controller coupled model
4.2.2
Behavioral modeling. Internal transition,
external transition, confluent, output and time advance
functions are all used to specify the behavior of an atomic
model. An atomic model has a state set which includes
two special state variables phase and sigma and other state
variables. The temporal property of the framework does
not require customized treatment of time across all the
models. Furthermore, since time is a first-class modeling
artifact, the formalism provides well-defined semantics for
modeling events that may be processed in zero time
intervals (i.e., it supports transient processing of events).

5

Simulation and execution of models

To observe behaviors of a model, its needs to have an
implementation that lends itself to execution. For models
described using a given modeling and simulation approach
(e.g., DEVS), there needs to exist a simulator that can
correctly interpret (i.e., execute) the model. A simulator
subjects a model to input stimuli and computes its
resulting output. Correspondingly, an execution engine
must exist to execute models described using a software
modeling methodology such as UML-RT.

5.1

UML-RT model execution

Models specified in UML-RT may be executed using the
UML-RT virtual machine or framework. This requires
mapping suitable models (capsules and statecharts) into a

high level language such as Java or C++. With models
expresses in appropriate programming language syntax,
they can be executed using a number of services such as
communication and timing. The framework consists of
Frame Service, Timing Service, Communication Service
and other services through the Service Access Point [26].
The UML-RT framework provides the necessary
capabilities for executing real-time models specified in the
Rational Rose Real-Time. The framework services are
coordinated by a central controller system, where the main
control of the application is held. The application model is
converted to a sub-application class inherited from the
framework classes. This allows invoking the framework to
execute the functions in the application to pass control to
the application objects as required. After it processes all
the messages in the queue, the application model will
return the control back to the central controller.
UML-RT uses an asynchronous execution model, in
which, the time instant at which a component reacts to a
received message is left unspecified; it only defines that
the reaction happens after receipt of a message. The
scheduling algorithm of the ROOM virtual machine does
not seem to be defined such that models can be executed
using simulation protocols – the multi-task schedule is
dependent on the underlying real-time operating system.
From the simulator point of view, UML-RT doesn’t
provide authoritative simulation capabilities – it simply
executes a model’s logical specification. Since the
framework doesn’t offer simulation algorithms, primitive
customized timing services must be defined to model
desirable, correct temporal behavior which in turns
undermines having a well-defined relationship between
model specification and the simulation algorithm.

5.2

DEVS model simulation

The DEVS formalism separates models from how they
may be executed. It has well-defined concepts and
protocols for executing (simulating) models. Similar to
atomic and coupled models, there exist corresponding
atomic and coupled simulators. Each atomic model has its
own simulator and each coupled model has its own
coordinator. The coordinator and simulator hold the last
event time tL and the next event time tN of the
corresponding model, respectively. The coordination
among atomic and coupled simulator is managed by a root
coordinator which ensures all messages are exchanged
based on timing atomic models which must either process
incoming messages or undergo internal state changes.
One of the advantages of separation between models
and simulators is modularity and therefore the ability to
change the underlying simulation engine without requiring
changes to the models. Furthermore, simulators can be
independently developed and verified which increases reusability, formal analysis, and model validation. The

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

simulation engine provides a global simulation time to
synchronize all modes. For a given set of model
components, initial states and input trajectories, a
simulation will generate output trajectories using a logical
clock, which is independent of the operating system.
RT-DEVS is an extension of DEVS specifically for
real-time system in real-time environment (pertinent
details of RT-DEVS can be found in [9]). It allows a
DEVS model to be developed in a simulation environment
and executed in real time rather than simulation time.
Therefore, instead of virtual time, in RT-DEVS, the
simulator checks a specified time advance of a RT-DEVS
model against a real-time (physical) clock.

6

Discussion

Table compares the methodologies of DEVS and UMLRT. The table focuses on modeling aspects as viewed in
terms of their suitability for developing simulation models
as well as software models. The capabilities as well as

shortcomings of these have been described in the previous
sections. In particular, the overarching examination of the
table suggests the importance of using each framework for
the fundamental capabilities it supports instead of
introducing extensions to it which may weaken and/or
otherwise result in unintended complexities affecting its
use. That is, software and simulation are inherently
distinct – software models are used to develop physical
structural and behavioral specification of a system whereas
simulation models are used to simulate alternative desired
designs of the system. Therefore, it seems important for
neither of these frameworks to subsume the other given
the separation of concerns between the simulated and
physical implementation of a system withstanding the fact
that these have important relationships with one another.
Computational aspect (e.g., performance and distributed
execution) of these frameworks is also a key factor in
resisting the desire to extend one framework to subsume
the other.

Table 1: A comparison of DEVS and UML-RT Frameworks
Modeling
framework
Structure
specification

Coupling
constraints and
communication

Dynamic structure
support
Behavior
specification

Stimuli events and
specification

Timing and
transition

Simulation and
synchronization

DEVS
It is based on systems theory and provides
multiple levels of specification abstractions.
Atomic model consists of I/O ports, coupled
model consist of I/O ports, couplings, and
atomic & coupled models. Strict
hierarchical composition.
All ports are uni-directional. Input ports can
only receive information and output ports
can only send information. Uni-directional
coupling is supported based on modularity
and hierarchy principles.
An extension of DEVS support dynamic
structure modeling (run-time structural
changes).
Internal/external transitions, output and time
advanced functions specify atomic model’s
behavior; coupling defines behavior for
hierarchical coupled models.
Events refer to messages arriving at input
ports or state changes in atomic models.
Messages are specified as part of atomic and
coupled models.
Transition may take zero to infinity.
Provides timing for atomic models as well
as a global time; one or more of simulation,
physical or wallclock time may be used.
Each atomic model has a simulator; each
coupled model has a coordinator; a root
coordinator is used for global coordination
among all models.

UML-RT
It is based on object-orientation; provides different
kinds of model abstractions.
Capsule consists of ports, container capsule
consists of sub-capsules and connectors specify
the capsule interaction. Hierarchical composition,
Generalization/Specialization, dependency, and
association are supported.
Uni- or bi-directional of a port depends on its
implemented protocol. Connections between any
two ports must be compatible based on its
protocols.
Capsule can dynamically create sub-capsules
during run-time.
Statechart is used to specify behavior of capsules
and/or containers.

Stimuli event is specified by a port and signal pair;
events can also be due to capsule’s state change.
Messages communicated between ports are
specified in a protocol.
Transition time is negligible, but must be greater
than zero. Timing service is derived from an
operating system clock and global time is not
supported.
Capsule and containers can be converted into an
implementation by combining them with an
execution framework; event scheduling is used for
coordination.

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

7

Conclusions

Real-time software-intensive system development needs
consideration of both software and hardware parts.
Successful development of such complex systems relies
on models for both high-level system simulation that can
establish a basis toward low-level software design and
development. System theoretic frameworks such as
DEVS provide a formal environment for system modeling
and simulation. Software engineering frameworks such as
UML-RT enable software analysis and design suitable for
real-world operation. System theoretical approach is more
capable for system analysis and verification while
software captures details that are necessary for
implementation of physical systems. Comparison of these
two frameworks, suggests that their combination has the
potential to lead to an overarching framework that is more
than the sum of its parts with respect to real-time software
intensive system design and software development. To
achieve this, it is key to support transformation of
simulation model to their software model counterparts.
However, examination of the simulation and software
models suggests mappings of simulation behavior
specifications to software behavior specification to be
non-trivial. Our ongoing and future work, therefore, is
focusing on developing schemes to support
transformation between simulation modeling and software
models. A key aspect of such schemes is handling of
timing and concurrency issues in real-time distributed
settings.
Acknowledgement: The authors thank the reviewers for
their comments and suggestions which improved the
presentation and quality of the paper. This research is
supported by NSF Grant No. DMI-0122227.

8

References

[1] Manna, Z. and A. Pnueli, The Temporal Logic of Reactive
and Concurrent Systems, Springer-Verlag, 1992.

[2] UML, http://www.uml.org/, 2004.
[3] Wymore, A.W., Model-based Systems Engineering: An
Introduction to the Mathematical Theory of Discrete Systems
and to the Tricotyledon Theory of System Design, CRC, 1993.
[4] Zeigler, B.P., H. Praehofer, and T.G. Kim, Theory of
Modeling and Simulation, 2nd ed, Academic Press, 2000.
[5] Mesarovic, M.D. and Y. Takahara, Abstract Systems Theory,
Springer Verlag, 1989.
[6] DEVSJAVA, http://www.acims.arizona.edu, 2003.
[7] Cho, Y.K., X. Hu, and B.P. Zeigler, “The
RTDEVS/CORBA Environment for Simulation-based design of
Distributed Real-time systems”, Simulation: Transactions of the
Society for Modeling and Simulation International, 79(4), 2003,
p. 197-210.
[8] Hong, J.S., et al., “A Real-time Discrete Event System
Specification Formalism for Seamless Real-time Software

Development”, Discrete Event Dynamic Systems: Theory and
Applications, 7, 1996, p. 355-375.
[9] Kim, T.G., S.M. Cho, and W.B. Lee, “DEVS Framework for
Systems Development: Unified Specification for Logical
Analysis, Performance Evaluation and Implementation”,
Discrete Event Modeling and Simulation Technologies, H.S.
Sarjoughian and F.E. Cellier, Editors, Springer, New York,
2001, p. 131-166.
[10] Kofman, E., Discrete Event Based Simulation and Control
of Hybrid Systems, Faculty of Exact Sciences, Ph.D. Thesis,
National University of Rosario, 2003, Argentina.
[11] Sarjoughian, H.S. and B.P. Zeigler, “DEVS and HLA:
Complementary Paradigms for Modeling and Simulation?”
Transactions of the Society for Modeling and Simulation
International, 17(4), 2000, p. 187-197.
[12] UML-RT,
http://www-306.ibm.com/software/awdtools/
developer/technical/, 2004.
[13] Selic, B. and G. Gullekson, Real-time Object-oriented
Modeling, John Willey & Sons, 1994.
[14] OMG, UML Profile for Schedulability, Performance, and
Time Specification, 2003.
[15] Shroff, M. and R.B. France, “Towards a Formalization of
UML Class Structures in Z”, Proceedings of the Twenty-First
Annual International Computer Software and Applications
Conference, 1997, Washington DC, USA.
[16] Moller-Pedersen, B., "SDL Combined with UML",
Telektronikk, 4, 2000, p. 36-53.
[17] OMG,
Model
Driven
Architecture,
http://www.omg.org/mda/, 2004.
[18] Varro, D. and A. Pataricza, "VPM: A Visual, Precise and
Multilevel
Metamodeling
framework
for
describing
mathematical Domains and UML", Software and System
Modeling, 2(3), 2003, p. 187-210.
[19] Hu, X. and B.P. Zeigler, “Model Continuity to Support
Software Development for Distributed Robotic Systems: a Team
Formation Example”, Journal of Intelligent & Robotic Systems,
Theory & Application, 2004, p. 71-88.
[20] Raynal, M. and S. Singhal, "Logical Time: Capturing
Causality in Distributed Systems", IEEE Computer, 29(2), 1996,
p. 49-57.
[21] Fujimoto, R.M., Parallel and Distributed Simulation
Systems, Parallel and Distributed Computing, John Wiley &
Sons, 2000.
[22] Praehofer, H., “Towards a System Methodology for
Object-Oriented Software Analysis”, in Discrete Event
Modeling and Simulation Technologies: A Tapestry of Systems
and AI-Based Theories and Methodologies, H.S. Sarjoughian
and F.E. Cellier, Editors, Springer, New York, p. 367-388, 2001.
[23] I-Logix, http://www.ilogix.com, 2004.
[24] Hoffmann, H.-P., “From Function/Data-Oriented Systems
Engineering to Object-Oriented Software Engineering: The
Statemate-to-UML
Bridge”,
http://www.ilogix.com/
whitepaper_PDFs/Stm2Rh_whitepaper.pdf, 2004.
[25] Ptolemy II, http://ptolemy.eecs.berkeley.edu/, 2004.
[26] Selic, B. and J. Rumbaugh, “Using UML for Modeling
Complex Real-time Systems”, Rational Software, 1998.
[27] Gullekson, G., “Designing for Concurrency and
Distribution with Rational Rose RealTime”, Rational Software
White Paper, http://www.rational.com, 2000.

Proceedings of the Eighth IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT’04)
1550-6525/04 $20.00 © 2004 IEEE

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W K V. Chan, I Moon, T. M K Roeder, C Macal, and M D Rossetti, eds,

BEHAVIORAL DEVS METAMODELING

Hessam S. Sarjoughian
Abdurrahman Alshareef

Yonglin Lei

Arizona Center for Integrative Modeling & Simulation
School of Computing, Informatics and Decision Systems
Engineering
Arizona State University
699 S. Mill Avenue
Tempe, AZ, 85281, USA

Simulation Engineering Institute
School of Information Systems and
Management
National University of Defense Technology
109 Yanwachi Rd
Changsha, Hunan, CHINA

ABSTRACT

A variety of metamodeling concepts, methods, and tools are available to today's modeling and simulation
community. The Model Driven Architecture (MDA) framework enables modelers to develop platform
independent models which can be transformed to platform-specific models. Considering model
development according to the MDA framework, structural metamodeling is simpler as compared to
behavioral metamodeling. In this paper, we shed light on and introduce behavioral metamodelingfor atomic
DEVS model. Behavior specification for an atomic DEVS model is examined from the standpoint of the
MDA framework. A three-layer model abstraction consisting of metamodel, concrete model, and instance
model is described from the vantage point of the DEVS formalism and the Eclipse Modeling Framework
(EMF), a realization of MDA. A behavioral metamodel for atomic DEVS model is developed in EMF
Ecore. This metamodel is introduced to complement the EMF-DEVS structural metamodeling. Some
observations are discussed regarding behavioral metamodeling, model validation, and code generation.
1

INTRODUCTION

A variety of methods may be used to represent time-based dynamics of systems. The behavior of a system,
for example, can be modeled using set-theory, UML diagrams, and pseudo code. Each kind of model serves
certain purposes and must ultimately be mapped to programming code suitable for execution in one or
possibly multiple target simulators. A mathematical model is useful for defining a system's structure and
behavior independent of software design and simulation technologies. UML Class and Statecharts diagrams,
among others, are useful for designing complex modeling and simulating engines which may or may not
necessarily have mathematical grounding. Computer code can be developed and/or partially generated based
on mathematical or certain kinds of software specifications. Each of these methods has its strengths and
weaknesses and none is currently considered to contain all the necessary capabilities required for generating
executable simulation code.
The atomic and coupled models in the DEVS formalism (Zeigler, Sarjoughian, and Au 1997) are
"metamodels". From the standpoint of MDA, DEVS has an abstract syntax and an execution semantics
which together define a modeling language for discrete event systems. The set-theoretic DEVS models are
abstract mathematical artifacts. An atomic DEVS has its elements defined, for example, as sets, functions,
and relations. These model elements individually and collectively satisfY certain general abstract properties
and constraints. For example, a model can receive a finite number of input events within a finite period of
time at arbitrary time instances, process these inputs with state changes within a time period, and generate a

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

2788

Sarjoughian, Alshareef, and Lei

finite number of output events. It is the responsibility of the modeler to show that the developed atomic
models for a given target simulator satisfy the properties and conform to the constraints defined for the
DEVS atomic formal specification.
In the MDA framework, a concrete atomic DEVS model for a system component, relative to its
metamodel, has specific structural (e.g., inputs and states with possible specific values) and behavioral
elements (e.g., state transitions for specific source and target states with assigned times to next events). The
metamodel is a language within which concrete models can be developed. Furthermore, a concrete model
may also satisfy constraints such as state variable types and state transitions sanctioned for specific
application domains. Full-fledge behavioral DEVS metamodeling can support automatic conformance of
concrete models to their metamodels. This capability can significantly reduce the amount of manual effort
required to show concrete models satisfy their metamodel properties and constraints.
From a tool's perspective, a simulator such as DEVS-Suite (ACIMS 2015) is designed as a collection of
UML classifiers and relations that capture some aspects of the set-theoretic atomic and coupled parallel
DEVS models. These models can also be collectively referred to as a DEVS UML "metamodel". The
inputs, states, and outputs, and internal, external, output, and time advance functions of the model are
defmed abstractly; they by themselves are not executable. For example, the data structure for input is
defmed as a pair (port-name and input-variable) where port has a string type and input variable has an entity
type. Similarly, the external transition function is defined as a method with specific arguments, but without
any actual implementations for the state transitions and conditions under which they are to be performed. As
in its mathematical counterpart, a concrete atomic model must have instances of the port-name and input­
variable attributes belonging to the UML classes and interfaces. The realization of the formal DEVS models
as UML specifications is advantageous. UML includes abstractions such as data typing, return types, and
control structures that enrich the abstract atomic DEVS model specification. These models can be
transformed to partial code for programming languages using professional tools dating back to the 1990s.
Simulators such as DEVS-Suite do not explicitly account for domain-specific modeling. A modeler can
develop domain-specific models using object-oriented modeling principles and design patterns. The
domain-neutral contracts embodied in the DEVS UML models can be enforced in an ad-hoc manner using
low-level techniques such as checking for data type compatibility and expected values for concrete models
that are implemented in some specific programming languages. These contracts cannot account for domain­
specific knowledge; they must be extended. This approach becomes complicated and unwieldy as scale and
complexity of the system to be simulated increase. Such resulting simulators lack rich capabilities to support
and develop domain-specific metamodels and also are unable to validate basic model properties and
constraints such as data typing and legitimate state transitions, for example. MDA-based modeling,
however, can lend itself to develop and automatically validate behavior of any domain-specific DEVS
concrete model against its metamodel and by extension the general-purpose atomic DEVS model.
Given the above discussions, we can make a few observations. When concrete atomic DEVS models
are developed using programming languages, it is difficult to ensure they conform to their abstract model. A
substantial amount of effort is required to concretize behavioral abstractions. Therefore, it is important for
the meta and concrete atomic models to be systematically related to each other as proposed in the MDA
framework. This is especially important given that the challenging part of developing models of complex
systems is specifying their behaviors. Therefore, we need an atomic DEVS metamodel which can support
behavioral modeling (e.g., receiving sanctioned input events and legitimate state transitions with timing).
Toward this goal, we propose behavioral metamodeling for the general-purpose and domain-specific atomic
models using the Eclipse Modeling Framework (Steinberg et al. 2008). Consistency between these models
can be specified and enforced (referred to as validated) with automation. Concrete models can be generated
from their domain-specific metamodels. Behaviors contained in these metamodels can significantly reduce
the amount of effort to create concrete models and improve their quality using automated code generation.

2789

Sarjoughian, Alshareef, and Lei
2

BACKGROUND

In this work, our goal is to develop concepts that can enable building a framework capable of specifying
meta-behavior for atomic DEVS models that can be used to create concrete atomic DEVS models. Toward
this goal, we employ Model-Driven Engineering (MDE) and in particular, the MDA framework with its
EMF realization. Although there are a variety of DEVS-based modeling and simulation tools, in this work
we use the DEVS-suite simulator for developing the proposed behavioral DEVS metamodel.
2.1

MDA and Model Layers

The Model Driven Architecture (MDA) framework has been proposed for developing software systems
(OMG 2003). Its main concept is a four-layer model abstraction hierarchy. A key abstraction concept in
MDA is for a classifier and its instances to form a two-layer hierarchy. A classifier has an abstract
specification that can have one or more instances. Classifiers can be said to be universal and instances can
be said to be specific. Every classifier is at a higher level of abstraction in relation to its instance. Instances
are related to one or more classifiers via conformance relationship. This implies having complementary
models each of which having a certain role to play and collectively provide a disciplined roadmap for
developing software systems. Each higher-level layer provides capabilities that are more abstract as
compared to those provided by lower-level layers. Conversely, each layer is built using the elements
provided in the layer above.
A realization of the MDA approach consists of Meta Object Facility (MOF), Unified Modeling
Language (UML), User Model, and User Object modeling layers (OMG 2003). At the meta-metamodel
(M3) layer, the MOF has an Ecore specification for defming metamodels in the OMG's family of MDA
languages. Defined using the UML metamodel, the M3 layer supports computation-independent metadata
management, metadata services, model management, tag capability, and reflective operations among others.
The metamodel (M2) layer can have models that conform to the M3 layer. The M2 layer is directed at
platform-independent modeling. These models can be domain-specific. The Ecore at the M2 layer can be
used to define concrete models at the Ml layer. The MO layer is used to define instances of models specified
at the Ml layer. The M3, M2, Ml, and MO layers support incremental development of models for
component-based systems. It is useful to note that the separation of concerns in MDA is important for
developing software system tools including simulators.
2.2

DEVS Atomic Model

The set-theoretic specification of parallel atomic model (X, 5, Y, 0ext, Oint' aeon!, il, ta) is domain-neutral.
Its input and output are defined in terms of port names and variables. The variables can be arbitrarily
complex. Atomic models are responsible for handling differences in the input and output variables. From
software design, appropriate 110 type consistency is required. For any user-defined (and domain-specific)
model, the internal, external, and confluent, time advance, and output functions can have arbitrary logic as
long as they satisfy the abstract definitions provided in the mathematical atomic model specification. A
restricted specification of parallel DEVS called Finite Deterministic DEVS (FD-DEVS) (Hwang and
Zeigler 2009) has been developed. Events and states are defined to be finite sets and external and internal
events are allowed to occur at time intervals restricted to rational numbers. No time interval between one
event and the next can be infmitely small. This is achieved by abstracting time to be rational instead of real
numbers. When states are simple, possible state transitions can be enumerated and unreachable states,
identified. These restrictions can simplify model validation for the EMF-DEVS modeling described next.
2.3

EMF-DEVS Atomic Model

The EMF-DEVS (Sarjoughian and Markid 2012) is proposed as a metamodeling approach for the parallel
DEVS formalism. The basic aim is to define and validate DEVS metamodels using the Eclipse EMF

2790

Sarjoughian, Alshareef, and Lei

framework. The EMF validation infrastructure is used to define the elements of DEVS models with a set of
constraints defmed according to the DEVS formalism and the target DEVS-Suite simulator which is
implemented in the Java programming language. Structures of atomic and coupled meta-DEVS models can
be modeled and validated. The generic capabilities provided in the EMF M3 and M2 layers are extended to
support concrete models for the DEVS-Suite simulator. The EMF-DEVS metamodel can support input,
output, and state sets as well as external, internal, output, and time advance functions. These abstract
functions (oext, Oint' 0eon!, it, ta) do not include the logic that is necessary to define behaviors. For
example, the external transition function Oext does not define a generic transition from a source state to a
target state with constraints and the output function it does not defme conditions for generating outputs.
In the context of metamodeling as in EMF-DEVS, the term validation refers to the Eclipse EMF
validation framework and its execution engine. The Eclipse EMF has built-in validation mechanisms such
as reflection for the metamodels at the M2 layer. Metamodels at the M2 layer can be validated for
conformance to the meta-metamodel at the M3 layer. Concrete models at the Ml layer can also be validated
to conform to DEVS metamodel. Here validation does not refer to execution of a metamodel over some
period of time and determine whether or not it produces behavior per user requirements and expectation.
Given a concrete simulation model (Ml layer), it can be verified to be specified correctly both in terms of
Ml and M2 layers. When executed over some period of time and its behavior is recognized as acceptable
for some defmed experimental condition, the model is said to be valid. With respect to the verification and
validation definitions for concrete models, the EMF-DEVS validation may be referred to as verification
when a metamodel has domain knowledge (e.g., external transition function has the necessary control
structure and other details to specify next state of a model given its current state and received input).
3

RELATED WORK

In this section, we primarily focus on behavioral DEVS atomic metamodeling and briefly consider the
extent in which detailed specifications can be supported. Model-driven design approaches have been playing
a greater role in developing complex simulation models. Focusing our attention on the OMG MDA
framework and DEVS, we find some approaches that follow the MOF Technology Space (Bezivin and
Kurtev 2005). In (Lei et al. 2009), a DEVS metamodel is devised for developing SMP2 (Simulation Model
Portability standard). This metamodel is mapped to SMP2 metamodel using QVT (OMG 2003). Basic
simple states and state transitions for atomic DEVS model are supported. In (Cetinkaya, Verbraeck, and
Seck 2012), structural DEVS metamodeling can be supported. As in EMF-DEVS, behavior specification for
atomic DEVS metamodel is not supported (see Section 2.3).
In the MOF technology space, some works have employed DEVS Natural Language (DNL), XML
Schema, and Extended BNF for defining DEVS models. These support behavioral modeling using mostly
the same ideas and methods. The MS4Me (Seo et al. 2013) focuses on modeling using DNL (Zeigler and
Sarjoughian, 2012). The DNL as meta-language supports Finite-Deterministic DEVS models (Hwang and
Zeigler 2009). MS4Me uses Xtext (Xtext 2013) to enforce DNL rules for simple inputs, outputs, states, state
transitions, and timing. As a modern Java-like language, Xtend (Xtext 2013) supports developing FD-DEVS
models. The MS4Me models can be augmented to become Parallel DEVS models using the full
expressiveness of the Java language. It supports adding Java code to the model and thus developing Parallel
DEVS models while maintaining a tight connection with the FD-DEVS models. The Java code is injected
into slots in a structured manner using tagged code blocks. These are inserted directly into the generated
source files. These tagged code blocks are used to specify additional behavior for initializing, internal
transition, external transition, and output. Compared with FD-DEVS, classic or parallel DEVS models that
have these kinds of code blocks are difficult to validate. The DEVSML (Mittal, Risco-Martin, and Zeigler
2007) is developed to for DEVS simulation models that can be executed in net-centric computing
environments.
Some works employ SysML (Nikolaidou 2008) and UML (Borland 2003) (Risco-Martin et al. 2009)
(Mooney and Sarjoughian 2009) (Pasqua et al. 2012). A SysML profile is developed for classical DEVS. An

2791

Sarjoughian, Alshareef, and Lei

atomic model is defined as a collection of stereotype blocks. The behavior is defined by States Definition
and Association diagrams. Atomic Internal and External diagrams are defmed for the internal and external
functions, respectively. The time advance and output functions are defined as part of the Atomic internal
diagram. Similar to the above approaches, simple states with constraints are defined. The external diagram
follows FSM with control elements such as choice, fork, and join elements. Time allocated to states can
only be defined in the internal diagram. The DEVS SysML profile and DEVS MOF are intrinsically
different due to their technology spaces. There exist other approaches that use "metamodeling" abstraction
(Fard and Sarjoughian 2015; Ighoroje, Mai'ga, and Traore 2012; de Lara and Vangheluwe 2004). A survey
discusses uses of some MDE approaches for DEVS (Garredu et al. 2014).
4

ATOMIC DEVS METAMODELING

The mathematical properties and constraints defining an atomic DEVS model can be applied to any
implementation of it. Therefore, it is useful to have a framework that can not only capture the atomic
model's formal specification (i.e., a metamodel), but also enforce its syntax and semantics for domain­
specific metamodels. Another important advantage is to define models independent of any particular
simulator-i.e., metamodels can be transformed to concrete models that can be executed in simulators that
are implemented in specific computing platforms. This framework must (help) validate behavior of any
concrete atomic DEVS model against its metamodel. To achieve this, we propose introducing behavioral
metamodeling to structural metamodeling. The resulting metamodeling framework must also lend itself to
developing metamodels for modelers' domains of interests. This framework is also desired to support
defming domain-specific concrete models for desired systems.
We intuitively define behavioral metamodeling as a set of concepts realized in a framework that
supports specifYing operational details of the internal, external, output, and time advance functions of any
atomic DEVS model. These generic operations can be used to define behavior for any domain-specific
DEVS metamodel. Domain-specific behavior can be specified by extending the generic DEVS metamodel
behavior. That is, behavior of these functions are defmed independent of computing platforms in which they
can be fully implemented. The properties and constraints in the domain-neutral and domain-specific
functions for the concrete models can be validated. The properties and constraints of the functions that are
not satisfied in any concrete model are automatically identified and reported.
Figure 1 illustrates the concept of "meta" and "concrete" mathematical and UML modeling. The
structure, unlike behavior, of mathematical atomic and coupled DEVS models can be completely specified
both abstractly (as a metamodel) and concretely (as a concrete model). In mathematical modeling, a
concrete model has more information relative to its metamodel. In the metamodel, 0ext, Oint, Dean!' it and
ta functions are abstract mathematical constructs. The abstract atomic DEVS model functions do not have
sufficient details, for example, as in Statecharts. Indeed Statecharts also does not capture the levels of detail
in the functions that an arbitrary atomic model can have. In contrast, arbitrary concrete atomic models must
have details including decision logics and control in the state, output, and timing functions.
The concept of meta and concrete models in UML are distinct as compared with the ones just described
for a mathematical model. While UML metamodels are independent of computing platforms, concrete­
models are not. Separating models to be platform independent and platform-specific is important (see
Section 2.1). Meta models are technology (simulator) agnostic. Concrete models include details that are
specific to target simulators. The meta and concrete models can be related to one another.
Focusing on behavioral modeling, the line arrows from the concrete model and metamodel are
conceptual. For mathematical modeling, one may construct relationships to show, for example, state
transitions in an external transition function in a concrete model conform to the abstract external transition
function specification. In UML modeling, one can include rules that can be applied to concrete models. The
block arrows at the metamodel and concrete model levels involve complex modeling and software
development tasks, requiring detailed design and code development.

2792

Sarjoughian, Alshareef, and Lei

Considering the distinct roles mathematical and UML modeling offer, a desirable goal is to support
both. The EMF framework (Steinberg et al. 2008) is a strong candidate as it already supports UML meta­
and concrete modeling and it can support developing specific metamodels as in EMF-DEVS. In particular,
the relationship between meta (M2 layer) and concrete models (Ml layer) is formalized. Furthermore, the
EMF includes the metametamodel (M3 layer) and instance models (MO layer). Given these, we extend the
EMF-DEVS (Sarjoughian and Markid 2012) structural metamodeling to enable behavioral (functional)
metamodeling. Generic and domain-specific metamodels with built-in and user-defined properties and
constraints for the external, internal, output, and time advance functions are supported. Modelers may
develop metamodels in a structured setting, thus leading to automation of metamodel validation as defined
in EMF. (We note that validation is not referring to simulation validation.) Constraints defined for the
generic and domain-specific atomic DEVS metamodels enable validating concrete atomic models.
4.1

Meta-behavior Modeling in EMF

We begin by sketching the basic details of the M2, Ml, and MO layers for the atomic DEVS model shown
in Figure 1. At the M2 layer, the Ecore is an instance of the Ecore at the M3 layer. The M3 Ecore
metamodel is at a higher level of abstraction with respect to the atomic DEVS metamodel. That is, the
DEVS metamodel extends the instance of the M3 Ecore. The role of the M2 layer is to support developing
concrete models at the Ml layer.
EMF Model

Mathematical

I
I

behavior

UML

¢

structure

Mathematical

I

¢[

( behavior
( structure 1
UML

meta

¢
concrete

I
I

layers

I

�core
Gene",1 DElIS meta-model

I structure II

behavior

I structure II

behavior

I

Dom.ln·speclnc DEVS meta-model

I
I

�msro

ton(;(l!te domCiina
specific model

I conforms fO
Mooel in'5lance

I

I
I

I
I

....

::;

...

::;

0

::;

Figure 1: From mathematical to UML to EMF modeling.
As noted earlier, the DEVS-Suite simulator is developed in Java, a strongly typed language. The kernel
of the modeling engine contains data structures and operations that satisfY the DEVS modeling formalism.
Thus, at the Ml layer, user-defined models can be generated from the DEVS metamodels. Suppose we want
a Processor model which can receive bags of input, process one of them, and generate one or more outputs.
Assuming we have an eProcessor metamodel, it can be used to create the concrete Processor model. This
concrete model at the Ml layer can be created for a platform-specific simulator such as DEVS-Suite. An
instance of the concrete model at the MO layer can be executed by the DEVS-Suite simulator.
In MDA, the MO layer refers to the instances of the user models. These can be physical objects or
executable software objects (e.g., compiled code). Such instances can be modeled as UML Object diagrams.
As software objects, they can exist at execution time and their states may be stored, for example, as XML or
byte code. In contrast, for simulation, the MO layer refers to the user's parameterized atomic and coupled
models. Therefore, at this layer, we have not only parameterized models but also their instances as part of
other coupled model instances (see Figure 1).

2793

Sarjoughian, Alshareef, and Lei

Although metamodeling is not as expressive as programming languages such as Java, it is shown to be
useful, for example, as in the Graphical Modeling Framework (Gronback 2009). The metamodel behavior
specification for DEVS functions is achievable using Statecharts (Harel 1987). The elements of a parallel
atomic model at Ml can be arbitrarily complex. An example is the external transition function. It can have
any attribute type, expressions, and control structures that a target computing platform supports. The
signature defmitions for the atomic model external and internal transition functions can be defined using
structural metamodel as in EMF-DEVS. The abstract definitions for these two functions must include some
operations needed to result in some appropriate state change. State changes in these functions can be defined
as transitions amongst source and target states. A transition may have input event, condition, and actions. A
prototypical state transition is defined to transition from a source state to a target state. Such a constraint for
state transitions can be defined and validated at the M2 layer. The output and time advance functions can
also be defined using operations and control structures. An operation can have attributes and statements
(McNeill 2008). A metamodel behavior specification requires identifying abstractions for state transitions in
the external, internal, and confluent transition functions. Similarly appropriate abstractions are needed for
the output and time advance functions at the M2 layer. The behavior of all DEVS functions as just described
can be validated using EMF. The defmitions for the atomic model functions must be consistent with the
abstract DEVS simulation protocol.
In order to model the content of EOperation, we need to extend the EMF Ecore metamodel (McNeill
2008). Therefore, we will extend the Ecore metamodel to model DEVS functions that have been defined as
EOperations (i.e., interface definitions) in EMF-DEVS. Our goal is not just to validate domain metamodels.
We also aim to execute these functions after concrete models are generated for a specific simulator, DEVS­
Suite for instance. The code generation creates the corresponding code for the defined elements in the
metamodel. In EMF, the generator model plays a significant role in how the resulting code could be
generated and organized via some settings that may differ based on the targeted platform. Those settings can
be configured separately to ensure that the model maintains its platform independency. The process can be
manipulated in a way that will lead to producing concrete models.
Thus, the general metamodel, shown in Figure 2, extends the EMF Ecore metamodel with some
defmitions for state transitions, actions, and conditions, basic elements of the atomic DEVS model. The
metamodel extends Ecore elements with DEVS functions and also others for defining behavior. By
extending Ecore, we are enabling EOperation (which is used to define DEVS functions) to include some
content which can be transformed into concrete code rather than just having operation signatures. The
extended EOperations will be contained in the extended EClass (eAtomic in our case) since they cannot be
contained in EClass itself. This is a reason for extending EClass and EPackage since the Ecore elements
themselves (EClass and EPackage) will not allow adding the extended ones (Extended EClass and
EOperation) (McNeill 2008). Therefore, we first extend EOperation as a basic step to support behavioral
DEVS metamodeling. Second, we extend EClass to allow adding the extended EOperation. The third step is
extending EPackage to allow adding the extended EClass.
The second part of the metamodel (shown in the middle of Figure 2) is specializing eDEVSOperation to
represent external transition, internal transition, output, and time advance functions. All of these can include
operations that have statements and local variables. They also may have return values. The eDeltExt and the
eDeltint represent external transition and internal transition functions. Both compose transitions defined to
capture the concept of state transition. State transition has a name defined as an EString, source and target
defined as an ETypedElement, input defined as an optional reference of type eInput to be used in the
external transition function. It can also have some actions and conditions. We also added two specialized
state transitions for the phase and sigma primary states. Source and target phase are added to the state phase
transition (StatePhaseTransition) and defined as an EString. Source and target states for sigma are added to
the state sigma transition (StateSigmaTransition) and defined as an EDouble. Any other specific state
transition can be also defined in the same manner for domain specific models. The behavior is consistently
captured at the general and domain-specific metamodeling at the M2 layer. The generic behavioral

2794

Sarjoughian, Alshareef, and Lei

metamodel is predefined for the modeler. The domain specific meta-behavior can be defined by the modeler
as needed. The same approach is followed for the actions and conditions that are defmed abstractly and then
specialized to provide the support for developing the behavior at the concrete model. The eOutput and eTA
elements refer to the eState in addition to the inherited composition feature from eDEVSOperation to
support having other operations for more functionalities.
•

GeneralBehavior

J

-I

I
§

I

I

§

I.

localVariable

1

i
I

§

J

I

DEVSBehavior

I

l

§

i

J

[0 .. *1 actions

l

§

Condition

I

J

§
D

§

§

EClass

l

!tl

111

§

l

I

.. 1 statement

§

dOData

r

I
J

I
l
I
J

eAtamic

I
l

§

EPackag.

fIo"l

§

r

I
J

eDEVSPackage

I
J

I

I

[2.. "'] exlTransitions

JiiL

!tl

source_sigma: EDouble

t:l

target_sigma: EDouble

l

[2.. "1 intTransitions

§

StateSigmaTransition

I::l

J §

=
=

0.0

0.0

.DeltExt

i

I

eState

l(-

I

J

'-----

r;;j

111

§

[L1J state

[l, .1J stat.

[l..lJ st.te

eDEVSOperation

l

StateTransition

§

I

r§

name: EString

[1..1] st.te

[O ..lJ output

[O..lJ input

I
J

i

[0,,"]conditions

I

to
JiiL

J

I

[O .. 'J inputs

'J'

I

�_ction

J
J

Statement

l
•

DEVSStructure

{O .. *]eoperationim"pl

(O..l]localva ria ble

I

I

•

I

if

EOperationlmpl

[0 ..1] returnType

EOperation

,

I

[O .. *]localvariable

1

§

!tl

I

eDeltlnt

t

I

J

I

§

I

eTA

I

!tl

I

I

I

§

I

eOutput

l

StatePhaseTransition

D

source_phase: ES-tring

t:l

target_phase: EString

Figure 2: A metamodel for atomic DEVS Model with state transitions.

4.2

Constrained Meta-behavior Modeling

The metamodel shown in Figure 2 is based on the parallel atomic DEVS model. This model has an infinite
state-space and therefore model validation (as in model checking) is impractical. A sub-class of DEVS
called Finite-Deterministic DEVS (FD-DEVS) (Hwang and Zeigler 2009) has fmite state-space which
makes it attractive for behavior modeling at the M2 layer. The total state of the atomic DEVS metamodel
can be defmed as {primary} x {secondary} x JR{[O,co). An atomic FD-DEVS model restricts the range of
values for the time advance function to �[O,ool' Model validation is computable when the values for inputs,
outputs, and states (including time to next event) are finite. These constraints can be validated for having
legitimate output, time advance, and internal and external transition functions. Constraints for state
transitions (belonging to both external and internal transition functions) can be validated. For example,
states in any state transition can be validated to include only the states defined in the model's state set and
there are no unreachable states. For the external event, its input event can be checked to be included in the
input set. State to output mappings can also be validated by checking whether or not every output belongs to
the output set. We can also check if outputs are computed using states that belong to the state set. Time to

2795

Sarjoughian, Alshareef, and Lei

next event for every state transition must also belong to �[O,ool' When the time interval is infinity, three is no
output. Validation of behavior domain-knowledge can be augmented with user-defmed constraints.
Considering a domain-specific metamodel, they may have their own constraints on the input, output,
and state sets as well as the atomic model functions. These constraints must be defmed by the user, for
example, by extending the EMF-DEVS metamodel. Users may specify domain-specific constraints using
the EMF Eclipse framework and tool. Of course, user-defined constraints cannot contradict those that are
defined for the generic metamodel. We note that the restrictions in the atomic FD-DEVS model and its
dynamics may require complex control structures. State transitions in the external (or internal) transition
function may have to be synthesized in complex patterns. Transitioning between external and internal
transition functions can have many configurations. Similarly, the output and time advance functions may
have complex structures. These considerations restrict the behavioral metamodeling describe above.
Nonetheless, the capabilities afforded by MDA is advantageous as compared with model development
where there is little or no means to start from metamodeling and reach executable models. Specific state
transitions can be individually validated at the M2 layer. Behavioral metamodeling developed in this
research aids model validation before transforming them to an Ml model and MO simulation. Once concrete
FD-DEVS models are generated from metamodels, they can be validated using existing techniques and tools
(Dill 1990, Hwang, and Zeigler 2009)
5

A PROCESSOR EXAMPLE BEHAVIORAL METAMODEL SNIPPET

In this section, we will demonstrate the process of developing a domain specific model (eProcQ as shown in
Figure 3), which represents a simple processor with a queue. The processor metamodel is developed using
the defmition provided at the atomic DEVS metamodel. The root element is eDEVSPackage, which can
contain the eAtomic models such as eProcQ and any other EClass such as Entity and Queue. Entity and
Queue EClasses are defined similarly to their definition in the DEVS-Suite GenCol library (ACIMS 2015).
Figure 3.a shows all the model elements in the EMF editor and Figure 3.b depicts the corresponding Class
Diagram for the eProcQ Ecore model. Detailed specifications are provided for the external transition
function relative to other modeled elements such as model states and variables.
We created two transitions and gave the values associated with each one. The first transition is for the
phase and the other one is for the sigma. Figure 3.c shows the specified properties for the state phase
transition that complies with the state phase transition definition. The phase transition has a condition and an
action. The condition is modeled as an inequality for the queue size and the action is modeled as a method
call for add operation, which is defined in Queue EClass. The action allows specifying the object, an action
name that can be any operation associated with that object, and parameters. All of them have been defined
as EReferences to their targeted model elements (see Figure 3.d). Figure 3.e shows an inequality condition
specified based on the queue size. It has a left hand side which is specified as an action (queue.sizeO as
shown in Figure 3.t) and right-hand side which is specified as an integer value of type EInt in this case.
Currently, the metamodel is limited for only those scenarios since they are the only scenarios defmed within
the atomic DEVS metamodel. The implementation is done on a Windows 7 Computer. The models are
created using Eclipse Mars Milestone 6 with Eclipse Modeling Tools and EMF Ecore 2.ll.
6

CONCLUSIONS

The term metamodel invokes different understandings since it refers to some model abstracted to another. It
can encompass theories, methods, tools and domains of discourse including simulation. As such,
"metamodeling" is used by theorists, developers, and practitioners in software and simulation engineering,
among others. In this paper, we considered the modeling formalisms, and in particular asked at what levels
of abstraction can the behavior of a prototypical atomic DEVS model be specified. Our inquiry is to
distinguish meta-, concrete, and instance modeling layers from the standpoint of Model Driven Architecture.
These layers can form a basis for building a new generation of modeling and simulation frameworks and

2796

Sarjoughian, Alshareef, and Lei

tools that can help move from metamodeling to simulation code step-by-step. It is helpful to have modeling
methods with tools that can not only represent mathematical abstractions within the MDA layers, but also
introduce capabilities to enforce verification and validation as much as possible in the M2 before resorting
to the Ml and MO layers.
�

Ii:l

p latform;/resouree/EM FDEVS_P rocessor1m odell ePro c. ecore

,. � eDEVS Package ProcPacK

I

[j

,. § eAtomic eProcQ

Input

.. � eDelt Ext deltExt
� e: EDouble
� input: Entity
.. oQo State Phase Transition U
� Action
.. � Inequality LESSJHAN
� Action
oQo State Sigma Transition t2
CI

Name
Source
Source pha.se
Target
Target phase

queue: Queue

c
...

job: Entity

r::l

.. <¢> eState
phase: EString

CI

sigma: EDouble

Localvariable

� e1nputJob
.. GI add(Entity)
� entity: Entity
.. GI remove(Entity)
� entity: Entity
GI sizeQ

[lj

name: EString

.. deltExt{e EDouble,

Il

§

0.0

input Entity)

r"lJiOb
,"my

=

Cl name: [String

!iii

add(Entity)

Object

c"

queue: Queue

[0 ..11 q u e ue

�

input: Entity

Properties �

Ll

$ add(entity Entity]
lit remove(entity Entity)
$ sizeD

[@ LESS_THAN

RH5

WlO

Prop erti es �

Property
Localvariable
Name
Object

I

Parameters

J

(b) A class diagram for the processor

Value

Operator

(e) Less than inequality for Transition t1

§ Queue

<eProcQ

Value
�

Property

(a) Ecore editor view for the processor
EI

phase: EString

CI

I@ active

(d) Action for Transition tl

.. § Entity

Cl processing_time: EDouble

pas>ive

Name
Parameters.

.. § Queue

CI

phase: EString

CI

!@'

Properties �

Property

CI

Value

� e1nputJob
I@U

(c) Phase change for Transition t1

proce,sing_time: EDouble

c
...

Properties �

Property

Value

I@
!iii
c
..

sizeO
queue: Queue

�

(t) Left hand-side for the less than inequality for Transition tl

Figure 3: Ecore for a processor with primary state transitions for the external transition function.
One of the challenges facing building such ideal modeling and simulation tools is the difficulty of
specifYing behavior of models. We focused our attention on the atomic DEVS model. We proposed defining
meta-behavior for general and domain-specific modeling using the concept of state transition from
Statecharts for external and internal transition functions (see Figure 3). We then extended the EMF Ecore
operation with the external, internal, output, and time advance functions. These functions, unlike the
mathematical counterparts, can have some of their behaviors defined. These functions can also be validated
to a limited degree. To validate, we described the necessity of restricting DEVS to Finite-Deterministic
DEVS. We developed an example to show behavioral metamodeling for the atomic DEVS model. We

2797

Sarjoughian, Alshareef, and Lei

focused this paper on the platform-independent metamodeling and briefly discussed its role for developing
platform-specific tools.
Looking further into metamodeling, we observe that a target simulator must lend itself to the behavior
defined in terms of state transitions, output, and time advance functions. Each function can have parts that
are arbitrary and specific to the system being modeled. Thus, mapping behavior at a higher-level abstraction
(as in the M21ayer) to lower-level abstractions (as in Ml and MO layers) involves execution semantics (e.g.,
simulators may handle simultaneous event and communication differently despite being consistent with the
abstract simulation protocol). Thus, it is desirable to lift behavior modeling as much as possible to the M2
layer with support to checking syntax and semantics with as little dependency as possible on the Ml and MO
layers, it is necessary to account for simulator design/implementation choices.
Knowing the high degree of DEVS expressiveness and the MDA framework, it is easy to see
approaches that such as FD-DEVS should simplify development of verification and validation methods and
tools. The degree to which the behavioral meta-model may be applicable to other kinds of modeling
formalisms also remains as future work. In particular, for models that cannot be represented as DEVS, our
approach for specifying meta-behavior may turn out to be useful. Finally, we believe exciting, challenging
theoretical, methodological, developmental, and practical research remain to be formulated and answered
for achieving general and domain-specific multi-layer behavioral modeling including meta-modeling.
REFERENCES

ACIMS. 2015. DEVS-Suite Simulator. Verso 3.0.0. http://devs-suitesim.sourceforge.netl.
Bezivin, J., and I. Kurtev. 2005. "Model-based Technology Integration with the Technical Space Concept."
In Proceedings of the Metainformatics Symposium. New York, NY .
Borland, S. 2003. Transforming Statechart Models to DEVS. McGill University.
Cetinkaya, D., A. Verbraeck, and M. Seck. 2012. "Model Transformation from BPMN to DEVS in the
MDD4MS Framework." In Proceedings of the Theory of Modeling and Simulation - DEVS Integrative
M&S Symposium. Orlando, FL.
de Lara, J., and H. Vangheluwe. 2004. "Defining Visual Notations and Their Manipulation Through Meta­
Modelling and Graph Transformation." Journal of Visual Languages and Computing 15 (3-4): 309-330.
Dill, D. L. 1990. "Timing Assumptions and Verification of Finite-state Concurrent Systems." In
Proceedings of the International Workshop on Automatic Verification Methods for Finite State Systems.

197-212. New York: Springer-Verlag.
Fard, M. D., and H. S. Sarjoughian. 2015. "Visual and Persistence Modeling for DEVS in CoSMoS." In
Proceedings of the Theory of Modeling and Simulation - DEVS Integrative M&S Symposium. 962-969.
Washington DC.
Garredu, S., E. Vittor, J. Santucci, and B. Poggi. 2014. "A Survey of Model-Driven Approaches Applied to
DEVS - A Comparative Study of Metamodels and Transformations." International Conference on
Simulation and Modeling Methodologies, Technologies and Applications. 179-187. Vienna, Autria.
Gronback, R. C. 2009. Eclipse Modeling Project: A Domain-Specific Language (DSL) Toolkit. Addison­
Wesley Professional.
Harel, D. 1987. "Statecharts: A Visual Formalism for Complex Systems." Science of Computer
Programming (Elsevier) 8 (3): 231-274.
Hwang, M. H., and B. P. Zeigler. 2009. "Reachability Graph of Finite and Deterministic DEVS Networks."
IEEE Transactions on Automation Science and Engineering 6 (3): 468-478.
Ighoroje, U., O. Marga, and M. Traore. 2012. "The DEVS-driven Modeling Language: Syntax and
Semantics Definition by Meta-modeling and Graph Transformation." In Proceedings of the Theory of
Modeling and Simulation - DEVS Integrative M&S Symposium. Orlando, FL.
Lei, Y., W. Wang, Q. Li, and Y. Zhu. 2009. "A Transformation Model from DEVS to SMP2 based on
MDA." Simulation Modelling Practice and Theory (Elsevier) 17 (10): 1690-1709.

2798

Sarjoughian, Alshareef, and Lei

McNeill, K. 2008. "Metamodeling with EMF: Generating Concrete, Reusable Java Snippets."
http://www.ibm.comldeveloperworks/library/os-eclipse-emfmetamodel/.
Mittal, S., J. L. Risco-Martin, and B. P. Zeigler. 2007. "DEVSML: Automating DEVS Execution Over SOA
Towards Transparent Simulators." In Proceedings of the DEVS Integrative M&S Symposium. 287-295.
Norfolk, VA.
Mooney, J., and H. S. Sarjoughian. 2009. "A Framework for Executable UML Models." In Proceedings of
the DEVS Integrative M&S Symposium. San Diego, CA.
Nikolaidou, M., Dalakas, V., Mitsi, L., Kapos, G.D. 2008. "A SysML Profile for Classical DEVS
Simulators." The Third International Conference on Software Engineering Advances. 445-450. Sliema,
Malta.
OMG. 2003. MDA Guide. http://www.omg.orglcgi-bin/doc?omgl03-06-01.
Pasqua, R., D. Foures, V. Albert, and A. Nketsa. 2012. "From Sequence Diagrams UML 2.x to FD-DEVS
by Model Transformation." In Proceedings of the European Simulation and Modelling Conference.
463-471. Essen, Germany.
Risco-Martin, J. L., J. M. Cruz, S. Mittal, and B. P. Zeigler. 2009. "eUDEVS: Executable UML with DEVS
Theory of Modeling and Simulation." Simulation Transactions 85: 750-777.
Sarjoughian, H. S., and A. M. Markid. 2012. "EMF-DEVS Modeling." In Proceedings of the Theory of
Modeling and Simulation - DEVS Integrative M&S Symposium. Orlando, FL.
Seo, C., B. P. Zeigler, R. Coop, and D. Kim. 2013. "DEVS Modeling and Simulation Methodology with
MS4 Me Software TooL" In Proceedings of the Theory of Modeling & Simulation - DEVS Integrative
M&S Symposium. San Diego, CA.
Steinberg, D., F. Budinsky, M. Paternostro, and E. Merks. 2008. EMF: Eclipse Modeling Framework.
Pearson Education.
Xtext. 2013. Xtext 2. 4. http://www.ecliPse.orglXtextidocumentatiOnl2.4.01D0cumentation.Pdf·
zeigler. B. p and H. s. sarjOughian. 2012. Guide to Modeling and Simulation of Systems of Systems.
Springer.
Zeigler, B. P., H. S. Sarjoughian, and V. Au. 1997. "Object-oriented DEVS." AeroSense '97 100-111.
Orlando, FL.
.•

AUTHOR BIOGRAPHIES
HESSAM S. SARJOUGHIAN is an Associate Professor in the School of Computing, Informatics, and
Decision Systems Engineering at Arizona State University (ASU), Tempe, AZ, and co-director of the
Arizona Center for Integrative Modeling & Simulation (ACIMS). His research interests include model
theory, poly-formalism modeling, collaborative modeling, simulation-based science, and simulation tools.
He is the director on the ASU Online Masters of Engineering in Modeling & Simulation. He can be
contacted at <sarjoughian@asu.edu>.
ABDURRAHMAN ALSHAREEF is a Computer Science PhD student in the School of Computing,
Informatics, and Decision Systems Engineering at Arizona State University (ASU), Tempe, AZ, USA. He
can be contacted at <alshareef@asu.edu>.

is an Associate Professor of Simulation Engineering Institute at the National University of
Defense Technology, Changsha, China. His research interests include model composability, model driven
architecture, domain specific modeling, and their applications in defense simulations. He can be contacted at
<yllei@nudt.edu.cn>.

YONGLIN LEI

2799

TECHNICAL ARTICLE
Simulation-based SW/HW Architectural
Design Configurations for Distributed
Mission Training Systems
Hessam S.

Xiaolin Hu

Sarjoughian

Arizona Center for Integrative Modeling and
Simulation
Computer Science & Engineering Department
Arizona State University
Tempe, Arizona, 85287-5406
E-mail: Hessam.Sarjoughian@asu.edu
URL: http://www.acims.arizona.edu

Arizona Center for Integrative Modeling and
Simulation
Electrical & Computer Engineering Department
University of Arizona
Tucson, AZ 85721-0104
E-mail: huxl@ece.arizona.edu
URL: http://www.acims.arizona.edu

Daryl R. Hild

Robert A. Strini

The MITRE Corporation
Colorado Springs, CO

Emerging Business Solutions
Smithfield, Virginia
E-mail: bob.strini@ebs-inc.org

E-mail, dhild@mitre.org
Distributed Mission

Training (DMT) is a concept
composed of promising technologies that support
training in a variety of domains such as defense and
medicine. To develop and deploy such systems, it is
important to account concurrently for hardware and
software requirements, given high demands for network bandwidth, computing resources, and complexity of software applications. In this paper, we
present the application of a distributed co-design
methodology (Discrete Event System Specification/
Distributed Object Computing) as applied to the
Mission Training and Rehearsal System (MTRS), a
DMT system. For an example, we show that the
developed simulation models allow prediction of the
network capacity below which messages cannot be
sent and therefore incorrect behavior results. The key
issues presented are (1) characterization of the DMT
style architectures in DEVS/DOC, (2) prediction of
DMT-like basic scalability traits via simulation, and
(3) discussion on some open problems underlying
the applicability of distributed co-design for systems
containing "off-the-shelf’ components.
I, DEVS, distributed
2
Keywords: Co-design, C
DMT,
JSAF, scalability
object computing,

1. Introduction
For many years,

more

and

more

advanced frame-

works, methodologies, and techniques have been pro-

posed to help architect software/system designs. For
example, in the arena of Distributed Mission Training
(DMT) systems, key capabilities such as voice recognition, large-scale simulation, and collaborative mission
training, demand an architecture that is scaleable, efficient, open, and fault-tolerant, among other features.
To meet such architectural requirements, it is imperative to not neglect combined software and hardware

requirements. Of course, consideration of relationships between hardware and software has been a foof research for more than two decades for embedded systems. Thus, from a conceptual point of view,
Distributed Mission Training systems must also trade
off the interplay that hardware and software constraints (i.e., architectural traits) have against one another-as has been required for the development of
embedded systems.
One of the main approaches in developing architectures for systems intended to perform DMT feats is
via simulation modeling. This approach models various aspects within the DMT system to gain an understanding of user requirements and developer constraints and enabling verification and validation of
simulation models within its architectural structure.
cus

23

The simulation model of this entire DMT architecture
may further provide a testbed for examining what-if
scenarios based on quantitative dynamic behavioral

analysis.
Therefore, given the importance of simulation
modeling, in this article we illustrate how modeling
and simulation can be applied to study one facet of a
DMT system’s architecture (i.e., scalability) using a
distributed co-design framework. We develop simulation models for MTRS and evaluate its architecture as
increase bandwidth requirements based on increased computing loads and more complex hardware platforms. Further, we discuss some issues surwe

rounding characterization of model components of
DMT-like

systems.

1.1 Mission

Training Rehearsal System

Distributed simulation training systems have
achieved various levels of success, replicating battlefield conditions for the warfighter. These systems are
quite complex and have limitations such as the ability
to handle the various input/output requirements as
the number of trainee and complexity of their interactions increases. A potential solution-Mission Training and Rehearsal System (MTRS) [1]-has been proposed, and to some extent developed to support a
wide range of capabilities (academic learning, proficiency training, and mission rehearsal) and yet supports a relatively large number of trainees. The prototype demonstrates an initial proof-of-concept for a
comprehensive training system to support Air Force
Command and Control (C2) training. One of the key
objectives is to enable the warfighter to conduct various phases of mission execution within one training
system on demand and with minimal Human-in-theLoop coordination at an acceptable level of resolution.
The prototype includes a distributed simulation
system with computer generated forces, terrain, environment, and cognitive agents such as pilots, to replace today’s human support role players. The combat scenarios developed for this prototype provide
training opportunities for four elements of the
Ground Theater Air Control System (GTACS): Control and Reporting Center (CRC), Aerospace Operations Center (AOC), Air Support Operations Center
(ASOC) and the Tactical Air Control Party (TACP). A
single operator duty position for each element was
designed to receive training through web-based
courseware, practice sessions and distributed simulation mission sessions. Funding cutbacks reduced the
AOC and ASOC positions to that of a role player observing the key elements of information necessary for
the operator to execute mission processes. The two
primary duty positions are the CRC Weapons Director (WD) and TACP Enlisted Terminal Attack Controller (ETAC). Differences in mission accomplishment by GTACS operators and individual pilot

training are noteworthy in several ways. Initially,
24

GTACS operator’s skill or positional training requires
small-scale forces and scenario elements. As exercise
levels progress toward team or crew training, MTRS
must be able to scale the event to meet a Major Regional Conflict (MRC) or Major Theater of War
(MTW). Operationally, this means the training system
must be able to include many tactical missions, Rules
of Engagements (ROE), Special Instructions (SPINS)
and pre-mission planning considerations to handle
the enormous number of combat situations that could
arise. It is the responsibility of the training provider to
determine if the training environment is valid to support this requirement. Typically, human instructors
maintain the realism factor by inserting expertise
when the system cannot meet realistic demands.
MTRS includes the use of intelligent tutors and Computer Managed Instruction to support the time management responsibilities of the human instructors.
This element of support adds to the challenge of distribution responsibilities and is a factor in the evaluation of the MTRS architecture.
1.2 Related Work

The amount of reported research findings for DMT
systems is relatively small. Organizations such as
Boeing and Lockheed Martin have been and are developing some types of Distributed Mission Training
systems. For example, Boeing has successfully demonstrated real-time distributed cooperative training
missions by linking its Joint Strike Force full-mission
simulator with US Air Force Air Combat Command
simulators [2]. Similarly, Lockheed Martin recently
was awarded to build F-16 Mission Training Centers
(MTCs) for use by the US Air Force Air Combat Command [3]. Each MTC connects two to four training devices to simulate F-16 tactical formations and operations.
However, while considerable amount of research
and development has been carried out by major defense industries (e.g., Boeing and Lockheed Martin), it
is not known to the outside community what methodologies, techniques, and frameworks may have been
employed and to what extent. In open literature, there
is a handful of publications revealing some technical
underpinnings for the analysis, design, realization,
and testing of such systems, primarily from the software engineering point of view.
We now briefly discuss four articles that are most
closely relevant and related to our work. In the first
paper, Murray and Monson consider the applicability
of real-time HLA/RTI [4] for the C-5 DMT simulators
[5]. The aim of the study was to determine the number of entities that RTI supports for a distributed
training system. The authors provide a schematic of
the system architecture hardware and outline a layered software architecture. The analysis and testing
measured network bandwidth, CPU utilization, transport delay, and behavioral correctness (entity dis-

placement inaccuracies between the true position
computed by one simulator and its corresponding
dead-reckoned position by another). Such measures
showed, for example, that the distributed C-5 Weapon
System Trainer could support a few 10s of entities.
Based on extrapolated network bandwidth, it was
concluded that hundreds of entities could be simulated. While this work underscores the importance of
accounting for hardware and software aspects of the
system, it does not provide a framework and associated methodologies.
In the second paper, Mealy and his colleagues discuss performance analysis and testing of HLA/RTI
with respect to network bandwidth utilization, RTI
latency, processor utilization, and aircraft position error [6]. The experiment’s
primary focus on RTI protime
for
cessing
incoming/outgoing messages and the
effect on network delay. In some experiments, up to a
maximum of 32 aircraft were accounted for. The experimental setup consists of three machines. Two
computers were networked together via a third computer responsible for emulating real-time network
traffic. The two computers host the simulation of the
aircraft and missiles. The authors, however, do not
offer or employ any integrative hardware and soft-

approach. Instead, they employ a layered approach to design the software aspect of C-5 DMT and
account for hardware constraints in the design of the
overall system - for example, by allocating a separate
processor to host the RTI and using a custom-built
ware

board to generate time stamps.
The authors Stytz and Banks, in the third article,
discuss a software architecture in support of designing HLA-compliant software applications such as
DMT [7]. The architecture highlights the importance
of defining system requirements, applying good software architectural paradigms, rules, and so on. The
authors discuss in detail the CODB (Common Object
Database Architecture) and rules. The architecture
addresses data handling for classes, containers, and a
central run-time data repository to store and route
data between major objects. Finally, in the last paper,
Seidel, Testani, and Wagner describe the details of the
CIMBLE system [8]. It focuses on system concepts,
technology, and execution. The authors emphasize the
importance of use-cases and scenarios and execution
sequence. These last two articles are primarily based
on the &dquo;pure&dquo; software engineering approach to architect DMT-like systems.
Of course, within the software engineering and
computer science communities, research has been underway to devise software frameworks and architectures [9,10]. This line of research is providing a basis
to architect systems as complex as DMT. Unfortunately, it is vital to consider not only software aspects
of a system, but equally important is hardware and in
particular network aspect-i.e., treating hardware
configurations and architectures in a peripheral man-

ner

is

to be

prevalent in software engineering and has yet
recognized and employed.

Integrative Distributed Co-design
Approach

2. An

The approach undertaken for study of the MTRS architecture follows the modeling and simulation methodology called DEVS/DOC (Discrete Event System

Specification/Distributed Object Computing) [11-13]
which is based on the concept of Distributed Object
Computing [14]. This recently developed methodology provides a generic framework to model and
simulate hardware and software components of distributed systems such as MTRS. An essential part of
this approach is based on the validation of model
components and their integration against their realworld counterparts. Hardware components of MTRS
(e.g., computers and network) are modeled based on
manufacturer’s specifications. Software components
are modeled based on suitable abstractions of their
execution behaviors. The general strategy is to model
various capabilities of MTRS (and its future incarnations) and then validate the model against measurements made on the actual prototype. It should be
noted that while our focus in this report is on MTRS,
the approach is applicable to other distributed systems such as those required to meet other DMT requirements. The approach relies on an iterative process through which the accuracy of models is
incrementally improved. This makes it possible to
gain confidence that the overall MTRS model is valid
at an adequate level of resolution. The model can then
be extended and simulated to predict alternative,
&dquo;scaled up&dquo; versions of MTRS without actually building them.
2.1 Concurrent Software/Hardware (SW/HW)
Architectures Consideration
A traditional simplified software component architecture for DMT is shown in Figure 1 and depicts the
major software components of MTRS. The figure
shows the data and control communications that take
place among the software components (Browser,
Courseware, SoarSpeak, SoarGen, Database (Computer Managed Instruction), JSAF, and AI-tutor

(iGEN)) primarily from the point of software applications. However, the hardware aspect of any distributed application such as DMT is integral to the behavior and performance of the software applications. For

example, an attribute (e.g., bandwidth) of a system’s
architecture - distinct from software architecture can only be complete by explicitly taking into account
hardware components such as physical links (fiber
optics) and intermediate nodes (e.g., hubs) between
any two processing nodes (e.g., laptop and worksta-

tion).

Unfortunately, such architectural diagrams, be it
layer or hardware layer alone, are in-

for software

25

Figure 1. Sketch of MTRS Architecture

creasingly limited in supporting the development of
complex, large-scale, heterogeneous environments
such as DMT. The development of software applications to meet bandwidth requirements of successively
larger organizations and the capabilities of the future
network infrastructures is tied directly to alternative
network architecture choices (e.g., LAN vs. WAN).
Hence, not only individual software and hardware
architecture specifications are required, but these
specifications must also be carried out concurrentlyi.e., a unified software/hardware specification, if necessary. Indeed, for systems such as DMT, architecting
systems in a concurrent fashion is likely to require a
repertoire of approaches and techniques including
UML, formal methods, and particularly, simulation
models.
2.2 Distributed

Object Computing Approach

Butler [14] proposed an abstract mathematical framework y for specifying a static, structural model of a
generic distributed object computing environment.
This framework takes a simple but powerful view. It
provides a model to characterize dynamic behavior of
a distributed object computing environment by representing two distinct layers of behavior-one for software objects and another for hardware objects inde-

26

pendently of one another

- and allows a mapping bethem (see Figure 2). That is, the framework facilitates modeling abstract behavior of the software
components independent of the computing and networking components. Similarly, the framework enables standalone modeling of hardware components
responsible for executing software components. The
software and hardware models are referred to as the
Distributed Cooperative Object (DCO) and Loosely
Coupled Network (LCN) layers, respectively. The
framework also defines the so-called Object System
Mapping (OSM) to provide for the mapping of software components to hardware components.
Finally, a set of metrics is defined to extract key parameters of interest to enable studies of alternative
architectural designs, given various choices for DCO
and LCN layers as well as their mappings. For the
DCO layer, transducers should capture metrics for the
Domain, Object and Interaction Arc classes [10, 14].
Metrics for the Domain and Object classes are computational work performed, active time/utilization, I/O
data load, percentage of active objects, degree of
multithreading, length of execution queues, number
of initialization invocations, total execution time, and
coefficient of interactions. For the Interaction Arc
class, metrics of interest are data traffic, percentage of
tween

Figure 2. Distributed object computing architecture

packet retransmission, net throughput of data, rate of
overhead, and gross and net response time. Similarly,
for the LCN layer, three classes (Processor, Gate, and
Link) are defined to contain key metrics. For the Processor class, metrics are computational work performed, active time/utilization, I/O data load, utilization of storage, percentage of active objects, and
degree of multithreading. For Gate and Link classes,
transducers should capture data traffic, percentage of
packet retransmission, net throughput of data, and
rate of overhead.

2.3 DEVS/DOC M&S
Methodology and Environment
Modeling and simulation (M&S) has become increasingly the most suitable vehicle to study the complexities associated with the development of distributed
object computing systems such as Distributed Mission
Training systems. The Discrete-event System Specification (DEVS) modeling and simulation framework
supports modular, hierarchical representation of components of such systems [15]. With it, we can model
hardware and software components (e.g., processors,
networking topologies, communication protocols,
software objects) of a distributed system with varying
degrees of resolutions and complexities in a systematic and scalable manner.

While the distributed object computing formulation
proposed by Butler formed the basis for representing
various components of a distributed object computing
environment, it did not provide a characterization
rich enough for representing dynamic, time-driven
behavior of software and hardware components nor
did it provide a modeling and simulation environment to enable simulation modeling. As a result, the
Discrete Event System Specification/Distributed Object Computing (DEVS/DOC) methodology and environment was proposed and implemented to enable
and support simulation studies of distributed object

computing systems [11, 12].
The DEVS/DOC environment is based on the
DEVS modeling and simulation framework [16,17],
providing a sound formal modeling and simulation
framework based on concepts derived from dynamic
systems theory. DEVS is a mathematical formalism
with well-defined concepts of coupling of components, hierarchical, modular construction, support for
discrete event approximation of continuous systems
and an object-oriented substrate supporting repository reuse. Of particular interest for our purposes are
the framework’s object-orientation traits and its ability to provide a fundamental, rigorous mathematical
formalism for representing dynamical systems. The

27

Figure 3. DEVS/DOC modeling and simulation architecture

DEVS/DOC modeling and simulation environment

view, the choice of data communication protocols, ob-

[18] by introducing a layer of
constructs
on top of generic DEVS modelmodeling

jects’ multi-threading capabilities and memory requirements are important considerations. These software/hardware design choices generally interact in
complex ways so as to make the development of distributed systems such as DMT challenging and demanding. Complexities in the behavior and perfor-

extends DEVSJAVA

ing constructs. As shown in Figure 3, the modeling
layer can employ alternative simulation engines (e.g.,
DEVSJAVA) to accommodate singular / parallel/distributed execution of DOC models.

of such systems arise from two sources: (1) the
intricacies of individual components (both software
mance

Modeling DMT System Architecture
The ability to evaluate architectural attributes (e.g.,
scalability) of distributed systems using dynamic
3.

models is invaluable to designers, developers, and
decision-makers. Within the DEVS/DOC modeling
and simulation environment, important components
of advanced systems such as DMT can be represented
and simulated to include the complex interactions occurring among hardware and software components.
Furthermore, alternative design choices and tradeoffs
among them can be studied. As an example, we might
be interested in evaluating alternative DMT architectures in terms of their scalability as the number of its
concurrent users increases.
Design choices to achieve scalability are available
at the hardware and software levels. For example,
from the hardware point of view, a processor’s speed,
buffer size, and a network’s bandwidth are key design parameters. Similarly, from the software point of

28

and

hardware), and (2) more importantly, the numerinteractions of hundreds of components dispersed
across a network. Without modeling and simulation
tools at hand, these complexities make it difficult to
know how to distribute software components across a
network in order to appropriately match software
ous

computational needs against hardware resources.
And, without proper distribution of software components, we are likely to greatly overestimate the bandwidth and processing requirements of the underlying
hardware, resulting in greater costs for less performance.

Table 1 shows DMT system components and their
counterpart simulation models, assuming three computers and a local area network. The table, as shown,
includes all major software and hardware components. The DEVS/DOC simulation models represent
either distinctly or as an aggregate, the DMT proto-

Table 1. Hardware/software components and their

it can perform and memory size
for
an
required
executing software application. Two
essential elements of any software object model are
attributes and methods. The methods of software applications are specified in terms of their data size and
resources required for processing. For example, an
executing JSAF application typically requires 256MB
of RAM. Messages produced by JSAF have an average
size of 1500 bytes, and messages received by JSAF
have an average size of 100 bytes (see Section 3.2 for
some details). Similar to software components, hardware components exhibit dynamic behavior and
therefore must be modeled (see Figure 4). Compared
to software components, hardware components may
generally be simpler to model and easier to customize. For DMT, off-the-shelf processors, hubs, and Ethernet models per specification of their real-world
components were modeled.

erations

type. Consider the Virtual Network Computing

(VNC) server and client applications. In the current
simulation of the DMT, VNC client and server are distinct software applications. However, in the DMT
model, they are aggregated as part of JSAF and
Weapon Director software applications. That is, while
the model of the DMT system is based on its prototype realization, the model and the prototype do not
have a one-to-one correspondence with one another
in terms of LCN and DCO layers and their OSM mapping. Nevertheless, despite the absence of such formal
relationships, key architectural design considerations
(e.g., scalability) of the system can be demonstrated
and studied via these approximate simulation models.
The system modeling effort for systems such as
DMT can proceed in four steps. First, the network is
defined in terms of processing nodes, gates, and links.
Second, the software objects and their interaction with
one another are defined. Third, software objects and
their interactions between distinct hardware components are mapped onto the processing nodes. Fourth,
a set of models is defined to set up the experiments
and control the simulation execution. These models
collect pertinent measurements/metrics for functional
and behavioral evaluation. The models to be developed for steps 1-3 are a Loosely Coupled Network
(LCN), a set of Distributed Cooperative Objects
(DCO), and an Object System Mapping (OSM), respectively. For step 4, a set of transducers and acceptor models are to be defined, developed, and incorporated with the rest of the models.
The software components for browser (the interface
to the trainee), courseware, SoarSpeak/SoarGen,
iGEN, database, and JSAF are modeled individually
(see Figure 4). Each model is specified in terms of op-

corresponding simulation models

&dquo;

(methods)

3.1 DEVS/DOC Simulation Models
In this section, we exemplify DMT models of the software and hardware components, LCN and DCO composite models, and the combined LCN/DCO layers
(see Figure 2). We also outline the list of transducer
and acceptor models and how they are comprised
with hardware and software models to form experimental frames [16]. These models fall into two categories. The first category represents dynamic behavior
of LCN and DCO components as well as their mappings via Object System Mapping. These models are
represented as DEVS atomic and coupled models.
The second category represents transducers aimed at
capturing metrics of interest emerging out of simulation of the system-that is, a host of transducers targeted for one or more of the software and hardware

29

Figure 4a. Candidate DMT Systems (MTRS) architecture

Figure 4b. Alternative DMT Systems (MTRS) architecture

30

Figure 5. Snippet of the System Level Model

Figure 6. Snippet of the Loosely Coupled Network Model
components. This category also includes an acceptor

responsible for controlling the overall behavior of the
entire simulation.
The DEVS/DOC modeling of DMT begins with declaring the highest-level architectural models (see Figure 5). The dynamic behavior of the DMT is captured
in a hierarchical fashion-that is, representing hardware layer (Loosely Coupled Network), software
layer (Distributed Cooperative Objects), and the mapping of the former to the later (Object System Map-

ping). Figure 5 also shows that the experimental
frame playing a vital role in extracting the manifestation of DMT dynamic behavior.
As shown in Figure 6, the components of the LCN
layer (see Figure 1) are modeled in terms of their attributes. For each of the processors, attributes are storage size, CPU memory swap time penalty, and speed.
Similarly, other components of the LCN-Ethernet
and gates (hubs and routers)-are modeled by capturing their key attributes such as Ethernet speed and

31

Figure 7. Snippet of the Distributed Cooperative Object Model
number of Ethernet segments. The LCN configuration
shown for Figure 4 does not include any gates.
The DEVS/DOC LCN models contain methods
specifying dynamic behavior such as collisions detection by Ethernet. However, in comparison with software components, the dynamic aggregate behavior of
hardware components is fixed, and generally there is
no need to specify specialized methods for them. Of
course, the DEVS/DOC environment supports incorporating new hardware components and behavior
[11]. Additionally, as shown in Figure 6, the hardware
components are composed of the LCN layer together
as coupled models. These models specify how messages / data are transmitted from one model to another (e.g., PC to hub) using their respective ports and

couplings.
The software components comprising the DCO
layer are also represented as atomic and coupled
models. Figure 7 depicts examples of key attributes
and methods that capture behavior of software objects. Examples of attributes are workloads and interaction message sizes. These attributes are in turn used

32

defining methods exhibiting functional behavior of
software objects. Consider Weapon Director activities.
The software component is defined to include initialization, login, open lesson, and user-input activities, a
series of which represents a training cycle for the
Weapon Director. Other software objects are defined
for Courseware, Database, and SoarSpeak.
The remaining models for the first category captures mapping of DCO components (e.g., the PC hosting JSAF (PCJSAF1)) onto LCN components (e.g.,
Joint Semi-Automated Forces (JSAF)). The collective
DOC models for DMT are represented as coupled
models (see Figure 8). These models specify how messages invoked by software objects are loaded into
hardware objects (PCs) and transmitted from these
nodes to others (e.g., Ethernet links and hubs) in the
LCN layer. The modeling activity for the second category is concerned with coupling existing DEVS/
DOC transducers to their respective software and
hardware components. For example, JSAPJand JSAF’L
are declared and coupled with coupled with the JSAF
transducer OSAF1_t).
in

Figure 8. Snippet of the System Object Mapping and Transducer Models

3.2

Representing Off-the-shelf SW/HW Components
As discussed earlier, characterizing systems such as
DMT requires having attributes and methods of interest

available for software and hardware components.

However, the scale and complexity of architecting distributed systems can vary drastically depending on
issues such as component types. For example, the heterogeneity degree in the DCO layer of DMT requires
models for voice recognition, Joint-SAF, iGen, and
support components (e.g., VNC client/server and
web client/server). Similarly, the elements of the LCN
can range from simple hubs to sophisticated custombuilt simulators [19]. Obviously, development of systems such as DMT must rely on using off-the-shelf
components if cost and delivery date requirements
are strict. Given the key role of employing existing
software/hardware components, the components of a
candidate system’s architecture, therefore, falls into
two categories: those that are Off-the-shelf and those
that are to be developed.
Off-the-shelf hardware components generally are
available and come with specifications including key
attributes (e.g., processor speed, data rate transmission, etc.), operations (methods), and schematics. In
comparison, in the current state of affairs, the behaviors of software components are generally unavailable
and therefore need to be determined. Software behav-

ior specification entails obtaining DCO attributes and
methods of interest such as multithreading.

Clearly, some hardware components may need to
be custom engineered and some software components
may also be available off-the-shelf with their attributes and methods either available or not. We assume software or hardware metrics can be obtained
(i.e., already exists or experimentally measured) if the
components are available. Therefore, it is likely in applying distributed co-design paradigm for applications such as DMT to be faced with the task of obtaining attributes and methods of interest for the
components of the LCN and DCO layers.
Approximate Models of the Off-the-shelf Components
Obtaining attributes and methods of components, as
proposed in the previous section, requires experimenting directly with software and hardware components. However, such experimentation requires first
defining the behavior of interest as well as developing
the means to measure it. As it turns out, defining and
measuring behavior for DCO/LCN layer components
can be quite challenging, particularly for software
components, given their degree of complexity and the
significant role they play in representing the system’s
behavior. In general, measurements need to be con3.2.1

ducted both in isolation but also

as an

active

part of

33

Figure 9. Memory and CPU usage for JSAF

system. For example, in DMT, the JSAF
component needs to be characterized while operating
the entire

on a single piece of hardware or in a networked
environment. We have conducted a series of measurements for JSAF and the database (MS Access database) under nominal conditions and independent of
the rest of the system to determine some of their essential parameters (i.e., memory usage and CPU resource utilization) (see Figure 8). Some software ap-

either

plications (e.g., courseware) were not available for
measurements and some others do not readily lend
themselves for standalone measurements (i.e.,
SoarSpeak/SoarGen). For these, we have obtained informally their parameters by gathering relevant heuristic measurements from their developers.
Aside from standalone testing, a set of measurements was obtained for the DMT system under the
one-vs-one scenario [1]. These measurements provide
messages exchanged among software applications.
Specifically, measurements were obtained in terms of
frequency of messages sent from one software application to another, size of messages, and their transmission periods. That is, we have used both component and system measurements to partially verify and
validate the DEVS/DOC models individually and collectively (see Figures 9 and 10).

34

Figure 10. Message exchanged among JSAF, SoarSpeak,
and iGEN

4. Simulation of MTRS
In this section, we discuss simulation results depicting
the scalability of the MTRS architecture. The experimental frames collect the metrics of interest as discussed in Section 3. The measurements are gathered
from multiple simulation runs. We discuss the DMT
architecture behavior in terms of its performance (e.g.,
network traffic, bandwidth effects, computational
loads, etc.) and its viability from the scalability point
of view. That is, we predict whether the expected, correct system’s behavior under normal settings is attainable or not. Of course, it is imperative to differentiate
between performance degradation under normal conditions and incorrect, faulty behavior. Therefore, we
can also predict under what conditions the system
will fail to operate correctly-for example, due to insufficient memory or excessive message latency, software objects fail to send/receive messages from one
another.

Scaling DMT Architecture
study the scalability of MTRS, we can use a variety
of alternative architectures (see Figure 4). Here, we
discuss the results of the typical three-tiered architecture shown in Figure 4a. This architecture is composed of two main computing nodes and a set of others for the trainees, along with their hardware links.
4.1
To

Table 2. MTRS hardware

configuration

however, becomes significant (0.269) assuming 1

One node executes VNC, JSAF, and Webserver software objects and another executes Database and
iGEN software objects. Each client node (Trainee) executes SoarSpeak, Web browser, Courseware, and
VNC client software objects. In this architecture, the
number of clients can be increased from 1 to n, assuming a single JSAF, iGEN, and database. The simulation
of the MTRS model reveals the impact of increased
clients’ demand on resources such as database, iGEN,
and JSAF. The MTRS architecture can be varied to examine other alternatives in terms of hardware configurations (e.g., changing Ethernet speed from 100
mbps to 100 kbps). In accordance to alternative architectures (e.g., variation in the number of users and
network bandwidth), the number of hardware and
software components is increased to examine suggested architectural scalability traits. For example,
LCN simulation models are modified.

planned. Whereas Command and Control personnel
interact on a second-by-second account, fighter aircraft platforms may need millisecond updates to de-

4.2 Simulation Results

termine accurate weapons
sions.

The following figures illustrate simulation results obtained for MTRS using the 3-tiered architecture. Figure 11 (a), (b), and (c) show the how increasing number of trainees (i.e., Weapon Directors (WD) and
SoarSpeak (SS)) from 1 to 40 affects Ethernet data
throughput, response time for invoked messages, and
job throughput for the machine hosting JSAF. The
measurements in Figure 10 are based on 10 multiple
independent simulation runs to account for randomness present in the DEVS/DOC models. In these
simulation runs, we kept the software attributes fixed.
As we stated earlier, hardware and software simulation models have been specified based on measurements obtained from the MTRS prototype components.

Table 2 shows some of the basic LCN layer attributes. The number of processors and key attributes
of the Ethernet were varied to obtain measurements
shown in Figures 11 and 12.
Examination of Figure 11 shows that with the
scaled 3-tiered architecture, it is assumed that MTRS
will begin to experience delays (see Figure 11(b)) as
the number of trainees increases. Assuming 10 mbps
Ethernet speed, 20 trainees accessing one JSAF computer supporting multiple simulation scenarios experiences small delays of up to 0.059 seconds. The delay,

mbps Ethernet speed.
The MTRS architecture can support many JSAF
back-end to front-end clients, and thus can reduce undesirable delays due to CPU loading. It should be
noted that although MTRS system responsiveness degrades significantly (sees Figure 6(b)), its behavior remains correct. The correct behavior is evident since
the number of messages and amount of data exchanged among software objects is proportional to
scaled variations of MTRS. The behaviors of the alternative simulation models only differ in terms of their
duration. Impacts of this degradation should be
evaluated against the customer’s requirements before
improvements to hardware or the architecture is

parameters or tactical deci-

Since bandwidth related constraints (Ethernet and
Network Interface Card (NIC) specification) are
known to affect the behavior of distributed systems,
we can evaluate the impact of alternative Ethernet
and NIC configurations. We have, therefore, conducted 5 simulation runs according to Table 3, where
decreasing capability of successive configurations is
evident. In all simulation runs, the number of trainees
is set to 5. To account for randomness incorporated in
the MTRS models, each scenario was repeated 10
times. The simulation results suggest that while it is
possible to reduce the quality of Ethernet and NIC by
varying bit per second data transmission, there exists
a breakpoint at which the system will not behave correctly. An examination of Figure 12 reveals that the
number of messages drops from 330 to 24 when going
from configuration I to V. While the MTRS architecture can maintain its correct behavior for scenarios I
and II, the remaining scenarios show that fewer messages are produced by software objects and thus sent
to others. Consider scenario III. In this scenario, the
configuration of Ethernet and NIC are responsible for
messages not reaching their destinations and thus resulting in incorrect behavior. The source of system
failure can be attributed to limited number of times a
message can be sent across the network. After a finite

35

Figure 11. Measurements for 1 to 40 MTRS trainees on LAN
number of attempts, messages will not be sent out
which obviously results in the intended software ob-

studying architectural and scalability aspects of MTRS
and DMT systems in general. The modeling approach

jects not receiving them. For example, in scenario III,

demonstrates that value of SW/HW architectural de-

IV, and IV, JSAF cannot receive all messages invoked
by SoarSpeak. Unfortunately, in complex systems
such as MTRS, lack of successful message transmission between two software

sign considerations from architectural, behavioral,
and performance viewpoints. The simulation results
revealed how significant alternative hardware configurations are in architecting DMT-like systems. For

Summary and Future Research
study, we showed for the first time the role
DEVS/DOC co-design methodology may play in

example, up to 10 trainees per site can be accommodated for nominal hardware configurations. Comparison of this work with those
presented in Section 1.2
suggests that architecting DMT-like systems requires
a systematic approach to transition from
high-level

objects results in cascading failures throughout the system.
5.

In this

36

techniques such as HLA’s Data Distribution Manage[4] and Quantization [20] into SW/HW models.
In this regard, the DEVS/DOC framework provides a
basis to capture detailed dynamics of software applications, and therefore a possible candidate for supporting the SBA initiative.
ment

Acknowledgments
greatly acknowledge the contributions of Bernard
Zeigler. We also sincerely acknowledge the contribu6.

We

tions of the MTRS team, and in particular Derek
Bryan (BMH), Paul Nielsen (Soar Technology, Inc.),
Thomas Richards (Langley AFB), and Keith Strini
(Emerging Business Solutions). This research has been

supported by Langley AFB, under Contract Number
1435-01-00-CT-31045 and
#EIA-997505.

NSF/NGS under grant

7. References
[1] Strini, B. Airpower and Simulation Assessment of Ground The-

Figure 12. Ethernet successful transmissions and number of
collisions

ater Air Control

System Training, 1999, Emerging Business

Solutions, Smithfield, Virginia.
[2] Boeing, Joint JSF and F-15 Training Simulation, http://

www.boeing.com/news/releases/2000/

system design

to low-level

component design. We

showed that simulation models allow the architects
and others to predict the minimal capacity for network components (e.g., Ethernet) below which messages cannot be sent and therefore incorrect system
behavior will result. This study also demonstrates
that the current DEVS/DOC methodology and environment is most suitable for quantum level architectural, behavioral and performance specifications instead of detailed design of timing constraint
interactions, for example, between voice recognition

news_release_000621n.htm, visited 2001.
[3] Global Defense Review, http://www.global-defence.com/

webpages/TSpart1.html, 2001.
[4] HLA, High Level Architecture, Defense Modeling and Simulation Office:

http://hla.dmso.mil, visited 2001.

B. and Monson, S. "Analysis of a Real-Time HLA Distributed Mission Training Federation," in Simulation Interoperability Workshop, 2000, 00S-SIW-045, Orlando, FL.

[5] Murray,

[6] Mealy, G.L.

et

al, "Analysis of the Applicability of HLA/RTI to

Distributed Mission

Training," in Simulation Interoperability Workshop, 99S-SIW-083,1999, Orlando, FL.
[7] Stytz, M.R. and Banks, S.B. "The Distributed Mission Training
Integrated Threat Environment System Architecture, Rules
and Design," in Simulation Interoperability Workshop, 99SSIW-133, Orlando, FL.

and JSAF.
We also discussed some issues surrounding the use
of off-the-shelf software and hardware components in
DEVS/DOC. Obviously, the unavailability of sufficiently detailed specification of these components affects adversely the application of the DEVS/DOC
framework and techniques. Further research is necessary to better and more completely characterize use of
existing software and hardware components. This re-

quires developing or utilizing existing approaches to
characterize various types of DMT style architectures
(e.g., real-time software applications). To carry out
detailed specifications, we can incorporate/represent

[8] Seidel, D.W., Testani, S. and Wagner,
Team
1999.

E. CIMBLE: Distributed

Training via HLA, SIMULATION, 73(5), pp. 304-310,

[9] Allen, R.J. "A Formal Approach to Software Architecture,"
Ph.D. Thesis, School of Computer Science, Carnegie Mellon

University, Pittsburgh, PA,

1997.

[10] Emmerich, W. Engineering Distributed Objects. John Wiley &
Sons, 2000.

[11] Hild, D.R.

Discrete Event System Specification (DEVS) DistribObject Computing (DOC) Modeling and Simulation,
2000, University of Arizona: Tucson, AZ.
[12] Sarjoughian, H.S., Hild, D.R. et al., DEVS-DOC: A Co-Design
Modeling and Simulation Environment, IEEE Computer,
2000,33(3): p. 110-113.

uted

Table 3. Ethernet and NIC alternative

configurations

37

[13] Hild, D.R., Sarjoughian, H.S. and Zeigler,

B.P. DEVS-DOC: A

Modeling and Simulation Environment Enabling Distributed
Codesign, IEEE SMC-Part A, Accepted.
[14] Butler, J.M. Quantum Modeling of Distributed Object Computing, in 28th Annual Simulation Symposium. 1995, IEEE
Computer Society Press. pp 175-184.
[15] Sarjoughian, H.S. et al., "Scalability Considerations and Evaluation for Mission Training & Rehearsal System," in Simulation Interoperability Workshop, 01S-SIW-132, 2001, Orlando,
FL.

[16] Zeigler, B.P., Multifacetted Modeling and Discrete Event Simula[17]

tion. 1984, Academic Press.
Zeigler, B.P., Praehofer, H. and Kim, T.G. Theory of Modeling
and Simulation. Second Edition, 2000, Academic Press.

[18] ACIMS, Arizona Center for Integrative Modeling & Simulation, www.acims.arizona.edu/SOFTWARE, visited 2001.
[19] PLEXSYS Interface Products, Inc., http: /

www.defense21.com/exhibitors/plexsys/default.cfm, visited 2001.

al., Bandwidth

Utilization/Fidelity Tradeoffs in
Predictive Filtering, in Simulation Interoperability Workshop, 1999, 99S-SIW-063,Orlando, FL.

[20] Zeigler, B.P,

et

Hessam S.

Sarjoughian is Assistant

Professor of Computer Science and

Engineering at Arizona State University, Tempe. Since 1996, his research activities have focused on
theory, methodology, and development of distributed/collaborative
modeling and simulation including
distributed co-design with applications to areas such as distributed
mission training and information
technology. His other ongoing research and development efforts are
concentrated in hybrid agent and
simulation modeling, software engineering, and artificial
intelligence. Prior to completing his PhD from the University of Arizona in 1995, he was employed at Allied Signal

Aerospace Corporation and IBM.
Xiaolin Hu is pursuing his PhD in
the Electrical and Computer Engineering Department at the University of Arizona. He completed his
MS and BS degrees from the Chinese Academy of Science in 1999,
and from the Beijing Institute of

Technology in 1996, respectively.
His research interest is in real time
and embedded system modeling
and simulation.

Daryl R. Hild is a lead engineer
with the MITRE Corporation. He
received his PhD in Electrical and
Computer Engineering from the
University of Arizona, Tucson. His
research interests include modeling
and simulation, distributed systems
integration, distributed systems

management, and security engineering.

Robert A. Strini received his BS in
Electrical Engineering from Tulane
University in 1978, and MS in Aeronautical Science in 1987 from
Embry-Riddle Aeronautical University. He was adjunct faculty and a
command pilot with 3000 hours in
fighter aircrafts. His staff tours include HQ Air Combat Command
and US Joint Forces Command, both
located in Virginia. He has operational and technical experience in leading modeling and simulation programs for
each command and retired from active duty in January
1999 to operate Emerging Business Solutions, Inc., a company dedicated to technology transition.

38

Design of Distributed Simulation Environments:
A Unified System-Theoretic and Logical
Processes Approach
James Nutaro
Arizona Center for Integrative Modeling and Simulation
College of Engineering and Mines
Electrical and Computer Engineering Department
University of Arizona
Tucson, AZ 85721-0104
nutaro@ece.arizona.edu
Hessam Sarjoughian
Arizona Center for Integrative Modeling and Simulation
Fulton School of Engineering
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85281-8809
sarjoughian@asu.edu
This article presents a framework for distributed simulation that is based on system-theoretic and
logical-process concepts. The framework describes a three-part worldview for developing simulation
models. These are modeling formalisms, abstract simulators, and computational environments. A
unified view of time and causality allows for the application of system-theoretic notions of causality
within a distributed simulation environment. Within this framework, the authors introduce a unified
notion of causality for use in parallel simulations. Furthermore, they describe an approach for developing distributed simulation models that evolve from modeling constructs to simulation algorithms
and their implementations. The framework is exemplified using the discrete event system specification (DEVS) modeling formalism, its abstract simulator, and a parallel algorithm that implements the
abstract simulator.
Keywords: Causality, DEVS, distributed, logical process, system theory, time

1. Introduction
The design and verification of distributed algorithms frequently relies on an event-oriented model of a process in
which processes communicate via messages (e.g., see [1,
2]). Given a set of processes, a single process is described
as a sequence of states and events. A process changes state
in response to events that can be generated internally or
that arrive from some other process. This formalization
is useful for proving that parallel algorithms have desired
characteristics, such as safety and liveness.
An extension of this formalism, in which messages are
assigned time stamps, describes the logical-process approach to distributed discrete event simulation (see, e.g.,
[3]). The local causality constraint restricts the allowable
sequences of local states and events in such a way that the
global sequence of states and events is causally consistent.
SIMULATION, Vol. 80, Issue 11, November 2004 577-589
© 2004 The Society for Modeling and Simulation International
DOI: 10.1177/0037549704050919

|
|
|
|
|
|

One difficulty with this approach is that while eventoriented models can be simulated by logical process-based
algorithms, discrete event systems have been described
that cannot be easily or straightforwardly mapped into an
event-oriented modeling paradigm (see, e.g., [4]). Examples of such models are partial differential equations and
systems of ordinary differential equations approximated
by event-based integrators [5]. While these models might
be addressed on a case-by-case basis (e.g., the discrete
time algorithms presented in Bagrodia, Chandy, and Liao
[6] and Fujimoto [7]), this presents considerable difficulty
when we wish to demonstrate the utility of an algorithm
for simulating a class of systems.
Another more generally applicable approach is to map
system-theoretic formalisms into a logical process worldview. If this mapping can be developed for a class of systems (e.g., the class of systems with discrete event system
specification (DEVS) representations), then it allows us to
construct simulation environments whose correct operation can be ensured for the class of systems covered by the
mapping.

Nutaro and Sarjoughian

This article presents a conceptual framework for mapping modeling constructs into a logical process worldview.
The significance of this development is that it allows the
task of model composition to be separated from that of
simulator construction, and it enables the construction of
simulation algorithms whose correct operation can be ensured (via a formal proof) for a class of systems.
The first half of this article presents background material from the abstract time systems theory (see [5, 8]). The
problems of time and causality in distributed simulation
systems are revisited in the context of simulating causal
systems. The conceptual framework is presented. The utility of the framework is demonstrated via the development
of a simple DEVS simulator.
2. Systems, Causality, and Causal Precedence
Time and causality are fundamental concepts that must be
addressed when designing parallel simulation algorithms.
These issues have been discussed at length in the parallel discrete event simulation literature (see, e.g., [9, 10]).
In this section, a definition of the causal precedence relation originally introduced in Lamport [11] is formulated in
terms of general systems. Nutaro [12] provides an extension of the causal precedence relation to support discrete
event simulations.
An explicit and suitably general notion of causal precedence is needed to support simulation algorithm design.
Here, we construct a definition of causal precedence from
the definition of causality that appears in general systems
theory. By starting from this base, we will have a definition
that is sufficient for handling a very large class of systems.
An input/output (I/O) relational system is a system for
which the input quantities and output quantities can be distinguished [5, 8]. A relational system S is described as the
cross-product of two sets X and Y , denoting the input set
and output set, respectively. The system S is an I/O functional system if S is a function, assuming some particular
initial state if necessary. At this level of abstraction, the system is a collection of inputs to the system and the resulting
outputs.
When dealing with systems that have time-dependent
behavior, we can think of an I/O functional system as being
a function that maps input trajectories and an initial state
to output trajectories (see [5, 8]). A trajectory is a function
from the system time base to a set of values. A set T can
be used as a time base so long as a total order, denoted by
<, and an equivalence relation, denoted by =, is defined
for T such that if t1 = t2 , then it is not the case that t1 < t2 .
The real numbers under the usual < and = relations are an
example of such a time base.
If we want to consider the values of a trajectory x over
a range [ti , tf ] (or [ti , tf ) or (ti , tf ] or (ti , tf )), we can write
x[ti , tf ] (or x[ti , tf ) or x(ti , tf ] or x(ti , tf )). The value of x at
point t is given by x[t, t] and can be written more conventionally as x(t). The output of an I/O functional system, for
an initial state s0 , is given by y<ti , tf > = F (s0 , x<ti , tf >),
578 SIMULATION Volume 80, Number 11

where F is a function from a set of initial states and input
trajectories to a set of output trajectories.
An I/O functional system is causal if the system output
is a function of past and current, but not future, inputs. That
is, the future cannot affect the past. In fact, there exist two
notions of causality [8]. The first requires that the system
outputs be a function of past and current inputs. Such a
system is weakly causal, or simply causal. The second requires that the system outputs be a function of past inputs
only. Such a system is called strongly causal.
Let F be a causal I/O functional system [13]. If y
<t1 , t3 > y <t3 , t4 > = F (s, x <t1 , t2 > x <t2 , t4 >), with
t2 ≤ t3 , then x <t1 , t2 > “happens before” y <t3 , t4 >. The
happens-before relation is denoted by x <t1 , t2 > → y
<t3 , t4 >. This relation is analogous to the happens-before
relation described in Lamport [11], where an event x “happens before” an event y if x appears before y in the execution of a program. In the context of distributed systems, the happens-before relation is also called “causal
precedence.”1 The definition of causal precedence given
in Lamport [11] is a special case of the definition just presented. This is developed more thoroughly in the appendix.
3. Logical Processes
The logical process worldview has been used as a framework within which to develop discrete event and discrete
time simulation algorithms (see, e.g., [6]). A key distinction between the logical process worldview and general
systems is the ability of the latter to describe weakly
causal systems. This fact accounts for many of the DEVS
models that cannot be adequately expressed in the logical process worldview (see, e.g., [4, 5]). However, the
large number of algorithms developed within the logicalprocess framework requires that any alternative framework
be sufficiently general to include it. Here, we present the
class of the systems that can be expressed as a network of
logical processes. This serves to motivate and provides a
foundation for an alternative framework built on the causal
precedence relation described in section 2.
A logical process is described by an initial state and
a function that takes a sequence of messages (or shared
variable values) over a simulated time interval from t0 to tf
and produces a corresponding sequence of output messages
(or shared variable values) over the same interval (see [6,
9]). Such a logical process is called realizable by Misra
[9]. Clearly, a realizable logical process is a weakly causal
system.
A predictable network of logical processes is proposed
by Misra [9] as a necessary condition for a logical process
to be implemented on a parallel computer. A predictable
1. The term happens before indicates that, from a computational point
of view, inputs must be known before outputs can be determined. In the
distributed computing literature, this (unfortunately) became a widely
used definition of causality. The term causal precedence appears to be a
consequence of this.

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

logical process is one whose output at time t is a function
of the inputs received up to some time t − ε, where ε is a
positive number. That is, a predictable logical process is a
strongly causal system. A predictable network of logical
processes is a network of logical processes in which each
cycle contains at least one predictable logical process.
Two aspects of the logical process worldview introduce
problems when trying to represent general systems models. The first is the requirement for a strongly causal logical
process to exist in every simulation. This presents problems when trying to simulate collections of weakly causal
systems (see, e.g., [14]).
The second difficulty is apparent from a careful study of
the process model from which the logical-process model is
derived. The input trajectories are sequences of messages
with exactly one message being processed at each point in
time. When a model is intended to process many events
simultaneously, we encounter the so-called simultaneous
event problem (see, e.g., [10, 15-18]). Solutions to this
problem have almost invariably relied on further information about the model itself (e.g., priorities) or are solved
by fiat (e.g., first come, first served).
Solutions to both of these problems have been proposed
in the context of specific types of models (see, e.g., [6]).
Unfortunately, these solutions require building information
about the model behavior into the simulation algorithm.
The overall effect is to blur the line between what is the
model and what is the simulator.
If we lose the distinction between a model and its
simulator, then the task of showing that simulation algorithms are correct with respect to a modeling formalism becomes impractical. This observation has been made
by several authors (e.g., [19, 20]) who have constructed
correctness proofs for parallel simulation algorithms. One
consequence of this impracticality is that existing correctness proofs restrict themselves to showing that messages
are processed in time stamp order. The issue of correctness with respect to a modeling formalism (e.g., DEVS)
is not addressed. This same observation has been made
by authors with a systems viewpoint when trying to integrate system-theoretic and logical-process paradigms in
the context of the High Level Architecture (HLA) [21].
Both the strong causality requirement and simultaneous
event problem are a result of the local causality constraint.
The local causality constraint is generally considered to be
a fundamental starting point for reasoning about parallel
discrete event simulation algorithms (see, e.g., [3, 22]).
DEFINITION 1. (local causality constraint). No causality
error can occur in an asynchronous logical process simulation if, and only if, every logical process processes events
in nondecreasing time stamp order.
Implicit in the local causality constraint is the assumption that the causal precedence relation over a set of events
is contained in the time stamp order of those events. When
simulating weakly causal systems, we can construct a
counterexample. Let F be an I/O functional representation

of a weakly causal system. Let x[t, t] be an input segment
for the system and y[t, t] be the corresponding output segment. So y[t, t] = F (x[t, t]) and the event x(t) causally
precedes y(t). Since t = t, this cannot be deduced from
the times associated with y(t) and x(t).
Since the local causality constraint is fundamental to
many parallel simulation algorithms, we need to preserve
an analog to it in any alternative framework. In fact, we
can do this by employing the causal precedence relation as
opposed to the simulation time stamps (i.e., event times)
assigned to events. This requires the definition of a suitable
time-stamping algorithm that can be shown to preserve the
causal precedence relation. Such an algorithm is developed
in the following sections.
4. Logical Clocks
Logical clocks are used for detecting causal precedence
relations in a distributed computing environment. As was
discussed in section 3, a causal precedence relation over
a set of events and the time ordering of a set of events
cannot be used interchangeably when dealing with general
system-theoretic models. This is a key point since it implies
that the model’s notion of time, by itself, cannot be used
to deduce causality.
In contrast to this, the logical process worldview, with
its basis in a strongly causal modeling paradigm, has generally treated time ordering and causal precedence as being
interchangeable. It is quite common to find that a parallel discrete event simulation algorithm uses the simulation
time at which an event occurs as the event time stamp.
When these time stamps are employed to detect causal
precedence relations, it yields a minimally consistent logical clock.
A minimally consistent clock describes the notion of
time and its relationship to causality in general systems,
where we allow the output and state at time t to be a function
of inputs up to and including time t. A minimally consistent
clock is one that meets the following condition [10]: Let e1
and e2 be two events and C a minimally consistent clock.
If e1 → e2 , then C(e1 ) ≤ C(e2 ). It is easy to see that if
C(e1 ) > C(e2 ), then ¬(e1 → e2 ).
A minimally consistent clock does not provide a mechanism for discerning causal relationships between apparently “simultaneous” events (i.e., events with equivalent
simulation times). This makes the minimally consistent
clock unsuitable for detecting causal precedence.
A weakly consistent clock is more useful for distributed
simulation since it ensures that only causally independent
simultaneous event have equal time stamps. A weakly consistent clock meets the following condition [1, 23]: Let e1
and e2 be two events and C be a weakly consistent clock.
If e1 → e2 , then C(e1 ) < C(e2 ). It is easy to see that
1. if C(e1 ) > C(e2 ), then ¬(e1 → e2 ), and
2. if C(e1 ) = C(e2 ), then ¬(e1 → e2 ), and
¬(e2 → e1 ).
Volume 80, Number 11

SIMULATION

579

Nutaro and Sarjoughian

The statement ¬(e1 → e2 ) and ¬(e2 → e1 ) is denoted
e1 ||e2 .
In the context of simulating strongly causal, single-input
systems, the use of simulation event times does, in fact, provide a weakly consistent clock. That is, the local causality
constraint is sufficient to ensure causal consistency in this
case. However, when simulating weakly causal or multiinput systems, the simulation event times provide only a
minimally consistent clock (i.e., the local causality constraint is necessary but not sufficient).
5. A General-Purpose Simulation Clock
A weakly consistent clock that is suitable for keeping time
in simulations of weakly causal systems is presented in
Rönngren and Liljenstam [10]. This algorithm was originally introduced for the purpose of detecting simultaneous
zero time events when simulating logical-process models.
Here, we show that the weakly consistent clock can be
used to detect the causal precedence relation described in
section 2. This provides a basis for simulating more general
systems models (e.g., DEVS models) using variations of
known parallel discrete event simulation algorithms (see,
e.g., [24, 25]).
A time stamp is a pair (t, c) where t is a model-derived
time stamp (i.e., the time associated with an event) and c is
an integer counter. The simulator maintains a time of last
event (tL , cL ) whose initial value is (0, 0). When a model
executes an event at model time t, the simulator compares
that t to tL . If t = tL , then the simulation time of next event
becomes (t, cL + 1). Note that the model event time is still
t. If tL < t, then the simulation time of next event becomes
(t, 0). The time of last event is then set to be the new time.
There are two rules for comparing time stamps:
1. (t1 , c1 ) < (t2 , c2 ) if t1 < t2 or t1 = t2 and c1 < c2 ,
and
2. (t1 , c1 ) = (t2 , c2 ) if t1 = t2 and c1 = c2 .
Theorem 1 relates this logical clock to causal precedence when we are concerned with discrete event systems.
THEOREM 1. Let x(t) and y(t + τ), τ ≥ 0 be two events
in event trajectories x and y, which are restricted to a finite interval and for which y = F (x). Let C denote the
above-mentioned logical clock. If x(t) → y(t + τ), then
C(x(t)) < C(y((t + τ)).
Proof. Let C(x(t)) = (t, c). Suppose τ > 0 and y[t, t +
τ) = Φ[t, t + τ). In this case, we have C(y(t + τ)) =
(t +τ, 0) > (t, c). If τ = 0, then C(y(t +τ)) = (t, c+1) >
(t, c). Now suppose there is a nondecreasing sequence
τ1 , τ2 , · · · , τn , such that y(t + τi )  = Φ and y is equal
to the nonevent everywhere else. Moreover, suppose that
C(x(t)) < C(y((t + τ1 ))). By induction, it can be concluded that C(x(t)) < C(y((t + τn ))).
Using this clock, the time stamp order of events will
580 SIMULATION Volume 80, Number 11

contain the causal order of those events. The clock’s operation can be imagined by drawing a directed, acyclic graph
in which each node represents a model that undergoes a
zero time state transition (i.e., processes a zero time event)
in response to an input, and each arc represents an input
to the model. If cycles are present in the graph, they are
broken by inserting nodes to represent a particular model
at different instances in time (i.e., one node per model state
change is allowed in the graph). The input-free nodes are
labeled 0. These are the first nodes to execute events at,
say, time t. The neighbors of these nodes are labeled 1.
They are the next set of nodes to execute events at time t.
Their neighbors are labeled 2. Eventually, this procedure
will label every node in the graph. Sets of nodes with identical labels can be computed in any order. Otherwise, nodes
must be computed in label order. A node with more than
one incoming arc has several simultaneous inputs, with
one input event corresponding to each arc. The labels are
the numbers assigned to different zero time events by the
second field of the simulation clock.
6. The DEVS Formalism
The DEVS formalism uses two types of models to describe
a system. Atomic models represent nondecomposable processes, entities, or other types of system. More complex
models are constructed hierarchically. A coupled model is
a multicomponent model constructed from atomic models
and other coupled models.
An atomic model is described by a state set, output set,
and input set and a collection of functions that define its dynamic behavior. The internal transition function describes
the autonomous behavior of the system. The external transition function describes the input response of the system.
The confluent transition function is used to resolve the case
in which an internal transition and an external transition occur simultaneously. The time advance function indicates
the time that must elapse before the next internal event
occurs, assuming that no input becomes available in the
interim. The output function gives the output of the system
as a function of its state immediately before applying the
internal transition function.
Formally, an atomic model is represented by a structure
< S, X, Y, δint , δext , δconf , λ, ta >,
where S is the set of system states; X is the set of input
events; Y is the set of output events; δint : S → S is the
internal transition function; δext : Q × X b → S is the
external transition function, where Q = {(s, c) such that
s ∈ S and 0 ≤ c ≤ ta(s)}; δconf : S × Xb → S is the
confluent transition function; ta : S → R ∪ {∞} is the
time advance function; and λ : S → Y b is the output
function.
Note that the formalism allows bags of input and output
events to be consumed and produced, respectively, by the
system.

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

The DEVS atomic model specification defines a state
space representation of a system (see [5, 8]) in the following way. Let
S = (X, Y, Q, ∆, Λ)
define the system of interest, where X is an input set, Y
is an output set, Q is a set of states, ∆ : Q × X → Q is
the state transition function, and Λ : Q → Y is the output
function.
Let x be an event trajectory defined on the interval [ti , tf ]
that takes the value of the nonevent everywhere except,
possibly, at a finite number of points. The nonevent will
be denoted by the symbol Φ. Let q = (s, c) be a state
in Q, where s is a state of the DEVS model and c is the
consumed time, which is an element of the time base, and
c ≤ ta(s), where ta(s) is the time to the next transition.
The state transition function for the DEVS model is defined
recursively as follows (see [5]):
∆(q, x[ti , tf ]) =

(1)

Case 1:∆((δint (s), 0), x[ti + c, tf ]) if c = ta(s) and
∀t ∈ [ti , ti + c], x(t) = Φ.
Case 2:∆((δext (s, c + t − ti , x(t)), 0), Φ[t, t] · x(t, tf ]) if
∃. t ∈ [ti , min {ti + ta(s) − c, tf }] such that
(i) x(t) = Φ and t  = ti + ta(s) − c and
(ii) ∀t 
 ∈ [ti , t), x(t 
 ) = Φ.
Case 3:∆((δcon (s, x(t)), 0), Φ[t, t] · x(t, tf ]) if
∃. t ∈ [ti , min {ti + ta(s) − c, tf }] such that
(i) x(t) = Φ and t = ti + ta(s) − c and
(ii) ∀t 
 ∈ [ti , t), x(t 
 ) = Φ.
Case 4:∆((s, ta(s)), x[ti , tf ]) if c < ta(s) and
∀. t ∈ [ti , ti + ta(s) − c], x(t) = Φ.
As a demonstration of the state transition function, consider the simulation of a system from an initial state (s0 , 0)
with an input trajectory x[0, 5], which takes the value of
the nonevent Φ at every point except x(3) and x(5), and
ta(s) = 2. In this case, the recursive function gives the
next states as
∆((s0 , 0), x[0, 5]) =
∆((s0 , 2), x[0, 5]) =
∆((s0 , 0), x[2, 5]),

and (consistent with the above definitions for ∆ where all
transitions are defined with c = 0),
∆((δint (s0 ), 0), x(2, 5]).
Letting δint (s0 ) = s1 , the next state is given by
∆((s1 , 0), x(2, 5]) = ∆((δext (s1 , 1, x(3)), 0),
Φ[3, 3] · x(3, 5]).
Letting δext (s1 , 1, x(3)) = s2 , the state of the system at the
end of the trajectory is given by
∆((s2 , 0), Φ[3, 3] · x(3, 5]) = ∆((δcon (s2 , x(5)), 0),
Φ[5, 5] · x(5, 5]).
The output function for the DEVS is given by
Λ(q) = λ(s) if c = ta(s), Φ otherwise.

(2)

The system described by an atomic DEVS model evolves
through a series of internal (case 1), external (case 2), and
confluent (case 3) events. The “no event” (case 4) is included for completeness. An internal event occurs when
an amount of time equal to the time advance has elapsed
and no external events have occurred. An external event
occurs when the input trajectory has a non-null value at
some time prior to the time advance expiring. A confluent
event occurs when x(t) takes on a non-null value exactly
when the time advance expires. The DEVS output function can take on non-null values only in conjunction with
an internal or confluent event.
The dynamic behavior of an atomic model is generated
by applying an abstract simulator to the model description.
An abstract simulator is an algorithm that will generate a
state and output trajectory for the atomic model, given a
model description and suitable input trajectory. For example, algorithm 1 describes an abstract simulator for an
atomic model [26]. Let x[t0 , tf ] be an input trajectory that
we wish to apply to an atomic model. Assume that the
atomic model begins in some initial state s0 . An output
trajectory y[t0 , tf ] is computed along the way. The output
trajectory is assumed to initially take the value of Φ everywhere, and algorithm 1 will assign values to particular
points as the computation progresses. This algorithm will
produce exactly the behavior required by equations (1) and
(2) (see [25]).
Algorithm 1
tN := t0
tL := t0
s := s0
while (tN ≤ tf )
tN := min {tL + ta(s), t ∈ [tL, tf ] such that
x(t)  = Φ}
6.
if (tN = tL + ta(s) and x(tN )  = Φ)

1.
2.
3.
4.
5.

Volume 80, Number 11

SIMULATION

581

Nutaro and Sarjoughian

7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.

y(tN ) := λ(s)
s := δcon (s, x(tN ))
tL := tN
x(tN ) := Φ
else if (tN = tL + ta(s) and x(tN ) = Φ)
y(tN ) := λ(s)
s := δint (s)
tL := tN
else if (tN < tL + ta(s) and x(tN)  = Φ)
s := δext (s, tN − tL, x(tN ))
tL := tN
x(tN ) := Φ
end

20. end
A coupled model is described by a set of atomic components and a coupling specification. The closure under the
coupling property for coupled models (see [5]) ensures that
a coupled model can be reduced to an atomic model that is
equivalent at the I/O functional level. This allows for the inclusion of coupled models as components of other coupled
models and so enables hierarchical model construction.
Formally, a coupled model is described by a structure
N = < X, Y, M, Z >, where X and Y are the model input
and output sets, M is a set of component models subject to
the constraint N ∈
/ M (i.e., the network model cannot be
a component of itself), and Z is a coupling specification.
The coupling specification is a set of functions that describe three types of mappings. The functions zN m , where
m ∈ M, are the external input couplings. These map the
coupled model input set X to the component model input
set Xm . Similarly, the functions zmN are the external output couplings that map the component output set Ym to the
network model’s output set Y . Finally, internal couplings
are described by the functions zmk , where m, k ∈ M and
zmk maps the output set Ym to the input set Xk .
Assume that every component model is an atomic model
or a reduction of a coupled model to a behaviorally equivalent atomic representation. For any particular component,
if we are given an input trajectory to apply to the component, we can use algorithm 1 to compute the resulting
output trajectory. When a coupled model is simulated, its
behavior is determined by applying an input trajectory to
each component model. Unlike algorithm 1, however, the
input trajectories are not known a priori and must be constructed as the simulation progresses. Let us suppose that
the intended result of simulating a coupled model is to apply an input trajectory xi [t0 , tf ] to the ith component of the
coupled model. How must the xi be defined?
Let yi [t0 , tf ] be the output trajectory that results
 from
applying xi [t0 , tf ] to the ith component. Letting
be the
bag union function over a set of bags, we must have that

xi (t) =
zj,i (yj (t)).
(3)
i∈M,i=j

582 SIMULATION Volume 80, Number 11

The primary problem in constructing a simulator for coupled models is to ensure that the simultaneous construction
of input and output trajectories is done correctly. By correct, we mean that at the end of a simulation run (i.e., when
the simulation clock has reached tf ), the output trajectories
generated by each component model are exactly those that
would result if the input trajectory given by equation (3)
were applied to the component via algorithm 1.
Constructing such an algorithm is our ultimate goal.
However, since we are interested primarily in algorithms
for distributed and parallel computers, it is helpful to first
introduce a suitable notation. The notation makes clear the
distinction between the simulation algorithm, modeling
constructs, and implementation technology. Moreover, it
incorporates the causal precedence relation that was developed in the previous sections so as to make the notation
suitable for describing algorithms similar to algorithm 1,
as well as algorithms developed around the local causality
constraint.
7. Computational Representations of System
Formalisms
Distributed computing environments can be developed
based on a variety of architectural styles (e.g., independent components or events systems [27]) and software design methods. To design a distributed environment that can
support the modeling and simulation approach discussed
in this article, we have chosen a layered software architecture specification [21]. This approach separates model
representation and simulation execution. The software architecture is designed based on three layers—modeling,
simulation, and middleware [28]. The software components belonging to the modeling layer support model specifications. In this layer, software components allow defining
structural and behavioral aspects of the models (e.g., specifying atomic and coupled models). The simulation layer
consists of software components capable of executing the
models specified in the modeling layer based on sequential,
parallel, or distributed simulation protocols. The middleware layer consists of software components that provide
general services (e.g., timing and sending/receiving messages) for the simulation layer. These services facilitate
the design and implementation of simulation protocols and
their interoperation. Figure 1 depicts the organization of a
simulation framework for a general modeling formalism.
An architectural framework facilitates studying simulation protocols and distributed computational techniques
by having an abstract view of the modeling formalisms
under consideration. This abstraction is useful because it
describes the formalism in a computationally friendly way.
Here we will focus on the I/O functional view of a model in
which the initial state is assumed and the input trajectory
is described by an event sequence. A particular choice of
clock (i.e., weakly or minimally consistent) describes the
relationship between time and causality in the modeling
formalism. The clock described in section 5 can be used to

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

System formalism

interpretation

3. e1 = e2 if, and only if, t1 = t2 .

Model

Abstract
simulator

computation

correctness

Simulation algorithm
implementation
Computational platform
(middleware, etc.)
Figure 1. Elements of a modeling and simulation framework

construct a weakly consistent clock for system formalisms
that allow weakly causal models. However, a minimally
consistent clock is not disallowed.
As shown in Figure 1, the models themselves are described in an algorithm-neutral way (i.e., the construction
of the model description requires no knowledge of the simulation algorithm). Associated with the model is an abstract
simulator that describes the rules for interpreting models
described using the system formalism. This is not the same
as rules for interpreting a particular model. The details of a
particular model’s behavior are contained within the model
itself.
From the abstract simulator, a particular simulation algorithm is derived that is behaviorally identical to the abstract simulator. This is mapped onto a computational platform (e.g., hand calculator, parallel computer, cluster, or
workstation) in which the actual computation is performed.
The computational model, to be presented in what follows, fits between the system formalism and the simulation
algorithm. The goal is to provide a means of expressing
simulation algorithms in a form that is concise and eases
the job of building correctness proofs for a computational
scheme, with correctness, of course, being relative to the
abstract simulator whose behavior we are trying to realize.
A computational representation of a system formalism
is a structure that is, at the I/O functional level, equivalent to
the formalism. The computational representation consists
of a set of input values, a set of output values, and a set
of time stamps associated with those values. An event is
a (time, value) pair. The event e and its associated pair
(t, v) are used interchangeably. Time-stamped events can
be ordered as follows:
1. e1 ≤ e2 if, and only if, t1 ≤ t2 ,
2. e1 < e2 if, and only if, t1 < t2 , and

It is expected that time stamps are assigned to values in
a way that is consistent with the expectation that smaller
time stamps indicate earlier events.
If e1 and e2 are two causally independent events with
equal time stamps, then we can concatenate their values to
get a new event, e3 . Concatenation will be denoted by •. For
example, let (t, a) and (t, b) be two causally independent
events. Then, (t, a) • (t, b) ≡ (t, {a, b}). Concatenation
allows bags of causally unrelated events with equal time
stamps to be denoted by a single event. Concatenation is
associative and commutative.
We rely on a particular interpretation of the simulation
clock to determine when two events are causally independent. Most commonly, weak consistency is assumed, and
so e1 = e2 is taken to imply e1 ||e2 . However, if minimal
consistency is assumed, we cannot in general determine if
e1 ||e2 .
A computational representation of a system formalism
has a state that is represented by a stack of events that record
the event history of the simulation. Formally, a computational representation is described by
ST C = < E, G, push, eN, out >,
where E is an event set; G is a set of event stacks (e0 e1
. . . ), where ei ∈ E, push : G × E → G is the transition
function; eN : G → E is the next event function; and
out : G → E is the output function.
The push function is defined such that
push((e0 e1 . . . en ), e) = (e0 e1 . . . en e).
The functions eN and out depend on the interpretation of
the simulation clock. If we assume a weakly consistent
clock, then these functions are constrained such that
eN (e0 e1 . . . en ) = e such that en < e, and
out (e0 e1 . . . en ) = y such that en < y ≤ eN (e0 e1 . . . en ).
If a minimally consistent clock is assumed, then
eN (e0 e1 . . . en ) = e such that en ≤ e, and
out (e0 e1 . . . en ) = y such that en ≤ y ≤ eN (e0 e1 . . . en ).
8. Computational Representation of Logical
Processes
The above model includes the logical-process worldview.
A stack corresponds to the event history of a logical process. Given a stack s, the operation push(s, eN (s)) represents the logical process executing an event that it scheduled for itself. If an event x is received from some other logical process, the operation push(s, x) constructs the event
Volume 80, Number 11

SIMULATION

583

Nutaro and Sarjoughian

history that results from executing event x. The out (s)
function is used to determine what events should be sent
to other logical processes.
A logical process simulator may or may not make use
of event concatenation. For example, if a particular simulator uses a first-come, first-serve processing order for
events with identical time stamps, then concatenation is not
needed. However, if events with equivalent time stamps are
delivered to the logical process for sorting, then concatenation provides a mechanism for doing so. Similarly, a particular logical-process implementation may choose either
form of clock—see, for example, Yaddes, which requires
nonzero look-ahead for efficient simulation (see [20]), and
SPEEDES, which allows for it (see [29]).
9. Computational Representation of DEVS Atomic
Models
We will consider only atomic DEVS models. Let s be the
stack that represents the event history of the system up
to some time t. The next event is determined by one of
three operations. An internal event is represented by the
operation push(s, eN (s)). Given an input event x, the external transition function is represented by push(s, x). The
confluent transition function is modeled by the operation
push(s, eN (s) • x). The output function is modeled by
out (s). The time advance function is wrapped up in the
time stamp of the eN (s) event, as will be shown in the
next section. For a more formal treatment of the relationship between DEVS and simulation processes, see Nutaro
[12].
It is clear from the preceding discussion that a logicalprocess simulator is not necessarily correct with respect
to the DEVS formalism. Such a simulator might violate
the formalism in several ways. For example, selection of
a minimally consistent clock that uses model-derived time
stamps will not produce results that are correct with respect
to DEVS [4]. Similarly, the algorithm must be sensitive to
how preemption of internal events is handled to maintain
the semantics of the time advance function. Last, DEVS
specifies event bags and the confluent transition function
for managing simultaneous events. These features need not
be supported by a logical-process simulator.
10. An Event-Stepped Parallel Algorithm
In this section, we develop a simple parallel simulation algorithm in terms of the above model. The algorithm is an
event-stepped algorithm equivalent to a single-level parallel DEVS abstract coordinator (see [5]). An inductive
argument is used to demonstrate the correctness of the algorithm with respect to the DEVS abstract simulator. The
construction of more complex optimistic and risk-free optimistic algorithms has been demonstrated in Nutaro [25].
However, this relatively simple algorithm better serves to
illustrate the notation and basic method of proof.
584 SIMULATION Volume 80, Number 11

Let < Ei , Gi , pushi , eNi , outi > denote the ith simulator, and assume we have N of them labeled 1, 2, . . . , N.
We use Φ to denote no event (i.e., x • Φ = x). It is assumed that the clock is weakly consistent. The “for each”
blocks are executed in parallel.
Algorithm 2 (Event-Stepped Simulation)
1. tN := min {outi (stci ).t} for all i in [1, N ]
2. while (tN < tstop )
3.
for each i in [1, N ]
4.
if (eNi (stci ).t = tN )
5.
xi := eNi (stci )
6.
else
7.
xi := Φ
8.
end
9.
end
10.
for each i in [1, N ] such that outi (stci ).t = tN
11.
for each j in [1, N ] such that i influences j
12.
xj := xj • outi (stci )
13.
end
14.
end
15.
for each i in [1, N ] such that xi  = Φ
16.
stci := push(stci , xi )
17.
end
18.
tN := min(outi (stci ).t) for all i in [1, N ]
19. end
Since each simulator is begun in its correct initial state,
the algorithm is trivially correct prior to the execution of
the first event. Now, suppose the algorithm has proceeded
without error, and each simulator has processed its first k
events correctly. Consider the next pass of the algorithm at
the j th simulator. The correct outcome of this pass will be
that event k + 1 is an internal event, an external event, or a
confluent event, or no event can be processed correctly in
this pass. These four cases are addressed separately.
Case 1. Assume that no correct event can be processed
in the next pass. From the induction hypothesis, at line 18,
tN is found to be less than eNj (sj ).t. So at line 7, the next
event is set to the null event. Line 12 is not executed since
that would require that some influencer of j processed an
incorrect event in a previous iteration of the algorithm,
which would contradict the induction hypothesis. So line
15 evaluates to false, and no event is pushed onto the stack.
Case 2. Assume that the next correct event is an internal
event. At line 18, tN is set to eNj (sj ). So line 4 evaluates
to true. Line 12 is not executed since this would require
that some influencer of j processed an incorrect event in a
previous iteration, contradicting the induction hypothesis.
So at line 16, eNj (sj ) is pushed onto the stack.
Case 3. Assume that the next simulation event is an external event. At line 18 in the previous pass, tN is computed
to be less than eNj (sj ). So line 4 evaluates to false. Since

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

the event stacks are assumed to be correct at the end of the
last pass, and no events have been pushed onto the stack
prior to line 12, we can conclude that line 12 is executed
for all and only the correct influencers of j , thereby constructing the next correct input event to be pushed onto xj .
At line 16, the event is processed.
Case 4. Assume that the next simulation event for j is
a confluent event. At line 18 on the previous pass, tN is
computed to be equal to eNj (sj ). As in case 3, we have that
line 12 is executed for all and only the correct influencers of
j , thereby constructing the next correct event to be pushed
onto sj . At line 16, the event is processed.
Additional properties that we would like to show are
liveness (freedom from deadlock) and correct termination
(the simulation halts when the desired stacks have been
created). Liveness holds because the simulation clock is
always advanced to the time of the next simulation event.
The algorithm terminates when the next simulation event
has a time stamp greater than tstop .
11. Conclusions
The model presented in this article provides a unified approach for describing time and causality in parallel simulation protocols when those protocols are required to be
correct with respect to system-theoretic modeling principles. The model provides a unique capability to describe
algorithms that can simulate models with a DEVS representation at the I/O functional level. The model maintains
important aspects of the logical-process model that has
been useful for studying distributed and parallel simulation algorithms.
The approach taken in this article started from the
logical-process worldview. A characterization of that
worldview in terms of general systems theory led to a solution for the simultaneous event problem when simulating
DEVS models. An alternative approach, beginning with
general systems theory and proceeding toward the logicalprocess worldview, is suggested in Barros [30]. The author
orders apparently simultaneous events by separating them
in time with an infinitesimally small value, denoted by ε.
A sequence of n events with identical time stamps can be
ordered by adding an appropriate multiple of ε to the each
event time stamp. The application of this approach to the
parallel simulation of logical-process models is an interesting topic for future research.
12. Appendix
An abstract time system is a system with input and output
objects that are abstract time functions. An abstract time
function is a function from a time base T to a set of values.
It is often necessary to consider abstract time systems
over finite time intervals. An interval is described using the
notation presented in section 2. A left interval for an abstract time function x is described using the notation x|t)

(or x|x]). This notation indicates that we are considering
values of time less than t (or less than or equal to t). A restriction of a function is the function obtained by restricting
the domain of the original function to a smaller interval.
An abstract time system is causal if the system output is
a function of past and current, but not future, inputs. There
are two notions of causality for abstract time systems [8].
The first requires that the system outputs be a function of
past and current inputs. Such a system is weakly causal, or
simply causal. The second requires that the system outputs
be a function of past inputs only. Such a system is called
strongly causal. Definition 2 gives a formal definition of
causality for abstract time systems [8].
DEFINITION 2. An abstract time system S ⊂ X × Y is
causal if
(i)x|t] = x 
 |t] ⇒ y|t] = y 
 |t].
The system is strongly causal if condition (i) is replaced
by the stronger condition
(ii)x|t) = x 
 |t) ⇒ y|t] = y 
 |t].
The following properties are direct consequences of definition 2.
PROPERTY 1. If S is a causal system, then S with inputs
restricted to [t0 , tf ] and outputs restricted to [t0 , tf ] is a
function.
PROPERTY 2. If S is a strongly causal system, then S with
inputs restricted to [t0 , tf ) and outputs restricted to [t0 , tf ]
is a function, and so is S with inputs restricted to [t0 , tf )
and outputs restricted to (t0 , tf ].
PROPERTY 3.
a function.

S[t0 , tf ] is causal if and only if S[t0 , tf ] is

Properties 1 and 2 state that special restrictions of a
(strongly) causal system yield an I/O functional system. So,
given a causal system, we can safely consider appropriate
restrictions of it to be functions. Property 3 states that on
an interval of finite width, every I/O functional system is a
causal system and vice versa.
Property 2 demonstrates the predictability of strongly
causal systems. Consider, for example, a simple delay function y(t) = x(t − τ). This system produces an output that
is a copy of the input as it appeared τ units of time in the
past. If we want to predict the output of this system at some
time t, it is sufficient to observe the input up to time t − τ.
In a simulation system, we might take advantage of this
by releasing future outputs based on current inputs (i.e., if
we see an input event at time t, we can release an output
event with time stamp t + τ). This observation translates
into the concept of look-ahead in conservative simulation
algorithms developed for logical-process models (see, e.g.,
[3, 5]).
Now we turn to constructing an analog of the eventoriented model for distributed computing systems (see [2])
Volume 80, Number 11

SIMULATION

585

Nutaro and Sarjoughian

in terms of state transition and coupled component models
(see [5]).
Consider a single run of a distributed program. Each
process in the run generates an execution trace e0 e1 . . . en ,
which is a finite sequence of local events. An event is some
action that causes a change of state (e.g., executing a procedure, sending or receiving a message, etc.). Three types
of events are considered significant in this model. These
are send events, receive events, and internal events. A send
event occurs when a process sends a message to some other
process. A receive event is the receipt of a send event. Internal events model autonomous behavior on the part of a
process. Definition 3 defines causal precedence in terms of
this event sequence model [11].
DEFINITION 3. An event e is said to immediately precede
an event f if
1. e and f are executed on the same processor and e is
the event that appears immediately before f in the
execution trace of the processor, or
2. e is a send event and f is the corresponding receive
event.
The causal precedence relation is the transitive closure of
the immediately-precedes relation.
If an event e causally precedes an event f , this is denoted
by e → f . When it is not the case that e → f and that
f → e, then e and f are said to be causally independent.
This is denoted by e||f .
Event sequences can be seen as the input and output trajectories of a transition system (see [2] for detailed treatment). A transition system is a state machine that has rules
for processing receive events, internal events, and send
events. In this model, receive events correspond to the consumption of a message, and send events correspond to the
production of a message. It is assumed that there is a set
of all possible messages, denoted by M, which can be sent
(via send events) and received (via receive events). This is
made precise in definition 4.
DEFINITION 4. A transition system is a tuple <C, I,
⊥r , ⊥s , ⊥i >, where C is a set of states, I is a set of initial states, ⊥r ⊆ C × M × C is the transition function
corresponding to a receive event for a message m ∈ M,
⊥s ⊆ C × M × C is the transition function corresponding
to a send event for a message m ∈ M, ⊥i ⊆ C × C is the
transition function corresponding to an internal event.
The state transition behavior of this system can be described by a system
< X, Y, S, ∆, Λ >,
where X and Y are sets of internal, send, and receive
events; S corresponds to the set of processor states C;
∆ = {(s, x, s 
 ) : x is an internal event and (s, s 
 ) ∈ ⊥i ,
or x is a receive event corresponding to a message m and
586 SIMULATION Volume 80, Number 11

(s, m, s 
 ) ∈ ⊥r , or x is a send event corresponding to a
message m and (s, m, s 
 ) ∈ ⊥s }; and Λ = {(s, y) : y is
the next internal, send, or receive event that the transition
system will execute when it is in state s}.
The state transition function for this system is the union
of state and event pairs occurring in the transition functions
of the transition system. The output function describes the
next event that the process will execute when it is in a
particular state.
This definition of the output function agrees with the
interpretation of an event sequence given in Garg [1]. An
event sequence can be augmented to include the sequence
of states that result from executing an event. So the sequence e0 e1 . . . en becomes s0 e0 s1 e1 . . . sn−1 en , where si is
a process state, ei−1 is the event that caused a transition
from state si−1 to si , and ei is the next action taken by the
system when it is in state si . In our state transition representation of a process, this same behavior is obtained by
using a feedback loop.
The input/output behavior of this system is a relation
over sequences of events. The relation maps sequences of
length N to sequences of length N + 1. An input sequence
of length N corresponds to the sequence of events that
caused the system to be placed into a particular state. The
output sequence of length N + 1 is prefixed by a copy of
the input sequence and ends with the next event that the
system wants to process. It is clear that the process is a
strongly causal system since we only require the first N
inputs to determine the (N + 1)st output.
When a distributed computation involves several processes, their joint behavior is modeled by another transition system that is constructed from the transition systems
that model each process.
DEFINITION 5. Let M denote the collection of multisets
with elements in the message set M. The transition system
induced under asynchronous communication by a set of
processes p1 , p2 , . . . , pn , is
< C × M, I, ⊥r , ⊥s , ⊥i >,
where each pk is described by a transition system
< Ck , Ik , ⊥rk , ⊥sk , ⊥ik >,
and C = C1 × C2 × . . . × Cn ; I = I1 × I2 × . . . × In × {∅};
⊥i = {(((c1 , . . . , ck , . . . , cn ), M1 ), ((c1 , . . . , ck
 , . . . , cn ),
M1 )) such that (ck , ck
 ) ∈ ⊥ik }; ⊥s = {(((c1 , . . . , ck ,
. . . , cn ), M1 ), ((c1 , . . . , ck
 , . . . , cn ), M1 ∪ {m})) such that
(ck , m, ck
 ) ∈ ⊥sk }; and ⊥r = {(((c1 , . . . , ck , . . . , cn ), M1 ),
((c1 , . . . , ck
 , . . . , cn ), M1 − {m})) such that (ck , m, ck
 ) ∈
⊥rk and m ∈ M1 }.
For each message, there is assumed to be a unique process that can receive the message. Note that the transition
system is defined not by transition functions but only by
transition relations. This is because we compute the next
state by executing a single event at a single process, and
we allow any order of events that result in equivalent executions [2]. If events a and b are causally independent (i.e.,

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

a||b holds), then we can apply these events in any order
and arrive at identical states. Consequently, the order in
which the events are applied is irrelevant.
Consider a state transition representation of the
message-passing system. This system will accept as input
send, receive, and internal events. The messaging system
has one input/output port for each process in the distributed
system.
When the message-passing system receives a send message from process pk intended for process pi , the system
stores a message for pi and immediately produces a send
event for pk . When the message-passing system is provided
with a receive event from process pk and there is a stored
message for process pk , then the message-passing system
removes the message from its storage and generates a receive event for pk . If the message-passing system receives
an internal event from process pk , then it immediately generates as output an internal event for process pk .
Any other input (i.e., a receive event when no message
is stored) causes the message-passing system to generate
a nonevent, which is treated by the process as an internal
event, and does not change the process state (e.g., it might
correspond to waiting for a message). A working example of a message-passing system such as the one described
can be found in the Message Passing Interface (MPI) standard (see [31]). Note that the message-passing system is
a causal, but not strongly causal, system since it produces
output immediately in response to an input.
Consider the multicomponent system depicted in Figure 2. The message-passing system is denoted by M ∗ , and
the state transition representation of process pk is denoted
by pk∗ . If the original transition system includes nonevents,
then this model can be reduced to a smaller image of its
transition system counterpart as follows.
This multicomponent model can be reduced to a closed
(i.e., input and output free) state transition model that is
given by
< S, ∆ >,
where S = C × M, which is the state space of the associated transition system; ∆ ⊆ {(((c1 , . . . , ck , . . . , cn ), M1 ),
((c1
 , . . . , ck
 , . . . , cn
 ), M2 ))}, where for each component
either
1. c
 results from c when an internal event is applied,
2. c
 results from c when an send event is applied corresponding to a message m, and m is in M2 , or
3. c
 results from c when a receive event is applied
corresponding to a message m, m is in M1 , and the
number of instances of m in M2 is exactly one less
than the number of instances in M1 .
State trajectories for this new system coincide with a
unique sequence of equivalent executions that correspond
to an execution of the original transition system.

p

p1*

p2*

p3*

p 4*

M*
.
.
.
.

.
.
.
.

Figure 2. A multicomponent system consisting of several
processes and a message-passing system

EXAMPLE 1. Consider two processes participating in a
distributed computation. Each process executes an internal
event. Next, the first process sends a message to the second
process. This is followed by the second process sending a
message to the first process. Each process can be modeled
by a transition system
C1 = C2 = {s1 , s2 , s3 , s4 },
I1 = I2 = {s1 },
M = {m12 , m21 },
⊥r = {(s3 , m21 , s4 )}
for the first process and
{(s2 , m12 , s3 )}
for the second process,
⊥s = {(s2 , m12 , s3 )}
for the first process and
{(s3 , m21 , s4 )}
for the second process,
⊥i = {(s1 , s2 )}
for both processes.
Any other event applied to the transition system (including the “nonevent”) does not change its state. The transition
system induced by these processes under asynchronous
communication is readily determined from definition 5.
Volume 80, Number 11

SIMULATION

587

Nutaro and Sarjoughian

Starting from state (s1 , s1 ), the induced transition system can proceed to state (s2 , s2 ) in two different ways. The
internal events can be applied to the first process and then
to the second process or vice versa. From state (s2 , s2 ),
the system must proceed through states (s3 , s2 ), (s3 , s3 ),
(s3 , s4 ), and (s4 , s4 ). So two possible (and equivalent) executions of the induced transition system are
(s1 , s1 , {})(s1 , s2 , {})(s2 , s2 , {})(s3 , s2 , {m12 })(s3 , s3 , {})
(s3 , s4 , {m21 })(s4 , s4 , {})
and
(s1 , s1 , {})(s2 , s1 , {})(s2 , s2 , {})(s3 , s2 , {m12 })(s3 , s3 , {})
(s3 , s4 , {m21 })(s4 , s4 , {}).
The corresponding sequence of states computed by the reduced multicomponent model is
(s1 , s1 , {})(s2 , s2 , {})(s3 , s2 , {m12 })(s3 , s3 , {})
(s3 , s4 , {m21 })(s4 , s4 , {}).
Notice that the equivalent state sequences (s1 , s1 , {})
(s1 , s2 , {})(s2 , s2 , {}) and (s1 , s1 , {})(s2 , s1 , {})(s2 , s2 , {})
have been replaced by the sequence (s1 , s1 , {})(s2 , s2 , {}).
The latter sequence is the unique result that must follow
from processing the causally independent internal events
(see theorem 2.21 in [2]).
It is possible to derive the classical (i.e., definition 3)
notion of causal precedence. Consider the restriction of
a transition system with an I/O function F restricted to
[k, k + m] and input sequences x and y. Then we have
y[k, k + m)y[k + m, k + m] = F (x[k, k]x(k, k + m]). It
follows that event x(k) causally precedes y(k + m), and so
we have arrived at condition (1) of definition 3.
Now suppose that a process consumes a send event x(k).
A message is then stored in the messaging system M ∗ , and
at some later time, a receive event y(k + m) is ejected
from M ∗ . This corresponds to x(k) being a send event and
y(k + m) being the corresponding receive event. Let M ∗∗
be the I/O function for M ∗ and F the I/O function for the
sending process. So we have
y[k, k + m)y[k + m, k + m]
= M ∗∗ ((F (x[k, k]x(k, k + m]))),
and x(k) causally precedes y(k + m). We have arrived
at condition (2) of definition 3. Transitive closure under
function composition completes the derivation.
13. Acknowledgment
This research was partially supported by NSF Next Generation Software (grant no. EIA-9975050) and Scaleable
Enterprise System (grant no. DMI-0122227) programs.
588 SIMULATION Volume 80, Number 11

14. References
[1] Garg, V. K. 1996. Principles of distributed systems. Boston:
Kluwer Academic.
[2] Tel, G. 2000. Introduction to distributed algorithms. Cambridge,
UK: Cambridge University Press.
[3] Fujimoto, R. M. 2000. Parallel and distributed simulation systems.
New York: John Wiley.
[4] Zeigler, B. P., G. Ball, H. Cho, J. S. Lee, and H. Sarjoughian. 1999.
Implementation of the DEVS formalism over the HLA/RTI:
Problems and solutions. Paper presented at the Simulation Interoperability Workshop, March, Orlando, FL.
[5] Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory of modeling and simulation. 2nd ed. San Diego: Academic Press.
[6] Bagrodia, R., K. M. Chandy, and Wen Toh Liao. 1991. A unifying
framework for distributed simulation. ACM Transactions on
Modeling and Computer Simulation 1 (4): 348-85.
[7] Fujimoto, R. M. 1998. Parallel and distributed simulation. In
Handbook of simulation, edited by Jerry Banks. New York:
John Wiley.
[8] Mesarovic, M. D., andY. Takahara. 1989. Abstract systems theory.
Berlin: Springer-Verlag.
[9] Misra, J. 1986. Distributed discrete-event simulation. Computing
Surveys 18 (1): 39-65.
[10] Rönngren, R., and M. Liljenstam. 1999. On event ordering in
parallel discrete event simulation. In Thirteenth Workshop on
Parallel and Distributed Simulation, pp. 38-45.
[11] Lamport, L. 1978. Time, clocks and the ordering of events in a
distributed system. Communications of the ACM 21 (7): 55865.
[12] Nutaro, J. J. 2000. Time management and interoperability in distributed discrete event simulation. Master’s thesis, University
of Arizona, Department Electrical and Computer Engineering.
[13] Zeigler, B. P. 1976. Theory of modeling and simulation. New
York: John Wiley.
[14] Kofman, E. 2003. Quantization-based simulation of differential
algebraic equation systems. Technical Report LSD0203, LSD,
Universidad Nacional de Rosario.
[15] Wieland, F. 1999. The threshold of event simultaneity. Transactions of the Society for Computer Simulation International
16 (1): 23-31.
[16] Nicol, D., J. Liu, and J. Cowie. 2000. Safe timestamps and largescale modeling. In Proceedings of the 14th Workshop on Parallel and Distributed Simulation, May, Bologna, Italy, pp. 71-8.
[17] Barz, C., Rolf Göpffarth, P. Martini, andA.e Wenzel. 2003.A new
framework for the analysis of simultaneous events. In Proceedings of SCSC’03 (Summer Computer Simulation Conference),
July, Montreal, Canada.
[18] Jha, V., and R. Bagrodia. 2000. Simultaneous events and lookahead in simulation protocols. ACM Transactions on Modeling
and Computer Simulation 10 (3): 241-67.
[19] Frey, P., R. Radhakrishnan, H. W. Carter, Philip A. Wilsey,
and P. Alexander. 2002. A formal specification and verification framework for time warp–based parallel simulation. IEEE
Transactions on Software Engineering 28 (1): 58-78.
[20] Ghosh, S. 1996. On the proof of correctness of yet another asynchronous distributed discrete event simulation algorithm (YADDES). IEEE Transactions on Systems, Man, and
Cybernetics—Part A: Systems and Humans 26 (1): 68-80.
[21] Sarjoughian, H. S., and B. P. Zeigler. 2000. DEVS and HLA:
Complementary paradigms for M&S? Transactions of the Society for Computer Simulation 17 (4): 187-97.
[22] Ferscha, A. 1995. Parallel and distributed simulation of discrete
event systems. In Handbook of parallel and distributed computing, edited by A. Y. Zomaya. New York: McGraw-Hill.

DESIGN OF DISTRIBUTED SIMULATION ENVIRONMENTS

[23] Mattern, F. 1994. Detecting causal relationships in distributed
computations: In search of the Holy Grail. Distributed Computing 7 (3): 149-74.
[24] Nutaro, J. J. and H. S. Sarjoughian. 2001. Speedup of a sparse system simulation. In 15th Workshop on Parallel and Distributed
Simulation, May, Lake Arrowhead, CA, pp. 193-9.
[25] Nutaro, J. J. 2003. Parallel discrete event simulation with application to continuous systems. Ph.D. diss., University of Arizona,
Department of Electrical and Computer Engineering.
[26] Chow, A. C.-H. 1996. Parallel DEVS: A parallel, hierarchical,
modular modelling formalism and its distributed simulation.
Transactions of the Society for Computer Simulation International, 13 (2): 55-67.
[27] Bass, L., P. Clements, and R. Kazman. 1997. Software architecture in practice. Reading, MA: Addison-Wesley.
[28] Sarjoughian, H. S., B. P. Zeigler, and S. B. Hall. 2001. A layered
modeling and simulation architecture for agent-based system
development. IEEE Proceedings 89 (2): 201-13.
[29] Steinman, J. S. 1992. SPEEDES: A multiple-synchronization environment for parallel discrete event simulation. International
Journal for Computer Simulation 2 (3): 251-86.

[30] Barros, F. J. 1998. Handling simultaneous events in dynamic
structure models. SPIE Proceedings: Enabling Methodologies
for Simulation 3369:355-63.
[31] Snir, M., S. Otto, S. Huss-Lederman, D. Walker, and J. Dongarra.
1998. MPI—The complete reference: Vol. 1. The MPI core. 2nd
ed. Cambridge, MA: MIT Press.

James Nutaro is a research assistant professor at the Arizona
Center for Integrative Modeling and Simulation, College of Engineering and Mines, Electrical and Computer Engineering Department, University of Arizona, Tucson.
Hessam Sarjoughian is an assistant professor at the Arizona
Center for Integrative Modeling and Simulation, Fulton School
of Engineering, Computer Science and Engineering Department,
Arizona State University, Tempe.

Volume 80, Number 11

SIMULATION

589

Future Generation Computer Systems 17 (2000) 89–105

Collaborative distributed network system: a lightweight
middleware supporting collaborative DEVS modeling
Hessam S. Sarjoughian∗,1 , Bernard P. Zeigler, Sunwoo Park
AI & Simulation Research Group, Department of Electrical and Computer Engineering,
University of Arizona, Tucson, AZ 85721-0104, USA

Abstract
The present and future of our world, in part, depends on the effective application of problem solving techniques and
tools that are able to bring together expertise from many disciplines, e.g. problems such as how world wide distribution
of commodities are inherently multidisciplinary in nature. As such they demand collaborative computer-based tools that
enable subject matter experts to work cooperatively while overcoming the constraints of space and time. The realization of
collaborative tools can benefit greatly by relying on “out-of-the-box” higher level network interconnectivity software. In this
paper, we discuss the collaborative distributed network model (CDNM). Based on this model, a lightweight middleware called
collaborative distributed network system (CDNS) is implemented using Java technologies. We discuss CDNM’s architecture
and the services it can provide to modeling and simulation applications built on its foundation. Furthermore, we discuss
Collaborative DEVS Modeler, a collaborative modeling environment, where dispersed clients interact with one another using
the services offered by the CDNS. The explicit architectural design of CDNM will facilitate its future migration into emerging
middleware standards such as High Level Architecture (HLA) and CORBA. © 2000 Elsevier Science B.V. All rights reserved.
Keywords: Architecture; Collaboration; CSCW; Design; Distributed computing; Java; Middleware; Modeling; Network; Simulation

1. Introduction
Use of modeling and simulation is increasing since
it provides insight into how complex systems work.
Indeed, many contemporary systems can only be
understood and manipulated using modeling and simulation techniques. Collaboration provides a powerful
approach for creating simulation models [1]. For example, problems such as shipment of commodities
can no longer be modeled and understood by under∗ Corresponding author.
E-mail addresses: hessam@ece.arizona.edu (H.S. Sarjoughian),
zeigler@ece.arizona.edu (B.P. Zeigler), sunwoo@ece.arizona.edu
(S. Park).
1 http://www.acims.arizona.edu

standing the inner working of the shipping company
alone. Instead, subject matter experts from the shipping company as well as others from the Airline
Company, Telephone Company, etc. must join forces
to understand the complexity and intricacies of an
entire shipping process. The objective in designing a
collaborative modeling environment is to enable multiple modelers to share each other’s knowledge and
build well-structured models of multifaceted systems
better and cheaper [2].
The development of collaborative modeling tools
requires wide-area networking capabilities. Many alternative middleware (networking) environments with
varying degrees of expressive power, cost and ease of
use exist. Such environments can be highly generic
or conversely, very specific. Consequently, a desir-

0167-739X/00/$ – see front matter © 2000 Elsevier Science B.V. All rights reserved.
PII: S 0 1 6 7 - 7 3 9 X ( 9 9 ) 0 0 1 0 6 - 5

90

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

able middleware can be customized from a general,
all-purpose middleware such as CORBA [3], or instead be conceived, designed and developed based on
its intended use. With this point of view in mind, our
objective has been to develop a collaborative network
architecture that can interconnect a relatively small
group of modelers (e.g., 5–30) independent of their
whereabouts. 2 Indeed, it is desirable to develop a collaborative network architecture that can support not
only collaborative modeling, but also other activities
of the modeling and simulation enterprise such as
simulation set-up and execution.
A detailed taxonomy of collaboration has been established based on time and place constraints [4]. In
this taxonomy, collaboration is defined to take place
in three modes: conventional, synchronous and asynchronous. Conventional collaboration entails immediate, face-to-face communication. This form of collaboration requires all participants to be in the same place
at the same time. Synchronous collaboration can occur when the participants have agreed on a time to
meet, but not necessarily in the same physical location.
Video-teleconferencing is an example of synchronous
collaboration. Asynchronous collaboration does not
require any agreements on time or place. In this mode,
neither time nor place is considered to be essential
for collaboration. Electronic mail and a computer bulletin board are examples of asynchronous collaboration. Based on the general taxonomy of collaboration
modes, two modes of model development have been
proposed: model discovery/invention and model synthesis [1]. The former emphasizes creation of primitive
models while the latter focuses on building composite
models from existing primitive, smaller models.
This paper addresses architectural design issues that
facilitate the development of collaborative modeling
systems. The main focus is on the use of a middleware (CDNS) to support the design of an extensible
collaborative DEVS modeling environment (CDM).
Section 2 provides a brief account of relevant architectural approaches for multiuser systems. Section 3
gives a precise definition for the concept of session
since it provides the basis for much of what is con2

The suggested number of modelers participating in model building is not based on any scientific studies. Group size and other
related issues that can support effective collaborative modeling
requires research on its own right.

tained in Section 4, where the collaborative distributed
network model (CDNM) is discussed in detail. The
objective of this detailed discussion is to provide an
“insider view” of network related issues such as objects moving around in a wide-area network, CDNS
architectural integrity (e.g., extensibility) and the implications of such a middleware for building specific
applications (e.g., CDM) based on its services. Specifically, the layered architecture of the CDNM exposes
innocent looking, yet critical, issues that should be
considered. For example, much detail is presented on
the types of objects and their roles so that the CDNS
services can be interpreted and utilized in a systematic
fashion by an application built on top of it. In Section
5, a brief view of the CDNM realization (i.e., CDNS)
is presented. Collaborative DEVS Modeler application is described in terms of its high-level architecture.
The remaining two sections consider related/future
research and conclusions.

2. Client/server system architectures
During the last several decades, three distinct
models of the distributed software/system architectures have emerged. These are client/server network,
peer-to-peer network and server-network. Each enables dispersed users to join forces to tackle problems
that cannot be effectively solved by a single machine
or user. The client/server model consists of clients and
a server where the server provides services that are
requested by its clients [5]. In a client/server network,
a client sends a request to the server which in turn
sends a reply back to the client. The interactions are
many-to-one from clients to server and one-to-many
from server to clients. In this model, clients do not
communicate directly, but use the server as an intermediary.
Several styles of client/server architectures are used
in network-enabled applications. With the emergence
of the World Wide Web, the thin-client style has
gained much popularity for ease of deployment and
version control. Thin-clients are typically very small,
indicating that they can be downloaded quickly from
a network server and used in a “one shot” fashion.
Versioning of the thin-client is almost trivial. When
the software is recompiled, a user has automatic access to the new version when she/he connects to the

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

server. The thin-client, however, has several disadvantages such as demand for significant bandwidth
for all, but small applications. Thin-clients rely on the
server to do most, if not all, of application’s requests.
Thick-clients, unlike thin-clients, tend to be more
robust, scalable and capable of handling even the worst
network conditions/constraints. The thick-client application requires that the client process maintain a local
copy of the relevant data and program. This allows
each client to act independently to the extent that it
has been empowered to. A server provides services
that the client cannot provide by itself. A thick-client
can make the network invisible to the user while the
software is being used.
The peer-to-peer (serverless) network allows all
clients (or subsets thereof) to be directly interconnected to one another. In such an arrangement, there
is no intermediary and the client broadcasts (or multicasts) its messages directly to every client in the
network. This removes the single point of failure and
eliminates the server bottleneck. However, it introduces ambiguities when any dispute arises.
The remaining client/server architecture style,
server-network model, provides hybrid architecture
based on both the client/server and the peer-to-peer
model [6]. Groups of clients are connected to a server.
Several servers are interconnected. The nodes of the
server network (i.e., servers) can be configured as
either permanent or dynamic where they can be periodical or otherwise according to the type of data
exchange requirements. For example, each server
may send a message to a specific server or all servers
(commonly referred to as flooding). A client in
server-network environment may access a local server
as well as remote servers. It also provides improved
scalability over the client/server model and better
control management than the peer-to-peer model.
Once again, the enhanced features of server-network
must be traded-off against added complexity.

3. Anyplace/anytime session
Before we proceed, we describe what is a Collaborative Session [7]. It is a workspace dedicated to a
group of clients who are working together via a network. It provides a set of services to its users during
a period having a start-time and an end-time. The

91

Fig. 1. Example of collaborative sessions in multiclient and multiserver system.

services are well-defined both in terms of the functionalities they provide and how they are requested
and delivered via specific communication protocols
(e.g., Java RMI). A session is the combination of
a session-client and a session-server as depicted in
Fig. 1. A session-client provides a client with services,
some of which are performed at the client side and
others at the server side. A session-server provides
services directly to itself and indirectly to its clients.
A user can have multiple session-clients when she/he
is involved in multiple distinct sessions at that time.
Furthermore, these sessions are independent of one
another with the implication that communication
among session-servers are not supported.

4. Collaborative distributed network model
The CDNM is a model for collaborative distributed
network system (CDNS) [8]. It provides generic network management services to both clients and servers.
It employs event multicasting, system management
and monitoring facilities. This section discusses the
main software components of the CDNM from a System Architecture perspective. The functional requirements for CDNM are the following:
• Extensible/flexible software development/evolution.
CDNM architecture should be extensible to support future capabilities such as (synchronized) publish/subscribe to an application component.

92

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

Fig. 2. Layered architectural view of collaborative client/server model.

• Simple design and implementation. Design, and
consequently, implementation should be devoid
of complexity (e.g., introducing fictitious classes
that have little or no impact in achieving a robust
software design).
• Be an adaptable middleware given lower level network layers. Network level should readily be replaceable with an alternative one (e.g., choosing
RMI instead of Socket).
• Support building non-monolithic applications. The
design should support a multitude of applications
such as modeling, simulation, animation, business
processes, etc. The CNDM should support adding
new applications without requiring modifications to
the ones already supported.
• Have well-defined interfaces to application layer
and network layer. Interface designs (GUI and
otherwise) should be well-defined to fit both upper
and lower layers. For example, the application
interface provided for the DEVS Modeling application should remain intact despite adding animation
capabilities.
• Conceptual and structural modularity. Design must
have integrity by coalescing a set of related concepts
and structures (e.g., various GUI components) into
a super-component. For example, Graphical User
Interface, Client Services and Server Services must
be independent super-components.

4.1. Collaborative network system model abstract
architecture
CDNM is comprised of two main layers: SystemLayer and Application-Layer (Fig. 2). The system-layer
provides collaborative networking (network layer),
event multicasting (event multicast layer) and management (management layer). The application-layer provides a number of session-clients and session-server
implementations (session-implementation layer). In
the following sections, each layer and its components
are discussed in detail.
Before we continue with the remainder of this
section, we define the types of objects that will be
handled by various components of the collaborative client/server system model. We have four types
of objects: Communication Object, Request Object,
Indication Object and Event Object.
Communication object is an object (e.g., Object
from the Java lang library) with object name (Object
ID), a source (Source ID) destination (Destination ID)
and message (Data object) as depicted in Tables 1
and 2. The source and destination IDs are the component IDs (e.g., connection-client, connection-server,
or session-server IDs). It also contains an object (i.e.,
Data object). The data object contains “data” that is
transmitted among components residing in a single
machine or between multiple machines (e.g., client

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

93

Table 1
CDNM limited lexicon
Machine IP
Source/destination IP
Component ID
Source/destination ID
Object name
Operation name
Data object

IP address for a machine (globally unique)
Machine IP
Each component in the CDNM model has a globally unique ID
Component ID
Unique name for an object type
Unique name for an operation type
An object containing data to be manipulated by other objects

Table 2
Specification of CDNM objects
Object

Fields

Communication
Request/indication
Event

Object ID
Object ID
Event ID

Operation ID
Operation ID

and server). The data object can be an object containing information such as host name, component name,
etc.
Request object is a type of communication object
that is sent from one logical machine to another in
order to request execution of an operation. It differs
from communication object by virtue of having an
Operation ID. It supports various operation requests.
Each operation name signifies what kind of operation
is asked for (e.g., create a session). Indication object is
also a type of communication object that is sent from
one logical machine to another indicating successful
execution of a previously requested operation. The
main characteristic of the object is its conversion to
an event object as described below. Event object is an
object (e.g., Event object from Java util library) which
has event ID (i.e., Event type), operation ID, source ID
and data object. The event-multicast layer sends the
event object to upper layers (e.g., session-manager) in
the same logical machine once its destination is identified (e.g., by client-event multicaster).
An important aspect of the CDNM is the concept of
an “event”. An object (e.g., indication object) must be
transformed into an event object (e.g., indication event
object) before it can be used by the multicast layer.
The requirement for using an event object, as opposed
to an object, is due to the following: (1) destination ID
of the event object (indication event object) must be
verified by the multicast layer for a given source ID,
(2) event objects support asynchronous transmission

Source ID
Source ID
Source ID

Destination ID
Destination ID

Data object
Data object
Data object

of data (i.e., it avoids polling), (3) enables system level
management — i.e., an event object can be sent to
multiple components (e.g., session-managers).
A distinguishing characteristic of the CDNM is that
it uses communication objects at the network layer and
event objects at the Multicast layer and above. The
event object cannot be transmitted across the client and
server systems. That is, only vertical transmission of
object events is allowed between the management and
the application system layers. The communication objects, however, can be transmitted vertically within the
client-side or the server-side and horizontally (across
the network layer).
4.1.1. Collaborative event model
The Java language (e.g., Java 1.1.* AWT) provides
an event handling model for GUI components. The
CDNM, however, requires non-GUI-event handling as
well. We have devised an event model called collaborative event model (CEM) for distributed computing
environments such as CDNM. It supports non-GUI
events in addition to the already available Java GUI
events. The principal benefit of such an event model is
to transform the CDNM into a control-framework that
can be used seamlessly for distinct and possibly complementary, applications such as modeling and simulation. As we will discuss in the later sections, two types
of interactions can take place between an application
system and client/server system. All interactions originated from the client-system (or server-system) and

94

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

Fig. 3. Event/object passing between application-system and
client/server-systems.

sent to client (or server) application-system will be
in the form of events (see Fig. 3). The interactions
originated from the application-system and sent to the
client-system (server-system) will be in the form of
objects.
The CEM model consists of event objects, event listener interfaces and event multicasters. An event object (e.g., indication event object) is a typical (possibly non-serializable) object which has source ID and
data object. Here the data object can be any object
including event object. An event listener interface is
an interface containing a set of methods without their
implementations. The objects responsible for handling
the events will implement the specifics of operations
(methods) that are declared in the interfaces’ methods (see Section 4.1.3 for a particular realization of
CEM). In terms of the CDNM, the event multicaster
layer and session implementation layer have different
reasons to implement the event listeners. Components
of event multicaster layer (i.e., client event multicast
and server event multicaster) implement those listeners to produce events. But, components of application
layer (i.e., session-client and session-server implementation) implement those listeners to consume events.
4.1.2. Network layer
The network layer consists of three components:
Connection-Client, Connection-Server and SessionServer. A connection-client and connection-server pair
provides the primitive (lowest level) 3 communication
3

These communication primitives sit on top of the transport layer.

link between a client-system and a server-system. A
client-system refers to a number of session clients
and its client-system network — session-managers,
client-event multicaster and connection-client (see
Fig. 2). Similarly, a server system refers to a number of session-server implementations and the serversystem network — server-manager, session-monitor,
server-event multicaster, session-servers and connection-server. Obviously, a client-system and a
server-system can be executing on a single machine or
on two distinct machines (having their own machine
IP addresses).
A client-system and a server-system have one
connection-client and one connection-server, respectively. A server-system, however, can have a number
of session-servers where each session-server has a
distinct component ID. The session-server represents
a session by furnishing session-related services and
common data access/storage. These services are realized at the application layer.
Connection-client and connection-server. The
connection-client and connection-server have reciprocal communications with each other. A connectionclient communicates by sending request objects to
the connection-server. The possible requests are:
(1) connect to a session, (2) disconnect from a session, (3) create a new session, (4) remove an existing session, or (5) query a session. Similarly, a
connection-server performs network communication
by sending indication objects to the connection-client.
The connection-client is informed of successful completion of its request by the connection-server. The
connection-client or connection-server acts as gateway to send objects to their respective upper layers.
Table 3 below shows the list of operations recognized
by the connection-client and the connection-server.
All defined object IDs (i.e., Request object and Indication object) and their corresponding operation IDs
(e.g., DISCONNECT) for the given request and indication operation parameters are shown in Table 4. The
objects operated upon by the network layer are sent
to the Event Multicaster Layer which is discussed in
Section 4.1.3.
Session-server. A session-server supports: (1)
session-connectivity, (2) object manipulation, (3)
object multicast operations (see Tables 5–7). While
the operations below enable system-level activities (e.g., joining a session), they do not support

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

95

Table 3
Connection-client and connection-server operations
Communication mode

Operation

Source ID

Destination ID

Operation parameter

Forwarda

Send
Receive
Send
Receive

Connection-client
Connection-server
Connection-server
Connection-client

Connection-server
Connection-client
Connection-client
Connection-server

Request object
Indication object
Request object
Indication object

Callback
a Forward

mode is initiated by the client and callback mode is initiated by the server.

Table 4
Connection request and indication object operation IDs
Functionality

Object

Object ID

Operation ID

Description

Establishing a connection

Request object

REQUEST OBJ

CONNECT
DISCONNECT

Establish connection to a logical machine
Terminate connection to a logical machine

Indication object

INDICATE OBJ

CONNECT
DISCONNECT

Confirm connection establishment
Confirm connection termination

Request object

REQUEST OBJ

CREATE
REMOVE

Create a new session
Remove an existing session

Indication object

INDICATE OBJ

CREATE

Confirm creation of a new session

Manipulating a session

Table 5
Session request and indication object operation IDs
Communication mode

Operation

Source ID

Destination ID

Operation parameter

Forward

Send
Receive

Connection-client
Session-server

Session-server
Connection-client

Request object or Communication object
Indication object

Callback

Unicast

Session-server

Connection-client

Communication object

Multicast

Session-server

Connection-client

Communication object

Table 6
Connection-client and session-server operations
Object (operation parameter)

Object ID

Operation ID

Description

Request object

REQUEST OBJ

QUERY
JOIN
DROP

Query a session
Join a session
Drop from a session

Indication object

INDICATE OBJ

QUERY
JOIN
DROP

Confirm a query
Confirm joining a session
Confirm dropping from a session

Table 7
Data manipulation IDs of session request and indication object
Object (operation parameter)

Object ID

Operation ID

Description

Request object

REQUEST OBJ

ADD
DELETE
UPDATE

Add an object to a session
Delete an object from a session
Update an object of a session

Indication object

INDICATE OBJ

ADD
DELETE
UPDATE

Confirm adding an object
Confirm deleting an object
Confirm updating an object

96

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

application-oriented activities. For example, in an
online class registration system, students may want
to “add” or “drop” a class from their class listings.
Such operations are application-dependent. These
application-layer operations are similar to the system
level operations (e.g., JOIN) in that they are handled
the same way in terms of their transmissions between
the client-system and the server-system.
The session connectivity operations are Join and
Drop. The Join operation enables a client to subscribe
(log on) to an existing session. Likewise, a client may
request to unsubscribe (log off) from a previously
“joined” session using the Drop operation. The object
manipulation operations are Add, Remove and Update. They provide services for adding, removing, or
updating (replacing) an object of a session when the
session-client sends an add request, a remove request,
or an update request to the session-server, respectively.
The server-event multicast layer supports “indication
object” unicast and multicast operations. These operations can send an object to a session-client (unicast)
or a set of session-clients (multicast).
Multicast control provides two supplementary
services: selecting a multicast mode and setting an
interval for it. A session-server can multicast an
object to all registered session clients. Three transmission modes are supported: periodic, non-periodic
and intelligent. The session-server sends an object
to session-clients every fixed interval in periodic
mode. In the non-periodic mode, the session-server
sends an object to session-clients immediately after serving a client request. In the intelligent mode,
the session-server dynamically switches transmission
mode (periodic versus non-periodic) based on an
analysis of network traffic or system performance.
Registration and deregistration service are performed when a server-manager creates or removes
one or more sessions. The session-server provides
only “signatures” of the above services. The specifics
(implementations) of the definitions are provided at
application layer (i.e., Session-Server Implementation). A critical feature of the CDNM model is the
separation between the signatures and their implementations. This facilitates devising application-specific
needs independent of the system layer, thus supporting customized application services without any modifications to the underlying system layer architecture
(see Figs. 4 and 5).

Fig. 4. Objects passing between components of network layer.

4.1.3. Event multicast layer
As shown in Fig. 2, the Event Multicast Layer
consists of the Client-Event Multicaster and the
Server-Event Multicaster. These support object dissemination (transport) and control between the network and management layers as well as between
the network and session implementation layers. The
client-event (or server-event) multicaster receives an
object (either communication object or indication object) from the network layer, converts it to an event object which is then sent (multicast) to the management

Fig. 5. Object passing between network layer and other layers.

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

97

Table 8
Multicast Table specification of event multicasters
Multicaster

Fields

Client-event multicaster
Server-event multicaster

Server reference ID
Server ID

layer or session implementation layer. 4 The conversion is performed by (1) validating source and/or
destination IDs of the given object (e.g., indication
object) and (2) if necessary retrieving its data object
(refer to Table 2) to compose the event object. The
client/server-event multicasters rely on a set of Tables
to route event objects to their destinations (Table 8).
Component IDs maintained in the client-event
multicaster table can be of two types (i.e., sessionmanager’s ID or session-client’s ID (see Fig. 7)), while
the component IDs maintained by the server-event
multicaster can be either session-monitor ID,
server-manager ID, or session-server implementation
ID (see Fig. 9). We are using the term reference to
indicate that it holds a “reference” to the component’s
ID as opposed to holding the ID itself.
Client-event multicaster. A client-event multicaster sends an event to a session-manager and/or
session-client(s) based on the type of the object (see
Section 4.1.2) it receives from the network layer
(Fig. 6). We begin by describing the event object
dissemination from the client-event multicaster to
session-clients (see Fig. 7). When a connection-client
provides a communication object to the client-event
multicaster, it is converted to a communication event
object which is then sent to components (i.e., list
of component IDs of the client-event multicaster in
Table 8). This goes to the components that have
already been registered to receive it. In general, there
may exist multiple session-servers where each must
serve its own (possibly unique) clients. Therefore,
it is necessary for the client-event multicaster to
know which session-server (i.e., server reference of
the client-event multicaster ID in Table 8) has been
the originator of an event object to be sent to its
session-clients.
4

The event multicast layer is based on the Java’s listeners AWT
multicast model where an event is sent to multiple listener objects.
The client–event multicaster, for example, multicasts event objects
to a set of registered recipients — e.g., session clients.

List of component IDs
List of component IDs

Fig. 6. Client-event multicaster.

Fig. 7 shows an example of event multicasting
for the client-system. It assumes client 1 and client
2 are already registered (i.e., listed in the Multicast
Table) with the session-server 1, i.e., both clients
have a “session-client 1”. Assuming the client-event
multicaster receives a communication object from the
session-server 1, the client-event multicaster converts
the communication object into a communication event
object and multicasts it to session-client 1 for each
client 1 and client 2.
The client-event multicaster also receives indication objects. These objects are converted to indication
event objects which are then sent to each client’s
session-manager where each client may have multiple session-clients. Every client must, of course,
be registered with the event multicaster through a
session-manager. In terms of Table 8, therefore, the
Server reference ID is the connection-server.
Server-event multicaster. A server-event multicaster
sends an event to server-side management components
and/or session implementation components when the
network layer provides a request object or a commu-

98

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

Fig. 7. Example of communication and indication event object multicast.

Fig. 9. Example of communication and request event multicast in
server-system.
Fig. 8. Server-event multicaster.

nication object (Fig. 8). Before we proceed, we note
that the server event multicaster has two distinguishing features: firstly it can send an event simultaneously to both the session-server-implementation and
session-monitor and secondly an event is sent to only
one of many session-server implementations.
We start with a description of how communication event objects are disseminated (see Fig. 9).
The session-server sends a communication object to
the server-event multicaster where it is converted to
a communication event object. This object, thereafter, is sent to the session-monitor and a particular

session-server implementation according to the multicast table of the server-event multicaster of Table 8.
The list of component IDs is always restricted to two
entries — one for the session-server implementation
and another for the session-monitor. These two components are the recipients of the communication event
object. The server ID is the ID of the designated
session-server (one of many) that is responsible for
sending the communication object to the server-event
multicaster.
Fig. 9 depicts an event multicasting example where
SS-2 Impl is a session-server implementation of

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

session-server 2. In this scenario, when session-server
2 (server ID in Fig. 9) sends a communication object
to the server-event multicaster, the multicaster converts the object to its corresponding communication
event object before sending it to the session-server 2
implementation and the session-monitor according to
the multicast table of the session-server.
In the case of a request object, it is initially sent
by the connection-server to the server-event multicaster. The server-event multicaster, in turn, converts
it to a request event object and then sent to the
server-manager. (Note that only one session-manager
and one session-monitor exists for the server-system.)
In the next section, we describe the role of the
server-manager in dealing with the communication
event object.
4.1.4. Management layer
This layer consists of three components: a
client session-manager, a server-manager and a
session-monitor (see Fig. 10). The session-manager
and server-manager components provide mechanisms for creating and removing session-clients and
session-servers. Within the server-system management layer, the session-monitor is responsible for
monitoring the activities that take place between the
server-event multicaster and session-server implementation.
A client-system provides distributed session management facilities by furnishing a session-manager to
each client. The session-manager maintains a record

Fig. 10. Management layer interacting with other layers.

99

of all session-clients that a client is interacting with.
A server-system, unlike a client-system, provides
centralized session management mechanism using
its server-manager. The server-manager controls and
coordinates all system components that are related to
session activities.
Session-manager. Each session-client is associated
with a session-manager. The session-manager keeps
track of all session-clients that a client owns. Its responsibilities are to support joining and dropping from
an existing session-server as well as creating and removing session-clients in cooperation with components from the event-multicaster and network layers.
By providing session management facilities to
each client, client-system provides robust and
client-customizable session management system.
Since each client has a separate session-manager,
anomalies associated with one session-manager do not
affect other session-managers. Furthermore, with a
dedicated session-manager, each client can efficiently
maintain a large number of session-clients.
For a client to join a session-server, it is the responsibility of the session-manager to provide a list
of available (existing) session-servers for a given
server-system. The client, therefore, will be able
to join one or more session-servers simultaneously,
thus creating one or more session-clients. Of course,
since multiple server-systems can be available, the
session-manager can provide a list of session-servers
for each server-system. Similarly, a client can drop
from any number of existing session-servers simultaneously.
For the joint operation, the session-manager receives an indication event object from the client-event
multicaster as a response to an inquiry (request
object) sent to the server-manager. The response
from the server-manager contains a list of existing
session-servers. Thereafter, a session-client is created
for the client by the session-manager which is distinct
from the one that is created on the server-system.
The indication event object entails knowledge about
the session-server such as server reference ID. Likewise, session-manager handles a client’s request to
be dropped from the list of session-clients that are
associated with the session-server.
The session-manager is responsible for creating
or removing session-clients. A specific session-client
is created for each session-server belonging to a

100

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

given server-system. Hence, if there exist three
session-servers, a client can create three session-clients.
A client can request for creation of a session-client on
the client-system. However, such a request can only
be granted after its corresponding session-server has
been created on the server-system. The creation of a
session-client is initiated by a communication object
sent by a client (i.e., application component) to the
session-manager. Once the session-manager receives
a communication event object from the client-event
multicaster, it creates the session-client where the
aforementioned communication event object is sent
by the session-server.
The removal of a session-client is similar to the
session-client creation with the exception that the
session-server is removed and consequently any
session-client is connected to it.
Server-manager/session-monitor. A server-manager
is a server-side session management system based on
a centralized strategy. It manages a connection-server
and multiple session-servers in the network layer. It
is responsible for registering, deregistering, creating
and removing session-server implementations using
the event multicaster layer and the network layer of
the server-system.
The creation or removal of session-servers and
their corresponding session-server implementations
is the responsibility of the server-manager. The
server-manager determines, for example, whether a
session-server already exists or not before creating
a new session-server. Likewise, it is responsible for
appropriate registering and deregistering of a given
session-server. The server-manager’s responsibility
includes sending event objects to the session-server
implementations as well as receiving objects from the
session-server implementation. The server-manager
does not have any interactions with the client’s application system directly. All such interactions take
place via the network layer.
A session-monitor’s role is to monitor various
communications that take place in the server-system.
It receives a copy of every message that is sent to
the session-server implementations. The messages
received by the session-monitor, however, are not restricted to those that are sent to the server-manager.
While the server-manager receives only communication event objects, the session-monitor also receives
request event objects.

Generally, the session-monitor relies on more information (inputs), as compared to the server-manager, to
carry out its operations (e.g., keeping track of the number of messages originated by each client). It should
be noted that the session-monitor must not alter the
dynamics of the systems that it “observes”. However,
the system’s operations may be affected adversely (for
example, reduced performance) if the session-monitor
requires substantial processing cycles. Another consideration in this model is that the session-monitor
is not responsible for monitoring session-clients. The
proposed model, however, does not preclude the extension of the monitor concept to the client-system.
4.1.5. Session implementation layer
Similar to the other layers in the CDNM, this
layer is comprised of components on the client and
server sides. The components of this layer provide
the gateway to specific application components.
The chief role of these components is to provide
application independent plug-ins for CDNM. The
session-implementation layer provides session-clients
for the client-systems and session-server implementations for the server-systems. Each session-client and
session-server implementation are deployed in the
collaborative environment by registering them with
the underlying CDNM.
Given the basic types of objects discussed in Section 4.1.2, the session-server implementation component sends communication objects to and receives
communication-event objects from the server-system.
The session-server implementation interacts with
the server-management layer, server-event multicaster and session-servers. The same principle holds
for the session-client. Specifically, the session-client
receives communication-event objects from the
session-manager and the client-event multicaster. The
session-client sends communication objects to the
connection-client and session-manager.
The obvious distinguishing feature between the
session-client and session-server implementation
is that the former is based on a one-to-one relationship and the latter on a many-to-one relationship. The one-to-one relationship involves a
session-client implementation (application) interaction with a session-manager. The many-to-one relationship involves many session-server implementations interacting with a single server-manager. That

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

is, while a client may have many session-clients,
each can only interact with a unique session-server
implementation, thus conforming to a one-to-one
relationship even though the “client” per se can be
interacting with several session-server implementations. Furthermore, on the server-side, the relationship between the session-server implementation and
the session-monitor is the same as the just defined
many-to-one relationship. Given the above interactions (relationships), the session-server implementation may interact with both server-manager and
session-monitor while adhering to the above relationship types.
5. Collaborative distributed network system
CDNS is an implementation of the CDNM in
Java [8]. The realization of CDNS makes use of
Object Serialization, Remote Method Invocation and
the SWING Java APIs. Object Serialization is the
mechanism of converting an object to byte stream
(marshaling) and byte stream to an object (unmarshaling) for network access or transmission [9]. Any
object can be defined simply as a serializable object.

101

All objects of CDNS requiring transmission between
client-system and server-system (i.e., communication
object, request object and indication object) are serializable. The only objects that are not serializable are
the “event objects”. This is due to the fact that there
is no need for transmitting event objects from one
machine to another across a network or even between
any two JVMs running on the same physical machine.
Upon launching a client application, a sessionmanager window (see Fig. 11) consisting of two
panels (Session-Manager and System Information)
is displayed to the user. Using the Enter button, the
session-manager can connect its client to one or more
server application sessions on one or more hosts (e.g.,
ais1 and ais4). At this point, for example, client can
create a new session or alternatively join an existing
session. Similarly, client may delete or leave a session
as appropriate. Each client’s selected server application session has its own associated panel through
which client commands are sent to the server application for processing. The server may in turn inform
other participants (clients) of the client’s activities.
The System Information displays system-related
information such as Host Name.

Fig. 11. Session-manager window.

102

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

6. Collaborative DEVS modeling
Collaborative DEVS modeler (CDM) [10] is an environment enabling geographically dispersed modelers to develop hierarchical models within the DEVS
framework [11]. The basic architecture of CDM is
depicted in Fig. 12. The DEVS Modeler and CDNS
components collectively contain either a Knowledge
Worker or a Knowledge Manager. The Knowledge
Worker represents the client and Knowledge Manager
the server.
In DEVS, models have input/output ports through
which they can send messages to one another. CDM’s
modeling syntax is based on Dispersed Clients that can
collaborate with each other to develop a model where
each client performs modeling tasks locally while a
central server maintains the overall model. Clients can

create, delete, join, or leave a modeling session (see
Fig. 11). Each modeling session has associated with it
a model with a unique name assigned to a given server.
While the CDNS design involves many kinds of
communications (i.e., client-side, server-side and between client and server sides) taking place among its
layers, it offers simple “plug-ins” for an application
to use its services. In the case of the CDM application, its server and client pieces “Session-Client”
and “Session-Server Implementation” components
(see Fig. 2) are simply extended to support DEVS
modeling constructs. For the client application,
only methods send(Request req), get(), userProtocolIndicated(commEvent ev) and updateIndicated(commEvent ev) are needed. These methods handle communication and event objects as well as
request/indication objects. Similarly, the server appli-

Fig. 12. CDM system architecture.

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

cation methods that are counterparts to those of the
client application are updateRequested(commEvent
ev), userProtocolRequested(commEvent ev), send(Request res) and userProtocolBroadcast(Object, ob).
For example, the method updateRequested responds
to an “update request” sent by a client. Session-related
methods such as a client’s request to resign an application session (using the Delete button in Fig. 11)
can be simply incorporated in the server application.

7. Related research
There exist varieties of middleware that can provide
client/server applications. These can vary extensively
depending on the range of capabilities they support.
For example, powerful middleware environments such
as common object broker architecture (CORBA) and
distributed COM (DCOM) have emerged during this
decade [12,13]. One of the principal objectives of these
environments is to support integration of legacy systems with their contemporary counterparts as opposed
to being used for modern systems. CORBA compliant
middleware such as VisiBroker [3] enable a variety
of computer languages (C, C++, Fortran, ADA, etc.)
and computing platforms to be integrated together
to form an “enterprise system”. These middleware,
however, provide a plethora of capabilities and services whereby requiring greater operational resources
compared with the CDNS. The exact interpretation of
computational demands, of course, depends on many
elements such as the languages (C++ versus Java)
and protocol communications (e.g., sockets) being
used.
Aside from these general middleware environments,
there also exist computer supported cooperative work
(CSCW) frameworks implemented in Java and Tcl/Tk
[14]. These environments have their basis in groupware where the emphasis has been generally on applications such as Voting Tool, Audio Chat, GIS Viewer,
etc. An example of such collaborative environments
is Habanero [15], which supports creation of virtual
communities where sessions can be recorded, persistent, or access controlled. From an operational point
of view, Habanero and CDNS have many essential
functions in common. However, at the time of writing this paper, we have not been able to obtain the
architectural design of the Habanero to compare it to

103

that of the CDNM (Fig. 2). In the next paragraph, we
compare CDNS with two other environments that are
closely related to it.
The Java collaborative environment (JCE) architecture consists of a session-server and a pair of event
controller and session control manager for each client
[16]. The collaboration is based on producer/consumer
model where the session-server acts as intermediary
to transmit an event to one or more event consumers.
Session server is also responsible for facilitating joining or leaving a session as well as control. The client
produces customized AWT events that are consumed
by other clients via session server.
The TANGO’s architecture is more complex
[17,18]. It is similar to JCE and CDNS in that it
uses a central server. Its other components are local
daemons, client applications, Java applets and control
mechanism. The local daemons provide low level collaborative functionality such as launching client applications, message passing among applications running
on a client machine and communication among user
application, Java applets and central server. Table 9
illustrates pros and cons of JCE, TANGO and CDNS.
Of particular importance is that CDNS supports distinct types of communications, layering of activities
(e.g., Management Layer) and an abstract layer upon
which applications can be developed with minimal intrusion into lower layers (e.g., Network Layer). That
is, CDNS provides generic and well-defined layered
architecture which can be extended in a straightforward manner to support newer services. These characteristics of the CDNM/CDNS support the Application Layer Support and Extensibility ratings listed in
Table 9.

Table 9
Comparison of CDNM with TANGO and JCE
CDNM/CDNS TANGO
√
Generic model
√
Layered design
√
Web-support
√
Multimedia support
√
Database support
Network protocol RMI
Socket, HTTP
Application layer
Best
Very good
support
Extensibility
Best
Very good

JCE
√
√
√
RMI, Socket
Good
Good

104

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105

8. Conclusions and future research
The central goal of this research was to devise a
Networking Environment that could support collaborative modeling. From the software engineering view,
we believe the CDNM benefits its own developers as
well as domain-specific application developers. We informally discussed the CDNM and its primitive and
higher-level services in order to shed light on issues
that need to be taken into account in devising collaborative modeling environments such as CDM. The
CDNM model provided insight into services that enabled architecting the Collaborative DEVS Modeler.
In particular, we discussed three separate layers (Network, Event-Multicaster and Management layers (see
Fig. 2)). The separation of services into these layers
aids the developers of collaborative modeling tool to
be aware of issues in a systematic fashion. The architects, designers and developers would have a model to
put to use, compare with their own, or perhaps use as
a basis to devise their own. While the CDNM architecture is a step toward, it also illustrates the need for
features that may prove to be essential in giving modelers greater support. For example, CDNM does not
allow multiple distinct modeling sessions to collaborate with one another at a higher level. Imagine two
teams work independently for some time to develop
two models. At some later time, these models need to
be combined. Neither CDNM nor CDM is able to support this higher level model construction/synthesis.
Obviously, tools such as Collaborative DEVS Modeler not only are expected to support basic modeling
capabilities, they are increasingly expected to support
basic simulation needs as well as a variety of multimedia needs that can greatly enrich the simulationist’s
power. Given the basic underlying framework adopted
by CDNM/CDNS, network capabilities (e.g., model
repository and intelligent synthesis of models) as well
as loosely dependent network features (e.g., graphics
and animation) can be supported. The proposed network environment, however, needs to be extended to
provide guidelines as to how domain-specific capabilities may be incorporated into it without being forced
to follow the CDNS mold. That is, other applications
may need to characterize their own model of a session.
In such a situation, CDNS needs to provide lower-level
plug-ins instead of providing the Session-Client and
Session-Server Implementation models. Furthermore,

the proposed architecture put forth an explicit separation between the application and the network layers (two-tier architecture). For applications where use
of database is paramount to the application, three-tier
architecture is most desirable.
Aside from the above unrealized feature, the proposed network environment does not provide time
management services, nor can it effectively support
large-scale knowledge repository. In applications such
as E-Commerce, the networking capabilities must
support directly time-critical demands at higher volumes. At the present time, collaborative modeling
(i.e., CDM) does not require real-time handling of
modeler’s commands in milliseconds or finer time
scale. In the current CDNS implementation, data and
time (in a very limited fashion) are manipulated at the
application layer. Therefore, it might be desirable (and
even necessary) for the CDNS to also manage time
among dispersed users where potential “users” can be
“simulation models”. Given such users, a time-based
CDNS would need to be devised to support distributed simulation. As of this writing, however, there
already exists middleware (e.g., High-Level Architecture [19]) that can support distributed simulation
(e.g., DEVS/HLA [20]). Another avenue for future
research inquiry is with respect to comprehensive
comparison of CDNS with other existing middleware
environments, especially HLA and CORBA, which
would entail performance measurements and security
assessment.

References
[1] B.P. Zeigler, H.S. Sarjoughian, S. Vahie, An architecture
for collaborative modeling and simulation, 11th European
Simulation Multiconference, Istanbul, Turkey, 1997.
[2] J.F. Nunamaker, Electronic meetings to support group work.
communications of the ACM 34 (7) (1991) 40–61.
[3] VisiBroker, VisiBroker Homepage, 1999, www.inprise.com/
visibroker.
[4] J. Grudin, Computer-supported cooperative work: history and
focus, . IEEE Comput. 27 (5) (1994) 19–26.
[5] R. Orfali, D. Harkey, J. Edwards, The Essential Client/Server
Survival Guide, Wiley, New York, 1997.
[6] R. Orfali et al., The Essential Distributed Objects Survival
Guide, Wiley, New York, 1995.
[7] H.S. Sarjoughian, J. Nutaro, B.P. Zeigler, Collaborative
DEVS Modeler, in: International Conference on Web-Based
Modeling and Simulation, San Francisco, SCS, 1999.

H.S. Sarjoughian et al. / Future Generation Computer Systems 17 (2000) 89–105
[8] S. Park, Collaborative Distributed Network System
Architecture: Design and Implementation, Electrical and
Computer Engineering Department, University of Arizona,
Tucson, AZ, USA, 1998.
[9] B. Eckel, Thinking in Java, Prentice-Hall, Englewood Cliffs,
NJ, 1998.
[10] AIS-CDM, Collaborative DEVS Modeler, AI & Simulation
Research Group, Tuscon, AZ, USA, 1999.
[11] B.P. Zeigler, T.G. Kim, H. Praehofer, Theory of Modeling
and Simulation, 2nd ed., Academic Press, New York, 1999.
[12] OMG, CORBA/IIOP 2.2 Specification, 1998, http://www.
omg.org/corba/corbaiiop.html.
[13] Microsoft, DCOM, 1998, http://www.microsoft.com/com/
dcom.asp#Technical.
[14] S. Usability, Groupware, 1999, http://www.UsabilityFirst.
com/groupware/index.html.
[15] NCSA, Habanero, 1999, http://havefun.ncsa.uiuc.edu/
habanero/.
[16] H. Abdel-Wahab et al., Using Java for multimedia
collaborative applications in PROMS, Third International
Workshop on Protocols for Multimedia Systems, 1996.
[17] L. Beca et al., TANGO — A Collaborative Environment for the World Wide Web, 1998, http://trurl.npac.syr.
edu/tango/Documentation/Papers/TANGO Interactive White
Paper/tango interactive white paper.html.
[18] NPAC, TANGOsim: A Java Based Collaborative System with
a Built-in Event Simulator, Syracuse University, 1998, http://
www.npac.syr.edu/users/gcf/PPTTango2096/index.html.
[19] DMSO, Runtime Infrastructure, 1997, http://hla.dmso.mil/
hla/rti.
[20] B.P. Zeigler et al., The DEVS/HLA Distributed Simulation
Environment and Its Support for Predictive Filtering, ECE,
The University of Arizona, 1998.

105

Hessam S. Sarjoughian is an Assistant
Research Professor of Electrical and Computer Engineering at the University of Arizona. His current research interests are
in the theory, methodology and practice
of Distributed/Collaborative Modeling and
Simulation.

Bernard P. Zeigler is a Professor of
Electrical and Computer Engineering at
the University of Arizona, Tucson. He
has written several foundational books
on modeling and simulation theory and
methodology. He is currently leading
a DARPA sponsored project on DEVS
framework for HLA and predictive contracts. He is a Fellow of the IEEE.

Sunwoo Park received his BS degree in
information and telecommunication engineering from the Chonbuk National University, Korea, in 1994 and MS in computer engineering from the University of
Arizona in 1988. He is currently a Ph.D.
student in the Department of Electrical and
Computer Engineering at the University
of Arizona. His primary research interests
are high performance discrete event driven
simulation and collaborative/agent-based/mobile computing.

Proceedings of the 2003 Winter Simulation Conference
S. Chick, P. J. Sánchez, D. Ferrin, and D. J. Morrice, eds.

SEMICONDUCTOR SUPPLY NETWORK SIMULATION
Gary W. Godding

Hessam S. Sarjoughian

Component Automation Systems
Intel Corporation
5000 W. Chandler Blvd – MS CH3-68
Chandler, AZ 85226, U.S.A.

Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Dept.
Arizona State University
Tempe, AZ 85287-5406, U.S.A.
Karl G. Kempf

Decision Technologies
Intel Corporation
5000 W. Chandler Blvd – MS CH3-111
Chandler, AZ 85226, U.S.A.

maximize efficiency. The resulting decision flows describe
when and how much of which materials to release into
manufacturing facilities and transportation links as well as
how much inventory to hold in warehouses.
The goal is to provide customer service to maximize
revenue (the right quantity of the right product in the right
place at the right time) while minimizing costs (materials,
production, storage, transport, obsolesce) to maximize
profits now and in the future. This requires a number of
difficult tradeoffs. For example, to maximize customer
service, high volumes of finished product might be stored
close to the customer. But for products with short effective
lives, costs are minimized by holding raw materials at the
beginning of manufacturing lines until firm orders are
placed. Decision policies are required to manage such
tradeoffs over time, and the quality measure of policies is
supply network profitability.
We have developed a software architecture to model
and simulate material, data, and decision flows and the interactions between them. Using this architecture a wide variety
of experiments can be implemented to explore the behavior
of supply networks that we demonstrate with a typical problem from semiconductor manufacturing. This test problem
shows our approach to the granularity of the objects to be
modeled and the granularity of time over which to simulate.
We address the division of functionality between the physical module and the decision module as well as the data passing requirements including synchronization. We explore the
ease of modifying the physical entities (in both parameters
and topology) and the decision policy.
Although this initial demonstration covers multiple
time periods, during a simulation the network topology and

ABSTRACT
More efficient and effective control of supply networks is
conservatively worth billions of dollars to the national and
world economy. Developing improved control requires
simulation of physical flows of materials involved and decision policies governing these flows. This paper describes
our initial work on modeling each of these flows as well as
simulating their integration through the synchronized interchange of data. We show the level of abstraction that is appropriate, formulate and test a representative model, and
describe our findings and conclusions.
1

PROBLEM DESCRIPTION

Improving the operational efficiency of supply networks
for physical goods is one way to boost the national and
global economies. Understanding the control physics of
this kind of network is key to providing continuous improvement. Developing such an understanding requires extensive experimentation to formulate and validate control
policies. Since this type of network generates enormous
wealth, direct experimentation involves untenable financial
risk. Some form of modeling and simulation is required.
Simulating supply networks requires the modeling of
different but interrelated flows. Physical flows represent
the goods being produced, stored, and shipped including
raw materials, work in progress, and finished goods. Data
flow represents the past, present, and forecast future state
of the physical flows as well as the demands from the marketplace. Such data is used by policies controlling the balance between variable demand and variable supply to

1593

Godding, Sarjoughian, and Kempf
products (to mention but a few) are fixed. We believe
however, that the architecture will support the dynamics of
actual semiconductor supply networks where new factories
come on line and new products are introduced (and other
dynamic changes) as a simulation unfolds over time. Furthermore, we are preparing to test the architecture over
multiple dimensions of scalability. The first is size scalability in terms of number of factories, products, customers, warehouses, transport links, etc., included in the material flow. The second is complexity scalability as the
relationships between product age and product pricing, or
between multiple customers for the same scarce product, or
other such complexities are included in the decision flow.

1994) to deal with unpredictable conditions often present
in supply network and logistics. Indeed, agent-based modeling is widely accepted to be necessary where local and
cooperative planning is required. However, given the level
of complexity required for this kind of agent-based modeling, it should not be used if simpler modeling techniques
can be used. For example, other researchers (e.g., Swaminathan, Smith, and Sadeh 1998) apply simpler types of
agents, called software components, to model static and
dynamic aspects of supply network entities such as transportation agents.

2

There has been much recent research using Systems Dynamics (SD) for supply network modeling (Angerhofer and
Angelides 2000). The use of SD, formally known as industrial dynamics, was first suggested in 1958 (Forrester
1958). The SD approach for analyzing behavior of complex systems includes (1) modeling the system as feedback
loops and delays, (2) turning the model into a set of quantitative equations, and (3) using simulation to observe the
behavior of the system. Users follow this concept of modeling and simulating system behavior to gain insights on
control policies. However, the modeling and numerical
simulation methods provided by current SD environments
do not provide the granularity needed to model the complex stochastic material flows and associated control algorithms for a semiconductor supply network without significant extension.

2.3 System Dynamics

RELATED WORK

A number of methods have been explored to model and
simulate supply networks. These include principally discrete event simulation, agent-based simulation, and systems dynamics simulation. Other less well explored approaches include fuzzy sets and spreadsheet analysis.
2.1 Discrete Event Simulation
For modeling and simulation of the physical flows in a
supply network, discrete event simulation (DES) might be
considered the most obvious choice since it has been used
extensively in the study of material flow in manufacturing
systems at the tool and factory level (Law and Kelton
1991) . It has been shown that using the appropriate abstraction, DES can be used for the factory components of a
supply network (Godding and Kempf 2001; Kempf,
Knutson, Fowler, Armbruster, Babu, Duarte 2001). However, DES does not lend itself as easily to decision flows.
While there are DES systems that include the simple kinds
of dispatching rules that can be used to control a set of
tools, more complex policies generally require user exits
and coding in some basic programming language. Rapid
experimentation with control policies across complex supply networks is not within the grasp of DES.

3

APPROACH

In developing a semiconductor supply network simulation
model, it is useful to distinguish between material processing (low-level) and decision-making (high-level) dynamics. Abstracting a complex system into low-level and highlevel layers supports using distinct modeling approaches as
appropriate (Sarjoughian, Zeigler, Hall 2001). For example, characterization of physical flow of materials independently from the modeling of decision-making offers a
basis for synthesizing a supply network using different
modeling methods (e.g., discrete event for materials and
mathematical optimization for decisions).
To illustrate the significance of this separation, consider Figure 1 where the decision and physical processing
layers are defined. The decision layer is responsible for
making higher-level decisions (controls) based on the
lower-level behavior (data) observed from the physical
processing layer. At the decision layer, multiple components can collaborate to generate control commands for the
physical processing layer. The physical processing layer
can represent processes, inventory, and shipping components as well as their interactions. In this layer, the products generated in the process are stored in the inventory be-

2.2 Agent-Based Modeling
Agent-based modeling is intended to support a variety of
behaviors that depend on autonomy, mobility, and rationality where handling independent and cooperative decisionmaking is necessary. However, unlike some other modeling approaches (e.g., discrete-event), to date there does not
exist any universally accepted theory for modeling agent
dynamics. Agent-based modeling, nonetheless, plays a
principal role in describing behavior that does not lend itself to a priori reasoning. In particular, in domains such as
enterprise engineering, decision-making is inherently nonmonotonic. Agent-based modeling can model important
autonomy and mobility traits (e.g., Muller and Pischel

1594

Godding, Sarjoughian, and Kempf
Simulation measurement and control (Decision Layer)

Decision Layer

data

Determines how much material to start, how much to release
from each inventory holding point, and where to ship.

Decision Module k

local control

external control

external data

Physical Processing Layer

control
variables

local control

Process

product

Inventory

local control

Decision Module

Inventory

Process
Process

Shipping

product

state
variables

Shipping

Supply Network Simulation (Physical Processing Layer)
local control

Simulates the physical flow of material through the supply
network. Reports how much inventory is in each supply
network entity at the end of the day, and when customer orders
were actually filled.

external control
local control

product/data

Figure 1: Decision and Physical Processing Layers
and their Interactions.

Figure 2: Two Layer Abstract View of Supply Network

fore being shipped. Components from these two layers can
interact with one another using external control and data as
shown in Figure 1.
The interactions between the decision and physical
processing layers are governed by the former sending external control commands to the latter. Examples of the external control for the physical processing line are “how
much to release into the line during some time interval”,
“how to configure material to demand,” and “which warehouse the material should be shipped to”. Correspondingly
the decision layer receives the state of material processing
for determining external control commands. Examples of
data sent by the lower-level (and consumed by the higherlevel) are “the amount of semi-finished goods available for
producing some desired finished goods” and “the amount
of product shipped to customer and Geo-warehouse”. The
separation of material processes from decision-making is
based on distinguishing data, product, local and external
controls from one another. The external control and data
between these two layers allow isolating tactical (local
control) and strategic decision-making (external control)
from one another. Figure 2 depicts an abstract view of the
physical processing and decision layers, their interactions,
and their collective role in fulfilling end-to-end demands of
a supply network.
To further illustrate the concept of separation of concerns as shown in Figures 1 and 2, consider a segment of a
real-world distribution network which has an assembly test
factory and a logistics network. The decision to separate
decision-making (inventory holding policy) from material
processing plays a vital role in model development of a
supply network. For example, the decision-making algorithms tend to consider cost/revenue tradeoffs by controlling how much material to build and where to store it while
the physical model considers physical characteristics and

constraints of how material flows through the network (e.g.
capacity, delay, and yield).
The physical processing layer of a semiconductor supply network has inventory holdings, processes, shipping,
and customer entities (see Figure 3). In this setting, material arriving is stored at the die inventory and processed
into different types of products according to physical characteristics of the die and some types of local and external
decision policy. Each of the die flows through the assembly test line, is assembled into packages, split into categories based on test results, and stored into unfinished inventory. When the material is released from the unfinished
inventory point, it is configured to a specific product which
is ready for shipment to customers. The processing steps
in the foregoing description can be categorized into material processing (e.g., categorizing die based on test results)
Die Supply

inventory

output data is a function of input data
and external control (strategic decisions)

process
process

output data is a function of input
data and local control (tactical decisions)

shipping

consumes, generates and tracks orders

customer

output data is a function of input data

Assembly Test Factory

Release

Die Inventory

Assembly
Assembly Test
Test

Release

Semi-Finished
Inventory

Logistics Network

customer
Shipping

Finish
Finish Line
Line

Shipping

Geographic Warehouse
Finished Product

Release

Shipping

Shipping

Shipping

Finished Goods Warehouse

Finished Goods
Inventory

Release

Release

Figure 3: Model for Physical Processing Layer of a
Semiconductor Supply Network

1595

Godding, Sarjoughian, and Kempf
and decision-making (e.g., release a certain number of die
to a given customer).

jects were defined. Finally, the message flow and synchronization were defined and modeled.

3.1 Modeling and Simulation Methodology

4.1 Physical Model Decomposition

As stated earlier, having the ability to use different modeling techniques for the decision and physical manufacturing
layer offers important advantages compared with monolithic methodologies where one modeling approach is used
for describing all types of dynamics (e.g., high-level decision-making and low-level physical processes). For example, while discrete event modeling is well suited for manufacturing processes, it may be necessary and advantageous
to use other techniques such as logic-based reasoning to
model intelligent decision-making. For modeling the
physical layer dynamics, we use DEVSJAVA software environment (ACIMS 2003) and its accompanying DEVS
(Discrete Event System Specification) modeling and simulation methodology (Zeigler, Praehofer, and Kim 2000)
(see Figure 4). For simplicity, in this study high-level decision algorithms are wrapped inside atomic DEVS model
components. This choice allows executing our existing
model using alternative simulation engines (e.g., parallel
execution) in a straightforward manner.

The DEVSJAVA environment supports inheritance for
creating atomic models and this feature has been used extensively in creating our model. The class hierarchy is
shown in Figure 5. The Viewable Atomic model is a
DEVSJAVA base model from which all the other atomic
models are derived. The Supply Network Entity model
provides common methods for synchronization, inventory
reporting, capacity management, and delay.
Viewable
Viewable
Atomic
Atomic

Customer
Customer

End
End Of
Of
Factory

Processing
Manufacturing
Line
Line

Assembly
Assembly
Test
Test

The physical model has been decomposed into four major categories of entities including processing lines, inventory
holdings, shipping, and customers. Specializations have been
created to support additional functionality. Descriptions of
each follow, and the detailed model is shown in Figure 6.

Physical Manufacturing Model Components

4.1.1 Inventory Holding

Domain Neutral Model Components

The inventory holding is modeled to behave like a store or
a warehouse. When material arrives, it is added to the current pool of inventory. Material will not leave until an explicit release signal is received. The inventory holding is
the only entity in the physical model that receives commands. Control of the physical network is accomplished
by controlling how much, what, and where to release material from the inventory holding points. Inventory holding
points can have capacity and delay.
The basic inventory holding has one input and one output for material flow. There have been two specializations
created; the two output inventory, and the end-of-factory inventory. The two output inventory has different paths through
which material can leave. The end-of-factory model is a specialization of the two output model. End-of-factory model is
used to simulate the back of a factory where there is limited
storage space. If any material remains in an end-of-factory
model at the end of a time period, the material is automatically released to the default output path.

Simulation Engine
Distributed
Processing

DEVSJAVA
Figure 4: Supply Network Modeling and Simulation Environment
4

Two
Two Output
Output
Inventory
Inventory

Figure 5: Object Hierarchy – Supply Network Components and Customer.

Decision Control Model Components

Sequential
Processing

Inventory
Inventory
Holding
Holding

Shipping
Shipping

Semiconductor Supply Network Model

Parallel
Processing

Supply
Supply
Network
Network
Entity
Entity

MODEL DEVELOPMENT

In DEVSJAVA, a model is either atomic or coupled. The
former specifies inputs, outputs, states, state and output
transitions, and timing function. The latter specifies hierarchical composite models where its components can be
atomic or coupled models. Each coupled model can have
inputs, outputs, and couplings among its children.
The supply network topology shown in Figure 3 has
been modeled using the DEVSJAVA framework. First, the
physical and decision layers were identified and decomposed into DEVSJAVA models. Second, the message ob-

1596

Godding, Sarjoughian, and Kempf

Experimentation Set-Up
Lot Generator

Command
Generator

Lot Release
Controller

Release
Inventory
Command

Lot

Assembly
Test Line

Semi
Finished
Inv

Finish
Line

Decrease
Demand

Order Filled

End of
Factory

Transducer

Finished
Good

Ship

Order
Changed

Geo
WH

Ship

Ship

Customer

ATFactory
Ship
Manufacturing Line
Inventory Point
Shipping Link
Customer

Product flow
Status data from model
Commands into model

Ship

Supply Network Model
Figure 6: Detailed Model of the Supply Network
ders. If customer cancels more than two orders, average
orders from that customer could decrease. This is intended
to simulate the behavior of customers switching to an alternate supplier when service level falls.

4.1.2 Shipping
Material is delayed to simulate the transport time. Shipping entities have capacity and delay. Shipping by different methods (air/land/sea) could be modeled and would
have different cost/time tradeoffs to explore.

4.1.5 ATFactory

4.1.3 Processing Line

The ATFactory (AssemblyTest factory) is a coupled model
that consists of an assembly test line, a semi finished inventory holding, a finish line, and an end-of-factory inventory holding.

Processing lines model manufacturing links or assembly operations. Processing lines can change the material. These
lines have capacity, delay, and yield. A simple example is
the finish line used in our supply network model. A specialization of the processing line has been created to simulate the
assembly test operation. The assembly test operation splits
one input product into three output products based on a stochastic distribution. This distribution simulates the behavior
of a semiconductor test operation where product is divided
based on physical characteristics of the die.

4.2 Decision Model Decomposition
4.2.1 Decision Layer Atomic Models
The decision model is made up of four different atomic
models including a lot release controller, a lot generator, a
command generator, and a transducer (see Figure 6). The
lot release controller implements the heuristic described in
the next section. The lot generator inputs new material
into the simulation, the command generator inputs commands. The transducer collects data from the simulation
for off-line analysis.

4.1.4 Customer
The customer can consume orders during each time period.
Customers may change orders within a certain time window from the order due date and they may cancel late or-

1597

Godding, Sarjoughian, and Kempf
4.2.2 Decision Algorithm

4.4 Data Flow and Synchronization

A heuristic has been implemented for the control algorithm. The heuristic attempts to keep inventory levels at a
target level, while meeting customer orders on time. The
heuristic can only consider a single shipping path, i.e.
product is shipped to customer via the geo warehouse, geo
warehouse is replenished from finished goods warehouse,
and finished goods is replenished from the ATFactory.
The heuristic calculates the difference between expected pipeline inventory and the actual inventory. If the
difference is positive, it will release that much for the day.
The heuristic is used to calculate both how many lots to
start and how much material to release from the warehouses. The required inputs to the heuristic are: total inventory currently in the supply network pipeline, the orders
that have been filled, the forecast demand over the time period of the network delay, the average delay of the pipeline, and the inventory targets. Additional details of the
heuristic can be found in (Armbruster, Chidambaram,
Godding, and Kempf 2001). The inventory and order data
is obtained from the physical network during the runtime.
The other data is input as parameters to the model.

Three types of flows have been identified in the model.
There is material flow, control flow, and information flow
(see Figure 6). Material flow represents the product being
processed and consumed by the customers. Information
flow is generated by the physical model and represents the
externally known state of the physical network at any
given time period. The control flow is generated by the
decision model and sent to the physical model to control
material flow. Material flow is controlled by how inventory is released.
4.4.1 Material Flow in Physical Model
Material flow starts from the lot generator in the decision
layer. When the material is released into the assembly test
line, it is split into three different products using a stochastic distribution. Processing time for assembly test is approximately 2 weeks, that is also drawn from a distribution. After material leaves assembly test, it flows into the
semi finished inventory. Material has to be released from
this inventory into the finish line. The release command
sent to semi finished inventory contains an additional field
specifying what product the material should be configured
to. After material leaves the finished line, it flows into the
end of factory inventory point. It can be shipped either to
the customer or to finished goods from here. If it is
shipped to a customer, an order ID has to be supplied to
specify which customer order is being filled. After finished goods, materials can be shipped to either the customer or the geo warehouse.
The physical layer sends the following messages to the
decision layer:
• Inventory level at the end of the day.
• Customer orders filled at the end of the day.
• Customer orders that have been changed.
• Customer demand decreased.

4.3 Message Types
Three types of messages have been defined to flow between the different components; the lot, an order, and a
command. These messages have been derived from the
DEVSJAVA entity class.
The hierarchy is shown in Figure 7. The lot is the primary unit of material that flows through the network. It
contains the quantities of the three different products, and
has some state variables used to track how long the material has been processed within the different entities. The
command is an extension of the lot and has additional
fields for specifying output path and special instructions.
The command is sent to the inventory holding points to tell
how much material to release, which output path to release
on, and any special instruction for next entity such as
which product the material should be configured to or
which customer order should be filled. Orders are sent
from the customer. Additional fields in orders are the order ID field, a due date, and a customer ID.

4.4.2 Decision Flow
At every time period the lot release controller module will
look at the data received from the previous period to calculate the release for the next period. The lot release controller sends its output to the command and lot generator. The
lot generator will send new material to the assembly test
line and the command generator will send release commands to the inventory points.
The messages sent from the decision layer are:
• New material to release into the simulation.
• How much material to release from each inventory holding point.
• How to configure material in the finish line.
• Which orders should be filled with the available
finished product.

Order
Entity

Lot
Command
Figure 7: Message Hierarchy

1598

Godding, Sarjoughian, and Kempf
4.4.3 Synchronization
Exp
#

The control for the model is based on knowledge of overall
inventory at each entity and what orders have been filled at
the end of a given time period. The decision algorithm
must have this knowledge to make decisions for the next
day. A clock is used to synchronize all entities by broadcasting an end of day message. The sequence of execution
for the simulation, repeated until the simulation is complete, is given below.
1. Simulation starts a new time period. Processing
starts on the new material and commands sent by
the decision layer in the previous period. Processing continues on material already in the simulation.
2. Decision layer calculates amount of material to release into the simulation for the next time period.
Calculation is completed by the middle of the current time period.
3. Decision layer calculates material to release from
each warehouse for the next time period. Calculation is complete by three quarters of the current
time period.
4. Decision Layer sends material and commands to
the simulation at three quarters of current time period. (Processing of new material and commands
will not start until the beginning of the next period).
5. The simulation completes the current time period.
It reports to the decision layer how much inventory is at each entity and which orders were filled.
The clock module sends out a synchronization message at the end of every time period. The synchronization
message is sent to all entities to report the timestamp of the
next time period. All entities use this to timestamp messages and increment their internal time.
The transducer records all data into a data structure.
At the end of the simulation, the transducer writes all data
into CSV files to enable offline analysis.

Ave.
Days
late
3.405
1.03
0
3.905
1.74
1
4.72
3.09
1.21
3.71
2.23
1.08

percent of orders received late by the customer, and the average number of days an order was late.
It can be seen that the control had better results for
steady demand (Experiment 1-6) than for increasing or random demands. The results also show that inventory played
important role in improving customer service levels.
Data collected from experiment 6 is shown in Figures
8, 9, and 10. Figure 8 shows how much material was released into assembly test on each day. This quantity is
close to the sum of demand for all three product because
assembly test will split its input into 3 output products.
Figure 9 shows inventory levels of each product in the
unfinished inventory store. The assembly test lot split distribution was %33 plus/minus %2 for products 1 and 3.
Product 2 split was the remaining difference from product
1 and 3 results. (e.g. If values drawn for product 1 and 3
were %31 and %34, then product 2 split would be %35).
The results show that product 2 inventory is climbing,
which is expected since the split percentage should be
slightly higher.
Inventory levels for finished goods is shown in Figure
10. Levels for all three products are centered around the tar-

SIMULATION RESULTS
AND FINDINGS

A set of 12 experiments was run to validate the system behavior. Four types of demand signals were used with 3 different inventory targets. Simulation data was collected for
200 days. Experiment descriptions and results are shown
in Table 1. For each type of demand, inventory targets of
500, 1000, and 2000 were tried. These targets were applied to all inventory points. Experiments 1 through 3 had
a constant demand of 500 for all three products. Experiments 4 through 6 had a square wave demand that changed
between 500 and 750 every 25 days for each product. Experiments 7 through 8 had a demand that increased by 100
every 25 days for each product, and experiments 10
through 12 had a demand selected from a uniform distribution between 0 and 1000. The measured results were the

3000

Die Starts Units

2500
2000
1500
1000
500

25
0

Time Days

20
0

50

15
0

Qty

0

10
0

5

1
2
3
4
5
6
7
8
9
10
11
12

Table 1: Experimental Results
Demand
InvenPercent
Profile
tory Tar- Orders
get
Late
Constant 500
500
100%
Constant 500
1000
88%
Constant 500
2000
0
Square 500-750
500
100%
Square 500-750
1000
98%
Square 500-750
2000
1%
Increasing
500
100%
Increasing
1000
100%
Increasing
2000
55%
Random 0-1000
500
86%
Random 0-1000
1000
80%
Random 0-1000
2000
40%

Figure 8: Die Released into Assembly/Test for Experiment 6

1599

Godding, Sarjoughian, and Kempf
dual processor 900MHz server with 1GB of
memory. This improved performance is a result
of the more efficient modeling allowed by the
DEVSJAVA environment, and perhaps from
DEVSJAVA itself.

Semi Finished Inventory Units

9000
8000
7000
6000
5000
4000

6

3000

FOLLOW-ON WORK

2000
1000

Prod1

Prod2

6.1 Supply Network Model

Prod3

0

0

This simulation study only included a subset of the semiconductor supply network. For the next phase, we will include
multiple assembly test factories, multiple customers in different geographies, wafer fabrication plants, and add a complicated bill of materials. With some specialization, the atomic
models implemented will support all these new capabilities.
The new structures will add many new complexities to
the control algorithm. The addition of fabrication plants
will multiply the cycle times. Multiple manufacturing facilities and customers will add interplant routing and logistic decisions, and the bill of materials will add additional
routing decisions on how to build product.

25

Time - Days

20

0
15

0
10

50

0

Figure 9: Unfinished Inventory Levels for Experiment 6
3000

Units GEO Inventory

2500

2000

1500

1000

500

Prod1
Prod2

6.2 Control Interface

Prod3

25
0

20
0

15
0

10
0

50

0

Interface specifications will be formalized to enable the
connection of different types of control applications.
Much research is being done on control and it will be useful to see how the different approaches work (e.g. mathematical optimization, AI approaches, etc…). Issues such
as what type of data to pass, how to transform the data, and
the type of architecture required must be considered.

Time Days

Figure 10: Finished Good Inventory for Experiment 6
get level at 2000. Oscillations are due to the control heuristic trying to compensate for the variability in assembly test.
Key findings of this effort include:
• The control algorithm quickly becomes complex
for this small subset of a semiconductor supply
network. With the control separated from the
physical flow, it would be straightforward to try
other types of control strategies. The supply network performance could be improved if the simple heuristic used in the initial testing were replaced with some more sophisticated control (e.g.
mathematical optimization).
• Separating the physical flow from the decision
engine simplifies the development effort for both
the physical model and decision model. The interactions between control flow and physical flow
is simplified.
• DEVSJAVA enabled modeling at a low enough
level of granularity for both entity objects and simulation timing.
• The DEVSJAVA environment tied with this modeling approach provided good performance compared to previous work. A 300 day simulation
took less than 2 minutes to run on a 500MHz Pentium III laptop with 128 MB of memory. A similar previous model in (Godding and Kempf 2001)
had runtimes greater than 1 hour when run on a

6.3 Multiple Modeling Formalisms
The approach described in Section 3 (see Figure 1) suggests the need for using distinct modeling techniques for
the physical processing and decision layers. In this paper,
however, we have separated the models according to the
separation of concerns while using only one modeling formalism. Our ongoing research includes selecting a suitable
modeling formalism for the decision layer and its integration with the DEVS formalism. Within such a framework,
we will be able to formally devise an interface specification and demonstrate the impact and value of the proposed
layering approach.
7

CONCLUSIONS

We have shown that the planning control algorithms can be
separated out from the simulation of the actual physical
flow of a small segment of the semiconductor supply network. Separation of these functions into two separate layers has not only simplified building the physical simulation, it has also provided an environment that facilitates
experimentation with different types of control policies.

1600

Godding, Sarjoughian, and Kempf
Both capabilities are very important, as scalability issues
quickly become a problem when building realistic models
for analyzing the supply network.

includes modeling and simulation of supply networks, software architecture, and artificial intelligence. He can be
contacted by e-mail at <gary.godding@intel.com>.

ACKNOWLEDGMENTS

HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science and Engineering at Arizona State University, Tempe. His research includes hybrid simulation
modeling and intelligent agents, collaborative modeling,
distributed co-design, and software architecture. His industrial experience has been with Honeywell and IBM. For
more information visit <www.acims.arizona.edu>.

This research is partially supported under the NSF Scaleable Enterprise Systems Grant No. DMI-0122227.
REFERENCES
ACIMS, DEVSJAVA Software, 2003. <http://www.
acims.arizona.edu/SOFTWARE>.
Angerhofer, B. J. and M. C., Angelides, 2000. System Dynamic Modeling in Supply Chain Management: A Research Review, in Proc. Winter Simulation Conference, 342-351.
Armbruster, D., R. Chidambaram, G. W. Godding, K. G.
Kempf, and I. Katzorke, 2001. Modeling and analysis
of decision flows in complex supply networks, in
Proc. IV SIMPOI/POMS, Sao Paulo, 1106-1114
Forrester, J.W. 1958. Industrial Dynamics: A Major
Breakthrough for Decision Makers. Harvard Business
Review, 36(4), 37-66.
Godding, G. W. and K. G. Kempf, 2001. A modular, scalable approach to modeling and analysis of semiconductor manufacturing supply chains, in Proc. IV
SIMPOI/POMS, Sao Paulo, 1000-1007.
Kempf, K. G., K. Knutson, J. Fowler, D. Armbruster, P.
Babu, and B. Duarte, 2001. Fast Accurate Simulation
of Physical Flows in Demand Networks, in Proc.
Semiconductor Manufacturing Operational Modeling
and Simulation Symposium, Tempe, AZ, 111-116.
Law, A.M., and W. D. Kelton, 1991. Simulation Design
and Analysis, McGraw-Hill Inc., New York.
Muller, J. P. and M. Pischel, 1994. An Architecture for
Dynamically Interacting Agents, International Journal
of Intelligent and Cooperative Information Systems,
3(1), 25-45.
Sarjoughian, H. S., B. P. Zeigler, S. B. Hall, 2001. A Layered Architecture for Agent-based System Development, Proceedings of the IEEE, 89(2), 201-213.
Swaminathan, J. M., S. F. Smith, N. M. Sadeh, 1998.
Modeling Supply Chain Dynamics: A Multiagent Approach, Decision Sciences, 29(3), 607-632.
Zeigler, B. P., H. Praehofer, T. G. Kim, 2000. Theory of
Modeling and Simulation, Second Edition, Academic
Press.

KARL G. KEMPF is Director of Decision Technologies
at Intel Corporation and an Adjunct Professor at Arizona
State University. His research interests span the optimization of manufacturing and logistics planning and execution
in semiconductor supply chains including various forms of
supply chain simulation. He can be contacted by e-mail at
<karl.g.kempf@intel.com>.

AUTHOR BIOGRAPHIES
GARY W. GODDING is a Software Engineer at Intel
Corporation and a Computer Science graduate student at
Arizona State University. He leads a software group responsible for factory planning automation. His research

1601

Proceedings of the 2016 Winter Simulation Conference
T. M. K. Roeder, P. I. Frazier, R. Szechtman, E. Zhou, T. Huschka, and S. E. Chick, eds.
Multi-Resolution Co-Design Modeling: A Network-on-Chip Model
Soroosh Gholami
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling and Simulation
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, AZ 85281, USA
ABSTRACT
This paper proposes a multi-resolution co-design modeling approach where hardware and software parts
of systems are loosely represented and composable. This approach is shown for Network-on-Chips (NoC)
where the network software directs communications among switches, links, and interfaces. The
complexity of such systems can be better tamed by modeling frameworks for which multi-resolution
model abstractions along system’s hardware and software dimensions are separately specified. Such
frameworks build on hierarchical, component-based modeling principles and methods. Hybrid model
composition establishes relationships across models while multi-resolution models can be better specified
by separately accounting for multiple levels of hardware and software abstractions. For Network-on-Chip,
the abstraction levels are interface, capacity, flit, and hardware with resolutions defined in terms of object,
temporal, process, and spatial aspects. The proposed modeling approach benefits from co-design and
multi-resolution modeling in order to better manage rich dynamics of hardware and software parts of
systems and their network-based interactions.
1

INTRODUCTION

Multi-Resolution Modeling (MRM) aims at describing individual parts of a dynamical system at different
resolutions with support for composing them to represent also the whole system at different resolutions.
Model resolution can be in terms of time, space, process, and object (Davis and Bigelow 1998). Models
can vary in terms of their time granularity, spatial sizes, process mechanisms, and object states and
functions. These can lead to a multitude of hierarchical model components with varying relationships.
MRM may be practiced using domain-neutral specification hierarchy (Zeigler, Praehofer and Kim 2000).
The levels within the system hierarchy establish different model type resolutions. MRM requires domainspecific abstraction levels. Target abstractions such as Interface, Capacity, Flit, and Hardware for
Network-on-Chip systems can be cast into one or more levels of a system specification hierarchy. These
NoC abstraction levels are derived from domain experts and targeted to satisfy certain needs. Within the
system specification hierarchy, modelers can begin developing a high-level model of a system (e.g., for
policy analysis) and gradually increasing the model’s resolution toward an actual system (e.g., specifying
blueprints for implementing the system).
As software continues to have a greater role in system complexity, the hardware/software co-design
paradigm that is used in embedded systems is becoming a necessity for systems such as Network-onChips. In this paradigm, designers embrace separating HW and SW development early on. This is because
regardless of the actual system we are working with, the requirements of the software and hardware pose
design constraints on one another. Co-simulation can help tackle some design requirements and solutions
(Liem et al. 1997). However, co-design must deal with the increasing heterogeneity, complexity, and
integration issues of HW and SW in electronic systems (Teich 2012).
Network-on-Chip as an integrated hardware-software system can handle the increasing complexity of
the communication subsystems of System-on-Chips. Some key capabilities include reduced messaging

978-1-5090-4486-3/16/$31.00 ©2016 IEEE

1499

Gholami and Sarjoughian
overhead and packet-based communication using configurable network topology. Like other networked
system, NoC can be modeled at multiple resolutions using object, temporal, process, and spatial
abstractions (Davis and Bigelow 1998). In NoC, individual operations can have temporal resolution. NoC
components have designated locations and dimensions (spatial resolution). Network processes such as
routing, flow control, and arbitration define process resolution. Finally, NoCs with different number of
objects and levels of hierarchy can be specified (object resolution).
New NoC designs are moving toward higher complexity (Lis et al. 2011), 3D structures (Pavlidis and
Friedman 2007), real-time support (Bolotin et al. 2004, Wiklund and Liu 2003), energy efficiency (Hu
and Marculescu 2004) and cache coherence (Martin, Hill and Sorin 2012). This calls for more
sophisticated methods for modeling, simulating, testing, and verifying the NoC before and after
implementation. Many modeling/simulation frameworks have been developed by the community to
address these needs. However, few consider the software and hardware components separately; instead,
NoC is viewed as a single system, ignoring its hybrid nature.
Our goal is to provide a new modeling framework for NoC where the static and dynamic aspects of
NoC are specified using both multi-resolution modeling and co-design. This framework considers the
NoC models at different resolution levels appropriate for various stages of SW/HW system co-design.
According to (Dally and Towles 2004), NoCs are defined at four abstraction levels: 1) Interface, 2)
Capacity, 3) Flit, and 4) Hardware. Although the abstraction levels have distinct hardware and modeling
resolutions, it is useful to cast them within a multi-resolution modeling framework. From a modeling
vantage point, NoCs can be described at multiple levels of abstraction. One approach is known as system
modeling hierarchy (Zeigler, Praehofer and Kim 2000). A system can be specified from highest level to
lowest level of abstraction: 1) I/O Frame, 2) I/O Relation, 3) I/O Function, 3) Iterative I/O Specification,
5) I/O System Specification, 6) Coupled System, and 7) Coupled Network of Systems (Zeigler, Praehofer
and Kim 2000). The I/O Frame affords specifying the least knowledge and the Coupled Network of
Systems affords specifying the most knowledge. In this modeling hierarchy, an abstraction with more
knowledge incorporates the abstractions that have less knowledge. We elaborate on abstraction levels 5
and 6 in this paper. We have network software dedicated to NoC and application software dedicated to
System-on-Chip. We consider simulation as a part of co-design which enables designers to synthesize the
system at some desired levels of resolution.

Figure 1: MRM-based HW/SW Co-Design Methodology.
2

BACKGROUND

This section provides an introduction to Multi-Resolution Modeling (MRM), Hw/Sw Co-design, and
NoCs. We highlight some existing works for NoC design/simulation and the key roles MRM and CoDesign can play in meeting current and future NoC design challenges. In (Marculescu et al. 2009), the

1500

Gholami and Sarjoughian
authors identify five major categories of open research and future challenges for NoC design and
designers: 1) application specification and modeling, 2) application optimization for communication, 3)
communication architecture synthesis and optimization, 4) communication architecture analysis and
evaluation, and 5) NoC design validation. In this research, we focus on the first, fourth, and fifth
categories. In the first category, the challenge is application traffic patterns and bandwidth requirements.
In the fourth category, the aim is identifying congestion points, hot spots, and performance evaluation. In
the fifth category, the aim is testing and validating the design (Marculescu et al. 2009).
2.1

Multi-resolution Modeling

Multi-resolution modeling (Davis and Bigelow 1998) is used in various field of research such as graphics
(Garland 1999) and defense systems (Davis and Bigelow 1998). The challenges associated with MRM
have been recognized for many years and application domains. MRM in the context of NoC modeling is
defined as a set of models, each focusing on serving a particular purpose. We use the NoC model
resolutions defined in (Dally and Towles 2004). The resolutions are defined from lower to higher
resolutions as Interface, Capacity, Flit and Hardware levels (see Figure 1). We can observe that lowerlevel resolution models are suited for verification using model checking while higher-level resolution
models are suited for validation using simulation. Thus, verification and simulation offer unique,
complementary capabilities for designing NoC systems.
One important point to note is that no MRM frameworks have been proposed for NoC and Systemon-Chips (Berekovic, Stolberg and Pirsch 2002). Without such a framework, it is difficult to classify what
could be highest or lowest resolution models (Catania et al. 2015). While one aspect of resolution (such as
object, process, time, or space) of a model may be increasing, another aspect may be declining (Davis and
Bigelow 1998). The power of MRM lies in defining levels of model abstractions and more significantly
how models that have different levels of resolution can be related. We cast the NoC’s levels of abstraction
in terms of MRM with SW/HW co-design consideration.
2.2

Hardware-Software Co-design

Hardware-software co-design process is considered within our proposed NoC modeling framework. Codesign is needed as in embedded systems (Chiodo et al. 1994). Figure 1 depicts a high-level process flow
of the NoC design approach. Phases colored in light grey are those applied to the NoC system as a whole.
These phases include the requirement specification, partitioning, modeling, simulation, and emulation.
After the partitioning phase, NoC design is carried out in two major branches (the hardware and the
software). The hardware is further divided into chip hardware and network software. The three
components (hardware, network software, and application software) are coordinated and tested against
one another using co-simulation. Multi-Resolution Modeling (MRM) is applied to the dark grey blocks.
Hardware and software models are devised at various resolutions and co-simulated for the purpose of
validation or making design decisions. In our integrated design approach, we refer to the software for the
Processing Elements as application software. As illustrated in Figure 1, the SW Model is not used
anywhere but for co-simulation to ensure the hardware is optimized for the application software it is
supposed to support. When a tangible model of hardware is reached, it is converted to real hardware
specification and validated against sample application software (if exists) via emulation. Via emulation
one can ensure the designed NoC possesses the desirable properties. After final model refinements, the
hardware is synthesized into a target system.
NoC design and simulation without considering the software is useful but incomplete. Many
modeling and simulation frameworks such as Booksim (Dally and Towles 2004), Noxim (Catania et al.
2015), DEVS-NoC (Gholami and Sarjoughian 2012), and NoC-Sim (Jantsch 2006) do not provide models
for application software. Instead, designers develop abstractions within the confines of the modeling
framework they are working with. This makes the design (and the analysis) unnecessarily complicated –

1501

Gholami and Sarjoughian
i.e., software and hardware model abstractions are not systematically separated. Without considering the
network and application software, design of the hardware cannot be optimized.
2.3

Network-on-Chip Modeling Metrics

The effectiveness of NoC simulation models should be evaluated via metrics. We have provided a set of
these in Table 1. Although there are many metrics for NoC framework evaluation (such as for verification
or GUI), we have limited ourselves only to performance and modeling capabilities of the framework.
Performance metrics evaluate the simulation framework based on its support for measuring the
throughput, utilization, power consumption, flit latency, application run-time, area requirements of the
chip, and the frequency required for the application. On the modeling metrics, we seek to measure the
capabilities of the framework for levels of NoC abstractions. The levels of abstraction supported by the
framework specify NoC model resolutions. Also, some frameworks have support for more NoC elements,
which lead to higher resolution models. Other metrics are the levels of support for multi-resolution
modeling and co-design.

Modeling
Features

Model
Performance

Table 1: NoC modeling metrics.
Area
Frequency
Latency
Power/energy
Throughput
Utilization
Modeling elements
Level of abstraction
MRM
SW/HW co-design

Area requirements of the current design
Minimum frequency to satisfy the application needs
Average latency of delivering flits
Average power usage w.r.t frequency
Transfer capability in the network
Utilization of the network capability
Structural (HW Components) and behavioral (Network SW )
Supported levels of abstraction in modeling NoC components
Environment support multi-resolution modeling and simulation
Environment capability for hybrid modeling and simulation

Booksim (Dally and Towles 2004) framework is a flit-level simulation engine with support for
throughput, latency, and utilization measurement. However, Booksim 2.0 lacks capabilities such as area
measurement or power consumption. From modeling point of view, Booksim provides a succinct textual
format for specifying the NoC, experiments, and the traffic pattern. The elements it supports are at the
flit-level (internal switch components). However, it does not take into account multi-resolution modeling,
co-design, and application software. Similar to Booksim is Noxim (Catania et al. 2015). It shares the
same capabilities and limitation as in Booksim.
Wormsim (CMU 2005) is a cycle accurate simulator developed in C++. This simulator supports a
wide range of topologies, routing algorithms, and switching policies while measuring basic performance
characteristics of the network. The traffic generations can be also trace-based in addition to synthetic.
Trace-based traffic gives the modeler the option of resembling the real application better than a synthetic
workload. This simulator can be coupled with Orion (Kahng et al. 2009) for NoC power modeling. This
simulator also lacks multi-resolution modeling, co-design, and application software modeling.
TOPAZ simulator (Abad et al. 2012) supports configuration parameters for various components of
the network such as router, topology, and traffic. One can integrate this simulator with full-system
simulation tools such as GEM5 (Binkert et al. 2011) for holistic performance evaluation and Orion
(Kahng et al. 2009) for power analysis. TOPAZ does not support application software modeling although
by integrating with full-system simulators it can.
Finally, DART (Wang, Jerger and Steffan 2011) simulator is unique among the ones introduced in
this section for its support for hardware (FPGA-based) execution. This capability substantially reduces
simulation runtime compared to other simulators (such as Booksim, NoC-DEVS, and GEM5). The DART

1502

Gholami and Sarjoughian
simulator specifies the NoC in flit-level as well and provides accurate performance measures. However, it
has limited or no support for hybrid multi-resolution modeling.
3

MRM-BASED CO-DESIGN FOR NOC

Depending on the resolution (object, time, space, and process) at which NoC is modeled (structure and
behavior), various components of NoC should be included in the model. Prototypical NoC component
belongs to one of three categories: Switch (SW), Link, and Network Interface (NI). NoC also must have
network software components required for managing data communication (see Section 3.1). We usually
add a fourth component known as processing element (PE). This is commonly used in developing
System-on-a-Chip (SoC) models. Adding the processing element paves the way for adding application
software where task execution, scheduling, and communication are necessary. Thinking of NoC as a 4component system (PE, SW, NI, and Link) is considered a low-resolution view of the system from the
object, process, and space resolution points of view. Higher resolution modeling of NoC reveals a number
of sub-components in each of these components, new processes (network software), and spatial
information, which the 4-component view does not provide. In flit-level abstraction, object resolution is
increased by decomposing the switch into input ports, output ports, crossbar, switches, routers, arbiters,
and allocators. Also, at this abstraction level, the NI is decomposed into packetizer/depacketizer
components. The process resolution is higher as new processes (such as flow control, routing, and
allocation) are added to the model. For the hardware, in addition to adding new components and processes
for them (such as error checking modules and link reconfiguration models for error handling), spatial
information is added so that heat generation and area requirements can also be calculated.
3.1

Multi-resolution Co-design Modeling

Our approach toward multi-resolution network-on-chip design leverages hardware/software co-design. In
contrast, existing MRM concepts and methods do not divide a system in terms of software and hardware..
Similarly, co-design methodology does not consider MRM. It is concerned with gradual progression
from low-resolution to high-resolution models with much as accuracy and precision as needed can be
developed. Co-design is an established process in which software, hardware, and their integration are
incrementally and iteratively specified, modeled, and implemented. A modeler cannot know the software
or hardware requirements of the system before experimenting using co-simulation. Also, at early stages of
system design, neither the hardware nor the software can be specified or implemented in detail. Therefore,
low-resolution models of software and hardware are co-simulated and experimented with in order to make
early-stage design decisions. Gradually, multiple models at different resolutions are developed which
contribute to the making of the final product.
In order to develop multi-resolution model components of NoC, we ask two questions: 1) what can be
the resolution of a component? and 2) what relationships model components at different resolutions can
have? Co-design divides a system into software and hardware with mapping. What follows are answers to
the two questions for the major components of the system (hardware and software).
Moving from Interface, to Capacity, to Flit, and to Hardware abstractions leads to more closely
modeling physical NoC systems. Resolution for the hardware and network software is defined based on
the abstraction levels introduced in (Dally and Towles 2004). Table 2 categorizes NoC abstraction levels
in terms of accuracy and precision resolution levels. As defined, the Interface level model has the lowest
level of resolution; its components are specified as objects without having Temporal, Process, and Spatial
abstractions. Only a set of objects exchanging data in an ordered discipline. We do not consider this order
as temporal specification because of its vast difference with temporal specifications in other hardware
abstractions of NoC. The capacity level introduces timing for delivering messages between two nodes and
higher resolution objects. The flit level extends the capacity level by introducing processes (for handling
flits) and more detailed model of time and objects. Finally, the hardware level, in addition to extending all
models with higher resolution concepts, adds chip spatial information to the specification.
1503

Gholami and Sarjoughian
Table 2: NoC abstraction levels with accuracy/precision resolutions.
NoC Abstraction Levels
Accuracy/ Precision →
Resolutions ↓
Object
Temporal
Process
Spatial

Interface

Capacity

Flit

Hardware















Since the Interface level is too high level of a model, we model NoC hardware using the other three
(capacity, flit, and hardware levels). Figure 2 (left) depicts a 2-node NoC at the Capacity level. Processing
elements (PEs) communicate via packets, which are routed in the network by the switches and the
BW/FW (backward/forward) links. Figure 2 (middle) depicts the NoC switch at flit-level abstraction. A
comparison between capacity and flit levels shows their differences. At the Capacity abstraction, adding
new components including Packetizer, Depacketizer, Crossbar, and Input ports increases object
resolution. The Capacity abstraction has temporal resolution. Switches can exchange flits (every packet
contains several flits) in accordance to clock cycles. Furthermore, allocators, routers, and flow controls
(i.e., process resolution) are supported. The Router, VC Allocator, and SW Allocators represent the
network software. Finally, in Figure 2 (right), the input and output ports of a single switch are modeled at
hardware level. This example clearly shows the amount of details which go inside each level of NoC
abstraction.

Figure 2: NoC hardware in three resolution levels.
Figure 2 provides not only the view of hardware parts of NoC (switch component in particular) but
the network software as well. A switch network software at the capacity level controls the entire operation
of the switch (receiving, scheduling, routing of flits and flow control), while the network software at the
flit level switch is broken up into many pieces for switch allocation, virtual channel allocation, routing,
flow control, etc. The network software would be broken even further at the hardware level.
Mutli-resolution modeling can also lend itself to developing application software. Application
software can be modeled as a set of distributed tasks, which transmit random messages, set of tasks with
pre-defined communication volume, or a set of tasks with additional specifications on the execution
times, threads, dependencies, and function calls (Butler 1994). From process point of view, the software
tasks are equipped with dependency, execution times, and probabilistic method calls as we model the
1504

Gholami and Sarjoughian
software in higher-resolution. For each abstraction level, a short description is provided along with the
specification of resolution in terms of temporal, process, and object resolutions. Network software has no
spatial resolution.
We specified resolution for software and hardware in the previous two paragraphs. Now, we should
take a bird’s eye view at the system. As mentioned, various levels of system model may be considered
high-resolution with respect to some aspects of system resolution (time, space, process, and object) and
low on some others. This is also the case in multi-resolution NoC modeling. While the software is at its
highest resolution (most accurate) for the Interface-level model, the hardware is at its lowest resolution
with respect to object and process resolution components. Similarly, the highest resolution of hardware
(the bottom row in Table 2) in the hardware abstraction level has the least detailed software. The highest
resolution software contains tasks, dependencies, communication volume, threads, and methods. Figure 3
visualizes high-resolution software executing on capacity-level hardware modules (inspired from
(Salminen et al. 2009)). At the application level, the software is defined in detail. The tasks and threads
are mapped to low-resolution hardware modules (PEs), and then the simulation is carried out. The
application level software does not exist in flit-level or hardware-level resolutions (see below).
Multi-resolution modeling can instrumental in developing NoC and SoC models. Moving from one
abstraction level to another (e.g., Capacity to Flit) can be systematic using accuracy/precision resolution
levels and system hierarchy levels. Different resolution levels can be used to categorize abstraction levels
as well as relating these levels to one another. Without having established relationships between the
abstraction levels, multi-resolution modeling is vague. For example, Flit NoC abstraction at the I/O
System level (level 5 in system hierarchy) can be related to the Object and Temporal resolution levels.
The Flit NoC abstraction is defined at the Coupled System level (level 6 in system hierarchy) and Process
resolution level (see Figure 2). Comparing models of NoC hardware at different resolutions reveals that
moving toward higher-resolution converts atomic models into coupled ones and adds new processes in
accordance with the new capabilities needed. For example, an atomic model of the switch at Capacity
level is converted to a coupled model at Flit level (Figure 2). The switch model at the flit level possesses
subcomponents inside the switch for input port, output port, crossbar, router, and allocators. Along with
these new components comes the urge to implement new processes. A router in addition to being a new
object, which increases the object resolution of the model, also requires new processes for the task it is
supposed to execute. These relationships are useful for designing, validating, and testing new models
relative to one another and ultimately to actual NoCs. The network software also changes from one
resolution to another. The important point to note here is that network software resolution is always
synchronized with the resolution of hardware. The relationship between hardware models (along with the
network software) at different resolutions is defined via Object and Process resolution components. As
explained above, higher resolution hardware model decomposes atomic models into coupled ones and
with the new components comes the need for new processes. This also holds true for network software.
The application software models define relationships based on Object and Process resolutions as well.
However, it might not be in sync with the hardware resolution (high-level hardware with low resolution
application software). High-resolution application software can be modeled with low-resolution (Capacity
level) as shown in Figure 3. The purpose of modeling the application software is to identify accurate
benchmark packet traffic among Processing Elements. As a result, the network software is not used with
high-resolution hardware model (Flit level). A good approximation of application software traffic)
replaces the application software and then the focus of the simulation shifts to high-resolution NoC
hardware and software modeling. For this purpose, a transducer model records the packet communication
between processing elements. This record is then used in the flit-level hardware as application software
model. The record can be used as only feed-forward (the software is reduced to tasks sending and
receiving flits using the record) or a feedback-enabled software (tasks are still data sensitive,
dependencies exist, and the record captures all of them). One way or another, there is no high-resolution
software application anymore. This record is used for generating packets for NoC. The flit-level model

1505

Gholami and Sarjoughian
places more emphasis on the hardware while the capacity-level model places more emphasis on the
application software. The NoC hardware model takes this even further by excluding data from flits. The
communication between components at the flit-level are recorded and is used in the simulation using
feedbacks. Similarly, no high-resolution software aspect such as the data in the flits, end-to-end
communication, and flit sequence number exists in this stage.

Figure 3: High resolution software at capacity level hardware (low-resolution).
4

NOC MODELS

As discussed in Section 2.2, NoC contains two types of software: application software running on the
hardware platform and Network Software as a collection of small pieces of software which control the
network aspect of NoC. This Network Software has to deal with new problems as NoC moves toward
more general software, size increase, energy-awareness, cache coherency, etc. Therefore, NoC cannot be
viewed as pure hardware anymore. Software-defined NoCs resemble hybrid systems in which the
software plays the crucial role of controlling the hardware. In this light, the NoC is a hybrid system with a
traditional hardware and network software modules which control the most dynamic aspects of hardware
relative to application software. As shown in Figure 1, simulation plays an important role at the latter
stages of NoC design where high-resolution hardware sketches are tested against network and application
software. It is possible to capture the hybrid nature of NoC system in one formalism too. In this work we
have modeled the NoC in a single formalism and simulated them in one environment. However, the HW
and network SW models are still inherently different due to the hybrid nature of NoC.
Now the question would be which resolutions of (network and application) software and hardware
can be used with one another. In order to answer this question, we consider link, a simple component of
NoC and model it’s hardware at three resolution levels, and then show hybrid multi-resolution models
can be defined given different abstractions for the network and application software. The resolution of
hardware, network software, and application software are defined as follows. Hardware resolutions are
stated with the level of abstraction used for modeling it (interface, capacity, flit, and hardware levels). As
for application software, we model it at three abstraction levels (i.e., random, distributed, and
probabilistic). Network software can be found in various NoC components such as processing elements
(cache coherence algorithms), switches (routing/flow control) and links (error checking and
retransmission). We use the same hardware resolution levels for network software due to its closeness to
hardware.
For the model of the link provided in Section 4.1, we do not consider the link as only a wire which
transfers electric charge; instead, based on the resolution the link is modeled at it may contain
complementary logic from upstream and downstream nodes for retransmission, error checking, channel
reconfiguration, error counter, and fail-stop.

1506

Gholami and Sarjoughian
4.1

Multiresolution Link HW/Network SW Models

Figure 4 depicts a low-resolution model of the link with the data (packets) that it can handle. A piece of
network software exists for the Fail-stop module which is in charge of disabling the channel if need be.
The network software inside the Fail-stop module is simple since the model is at the capacity level. For
testing purposes, the role of this module might be to use a distribution function to inject errors. This way
the system can be tested under erroneous conditions. The Fail-stop module communicates with the
hardware at the channel entry point. That is where the software signal interacts with the hardware
component and disables the transmission operation.

Figure 4: Low-resolution link model with the type of data it communicates.
The Fail-stop consists of a hardware logic which is governed by a network software. However, there
are components in NoC that are purely hardware, such as buffers. For the low-resolution model of the
link, channel is pure hardware while the Fail-safe module is both hardware and network software. The
network software will be modeled as the behavior of the hardware component. The hardware component
is defined by specifying the state space and input/output ports. The rest of the specification (how inputs
are handled, how outputs are generated, how the state changes, etc.) is the network software which
specifies how the hardware behaves. At each of the resolution levels (described below) the hardware and
software components of the link are extended.
𝑃ℎ𝑎𝑠𝑒

𝑠𝑖𝑔𝑚𝑎

𝑃ℎ𝑖𝑡

⏞ ×⏞
𝑆=⏞
{𝐴𝑐𝑖𝑡𝑣𝑒, 𝐼𝑑𝑙𝑒} × 𝜎
{0,1}16

(1)

𝐴 = {𝑑𝑒𝑙𝑖𝑣𝑒𝑟𝑃ℎ𝑖𝑡}

(2)

𝛿𝑒𝑥𝑡 (𝐼𝑑𝑙𝑒, 𝜎, 𝑃ℎ𝑖𝑡, 𝑒, (𝑖𝑛, 𝑥)) = (𝐴𝑐𝑡𝑖𝑣𝑒, 𝛿𝑡, 𝑥)

(3)

𝛿𝑖𝑛𝑡 (𝐴𝑐𝑡𝑖𝑣𝑒, 𝜎, 𝑃ℎ𝑖𝑡) = (𝐼𝑑𝑙𝑒, ∞, ∅)

(4)

𝜆(𝐴𝑐𝑡𝑖𝑣𝑒, 𝜎, 𝑃ℎ𝑖𝑡) = (𝑜𝑢𝑡, 𝑃ℎ𝑖𝑡)

(5)

𝜓(𝐴𝑐𝑡𝑖𝑣𝑒, 𝜎, 𝑃ℎ𝑖𝑡) = {𝑑𝑒𝑙𝑖𝑣𝑒𝑟𝑃ℎ𝑖𝑡}

(6)

𝐼𝑛𝑃𝑜𝑟𝑡𝑠 = {𝑖𝑛}, 𝑂𝑢𝑡𝑃𝑜𝑟𝑡𝑠 = {𝑜𝑢𝑡}

(7)

𝑃𝑝 = 1 − (1 − 𝑃𝑒 )𝑁

(8)

0 (𝑒𝑛𝑎𝑏𝑙𝑒) 𝑃𝑝 < 10−12
Fail-stop Signal = {
1 (𝑑𝑖𝑠𝑎𝑏𝑙𝑒) 𝑃𝑝 ≥ 10−12

(9)

As a showcase of a simplified capacity-level model of the link, we modeled the channel using ALRTDEVS (Sarjoughian and Gholami 2015) in equations 1-7. The model is defined to have a state set (𝑆),
input/output ports, external transition (𝛿𝑒𝑥𝑡 ), internal transition (𝛿𝑖𝑛𝑡 ), and output functions (𝜆), actions set
(𝐴), and activity mapping (𝜓). As for the network software operating on the Fail-stop module, we used
the concept of BER (bit error rate) (Proakis and Salehi 2007). The software disables the channel if the
actual frequency of channel malfunction (bit error) is greater than the packet error ratio (PER) which is
characterized by 𝑃𝑝 . The Fail-stop module can be developed within a DEVS model and thus simplify its
composition with the channel model.
The link model is modeled at higher resolution in Figure 5 (left) by increasing its details in the object
and process resolution dimensions. The hardware is extended by flit buffer and additional logic for
retransmission and error checking. The network software is also extended with error checking algorithm
and retransmission decision making module. The retransmission logic, upon receiving an error signal
from Error Checking Module, reconfigures the MUX to pass the buffer data through and enables the
buffer to transmit the previously stored data. This way, the data sent in the previous cycle and rejected by
the error checking module is retransmitted. In addition to the extensions made to hardware and software,
the data which is communicated is changed to flit which is the breakdown of a packet into 8 chunks of
smaller data. The higher resolution for the packet and flit structures are shown in Figure 5 (bottom).
Finally, in Figure 5 (right), we have modeled the link at higher resolution. The hardware is extended
with additional necessary modules for wires, error counter module, and channel reconfiguration module.

1507

Gholami and Sarjoughian
Consequently, the network software is also extended for channel reconfiguration management. The Failstop module works based on an error counter module. If the number of errors for the channel becomes
greater than acceptable, the channel reconfiguration module orders the fail-stop module to block the
channel. In bit reconfiguration scenario, the channel is reconfigured to change the data wires due to bit
errors in one of them (Dally and Towles 2004). Thus, the channel is reconfigured to use less or a different
set of wires for data communication. In high-resolution, the communication unit of data is still the flit.

Figure 5: Link models at the Flit and hardware abstraction levels.
4.2

SW/HW NoC Models

In this section, we devise co-design of NoC in three parts: 1) hardware, 2) network software, and 3)
application software. The reader must keep in mind that modeling application software is considered as
future work. This helps in describing our approach to NoC multi-resolution co-design modeling.
At early stages of design, the model of the hardware (and consequently the network software) are low
resolution. For the link model, this is shown in Figure 4 where the link consists of a channel and a simple
piece of software in charge of fail-stop module. This is only modeling the link, however, once applied to
the switch and network interface components, together they provide us with the capacity-level model of
NoC. The model of NoC at this level incorporates the most detailed application software (probabilistic
model) on Capacity-level NoC. The reason for this choice is that at early stages of design, the designers
need to verify whether their high-level sketches of the hardware and network software are capable of
handling the load imposed by the application software. Therefore, the most accurate model of the
software is applied and the results analyzed in case changes in the design are necessary.
Furthermore, for the flit-level model of NoC (for which the link is depicted in Figure 5-left), the midresolution software (distributed tasks) are used. At this stage, the designer focuses more on the hardware
(such as buffers and virtual channels) and network software (such as routing flow control). In order to test
the design, they require a model of the software with a wide variety of communication patters. For this,
the distributed model is most suitable since it can be configured to imitate a large variety of software
applications with various end-to-end communication requirements. This enables designers to verify their
design using various scenarios which can be easily produced by the distributed model of the software
such as real-time/quality of service constraints and bandwidth requirements. The reason that probabilistic
model is inappropriate for this level is reconfiguration inflexibility. The probabilistic model is devised to
present a specific (or at least a limited number of) software applications. However, at this stage,
configurability and flexibility of software for producing various sorts of traffic and scenarios is desirable.

1508

Gholami and Sarjoughian
Finally, at hardware level (presented in Figure 5-right), the circuit is designed at highest resolution.
Here, the focus is on testing individual components (pure hardware and network software) to ensure
correct behavior under various scenarios. For example, the model of the link in Figure 5-right, can be
tested under severe electromagnetic interference to verify retransmission and error checking modules. For
this form of testing/simulation, one does not need an actual software to test the component. The only
thing needed is random flits (which their data is entirely random) travelling through the channel under
electromagnetic interference. Therefore, the hardware-level NoC model uses the random model of
application software as the most appropriate one for verification purposes.
5

CONCLUSION

In this paper we described a method for NoC modeling using Multi-Resolution Modeling and co-design
in order to help tackle some of the challenges facing Network-on-Chip system design. In this method,
NoC hardware and software can be designed in various levels of resolution and later simulated for
verification purposes. We described the role of multi-resolution modeling in SW/HW co-design. We
observed the domain-specific NoC abstraction levels can be succinctly defined in terms of the domainneutral I/O System and I/O Coupled System hierarchy levels. We used the proposed framework to
develop models for the NoC Link component and described how various resolutions of hardware and
software can be coupled for simulation. In the MRM Co-Design framework, we observed that the existing
NoC abstractions need to be extended to handle application software and network software to support
multiple levels of resolution. Later all these can be used for verification purposes. Future research
includes devising a specific NoC application software in various resolutions and couple it with our
existing models of NoC system in different resolutions. This can be extended to synthesizing the
hardware model on a FPGA and running the application software on it.
REFERENCES
Abad, P., P. Prieto, L. G. Menezo, A. Colaso, V. Puente, and J. A. Gregorio. 2012. "TOPAZ: an opensource interconnection network simulator for chip multiprocessors and supercomputers." Sixth
IEEE/ACM International Symposium on Networks on Chip (NoCS). 99-106.
Berekovic, M., H-J Stolberg, and P. Pirsch. 2002. "Multicore system-on-chip architecture for MPEG-4
streaming video." IEEE Transactions on Circuits and Systems for Video Technology (IEEE) 12
(8): 688-699.
Binkert, N., B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, Joel Hestness, et al. 2011. "The
GEM5 simulator." SIGARCH Comput. Archit. News (ACM) 39 (2): 1-7.
Bolotin, E., I. Cidon, R. Ginosar, and A. Kolodny. 2004. "QNoC: QoS architecture and design process for
network on chip." Journal of systems architecture 50 (2): 105-128.
Butler, J. M. 1994. "Quantum modeling of distributed object computing." ACM SIGSIM Simulation
Digest 24 (2): 20-39.
Catania, V., A. Mineo, S. Monteleone, M. Palesi, and D. Patti. 2015. "Noxim: an open, extensible and
cycle-accurate network on chip simulator." IEEE International Conference on Applicationspecific Systems, Architectures and Processors. Toronto: IEEE.
Chiodo, M., P. Giusto, A. Jurecska, H. C. Hsieh, A. Sangiovanni-Vincentelli, and L. Lavagno. 1994.
"Hardware-software codesign of embedded systems." IEEE Micro 14 (4): 26-36.
CMU. 2005. "Wormsim 4.2." available: http://www.ece.cmu.edu/~sld/software/worm_sim.php.
Dally, W. J., and B. Towles. 2004. Principles and practices of interconnection networks. Morgan
Kaufmann.
Davis, P. K., and J. H. Bigelow. 1998. Experiments in multiresolution modeling. DTIC Document.
Garland, M. 1999. "Multiresolution modeling: survey & future opportunities." State of the art report 111131.

1509

Gholami and Sarjoughian
Gholami, S., and H. S. Sarjoughian. 2012. "Real-time network-on-chip simulation modeling."
Proceedings of the 5th International ICST Conference on Simulation Tools and Techniques.
Desenzano, Italy.
Hu, J., and R. Marculescu. 2004. "Energy-aware communication and task scheduling for network-on-chip
architectures under real-time constraints." Design, Automation and Test in Europe Conference
and Exhibition. 234-239.
Jantsch, A. 2006. "NoCSim: A NoC simulator." School of Information and Communication Technology,
Royal Institute of Technology. Stockholm.
Kahng, A. B., B. Li, L. S. Peh, and K. Samadi. 2009. "ORION 2.0: a fast and accurate NoC power and
area model for early-stage design space exploration." Proceedings of the conference on Design,
Automation and Test in Europe. 423-428.
Liem, C., F. Nacabal, C. Valderrama, P. Paulin, and A. Jerraya. 1997. "System-on-a-chip cosimulation
and compilation." IEEE Design Test of Computers 14 (2): 16-25.
Lis, M., P. Ren, M. H. Cho, K. S. Shim, C. W. Fletcher, O. Khan, and S. Devadas. 2011. "Scalable,
accurate multicore simulation in the 1000-core era." IEEE International Symposium on
Performance Analysis of Systems and Software (ISPASS). 175-185.
Marculescu, R., U. Y. Ogras, L. S. Peh, N. E. Jerger, and Y. Hoskote. 2009. "Outstanding research
problems in NoC design: system, microarchitecture, and circuit perspectives." IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems (IEEE) 28 (1): 3-21.
Martin, M. K., M. D. Hill, and D. J. Sorin. 2012. "Why on-chip cache coherence is here to stay."
Communications of the ACM (ACM) 55 (7): 78--89.
Pavlidis, V. F., and E. G. Friedman. 2007. "3-D topologies for networks-on-chip." IEEE Transactions on
Very Large Scale Integration (VLSI) Systems 15 (10): 1081-1090.
Proakis, J. G., and M. Salehi. 2007. Digital communications. McGraw-Hill Education.
Salminen, E., C. Grecu, T. D. Hamalainen, and A. Ivanov. 2009. "Application modelling and hardware
description for network-on-chip benchmarking." IET Computers & Digital Tech. 3 (5): 539-550.
Sarjoughian, H. S., and S. Gholami. 2015. "Action-level real-time DEVS modeling and simulation."
Simulation: Transactions of the Society for Modeling and Simulation International 91 (10): 869887.
Teich, J. 2012. "Hardware/Software codesign: the past, the present, and predicting the future."
Proceedings of the IEEE 100: 1411-1430.
Wang, D., N. E. Jerger, and J. G. Steffan. 2011. "DART: a programmable architecture for NoC simulation
on FPGAs." Proceedings of the Fifth ACM/IEEE International Symposium on Networks-on-Chip.
New York: ACM. 145-152.
Wiklund, D., and D. Liu. 2003. "SoCBUS: switched network on chip for hard real time embedded
systems." Parallel and Distributed Processing Symposium.
Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory of modeling and simulation: integrating
discrete event and continuous complex dynamic systems. Academic press.
AUTHOR BIOGRAPHIES
SOROOSH GHOLAMI is a Computer Science PhD student at ASU. He can be contacted at
sgholami@asu.edu.
HESSAM S. SARJOUGHIAN is Associate Professor of Computer Science & Engineering at Arizona
State University and Co-Director of the Arizona Center for Integrative Modeling and Simulation. His
research focuses modeling & simulation theory, model composability, distributed co-design modeling,
visual simulation modeling, and agent-based simulation. He can be contacted at sarjoughian@asu.edu.

1510

A WESS-based Method for Anti-submarine Simulation
through Planning Waypoints of Helicopter (WIP)
Zhi Zhu, Yifan Zhu, Yonglin Lei
Institute of Simulation Engineering
College of Information System & Management,
National University of Defense Technology,
Changsha 410073, China
Zhi.Zhu.1@asu.edu, yfzhu|yllei@nudt.edu.cn
ABSTRACT

Joint Anti-submarine Systems (JAS) is a complex system
employed in Anti-Submarine Warfare (ASW). Current
research in this domain, however, mainly focuses on pure
theoretical analysis or unable to support efficient
simulations. Consequently these works cannot be
effectively applied to develop simulations that can represent
and evaluate requirements and tactical designs. This paper
discusses a WESS (Weapon Effectiveness Simulation
Systems)-based method for JAS simulation through
designing routes for helicopters. Firstly, we specified the
hierarchical structure of JAS complexity from two
perspectives. One is the ‘part of’ hierarchy which specifies
parts of systems as component diagrams. While the other is
the ‘is a’ hierarchy specifying inheritance relationships
among parts of systems as class diagrams. After that, in
order to lock the adversary submarine in a closed area,
through a circle calculation the circle search pattern is
introduced to display a tactical way to design waypoints for
helicopters. Upon the WESS framework we developed an
exemplar simulation scenario integrated with script-based
decision scripts to process simulation and demonstratively
presented the two-dimensional screenshot of the circle
pattern. It is shown that the proposed method can improve
the efficiency of simulation application development to
meet those requirements mentioned above.
Author Keywords

Effectiveness simulation; decision modeling; ASW; WESS;
ACM Classification Keywords

D.2.13 Reusable Software: Reusable models; F.1.3
Complexity Measures and Classes: Complexity hierarchies;
I.6.3 Applications; I.6.4 Model Validation and Analysis;
I.6.5 Model Development; I.6.7 Simulation Support
Systems; I.6.8 Types of Simulation: Visual; J.7 Computers
in Other Systems: Military;

SummerSim-SCSC, 2016 July 24-27, Montréal, Québec, CA
© 2016 Society for Modeling & Simulation International (SCS)

Hessam S. Sarjoughian
Arizona Center of Integrative M & S
School of Computing, Informatics & Decision
Systems Engineering, Arizona State University,
Tempe, AZ 85281, USA
Sarjoughian@asu.edu
INTRODUCTION

In an encounter between a surface ship and a submerged
submarine, the advantage definitely tends to latter due to
the stealth of submarine with a quieter acoustic signature.
But the advantage diminishes significantly when a surface
ships act in concert with other friendly assets, such as
submarines or especially helicopters [13]. Actually, in this
research planning the route of a helicopter to search
submarines refers to designing several waypoints at which
the helicopter will launch sonobuoys or hover to drop
dipping sonars. In order to overcome the limitation of a
single sonar system to scour the ocean for a hint of an
adversary submarine, the proverbial search of a “needle in a
haystack”, at each of these waypoints a helicopter must
drop dipping sonars with retractable cables, or launch a
certain number of sonobuoys in specific formations.
Currently, there are many simulation systems designed for
certain domains (e.g., [2][5][6]]). Each of these systems has
itself advantages and is well used to develop applications
by various companies or research institutes. However, there
are still some parts that need to further development. For
example, in these systems there does not exist a strong
distinction between battle levels (mainly contain campaign,
engagement and engineering) [8], there is a high degree of
coupling between modeling domains (mainly contain
physical, information and cognitive) and there is no unified
composable modeling framework to support model
portability, reuse, interoperability, etc., resulting in time
consuming application development efforts. In this paper
we propose a WESS-based [10] method to develop
applications for JAS. WESS is an engagement-level
effectiveness simulation method which contains a suite of
tools, such as SMP2-based [11] model development
framework, scenario editing, data management, script-based
decision modeling, experiment designing, simulator,
displayer, and automatic output analysis. Based on a unified
composable modeling framework [12], WESS allows
developers to easily reuse common knowledge. In addition,
it allows users to include decision which is the key to the
development of human-in-loop combat simulations.

Figure 1. A ‘part of’ hierarchy specified as component diagrams

The remainder of this paper is structured as follows: section
2 gives an introduction into JAS followed by section 3 that
discusses the Circle search pattern. Based on WESS,
section 4 illustrates the implementation of an antisubmarine scenario, and presents its 2D diagram followed
by section 5 that concludes this paper.
HIERARCHICAL STRUCTURE OF JAS

As a typical kind of complex systems, JAS is usually
located in a large space (air, surface and underwater),
mainly consists of three major subsystems (command and
control, submarine, warship), and is expected to complete
the mission of anti-submarine through the cooperative
activity of these subsystems. Based on the definition of
hierarchical complexity from [4], here we study the
hierarchical structure of JAS mainly from two aspects.
As illustrated in Figure 1, each of those subsystems is
further decomposed in a specific structure. For example, C2
(command and control) encompasses a commander, a
communication system, the common operational picture
(COP), a displaying system and a decision system.
Similarly, a submarine can be divided into C2, towed sonar,
hull sonar, torpedoes and mines, and a warship is composed
of helicopters, C2, fire control radar, cruise missiles, towed
sonar, hull sonar, torpedoes and mines. Each of these
structures is further composed of a collection of elements,
and inside each element we find yet another level of
complexity. For example, the towed sonar generally
consists of a cable and the sonar system which is further
divided into phased array, electronic system and auxiliary
equipment. In each level, the decomposition represents a
structural, or ‘part of’ hierarchy.
At the same level of decomposition all parts shown within
the same block cooperatively interact with each other to
complete the common task in a well-defined way (see
Figure 1). For example, in the first level of decomposition,

ground-based C2 is responsible for the command and
control of the whole battlefield. To conduct a mission of
anti-submarine, warship and submarine departure from their
harbors when receiving combat commands from the
ground-based C2, then they cruise in a predefined route.
When cruising close to the target, the helicopter takes off
from warship to process a more precise activity of
searching submarine. If a threat was found and recognized,
as well as the precise location of the threat was measured,
these subsystems in this level will enter the phase of attack
to destroy threats by using torpedoes collaboratively. This
is also an example of emergent behavior [4] in science of
complexity, which can be represented by the formula:
1+1>2.
Alternatively, we can cut across JAS in an entirely
orthogonal abstraction, or a ‘is a’ hierarchy. In this way we
can find a completely different representation. As shown in
Figure 2, the top common entity SimEntity is inherited by a
few classes, such as C2, platform, sensor and firepower.
Different from the ‘part of’ decomposition hierarchy, the
platform can be further inherited by space, air, surface and
underwater platforms according to the space in which a
special platform is located. Like the abstraction of platform,
the sensor can be further inherited by radar, sonar and
vision according to the physical mechanism of a sensor that

Figure 2. A ‘is a’ hierarchy specified as a class diagram

is used. Similarly, the firepower is inherited by missiles,
torpedoes, mines and so on. Each level of abstraction
represents the common knowledge of a special domain, and
should have a well-formed boundary from higher and/or
lower levels, to support strong model reuse.
PLANNING WAYPOINTS OF HELICOPTER

Sonar is generally used to exploit underwater acoustic
sound propagation for navigating, communicating, or as a
means of locating objects. There are usually two typical
kinds of sonar when a helicopter is commanded to conduct
the searching mission of an adversary submarine: dipping
sonar and sonobuoy [15].

Figure 3. The circle pattern using sonobuoys

For a limited space, here we only take the sonobuoy with
the Circle search pattern as an example shown in Figure 3.
We defined d as the distance between every two adjacent
waypoints, r as the maximum detecting radius of dipping
sonar, and N as a total of waypoints, so it can enable a
seamless search pattern that brings a lower probability for
the adversary submarine to elude if r ≤ d ≤ 2r .
If the radius D (ow0) of Circle is initialized to guarantee
that the distance d between every two adjacent waypoints is
subjected to r ≤ d ≤ 2r , then a better search effect is more

workflow of simulation application development, and more
details about the workflow of physical model development
can be found in [12].
Consider the simulation application development shown in
Figure 4. It consists of eight tasks and nearly each of which
is supported by a tool. The first two tasks can be carried out
concurrently. The task of Data collecting is to prepare the
values of kinds of prototype parameter and to construct the
relationships between different models. For example, for a
certain prototype of dipping sonar we can easily set the
maximal value of detecting range to 5km by using the data
management tool, also we can set the kill probability of a
certain prototype of torpedo to 0.8 with respect to a certain
prototype of submarine. While in the task of Scenario
editing a scenario editing machine is used to design the
situation and strength of both sides. For example, the red
(blue) side consists of two warships and each of which
further contains a helicopter and different prototypes of
torpedoes, mines, and sonars.
When the data is collected and the scenario is edited, we
can synthesize the scenario to generate assembly files (more
details we published in [10]). The next task is to design an
experiment, and this task can be conducted with cognitive
modeling concurrently. Designing an experiment is
significant for WESS simulation because that the goal of
running a simulation is not to watch a warfare like a
computer game. We must design the simulation experiment
to answer the problems of sponsors or to satisfy their
requirements. After that we can start the simulator engine to
run a simulation. If the simulation is running without errors,
we can monitor the simulation in real time and thereafter
analyze the simulation outputs.

likely to occur. Note that D = 4 + 2 2 r , d = 2r , N = 8 .
In such a search pattern, a regular octagon centered at the
origin o is obtained through the connectivity of each two
adjacent waypoints, and the coordinate of every waypoint
for Circle can be known by the following formulations:
360 × i 
xi = D × cos

 N 
+
 360 × i where 0 ≤ i ≤ N −1, i ∈ Z .
y = D ×sin
i
 N 


WESS-BASED IMPLEMENTATION

Here we consider how to develop a simulation application
using the proposed WESS-based method with an example
of JAS simulation to illustrate its feasibility.
Workflows of WESS

There are factually two workflows for WESS. One is the
workflow of physical model development which is usually
from the perspective of modelers. The other is the workflow
of simulation application development which is from the
perspective of domain users. Here we mainly discuss the

Figure 4. Workflow of Simulation applications development
A Demonstrative Example: JAS Simulation

For all of the physical JAS models, such as warship,
helicopter, submarine and sonars etc., they are already built
into WESS. Thus, we will only illustrate the example of
JAS simulation according to the workflow of simulation
application development. Given above WESS-based
workflow we only discuss three parts for JAS simulation

application development. They are scenario editing, scriptbased decision modeling (developing in Python
programming language), and simulation displaying.
Scenario Editing

The purpose of scenario editing is to deploy the forces
belonging to two opposing sides in a wargame and their
initial status as well as configuring attributes of each entity.
The configuration, commonly contains four things: setting
up the prototype and tactical data of each entity, associating
the decision script with each platform entity, adding
weapons and sensors for each platform entity, and planning
the initial waypoints of each platform entity (for details
refer to [10]). Here note that the waypoints for a helicopter
are given by its carrier warship. Assume that an adversary
will cruise through an ocean area at a certain velocity with
its acoustic signature already detected by the ground-based
sonar or by other means. Thus the warship, equipped with a
helicopter, is ordered to conduct the mission of submarine
searching then attacking it in an operational area. As stated
earlier, the helicopter has usually two ways to search an
adversary submarine. For sonobuoy, we just need to set the
number of sonobuoys in the panel of attributes
configuration. For dipping sonar, we need to set the number
of dipping points in the decision script that will be
discussed in the next section.
Script-based Decision Modeling

Physical models are assumed to have known (predefined)
structures and stable behaviors. In contrast, cognitive
models must be flexible, especially in the domain of
warfare where decision models must change with different
combat missions, battlefield conditions can change rapidly,
and commanders may make different choices [3].

JASUsingDS and JASUsingSB respectively, to
be
responsible for searching with dipping sonar and sonobuoys.
The decision modeling environment for the latter interface
is shown in Figure 5.
Simulation Displaying

When all of the physical and cognitive models are prepared,
the next task is to run simulations and analyze the results to
validate these models. In order to grasp the whole real-time
situation of a battlefield, a simulation displaying tool is
developed to assist us in a visual way. Here we just need to
change the value of any decision parameter in the same
scenario, not to construct a new scenario from scratch, thus
a different search pattern can be acquired. For example, a
triangle search pattern is easily acquired by an assignment
of searchMode = C_searchTriangle which is originally the
assignment of searchMode = C_searchCircle.
The 2D display representing latitude and longitude, as
shown in Figure 6, provides us a visual way to observe the
whole process of simulation running, freeing us from
writing thousands of lines of code or texts. In this display
each diamond represents a sonobuoy, and each sonobuoy
has an outside circle that represents its detecting range. We
can easily see the search pattern consists of 12 sonobuoys,
and each pair of two adjacent sonobuoys connects to form a
big polygon. If an adversary submarine acts inside the circle,
then it cannot be able to escape from this search pattern.
Note that the values of decision parameters in this scenario:
lon_deg = 121.91, alt_deg=37.96, searchMode =
C_searchCircle, C_searchRadius = 3000, rotate_deg = 0.

legend:

helicopter

submarine

sonobuoy

range

route

Figure 6. 2D display of JAS simulation
Figure 5. JASUsingSB interface

Consequently, the cognitive models should be independent
of the physical models, and thus we just need to consider
the inputs and outputs for decision interfaces without
considering the concrete implementations of physical
entities. This is important for easing development of
cognitive models. We designed two interfaces, called

CONCLUSION

The introduction of submarines brought about a new kind
of battlefield which in turn resulted in a hotbed to
revolutionary kinds of battle in naval warfare [9]. A
submarine featuring stealth and offensive attack allows it to
closely approach the most powerful warships and then
launch a torpedo below the waterline, thus resulting

warships to be destroyed. As a result, what the detecting
equipment to use effectively and how to get the underwater
acoustic signatures early become a key to winning naval
warfare. Usually there are two kinds of devices that are
used to detect underwater acoustic signatures for searching
submarines. For the dipping sonar, the route of the
helicopter is planned. The route is a combination of the
helicopter hovering points. For the sonobuoys, the route is
also a combination of the helicopter waypoints but not
hovering, which means at each of these waypoints the
helicopter delivers a sonobuoy then drives to the next
waypoint immediately. Finally based on the WESS
framework, an application about JAS is developed
efficiently to validate the feasibility of the search pattern.
The future work concentrates on the following aspects.
Firstly, when the physical and cognitive models are
validated correctly, experiments should be designed to
evaluate and explore which routes could be the best pattern
in a specific scenario. Secondly, to promote the application
of the proposed method, many models for different kinds of
domains should be created appropriately, such as air
defense [14]. Finally, due to the complexity of warfare,
many properties cannot be captured by the existing
modeling tools, thus domain-specific modeling languages
[1][7] will be used to capture the domain concepts,
relationships and rules.
ACKNOWLEDGMENTS

We are grateful to the anonymous referees for their helpful
reviews, and all the volunteers who wrote and provided
helpful comments on previous versions of this document.
As well gratefully acknowledge the grant from NSFC
(National Natural Science Foundation of China, #61273198)
and CSC (China Scholarship Council).
REFERENCES

1. D. Steinberg, F. Budinsky, et al. EMF: Eclipse
Modeling Framework, 2th Edition, Addison-Wesley,
2008.
2. EADSIM. http://www.eadsim.com/, last accessed 2014.
3. G. C. Critchfield, K. E. Willard. Probabilistic analysis of
decision trees using monte carlo simulation Med Decis
Making, 6 (2) (1986), pp. 85–92.

4. H. A. Simon. The Architecture of Complexity, 1962,
Carnegie Institute of Technology.
5. S. B. Hall, B. P. Zeigler, H. S. Sarjoughian, “Joint
Measure: Distributed Simulation Issues in a Mission
Effectiveness Analytic Simulator,” Simulator
Interoperability Workshop (SIW), 99F-SIW-159.
6. JMASS Overview. https://www.jmass.wpafb.af.mil.
Accessed 25 Apr 2013.
7. S. Kelly, J-P. Tolvanen. Domain-Specific Modeling:
Enabling Full Code Generation, Wiley-IEEE Society
Press, 2008.
8. K-M. Seo, C. Choi, T. G. Kim and J. H. Kim. DEVSbased combat modeling for engagement-level
simulation, Simulation: Transactions of the Society for
Modeling and Simulation International, 2014.
9. C. Lance, H. Raymond. Search Theory, Agent-based
Simulation, and U-boats in The Bay of Biscay. Winter
Simulation Conference Proceedings, 2003, 1:991-998.
10. Y. L. Lei. An introduction to WESS.
http://acims.asu.edu/wpcontent/uploads/2015/06/Introduction-to-WESS.
11. Y. L. Lei, N. L. Su, et al. New simulation model
representation specification SMP2 and its key
application techniques. System Engineering - Theory &
Practice, 2010. 30(5): 899-908.
12. Y. L. Lei, Q. Li, F. Yang, et al. A composable modeling
framework for weapon systems effectiveness
simulation. Systems Engineering-Theory & Practice,
2013. 33(11):2954-2966.
13. Metron Inc.. Naval Simulation System Brochure,
http://www.metsci.com/Portals/0/Documents/NSS
Brochure 02-22-12 v2.pdf, 2012.
14. N. R. Bourassa. Modeling and Simulation of Fleet Air
Defense Systems Using EADSIM, MS Thesis: Naval
Postgraduate School, 1993.
15. B. R. Stephen, J. F. Lawrence. Helicopter Navigation
Algorithms for the Placement of Sonobuoys in An
Antisubmarine Warfare (ASW) Environment. IEEE
Proceedings of the National Aerospace and Electronics
Conference, 1988, 1:280-286.

Synchronizing DEVS/SOA Simulator with Ping Monitoring Application
Jonathan D. Gibbs (jdgibbs@asu.edu) and Hessam S. Sarjoughian (sarjoughian@asu.edu)
Arizona Center for Integrative Modeling and Simulation – Arizona State University, Tempe, AZ, USA

Keywords: DEVS/SOA, Ping, Synchronization
Abstract
This paper describes our experiences and observations
in developing an approach for integrating an external
software application – ping – into the DEVS/SOA
simulation testbed. A prototype implementation of the
testbed is under development as part of a project to help
automate testing of service-based software systems in
accordance with US Department of Defense (DoD)
objectives. The “ping” capability is necessary for gathering
information on network health such as server availability.
The process of invoking ping from Java can be fairly
straightforward, but adding this capability to the
DEVS/SOA simulation testbed requires far more careful
consideration. We describe some of the decisions made in
planning the integration, including several design decisions
which had to account for issues such as synchronization, the
requirements of the project, and elements of the end-user
experience. To design the integration, we focused on the
fundamental need for synchronizing the ping software
application within the DEVS/SOA simulator testbed. The
integrated DEVS/SOA Ping prototype demonstrates the
kinds of interactions that should be accounted for when
planning and executing testing of services. The approach
presented in this paper is positioned to aid the design and
development of other kinds of simulation-based SOA-based
testing environments.
1.

INTRODUCTION
The US Department of Defense (DoD) has a critical
need for automated testing of its services, tools, and
technologies [1, 2]. A key part of achieving this goal is to
employ simulation as a core capability to overcome a
variety of constraints (e.g., assessing scalability of NetCentric Enterprise Systems) while meeting their immediate,
short- and long-term needs. As part of Net-Centric Test
Agent Capability (NTAC), a DEVS/SOA simulation-based
tool has been proposed for use in developing an automated,
simulation-based environment called ATII (Agent-based
Test Instrumentation Infrastructure) for testing services [3].
The requirements for the proposed simulation
environment include the use of data describing performance
at lower-levels of abstraction than individual invocations of
services, in order to determine if parameters describing the
conditions within the testing environment remain within

© 2010 SCS. All rights reserved. Reprinted here with permission.

their prescribed ranges throughout the duration of the test.
While the DEVS/SOA simulation environment [4] and the
Java Platform do not directly provide a means of gathering
this information from the surrounding network, a variety of
open-source and commercial-grade tools exist which offer
such capabilities. During the initial phase of development
of ATII, it was decided that a tool should be identified
which is capable of gathering the appropriate data during
testing, and that this tool should then be integrated into the
existing prototype implementation of ATII. This process
required examining each tool – and eventually each
approach towards integration – in terms of not only the
requirements for the simulation, but also the constraints
resulting from the Java Platform, DEVS/SOA, and the
existing design for the prototype.
Of course, there are plenty of established precedents for
integrating elements of a real-world network into a
simulation model. Works such as [5] and [6] describe
frameworks for simulating sensor networks in which the
simulated sensor nodes are able to interact with a network of
physical sensor nodes. Work has also been done on
integrating software components from real-world computer
networks into simulations; reference [7], as an example.
recounts an attempt to integrate the FreeBSD network stack
into the ns-2 network simulator. Many computer network
simulation tools, including OPNET Technologies' IT Guru
Network Planner [8] and OPNET Modeler [9], allow
simulation models to be parameterized using packet traces
or other data recorded through observations of a real-world
computer network. One recent study [10] evaluates the ns-2
simulation environment by comparing its performance
against an "emulation" environment, in which the ns-2
simulator is augmented through the use of packet sniffing to
retrieve ping packets sent through a real-world network.
However, one key consideration which differentiates our
task from all of the examples we have given is the fact that
DEVS/SOA, in addition to being SOA-based, is designed to
employ a general-purpose simulator, rather than to
exclusively support models from a particular application
domain. As a result, it was highly desirable throughout the
process of integration to confine all changes to the models
themselves, and leave the underlying simulator and
DEVS/SOA core unaltered.
We will begin by presenting some background
information relating to DEVS/SOA and discuss why ping
(ICMP echo) was chosen as the technology to be used in

Page 1

gathering the required performance information. We will
then discuss the integration of ping into ATII, with
particular attention paid to how the design of the existing
prototype and constraints derived from the use of Java and
DEVS/SOA affected the integrated design. We will
conclude with a summary of some of the key points touched
upon within the paper, as well as a brief discussion of other
potential applications for integrating tools into DEVS/SOA
and other distributed M&S approaches.
2.

BACKGROUND
The automated, simulation-based ATII testing
environment seeks to use DEVS/SOA to implement and
deploy models, referred to as “agents,” which serve as
simulated users during the testing process, invoking services
as dictated by their roles within the test plan [2].
Throughout the execution of the test plan, agents are
expected to gather data relating to the performance of the
services which they invoke, as well as relating to the
performance of the underlying network infrastructure.
While an existing class library could be used to support the
invocation of services, no direct means were available for
assessing the lower-level performance of the network from
within Java or DEVS/SOA. A set of tools which could
enable required activities such as assessing network host
availability and measuring latency were evaluated, with the
goal of identifying one that could be integrated into ATII.
Such a tool would not only have to be accessible from Java,
but it would also have to be able to adapt to the needs of
DEVS/SOA.
2.1.

DEVS/SOA
It is desirable to have simulators that can benefit from
service oriented computing capabilities. Simplified software
development with strong support for scalability offers key
incentives for building distributed simulators. In particular,
DEVS/SOA employs pre-built, robust, and standardized
software technologies. The key concept of software-as-aservice is used to build a SOA-based DEVS simulator. The
separation between models and simulators is maintained
from the perspective of the DEVS formalism – i.e., models
developed in different programming languages can be
guaranteed to be correctly executed by their target
simulators. DEVS/SOA supports simulating models that are
developed in DEVSJAVA [11]. These models can be
executed, for example using DEVSJAVA, xDEVS [12], and
Microsim/Java [4] simulators as alternative simulation
services. The use of publisher, subscriber, and broker
entities with standardized SOA infrastructure and
technologies simplifies integration of different DEVS
models and their execution.

2.2. Tools for assessing network health
Many technologies already exist for assessing network
conditions and the statuses of individual nodes, including
industry-wide standards such as the Simple Network
Management Protocol (SNMP) [13] or the “echo” portion of
the Internet Control Message Protocol (ICMP) [14], and
vendor-specific solutions such as Microsoft’s Windows
Management Instrumentation (WMI) [15]. In addition to
tools built around such technologies, one could also employ
a program such as tshark [16] (a command-line version of
the open-source packet-sniffing tool Wireshark) [17] to
derive the necessary performance information through
observations of existing network traffic. Another option is
to employ a pre-built library of classes which can be used to
gather the desired information, such as the open-source
libpcap [18] and WinPcap [19] libraries used by Wireshark.
While neither of these libraries are directly accessible from
Java, a variety of Java class libraries are available which
enable interacting with libpcap or WinPcap by means of the
Java Native Interface (JNI) [20], including the open-source
jNetPcap [21], jpcap [22], and Jpcap [23] [24] projects.
Eventually, we grouped the above solutions into four
categories, based on the underlying monitoring paradigm
which they employed; note that this is not intended to serve
as a thorough list of all possible monitoring solutions (and,
indeed, many tools exist which can support more than one
of these techniques), but is merely intended to indicate
which options were considered.
The categories we
employed were:





Ping (ICMP echo)
Packet sniffing (i.e., tshark, jpcap, Jpcap, and
jNetPcap)
SNMP (Simple Network Monitoring Protocol)
WMI (Windows Management Instrumentation)

Evaluating the available network monitoring solutions
required much more than simply determining if they were
capable of gathering the required information. It was also
necessary to consider the challenges involved in
implementing each solution within ATII, including those
anticipated when installing and configuring the software
needed to enable the monitoring solution, when designing
and implementing the Java classes that will serve as the
interface between the DEVS/SOA models and the
monitoring solution, when constructing new models and
modifying existing ones to incorporate the monitoring
solution into the ATII prototype, and when actually
applying the solution during the operation of ATII. These
and other factors, including the cost -- when applicable -- of
licensing software which implemented the monitoring
solution, would influence the decision as to which tool was
most appropriate. Some of the considerations were fairly
easy to assess, such as the ease or difficulty of rendering

Page 2

each monitoring solution accessible to the Java Virtual
Machine. Substantially more thought was required to assess
the amount of work needed to construct an interface capable
of enabling the models to easily interact with the monitoring
solution, as this assessment would have to identify which
specific capabilities of the monitoring solution would
actually be required by ATII. Issues relating to installing
and configuring required software, as well as the potential
impact of the tool on the runtime performance of the ATII
simulations, were among the first potential challenges
considered when assessing each tool, as they would likely
be impractical to overcome within the intended scope of the
integration efforts.
The Windows Management Interface was removed
from consideration at an early stage of the evaluation, as
few practical options were found for interacting with WMI
from the JVM, and enabling some portions of the required
data to be gathered through WMI might require substantial
configuration changes to be made to the servers hosting the
services (as well as constraining which Operating Systems
those servers could employ). SNMP was the next candidate
to be discarded; while some open-source libraries were
found which provided support for using SNMP under Java,
employing SNMP could again require that substantial
configuration changes be applied to the servers. An
additional characteristic shared by WMI and SNMP is that
they both provide capabilities which dramatically exceed the
scope of ATII's requirements; partly as a result, the
implementation of a robust solution for gathering the
required data through WMI or SNMP would have exceeded
the allotted timeframe for integrating a network monitoring
solution into ATII.
Eventually, ping (ICMP echo) and packet sniffing were
identified as the two candidate solutions best suited to the
needs of ATII. There were many points in favor of using
packet sniffing. For instance, if packet sniffing was used
exclusively on the machines which already hosted the agent
models, it would not require any new traffic to be
introduced into the network, and it would place no new
requirements on the machines hosting the services to be
tested. Also significant was the fact that, unlike aggregate
measurements derived from SNMP or WMI, or the
measurements produced by pinging, measurements from
packet sniffing could be directly correlated with individual
service invocations, potentially enabling a detailed diagnosis
of each service call that failed to meet performance
requirements (although it should be noted that the
requirements for ATII did not call for such a capability).
However, packet sniffing also carries an interesting
limitation relative to ping in that a "capture" (a sequence of
recorded packets) obtained from a single host is generally
insufficient for determining network latency. In order to
obtain the required information through packet sniffing, it
would likely be necessary to combine captures made at both

the machine hosting the agent and the server hosting the
service. This could present several challenges, including the
need to map the timestamps recorded at the two machines
onto a common time scale. However, a far more significant
concern in the case of ATII is the fact that capturing packets
at the server, and then transmitting the capture file back to
the agent, could potentially consume a non-trivial amount of
resources both at the server and across the intervening
network, which might significantly reduce the measured
performance of the service being tested. Finally, even if
captures were only created by the machines hosting the
agents, there is still a chance, depending on the volume of
traffic and the efficiency with which packets are captured
and analyzed, that the runtime performance of the
simulation may be significantly affected.
In comparison to packet sniffing, ping presents a
relatively lightweight and easily-implemented solution when
one considers the specific needs of the ATII project. In
comparison to the class libraries needed to support packet
sniffing, or the frameworks underlying SNMP or WMI, the
specifications for the echo portion of the ICMP protocol are
quite simplistic. Ping is also easy to deploy, and some
Operating Systems, such as most recent versions of
Microsoft Windows, even come pre-packaged with
implementations of both the client-side and server-side
portions of ping. It should be noted that ping, although well
suited for measuring latency, is a poor choice for attempting
to measure the usage or availability of bandwidth. In fact,
several common ping clients either do not directly allow the
user to define how frequently pings are sent (as is the case
with the ping client included in Windows XP [25]), or
prevent users without special privileges from sending pings
very frequently (as is the case with the ping client included
in the Linux iputils package [26]). While ping does require
additional packets (albeit very small ones) to be sent
through the network, ATII does not actually need for these
packets to be sent very frequently. This is partly because
ATII's application-layer measurements of service
performance implicitly provide a degree of information
regarding performance at lower layers of the network stack,
in that the aggregate performance within each layer must at
least be sufficient to theoretically permit the observed
service performance. In the end, it was determined that,
given the requirements for ATII, ping appeared to be both
the easiest solution to implement among those available, and
also the least demanding in terms of both its requirements
during deployment and setup and its resource consumption
during operation.
3.

INTEGRATING PING WITH SIMULATOR

3.1. Integrating Ping into Java
Once ICMP echo had been identified as the most likely
candidate for use in gathering the desired network

Page 3

performance information, the project team considered in
more detail how it could be accessed from within Java.
While the specifications for ICMP echo are not particularly
complicated, the prebuilt classes in the Java Platform
primarily support communications employing the transportlayer UDP or TCP protocols, and do not directly support
constructing and sending ICMP echo requests. As a result,
the team decided to use a preexisting tool which
implemented pinging, and manage invocations of that tool
through instances of Java’s Process class.
This was achieved through the creation of new Java
class, "Ping," which:
1.

2.

3.

4.

Builds an array of Strings containing the location
and name of the ping program, as well as the
arguments it should be provided;
Creates an instance of ProcessBuilder by supplying
the array of Strings to its constructor, then invokes
the ProcessBuilder's start() method – this causes
the JVM to invoke the ping program and return a
Process object which can be used to interact with
the resulting native process;
Reads from the stream of output produced by the
native ping process until that process has
terminated;
Parses the collected output.

Other techniques for interacting with the ping program
could also be employed: for instance, one could have the
native process redirect its output to a file, or design a new /
modified ping program with support for JNI.
3.2. Integrating Ping into the DEVS/SOA Agent Model
While accessing ping from Java was fairly
straightforward, more thought was needed when providing
that capability to the DEVS/SOA agent models. The
existing prototype of the project employed a blocking
method to invoke each service, which was called from the
agent model during an internal state transition. As a result,
the simulation (which, at least at this stage, was not
designed to support parallel-execution) would, if left
unchanged, be unable to respond to ping results in a timely
manner. While many solutions were available for resolving
the timing concerns of pings and service invocations, the
team primarily considered the following four scenarios:
A. Design a class outside of the DEVS framework,
which, after being instantiated and started by the
agent, will autonomously create Pings (using a
separate thread of execution) and store their results
until the agent tells it to halt.
B. Have the agent create and run each Ping through a
call to a single blocking method.

C. Have the agent create and run each Ping through a
call to a single non-blocking method (which creates
a separate Thread to handle the Ping), and then
have the Ping either:
1. invoke a callback method on the agent to
report the result of the ping, or
2. store the result of the ping until the agent
is ready to retrieve it.
D. Wrap a thread around the blocking method
currently used to invoke services, and either:
1. have the agent manage both types of
threads (service invocation threads and
Ping threads) directly, callback methods
used to report the result from each thread,
or
2. construct two classes outside of the DEVS
framework, one which is identical to that
described in choice A, and the other,
which is an analogous class used to
invoke services as separate threads.
One can find arguments in favor of each of the above
options. Obviously, one benefit shared by all of the choices
is that they do not require any changes to be made to
DEVS/SOA itself, or to the DEVSJAVA simulator on
which the models will be simulated. Choice A, while not
the simplest to program, requires very little interaction from
the agent model itself, and consequently would help
minimize the amount of changes to be made to the
prototype. Choice B is both relatively simple to program
and fairly straightforward to design, as it does not call for
any explicit use of multithreading within the prototype.
Choice C can be viewed, in some ways, as a compromise
between choice A and choice B; while it still is likely to
require more changes to be made to the model itself than in
the case of choice A, it at least partially retains support for
the parallel execution of pings and service calls. Choice D,
meanwhile, is capable of supporting parallel execution of
pings and service calls, and could be viewed as simplifying
the design of the agents themselves, while also paving the
way for an individual agent to potentially invoke multiple
services concurrently.
However, not all of these advantages are equally
important or relevant to the project itself. For instance, it is
expected that agents will only be called upon during testing
to invoke services in a sequential manner, rendering at least
one potential advantage of choice D unimportant. Some of
the choices also contain drawbacks which may not be
immediately apparent. Consider choice B: while the
decision to not explicitly enable concurrent execution of
service calls and pings seems quite inefficient, this does not,
in itself, mean that choice B is inherently “bad” or
“useless.” However, given that ping is being envisioned as
a fully-user-configurable feature within the final version of

Page 4

ATII, if users were to observe that tests ran noticeably and
consistently faster with pinging disabled, it would present a
strong incentive for them to disable pinging altogether,
which would dramatically reduce the effectiveness of the
ping feature in fulfilling its intended purpose. Also, while
the project will not require ping to gather extensive amounts
of information or to report that information in a timely
manner, the project’s requirements for service invocations
are much stricter. Due to the greater importance placed
upon service invocations relative to pings, it was decided
that choice A, which provided timely results to the agent
regarding service invocations, while allowing the agent to
retrieve ping results at its leisure, would be employed. As
has already been mentioned, this option required relatively
few changes to be made to the agent model; this was
significant, as it would also make it much easier to disable
pinging within the final version of the model when so
desired by the user. At the same time, in comparison to
choice C, this approach was less likely to require pinging to
be significantly delayed while waiting for service calls to
complete.
3.3. A Ping DEVS/SOA Simulator Design with
Implementation

Figure 1 provides a high-level summary of several of
the key classes and interfaces which were used to integrate
Ping into the DEVS/SOA simulator in accordance with the
design described in choice A; these classes and interfaces
were separated into two packages, "ping" (which was not
considered part of the DEVS framework), and "devs." In
addition to the items shown in Figure 1, we used two other
classes (both belonging to the "ping" package): PingResult,
which stores the parameters and outcome of a single ping
invocation, and PingAlert, which describes a single
violation of the prescribed minimum level of network
performance. The classes and interfaces presented in the
figure are as follows:








Figure 1: Partial class diagram of Ping-DEVS/SOA
Integration



PingListener and PingSender: these interfaces are
used to enable an object (a PingSender) which
produces pings, to send synchronous updates to
other objects (PingListeners) which are interested
in receiving the corresponding PingResults. Note
that these interfaces should not be confused with
the client-side and server-side portions of the
ICMP echo protocol.
Ping: as described in section 3.1, Ping is
responsible for launching the external ping
program, reading the program's output, and then
parsing that output.
Ping implements the
PingSender interface, and is listened to by a single
PingController.
PingController:
PingController
is
an
implementation of the class described in the
definition for choice A. PingController, once
started, will launch a new thread of execution
which is used to schedule and run Pings, and
preserve their PingResults, until the PingController
is halted. The PingController is started and
stopped using the public startPinging() and
stopPinging() methods (which, for simplicity, are
not shown in Figure 1).
PingController
implements both the PingListener and PingSender
interfaces: it is a PingListener from the perspective
of its Pings, and it is a PingSender from the
perspective of the surrounding simulation model.
PingEvaluator: this interface is used to interact
with an object which is able to receive and process
PingResults, and then determine if those results
indicate a violation of prescribed levels of network
performance. The checkForAlerts() method is
intended to return all PingAlerts which have
accumulated since the last time the method was
invoked.
SimpleRTTEvaluator – As the name implies, this
basic implementation of PingEvualator compares
the round-trip-time in each PingResult to a
threshold value, and produces a PingAlert if that

Page 5



threshold was exceeded (or if the ping program
timed-out or produced an error). In addition to
implementing PingEvaluator, SimpleRTTEvalutor
also implements PingListener; this allows
SimplePingAtomic
to
instruct
the
SimpleRTTEvaluator to subscribe directly to the
PingController, rather than having to manually
relay PingResults from the PingController to the
SimpleRTTEvaluator.
SimplePingAtomic – a DEVS atomic model which
uses a PingController and a PingEvaluator (such as
SimpleRTTEvaluator) to monitor a single remote
host. This model starts its PingController when it
receives a "start" message, stops its PingController
when it receives a "stop" message, and checks for
alerts during internal transitions; when alerts our
found, it sends them through its output port
following
a
brief
internal
transition.
SimplePingAtomic implements PingListener, but
will only subscribe to the PingController if it
detects that the PingEvaluator is incapable of
subscribing to the PingController directly.

Several of the components presented in Figure 1 were
deliberately intended to lend a degree of modularity to the
overall design of the ping package.
While the
SimplePingAtomic model is clearly an implementation of
the previously-mentioned choice A, the underlying classes
could be reused to implement any of the alternative designs
discussed in section 3.2. For example, Ping could still be
used directly by a model without the need for a
PingController, and the data which PingSenders
synchronously supply to their PingListeners could also be
retrieved asynchronously through other public methods. It
should also be emphasized that, outside of the
SimplePingAtomic model, all of these classes and interfaces
belong to the ping package, and reside outside of the DEVS
framework. The flexibility which resulted from these
decisions was especially helpful in the ATII project, as
additional applications for the ping capability were
eventually identified. Figure 2 demonstrates a very early
prototype of one such application, in which pinging is used
to monitor a remote host prior to deploying the agent
models and initiating the test. The simple GUI shown in
Figure 2 is underpinned by a single instance of
PingController, and displays some of the parameters used
by that PingController, as well as a summary of the
outputted PingResults. Note that this exact same GUI could
be very easily reused to locally monitor the PingController
of a SimplePingAtomic model during an ongoing
simulation.

Figure 2. Simple GUI for Configuring and Observing a
PingController
4.

CONCLUSION
The concept of incorporating capabilities such as
“network health” monitoring into a simulation environment
can be quite important. Other simulators [27, 28] can also
benefit from the proposed concept and approach for
supporting simulation executions that are informed about
network health, and thus support simulation of the kinds of
scenarios needed for simulating a net-centric environment
[29].
It is also important to note that the approach we have
demonstrated can be used with other tools aside from just
ping. While only one of the candidates described in section
2.2 turned out to be a good fit for the ATII project, other
projects with different requirements for network monitoring
could likely benefit from integrating a different tool, such as
tshark, into the simulation environment. Also, as we had
recently implied, this integration does not need to be
designed to support just one specific project; indeed, while
our integration of ping into DEVS/SOA was driven by the
needs of ATII, the resulting classes we have created can still
be easily reused by other models. It is our belief that
integrating such tools into a simulation environment while
non-trivial even in the case of a simple tool like ping – has
the potential to greatly expand the common capabilities of
the simulation environment, and render it more useful for
applications like the ATII project.
Acknowledgement
We extend our thanks to Dr. Bernard P. Zeigler from
the University of Arizona and Dr. Doohwan Kim and Dr.
Chungman Seo from RTSync Corporation for their support.
References
[1]. US Department of Defense. 13 May 2003. Department
of
Defense
Instruction
5000.61,
Enclosure
3.
http://www.dtic.mil/whs/directives/corres/pdf/500061p.pdf.
[2]. Zeigler, B. P., and P. E. Hammonds. 2007. Modeling &
Simulation-Based Data Engineering: Pragmatics into
Ontologies for Net-Centric Information Exchange, Elsevier.
[3]. Zeigler, B. P.; D. H. Kim; S. Seo. 2009. Personal
Communication.
[4]. Mittal, S.; J. L. Risco-Martín; B. P. Zeigler. 2009.
“DEVS/SOA: A Cross-platform Framework for Net-centric

Page 6

Modeling and Simulation in DEVS Unified Process,”
Simulation Transactions, Vol. 85, No. 7, 419-450.
[5]. Wenn, Y.; W. Zhang; R. Wolski; N. Chohan. 2007.
"Simulation-Based Augmented Reality for Sensor Network
Development," Proceedings of the 5th International
Conference on Embedded Networked Sensor Systems,
(Sydney, Australia, Nov. 6-9). ACM, New York, NY, 275288.
[6]. Park, S.; A. Savvides; M. B. Srivastava. 2000.
"SensorSim: a Simulation Framework for Sensor
Networks," Proceedings of the 3rd ACM International
Workshop on Modeling, Analysis and Simulation of
Wireless and Mobile Systems, (Boston, MA, Aug. 11).
ACM, New York, NY, 104-111.
[7]. Jansen, S., and A. McGregor. 2005. "Simulation with
Real World Network Stacks," Proceedings of the 37th
conference on Winter Simulation, Orlando, FL, 2454-2463.
[8]. OPNET Technologies, Inc. 2009. "Network Planning |
Network Optimization," OPNET Technologies, Inc.
http://www.opnet.com/solutions/network_planning_operatio
ns/itguru_network_planner/index.html.
[9]. OPNET Technologies, Inc. 2009. "Network Modeling |
Network Simulation," OPNET Technologies, Inc.
http://www.opnet.com/solutions/network_rd/modeler.html.
[10]. Shin, J. Y.; J. W. Jang; J. M. Kim. 2009. "Result Based
on NS2, Simulation and Emulation Verification," 3rd
International Conference on New Trends in Information and
Service Science, Beijing, China, 807-811.
[11]. ACIMS. 2009. DEVSJAVA modeling and simulation
tool. http://www.acims.arizona.edu/SOFTWARE.
[12]. Risco-Martín, J. L. "xDEVS (DEVS library)."
https://sourceforge.net/projects/xdevs/.
[13]. Case, J.; M. Fedor; M. Schoffstall; J. Davin. 1990.
“Request for Comments 1157: A Simple Network
Management Protocol (SNMP).” SNMP Research,
Performance Systems International, Massachusetts Institute
of Technology Laboratory for Computer Sciences.
http://tools.ietf.org/html/rfc1157.
[14]. Postel, J. 1981. “Request for Comments 792: Internet
Control Message Protocol.” University of Southern
California
–
Information
Sciences
Institute.
http://tools.ietf.org/html/rfc792.
[15]. Microsoft Corporation. 2009. “About WMI,”
Microsoft
Corporation.
http://msdn.microsoft.com/
en-us/library/aa384642(VS.85).aspx.
[16]. Wireshark Foundation. 2009. “tshark – The Wireshark
Network Analyzer 1.3.1,” Wireshark Foundation.
http://www.wireshark.org/docs/man-pages/tshark.html.
[17]. Wireshark Foundation. “About Wireshark,” Wireshark
Foundation. http://www.wireshark.org/about.html.
[18]. tcpdump.org. 2009. "TCPDUMP/LIBPCAP public
repository," tcpdump.org. http://www.tcpdump.org/.

[19]. WinPcap.org. 2009. "WinPcap, The Packet Capture
and Network Monitoring Library for Windows,"
WinPcap.org. http://www.winpcap.org/.
[20]. Sun Microsystems, Inc. 2006. "Java Native Interface,"
Sun Microsystems, Inc. http://java.sun.com/javase/6/docs/
technotes/guides/jni/.
[21]. Sly Technologies, Inc. 2009. "jNetPcap OpenSource |
A Libpcap/WinPcap Wrapper," Sly Technologies, Inc.
http://jnetpcap.com/.
[22]. Charles, P. 2004. "jpcap network packet capture
library." http://jpcap.sourceforge.net/.
[23]. Fujii, K. 2007. "Jpcap – a Java library for capturing
and sending network packets." http://netresearch.ics.uci.edu/
kfujii/jpcap/doc/.
[24].
Fujii,
K.
2007.
"Jpcap
FAQ."
http://netresearch.ics.uci.edu/kfujii/jpcap/doc/faq.html.
[25]. Microsoft Corporation. 2010. "Microsoft Windows XP
- Ping," Microsoft Corporation. http://www.microsoft.com/
resources/documentation/windows/xp/all/proddocs/enus/ping.mspx?mfr=true.
[26]. Woolley, S. 2002. "ping," Seth Woolley's Man
Viewer. http://swoolley.org/man.cgi/ping.
[27]. Tsai, W. T.; Z. Cao; X. Wei; R. Paul; Q. Huang; X.
Sun. 2007. “Modeling and Simulation in Service-Oriented
Software Development,” Simulation Transactions, Vol. 83,
No. 1: 7-32.
[28]. Sarjoughian, H.S.; S. Kim; M. Ramaswamy; S.S. Yau.
2008. “A Simulation framework for Service-oriented
computing Systems,” Winter Simulation Conf., Miami, FL,
USA, 845-853.
[29]. Gibbs, J.D., and H.S. Sarjoughian. 2009. “Assessing
the Impact of a Modeling Tool and its Support for
Verification and Validation,” International Symposium on
Performance Evaluation of Computer and Teleco. Systems,
Istanbul, Turkey, 73-80.

Page 7

Proceedings of the 2015 Winter Simulation Conference
L. Yilmaz, W. K. V. Chan, I. Moon, T. M. K. Roeder, C. Macal, and M. D. Rossetti, eds.

MODEL-ARCHITECTURE ORIENTED COMBAT SYSTEM EFFECTIVENESS
SIMULATION
Yonglin Lei,
Ning Zhu, Jian Yao, Zhi Zhu

Hessam S. Sarjoughian

Simulation Engineering Institute,
109 Yanwachi Rd, National University of Defense
Technology, Changsha, Hunan, CHINA

Arizona Center for Integrative Modeling & Simulation,
699 S. Mill Avenue, Arizona State University,
Tempe, AZ, 85281, USA

ABSTRACT
Combat system effectiveness simulation (CESS) is a special type of complex system simulation. Three
non-functional requirements (NFRs), i.e. model composability, domain-specific modeling, and model
evolvability are gaining higher priority from CESS users when evaluating different modeling methodologies for CESS. Traditional CESS modeling methodologies are either domain-neutral (lack of domain
characteristics consideration and limited support for model composability) or domain-oriented (lack of
openness and evolvability) and fall short of the three NFRs. Inspired by the concept of architecture in
systems engineering and software engineering fields, we extend it into a concept of model architecture
for complex simulation systems, and propose a model-architecture oriented modeling methodology in
which model architecture plays a central role in achieving the three NFRs. Various model-driven engineering (MDE) approaches and technologies, including SMP, UML, DSM, and so forth, are applied
where possible in representing the CESS model architecture and its components’ behaviors from physical
and cognitive domain aspects.
1

INTRODUCTION

Combat system effectiveness simulation (CESS) is a special type of complex system simulation. Generally, there are two kinds of requirements for simulation modeling. One is functional requirement (FR), i.e.
what functionalities should be provided by the simulation models. Typical concerns include what elements and relations should be taken into consideration; what measures should be evaluated based on the
simulation outputs; what precision should be supported in representing certain variables; etc. The other is
non-functional requirement (NFR), i.e., how well the simulation models are structured and represented.
There are many possible criterions for judging “how well”. Any validated simulation model must have
met the NRs, but not necessarily NFRs. Usually, NFRs are the main focus where one modeling methodology is preferred over others. In CESS field, three such kinds of criterions or NFRs are found significant
and prioritized by the users and experienced modelers, i.e. model composability, domain-specific modeling, and model evolvability.
Traditional CESS modeling methodologies can be roughly divided into two categories. One is domain-neutral and application-specific by using generic M&S technologies and providing a powerful infrastructure and a model library with many domain components inside. Examples include simulation protocol standards, like HLA and SOA; model specification standards, like BOM and SMP2; unified/universal
modeling formalisms, like DEVS and UML. The second category can be called domain-oriented by
providing a CESS-oriented simulation system, within which different simulation applications can be
composed from built-in components and configured with application-specific parameter values. Prominent examples include EADSIM, FLAMES, OneSAF, NSS, etc. The limitations of domain-neutral methodologies lie in lack of domain characteristics consideration and limited support for model composability;
whereas domain-oriented methodologies usually fall short of openness and evolvability.

978-1-4673-9743-8/15/$31.00 ©2015 IEEE

3190

Lei, Zhu, Yao, Zhu, and Sarjoughian
Inspired by the concept of architecture in systems and software engineering fields, we extend it into a
concept of model architecture for complex simulation systems, and propose a model-architecture oriented
modeling methodology in which model architecture plays a central role in achieving the three NFRs. Various model-driven engineering (MDE) approaches and technologies, including SMP, UML, DSM, EMF,
GMF, etc., are applied where possible in representing the CESS model architecture and its components’
behaviors from physical and cognitive domain aspects.
2

FROM ARCHITECTURE TO MODEL ARCHITECTURE

Architecture is the “Fundamental concepts or properties of a system in its environment embodied in its
elements, relationships, and in the principles of its design and evolution.” A complex CESS is a system.
Its architecture would include elements like a simulator, simulation services, and simulation models; and
relationships between simulator and simulation models, and those among different simulation models as
schematically shown in the left part of Figure 1. Since the simulator, simulation services, and relationships between simulator and simulation models are largely determined by the M&S technology chosen,
the remaining complexity will mainly lie in simulation models and their relationships. Following the definition of architecture of a system, we define model architecture of a simulation system as follows:
Model architecture of a simulation system is the fundamental concepts or properties of the simulation system in its execution environment embodied in its model components, their relationships, and principles of building or evolving these models.
In a sense, model architecture is the kernel of a CESS-like complex simulation.
Model architecture is also a key to resolve
the NFRs emerged from CESS and other
complex simulations. To achieve this, the
model architecture is further divided into a
domain model architecture (DMA) and many
possible application model architectures
(AMAs). Each AMA will reuse the DMA
Figure 1 from simulation’s architecture to its model architecture
by either customizing or extending.
3

MODEL ARCHITECTURE-ORIENTED CESS MODELING METHODOLOGY

From the viewpoint of model architecture, traditional CESS modeling methodologies can be structured as
in Figure 2 (a, b) with four logic layers generic to simulation modeling, including experiment, simulation
application, simulation application environment (SAE), and simulation development environment (SDE)
layer. Model-architecture oriented methodology (see the c part of figure 2) basically follows ideas behind
domain-oriented methodology and incorporates powerful M&S technologies found in domain-neutral
methodology and make three steps further: 1) Building complex simulations around model architecture. 2)
Making domain abstraction in model architecture. 3) Applying MDE to model architecture and its components’ behaviors.

a) Domain-neutral methodology

b) Domain-oriented methodology

c) Model architecture-oriented methodology

Figure 2 Traditional CESS modeling methodologies from the viewpoint of model architecture

3191

Proceedings of the 2008 Winter Simulation Conference
S. J. Mason, R. R. Hill, L. Mönch, O. Rose, T. Jefferson, J. W. Fowler eds.
A SIMULATION FRAMEWORK FOR SERVICE-ORIENTED COMPUTING SYSTEMS

Hessam Sarjoughian
Sungung Kim
Muthukumar Ramaswamy
Stephen Yau
Arizona Center for Integrative Modeling and Simulation
School of Computing and Informatics
Arizona State University, Tempe, Arizona, 85271-8809, USA

technical requirements and architectural design of a service-based system. Such models may, for example, represent dynamics of the services and their interactions in order
to study the system’s capability to support the quality of
service attributes such as performance, timeliness, accuracy, and security.
To design service-based software systems capable of
satisfying multiple Quality of Service (QoS) attributes, simulation-based modeling is desirable. For instance, in the
context of our research (Yau et al. 2008), simulation plays
a central role in enabling tradeoff study among time-based
quality of service attributes. The basic need is to have an
Adaptive SBS (ASBS) where its QoS can be observed by a
Monitoring system and controlled by an Adaptation system. The users can select services and list their expected
QoS under the presence of some uncontrollable, but predictable environmental fluctuations. To develop the ASBS
framework – design, implement, and test the Monitoring
and Adaptation systems – we can develop a set of real
composite and simulated services. Together, real and simulated services enable analysis and design capabilities that
are impractical to support by either real or simulated services alone. With simulated services, the Monitoring and
Adaptation systems can themselves be real services and
thus support carrying out varying kinds of experimentations.
In order to develop and use simulated services, it is
important to have a modeling and simulation framework
that is theoretically sound, has one or more robust implementations, and is simple to use. One such framework is
the Discrete Event System Specification (DEVS) formalism (Zeigler, Praehofer, and Kim 2000) with implementation such as DEVSJAVA (ACIMS 2001). This modeling
formalism is positioned to specify and simulate SOAcompliant simulation models.

ABSTRACT
An SOA-compliant DEVS (SOAD) simulation framework
is proposed for modeling service-oriented computing systems. A set of novel abstract component models that conform to the SOA principles and are grounded in the DEVS
formalism is developed. The approach supports construction of hierarchical composition of service models with
feedback relationships. A SOAD Simulator (SOADS) is
designed and implemented. An exemplar model of a basic
service-oriented computing system is described. A representative experiment capturing throughput and timeliness
QoS attributes for the exemplar model is devised, simulated, and described. The paper concludes with the concept of community-based development of the SOAD
framework and tools.
1

INTRODUCTION

Many of today’s computer-based systems are challenging
to build since they are distributed and operating in changing environments. A central requirement for a system is to
be flexible in that its parts are loosely coupled while the
system as a whole can satisfy quality attributes known as
run-time (e.g., performance and availability) and non-runtime (e.g., reusability and integrability) observable. To
build such systems, service-oriented computing paradigm
based on Service Oriented Architecture (SOA) framework
has been proposed (Y. Chen and Tsai 2008; Erl 2006).
To achieve the goals set forth for service-oriented
computing, a growing number of researchers are formulating detailed concepts, methods, and techniques that can be
used to build service-based systems. The most common
approach in defining a system’s structure and behavior is
to develop models. The choice of a model is driven by the
role it can play in the system development and operation
lifecycle. For example, a model can be at the architectural
level or be complete and sufficiently detailed to be automatically implemented. Models can be developed to define

978-1-4244-2708-6/08/$25.00 ©2008 IEEE

845

Sarjoughian, Kim, Ramaswamy, and Yau
2

MOTIVATION

Simulator is developed to support Items 1 and 2. The advanced concepts and capabilities contained in Items 3 and
4 can be introduced into SOADS to simulate systems having dynamic structures and complex behaviors arising from
mixed software and hardware interactions.

There exist basic differences between SOA principles and
the underlying concepts of general-purpose modeling and
simulation frameworks. They do not account for the SOA
concepts and principles such as service autonomy and
loose coupling. Based on this observation and the anticipated growth in simulating service-oriented software systems, we propose developing an SOA-compliant simulation framework. A suitable modeling framework is DEVS.
A set of generic model abstractions for services and their
relationships are needed. The simulation of the models
should capture the inherent properties of SOA-compliant
software systems. The simulation framework must have a
set of SOA elements (i.e., publisher, subscriber, broker
services, and messages) and relationships (e.g., subscriber
can discover published services only via a service broker)
that comply with the SOA principles. The resulting SOAcompliant simulation framework can support creating different user-specific simulation models that are built on the
top of verifiably correct SOA model components.
Before we proceed further, we note that simulation of
an SOA-based software system should account for both
software and hardware aspects. The software aspect refers
to the core SOA principles. The concept of an SOAcompliant DEVS Simulator is based on partitioning the
core SOA principles into two parts called simple and complex. The term simple is used to refer to service-based systems that cannot have services added/removed at run-time;
there is limited support for loose coupling. The term complex is used to refer to service-based systems that can have
their structures changed at run-time. Therefore, separation
of the SOA principles into simple and complex parts helps
build the proposed SOAD framework in two stages. The
simple part focuses on the autonomy, abstraction, service
contract, reusability, composability, and statelessness principles. The complex part focuses on the loose coupling and
discoverability principles. The hardware aspect refers to
physical and non-service components that are responsible
for the execution of services and their interactions. Modeling of hardware – i.e., a collection of computing nodes
(processors and routers/switches) and network links – is
essential for capturing the dynamics of the services. This is
because hardware components responsible for the execution of the services and communication of messages directly impact QoS attributes.
The above considerations lead us to the development
of the SOAD framework and realization where (1) Models
represent the static software aspect of the SOA capabilities,
(2) Simple models of the hardware accounting for communication delay and bounded data transmission volume are
used, (3) Models defined in (1) are extended to represent
the dynamic aspect of the SOA capabilities, and (4) Models defined in (2) are extended to represent details of computing nodes including routing devices, communication
links, and protocols are used. In this paper, a basic SOAD

3

BACKGROUND

The DEVS and SOA share important concepts even though
their uses are intrinsically different – one is intended to
build service-oriented software systems and the other to
simulate component-based systems. Their commonality
lies in their view of (i) software or simulated systems to be
either flat or hierarchical, (ii) feed forward and feedback
interactions, and (iii) sequential and parallel execution.
However, these system-level concepts have different abstractions. The SOA framework’s abstractions are relatively at a higher level compared with those of the DEVS
framework. The basic concepts, principles, and artifacts of
these frameworks are described next. Some of their main
similarities and differences are also exposed.
3.1

SOA Framework

The desire for enterprise systems that have flexible architectures, detailed designs, implementation agnostic, and
operate efficiently continues to grow. A major effort toward satisfying this need is to use Service Oriented Architecture. Moreover, there is new research and development
in order to achieve more demanding capabilities (e.g.,
workflow service composition with run-time adaptation to
changing QoS attributes) that have been proposed for service-based systems, especially in the context of system of
systems.
A basic concept is for SOA to enable specifying the
creation of services that can be automatically composed to
deliver desired system dynamics while satisfying multiple
QoS attributes. The principal artifacts of SOA are publisher, subscriber, and broker services (Erl 2006). The
communication protocols for these general-purpose services are supported with WSDL, UDDI, and SOAP
(Møller and Schwartzbach 2006). The publisher and subscriber services are also sometimes referred to as provider
and requester, respectively. A publisher registers its service
descriptions (WSDL) with the broker service and a subscriber can find services it is searching for if they are registered with a broker. The broker uses its service registry using UDDI to identify matched service descriptions. Then, a
subscriber can invoke a publisher and obtain the requested
service. The message interactions among the services are
supported by the SOAP mechanism.
A fundamental SOA concept is to enable flexible
composition of independent services in a simple way. The
simple concept is crucial since it separates details of how a
service is created and how it may be used. This kind of
modularity is defined based on the concept of brokers and
2

846

Sarjoughian, Kim, Ramaswamy, and Yau
defined in terms of its constituent atomic and/or coupled
models. A coupled model can be constructed by composing models into hierarchical tree structures, and is defined
in terms of its constituent (atomic and/or coupled) models.
The input and output sets X and Y have the same specification as those of the atomic model. D is a set of component
names and Md is a set of atomic and/or coupled components, and EIC, EOC, and IC are external input, external
output, and internal couplings, respectively. The behavioral
semantics of the DEVS models are defined in atomic and
coupled abstract simulation protocols. The execution ordering of the atomic model functions is determined by the
atomic simulator. Similarly, the transmission of the messages among the atomic and coupled models is determined
by the coupled simulator. One of the object-oriented realizations of the DEVS formalism and its associated simulation protocol is DEVSJAVA.

its realization as the broker service. The SOA conceptual
framework lends itself to the separation of concerns ranging from application domains (e.g., business logic) IT infrastructure to the choices of programming languages and
operating systems. The interoperability at the level of services means loose coupling of reusable services.
The high-level description of the SOA principals does
not account for the operational dynamics of SOA, especially with respect to time-based operations. Therefore,
understanding the dynamics of a service-based system using simulation is important. Simulation can also support
specific kinds of service-based software systems that are
targeted for business processes with specialized domain
knowledge. For example, given the steps in creating a service (e.g., defining service capabilities, selecting services,
specifying service flows, and deploying services) they may
be supported with component-based, scalable, and efficient
simulation. A simulation framework capable of modeling
SOA-compliant software systems offers a basis that can be
extended to conceptualize and evaluate interesting aspects
of higher-levels of services (e.g., automated service composition) for different application domains.
3.2

3.2.1 Dynamic Structure and SW/HW Models
The basic atomic and coupled models are not sufficient for
modeling the kinds of SOA complexities that need to be
simulated. For example, to model the addition or removal
of services at run-time, it is important for a DEVS simulation model to change its structure dynamically, which can
be by adding or removing atomic and coupled models.
This concept is known as variable or dynamic structure
DEVS (Zeigler et al. 2000). One realization of this concept
is called Dynamic Structure DEVS, which at its core has
an Executive model component with rules for adding and
deleting model components during simulation (Barros
1997). Another important contributor to the complexity of
SOA is the dependency on hardware. While atomic and
coupled models can represent the software aspect of a service, it is also important to model the hardware aspect of
the resources (e.g., processors, switches, and network
links) on which the services execute and interact. To model
both software and hardware aspects and the mapping of the
former to the latter, the DEVS/DOC, a software/hardware
co-design approach has been developed (Hild, Sarjoughian,
and Zeigler 2002). In this environment, disparate software
components executing on distributed hardware components
can be modeled and simulated. This approach supports
quantum level abstraction of software and hardware components.

DEVS Framework

Simulation is considered useful and increasingly indispensible across all phases of system development lifecycle
(i.e., conceptualization, design, implementation, deployment, and operation). This observation applies to servicebased systems since component-based simulation and service-based systems are based on fundamental concepts of
components and their interactions. A component-based
modeling framework such as (DEVS) is well positioned to
create model abstractions for service-based systems. The
SOA principles including autonomy, composability, and
reusability of services with message-based interactions fit
well the DEVS modeling formalism. This is because the
dynamics of a typical SBS system can be characterized in
terms of time-based modular and hierarchical reactive simulation model components. These simulation model
components can process input events (messages) and generate output events (messages). The DEVS formalism provides abstract formulation for describing concurrent processing and the event-driven nature of arbitrary system
configurations and executions. Parallel atomic/coupled
DEVS models can be executed in distributed settings (including grid services), and therefore is a suitable modeling
framework to characterize complex, large-scale servicebased systems.
An atomic model (formalized as 〈X, S, Y, δext, δint,
δconf, λ, ta〉) characterizes the structure and behavior of individual components in terms of inputs (X), outputs (Y),
states (S), and functions. The external (δext), internal (δint),
confluent (δconf), output (λ), and time advance functions
(ta) define a component’s behavior over time. A coupled
model (formalized as 〈X, Y, D, {Md}, EIC, IC, EOC〉) is

4

SOAD FRAMEWORK CONCEPT

Earlier, we described the details of the SOA and DEVS
frameworks. An important consideration in choosing a
modeling and simulation framework is its direct support
for message-based communication among independent
model components. This is important since the concept of
SOA is grounded in autonomous services that can only influence each other via messages. The combination of publisher and subscriber interaction via messages matches well
3

847

Sarjoughian, Kim, Ramaswamy, and Yau
• The concept of autonomous services corresponds to
the concept of modularity of atomic and coupled
models. DEVS models are defined in terms of generic
transition (δext, δint, δconf), output (λ) and time advance (ta) functions.

the strict modularity of the DEVS framework. Furthermore, as noted above, the capability for parallel simulation
of services with arbitrary combined feed forward and feedback message flows has been a key consideration in the selection of the DEVS framework in this research.
The basic idea for the SOAD framework is to enable
modeling and simulating primitive and composite services
as if they were real as in actual services. The concept of
simulated services is distinct from that of simulated objects. The DEVS and object-orientation concepts, compared to DEVS and SOA, are closely related. The SOA is
defined in terms of principles that are intended to guide architecture, design, implementation, testing, and operation
of service-based systems. These principles may be used to
develop details of SOA which can result in different realizations both for the SOA itself as well as user applications.
The DEVS formalism, on the other hand, is a mathematical
specification intended for developing time-based models
that can be simulated. We also note that while atomic and
coupled models require abstract atomic and coupled simulators in order to be executed, the services (publisher, subscriber, and broker) contain their own execution logics.
Given the disparities between DEVS and SOA frameworks, our aim is to develop a framework for SOAD. Two
basic approaches can be taken. One is to infuse the concept
and capabilities of DEVS concepts and capabilities into the
SOA framework. The other is to extend the DEVS framework such that it can account for the SOA concept and capabilities. In this work, we choose the latter approach.
Before we consider the SOA and DEVS frameworks
together, it is important to recall that one is intended to
build real services and the other to build simulated services. In this section, DEVS framework refers to DEVS
with Dynamic Structure capability. Also, it is useful to appreciate that while a primitive (subscriber or publisher) service and atomic model can be considered as components
(or objects), their underlying concepts are inherently distinct. Furthermore, the concept of a composite service (either as publisher or subscriber) differs from that of a coupled model. The following reveals similarities and
differences between the SOA and DEVS frameworks.

• The formal contract corresponds to the input/output
ports and messages (X and Y), and their couplings
(EIC, EOC, IC) subject to the strict coupled model
specification. The couplings in DEVS are fixed, although the use of coupling in a simulation can be decided during simulation. The concept of coupling
components via ports is absent in SOA.
• The concept of service composability is similar to
coupled model hierarchy. SOA composability is not
constrained to have strict hierarchy. This is because
DEVS hierarchy requires strict tree structure relationships among (atomic and coupled) model components. In SOA, composability is based on the broker
service which is not defined in DEVS. In DEVS, input and output messages are sent and received via direct couplings – i.e., the coupled model contains the
coupling relations between model components.
• The concept of abstract logic in DEVS has a theoretical basis (abstract structural and behavior syntax with
operational semantics) whereas SOA does not. For
example, δext has template syntax that has to be
completed given a component’s specific functions. In
contrast, a service has an interface template, but
without functionality.
• The basic concept of reusability in SOA is more powerful than that of DEVS. This is because the broker
concept with support for publishing services and
identifying services are not defined in DEVS.
• The concept of stateless service promotes loose coupling of composite services. Atomic/coupled model
components require state information which includes
time t (t ∈ S) in order to allow input and output event
synchronization.
The concepts of loosely coupled and discoverable services are similar to dynamic structure DEVS where the
structure of a model can change during simulation execution – i.e., capability is provided for adding and removing
atomic and coupled models. The concept of executive in a
dynamic structure resembles that of a broker service, but it
is not the same as described above. As noted earlier, the
fundamental difference between DEVS and SOA is the
‘broker’ concept. The message-based interactions between
the publisher and subscriber services can only be established by the broker service. The concept of broker is not
defined in the DEVS formalism and thus the DEVS atomic
or coupled components are not service-enabled – i.e., the
generic syntax and semantics of the atomic and coupled
components are insufficient for describing service-based

Broker
Service

3

2

Subscriber
Service

publish, request,
response messages

1

5

data service messages
4

Publisher
Service

Figure 1: Basic SOA services and messaging patterns

4

848

Sarjoughian, Kim, Ramaswamy, and Yau
Table 1: DEVS and SOA elements.

software systems. Furthermore, the SOA is not the same
as dynamic structure DEVS even though the structure of a
coupled model can be modified during simulation.
5

SOA Model Elements
services (publisher, subscriber, broker)
service description
messages

SOAD SIMULATOR FRAMEWORK

In the previous section, we described that there are basic
similarities and differences between the SOA elements and
those of DEVS. SOA framework has a higher level of abstraction as compared with DEVS framework. The basic
SOA model elements can be divided into two groups. First,
services, service description, and messages represent the
‘static’ part of SOA. Second, communication agreement,
messaging framework, and service registry and discovery
represent the ‘dynamic’ part of the SOA. To create the
SOAD Simulator (i.e., a generic SOA-complaint simulator), counterparts of the basic elements of SOA are needed.
As shown in Table 1, we have defined a set of DEVS elements that represent the static and dynamic aspects of the
SOA. Three DEVS atomic models are proposed. Three of
these have a one-to-one correspondence with the SOA services. The generic DEVSJAVA entity class is extended to
represent SOA service description. Entity is also extended
to represent SOA messages.
The publisher, subscriber, and broker services are the
basic elements for both service-oriented software systems.
The services can be synthesized to form primitive and
composite service composition. Next, these two service
compositions are described. A simple model of a network
is used to complement the software aspect of SOA with the
hardware aspect. It is defined as a link with finite capacity,
transportation delay, and FIFO message queuing. This
component is not a service – it models the medium through
which services send and receive messages.

messaging framework
service registry & discovery
service composition

SOAD Model Elements
atomic models (publisher, subscriber, broker)
entity (service-information)
entity (service-lookup & service-message)
ports & couplings
executive model
coupled models (primitive and
composite)

Publisher/Subscriber with Broker Coupled Model
foundpublisher

identifypublisher
Broker

publishservice
msg

Subscriber

requestservice

publishservice

publishservice

requestservices

msg
request and response messages
data service messages

5.1

Identifyidentifypublisher
publisher

Foundpublisher

publishservice
Publisher

input port

output port

publish messages

Figure 2: SOAD primitive service composition

Primitive SOAD Models
5.2

The generic primitive service composition using DEVS
atomic models (publisher, subscriber, and broker) is shown
in Figure 2. Messages produced by a service and consumed
by another are shown as envelops. As noted above, a message may contain a service description or other content
consistent with a chosen messaging framework. For example, the message from the Broker to the Subscriber is a service description which contains an abstract definition (an
interface for the operation names and their input and output
messages) and a concrete definition (consisting of the binding to physical transport protocol, address or endpoint, and
service). Another message could be from the Publisher to
the Subscriber where the result of the requested service (returned message from the Publisher). The implementation
of these messages can be based on SOAP. In the basic
SOA framework, the internal operations of atomic services
and their interactions are deferred to specific standards and
technologies (e.g., .NET (Lenz & Moeller 2003)).

Composite SOAD Models

An essential capability for simulating service-based software systems is to support modeling of composite service
composition. As shown in Figure 2, a composite service
composition has publisher or subscriber service which itself is a primitive service composition. Since broker service is required for both primitive and composite service
composition, two cases can be considered – i.e., either a
single broker service or multiple broker services are used.
Both cases can be supported. Use of a single broker service
is shown in Figure 3. To avoid cluttering Figure 3, the brokers shown in the Subscriber and Publisher services are the
ones that are used for these brokers (this is shown with
shaded background for the two brokers and their couplings). The three kinds of couplings provided in coupled
DEVS models supports use of a single broker for the primitive service compositions (i.e., Subscriber and Publisher) and their composite (hierarchical) service composi5

849

Sarjoughian, Kim, Ramaswamy, and Yau
tion. As can be seen, for example, Publisher1 service has
the role of a subscriber with respect to the Subscriber2
which has the role of a publisher. The common concept of
DEVS and SOA modularity allows creating composite service composition without restrictions. The DEVS hierarchical coupled modeling naturally supports multiple hierarchical broker services.

one-to-one correspondence the WSDL’s port and binding
elements – they serve as counterparts to the physical address at which a service can be accessed and the transport
technology for message communication. The ServiceLookup message is used to find the desired service in the
broker using a service name and an endpoint in the message. ServiceMessage message type corresponds to the
SOAP specification. It is required to define the data content that is exchanged between a subscriber and a publisher. The differences between the ServiceMessage and
the other messages are the data is actually used in the subscribed publisher and it must specify the destination of the
message.

Broker

Publisher1
(subscriber)

6.1.2 Primitive Services

Broker

Broker

Subscriber1

Subscriber

Publisher2

The specifications for the primitive SOA publisher, subscriber, and broker service are defined as DEVS atomic
models shown in Figure 4. The ServiceBroker has a container (UDDI) to store ServiceInfo messages. The ServiceSubscriber maintains a list of services to lookup a broker.
The ServicePublisher defines specific behaviors of its endpoints in the performService method. Depending on the
subscribed port (i.e., an endpoint), the performService can
execute different functions and return a Pair which defines
data type and value (this is used as data in the ServiceMessage). Since multiple users can subscribe to an endpoint at
the same time, the RequestList in the ServicePublisher is
devised to handle multiple user subscriptions simultaneously. These requests are processed using FIFO scheme.
For brevity, some methods such as δext and δint (see Section
3.2) for the services are not shown in the class diagrams.

Subscriber2
(publisher)

Publisher

Figure 3: SOAD composite service composition
6

SOAD SIMULATION ENVIRONMENT

The above SOAD modeling and simulation approach has
been realized using the DEVS-Suite (Kim, Sarjoughian, &
Elamvazhuthi 2008), a new generation of the DEVSJAVA
simulation environment. A set of generic SOAD models
are designed and implemented (Kim 2008). They represent
static and dynamic aspects of service-oriented software
systems. These models are partitioned according to the
SOA Models which extend the DEVS Models. The SOAD
models are generic in the sense of the generality supported
by SOA and DEVS. Specific SOA models (called Application Model) can be used to describe hierarchical serviceoriented software systems. A basic hardware model is also
developed, but due to lack of space it is not included here.
These models are executed using the DEVSJAVA simulation engine.

6.1.3 Primitive Service Composition
As shown in Figure 5, the SOAD has primitive services
with a network link and transducers). Based on generic interfaces defined for SOA services, default couplings are
defined. Furthermore, default couplings are also defined
for the network and transducer models. Therefore, to model a primitive service composition, it is necessary to construct the list of subscribers and publishers.
ViewableAtomic

6.1.1 Messages

(from simView)

Three generic message types called ServiceInfo, ServiceLookup, and ServiceMessage are specified. They are abstractions of the WSDL and SOAP specifications. The ServiceInfo and ServiceLookup correspond to the WSDL
specification. These two message types are needed for publishing services and their discovery. The ServiceInfo message is used for publishing a service with the broker. It
contains a service definition given a service name, service
description, service type, the list of endpoints, and binding
information. The port and coupling concepts do not have a

ServiceBroker
start : double
available_time : double
UDDI : ArrayList

ServiceSubscriber
startTime : double
lookupList : ArrayList
ServiceRequest : ServiceMessage

publish()
subscribe()
publishCompositeService()

out() : message

ServicePublisher
Processing_time : double
ServiceName : String
ServiceDescription : String
ServiceType : String
Endpoints : ArrayList
RequestList : ArrayList
msgQ : Queue
performService(data : Pair) : Pair

Figure 4: Primitive publisher, subscriber, and broker service
models
6

850

Sarjoughian, Kim, Ramaswamy, and Yau
6.1.4 Composite Service Composition
The simple Travel Agent Service can be used to illustrate modeling the basic throughput, timeliness, and accuracy quality attributes of SOA-compliant software-based
systems. For example, the dynamics of the Travel Agent
can be observed in terms of the events it generates and
consumes. The output events are defined for the service
lookup, the service lookup retry, and the publisher service
request. The scheduling of these events is defined in δint
and δext. The output events times relative to time instances
at which they can be generated are defined to be 0.5, 0.0,
and 1.0 second, respectively. The first event is scheduled
by the internal transition function. The second and third
events are due to the external transition function – i.e.,
processing of the input events from the Broker. There is
also another external transition function for processing the
input event as it is received from a publisher (either USZip
or Ski Resort). The time allocated for δext is 1.0 second.
The dynamics of the USZip and Ski Resort are the same.
Each takes 1.0 second to process a request received from
the Router Link and produce an output event. The Router
Link takes 0.5 second to deliver a publisher’s output event
as an input event to a subscriber. The Router Link takes
also 0.5 second to deliver a subscriber’s output event as an
input event to a publisher. The Broker takes 0.0 seconds to
respond to the Travel Agent (whether it finds a requested
service or not). For simplicity, in this example, the subscriber sends its requests to the publishers sequentially, but
simultaneous requests are straightforward to model. Table
2 shows sample quality of service measurements for the
Travel Agent Service operating for a period of 71.5 seconds. These generic metrics are captured by the transducers. The generic SOA DEVS models have stochastic timings, but the results given in Table 4 are based on
deterministic timings in order to verify the logical correctness of the primitive service composition.

The composite service composition is similar to the primitive service composition, except there is no list for subscribers since publishers in the composite service composition can be also subscribers. The flow of service
invocations needs to be specified given the specifics of the
service-based systems that are being modeled. This is a basic capability for hierarchical service composition which
has to be extended to support different kinds of workflow
patterns (Russell, Hofstede, Aalst, & Mulyar 2006).
ServiceBroker

1
1

1..*
ServiceRouter
trasmissionTime : double
network_traffic : double
outputPort : String

(from GenService)

(from GenService)

publish()
subscribe()
publishCompositeService()

(from GenService)

ServiceTransducer

ServiceComposition

(from GenService)

start : double
available_time : double
UDDI : ArrayList

1

BrokerList : ArrayList
RouterList : ArrayList
PublisherList : ArrayList
CoupledPublishersList : ArrayList
SubscriberList : ArrayList
TransducerList : ArrayList
ServiceComposition()
ServiceComposition()
BrokerRouterConstruct()
PublisherConstruct()
CompositeConstruct()
SubscriberConstruct()
TransducerConstruct()
CouplingConstruct()
1
1..*
ServiceSubscriber
(from GenService)

startTime : double
lookupList : ArrayList
ServiceRequest : ServiceMessage
lookUp : ServiceLookup

1..*

in : ArrayList
out : ArrayList
observation_time : double
compute_TP()
compute_TA()

1

1
ServicePublisher
(from GenService)

1..*

Processing_time : double
ServiceName : String
ServiceDescription : String
ServiceType : String
Endpoints : ArrayList
RequestList : ArrayList
msgQ : Queue
performService()

Figure 5: Model of primitive service composition
6.2

Example Simulation Model

Models for the primitive and composite service compositions are developed in the DEVS-Suite simulator which
supports SOAD. The model shown in Figure 6 has 4 software components (one subscriber (Travel Agent), two publishers (USZip and Ski Resort), and one broker (Broker))
and one simple hardware component (Router Link) (Kim
2008). The model also includes five transducers for each of
the software and hardware components. Another simulation model is for a real Voice Communication Service
(Yau, Ye, Sarjoughian, & Huang 2008) which was used to
validate the design and implementation of the SOADS environment. The Travel Agent Service and Voice Communication Service are used to show the primitive and composite service compositions.

Table 2: Selected metrics for the Travel Agent Service
model
Component

Quality of Service Measurements

Travel
Agent

Average Turnaround Time (sec): 2.0
Total size of data received (Kbytes): 640.0
Number of subscribed publishers: 2

USZip

Publisher Throughput (msgs/sec): 0.156
Amount of data received (Kbytes): 320.0
Number of subscribers: 1

Ski Resort

Publisher Throughput (msgs/sec): 0.156
Amount of data received (Kbytes): 320.0
Number of subscribers: 1

Router

Average Transmission Time (sec): 0.5
Total size of message received (Kbytes): 1280.0
Utilization for a period of time (%): 1.7073

Figure 6: Travel Agent Service primitive composite model
7

851

Sarjoughian, Kim, Ramaswamy, and Yau
7

RELATED WORK

Considering the approaches briefly reviewed in relation to SOAD, it is useful to consider support for representing (logical and real) time. The explicit use of time (discrete values) in services is crucial in developing verifiably
correct simulation models of dynamical real services. The
time-based execution of each model plays an important
role in developing dynamical simulations that can be validated. For example, a simulated service where its operations take real time to complete can be used instead of a
real service. Direct representation of time, therefore, is
necessary for characterizing complex structures and behaviors of services. This, in turn, supports evaluating timebased quality of service attributes such as throughput. Of
these, the approach which uses the Petri Nets formalism
enjoys explicit use of time. However, the situation calculus
for the DAMIL-S supports sequencing of actions (i.e., time
is not explicitly accounted for). Furthermore, unlike the
proposed SOAD, none of the above approaches are formalized to model and simulate services that may change their
structures at run-time and separately modeling serviceoriented software systems in terms of their hardware and
software layers.

Within the simulation community the interest has focused
on the use of web services for distributed simulation. For
example, the core HLA capabilities (IEEE 2000) can be
extended with SOA concepts (e.g., (X. Chen, Cai, Turner,
and Wang 2006)) or web services used for distributed simulation (e.g., (Hu, Zeigler, Hwang, and Mak 2007)). Web
services are also proposed to define an ontology with a corresponding software infrastructure for simulation model
reuse (Bell, Cesare, Lycett, Mustafee, and Taylor 2007).
A framework has been developed using HLA to support web services verification and validation (Tsai et al.
2007). Processes, services, and workflows are described
using the Process Specification and Modeling Language
(PSML). The modeling language used in this framework
uses HLA for simulation execution. The PSML and DEVS
models have basic differences such as explicit representation of time, event preemption, and closure under coupling.
Another important difference is the mapping from DEVS
and PSML to SOA. SOAD is defined in terms of the basic
SOA elements (subscriber, publisher, and broker) as well
as the primitive and composite service composition. From
a higher perspective, SOAD is targeted for modeling and
simulation of service-based computing systems whereas
PSML is targeted for their actual realizations.
Some other approaches have also been proposed to
support some software engineering phases of serviceoriented systems such as workflow designs. One approach
is based on use of Petri Nets formalism (Srini and Sheila
2003). It has been developed to analyze various aspects of
web services such as complexity. The DAML-S ontology
is used for describing web services that can be simulated
using KarmaSIM simulator. An execution scheme based on
situation calculus is mapped to Petri Nets modeling elements and thus supports performance analysis, verification,
and validation of web services. This approach, however,
does not provide a direct mapping from the SOA basic
elements to the Petri Nets modeling elements. Agent-based
simulation is used to model service chaining (Anderson,
Rothermich, and Bonabeau 2005). The simulator allows
macro-level modeling and testing of web services with
support for network-like visualization. This simulation focuses on the Web services flow patterns. Another simulator
which is a Java-based tool has been proposed for studying
performance of service-oriented software systems (John,
John, Lei, and Na 2006). For validation of service-based
software systems, a UML simulator has also been proposed
(Hiroyuki, Taku, Toshiyuki, and Sadatoshi 2006). It supports execution of BPEL4WS models described in UML.
The simulator is developed for BPEL/UML models where
interface of services can be simulated and used in conjunction with real services. The execution of the BPEL/UML
models are defined in terms of Activity Hyper-graph and
implemented as web services.

8

CONCLUSION

The basic goal for the proposed SOAD framework is to
take advantage of fundamental commonalities between
SOA and DEVS. As we have shown, the simulated services share important characteristics with those of real services. This is useful because users interested in simulating
service-oriented services can use the SOA principles and
the component-based modeling concepts. An important observation for the proposed framework is that the DEVS
formalism is well positioned to support modeling of (i)
services with dynamic structures and (ii) separately modeling software and hardware aspects of service-based software systems. The extension of the SOAD with the key
capabilities is under development. The SOAD framework
has the potential to inspire and serve as a basis for community-based development of realistic SOA. A community of
researchers and developers, akin to the community who
has developed the ns-2 simulator, can introduce important
capabilities such as modeling and simulating complex
workflow patterns. Development of expressive and robust
model libraries are very useful for advancing simulationbased design of service-based software systems where disparate quality of service attributes such as timeliness and
accuracy can be evaluated and analyzed systematically and
efficiently.
ACKNOWLEDGEMENT
This research is supported by NSF Grant number CCF0725340.

8

852

Sarjoughian, Kim, Ramaswamy, and Yau
REFERENCES

Lenz, G., and T. Moeller. 2003. .NET: A Complete Development Cycle: Addison-Wesley.
Møller, A., and M. I. Schwartzbach. 2006. An Introduction
to XML and Web Technologies: Addison-Wesley.
Russell, N., A. H. M.t. Hofstede, W. M. P. v. d. Aalst, and
N. Mulyar. 2006. Workflow control-flow patterns: A
revised view. BPM Center Report BPM-06-22.
Srini, N., and M. Sheila. 2003. Analysis and simulation of
Web services. Computer Networks, 42(5), 675–693.
Tsai, W. T., Z. Cao, X. Wei, R. Paul, Q. Huang, and X.
Sun. 2007. Modeling and Simulation in ServiceOriented Software Development. SIMULATION
Transactions, 83(1), 7–32.
Yau, S. S., N. Ye, H. S. Sarjoughian, and D. Huang. 2008,
October). Developing Service-based Software Systems
with QoS Monitoring and Adaptation. Proceeding of
the 12th IEEE Int'l Workshop on Future Trends of
Distributed Computing Systems, Honolulu, Hawaii,
USA.
Zeigler, B. P., T. G. Kim, and H. Praehofer. 2000. Theory
of Modeling and Simulation: Integrating Discrete
Event and Continuous Complex Dynamic Systems
Second Edition: Academic Press.

ACIMS. 2001. Arizona Center for Integrative Modeling
and
Simulation.
2007,
from
http://www.acims.arizona.edu/SOFTWARE.
Anderson, C., J. A. Rothermich, and E. Bonabeau. 2005.
Modeling, quantifying and testing complex aggregate
service chains. Proceedings of the 2005 IEEE International Conference on Web Services, Orlando , Florida
, USA.
Barros, F. 1997. Modeling formalisms for dynamic structure systems. ACM Transactions on Modeling and
Computer Simulation, 7(4), 501–515.
Bell, D., S. Cesare., M. d., Lycett, N. Mustafee, and S.
Taylor. 2007, October. Semantic Web Service Architecture for Simulation Model Reuse. Proceedings of
the 11th IEEE International Symposium on Distributed Simulation and Real-Time Applications, Chania,
Crete Island, Greece.
Chen, X., W. Cai, S. J. Turner, and Y. Wang. 2006. SOArDSGrid: Service-Oriented Architecture for Distributed
Simulation on the Grid. Workshop on Parallel and
Distributed Simulation Washington, DC, USA.
Chen, Y., and W. T. Tsai. 2008. Distributed ServiceOriented Software Development. Kendall/Hunt Publishing.
Erl, T. 2006. Service-Oriented Architecture Concepts,
Technology and Design: Prentice Hall.
Hild, D. R., H. S. Sarjoughian, and B. P. Zeigler. 2002.
DEVS-DOC: A Modeling and Simulation Environment Enabling Distributed Codesign. IEEE Transactions on Systems, Man and Cybernetics, Part A, 32(1),
78–92.
Hiroyuki, K., F. Taku, M. Toshiyuki, and K. Sadatoshi.
2006. A UML Simulator for Behavioral Validation of
Systems Based on SOA. Proceeding of the International Conference on Next Generation Web Services
Practices, Seoul, Korea.
Hu, X., B. Zeigler, M. H. Hwang, and E. Mak. 2007. DEVS
Systems-Theory Framework for Reusable Testing of
I/O Behaviors in Service Oriented Architectures. Proceeding of the IEEE International Conference on Information Reuse and Integration, Las Vegas, NV,
USA.
IEEE. 2000. HLA Framework and Rules, Version IEEE
1516-2000, IEEE Press.
John, G., H. John, L. Lei, and L. Na. 2006. Performance
engineering of service compositions. Proceedings of
the 2006 international workshop on Service-oriented
software engineering, Shanghai, China.
Kim, S. 2008. Simulation of Service Based System: Modeling and Implementation using the DEVS-SUITE. Arizona State University, Tempe, Arizona, USA.
Kim, S., H. Sarjoughian, and V. Elamvazhuthi. (inpreparation). DEVS-Suite: A Component-based Simulation Tool for Rapid Experimentation and Evaluation.

AUTHOR BIOGRAPHIES
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science & Engineering at Arizona State University and Co-Director of the Arizona Center for Integrative
Modeling and Simulation. His research focuses on multiformalism modeling, collaborative modeling, distributed
simulation, and software architecture. He can be contacted
at <sarjoughian@asu.edu>.
SUNGUNG KIM is a Master student in the Computer
Science and Engineering department at ASU. His research
is in the development of SOA-based simulation models. He
can be contacted at <skim109@asu.edu>.
MUTHUKUMAR RAMASWAMY completed his Masters of Engineering degree in Modeling & Simulation program at ASU. His research area is system theory based simulation of SOA-based software systems. He can be
contacted at <muthukumar.ramaswamy@asu.edu>.
STEPHEN S. YAU is Professor of Computer Science and
Engineering and Director of Information Assurance Center
at Arizona State University, Tempe. His current research
interests include cyber security, trust, privacy, software engineering, distributed computing systems, and ubiquitous
computing. He can be contacted at <yau@asu.edu>.

9

853

Journal of Simulation (2011) 5, 77–88

r 2011 Operational Research Society Ltd. All rights reserved. 1747-7778/11
www.palgrave-journals.com/jos/

Towards collaborative component-based
modelling
HS Sarjoughian1*, JJ Nutaro2 and G Joshi1
1

Arizona State University, Tempe, AZ, USA; and 2Oak Ridge National Lab, Oak Ridge, TN, USA

Collaborative modelling enables dispersed users to develop component-based system models in group settings.
A realization of such an approach requires coordinating and maintaining the causality of the users’ activities. We
propose the Collaborative DEVS Modelling (CDM) approach and its realization based on the Computer Supported
Collaborative Work (CSCW) and the Discrete Event System Speciﬁcation (DEVS) concepts and technologies. The
CSCW concepts are introduced into the DEVS modelling framework in order to support model development in virtual
team settings. A set of modelling rules and tasks enabling collaborative, visual, and persistent model construction and
synthesis is developed. To support separate groups of modellers to independently develop models, the realization of
CDM supports independent modelling sessions. An illustrative example is developed to demonstrate collaborative and
incremental model development. The design of the CDM realization and future research are brieﬂy described.
Journal of Simulation (2011) 5, 77–88. doi:10.1057/jos.2010.5; published online 14 May 2010
Keywords: collaborative modelling; computer supported collaborative work; discrete event system speciﬁcation

1. Introduction
Modelling is instrumental to analysis, design, deployment,
and evolution of systems that have complex structures
and behaviours. Visual and persistent modelling support is
considered useful as they signiﬁcantly simplify common
model development activities such as deﬁning a system’s
parts and their relationships (Rhee, 1999; Sarjoughian et al,
1999a; Burmester et al, 2005). An important limitation of
many of today’s modelling approaches and tools is that they
are historically developed primarily for single users. To
support a team of modellers, it is important to account
for space and time constraints on geographically dispersed
collaborators. Thus, a collaborative modelling environment
should intrinsically support a group of analysts, designers,
and system engineers to collectively develop models of systems (Bidarra et al, 2002; Delinchant et al, 2004; Filho et al,
2004). As a team, the modellers can develop, for example,
a system-level model of a vehicle that includes engine,
controller, and sensor components using the Discrete Event
System Speciﬁcation (DEVS) modelling framework (Zeigler
et al, 2000). Based on groupware concepts, a collaborative
modelling environment can support creating, modifying,
viewing, and sharing of the vehicle system model and its
components.
Collaboration among modellers can be fostered using
Computer Supported Collaborative Work (CSCW) principles and concepts such as sessions, awareness, privacy, and
*Correspondence: HS Sarjoughian, Computer Science and Engineering,
Arizona Center for Integrative M&S, Arizona State University, Tempe,
AZ 85258-8809, USA.

control of individual and group activities (Grudin, 1994;
Usability, 2004). CSCW-based environments have been
developed to enable multiple users to work as a team under
different time and space constraints. A team, for example, can
be a group of individuals who are located in different but
predictable (or discoverable) places while collaborating in
different but predictable time periods. The knowledge and
information exchanges can also range from being unstructured (eg unscripted text messaging) to structured (eg orderly
creation of a hierarchical model design). Given such alternative collaboration settings, users can manipulate text and
documents through blackboard or other web technologies,
and further improve their productivity through voice and
video-conferencing (Subramanian et al, 1999). CSCW is
necessary but insufﬁcient to support developing simulation
models in collaborative settings (Taylor, 2001). For example,
the concepts and methods that have been developed for
supporting group activities do not account for the rules under
which hierarchical DEVS simulation models can be speciﬁed.
In this article, the Collaborative DEVS Modelling (CDM)
approach is proposed. The concept of the CDM is realized
through extending the component-based DEVS system
modelling with groupware support. Therefore, the resulting
CDM environment can be viewed to be a combination of the
DEVS modelling engine and the Collaborative Distributed
Network System (CDNS) engine (Sarjoughian et al, 1999b).
The modelling engine provides modelling constructs (ie
creating components, ports, and couplings according to the
DEVS formalism). It also supports visual model development and persistence. These capabilities are very useful not
only for single-user modelling tools, but even more so in

78 Journal of Simulation Vol. 5, No. 2

collaborative settings. The collaborative engine supports
team-oriented capabilities such as modellers creating or
joining collaborative model sessions. Capabilities such as
concurrent control, data exchange, and data persistence are
used for creating one or more collaborative modelling
sessions. This requires maintaining logical ordering among
the activities of multiple users since it is impractical for any
two users to simultaneously modify or create a model.
Collaborative modelling activities must be processed in some
well-deﬁned order. For example, a model component can
be removed only after it is created. Similarly, concurrent
activities of modellers may not be inhibited if they are
independent (eg one modeller is viewing a hierarchical model
while another modeller is creating a component which is to
be added later to the hierarchical model).
The remainder of this article is organized as follows. In
Section 2, we review the essentials of CSCW, a selection of
related research, and the DEVS modelling approach. In
Section 3, we introduce the modelling concepts in the context
of a collaborative model development process and devise our
approach for DEVS modelling approach. In Section 4, we
describe the details of the CDM, an example model, and a
sketch of the design of the CDM environment. Finally, in
Section 5, we present a summary and discuss open problems
and future research directions.

2. Background
It is common to develop models iteratively, especially for
systems that are large and complex. System development
activities, including model conceptualization and speciﬁcation, increasingly depend on multiple analysts, designers,
and coders collaborating with one another (Filho et al, 2004;
AnyLogic, 2008). In this section, we brieﬂy review some
collaborative environments and their capabilities that have
been developed to support better use of information systems
and modelling tools. We then turn to the combined role of
the system-theoretic and collaborative model development
concepts that are used in devising the CDM approach.

2.1. Computer supported collaborative work
CSCW capabilities are universal in that they are intended for
a variety of application domains including scientiﬁc research
in astronomy, bioinformatics, software engineering, and
medicine. Based on the key beneﬁts promised by CSCW
and pre-packaged software tools, a variety of collaborative
environments have emerged in recent years to support teamoriented business and engineering needs (Hill et al, 1994; Sun
et al, 1998; Subramanian et al, 1999; Xia et al, 2001; Yang
et al, 2001; Zhang et al, 2002). To enable use of single-user
software applications in collaborative settings, products such
as CodeBeamer (CodeBeamer, 2004) are also developed to
support or otherwise augment information sharing and

communications. These environments offer collaborative
features such as the exchange and creation of information,
access to global and local data, and joint use of software
applications. For example, AnyLogic (AnyLogic, 2008) uses
a version control system and thus can support partitioning a
project into parts. Subsequently different users can develop
models and carry on related development activities in a
collaborative workspace.
Research in the development of collaborative tools has
focused on general concepts and communication and control
approaches as well as their extensions to speciﬁc domains
such as process engineering, scientiﬁc investigations, and
remote health-care diagnosis and planning (Rhee, 1999;
Sarjoughian et al, 1999b; Subramanian et al, 1999; Yang
et al, 2001). Each of these approaches supports some of the
collaboratory concepts (eg shared workspace) and principles
(eg causality preservation of collaborators’ actions). However, since CSCW capabilities and features are generic, they
need to be extended and complemented given the speciﬁc
needs of the collaborative applications. A recent survey
shows that CSCW applications are, for example, in decision
support, sharing information, and conferencing with little
effort in direct support of constructing and synthesizing
simulation models (Jacovi et al, 2006). To understand the
use of the CSCW concepts and technology, we reviewed
some approaches and tools that are developed for speciﬁc
application domains (Grundy et al, 1998; Subramanian et al,
1999; Kim et al, 2001; Bidarra et al, 2002; Filho et al, 2004;
CanyonBlue, 2005). A detailed review of these can be found
in Joshi (2004). The following brief discussion for three of
these collaborative environments is intended to highlight the
use and adaptation of CSCW concepts and technologies.
Co-Surgeon (Kim et al, 2001) is a collaboratory supporting surgical simulation for 3-D anatomical models using a
client-server architecture style. The server maintains the
database consisting of medical images, patient records,
treatment procedures, and surgical plans. To maintain
model consistency, activities of participants, the server uses
a token-control mechanism in order to allow only one user
at a time to control the global view of the model. To obtain
control of the session, a participant must ﬁrst obtain a token.
This request is placed in a FIFO queue, which is maintained
by the session supervisor. The participants can collaborate
synchronously or asynchronously. In synchronous collaboration the session initiator prepares and uploads a model
to the server for a prescheduled meeting. Collaborators then
obtain a local copy of the model from the server. When a
client changes the current view of the model, the server
broadcasts those changes to all other clients. There are three
basic types of model manipulation: selection, translation,
and rotation. To support surgical simulation, operations
such as marking, measuring, and cutting are supported.
Collaborative Computer Aided Design (CCAD) systems
enable specialists to work collaboratively on the design and
development of mechanical and electrical systems (Bidarra

HS Sarjoughian et al—Towards collaborative component-based modelling 79

et al, 2002). Because of the computational intensity of CAD,
the architecture of CCAD may use thin- or thick-client/
server design. A thin-client has a local view of the model
located on the server while a thick-client has its own local
copy of the model with the original model on the server.
CCAD also allows various techniques, such as token control
or locking, to manage concurrency. A token control mechanism grants model ownership to only a single client who can
modify the model while others are only allowed to view the
model. In addition, locking may also be used to provide
greater user-control. Depending on the granularity (eg deep
versus shallow) of the lock, different levels of collaboration
are possible. For example, locking can be used to restrict the
manipulation of the entire or part of a model to a single client.
GroupSim is a collaborative environment for modelling
discrete event simulation systems (Filho et al, 2004). It uses
the Activity Cycle Diagram notation for non-hierarchical
visual modelling activities and their relationships. This
environment uses the GroupPlaces groupware architecture
which offers the CSCW workspaces that are used in CDM
(Sarjoughian et al, 1999a). Unlike CDM, requirements such
as access control among multiple concurrent modellers and
performance are left as future work. Users can synthesize
Java classes of the models via their visual object representations. This environment is proposed to be extended to
automatically combine the models that are in the form of
Java classes and support a user to start a simulation which
can be stopped or resumed by a collaborating user.
The CDM approach described in this article uses the basic
CSCW concepts and technologies described above to create
a collaborative environment for building DEVS models. The
above approaches and tools primarily support collaborative
model development similar as in collaborative software
development. Furthermore, although models constructed
with such approaches can be characterized as discrete-event,
there is no direct support for constructing models that
conform to systems theory and DEVS in particular. The
structural speciﬁcations including component ports, couplings, and hierarchical construction are not supported.
CDM supports public and independent private workspaces
and it is possible for any user to simultaneously participate
in multiple private sessions. The client view of a model’s
block diagram is automatically updated as the model
changes by multiple users. The model layout is important
for collaborative sessions since it allows all modellers share a
common view of the system. The updates to the models are
generated and sent in a well-deﬁned order to the clients.
Since only updates to the models are necessary, there is a
reduced need for network resources which in turn improves
the performance of the CDM environment.

2.2. Component-based simulation modelling
Systems theory offers a formal foundation for conceptualizing and specifying modular, hierarchical models (Wymore,

1993; Zeigler et al, 2000). It supports characterizing statebased hierarchical structure and behaviour of a system.
The structure of a system refers to its parts (atomic or
composite), input/output interfaces, and how parts may be
composed to form the system. The behaviour of a system
refers to how inputs are processed and how the state of a
system changes both internally and due to the interactions
among system components. Object-orientation concepts and
constructs such as abstraction, encapsulation, inheritance,
and polymorphism offer capabilities for realizing systemtheoretic modelling formalisms and extending their modelling capabilities.
The DEVS is a system-theoretic approach to modelling
discrete event systems (Zeigler et al, 2000). Its atomic and
coupled models are deﬁned to have input and output ports
which are the only means by which inputs can be received
and outputs can be sent. An atomic model speciﬁes the
dynamic behaviour of a system’s component in terms of
states, inputs, outputs, transition functions, an output
function, and a time advance function. All leaf nodes of a
hierarchical model are atomic models and these cannot be
further decomposed. Hierarchical models are deﬁned in
terms of atomic and coupled models. A coupled model is
deﬁned to have a ﬁnite set of atomic or coupled models and
every coupled model conforms to strict hierarchy—that is, a
model cannot contain itself anywhere in its hierarchy. All
model interactions occur through sending and receiving
messages through couplings. A coupled model can have
internal and external couplings which are used to send and
receive outputs and inputs from its parts. Internal couplings
from output ports to input ports capture how the coupled
model’s components inﬂuence one another. External input
couplings send messages originating from the parent model
to its components. External output couplings send messages
originating from within the model components to the parent
model. These models adhere to causality and timing
constraints—inputs sent to atomic (and coupled) models
will take some period of time to be processed and when two
models are combined into a coupled model, the inputs sent
and outputs received to and from the components must be
through the input and output ports of their coupled model.
Environments such as DEVS-Suite (DEVS-Suite, 2008)
and MATLABs/Simulinks (Mathworks, 2002) support
model development and execution, viewing the structure
of models, and animation of their behaviour. Modelling
environments also exist to support visual speciﬁcation of
models for single users (Delinchant et al, 2004; Burmester
et al, 2005; Sarjoughian, 2005; CoSMos, 2010). These are
called object-oriented systems-theoretic modelling environments. A key beneﬁt of object-oriented systems-theoretic
M&S approaches such as DEVS is support for incremental
model development which is crucial for synthesizing larger
models from smaller models while allowing specializing
model components.

80 Journal of Simulation Vol. 5, No. 2

3. Collaborative modelling concepts
Development of simulation models can take place in
conventional, synchronous, or asynchronous modes.
Multiple users could be allowed to create and view models.
Conventional Collaboration (CC) involves immediate,
face-to-face communications and requires all participants
to be in the same physical space at the same time.
Synchronous Collaboration (SC), however, can occur when
the participants have agreed on a time to collaborate, but are
not necessarily in the same physical place (eg videoconferencing). Unlike the other two modes of collaboration,
Asynchronous Collaboration (AC) does not require any prior
agreements on time or space (eg computer bulletin board).
These collaboration modes categorize time and place as
(a) same, (b) different but predictable, and (c) different and
unpredictable (Grudin, 1994; Filho et al, 2004). This smaller
category of collaboration modes emphasizes teamwork
with respect to collaborators joining and leaving in both
predictable and unpredictable time instances. In particular,
the collaboration modes are not distinguished with respect to
the locations of the collaborators, but instead emphasize the
concept of simultaneity. This smaller set of collaboration
modes is appropriate given the aim of collaborative model
development considered in this article.
Many research and commercial environments have been
developed to support different kinds of modelling activities
across different domains. Examined from the perspective of
enabling formal model speciﬁcation, environments such as
DEVS-Suite and MATLABs/Simulinks do not have the
concepts and support for model development in a collaborative setting; instead these tools and their underlying
formulations support single users. They offer support for
dispersed modellers in the form of sharing data, models,
and internet-based communication. They, however, do not
provide the necessary logic that can ensure single-user
modelling activities can be carried out in a collaborative
setting. For example AnyLogic supports multiple users
developing models in a group setting, but there is a lack of
built-in modelling rules for constructing models. Since the
general-purpose nature of collaborative concepts and
technologies, CSCW must be complemented with the needs
of speciﬁc modelling approaches. Before proceeding further,
we ﬁrst develop a mapping of the CSCW collaboration
modes to the phases of a common model development
process.

3.1. Model construction and synthesis processes
Modelling and simulation is well known to be inherently an
iterative process. A comprehensive process called Federation
Development Process (FEDEP) has been developed for
large-scale, distributed simulation models (IEEE, 2003).
The steps for developing new models are requirements
gathering, development planning, conceptual modelling,

design, and implementation and testing. A simpliﬁed
process model based on the Grab-and-Glue framework
is also considered (Pidd, 2002; Eldabi et al, 2004). This
process focuses on web-based model construction and
model reuse. The key steps in this framework are Graband-Glue processes with the purpose to overcome
the limitations of traditional modelling and simulation
approaches. The approach considers the time and effort
for collecting data, constructing models, and executing
simulations. It identiﬁes a variety of limitations and
challenges including ﬁnding models from repositories, model
compatibility and adaptation, and visual modelling. Neither
the FEDEP model nor Grab-and-Glue approach takes
into account collaborative model development modes as
described next. Collaborative process development frameworks have also been proposed for software development.
For example, SPEARMINT/XCHIP (Fernández et al, 2004)
supports a graphical, hypermedia structure approach for
model development and documentation. In comparison
with this approach, the process proposed below is targeted
for simulation model development and in particular
accounts for system-theoretic modelling with different
collaboration modes.
In view of collaborative model development, a process
model shown in Figure 1 is proposed. This process consists
of the six phases. The bi-directional arrows show the
iterative nature of the model and simulation development
process. For all but simple and trivial systems, model development demands a mixture of useful modelling concepts
with the ability to map domain knowledge to generalpurpose modelling constructs provided by modelling formalisms. The process starts with the obtain requirements phase
which involves clarifying the concepts and the purpose for
which the model is to be developed. For non-trivial
modelling efforts, the objectives and the formulation of
requirements are carried out in CC mode, and sometimes
augmented with other modes such as video-teleconferencing
to review and share model description or conceptual
drawings. After the requirements have been established,
data speciﬁc to the problem domain is usually collected in an
AC mode by the individual team members. During the
collect data and search for models phase, information necessary to specify models is gathered from expert consultation
to searching for models that are developed by others.
Models are typically speciﬁed in two ways: (i) model
construction and (ii) model synthesis. The former is concerned with creating new model components and, the later
with creating models from pre-built model components
(Sarjoughian et al, 1997; Lee et al, 1998). Model construction includes specify and verify models phases (see Figure 1).
These phases can take place in SC mode and the other two
collaboration modes (CC and AC). Model synthesis includes
simulate and validate models phases (see Figure 1). The
model construction and synthesis are not exclusive; rather
they are complementary to one another since both model

HS Sarjoughian et al—Towards collaborative component-based modelling 81

Model Construction
specify
models
CC

verify
models

AC
SC, AC, CC

obtain
requirements

collect data
and search for
models

model
database

validate
models

simulate
models
Model Synthesis

Figure 1

Model construction and synthesis with assigned collaboration modes.

veriﬁcation and simulation validation are dependent on
domain knowledge and simulation experiments. The
obtain requirements and the collect data and search models
phases are appropriately tailored to model construction and
synthesis.
Model construction and synthesis are iterative since, when
specifying or simulating models, it is often necessary to
clarify objectives, collect new data and customize or change
models that meet the objectives of the simulation study.
Since developing large-scale, complex models requires
describing many distinct parts of a system and their interactions, not only do modellers need to know how to specify
models in a given modelling formalism, but they also must
have knowledge of the application domain. Each step in
the model construction and model synthesis processes need
to be assigned one or more suitable collaboration modes.
In Figure 1 the collaboration modes that are considered
appropriate for developing simulation models are shown.
Although all collaboration modes may be used for any of
these steps, some modes are more appropriate than others.
For example, a modeller may construct and verify a model
for a part of an aircraft and at a later time join a
synchronous collaboration session where the aircraft model
is synthesized from parts. A user may then simulate the
aircraft model and thereafter a group of dispersed users will
evaluate the simulation results and validate the model.

3.2. Modelling formalism and collaboration modes
Previously it was noted that existing component-based
modelling approaches do not provide concepts and capabilities that can support collaborative model development. A collaborative modelling environment must account
for model construction and synthesis which take place
over several modelling sessions and in different places.
Concurrent model creations and modiﬁcations (eg adding a

model component or deleting a coupling relation between
two model components) among multiple modellers require
capabilities that can ensure logical correctness beyond what
is assumed for DEVS and other modelling approaches. This
requires developing appropriate relationships between the
constructs of a modelling formalism and their use in a group
setting. The DEVS modelling formalism supports model
construction and synthesis activities such as creating a
model, adding input ports, and changing coupling between
model components.
The ordering of model synthesis steps between two
modellers must be well-structured. For example, both
modifying a model and viewing the changes to the model
must be controlled. The result of removing a model
component must be presented consistently to all modellers
that are collaborating in synchronous collaboration mode.
Likewise, when a component is added to a coupled DEVS
model and then coupled to another component, the order of
modelling steps must be preserved (ie coupling occurs
only after the component is added and the user cannot view
the coupling without also viewing the component added to
the coupled model). Thus, the CDM collaborative modelling
approach must support the DEVS legitimacy property which
prohibits any atomic or coupled model to have direct
feedback.
Next, the logical DEVS model speciﬁcation is extended
with visual, and persistent models (Sarjoughian, 2005). A
logical model refers to the speciﬁcation of a model’s syntax
and semantics for a given modelling formalism. A visual
model refers to the visual representation of a logical model
given the constraints of displaying complete componentbased view of models as tree structures or block diagrams.
The persistent model refers to storage of logical models. The
inclusion of visual and persistent modelling concepts and
capabilities with logical modelling are key for collaborative
model development.

82 Journal of Simulation Vol. 5, No. 2

Modelling formalism:
 The DEVS logical speciﬁcations of atomic and coupled
model structures must be preserved when used in a
collaborative setting. For atomic models, inputs, outputs,
states, and functions can be speciﬁed once a model
component is created. Each modelling step is ‘atomic’ and
no ordering is required among these steps. A coupled
model can be created and other models added or removed
from it. Flat and hierarchical coupling of models conform
to the DEVS modelling formalism.
 Modelling constructs must remain invariant under alternative collaboration modes—that is, collaborative model
development may neither weaken nor restrict the DEVS
model speciﬁcations. If a modeller adds a component to
another model component and at some later point the
added model is removed by another modeller, both
modelling steps are visible to both modellers in the order
in which they occurred. Furthermore, there is no
guarantee that any modelling step is carried out or viewed
at the same wall clock time. All logical modelling steps
and their visual representation for atomic and coupled
model development must be guaranteed to conform to the
DEVS modelling formalism.
Collaboration modes:
 One or more logical model components may be accessed
by multiple dispersed users only if the model speciﬁcations
are consistent with the modelling formalism within which
they are described. The creation and modiﬁcation of
model components in synchronous and asynchronous
group collaboration must be guaranteed to be consistent
with the chosen modelling formalism.
 Every atomic and coupled visual model is a graphical
representation of its corresponding logical model. The
visual models must remain invariant to time and space.
Every dispersed modeller must be assured a correct view
of every model as it evolves. That is, the order of logical
modelling steps must also be preserved for visual
modelling. The order preservation of the modelling steps
is independent of network delay and every modeller must
view all of the modelling steps whether or not a modelling
step is retracted at a later time.
 Logical models, possibly with versioning, must persist
across time and space. Models may be stored in alternative media such as ﬂat ﬁles or databases. The visual
representations of the models are rendered to each
modeller separately given one or more repositories of
logical models.
The DEVS modelling and CSCW concepts are needed for
developing the CDM. A proper mapping of the modelling
steps and collaboration modes is required. As noted earlier,
model construction and synthesis are complex and iterative

which prohibits having more than a handful of modellers to
collaboratively develop models. Indeed, at the present time
collaborative model construction and synthesis remains
challenging to support beyond a handful of people.

4. CDM approach
The development of models among a group of modellers can
be realized by enabling system-theoretic modelling capabilities within CSCW workspaces. To support collaborative
model development it is important to begin with the activities of a single modeller. His modelling activities need to be
formulated in terms of the collaborative session concept
and principles. It is also important for concrete realization of
the collaborative modelling framework to be simple and
efﬁcient.
A collaborative session is a loosely bounded workspace
within which a group of clients develop a model jointly.
Compared to a session in a classical client/server environment in which clients carry out independent tasks, a collaborative session’s content is based on interdependent activities of clients. Within a collaborative session, user actions
that operate on shared data are synchronized so as to
enforce the rules of the modelling constructs. A collaborative
session has a ﬁnite duration with a start-time and an endtime with the implication that initialization, operation, and
termination steps must be supported separately for every
modelling session. For example, given a group of modellers,
one user ﬁrst creates a session so that all users can join the
session. Then the users, which may not include the person
who created the session, can collaboratively develop models.
The session remains active until the modelling activity
comes to a conclusion (eg the model is frozen to establish a
baseline).
A collaborative system-theoretic modelling framework
consists of a well-deﬁned modelling approach and a collaborative scheme. For the modelling formalism, the DEVS
approach is selected. For collaboration support (eg dispersed
users sending and receiving modelling queries/actions), the
CDNS (Park, 1998; Sarjoughian et al, 1999b), a lightweight
distributed computing environment, is selected. The combination of the DEVS and CDNS with collaborative session
is the proposed CDM (Collaborative DEVS Modeller)
approach. In the remainder of this section, we describe the
details of how the DEVS and CDNS are integrated.

4.1. Modelling activities
The modelling activities for the specify model phase are given
in Table 1. The activities 1 through 3 differ for the single and
group modellers in that these activities are inherently
sequential for a single modeller and concurrent for a group
of modellers. A common aspect of the activities is to support
relatively large-scale model development, especially from the

HS Sarjoughian et al—Towards collaborative component-based modelling 83

visualization perspective. This is important for supporting a
single modeller and more importantly a group of modellers.
Model organization is integral not only in the context of
collaborative model development but also for models that
have tens to several hundred parts and links (or couplings).
As will be described in section ‘Visual model representation’,
special care is needed to support logical and visual modelling
among the members of a group.
The CDM environment needs to offer basic capabilities
such as loading logical models and manipulating visual
models. From the collaborative perspective, it needs to
provide the capabilities given in Table 2.
Furthermore, the CDM environment needs to be simple
for deployment. The logical and visual representations of the
models must be separated, but kept consistent with one
another. This separation is important both in terms of users’
ability to develop models and to design an efﬁcient environment. For example, logical models can be stored on servers
and visual models can be rendered locally on each modeller’s
computer which signiﬁcantly reduces data and transfer
frequency.

collaborative session. A modeller can deﬁne a coupled
model without its parts and another modeller may deﬁne one
or more parts of the coupled model. Similarly, a model may
be deﬁned without ports and only later speciﬁed to have
ports. The key concept is that collaborative model construction and synthesis imposes requirements that may be
unnecessary from the perspective of a single modeller.
Modellers in a collaborative session develop models with the
understanding that a collaborator can specify a partial
atomic or coupled model that may be completed by any
modeller. The synthesis of CSCW and DEVS, therefore,
provides the basis to account for modelling activities among
groups of modellers that span some ﬁnite period of time and
thus assuring all activities are synchronized to ensure
syntactically correct structural logical speciﬁcation of
models. To achieve this, the additional rules provided in
Table 4 are deﬁned and supported in CDM.
Given the above, we deﬁne the collaborative modelling
constructs add, delete, cut, remove, link, copy, and edit. It is
important to note that these logical modelling constructs

Logical model speciﬁcation. The logical representation of
the models is deﬁned according to the DEVS formalism.
The coupled models have hierarchical tree structure
representations. In this article, rather than giving the
formal speciﬁcation of the DEVS, we focus on its structural
modelling artefacts and their use. The root of the tree
provides the most compact view of the system. Subsequent
levels (tree branches) reveal greater detail via decomposition, input/output interfaces, and couplings. The leaf nodes
of the tree are atomic components that are not further
decomposed. The DEVS logical model speciﬁcation should
enable users to construct hierarchical models under the
rules listed in Table 3.
The above rules are extended to concretize the collaborative session concept in terms of the DEVS model development activities. The rules below underscore the importance
of modelling activities (eg adding a component or a
legitimate link between two model components) in a

Table 3 Logical model speciﬁcation rules

Table 1 Model construction and synthesis activities
1. Logical models can be speciﬁed and revised
2. Visual models represent tree structure and block diagram
views
3. Logical models persist in time and space and can be revised
and retrieved

Table 2 Collaborative activities
1. Enter and exit modelling sessions
2. Joint and leave modelling sessions
3. Create, delete, and visualize models

1. The root node of the tree must be a coupled model.
2. An atomic model does not have any other model
component contained within it.
3. A coupled model can have a ﬁnite number of atomic or
coupled components; a coupled model cannot contain itself
at any level in the model hierarchy.
4. Atomic and coupled models can have a ﬁnite number of
input and output ports. At most one link can exist between
any two ports that are eligible to be coupled.
5. A coupled model may have a ﬁnite number of links with its
components; its components can have a ﬁnite number of
links with each other. A link is unidirectional and is
speciﬁed in terms of (source model, output port) and
(destination model, input port). The input port of a coupled
model can be connected to the input of any of its
components. Any output port of any component of the
coupled model can be linked to any output port of the
coupled model. Any output port of any component of the
coupled model can be linked to the input port of any
component of the coupled model. A model’s output and
input ports cannot be coupled to one another.
6. Each model has a unique identity and its input and output
port names are unique among themselves.

Table 4 Additional logical model speciﬁcation rules for a
collaborative session
1. A coupled model may have no components at a given time
during a modelling session. Atomic and coupled models
may not have any input and output ports.
2. A coupled model may have no links at a given time during
the modelling session.
3. A model that is not the root may belong to a coupled
model; an atomic model may be deﬁned, but not be part of
any coupled model.

84 Journal of Simulation Vol. 5, No. 2

satisfy the syntax and semantics of the DEVS models, but in
addition, are deﬁned as operations that take place among
dispersed modellers and may be carried out at different
time instances of a collaborative modelling session. Further
details of these modelling actions are deﬁned in the next
section.

Visual model representation. Modellers located in dispersed locations can collaborate to build a model of a
system; for example, the cabin, navigation, and control
components of the aircraft model depicted in Figure 2. To
graphically represent such a model, CDM environment
provides graphical views for the logical elements (ie atomic,
coupled, input/output ports, links) of a model described
above. The Graphical User Interface supports two complementary views of the same logical model—tree structure
and block diagram. The tree structure shows the entire
model as a labelled tree and the block diagram shows block
representation of one coupled or one atomic model. The
block diagram shows ports and couplings of a selected
model. By interacting with the tree structure, the user can
traverse the model hierarchy and cut or delete any part of a
model. The part can be an atomic component or any
branch of the tree structure. In the block diagram, a user
can add or cut atomic and composite model components,
add or cut coupling relationships, add or cut ports, and
rename components. Consistency between the alternative
views is maintained automatically.
Of these two visual models, it is important to examine the
representation of the block diagram. The diagonal block
diagram layout is important for individual and collaborative
sessions. First, it simpliﬁes visualizing unidirectional links
among components when both feedforward and feedback
couplings are used in a coupled model. It reduces crossing of
the links and simpliﬁes visual complexity of models. The
disadvantage is that for large-scale models with sparse
couplings, it can be difﬁcult to visualize and work with the
models since a relatively large space becomes necessary.
Second, the pre-determined block diagram layout is
important for collaborative sessions since it is otherwise
difﬁcult to guarantee that the modellers have an identical
view of the model. The automatic layout is important
especially in a collaborative environment where unanticipated changes to the diagram may occur as a result of
actions performed by multiple modellers. Moreover, a uniform representation of the block diagram view is important
when modellers’ interactions are complemented with voice.
Figure 2 shows a simpliﬁed hierarchical model of an
Aircraft composed of a Cabin and Navigation and
Control components. The visual modelling environment is
made of left and right panels and bottom and top tool bars.
The left panel shows the tree structure of the System (see
Figure 2). While each model component is unique, it can
appear in multiple places if it is part of a larger model. For
example, the coupled Environment component appears in

two places subject to the rules deﬁned in section ‘Logical
model speciﬁcation’—as a standalone coupled model it
appears on the far left side of the tree, and as a component of
the System it appears as part of the ﬁrst branch of the tree
structure. The tree structure view does not show port names
and couplings. The right panel displays the composite
Aircraft model, its parts, input and output ports, and
couplings. The models are represented by blocks and are
associated with the model component highlighted in the left
panel (ie Aircraft shown in Figure 2). The input and output
ports for every (atomic and coupled) model is shown in
the left-hand and right-hand side of each block, respectively.
For example, the Aircraft model has input ports temp and
ﬂight and output ports trajectory and cabinPressure.
It is useful to analyse Figure 2 from a collaborative session
point of view. Let us suppose the Controls model is being
developed in San Francisco, the Navigation model in
Atlanta, and the Cabin model in Phoenix and Boston.
Assuming that each member of the modelling group is
developing a subset of the models for the System, the
collaborative environment provides an anyplace/anytime
workspace local to each modeller, but also supporting
a shared workspace for all the modellers. The ability to
develop, view, and subsequently modify hierarchical models
is essential since all modellers need to work both independently (to specify their models of the system parts) and
collaboratively (to ensure their models are synthesized in
accordance to a common set of requirements and abstraction of the system).
Figure 2 also shows the three types of coupling that can
be speciﬁed: (i) external input coupling from the Aircraft
model component to the Cabin model component,
(ii) internal coupling from the Cabin model component to
the Controls model component, and (iii) external output
coupling from the Controls model component to the
Aircraft model component. The modularity afforded by
the modelling approach enables modellers to develop their
own models independent of how their collaborators develop
theirs. The consequence is that a modeller would be able
to devise the components of the Aircraft and how they
inﬂuence one another through output/input couplings while
making the interface (ie input and output ports) of the
Aircraft model available to his/her collaborators.

4.2. CDM software architecture
The CDM environment shown in Figure 3 is built based on
the CDNS (Park, 1998; Sarjoughian et al, 1999b). CDNS
provides collaborative session management and basic object
exchange capability via Client and Server CDNS modules. It
separates Individual Tasks and Group Tasks consistent with
the collaborative system-theoretic modelling concepts and
the rules deﬁned in Section 4. The Server Modelling Engine
is responsible for the Group Tasks: maintaining the master
copy of the model, coordinating client access to the model,

HS Sarjoughian et al—Towards collaborative component-based modelling 85

Figure 2

Component representation of the Aircraft model.

Client Modeling Engine
local
model

Client

logical
modeling
component

Networking Tasks:
• connect client to server
• send model to server

visual
modeling
component

Individual Tasks:
• add component
• add link

Client
CDNS

network

Server
CDNS

Group Tasks:
• create model
• update model

server

Networking Tasks:
• grant/deny client access
• broadcast model updates

logical
modeling
component

master
model

Server Modeling Engine

Figure 3

Software architecture for the Collaborative DEVS Modeller.

and informing clients whenever the master model is changed.
The Client Modelling Engine is responsible for Individual
Tasks; it maintains a local copy of the model that is

synchronized with the master copy and is used only for local
tasks (eg generating views and performing local consistency
checks before sending change requests to the server).

86 Journal of Simulation Vol. 5, No. 2

Collaborative distributed network system. The CDNS is
a middleware supporting initiation, termination, and interactions among a set of distributed software applications.
This application-neutral middleware, implemented in the
Java programming language, provides a ﬂexible foundation
for software components (ie Client Modelling Engine and
Server Modelling Engine) to send and receive messages
transparently. The CDNS architecture is deﬁned in terms of
two layers: the system layer and the application layer. The
system layer can support multiple concurrent servers and
each server can manage multiple collaborative modelling
sessions. Within this environment, a client can be engaged
concurrently in two or more different modelling sessions
that are hosted by one or more servers. The application
layer provides general session-client and session-server
services that can be specialized and used for tools such
as CDM.
The system and application layers support transparent
complex object transmissions (such as trees and lists)
through automatic object encoding and decoding that is
necessary for distribution across the collaborative workspace. Speciﬁcally, it provides a set of primitive operations
including send and receive methods between a client and a
server. Examples of such operations would be sending a
model from the server to the client. It also provides

Figure 4

commands such as connection request and indication object,
where the former can be used to create a modelling session
and the latter to conﬁrm its creation.
CDNS provides its own user interface for managing
collaborative sessions. Every modeller initially is provided
with two windows: Session Manager and System Information.
Before any modeller (client) can request create/delete or join/
leave operations, the modeller needs to ﬁrst enter a collaborative workspace hosted by a server. Correspondingly,
a modeller can exit the collaborative workspace at any
time during a collaboration session. These operations are
supported by Enter and Exit commands from the Session
Manager window (see Figure 4). Additionally, CDNS also
supports Create, Delete, Join, and Leave operations.
The Create and Delete operations allow creation of
a collaborative session and its deletion. The Join and
Leave operations enable a modeller to join or leave an
existing collaborative session. For example, in Figure 4,
ais1 is a host server offering two collaborative modelling
sessions: Demo1-HostA and Demo3-HostA. Each
host session on ais1—Demo1-HostA@ais1 and Demo3HoatA@ais1—has its own model. A prospective modeller
may enter ais1 and upon successful entry, for example, join
the Demo3-HostA collaborative modelling session. Another
modeller can initiate his own collaborative modelling session

Collaborative modeller workspace for two separate hosts Collaborative Distributed Network System.

HS Sarjoughian et al—Towards collaborative component-based modelling 87

(ie Demo2-HostB) on the ais4 host server and join the
Demo1-HostA collaborative modelling session on ais1.
Thus, a modeller, who may be hosted by one server can join
multiple modelling sessions hosted by another server or
multiple other servers. Multiple modellers hosted by
different servers may collaboratively develop a model using
a standalone modelling session (ie a server owns the model
and handles the modellers’ development activities).

Client and server modelling engines. The client and
server both have modelling engines that communicate
using the services provided by CDNS. This architecture is
based on the principle of a layered software architecture
comprised of generic network services (eg broadcasting
a legitimate model change to all members of a collaborative
modelling session) and application-speciﬁc capabilities
(eg adding a port to a model).
The Modelling Engine for the client consists of logical and
visual modelling components. The visual modelling component accepts user commands and which are then send to
the server. As describe above, the server must verify the
user commands are legitimate (eg a model can be added
to a coupled model without violating the strict hierarchy
constraint). For every legitimate modelling action, as deﬁned
in Tables 3 and 4, the local copy of the model is modiﬁed
and the visual model is updated accordingly. The local
model supports visual model manipulation and identifying
operations that are legitimate given the local model, but still
have to be veriﬁed by the server.
A client may change the master model by sending a
change request to the server. The server veriﬁes the change
against the master model by checking that the modelling
rules (see Tables 3 and 4) are not violated. The server then
notiﬁes the client as to whether the change was successfully
applied to the master copy and, if it was successful, also
notiﬁes other clients of the change. The clients, in turn,
update their local models as directed by the server. Clients
only change their local model when directed to do so.
The Modelling Engine for the server consists of a logical
modelling component and a master model. The logical
modelling component extends the capability of the clientside logical modelling component by guaranteeing modellers’ actions are processed in a well-deﬁned order. The
server-side logical modelling component orders modellers’
requested actions in a FIFO queue. The requests in every
queue are processed one at a time, and those that are
legitimate are applied to the master model and pushed to all
of clients including the one who requested the operation.
Messages from the server to a client use the TCP/IP protocol
to ensure reliable, in order delivery. Clients apply change
notiﬁcations to their local model in the order that they are
received from the server.
The only exception to this model update procedure is for
clients that are joining a session. A join request is queued by
the server in the same way as model change requests. When

the join request is processed, the server sends a complete
copy of the master model to the joining client. This ensures
that new clients begin with a model that is consistent with
the master model and the current view of all other clients.
This arrangement allows for synchronous collaboration;
each user observes the changes as they occur. If server push
updates are removed from this design, only asynchronous
collaboration is possible. Whether synchronous or asynchronous, in order for every client to maintain a consistent
model, all clients must process operations in the order in
which they were received from the server, and message
delivery must be reliable. The CDM server guarantees a
global ordering of all model actions; this is important
to ensure consistent logical and visual views of model
modiﬁcations among all modellers. The master model has an
important role since it supports maintaining consistency
among all local models of those modellers who are
participating in a modelling session. The server and clients
collectively are responsible for maintaining the consistency
of all local models with the master model during a
collaborative modelling session.

5. Conclusions
We have presented a collaborative system-theoretic modelling environment called CDM, which extends the DEVS
modelling concepts and methods for use in group settings.
This approach to collaborative modelling introduces a novel
alliance between the systems-theoretic modelling paradigm
and CSCW for constructing and synthesizing hierarchical
component-based simulation models. The realization of the
CDM enables developing structural DEVS models in a
collaborative setting. It offers modellers the ability to develop structural atomic and coupled DEVS model components. The underlying approach can be extended to support
continuous and discrete-time modelling formalisms. The
CDM tool may also be used more generally for hierarchical
component-based modelling that uses component, ports,
and couplings. The modelling environment facilitates multiple, independent modelling sessions, management of modelling actions, and model persistent with complementary tree
structure and block diagram model views.
The collaborative modelling approach and its realization
provide a basis to handle basic modelling activities that
are required in developing simulation models. The underlying collaborative framework supports model creation,
management, persistence, and visualization. Since the
systems-theoretic approach to modelling dynamical systems
is modular and hierarchical, its collaborative realization
extends these traits in group settings. The separation of
logical and visual models is supported by use of databases
which also can simplify creation and use of model libraries.
In terms of future research, this environment offers a basis
for automatic mapping of the models to their counterpart

88 Journal of Simulation Vol. 5, No. 2

simulations that are amenable for simulation. The underlying infrastructure of the CDM can support adding the
capability to model behaviours of atomic model components. Such a capability can support speciﬁcation of state
transitions, inputs and outputs, and output functions of
atomic models and thus can pave the way to automatically
generate executable simulation models.
Acknowledgements—The authors are grateful to the anonymous
referees for providing constructive reviews of an earlier version of
this article.

References
AnyLogic (2008). XJ Technologies. http://www.xjtek.com/anylogic/,
accessed May 2009.
Bidarra R et al (2002). A collaborative framework for integrated
part and assembly modeling. J Comput Inform Sci Eng 2(4):
256–264.
Burmester S et al (2005). Visual model-driven development of
software intensive systems: A survey of available techniques and
tools. In: Proceedings of the Workshop on Visual Modeling for
Software Intensive Systems (VMSIS) at the IEEE Symposium
on Visual Languages and Human-Centric Computing (VL/
HCC 0 05) Dallas, TX, USA. IEEE Computer Society: Los
Alamitos, CA.
CanyonBlue (2005). Enterprise Konesa. http://www.canyonblue
.com/products.htm, accessed 30 March 2007.
CodeBeamer (2004). Collaborative software development solutions.
http://www.intland.com, accessed 23 April 2007.
CoSMoS (2009). Component-based System Modeler and
Simulator. http://sourceforge.net/projects/cosmosim, accessed
15 March 2010.
Delinchant B et al (2004). A component-based framework for the
composition of simulation software modeling electrical systems.
Simul Trans 80(7–8): 347–356.
DEVS-Suite (2008). DEVS-suite simulator. http://devs-suitesim
.sourceforge.net/, accessed 30 April 2009.
Eldabi T et al (2004). Examining the feasibility of constructing
simulation models using the web-based ‘grab-and-glue’ Framework. In: Winter Simulation Conference, Washington DC,
ACM, NY.
Fernández A et al (2004). Guided support for collaborative
modeling, enactment and simulation of software development
processes. Software Process Improve Pract 9: 95–106.
Filho WA et al (2004). GroupSim: A collaborative environment for
discrete event simulation software development for the World
Wide Web. Simul Trans 80(6): 257–272.
Grudin J (1994). Computer-supported cooperative work: History
and focus. IEEE Comp 27(5): 19–26.
Grundy J et al (1998). Serendipity II: A decentralized architecture
for software process modeling and enactment. IEEE Internet
Comp 2(3): 53–62.
Hill R et al (1994). The rendezvous architecture and language for
constructing multiuser applications. ACM Trans ComputerHuman Interact 1(2): 81–125.
IEEE (2003). HLA federation development and execution process
Version IEEE 1516.3. IEEE: New York.
Jacovi M et al (2006). The chasms of CSCW: A citation graph
analysis of the CSCW conference. Computer Supported
Cooperative Work: Alberta, Canada.
Joshi G (2004). Collaborative component-based modeling using
relational databases: Software design and implementation.

Computer Science and Engineering. Master Thesis, Arizona
State University, Tempe, AZ, p 111.
Kim Y et al (2001). Collaborative surgical simulation over the
Internet. IEEE Internet Comp 5(3): 65–73.
Lee J et al (1998). A group-based approach for distributed model
construction. In: 31st Hawaii International Conference on System
Sciences, Big Island, HI, USA. IEEE Computer Society: Los
Alamitos, CA.
Mathworks (2002). MATLAB. http://www.mathworks.com/,
accessed 18 June 2008.
Park S (1998). Collaborative distributed network system architecture:
Design and implementation. Electrical & Computer Engineering
Department, University of Arizona: Tucson, AZ, p 120.
Pidd M (2002). Simulation software and model reuse: A polemic.
In: Winter Simulation Conference, San Diego, CA: ACM Press,
NY.
Rhee I (ed.) (1999). Support for global teams, Guest Editor’s
Introduction. IEEE Internet Comput 3(2): 30–32.
Sarjoughian HS (2005). A scaleable component-based modeling
environment supporting model validation. In: 39th Interservice/
Industry Training, Simulation, and Education Conference,
Orlando, FL, USA, IEEE Computer Society: Los Alamitos,
CA.
Sarjoughian HS et al (1997). Group-enabled DEVS model construction methodology for distributed organizations. In: 11th
SPIE, Orlando, FL. The Society for Modeling and Simulation
International, CA.
Sarjoughian HS et al (1999a). Collaborative DEVS modeler. In:
Western Simulation Multiconference, San Francisco, SCS. The
Society for Modeling and Simulation International: Alamitos CA.
Sarjoughian HS et al (1999b). Collaborative distributed network
system: A lightweight middleware supporting collaborative
DEVS modeling. Future Generat Comp Syst 17: 89–105.
Subramanian S et al (1999). Software architecture for the
UARC web-based collaboratory. IEEE Internet Comput 3(2):
46–54.
Sun C et al (1998). Achieving convergence, causality, preservation,
and intention preservation in real-time cooperative editing
systems. ACM Trans Computer-Human Interacts 5(1): 63–108.
Taylor SJ (2001). Netmeeting: A tool for collaborative simulation
modeling. Int J Simul Syst, Sci Technol 1(1–2): 59–68.
Usability (2004). First Groupware. http://www.usabilityﬁrst.com/
groupware/, accessed 21 April 2007.
Wymore AW (1993). Model-based Systems Engineering: An Introduction to the Mathematical Theory of Discrete Systems
and to the Tricotyledon Theory of System Design. CRC: Boca
Raton.
Xia J et al (2001). Three-dimensional virtual-reality surgical
planning and soft-tissue prediction for orthognathic surgery.
IEEE Trans Inform Technol Biomed 5(2): 97–107.
Yang Y et al (2001). Real-time cooperative editing on the internet.
IEEE Internet Comput 4(3): 18–25.
Zeigler BP et al (2000). Theory of Modeling and Simulation: Integrating Discrete Event and Continuous Complex Dynamic Systems.
Academic Press: New York.
Zhang L et al (2002). A feature-based collaborative CAD system.
In: The 7th International Conference on Computer Supported
Cooperative Work in Design, pp 193–197, Rio de Janeiro, Brazil.
IEEE Computer Society: Los Alamitos, CA.

Received 29 July 2008;
accepted 9 November 2009

Proceedings of the 2005 Winter Simulation Conference
M. E. Kuhl, N. M. Steiger, B. F. Armstrong, J. E. Joines, eds.

HYBRID DISCRETE EVENT SIMULATION WITH MODEL PREDICTIVE CONTROL FOR
SEMICONDUCTOR SUPPLY-CHAIN MANUFACTURING
Hessam S. Sarjoughian
Dongping Huang
Gary W. Godding

Wenlin Wang
Daniel E. Rivera

Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Dept.
Arizona State University
Tempe, AZ 85287-2803 U.S.A.

Control Systems Engineering Laboratory
Chemical & Materials Engineering Dept.
Arizona State University
Tempe, AZ 85287-2803 U.S.A.

Karl G. Kempf

Hans D. Mittelmann

Decision Technologies
Intel Corporation
Chandler, AZ 85226 U.S.A.

Mathematics & Statistic Dept.
Arizona State University
Tempe, AZ 85287-2803 U.S.A.

process models independently, modeling of their interactions depends mostly on customized abstractions viewed
from decision models or simulation models. For example,
we can consider discrete event and model predictive control to represent process dynamics and decision controls
that can manage material flow of a semiconductor supplychain, respectively. Discrete Event Simulation (DES) is
generally considered suitable for modeling and simulating
the operations (processes) of semiconductor supply-chain
network systems. Likewise, a variety of optimization algorithms are most suitable for planning. An attractive approach for planning is known as Model Predictive Control
(MPC) (Wang, Rivera and Kempf 2005). Unlike Linear or
Integer Programming which is considered for strategic
control of supply-chain processes, MPC is aimed at tactical
control. Tactical control is concerned with short term
(daily to several weeks) decision making whereas strategic
control focuses on long term decisions (few to several
months).
Use of different modeling methodologies requires integrating the process dynamics and decision planning.
Such integration could be specified at different levels of
abstraction. Integration of models could be via low-level
programming, high-level interoperability techniques, or
multi-model composability approaches. The basic concept
and approach for Knowledge Interchange Broker (KIB)
was proposed to support multi-formalism model composition [i.e., discrete-event and agent models (Sarjoughian and
Plummer 2002) and more recently has been used for composing discrete-event and linear program models for semi-

ABSTRACT
Simulation modeling combined with decision control can
offer important benefits for analysis, design, and operation
of semiconductor supply-chain network systems. Detailed
simulation of physical processes provides information for
its controller to account for (expected) stochasticity present
in the manufacturing processes. In turn, the controller can
provide (near) optimal decisions for the operation of the
processes and thus handle uncertainty in customer demands. In this paper, we describe an environment that synthesizes Discrete-EVent System specification (DEVS) with
Model Predictive Control (MPC) paradigms using a
Knowledge Interchange Broker (KIB). This environment
uses the KIB to compose discrete event simulation and
model predictive control models. This approach to composability affords flexibility for studying semiconductor
supply-chain manufacturing at varying levels of detail. We
describe a hybrid DEVS/MPC environments via a knowledge interchange broker. We conclude with a comparison
of this work with another that employs the Simulink/MATLAB environment.
1

INTRODUCTION

In the semiconductor supply-chain, models of decision
planning and manufacturing processes are essential to deal
with frequently changing customer demands while remaining financially competitive (Kempf 2004). While there exist well-known approaches to model complex decision and

256

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
conductor supply-chain networks (Godding, Sarjoughian
and Kempf 2004)]. The KIB prescribes data and control
mapping, synchronization, concurrency and timing across
composed formalisms. Given these basic capabilities,
composition of different modeling formalisms are specified
in terms of generalized and domain-specific syntax and
semantics.
In this paper we describe a prototype environment
where a discrete event semiconductor process model is
composed with a model predictive control decision model.
In Section 2, we highlight closely related work in modeling
and simulation of semiconductor supply-chain networks
considered in this paper. In Sections 3 and 4, we describe
the DES and MPC models that are to be composed. In
Section 5, we describe a general approach to composing
DES and MPC modeling approaches. A prototyped environment integrating the DEVSJAVA and MATLAB environments is described in Section 6. Conclusions and future
work are discussed in Section 7.
2

discrete-time simulation model of the discrete event manufacturing processes with an MPC consisting of a linear,
time-invariant process model and a quadratic programming
optimization model (Wang, Rivera and Kempf 2005).
This Simulink/MATLAB block diagram discrete-time
modeling engine provides connectivity to the MPC tool
box. Synchronous interactions between discrete-time and
MPC models are supported for primitive data types, although given the knowledge for the built-in Simulink/MATALB data and control schemes, customized exchange may be developed. These customized interactions,
however, seem difficult to support in a generic setting and
thus rely on ad-hoc customization in developing process
and MPC models and, more importantly, their combination. This observation can be made about other environments where a core modeling and simulation paradigm is
extended with others using a combination of interoperability and software engineering techniques.
Finally, in recent years, model composability has attracted researchers and increasingly turned their attention
to developing concepts and techniques to tackle inherent
complexities associated with component-based model
specification. The main goal of these efforts is to develop
basic theories and techniques to tackle dissimilarities of
models to be synthesized using model abstractions, metamodeling, and model transformation [for examples see
(Sarjoughian and Cellier 2001; Mosterman and Vangheluwe 2002; Davis and Anderson 2004)].

RELATED WORK

Numerous articles have been devoted to the study of combining manufacturing processes and decision control models for the domain of semiconductor supply-chain networks. A common approach is to develop customized
code to integrate different simulation and planning environments. These customizations may use a variety of software engineering techniques ranging from low-level programming
to
middleware
technologies
where
interoperability concerns between disparate implementation can be systematically accounted for. These approaches
primarily rely on interoperability to handle model composability.
Recently an agent framework has been proposed as a
common basis to model and integrate different parts of
supply-chains. In particular, a multi-agent framework has
been developed where physical and decision models are
integrated using a library of common elements of a supplychain such as factories and control policies, and other elements that support their interactions (Swaminathan, Smith
and Sadeh 1998). An alternative approach has been developed using the concept of Knowledge Interchange Broker
to compose Discrete-Event Simulation (DES) and Linear
Programming (LP) optimization models (Godding, Sarjoughian and Kempf 2004). In this work, appropriate data
transformations and control have been developed for models that can be described in the DEVS and LP formalisms.
To our knowledge, no modeling and simulation environment can support hybrid discrete-event modeling and
model predictive control in general and in particular for
semiconductor supply-chain networks. Instead, there exist
approaches where MPC is used with continuous and discrete-time models. For example, the commonly used Simulink/MATLAB environment has been used to develop a

3

PHYSICAL PROCESS MODELING

Common physical models of manufacturing a supply-chain
network consists of four types of nodes (models). These
are inventory, factory, shipping link, and customer. These
entities have common structures and behaviors. It is, therefore, important to develop a common interface specification for these nodes. Each node type must be able to receive materials or products (data) and accept decision
commands (control). Specific functions need to be specified for each node type. Generally material flow is assumed to be uni-directional (feed-forward) from the supplier to final customers. Control flow, however, may
include feedback.
Release (or receipt) of materials from inventory (or
other nodes) can be characterized in terms of (i) quantity,
(ii) type, (iii) time, and (iv) destination. Quantity refers to
the number of (specific) items to be built and/or sent out.
Type distinguishes among different kinds of materials to be
sent out. Time refers to a specific time instance for release
of materials. Destination refers to nodes that are to receive
the released materials.
The inventory model has capacity and delay. The
states of the inventory model (e.g., Die and Package) include inventory levels (possibly stochastic) for each prod-

257

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
uct it can hold. Inventory stores materials at the time they
are received and releases them as it is instructed.
The factory model represents manufacturing, assembly, test, and finish processes, or some combination such as
assembly and test (see Figure 1). It can have capacity,
throughput time (TPT), and yield. The actual values of TPT
and yield are generally stochastic; it may depend on the
current load. The factory node can change, assemble, and
split products. With the change operation, one input product is made into another product. For example, as shown in
Figure 1, raw silicon wafers are fabricated into die and
then tested in Fab/Test1. The assembly operation represents two or more products that are assembled into one
product. In the Assembly/Test2, one package must combine with one die. There must exist enough packages and
dies to begin their assembly into semi-finished goods. The
split operation involves the manufacturing of two products
with different properties. For example, the items coming
into Assembly/Test2 may be stochastically split into two
bins, one of which contains high-speed devices while the
other contains low-speed devices. This kind of separation
can also be modeled as two separate inventories. The assembly operation is generally controlled externally (via decision models), while the split operation is a characterization of the factory itself.
The shipping node models transportation delay. It can
be thought of as a specialized factory model where it cannot change products. The customer node models customer
behavior. It can send product demands to both the decision
system and process system (e.g., customer warehouse) for
decision making and order processing respectively. Two
types of customer demands may be modeled: forecasting
demands and actual demands.

External and local controls are defined for the supplychain network nodes. The external control represents the
control commands from the decision model to the supplychain network nodes. For example, the release commands
for each inventory can be determined using an optimization
scheme given the current inventory levels, factory work-inprogress (WIP), customer demands and some constraints
among them. Some decisions, however, may be formulated
locally. For example, an inventory may release a quantity
of products given the external control release command.
However, an inventory’s external control release may be
constrained given the maximum capacity of the receiving
(downstream) node and its available inventory level. Such
a local control policy is shown in Figure 2.
release command

available
quantity

Fab/Test1

Die

Package

Assembly/
Test2

SemiFinished

Finish

local control

Figure 2: Local Control Policy for Inventory
3.1 DEVS Models
The above manufacturing process network dynamics are
modeled and simulated using the Discrete Event System
Specification (Zeigler, Praehofer and Kim 2000). This approach supports developing models with well-defined
structures and behaviors. It offers a framework for modeling and simulating discrete or continuous systems as
atomic and hierarchical coupled models. The simulation of
models is based on a simulation protocol that ensures correct execution of the models—i.e., enforcing causality,
concurrency, and timing among atomic and coupled models.
Atomic models have input/output ports and values.
The behavior of the atomic model is specified in term of
state variables and functions. A model can have autonomous and reactive behavior specified in terms of internal
transition and external transition functions. Output function allows the model to send out messages. Time advanced function captures timing of models. Confluent
function can be used for modeling simultaneous internal
events and external events. Every coupled model has welldefined interfaces, as in atomic models, and consists of one
or more atomic model or coupled models. Complex models
can be hierarchically constructed from these two types of
models.

Customer
Warehouse

Manufacturing Process Network

actual release

maximum
downstream capacity

Customer

Raw
Resource

∑

external control

Shipping

Figure 1: Semiconductor Manufacturing Process Network
Ports can be used to represent input/output interfaces
for all network nodes. The ports are distinguishable as data
and control input and output ports (Singh, Sarjoughian and
Godding 2004). A node may have multiple data ports to
support sending different products to multiple destinations.
The decisions of how much (quantity), what (material
type), time (when to send), and destination are determined
based on external information that is local to the process
model. For example, where to send material is determined
by a decision model and product split is determined by the
manufacturing node.

258

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
Optimizer. The real system has been modeled using
DEVSJAVA. In this paper, the real-system is replaced with
a DEVS simulation model. In this study, the operation of
MPC can be described informally as follows. The real system (i.e., simulatable DEVS process model) sends its current outputs (e.g., semi-finished goods inventory level) to
the system prediction model given measured disturbances
(i.e., actual customer demand). The system prediction
model then computes future outputs (i.e., controlled outputs) for some finite number of time steps. The error between future outputs and target trajectories (i.e., expected
customer demand) is sent to the optimizer where optimized
control outputs (referred to as manipulated variables) are
calculated based on some constraints and objective functions over some time horizon—i.e., moving horizon (for
manipulated variables) and prediction horizon (for controlled variables). This optimization will be repeated using
the receding horizon concept once the new information is
available. In addition, the MPC has a filter gain that can
respond quickly to inevitable signal to noise ratio changes
while avoiding undesirable oscillatory control regimes.
The predictive control for the first time step is sent to
simulated system as well as the system prediction model.
The above steps are repeated using the updated simulated
system states and disturbances for a desired simulation period.

Inventory-Factory Coupled Model
Control-out

Factory

Output port

Data-out

Input port

Material flow

localControl-out

Data-out

External control flow

Data-in

Data-out

Data-in

Data-in

localControl-in

Control-out

Control-in

Control-out

Control-in

Control-in

Inventory

Local control flow

Figure 3: Structural Composition of Inventory and Factory
Models
The supply-chain network nodes lend themselves to be
specified in terms of atomic and coupled models. These
models can be interconnected to form an arbitrary supplychain network using coupling topologies. Figure 3 shows a
coupled model which includes one atomic inventory model
and one atomic factory model where product and local and
external controls are separately modeled.
DEVSJAVA is a software environment that was developed based on the DEVS framework and implemented
in the JAVA programming language (DEVSJAVA 2002).
It provides a component-based (object-oriented) engine for
atomic and coupled simulation modeling. The combination of component-based modeling and software implementation of DEVSJAVA provides flexibility and scalability to develop the supply-chain network processing models.
4

Measured disturbances D(k)
(actual customer demand)

Real System or
Simulated System

Target trajectory
r(k)…r(k+P)

Output variables Y(k)
(inventory levels)

MODEL PREDCITIVE CONTROL MODEL

Measured disturbance
with anticipation D’(k)
(customer forecasted
demands)

Model Predictive Control (MPC) is commonly used for
control of highly stochastic processes where selection of
control actions, based on optimization, is desired. The importance of MPC compared with traditional approaches is
due to its suitability for large multi-variable systems, handling of constraints placed on system input and output
variables, and its relative ease-of-use and applicability. In
MPC, current and historical measurements of a process are
used to predict its behavior for future time instances. A
control-relevant objective function is optimized for calculating a sequence of future control commands that can satisfy some predefined system constraints. For details of the
objective function, refer to (Wang, Rivera and Kempf
2005).
It has been shown that MPC can be used effectively as
a tactical controller for high volume supply-chain semiconductor networks having capacitated, nonlinear, and stochastic demands (Wang, Rivera and Kempf 2005). An
MPC design for semiconductor supply-chain is shown in
Figure 4. It consists of a System Prediction Model and an

System
Prediction
Model

Future outputs
Y(k+1)…Y(k+P)

+
─

Future errors

Optimizer
Manipulated variables U(k)
(starts of factories)
Objective functions

Constraints

MPC

Figure 4: Canonical MPC Model
The MPC is supported by commercial tools such as
MATLAB (Mathworks 2002). In our work, the
MATLAB’s QP solver is replaced with the more efficient
and robust MATLAB-QP version of the interior point NLP
code LOQO (Vanderbei 1999). MATLAB supports a
graphical user interface for modeling and observing simulation results.

259

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
5

COMPOSING DEVS AND MPC USING KIB

starts of each factory Uk −1,L,Uk −m , and the previous
customer demands Dk −1 , Dk −2 ,... . To calculate the next
start for each factory, the controller operates in two
phases:

The key concept behind Knowledge Interchange Broker is
the ability to compose models even though each model has
its own distinct syntax and semantics. Clearly, not only
must each model be executed using its own well-defined
protocol, but their interoperation must also be guaranteed
correct. In the case of DEVS and MPC, the KIB’s execution engine is designed to ensure that the DEVSJAVA
simulation and the MATLAB solver interact correctly.
Next, we detail the generic KIB approach for composing
and interoperating the classes of DEVS and MPC modeling
and simulation paradigms.
For tactical control of a semiconductor supply-chain, we
consider composing a discrete event simulation (i.e., DEVS)
and a tactical controller (i.e., MPC) for the simple example
shown in Figure 4. The system prediction model of the
DEVS Process Model used within the MPC is modeled as
discrete linear and time-invariant model (see Figure 4). The
system predictive model is described as in equation (1).
X k +1 = AX k + BuU k + Bv Dk
Y k +1 = CX k

(a) Estimation. The controller uses all the past measurements, inputs, and the current controlled variables to
calculate the inventory levels for a prediction horizon
P (a finite integer ≥ 1 ), Yk +1 , Yk + 2 ,...,Yk + P .
(b) Optimization. Values of the future inventory level trajectories, anticipated customer demands, and constraints are specified over a finite horizon of future
sampling instants k + 1, k + 2,K, k + P . By solving a
constraint optimization problem, it computes the starts
of
each
factory
in
the
future
horizon M ( P ≥ M ≥ 1 ), U k ,U k +1 ,..., U k + M −1 .

3.

4.
(1)

my

is a vec-

tor of output (or controlled) variables, and Dk ∈ ℜ mv is a
vector of measured disturbance variables.
As a tactical controller, the MPC manipulates the
starts of the factories to satisfy the forecasted customer
demands ( D ' k ) given the actual customer demands ( Dk )

X = {(data − in, Lots ), (control − in, Command ),

while maintaining the inventories at their desired levels.
The controlled variables are the inventory levels; the manipulated variables are starts of each factory; the customer
demands are treated as measured disturbance variables
with anticipations. We note that the MPC does not receive
the actual releases (see Figure 2) from the manufacturing
process model, although a different formulation of the
MPC may.
The MPC manipulates the start of the factories of the
semiconductor process model as follows. We have simplified the process model to have one product.
1.

2.

5.1 Structural Composition Specification
Specification of model compositions needs to support
structural composition of DEVS and MPC models. As described earlier, (data and control) input and output ports are
used as interface structures for DEVS models to interact
with DEVS and non-DEVS model components. The interface structure of an (atomic or coupled) model consists of
sets of input and output ports with values to be exchanged
with model components. An example of structural specification of an inventory model is given as equation (2).

where U k ∈ ℜ mu is a vector of manipulated variables,
X k ∈ ℜ mx is a vector of state variables, Yk ∈ ℜ

The starts at time k ( U k ) are sent to the process simulation model. Each inventory model releases products
to its downstream factory given its local control policy
shown in Figure 2.
At the next time interval k+1, continue with step 2.

(localControl − in, Capacity )}
Y = {(data − out , Lots ), (control − out , [BOH , Released ])}

(2)

The set of inputs and outputs are X and, Y with (port,
message) representing port name and message type. The
message can be complex such as Lot which contains a collection of multiple product types with different cardinalities. In DEVSJAVA, each lot is modeled to have a default
or user-specified size (e.g., 100 products per lot).
For the MPC model, its discrete-time system model
(denoted as I10, I20, I30, M10, M20 and M30 in Figure 5) and
optimizer do not use ports. Instead, the interface structural
specification of MPC is “vectors of variables.” The input to
the controller from the simulation system is a vector of
controlled variables and a vector of measured disturbance
variables, while the output from the controller to the simulated system is a vector of manipulated variables (see Figure 5). Table 1 shows an example of input/output mapping
between the DEVS and MPC models.

At the initialization, the inventory set-point trajectories
are specified. The model attributes such as average TPT
and yield for each factory model are set. The distribution of some stochastic behaviors, such as distribution
of the TPT and yield, are also set at initialization.
At time interval k, the MPC receives the current inventory levels ( Yk ). It also receives forecasted customer
demands ( D ' k ). The system prediction model has the
previous inventory levels Yk −1 , Yk −2 ,... , the previous

260

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
sages) into a single data value (a message) and the latter
refers to the inverse. For example, an MPC data variable
can be disaggregated so that it can be sent to multiple
DEVS input ports.

Model Predictive Controller
Output vector (ctrlOut)

Input vector (u)

Knowledge Interchange Broker
BOH

BOH

BOH

Demand

5.2 Behavioral Composition Specification
Customer
Command

Raw
Resource

Command

Execution of composed DEVS and MPC models needs to
be guaranteed correct. To satisfy correct execution of process and decision models, we need to devise synchronization and timing that conform to the DEVS simulation protocol and the MPC’s simulation algorithm and solver.
The DEVS specification includes a set of states and
transition functions. The state variables (e.g., average
throughput time and average yield in the factory model)
must be updated such that output to input causality and
timing are satisfied. This requires ensuring the ordering of
external, internal, and output function executions while interacting with the KIB (and therefore the MPC).
Likewise, the execution of the MPC must be ensured
given its interaction with the KIB. The system prediction
model has a homomorphic relation to the DEVS process
mode. The optimizer is a set of constraints and objective
function specification based on mass conservation relationships among the inventory, manufacturing, and transportation models. For example, the mass conservation relationships for Die/Package inventory level (I10) and for
Fab/Test1 WIP (M10) can respectively be expressed as
equation (3) (Wang, Rivera and Kempf 2005):

Command

Fab/Test1

Die/
Package

Assembly/
Test2

SemiFinished

Finish

M10

I10

M20

I20

M30

Customer
Warehouse

I30

Shipping

Manufacturing Process Network

Figure 5: Composition of Manufacturing Process Network
and MPC via KIB
The structural specification must provide well-defined
structural information translation from the DEVS model to
the MPC and vice versa. Clearly, the data types described
in the two modeling formalisms are different. Messages
used in the DEVS models are of type Entity, which is the
base data structure used for building messages with arbitrary complexity. The MPC variable types, in contrast,
generally have primitive types (e.g., reals), although complex variable types may be defined by users.
Table 1: Message Mapping for DEVS and MPC Models
DEVS Model
MPC Model
(Inventory, control- yi, a member of vector my conout, BOH)
trolled variables
(Inventory, control- ui, a member of vector mu main, command)
nipulated variables
(Customer, control- vi, a member of vector mv
out, demand)
measured disturbance variables

I10 (k + 1) = I10 (k) + Y1C1 (k −θ1 ) − C2 (k)
WIP10 (k + 1) = WIP10 (k) + C1 (k) − Y1C1 (k −θ1 )

(3)

The variables θ1 and Y1 represent the nominal
throughput time and yield for the Fab/Test1 node, while C1
and C2 represent the daily starts that constitute inflow and
outflow streams for I10 and M10. Similar constraints can be
written for other elements of the manufacturing process.
Consistent model attributes of the composed model—
e.g., stochastic and non-stochastic TPT and Yield for the
simulated (e.g., Fab/Test1) and predictive factory (e.g.,
M10) models, respectively—are required. That is, the
common attributes of the process and decision models
must be kept consistent since they represent semantically
the same knowledge across the composed model. These
attributes, however, do not necessarily have to be identical,
but they carry the same information at different levels of
abstraction. The consistency of the model attributes and
their exchanges must be ensured by the KIB both at initialization of and during the simulation.
Both DEVS and MPC models have well-defined timing
properties. The former has a continuous-time base while the
latter has a discrete-time base. Since the MPC models execute using time-stepping, the KIB’s input and output events
are synchronized using a discrete time clock. This is appro-

The principles of knowledge reduction and augmentation are important for handling differences between two
modeling formalisms. Knowledge reduction is generally
simpler as we throw away information in the process of
translating one message type to another. For example, the
message Beginning On Hand (BOH) in the DEVS inventory model represents current inventory level. It may contain not only the product amount but also the product
name. When the message is transformed to an MPC variable, the name may not be necessary. In contrast, knowledge augmentation is more difficult. For example, the manipulated variable sent from the MPC to the DEVS model
represents a release command for some specific product
from a specific inventory. How to augment (map) the MPC
release command such that it becomes a DEVS message
needs to be specified based on the MPC and DEVS formalisms (see Section 6 for details).
Knowledge reduction and augmentation can contain
data (message) aggregation and disaggregation. The former
refers to combining multiple data values (or a set of mes-

261

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
priate since the physical process model timing is a multiple
of the decision model timing. For example, a process simulation execution cycle can be hourly, daily, or weekly. The
decision planning can run at the same frequency as the process model simulation, faster, or slower. For example, the
process model can be simulated using daily time-step while
the planning can have a weekly time horizon.
From the MPC point of view, when it receives the
process model states (i.e., controlled variables), the controller assumes the process model states remain static while
it computes the values of the manipulated variables and
gives control back to the process model—until it receives
the process model states for the next time step. For the
DEVS models, the timing period from sending out state information to receiving control messages is dependent on
the frequency selected between the two models. If the
process and decision models have the same frequency, the
process model must receive control messages before it
changes its states and starts a new cycle. If they run in different frequencies, the process model must receive the control messages at the correct time step.
For the DEVS/MPC composition, the KIB is designed
to support synchronous control. In this mode, once the
DEVS simulator sends outputs to the KIB, the simulation
stops until it receives the MPC manipulated variables. This
form of synchronization needs to only maintain a single
logical clock for executing composed models. In the prototype described next, the KIB execution protocol synchronization is defined in terms of the DEVS simulation event
ordering and clock.

JNI
(JMatLink)

KIB Data
Transform Manager

MPCInterface

DEVSDecisionInterface

MPCBridge

6

KIB Data
Transform Spec

KIB

Figure 7: Software Components in KIB
6.1 DEVS Decision Interface and MPC-Bridge
The responsibility of this interface is to pass messages
from the DEVS models to the KIB and vice versa. The
messages include initMessage and statusMessage from
DEVS to KIB and controlMessage from KIB to DEVS.
The process model can interact with the KIB via the
DEVSDecisionInterface object method invocations or via
ports (see Figure 8). If we choose method invocation, it
turns out that the communication between the process
model and the KIB is synchronous. If we use ports, we can
take advantage of the DEVS simulation control protocol to
handle timing properties for the composed DEVS and
MPC model. Therefore, a special atomic model MPCBridge was developed as a proxy between the KIB and
DEVS models. It collects state information from the process models through the input ports and then transfers them
to the KIB.

PROTOTYPE KIB DESIGN

StatusMessage

A prototype of the KIB targeted for composing DEVS and
MPC models was designed and implemented using parts of
two existing KIBs (Sarjoughian and Plummer 2002; Godding, Sarjoughian and Kempf 2004). The DEVS/MPC
simplified architecture is shown in Figure 6. It consists of
DEVSJAVA, KIB, and MATLAB.

(from Inventory models)

MPC-Bridge

ReleaseMessage
(to Inventory models)

TimeMessage
(from Clock model)

DEVSJAVA

JAVA

MATLAB

Figure 8: MPC-Bridge Structural Specification
Process Models

coupling

MPC-Bridge

KIB

JNI MPC Function

Similarly, the MPC-Bridge as shown in Figure 8 receives the control messages (i.e., MPC manipulated variables) from the KIB which are then sent to the process
models at appropriate time intervals. The interaction of the
MPC-Bridge and the process models are through DEVS
ports, while the interaction between the MPC-Bridge and
the KIB uses method invocations. We note that the MPCBridge supports the KIB in two ways. First, it acts as a
bridge between KIB and DEVS and second it provides
message synchronization with MPC—data sent to the MPC
model and commands sent to the DEVS model.

Figure 6: DEVS/MPC with KIB Conceptual Architecture
A software design and implementation of the KIB is
shown in Figure 7. It includes DEVSDecisionInterface,
KIB Data Transformation Manager, and MPCInterface.
The DEVSDecisionInterface is the interface between
DEVSJAVA and the KIB. The KIB Data Transform Manager handles data transformation (mappings). The
MPCInterface is an interface between MATLAB and the
KIB.

262

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
A simplified structural composition specification is
shown in Figure 9. The XML specification shows the
DEVS output variables and the MATLAB input variables
(see Equation (4)). The sequence diagram shown in Figure
10 illustrates the processing taking place inside the KIB
including message mapping and transformation. It shows
message passing and method invocations from the MPCBridge to the MPCInterface.

6.2 MPCInterface
This interface defines interactions between the KIB and the
MPC which is specified as a MATLAB function. The MPC
is defined as shown in equation (4).
function ctrlOut = mpcFunction (time, u )
where time∈ Ν , u ∈ℜ 4 , ctrlOut ∈ ℜ 3

(4)

Here time is an input variable representing a discretized time index. The controlled input variable u is a vector. The manipulated variable ctrlOut is the output variable. Both u and ctrlOut variables are of the same type
(i.e., double).
The functionality of the MPCInterface includes (a)
loading the MPC function file to the appropriate internal
MATLAB workspace, (b) transforming the MPC model in
the KIB to the corresponding MATLAB function call, (c)
making the MATLAB call through the JMatLink (Müller
2002) which is based on the Java JNI, and (d) transforming
the output variables from MATLAB to the KIB.
As mentioned above, the MPC needs the real-system’s
(i.e., simulation model’s) previous input, output, and state
variables to calculate its next manipulated variables (see
Figure 5). This previous information can be stored in MPC
function space instead of the KIB. At initialization, the
MATLAB engine creates a workspace for MATLAB functions, in which the previous information can be stored until
it is closed. Therefore, given the MATLAB environment,
it is not necessary to transfer its previous state information
back and forth to the KIB.

<?xml version="1.0" encoding="utf-8" ?>
<!-- MATLAB function configuration for interacting with DEVS -->
<FUNCTION Name="mpcFunction">
<!--type can be customized-->
<INPUT Name="Time" Type="double" size="1">
<VALUE>0</VALUE>
</INPUT>
<INPUT Name="u" Type="double" size="4">
<VALUE Model=“DiePackage" Product="x">0</VALUE>
<VALUE Model="SFGI" Product="x">0</VALUE>
<VALUE Model="CW" Product="x">0</VALUE>
<VALUE Model="Customer" Product="x">0</VALUE>
</INPUT>
<OUTPUT Name="yOut" Type="double" size="4">
<VALUE Model="RawI" Product="x" Destination="Fab"></VALUE>
<VALUE Model=“DiePackage " Product="x" Destination="AT"></VALUE>
<VALUE Model="SFGI" Product="x" Destination="Finish"></VALUE>
<VALUE Model="CW" Product="x" Destination="Shipping"></VALUE>
</OUTPUT>
</FUNCTION>

Figure 9: KIB Composition Specification
MPCB:
MPC-Bridge
: MPCBridge

DEVSDecisionInterface
:DDI:
DEVSDecisionInterface

KIBM:
KIBManager
: KIBManager

MPCI
: MPCInterface
: MPCInterface

1: processStatusMessages( )
2: updateStatusInformation( )
3: transformFromStatusToControInput( )

4: makeMPCCall( )

6.3 KIB Data Transformation Manager

5: transformFromControlOutpu
tToCommand( )

This main responsibility of the transformation manager is
to coordinate interactions between the DEVSDecisionInterface and the MPCInterface given the defined data and
control mappings and transformations. The main functionalities of the manager can be summarized as

Figure 10: Data and Control Mapping Snippet
6.4 Simulation Test Case

 Maintain DEVS models and MPC function information and their composition specifications;
 Transform data information between the two modeling
formalisms;
 Control the interactive execution between the two
modeling formalisms.

The example described in earlier sections has been developed using the DEVS/MPC approach. The manufacturing
network model components are described as atomic and
coupled components which interact with the MPC model
described above. The average throughput times and capacities [days, # of products] for Fab/Test1, Assembly/Test2, Finish, and Shipping are [35, 45000], [6, 7500],
[2, 3000], and [1, 2500], respectively. The delays and targets [days, # of products] for Die/Package, Semi-finished
goods, and Customer Warehouse inventories are [1, 5712],
[1, 2856], and [1, 1787], respectively. The customer input
demand is set to vary between 939 and 968 starting from
day 61 for 577 days (see Figure 11).
The MPC model controls the inventory levels while
minimizing changes in the manipulated factory starts. The
optimization model uses the inventories to absorb stochas-

The manager contains two types of data nodes:
DEVSModelNode and MPCFunction to keep the DEVS
and MPC function information, respectively. Each process
model in DEVS has a corresponding DEVSModelNode in
the KIB to store its information (e.g., status information).
The MPCFunction consists of MPCParameters, each of
which has well-defined mapping information to the
DEVSModelNode. The mapping information corresponds
to the KIB composition specified in XML, which is a standardized schema for data exchange.

263

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
ticity present in the factory models while meeting customer
demand. The MPC model is tuned via a suitable filter gain
and forecast window for the MPC given the setting of the
factory capacities. The combined tuning of the MPC and
configuration of the process model handles mismatches between the discrete event model and its deterministic predictive counterpart. The MPC feedback control (enabled with
a filter gain greater than zero) handles the difference between actual average and forecast demands using the system prediction model forecasting window.
We have selected the filter gains 0.01 and 0.05 for
slow and fast control of the factory starts. The two sets of
simulation results shown in Figure 11 use lot size equal to
20—i.e., every lot is assigned a value generated using a triangular distribution function. Fine-grain control of the factory starts is achieved with the filter gain set to 0.01. In
general, while a filter gain greater than zero is necessary
for having feedback, its value need to be determined judiciously in order to have an acceptable tradeoff between fast
response to changes in the process model while preventing
potential instability in starts of factories.
The above simulation results are consistent with those
that were obtained using the Simulink/MATLAB environment. Clearly, identical behavior cannot be guaranteed between
the
DEVSJAVA/MATLAB
and
Simulink/MATLAB due to (i) the stochasticity in the factories
and (ii) the discrete-event and discrete-time model specifications for the process model components. The correctness
of the prototype environment, however, was verified using
standard step, impulse, and sinusoidal demands where stochasticity is absent in the manufacturing process network.

7

CONCLUSIONS

Tactical (weekly, daily, or hourly) control of a semiconductor supply-chain network via model predictive control offers
important and unique capabilities to decision-makers. We
have presented a novel hybrid modeling approach using discrete-event and model predictive control enabled by a
Knowledge Interchange Broker. The realization of this approach supports transparent and systematic specification of
interactions between process dynamics and control decisions
without relying on any single monolithic modeling paradigm. From a multi-formalism modeling perspective, we can
employ high-level model composition as opposed to embedding model interactions inside the process and control models as is required when using interoperability in combination
with model exchange standards based on XML and its current and proposed extensions (XML 2004).
A feature of the DEVS/MPC environment is its inherent support for scalable model composability and therefore
simulation interoperability—i.e., not only can manufacturing process networks and model predictive control grow in
their complexity, but also in their interactions (data and
control message mappings and transformations). Furthermore, this approach can lend itself toward domain specific
modeling which is becoming appealing for complex, largescale domains such as the one considered in this paper.
Investigation is underway to enable the presented
knowledge interchange broker to support asynchronous
data and control. With this capability, we could build an
environment where tactical and strategic decision control
could be composed with manufacturing process networks.

Die/Package Inventory

Starts on Fab/Test1

6200

1100

6000
5800

1050

5600

1000

5400
5200

950

5000
110

160

210

260

310

360

filtergain=.05

410

460

510

560

900
110

610

160

210

260

filtergain=.01

310

360

filtergain=.05

Semi-Finished Inventory

410

460

510

560

610

filtergain=.01

Starts on Assembly/Test2
1050

3200
3000

1000

2800
950

2600
2400
110

160

210

260

310

360

filtergain=.05

410

460

510

560

900
110

610

160

210

260

filtergain=.01

310

360

filtergain=.05

Shipping & Demands

410

460

510

560

610

510

560

610

filtergain=.01

Starts on Finish

1200

1000

1000
800

980
960

600

940

400
200

920

0
1

51

101

151

Demands

201

251

301

filtergain=.05

351

401

451

501

551

900
110

601

filtergain=.01

160

210

260

310

360

filtergain=.05

410

460

filtergain=.01

Figure 11: Simulation Plots of Inventory Levels and Factory Starts

264

Sarjoughian, Huang, Godding, Wang, Rivera, Kempf, and Mittelmann
Moreover, asynchronicity may offer a flexible basis for
distributed and efficient large-scale semiconductor supplychain system simulations. Finally, customized aggregation
and disaggregation models are important for handling userlevel domain-specific transformation for the semiconductor
supply chain. This capability in combination with visually
driven user interfaces would make this approach accessible
to a larger community of stakeholders including decisionmakers.

Singh, R., H. S. Sarjoughian and G. W. Godding. 2004.
Design of Scalable Simulation Models for Semiconductor Manufacturing Processes. Summer Computer
Simulation Conference, San Jose, CA.
Swaminathan, J. M., S. F. Smith and N. M. Sadeh. 1998.
Modeling Supply Chain Dynamics: A Multiagent Approach. Decision Sciences, 29(3): 607-632.
Vanderbei, R. J. 1999. An interior point code for quadratic
programming. Optimization Methods and Software
11: 451-484.
Wang, W., D. E. Rivera and K. G. Kempf. 2005. A novel
model predictive control algorithm for supply chain
management in semiconductor manufacturing. American Control Conference, Portland, OR.
XML.
2004.
eXtensbile
Markup
Language.
http://www.w3.org/XML/.
Zeigler, B. P., H. Praehofer and T. G. Kim. 2000. Theory
of Modeling and Simulation: Integrating Discrete
Event and Continuous Complex Dynamic Systems,
Academic Press.

ACKNOWLEDGMENTS
This research has been supported by the NSF Grant No.
DMI-0432439 and grants from Intel Research Council
since 2003. Their support is gratefully acknowledged. We
would like to thank the members of the Semiconductor
Supply-Chain research group at ASU/Intel including Dieter
Armbruster and Christian Ringhofer of the Mathematics
Department at ASU and Kirk Smith at Intel Corporation.
REFERENCES

AUTHOR BIOGRAPHIES

Davis, P. K. and R. H. Anderson. 2004. Improving the
Composability of Department of Defense Models and
Simulations. Santa Monica, CA: Rand.
DEVSJAVA. 2002. DEVSJAVA Modeling & Simulation
http://www.acims.arizona.edu/SOFTWARE
Tool.
[Accessed February, 2005].
Godding, G. W., H. S. Sarjoughian and K. G. Kempf.
2004. Multi-formalism modeling approach for semiconductor supply/demand networks. Proceedings of
the 2004 Winter Simulation Conference, ed. R. G. Ingalls, M. D. Rossetti, J. S. Smith, and B. A. Peters,
Piscataway, NJ: Institute of Electrical and Electronics
Engineers.
Kempf, K. G. 2004. Control-oriented approaches to supply
chain management in semiconductor manufacturing.
Proceedings of IEEE American Control Conference,
Boston, MA.
Mathworks.
2002.
MATLAB.
http://www.mathworks.com/.
Mosterman, P. J. and H. Vangheluwe. 2002. Guest editorial: Special issue on computer automated multiparadigm modeling. ACM Transactions on Modeling
and Computer Simulation. 12(4): 249-255.
http://www.heldMüller,
S.
2002.
JMatLink.
mueller.de/JMatLink/. [Retrieved November 2004].
Sarjoughian, H. S. and F. E. Cellier, eds. 2001. Discrete
Event Modeling and Simulation Technologies: A Tapestry of Systems and AI-Based Theories and Methodologies, Springer Verlag.
Sarjoughian, H. S. and J. Plummer. 2002. Design and implementation of a bridge between RAP and DEVS.
Tempe, Arizona, Computer Science and Engineering,
Arizona State University: 1-26.

HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science & Engineering at ASU. His research interests are in modeling frameworks including multiformalism and collaborative approaches, agent-based simulation, and software architecture. For more information
visit http://www.acims.arizona.edu.
DONPING HUANG is a PhD student in the Computer
Science and Engineering department at ASU. Her research
includes modeling and simulation of distributed supplychain networks and software design. She can be contacted
at dongping.huang@asu.edu.
GARY W. GODDING is a Technologist at Intel Corporation and a PhD candidate in the Computer Science and Engineering department at ASU. His research includes modeling & simulation of semiconductor supply-chain systems,
software architecture, and artificial intelligence. He can be
contacted at gary.godding@intel.com.
WENLIN WANG is a PhD candidate in the Chemical and
Materials Engineering Department, ASU. His research focus spans model predictive control for semiconductor supply-chain systems and control theory of complex processes. He can be contacted at wenlin.wang@asu.edu.

265

Sarjoughian, Huang, Wang, Rivera, Kempf, Godding, and Mittelmann
DANIEL E. RIVERA is Associate Professor Chemical
and Materials Engineering, ASU. His areas of research include control-oriented approaches to supply chain management and scalable enterprise systems, system identification, and advanced control concepts. He can be contacted
at daniel.rivera@asu.edu.
KARL G. KEMPF is Director of Decision Technologies at
Intel Corporation and an Adjunct Professor at ASU. His research interests span the optimization of manufacturing and
logistics planning and execution in semiconductor supply
chains including various forms of supply chain simulation.
He can be contacted at karl.g.kempf@intel.com.
HANS D. MITTLEMANN is Professor of Mathematics at
ASU. His research focuses on nonlinear optimization, partial differential equations, and their combination. He can be
contacted at mittelmann@asu.edu.

266

Visual and Persistence Behavior Modeling for DEVS in
CoSMoS
Mostafa D. Fard
School of Electrical and Computer Engineering
University of Tehran, Tehran, Iran
Email: M.Derakhshandeh@ut.ac.ir

Hessam S. Sarjoughian
Arizona Center for Integrative Modeling
&Simulation
School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University, Tempe, AZ, USA
Email: Sarjoughian@asu.edu

ABSTRACT

The CoSMoS tool enables a simulation-based system
design process with support for structural model
verification and using simulation validation. CoSMoS
stores all models in relational databases.

An integrated visual modeling and simulation tool called
Component-based System Modeling and Simulation
(CoSMoS) is extended to support behavioral specification
of parallel atomic DEVS model. An approach based on
Statecharts and Graphical Modeling Framework has been
developed and implemented for specifying behaviors of
atomic models. One or more Statecharts can be developed
for any atomic model and stored in a relational database.
The behavioral modeling complements atomic and coupled
DEVS structural modeling where families of models with
semi-automated code generated for the DEVS-Suite
simulator are systematically stored and retrieve. The
behavioral modeling enriches visual development of
models that have DEVS-compliant specifications with
modular, component-based visual representations. The
visual representation of hierarchal coupled models are
automatically generated or restored. An example model is
employed to show the degree of details supported both in
visual representation and database representation. Current
and future works are briefly described.

In DEVS, any component that contains other components is
called a coupled model; non-container components are
called atomic models. The DEVS formalism [6] includes
the means to build hierarchical, modular models from
components. A model’s behavior is derived from the
behavior of atomic models. Behavior of an atomic model is
defined by autonomous or input-driven state-based
transition functions, which are known as internal and
external transition functions.

CoSMoS; EMF; GMF; Parallel DEVS; Statecharts; Visual
and Persistence Modeling.

Beyond basic atomic DEVS, visual behavior modeling can
aid in domain-specific applications and particularly those
that have complex state-based dynamics. Furthermore,
other modeling methods such as standardized Base Object
Model component [7] may be enriched with the visual
behavior modeling capabilities described in this paper. This
is helpful toward making complex model specifications
more accessible to domain experts. Visual behavior
modeling is also promising for meta-modeling methods
such as EMF-DEVS [8].

ACM Classification Keywords

ATOMIC MODEL BEHAVIOR MODELING

Author Keywords

There exists different ways to specify the behavior of an
atomic model. Behavior may be defined through
mathematics, programming code, or visual notation. Each
has its own benefit; the visual notation is more attractive,
particularly for domain experts who may not prefer the
other approaches. In CoSMoS, the modelers can visually
develop the structure of a model, and then automatically
translate it to source code for the DEVS-Suite simulator.
Behaviors of atomic models must be manually added to the
partially generated source code as in any other existing
general-purpose visual modeling approach/tool. DEVSSuite can execute these simulation models.

H.2.4 [Systems]: Relational databases; I.6.1 [Simulation
Theory]: Model classification; I.6.4 [Model Validation and
Analysis]; I.6.5 [Model Development]: Modeling
methodologies; I.6.7 [Simulation Support Systems]:
Environments
INTRODUCTION

The Component-based System Modeling and Simulation
(CoSMoS) framework was conceived to support a unified
logical, visual, and persistent model specification with
support for automated simulation code generation for
component-based and cellular automata models [1-4]. This
modeling engine is integrated with the DEVS-Suite
simulator [5], which supports visual experimentation
configuration and run-time data collection and observation.

The UML Statecharts is a standard, powerful and effective
visual notation for specifying a system’s behavior [9]. At
the heart of the Statecharts is the notion of discrete states
and the transitions between any two states. Using parallel
DEVS is superior to UML Statecharts since the concept of

6SULQJ6LP706'(96$SULO$OH[DQGULD9$86$
6ociety for Modeling & Simulation International (SCS)

227

time is explicitly accounted for. On the other hand, unlike
DEVS, Statecharts has a rich visual notation. However,
neither DEVS nor Statecharts is concerned with persistence
modeling (i.e., storing and accessing models).

conforms to its meta-model. EMF provides a plug-able
framework to store the model information; the default uses
XMI (XML Metadata Interchange) to persist (i.e., store) the
model definition. EMF allows creating the meta-model
using other languages including Java annotations, UML,
and XML Schema.

Given these observations, our research considers three
topics. First is supporting visual notation for specifying
behavior, second is storing models in a relational database,
and third is implementing and integrating these into
CoSMoS.

EMF itself is defined using the concept of meta-modeling.
It has two meta-models; the Ecore and the Genmodel metamodels. The Ecore contains the information about the
defined classes for a domain. The Genmodel contains
additional information for the code generation, e.g., the
path and file information. The Genmodel also contains the
control parameter for code generation. EMF makes domain
model explicit, which helps to provide clear visibility of the
model. EMF also provides change notification functionality
to the model in case the model changes. EMF will generate
interfaces and factory classes for creating objects; therefore
it helps to keep the application clean from the individual
implementation classes. Another advantage is the ability to
regenerate the Java code from the model at any point in
time.

For the first topic, time and ports are required in DEVS, but
the handling of time, especially as required for simulation,
has shortcomings in UML Statecharts [9, 10]. Handling
concurrent events as defined in the confluent function is not
accounted for in UML. Simultaneous events are supported
in UML2.0, but not in DEVS. Ports are directly handled in
the UML2.0 component model.
For the second topic, in software tools such as Rational
Rose [11], specifications including Statecharts are stored in
flat files, but in CoSMoS all models including their
relationships are stored in a relational database [12].
Therefore, new schemas need to be developed for storing
visual specifications. Persistence modeling is important for
model management, collaborative model development,
structural model complexity metrics, and code generation
for different target simulation engines[1].

V
Graphical
Model

I
Create
GMF
Project

I

Domain
Model
(.ecore)

V

T

Mapping Model
(.gmfmap)

V

For the third topic, CoSMoS needs to be extended with a
new editor for visual behavior modeling. The editor has to
support storing and retrieving both structural and behavior
models uniformly.

Tools Model
(.gmftool)

Domain Generation
Model (.gmftool)

BACKGROUND

Mapping Model
(.gmfmap)

Edit &
Editor

Eclipse is an integrated development environment (IDE)
that contains a base workspace and an extensible plugin foundation for customizing and extending it. GMF
(Graphical Modeling Framework) is a modeling framework
that supports creating a specific graphical editor for logical
models. It uses a meta-model following the MVC (ModelView-Controller) architecture. It is based on the EMF
(Eclipse Modeling Framework) and GEF (Graphical
Editing Framework). GEF is used in tools such as CD++
Builder [13]. EMF handles the model’s definition and GEF
handles the model’s view and controller [14].

T
Diagram
Package

Figure 1. GMF Workflow Process.
Graphical Modeling Framework

The GMF provides a workflow for generating graphical
editors (see Figure 1). The starting point is to define a
domain model using the Ecore Domain Generation Model,
which generates the implementation code. The graphical
model defines the symbols used for representing different
object classes visually (as nodes), as well as labels for
displaying and editing attribute values. Classes and
references may also be mapped to connections (links
between nodes), and some references may be defined
as containment for decomposition of a node into sub-nodes
in compartments. A wizard helps generate a basic graphical
model from the domain model, though custom figures and
decomposition must be added manually.

GMF provides basic entities for creating graphs consisting
of nodes, links, and labels. It can be used to define special
types of graphs, such as Statecharts. GMF includes wizards
for generating intermediary graphical tools and mapping
definitions based on an initial meta-model (EMF Ecore), as
well as the final runtime code [14].
Eclipse Modeling Framework

Eclipse EMF can be used to model platform independent
software applications from which platform specific code
can be generated. EMF distinguishes between meta-models
and concrete models. A meta-model describes the structure
of the concrete model. Therefore, a concrete model

The tools model defines the services for creating new nodes
and connectors in the model editor. A wizard helps
selecting which of the object classes in the domain model

228

Figure 2. An Ecore DEVS meta-model using Statecharts.

will be available. The mapping model connects the domain,
tools, and graphical models, defining which node and
connector symbols are used for representing which domain
model class and which tools create new elements. Indeed,
this part makes the relation between graphical elements,
tool elements and their classes. By using the GMF wizards,
the basic mappings are established automatically, though
decomposition rules and links between different diagrams
must be added manually.

define to show a simple time advance function. States,
OtherStates, SourceState, TargetState, TransitionFunctions
and Transitions are abstract classes.
We can define some rules for the DEVS atomic model
specification in the Statecharts just described fined. For
example, we just can have one initial state in a model
and/or any composite state. We observe this rule by adding
initialState connections (EReferences connections) from
Model and CompositeState classes to InitialState class with
Upper Bound property set to one. Initial state is just the
source state of Transition; also target of Transition just can
be form SourceState (State or Composite State).

The Java code for editing diagrams is generated from the
mapping model. It uses the code generated from the domain
model. The dark arrows embedded with I, V, and T
characters in Figure 1 reflect the additional configuration of
the model editor that the modeler may input at each stage.
The domain model reflect the information content aspect,
while the graphical and tools models represent the view
aspects of model elements and tools (tasks or services),
respectively. I, V, and T are for the information, view and
task aspects. Additional tasks or services can only be
defined through programming. The tools model identifies
services for creating new model elements, but all other
kinds of tasks are implicit. The Diagram Generation Model
configures GMF to generate diagramming code. The
purpose of this model is similar to the Domain Generation
Model.

Final state cannot be the source state of any transition. This
rule is observed by adding two abstract classes
(SourceState and TargetState) and define that FinalState
just inherit from TargetState. Any internal or external
transition has source state (State or Composite State) and
target state (State, Composite State or Final State). Any
composite state is like a new model which can have its
initial state, states, composite states, etc.
We have developed an editor to visually specify the
behavior of atomic DEVS model by applying the steps of
the GMF workflow. Figure 3 is a snapshot of the generated
Statecharts Editor (SE) that is an RCP Application. The
editor is divided into main canvas (left side / drag and drop
environment), toolbox (right side), and property window.
Models are created in the main canvas. Menu bar and Tool
bar are located in the top, and there are some tools for
arranging elements in the main canvas. Position and size of
all windows can change manually.

STATECHARTS EDITOR

EMF Ecore model is the basic model to define the structure
of a desired visual editor in GMF. Figure 2 depicts the
Ecore model (Class Diagram) for the atomic DEVS model.
There is ConfluentType attribute in Model class to define
confluent function. States in DEVS are defined using four
classes; InitialState, State, CompositeState and FinalState.
InternalTransition and ExternalTransition classes capture
the Internal Transition Function and External Transition
Function, respectively. To initialize the model, we add a
Transition class which just starts from an InitialState. The
OutputFunction class defines the output function of DEVS
that just can be added to the State. TimeAdvanceType and
TimeAdvanceValue attributes in the Transitions class

STATECHART EDITOR ELEMENTS PROPERTIES

The Statecharts Editor’s meta-data (Ecore Model) and
generated editor has specific entities to visually model the
behavior of atomic models, thus the behavior of the system.
Although certain parts of visual behavior model
development are supported by the Ecore in Figure 2, some
other parts still have to be completed in code. For example,
guards are implemented by simple string properties; thus, if

229

Figure 3. Statecharts Editor.

there is a need to define a function, it has to be manually
added to the generated code. Each simple and complex
entity has a unique visual icon with one or more properties.
An example of a simple entity is State defined to have a
name with data type string. The Initial State, Final State,
Composite State and State just have Name property. It has
to be filled before saving a model. Actions, Guards and
Messages are arrays of String. Action uses to set states as
assignments, Guard uses to check conditions, and Message
uses to define output values, to send by output function.
In Transition, the SourceState can be an initial state, and
TargetState can be a state or composite state. In the Internal
and External Transition, the Source property can be a state
or composite state; and the Target property can be a state,
composite state, or final state. The External transition
activates by receiving message on its Input Port. The output
port with message/messages is set by Output Port property
in the output function. Specifications for the internal,
external, and output functions are shown in Figure 4. The
Time Advance Type can be set to Infinity, Update or
Value. Time Advance Value sets to a value ranging from 0
to less than infinity when Time Advance Type sets to
Value. An example of specifying the time advance with a
finite value is shown in the External Transition in Figure 4.
For the output function, Message property is defined to be
FWE and the port to be out1. An action can be a simple
assignment or complex (see the internal and external
transition functions). The confluent function is defined to
be either FIT (First Internal Transition) or FET (First
External Transition).

Figure 4. Sample specifications for the internal, external and
output functions.

230

ATOMIC MODEL STORAGE

SE works as a standalone application; it uses flat files
whose content are identical to the data contained in the
CoSMoS database.

The data for every atomic model developed in the
Statecharts Editor is stored in two flat files. The Domain
file has the model’s data, such as the atomic model’s
attributes and functions. The Diagram file has the model’s
graphical attributes, such as position, font, and color for
every element of the model.
In contrast, CoSMoS stores every model in a database
without any data about their graphical representations. The
CoSMoS client generates visual representations of the
models using a set of rules. For CoSMoS, it is useful to
store the behavioral model (i.e., the Domain file) with its
corresponding structural model. One option is to use
existing plug-ins, such as Teneo, Hibernate, and CDO [15].
Another option is to modify the Diagram Generation Model
package in GMF to store the Statecharts model in the
CoSMoS database. Table 1 lists some of the basic pros and
cons that pertain to this work.
Plug-ins Architecture

Figure 5. Database schema to storing DEVS atomic model
logical and visual specifications.

Classical

Pros

Automatic database (re-)
generation

Full control in
designing and
implementing databases

Built-in multi-client
support with minimal
manual programming

Flexibility in using
databases with existing
client applications

Cons

When a modeler wants to save one or more Statecharts
models in the database, CoSMoS will parse the domain file
to extract the relevant data that is to be stored in the
database. Figure 6 illustrates the sequence diagram to
extract and save data, for states and transitions. It is the
same for other model’s elements such as Guards, Actions,
Messages, and Output Functions.

Dependency on complex
plug-ins and Eclipse
Manual development
Framework
Table 1. Model Persistence Approaches in Databases.

: Controller

: Modeler

Based on our requirements, we have chosen the classical
approach for adding the Statecharts model to the existing
CoSMoS database. In particular, automatic database
generation using plug-ins does not offer a tangible benefit
since the SE is not subject to user-specific changes.

: Parser

: Statecharts

: States

: Transitions

1: Save
2: Parse()
3: Parse
4: OK
5: CallUpdate()

There are two approaches to achieve the storing data
mechanism. The first is to read and modify all the classes
that are automatically generated by GMF and the second is
to implement the needed changes in the CoSMoS without
making any changes to the SE. We decided to choose the
second approach due to the complexity of the GMF’s plugins.

6: CallUpdateFiles()
7: GetStates()
8: StatesList
9: DeleteAllStates(SCname)
10: InsertNewStates(SCname)

11: GetTransitions()

As mentioned above, every model in the SE has the
Domain and Diagram files. The names of the domain and
diagram models are the same as the statechart’s name
which defined by modeler. There is no need to store the
properties in the Diagram file in separated tables.

12: TranitionsList
13: DeleteAllTransitions(SCname)
14: InsertNewTransitions(SCname)

Figure 6. Sequence diagram for storing behavioral atomic
models in database.

Figure 5 shows the schema for storing Statecharts models.
The CoSMoS database is extended with this schema. The
primary key for these tables are AMname (Atomic Model
name) and SCname (Statecharts name). When a modeler
saves a model, its domain file is parsed and stored in the
database tables. The database is used to recreate the
Domain and Diagram files that are needed in the SE. The

When a modeler wants to save a model, the Controller
object calls Parse() method of the Parser object. It
parses the domain file, creates a list of elements and returns
OK. The Update() and UpdateFiles()methods copy
the whole content of the domain and diagram files into the

231

domain and diagram fields of the Statecharts table. Then,
all instances of an element are returned with their
properties on a list (e.g., States). Next, the current data,
which is for a specific atomic model, is deleted and
replaced with new data. Similar steps are executed for all
the elements of a Statecharts model.

“calling mechanism” where CoSMoS is embedded in the
SE.
Figure 8 details the workflow between the CoSMoS Java
application (referred to as CoSMoSA) and the SE. The
integration required converting the Statecharts editor RCP
Application to its executable counterpart. It is called
CoSMoSR. It supports both structural and behavioral
atomic DEVS modeling while retaining the complete range
of modeling already supported in CoSMoSA. In this setting,
an instance of the CoSMoSA is launched as a thread in
CoSMoSR which is the master thread. The part of Figure 8
enclosed in the dotted rectangle is for CoSMoSA. The
remaining steps and flows are for the master thread. The
atomic model for which Statecharts models are to be
developed is selected in CoSMoSA. Then, the Statecharts
can be developed (see the “Wake up RCP’s thread” step in
Figure 8).

Figure 7 illustrates the sequence diagram for opening an
atomic model from the database. When a modeler wants to
open an atomic model, one or more Statecharts model will
be opened on individual tabs in the SE (see Figure 3). At
first, all the Statecharts of an atomic model are read from
the Statecharts table in database. Then, the Domain and the
Diagram flat files are created in their pre-specified location.
Finally, new tabs will be created in the main canvas of the
SE and automatically filled with the data retrieve from
CoSMoS database.
: Controller

: Statecharts

: Modeler

: IOController
Run
(CoSMoS.exe)

1: Open
2: getStatecharts()

Launch an instance of
CoSMoS by new thread

3: statechartsList

Wait current
thread of RCP

4: CreateDomainFile()
Create/Select a
database

5: CreateDiagramFile()

NotExist

Read temp
file data
Create temp file and
save data

Figure 7. Sequence diagram to open a model.

Set return
value
Wake up RCP’s
thread

CoSMoS

INTEGRATING COSMOS AND STATECHARTS EDITOR

Until now, we have two visual editors for atomic DEVS
models (one for developing structural models and another
for developing behavioral models). These modeling
environments work independently, but use a shared
database. It is important for these two environments to be
integrated and have a workflow with the ability to
iteratively and incrementally develop models. The
workflow must ensure both environments use the database
consistently and in particular guarantee that the database
and the SE flat files remain consistent (i.e., the database
and flat files are not changed simultaneously).

Close
Statechart

Create & Show
Statechart

CoSMoS
initialization
Show
CoSMoS

Communications between the SE and the CoSMoS with the
database are different. In CoSMoS all validations are
checked against data which are stored in the database. In
the SE validations are enforced by the editor, by the
database, and by the domain flat file.

Run new instance of
CoSMoS.exe

Temp file

Exist
6: Add new tab to
Editor

Dispose
Statechart

Set Atomic
Model’s name
ShowStatechart
Return
value

Exit

Delete temp
file
Dispose RCP’s
thread

Close & Dispose
CoSMoS

Figure 8. Model Development Workflow Process.

The CoSMoSR thread looks for a temporary file
(CurrentDB.xml) along with the CoSMoSR. If it does not
exist, the modeler must either create a database or select an
existing one. Information about the selected database and
the path to its flat files will be stored in the
CurrentDB.xml temporary file. Then CoSMoSA is
initialized and launched. If a temporary file exists,
information will be read from it and automatically
initialized. The CoSMoSA visual editor is displayed and the
modeler can modify the structure of the model. The
modeler has two options: launch the SE or terminate
CoSMoSR. In the former scenario, the CoSMoSA thread
sets appropriate results for waking up the CoSMoSR thread,
which launches the SE. When CoSMoSA is to exit, then the
CurrentDB.xml will be deleted and the master thread
will be disposed. Otherwise, the SE is launched with the
name of atomic model. If there are any behavioral atomic

The first approach was to embed the SE within CoSMoS.
However, since the SE is an RCP Application, it could not
be included in CoSMoS, a Java Application. This is due to
the SE launching with eclipse configurations which cannot
be supported in Java applications. Also, we could not find a
way to convert the RCP application to a Java application.
As an alternate solution (second approach), we devised a

232

models for the named atomic model, they are loaded in the
SE and the modeler can change and/or add new Statecharts
models.

states and transitions at the Statecharts (and therefore State
Machines) abstraction level. Key differences are the
specification of input and output ports. The external and
internal transitions for DEVS atomic model are directly
specified in terms of these ports. In the context of CoSMoS
as an integrated modeling and simulation framework,
families of models in a disciplined setting, multiple
behavioral models for a model may be constructed. This
capability leads to specializing an atomic model in terms of
not only its structure, but also its behavior.

As shown in Figure 9, any behavior atomic model has a
defined Template Model (TM). When an atomic model is
selected from the TM tree, new Statecharts behavior can be
developed for it or, alternatively, existing models can be
modified or deleted. Every Statecharts has a unique name.
The modeler can select and delete any model from a list of
Statecharts. The database and flat files are updated and
removed accordingly.

In comparison to other approaches, including the tools
mentioned above, models in CoSMoS are stored in
relational database. Disciplined model persistence is crucial
when modelers are faced with creating alternative models
[24]. Traditional methods of storing models in flat files can
become unnecessarily complex when more than a few
model components are to be developed. Synthesizing a
family of models visually is attractive especially as an
atomic may have different behaviors in different coupled
models. From a broader perspective, CoSMoS is compared
with MS4Me as an exemplar (commercial) DEVS and
Ptolemy II as an exemplar (open source) component-based
tool. Structural and behavioral DEVS models are stored in
relational database, whereas in MS4Me and Ptolemy II
structural and behavioral models are stored in flat files.
Among these tools, CoSMoS and MS4Me frameworks can
support similar, yet distinct, approaches for creating
families of models.

Figure 9. Statecharts modeling in CoSMoS.
RELATED WORKS

Visual behavior specification for atomic DEVS model
behavior has been of interest for many years. States and
transitions of atomic models are commonly used to visually
represent DEVS model behaviors of atomic models with
tools supporting varying levels of specifications. A
common approach is to use State Machines. The behavior
of a generic classic atomic model is reduced to the syntax
and semantics afforded by the State Machines. Another
approach for behavior modeling is Statecharts [16]. There
are different ways of representing (mapping) the behavior
of atomic models to Statecharts (e.g., [10, 17]). Another
approach is developed for CoSMoS (see Figure 3). Atomic
model behavior may also be defined using custom notations
(e.g., [18]). Recent approaches and tools that use or adapt
basic State Machines include CD++ Builder [13], LSIS
DME DEVS [19], DEVS Graph [20], and MS4Me [21].
Other simulation tools such as Simulink [22] and Ptolemy
II [23] use variants of State Machines.

CONCLUSIONS

The work presented in this paper introduces a rich visual
behavior modeling capability to the Component-based
System Modeling and Simulation (CoSMoS) environment.
As described in the earlier sections, specifying the states,
transitions, and timing for atomic models as Statecharts is
developed using the Graphical Modeling Framework
(GMF). The resulting Statecharts Editor offers a rich
environment to develop non-trivial behavior models. DEVS
behavior model syntax is readily captured and supported in
GMF. Furthermore, these behavioral atomic models,
alongside their structural counterparts, are stored in the
CoSMoS relational database. Consequently, both structural
and behavioral DEVS modeling is supported. For future
work, it is useful to support code generation behavioral
models for the DEVS-Suite simulator. This requires
extending structural simulation code generation with the
external, internal, confluent, time advance, and output
functions. A benefit of this generated code is that basic
syntactic specification of atomic models can be
automatically verified.

Amongst the above approaches, Statecharts is more
attractive since it is a visual language and more expressive
as compared with State Machines and its variants such as
Stateflow [22]. Statecharts is widely adopted and is
included in the UML standard. The SE has a well-defined
UI partitioning which simplifies model development, for
example, through the palettes and property panels (as
opposed, for example, to use of dialogue boxes).

REFERENCES

1.

The visual behavior modeling supported in CoSMoS tool
has similarities and differences with those mentioned
above. Basic similarities with the above tools are defining

233

H. S. Sarjoughian and V. Elamvazhuthi, 2009,
“CoSMoS: a Visual Environment for Componentbased Modeling, Experimental Design, and
Simulation,” in Proceedings of the 2nd International
Conference on Simulation Tools and Techniques.

2.
3.
4.
5.
6.

7.
8.

9.
10.

11.
12.
13.

H. S. Sarjoughian, V. Elamvazhuthi, and S. Sarkar,
2002-2009, “CoSMoS 2.1.0 Help Document/Guide,”
Arizona Center for Integrative Modeling and
Simulation, 2014, http://www.acims.arizona.edu.
CoSMoSim Verions 3.0, 2009,
http://cosmosim.sourceforge.net.
DEVS-Suite Simulator Version 3.0, 2015, http://devssuitesim.sourceforge.net.
B. P. Zeigler, H. Praehofer, and T. G. Kim, 2000,
Theory of Modeling and Simulation: Integrating
Discrete Event and Continuous Complex Dynamic
Dystems, 2nd Ed, San Diego: Academic Press.
B. O. M. SISO, 2006, “Base Object Model (BOM)
Template Specification. Simulation Interoperability
Standards Organization,” SISO-STD-003-2006.
H. S. Sarjoughian and A. M. Markid, 2012, “EMFDEVS modeling,” in Proceedings of the 2012
Symposium on Theory of Modeling and SimulationDEVS Integrative M&S Symposium.
OMG Unified Modeling Language, 2004,
http://www.omg.org/docs/formal/05-07-04.pdf.
J. Mooney and H. S. Sarjoughian, 2009, “A
Framework for Executable UML Models,” in
Proceedings of the 2012 Symposium on Theory of
Modeling and Simulation-DEVS Integrative M&S
Symposium.
IBM - Software - Rational Rose family, 2014,
http://www.03.ibm.com/software/products/en/ratirosef
ami.
T. S. Fu, 2002, “Hierarchical Modeling of Large-scale
Systems Using Relational Databases,” The University
of Arizona.
M. Bonaventura, G. Wainer, and R. Castro, 2010,
“Advanced IDE for Modeling and Simulation of
Discrete Event Systems,” in Proceedings of the 2010
Spring Simulation Multiconference.

14. R. C. Gronback, 2009, Eclipse Modeling Project: a
Domain-specific Language Toolkit. Upper Saddle
River, NJ: Addison-Wesley.
15. CDO/Hibernate Store - Eclipsepedia, 2014,
https://wiki.eclipse.org/CDO/Hibernate_Store
16. D. Harel, 1987, “Statecharts: A Visual Formalism for
Complex Systems,” Science of Computer
Programming, Vol. 8, No. 3, 231-274.
17. S. Borland, 2003, “Transforming Statechart Models to
DEVS,” McGill University.
18. U. B. Ighoroje, O. Maïga, and M. K. Traoré, 2012,
“The DEVS-driven Modeling Language: Syntax and
Semantics Definition by Meta-modeling and Graph
Transformation,” in Proceedings of the 2012
Symposium on Theory of Modeling and SimulationDEVS Integrative M&S Symposium.
19. M. Hamri and G. Zacharewicz, 2012, “Automatic
Generation of Object-oriented Code from DEVS
graphical specifications,” in Proceedings of the Winter
Simulation Conference.
20. H. S. Song and T. G. Kim, 2010, “DEVS Diagram
Revised: A Structured Approach for DEVS Modeling,”
in Proceedings of the European Simulation
Conference.
21. C. Seo, B. P. Zeigler, R. Coop, and D. Kim, 2013,
“DEVS Modeling and Simulation Methodology with
MS4 Me Software Tool,” in Proceedings of the
Symposium on Theory of Modeling & SimulationDEVS Integrative M&S Symposium.
22. G. Hamon and J. Rushby, 2004, “An Operational
Semantics for Stateflow,” Fundamental Approaches to
Software Engineering, 229-243, Springer Berlin
Heidelberg.
23. Ptolemy II Home Page, 2014.
http://ptolemy.eecs.berkeley.edu/ptolemyII/.
24. H. S. Sarjoughian, J. J. Nutaro, and G. Joshi, 2011,
“Collaborative Component-based System Modeling,”
Journal of Simulation, Vol. 5, No. 2, 77-88.

234

System Modeling with Mixed Object and Data Models
Hessam Sarjoughian
Robert Flasher

Arizona Center for Integrative Modeling & Simulation
School of Computing and Informatics
IRA Fulton School of Engineering
{sarjoughian | robert.flasher}@asu.edu
Keywords: data engineering, model persistent, model
transformation, system design, visual modeling.

models may be specified at varying levels of resolution. A
computing node model may include probabilistic processing
time or network traffic may be specified as primitive data or
complex objects. Moreover, a complex system may have
different architectures. For example, the wireless computer
network may be modeled as a set of computers and switches
having a hierarchical topology.

Abstract
System designs are described from object and data
centric
perspectives.
Component-based
modeling
approaches are well suited to specify structure and behavior
of systems in terms of objects and relationships. Ontologybased modeling approaches describe a system’s
specification using data types and relationships. The
Scalable Entity Structure Modeler is a component-based
modeling framework suitable for specifying alternative
system designs. This approach is extended with XML
Schema to support unified dynamic objects and static data
specifications for system design specifications. An example
illustrating mixed component and data modeling is
described using an extended realization of the SESM
environment.
1.

To support such needs, various modeling concepts
approaches have been developed. Two of these modeling
frameworks are DEVS [18] and UML [8]. They support
modeling dynamics of simulation software models. Each is
well-suited for a particular kind of modeling – i.e., DEVS is
targeted for simulation modeling and UML for software
modeling. A common theme among these approaches is to
describe specific system models with strong emphasis on
component. In contrast to these, XML [3, 14] is primarily
used to model the static view of a system. It can also be
used to describe static structures among simple and complex
data elements. Other modeling approaches are Scalable
Entity Structure Modeler (SESM) [2, 5, 7, 11], System
Entity Structure (SES) [9, 16, 19], XML Schema, and DoD
Architecture Framework (DoDAF). the Scalable Entity
Structure Modeler (SESM) emphasizes a unified logical,
visual, and persistent modeling framework for componentbased model development [12]. The SES emphasizes
modeling concrete alternative model structures. XML
Schema allows describing arbitrary structures, but it does
not have axioms that can establish similarity relationships
among different structures of a system. DoDAF focuses on
conceptual separation of models for describing
complementary systems, operational, and technical views.

INTRODUCTION

Modeling is crucial for system analysis and design. An
important problem in system modeling is how to create and
manage multiple system architecture specifications.
Enabling modelers to develop multiple models of a system
based on sound principles, therefore, is highly desirable. To
this end, various modeling languages and approaches have
been developed to aid specifying hierarchical object and
data model specifications at varying levels of abstractions.
Two of the main genres of modeling languages are
targeted for developing software and simulation models. A
common need is specifying alternative designs of a system.
For example, a computer network model may specify a set
of computers that are connected to one another using
wireless connectivity. Such a model can be used to evaluate
network topologies (e.g., communication range and its
impact of quality of service) or to develop a design that can
efficiently protect a wireless network against attacks. These

SpringSim Vol. 1

Given the above varying modeling approaches, it is
desirable to support detailed object and data modeling. To
achieve this goal, the importance of combined logical,
visual, and persistent modeling is briefly described in
Section 2. The SESM framework is detailed and the
conceptual basis for its extension with XML Schema is

199

ISBN 1-56555-313-6

described in Section 3. In Section 4, the XML Schema
models and its relationship with SESM are presented. In
Section 5, a realization of the SESM environment is
presented with an example model. A summary with a sketch
of future work is given in Section 6.
2.

define behavior as is in UML. The visual model type defines
symbols for the logical models to support visual
specifications of models. The model components and their
relationships conform to the visual modeling language. The
persistent model type specifies persistent memory patterns
for the logical model structures across space and time. The
model entities and relationships comply with syntax and
semantics of databases.

BACKGROUND

Complex systems are commonly described by using a
set of model abstractions and relationships among them.
Modeling approaches such as Entity-Relation (ER) and
Unified Modeling Language (UML) are used to model a
system’s specification from specific points of view. ER is
used to describe a system’s structure in terms of data entities
and relations. The entities are generally simple, but can have
intricate relationships which together may describe complex
structures. UML describes objects and their relationships.
UML classifiers represent various kinds of structures and
behaviors. These classifiers may be combined together with
data entities to describe complex dynamic systems. The ER
and UML abstractions, therefore, support describing datacentric and object-centric structures, respectively. These
modeling approaches are used to formalize different logical
model abstractions.

The principal features of the SESM are scalable multiaspect/resolution model specification, iterative/incremental
model development process, and quantifying complexity
metrics for models. The basic concept of the model types
and their synthesis facilitate visual and persistent modeling.
These capabilities afford automatic creation of well-formed
model specifications according to the DTD and XML data
language as well as object-oriented programming languages.
2.1. Data Engineering
An important application domain for SESM is data
engineering which can be considered as system design with
emphasis on data modeling. A major area of interest is
handling of data sets obtained from Synthetic Aperture
Radar (SAR) system. Modeling (or more specifically
organizing) rich SAR data sets in a systematic way is
essential for data gathering, processing, and analysis. For
this purpose, the Universal Phase History Data (UPHD)
standard has been developed using the SES and XML
modeling approaches [17]. The SAR application domain is
used to represent geospatial data obtained from sensors.
Knowledge engineering of large data sets and complex
relationships using the above modeling concepts is
considered crucial for knowledge management, processing,
and dissemination. This is because data sets need to be
architected for data storage and making available processed
data using technologies such as web-services.

The above modeling approaches offer capabilities to
describe different kinds of logical models. Each of these
approaches is grounded in a particular kind of modeling.
Entity-Relation is targeted for describing structural, nonbehavioral models. ER models lend themselves well for
standardized logical model representation and model
persistence in relational databases, but provide limited
concepts and capabilities for visual modeling. UML is
targeted for describing both object-oriented structure and
behavior (models). The UML standard supports logical and
visual modeling, but lacks a strong foundation for model
persistence [6].
The System Entity Structure (SES) and XML modeling
frameworks are aimed at ontological representation of highlevel system and low-level data specification, respectively.
SES is a labeled tree with a set of axioms that constrain the
relationships among the tree’s entities. Entities represent the
parts of a system as a collection of similar, related
structures.

Here, the data engineering concepts are used to extend
the SESM modeling framework to support mixed object and
data models. The extended approach described in the
remainder of this paper supports separately modeling data
and object models and their composition. The unified
logical, visual, and persistent modeling framework supports
developing data and object models that have rich structural
and behavioral specifications. This in turn supports
automatic transformations of SESM models to alternative
specifications including DTD and XML Schema and semiautomatic transformation to simulation code.

As suggested above, models of a system can be given in
terms of logical, visual, and persistent abstract model types.
Each of these model types has its own syntax and semantics.
The logical model type describes structure and behavior.
The model specifications can be simple to complex – the ER
models are generally low-level and do not describe behavior
which makes them less complex as compared with the UML
models. A logical model conforms to a common set of
constructs and axioms. The syntax and semantics of the
logical model type defines all model structures that consist
of elements and relationships. The logical model may also

ISBN 1-56555-313-6

3.

SCALABLE ENTITY STRUCTURE MODELING
FRAMEWORK

The Scalable Entity Structure Modeler is a modeling
framework based on Entity-Relation (ER), System Entity
Structure (SES), Object-Orientation (OO), visual modeling,
simulation modeling, and model transformation (see Figure

200

SpringSim Vol. 1

1). The ER concepts support scaleable representation and
storage of entities and their relations in databases. The
concept for representing a system’s structural representation
is given by SES. The object-orientation composition,
inheritance, and hierarchy are used for organizing
alternative structures of a system. The visual modeling
concepts offer visual abstractions that are key for systematic
handling of tedious, error prone modeling tasks faced by
designers and analysts. The simulation concepts are used to
account for behavioral modeling of system specifications.
Finally, well-formed exchangeable representations play a
crucial role for generating alternative models that conform
to standardized modeling languages such as XML and
programming languages that can be executed with
simulation engines.

Logical
Modeling

Visual
Modeling

sessions, each with its own database. Next the specification
of logical models is presented. The details of the persistence
and visual models are deferred to [12].

Component Modeling
primitive

simple

specialized
complex
composite

simulatable
components

Simulation
Code

non-simulatable
components

Figure 2: SESM component models
3.1. Logical Models

Model
Translator

Persistent
Modeling

The primitive and composite model types are logical
models. Each of these two model types has Template,
Instance Template, and Instance models. A primitive
Template Model can have a finite number of state variables.
Each state variable is defined to have a type and may have
an initial value. State variable types can be simple or
complex objects including UML data types and classes. A
primitive Template Model can also have inputs and outputs.
Each input or output, which can be either simple or
complex, is defined to have a unique port name. The
collection of input and output ports for each component is
defined as its interface. Input and output ports may be used
to receive or send simple data or complex objects.

Standardized
Models

Scalable Entity Structure Modeler

Figure 1: Logical, visual, and persistent model types with
model translators
SESM is a component-based modeling approach in
which families of system specifications are defined in terms
of elementary Template, Instance Template, and Instance
model types (refer to Figure 2) [10]. Each of the model
types is defined to have primitive and composite model
components. Every composite model component is a
hierarchical tree where its leaf nodes must be primitive
model components. A model component can be specialized
such that its specializations are distinguishable. SESM
defines a set of axioms that characterize compositional and
specialization relationships across the elementary model
types with support for object-oriented and markup
languages (see Section 3.1). Two kinds of models are
defined – i.e., simulatable components are used to define
simple and complex dynamic models and non-simulatable
models are used to define the static models (models that
describe structure, but not behavior).

A primitive component can correspond to either a
Template Model or an Instance Model. For the Template
Model, its primitive component can be specialized using the
is-a relationship. The term specializee is used to refer to the
component that has a specialization relation to a component
called specialized. The input/output interface of every
specialized component is defined to be the same as the
interface of its specializee. The state variables of specialized
components may be different. A pair of specializee and its
specialized can be distinguished based on their names.
There are no specializee components for Instance Models.
All
primitive
instance
model
components are
distinguishable based on their assigned (or given) names.

A realization of the SESM approach has been
developed using Java and DBMS technologies [1, 5, 7].
SESM’s underlying software architecture style is
client/server. The first generation of SESM used the Oracle
database [5] and subsequently was replaced with MS Access
[1, 7]. A modeler can have multiple, independent modeling

SpringSim Vol. 1

A composite model corresponds to a Template Model,
an Instance Template Model, or an Instance Model. A
composite model consists of one to many primitive and/or
composite components. The composite model and its

201

ISBN 1-56555-313-6

components have the same model type. A composite model
may also have one or more states, inputs, outputs, and a set
of links connecting the components that are contained
within it. Any two components can send and receive
information using links. Every composite component for a
Template Model or Instance Template Model has a unique
name and tree structure.

A model with behavior specification needs to provide its
own execution regime with or without use of logical or real
time.
A simulatable structure of a system has non-simulatable
structures. In non-simulatable structures, it is important to
specify the types for state or port variables. A state variable
can have a complex type such as a list, and a port variable
can have a simple type such as a string.

The allowed relationships among composite
components are whole-part and is-a. Given a component, a
sub-component
and
super-component
composition
relationship may exist only when no sub-component can be
the same as its (immediate or higher) super-component. The
sub-component is referred to as part and the component and
super-component are referred to as whole. The number of
components of a composite model can be either specified or
left unspecified. A composite component can also be
specialized as in a primitive model. Composite components
can be used in multiple composite Instance Template
Models. The hierarchy depth of a composite component is
equal or greater than two. All instances of a composite
component (corresponding to the Instance Model) are
distinguishable from one another using their assigned (or
given) names. The primitive and composite Instance Models
are instances of their respective Instance Template Models.
The whole-part and is-a relationships are constrained as
described. The uniformity constraint – i.e., two components
which have the same name have identical structures.

The semantics of the composition and specialization
relationships used in UML are distinct from those that are
defined for the Template, Instance Template, and Instance
Models. The composition relationship among nonsimulatable UML classes allows a class to have a
composition relationship with itself. A class may have a
dependency or realization relationship to another class.
These relationships are not allowed in the Template Model.
Similarly, the UML inheritance (i.e., specialization)
relationship allows a child component to extend or restrict
its parent component. In the Template Model, the
specialization relationship is restricted to a specialized
component to replace its specializee component. There are
no extensions or restrictions between the specialized and
specializee.
Given the distinct roles the simulatable and nonsimulatable models play, the importance of differentiating
them becomes evident. The abstractions defined for the
Template, Instance Template, and Instance models are
principally targeted for specifying alternative architectural
system models, whereas the simple and complex models are
intended for the specifications encapsulated within them.
The simulatable model specifications can be transformed
into simulation models that can be simulated. The nonsimulatable model specifications can also be forward
engineered into programming code. In the following section,
the non-simulatable models are informally characterized in
terms of the XML Schema language.

Instance models can only be generated from Instance
Template models. An Instance model can be total or partial
— i.e., a model hierarchy can be of any hierarchical depth
depending on the model that is being transformed. For every
model component that is specialized, one of its
specializations must be selected to replace its specializee. If
the number of sub-components of a component is left
unspecified, the number needs to be determined when an
Instance Template Model is transformed into an Instance
Model.

4.

The state variables and the input and output port
variables may also be specified using data and object
modeling languages. These simple and complex components
and their composition and specialization relationships are
defined with UML, DTD, or XML specifications.

Instead of using objects as basic ingredients for
specifying dynamics of systems, it is useful to use data
types as in XML Schema (XML-S). The data types, unlike
objects, are void of behavior. Data types, in contrast to
objects, offer a rich set of constructs to specify data
elements, attributes, and data. The XML Schema language
constructs allow describing structures having whole/part and
is-a relationship. Also, unlike object-based modeling
languages the language supports creating and using simple
and complex data types.. For example, an element can
contain text and unconstrained child elements or contain
text with strict rules.

3.2. Simulatable and Non-simulatable Models
To represent different possible structures of a system, it
is important to use simple and complex non-simulatable
model types. These model types referred to as nonsimulatable models are distinguished from the simulatable
Template, Instance Template, and Instance models. A
simulatable model specification has a simulation protocol
that dictates how the simulatable model is to be executed in
(logical or real) time. In contrast, a non-simulatable models
specified in UML may or may not have behavioral aspects.

ISBN 1-56555-313-6

XML SCHEMA MODELS

Conceptually XML-S primitive and complex data types
are similar to the SESM primitive and composite model

202

SpringSim Vol. 1

component (see Figure 3). The SESM primitive and
composite model components can be used to specify XMLS primitive and composite data types. The state variables of
the SESM model components can also be specified as
XML-S primitive and composite data types. The state
variables can be specified using object-orientation and
XML-S modeling languages according to the degree of
sophistication required. For example, an element can have
simple content – a name having simple data type String. The
element containing text conforms to a specific data type
such as Integer. Alternatively, a simpleContent element
can have attributes with constrained content. The content is
simple data types and can hold attributes with extension or
restriction elements. While these kinds of specifications are
possible to specify using the extensibility afforded by UML
MOF or others (e.g., by extending the SESM modeling
approach), the XML-S standard is well suited for such data
modeling.

persistent modeling with capability to model object
structures and behaviors as well as complex structured data
models enabled by XML-S.
The other capabilities are forward and reverse
engineering. With forward modeling, XSD and DTD
models can be derived from SESM specifications. With
backward modeling, XSD and DTD models that are
consistent with SESM can be put into the database and thus
support visual modeling, analysis of model complexity (i.e.,
using complexity metrics). These two capabilities offer a
round-trip modeling approach for specifying alternative
system designs. Furthermore, component-based simulation
code can be generated semi-automatically. In particular,
DEVSJAVA atomic and coupled models can be generated
from SESM models. The SESM environment supports
partial specification of atomic models (i.e., input/output
ports, state variables, and the skeletons of the transition
functions). Coupled models can be specified completely
(i.e., input/output ports, couplings, and hierarchical
decomposition).

Data Schema

5.

Before presenting an example model, an overview of
the SESM environment is given. This environment consists
of three parts: Client, Server, and Network. All write
operations requested by a client are managed through the
Network part and processed by Server. The Server enforces
the axioms of SESM and consequently legitimate client
operations are handled by the database. All read operations
are directly handled by the database.

element

primitive
data type
attribute

As shown in Figure 4, a client can model a system in
terms of the simulatable and non-simulatable modeling
elements. In the left-hand frame, there are two main tabs:
Simulatable and Non-Simulatable. The Simulatable tab has
three tabs corresponding to the Template, Instance
Template, and Instance models. The menu item Model
shown in Figure 4 allows a user to create Template,
Instance, and Data Type models. The tree structures of the
Template, Instance Template, and Instance models are
shown in each tab and can be manipulated (i.e., new models
can be added, modified, and deleted). The Non-Simulatable
tab has two tabs corresponding to the non-simulatable
models (NSM) and Data Types. The simulatable and Data
Types models are stored in the database.

complex
data type

Figure 3: SESM XML Schema data models
The XML-S sequence and choice elements serve a role
similar to the composition and specialization relationships
defined for SESM. The sequence element defines the
enclosed elements that appear both in the instance structure
and declaration. The choice element defines required and
exclusive elements that are enclosed in an element. These
elements can be combined to specify complex models as
supported in SESM using the whole/part and is-a
relationships defined in SESM.

In the right panel, a client can view the color coded
block models of the TM, ITM, and IM models. Rounded
rectangles are used to visually represent primitive
components. A rectangle is used to identify composite and
specializee components. Components with a multiplicity
range are shown as rectangles with dashed lines. The same
color coding is used to differentiate model types in the
model tree representations.

As shown in Figures 2 and 3, on the one hand, the
SESM simulatable components can be used to represent
XML-S primitive and complex data types. On the other
hand, UML and XML-S languages can be used to represent
SESM non-simulatable components. A key benefit is the
SESM foundational concept of a unified logical, visual, and

SpringSim Vol. 1

SESM ENVIRONMENT

203

ISBN 1-56555-313-6

(a): Template tree and block models

(b): Instance template model

Figure 4: SESM UI for tree and block model specification
The tree representation uses ‘folder’ and ‘page’ visual
notations with a letter S to distinguish specializee models.
The SESM naming convention tabs for the Template,
Instance Template, and Instance models include the SES
terms to aid modelers working with the SES concepts. The
SES trees can be represented with TM and IM models and
the pruned entity structure (PES) can be represented with
IM. The block model components are placed diagonally for
more efficient and simple visual representation and
manipulation. The ordering of the block models (which do
not have ports and couplings) for the WineOntology
composite model shown in Figure 4(a) does not have any
specific semantics within the SESM modeling framework.
The order of RegionGrows, Regions, and Wine is
based on the order in which they are added to the
WineOntology model.

models). Changes to NSMs are not supported within SESM;
instead creation, modification, and deletion can be
performed using other means (e.g., UML tools, Eclipse
Modeling Framework [4], and XML-Spy [15]). The
structural complexity of every template model is also
available (see Figure 6) [7, 11].

Visual modeling of coupling relationships, specification
of states (variables and types) and ports (port names,
variables, and types) are supported in this panel. Complexity
metrics and translation to XML and simulation code are
supported in the tree structures, block models, and the menu
items (Edit, Metrics/Views, and Transformations lists). The
Model menu item supports creating Template Model,
Instance Model, and Data Type elements. These Data Types
such as the one shown in Figure 5 complement the NSM
models [2, 11] (see Section 3.2).

Figure 5: XML-S specification for dewPoint model

The Database menu item which is available in the main
menu can be used to initialize the model (i.e., removing all
entries in the database – simulatable and Data Type

ISBN 1-56555-313-6

Figure 6: Behavioral metrics for Region model

204

SpringSim Vol. 1

Once all multiplicities are determined, instance models
can be generated for every primitive and composite Instance
Template model. The Instance model for the
WineOntology model is shown in Figure 8. This model
has
RegionGrows_1_1,
Regions_1_1,
and
RoseWineColor_0_2 which is a specialized from the
WineColor primitive Template model. Similarly,
Region_1_2 and RedWineColor_0_1 instance models
are generated for the WineCountry_1_2 instance model.
Finally, Region_2_3 and Region_3_4 instance models
are generated for the Regions_1_1 instance model.
The element Region, shown in Figure 9, is a wellformed XML-S model transformed from the SESM
primitive component Region along with XML-S
dewPoint and GeoPosition data types. Such XML-S
or DTD models can be automatically generated for every
model that is developed using DTD and XML-S translators
added to the SESM environment. The generation of an
instance model in SESM results in a model that can have the
same representation as those that can be generated with the
SES/DTD and SES/XML translators developed for SESJava [13]. This requires (i) there is a correct mapping
between SES and SESM and (ii) the representation of XML
or DTD data types is also supported in the same way for
SES and SESM [12]. For example, the resulting XML-S
shown in Figure 9 represents a leaf entity of an SES model
that has attributes specified according to the XML-S data
types.

Figure 7: Multiplicity for the WineCountry model
Given the combined SESM and XML-S specification,
every Instance Template model can be transformed into an
XSD specification. Given the expressiveness of SESM, it
can be used to visually specify persistent SES models.
Given the Instance Template models such as those for the
WineOntology, Instance models can be generated from
them. If an Instance Template model has a multiplicity
range, the user first chooses a desired multiplicity. For
example, WineCountry has multiplicity ranging from 1 to
n (see Figure 4(b)). Figure 7 shows the multiplicity for the
WineCountry model to be 1.

Figure 9: XML-S Region model
Figure 8: WineOntology Instance model

SpringSim Vol. 1

205

ISBN 1-56555-313-6

6.

CONCLUSIONS

The importance of modeling complex systems from
complementary object and data perspectives were described.
This offered a motivation for extending the SESM modeling
framework with XML. The simulatable and non-simulatable
model types, roles, and relationships were presented. The
collective capabilities of object and data model development
were demonstrated using the extended SESM realization. It
offers a step forward toward realization of multifaceted
model development as proposed with DoDAF. The SESM
framework supports system design from object-centric and
data-centric perspectives. With the addition of the XML
models, design specifications can be supported from
simulation and non-simulatable perspectives. The use of the
SESM modeling framework for specifying the class
SES/XML models was described. Future work includes
application of the SESM framework for simulation-based
design and automated testing. The SESM environment may
be used toward engineering of data intensive systems with
capability to process data available across grid networks and
delivery of information using web-services.

8.
9.

10.

11.

12.
13.
14.

Acknowledgement
This research is supported in parts by grants from NSF,
Intel Corporation, and Northrop Grumman. We would like
to acknowledge the fruitful discussions regarding the Data
Engineering project with Bernard Zeigler of the University
of Arizona and Philip Hammonds, Rodney Leist, Steven
Madden, and Chad Schulenberg of JITC/DISA and
Northrop Grumman.

15.
16.
17.

References
1.

2.

3.
4.
5.

6.
7.

18.

Bendre, S., Behavioral Model Specification Towards
Simulation Validation Using Relational Databases, in
Computer Science and Engineering, 2004, Arizona
State University: Tempe, AZ, p. 1-150.
Bendre, S. and Sarjoughian, H.S. Discrete-Event
Behavioral Modeling in SESM: Software Design and
Implementation, Advanced Simulation Technology
Symposium, 2005. p. 23-28, San Diego, CA.
Bradley, N., The XML Schema Companion, 2004:
Addison Wesley.
Budinsky, F., Steinberg, G., Merks, E., Ellersic, R., and
Grose, T., Eclipse Modeling Framework, 2003:
Addison-Wesley.
Fu, T.-S., Hierarchical Modeling of Large-Scale
Systems Using Relational Databases, in Electrical and
Computer Engineering, 2002, University of Arizona:
Tucson, AZ, p. 1-114.
Jordan, D. and Russell, C., Java Data Objects, 2003:
O'Reilly.
Mohan, S., Measuring Structural Complexities of
Modular, Hierarchical Large-scale Models, in

ISBN 1-56555-313-6

19.

206

Computer Science and Engineering, 2003, Arizona
State University: Tempe, AZ, p. 1-112.
OMG.
Unified
Modeling
Language,
2004,
http://www.omg.org/technology/documents/formal/uml
.htm
Park, H.C., Lee, W.B., and Kim, T.G., RASES: A
Database Supported Framework for Structured Model
Base Management, Simulation Practice and Theory,
1997, 5(4), p. 289-313.
Sarjoughian, H.S., An Approach for Scaleable Model
Representation and Management, 2001, Computer
Science & Engr., Arizona State University: Tempe, AZ,
p. 1-9.
Sarjoughian, H.S. A Scaleable Component-based
Modeling Environment Supporting Model Validation,
Interservice/Industry Training, Simulation, and
Education Conference, 2005. p. 1-11 Orlando, FL.
Sarjoughian, H.S., Fu, A., Bendre, S., and Flasher, R.,
A Unified Logical, Visual, and Persistent Modeling
Framework, in-preparation.
SES-Java.
Virtual
Work
Table,
http://www.devsworld.org/, 2006.
XML.
eXtensible
Markup
Language,
2005,
http://www.w3.org/XML/.
XMLSpy. XML editor for modeling, editing,
transforming, & debugging XML technologies, 2006.
Zeigler, B.P., Multi-Facetted Modeling and Discrete
Event Simulation, 1984, New York: Academic Press.
Zeigler,
B.P.
and
Hammonds,
P.E.,
Modeling&Simulation-Based
Data
Engineering:
Introducing Pragmatics into Ontologies for Net-Centric
Information Exchange, in-press, 2006.
Zeigler, B.P., Praehofer, H., and Kim, T.G., Theory of
Modeling and Simulation: Integrating Discrete Event
and Continuous Complex Dynamic Systems, Second
Edition, 2000: Academic Press.
Zeigler, B.P. and Zhang, G., The System Entity
Structure: Knowledge Representation for Simulation
Modeling and Design, in Artificial Intelligence,
Simulation and Modeling, K.A.L. L.A. Widman, and N.
Nielsen, Editor. 1989, John Wiley. p. 47-73.

SpringSim Vol. 1

Proceedings of the 2010 Winter Simulation Conference
B. Johansson, S. Jain, J. Montoya-Torres, J. Hugan, and E. Yücesan, eds.

DEVS-SUITE SIMULATOR: A TOOL TEACHING NETWORK PROTOCOLS
Ahmet Zengin

Hessam Sarjoughian

Computer Science Department
Sakarya University Faculty of Technology
Sakarya, 54187, TURKEY

School of Computing Informatics
Decision Systems Engineering
Arizona State University, Tempe, AZ, USA

ABSTRACT
Understanding of the underlying concepts, principles, and theories of computer network can signiﬁcantly beneﬁt
from simulation tools. Usage and development of simulation models for computer networks, however, can be
demanding in educational settings. While a variety of open source and commercial tools are available, there remains
a desire for simulators that can better support student learning and instructor teaching. In this work, the DEVS-Suite
general-purpose simulator is extended to support modeling of network protocols. A model library for the OSPF
protocol has been developed such that the emphasis is placed on education to capture the basic principles of
network protocols using sound modeling and simulation principles instead of supporting highly detailed network
protocol simulations. The use and pedagogical effectiveness of the DEVS-Suite network simulator is carried out in
a classroom setting. The results of the student survey are presented and discussed.
1

INTRODUCTION

Computer networks including communication protocols (e.g., TCP/IP and OSPF) are essential for computers to
communicate with each other. Although communication protocols can be simply described as a set of rules governing
the syntax and semantics of computing resources, simulation modeling is necessary to understand their complex
nodes. In particular, simulation tools are commonly used in teaching computer network principles and practices
(e.g., (Guo, Xiang, and Wang 2007)). In response, a variety of tools (e.g., ns-2 (Simulator 2010), OMNeT++
(Varga 2010), OPNET (OPNET 2010)) have been developed to meet the needs of scientists and engineers primarily
for research purposes. This is unlike simulation tools (e.g., Matlab /Simulink ) that are commonly used for
teaching engineering & science subjects.
A simulation tool may be classiﬁed as either domain-speciﬁc or domain-neutral. A domain-speciﬁc simulation
tool has a library of domain-speciﬁc components that deﬁne models for a domain such as computer networks. Tools
such as OPNET are targeted for domain experts and assume users, among other things, understand fundamentals of
modeling and simulation concepts and methods. In contrast, a general-purpose simulator is developed based on a
generic modeling and simulation theory or framework. An example of this kind of simulation tool is DEVS-Suite
(DEVS 2010). To use such a tool for simulating computer networks, it is desirable to provide a library of model
components such as processing nodes and communication protocols. Such a tool can be well suited for educational
use since the rigorous DEVS model formalism play an important role in teaching and learning domain knowledge
since the principles of modeling and simulation theory (e.g., separation of a model from its simulator) are already
established (Chen and Sarjoughian 2010).
In this paper, we will introduce a domain-speciﬁc model component library for OSPF protocol into the DEVSSuite simulator. In Section 2, selected network simulation tools and the OSPF protocol are brieﬂy presented.
In Section 3, DEVS-Suite simulator is described. In Section 4, network component models are developed in
DEVS-Suite. In Section 5, an exemplar network protocol model is used to demonstrate the key features of the
DEVS-Suite network simulator. In Section 6, the role of the simulator in student learning and teacher instruction
in an educational setting is discussed. In Section 7, concluding remarks including future work is brieﬂy presented.

978-1-4244-9865-9/10/$26.00 ©2010 IEEE

2947

Zengin and Sarjoughian
2

BACKGROUND AND RELATED WORK

2.1 Network simulation tools
An important method for design and analysis of such systems and their routing protocols is simulation modeling,
especially when analytical methods are known to be inapplicable (Floyd and Paxson 2001). Basically, network
simulators try to model real world network systems and often are required since experiments may not be possible
with actual computer networks. However, real network systems can be modeled by means of abstraction mechanism
which renders possible to model complex systems into the limited resource computers. A network simulator should
enable users to represent a network topology, prepare different scenarios, specify the nodes and links, and analyze
the results. Graphical user interfaces allows the simulation users to visualize and track working of simulated
environment.
The basic limitations of analytical approaches have led to a variety of modeling and simulation approaches and
tools. Tools such as ns-2, ns-3 (ns 3 2010), OPNET, OMNeT++, Glomosim (Zeng, Bagrodia, and Gerla 1998) and
SSFNet (Cowie, Ogielski, and Nicol 2002) are used to reveal the inner workings of computer networks in virtual
settings. A key emphasis has been on enabling design and testing of routing algorithms, MAC layers, and end-to-end
queuing. Although the capabilities of these simulation tools support describing (wired and wireless) computer
and device network protocols and communications in great detail, their underlying foundation lacks support for
developing models in system theoretic manner. The conceptual models of these tools are derived from computer
network hardware and software abstractions. These models are mostly implemented in object-oriented programming
languages and simulated in virtual and/or emulated in physical testbeds. But these tools have some disadvantages in
terms of underlying methodology, implementation and scalability (Fujimoto et al. 2002). These reasons are limited
or lack object-oriented concepts for designing and implementing simulators. They lack support for casting real world
entities to non-object simulation components. Furthermore, scalable and efﬁcient execution requires support for not
only concurrent simulation execution methods but also simple, yet sound modeling concepts. That is despite tools
such as pdns (Riley, Fujimoto, and Ammar 1999) and SSFNet being implemented in object oriented programming
languages, they lack formal modeling theories. Furthermore, other tools such as OPNET are inherently not well
suited for parallel and distributed execution. Other concerns about these simulators are limited or weak support
for visualization as well as difﬁculties in tool installation and ease of use. Detailed comparisons of the network
simulators are presented in (Begg et al. 2006) and (Lessmann et al. 2008).
2.2 OSPF Protocol
Routing protocols in the network systems can be split into two main categories: link state routing and distance
vector routing. Currently, in particular for Internet, while distance vector protocols are used for inter-gateway
interactions, link state protocols are used for intranet case (Steenstrup 1995). Open Shortest Path First (OSPF)
as one of the famous link state routing protocol is an open standards routing protocol and a particularly efﬁcient
interior gateway (IGP) routing protocol that is faster than routing information protocol (RIP) which is one of the
most known kinds of the distance vector protocols family. It employs the Dijkstra algorithm when estimating the
shortest paths (Dijkstra 1959).
The OSPF routing protocol was developed to provide an alternative to RIP, based on Shortest Path First
algorithms instead of the Bellman-Ford algorithm which is the basis for RIP. It uses a tree that describes the network
topology to deﬁne the shortest path from each router to each destination address. Since OSPF routing protocol
keeps track of entire paths, it has more overhead than RIP, but provides more options. The main difference between
OSPF and RIP is that RIP only keeps track of the closest router for each destination address, while OSPF keeps
track of a complete topological database of all connections in the local network. The OSPF algorithm works on
three phases. They are important for describing how the algorithm behaves in discrete event fashion. In the Startup
phase as soon as a router connects to the network, it sends Hello packets to all of its neighbors, receives their Hello
packets in return, and establishes routing connections by synchronizing databases with neighbor routers that agree to
synchronize. In the Update phase, each router sends an update message at regular intervals. This message is called
its “link state” describing its routing database to all the other routers, so that all routers have the same description
for the local network topology. Finally, in the shortest path tree phase, each router then estimates a mathematical
data structure called a “shortest path tree” that describes the shortest path to every destination address indicating
the closest router for communication.
3

DEVS-SUITE SIMULATOR

Network systems exhibit very high level complex, dynamic and parallel characteristics. Discrete event modeling
brings abstraction and simpliﬁcation mechanisms which facilitate modeling and simulation study of complex system
dynamics. The dynamics of network systems can be characterized in terms of components that can process and

2948

Zengin and Sarjoughian
generate events. Among discrete event modeling approaches, the Discrete Event Systems Speciﬁcation (DEVS)
(Zeigler et al. 2000, Chow 1996) is well suited for formally describing concurrent processing and the event-driven
nature of arbitrary conﬁguration of network systems. This modeling approach supports hierarchical modular model
construction, distributed execution, and therefore characterizing complex, large-scale systems with atomic and
coupled models.
DEVS-Suite (DEVS 2010, Kim, Sarjoughian, and Elamvazhuthi 2009) is an open source, general-purpose
discrete event simulation environment supporting parallel DEVS models. It is the next generation of the DEVSJAVA
simulator (ACIMS 2010) and supports execution of models using sequential and concurrent schemes. Hierarchical
models can be displayed as components within component style with support for animation of I/O messages among
model components and viewing of atomic model’s states.
DEVS-Suite introduces new concepts and capabilities to support the complex task of simulation modeling. It
offers on-the-ﬂy conﬁguration of experiments (GUI-based selection and monitoring of I/O and states) and visualization
of each component’s behavior as time-based trajectories as well as synchronization of animation and time trajectories
(Helser 2009). This simulator consists of the DEVSJAVA simulator kernel and Simview for hierarchial viewing of
models and their message-passing animation, Timeview for plotting time trajectories, DEVS Tracking Environment
for automated design of experiments, and controller for time-stepped, combined visualization of component animation
and time trajectories. The DEVS-Suite simulator core software architecture uses the Model Facade View Control
(MFVC) which was ﬁrst used in the DEVS Tracking Environment (Sarjoughian and Singh 2004). These capabilities
along with support for use via web are important for education and distance learning (Sarjoughian 2010).
In DEVS-Suite, execution of the models can be tracked as both the animation of the input/output messages for
atomic/coupled models and the state changes of the atomic models, as well as log ﬁles. Simulation experiments can
be triggered with test input which can be selected via a dialogue box at the beginning of the simulation and time-based
trajectories generated during simulation. At the end of the simulation, statistical outputs and trajectories can also
be obtained for pre-deﬁned phase and sigma state variables. Simulator has also an option window when loading
the model which includes simview (Mather 2003) and tracking options. simview is inherited from DEVSJAVA that
provides visualization of DEVS models. However, visualization can signiﬁcantly decrease simulator’s performance
due to high demand for hardware resources. In DEVS-Suite, users can choose in any combination animation of
model components, viewing of components’ time trajectories as well as storing the observed date in a CSV ﬁle, or
writing user-deﬁned data in source code to console. The degree of ﬂexility has important beneﬁts including reducing
execution time of large-scale experiments. Obviously, different users have different needs as they develop their
simulation models and experiment with them. Hence, different needs of users can be satisﬁed given the simulator’s
different capabilities and ﬂexibility.
4

DEVS MODEL FOR NETWORK PROTOCOL SIMULATION

In this section, we summarize some features of the DEVS-Suite network simulator. Detailed information of underlying
modeling and simulation knowledge about developed system together with validation experiment can be found in
(Zengin and Sarjoughian 2009). DEVS models of the other simulation protocols are implemented as such Routing
Information Protocol (RIP) and Swarm Intelligence based protocols in (Zengin, Sarjoughian, and Ekiz 2004), AODV
wireless protocol in (Farooq, Wainer, and Balya 2007) and wireless sensor networks in (Santoni et al. 2008). For
handling model and simulation scalability, a high level model abstraction for real networks is adopted - e.g., MAC
layer and acknowledgement packets details are carefully considered and simpliﬁed.
4.1 Basic Components
In order to develop a domain-speciﬁc modeling and simulation environment such as DEVS-Suite network simulator,
it is important to deﬁne basic components such as nodes and links. The basic network components are depicted
in Figure 1 in the form of interacting elements composing a network and its experimental frame. They represent
application layers tasks. Since the main focus is aiding teaching and learning of network routing protocols, classical
seven-layered approach is not considered. Instead, a ﬂat hierarchical, but modular approach is chosen and the main
components desired for protocol implementation such as queues, packets and routing tables are modeled. Atomic
node behavior depends on interacting modules via messages that lead to whole design cross-layer interdependencies.
Modular design allows integration of the new models easily and models designed based on DEVS atomic and
coupled model speciﬁcation are ﬂexible to add enhancements. A coupled model can be used to create user deﬁned
network topologies in network system to evaluate network functionality and educational purposes. Coupled models
is used to model Autonomous Systems (AS) in large networks.
To run the network under speciﬁc scenario, applications such as user trafﬁc and errors have to be implemented.
All transactions in terms of session and application layer’s tasks are treated as events in network model. The
experimental frame has simply an event generator and a transducer to generate different kinds of event and analyze

2949

Zengin and Sarjoughian


















































	




































	







"





%

'














!















#

$







#

"








	

	


&



"

)

#

#





(







Figure 1: Network model components

*

+

,

-

.

/

0

,

1

2

3

4

+

5

*

+

,

-

.

/

0

,

6

+

7

8

.

9

:

D

A

.

4

9

0

,

I

D

,

2

=

D

;

<

3

,

B

<

,

,

G

L

3

<

2

+

?

7

@

.

/

0

,

+

?

H

=

D

=

@

K

.

5

H

,

>

,

?

2

C

,

3

<

2

,

2

E

=

F

8

.

2

3

8

F

=

>

,

?

2

@

8

.

?

A

B

<

5

,

8

2

J

L

?

,

E

K

1

B

B

8

,

A

A

6

+

O

H

A

2

8

.

,

J

?

G

A

+

2

?

.

H

?

,

5

B

,

@

M

N

3

Figure 2: Network model class diagram

the outputs, respectively (see Figure 1). Transducer’s observation capability together with the tracking environment
provide enhanced control and observation over developed models under experimentation.
Figure 2 depicts a simpliﬁed class diagram for the DEVS-Suite network simulator. All processing models such
as nodes, links, generator and transducer are derived from the viewable atomic classe. Atomic and digraph classes
are derived from the “devs” class which supports generic structure and behavior (e.g., I/O interface, timing) for all
atomic and coupled models developed for the DEVS-Suite network simulator. Dashed lines in the diagram show
associations of the classes while solid lines depict dependency. Lowest level component is the Route class that
deﬁnes the basis for evaluation and training of OSPF and other protocols.
4.1.1 OSPF dynamics
The DEVS-Suite OSPF implementation is developed on top of the DEVS-Suite kernel. DEVS-Suite OSPF environment
exploits DEVS formalism and proven software engineering techniques to achieve performance, ease of deployment
and use, accuracy, scalability and system theoretic design. As depicted in Figure 3, events processed by nodes
and links can be described as statecharts. They are important for showing state changes in response to internal
and external events and thus visualizing the behavior of simulation models. A node atomic model is born with
“startup” phase and does not have any information for other model components. After sending a hello message to
its neighbors, every node starts to create tables and learns what happens in the network. Ten events in the system
are selected for educational and performance purposes. Increasing number of events decrease performance and
increase accuracy. These ten events are sufﬁcient for understanding the OSPF routing protocol logic. Only external
and internal transition functions can cause a node to change its context for new events.

2950

Zengin and Sarjoughian
P

Q

R

R

S

T

P

f

U

X

Y

Z

[

\

]

V

W

R

X

o

X





]





W

g

T

Q

R

R

S

T

P

Q

^

Y

Z

[

\

]

Y

Z

p



|

[





\

]

r

{

^



r



{

s

q

|

{

s

w

q

s



|





y

r





^



X

Y

Z

[

\

]

^

X



]







?packet

!packet
_

f

R

j

U

R

W

`

a

b

c

b

c

_

d

c







if queue is empty




S

k



Q



W

Q

W

S

T

P
j

X





T



X

Y

Z

[

\

]





a

b

c

b

c

_

d

S

T

P

Q

S
P



g

V

W

U

up



X

`

k

^

X

_

k



`

b

e

Y

Z

[

\

]



]







^

?packet

transmitting

o

p

q

r

s

t

u

v

w

x

y

z

{

|

v

}

r

~



s



r









q

s



z

q

j



V

T

P

Q

f

R

Q

k

k

S

T

?packet
P

k

l
h



V

V

k

S

T

m

n

?packet

P

if queue is not
empty

if queue is full
X

Y

Z

[

\

]

!packet

e

^









if queue is full

h

V

U

i

j

l

U

m

k

S

T

P

!packet

n

congested
£








































 



 







¡



¡

£






¡

¤
¡

¤




¥
¥

¤
¤








¦
¦









§







¢
¢































 
 










¡
¡










¥

¤





¦






(a)

(b)

Figure 3: (a) Node states, state transitions and output, (b) Link states, state transitions and output.

4.2 Buffering
In developed modeling and simulation environment, queue is a Linked List data structure which maintains the FIFO
(First In First Out) order of the objects inserted into it. Some new objects can be inserted and others removed, but
the order of the objects in both states is maintained. Packets have different priorities which means insertion order
of newly incoming packets is determined by priority value (0 lowest, 7 highest). The queue implements a very
simple drop-tail queue with the following behavior.
•
•
•
•
5

If packet’s priority value has highest value, it is inserted at the front of the queue.
If a packet is to be enqueued and is bigger than the maximum queue size, it is rejected.
If a packet to be enqueued is smaller than the maximum queue size, but there is not enough space for it,
the packets at the end of the queue are dropped until enough space becomes available.
If there is enough space for a packet to be enqueued, it is inserted at the front of the queue.

OSPF PROTOCOL SIMULATION WITH DEVS-SUITE

In this section, we go step by step through how a model is loaded and simulated in DEVS-Suite network simulator.
The emphasis is to describe how a model can be experimented with by focusing on key needs during the simulation
execution, particularly in terms of visualization and tracking aspects.
5.1 OSPF Protocol Visualization by Simview
5.1.1 Model loading options
The visualization capability is an optional property for simulation study in DEVS-Suite. It is important in particular
when we simulate large-scale simulation systems due for visual components in a software system may occupy a
great deal of computation resources. Hence, in general during research, a pure textual output of simulation results is
preferred for the sake of performance. Visualization has an important role in education. A graphical representation
of a simulated scenario can help to comprehend and track basic logic behind the routing protocol and whole network
dynamics. Therefore, DEVS-Suite provides a toggle option while simulation model is loading (see Figure 4).
5.1.2 Model visualization
A modeler can view the entire implementation of a computer network as well as, for example, the OSPF protocol.
Figure 5 depicts a network model consisting of four nodes, four links, one generator, and one transducer. A model
component is represented by a rectangle, with input ports on the left and output ports on the right. Inside the box
view of the model, current state, model name and its sigma value can be seen as binary strings. Couplings are
represented by lines. The entities belonging to network systems such as routes, routing tables, packets and queues
can also be visually tracked by developed simulator (see Figure 4). If the mouse is placed on a network node, a
tooltiptext shows neighbors table and routing database of related node together with all user deﬁned model states
that are selected for viewing (see Figure 4). No such visualization is made available in ns-2 and OMNeT++.

2951

Zengin and Sarjoughian

(a)

(b)

Figure 4: (a) DEVS-Suite model load pane toggling between simview and tracking visualization, (b) run-time
viewing of routing table state.

Figure 5: DEVS-Suite simulation viewer and a sample OSPF protocol simulation model.

5.1.3 Controls over simulation experiments
DEVS-Suite together with Tracking Environment provide complementary mechanisms for controlling simulation
experiments. As seen in Figure 5, current simulation time is displayed at the bottom. A slider serves a real time
factor at any time during the simulation. The simulation execution can be controlled using Run, Step, Step(n),
Request Pause, and Reset (see Figure 5).
The other controls show coupling, real-time factor, animation speed, and TimeView update speed sliders.
DEVS-Suite separates animation of messages and time-based trajectory plotting and thus provides ﬂexibility in
simulation speed vs. simulation visualization. Simulated model can be triggered manually with an input. Though
generator atomic model in experimental frame can schedule events in automated manner, modeler can create an
input event manually during simulation by clicking on the input ports, a small pane appears (DEVS 2010), than
click step button to observe the change of the states and outputs. An input event consists of a port name, data value,
and elapse time (e.g., port: in, value: packet1 packet, e: 0.0). The elapsed time is a time stamp of the associated
event and is used to schedule and inject a speciﬁc event at some, ﬁnite, future time instance. It is in units of time
after the current time, shown by simulator clock.
5.2 Envisage the discrete event logic of OSPF protocol in a virtual lab
It is important to track a network protocol’s execution visually in teaching computer network concepts and theories
(see Figure 6). Students have to master protocols’ logic in a step by step manner. Though ns-2 provides a level
of detail in nam visualization, it is impractical to understand much of the protocols details via animation. The
DEVS-Suite network simulator allows students to comprehend the creation of routing databases as well as detailed
behavior of the simulated communication protocol. The basic underlying details of the OSPF protocol simulation

2952

Zengin and Sarjoughian

Figure 6: An animated view of the network exchanging the learned topology of LSA messages.

Figure 7: Tracking network model

model are described in (Zengin and Sarjoughian 2009). In the following, discretized execution of the OSPF protocol
logic is divided into several stages and assisted with screenshots to demonstrate the educational role of the simulator.
As already mentioned in Section 2, a link-state protocol uses a neighbor table and a topology database in
addition to adding routes to the routing table. In order to mimic the OSPF routing table creation, updating, and
registration, the node model accommodates vector data objects for two data sets. Then these two objects pass to
the Dijkstra class as parameters and therefore shortest path is generated (see Figure 5).
5.3 OSPF Protocol Tracking by Timeview
The TimeView module in DEVS-Suite is developed for runtime plotting of data sets as two dimensional plots
and allows to display all dynamics and properties of every node, link atomic and network coupled model. As
already mentioned, modelers have the ﬂexibility to select animation and tracking view options for any number of
atomic/coupled models. They can set the unit for data that is to be monitored as well as the time axis. The time
increment, units, and the selection of data to be observed can be set as shown in Figure 7. Modeler is able to view
inputs, outputs, states and time-based trajectories using TimeView. A few dynamics of the OSPF protocol model
can be seen in Figure 8. These charts together with log ﬁles are useful in examining time-based operations of the
OSPF protocol simulation components.
5.4 Manipulating Simulation Data
Data collection is one of the main process in any simulation study (Robertson and Perera 2002). Besides, data is
used for measuring performance of the simulator, it can be employed for veriﬁcation and validation processes. In
addition, data collection is difﬁcult particularly as the scale of the model increases. In that case, automated data
collection become crucial. DEVS-Suite supports selection, manipulation and alternative viewing of simulation data
sets.
As mentioned above, modeler can observe simulation data for any number of atomic and coupled models without
writing any code. The data can also be obtained in the form of the tabular using a tracking logger. An alternative
data presentation is to export these values to CSV ﬁles. After collecting data from the simulation, the analysis of
network performance is done and commonly available tools can be used to customize and plot simulation results.
6

SIMULATORS’ EVALUATION

To evaluate the effectiveness of the DEVS-Suite network simulator, it is used in a graduate course called Design
and Simulation of Computer Networks. This course with 30 students is taught in the fall 2009 EBE507 at Sakarya

2953

Zengin and Sarjoughian

Figure 8: Trajectories of OSPF protocol model

University Technical Education Faculty in Turkey. The students are required to use the DEVS-Suite and ns-2
simulators in three of their homework assignments. The ﬁrst assignment was to evaluate the deployment of the
tools. During the ﬁrst week of the semester, all students installed ns-2 on Cygwin (Windows machine). They copied
the DEVS-Suite java archive (jar) on their ﬂash memory devices. Students were required to evaluate and compare
the deployment of the simulators. For the second assignment, students were asked to develop a network topology
composed of 10 routers and 11 links both in the DEVS-Suite and ns-2 simulators with almost the same parameters
and underlying protocols. Students were asked to observe the simulation runs and analyze their results. In this case,
visualization and tracking capabilities of the simulators were the focus of the evaluation. For the third assignment,
students studied scalability aspect of computer networks. Students needed to develop a network topology with a few
thousands of nodes using a recursive algorithm and later coded with pure Java and oTcl. The students simulated
these models under varying conditions (experimental settings), analyzed the results, and evaluated the strength and
weaknesses of the DEVS-Suite and ns-2 simulators.
They were asked the following twelve questions. The survey questions are divided to Parts I, II and III. In
Part I, the generic capabilities of the DEVS-Suite simulator were evaluated such as visualization and animation. In
Part II, the degree of the simulator’s support for teaching routing protocol, network structure and behavior were
evaluated.
1.
2.

Did you ﬁnd the simulator interface, layout, and collapsible panels simple and sufﬁcient for your need?
Did you ﬁnd the combination of component animation, time-based I/O and state trajectories views and
console outputs useful and complementary?
3. Did you ﬁnd the simulator’s execution and animation speeds acceptable?
4. Did you ﬁnd the simulator’s control panel and icons (buttons) simple and easy to use?
5. Did you ﬁnd the simulator’s TimeView Update Speed useful?
6. Did you ﬁnd the Enable Governor support useful?
7. Did the simulation of the OSPF increase your understanding of the theoretical principles of OSPF control
packets ﬂow, package queuing and delays?
8. Did you ﬁnd conﬁguration of experimental data simple to view?
9. Did you ﬁnd creation of network composed of nodes and links together with their parameters easy?
10. Did you ﬁnd the effects of link and node errors such as link down and node congestions tractable and
understandable during simulation?
11. Did you ﬁnd observing instantiations and updating of routing tables helpful in the understanding of the
OSPF logic?
12. Did you ﬁnd the assignment instructions explaining how to load and use the simulator clear and understandable?
In Part III, students compared the DEVS-Suite and ns-2 simulators (see Table 2). The questions focused on
relative ease of use as a measure of learning each tool and using it effectively and quickly. Other important factors
in evaluation of each tool were existence of model libraries, support for visualization, execution efﬁciency, and
availability of documentation (most notably textbooks, technical references, user guides, and papers) detailing the
fundamentals of modeling and simulation concepts and methods.

2954

Zengin and Sarjoughian
Table 1: Survey results of DEVS-Suite simulator and OSPF network protocol
Likert Scale
Strongly Agree
Agree
Neutral
Strongly Disagree
Strongly Disagree

Q1
27
65
8
-

Q2
27
54
15
4
-

Q3
62
30
4
4
-

Q4
34
50
12
4
-

Q5
15
42
35
8
-

Q6
15
66
15
4

Q7
46
35
19
-

Q8
30
39
23
8
-

Q9
30
35
19
12
4

Q10
15
31
35
19
-

Q11
42
39
15
4
-

Q12
23
30
27
20
-

Table 2: Survey results - comparing DEVS-Suite and ns-2 simulators
Aspect
Relative ease of use
Model libraries
Visualization
Execution Performance
Documentation

DEVS-Suite
62
30
73
65
15

ns-2
38
70
27
35
85

At the end of the fall semester 2009, an anonymous evaluation of the surveys was completed by 26 out of 30
students. The anonymity of the survey allows students to objectively evaluate the DEVS-Suite and ns-2 simulator.
A ﬁve-point Likert scale (Likert 1932) with values ranging from Strongly Agree to Strongly Disagree was used.
The results of their responses are shown in Tables 1 and 2. All entries in the tables represent percentages. The
individual evaluation of the questions in Table 1 are self explanatory.
Considering the DEVS-Suite simulator by itself, 86% of the surveyed students found the simulator’s visualization
and model layout to be well-suited to their needs (see Table 1). This is according to a cumulative percentage of the
responses to Q1, Q2, and Q4. The remaining 12% and 2% of the students were neutral and unsatisﬁed, respectively.
This is consistent with student responses shown in Table 2 where 73% and 62% of the respondents prefer the
DEVS-Suite simulator’s visualization and ease of use over ns-2, respectively. Similarly, 75% of the respondents to
Q3 and Q5 were very satisﬁed or satisﬁed with the DEVS-Suite simulator’s execution performance, animation, and
time-based trajectory viewing. Again, Table 2 shows the DEVS-Suite simulator offers relatively better performance
as compared with ns-2. In terms of Q6, students were mainly neutral. This can be attributed to not needing to use
DEVS-Suite simulator’s Enable Governor (i.e., synchronizing the speed of the animation and time-based trajectory
plotting) which slows down the simulation execution and inadequate documentation.
The survey results shown in Table 1 show that 81% of the students were very satisﬁed or satisﬁed with the
support provided for OSPF network simulation modeling (see responses to Q7 and Q11). Responses to Q8 and
Q9 suggest 67% of the students found the tool useful for model development and simulation experimentation. The
remaining 21% and 2% were neutral and dissatisﬁed, respectively. The results of the remaining survey questions (i.e.,
Q10 and Q12) are not surprising since on the one hand detecting errors is usually non-trivial and the assignments
used for the surveys were developed for the ﬁrst time.
Considering the overall percentages for the DEVS-Suite and ns-2 simulators, the latter is preferred assuming
equal weight for all the entries in Table 2. The preference for ns-2 over DEVS-Suite is not surprising since the
former offers many protocol implementations belonging to seven segments of the networks such as TCP, AODV,
FTP, etc. Furthermore, ns-2 has a large user base spanning several years with many contributors. The difference
between the documentation for ns-2 and DEVS-Suite simulator is surprising. The ns-2 offers more in terms of
user-guide and technical manuals as well as forums compared with DEVS-Suite. In comparison, textbooks and
other publications on DEVS describe M&S principles, theory, and methods that are the basis for developing model
speciﬁcations and simulation. The signiﬁcant gap between the evaluations suggests that most students did not ﬁnd
the DEVS documentations accessible as it is the case with the use of formal methods in computer science. Students
seem to prefer elaborate user-guides, manuals, and user-community support. These shortcoming lend support for
further development for model libraries and more detailed user-guide documentation for DEVS-Suite.
In our course activities, we did an examination of the emerging ns-3 discrete-event network simulator which is
intended to replace the popular ns-2 simulator. The latest version of ns-3 has been under development since 2006
and release 3.7 is tentatively scheduled for January 2010 (ns 3 2010). Based on basic experiments in classroom
context, ns-3 as compared with ns-2 offers the important improvements listed below:
•

Design and implementation: Scalability, modularity, coding style, and documentation are improved. The
simulator kernel and its auxiliary components such as generators and tracers are written in pure C++.
This has strengthen the object-orientation and thus helps with managing model implementation complexity.

2955

Zengin and Sarjoughian

•
•
•

7

The simulator can also work with Python scripting interface (instead of OTcl) for simulation conﬁguration
(Henderson, Lacage, and Riley 2008). Students prefer Python over OTcl since it is simpler to learn and
use.
Performance: ns-3 provides better performance; in particular, large models execute faster.
Deployment: ns-3 is simpler to install. Its installation is supported with build tools such as waf and Python.
Installation time is less than ns-2 and requires less user interaction. After installation, network models
written in C++ ﬁles are easy to build and run.
Visualization: ns-3 visualization capability remains limited, visualization tools are under development .
For example, Gustavo Carneiro is developing the ns-3 PyViz (ns 3 2010).

CONCLUSIONS

Use of simulation is common across many scientiﬁc and engineering ﬁelds including computer networks. Our
experience and those of others had revealed limitations of popular network simulators for education purposes. That
is despite major efforts to fulﬁll this need with tools that are targeted for research and/or commercial applications.
Given the advantages of the general-purpose DEVS-Suite simulator, we extended it with a library of computer
network model components. The goal was to have a simulator that can better support teaching and learning of
computer network simulation such as OSPF protocol. To evaluate the DEVS-Suite simulator, graduate students of
computer network course are asked to use it and the ns-2 simulator in three homework assignments. A collection of
questions focusing on both domain-neutral and domain-speciﬁc aspects of the simulators. The students’ evaluations
and our analysis of their answers are expected to show the extent to which each of these tools can support computer
network protocol education. The results of this study should help further development of network simulator tools
may also lead to development of new kinds of simulators for computer networks and possibly other types of complex
systems.
ACKNOWLEDGMENTS
This work has been funded by the Sakarya University Scientiﬁc Research Projects Agency under contract 200705-02-001. The views and conclusions contained in this document are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the
Sakarya University and Arizona State University.
REFERENCES
ACIMS 2010. DEVSJAVA modeling and simulation tool. http://www.acims.arizona.edu/SOFTWARE.
Begg, L., W. Liu, K. Pawlikowski, S. Perera, and H. Sirisena. 2006, February. Survey of simulators of next generation
networks for studying service availability and resilience. Technical report, Department of Computer Science
and Software Engineering University of Canterbury, Christchurch, New Zealand.
Chen, Y., and H. S. Sarjoughian. 2010. A component-based simulator for MIPS32 processors. Transactions of the
Society for Computer Simulation International. In Press.
Chow, A. 1996. Parallel DEVS: A parallel, hierarchical, modular modeling formalism and its distributed simulator.
Transactions of the Society for Computer Simulation International 13 (2): 55–67.
Cowie, J., A. Ogielski, and D. Nicol. 2002. The SSFNet network simulator. http://www.ssfnet.org/homePage.html,
Renesys Corporation.
DEVS 2010. DEVS-Suite. http://sourceforge.net/projects/devs-suitesim/.
Dijkstra, E. 1959. A note on two problems in connexion with graphs. Numerische Mathematik 1:269–271.
Farooq, U., G. Wainer, and B. Balya. 2007. Devs modeling of mobile wireless ad hoc networks. Simulation Modelling
Practice and Theory 15 (3): 285 – 314.
Floyd, S., and V. Paxson. 2001, August. Difﬁculties in simulating the internet. IEEE-ACM Transactions on Networking 9
(4): 392–403. http://www.icir.org/ﬂoyd/papers.html.
Fujimoto, R., W. Lunceford, E. H. Page, and A. Uhrmacher. 2002. Grand challenges for modelling and simulation.
http://www.dagstuhl.de/Reports/02351.pdf.
Guo, J., W. Xiang, and S. Wang. 2007. Reinforce networking theory with opnet simulation. JITE 6:215–226.
Helser, E. 2009. Design and analysis of view syncrhonization in DEVS-Suite. Master’s thesis, Computer Science
and Engineering Department, Arizona State University.
Henderson, T. R., M. Lacage, and G. F. Riley. 2008, August. Network simulations with the ns-3 simulator. In
SIGCOMM. Seattle, Washington, USA.
Kim, S., H. Sarjoughian, and V. Elamvazhuthi. 2009, March. DEVS-Suite: A simulator supporting visual experimentation design and behavior monitoring. In Proceedings of the Spring Simulation Conference, 29–36. San
Diego, CA.

2956

Zengin and Sarjoughian
Lessmann, J., P. Janacik, L. Lachev, and D. Orfanus. 2008. Comparative study of wireless network simulators.
In ICN ’08: Proceedings of the Seventh International Conference on Networking, 517–523. Washington, DC,
USA: IEEE Computer Society.
Likert, R. 1932. A technique for the measurement of attitudes. Archives of Psychology 140:155.
Mather, J. 2003. The devsjava simulation viewer: A modular gui that visualizes the structure and behavior of
hierarchical devs models. Master’s thesis, University of Arizona.
ns 3 2010. The ns-3 network simulator. http://www.nsnam.org/.
OPNET 2010. Opnet simulator. http://www.opnet.com/.
Riley, G. F., R. M. Fujimoto, and M. H. Ammar. 1999. A generic framework for parallelization of network simulations.
In MASCOTS, 128–135.
Robertson, N. H., and T. D. Perera. 2002. Automated data collection for simulation? Simulation Practice and
Theory 9 (6-8): 349–364.
Santoni, A. T., J. F. Santucci, E. De Gentili, and B. Costa. 2008. Discrete event modeling and simulation of wireless
sensor network performance. Simulation 84 (2-3): 103–121.
Sarjoughian, H. 2010. DEVS-Suite WebStart. http://acims1.eas.asu.edu/WebStarts/.
Sarjoughian, H. S., and R. Singh. 2004, April. Building simulation modeling environments using systems theory and
software architecture principles. In Proceedings of the Advanced Simulation Technology Conference, 99–104.
Washington DC.
Simulator, N. 2010. ns-2 network simulator. http://www.isis.edu/nsnam/ns/.
Steenstrup, M. 1995. Routing in communications network. Prentice-Hall.
Varga, A. 2010. The OMNeT++ discrete event simulation system. http://www.omnetpp.org/.
Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory of modeling and simulation. New York: Academic Press.
Zeng, X., R. Bagrodia, and M. Gerla. 1998. Glomosim: A library for the parallel simulation of large scale wireless
networks. In In Proceedings of Parallel and Distributed Simulation Conference, 154.
Zengin, A., and H. Sarjoughian. 2009, July. Teaching and training of network protocols with devs-suite. In International
Symposium on Performance Evaluation of Computer & Telecommunication Systems(SPECTS 2009), Volume 41,
104–111.
Zengin, A., H. S. Sarjoughian, and H. Ekiz. 2004. Honeybee inspired discrete event network modeling. In 16th
European Simulation Symposium, 176–182. Budapest, Hungary.
AUTHOR BIOGRAPHIES
AHMET ZENGIN is Assistant Professor in the Department of Computer Science Engineering, Sakarya University.
Faculty of Technology. He was with the Arizona Center for Integrative Modeling and Simulation (ACIMS) at Arizona
Stte University. His research interests are modeling and simulation of network systems, routing protocol design and
implementation and swarm intelligence applications. His email address is <azengin@sakarya.edu.tr>.
HESSAM SARJOUGHIAN is Associate Professor of Computer Science & Engineering at Arizona State University and Co-Director of the Arizona Center for Integrative Modeling and Simulation. His research focuses
on multi-formalism modeling, co-design, collaborative modeling, and SOC simulation. He can be contacted at
<hss@asu.edu>.

2957

Domain Driven Simulation Modeling for Software Design
Andrew E. Ferayorni
Andrew.Ferayorni@asu.edu

Hessam S. Sarjoughian
Hessam.Sarjoughian@asu.edu

Arizona Center for Integrative Modeling & Simulation
School of Computing & Informatics
Arizona State University
Tempe, AZ 85281-8809

approaches and tools can be used for developing conceptual
and architectural design of software-intensive systems.
Rather than relying entirely on logical and physical system
specifications as in [7] and before entering detailed design
followed by implementation, simulation enables evaluation
of system architecture behavior and identifying major flaws
or shortcomings in a system’s architectural specifications in
the early stages of the analysis and design phases.
Simulation of system architecture produces benefits such as
higher quality architectural specifications and reduced costs.

Keywords: Astronomical observatory, design patterns,
DEVS, domain specific modeling, software design
simulation.
Abstract
System-theoretic modeling and simulation frameworks such
as Object-Oriented Discrete-event System Specification
(OO-DEVS) are commonly used for simulating complex
systems, but they do not account for domain knowledge. In
contrast, Model-Driven Design environments like Rhapsody
support capturing domain-specific software design, but offer
limited support for simulation. In this paper we describe the
use of domain knowledge in empowering simulation
environments to support domain-specific modeling. We
show how software design pattern abstractions extend the
domain-neutral simulation modeling. We applied
Composite, Façade, and Observer patterns to an
astronomical observatory (AO) command and control
system and developed domain-specific simulation models
for the system using DEVSJAVA, a realization of OODEVS. This approach is exemplified with simulation
models developed based on an actual AO system.
1

To achieve software architecture simulation we must have
the ability to model systems hierarchically and model
component behavior. Systems theoretic simulation
approaches give us design capabilities such as composition,
component connectivity, and time dependent state
transitions based on input and output trajectories.
Furthermore, Discrete Event System Specification (DEVS),
a class of systems theoretic models, provides additional
design aspects such as event-based behavior specification
and concurrent execution [10]. DEVSJAVA [1], an objectoriented extension of the DEVS formalism, incorporates
object-oriented concepts into its simulation modeling
capabilities, but by itself does not provide concepts for
domain-specific modeling. In order for DEVS to support
domain-specific modeling we can extend its modeling
capabilities by incorporating design patterns that are
appropriate for a given application domain such as AO
command and control systems.

INTRODUCTION

With the rapid growth in complex software-intensive
systems, we are faced with the need for capabilities that can
reduce their time to market. These systems can benefit
appreciably through the use of simulation for analysis
during various phases of the system/software engineering
lifecycle and especially in the design phase. Simulation
provides a powerful approach for dealing with the large
number of components and interactions that comprise
today’s and tomorrow’s complex systems since it provides
concepts and capabilities that are absent in software analysis
and design methods and tools. Software engineering
methodologies and tools such as Rhapsody [7] are aimed at
developing and validating detailed specifications that are
ready for implementation. In contrast, simulation

SCSC 2007

In this paper we will describe an approach that accounts for
software design patterns in the context of simulation
models. We exemplify how general-purpose discrete-event
simulation modeling can be extended with the Composite,
Façade, and Observer design patters for modeling and
simulating an AO command and control system. We present
the implementation of the models in an extended
DEVSJAVA environment and conclude with a discussion
on the benefits of simulation modeling of software design.

297

ISBN # 1-56555-316-0

2

simulation is not a primary component and thus the
approach is limited to implementation level analysis such as
scalability and verification.

BACKGROUND

2.1 Software Modeling
The use of object-oriented modeling methods and sound
architectural principles (including design patterns) has been
well utilized in the software design realm [8]. Software
modeling
emphasizes
structural
and
behavioral
specifications of executable software. Models describe
conceptual and formal specification of software prior to
implementation and testing activities. For example,
Statecharts serve as a suitable artifact to describe behavioral
blueprint of a system. However, simulating state space of a
(hierarchical) Statechart relies on detailed specifications as
they were to be implemented.

2.2 Simulation Modeling
Simulation modeling is concerned with developing model
descriptions that can be experimented under artificial
settings. Therefore, they need to exhibit dynamics beyond
what could eventually be supported by the real system. A
central feature of simulation is its support for treating time
in logical and/or real-time scales using simulation protocols.
Logical time and real-time time are complementary
concepts with the former supporting artificially slow or fast
passage of time. The importance of manipulating time in
simulation is central to simulation models as compared with
software models.

In recent years software architecture has emerged as a
crucial step in the design process of complex software
systems. The need for software architecture specifications
has brought forth tools and standards for documenting and
analyzing them. We have seen the use of simulation in
conjunction with architecture specification to produce
Executable Architecture Description Languages (EADL)
such as Rapide [5]. It is an event-based, concurrent, objectoriented language specifically designed for prototyping
architectures of distributed systems. In Rapide, the
components of system architecture are defined in terms of
their interface connection architecture - defining their ports,
constraints on those ports, and connectivity with other
components’ ports - and their behavior using a partially
ordered set of events (POSET). EADLs such as Rapide offer
strong support for high level architecture analysis in terms
of system components. However, they are limited in their
ability to support design patterns.

Systems theory provides us with the ability to define a
system in terms of its structure and behavior. The structure
and organization of system components is modeled
hierarchically, whereas the behavior of system components
can be modeled in continuous or discrete time. Components
can be configured with input and output ports, which when
connected to the ports of other components, allow
interactions between them.
Discrete-Event System Specification (DEVS) is a class of
system theoretic models which support the modeling of
hierarchical interacting components that can exhibit
autonomous and reactive based behaviors. System structure
and behavior are captured with atomic and coupled models.
Parallel atomic models allow for multiple ports that can
accept bags of inputs and produce bags of outputs. Parallel
coupled models can consist of any number of atomic and
coupled models but must i) consist of atomic models at the
lowest level of any coupled model; ii) no coupled model can
contain itself; and iii) output to input port coupling resulting
in direct feedback is not allowed for atomic and coupled
models.

At the forefront of software modeling techniques is an
approach known as Model Driven Engineering (MDE) [6].
A key component of emerging MDE technologies is
Domain Specific Modeling Languages (DSML) [2]. The
idea behind DSMLs is their ability to define the
relationships between concepts in a domain and specify key
semantics and constraints associated with those domain
concepts. The languages defined by these meta-models
account for domain knowledge therefore supporting a
declarative approach to modeling design intent. The second
key component used in MDE technologies is model
transformation. These are transformation engines and
generators that analyze aspects of software models in order
to support automated mappings to software implementation
artifacts. These mappings help to ensure functional and QoS
attributes captured in the software models are applied
appropriately during implementation [2]. The use of MDE
technologies
incorporating
DSMLs
and
model
transformation in software design is motivated from the
standpoint of domain driven software modeling and
transition to software implementation. In this regard,

ISBN # 1-56555-316-0

However, the modeling capabilities of systems theory do
not support some important software design techniques. For
example, UML’s classifiers (e.g., Interface) and
relationships among classifiers (e.g., inheritance,
dependency, and realization) are not supported. Without
these kinds of software specification abstractions, it is
difficult to specify simulation models of complex software
designs that include design patterns and thus explicit
support for domain-specific simulation modeling.
2.3 Application Domain
During this research we contacted individuals from different
institutions that were responsible for the design of command
and control software for astronomical observatories (AO).
Braeside Observatory [3], an observatory supported by
Arizona State University’s Department of Physics and

298

SCSC 2007

There are a number of benefits. First, modelers can take
advantage of design patterns to develop domain-specific
simulation models. A simulation environment which has
built-in design patterns, for example, helps simulating rich
dynamics of AO command and control software. Second,
since design patterns are incorporated into simulation model
components, they can support simulating “software
architecture” without first developing UML models which
are close to actual detailed designs. Third, basic differences
between simulation and software models can be bridged in a
logical fashion since high-impact architectural specifications
can be evaluated via simulation instead of delaying them
until detailed design, implementation, and testing phases.

Astronomy, was one of the designs we reviewed. The
approach taken in designing the Braeside system did not
involve any simulation based analysis, nor did it utilize
current software design techniques such as object
orientation or design patterns. The reason these techniques
are not used is primarily due to the desire to reuse legacy
software that has been proven to work well for control of
other observatories. Although simulation did not play a role
in the design of the Braeside system, we have seen its use in
support of a toolbox for AO systems simulation. The VLTI
end to end simulation package [9] provides a logical
framework for developing models of an optical telescope.
This framework supports modeling the physical
components, including control software, that comprise the
observatory system. The primary purpose is to evaluate how
the components of the observatory will work together in an
effort to determine the type of results that can be obtained
using the system. Although the toolbox provides a
framework for system simulation that includes the control
software, it does not attempt to capture architectural details
of the control software for the purpose of design analysis.
3

Customized Simulation Model Components
(Braeside Observatory)

APPROACH

In this work, we introduce design patterns [4] into discreteevent simulation modeling for the AO domain. Instead of
solely using systems theory and object-orientation for
specifying simulation models, we also use design patterns
to capture important traits of common solutions for building
command and control systems for AO. This kind of
simulation modeling provides principled use of design
patterns as applied to this domain. As shown in Figure 1,
design patterns specifically suitable for the AO application
domain can be added to the DEVSJAVA simulation
modeling environment. We have extended the OO-DEVS
with a select set of design patterns to enable specifying and
simulating domain-specific simulation model components.
The result of the extended OO-DEVS is a collection of
simulation model components where relationships among
them include patterns of interaction and dependency beyond
whole-part and is-a relationships which have formal
underpinnings in OO-DEVS (see Section 4 for the details of
design patterns used in AO domain). For the AO domain we
have developed DEVSJAVA-AO which extends domainneutral DEVSJAVA structural and behavioral modeling
constructs with the AO dynamics. The result is a domainspecific simulation environment which can be used to
develop specialized simulation models and evaluate
alternative AO system architectural and design
specifications. Specifics and detailed models of the
DEVSJAVA-AO are described in Section 5.

Design Patterns

Object Orientation

Systems Theory

Modeling
Engine

Simulation
Engine

General Purpose (DEVSJAVA)

Domain-Specific Component-based
Modeling & Simulation Environment
(DEVSJAVA-AO)

Figure 1: A conceptual view of simulation model
components for an AO command and control system.
Consequently this approach can help with reuse of
“solution” simulation models for developing software
design models which in turn should lead to improved time
to market and increased quality of the end software/system
product. The main benefit of design patterns, therefore, is
the ability to create simulatable software architectures and
designs.
4

SIMULATION
PATTERNS

MODELING

WITH

DESIGN

In the previous sections we discussed how the use of
system-theoretic and object-oriented approaches supports
modeling of complex distributed software systems. We also
looked at the benefits of considering system software design
goals when we are developing the simulation model design.
In this section we will show in detail how to develop an

This approach allows developing prominent features of an
application domain on top of the general-purpose
capabilities of a modeling and simulation environment.

SCSC 2007

Domain-Specific Simulation Model Components
(Astronomical Observatory)

299

ISBN # 1-56555-316-0

extension of the simulation environment with design
patterns specific to a domain.

controller would specify the method signatures that when
realized can be used for interacting with the detector subsystem. The use of interfaces in design is powerful because
it allows us to show what functionality is expected
(syntheses, interaction, and collaborations) but not how that
functionality will be achieved. The method signatures for
each operation of an interface will specify what inputs are to
be given and what outputs are expected. The details of how
interface operations will be implemented are left to the
models that realize them.

4.1 Astronomical Observatory Systems
An Astronomical Observatory (AO) command and control
system is complex and well suited for examining the pattern
based simulation model design methodology. Typically
these systems consist of software components for
controlling features of the telescope, detector, and mount.
Each of these will perform the functions necessary to
interface with the user and the physical system components.
The mount control software will be responsible for
processing requests to point the telescope to specific
coordinates as well as tracking those coordinates over time.
The telescope control software will be responsible for
handling requests to control telescope accessories, such as
adjusting the position of a focuser. Finally, the detector
control software will execute requests for taking images and
downloading image data. Communication between software
components is necessary to coordinate certain system
functions.

The concept of interfaces maps nicely into our first design
pattern for the AO domain. The Façade design pattern
provides a unified interface to a set of sub-interfaces. AO
control systems are comprised of many sub-components that
work together to complete a user request. Once we have
identified interfaces that capture the services provided to the
system users, we can combine all or part of those interfaces
to create a Façade. This higher level interface will hide the
details of how sub-system components are used to execute
the request. In addition, changes to how the sub-system
components carry out the request can be made without
impacting the user of the Façade. This is an important
pattern for the AO domain because instrumentation is
frequently upgraded to stay atop research needs and
evolving technologies.

4.2 Domain Analysis
Identifying domain-specific design patterns for system
software requires domain expert knowledge. This
knowledge helps us understand what is common among
systems in the domain in terms of the structure and
interaction of their components. Since there are many
different ways to architect a system we cannot attempt to
identify patterns that will work for all systems in a domain.
Instead we focus on those patterns that can be applied to
most systems in the domain. In the next few sections we
will discuss different approaches to analyzing the domain
and identifying these patterns.

In
Figure
3
we
show
the
use
of
an
DetectorControllerInterface (DCI) in support of the
Façade pattern between an ObservatoryClient (OC) and
the AbstractDetectorController (ADC). This component
of the system may be implemented with one software
component (as shown in this example), or with coordination
of many sub-components. Therefore applying the Façade
pattern allows us to hide the details of how the interface
methods are actually carried out.

4.3 System Functionality

4.4 System Structure

An excellent approach to collecting requirements for the
functionality of a system can be achieved through
generation of use case diagrams. Use case diagrams allow
us to capture the scenarios (use cases) under which the
system will be used from the perspective of its users
(actors). Each use case that is generated represents a goal
that the user wants to achieve with the system. These goals
help model the requirements of the system by specifying
what is expected of it. However, these requirements should
not specify the details of how the system will achieve these
goals.

When we look to identify design patterns we must consider
the structure and interaction of components in the system.
Through the use of domain knowledge we can study these
systems and look for patterns in how they can be modeled.
Patterns that are more common in the domain will become
the focus of the design pattern selection.
When we analyze the structure of the system components
we have to look for common patterns in how they are
organized horizontally and vertically. Horizontally we look
to identify the major nodes of the software architecture that
will consist of one or more components working together to
produce some behavior. These nodes often separate major
functionalities of the system that generally operate
independently. Some interaction between nodes may be
necessary but is usually kept to a minimum. Beneath these
nodes is where we look for vertical organization patterns,

Once we have developed a strong set of use cases for our
system we can easily see the high level functions expected
of it. This information naturally maps to another objectoriented concept known as interfaces. With interfaces we
can map the expected system functions into collections of
operations that can be realized by models in our
implementation. For example, an interface for a detector

ISBN # 1-56555-316-0

300

SCSC 2007

where emphasis is placed on how the patterns will support
modification to adapt to different modeling configurations.

Subject-Observer. This pattern is important for the AO
domain because subject component state changes are often
shared with many observing components that may vary
from one system configuration to another. For example,
when the detector is taking images the mount will need to
block any incoming slew requests from the user. Similarly
when the detector is finished taking an image the mount will
need to unblock. This common coordination between the
control software of the mount and detector can be managed
via the observer pattern. Furthermore, a new system
configuration may introduce a second detector that also
needs to be observed by the mount. In this case the pattern
supports the client subscribing the new observer to the
subject through well defined interfaces. Figure 3 shows how
the observer pattern interfaces can be used by two atomic
components representing the mount and detector control
software.

Another driving force in the structure of a system is how its
components interact. This involves understanding how
control and data will flow in and out of each component as
well as what other components it flows to and from. Areas
where various configurations are desirable and generic
configurations cannot be achieved will be considered when
designing for integrability.
Analysis of the AO domain in terms of component structure
and interaction revealed two significant design patterns.
The first is the composite design pattern, which allows for
the composition of objects into structures that represent
part-whole hierarchies. This pattern can be used in the AO
domain to support the use of common I/O channels for
components communicating within a tree structure. For
example, all software components that comprise the mount
controller module can provide a common I/O channel
structure such as command input, command output, data
input, and data output. This allows simplified identification
of where command and data information can be obtained
and delivered between components of the same tree.

5

Implementation of our simulation models will require an
environment that supports object orientation and provides
the ability for extension of the core components with design
patterns. Commercial Off The Shelf simulation packages
generally do not allow access to core components of the
environment, and therefore are not well suited for extension
with design patterns. Simulation packages such as
DEVSJAVA support modeling using object orientation and
also allow for extension of core environment components.
For our implementation we extended the DEVSJAVA
environment with our AO domain design patterns, and
utilized these patterns to implement models of a simple
observatory control system. In the following sections we
will look at implementation details and discuss challenges
faced. We will also present simulation results obtained from
an experiments conducted with fully implemented models.

The composite pattern is naturally supported by the
component modeling capabilities of systems-theory and
DEVS. The pattern can be specialized for the AO domain to
support command and data I/O channels by using two
classes,
CompositeControlElement
(CCE)
and
PrimitiveControlElement (PCE) as shown in Figure 2.
These classes support the composite design pattern in its
ability to be a parent or leaf node in a hierarchical modeling
structure. These classes also extend the DEVSJAVA
ViewableDigraph class, which extends DEVS coupled
component modeling with visualization abilities.
ViewableAtomic

ViewableDigraph

5.1 DEVSJAVA-AO Implementation

(from DEVSJAVA)

(from DEVSJAVA)

The three major software components of an AO control
system are those controlling the mount, telescope
accessories, and detector instruments. Using the composite
design pattern we can define three observatory nodes. They
are
AbstractMountControllerNode
(AMCN),
AbstractTelescopeControllerNode
(ATCN),
and
AbstractDectectorControllerNode (ADCN). Each of
these is an abstract class extending the CCE abstract base
class from Figure 2, thus inheriting its generic I/O port
structure. These nodes serve as parent nodes in the
composite structure, thus allowing any number of additional
parent and child nodes to reside below them. In addition,
they can be specialized to capture details of a specific type
of configuration that is needed. For example, the control
system for Braeside Observatory has a single software
component for controlling a CCD Camera detector. We can
represent this in DEVSJAVA-AO by specializing the

0..n
CompositeControlElement
inData : Port
inCommand : Port
outData : Port
outCommand : Port

Primitiv eControlElement

1

1..n

inData : Port
inCommand : Port
outData : Port
outCommand : Port

1
Self -containment not
supported by DEVS

Figure 2: Composite design pattern for AO domain.
The second design pattern identified is the observer pattern
which allows for components (the observers) to be notified
when the state of other components (the subjects) change.
This pattern is also referred to as Publish-Subscribe or

SCSC 2007

SIMULATION MODEL IMPLEMENTATION

301

ISBN # 1-56555-316-0

ADCN with SingleDetectorControllerNode (SDCN).
This specialized class will ensure that only one detector
controller (coupled or atomic) can exist within the node.

ForkMountController (FMC) class shows how we can
further specialize the AMC class with details specific to how
a controller for a fork type mount would implement the
interface methods.

Figure 3 shows our implementation of simulation models
for a simple AO control system. In this system the node
level abstract classes are specialized using concrete classes
that allow each to have a single controller. Each controller
node can therefore contain one atomic model (controller)
realizing all the associated Façade interface methods. For
example, the SMCN consists of one atomic model named
AbstractMountController (AMC) that contains methods
realizing the MountControllerInterface (MCI). Under a
different observatory configuration, we may have modeled
the mount controller using multiple components. Because
the ObservatoryClient uses the MCI Façade it would not
be impacted by the change in the number of mount control
components
supporting
that
interface.
The

The observer pattern is utilized for state change notification
between the mount and detector. In our simple system we
have chosen to have a single controller for the detector,
represented by the AbstractDetectorController (ADC)
atomic model. This class can act as the subject by inheriting
from the AbstractDetectorController_Subject (ADC_S),
thus providing subscribed observers access to its state. The
mount controller is also represented with a single atomic
model class named AbstractMountController (AMC),
which can be setup as an observer of the detector controller
because
it
realizes
the
AbstractDetectorController_Observer
(ADC_O)
interface.

ObservatoryControlSystem
AbstractMountControllerNode
(from Mount)

1

1

1

OCS

1

1

AMCN

AbstractTelescopeControllerNode
1

(from Telescope)

AbstractDetectorControllerNode

ATCN

(from Detector)

SingleMountControllerNode

ADCN

<<creates>>

SingleTelescopeControllerNode

(from Mount)

(from Telescope)

SingleDetectorControllerNode

SMCN

(from Detector)

1

STCN

detectorController : AbstractDetectorController

SDCN
AbstractDetectorController_Observer

1
AbstractDetectorController_Subject

<<uses>>

n

1

1

(from Mount)

(from Detector)

observers : Array
update(subject_in : AbstractDetectorController_Subject) : void
attach(observer : AbstractDetectorController_Observer) : void
dettach(observer : AbstractDetectorController_Observer) : void
notify() : void

ADC_O
<<realize>>

ADC_S

1

1

AbstractMountController
(from Mount)

<<realize>>

AbstractDetectorController

observerState : String

<<realize>>

(from Telescope)

(from Detector)

subjectState : String

MountControllerInterface

AMC

ATC

(from Mount)

DetectorControllerInterface

(from Mount)

FMC

getState() : String

(from Detector)

startMoveRAWest()
startMoveRAEast()
startMoveDECNorth()
startNoveDECSouth()
stopMoveRAWest()
stopMoveRAEast()
stopMoveDECNorth()
stopMoveDECSouth()

ForkMountController

<<realize>>

startExposure()
stopExposure()
setBinningType()
getBinningType()
downloadExposure()

CCDCameraController
FocuserController

(from Detector)

(from Telescope)

CCDCC

DCI

FC

<<uses>>

MCI

AbstractTelescopeController

TelescopeControllerInterface

<<uses>>

(from Telescope)

<<uses>>
ObservatoryClient

TCI

OC

startMoveFocusIn()
stopMoveFocusIn()
startMoveFocusOut()
stopMoveFocusOut()

Figure 3: Simulation models for simple AO control system.

ISBN # 1-56555-316-0

302

SCSC 2007

camera detector will be used to capture data from the visible
light spectrum over time while a spectrograph detector will
be used to record the individual spectrums of the light. As
we discussed before, the mount controller will subscribe to
state changes in the detectors via the observer pattern.
Therefore, when adding a new detector to the system we
immediately see how well the pattern supports a new
configuration. In addition, we can run simulations to see
how the state changes of the new detector controller impact
the behavior of the mount controller.

Communication between components in the simulation is
done via message passing. To simplify this we have defined
the ObsMessage class for presenting all the required
message data. This class provides member variables and
methods for storing and retrieving the specifics of the
message. The benefit to having a common message format
is reduced complexity in creating and processing messages
throughout the system.
5.2 Simulation Using DEVJAVA-AO
The DEVSJAVA-AO environment provides us with the
base classes necessary to build atomic and coupled models
for a simple AO control system. In addition, it provides
classes for basic block diagram visualization of simulation
executions. In Figure 4 below we show the DEVSJAVAAO simulation view for our implementation. The
ObservatoryControlSystem (OCS) coupled model serves
as the top level node and contains all models in the AO
control system implementation. The OC model, which is
also specified in the DEVSJAVA-AO environment,
stimulates the OCS by sending ObsMessage type
messages as input commands over time trajectories. Output
data and commands are gathered during many time periods.
These simulation results are evaluated to choose suitable
command and control designs under a range of operational
settings.

Another aspect of software that we can test is the behavior
of the system when differing algorithms are used to perform
certain system functions. For example, when an observation
request is received the system must determine if the request
is valid. There are two algorithms that can be used to
determine whether to accept or reject the request:
1.

Check only if the object is currently in view

With this algorithm the mount software will simply
check that the coordinates of the observation request are
currently in the area of the celestial sphere that the
telescope can be pointed to. If they are, the request is
accepted and the observation begins. If they are not,
then the request is rejected.
2.

Check if the object is in view now and that it will
be in view for the length of the observation

This algorithm is more advanced than the first in that it
does one additional check. It will not only ensure that
the coordinates are obtainable now, but also that they
are available for the entire length of the exposure. This
means that the object we are tracking will no go below
the horizon before the exposure has concluded.
To conduct the experiment we create two versions of the
ForkMount atomic model, one that implements the first
algorithm and another that implements the second. To see
the impact this algorithm change has on the system we
conduct simulations in which observation requests
generated by the experimental frame are created by
selecting random coordinates and a random exposure time.
The exposure time is limited by a maximum exposure time
(MET) value that we increase by two hours with each
simulation run (i.e. 2, 4, 6, 8, and 12). A successful
observation request is one in which the coordinates of the
object being imaged can be tracked by the telescope from
start to finish of the exposure. This scenario is considered to
return the desired image. A failed observation request
results if the exposure is cut short because the object goes
below the horizon. This scenario is considered to return an
erroneous image.

Figure 4: Astronomical observatory simulation models.
5.3 Simulation Results
Simulation provides us with the ability to run experiments
on a system and learn about its behavior under certain
conditions. Our models of a simple AO control system can
be simulated in various experiments to measure many
aspects of the system such as correctness, performance, and
“what if” scenarios. Extending the DEVSJAVA
environment with the DESVJAVA-AO library enforces use
of domain specific design patterns in our simulation
modeling, but it should not impair our ability to perform
DEVS simulations.
One interesting aspect of software architecture that we can
test is how addition of a new module will impact the rest of
the system. For example, one observatory configuration may
require two detectors to be used instead of one. A CCD

SCSC 2007

The same set of observation requests are fed as input to the
models for each MET, once for algorithm 1 and a second
time for algorithm 2. This will allow us to see how many

303

ISBN # 1-56555-316-0

plays a significant role in creating software design that can
be simulated prior to detailed software design specification,
with a key benefit being reduction in the overall software
development effort. A future direction for this research is
applying the simulation models for developing software
controlling an astronomical observatory. Another future
research opportunity involves forward engineering from
simulation models to software models. A related area of
interest is the inclusion of design patterns in real-time
simulation modeling.

successful observation requests occur given the chosen
decision algorithm and allowed MET. The first chart in
Figure 5 shows the percentage of observations that were
erroneous as the MET was increased. Since algorithm 1
only checks that the object is currently in view when the
request is received, we start to see more erroneous
exposures as the MET increases. The second chart in Figure
5 shows the percentage increase in the number of
observations that were successfully carried out using
algorithm 2 versus algorithm 1. Here we can see that as the
MET increases, we start to see more benefit in using
algorithm 2. This is due to an increase in the number of long
exposure requests that are received as the MET is increased.
Overall the results show us that the second algorithm is a
better choice because it eliminates requests that will be
erroneous because the object is going out of view before the
end of the exposure.

7

[1] ACIMS, DEVSJAVA, http://www.acims.arizona.edu,
2006.
[2] Balasubramanian, K., A Gokhale, G. Karsai, J.
Sztipanovits,
S.
Neema,
2006,
“Developing
Applications
Using
Model-Driven
Design
Environments”, IEEE Computer, Vol. 39, No. 2, pp.
33-40.

Current View Only Algorithm :
Percentage of Erroneous Observations vs. Max Exposure Time
Percentage of Erroneous
Observations

References

80.00
60.00

[3] Braeside Observatory, Arizona
http://braeside.la.asu.edu, 2005.

40.00
20.00

2

4

6

8

10

12

University,

[4] Gamma, E., R. Helm, R. Johnson, J. Vlissides, 1995,
Design Patterns: Elements of Reusable Object-Oriented
Software, Addison-Wesley.

0.00
0

State

14

Max Exposure Time

[5] PAVG, Rapide, http://pavg.stanford.edu/rapide/, 1998.
[6] Schmidt, D., 2006, “Model-Driven Engineering”, IEEE
Computer, Vol. 39, No. 2, pp. 25-31.

Percentage Increase in
Successful Observations
with Algorithm2 over
Algorithm1

Curre nt View and End of Expos ure Algorithm :
Pe rce nt Increas e in Success full Obse rvations w ith Algorithm 2 over Algorithm 1
vs
M ax Exposure Tim e

[7] Telelogic, Rhapsody, http://modeling.telelogic.com/
modeling/products/rhapsody, 2007.

700
600
500
400
300
200
100
0

[8] UML, http://www.omg.org/cgi-bin/doc?formal/05-0704/, 2006.
0

2

4

6

8

10

[9] Wilhelm R.C., B. Koehler, 2000, “Modular toolbox for
dynamic simulation of astronomical telescopes and its
application to the VLTI”, SPIE, Vol. 4006, pp. 124135.

12

M ax Exposure Tim e

Figure 5: Simulation results using two alternative control
algorithms.
6

[10] Zeigler, B.P.; H. Praehofer; T.G. Kim. 2000. Theory of
Modeling and Simulation, 2nd Ed. Academic Press.

Conclusion

The motivation behind this work is Simulation Based
Acquisition (SBA) which promotes systematic use of
simulation across lifecycle of systems from conception to
retirement. In this respect, the presented approach focuses
on supporting simulation-based software design. This work
shows the use of design patterns in support of command and
control paradigm for the software development of
astronomical
observatory
control
systems
using
DEVSJAVA-AO. The inclusion of design patterns in a
modeling and simulation environment for specific domains

ISBN # 1-56555-316-0

304

SCSC 2007

Proceedings of the 2007 Winter Simulation Conference
S. G. Henderson, B. Biller, M.-H. Hsieh, J. Shortle, J. D. Tew, and R. R. Barton, eds.

APPLICATION OF COMBINED DISCRETE-EVENT SIMULATION AND OPTIMIZATION MODELS IN
SEMICONDUCTOR ENTERPRISE MANUFACTURING SYSTEMS
Gary Godding
Hessam Sarjoughian

Karl Kempf

Arizona Center for Integrative Modeling & Simulation
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85281-8809, U.S.A.

Corporate Planning and Logistics Group
Intel Corporation
Chandler AZ, 85226, U.S.A

and executed together through the KIB to reveal dynamics
of the supply-chain system parts and their interactions.
The previous models of multi-formalism semiconductor supply-chain systems have been developed in laboratory settings with reduced scale and scope. They were
shown to exhibit behavior consistent with the real-world
systems by carrying out a suite of experiments. Although
these integrated models have been crucial for demonstrating the viability of KIB for semiconductor supply-chain
networks, their use in industry had not been undertaken
previously.
In this paper we describe a simulation/optimization
testbed developed to enable the specification and testing of
a production MPC model. We show the application of the
systematic integration of DEVS and MPC models using a
KIB model has resulted in an environment that enabled the
specification of a MPC suitable for operation in an actual
semiconductor supply-chain system. The resulting MPC
configuration was deployed in an industrial scale pilot
study to control actual material flow over a period of several months. We conclude with an analysis of the role of
KIB in a real world operational setting, the lessons learned,
and future work.

ABSTRACT
It is a common practice to use simulation for validating different types of control and planning algorithms. However,
the science of how to rigorously integrate simulation and
decision models is not well understood and becomes critically important as the complexity and scale of these models increase. In our research, we have developed a methodology for integrating different types of models using a
Knowledge Interchange Broker (KIB). In this paper we describe a supply-chain semiconductor application where the
KIB has been used as an integral part of developing and
deploying a commercial Model Predictive Control model
for use in operating a multi-billion dollar supply chain. The
simulation based experiments facilitated developing and
validating the controller design and data automation for a
real-world semiconductor manufacturing system.
1

INTRODUCTION

The mounting complexity and scale of semiconductor
manufacturing supply-chain systems have demanded advances in contemporary modeling and simulation approaches, tools, and practices. A key concept for handling
complexity of discrete-part supply-chain systems is to partition them in ways to allow modeling each separately. A
particular requirement of operating these types of supplychain systems is to account for interactions among manufacturing and decision systems.
To achieve optimal inventory of parts, efficient processing of manufacturing units and delivery of products to
their destinations in a cost-effective manner, a variety of
process and planning models are needed (Kempf 2004). A
class of semiconductor supply-chain system models have
been developed using Discrete Event System Specification
(DEVS)(Zeigler, Praehofer, and Kim 2000), Model Predictive Control (MPC) (Qin and Badgwell 2003), and Knowledge Interchange Broker (KIB)(Sarjoughian 2006). These
models described in multiple formalisms can be composed

1-4244-1306-0/07/$25.00 ©2007 IEEE

2

BACKGROUND

Simulation and decision science are two distinct disciplines
with many facets of ongoing research across different application domains. In the domain of semiconductor manufacturing, a variety of approaches have been devised to integrate discrete event simulation and optimization models.
Among existing works, the most common approach is to
implement ad-hoc interfaces to allow models written in different programming languages to exchange input and output via custom software.
Unlike those ad-hoc techniques, an approach has been
developed where disparate model specifications using different execution algorithms can be rigorously composed
using the Knowledge Interchange Broker (KIB). The con-

1729

Godding, Sarjoughian, and Kempf
create data in discrete batches of lots over small time intervals (hourly asynchronous), whereas the decision models
generally needs an aggregated view of the data over longer
intervals (days/weeks) of time. Third, coordination of execution between the two models must be addressed. We
must consider when to run the control model in respect to
the simulation and vise versa. This requires the synchronization of solver runs and simulation events.
The above requirements suggests that the integration
methodology must enable the mapping of different types of
data structures, provide aggregation/disaggregation data
transformation capabilities, and enable a flexible synchronization capability. These are the capabilities afforded by
the KIB approach and exemplified below.

cept behind this approach is to match model semantics and
enable execution interoperability through the KIB (Figure
1). This concept has been applied for the class of Discrete
Event System Specification (DEVS) integrated with Reactive Action Packages (Sarjoughian and Huang 2005), Linear Optimization (Godding, Sarjoughian, and Kempf
2004), and more recently Model Predictive Control (MPC)
(Huang and Sarjoughian 2006).
In the case of DEVS and MPC, the Knowledge Interchange Broker accounts for specificities of the DEVS and
MPC structural specification and dynamical behaviors. The
KIBDEVS/MPC model specification accounts for combining
the DEVS and MPC models and thus ensures the correctness of their integrated structures and behaviors. The
KIBDEVS/MPC execution algorithm accounts for the combined execution of the DEVS simulator and the MPC
solver in such a way that it can correctly execute the DEVS
and MPC model specifications. Separating model composability and execution interoperability is found to be key for
modeling of complex interactions among models that are
described in disparate modeling formalisms. The interactions account for simple and complex data transformations
and synchronous execution of DEVS and MPC models.

3.1

Figure 2 shows a semiconductor manufacturing topology
consisting of two fabrication factories, two assembly warehouses, and two semiconductor assembly test (AT) sites
being controlled by a wafer shipping decision system. The
factories can ship their products to the two assembly warehouses. Material from the warehouses can be released into
semiconductor assembly test. When and how much product
being released from the assembly warehouses is determined by starts schedules from the associated assembly
test site.

model composability

Model
Specification

Model
Specification

Example Problem

Model
Specification

Wafer Shipping Decision System
What warehouse to send products from the factories
Control
Manufacturing

Execution
Algorithm

Execution
Algorithm

Execution
Algorithm

DEVS

KIBDEVS/MPC

MPC

Fabrication
Plant 1

Assembly
Warehouse 1

AT1

Ship 2
Ship 3
Fabrication
Plant 2

execution interoperability

Instructions
State
Material Flow

Figure 1: Integration of DEVS and MPC models with KIB
3

Ship 1

PROBLEM DESCRIPTION

Ship 4

Assembly
Warehouse 2

Factories
Warehouses

AT2

Shipping
Assembly

Figure 2 – Example Problem

The MPC must be tested before it can be put into production. This is to assure correct operation and avoid unplanned disruption to the business. We have developed detailed and realistic simulations of the manufacturing lines.
How to integrate these DES models with production controllers is the challenge.
First, the I/O for the two models is very different. The
control model requires an initial state to be populated into
one set of variables and the results to be read from another
whereas the DES reads and writes data via event messages.
Second, the granularity of data sent between the models is
generally very different. In semiconductor planning /
manufacturing types of problems, manufacturing systems

A controller is connected to make decisions on routing
material leaving the fabrication factories. The objective of
the controller is to keep the warehouse inventory within
upper and lower control limits. Input states to the controller
are product that was shipped, warehouse inventory levels,
forecasted builds of the fabrication plants, and forecasted
starts from the assembly test sites. The output of the controller are commands that dictate the quantity of each
product to be shipped from a given fabrication factory to
the assembly test warehouses.
This example problem has several sources of stochastic behavior. The fabrication and assembly processes have

1730

Godding, Sarjoughian, and Kempf
fined in terms of collections of lots. The lot has a unique
name and a quantity of one type of product. A structure for
a lot is defined as:

stochasticity in the throughput time and yield. This introduces error in the forecasted supply and starts schedules
seen by the controller. The shipping has variability in transit times to different world geographies. This causes stochastic arrival times of material to the assembly warehouses. And finally, the boxes of lots leaving the
fabrication plant are variable in size due to yield differences. Lot sizes are not adjusted prior to shipping so the
quantity of material shipped to a material warehouse may
be different than what was requested by the controller. For
example, the controller may instruct 200 units be sent,
however, if there are 3 lots of quantity 90, all 3 lots would
be sent resulting in an actual ship quantity of 270. The
third lot would not be split to match the actual requested
quantity.
The material in the manufacturing system exhibit forward flow, however their dynamics are implicitly controlled through feedback effects via the controller (decision
system). The decision system uses feedback from historical
events and current state to generate instruction on what to
do in the near future. Combination of feed-forward and
feedback flows creates complex dynamics in the individual
manufacturing and decision systems and across the supplychain system.
The types of studies we want to carry out are to find
the right data feeds, data granularity, and control frequency
to effectively keep the warehouse levels within desired
limits. This implies the simulation environment needs to
support experimentation with data interfaces to the controller at different synchronization frequencies. Two types of
questions we could answer would be the granularity of
forecast data (e.g. weekly and daily) and how often do we
need to rerun the control (daily, shiftly, or hourly).
Using our multi-formalism modeling approach, this
problem is separated into three different models. The controller is modeled and implemented using MPC. The
manufacturing topology is modeled using DEVS. The data
and control integration is modeled using the KIB.
3.2

Lot Structure
Name: Unique identifier
ProductName: String
Quantity: Integer

The data for the supply forecast is specified as a vector
of material currently in the factory. The factory can be divided into a number of ‘buckets’. For example, the factory
could be divided into two buckets, material in the front and
back halves of the factory. The controller can use this vector to predict supply.
The demand forecast is a vector giving the controller a
view into the future orders. This vector is specified in time
units. In the real world this vector would be supplied by the
assembly test sites as future starts schedules. In the simulation it is generated from a distribution.
3.2.2 Controller Integration Data
Assume the controller outputs a matrix of quantities for
each factory that specifies how much to release of each
product and where it should be shipped. For the model
shown in Figure 2 there would be two output matrices, one
for each factory.
An example of how this matrix could look is shown
below (1). The matrix is a 3 u n vector that specifies a
product number, a destination warehouse number, and a
quantity where n is the number of product and destination
combinations.
Product
Destination
Quantity

ª P1 ... Pn º
« D ... D »
n»
« 1
«¬Q1 ... Q n »¼

(1)

The product number is mapped to one of the products
built by the factory. The destination number specifies
which warehouse the material should be shipped to, and
the quantity specifies how much.

Integration Example

We will work through an integration example based on the
model in Figure 2 using MPC and DEVS. The MPC will
send instructions to the DEVS simulation based on state
values it has received from the simulation. The integration
requires modeling of instructions going to the fabrication
plants and all required state messages back to the MPC.
How to coordinate the execution of the MPC and DEVS
simulation also needs to be modeled.

3.2.3 Data Transforms
The control model requires data to be input in terms of
units. For some types of data such as factory shipments, the
quantity of product that leaves must be aggregated over a
controller interval. For example, if the controller interval is
one day, the quantities from all lots that left the factory in
the previous day would be aggregated into a single value
for input into the controller on the start of the current day.
The value is calculated as shown in equation (2):

3.2.1 Manufacturing Integration Data
The fabrication plant processes material in batches or lots.
Each lot has a quantity and product name. The output
states for factory ships and warehouse inventory are de-

1731

Godding, Sarjoughian, and Kempf

f ( p, d )

¦ lot

lot  LotsOut

quantity

this event is for, what destination warehouse the product
should be shipped, and the quantity to ship.
To accomplish the mapping and transform we need to
first consider which entity the controller output matrix is
for. Since the name of the matrix is Factory1Ships, we
can assume it is for factory1. This must be explicitly
mapped in the KIB integration model. The controller output matrices with name Factory1Ships will be mapped to
DES events going to the release input port for factory one.
Next, each column of the matrix needs to be transformed
into simulation input events. Each matrix element [column
i, row j] is a positive scalar value (e.g., Factory1Ships
[0,2]=1000). We will work through the first column in matrix (4). This column shows that 1000 units of product 1
should be sent to warehouse 1. Lets assume that both the
controller and simulation use the same number scheme for
products and destinations. Since the controller is running
once a day and the simulator running hourly, we will need
to generate 24 simulation events for each hour of the simulation on that day. The quantity will need to be transformed
using equation (3). The transformed quantity value for the
first column in (4) would be 1000 * 1/24. The simulation
data event values in the 24 generated events would be:
data(1,1,1000/24). The mapping between structures would
look like:

(2)

p , d ,t

where p  products, d  destinations, previousControlTime < t  currentControlTime.
If the controller interval is daily and the manufacturing
model runs hourly, the controller instructions need to be
disaggregated. For the mapping of the controller release to
the simulation, lets assume that the value needs to be divided equally over each of the simulation time intervals.
For example, if the simulation is running at an hourly
granularity and the controller is generating instructions
once a day, the controller instruction would be divided by
24. In general, the disaggregation could be more complex,
but for illustration this simplified algorithm will be used.
The equation for the equally divided disaggregation is:
§ cf ·
(3)
cr p ,d ,q 
 ¨¨ ¸¸
© sf ¹
where g(p,d) is factory release quantity for product p going
to destination d, cr is controller release quantity, cf is controller frequency, and sf is simulation frequency.
g ( p, d )

3.2.4 Mapping between Vectors and Events
We must now consider how the data and control will transferred between the two formalisms. For the discrete event
simulation we must read and write events to a running
simulation. For the controller, we must populate input variables, initiate a solver run, and then read the output variables.
Suppose the simulation is running at hourly granularity and the controller is generating instructions once a day
using the format shown in (1). Next lets assume the controller outputs a matrix of quantities specifying how much
to release of each product and where it should be shipped
for factory 1 as shown below:

1
2 º
ª 1
«
2
1 »»
Factory1Ships =
« 1
«¬1000 1000 500»¼

ReleaseEvent(FactoryOneReleasePort,
Data(Factory1Ships[0,0],
Factory1Ships[1,0],
g(Factory1Ships[0,0],
Fatory1Ships(1,0])))

Pictorially, the mapping of data between the two models is shown in Figure 3. Read and writes of the data are
happening between the simulation event structures and the
controller matrices across different time scales. In addition,
the data is being transformed to match the semantics of the
target model.

Interval 1

(4)

st

ct

Simulation
Events

f(p,d)

The controller output instruction needs to be mapped to
simulation input events. The simulation input events have
the following structure:

g(p,d)
Interval2
MPC
Controller

ReleaseEvent(
SimulationPort,
Data(product, destination, quantity))

ct + 1

st + 24

Days

Hours

Discrete Event
Simulation

Figure 3. Transforms over Different Time Scales

The simulation port specifies where the event should
be directed to. The data has three elements, what product

1732

Godding, Sarjoughian, and Kempf
3.2.5 Data and Time Synchronization

4.1

For the experiments, we require that the models can be
configured to run in multiples from the other. For example,
we could have the simulation run every hour and have the
controller to run every simulation interval, or it could run
once every 8 hours, 12 hours, etc…
When we change the synchronization frequency it will
impact when to send the data between the models and the
aggregation/disaggregation data transformations. For example, when running the controller daily and simulation
hourly, the quantity of die in all the lots that have left the
factory in the last 24 hours on each control cycle need to be
added. If we were to change the frequency to control once
a shift (8 hours), then we would only need to sum the die
over the last shift. Conversely for the factory release commands, in the daily control cycle, 24 events would need to
be sent. In the shiftly control scenario, only 8 events sent
per control cycle.

The KIB must support both the mappings of fields within
data structures and transformation of the data. The data in
an array entry may need to be mapped to a record field, but
the array data may also may need to be transformed to a
different abstraction.
The general requirements for mapping between the
source and target transforms in our supply network problem are:

4

x

x

x
x

KIB APPROACH

To integrate the models we use a Knowledge Interchange
Broker (KIB). The KIB provides a methodological way to
integrate multi-formalism models. The KIB enables the
modeling of data transforms, mapping of data elements,
and the specification of control. Figure 4 shows the conceptual view of a KIB model applied to the model shown
in Figure 2.
Predictive
Models

Variable Data

Event Data

Supply
Variables

F2 Supply

f1

Fabrication
F1

g1
Ships
Variables

4.1.1 Mappings
This section lists the data mappings that had to be provided
by the KIB configuration modeling language.
UnorderedSetToArray: This mapping copies an unordered set of tuples to an ordered array. One of the data
fields in the set has to be specified as an index field. The
transform uses the index data values to do the ordering.
ArrayToUnorderedSet: An array of values is copied
to a set of unordered tuples. The array index value can be
copied to a set tuple field. If the array is part of structure,
the other fields of the structure will be copied to one of the
set tuple fields. For example, if you have a structure that
contains field for product name and another field that contains an array of n values, a set can be created with n tuples
where each tuple contains the product name, one of the array values, and the index for that array value. The starting
value for the index can be specified.
ArrayValueToVariable: This mapping copies a specific array value to a single variable. The array index needs
to be specified. Also, if the array is part of a structure, a
key field with its matching data value can be specified. An
example is would be a structure that contains a product
name and an array of values. You could specify that array
entry 3 of productX be written to this variable.
VariableToArrayValue: This mapping copies a variable value to a specific array entry. An index number needs

Fabrication
F2

F1 Supply

Inventory
Control
Limits

Ship 3

Ship 2

Ships 2

Ship 4

Ships 3
Ships 4

Inventory
Control
Variables

f3

Orders
Variables

f4

Solver
Coordination

MPC

Ship 1

Ships 1

f2
Dependent
variables

F2 Ship Cmd

Control
Function

KIB

W2 Inventory

Assembly
Warehouse1

W1 Inventory

Assembly
Warehouse2

AT2 orders
AT1 Orders

AT1

Multiple source data structures can be mapped to the
same target structure. An example would be arrays
from two different sources need to be mapped to single target set structure.
One source structure can have multiple targets. Example is when elements of a source set need to be
mapped to multiple target variables.
Different fields of a source structure may map to different target structures
Multiple mappings and transforms must be configurable on the same data.

In the following two sections we will describe the
mappings and transformations needed to implement our
DEVS/MPC simulation test environment.

F1 Ship Cmd

Independent
variables

KIB Mapping and Transforms

AT2

Event
Coordination

DEVS

Figure 4. Composition of DEVS and MPC with KIB
The KIB allows the data integration to be modeled independently from the other models. In the case where we
want to design a controller, it enables the experimentation
of running a controller against an established detailed
simulation. When changing controller interfaces to read
different abstraction of the data, we can change the KIB
model independently of the simulation model.

1733

Godding, Sarjoughian, and Kempf
execution. It also needed to execute aggregation/disaggregation transforms across the correct time intervals. The ability to adjust the aggregation/disaggregation
of data based on execution frequency is a key enabler of
experimentation at different control frequencies. For example, if you want to try daily control against hourly data,
you must aggregate all events that occurred over last 24
hours of logical simulation time.. If you then want to try
shiftly (8 hour shifts) control, then you must only aggregate over the last 8 hours for logical simulation time.

to be specified. Also, a key entry can be specified for arrays that are part of a structure.
SetFieldValueToVariable: This mapping copies a
specific field value to a single variable. A key field needs
be specified to determine which set tuple to use. For example, if you have a set of tuples where each contains a product name and quantity, you can specify that the quantity for
productX be copied to this variable.
VariableToSetFieldValue: This transform copies a
variable to specific field in a set tuple. The field name in
the tuple needs to be specified, along with a key value.
Copy: The values of the fields from one model is copied into variable in the other model. The names do not need
to match, just the data type. For example, integers can only
be copied to integer fields.
Copy Exactly: The structure and data is copied to
identical structure in other model. Names of variable and
their values are maintained.

5

PILOT EXPERIMENT

The real world supply network topology is shown in Figure
5. This topology is an extension to the model shown in
Figure 2. For simplicity, shipping components are depicted
as arrows. This topology has 3 factories, 27 shipping lanes,
9 warehouses, and 9 assembly sites. Each of the fabrication
plants can produce up to 15 different products.

4.1.2 Transformations
This section lists the data transformations that had to be
provided by the KIB modeling language.
FloatToInteger: This transforms the data value from
float to integer. A rounding algorithm of floor, ceiling, or
round must be specified.
IntegerToFloat: Converts an integer value to a float.
AssignValue: Assigns a value that is configured in the
KIB model to a data field. This is a static value that cannot
change during the execution.
Aggregations (Mean, Median, Min, Max, Sum): All
these transforms aggregate multiple values into a single
value. The aggregation can be for all values in the current
time period, or for all values in multiple time periods.
Also, if data values are in arrays, the aggregation can return an array where the entries in the returned array are aggregated from multiple arrays.
Disaggregation: Different types of disaggregation can
be supported. A general purpose disaggregation is to divide
the source value into equal target values. The design of the
KIB enables extensions for customized disaggregation algorithms.
Scale: Multiplication or division operations can be
specified to scale the data values.

Fabrication 1

WH1

AT1

WH2

AT2

WH3

AT3

WH4

AT4

WH5

AT5

WH6

AT6

WH7

AT7

WH8

AT8

WH9

AT9

Fabrication 2

Fabrication 3

Worldwide
Shipping

Figure 5. Real world topology

4.1.3 KIB Control Modeling

The MPC was implemented using the Honeywell
Profit SuiteTM set of applications. The DES had been developed using the DEVSJAVA simulation environment.
The MPC controller design required a different model
instance for each product built from the fabrication plants.
This resulted in 15 different product controllers that needed
to run concurrently. The product controllers were coordinated using a dynamic, real-time optimizer. This optimizer

The KIB is required to support a synchronization model
that enables the controller to run in multiples of the simulation. Experiments were designed with daily, shiftly, and
hourly control against a simulation that could run at hourly
granularity. The KIB configuration model allowed the execution of either model to run in multiples of the other.
To provide this capability, the KIB had to coordinate
the timing of simulation input/output events with the solver

1734

Godding, Sarjoughian, and Kempf
The first iteration of experiments supported the development of the controller. Realistic stochastic simulations
were run and validated against historical data. The controller was then developed and integrated with these simulations using the KIB. This stage of simulation supported the
development of the controller and its required data interfaces. Base issues were worked out such as scalability and
controller design.
In iteration two the KIB was changed to output data to
the controller in a format that matches the corporate data
systems. The production data automation toolkits were developed during this iteration. The same simulation of the
physical models were used, however, the KIB data output
to the MPC was different. This enabled testing and development of the production data automation toolkits for the
controller.
After the two iterations of development, the controller
and associated data automation toolkits were put into the
production configuration.

provided both dynamic coordination and steady-state optimization to the underlying 15 control applications.
A separate simulation model was connected to each of
the 15 product controllers resulting in a distributed simulation. That is, each of the fifteen controllers had a designated simulation model. Each simulation matched the topology shown in Figure 5 but had different stochastic
distributions configured to match each of the products
characteristics for the supply and demand forecast vectors.
The KIB had to coordinate the execution of the 15 different simulations, 15 different controllers, and one dynamic, real-time optimizer. It also needed to map and
transform the data between each of the simulation and controller models.
5.1

Controller Development Approach

The goal of the simulation environment was to enable the
development and validation of the controller prior to putting it in production. Although this kind of simulationbased design is common practice, the use of the KIB enabled experiments and engineering of the complex interactions between discrete manufacturing processes and controller. A two step iterative process was devised (Figure 6).

Controller

KIB data
transformation
models

5.1.1 Findings
On the first simulation/controller runs, it was found there
would be scalability issues with the controller design and
the simulation. Although each of the models ran OK in
standalone mode, the integration highlighted invalid assumptions each had made about the other system. The controller had to be changed into a hierarchical design where
separate instances controlled each product. Performance
tuning had to be performed on the simulation to manage
the large numbers of active simulation entities. Changes
also had to be made in the simulation to correctly model
how discrete lots are shipped from the factory. It was
found this was an important behavior to simulate for the
controller. The discrete nature of lot sizing errors had impacts on how the controller needed to be tuned.
In the second iteration, we changed KIB models to exactly reproduce how data is sent and received from the
production data systems. This resulted in development of
the data automation toolkits prior to plugging the controller
into production. The KIB enabled experimentation and refinement of the aggregation needed for forecast vectors. It
also highlighted some invalid assumptions on how data is
provided from internal company systems versus subcontractors. The KIB provided a quick and efficient way to do
experimentation with many different types of aggregation
strategies.

Simulation of
physical
supply network

Simulation environment iteration 1

Controller

Data
Automation
toolkits

KIB data
transformation
models

Simulation of
physical
supply network

Simulation environment iteration 2

Controller

Data
Automation
toolkits

Corporate Data
Systems

Physical
supply network

Real world components

Figure 6. Simulation Iterations vs. Real World
First, the KIB model generated data that was directly
read and written into the controllers variables. Second, the
KIB model was revised to generate data the same way as
the enterprise data systems. The production data automation toolkits would be developed against simulated data
feeds.
The real world components section of Figure 6 shows
the physical systems we needed to work with. There is the
physical supply network, the corporate data systems that
capture current states and information about the physical
systems, some data automation software to transform the
data into the format required by the controller, and the actual controller system.

5.2

Results

The MPC models and production data automation toolkits
worked as designed on the first run in production. This was
a significant accomplishment since it was the first time
MPC was used in an actual production instance of a discrete semiconductor manufacturing problem.

1735

Godding, Sarjoughian, and Kempf
The controller design worked with a daily update to
the shipping signals. The supply and forecast vectors
needed daily granularity for the first few days, and then
could use weekly buckets for the next few weeks. When
the controller was plugged into production, it was able to
keep inventory within limits automatically at all warehouses as good as the current manual processes.
The team that developed the controller and simulation
comprised of 3 engineers, two senior control engineers one
software / simulation engineer. Projects of this scale typically take much more resources. Without the KIB modeling approach, the ability to experiment with different control frequencies and data sources would have been limited
or impossible within the time constraints. Either more time
would have been required for the simulation or less robust
controller put in at start of production.
6

REFERENCES
Godding, G., H. Sarjoughian, and K. Kempf. 2004. Multiformalism modeling approach for semiconductor supply/demand networks. In Proceedings of Winter Simulation Conference, 232-239. Washington DC, USA.
Huang, D., H. Sarjoughian, D. Rivera, G. Godding, K.
Kempf. 2006. Experiment Analysis of Hybrid Discrete
Event Simulation with Model Predictive Control for
Semiconductor Supply Chain Systems, In Proceedings
of Winter Simulation Conference, 1863-1870. Monterey, CA, USA.
K. Kempf. 2004. Control-Oriented Approaches to Supply
Chain Management in Semiconductor Manufacturing.
Proceeding of the American Control Conference,
4563-4576. Boston, MA, USA.
Qin, S., and T. Badgwell. 2003. A survey of industrial
model predictive control technology. Control Engineering Practice 11 (7): 733-764.
Sarjoughian, H. 2006. “Model Composability”, In Proceedings of Winter Simulation Conference, 149-158.
Monterey, CA, USA.
Sarjoughian, H., and D. Huang. 2005. A multi-formalism
modeling composition framework: Agent and discreteevent models. In Proceedings of the 9th IEEE International Symposium on Distributed Simulation and Real
Time Applications, 249-256. Montreal, Canada.
Zeigler, B., H. Praehofer, and T. Kim. 2000. Theory of
Modeling and Simulation: Integrating Discrete Event
and Continuous Complex Dynamic Systems. 2nd ed.
Academic Press.

CONCLUSIONS

An important benefit was the combined flexibility and
rapid controller design prototyping enabled with the KIB.
The pilot experiment was carried out concurrent with the
actual control of the production line and thus demonstrated
the impact of the KIB in industrial strength setting. This
approach to simulation-based design is indispensable in
employing new mixed tactical and strategic operation of
multi-billion dollar industries. The KIB enabled much
more experimentation (e.g., validating controller) than
would be normally possible in short time frames.
The KIB also allows for better experiments since it
highlights the integration mismatch between the models.
That is, it explicitly provides visibility to the integration
issues and provides a capability to understand the issues
and methodologically design solutions around them.
7

AUTHOR BIOGRAPHIES
GARY W. GODDING is a Technologist at Intel Corporation and a PhD candidate in the Computer Science and Engineering department at ASU. He can be contacted at
<gary.godding@intel.com>.

FUTURE WORK

We plan to extend the existing models onto other product
lines and other segments in the supply network. This adds
complications such as the increased number of planning
product configurations and consideration of complex bills
of material. We also plan to develop more advance aggregation and disaggregation transforms into the KIB. The
advanced functions would be based on more detailed
knowledge of the segments of the supply network domain
being modeled.

HESSAM S. SARJOUGHIAN is an assistant professor of
Computer Science and Engineering at ASU. He can be
contacted at <sarjoughian@asu.edu>.
KARL G. KEMPF is Director of Decision Technologies
at Intel Corporation and Adjunct Professor at ASU. He can
be contacted at <karl.g.kempf@intel.com>.

ACKNOWLEDGMENTS
We would like to acknowledge Kirk Smith of Intel Corporation and Duane Morningred of Honeywell Corporation
for their work on the MPC.

1736

Proceedings of the 2000 Winter Simulation Conference

J. A . Joines, R. R. Barton, K. Kang, and P. A . Fishwick, eds.

CREATING DISTRIBUTED SIMULATION USING DEVS M&S ENVIRONMENTS
Bernard P. Zeigler
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling and Simulation
Electrical & Computer Engineering Department
University of Arizona
Tucson, AZ 85721, U.S.A.

by systems theory, and the framework relations are
formulated in terms of the morphisms (preservation
relations) among system specifications. Conversely, the
abstractions provided by mathematical systems theory
require interpretation, as provided by the framework, to be
applicable to real world probIems.

ABSTRACT
We briefly review the theory of modeling and simulation
and its support for constructing distributed simulations.
Formal representation of simulation models can contribute
to a number of aspects in the modeling and simulation
enterprise. Separation of models from simulation execution
engines is a prerequisite transferring model among phases
of a project as well as from project to project. The Discrete
Event System Specification (DEVS) formalism, drawing
on its system theoretic basis, provides a number of
important properties such as hierarchical, modular
composition, universality and uniqueness that can support
development of simulation models and environments their
development. An layered architecture for supporting
comprehensive M&S

environments is

141

Expaurmtal Frame

dh

discussed that

Fxpaurmralfmmsponfih m lmdR which systcm
IS orpcnrrcntedwith and obsaved

unifies the theoretical framework with implementation in
distributed computational environments.
1

/

Yrmlam

ml2Jlm

d

EEfm

Figure 1: Entities and Relations of M&S in Framework

ELEMENTS OF THE THEORY OF
MODELING AND SIMULATION

The theory of modeling and simulation presented in
(Zeigler et al. 2000) provides a conceptual framework and
an associated computational approach to methodological
problems in M&S. The framework (Figure 1) provides a
set of entities (real system, model, simulator, experimental
frame) and relations among the entities (model validity,
simulator correctness, among others) that, in effect, present
a model of the M&S domain.
The computational approach is based on the
mathematical theory of systems and works with object
orientation and other computational paradigms. It is
intended to provide a sound means to manipulate the
framework elements and to derive logical relationships
among them that are usefully applied to real world
problems in simulation modeling.
Interestingly, the
framework and systems theory are intimately linked as
suggested in Figure 2. The framework entities are
formulated in terms of the system specifications provided

Q.

@

Figure 2: Interaction of Systems and M&S Concepts
158

Zeigler and Sarjoughian

2

AN ARCHITECTURE FOR M&S

Collaboration Layer
SamelDiff TimelPlace SmalllLarge Group

Figure 3 shows a conceptual layered architecture that is
intended to provide a framework for implementing comprehensive environments for M&S. We outline the layers:
SESlMB

Network Layer contains the actual computers
(including workstations and high performance
systems) and the connecting networks (both LAN
and WAN, their hardware and software) that do
the work of supporting all aspects of the M&S
lifecycle.
Simulation Layer is the software that executes the
models to generate their behavior (the essence of
simulation). Included in this laver are the motocols
that provide the basis for distributed simulation
(which are standardized in the High Level
Architecture (HLA)). Also included are database
management systems, software systems to support
control of simulation executions, visualization and
animation of the generated behaviors.
Modeling Layer supports the development of
models in formalisms that are independent of any
given simulation layer implementation. HLA just
mentioned also provides an object-oriented
templates for model description aimed at
supporting confederations of globally dispersed
models. However, beyond this, the formalisms for
model dynamics, whether continuous, discrete or
discrete event in nature are also included in this
layer. Model construction and especially, the key
processes of model abstraction and the DEVS-Bus
representation are also included.
Search Layer supports the investigation of large
families of altemative models, whether in the
form of spaces set up by parameters or more
powerful means of specifying altemative model
structures provided by the SES/MB methodology. Artificial intelligence and simulated natural
intelligence (evolutionary programming) may
be brought in to help deal with combinatorial
explosions occasioned by powerfhl model synthesizing capabilities.
Decision Layer applies the capability to search
and simulate large model sets to real problem
domains to support explorations, "what-if "
investigations, and optimizations.
Collaboration Layer enables people with partial
knowledge about a system, whether based on
discipline, location, task, or responsibility
specialization, to bring to bear individual
perspectives and contributions to achieve an
overall M&S goal.

AI

Evolutionary Prog.

Modeling Layer
Systems Formalisms, DEVS-Bus, Model Abstraction
Behavior Generation DBMS, GUI Animation Visualization
Workstation

Network Layer
Distributed Weblinternet

Parallel

Figure 3: An Architecture for Modeling & Simulation
The DEVS/HLA M&S environment was developed to
support HLA-compliant simulation based on the layered
architecture (Kim, Chow, et al. 1999, Zeigler, 1999). The
DEVS/CORBA M&S environment is being developed to
provide a prototype of comprehensive environment which
instantiates all of the features of the architecture (Kim,
Buckley, and Zeigler 1999).

3

CONCLUSIONS

As computing infrastructure (computers and networks)
becomes ever more powerful and the types of models that
are simulated become more and more sophisticated, the
software-based approach to simulation technology becomes
less and less viable and the theory-based approach becomes
more and more critically necessary. The DEVS formalism
and its underlying system theory foundations provide a
framework and a set of concepts to develop the infrastructure for supporting reusable and well-validated models
executed within well-verified and fully-capable simulation
middleware. DEVS has already been demonstrated to
support industrial strength environments, such as Joint
MEASURETM, and innovative modeling and simulation
domain applications such as for distributed system design.

ACKNOWLEDGMENT
This research has been supported in part by NSF Next
Generation Software (NGS) grant #EIA-9975050 and
DARPA Advanced Simulation Technology Thrust (ASTT)
Contract #N6 133997K-0007.

REFERENCES
Hong, G. P. and T. G. Kim. 1996. A framework for
verifying discrete event models within a DEVS-based
system development methodology. Transactions of the
Society for Computer Simulation, 13(1): 19-34.

159

Zeigler and Sarjoughian
Hong, J. S. and T. G. Kim. 1997. Real-time discrete event
system specification formalism for seamless real-time
software development. Discrete Event Dynamic
Systems: Theory and Applications, 7 : 355-375.
Kim, D., S. J. Buckley, and B. P. Zeigler.1999. Distributed
supply chain simulation in a DEVSKORBA execution
environment. In Proceedings of the 1999 Winter
Simulation Conference, ed. P. A. Farrington, H. B.
Nembhard, D. T. Sturrock, and G. W. Evans, 13331340. Institute of Electrical and Electronic Engineers:
Piscataway, N.J.
Kim, T. G., S. M. Cho, et al. 2000. DEVS framework for
systems development: Unified specification for logical
analysis, performance evaluation and implementation.
Discrete Event Modeling and Slmulation: Enabling
Future Technologies. ed. H. S. Sarjoughian. New
York, NY, Springer-Verlag.
Kim, Y. J., J. H. Cho, et al. 1999. DEVS-HLA:
Heterogeneous simulation framework using DEVS
Bus implemented on RTI. In Proceedings of the 1999
Summer Computer Simulation Conference, Chicago,
IL.
Lake, T., H. Sarjoughian, et al. 2000. DEVS Simulation
and HLA Lookahead. SIW, Orlando, FL.
Sarjoughian, H. S . , D. Hild, and B. P. Zeigler. 2000.
Integrated Engineering. Computer 33(3): 110-113.
Zeigler, B. P. 1976. Theory of modeling and simulation.
New York, John Wiley (Under Revision for 2nd
Edition 1998).
Zeigler, B. P., G. Ball, et al. 1999. Implementation of the
DEVS Formalism over the HLNRTI: Problems and
Solutions. SZW, Orlando, FL.

Zeigler, B. P., S. Hall, et al. 1999. Exploiting HLA and
DEVS to Promote Interoperability and Reuse in
Lockheed’s Corporate Environment. Simulation
Journal. 74(4): 288-295.
Zeigler, B. P., T. G . Kim, and H. Praehofer. 2000. Theory
of Modeling and Simulation. New York, NY,
Academic Press.

160

Observations on Real-time Simulation Design and Experimentation
Soroosh Gholami
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
{soroosh.gholami, hss}@asu.edu
Keywords: ALRT-DEVS, Experimentation,
Modeling, Real-time Simulation

Real-time

Abstract
Modeling and simulation provides a convenient way for evaluating operability of designed systems (e.g., Network-onChip) in their desired target environments. Where applicable,
real-time modeling with real-time simulation extends logicaltime M&S capabilities by evaluating the degree to which system requirements can be satisfied in actual operational scenarios. Simulations can be directly observed and evaluated in
realistic settings. This brings about a well-known challenge –
simulation experimentations are never without computational
cost. This leads to real-time simulation (adversely) affecting
simulation results, sometimes drastically, if not carefully designed for. In this paper, in view of finite computational resources for executing simulations in real-time, new observations on data collection and evaluations are described for
the Action-Level Real-Time DEVS (ALRT-DEVS) simulator. Simple and complex example real-time models are developed in selected simulators, experiments with varying degrees of data collection volume are conducted, and results are
discussed.

1.

INTRODUCTION

With the growing size and complexity of systems, more
practical design and analysis methods are required. Logicaltime modeling and simulation offers both design (via modeling) and analysis (simulation of models) for predicting system behavior before implementation. Simulation modeling
cannot provide complete assurance for system performance,
it is more capable than analytical approaches for systems with
large state spaces.
Logical-time simulation modeling does not prove very effective when the system to be simulated is part of a larger
system with complex and often partially understood dependencies. The resulting model can be very large and inaccurate
to be effectively simulated. To handle this difficulty, the system can be partitioned to two parts. One part is simulated in
real-time. The other part is the actual (physical) system. For
this reason, real-time simulation is an attractive alternative to
logical-time simulation as it can afford its use in actual environments. For example, designing a Network-on-Chip (NoC)
system needs to account for constrained resources. Rather

than modeling an environment (e.g., sensors and actuators)
which together with an NoC form a mobile device, it is advantageous to consider real-time simulation of an NoC alone.
The mobile device has two parts: one is NoC and the other is
the collection of the sensors and actuators. This allows modeling and simulating NoC in ’isolation’ and thus can significantly reduce scale and complexity of simulating NoC with
its environment as a whole.
The host computer for the simulator can also be considered
its ‘environment’. Hardware, OS, and other applications that
run alongside the simulator can perturb the simulation in a
myriad of unpredictable ways. The hardware and network related operations have no effect on the logical-time simulated
behaviors (the model dynamics is independent of the computer executing the simulator due to its use of abstract, idealized clock). In other words, logical-time simulation requires
the execution environment (i.e., host computer) to have no
side effect on the simulated model behavior. The same cannot be said about real-time simulation.
Real-time simulation poses several limitations. It has to
complete its execution cycles within real-time deadlines in order to remain synchronized with physical time (and thus the
environment in which it is being used). Simulation’s real-time
clock, as compared with logical time clock, is not absolute.
Physical clock is imperfect and thus real-time clock is not the
same as abstract logical clock. Lack of perfect physical clock
in real-time simulation fundamentally changes the nature of
simulation. A prescribed time period defined for an operation
to be executed multiple times during simulation at different
time instances can change. Real-time simulation cannot guarantee perfect behavior due to inability to equate logical- and
real-time clocks.
As real-time simulation execution is subject to time constraints, operations (except model execution) which consume
physical time should be reduced. Operations can include data
collection and analysis at run-time. Execution of these operations in logical-time simulations, if handled in a modular
fashion as in Experimental Frame concept [10], do not introduce any inaccuracy or wrong model behaviors. This is because time can be stopped or elongated. The only one side
effect is on the length of physical time it takes to simulate the
model. However, physical time is an irreplaceable resource
and all operations should be carefully designed and tested for

time feasibility before they are used in real-time simulation.
For example, if a control signal to the mobile device actuator is transmitted but reaches its destination late (e.g., due to
long execution time), the simulation becomes unreliable as
compared with the behavior of its real system counterpart.
In the remainder of this paper, related works on real-time
M&S, various methods for data collection, analysis, and experimentation are described. The focus is on real-time data
collection, in relation to its logical-time counterpart, using
the Action-Level Real-Time DEVS simulator. This approach
is compared with selected simulators of the same genre. The
results demonstrates how ineffective real-time simulators and
their support for data collection can be and in particular the
benefits of ALRT1 )- DEVS for real-time modeling and realtime execution [4][3] offers from experimentation perspective
given real-time modeling at the level of actions defined for
external and internal transition functions.
The structure of this paper is as follows. We present background on real-time modeling, simulation and experimentation, and related works in Section 2. Challenges for real-time
experimentation with emphasis on ALRT-DEVS and Ptolemy
II are discussed in Section 3.1. Examples including data from
NoC simulation and their results are detailed in Section 5.
Conclusion and further work is presented in Section 6.

2.

BACKGROUND

In this section we review some of the base concepts mentioned in this paper. First of all, our method for real-time modeling (ALRT-DEVS) and simulation (Real-time DEVS-Suite)
is introduced. This will provide an insight for the reader how
real-time modeling and simulation is done. In addition, we
focus on logical-time experimentation and how it is done in
current formalisms and simulation tools. Then, it is discussed
how these methods of experimentation are unsuitable for realtime simulation.

2.1.

ALRT-DEVS models

Action-level Real-time model is an atomic Discrete EVent
System Specification (DEVS) [9] that can handle concrete
real-time modeling of systems. Real-time modeling is aimed
at capturing accurate timing information of the target system and reflect it in the model. Furthermore, this model
can be later used for real-time simulation. ALRT-DEVS
uses abstract time interval, actions, and activity mapping
function definitions provided in the atomic RT-DEVS =
hX, S,Y, δext , δint , λ,ti, ψ, Ai real which is an extension of
classic atomic DEVS model [9]. An activity mapping function (ψ : S → A) is introduced for assigning actions (A) to
states. Time advance function is defined to have lower and
+
upper bounds (ti : A → R+
0,∞ × R0,∞ ).
1 It

is pronounced alert.

G1 = [outgoingDataBuffer 6= ∅]
G2 = [incomingFlitQueue 6= ∅]
G02 = EventExternal [G2 ]
G3 = [incomingFlitQueue = ∅ ∧ outgoingDataBuffer = ∅]
Figure 1. Real-time Statechart for depacketizer component

In ALRT-DEVS we introduced concrete syntax and operational semantics at the level of actions for external and internal transition functions. This is because abstract functions for
time windows and actions are insufficient for real-time modeling with direct support for real-time simulation.
Specifically, the ‘state’ of a model has to be rich enough to
lend itself for specifying actions individually and collectively
for both external and internal transition functions. Dynamic
decision making for actions given limited time periods is
specified using the concepts of locations, transitions, and actions provided in Statecharts [5]. In this approach we assumed
a model to have different locations within a state based on remaining time. Also, actions are mapped to each location with
a predefined sequence of execution. Finally, guarded transitions between these locations provide a dynamic method of
decision making for the model. Guards on transitions are chosen from secondary state variables which can be any variable
in the specification other than state and time. This could be
the length of the incoming queue in server, the height of water level in a water tank, or the temperature in a nuclear power
plant.
Figure 1 depicts a simple statechart model for depacketizer component. Three phases are assumed for depacketizer:
Depacketize in which flits are converted to streams of data,
Transmission in which depacketized data is sent to the processing element, and Idle for other stages of this component in which the input/output queues are empty. If any of
the guards specified in the diagram are satisfied, the system
changes its phase to the one conforming to the current evaluation of the secondary state variables.

2.2.

Real-time DEVS-Suite simulator

Real-time models can be potentially used for real-time execution. Time in a real-time simulation engine is in synchrony
with real-time. A model simulated in a real-time simulator
can be connected to its operating environment and communicate via sensors and actuators in order to react to stimulations
from outside and provide performance data.
Real-time DEVS-Suite provides real-time simulation by
guaranteeing the execution of each action within its specified time window. Each action which violates its firm deadline would be discarded and the model may modify the line of
execution using its dynamic decision making system. In addition to executing actions within their respective time window,
Real-time DEVS-Suite incorporates Java’s threading capability to introduce parallel execution to atomic model execution.
Before this, models were executed sequentially and this was
not useful for real-time execution. Aside from these, basic
features of ALRT-DEVS such as locations, transitions, actions, and time windows are also added to the DEVS-Suite
[1] to support real-time modeling in addition to real-time simulation.
Other simulators such as Ptolemy II [6] supports realtime execution akin to DEVS-Suite. eCD++ [8] is a DEVSbased simulator for embedded system simulation. It supports best effort real-time execution. Also, real-time testing
in MathWorks by xPC Target [2] is designed to enable HIL
(Hardware-in-the-Loop) and RCP (Rapid Control Prototyping) on general purpose computers or on xPC Target Turnkey
(real-time target machine for xPC Target). For the purpose of
this paper, ALRT-DEVS and Ptolemy II are considered.

3.

EXPERIMENTATION

This section discusses some of the inherent differences between logical and real-time experimentations. It shows handling of the time in the contexts of logical-time and real-time
simulations with important implications on system’s dynamics.

3.1.

Logical-time

In logical time simulation, experimentation is usually handled by recording interesting events in a file or a transducer
in an experimental frame (EF). The experimental frame is responsible for generating events, injecting them into the simulation model, and recording the simulation outcomes using
transducers. DEVS modeling insists on modularity. Therefore, data collection is to be carried out using ports from
atomic and coupled models. Hierarchical modeling enforces
each level to communicate with its immediate higher/lower
level models. This also applies to outside simulation interaction in which communication should occur through an interface and not individually by each atomic model.
In the DEVS preferred data collection and analysis scheme,
EF is a coupled model consisting of a transducer and a gen-

erator. These atomic models are executed along with other
atomic/coupled models of the system by the central simulator engine. To record events, one should connect every port
which transmits interesting events to the experimental frame
(transducer in specific). The transducer organizes and records
the events for analysis. Other simulation engines use various
methods for data collection. As for Ptolemy, data analysis is
done in specific actors (such as plotters) which is similar to
the concept of EF. In order to show the similarity in the function of DEVS-Suite and Ptolemy, a simple ramp example is
incorporated and executed in logical-time. The model contains a CurrentTime actor which outputs the current time of
the whole model, a Ramp actor, and a X-Y Plotter to sketch
the outcome. This model works under a Continuous director.
In DEVS-Suite the model consists of a generator which triggers the ramp model to generate data. The output of both are
lines with steady slope which was expected. The same test
is done in real-time in Section 3.1 to show the difference in
real-time simulation. Various methods of data collection and
analysis are used in different simulators, however, many of
them would be problematic when used for real-time simulation. This is discussed in Section 3.1.

3.2.

Real-time

ALRT-DEVS supports defining parallel atomic models that
use default confluent function. Every action is defined to require positive, non-zero duration to be executed in physical
time. Output events are to be generated prior to handling input events. The role of real-time data collection, however, is
not accounted for in ALRT-DEVS. Coupled parallel models
for this kind of parallel atomic model can be supported.
Simulations were carried out without adhering completely
to the Experimental Frame (EF) concept where outputs for all
atomic models are collected directly from them. We noted the
impact of data collection on real-time execution time, but we
did not formulate the role of real-time for action-level operations given physical time needed to collect (observe) simulation data (output events). Transient states (requiring zerotime actions) which are needed for confluent function is not
defined for ALRT-DEVS. Calling an output function prior to
external transition causes the model to undergo transient operations. All simulation operations including generating outputs take time to execute and can cause accuracy degradation if execution of actions cannot be guaranteed to complete
fast enough (within each simulation cycle addition time is reserved for data collection). That is, use of transient states has
to be accounted for when simulation data collection and analysis is to occur in real-time. Furthermore, if a piece of data is
ready but not captured at the time it was created because of
time constraints, then the result of the experiment is inaccurate from timing perspective.
We can divide physical time it takes for a complete atomic
model simulation cycle to be executed to two parts. One part

is for model execution (simulation, ∆tsim ). The other part is
for data collection (observation, ∆tobs ). The model execution
part can be further divided into two parts. One is the time
duration specified for actions in internal and external transition functions (∆text,int > 0). The other part is the time for
executing output, time advance, and all other tasks such as
state-based data aggregation and disaggregation. The time allocated for this part (∆trest > 0) is needed for having a complete atomic model simulation cycle. The total time for an
atomic cycle ∆tac > 0 is equal to ∆text,int + ∆trest + ∆tobs . Note
that 0 < ∆tobs < ∞ accounts for processing time that is needed
for collecting data (e.g., writing data to a file or to a display)
which is distinct from processing time needed for generating
output events.
Aside from atomic model, real-time execution processing
time for coupled models also has to be accounted for. This
is because logical-time coupled models assume instantaneous
communication (i.e., outputs generated from an atomic model
can be delivered as input to another atomic model without
simulator consuming time). In real-time simulation of coupled models, it takes some non-zero, positive time period
(∆tI/O > 0) for the coupled model to route and deliver output messages as input messages between any two (atomic
or coupled) DEVS models. The ∆tI/O > 0 accounts for messages that are communicated across external input, internal,
and external output couplings. Therefore, the flat coupled cycle time for a coupled model with one atomic model can be
defined as ∆t f cc = ∆tac + ∆tI/O where time allocated for communication is for communications that may occur across external input and external output couplings. More generally,
for a flat coupled model with n atomic models, time for a
flat coupled cycle is defined as ∆t f cc = ∑ni=1 ∆tac,i + ∆tI/O .
For a hierarchical coupled model with n atomic models, p
flat coupled models, and q hierarchial coupled models, its
execution time for one complete cycle can be defined as
q
p
∆thcc = ∑ni=1 ∆tac,i + ∑ j=1 ∆t f cc, j + ∑k=1 ∆thcc,k + ∆tI/O (see
[7] for more details).
The above formulation affords defining lower bound on
physical time that is needed for real-time data observation in
real-time simulation given some finite computing resources.
The lower bound can be defined given ∆tobs for the atomic
and coupled models that are to be observed. The observation
time can be controlled as a function of the number ports that
are observed. It should be noted that observations for events
(data or messages) in coupled models should be avoided because they can only contain data from their respective atomic
models. Data that is observed on output ports of any coupled
model are intrinsically the same as those generated by atomic
models, but delayed in time.

4.

REAL-TIME EXPERIMENTATION

In logical-time simulation, execution of models and the
communication among them are all handled sequentially.

Therefore, data collection is not an issue. Naturally, data collection requires time. Usually an experimental frame is involved in the process of data collection which gathers all interesting events within the model via ports. Handling all these
events in each instance of time can be time consuming. However, since time advance is controlled by the simulation engine, it could be shortened or elongated whenever required.
However, this is a major issue in real-time simulation where
time cannot be stopped or slowed down. This poses serious
limitations on the number of probes an experimental frame
can observe, the frequency of occurrence of that event, and
the amount of processing that can be done on the gathered
data. An experimental frame is itself one model among the
other simulation model which consumes memory and computation time. For every single event, the EF model goes through
an external transition cycle and analysis. Therefore, the frequency of the EF event processing is the sum of the output
generation frequency of all components connected to it! This
can be an important source of time loss for the rest of the
model. In addition, processing of events can be also problematic if not taken into account. In logical time experimentation,
the analysis can be done in the time of the simulation and it
does not interfere with the validity of the outcome. However,
for real-time simulation, the amount of processing per event
can make a substantial difference in the result of the simulation and deadline losses. In cases which the continuation of
the simulation is dependent on the analysis of gathered data,
the designer must deal with the trade-off of accuracy and validity: if analysis takes so much of the computation time, the
results generated by the simulation will be inaccurate because
of the frequent deadline losses (contradicts with accuracy)
and on the other hand if analysis is done partially or with
lower precision, the simulation can go the wrong way since
the controller cannot make the right decision based on the
feedback from the environment (contradicts with validity).
We faced the differences between experimentations in real
and logical-time simulators when working with an NoC
model. In Section 5, an experiment on NoC data collection
demonstrates this difference. For example, NoC data collection can be done with the usual transducer component. The
implications of using this method of data collection was discussed above. One other way is to use the flit itself as the data
collector. In this approach, the flit (which is a non-simulatable
object) collects timing data while moving in the network from
one node to the other. This can include very detailed information such as operations done inside a switch (moving
from incoming queue to crossbar or from there to the output buffer) which are not sensed outside the switch. The flit
is then archived when it sinked at its destination and used
for analysis after the simulation is over. Data can also be collected in each component and reported periodically to a transducer component. This is more efficient that event-by-event

Figure 2. Ramp model: (a) ALRT-DEVS and (b) Ptolemy II

reporting and the data is centralized in case they were needed
for making central decisions. Of course the method of data
collection is tied with the type of the system under simulation. For example, in some of them a moving object (such
as flit in NoC) does not exist. System specific characteristics,
may eliminate some methods of data collection and analysis.
These are discussed in Section 4.1.
In order to see the effect of data observation on the pure
model behavior, one does not need to consider complicated
models. Here, we used a simple ramp model to show the seriousness of the situation. This quick experiment is done with
ALRT-DEVS and Ptolemy II on a Windows 7 64-bit machine
with 2 GB memory and Intel dual core, 1.83 GHz processor. The real-time ramp models designed in DEVS-Suite and
Ptolemy II are depicted in Figure 2. In order to build the realtime ramp in Ptolemy, we used a Pulse to start the process
and a Sleep component (from Ptolemy’s real-time package)
to trigger the Ramp component every 8 ms. The output is retrieved using a Display. Also, the physical time is captured
by the WallClockTime component and the director we used
was PN (Synchronous Data Flow). The ALRT-DEVS model
is consist of three components: a ramp model, a generator
for triggering the ramp, and a collector component which
records the outputs and their respective timestamps. What we
expect from the output is a clean ramp line. In Figure 3-a,
the output of Ptolemy along with ALRT-DEVS and the realtime deadline of the ramp are depicted to show the difference between them. The real-time deadline is the expected
time which we expect the output to be generated from the
ramp. In the top left of the plot, the first 20 steps are magnified which clearly show the difference between the outputs
retrieved from Ptolemy II and ALRT-DEVS. This difference
is caused by a slow initialization phase in Ptolemy II which
causes the whole plot to have a displacement when compared
to the real-time deadline and ALRT-DEVS outputs. Therefore, the output of Ptolemy is not conforming to the deadline
and since this is real-time, we cannot expect similar results

Figure 3. Ramp model behaviors in (a) ALRT-DEVS vs.
Ptolemy II and (b) Soft real-time DEVS vs. ALRT-DEVS
against real-time deadline
from two different executions. Other runs of the same experiment resulted in quite different plots. This example shows
how unreliable the results of a sophisticated system would
be if real-time experimentation problems are not addressed.
In this paper, we are trying to emphasize on the importance
of this issue and suggesting several guidelines for alleviating
this problem. The difference in curves resulted from Ptolemy
and Real-time DEVS-Suite has its origins in different modeling formulations and simulation protocols. In our ALRTDEVS modeling approach, as introduced in Section 2, we
increased operation granularity of a component to actions
and mapped time windows to each of them. This way, time
constraints are enforced in each model and for every action.
However, including timing information in the model is one
thing and using them in the execution platform is another.
This is why, Real-time DEVS-Suite adopts a novel real-time
execution protocol which is different from the logical-time
DEVS-Suite simulation protocol. This protocol insures executing models based on the timing constraints specified in the
models. As for Ptolemy, execution in real-time is not guaranteed in the simulation protocol. Also, real-time actors (such as
execution time or real-time Plotter) do not impose time constraints, therefore, real-time execution happens at the level of

best-effort instead of absolute guarantee.
In order to show the impact of using Real-time modeling and simulation technique (such as the one we developed
in ALRT-DEVS and Real-time DEVS-Suite) we compared
the output of ALRT-DEVS with the output of soft real-time
DEVS-Suite for the ramp model. However, this time the emphasis is on time. Soft real-time DEVS provides best effort (soft) real-time execution. Here we intend to show how
much our hard real-time extension affects the timeliness of
the output. The hardware configuration of this experiment is
the same as above. Figure 3-b contains three lines. The long
dashed-line is the output of the soft real-time DEVS-Suite and
the solid line is the output of the ALRT-DEVS ramp model.
The dotted line shows the deadline of the output as if this
was a real system. The result clearly demonstrates the perfect
timeliness (the solid line is on the dotted one) which Realtime DEVS-Suite provides when compared to the soft realtime output of logical-time DEVS-Suite. Also, it is important
to notice that the soft real-time output is diverging from deadline over time based on a monotonically increasing function
which results in a two second divergence after 30 seconds into
the simulation.

4.1.

Addressing RT-Exp. challenges

In general, there are three requirements in order to reach
reliable real-time experimentation: 1) real-time modeling, 2)
real-time simulation protocol, and 3) real-time data collection. Each of these requirements are further illustrated below.
The aim of real-time modeling (which was partially discussed above) is adding time information to the model of the
system. These timing information specifies upper-bounds and
lower-bounds for actions to finish their execution inside them.
The resulting model is capable of being executed in real-time
since all time constraints are stated in the model. We encourage the reader to refer to [4].
The simulation protocol, should take advantage of the realtime model and apply those constraints in the execution of the
model. As described in the case of Real-time DEVS-Suite,
the simulation protocol must guarantee the execution of each
action within its time window. The simulation engine must
cuts off the execution of one action which is going beyond its
specified time window in order to prevent time inconsistency
to escalate in the system and affect the results which are to
follow.
4.1.1. Real-time data collection & analysis
Real-time M&S provides us with an executable model of
the system which operates in real-time and is capable of communicating with the environment. However, experimentation
is the primary reason of putting this amount of effort in modeling and simulation. The issue however is the impact of data
observation and analysis on the simulation which we call observation impact phenomenon. Based on this, observation al-

ways impacts the phenomenon which is under observation.
However, this does not hold for logical-time simulation since
it is isolated from the real world (even from the point of view
of time advance). On the other hand, real-time simulation
is part of the real world and is partially under the influence
of this phenomenon. The problem is that every simulator related operation takes its resources from the simulation model.
This results in deadline miss and time inconsistency if not addressed properly. Therefore, data collection should be done
carefully or it might cause the simulation to diverge from the
physical/virtual system it represents. Below are four points
that are important in implementing methods of data collection and analysis.
The system which is represented by the model should always be considered when implementing data collection and
analysis methods. For logical-time simulation, a general data
collection method (such as EF) is always sufficient. However,
in real-time simulation of a system one method may be more
efficient than the others. In NoC simulation, the number of
components (flit-level modeling) may exceed several hundred
for a chip with 32 cores. Handling of all events emitted from
these components by one transducer is very time consuming.
However, flit-based data collection (discussed above) may alleviate this situation. The benefit of this method is that no extra event for the purpose of data collection is needed. To conclude, by designing a domain specific data collection method,
higher level of efficiency might be reached which is always
welcomed for real-time simulation.
Online data analysis means when analysis on data is done
simultaneously with the simulation instead of postponing it
to the end. Online analysis is useful when the result of the
analysis at every instance is used as an input to the simulation
(feedback), the user needs intermediate results to be shown,
or the analysis determines when the simulation should end.
If online analysis is needed for any of these reasons, data
collection method is also affected because centralized data
is needed. If data is gathered in distributed fashion (like flitbased data collection), a reporting mechanism must periodically submit the collected data to a centralized transducer for
online analysis.
Analysis data loss can happen in real-time simulation. We
use this term when a performance data which is to be collected for analysis is not collected because there is no time
to complete that task (due to firm deadlines). We should look
at data collection as part of the real-time simulation which
may sometimes miss deadlines and lose data. So, based on
the amount of resources dedicated to the simulation, there is
a tradeoff between the accuracy of the simulation and the accuracy of data analysis.
The amount of resources given to the simulation engine is
effective on the result. In other words, there is a direct relationship between the amount of resource and the accuracy

of results. Therefore, one should always consider this factor when analyzing the performance data resulted from the
simulation. Logical-time simulation is executed on infinite resources (time is also labeled as a resource). Therefore, accuracy of results and granularity of the model are the only factors to consider when analyzing data from a logical-time simulation. As for real-time simulation, the impact of resources
on the final results brings in the amount of resources into consideration as well. In order to illustrate this point, two of the
experiments above are done using an Intel 2.93 GHz Dual
Core processor with 4 GB of memory. We repeated the experiment done in Figure 3-b on the new configuration. Results
showed that the hard real-time still strictly conforms to the
deadlines but the soft real-time execution experienced only 1
(instead of two) second divergence after 30 seconds.

5.

EXPERIMENTS AND RESULTS

One of the ways of reducing the impact of data collection
on the results is periodic reporting instead of event-by-event
reporting. We call it: Periodic Data Collection. In this approach, the data is gathered on periodic cycles which reduces
the number of events to be processed by the transducer. This
reduction means less time to be spent on data collection and
analysis. In order to illustrate this, three experiments are designed: two of them use the ramp model and the other uses
NoC which is relatively more complex. All experiments are
carried out on a computer with a Windows 7 32-bit machine,
Intel 2.93 GHz Dual Core processor with 4 GB memory.

5.1.

Figure 4. Periodic data collection effect for ALRT-DEVS
ramp model

Figure 5. Impact of data collection frequency on Ptolemy II

Experiment I

In this experiment, the ramp model (used above) is changed
in a way to report its output value in a periodic fashion. We
have used four different values: 1, 5, 10, and 50 which represent the number of cycles the ramp waits to produce an output. In Period = 1, every step is reported to the data collector but in Period = 50, the report is made at every 50 cycles. The ramp model in this experiment produces an output every 0.005 seconds. Figure 4 shows the difference between the results extracted from different reporting periods.
For Period = 1, data is approximately gathered 10.5 ms before the deadline. Occasionally, this value changes due to
the computation load on the system. Comparing these results
with other periods of data collection results in two conclusions: 1) data collection takes less time in longer periods and
2) the number of occasional variations are more when data
is collected more frequently which reduces simulation predictability.
This experiment shows that the effect of data collection is
more than generally assumed. The ramp model is relatively
simple but still shows significant changes when the observation frequencies is changed. This effect can be substantial for
more complex systems (see Experiment III) . Also, there is
a limit to the periodic data collection effect. After a certain

point, the impact on the results becomes trivial (∆tobs → 0).
The rest of the difference with the actual model relates to the
model formulation and simulation protocol (represented by
∆thcc , ∆trest , and ∆text,int ).

5.2.

Experiment II

We changed the Ptolemy II model in order to see the periodic data collection effect. For this purpose, a Modal Model
was used and a simple five-state filter which passes one piece
of data out of every five. When tested the model on 125 (steps
per second) data generation rate and the results were as depicted in Figure 5 with a 5-cycle data filtering (individual dots
marked with Freq = 5). Improvements are evident when this
is compared with Ptolemy’s ramp output with no data filtering
in the upper line.

5.3.

Experiment III

To carry out an experiment on a larger model with more
complexity, periodic data collection in NoC is considered.
The formulation of NoC model used in this experiment is
discussed in [4]. The NoC in this experiment is a 3 × 3 network with mesh topology which contains 195 atomic and
31 coupled models with 3 levels of hierarchy. The param-

eters watched in this experiment are average flit latency
from source to destination and average waiting time at every
switch. Here, in order to demonstrate the impact of data collection on final results, we reduced the number of components
being observed. In one scenario, the transducer is connected
to all 9 processing elements and in another it is connected to
(two nodes that are farthest apart). Furthermore, the effectiveness of the injection rate on this phenomenon is analyzed by
increasing it from 2.22 (flits/sec) to 4 (flits/sec). The results
shown in Table 1 are average points of 5 runs for each configuration. For small injection rates (no network congestion
exists and flits are delivered on time), the difference between
results are trivial. However, as we move toward higher injection rates (such as 4 flits per second) the difference becomes
more substantial.
For this experiment, it is important to notice that the difference in the number of elements under observation is only 7
but the difference observed in the results cannot be neglected.
This experiment is a clear demonstration of how –even for
this small differences– data observation can significantly affect the output. This is more evident when the system becomes more complex (such as NoC as compared with Ramp)
or the simulation is at its Saturation point. At the saturation
point, the hardware reaches its limits in executing simulation
tasks and a sudden increase in deadline losses and time inconsistency is observed. At 4 packets per second injection rate,
the impact of observing 7 extra nodes on ∆tobs is 6% of the
total execution time (∆tobs + ∆tsim ) (see Table 1).
Table 1. 2-node vs. 9-node observation effect on avg. flit
latency (Avg. FL) and avg. switch waiting time (Avg. SW)
with various injection rates

Avg. FL
Avg. SW

6.

9
2
9
2

Avg. Injection Rate (per sec)
2.22 2.86 3.03
4.00
2.95 3.72 3.97 18.98
2.92 3.52 3.86 17.73
0.74 0.93 0.99
4.75
0.73 0.88 0.96
4.43

CONCLUSION AND FUTURE WORK

In this paper, we showed that data collection and experimentation impacts dynamics of real-time simulations. We
used DEVS-Suite and Ptolemy II as examples to show the
Observation Impact Phenomenon in a quantitative setting.
We suggested methods for limiting the impact of data collection on simulation execution and demonstrated the differences that resulted from these methods using a simple ramp
model and a 9-node Network-on-Chip model. It is important
to notice that unlike logical-time simulation, in real-time simulation, the impact of data collection and analysis (i.e., experimentation) should be accounted for as illustrated with the

ALRT-DEVS NoC real-time simulation. Thus, experiments
must be precisely designed and the results carefully evaluated
depending on the class problems under study. In summary,
the findings presented here should hold for similar classes of
simulators as noted in Section 2. Extending this work to distributed, mixed real-time and logical simulation with use in
Cyber-Physical Systems research remains as future work.

REFERENCES
[1] DEVS-Suite simulator, version 2.1.0.
suitesim.sourceforge.net/, 2009.

http://devs-

[2] J. Burck, M. Zeher, R. Armiger, and J. Beaty. Developing the world’s most advanced prosthetic arm using
model-based design. http://www.mathworks.com, 2009.
[3] S. Gholami and H. Sarjoughian. Action-level, real-time
network-on-chip modeling with DEVS and statecharts
specifications. submitted. ACM Transactions on Embedded Computing Systems, 2012.
[4] S. Gholami and H. Sarjoughian. Real-time network-onchip simulation modeling. In SIMUTools, Desenzano,
Italy, pages 103–112. ICST, 2012.
[5] H. Giese and S. Burmester. Real-time statechart semantics. TR-RI-03-239, University of Paderborn, 2003.
[6] E. Lee and S. Neuendorffer. Tutorial: Building ptolemy
II models graphically. University of California, Berkeley, Tech. Rep. UCB/EECS-2007-129, 2007.
[7] H. Sarjoughian and S. Gholami. Action-level real-time
devs modeling and simulation. submitted. ACM Transactions on Modeling and Computer Simulation, 2013.
[8] Y. Yu and G. Wainer. eCD++: an engine for executing
DEVS models in embedded platforms. In Proceedings
of the 2007 summer computer simulation conference,
pages 323–330. ACM Digital Library, 2007.
[9] B. Zeigler, T. Kim, and H. Praehofer. Theory of Modeling and Simulation. Academic Press, Inc., Orlando, FL,
USA, 2nd edition, 2000.
[10] B. Zeigler and H. Sarjoughian.
Introduction to
DEVS modeling & simulation with JAVA: Developing
component-based simulation models. Arizona Center
for Integrativ Modeling & Simulation, 2003.

Proceedings of the 2000 Winter Simulation Conference
J. A. Joines. R. R. Barton, K. Kang, and P. A. Fishwick, eds.

CONCEPTIONS OF CURRICULUM FOR SIMULATION EDUCATION (PANEL)
Helena Szczerbicka (Panel Chair)
Institute for Computer Science
Fachbereich Mathematik und Informatik
University of Hannover
30167 Hannover, GERMANY

1

Jerry Banks

Ralph V. Rogers

AutoSimulations
A Brooks Automation company
Marietta, GA 30067, U.S.A.

Department of Engineering Management
Virginia Modeling, Analysis, and Simulation Center
Old Dominion University
Norfolk, VA 23529, U.S.A.

Tuncer I. Oren

Hessam S. Sarjoughian
Bernard P. Zeigler

Turkish Science and Technical Research Council
Marmara Research Center
Information Technologies Research Institute
41470 Gebze, Kocaeli, TURKEY

Arizona Center for Integrative Modeling and Simulation
Department of Electrical and Computer Engineering
The University of Arizona
Tucson, AZ 85721, U.S.A.

INTRODUCTION (Helena Szczerbicka)

What skills should professionals develop during
the education and training?
Impact of developments in simulation technology:
what do we educate simulation professionals for?
What are educational strategies to meet current
and anticipated world needs in simulation?
What are the goals of an educational cumculum
for simulation?
How to organize education of simulation to make
it attractive for students?
What are criteria on selection of tools for teaching
simulation?
Are there initiatives currently going on in the
Modelling and Simulation community to establish
some structure in the M&S education and
training?

In the Winter Simulation Conference 1999 in Phoenix, a
series of discussions, conversations, and exchanges on the
topic of personnel to meet the current modeling and
simulation demands of the civilian application and world
military as well led to the idea of giving this discussion a
more structured shape in the Winter Simulation Conference
2000. We continue to discuss the complex issue of Simulation Education. A general demand for modeling and
simulation professionals can be observed in a large number
of enterprises. However computer science graduates are
not adequately prepared for employment opportunities
involving simulation as a tool in solving problems. Most
computer science majors have very limited exposure to
simulation. They gain experience in handling of simulation
problems by on-the-job-training. Moreover, there doesn’t
exist any consensus of simulation as a discipline. The
following questions hence emerge:

The panel collects 6 simulation professionals from
educational institutions that currently offer simulation
programs, and non-educational organizations with interests
in simulation education. The objective is to address issues
related to the growth and need of degree programs in simulation. The panel members from academia, enterprises

What are the reasons for shortages of modeling
and simulation professionals?
What would make simulation into a discipline?

1635

Szczerbicka, Banks, Rogers, Oren, Sarjoughian. and Zeigler

using simulation software and developers of simulation
software provide a solid basis for evaluation of the situation and formulate some recommendations for the future
direction of simulation education. In this paper we provide
their views in particular on:
reflections on Modeling and Simulation as a
discipline,
experience of academicians teaching simulation
regarding curriculum, especially universities
offering a degree in simulation,
requirements on skills of simulation professionals
formulated in application domains, and by
simulation software developers, and
requirements on content of simulation courses
coming from engineering and science. Should
simulation program be an interdisciplinary education with an independent degree on simulation, or
should be a collection of specialized, on-demand
assembled courses within an engineering degree?
should different types of students be reflected in
curricula?:

2

Computer science majors with strong
knowledge of software engineering
Engineering students with strong knowledge
in particular technical application domains
Business students, with skills in OR but without knowledge on software engineering as
well
Training programs for managers and
engineers.

assembly line or microchip fabrication facilities would not
be built today without extensive simulation of every aspect
of the proposed operation. In the development of new aircraft, parts are tested for form, fit, and hnction in a simulation before they are tumed over to manufacturing to build.
The simulation gaming industry is larger than the motion
picture industry. The military personnel offices are creating
job classifications and career paths in modeling and
simulation. Public safety organizations train for natural
disasters in virtual environments. Impacts of transportation
systems on neighborhood quality of life are simulated and
results presented at city council meetings and public
hearings. Simulation is touching all aspects of human
activity and quality of life as we move into the 2lStCentury
Despite appearing on the critical technologies lists in
the 199Os, computer simulation is a technology continuing
to struggle with a lock of focus and identify. The rapid
advances in computing technology, especially the development of high performance networks, have driven the
expectations and capabilities for simulation studies of
larger, more complex systems. However, the educational
background and technical skills to function as a professsional “simulationist” are no better supported in extant
academic programs than for the older forms: continuous
and Monte Carlo simulation (see [Rogers 19971).
2.2 Background

Both a blessing and a curse for simulation is the lack of a
circumscribing or inclusive discipline, that is, an academic
focus or home. Likewise, melding simulation and high
resolution graphics (in some cases, providing a virtual
reality environment) enables new applications but can give
users false confidence in the correctness or appropriateness
of underlying models. Ever expanding hardware capabilities entice the construction of highly complex models. If
the system being modeled is complex, or if the model
includes stochastic elements, both model creation and the
correct interpretation of simulation-produced behaviors can
require a nontrivial statistical analysis. The application of
continuous simulation in engineering for fluid flow, motion
and maneuvering, and similar physical problems relies on
solution of difference-differential equations using techniques from advanced approximation theory in numerical
analysis. While Monte Carlo simulation finds acceptance
in experimental statistics, the necessary knowledge of
random number generation, discrete structures and programming languages lie outside that discipline.
The consequence of this necessary, almost overwhelming, diversity is either the staking of limited claims by
several academic disciplines (most commonly, computer
science, industrial engineering, or management science) or
the benign neglect by all. Like “the man without a country,”
simulation floats almost invisibly in a sea of academic noncommitment. Note that this is the situation from the

SIMULATION DEGREE PROGRAMS:
NEEDS AND CURRICULUM PHILOSOPHY
(Ralph Rogers)

In 1996, Old Dominion University began a broad and
sustained effort to expand the role and vision of modeling
and simulation for the university. This lead to the creation
of the Virginia Modeling Analysis and Simulation Center,
followed by the founding in 1998 of a master’s program in
modeling and simulation and a Ph.D. program in modeling
and simulation in 2000. Motivation for establishing these
programs and the general philosophy and structure of the
programs are discussed.

2.1 Introduction
Modeling and Simulation is ubiquitous in today’s leadingedge concepts and applications in entertainment, training,
design, planning, engineering, research, education, and
decision making. Current realities include virtual environments where soldiers conduct exercises with teams around
the world without leaving their home base. New auto

1636

Szczerbicka, Banks, Rogers, Oren. Sarjoughian, and Zeigler
perspective of potential “claiming” disciplines. As appreciation of the power of simulztion has burgeoned, the perspective of “using” disciplines such as medicine, the natural
sciences and the social sciences has become even more
desperate: learn only what you need to apply a software tool
to render a “solution to the problem at hand.
Juxtapose this disciplinary characterization with the
clear evidence of societal needs in simulation. The Defense
Science Board in 1990 recognized the importance of simulation in reducing the system acquisition costs by focusing
the testing required for such decisions [Horowitz 19901.
The military and defense-related industries are becoming
more vocal in their expression of need for simulation
knowledge and expertise for both training and decision
support, while at the same time, other sectors see decision
support as the driving need. Training and the delivery of
scarce, expensive treatment in remote areas are major
motivators in medicine. Further, a rapidly expanding
entertainment industry sees simulation supporting virtual
reality as their driving technology for the next five years.
(Horizons beyond five years are unheard of in this area.).
Not surprisingly, in the last five years, primarily
driven by the military and entertainment requirements
(fuelled by the developments in computer technology), the
simulation industry has experienced explosive growth in
the demand for its products and services. These increased
product and service requirements have rippled through the
simulation industry creating increased demand for new
simulation technology, research and, especially, personnel.
I’

2.3 The Simulation Professional
Simulation professionals appear to be clearly different
from engineers, scientists, and technical managers whose
duties are closely identified with traditional academic
disciplines. Further, they are not readily identifiable in the
traditional pools of technical and managerial talent. A
typical response from simulation industry representatives
seeking simulation professional when asked what they are
seeking in an individual is, “I want someone like Joe!” Joe,
of course, is unique and his path to simulation competence
is unlikely to provide a guideline for finding other Joes.

primarily through direct trial and error empiricism or
through a corporate oral tradition of lessons learned.
What , i s surprising is that with the importance of
simulation as a fundamental tool for enterprise operations,
as a significant economic segment of the economy, with its
dependence on brain-capital, and as an increasingly
important tool in knowledge creation and discovery, most
simulation exposure in formal education programs has
been through adjunctive or elective courses and projects.
Few formal programs exists which purport to emphasize
simulation or integrate it thoroughly in the philosophy of
the curriculum. Simply, the academic infrastructure
necessary to educate and prepare the modeling and
simulation professionals or even professionals who must
use simulation is minimal, at best. The formal academic
infrastructure necessary to educate and prepare the teachers
and research leaders to staff emerging academic programs
and research laboratories does not exist at all.
2.5 Simulation Education at Old
Dominion University
Starting in 1996, Old Dominion University began a broad
and sustained effort to expand the role and vision of
modeling and simulation for the university. This expanded
role and vision included the creation of a research center
for modeling and simulation and the creation of graduate
programs focused on modeling and simulation. The
visions started to take form in July of 1997, when Old
Dominion opened the Virginia Modeling, Analysis, and
Simulation Center (VMASC). This was followed in
September of 1998 when Old Dominion University
admitted its first class of students to it new multidisciplinary Master’s program in modeling and simulation.
Efforts have continued with the Fall 2000 semester
bringing the establishment of a Ph.D. in Engineering with a
concentration in modeling and simulation.
2.5.1 Old Dominion’s Simulation Worldview
Old Dominion’s approach to simulation education is based
on the following worldview: Simulation is an artificial
experiential environment. Simulation is not the real thing.
Simulation is used as a surrogate for something else. The
goal of the experiential environments is to turn abstractions
into experiences. Modeling is the process of creating these
artificial environments in whatever media is appropriate.
We believe these environments can be classified, based on
their expected outcomes, into the three broad categories of
TraininglEducation, Discovery, and Entertainment.
TraininglEducation simulation provides an experience
with a known sought after outcome. That is, the attainment
or refinement of some known knowledge or skill. The
attainment (or lack of attainment) can be evaluated to standards. Discovery simulation is used to provide experience

2.4 Simulation Education
Today’s simulation professionals are typically a product of
the equivalent of a high-tech apprenticeship and on-the-job
training. “Simulationists” have typically evolved under the
mentoring/apprenticeship of others such as a university
researcher or organizational leader, or simply by successfully responding to pressures of competition and opportunity. While the simulationist’s education has typically been
grounded in traditional science and engineering degrees,
their breadth and depth of simulation knowledge has come

1637

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and Zeigler

degrees in an appropriate field from a regionally accredited
institution. Fields of study typically considered appropriate include science and engineering based disciplines
characterized by heavy emphasis on analytical models and
analysis, Example of fields could include all engineering
disciplines, physics, chemistry, psychology (human
factors), economics, as well as certain life and earth
sciences. The major focus of the Ph.D. degree is the
conducting of independent, original research in an area of
modeling and simulation. The major focus of the master’s
program is the preparation for professional who need to
apply simulation environments in a domain area.

when the outcome is not known but is sought to aid in the
creation of new knowledge either for its own sake or aid
decision maker in a decision process. Entertainment
simulation provides an experience where the outcome is
simply entertainment While these three categories are
useful, there are not sharp boundaries between the three.
There is overlap among all three categories. Simulation
environments can be used for all three simultaneously. At
Old Dominion, the focus is on environments for training
and education and, especially, on environments for
discovery. Of particular interest are those instances where
outcomes of the simulation environment include both
trainingleducation and discovery.

2.6 Summary
2.5.2 Simulation Program Goals
Old Dominion University has recognized the need to create
an academic infrastructure for modeling and simulation
including research and development centers and academic
programs. Old Dominion’s approach is broadly based to
recognize the multidisciplinary character and diversity of
experience required of those who pursue simulation
careers. Old Dominion’s perspective’s emphasizes an
ecumenical view of other academic disciplines and the
need to be inclusive to expand the useful applications of
modeling and simulation as well as the frontiers of
modeling and simulation knowledge.

The goals of Old Dominion’s modeling and simulation
graduate master’s programs are to educate individuals who
can: 1.) Communicate and apply a common structure,
foundation and philosophy of modeling and simulation to
advance simulation science, methodology, and technology;
and 2.) Lead in the application of simulation to address
research, societal and market needs in multiple disciplinary
problem domains. Graduates of the master’s program are
expected to provide simulation support and expertise as
part of multi-disciplinary research and development teams,
lead the development of simulation systems, and conduct
simulation studies and experiments. In addition, graduates
of the Ph.D. program are expected to conduct original and
publishable research in modeling and simulation areas and
advance the instruction of modeling and simulation at the
university level.
The Modeling and Simulation Program is a multidepartment program reflecting the interdisciplinary nature
and multi-disciplinary breadth of simulation. As such, the
program draws heavily on current faculty, course laboratories and other institutional resources such as VMASC.

2.7 References
Horowitz, Barry M. 1990. Defense Science Board recommendations: an examination of defense policy on the
use of modeling and simulation. In Proceedings of the
I990 Winter Simulation Conference, eds. 0.Balci, R. P.
Sadowski and R.E. Nance. Piscataway, NJ: Institute of
Electrical and Electronics Engineers. pp. 224-230.
Rogers, Ralph. 1997. What is a Simulationist? lndustry
sponsored workshop, Orlando, FL, February 11- 13.

2.5.3 Simulation Curriculum Philosophy

3

Both the master’s and Ph.D. program consist of three
structural elements of course work: 1) Foundation Knowledge, 2.) Simulation Common Core and 3.) Simulation
Systems Concentration. The distinguishing feature an
emphasizes a structured approach built around a common
organizing framework (i.e. Common Core) which reflects
the breadth of simulation and the relationship between subspecialties of simulation. This emphasis on a structured
approach is in contrast to simulation’s current exposure in
academia that emphasizes domain specific problems and
market specific products. Additionally, this approach
provides the synergistic mechanism to foster exploitation
and use of simulation in other domains and disciplines.
The modeling and simulation programs are available
for students who have completed bachelors and/or master’s

At the present time, there exist a shared belief among
modeling &. simulation (M&S) stakeholders (government,
industry, and educational institutions) to make M&S into a
discipline. However, there does not exist a consensus as
how to proceed to establish M&S as a discipline. Arguably,
a concerted effort is needed to find answers to the “what”,
“how to”, and “why” questions pertaining to the making of
the M&S discipline. To further prior efforts, in July 2000, a
new track on M&S Education and Training (MSET)
spanning three regular and two panel sessions was introduced as part of the Summer Computer Simulation
Conference (SCSC). The participants of the Education &
Training Track included the organizers and participants of

1638

TOWARDS MAKING MODELING AND
SIMULATION INTO A DISCIPLINE
(Hessam S. Sarjoughian and Bernard P. Zeigler)

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and Zeigler

the previous two workshops as well as new constituents
across a variety of sectors. A particular purpose for the
M&S Education & Training Track was to depict a representative picture of the current status of the education and
training in M&S from academic, government, and industry
perspectives. In this condensed writing, we provide views
presented during the MSETT and the panel sessions. In
addition, we suggest a strategic structured approach to the
realization of the M&S discipline with the expectation to
stir constructive criticism, new thoughts, and active
participation of all concerned, interested parties in this
promising undertaking.
3.1 A Brief View of M&S Education
In recent years, some members of the international
modeling and simulation community have voiced their
desire for introducing some structure into the Modeling &
Simulation (M&S) arena. One such broad-based effort was
conducted in a workshop organized by Ralph Rogers
(Rogers 1997; Rogers 2000) with more than forty participants representing industry, government, and academia
from several countries. Industry and government, in part,
represented customers of M&S professionals while academicians principally represented educators for such
professionals. In August 1998, Ruth Silverman (Yurcik and
Silverman 2000) organized another workshop focusing on
how to teach simulation to undergraduate computer science
majors. The workshop participants ranged from newly
Ph.D. graduates to authors, educators, and researchers in
the M&S arena. The workshop invitee also included simulation software developers and vendors. Both of these
workshops focused on M&S education and to some extent
on alternative educational programs within a variety of
existing disciplines such as computer science, electrical
engineering, industrial engineering, management information systems, and systems engineering.
Aside from these workshops, a number of individuals
have written on specific topics such as how to train
modeling and simulationist professionals, academic
curriculum development, and principles for creating M&S
educational programs (Fujimoto 2000; Tucker, Fairchild et
al. 2000). While such focused efforts can (and should)
significantly impact the realization of M&S as a discipline,
they are insufficient to establish M&S as a discipline
worthy of its own recognition by academia, government,
and industry. In contrast to this approach, we advocate a
top-down approach while taking into account the
contributions made from existing individual programs
some of which have opted not to label their programs
explicitly. Indeed, it is most important to bring about a
realistic vision for the establishment of the M&S discipline
supported by all stakeholders who have contributed to its
long history and its present and future.

3.2 Making M&S Into a Discipline

Unfortunately, given the existing status of M&S, it is no
simple matter to bring consensus on various topics (Rogers
1997). For example, is there a need for the so-called M&S
professionals, what would be the criteria for M&S curricula development, and what the role academia, professional
society, and industry should be? Perhaps, the chief
obstacles in instituting a common vision for M&S is its
extensive widespread use in virtually all-existing scientific
fields. Unlike more recent areas such as software engineering and robotics, everyone claims and does modeling and
simulation of some form to some extent.
During the last several years, individualistic M&S
program have been established at several universities
(California State University at Chico - McLeod Institute
Simulation Sciences, Old Dominion University, Naval Post
Graduate School, and the University of Hamburg). While
some of these programs are based on the findings of
workshops and serious discussions within the M&S
community, others appear to have forged ahead in creating
specialized areas within programs such as Computer
Science. While some of such programs server positively
the establishment of M&S discipline, unfortunately others
can have negative impact on a unified, concerted effort that
is required to overcome numerous challenges that lie
ahead. A critical view of existing programs raises a number
of issues such as the followings. What should an accredited
M&S undergraduate program be like? What are the
implications of establishing programs independently of
organizations such as ACM, IEEE, and SCSI (Society for
Compute Simulation International)? What are the
implications of an M&S discipline given the existence of
other disciplines such as various engineering and science
programs? Does M&S the province of engineering,
science, both, or perhaps something principally different?

3.3 A Strategic Approach
The impetus for making the fragmented M&S field into a
“discipline” need come from practitioners and users (e.g.,
government and industry,) tool developers (e.g., commercial entities and academia,) and theorists and methodologists (e.g., academia and research institutions). Since, we
believe there is a growing body of constituents to structure
M&S education and training, in this short writing we
suggest a preliminary comprehensive foundation appropriate for transforming M&S into a discipline. This approach
is inspired by underpinnings developed in the making of
Software Engineering into a discipline (Software 1999).
A fully recognized professional status is achieved by
prescribing to a developmental path identified by G. Ford
and N.E. Gibbs (Ford and Gibbs 1996). The ideal professsional (e.g., Electrical Engineer or an architect) is educated
by obtaining initial professional education (generally

1639

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and ZeigIer

definihons for M&S professionals

-

computer science
mathematics

system science
mgineenng
computer
clectncal
industnal
management

...

core
modeling (conhnuous. ch scret e.
simulahon algorithms

)

%ectahzatron
simulah on techno1ogles
&stnbutcdM&S paradgms
visualizah on

...

Figure 1: An infiastructure for the Establishment of M&S Discipline
Bachelorette Degree), receiving on-the-job training and
experience (skill development), obtaining license or
certificate, exercising code of ethics, and continual professsional development.
Within some professions (e.g., M&S,) government and
commercial entities (Defense Modeling and Simulation
Organization, Aegis Technologies, Cisco, Sun Microsystems, and Rational to name a few) offer short courses
and alike to educate studentlprofessionals about their
standards, tools, and methodologies. Henceforth, academic
institutions, employers (e.g., industry), professional
societies (e.g., ACM and IEEE), and commercial entities
collectively realize the necessary infrastructure in the
creation of M&S professionals.
In Figure 1 above, we sketch an infrastructure for the
establishment of M&S discipline. In this short writing, we
do not expound on key details of components and their
various existing or potential relationships. The relationships exist both vertically and horizontally. For example, a
potential debate is on whether the M&S discipline should
primarily be based in engineering education principles
alone or to include others such as science and liberal arts.
Similarly, there exist concrete interdependencies between,
for example, curriculuddegree programs and body of
knowledge. Detailed exposition of such fundamental
relationships will require in-depth discussions are subjects
of forthcoming writings. To transform current ad hoc
M&S education efforts into a cohesive, disciplined one, not
only we need to arrive at a balanced view of our field but
also work closely in collaboration with other related

disciplines to achieve short and long terms goals of the
M&S discipline.
3.4 Panel: Visions For the Future M&S
Education and Training
The development and growth of Modeling and Simulation

- as a discipline, a profession, and an industry - is strongly
bound up with the growth of education and training.
Modeling and Simulation has become a technical field that
pervades a wide cross-section of science, business and
engineering applications and projections for growth in the
hture indicate an exponentially increasing curve. Unlike
some software related tasks, education and training in
M&S is essential to enable people to carry out tasks
involving M&S with a competence that is not otherwise
possible. At this time, the offerings in education and
training are not capable of meeting the current and future
demand and there must be significant developments ,in this
regard. The panel members brought up some major
objectives to hrther this process:

0

1640

Professional modeling and simulationists must be
defined and accreditation mechanisms developed.
University degrees at the undergraduate and
graduate levels must be defined and
institutionalized.
Professional development - distinct from university degree programs - must be an essential component of the full education and training package.

a

Szczerbicka, Banks, Rogers, ren, Sarjoughian, and Zeigler

To realize each of these objectives .will require a lot of
effort.
Universities
need to characterize the discipline of M&S and
clearly delineate the discipline from the
neighboring ones such as systems engineering and
computer science/ engineering,
must work with other sources of professional
training to work out areas in which each should
concentrate and combinations of offering that
work as a coherent whole, and
must work with funding agencies to establish
programs of research and education needed to
advance the field and adequate funding for their
implementation.

Simulation Conference. Piscataway, NJ: Institute of
Electrical and Electronics Engineers.
Rogers, R. (2000). Results of the WorkshoD “What Makes
A Modeling & Simulation Professional”. SCSC,
Vancouver, Canada, SCS.
IEEE Software. (1999). Special issue on professional
software engineering. IEEE Sofbare 16(6): 13-64.
Tucker, W. V., B. T. Fairchild, et al. (2000).
Simulationists: What Does Industry Want? SCSC,
Vancouver, Canada, SCS.
Yurcik, W. and R. Silverman (2000). The 1998 NSF Workshop on Teaching Simulation to Undergraduate Computer Science Majors. SCSC, Vancouver, Canada, SCS.
4

EDUCATING THE SIMULATIONISTS
(Tuncer Oren)

4.1 Introduction

M&S-based Companies/ Corporations using M&S
must work with Universities to characterize the
current and future types of M&S professionals
they will hire and what their educational
background should be,
must coordinate their education and training
programs with those of Universities for a coherent
set of offerings,
should collaborate with Universities to establish
research teams that can respond to requests for
proposals from government funding sources.
(Each company/corporation will have a different response
to these imperatives depending on its own situation.)

The question of “education for simulation” can better be
analyzed by understanding who are the beneficiaries. It
seems to me there are two types of beneficiaries: the
(future) simulationists and other professionals who may be
benefiting from the use of simulation. Hence in this brief
presentation, the following is done: (1) a definition of the
term simulationist is given to provide a framework for
analysis, (2) the need of the simulationists is outlined to
answer the question, and (3) the complementary question,
i.e., “simulation for education and training” is analyzed to
address the educational need of simulationists who have to
be educated to offer the solutions.

3.5 Acknowledgments

4.2 Definitions

Several people including V. Amico, L. Birta, W.
Lunceford, R. Rogers, H. Szczerbicka, W. Tucker, and
W.F. Waite have provided valuable insights into M&S
education and training from a variety of perspectives. We
sincerely acknowledge contributions of the above
individuals and those who actively and enthusiastically
participated in the SCSC Education and Training Track.

Simulation is goal directed experimentation using dynamic
models. Hence, it provides repeatable experimentation
opportunities under controlled and extreme conditions. It
can be used for analysis, design, and control problems. The
aims of using simulation are, respectively, gaining insight,
performance prediction, and finding appropriate values for
input variables for a desired behavior.
Simulation has many facets and depending on the
scope of education or training, different abilities should be
acquired by fbture simulationists.
In a recent article on responsibility, ethics, and
simulation a broad definition of simulationist was given:

3.5 References
Ford, G . and N. E. Gibbs (1996). A Mature Profession of
Software Engineering. Pittsburgh, Pennsylvania,
Carnegie Mellon University.
Fujimoto (2000). Principles for M&S Education.
<http://www.sisostds.org/webletter/s
iso/iss-6l/art-299. htm>, Simulation and
Technology Magazine.
Rogers, R. (1997). What makes a modeling and simulation
professional?: The consensus view from one
workshotx In Proceedings of the 1997 Winter

“A simulationist is somebody who is involved, fulltime or part-time, with at least one of the following
activities:

-

1641

Collects and/or specifies data to be used
forhy simulation models. (In analysis
problems, by designing experiments, by
performing instrumentation, calibration, a.s.0.

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and Zeigler

-

-

-

-

-

In design problems, by providing explicit
assumptions,
by
allowing
implicit
assumptions, and by formulating and
certifying specifications).
Develops models to be used for simulation
purposes.
Engages in VV&A (validation, verification,
and accreditation) studies.
Performs simulation studies; ie. specifies
simulation problems, causes generation of
model
behaviour
and
performs
analysishnterpretation of the generated model
behaviour.
Formulates (specific or policy) solutions to
problems based on simulation.
Develops simulation software, simulation
software generators, or simulation tools.
Manages simulation projects (engineering or
administrative management).
Advertises and/or markets simulation
products and/or services.
Maintains simulation products andor
services.
Advises other simulationists.
Promotes simulation-based solutions to
important problems.
Advances simulation technology.
Advances simulation methodology and/or
theory.” (Oren 2000).

In defense applications, three types of simulations are
distinguished: live simulation, virtual simulation, and constructive simulation. The term “live simulation” reflects
the military point of view that “anything other than war is
simulation.” In a live simulation, real people operate real
systems and often use laser-simulated guns. A virtual
simulation is basically use of simulators, such as aircraft or
tank simulators for training purposes. A constructive
simulation is gaming simulation.
Gaming simulations can provide extensive experience
with complex systems where decisions of the opponents
have to be taken into account. Business games, war games,
and emerging conflict management games provide training
of decision-makers in the respective areas. Some less known
applications of use of simulation include ethical issues.
In summary, at practically every level of education
and training there are several opportunities for the
applications of simulation for education and training.
These applications of simulation can be for the education
and training of individuals or groups of people and can
work on different types of platforms: PCs, intranet, and on
Intranet. Web-based simulation as well as applications of
simulation in virtual reality and augmented reality has
several professional application areas. Some simulationists,
working alone or in groups have to deliver such solutions.
4.5 Reference
Oren, T.I. 2000.. Verantwortung, Ethik und Simulation.
(In English: Responsibility, Ethics, and Simulation).
In: R. Rimane (ed.) Gedanken zur Zeit (Invited
contribution). Translated from the original English
into German by: G. Horton. SCS Europe BVBA,
Ghent, Belgium. pp. 213-224. (An extended version is
in Press - Transactions of the SCSI).

4.3 Educational Needs of Simulationists

The educational needs of simulationists show a large
variety. The scope of activities of simulationists can be
broadened. Taking the definition as a framework, one can
perceive that different type of simulationists have different
educational needs. For some, teaching a simulation software may appear to be sufficient. Some others, may need
in-depth knowledge on modelling and simulation methodologies, techniques such as statistics, or knowledge of the
application area.
In addition of these educational requirements, simulation as a field should develop a code of ethics that has to be
part of the educational requirements of simulationist.

5

SIMULATION EDUCATION, INDUSTRY VS
ACADEMIA (Jerry Banks)

After spending all of one year in industry, I am now
qualified to discuss the differences between industry
training and academic education in the area of simulation
software. I will make this comparison using Table 1.
First, the objective of training for industry and
education for academia is different. Persons that come to
industry training usually have a pressing need for use of
the software. This keeps their interest higher than that of
students in a classroom who are learning to use simulation
software as part of a required course on simulation.
Next, the industry audience is non-homogeneous.
People come to class ranging from those with little or no
computing background to those with a thorough computing
background. This presents challenges to the instructor.

4.4 Simulation for Education and Training
Simulation can be used to support education and training in
any area where systems are dynamic andfor dynamic
models are involved. A wide spectrum of application areas
exists, such as simulation involving queuing networks -as
it is the case in most business simulations- and simulation
of systems that can be modelled by ordinary or partial
differential equations -as it is the case in most engineering
or scientific applications.

1642

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and Zeigler

Consideration
Objective

Industry
Provide training for use
products
Non-homoeeneous
Audienc
Preferred teaching method
By example
Categories of students
Managers and engineers
Go slower
Pace requested
Small
Class size
Responsibility for information Instructor
transfer
Evaluation
Little or none
Assignments
Frequency
Length of training
Quality
Use of training

Solve in class
Respond to demand
Short (one week)
Must be high
High % solve real Droblems

How to keep the audience interested and involved
when it is sometimes necessary to run in low gear? The
academic group is relatively homogenous, with the same
course material as prerequisites.
After many years of teaching simulation in academia,
and some experience with industry, it seems that the best
way to teach is by example. It is much easier to learn a
software application when it is presented in this manner.
Class attendees in industry are usually managers and
engineers. The managers attend class to see what their
employees need to be doing. Engineers come to a course
to achieve suficient competency with a tool that they can
use in practice. In contrast, the attendees in academia are
students only with virtually the same background.
In industry, the instructor is responsible for doling out
all of the information that is needed. In academia, the
student ofientimes has to dig for the necessary information.
It may be peculiar to my teaching method, but both groups,
industry and academic, say ‘slow down.’ The material that
we are teaching is quite familiar to those doing the
teaching, but when it comes at such a high transfer rate it
becomes overwhelming for the newcomer.
Class size in industry is limited so that each attendee
can receive personal attention as required. In academia,
class sizes are big in order to reduce the cost of education.
In industry, the evaluation of attendees is subjective, based
on instructor impressions. In academia, the student is
tested or has to submit solved problems to attain a grade.
In industry, the student solves assignments in class,
working alone or in a group. In academia, assignments are
given and the student is expected to complete these out of
class.
The frequency of course offering is a function of
demand in industry. In academia, although demand is
important, the availability of instructors governs the
frequency of course offerings. The length of training for

of

Academia
Support course on
aspects of simulation
Homogeneous
By example
Students only
Go slower
Large
Student

statistical

Grades
Solve out of class
Function of supply (of teachers)
One academic term
Can vary
Low % solve real Droblems

industry is typically one week or less. In academia, the
length of the instruction period is one term, or a large
fraction of that term. The quality of instruction in industry
must be high to alleviate valid complaints from the
audience. The quality of instruction in the classroom can
vary. Complaints oftentimes fall on deaf ears.
Finally, a high percentclge of those taking training in
the industry setting are going to use the software (very
soon). Otherwise, they would not be spending all of the
money to come for a week to training. Undoubtedly, a low
percentage of those taking a simulation course in academia
will ever use the software. Again, these are some of my
observations. Other persons, with varying vantage points,
may see things entirely different than me.
AUTHOR BIOGRAPHIES
HELENA SZCZERBICKA is Chair of the Institute for
Computer Science, Department of Mathematics and
Computer Science, University of Hannover, Germany. She
received her Ph.D. in Engineering and her M.S in Applied
Mathematics from the Warsaw University of Technology,
Poland. Dr. Szczerbicka was formerly Professor for
Computer Science at the University of Bremen, Germany.
She teaches courses in discrete-event simulation, modeling
methodology, queuing theory, stochastic Petri Nets,
distributed simulation, computer organization and
computer architecture. She is a Speaker of a Working
Group FG .3: Artificial Intelligence and Simulation in the
German Society for Computer Science. Her email address
is<hsz@informatik.uni-hannover.de>.
JERRY BANKS is Senior Simulation Technology
Advisor, AutoSimulations, a Brooks Automation company
in their Marietta, Georgia office. He was formerly
Professor of Industrial and Systems Engineering at Georgia

1643

Szczerbicka, Banks, Rogers, Oren, Sarjoughian, and Zeigler

framework for HLA and predictive contracts. He is a
Fellow of the IEEE.
His email is <ziegler@
ece.arizona.edu>.

Tech. He was the 1999 recipient of the Distinguished
Service Award from INFORMS/CS. His e-mail address is
<jerry-banks@autosim.com>.

RALPH ROGERS is Chair of the Department of
Engineering Management and also Director of Academic
Programs for the Virginia Modeling, Analysis, and
Simulation Center at Old Dominion University. He
received his Ph.D. in System Engineering from the
University of Virginia in 1987. He received his M.S. in
Industrial and Systems Engineering and B.S. in Electrical
Engineering from Ohio University, A 1983 and 1971,
respectively. Prior to his assignment at Old Dominion
University, Dr. Rogers was the Program Coordinator for
the University of Central Florida’s (UCF) Modeling and
Simulation Academic Initiative. Dr. Rogers has taught
courses in systems analysis, systems engineering, discreteevent simulation, object-oriented simulation, and
knowledge-based simulation. His research interests
include: systems engineering, discrete-event simulation,
simulation education, and intelligent simulation. His email
is <rrogers@odu.edu>.
TUNCER OREN has been active in simulation since
1965. His interest areas include: (1) advancement of the
methodology of simulation (including synergy of artificial
intelligence techniques such as software agents and
understanding, simulation, and system theories); ( 2 )
software systems engineering, including high-quality
user/system interfaces, computer-aided problem solving
environments, and program generators from high-level
specifications; and (3) reliability issues of modelling and
simulation, software, and AI applications. His recent
interest areas include ethics in simulation that he considers
as the missing link in VV&A studies as well as the use of
simulation in education and training for conflict
management and for peace support. He published over 300
documents, and actively contributed in about 280
conferences or seminars held in 26 countries in Europe,
Asia, and the Americas. Dr. Oren’s email and web
addresses are <tuncer@btae.mam.gov.tr> and
<http://www.btae.mam.gov.tr/-tuncer>.
HESSAM S. SARJOUGHIAN is Assistant Research
Professor of Electrical and Computer Engineering at the
University of Arizona. His current research interests are in
theory, methodology, and practice of distributed/
His email is
collaborative modeling & simulation.
<hessam@ece.arizona.edu>.
BERNARD P. ZEIGLER is Professor of Electrical and
Computer Engineering at the University of Arizona,
Tucson. He has written several foundational books on
modeling and simulation theory and methodology. He is
currently leading a DARPA sponsored project on DEVS
1644

Using Simulation to Facilitate the Study of Software Product Line Evolution1
Yu Chen†, Gerald C. Gannod‡2, James S. Collofello†, and Hessam S. Sarjoughian†
†
‡
Dept. of Computer Science and Engineering
Division of Computing Studies
Arizona State University – Tempe Campus
Arizona State University – Polytechnic Campus
Tempe AZ 85287, USA
Mesa AZ 85212, USA
{yu_chen, gannod, collofello, sarjoughian}@asu.edu
Abstract
A product line approach is a disciplined methodology
for strategic reuse of source code, requirement
specifications, software architectures, design models,
components, test cases, and the processes for using the
aforementioned artifacts. Software process simulation
modeling is a valuable tool for enabling decision making
for a wide variety of purposes, ranging from adoption
and strategic management to process improvement and
planning. In this paper, discrete event simulation is used
to provide a framework for the simulation of software
product line engineering.
We have created an
environment that facilitates strategic management and
long-term forecasting with respect to software product
line development and evolution.

1. Introduction
A software product line is defined as a set of softwareintensive systems sharing a common, managed set of
features that satisfy the specific needs of a particular
market segment or mission and are developed from a
common set of core assets in a prescribed way [8]. A
product line approach is a disciplined methodology for
strategic reuse of source code, requirement specifications,
software architectures, design models, components, test
cases, and the processes for using the aforementioned
artifacts. Software product line engineering promises
large-scale productivity gains, shorter time-to-market,
higher product quality, increased customer satisfaction,
decreased development and maintenance cost [8].
However, those benefits are not guaranteed under all
situations, and they are affected by many factors such as
the initiation situation, the adoption and evolution
approaches, the market demands, and the available
resources. The goal of this research is to develop a
1
2

simulator that facilitates software product line decision
making at an early stage by providing time and cost
estimates under various situations.
In this paper, discrete event simulation theory and
Constructive
Product
Line
Investment
Model
(COPLIMO) [2] are used to create an environment that
facilitates strategic management and long-term
forecasting with respect to software product line
development and evolution. Specifically, the simulator
facilitates the study of the effect of a number of process
decisions, including choice of evolution approach, upon
factors such as effort and time-to-market. The simulator
not only gives statistical results at the end of the
simulation, but also visually presents how major product
line engineering activities progress and interact over
time. The simulator is built upon DEVSJAVA [1], a
general-purpose Java-based discrete event simulation
framework. The tool is extensible and allows other
simulation frameworks and cost models to be used.
The remainder of this paper is organized as follows.
Section 2 presents background information. Section 3
describes the simulation model and the simulator.
Results are discussed in Section 4. Section 5 contains
related work and Section 6 draws conclusions and
suggests future investigations.

2. Background
This section describes background information on
Software Product Lines, software process simulation,
DEVSJAVA [1], and COPLIMO [2].

2.1. Software product lines
Software product line development involves three
essential activities: core asset development, product
development, and management [8].
Core asset

This material is based upon work supported by the National Science Foundation under grant No. CCR-0133956.
Contact author.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

development (domain engineering) involves the creation
of common assets and the evolution of the assets in
response to product feedback, new market needs, etc.
Product development (application engineering) creates
individual products by reusing the common assets, gives
feedback to core asset development, and evolves the
products.
Management includes technical and
organizational
management,
where
technical
management is responsible for requirement control and
the coordination between core asset and product
development activities.
There are two main software product line adoption
approaches: big bang and incremental [10]. With the big
bang approach, core assets are developed for a whole
range of products prior to the creation of any individual
product. With the incremental approach, core assets are
incrementally developed to support the next few
upcoming products. In general, the big bang approach
has a higher return on investment but involves more
risks, while the incremental approach has lower entry
costs but higher total costs. The four common software
product line adoption situations are: independent,
project-integration, reengineering-driven, and leveraged
[10]. Under the independent situation, a product line is
created without any pre-existing products. Under the
project-integration situation, a product line is created to
support both existing and future products. Under a
reengineering-driven scenario, a product line is created
by reengineering existing legacy systems. And the
leveraged situation is where a new product line is created
based on some existing product lines.
Some common product line evolution strategies are:
infrastructure-based, branch-and-unite, and bulkintegration [10]. The infrastructure-based strategy does
not allow deviation between the core assets and the
individual products, and requires that new common
features be first implemented into the core assets and
then built into products. Both the branch-and-unite and
the bulk-integration strategies allow temporal deviation
between the core assets and the individual products. The
branch-and-unite strategy requires that the new common
features be reintegrated into the core assets immediately
after the release of the new product, while the bulkintegration strategy allows the new common features to
be reintegrated after the release of a group of products.

2.2. Simulation
A software process is a set of activities, methods,
practices, and transformations that people use to develop
and maintain software and associated products, such as
project plans, design documentations, code, test cases,
and user manuals [4]. Adopting a new software process is

expensive and risky, so software process simulation
modeling is often used to reduce the uncertainty and
predict the impact.
Software process simulation
modeling can be used for various purposes and scopes,
and have been supported by many technologies [3]. The
software product line process simulator described in this
paper is for long-term organization strategic
management, and is implemented in DEVSJAVA [1], a
Java implementation of the Discrete Event System
Specification (DEVS) modeling formalism [1].
The external view of a DEVSJAVA model is a black
box with input and output ports. A model receives
messages through its input ports and sends out messages
via its output ports. Ports and messages are the means
and the only means by which a model can communicate
with the external world. A DEVSJAVA model is either
“atomic” or “coupled”. An atomic model is undividable
and generally used to build coupled models. A coupled
model consists of input and output ports, a finite number
of (atomic or coupled) models, and couplings. The
couplings link model ports together and are essentially
message channels. They also provide a simple way to
construct hierarchical models.
To execute atomic and coupled models, DEVSJAVA
uses distinct atomic and coupled simulators that support
incremental simulation model development. These
simulators can execute in alternative settings (i.e.,
sequential, parallel, or distributed). An important feature
of the DEVS framework is the ability for models to
seamlessly execute either in logical or (near) real-time.
Furthermore, due to its availability of its source code and
object-oriented design, DEVSJAVA can be extended to
incorporate domain-specific (e.g., Software Product Line)
logic and semantics.

2.3. COPLIMO
In the simulator, COMPLIMO [2] is used as the cost
model to provide cost estimates. COPLIMO is a
COCOMO II [9] based model for software product line
cost estimation, and has a basic life cycle model and an
extended life cycle model. The basic life cycle model has
two sub-models: a development model for product line
creation and a post-development model for product line
evolution. Although the basic life cycle model has many
simplification assumptions, it is thought to be good
enough for early stage product line trade-off
considerations [2]. The basic model also can be easily
extended to the extended life cycle model, which allows
products have different parameter values instead of the
same values. In the implementation, the cost model is
designed as a plug-in model, thus other cost models can
be plugged in to meet other needs.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

3. Approach
A simulation framework and a software cost model
were used to develop the simulator.
Although
DEVSJAVA [1] and COMPLIMO [2] are currently used,
they can be replaced by other suitable simulation
frameworks and cost models. This section presents the
abstract software product line engineering model, the
specifics of the simulation models, the assumptions, and
the simulation tool.

products are developed by reusing the existing core
assets, and existing products are updated after the change
of the core assets. Figure 3.1 depicts the process flow.
Costs associated with this approach include core asset
development costs, new product development costs, and
existing product maintenance costs. Compared to the big
bang approach, the incremental approach has higher
product development costs because of the incompleteness
of the core assets, and extra product maintenance costs as
the result of a short-term planning penalty.

3.1. Abstract product line model

Figure 3.2. SPL evolution approaches
Figure 3.1. SPL adoption approaches
Software product line engineering typically involves a
creation phase and an evolution phase [10]. Currently
the simulator provides two options (big bang and
incremental) for the creation stage and two options
(infrastructure-based and branch-and-unite) for the
evolution stage. In the following, we will discuss the
costs associated with those cases in detail.
With the big bang adoption approach, core assets are
first developed to meet the requirements for a whole
range of products. Products are then developed by
reusing the core assets [10]. Figure 3.1 illustrates the
process flow. Costs associated with this approach
include core asset development costs and new product
development costs.
With the incremental adoption approach, the core
assets are incrementally developed to meet the
requirements of the next few upcoming products, new

With the infrastructure-based product line evolution
strategy, the process for building a new product is the
following: core assets are updated by incorporating new
common requirements, and the new product is developed
and existing products are updated. Figure 3.2 shows the
process flow. The COPLIMO [2] basic life cycle model
assumes that a change to a product causes the same
percentage of change on reused code, adapted code, and
product unique code. So if the change rate caused by
new product requirements is α, then the costs for one
product development iteration include the costs of
maintaining the core assets with a change rate of α, the
costs of developing the new product, and the costs of
maintaining existing products with a change rate of α.
With the branch-and-unite product line evolution
strategy, the process for building a new product is the
following: the new product is developed, core assets are
updated to incorporate new common features, and
existing products are updated (including the newly

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

created product). Figure 3.2 illustrates the process flow.
If α is the change rate caused by new product
requirements, then the costs for one product development
iteration include the costs of developing a new product
with (1-α) percentage of the reuse rate, the costs of
maintaining the core assets with a change rate of α, and
the costs of maintaining existing products (including the
newly created one) with a change rate of α. Product
maintenance costs are higher in this case because it has
one more product to update, the newly created one. The
new product is first created with new features that are not
supported by the core assets, then after the core assets are
updated to incorporate the new features the new product
needs to be updated to keep consistent with the core
assets. The new product development costs are also
higher with this approach, because of the lower reuse
rate.

project cannot be started until the requested resources are
granted from the Employee Pool. If the number of
employees in the employee pool is not less than the
requested number of employees, the requested amount of
employees will be granted. Otherwise, if the number of
available employees meets the minimum employee level
(a model parameter, between 5/8 and 1), then the number
of available employees will be granted. In that case, a job
can be started with fewer resources but longer
development time. In other cases, the employee pool will
not grant any resources until enough resources are
returned.

3.2. Model development
Twelve DEVSJAVA [1] models were developed to
model software product line engineering activities.
Figure 3.3 shows a UML diagram depicting the
hierarchical structure of the model.
Some time
constraints are imposed in the simulator: for each atomic
model, jobs are processed one by one in FIFO order (or in
combination with some priority).
The PLPEF (Product Line Process Experimental
Frame) is the top level coupled model and contains a
Product Line Process instance and an Experimental
Frame instance.
The Product Line Process models software product
line engineering activities. It contains an instance of
Technical Management, Core Asset Development, and
Employee Pool, and a finite number of Product
Development instances.
The number of Product
Development models to be included depends on the
number of projected products in the product line. The
Product Line Process receives market demands and
dispatches them to Technical Management. It sends out
requirements (generated by Technical Management and
Maintenance Requirement Generator) and development
reports (generated by Core Asset Development and
Product Development), which can be used for process
monitoring.
The Employee Pool models human resource
management. It receives resource request and resource
return messages, and sends out reply messages to grant
resources.
Currently, Employee Pool manages the
resource requests in either a pure FIFO manner or a
FIFO manner where new development jobs are given
higher priority. Before starting any development activity,
a resource request must be sent to Employee Pool. A

Figure 3.3. Model hierarchical structure
The Product Development models the application
engineering activity. It has a Development instance for
product creation and inter-product synchronization
(development instance), a Development instance for
inner-product maintenance (maintenance instance), and a
Maintenance Requirement Generator instance. When the
Product Development gets the first requirement, the
development instance starts product creation, once that is
done, the Maintenance Requirement Generator sends out
a requirement to maintain the product for N years (the
number of years in the product life cycle), which starts
the maintenance instance.
After N years, the
Maintenance Requirement Generator sends out a stop
message, which stops the maintenance activity and the
acceptance of new development requirements.
The Development models a general software
engineering activity. When a new requirement is
received, Development sends a resource request to the
Employee Pool, waits for the reply from the Employee
Pool, starts developing activity when the resources are
granted, then returns resources to the Employee Pool and
sends a report to Technical Management upon
completion. The Development model will stop accepting
new requirements when it receives a stop message on its
stop port, which means the product reaches the end of its
life cycle and needs to be phased out.
The Maintenance Requirement Generator models
product maintenance requirement generation. Once a
new product is released, it sends out a requirement to
maintain the product for N years (the number of years in

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

the product life cycle), and sends a stop message when
the product reaches the end of the product life cycle.
The Core Asset Development models domainengineering processes. Currently, it is modeled in the
same way as the Development model. The domain
engineering is not modeled as the same as the application
engineering, because in practice technical management
often collects the core asset feedback from product
development and issues the core asset requirements in the
context of product development.
Table 3.1. Behavior of technical management
Stage
Creation

Approach
Big bang

Incremental

Evolution

Infrastructure
-Based

Branch-andUnite

Activities
1. Create core assets if they
do not exist
2. Create new product by
fully reusing core assets
1. Increase core assets if
necessary
2. Create new product by
fully reusing core assets
3. Update existing products
1. Update core assets
2. Create new product by
fully reusing core assets
and updating existing
products (excluding the
newly created product)
1. Create new product by
partially reusing core
assets
2. Update core assets
3. Update existing products
(including newly created
product)

The Technical Management models requirement
generation and control as well as the coordination
between core asset and product development. It receives
market demands (which are processed in FIFO order),
generates requirements for core asset or product
development, and accepts development reports. Which
requirements will be generated, when will they be
generated, and where they will be sent depend on the
selected product line adoption and evolution approaches.
Technical Management coordinates core asset
development and product development activities through
keeping the requirement generation in a certain order.
Table 3.1 summaries the behavior of Technical
Management according to the given strategies.
The Experimental Frame consists of a Market
Demand instance and a Transducer instance. It feeds
Product Line Process with inputs and receives Product
Line Process’ outputs.

The Market Demand models the demands for new
products from the market. It sends out a new product
request after a certain interval, which can be set through
the model parameter, “interval”.
The Transducer observes product line engineering
activities for a certain amount of time. During the
observation period, it receives development requirements
and reports, and tracks the generating and finishing time
of each requirement. At the end of a simulation run it
will output some statistical information to a file.

3.3. Assumptions
In the simulator, we made a number of assumptions as
follows.
1. All the employees have the same capability and can
work on any project.
2. If task B needs to make use of the results from task
A, task B cannot start until task A is finished.
3. Product maintenance starts right after the release of
the product and the maintenance activity holds the
staff until the product is phased out.
The assumptions made by the COPLIMO [2] basic
life cycle model are:
4. All products in the product line have the same size,
the same fractions of reused (black-box reuse) code,
adapted (white-box reuse) code, and product unique
code, and the same values for cost drivers and effort
modifiers.
5. For each product in the product line, the size, the
values of cost drivers, and the values of effort
modifiers remain constant over time. For each
product, the fractions of reused code, the fractions of
adapted code, the fractions of product unique code
remain constant during the time when core assets
stay the same.
6. A change to a product will cause the same
percentage of change to reused code, adapted code,
and product unique code.
Assumption 2 states that concurrency between
interdependent tasks is not supported in the current
version, which will be supported to some extent in the
future. Assumption 4 can be relaxed by using COPLIMO
[2] extended life cycle model, which allows products to
have different parameter values. Assumption 5 can be
relaxed by allowing users to specify the change trend.
Assumption 6 can also be relaxed by allowing users to
specify the change rate on different portions of the
products. Because COPLIMO is currently used as the
underlying cost model, its assumptions are adopted in the
simulator.
If another cost model is used, these
assumptions would be replaced by those made by the new
cost model.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Figure 3.4 Simulation tool in execution

3.4. Simulation tool
The simulation tool was developed in Java and runs in
the DEVSJAVA [1] environment. Figure 3.4 shows the
user interface. The upper part of the interface shows the
current running model and its package, which are
“PLPEF” and “productLineProcess”, respectively. The
middle part shows the models and their hierarchical
relationships.
The bottom part contains execution
control components. The “step” button allows running
the simulator step by step, the “run” button allows
executing the simulator to the end, and the “restart”
button allows starting a new simulation run without
quitting the system. The “clock” label displays the
current simulation time in the unit of months, and
selecting the “always show couplings” checkbox will
allow couplings between models to be displayed. The
simulation speed can be manipulated at run time to allow
execution in near real-time or logical time (slower/faster
than real-time).

Figure 3.4 shows that at time 27.783, Core Asset
Development is idle, Products 1 is under initial
development, Product 2 and 3 are waiting for resources,
and Products 4 and 5 are in planning. The messages tell
that Product 2 just received requested resources and
Product 3 just sent out a resource request. Because of the
lack of resources, the Employee Pool cannot grant the
requested resources and is waiting for more resources to
be returned, which in turn puts Product 3 in wait.
Technical Management is idle, Market Demand
generates new product requirement in every 12 months.
Finally, and Transducer is observing the simulation.
This case shows a situation where limited resources cause
development activity delay.
At the end of each simulation run, a result table is
generated similar to Table 3.2. The table has two
sections that are divided by a blank line. The top section
lists the created products, their first release time (FR),
time-to-market (TTM), initial development effort (DPM),
initial development time (DT), accumulated development
and
maintenance
effort
(APM),
accumulated

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

TPM 2810.1
FT
220.1
APM
153.2
TR
20
ATTM
44.9

1
2
3
4
5
6
7

4. Results
In this section some simulation cases are presented to
illustrate the use of the simulator and to demonstrate the
analytical capability of the simulator.

4.1. Overview
The inputs to the simulator include general
parameters and product (core asset) parameters. The
general parameters are used to describe software product
line process attributes and organization characteristics.
These parameters include the maximum number of
products that will be supported by the product line, the
number of products that will be created during the
creation stage, the product line adoption and evolution
approaches, the number of employees in an organization,
and the market demand intervals. The product (core
asset) parameters are primarily determined by the
employed cost model (COPLIMO, in this case). These
parameters include the size of the product (core assets) in
source lines of code, fraction of product unique code,
fraction of reused code, fraction of adapted code,
percentage of design modified, percentage of code
modified, and percent of integration required for
modified software. In addition, a number of parameters
related to reuse and maintenance are included, such as
software understanding of product unique code, software
understanding of adapted code, unfamiliarity with

12
12
12
12
12
12
12

50
40
30
50
50
50
50














Branch-and-unite

AT FR
50.5
3
159.0
4
159.0
4
159.0
4
149.6
3
140.3
2

Infrastructure-based

APM
651.6
443.0
443.0
443.0
424.2
405.4

Resources

DT
27.8
20.3
20.3
20.3
20.3
20.3

Scenario

core
p01
p02
p03
p04
p05

DPM
582.2
217.7
217.7
217.7
217.7
217.7

Incremental

Table 4.1. Scenarios

TTM
27.8
48.1
36.1
44.4
43.8
52.1

Big bang

Table 3.2. Simulation Result
FR
27.8
48.1
48.1
68.4
79.8
100.1

Single product only

product unique code, unfamiliarity with adapted code,
and average change rate caused by new market demands.
To study the effect of resources, adoption approaches,
and evolution approaches on software product line
engineering, we ran the simulator seven times using the
same basic parameter values. Accordingly, we varied the
number of resources, the type of adoption approach, and
type of evolution approach.

Market Demand Interval

development and maintenance time (AT), and the
number of finished requirements (FR). The bottom
section summarizes the total product line evolution effort
(TPM), the time when all the requirements are finished
(FT), the average annual development effort (APM), the
number of requirements generated (TR), and the average
time-to-market (ATM). The unit of effort is personmonths and the unit of time is months.






4.2. Effect of resources
The inputs to Scenarios 1, 2 and 3 only differ in the
values of number of employees (50, 40, and 30,
respectively). The results differ in time-to-market, as
shown in Figure 4.1. At the beginning, there is little
difference, as time progresses, the gap of time-to-market
between
resource-constrained
and
non-resourceconstrained scenarios increases. The reason is that as
more products are developed, more resources for product
maintenance are required, thus less resources are left for
new product development, which may increase the
resource waiting time and in turn result in a longer timeto-market. The effort associated with Scenario 3 is
smaller than the other two cases. That is because in
Scenario 3, when Product 10 is released at 202.69,
Product 1 and 2 are already phased out (at the time
168.1). Accordingly, no effort is needed to update those
two products due to the change of the core.

4.3. Effect of adoption approach
The inputs to Scenarios 1 and 4 only differ in product
line adoption approach (big bang and incremental,
respectively). The results differ in time-to-market, as
shown in Figure 4.2. As specified by the inputs, the core

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Time-to-Market

100
80
60
40
20
0
1

2

3

4

5

6

7

8

9

10

Products
numEmp=30

numEmp=40

numEmp=50

Time-to-Market

Figure 4.1. Effect of resources
60
50
40
30
20
10
0
1

2

3

4

5

6

7

8

9

10

9

10

Products
Big Bang

Incremental

Figure 4.2. Effect of adoption approach

Time-to-Market

100
80
60
40
20
0
1

2

3

4

5

6

7

8

Products
Branch-and-unite

Infrastructure-based

Figure 4.3. Effect of evolution approach

Time-to-Market

120
100
80
60
40
20
0
1

2

3

4

5

6

7

8

9

10

Products
Big_Inf

Big_Bra

Inc_Inf

Inc_Bra

Tra

Figure 4.4. Effect of combined adoption and evolution approach

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

assets are developed in two steps with the incremental
approach. The first increment happens right before the
development of Product 1, and implements half of the
core assets. The second increment happens right before
the development of Product 4, and implements the rest of
the core assets. For the first three products, the
incremental approach appears to have shorter time-tomarket, mainly because fewer core assets means less time
is required for asset development. As request for Product
4 comes, with the incremental approach, the development
of the new product can not be started until the rest of the
core assets have been implemented. So, we see a big
jump in time-to-market from Product 3 to 4. The
incremental approach results in higher total effort
(6173.36) than the big band approach (5338.47). That is
the nature of its process flow.

4.4. Effect of evolution approach
The inputs to Scenarios 1 and 5 differ in product line
evolution approach (infrastructure-based and branch-andunite, respectively). Figure 4.3 shows the comparison of
the results in time-to-market. As specified by the inputs,
the evolution stage starts from Product 7. For Product 7,
the branch-and-unite approach has smaller time-tomarket because the product gets developed earlier and
does not have to wait for core asset updates. For the later
products, the branch-and-unite approach results in longer
time-to-market because it requires extra effort to rework
new products and imposes more task dependencies, thus
reducing concurrency. The total effort of the branchand-unite approach (5340.37) is only a slightly higher
than the infrastructure-based approach (5338.47). That
is because when Product 9 and 10 are released, some
early products have already been phased out, so the costs
for updating existing products are reduced.

4.5. Effect of adoption and evolution approaches
A situation an organization might face is the need to
determine which software development and evolution
approaches best fit its goals. Scenarios 1 and 4 – 7 show
the alternatives the organization might have. Scenario 7
is the case where a traditional software development
approach (single product only) is taken, where products
are created and evolved independently.
Figure 4.4 shows the comparison of the results in
time-to-market. As we can see, the big bang with
infrastructure-based approach has the shortest average
time-to-market, and the incremental with branch-andunite approach has the longest average time-to-market.
The traditional approach has the shortest time-to-market
for the first two products, the longest time-to-market on

the third product, then its time-to-market stays between
the incremental and the big bang approaches, afterwards
its time-to-market starts climbing dramatically but still
stays in between the branch-and-unite and infrastructurebased approaches. In our experiment, the reuse rates are
not very high (30% for both black-box and white-box
reuse) and the product is relatively small (100KSLOC),
so the traditional product development time is only
slightly longer (about 5 months) than product line
engineering approaches. In the case of branch-and-unite
evolution approach, the dependencies imposed by that
approach overweighs the benefits of reusing the core
assets. The total effort of Scenario 1 and 4-7 are
5338.47, 5340.37, 6173.36, 6194.03, and 8760.55,
respectively. As we have expected, the traditional
approach requires considerably more effort. By largescale reuse, product line approaches generally result in
smaller code size to development and maintain. Thus,
the total effort on creating and evolving the products in a
product line is smaller.

4.6. Validation of model and results
Several steps have been taken to verify and validate
the model. First, the results of the simulator have been
compared with the results of COCOMO II [9] to make
sure the mathematic calculations are correct, and the
results are the same (ignoring rounding errors). Second,
the results of the simulator have been compared with the
common knowledge about the product line, and we feel
the results confirm to the common knowledge. Third, a
initial small set of experts have reviewed the simulation
results, and they feel that the results are consistent with
what have been observed in the real world and the
abstract model reflects the real process flow at a high
level. In future investigations, we plan to continue
soliciting expert feedback and compare simulation results
with real product line data.

5. Related work
Cohen [7] presents an approach for making a software
product line investment determination. The approach
uses three factors to justify software product line
investment:
applications,
benefits,
and
costs.
Applications include the number of projected products in
the product line, the time they will be developed, and
their annual change traffic; benefits consists of the
tangible and intangible goals the organization wishes to
achieve through a product line approach; costs are the
life cycle costs associated with core assets and individual
products. Costs are affected by some factors, such as

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

costs of reuse, degree of reuse, and core assets change
rate. Our cost estimation method is consistent with the
Cohen approach but provides more capabilities.
Regnell et al. use a simulator to study a specific
market-driven requirement management process [5].
The goal of simulator is to help in exploring bottleneck
and overload situations in the requirement engineering
process, investigating which resources are needed to
handle a certain frequency of new requirements, and
analyzing process improvement proposals. The specific
process is modeled using queuing network and discrete
event simulation [6]. Our simulator also uses discrete
event simulation, but its purpose is to study life cycle
issues for a product family instead of a portion of a
software engineering process for a single product.
Riva and Delrosso recently discussed issues related to
software product family evolution [11]. They state that a
product family typically evolves from a copy-and-paste
approach to a mature software platform. They point out
some issues that harm the family evolution, such as
organization bureaucracy, dependencies among tasks,
slower process of change, and the new requirements that
can break the architectural integrity. Their notion of
product family used in that paper is different from the
definition of a product line [8]. Creating a product
family by copy-and-paste is not a product line approach,
because the product line approach emphasizes a
disciplined strategic reuse, not opportunistic reuse. A
product line is actually a product family that has already
evolved to a mature software platform. Our simulation
results also show that in some cases dependencies
imposed by product line approaches result in slower
market response than the traditional software
engineering approach.

6. Conclusions and future investigations
Software product line engineering promises of reduced
cost while still supporting differentiation makes adoption
and continued use of the associated approaches attractive.
However, in order to make appropriate planning,
decision tools are necessary. In this paper, we described
a simulator that is intended to support early stage
decision-making. The simulator provides both static and
dynamic information for the selected software product
line engineering process. The statistical result generated
at the end of the simulation can be used for trade-off
analysis. Stepping through the simulator helps analyzing
product line processes, uncovering problems, and
improving the understanding of software product line
evolution.
Currently the simulation tool supports the study of
independent product line initiation using big bang or

incremental product line adoption approaches and
infrastructure-based or branch-and-unite product line
evolution strategies. Our future investigations include
providing estimates for other software product line
initiation
situations and approaches, allowing
concurrency between inter-dependent tasks to some
extent, providing probabilistic demand intervals,
incorporating other cost models, and removing a number
of the simplification assumptions. Furthermore, we plan
to validate the model by comparing the results with real
product line data and getting more expert feedback.
Also, we want to combine the simulator with an
optimization model, so users can specify their end-goal
criteria and then allow the simulator to search for the
best results.

7. References
[1] B.P. Zeigler and H.S. Sarjoughian, “Introduction to DEVS
Modeling & Simulation with JAVA(TM): Developing
Component-based
Simulation
Models”,
2003,
http://www.acims.arizona.edu/SOFTWARE/software.shtml.
[2] B. Boehm, A.W. Brown, R. Madachy, and Y. Yang, “A
Software Product Line Life Cycle Cost Estimation Model”,
USC, June 2003
[3] M. I. Kellner, R. J. Madachy, and D. M. Raffo, “Software
Process Modeling and Simulation: Why, What, How”, The
Journal of Systems and Software, April 1999, pp. 91-105.
[4] M. Paulk, et al., “Key Practices of the Capability Maturity
Model”, Version 1.1, Tech. Rept. CMU/SEI-93-TR-25,
Software Engineering Institute, Feb 1993.
[5] M. Höst, B. Regnell, et al, “Exploring Bottlenecks in
Market-Driven Requirements Management Processes with
Discrete Event Simulation”, The Journal of Systems and
Software, Dec 2001, pp. 323-332.
[6] J. Banks, J.S. Carson, and B.L. Nelson, Discrete-Event
System Simulation, 2nd Ed., Prentice Hall, Aug 2000.
[7] S. Cohen, “Predicting When Product Line Investment Pays”,
Proceedings of the Second International Workshop on Software
Product Lines: Economics, Architectures, and Implications,
Toronto Canada, 2001, pp. 15--18.
[8] P. Clements and L.M. Northrop, Software Product Lines -Practices and Patterns, Addison-Wesley, Aug 2001
[9] B. Boehm, B. Clark, E. Horowitz, C. Westland, R.
Madachy, and R. Selby, “Cost Models for Future Software Life
Cycle Processes: COCOMO 2.0,” Annals of Software
Engineering Special Volume on Software Process and Product
Measurement, Science Publishers, Amsterdam, The
Netherlands, 1995, pp. 45 - 60.
[10] K. Schmidt and M. Verlage, “The Economic Impact of
Product Line Adoption and Evolution”, IEEE Software, Jul/Aug
2002, pp. 50-57.
[11] C. Riva and C.D. Rosso, “Experiences with Software
Product Family Evolution”, Proceedings of International
Workshop on Principles of Software Evolution, Helsinki
Finland, Sep 2003, pp. 161-169.

Proceedings of the 7th International Workshop on Principles of Software Evolution (IWPSE’04)
1550-4077/04 $ 20.00 IEEE

Validation of Service Oriented Computing DEVS Simulation Models
Hessam S. Sarjoughian and Mohammed Muqsith
Arizona Center for Integrative Modeling & Simulation
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, Arizona, U.S.A
{sarjoughian, mmuqsith}@asu.edu
Dazhi Huang and Stephen S. Yau
Information Assurance Center
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, Arizona, U.S.A
{dazhi, yau}@asu.edu
Keywords: Co-design, Service based software system, SOAComplaint DEVS, SOC-DEVS, verification & validation.

Abstract
In the simulation based design and development of Service
Based Software Systems (SBS), it is important to analyze the
models for correctness and improve confidence in the validity of the model. Of particular interest is the SBS simulation where models are used towards evaluation of time dependent QoS metrics (e.g., service delay, throughput etc.). In
this paper, an experiment based validation of Service Based
Software Systems using SOC-DEVS (i.e. Service Oriented
Computing DEVS) framework is presented. As formal basis towards validation, the real system under consideration
is abstracted as a DEVS I/O System model and experiments
are developed to observe its time-based I/O trajectories. An
exemplar Voice Communication System is simulated and results are analyzed towards model validation w.r.t. a real system prototype.

1.

INTRODUCTION

Evaluation and trade-off studies for alternate architectural designs are integral to the design and development of
complex software-based systems [11]. To attain a level of
tractability in developing such systems, the use of modeling
and simulation tools has attracted researchers in supporting
evaluation of designs that are generally impractical with real
testbed [5, 7, 9, 14].
Current research efforts are underway to develop simulation models that can aid in the design of Service Oriented Systems (SOC) [6]. For example, the SOC-DEVS simulation environment [4] can be used to model and simulate time-based
dynamics of service-based software systems [10]. Although
the model abstractions provided in SOC-DEVS are specified
according to the SOA principles and the parallel DEVS modeling formalism (and more specifically grounded in SOAcompliant DEVS [13] and DEVS/DOC [7]), they need to be

augmented with application specific domain knowledge to be
used. For example, a Voice Communication system (VCS)
can be developed using SOA architecture [19]. To develop a
VCS simulation model, particulars of the system and its components (e.g., processor, voice service, and communication
link) must be specified. As an example, a publisher service
should provide voice data for subscriber services. Formulation of the functions for the voice publisher service must be
correct and parameterized at some desired level of abstraction
independently and as part of a voice communication system.
The importance of model verification and simulation validation are well established [8]. Verification and Validation
(V&V) concepts and approaches exist (e.g., [1, 3, 12, 16, 20].
The basis provided should be extended for specific application domains. In terms of service oriented systems with emphasis on simulating their time-based dynamics, it is necessary to devise an approach where I/O behaviors of the simulated models can be validated against its real system counterpart. This requires instrumenting a real system (e.g., Voice
Communication System) such that data containing timing information can be measured. The obtained data can then be
used to parameterize the VCS simulation models. This supports a common approach for validating not only the logical
operations of the simulation, but also quality of service attributes such as delay and throughput. A key consideration
in our approach is to identify simulation behaviors that can
be considered acceptable (i.e., fall within normal operations)
and thus may help conditions under which the system fail to
operate as expected.
Therefore, the objective of this research has been to devise
an approach that can support validation of SOC-DEVS simulation models. The use of DEVS I/O System [20] concept
in model validation is not new, however applying the concept
in SOC system model validation with detailed timing data
collected from real and simulated system has been not been
used.
The paper organization is as follows, background is in Sec-

tion 2 followed by a brief overview of Voice Communication
System under consideration in Section 3. In Section 4, aspects
of model validation is outlined followed by the overview of
experiment testbed and measurements of interest in Section 5.
Results and analysis are described in Section 6. Finally in
Section 7, conclusions and future research is presented.

2.

BACKGROUND

Service Oriented Computing (SOC) [11] is an attractive
approach for developing enterprise distributed software systems. It emphasizes loosely coupled, protocol independent
distributed system platform with the software as service concept – a self-contained component provided as a publishable
contract for use by independent subscribers. SOC has evolved
to address the demand to develop & deploy large-scale software systems that are cost effective to reuse, maintain and
easily adaptable to infrastructure change [6]. As design decisions spanning software, hardware, and their combination
have significant roles in achieving the desired runtime QoS
[2] – early architectural design evaluation of such systems
is important for system architects and designers (e.g., systems engineer, software engineers, and hardware engineers)
as it provides insight on the system dynamics and the performance.

2.1.

SOA

Service Oriented Architecture (SOA) represents architectural design principles for developing SOC systems. It is
founded on the concept of loosely coupled computational
components called “services” that provide service to interested clients with the help of a broker service [6]. All the
software resources in SOA are termed as services. They are
defined using standard languages (e.g., Web Service Description Language (WSDL)) [17], provide publishable interfaces,
and interact with each other to collectively execute a common task. Each service is independent of the state and context
of other services. Furthermore, the interaction and communication is done using protocol independent message schemes
(e.g., Simple Object Access Protocol (SOAP)) [15].

2.2.

through simulation of Service Based Systems with services
distributed over networked hardware. Characterization of codesign for Service Based Software System, entails modeling
and simulation for high-level specification of software and
hardware layers as well as separate mappings of the former
to the latter.

SOA
Compliant
Service
Service
Based
System

Partition

Flexible
Map

Hardware

Figure 1. Basic concept for SBS HW/SW co-design
The SOC-DEVS simulation models contain appropriate
details required for architectural design verification and validation. The SOC-DEVS model is conceptualized, formulated, and developed as two modeling layers which consist
of a software layer and a hardware layer as illustrated in Figure 2). The software service (swService) in the software layer
is specified to account for basic service interaction semantics (e.g., message exchanges and service invocations). The
swService accounts for atomic service concept. The hardware layer is modeled to represent computing node (e.g.,
CPU speed and memory capacity) as well as network system resources (e.g., network bandwidth) that are important
for modeling composite service execution. Software service
interactions is supported through hardware components using the concept of jobs and messages. The jobs capture CPU
resources (CPU cycles and memory) consumed for software
service execution and the messages represent the software
service to software service communication.

SOC-DEVS Simulation Models

SOC-DEVS is a co-design methodology towards Service
Based Software System modeling and simulation. The emphasis in SOC-DEVS is on the introduction of the concept
of co-design in Service Based Software System design (see
Figure 1) to address the lack of hardware representation in
SBS. Use of co-design concept in modeling Service-Based
Software Systems with representation of service and network
hardware allows a modeler to separately specify service and
hardware models with flexible synthesis capability to observe system behavior under different configurations. SOCDEVS is designed to support architectural design evaluation

Figure 2. Service interactions in Networked Hardware in
SOC-DEVS
In the design of SBS, the networked HW/SW co-design
concept allows the following:

1. Specification of SOA-compliant service as software
component and hardware components separately and establishing a well-defined relation to allow synthesis as
well as service mapping to hardware.
2. Specification of software to software interaction in order to account for limited hardware resource (e.g., CPU
speed and memory capacity).

of service-based software systems shows functional services
with QoS monitoring and adaptation systems. The QoS monitoring system collects the measurements of desirable QoS
features. Decisions provided by the QoS adaptation system
are made to adjust the configurations and service operations
of ASBS.
QoS Expectations

3. Accounts for the impact of multiple software component
interactions that are connected by a mesh of network
hardware resources (e.g., network bandwidth, router
speed, and link capacity).

2.3.

Service-based Software System

This simulation environment shown in Figure 3 supports
study of QoS for service-based software Systems. This system is built in the .NET framework. It is also suitable for
rapid system prototype development particularly for experimentation testbeds. Experimentation requiring support for
data collection from different layers of the system (i.e., device driver, kernel, application) is supported by the Windows
Performance Object (WPO) [18] related .NET Application
Programming Interfaces. Windows performance objects provides access to system component (e.g., CPU, Memory, Network Card) performance data. Applications can access performance data using WPO API set to collect system status.
Such data collection capability are important for experimentation testbed intended for system performance evaluation.
Service-based Software System demands making tradeoffs
among multiple QoS features. However, satisfying multiple
quality of service features such as timeliness, throughput and
accuracy requires modeling the logical specifications of the
services as well as their dynamic behaviors. Development
of Service-based Software Systems demands the capability
to monitor the changing system status, analyze and control
trade-offs among multiple QoS features. A conceptual view

Adaptation
QoS measurements

The components capture the basic functional and resource
capabilities of the system. The service performance is related to the hardware resources by developing an assignment
between service and hardware in terms of their interactions
and resource requirement. A flexible mapping provides assignment that specifies which service is assigned for execution onto which hardware. We define the co-design of networked services executing on distributed hardware components as two types. First, resource constraints (available CPU
processing cycles and memory) for interaction of services are
restricted to a single hardware. Second, resource constraints
(CPU time, available memory, communication bandwidth)
for interaction of services are allowed for networked hardware components. Both types of interaction are supported in
SOC-DEVS.

adaptation commands

user
produce
events

Services
SBS
consume
resources

exogenous
events

affect
QoS

Monitoring

Resources
measure changes of resource states
Adaptable Service-based Software System

Figure 3. Illustration of Service-based Software System

3.

VOICE COMMUNICATION SYSTEM

Proliferation of VoIP and digital music makes audio data
transmission a major contributor to Internet traffic. Based on
end user preference and policies of service providers, real
time audio traffic may require minimum QoS to be maintained under various network traffic scenarios. For our case
study, we consider an end-to-end Voice Communication System (VCS) capable of streaming audio data to the subscribers
[19]. The system is subject to adaptation to support various
load scenarios so that it can maintain the user specified QoS.
The VCS publishes various quality audio data specified by
sampling rates (e.g., ranging from 44.1 to 220.5 KHz) and
the interested clients can subscribe to the channels with certain QoS constraints on the data quality. The higher sampling
rates produce higher quality audio data as it encodes more
audio information per second. For example, sampling rate
of 220.5 KHz will produce superior quality audio data w.r.t.
44.1 KHz sampling rate. The VCS under consideration supports 2 channel (i.e. stereo) audio data that can be sampled at
any of the following rates – 44.1, 88.2, 136.4, 176.4, 220.5
KHz. The subscribers (which may be added or removed at
run-time) request audio data stream for a specified amount of
time over the network and expect the VCS to ensure QoS. The
subscriber requests are processed by the VCS and it streams
audio data for the specified duration. The VCS can support
multiple subscribers simultaneously such that each subscriber
may request different quality audio data. The throughput provides a measure of the VCS performance. In general (i.e. un-

der normal operating conditions), the VCS throughput is proportional to the number of audio streams being delivered.

3.1.

Basic Attributes of VCS

The VCS publishes various quality audio data specified
by sampling rates (e.g., ranging from 44.1 to 220.5 KHz)
and the interested subscribers can subscribe to the channels
with certain QoS constraints on the data quality [19]. The
higher sampling rates produce higher quality audio data as
it encodes more audio information per second. For example,
sampling rate of 220.5 KHz will produce superior quality audio data w.r.t. 44.1 KHz sampling rate. The VCS under consideration supports a 2-channel (i.e., stereo) audio data that
can be sampled at 44.1, 88.2, 136.4, 176.4, and 220.5 KHz
rates. The subscribers request audio data stream for a specified amount of time over the network and expect the VCS
to ensure QoS. The subscriber requests are processed by the
VCS and it streams audio data for the specified duration. The
VCS can support multiple subscribers simultaneously such
that each subscriber may request different quality audio data.
The throughput provides a measure of the VCS performance.
In general (i.e., under normal operating conditions), the VCS
throughput is proportional to the number of audio streams being delivered.

4.

5.

EXPERIMENT TESTBED: REAL AND
SIMULATION

The real experimental testbed consists of a server hosting VCS service and two client machines emulating multiple
clients using threads (i.e. each thread generates a service request). The server and the two client machines are identical
in hardware configurations (2.2GHz,1024 RAM, 100 Mbps
NIC). The machines are connected by a 8 port 100Mbps
hub.The real testbed organization is designed to support basic
VCS experiments under moderate scales to observe system
behavior under normal and stressed system conditions. It allows simple and easy parameter modifications of service configurations and automates experiment runs and data collection
process. The real testbed is developed to support basic experiments with the Voice Communication System to test system
behavior under defined parameter settings. The primary parameters with the VCS service is the sampling rates (44.1 220.5 KHz) and data buffer size (16 - 48 KBytes) for any
client request. In addition, the number of simultaneous clients
(i.e. 5, 10, 15, etc.) is also considered. Data is collected for 60
wall clock seconds. For any configuration, a set of 10 runs is
collected and analyzed to have high confidence on the data
and analysis results.

• Develop a simulated instrumentation model and use it
collect data from the VCS simulation model.

To validate the SOC-DEVS models, the above Voice Communication System (VCS) is modeled. The simulation of the
VCS is supported by extending the DEVS-Suite simulator
[10] that supports SOA-compliant DEVS based software and
hardware model development. The VCS represents a number
of subscribers subscribing to a voice publisher. A model of
this system is simulated. The resultant throughput behavior
is compared to similar scenario (i.e., testbed setup with same
configuration) in the real VCS. To validate the SOC-DEVS
models, the same scenario similar to the real experiment stup
is consdered in a networked hardware system consisting of
two processors connected via a network switch. The VCS
publisher and the broker are mapped on a single processor
and the subscribers are assigned to the other processors. Each
subscriber requests voice data at 220.5 KHz sampling rate for
2 channel stereo data encoded in 16 bits. The dynamics of the
system is captured by the average voice publisher throughput
over 60 simulated logical seconds. For a comparative view,
we also run the same scenario in the real VCS system (see
Table 1) and collected the Round Trip Delay, Throughput,
Mean Failure Rate data. The simulation experiments are designed to address on the effect of sampling rate and client
number on the system QoS and system states similar to real
system setup. The results are discussed in Section 6..

• Show the degree to which to the simulated VCS is acceptable given its real world counterpart. This includes
identifying possible boundaries beyond which the simulation model may not produce realistic behaviors

SOC-DEVS software layer models generic message based
service interactions and the hardware layer is modeled to
support such interactions so that model abstractions provide
building blocks for SBS. However, like any system modeling, use of modelers domain knowledge is important. For ex-

DEVELOPING VALIDATED SOC-DEVS
SIMULATION MODELS

To develop validated simulation models for service-based
software systems and in particular VCS, the following steps
are proposed:
• Develop an instrumentation set-up to probe and collect
data from a real voice communication system that is developed as software services.
• Develop a simulation model for the Voice Communication System that is SOA-compliant. The VCS simulation
model is synthesized from software service models and
hardware models with a mapping from the services to
the hardware models.
• Parameterize the simulation model using data obtained
from its real software system counterpart.

Table 1. Experiment setup configuration for Real & Simulated system
Category
Processor
(CPU, Memory, NIC 1 )
Network Link Bandwidth
Subscriber Number
Data Collection Duration

Real System
2.2 GHz,1 GB,100 Mbps

Simulated Systems
2.2 GHz,1 GB,100Mbps

100 Mbps
1-40
60 sec (wall clock)

100Mbps
1-40
60 sec (logical clock)

ample, service interactions through hardware layer requires
domain knowledge of representative cpu load, memory load,
communication load etc. for different supported service operations. Such parameters are important for capturing resource
dependent (which impacts time related behavior) system dynamics and needs to be estimated based on domain knowledge [19]. However, it is important to note that the modeler is
relieved of capturing SOA-compliant service interactions and
the resultant system dynamics on a case by case basis. The
processor parameters in the simulation are assigned based on
real system’s CPU configuration. The CPU speed for the processor is set at 2.2 GHz and 1 GByte of memory. For processor configuration test, the CPU speed of 3.2 GHz and 1
Gbyte of memory is used. The swap penalty is set for 0.1 logical second. The network parameter is 100 Mbps which is the
bandwidth of I/O link at the network switches and network
cards.

5.1.

Measurements of Interest

5.1.1.

5.1.2. Average Data Throughput
Average Data Throughput is defined as the amount of audio data bytes sent by the VoiceComm Service over the requested service period. It is measured at VoiceComm Service
running at the server. All simultaneously active client’s aggregated data bytes sent by the VoiceComm Service is considered in calculating the throughput. If TBS bytes are sent in
time span t then, T Pavg (server) = T BS/t.

5.2.

Data Collection Process

The testbed is designed to support automated data collection process. Automated software modules are developed to
drive the experiments with predefined parameter settings and
configurations. Data collection is done both at the software
code level as well as Windows Performance Object and network packet layer traces. The Figure 5 illustrates the flow of
data collection. A special network packet trace utility (NetMon) and code level instrumentation technique is used for
data collection.
Data Collection Flow Chart
Start Experiment

Round Trip Delay

Round Trip Delay (i.e. RT Delay) is defined as the time between sending of a client request and the receiving of the first
data packet from the VoiceComm Service. The RT Delay provides a measurement of service response time and accounts
for network delay and processing delay both at the client and
server end. For example, let Ci denote the ith client, requesti
denote its service request event and receivei denote the 1st
audio data packet received from the server. Then RT Delay of
client (Ci ) is defined as, RT-Delay(Ci ) = treceivei - trequesti (see
Figure 4.

Invoke/Request Service

Start Audio Data
Output Event Logging
Audio Data
Output Event Logging
At Probe Point 1

No

Start UDP/IP Data
Event Logging
UDP/IP Data
Event Logging
At Probe Point 2, 3

Stop ?

Service
Completed?

Yes
Stop Audio Data
Output Event Logging

No

Yes
Stop NetMon
7

Figure 5. Data Collection Process

Figure 4. RT-Delay Definition

The automated process has experiment coordinators running at the server and the client sides. The client side coordinators waits for execution command from the server side.
Once the command is received at the client end it starts the
clients. Once a run is completed it sends the fished command
to the server side coordinator. Once the server side coordinator gets all the finish run signal from client end coordinators

- it continues with execution for other configurations or stops
all data collection process and terminates.
Probe Point (2,3,5)
via NetMon

Probe Point (1)
at VoiceCommService

5.4.

UDP/IP
NDIS Driver

SC API’s

NIC Driver

SC Driver

NetworkCard (NIC)

Sound Card (SC)

block that receives and sends audio data to the client end (see
Figure 6). Similarly in the client code, the instrumentation is
inserted in the data receiving code block.

OS

HW

System Status Data Using Windows Performance Object

Windows operating system provides a performance profiling capability through a set of counters known as Windows
Performance Object. These counters collect system status and
performance information in the background as part of the OS
itself. API is provided to use the counters to collect application as well as system specific data on the machine. Low
level system information (for example: IP Fragments Created
per Second, Fragments Per Second etc.) is collected through
WPO counters in the data collection process.

Figure 6. Data Probe Point Layer View
The underlying block diagram for network packet capture
and sound card interaction with the application layer is given
in the Figure 6. Once the VCS gets a audio data event from
the sound card it sends the data as UDP/IP packet which is
captured by the Netmon utility (outgoing and incoming packets).

5.3.

Code Instrumentation

Code instrumentation helps in collecting data at the application layer. The instrumented code region is minimal in nature to avoid unnecessary overheads or altering fundamental
behavior of the services and clients. Since high precision (micro/nano second granularity) is not needed for the evaluation
purpose and the instrumented code has low overhead, the data
and time stamps are accurate in millisecond range (i.e. ignoring instrumented code overhead).

5.5.

Network Packet Sniffing

Network Packet Sniffing allows packet level data observation capability. The data collection process uses Microsoft
Network Monitoring Tool NetMon 3.4. The captured network
traces at server and client end is used to verify the service
input/output and client input/output behavior. The network
packet traces allow corelating WPO counter data as well as
instrumented code level data traces to more precisely relate
data relation at different layers of abstractions. For example,
WPO’s FragmentsCreated/Second counter requires the MTU
(i.e. maximum transmission unit) value to relate to the network throughput. The packet traces provide the precise value
of MTU to be 1500 bytes with payload size of 1480 bytes. In
addition, UDP, IPv4 packet related information also provides
important details of the system that aids in building improved
abstractions for VCS.

6. RESULT AND ANALYSIS
6.1. Effect of Sampling Rate
The sampling rate defines the precision of audio data.
Higher sampling rate captures a higher audio details with
a higher audio data rate. The effective rate of data transmission is dependent on the sampling rate as it directly impacts the average amount of packets per second (and the link
layer frames/sec) sent. Higher sampling rates generates audio
frames very quickly compared to lower sampling rates. As
a result, under normal operating conditions higher sampling
rate results in higher throughput. As shown in the Figure 8,
throughput increases with increasing sampling rate.

6.2.
Figure 7. VCS Code Instrumentation
Code instrumentation is done both at the service and the
client code. For the service code, it is inserted inside the code

Effect of Simultaneously Active Clients

The VoiceComm service streams data for each concurrently active client simultaneously. As a result, under normal
operating condition, the net throughput of the system is the
aggregate of each active streams. The higher sampling rate

client number is observed in the Figure 10.

Throughput vs Sampling Rate
45

Round Trip Delay vs Client Number
Round Trip Delay (Seconds)

Throughput (Mbps)

40
35
30
25

Real
Simulated

20
15
10
5

10
9
8
7
6
5
4
3
2
1
0

Real
Simulated

5

10

0

15

20

25

30

35

40

Number of Clients (#)

44.1

88.2

132.3

176.4

220.5

Figure 10. Effect of Client on RT-Delay

Sampling Rate (KHz)

Figure 8. Average VCS throughput at various sampling rate
(5 client scenario)
requires data processing by the system at a very high rate ,
however with increasing client number the system starts to
stress due to IO processing overhead and results in saturated
average throughput with increasing clients at higher sampling
rate. The effect is shown in the Figure 8.
Throughput vs Client Number

One important aspect of the VCS system is the average
number of clients it can service simultaneously. The client request processing and sending of voice data can create request
timeouts. Under such circumstances the system would start to
deny requests. As shown in the Figure 10, the system starts to
drop client request above 25 client scenarios. The reason for
such trend is two folds: sequential processing of requests and
request timeouts. Requests that starts early begin to saturate
the system and the network due to increasing packet processing. So the remaining requests time out and gets dropped by
the system. The higher sampling rates increases the average
failure rates due to this phenomenon.

100

Mean Rate of Failure vs Client Number

80
Real
Simulated

60
40
20
0
5

10

15

20

25

30

35

40

Number of Clients (#)

40%
Mean Rate of Failure (%)

Throughput (Mbps)

120

35%
30%
25%
Real
Simulated

20%
15%
10%
5%
0%
5

Figure 9. Effect of number of clients on average throughput
The Round Trip Delay shows a high correlation to active
client number. This is due to the fact that the IIS server queues
simultaneous requests and spawns a separate thread for servicing each request. Thread spawning is a high over head activity so request processing delay can be longer. So, the later
processed requests has a higher RT-Delay as opposed to earlier ones. In the simulation models, the request is queued and
then processed sequentially. Each request processing is associated with a mean random delay in addition to the packet
delays in the routers. The mean RT-Delay of all the client averages the net effect and a nondecreasing RT-Delay w.r.t to

10

15

20

25

30

35

40

Number of Clients (#)

Figure 11. Effect of simultaneous requests

7.

CONCLUSION

In this paper, we have presented a SOC-DEVS model validation approach. It is based on evaluation and analysis of
real and simulated experiments designed to address a set of
system measurements. The experiments are executed to observe the system characteristics for both the real and the sim-

ulated scenarios. The validity of the models are based on observed simulated system characteristics considering the real
experimental data as the basis. The results show that the
SOC-DEVS model abstractions are able to capture system
behavior with reasonable accuracy for the base case scenarios. As part of future work,we plan to develop a simulation
testbed to experiment with various scenarios towards evaluation of larger scale service based system based on this validated SOC-DEVS models.
Acknowledgement: This research is supported by NSF
Grant #CCF-0725340.

REFERENCES
[1] Balci, O., 1998, “Verification, validation and accredition of simulation models”, Proceedings of the Winter
Simulation Conference, pp. 41-48, Washington DC.
[2] Bass, L., P. Clements, R. Kazman, 2003, Software Architecture in Practice, Second edition, Addison-Wesley.
[3] Davis, P.K., 1992, “Generalizing concepts of verification, validation, and accreditation (VV&A) for military
simulation”, RAND, Technical Report R-4249-ACQ,
The RAND Corporation, Santa Monica, CA.
[4] DEVS-Suite
Simulator,
2009,
http://devssuitesim.sourceforge.net/, accessed June, 2011.
[5] Edwards, S., L. Lavagno, E. A. Lee, A. SangiovanniVincentelli, 1997, Design of Embedded Systems: Formal Models, Validation, and Synthesis, Proceedings of
the IEEE, Vol. 85, No. 3, pp. 366-390.
[6] Erl, T., 2006, Service-Oriented Architecture Concepts,
Technology and Design, Prentice Hall.
[7] Hild, D.R. Sarjoughian, H.S., B.P. Zeigler, 2000,
DEVS-DOC: A Co-Design Modeling and Simulation
Environment, IEEE SMC-Part A, Vol. 32, No. 1, pp. 7892.
[8] IEEE Standard for Modeling and Simulation High Level
Architecture (HLA) Framework and Rules, 2010, IEEE
Std 1516-2010.
[9] Kurahata, H., T. Fuji, T. Miyamoto, S. Kumagai, 2006,
“A UML Simulator for Behavioral Validation of Systems Based on SOA”, Proceedings of the International
Conference on Next Generation Web Services Practices,
pp. 3-10.
[10] Muqsith, M.A., H.S. Sarjoughian, 2010, “A Simulator
for Service Based Software System Co-design”, Proceedings of the International Conference on Simulation

Tools and Techniques, pp. 1-9, Torremolinos, Malaga,
Spain.
[11] Papazoglou, M.P., Traverso, P., Dustdar, S., Leymann,
F., 2007, “Service-Oriented Computing: State of the Art
and Research Challenges”, IEEE Computer, Vol. 40,
No. 11, pp. 38-45.
[12] Sargent, R.G., 2005, “Verification and validation of simulation models,” Proceedings of the Winter Simulation
Conference, pp. 130-143, Orlando, FL.
[13] Sarjoughian, H.S., S. Kim, M. Ramaswamy, S. Yau,
2008, “An SOA-DEVS Modeling Framework for
Service-Oriented Software System Simulation”, Proceedings of the Winter Simulation Conference, pp. 845853, Miami, FL.
[14] Schulz, S., J.W. Rozenblit, M. Mrva, K. Buchenrieder,
1998, Model-Based Codesign, IEEE Computer, Vol. 32,
No. 8, pp. 60-68.
[15] Simple
Object
Access
Protocol,
http://www.w3.org/TR/soap/, accessed January 2011.
[16] Tsai, W. T., X. Zhou, Y. Chen, 2008,“SOA Simulation
and Verification by Event-driven Policy Enforcement”,
Proceedings of the Annual Simulation Symposium, pp.
165-172, Ottawa, Ontario.
[17] Web
Services
Description
Language,
http://www.w3.org/TR/wsdl, accessed January 2011.
[18] Windows
Performance
Object,
http://www.microsoft.com/resources/documentation/,
2011.
[19] Yau, S., N. Ye, H. Sarjoughian, D. Huang, A. Roontiva, M. Baydogan, M. Muqsith, 2009, A PerformanceModel-Oriented Approach to Developing, Adaptive
Service-based Software Systems, IEEE Transactions on
Service Computing, Vol. 2, No. 3, pp. 245-246.
[20] Zeigler, B.P., H. Praehofer, T.G. Kim, 2000, Theory of
Modeling and Simulation: Integrating Discrete Event
and Continuous Complex Dynamic Systems, Second
Edition, Academic Press.

Proceedings of the 2011 Winter Simulation Conference
S. Jain, R.R. Creasey, J. Himmelspach, K.P. White, and M. Fu, eds.

TOWARDS A METHODOLOGICAL APPROACH
TO IDENTIFY FUTURE M&S STANDARD NEEDS
Andreas Tolk

Osman Balci

Old Dominion University
Engineering Management & Systems Engineering
241 Kaufman Hall
Norfolk, VA 23529, USA

Virginia Tech
Department of Computer Science
3160B Torgersen Hall, MC 0106
Blacksburg, VA 24061, USA

Saikou Y. Diallo

Paul A. Fishwick

Virginia Modeling Analysis & Simulation Center
Old Dominion University, 1030 University Blvd
Suffolk, VA 23435, USA

University of Florida
Digital Arts and Sciences CISE Department
Gainesville, FL 32611, USA

Xiaolin Hu

Margaret Loper

Georgia State University
Computer Science Department
Atlanta, GA 30303, USA

Georgia Tech Research Institute
Information & Communications Laboratory
Atlanta, GA 30318, USA

Mikel D. Petty

Paul F. Reynolds Jr.

University of Alabama in Huntsville
301 Sparkman Drive, Shelby Center 144
Huntsville, AL 35899, USA

University of Virginia
151 Engineer’s Way; P.O. Box 400740
Charlottesville, VA 22901, USA

Hessam Sarjoughian

Bernard P. Zeigler

Arizona State University
Center for Integrative Modeling and Simulation
Tempe, AZ 85271, USA

RTSync Corp.
11900 Parklawn Dr. Suite 130
Rockville, MD 20852, USA

ABSTRACT
Although Modeling and Simulation is successfully applied for several decades, the community only established a handful of M&S specific standards. Although the standards were applied enabling worldwide
distributed simulation applications, in particular in the training application domain of military simulation
systems, the general success of M&S standard efforts and their potential for general applicability has been
debated repeatedly during several conferences and workshop. This collection of position statements discusses related questions, such as, “What makes M&S special that we need M&S standards,” “Are M&S
standards truly different from Software Engineering Standards,” and “What metrics can be used to measure M&S standard success,” and tries to contribute to establishing a methodological approach to identify

978-1-4577-2109-0/11/$26.00 ©2011 IEEE

2975

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
future M&S standard needs. These position statements have been contributed in preparation of a panel
discussion and edited for the supporting proceedings.
1

INTRODUCTION

When approaching the question of developing a methodological approach to identify future M&S standards, a couple of questions arise immediately, such as:
 What makes M&S standards special (if they are indeed special)?
 What categories of M&S standards are necessary?
 What phases of the M&S life cycle are supported?
 What in M&S should be standardized, and what not?
 Can we plan for a successful standard (or do they just happen)?
 Are there metrics that can help?
Internationally recognized experts with experience in Modeling and Simulation (M&S) as well as in
standardization activities were asked to contribute to a panel discussion on these and related topics. Part
of these efforts is driven by experience, some was derived from funded research on this topic conducted
in response to a task articulated by the US Congress: “What standardization activities are needed to better
support the M&S industry?”
A general look at successful standards reveals that three requirements need to be fulfilled in order to
enable a successful standard: the approach must be valuable, desirable, and reasonable. Figure 1 shows
these three pillars for a successful standard.

Figure 1: Requirements for successful standards




Valuable deals with the need for economic incentives. If the development of a standard does not
contribute to making developments and applications faster, cheaper, or better, it will not be accepted. The return of investment for a significant number of stakeholders must be obvious. If,
e.g., a company has a monopoly in a good solution protected as intellectual property, why should
they share their knowledge by making the solution a potentially open standard? If, however, a
group of industry partners build a consortium to establish a competition, they may use an open
standards to increase their overall market value by bringing new partners in via this standard.
Desirable describes the need to reach the critical mass. A very good solution for a problem that is
not yet recognized by a significant part of the community of practice to be a real problem will not
stick. Very often, the time for good ideas is simply not there yet. The graphical user interfaces of
the early Apple computer and other features of its Local Integrated Software Architecture – better
known as Apple Lisa – are examples for this syndrome: only several years after the solutions

2976

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
were provided the need for these advanced concepts was recognized and often reinvented before
standards helped to support easier integratability of components.
 Reasonable introduces academic rigor. The solution proposed for standardization must be academically sound, aligned with current research, and must be technically mature. Many standardization organizations are therefore requiring that the feasibility demonstration in form of a prototype based on the recommended standard be a part of the standardization effort.
If one of these pillars is missing, the success of the standardization effort is doubtful. In his essay on the
“Law of Standards,” John Sowa (2004) observes the following: “What has consistently failed are the
‘proactive’ attempts to design new systems from scratch that are declared to be standard before anyone
has had a chance to implement them, test them, use them, and live with them.” Instead, such attempts did
often lead to the development of much easier de facto standards. Sowa (2004) gives several examples that
could be observed in the information technology world, ranging from operation systems to programming
languages. While many arguments can be made if these close relations really exist, they all show that the
workforce not only needs to understand the problem, they will also go – similar to following Occam’s
Razor idea – for the easiest and simplest solution to solve this problem.
This panel discussion to be conducted during the Winter Simulation Conference 2011 shall help to
present constraints and research results in support of establishing a research agenda in support of a methodological approach to identify future M&S standard needs. Some of the authors of these contributions
and position papers participated in a series of workshops conducted on behalf of the US Congress and facilitated by the US Modeling and Simulation Coordination Office (MSCO), as described by Collins et al.
(2010). However, we are still in the research phase to better understand the challenge of M&S standardization. Others draw from several years of experience in this and related domains. The following sections
present related ideas and propose solutions.
 Tolk focuses on the question “What makes M&S special?” and identifies two factors: the different roles of simulation that need to be supported, and the different interoperation layers that can
be addressed by standards.
 Fishwick deals with cultural issues. He concludes that before addressing the technical maturity of
solution, a change of culture may be needed that is more directed at value of the ideas of sharing
within a common and broader M&S community.
 While the intuitive understanding in the community is that a standard is a commonly agreed solution to an existing problem, Diallo introduces the idea of identifying problem situations that are
exposing a new set of challenges.
 Balci draws the attention of the community to the challenges in standardization of M&S life cycle
processes that needs to be addressed in future activities.
 Using their experiences with DEVS, Sarjoughian, Zeigler, and Hu show the need for simulation,
modeling, and domain-specific standards.
 Based on experiences in a current study, Petty documents efforts in defining metrics for successful standards.
 Loper reports from one of the larger preparation efforts to identify standards supporting the coupling of life systems with virtual simulators and constructive simulations.
 Reynolds focuses on some limits of standards that are rooted in the computational nature of simulation systems as well as supporting standards.
2

WHAT MAKES M&S SPECIAL? (TOLK)

When shifting the general view to the more specific challenges of M&S, the different application domains
depicted in Figure 2 and the role of M&S may define what can and should be standardized. Reynolds
(2008) distinguishes two major roles for M&S: Using simulation to solve problems by providing
knowledge, and using simulation to gain insight by supporting understanding.

2977

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler

Figure 2: Application domains and M&S roles
If the simulation system represents established and validated knowledge in the form of executable
theories, it allows for a systematic evaluation of alternatives. The simulation system becomes a
knowledge base that represents known facts about the truth. The objects, processes, and relations behave
accurate enough to represent the real system. This role represents the classical view on M&S: an accurate
simulation of a real system that behaves in accordance with the underlying requirements and observations. In particular physics-based models fall into this category.
Not so the new application domain of human social cultural and behavioral (HSCB) modeling (Tolk
et al. 2010). Several alternative hypotheses coexist without clear evidence of superiority. Instead of a
clear and unambiguous representation of one common truth, the simulation systems represent interpretations in the light of the supporting theory. Instead of evaluating alternatives all following one common
view, simulations represent clearly different options and provide the basis for exploratory analysis of consequences of these options. A better known example can be the different weather models that are used to
make forecasts when hurricanes approach: each simulation system produces a projected paths, and the
common understanding of all forecasts – the map that shows the projected paths – allows to explore options and consequences.
While standards for physics-based models and other examples that support an accepted common theory on how to deal with related problems make perfect sense, standards for models routed in interpretivism
may exclude valuable options. When simulation systems are used to test equipment, the provided environment must behave exactly as expected by the test developers. Validated and verified systems are essential in this context. When conducting analyses or experimentation on the other hand, we are actually
interested in evaluating new options and behavior that may not even have a reference systems in the real
world. Simulations are used to gain insight in the expected behavior of complex systems. Validation and
verification are in many cases not even realistic options and need to be replaced with plausibility evaluations. However, this observation should not be an excuse not to conduct validation and verification where
it is possible. If, e.g., new doctrine is evaluated in the military domain, the weapon models used should be
validated and verified; if a new treatment procedure in medical simulation is modeled, the effect models
of treatment phases must represent reality, etc. Training and support of real operations during their execution normally utilize both roles of simulation in support of fulfilling their tasks. Of additional interest is
that the role of M&S in the lifecycle can change.
This leads to the second point that shall be made in this section. While many members of the M&S
community of practice see M&S as a special field of software engineering, M&S is not just another domain to use software. As stated by Hester and Tolk (2010): “While modeling is the process of abstracting,
theorizing, and capturing the resulting concepts and relations in a conceptual model, simulation is the
process of specifying, implementing, and executing this model. Modeling resides on the abstraction level,
whereas simulation resides on the implementation level. Simulation systems are model-based implementations. Whether or not it is possible to merge two simulation systems in collaborative support of user re-

2978

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
quirements not only depends on the integratability of the underlying networks and the interoperability of
the simulation implementations, but also on the composability of the underlying models.” Software engineering focuses on integratability and interoperability, but modeling requires the alignment for composability. As a model is a purposeful abstraction and simplification of a perception of reality created in support of conducting a special task, like answering a research question or supporting a training event,
standardization seems to be very difficult: if we standardize a model – or parts thereof – we also standardize a solution, and that may be counterproductive to the creative process of modeling.
It is recommended to think in layers representing these ideas, such as the three categories originally
recommended by Page, Briggs, and Tufarolo (2004) and further specified by Tolk (2009):
 Integratability contends with the physical/technical realms of connections between systems,
which include hardware and firmware, protocols, networks, etc. Hardware standards are already
successfully applied in this category.
 Interoperability contends with the software and implementation details of interoperations; this includes exchange of data elements via interfaces, the use of middleware, mapping to common information exchange models, etc. Software engineering standards can help here. Also, current
M&S standards, such as IEEE1278 and IEEE1516, focus on this category.
 Composability contends with the alignment of issues on the modeling level. The underlying models are purposeful abstractions of reality used for the conceptualization being implemented by the
resulting systems. As stated before: these models are the reality for the simulation. Tolk et al.
(2011) observe that the conceptualization of the referent system replaces the real world referent.
Best practices and guidelines can help here, but are not specified with the necessary rigor of engineering methods yet.
In summary, successful interoperation of solutions requires integratability of infrastructures, interoperability of systems, and composability of models. Successful standards for interoperable solutions must address all three categories. While the M&S community understands the problems in the categories of integratability and interoperability, many of them shared in the broader environment of network and software
engineering – which also ensures mature solutions and economic incentives, the conceptual level of composability is still not addressed sufficiently and unique to M&S (King 2009).
Understanding the different roles of simulation and the layered nature of M&S will support addressing standardization issues in a methodological structure, reflecting roles and targeted interoperation category, with particular emphasis on composability, as this is the interoperation layer special to M&S.
3

CULTURAL ISSUES REGARDING STANDARDIZATION (FISHWICK)

The process of standardization within M&S is fundamentally a cultural one. A culture is a social product
defined as a set of norms established and nourished by a set of individuals. These individuals come together in groups with a perceived need to better communicate with one another either using a common
language or set of formal procedures. There are different forms that standardization may assume, and
each form is based on how the M&S field is organized. For example, we might standardize a dynamic
system model in terms of either its syntax or semantics. For syntax, the cultural goal is one of a common
look and feel to the model definition; a mathematical model will employ the same number of variables
and overall functional form. A visual model will use commonly defined graphical icons and perhaps a
graph grammar to ensure its coherence among individual examples.
In either situation, the concept of model is one rooted in language (Fishwick 1995, Fishwick 2007).
For semantics, we would attempt to ensure that models of a specific form behave in the same way regardless of computational platform. Given that syntax and semantics are both critical sub-components of language theory, if we are to standardize, we should be considering both simultaneously. Slightly more flexible, and pragmatic, definitions of standards could be organized around specific types of application
software. If everyone were to use GPSS to craft models and perform simulations, this could be the basis
for a way to standardize within M&S. For popular software, this type of thinking is fairly commonplace-

2979

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
scientists using MATLAB (2011) represent a cultural collective organized around a consumer product
that performs well in the marketplace.
Andrew Tanenbaum is credited with the oft-heard phrase "The good thing about standards is that
there are so many to choose from." This quote gives rise to questions of the plurality of standards, and
leads to the questions of where standards may lead within M&S if we are to look at related fields, such as
computer programming languages, or indeed, natural languages such as Chinese and English. For both
natural and artificial languages, it is generally assumed that these languages follow a growth-decay curve
not unlike those found in biological natural selection. There are efforts to standardize within specific cultures, and some languages eventually emerge on an endangered species list, while other languages seem
to grow without abatement.
Some cultures are influenced from the top and others from the bottom. In politics, we might consider
a top-down approach to be authoritarian or dictatorial; however, our standards organizations usually have
some element of top-down structure if only to manage and organize rather than to direct. If we consider
the early emphasis on the Distributed Interactive Simulation (DIS, IEEE1278) standard (Neyland 1997),
and the evolutionary branch of the High Level Architecture (HLA, IEEE1516) (Kuhl, Weatherly, and
Dahmann 1999; Fujimoto 2000), these standards were strongly encouraged, and funded, by the Department of Defense (DoD). These standards yielded a rich set of protocols for distributed simulations, and
with DoD backing, the standards were widely employed in industry. All branches of the federal government have played key roles in top-down standards formation. The only downside is when the funding
runs out. If there is no continued top-down "push" for an M&S standard, it may wither. In 1980s, the DoD
mandated the use of a computer programming language, Ada, which became an ANSI standard. Despite
this backing, Ada is not a language that enjoys mass popularity today. Even though Ada is not directly related to the M&S field, there are lessons that might be considered, one of which being that the top-down
approach is not always successful in the long run. On the positive side, a top-down approach can be successful if a self-sustaining community results over the long haul, well beyond the startup phase. A bottom-approach approach to standards development is more chaotic, but more natural: someone, or some
group, creates a modeling approach and others find themselves attracted to it. Petri nets (Peterson 1981)
are one of many examples of this phenomenon. Communities of scholars found themselves drawn to the
Petri net modeling approach or perhaps to the corresponding visual diagrams. This resulted in a cultural
movement that began in the early 1960s and continues to grow.
4

MOVING FROM REACTIVE TO PROACTIVE STANDARDS AND STANDARDS
MANAGEMENT IN MODELING AND SIMULATION (DIALLO)

There is an intuitive understanding that a standard is a commonly agreed solution to an existing problem.
This understanding assumes the existence of a well defined problem for which there is a solution. However, it is important to note that in most cases this is not true as we are not dealing with problems but with
problem situations.
Problem situations are problems for which there is no agreement on the nature of the problem or
whether there is even a problem (Vennix 1996). The process of determining where a standard is needed
necessarily involves at a minimum the transition from a problem situation to a problem. In general, the
context in which a standard exist is depicted in Figure 3. While the problem situation is the top layer in
this figure, it does not indicate an order or sequence. The process depicted can be top-down, bottom-up or
inside out. The only requirement is consistency across the layers. For the sake of discussion, let us start
with the problem situation:
 From problem situation to problem: The transition from situation to problem is a social and cultural activity driven by individuals and organizations as described by Fishwick. It is important to
note that stakeholders have to agree whether there is a problem and if so what is the nature of the
problem or the problem space. It is essential to ensure a sufficient level of consensus at this level
in order to bound the problem space such that it reflects the agreed upon worldview. As a simple
example many stakeholders can look at a situation where multiple systems have to interoperate.
2980

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
After several discussions they might agree that there are two problems, namely how to initialize
the systems consistently and how to make them interoperable during run time.

Figure 3: Standards in The Context of Problem Situations






From problem to solution: Once a problem is well defined and bounded, there might be a large
number of solutions to select from. If the problem is common enough and/or commonly encountered, it is very likely that solutions already exist and the standardization process reduces to selecting one of them. However, it is possible that even if there is an agreement on the problem, the
conclusion is not to standardize a solution or simply not to bother solving the problem for one
reason or another (resources, maturity of existing solutions, etc.). In keeping with the example,
the group might decide that interoperability during run time is worth solving but in light of the resources available initialization will be solved ad hoc or at a later date.
From solution to domain: A domain is a generalization of a community of interest (military, medical, transportation, Command and Control, etc.). A standard solution often applies to a domain
and usually this domain is directly mappable to the problem space associated with a problem situation. A solution can apply to one or more domains and in general one has to be careful to understand the domain of applicability of a standard before contemplating its reuse in another domain.
Staying with our example, the solution that is standardized for run time interoperability might be
designed to support the interoperability of simulations in the military domain and more specifically the interoperability of combat simulations.
From domain to requirements: In this case, each domain has a set of requirements that defines
whether a solution is applicable or not. The standard might be applicable given one set of requirements and completely inadequate given another. Each application of a standard should be
evaluated on a case by case basis and in most cases, more than one standard is needed to fulfill all
the requirements of a domain. To finish with the example, interoperability of combat simulations
in the military domain must support several requirements one of which is the ability to compute
and exchange the effects of an engagement. The interoperability of Command and Control (C2)
systems with combat simulation systems can probably reuse the standard described here but it
would have to be adapted to support the requirements of C2 systems for instance.
2981

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
As stated before, this process can be viewed as bottom up or top down. In general, a standard lives in
the quintuple of problem situation, problem, solution, domain and requirement. In terms of M&S and
M&S standards, it means that a standard is a solution to a problem, not the solution to a problem situation
which is the area of interest of M&S. In other words, a problem is a modeling question and a standard is
one of the possible solutions to that question. As noted earlier, a standard is always associated with a
worldview shaped by the quintuple in which it lives which means that by the very nature of M&S, we are
bound to have a multitude of solutions for a given problem situation and not an end all be all standard that
solves all of the possible problems that can be derived from a problem situation. Consequently, current
and future standards must be modular and be able to work together as a cohesive whole. They must be
developed and managed as piece-parts of a overarching solution to a problem situation. This is especially
true for language based standards (XML-based standards) which are more and more prevalent. For this
family of standards, the recommendation is to start from an ontology of the problem situation that can be
refined until it becomes an agreed upon problem. A language can then be generated as potential solutions
for aspects of the problem. The semantic rules representing the requirements of each domain can be captured separately by another language to express the context in which the standard is applicable.
5

CHALLENGES IN STANDARDIZATION OF M&S LIFE CYCLE PROCESSES (BALCI)

A life cycle for Modeling and Simulation (M&S) is defined as a framework for organization of the processes, work products, quality assurance activities, and project management activities required to develop,
use, maintain, and reuse an M&S application from birth to retirement (Balci 2011). A process is defined
to refer to a set of activities, actions, and tasks within the life cycle. Balci (2011) presents a life cycle for
M&S and describes more than 20 life cycle processes including requirements engineering, conceptual
modeling, architecting, design, and certification.
Certainly, not every life cycle process qualifies to have a Standard. Some may have a Best Practice,
Recommended Practice, or Guide/Guidebook. We define these terms below.
 M&S Process Standard: An authoritative and formal specification of an M&S process, which is
substantiated or supported by documentary evidence and accepted by most authorities in the
M&S field.
 M&S Process Best Practice: The definition of an M&S process that has proven to provide the
best results based on the consensus opinion in the M&S field.
 M&S Process Recommended Practice: The definition of an M&S process that is recommended
by an organization or society.
 M&S Process Guide / Guidebook: Tutorial information or instruction on how to conduct an M&S
process.
The use of the above terminology is inconsistent in the published literature. Many Standards exist when in
fact they are Best Practices or Recommended Practices.
We face many technical, managerial, and organizational challenges in developing M&S Standards including the following:
1. A Standard is created by using a complex set of activities as outlined below:
a) Research and drafting of a Standard
b) Consensus development in M&S community
c) Approval and declaration of the Standard
d) Publication of the Standard
e) Application of the Standard by M&S developers
f) Accreditation of M&S developers
g) Certification of M&S applications created under a Standard
2. A Standard can be developed only under the leadership of a society or organization, e.g., DoD,
IEEE, ISO, NATO, NIST, SISO.

2982

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
3. Effective leadership by a society or organization is required to develop Standards by establishing
M&S community consensus on a solution to a commonly encountered problem.
4. Development of a Standard, Best Practice, Recommended Practice or Guide must be considered
for a particular M&S life cycle process.
5. A Standard should be created in such a way that it does not prevent innovation.
6. A Standard, Best Practice, Recommended Practice or Guide must be developed to be applicable
for all areas (types) of M&S described by Balci et al. (2011). Creating a Standard to be applicable
for all types of M&S poses significant technical challenges.
7. What is dictated by the phrase “The wonderful thing about Standards is that there are so many of
them to choose from!” must be avoided.
8. Standards age due to technological advancements and therefore require periodic updates. Management of the evolution and maintenance of a Standard is known to be very challenging.
Future M&S standardization must not only address these issue, the M&S life cycle processes will also
guide future M&S standard development.
6

DEVS STANDARDS AT SYNTACTIC, SEMANTIC, AND PRAGMATIC LEVELS
(SARJOUGHIAN, ZEIGLER, AND HU)

Modeling and simulation (M&S) standardization efforts have been underway since 1980s. Standardization efforts have primarily focused on simulation interoperability (Wainer and Mosterman 2011). Recently, standards targeted at the levels of generic and domain-specific models have been proposed (Sarjoughian and Chen 2011). Interest in relating M&S standards to external standards such as those of Service
Oriented Architecture (SOA) (Sarjoughian et al. 2008) and Open Geospatial Consortium (OGC) has been
expressed as well.
It is useful to consider a coordinated set of standards at the syntactic, semantic, and pragmatic levels
under the DEVS formalism and framework (Zeigler and Hammonds 2007). Coordinated development of
these standards would help address well known challenges in the M&S world that independent development of separate standards would not be capable of. For example, use of a standardized simulation tool
can help increase composability, reuse, and creditability of models at the same time. We, therefore, propose developing a coordinated set of standards at (i) Syntax Level: simulation interoperability standard using DEVS simulation protocol, (ii) Semantics Level: generic modeling standard using DEVS model specification, and (iii) Pragmatics Level: domain-specific modeling standard based on application domains
which may also have their own standards
We use the term simulation system as it is commonly used to designate a simulation component or
federate in a distributed simulation. We use this term in the sense that the underlying assumptions and
constraints used in building simulation systems must be properly aligned to assure meaningful results
(Davis and Anderson 2004, Muguira and Tolk 2006).
A simulation interoperability standard should enable simulation systems to be federated to execute
together under a middleware providing specific time management and data exchange services (IEEE
2010). This requires defining interface signatures that the middleware expects from both services. The
standard must provide a class of simulation systems for which the resulting simulation would be demonstrably correct. To achieve these needs, the DEVS simulation standard which employs the DEVS simulation protocol (Zeigler, Praehofer, Kim, 2000) is proposed for time and data management. The simulation
protocol is provably correct for models that are specified in the DEVS formalism.
The benefits of a simulation interoperability standard include:
 A class of simulation systems exists for which the resulting hierarchical I/O behavior can be
guaranteed to be syntactically correct.
 Internal functions of simulation systems can be treated proprietary (i.e., black boxes) with only
their standardized I/O operations exposed.
 Simulation systems, as web services (Seo and Zeigler 2009), could be discoverable and composable to achieve a desired behavior to the extent that their I/O behaviors are well characterized.
2983

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
The limitations of a simulation standard include that verification is limited as there is no guarantee the resulting behavior is correct, the validation is limited as there is no guarantee that resulting behavior is the
one desired, and the reuse is limited in granularity to the simulation system as a black box. A modeling
standard would address some of these deficiencies.
A modeling standard provides a formal syntax and semantics for simulation models. The internals of
simulation systems can be identified and claims made on the structure and behavior of a real-world or referent system. A DEVS-based standard would provide a large (universal for dynamic systems) class of
models to which the simulation interoperability standard applies (Sarjoughian and Chen 2011). Separate
and compatible DEVS-based modeling, i.e., DEVS formalism, and simulation, i.e., DEVS protocol,
standards offer a basis for developing generic DEVS modeling and simulation tools and benefit the process of building and using simulation systems. Benefits of a modeling standard include:
 The internals of simulation systems can be exposed to the degree enabled by the standard.
 Tools can perform syntactic and semantic checking on model specifications to improve verification and validation.
 Mappings can translate from one implementation language to another allowing more flexibility in
implementation and finer granularity reuse and sharing.
Limits of a modeling standard include that it is insufficient to directly represent domain knowledge. By
itself, it cannot account for interoperable and efficient simulation execution. Domain-specific modeling
standards would help to address some of these deficiencies.
Domain knowledge must be included in models in order to be of practical use. To achieve this, domain-specific modeling standards are needed. Furthermore, modeling standards for standardized application domains, e.g., SOA-compliant DEVS (Sarjoughian et al. 2008), can be developed. These serve a key
role since standardized application-domain knowledge is standardized from the modeling and simulation
perspective. Developing such standards as extensions of the DEVS modeling standard would in effect
provide a significant start toward standards at the pragmatic level where the ultimate alignment of constraints and assumptions of simulation systems must be aligned to achieve meaningful results.
Benefits of a DEVS-based domain-specific modeling standard include:
 The standard can account for domain-specific data types, functions, logical, and physical structures of systems.
 Standards can be specialized for external (non-simulation) standards (i.e., develop domainspecific model libraries).
 Standards can support developing simulation profiles consistent with domain-specific application
standards.
Limits of a domain-specific modeling standard include that the standard cannot guarantee consistency
among a family of model abstractions in a given application domain, and the standardized domainspecific applications may not necessarily be compatible with each other although there would be better
prospects that they would be developed in a coordinated manner as being based on the underlying DEVS
modeling standard.
Combination of standardizations that are based on a theoretical modeling and simulation have the potential to improve carrying out each of the facets of the M&S Life Cycle (Balci 2011). They can help developing robust, scalable, and efficient simulation systems while furthering their usage and credibility
through unified verification, validation, and accreditation. Treating the syntactic, semantic, and pragmatic
levels independently has advantages, perhaps easing adoption through separate development activities.
However, employing the DEVS formalism and framework, can achieve a coordinated set of standards
with critical benefits to support the complete M&S Life Cycle.

2984

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
7

CORRELATION OF CHARACTERIZING ATTRIBUTES AND SUCCESS IN MILITARY
M&S STANDARDS (PETTY)

A crucial enabling factor in the success of defense-related M&S has been the development and use of
standards. Interoperability standards (e.g., DIS, TENA, and HLA), natural environment standards (e.g.,
SEDRIS), simulation development process standards (e.g., FEDEP), and many others have all made contributions to enhancing the interoperability, reusability, and capability of defense-related M&S systems.
The technical capabilities of these standards are important predictors of their success. However, the governance structures and processes and other non-technical characteristics of the various standards have also
affected their acceptance and utilization and ultimately their success and impact.
An initial study was conducted to identify possible correlations between characterizing attributes of
military M&S standards and the success of those standards. A total of 22 standards in 9 categories were
studied and 10 attributes of those standards were identified and evaluated. The attributes were name, category, status, year, type, form, governance type, governance formality, and technical specificity. Of special interest were the latter two; governance formality, defined as “degree to which the process of setting
and changing the standard is controlled by formally prescribed processes,” and technical specificity, defined as “degree to which the standard defines or provides content which is implementable or executable
as written,” were conjectured to positively correlate with the success of a standard. Values for these two
attributes were defined on a 1–5 scale, with higher values indicating more formality or specificity respectively. Table 1 lists the standards studied and a subset of their attributes.
Table 1. M&S standards studied.
Governance Governance Technical
type
formality specificity

Name

Category

SIMNET

Distributed
simulation

ALSP

Distributed
simulation

DIS

Distributed
simulation

Architecture
management
group
Standards
body

HLA

Distributed
simulation

Standards
body

TENA

Distributed
simulation

XMSF

Distributed
simulation

MILES

Live
training

CTIA

Category

UML

Conceptual
modeling

DoDAF

Conceptual
modeling

SDIS

Synthetic
environment

Closed

1

4

SEDRIS

Synthetic
environment

Standards
body

5

4

NFDD

Synthetic
environment

Standards
body

4

3

1

4

3

4

5

3

5

4

3

5

2

3

FEDEP

Simulation
development

Standards
body

5

2

Closed

1

4

VV&A
RPG

Simulation
development

Closed

2

2

Live
training

Architecture
management
group

3

4

VV&A
Overlay

Simulation
development

Standards
body

5

2

RPR FOM

Object
modeling

Standards
body

5

4

DSEEP

Simulation
development

Standards
body

5

2

BOM

Object
modeling

Standards
body

5

3

MSDL

Scenario
definition

Standards
body

5

3

Standards
body

5

3

C-BML

Command
and control

Standards
body

5

3

DIS
Enumerations
Enumerations

Closed

Governance Governance Technical
type
formality specificity
Architecture
3
2
management
group
Architecture
management
3
3
group

Name

Architecture
management
group
Architecture
management
group

The standards' success was assessed by two groups of standards experts, totaling 39 persons. The first
group (18 persons) were selected for their known expertise by the study author; the second group (21 persons) attended a workshop on M&S standards organized by Old Dominion University. The experts used a
Likert-type scale to assess the success of each standard; the success scale values were: Very Unsuccessful
(VU), Somewhat Unsuccessful (SU), Neither Unsuccessful Nor Successful (NUNS), Somewhat Successful (SS), and Very Successful (VS).

2985

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
The standards’ attribute values for governance formality and technical specificity were examined for
correlation with the experts’ assessments of standards’ success. Conventional correlation statistics (e.g.,
Pearson’s correlation coefficient) were not calculated because converting the discrete Likert assessment
values into numerical scalars was considered methodologically questionable. Instead, categorical or qualitative correlation was tested by defining a band of corresponding values for each attribute as correlating.
Tables 2 and 3 summarize the results for the 22 standards. The green portions of the tables show values considered to correlate and the red portions show those that do not correlate. For 16 of the 22 standards, governance formality was found to correlate with standards success. For 14 of the 22 standards,
technical specificity was found to correlate with standards success.
Table 2. Correlation of governance formality and success in 22 military M&S standards.
VS
SS
NUNS
SU

1
1
1

2

4

1

3
1
3

1

1

1

1

5
4
6
1

Correlated

16

Non-correlated
Total

6
22

VU

Table 3. Correlation of technical specificity and success in 22 military M&S standards.
1
VS

2
1

3
2

4
2

5
1

SS

4

3

3

1

Correlated

14

1

Non-correlated
Total

8
22

NUNS
SU

4

VU

1

Although it cannot be claimed that this approach can easily be generalized based on the currently
limited experiences, it seems plausible to assume so. In any case, such metrics are necessary to measure
success or failure of standardization efforts to support management decisions.
8

COMPARISON OF STANDARDS MANAGEMENT PROCESSES (LOPER)

A number of distributed simulation architectures are commonly used today. Each was developed by specific user communities and each owes much of its success to well defined standards. Unfortunately, live,
virtual, and constructive (LVC) federates that choose different architectures can't natively interoperate.
An overarching study was conducted in 2008 called LVC Architecture Roadmap (LVCAR) to define a
proposed “way ahead” for improved interoperability across the major distributed simulation architectures
and protocols. One component of the LVCAR study dealt with standards development, including the associated standards organizations and standards development processes that would best meet the needs of
the broader LVC distributed simulation community.
The methodology applied in the standards study is shown in Figure 4. It took the existing LVC distributed simulation standards, characterized their current state, and defined an “idealized” model against
which they could be compared. The standards study began by examining the various types of standards
and products developed by the LVC architectures, the organizations responsible for developing those
standards (commercial and government), process attributes used to maintain and evolve standards, and the
compliance certification approaches used by each architecture.
The next step in the study was to characterize the vision state of future LVC standards evolution and
management. To do this, several questions had to be addressed, including whether to use a commercial or

2986

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
a government standards approach, how to balance the need for stable standards with the need for timely
evolution, and how to make the standards easily available to the M&S community. These questions resulted in a set of desirable attributes for future LVC standards. By comparing the desirable attributes with
the existing organizations and processes in use today, the standards study team was able to identify the
gaps that need to be addressed in order to meet the needs of LVC standards development.

Figure 4: Analytic framework for standards evolution and management
Based on the vision state characterization and the analysis to identify gaps in existing organizations
and processes, a set of courses of action (COA) were developed to characterize a potential solution space
for LVC standards evolution and management:
 COA 1: Maintain Status Quo.
 COA 2: Government Standards Management Approach.
 COA 3: Commercial Standards Management Approach.
 COA 4: Hybrid Standards Management Approach.
The pros and cons of each COA were analyzed, and recommendations about future LVC standards development organizations, standards processes, and compliance certification were developed. Based on the
analysis and subsequent pruning of the possible strategies for the standards dimension, the standards
study team believed that COA 4: Hybrid Standards Management Approach was the best standardization
approach for future LVC architectures. In order to realize this COA, the following recommendations were
developed to address the standards management and evolution aspects of the LVC Roadmap. These recommendations included:
 Engage the Simulation Interoperability Standards Organization (SISO) and the broader LVC
standards community
 Make IEEE standards more accessible to the LVC community
 Coordinate activities and fund participation in commercial standards development groups
 Increase the sphere of influence in SISO
 Develop an evolutionary growth path for LVC standards
 Develop a hybrid compliance certification process

2987

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
Since this study concluded, several of these recommendations have been implemented. For example,
all active members of SISO have access to the IEEE M&S standards that were developed under the SISO
standards activity committee (including the Distributed Interactive Simulation (DIS) and High Level Architecture (HLA) series of standards). Details about the standards study were documented in a final report
(Loper and Cutts 2008) and can be found in the M&SCO online library.
9

CONCEPTUAL AND COMPUTATIONAL LIMITS FOR M&S STANDARDS
(REYNOLDS)

The utility of standards for informing design processes and engineering practices can be seen in an abundance of examples. For example, modern societies rely heavily on common standards for weight, distance
and time. Consequently, there is significant appeal to extending a standards-driven process to domains in
which, unfortunately, the undesirable influences of complexity, uncertainty, and uncomputability play a
growing role. The modern computer programmer understands the limits well: just because a program
meets all of the syntactic standards enforced by a comprehensive, type checking compiler does not mean
the same program makes the least bit of sense for its target application. Electrical engineers working in a
highly standardized world of components understand similar limits as well. There are provable limits to
what can be tested for conformance and there are limits to the utility of rules set down in the face of unknowns, ambiguities and uncertainties. Here, the standards development process meets its match.
Standards development should begin with identification of a mission and of a reference model. Mission defines context and scope, and a reference model provides a common worldview, comprising terminology, structure, world objects and processes describing how objects interact. For example, a mission
could be to establish accurate, reliable and reproducible measurements of weight for objects in a common
spatial frame of reference, say Earthborne. The reference model could be the set of all weighable objects,
all relevant ways they can interact with their world, and all methods for weighing the objects.
Standards, themselves, comprise prescriptive and proscriptive rules and often a set of conformance
tests. When the mission and reference model are largely unambiguous and uncontested, a standards development process can generally proceed smoothly. In the object weight example, standards would identify methods and conformance tests for establishing accurate, reliable and reproducible measurements of
weight of Earthborne objects. A sufficiently unambiguous mission and reference model can be established
in this example and thus enable creation of standards ensuring reliable, accurate and reproducible methods for weighing objects.
A standards development process is much more likely to suffer or fail when the mission and/or the
reference model are ambiguous. Ambiguity is a natural artifact in a standards development process when
it encounters challenges here named “the heavy C’s”: complexity, uncertainty and uncomputability. My
thesis is that increased presence of any part of the heavy C’s in any part of the standards development
process (mission, reference model, standards) diminishes the likelihood of a useful outcome. As a corollary, modeling and simulation standards of any extent (e.g., those envisioned for model reuse and interoperability) are challenging to realize because of the common presence of the heavy C’s in that which we
wish to model and in related components of the simulation development process (mission, reference model and standards).
Consider, as a case in point, models of human behavior and the mission to make such models reusable
and interoperable. It is possible to develop standards and test for conformance at a simple level, for example a standard that human models communicate using a common language. However, a more meaningful standard, say, that a listening human model must always understand as a speaking model intended, encounters heavy C’s. The portion of the reference model relating to human interactions is complex and rife
with uncertainty, making the meaning of a common understanding difficult to define. Additionally, a conformance test for whether two humans share a common understanding is uncomputable (Test for equivalence in two formal languages is uncomputable. Natural language only makes things worse.) Similar conclusions occur regarding interoperability standards for economic, environmental, systems biology and

2988

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
internet commerce models. Complexity, uncertainty and uncomputability prevent identification of standards that can guarantee the kinds of interoperability and reusability users seek.
Those times when standards committees forge ahead in the face of the heavy C’s to define a set of
standards for, say, model interoperability (for interesting models), the heavy C’s persist! Consider a complex reference model, for example our model of communicating humans. Complexity in the reference
model can lead to
 Omissions of standards due to a failure to appreciate a need,
 Complexity of the standards,
 Use of abstraction as a method for concealing complexity, and
 Conflict among committee members over the design of the standards.
These in turn can lead to ambiguity and/or compromise in standards development, and abandonment of
standards or splintering of standards development efforts. Ambiguity, abandonment, splintering and compromise can lead to uncomputability of standards conformance tests, user uncertainty about the meaning
of standards and increased costs in both the standards development process, and in subsequent use of the
standards. In sum, start with complexity in a reference model and end with uncomputability, uncertainty
and increased costs in the standards development process (and their subsequent use). One can build similar chains that begin with uncertainty in standards or uncomputability of conformance that lead to uncertainty regarding the utility of a reference model and/or related standards, with eventual abandonment of
the standards, or splintering of standards development efforts, leading to increased costs and uncertainty
about the standards process.
The dismal outcomes characterized here are not meant to deter standards development efforts. Many
standards efforts produce useful results. However, those useful results tend to occur when the reference
model is well understood and widely agreed upon, or when standards committees deal with heavy C’s by
embracing modest expectations about what can be standardized. Standards development efforts are limited by the complexity and uncertainty associated with a reference model, the uncomputability of conformance to many desirable standards and the recurring appearance of heavy C’s when standards committees become too ambitious. Participants in a well-managed standards development process recognize
these limitations and perceive themselves as part of a discovery process for reducing complexity and uncertainty and coping with the limits of uncomputability.
10

SUMMARY

This collection of position statements is the compilation of inputs from M&S experts with significant
standardization experiences and shows some of the highlights and the variety of ongoing standardization
efforts. However, the list can neither be complete nor exclusive. There are many other worthwhile standardization activities going on. As limited as the list is, it shows the trend that a more methodological approach is needed. It also shows that the focus may have to shift from simulation interoperability to the often neglected activity, processes, and products of the modeling part of M&S, as many challenges require a
solution on the conceptual level, not just temporary on-the-spot solutions on the implementation level.
However, standards are needed for the simulation level, the modeling level, and also for the application
level. These efforts have to be well orchestrated, while reducing complexity and uncertainty and coping
with the limits of uncomputability, and metrics are needed to measure success. This requires a common
theory to align the methods that will drive standardized solutions, and may require a shift in our culture as
well.
ACKNOWLEDGMENTS
Significant parts of the work presented here are funded by or prepared in support of a federally funded
three year study entitled ‘Standards in Modeling and Simulation’ which is being led by the Virginia Modeling Analysis and Simulation Center, Old Dominion University, Suffolk, VA.

2989

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
REFERENCES
Balci, O. 2011. “A Life Cycle for Modeling and Simulation.” Technical Report, Department of Computer
Science, Virginia Tech, Blacksburg, VA.
Balci, O., J. D. Arthur, and W. F. Ormsby. 2011. “Achieving Reusability and Composability with a Simulation Conceptual Model.” Journal of Simulation 5: 157-165, doi:10.1057/jos.2011.7.
Collins, A. J., S. Y. Diallo, S. R. Sherfey, A. Tolk, C. D. Turnitsa, M. D. Petty, and E. Weisel. 2010.
“Standards in Modeling and Simulation: The Next Ten Years.” In Proceedings of the MODSIM
World Conference and Expo, Hampton, VA, October 13-15.
Davis, P. K., and R. H. Anderson. 2004. “Improving the Composability of DoD Models and Simulations.” Journal of Defense Modeling and Simulation 1(1):5-17.
Fishwick, P. A. 1995. Simulation Model Design and Execution: Building Digital Worlds. Prentice Hall,
Inc.
Fishwick, P. A. 2007. “The Languages of Dynamic System Modeling.” In Handbook on Dynamic System
Modeling, edited by P. A. Fishwick, 1-10. CRC Press.
Fujimoto, R. M. 2000. Parallel and Distributed Computing Systems. Wiley Interscience.
Hester, P. T., and A. Tolk (2010). “Applying Methods of the M&S Spectrum for Complex Systems Engineering.” In Proceedings of the Emerging Applications of M&S in Industry and Academia (EAIA)
Spring Simulation Multiconference, 17-24. April 11-15, Orlando, FL.
IEEE. 2010. IEEE 1516-2010 Standard for Modeling and Simulation (M&S) High Level Architecture
(HLA). IEEE Standards Association. Accessed May 1. standards.ieee.org/findstds/standard/15162010.html.
King, R. D. 2009. “On the Role of Assertions for Conceptual Modeling as Enablers of Composable Simulation Solutions.” Ph.D. Dissertation, Batten College of Engineering, Old Dominion University, Norfolk, VA.
Kuhl, F., R. Weatherly, and J. Dahmann. 1999. Creating Computer Simulation Systems: An Introduction
to the High Level Architecture. Prentice Hall, Inc.
Loper, M. L., and D. Cutts. 2008. Live Virtual Constructive Architecture Roadmap (LVCAR) Comparative Analysis of Standards Management. Final Report, M&S CO Project No. 06OC-TR-001.
MATLAB. 2011. Accessed June 10, 2011. http://www.mathworks.com/products/matlab.
Muguira, J., and A. Tolk. 2006. “Applying a Methodology to identify Structural Variances in Interoperations.” Journal of Defense Modeling and Simulation 3(2):77-93.
Neyland, D. L. 1997. Virtual Combat: A Guide to Distributed Interactive Simulation. Stackpole Books.
Page, E. H., R. Briggs, and J. A. Tufarolo. 2004. “Toward a Family of Maturity Models for the Simulation Interconnection Problem.” In Proceedings of the Simulation Interoperability Workshop, April 1823, Arlington, VA. IEEE CS Press.
Peterson, J. 1981. Petri Net Theory and the Modeling of Systems. Prentice Hall, Inc.
Reynolds, P. F. 2008. “The Role of Modeling and Simulation.” In Principles of Modeling and Simulation:
A Multidisciplinary Approach, edited by J. A. Sokolowski and C. M. Banks, 25-43. John Wiley &
Sons.
Sarjoughian, H. S., and Y. Chen. 2011. “Standardizing DEVS Models: An Endogenous Standpoint.” In
Proceedings of the Spring Simulation Multi-Conference, DEVS Symposium, 1-9. Boston, MA.
Sarjoughian, H. S., S. Kim, M. Ramaswamy, and S. Yau. 2008. “A SOA-DEVS Modeling Framework for
Service-Oriented Software System Simulation.” In Proceedings of the 2008 Winter Simulation Conference, edited by S. J. Mason, R. R. Hill, L. Mönch, O. Rose, T. Jefferson, and J. W. Fowler, 845853. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Seo, C., and B. P. Zeigler. 2009. “Interoperability between DEVS Simulators using Service Oriented Architecture and DEVS Namespace.” In Proceedings of the Spring Simulation Conference, San Diego,
CA.

2990

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
Sowa, J. F. 2004. The Law of Standards. Annotated Reprint Email to the Committee on Shared Reusable
Knowledge Bases (SRKB). Accessed June 10, 2011. http://www.jfsowa.com/computer/standard.htm.
Tolk, A. 2009. “Interoperability and Composability.” In Principles of Modeling and Simulation: A Multidisciplinary Approach, edited by J. A. Sokolowski, and C. M. Banks, 403-433. John Wiley & Sons.
Tolk, A., P. K. Davis, W. Huiskamp, H. Schaub, G. L. Klein, and J. A. Wall. 2010. “Challenges of Human, Social, Cultural, and Behavioral Modeling (HSCB): How to Approach them Systematically?” In
Proceedings of the 2010 Winter Simulation Conference, edited by B. Johansson, S. Jain, J. MontoyaTorres, J. Hugan, and E. Yücesan, 912-924. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.
Tolk, A., S. Y. Diallo, J. J. Padilla, and C. D. Turnitsa. 2011. “How is M&S Interoperability different
from other Interoperability Domains?” In Proceedings of the Spring Simulation Interoperability
Workshop, Boston, MA. IEEE CS Press.
Vennix J. A. M. 1996. Group Model Building: Facilitating Team Learning Using System Dynamics.
Chichester: Wiley.
Wainer, G., and P. Mosterman. 2011. Discrete-Event Modeling and Simulation: Theory and Applications.
Taylor and Francis.
Zeigler, B. P., and P. Hammonds. 2007. Modeling & Simulation-Based Data Engineering: Introducing
Pragmatics into Ontologies for Net-Centric Information Exchange. Academic Press.
Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory of Modeling and Simulation: Integrating Discrete Event and Continuous Complex Dynamic Systems. Academic Press.
AUTHOR BIOGRAPHIES
ANDREAS TOLK is a Professor for Engineering Management and Systems Engineering of Old Dominion University. He is also a Senior Research Scientist at the Virginia Modeling Analysis and Simulation
Center. He holds a M.S. in Computer Science (1988) and a Ph.D. in Computer Science and Applied Operations Research (1995), both from the University of the Federal Armed Forces of Germany in Munich.
He is senior member of IEEE and SCS and member of ACM, ASEM, MORS, NDIA, and SISO. His area
of expertise is interoperability and composability and modeling and simulation based systems engineering. His e-mail address is atolk@odu.edu.
OSMAN BALCI is a Professor of Computer Science at Virginia Polytechnic Institute and State University (Virginia Tech). He received B.S. and M.S. degrees from Boğaziçi University (Istanbul) in 1975 and
1977, and M.S. and Ph.D. degrees from Syracuse University in 1978 and 1981. Dr. Balci currently serves
as an Area Editor of ACM Transactions on Modeling and Computer Simulation, Modeling and Simulation
(M&S) Category Editor of ACM Computing Reviews, and Editor-in-Chief of ACM SIGSIM M&S
Knowledge Repository. His current areas of expertise center on software engineering, software/system architecting, and M&S. His e-mail and web addresses are balci@vt.edu and http://manta.cs.vt.edu/balci.
SAIKOU Y. DIALLO is Research Assistant Professor at the Virginia Modeling, Analysis and Simulation Center at Old Dominion University. He received his M.S and Ph.D. in Modeling and Simulation
from Old Dominion University. His research interests are formal approaches to M&S interoperability and
the application of semantic web tools and methods in support of distributed, heterogeneous simulation.
His email address is sdiallo@odu.edu.
PAUL A. FISHWICK is Professor of Computer and Information Science and Engineering at the University of Florida. He received his Ph.D. from University of Pennsylvania. Fishwick’s research interests are
in modeling methodology, aesthetic computing, and the use of virtual world technology for modeling and
simulation. He is a Fellow of the Society of Modeling and Simulation International, and recently edited

2991

Tolk, Balci, Diallo, Fishwick, Hu, Loper, Petty, Reynolds, Sarjoughian, and Zeigler
the CRC Handbook on Dynamic System Modeling (2007). He served as General Chair of the 2000 Winter
Simulation Conference in Orlando, Florida. His email is fishwick@cise.ufl.edu.
XIAOLIN HU is an Associate Professor in the Computer Science Department at Georgia State University, Atlanta, Georgia. He received his Ph.D. degree from the University of Arizona, M.S. degree from Chinese Academy of Sciences, and B.S. degree from Beijing Institute of Technology in 2004, 1999, and 1996
respectively. His research interests include modeling and simulation theory and application, agent and
multi-agent systems, and complex systems science. Dr. Hu is a National Science Foundation (NSF)
CAREER Award recipient. His e-mail address is xhu@cs.gsu.edu.
MARGARET LOPER is the Chief Scientist for the Information & Communications Laboratory at the
Georgia Tech Research Institute. She holds a Ph.D. in Computer Science from the Georgia Institute of
Technology, a M.S. in Computer Engineering from the University of Central Florida, and a B.S. In Electrical Engineering from Clemson University. Margaret has twenty-five years of experience in M&S and
her technical focus is on parallel and distributed simulation. She is a member of ACM SIGSIM, SCS,
SISO and IEEE. Her research contributions are in the areas of temporal synchronization, simulation testing, and simulation communication protocols. Her email address is margaret.loper@gtri.gatech.edu.
MIKEL D. PETTY is Director of the University of Alabama in Huntsville's Center for Modeling, Simulation, and Analysis. He received a Ph.D. in Computer Science from the University of Central Florida in
1997. Dr. Petty has worked in modeling and simulation research and development since 1990 in areas that
include simulation interoperability and composability, human behavior modeling, multi-resolution simulation, and applications of theory to simulation. His email address is pettym@uah.edu.
PAUL F. REYNOLDS, JR. is a Professor of Computer Science at the University of Virginia. He has
conducted research in Modeling and Simulation for over 30 years, and has published on a variety of topics including parallel and distributed simulation, multi-resolution modeling, and simulation. He is a
Plank-Holder in the DoD High Level Architecture. His email address is reynolds@virginia.edu.
HESSAM S. SARJOUGHIAN is an Associate Professor of Computer Science in the School of Computing, Informatics, and Decision Systems Engineering at Arizona State University and Co-Director of the
Arizona Center for Integrative Modeling and Simulation. His research focuses on multi-formalism model
composability, system network co-design, collaborative modeling, and domain-specific modeling. His email address is hss@asu.edu.
BERNARD P. ZEIGLER is Chief Scientist for RTSync Corp. Zeigler has been chief architect for simulation-based automated testing of net-centric IT systems with DoD’s Joint Interoperability Test Command
as well as for automated model composition for the Department of Homeland Security. He is internationally known for his foundational text Theory of Modeling and Simulation, second edition (Academic Press,
2000). He was named Fellow of the IEEE in recognition of his contributions to the theory of discrete
event simulation. His e-mail address is zeigler@rtsync.com.

2992

Proceedings of the 2006 Winter Simulation Conference
L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto, eds.

MODEL COMPOSABILITY

Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
Computer Science & Engineering Department
Arizona State University
Tempe, Arizona 85281-8809, U.S.A.

ABSTRACT

models are challenging when models are heterogeneous in
terms of their formal specifications – e.g., discrete-event and
optimization models have different structural and behavioral
specifications.
Given the diversity of parts and resulting complexity of
the systems, criticality of operation, and enormous financial
consequences, simulation is being used more and more as
the primary science and technology to aid analysis, design,
implementation, and testing. Indeed, simulation can be used
across each phase of system development as proposed in
Simulation-Based Acquisition (SBA 1998). For example, a
suite of system-level simulation models may be developed
to help analyze requirements and evaluate potential architectural solutions far in advance of defining detailed design
specifications. Yet, another suite of models may be used
to help develop detailed design specifications which can be
implemented.
Systems theory, object-orientation, and logical processes worldviews are well known approaches for describing
dynamic systems (Cellier 1991, Banks et al. 2004, Fishwick
1995, Fujimoto 2000, Zeigler et al. 2000). Different modeling paradigms are suitable for different kinds of needs – for
some examples of modeling approaches see (Sarjoughian
and Cellier 2001, Mosterman and Vangheluwe 2002, Barros
and Sarjoughian 2004). A discrete event modeling approach
may be used to describe manufacturing processes and their
interactions as a collection of model components. Alternatively, given historical data, a continuous model can be
developed to show how inventory holding varies in relation to factory throughput. Optimization models may be
developed to understand logistics and financial impact of
satisfying customer demand.
Manufacturing supply chains and military systems of
systems are two well known network systems that are often modeled at varying levels of details and from different
aspects. Depending on the system’s different types of behaviors, system simulation models may be developed using
a monolithic modeling paradigm in one extreme and a
combination of modeling paradigms at the other extreme.

Composition of models is considered essential in developing
heterogeneous complex systems and in particular simulation
models capable of expressing a system’s structure and behavior. This paper describes model composability concepts
and approaches in terms of modeling formalisms. These
composability approaches along with some of the key capabilities and challenges they pose are presented in the context
of semiconductor supply chain manufacturing systems.
1

INTRODUCTION

Many contemporary and future systems are integrated from
a range of simple to complex sub-systems. Examples are
information, manufacturing, and transportation enterprises.
The purposes for these systems vary significantly as each
system is to satisfy a set of users and system requirements. To
understand a system’s capabilities and limitations, dynamical
models are developed. They help to specify structural and
behavioral specifications that can be simulated and, thus,
can be used to evaluate analysis, design, development, and
testing decisions.
Enterprise (or distributed) systems are complex and to
understand their intricate dynamics it is often beneficial
or necessary to use different kinds of models to represent
each sub-system. Heterogeneous model types can offer
greater flexibility as opposed to homogenous model types.
However, combining different model types poses a variety
of challenges depending on the system being modeled (Page
and Opper 1999, Davis et al. 2000, Kasputis and Ng 2000,
Sarjoughian and Cellier 2001, Davis and Anderson 2004,
Fujimoto et al. 2002). Views regarding the kinds of problems
encountered in simulation composability and future advances
and investments in the context of the Department of Defense
needs are described in (Davis and Anderson 2004).
Partitioning a system into layers, each of which consists
of a set of components, is a crucial step in high-level
model specification. Decomposition and composition of

1-4244-0501-7/06/$20.00 ©2006 IEEE

149

Sarjoughian
For example, a manufacturing process may be modeled using a single modeling approach (e.g., agents or Petri-net).
However, to effectively design, assess, and operate a manufacturing supply chain in an optimal fashion, it is useful
to employ discrete-event and optimization models (Kempf
2004). Similarly, to evaluate possible system design alternatives given a variety of operational and engagement
patterns among a sensor, weapons, and command/control,
and communication models, discrete-event, agent, and GIS
models are important to be used (Hall 2005).
2

Controller
Network Control
control

data

Plant
Network Process

Figure 1: A Two-Level Plant and Control Model

MULTI-LAYER MODELING

Many real world network systems can be abstracted to
consist of two parts – one is a plant and another is a
controller (see Figure 1). At a high level of abstractions,
the plant and controller models of a network system can
be considered as computations which exchange data (i.e.,
states) and control (i.e., commands) under some well-defined
constraints. The plant/controller pair can form a symbiotic
relationship, where one is concerned with operations of
a network process and the other is concerned with the
control of the network management. The plant dynamics
can be described in a variety of forms – e.g., discrete,
continuous, or some combination thereof. The controller can
be based on feedback control, event-based, and fuzzy control.
For example, a plant can have stochastic discrete-event
operations taking place at the present time and the controller
can be an optimization-based feedback control. The plant’s
operations occur from one time instant to the next – i.e., the
dynamics of the plant can be modeled as deterministic (or
stochastic) continuous and discrete equations. The controller
operations can be based on the current or future state of the
plant as well as present and expected inputs that can affect
controller’s decision making.
The plant and controller is considered a two-layer system – plant and controller are at lower and higher levels
of abstractions, respectively. The plant is responsible for
carrying out low-level operations, some of which are requested from the control. It carries out some operations
given some inputs and generates some outputs. The controller is responsible to carry out higher level operations
given details provided from the plant. Its responsibility is
to constrain the operations of the plant to those that are
deemed desirable or acceptable for some finite time period
in the future.
The controller itself can be viewed as plant and its
operations sanctioned by another controller. This leads to
multi-layer plant/controller specifications. This separation
offers fundamental benefits in terms of developing separate
and hybrid models and simulations (for example see Praehofer 1991, van Beek et al. 1997, Barros 2002). It provides
conceptual separation of knowledge. Plant and controller
models play distinct roles where one supplies data and the

other provides control. It also enables supporting multiple
levels of control for a plant. One (tactical) controller provides tactical commands to the plant and another controller
provides strategic plans to the tactical controller. Another
advantage is that alternative modeling choices may be used
for the plant or controller. For example, the plant may be
described in terms of discrete and continuous models and the
controller may be described in terms of linear optimization
or event-based control models. In the case of semiconductor
manufacturing supply chain, the plant is the manufacturing
process and the control is operational/logistics controller
(see Figure 1).
3

PLANT AND CONTROL MODELS

In the domain of semiconductor manufacturing supply chain
systems, models of discrete processes, control policies, and
decision plans are important in handling stochastic dynamics
of individual parts of manufacturing processes under tactical
control and strategic planning, given short- and long-term
supply and customer demand (Kempf 2004). These kinds of
aggregated models play a central role in the understanding
of not only physical operations of processes, but also how
they are managed using tactical control and strategic planning. Such complex industries can gain both short-term and
long-term technical competitiveness as well as financial advantages. Another crucial benefit of synthesizing different
kinds of models is the ability to better handle mismatches
(between competing objectives arising from operational vs.
decision aspects, for example).
A canonical manufacturing process can be modeled as
a collection of inventory, factory, and transportation nodes.
The inventory (and transportation) nodes store (transport)
products such as raw material, semi-finished goods, and
finished goods. The factory node processes the products it
receives from inventories and sends the processed products
to inventory or transportation nodes after some time period.
These nodes together characterize the state of the supply
chain process at any one time instant. Each factory, inventory, and transportation node can have stochastic yield and

150

Sarjoughian
duration which in turn results in a chain of nodes that can
exhibit complex dynamics.
A well known approach for controlling a supply chain
is to use linear optimization (LP) (Rardin 1998). An optimization model describes a set of formulas and relationships
(constraints c1 , c2 , etc.). A solver can be used to optimize
the optimization model’s variables of interests (e.g., inventory holdings) given some objective functions. Some
examples of decisions can be how much should be held in
the Finished Goods inventory, how many of which products
should a factory produce, and what transportation mode to
use for shipping products from an inventory to customer
(see Figure 2). The responsibility of the controller is to
optimize expected vs. actual demand and supply over some
future time horizon given key data about manufacturing
nodes (e.g., work-in-progress in a Finish node) (Godding
et al. 2004). Alternatively, it may be useful to use a combination of optimization constraints and control-theoretic
feedback/feed-forward filters which is known as Model Predictive Control (MPC) (Qin and Badgwell 2003, Wang et al.
2004, Sarjoughian et al. 2005, Huang et al. 2006).
To integrate these models requires accounting for differences between data types that are used in, for example,
DEVS and LP models. The structure of input and output
data for DEVS models is complex as compared with the
data for LP models. Data for DEVS models are specified
in terms of messages each defined in terms of port and
value pairs – the port is a string and the data can be an
arbitrary expression (e.g., a queue of key and value pairs).
The structure of input and output data for LP models are
simple data types which may be formed into vectors (e.g.,
array of reals). Therefore, for manufacturing process and
optimization models to interact, their data types needs to
be syntactically mapped from one to the other (Godding
et al. 2004, Huang et al. 2006). For example, the Finish
process produces a set of finished products Lots at output
port Data-Out. The number of tested products is provided
asinput to the LP model.
Aside from input and output exchanges and mappings,
it is also necessary for the behaviors of the manufacturing
process and optimization models to be well-defined – data
exchanges need to occur at appropriate times and frequency.
For example, the manufacturing process model advances
from day to day whereas the optimization model is solved
at the start of each day. In this scenario, a command
determined by the optimization model requests n products
from the Semi-Finished Goods to be released into Finish i.e., the Finish receives a release command on port ControlIn with value n. Details of the DEVS, LP, and MPC models
partially shown in Figure 2 can be found in Singh et al.
(2004), Godding et al. (2004), Sarjoughian et al. (2005),
Wang et al. (2005), Huang et al. (2006).

Optimization Model
Pa

Semi-Finished Goods

C1

C2

Pc

C3

Pd

Pb

C4
Finish

commands

Pf

states
Customer

Die/
Package

Assembly/
Test

SemiFinished
Goods

Finish

Finished
Goods
Transportation

Manufacturing Process Model

Figure 2: A Snippet of a Semiconductor Manufacturing
Process and Optimization Model
4

MODEL COMPOSABILITY CONCEPTS AND
APPROACHES

Considering the multi-layer modeling concept, it is possible
to use a single modeling formalism to describe a plant and
its controller. However, if a system under consideration
has parts with dynamics that are intrinsically different, it
is crucial to use multiple modeling formalisms. The above
semiconductor manufacturing supply chain system is an example where the plant (manufacturing process) is suitable
to be modeled as discrete-event model and the controller
(logistic control) as an optimization model. A key difference between these models is that the discrete-event model
describes how nodes of a manufacturing process network
affect one another and the optimization model searches for
optimal operation of the overall manufacturing process.
4.1 Modeling Formalisms
A modeling formalism can be defined to consist of two parts:
model specification and execution algorithm (Sarjoughian
and Zeigler 2000). The former is a mathematical theory
describing the kinds of structure and behavior that can be
described with it. The latter specifies an algorithm that can
correctly execute any model that is described in accordance
with the model specification.
A model A that can be specified in a modeling formalism
Ψ is denoted as MA,{Ψ} – i.e., model A adheres to the model
specification Ψ and can be executed using its execution
algorithm. A model composed of a finite number of disparate
models (e.g., A, B, . . . , K) specified in a finite number of

151

Sarjoughian
distinct modeling formalisms (e.g., Ψ, Φ, . . . , Ω) is denoted
as M∪A,B,...,K,{Ψ,Φ,...,Ω} .
Approaches to model composability are classified into
mono, super, meta, and poly modeling formalisms. These
approaches provide different kinds of capabilities toward
model composition. The first two are grounded in the concept that in some cases principally a single formalism is
well suited for modeling different parts of a system. In
contrast, the latter two are aimed at some other cases where
disparate modeling formalisms are crucial to describe the
parts of a complex system. Despite the differences among
model composability approaches, every approach must ensure interactions among composed model components are
structurally and behaviorally well-defined.
Exchange of data and control, causality of input and output interactions, and synchronization of models’ executions
with respect to time must be well-defined. This requires
that not only individual model specifications are executed
correctly, but also their compositions (i.e., the execution of
multiple execution algorithms are well-defined with respect
to the composition of their model specifications).
To help describe the above model composability approaches, model specifications are shown as rectangle,
rounded rectangle, oval, and hexagon icons (see Figures 3
and 4). Process and control models are shown as rounded
rectangles and ovals, respectively. A composite model that
consists of two or more models (whether specified in one or
multiple modeling formalisms) is shown as a rectangle. For
example, Figure 3 shows that the manufacturing and control
models as well as their composition are described in Ψ.
A Knowledge Interchange Broker model (Sarjoughian and
Plummer 2002) that composes multiple models specified in
multiple modeling formalisms is shown as a hexagon (see
Figure 4).
A modeling formalism can have several implementations (i.e., software realizations) based on the choice of
programming languages – e.g., a mathematical model can
be designed and implemented using object-oriented modeling concepts and a programming language. Software
realizations of a modeling formalism also can be affected
given the intended or expected modeling and simulation
applications (e.g., parallel/distributed simulation for largescale application domains). In addition, entity relations (ER
models) and the XML family of models are also referred
to as models. Here these are not considered since (i) such
models are primarily aimed at describing structure of data
instead of a system’s structure and the behavior and (ii) they
can be subsumed in component-based and object-oriented
modeling approaches.
Before proceeding further, it is useful to note that
model specifications are also described and implemented
using (high-level) programming languages. Therefore, a
model specification can be referred to as a mathematical model specification or a software model specifica-

tion. Here mathematical specifications are considered (e.g.,
hX,Y, S, δext , δint , δcon f , λ ,tai (Chow 1996) is the mathematical specification for Parallel DEVS atomic model). It is
also helpful to note that different terms are used for execution algorithms. An algorithm which is void of software
design choices and implementations is sometime referred
to as an abstract simulator or solver.
4.2 Mono and Super Model Composability
It is common to use a single modeling formalism to specify
one aspect of a system. Using a mono modeling approach
provides key advantages - decomposition (or hierarchical
composition) of a model into (from) parts can be carried out
systematically (e.g., Wymore 1993). Modeling formalisms
help modelers specify dynamical systems given well-defined
syntax and semantics. The syntax specifies the allowed
structures for inputs, outputs, states, and functions. This is
the model specification. The semantics specify the behavior
of the structural elements. This is the execution algorithm.
For example, discrete-event modeling is often used to
specify discrete processes. A mono modeling formalism
can also be used to specify different aspects of a system detailed process flow dynamics with simplified event-based
control. Using this modeling approach, models of different
parts of a system adhere to a single structure and behavior
specification. That is, MA∪B,{Ψ} may be used to model
discrete-event process model A, event-based control model
B, and their interactions using modeling formalism Ψ. For
the semiconductor manufacturing supply chain systems, the
manufacturing process and control models are identified as
A and B (see Figure 3). For example, these models can be
specified in the DEVS formalism (Zeigler, Praehofer, and
Kim 2000) (shown as in Figure 3(a)). A fundamental benefit
of using a mono modeling formalism is that manufacturing
and control models as well as their interactions are described
in Ψ. This approach significantly simplifies integration of
models and their executions.
Use of a mono modeling formalism, however, may
not be suitable if the models that are to be composed are
intrinsically different — the models are best described in
different modeling paradigms — and are not fully supported. This is because a formalism is suitable to model
only some parts of a complex, heterogeneous system while
all the remaining parts must either be abstracted away or
formulated within other modeling paradigms. To overcome
this difficulty, a super modeling formalism may be used as
shown in Figure 3(b).
In the super modeling formalism approach, a modeling formalism (MB̃,{Φ} ) is encapsulated within the superformalism MA∪B,{Ψ} . Models B̃ and B are different and the
former must be encapsulated inside the latter. A general
approach is to use multiple model specification abstractions
and hide the details of an encapsulated lower-level model

152

Sarjoughian
5
M A<B,{
M A<B,{

}

A model can also be composed from models that are described according to two or more modeling formalisms. In
the meta modeling formalism approach, different types of
model specifications are transformed or abstracted to another
modeling formalism. The meta modeling formalism must
be able to account for the differences between disparate
model specifications that can be composed.
As depicted in Figure 4(a), MÂ∪B̂,{Θ} is a composition
of models with their interactions specified in formalisms Θ .
Here models A and B specified in Ψ and Φ are mapped to Â
and B̂. This requires Ψ → Θ and Φ → Θ. The transformation
→ defines structure and behavior of formalisms Ψ and Φ
to that of the formalism Θ – i.e., the interactions between
models A and B are specified in terms of Â and B̂.
A standardized modeling approach as shown in Figure 4(a) is the High Level Architecture (HLA) (Dahmann
et al. 1999, HLA 2000a, HLA 2000b). It provides a suite
of generalized services where different simulation models
are mapped to. This approach is strongly aimed at handling interoperability needs with some limited capability for
model composability as supported by the Object Model Template (HLA 2000c). The interaction of the different types of
model specifications is supported with the publish/subscribe
technique and data management service while handling of
timed interactive behavior of execution algorithms is the
responsibility of the time management service (Allen 1997,
Fujimoto 1998).

}

M B,{<}
M B,{<}

M B ,{)}

M A,{<}

M A,{<}

(a)

META AND POLY MODEL COMPOSABILITY

(b)

Figure 3: (a) Mono and (b) Super Modeling Formalisms

specification inside an enclosing higher-level model specification - e.g., a simple optimization model described in LP
can be encapsulated as an I/O System (i.e., atomic) DEVS
component. This allows the specification of the interactions
between models A and B to be described within Ψ. This
approach, however, has to ensure the modeling formalisms
that are encapsulated within it have a well-defined relationship with respect to one another. That is, the super modeling
formalism must assert the kinds of disparate dynamics that
can be specified within it. Model encapsulation (e.g., enclosing B̃ inside B) requires the encapsulated model to be
well-defined - the input/output structure and behavior of
the encapsulation of the optimization model is guaranteed
by the enclosing model specification. In the semiconductor
manufacturing supply chain example, a logistic controller
can be a linear optimization model B̃ in formalism Φ and B
can be a discrete-event model in formalism Ψ. This enables
composition of the plant and the controller models to be
specified with the DEVS formalism, for example.
Unlike the above scenario, it is possible for a super
modeling formalism to support model specifications that
are at the same level of abstractions. Super formalism
can support composition of state-based models rather than
composing a state-based model and an input/output model
(Fishwick 1992, Zeigler et al. 2000). This is a strong form
of super-formalism since it supports composition of different
kinds of model specifications and their composition at the
level of state-based specification rather than input/output.
For example, the DEVS formalism can support combined
DEV&DESS modeling (Zeigler 2006) – i.e., model transformation (and thus user-defined model encapsulation) is
unnecessary. Another approach is Ptolemy which supports
modeling of mixed signal systems (Eker et al. 2003). It
formalizes input/output interactions among actors (model
components) under the control of directors.

M A:
C B ,{
M Aˆ 4Bˆ ,{

}

}

M B,{)}
M B,{)}

M C ,{:}

M A,{<}

M A,{<}

(a)

(b)

Figure 4: (a) Meta and (b) Poly Modeling Formalisms
Another approach is to use meta-modeling and model
transformation (Jaramillo, Vangheluwe, and Alfonseca
2002). In this approach, meta-modeling allows determining
whether or not two models described in different modeling
formalisms can be transformed completely or partially de-

153

Sarjoughian
scribed within a meta-modeling formalism. In some cases
a model described in Ψ can be transformed completely into
another model described in Φ (without any loss in model
dynamics) whereas in other cases some constraints must be
defined and placed on models described in Ψ to be represented in Φ. For example, a discrete-time model specified
in Discrete Time System Specification and a discrete-event
model specified in Petri-nets can be completely transformed
into models described in Discrete Event System Specification (Vangheluwe 2000).
Given the semiconductor manufacturing supply chain
system, its manufacturing process and logistic control can
be specified in the DEVS and LP modeling formalisms
(denoted as Ψ and Φ in Figure 4(a)). The composition
of the meta models of A and B is denoted as MÂ∪B̂,{Θ} .
The composite (or federated) DEVS and LP models may
be specified in HLA (denoted as Θ in Figure 4(a)).
The remaining approach, called poly modeling formalism (or multi-formalism modeling), composes disparate
models using a model (referred to as Knowledge Interchange
Broker) which handles the differences between disparate
modeling formalisms (Sarjoughian and Plummer 2002). The
KIB formalism enables composing model specifications and
execution algorithms of disparate modeling formalisms. The
concept and formulation of KIB was developed in the context
of a simple intelligent transportation system (Sarjoughian
and Plummer 2002, Sarjoughian and Huang 2005). As
shown in Figure 4(b), MA∪C∪B,{Ω} is the composition of
MA,{Ψ} and MB,{Φ} , using MC,{Ω} . The interactions between
models A and B are specified in Ω. The KIB approach in
realistic semiconductor manufacturing supply chain systems
has been realized – i.e., detailed models described in the
DEVS, LP, and MPC modeling formalisms are developed
and composed with KIBDEV S/LP and KIBDEV S/MPC (Godding et al. 2004, Sarjoughian et al. 2005, Huang et al.
2006). These introduce modeling capabilities to the KIB
that can support discrete-event and optimization model interactions that are essential in modeling of discrete part
manufacturing supply chain systems.
Unlike the meta modeling formalism approach, in poly
model composability approach models are not transformed
to a set of models all of which are described in accordance
to a single modeling formalism. Furthermore, poly modeling formalism is distinct from the strong form of super
formalism as KIB can both support the interactions (data
and control exchanges) among different model types (e.g.,
discrete-event and linear optimization) and also support different kinds of data transformation and control schemes
that are described external to the models that are composed.
Using the poly model composability approach, common
forms of data transformation (i.e., aggregation and disaggregation) is inherently supported. Furthermore, it supports
alternative forms of common control schemes (i.e., sequential, synchronous, and asynchronous). These general data

transformation and control scheme concepts need to be extended based on individual the application domains that are
may be modeled.
6

MODEL SPECIFICATION AND EXECUTION
ALGORITHM

In the previous section, the composability modeling approaches were described in terms of model specifications.
However, as noted earlier, a formalism is defined as a
pair. The separation of model specification and execution
algorithm enables model specification composability and
execution algorithm interoperability differently depending
on the composability approach (Sarjoughian and Plummer
2002; Godding, Sarjoughian, and Kempf 2004; Sarjoughian
and Huang 2005; Huang et al. 2006). As shown in Figure 5, the former is concerned with composition of syntax
and semantics of different formalisms, whereas the latter is
concerned with execution protocols and their interoperation.
model composability

Model
Specification

Model
Specification

Model
Specification

Execution
Algorithm

Execution
Algorithm

Execution
Algorithm

<

:

)

execution interoperability

Figure 5: Separation between Model Specification and Execution Algorithm
In the mono composability approach, one execution
protocol is used for a set of models that are all defined
using a single model specification approach. In the super
composability, execution algorithms of the enclosing models
and those that are encapsulated are interoperated under
the super formalism’s execution algorithm. That is, the
execution algorithm of the model that is encapsulated in
super formalism is viewed as an atomic operation within
the super formalism execution algorithm.
The meta composability approach, in contrast to super
composability, is primarily concerned with interoperability.
The execution algorithms of the disparate model specifications are cast into model constructs and operations (i.e., a
set of services as in HLA) that are intended for interoperating different execution algorithms. In this approach, model
composability is not strongly supported and the model specification and execution algorithm are weakly separated from
one another.

154

Sarjoughian
The poly composability approach, unlike the other
approaches, is concerned with both model composability
and simulation interoperability (Sarjoughian and Plummer
2002). As shown in Figure 5, not only are modeling specifications of Ψ and Φ composed using the model specification
of Ω, but the execution algorithm is responsible for interoperability between the execution algorithms of Ψ and
Φ.
In the domain of semiconductor manufacturing supply
chain systems, Ψ can be DEVS, Φ can be LP or MPC,
and Ω can be KIBDEV S/LP or KIBDEV S/MPC (Godding et al.
2004, Sarjoughian et al. 2005). In the domain of intelligent
transportation systems, Ψ can be DEVS, Φ can be RAP
(Firby and Fitzgerald 1999), and Ω can be KIBDEV S/RAP
(Sarjoughian and Plummer 2002, Sarjoughian and Huang
2005).
This separation between model composability from simulation interoperability is key given different application domains (Sarjoughian and Plummer 2002). For example, the
DEV S/LP or DEV S/MPC model composability approach
(Godding et al. 2004, Sarjoughian et al. 2005, Huang et al.
2006) is based on the manufacturing process (plant) and decision (control) models to interact with one another based
on a synchronous control scheme consistent with actual
operation of a semiconductor manufacturing supply chain
system. In contrast, the DEV S/RAP model composability
approach (Sarjoughian and Huang 2005) is based on asynchronous interaction between plant and control models. The
differences between control schemes defined for interactions
in the KIBs play a key role in poly model composability
approaches given not only different classes of modeling
formalisms, but also different application domains.
With poly model composability shown in Figure 5 where
only two modeling formalisms are composed, another type
of model (e.g., RAP) that is not well suited to be specified
in Ψ (e.g., DEVS) or Φ (e.g., LP) needs to be cast into
either Ψ or Φ. This suggests, when more than two modeling
formalisms need to be composed, multiple or hierarchical
KIBs are needed or alternatively a combination of super,
meta, and poly formalism may be used instead.
A key practical distinction between the meta and the
poly composability approaches is that the latter can systematically support composition of syntax and semantics of
known disparate modeling formalisms – it provides mappings and rich data transformations. For example, the differences between DEVS and LP/MPC syntax and semantics
is supported with a suite of general mappings and domainspecific data transformations. This is in contrast to the
meta composability approach where differences between
different types of models can be partially accounted for in
the meta modeling formalism, thus requiring all remaining
differences to be handled on a case-by-case basis. This is
because models described in different modeling formalisms

need to be augmented to handle differences that cannot be
handled via meta modeling and interoperability regimes.
A common approach to overcoming differences between
different types of models is illustrated in Figure 6. Adapters
can be used to facilitate model interactions in the absence of
super, meta, and poly formalisms. Here an adapter is needed
to transform the data that is specified in Φ but also needs
to be used with models specified in Ψ. For each modeling
effort, this approach relies on uni- and bi-directional data
and control in terms of software design techniques and
their implementations developed. This approach requires
defining data mappings and transformations individually for
every model that is described either using Φ or Ψ. With this
approach, individual adapters need to be defined separately.
The consequence is that consistency among the definitions
of the adapters must be maintained for all models manually
– i.e., data mappings and transformations must be handled
for every set of models that are defined in disparate modeling
formalisms on a case-by-case. Furthermore, the control of
execution between execution algorithms must be handled as
part of the modeling effort instead of making use of wellknown sequential, synchronous, or asynchronous execution
schemes.

M A,{<}

adapter < m)

adapter < o)

M B,{)}

control
data

Figure 6: Model Integration with Adapters
For example, given the DEVS, RAP, LP, and MPC
formalisms, the models described in them can be augmented
with adapters written to handle the disparities among them.
For example, while inputs and outputs of DEVS model
specification are messages, the inputs and outputs of MPC
model specifications are primitive data types (e.g., integer
and string arrays). This requires customizing DEVS and
MPC models to handle data received for every model.
This is in contrast to a modeling formalism that supports
all models that can be described in the DEVS and MPC
modeling formalisms. The aggregation and disaggregation
of data and alternative control schemes can be supported at
the level of model specifications instead of using adapters
or interoperability services. For example, data and control
interactions between the manufacturing process (plant) and
the optimization control (controller) models are handled

155

Sarjoughian
using modeling constructs that are given in the KIB. With
the poly model composability formalism approach, reuse is
supported at the level of modeling formalisms.

composed, existing approaches to domain-specific modeling
may be used. The existence of the KIB offers a basis to
explicitly account for interaction of domain knowledge.
Given the composability approaches, each modeling
formalism provides a different degree of support for describing domain knowledge. The degree of support for
domain-specific modeling is grounded in the kind of model
composability that is enabled in each approach.

6.1 Role of Domain Knowledge
A general purpose modeling formalism supports describing and executing a model; it is not intended to account
for domain-specific modeling. Therefore, until now, model
composability has been considered independent of the application domains to which it may be applied. However,
given the central role domain knowledge plays in developing
composable models, it is important to consider it for model
composability. Domain-neutral modeling formalism is key
for modeling specific systems (application domains). This is
because the complexity of composite models is in part due to
the application domain that is being modeled. For example,
discrete-event modeling can be used in the domains of manufacturing, information systems, and event-based control.
The interactions to be modeled among disparate models
need not only to capture data transformations and executions between composed models, but also be appropriate to
the system being modeled since general purpose modeling
formalisms are void of domain knowledge. For example,
data transformation between a discrete-event model and an
LP optimization model of a manufacturing process has to
handle a list of products that are specialized to hold Finished
Goods having some defect distribution (Godding et al. 2004,
Huang et al. 2006). The frequency between the process and
controller models can also depend on the domain – e.g.,
interactions can be sequential or synchronous.
Domain specific modeling is not only central to the
mono and the super model composability approaches, but
also the meta and the poly model composability approaches.
Separating domain-neutral and domain-specific model interactions is also key for handing general purpose modeling
concepts while extending them with specific needs of a domain. Given the separation between model specification and
the execution algorithm of a modeling formalism, the model
specification can be extended to support domain knowledge.
In the case of the mono and the super formalism model composability which are based on the concept of components,
general purpose model components can be specialized for
a given domain. For example, DEVS model components
are specialized to the entities of a manufacturing process
network. In the case of the meta model composability and
HLA, in particular, domain specific federates are modeled
using specialization supported by object-oriented concepts
and methods.
In addition, data engineering (XML family of data
representations and ontologies) may also be used for handling static (i.e., non-behavioral) domain-specific knowledge
(Tolk and Diallo 2005). For poly model composability, due
to strong separation between modeling formalisms that are

6.2 Distributed Execution of Composable Models
Another consideration for model composability is distributed
execution of execution algorithms and their interoperation.
All of the composability approaches lend themselves to
distributed execution. For example, HLA supports distributed execution of federates and federations (Fujimoto
1998). These allow distributed execution of models synthesized using any of the mono, super, meta, and poly model
composability approaches.
To support distributed execution of models, it is important to develop software design and implementation that
account for efficient data exchanges and overall execution
speed. Given disparate model specification and execution
environments (e.g., DEVSJAVA (ACIMS 2001), Opl-Studio
(ILOG 2005), and Matlab/SIMULINK (Mathworks 2005)),
other considerations (e.g., scale of individual models and
data exchanges, frequency of model interactions, and length
of experiments) that can affect execution of combined models must be accounted for. This is because the separation of
model specification and execution algorithm can adversely
affect execution of the composed models. Nonetheless,
given sound software designs and implementations, models
that are specified with an eye on simple model designs and
appropriate use of programming constructs can be executed
efficiently.
7

CONCLUSIONS

A classification of model composability approaches is presented. Formulation for each of the model composability
approaches is described in terms of modeling formalisms.
These approaches are described from a multi-layer modeling vantage point to highlight the importance of using
different modeling formalisms. How disparate modeling
formalisms affect model composability is described using
a simplified example from the domain of semiconductor
supply chain systems. The discussion on separating model
specifications and execution algorithms reveals the impact
of model composability choices and in particular support for
varying degrees of model compositions and consequently
limitations and complexity of specifying disparate composite models, level of support for domain-specific modeling,
and degree of support for distributed execution.

156

Sarjoughian
ACKNOWLEDGMENTS

Eker, J., J. Janneck, E. Lee, J. Liu, X. Liu, J. Ludvig,
S. Neuendorffer, S. Sachs, and Y. Xiong. 2003. Taming
heterogeneity — the Ptolemy approach. Proceedings of
the IEEE 91 (2): 127–144.
Firby, R., and W. Fitzgerald. 1999. The RAP system language manual, version 2.0. Evanston, IL: Neodesic
Corporation.
Fishwick, P. 1992. An integrated approach to system modelling using a synthesis of artificial intelligence, software engineering and simulation methodologies. ACM
Transactions on Modeling & Computer Simulation 4
(2): 307–330.
Fishwick, P. 1995. Simulation model design and execution:
building digital worlds. Prentice Hall.
Fujimoto, R. 1998. Time management in the High-Level
Architecture. Simulation 71 (6): 388–400.
Fujimoto, R. 2000. Parallel and distributed simulation systems. John Wiley and Sons, Inc.
Fujimoto, R., D. Lunceford, E. Page, and A. Uhrmacher.
2002. Grand challenges for modeling and simulation.
Schloss Dagstuhl.
Godding, G., H. Sarjoughian, and K. Kempf. 2004. Multiformalism modeling approach for semiconductor supply/demand networks. In Proceedings of Winter Simulation Conference, 232–239. Washington DC, USA.
Hall, S. 2005. Learning in a complex adaptive system for
ISR resource management. In Proceedings of Spring
Simulation Conference, 5–12. San Diego, CA.
HLA. 2000a. IEEE high level architecture framework and
rules — IEEE 1516-2000. IEEE.
HLA. 2000b. IEEE high level architecture framework and
rules — IEEE 1516.1-2000. IEEE.
HLA. 2000c. IEEE high level architecture object and model
template — IEEE 1516.2-2000. IEEE.
Huang, D., H. Sarjoughian, D. Rivera, G. Godding, and
K. Kempf. 2006. Flexible experimentation and analysis
for hybrid DEVS and MPC models. In Proceedings of
Winter Simulation Conference. Monterey, CA, USA.
ILOG. 2005. OPL Studio. Available from <http:
//www.ilog.com/products/oplstudio/>
[cited 2005].
Jaramillo, J., H. Vangheluwe, and M. Alfonseca. 2002.
Using meta-modelling and graph grammars to create
modelling environments. Electronic Notes in Theoretical Computer Science 72 (3).
Kasputis, S., and H. Ng. 2000. Composable simulations. In
Proceedings of Winter Simulation Conference, 1577–
1584. Orlando, FL, USA.
Kempf, K. 2004. Control-oriented approaches to supply
chain management in semiconductor manufacturing. In
Proceedings of IEEE American Control Conference,
4563–4576. Boston, MA, USA.
Mathworks. 2005. MATLAB/Simulink. Available from
<http://www.mathworks.com> [cited 2005].

The author acknowledges support of this work by Intel,
Lockheed Martin, and the National Science Foundation.
Thanks are due to Dongping Huang, Gary Mayer, Hans
Mittelmann, Daniel Rivera, and Wenlin Wang of Arizona
State University, Gary Godding, Karl Kempf, and Kirk Smith
of the Intel Corporation, and Steven Hall of Lockheed Martin
Space Systems for stimulating discussions on integrating
agent, discrete-event, linear optimization, discrete-time, and
cellular automata models in the domains of semiconductor
manufacturing supply chain systems, system of systems, and
human landuse studies. Acknowledgement is due to Paul
Davis of the Rand Corporation for fruitful discussions on
simulation composability and a review of an earlier version
of this paper. Special thanks to David Goldsman of Georgia
Tech for his encouragement and support of this paper.
REFERENCES
ACIMS. 2001. DEVSAJVA, Arizona Center for Integrative
Modeling and Simulation. Available from <http://
www.acims.arizona.edu/> [cited 2006].
Allen, R. 1997. A formal approach to software architecture.
Ph. D. thesis, Computer Science Department, Carnegie
Mellon University, CMU-CS-97-144.
Banks, J., J. Carson, B. Nelson, and D. Nicol. 2004. DiscreteEvent System Simulation. 4th ed. Prentice Hall.
Barros, F. 2002. Modeling and simulation of dynamic structure heterogeneous flow systems. Simulation: Transactions of the Society for Modeling and Simulation
International 78 (1): 18–27.
Barros, F., and H. Sarjoughian. 2004. Guest editorial:
Component-based modeling and simulation. Transactions of The Society for Modeling and Simulation International 80:319–320.
Cellier, F. 1991. Continuous system modeling. Springer
Verlag.
Chow, A. 1996. Parallel DEVS: A parallel, hierarchical,
modular modeling formalism and its distributed simulator. Simulation: Transactions of the Society for Modeling and Simulation International 13:55–67.
Dahmann, J., C. T. M. Salisbury, P. Barry, and P. Blemberg.
1999. HLA and beyond: Interoperability challenges.
In Simulation Interoperability Workshop. Orlando, FL,
USA.
Davis, P., and R. Anderson. 2004. Improving the composability of DoD models and simulations. Journal of Defense
Modeling and Simulation: Applications, Methodology,
Technology 1 (1): 5–17.
Davis, P., C. Overstreet, P. Fishwick, and C. Pegden. 2000.
Model composability as a research investment: Responses to the featured paper. In Proceedings of Winter
Simulation Conference, 1585–1591. Orlando, FL.

157

Sarjoughian
Mosterman, P., and H. Vangheluwe. 2002. Guest editorial.
ACM Transactions on Modeling and Computer Simulation 12 (4): 249–255.
Page, E., and J. Opper. 1999. Observations on the complexity
of composable simulation. In Proceedings of Winter
Simulation Conference, 553–560. Orlando, FL, USA.
Praehofer, J. 1991. System theoretic formalisms for combined discrete-continuous system simulation. International Journal General Systems 19 (3): 219–240.
Qin, S., and T. Badgwell. 2003. A survey of industrial
model predictive control technology. Control Engineering Practice 11 (7): 733–764.
Rardin, R. 1998. Optimization in Operations Research. Prentice Hall.
Sarjoughian, H., and F. Cellier. (Eds.) 2001. Discrete event
modeling and simulation technologies: a tapestry of systems and AI-based theories and methodologies. Springer
Verlag.
Sarjoughian, H., and D. Huang. 2005. A multi-formalism
modeling composition framework: Agent and discreteevent models. In Proceedings of the 9th IEEE International Symposium on Distributed Simulation and Real
Time Applications, 249–256. Montreal, Canada.
Sarjoughian, H., D. Huang, W. Wang, D. Rivera, K. Kempf,
G. Godding, and H. Mittelmann. 2005. Hybrid discrete
event simulation with model predictive control for semiconductor supply-chain manufacturing. In Proceedings
of the Winter Simulation Conference, 256–266. Orlando,
FL, USA.
Sarjoughian, H., and J. Plummer. 2002. Design and implementation of a bridge between RAP and DEVS.
Computer Science and Engineering, Arizona State University, Tempe, AZ. Internal Report.
Sarjoughian, H., and B. Zeigler. 2000. DEVS and HLA:
Complementary paradigms for modeling and simulation? Transactions of the Society for Modeling and
Simulation International 17 (4): 187–197.
SBA. 1998. Simulation based acquisition: a new approach.
Defense Systems Management College. Report of the
Military Research Fellows DSMC.
Singh, R., H. Sarjoughian, and G. Godding. 2004. Design
of scalable simulation models for semiconductor manufacturing processes. In Proceedings of the Summer
Computer Simulation Conference, 235–240. San Jose,
CA, USA.
Tolk, A., and S. Diallo. 2005. Model-based data engineering for web services. IEEE Internet Computing July/August:54–59.
van Beek, D., S. Gordijn, and J. Rooda. 1997. Integrating
continuous-time and discrete-event concepts in modelling and simulation of manufacturing machines. Simulation Practice and Theory 5:653–669.
Vangheluwe, H. 2000. DEVS as a common denominator for
multi-formalism hybrid systems modelling. In IEEE In-

ternational Symposium Symposium on Computer-Aided
Control System Design. Anchorage, Alaska: IEEE
Computer Society Press.
Wang, W., D. Rivera, and K. Kempf. 2005. A novel model
predictive control algorithm for supply chain management in semiconductor manufacturing. In American
Control Conference, 208–213. Portland, OR, USA.
Wang, W., D. Rivera, K. Kempf, and K. Smith. 2004. A
model predictive control strategy for supply chain management in semiconductor manufacturing under uncertainty. In American Control Conference. Boston, MA,
USA.
Wymore, A. 1993. Model-based systems engineering: an
introduction to the mathematical theory of discrete systems and to the tricotyledon theory of system design.
Boca Raton:CRC.
Zeigler, B. 2006. Embedding DEVS&DESS in DEVS. In
DEVS Integrative Modeling & Simulation Symposium,
125–132. Huntsville, AL, USA.
Zeigler, B., H. Praehofer, and T. Kim. 2000. Theory of
modeling and simulation: integrating discrete event and
continuous complex dynamic systems. 2nd ed. Academic
Press.
AUTHOR BIOGRAPHY
HESSAM S. SARJOUGHIAN is Assistant Professor of
Computer Science and Engineering at Arizona State University, Tempe and Co-Director of the Arizona Center for
Integrative Modeling and Simulation. His research includes simulation modeling theories and methodologies with
emphasis on multi-formalism model composability, visual
component-based system modeling, collaborative modeling, co-design modeling, agent-based modeling, software
architecture, and distributed simulation. His educational
activities have led to the establishment of an Online Masters of Engineering in Modeling & Simulation in the Fulton School of Engineering at ASU. For more information
contact the author at <hss@asu.edu> or visit <http:
//www.eas.asu.edu/˜hsarjou/index.htm>.

158

IN TEG RAT E D E NGI N E E RING

tocols, and software objects—with varying degrees of resolutions and complexities.

A Codesign
Approach for
Distributed
Systems

DISTRIBUTED SYSTEMS CODESIGN

Hessam S. Sarjoughian and Bernard P. Zeigler,
University of Arizona, Tucson
Daryl R. Hild, Mitre Corp.

D

istributed systems have become
prevalent in response to the
rapidly expanding Internet’s
demands. Their design presents
new challenges because it involves the interaction of hardware and
software. Continual marketplace innovation drives computing toward heterogeneity in both hardware and software
and generates a complexity that goes
beyond the earlier codesign approaches,
which were developed for more homogeneous systems executing in non-distributed environments. Codesign of heterogeneous systems requires the support of a
powerful modeling and simulation environment because analysis alone cannot
deal with all the challenges such complex
systems pose. Consider the design of realtime, distributed training systems where
hundreds of simulated and live entities
must be accounted for. Specifying legitimate, acceptable software configurations—how to distribute a set of cooperating software applications—for candidate distributed network topologies depends on detailed simulation studies.
Without disciplined, repeatable simulation studies, evaluation and selection of
suitable design alternatives among many
hundreds of choices are simply impractical. The design of e-commerce technologies such as stock trading provides

110

Computer

another example in which communication latencies among software applications must be controlled and minimized
to ensure customer satisfaction.

Our approach to distributed codesign
is derived from a modeling paradigm proposed by James M. Butler (“Quantum
Modeling of Distributed Object Computing,” Proc. 28th Ann. Simulation
Symp., IEEE CS Press, Los Alamitos,
Calif., 1995). As shown in Figure 1,
Butler’s approach models distributedobject computing (DOC) systems as a set
of two distinct layers that map to each
other: the software layer, labeled distributed cooperative objects (DCO); and the
hardware layer, labeled loosely coupled
network (LCN). The object system mapping (OSM) defines the allocation of software objects to hardware objects. Based
on DOC’s separation of concerns and
their mapping, we’ve developed DEVSDOC atop DEVSJAVA—a parallel
DEVS-based modeling and simulation
environment. These modeling artifacts let
the designer model software and hard-

Today’s distributed systems
require a tighter level of hardware and software integration.
The DEVS-DOC framework
provides a codesign solution.
THE DEVS FRAMEWORK
We believe that modeling and simulation, using the Discrete-Event System
Specification modeling and simulation
framework (Bernard P. Zeigler, Herbert
Praehofer, and Tag Gon Kim, Theory
of Modeling and Simulation, 2nd edition, Academic Press, Burlington, Mass.,
2000), are the most suitable vehicles to
study the complexities associated with
developing distributed-object computing
systems. DEVS supports representation
of discrete time, discrete events, and a
large class of continuous systems. With
DEVS, we can systematically develop
scalable models of a distributed system’s
components—such as processors, networking topologies, communication pro-

ware components concurrently. Further,
DEVS provides the means to bind together these components to enable their
collective simulation and analysis.
With these modeling artifacts, the designer can follow a process that concurrently models software- and hardwarelayer components. We decompose the system modeling effort into four steps:
• Define the network by identifying
processing nodes, gates, and links.
• Define the software objects and how
they might interact with one another.
• Map the software objects onto the
processing nodes.
• Define an experiment to stimulate
the model during simulation and to

collect pertinent data for functional
and behavioral analysis.
Following Butler’s terminology and conventions, the first three steps result in the
following products:

Software
object

Distributed cooperative objects
Arc

• a loosely coupled network,
• a set of distributed cooperative objects, and
• an object system mapping.

Object
system
mapping

LCN-layer components
Abstracting networked computer components with the LCN results in the specification of processors, network gates,
and links. A DEVS component represents
the dynamics of each LCN component—
either as an atomic or a coupled DEVS
model. Coupled models contain a hierarchical composition of atomic and coupled
DEVS components.
The processors serve as computing
nodes on which software objects of the
DCO layer can be loaded and executed.
The two critical parameters for these processing nodes are storage capacity and
speed. Processor storage capacity constrains software objects in their competition for memory resources. Processor
speed constrains the rate at which software jobs are processed.
Network gates in the LCN represent
hubs and routers. The critical parameters
that constrain gate performance are operating mode (hub or router), buffer capacity, and bandwidth. Links represent the
communication media, such as a T1 carrier link, between processors and gates.
Critical link parameters include number
of channels, bandwidth per channel, and
error rates. We define the LCN network
topology by mapping the processors and
gates onto the links.

DCO-layer components
The DCO abstraction of software components specifies computational domains,
software objects, methods, and object
interaction arcs. We also represent these
components as DEVS components.
A set of software objects form a computational domain, which represents an
executable application. Software objects
can belong to multiple domains and a
software object’s collective-memory re-

Processor

Link
Gate

Loosely coupled network

Figure 1. Distributed-object computing framework. The oval shapes indicate software objects.

quirements for attributes and methods
characterize the object’s size. Upon the
instantiation of a software object, its size
is assigned to a processor’s memory.
Software objects can invoke themselves
autonomously. Each software object has
a thread-mode parameter, which defines
the granularity of its multithreaded behavior as method, object, or none. At the
method level, all requests to the object can
execute concurrently. At the object level,
only one request per method may execute
concurrently; additional requests against
an executing method enter a queue. At the
none level, only one request per object
executes at a given time; the system
queues any additional request to the
object for later processing.
Software object methods are characterized by a computational workload factor and an invocation probability. The
computational workload factor represents the amount of processing required
to completely execute the method. The
invocation probability indicates how
often the system invokes methods.
Messaging arcs and invocation arcs,
shown as arrows in Figure 1, let software
objects interact with one another. A messaging arc represents peer-to-peer message
passing. When a source object sends a
message, it can target multiple destination
objects. The frequency of firing a messaging arc is based on the computational

progress of the source software object.
An invocation arc represents clientserver-type interactions such that when
the client object fires an invocation arc,
the message-size parameter specifies the
amount of data sent. The destination
server object invokes its own method and
sends a response after execution completes. Invocation arcs employ the two
common types of synchronous and asynchronous blocking modes. In synchronous mode, a client object cannot proceed
with any other method invocations until
it receives the response from its previously
issued invocation. In asynchronous mode,
the client object can continue with method
execution independent of the invocation
response.
When software objects execute on dispersed processing units, the invocation
and messaging arcs route through the
gates and links of the LCN layer. Upon
receipt of an incoming arc, a software
object loads into the memory of its
assigned processor.
Arc reception also triggers the software
object to select an execution method. The
system sets up the selected method as a
unique job thread, then selects either initiated-for-execution or queued status based
on the software object multithreading
mode and its current state. Initiated jobs
go to their assigned processor for execution and, upon completion, the software
March 2000

111

Integrated Engineering

Network Management Protocol Example
Using DEVS-DOC, we modeled a Simple Network Management Protocol (SNMP) monitoring system, shown in Figure
A, and compared it against its real-world counterpart (Daryl
R. Hild, DEVS-DOC Modeling and Simulation, doctoral dissertation, The University of Arizona, Tucson, Electrical and
Computer Engineering Department, 2000). The system’s LCN
layer consists of five processors interconnected with 10-Mbps
Ethernet links through a central hub with a star topology. The
DCO layer consists of SNMP agents for each of the processors. Two software objects, the SNMP manager and a loop
controller, provide the remaining DCO layer components and
are both assigned to one of the five processors. We also defined
an experimental frame for the computational domain that consists of SNMP agents, manager, and loop controller.

The SNMP monitoring system shown in Figure A uses the
loop controller to fire an arc, which in turn invokes the SNMP
manager. We set the SNMP manager’s multithreading behavior at object level. The simulation results match closely the
real-world experimental results which were obtained using the
Unix System Activity Reporter utility. The SAR report shows
that the experiments with a higher number of arc firings consume increasingly more time due to data transfers into and out
of memory. The close match shows the significance of accounting for a distributed system’s combined hardware and software components. By changing the system’s underlying
network topology or software configurations, we can easily
study alternative design decisions and how well each may satisfy user’s requirements.
CEC/ITP

MS DOS Prompt
File

snmpwalk#

loop
controller

Edit

Locate

View

Option

Monitor

Administrator

Diagnose

Misc

Help

snmp
mgr

CEC/ITP [Auto-Layout]
Close

Home

Root

Parent

snmpget#
Event Log Settings

Event Log Settings
OK

Cnfef Meineoldfkef envo

Event Log Settings
OK

Cnfef Meineoldfkef envo

Cancel
Mgone ner yoop

Default

Eenef Lwqonf Kawibe

snmp4

Help

Event Log Settings
OK

Cnfef Meineoldfkef envo

Cancel
Mgone ner yoop
Eenef Lwqonf Kawibe

Default
Help

snmp6

Event Log Settings
OK

Cnfef Meineoldfkef envo

Cancel
Mgone ner yoop

Default

Eenef Lwqonf Kawibe

snmp15

Help

OK

Cnfef Meineoldfkef envo

Cancel
Mgone ner yoop

Default

Eenef Lwqonf Kawibe

snmp17

Help

Cancel
Mgone ner yoop

Default

Eenef Lwqonf Kawibe

Owonef Noefnvnts as Needed

Owonef Noefnvnts as Needed

Owonef Noefnvnts as Needed

Owonef Noefnvnts as Needed

Omne Mnoef nts Other Than

Omne Mnoef nts Other Than

Omne Mnoef nts Other Than

Omne Mnoef nts Other Than

Dowe Meonf Jjkeonv a Nokwer Mmnowefn

Dowe Meonf Jjkeonv a Nokwer Mmnowefn

Dowe Meonf Jjkeonv a Nokwer Mmnowefn

Dowe Meonf Jjkeonv a Nokwer Mmnowefn

Dowe Meonf Jjkeonv a Nokwer Mmnowefn

Pasc4

Pasc6

Pasc15

Pasc17

L15
L6

Help

snmp19

Owonef Noefnvnts as Needed
Omne Mnoef nts Other Than

Pasc19

L17
Ghub

L19

L4

Figure A. An SNMP monitoring system. The five processors of the loosely coupled network layer connect via 10-Mbps Ethernet links through
a central hub with a star topology. The SNMP agents for each processor, and the SNMP manager and loop controller software objects, comprise the system’s distributed cooperative objects layer.

object’s computational progress is updated.
Based on computational progress, the
software objects can select additional interaction arcs for exchanges with other software objects. Selected arcs fire (transmit)
across the LCN. Job threads that trigger
arc firings continue execution unless
112

Computer

blocked by the firing of a synchronous arc.
In this case, the job thread continues execution after receipt of the associated return
arc. When all job threads complete execution, and all communicating arcs that
expect return arcs receive them, the object
unloads the processor memory.

Integrating software
and hardware components
To support software-to-hardware
mapping, a processor represents a composition of CPU, disk, system memory,
system bus, and I/O ports. Likewise, a
gate represents the combination of a net-

Modeling layer

Distributed object computing
modeling constructs
DEVS-DOC

work interface and an intraprocessor
communication channel for cohosted
interacting software objects. We then represent the assignment of software objects
to processors as DEVS coupled models.
Given a set of software objects and a
processor, software objects will communicate with the processor using DEVS
ports and couplings.
To visualize this, consider a software
object’s method selected for execution.
Selection requires sending a computational job to a processor for execution:
For example, output port outJobs of each
software component can be coupled to
input port inJobs of the processor. On
completion of the computation, the system sends the job to the software object
via another coupling—the processor’s
output port outJobs to the software
object’s input port doneJobs.
Aside from assigning DCO software
objects to LCN processors, we must also
represent how interaction arcs can be
mapped onto links. A link’s communication mode specifies how it segments data
from DCO interaction arcs into packets
and transmits data. The communication
mode constrains packet size, which divides an arc into packets, as well as prescribing packet overhead size, packet
acknowledgment size, and acknowledgment time-out values.

DESIGN METRICS
With distributed codesign, we intend to
obtain metrics to support selection and partitioning of software and hardware components. A variety of experimental frames
can collect metrics for attributes of interest
for hardware and software components.
Some metrics combine attributes of both
hardware and software components.
For example, in simulating a distributed system’s set of processors and software objects, we can measure the
computational workload performed and
the percentage of active objects. Similarly,
we can compute utilization of storage for
processors, execution times for computational domains, and data traffic for
gates and interaction arcs. Designers can
also obtain metrics for a specific component such as total execution time for a
software computational domain. Further,
designers can supplement the set of pre-

Auxiliary modules
(GUI)
DEVS modeling constructs

Distributed simulation
(DEVS/CORBA)
Persistent object store

Middleware (CORBA)

Uniprocessor simulation
Parallel DEVS (DEVSJava)

Operating system

Simulation layer

Figure 2. The DEVS-DOC modeling and simulation environment’s loosely coupled network and
distributed cooperative objects components can be executed in either a centralized or distributed setting.

defined metrics by devising additional
metrics of their own for specific domains.

DEVS-DOC MODELING AND
SIMULATION ENVIRONMENT
Figure 2 depicts the DEVS-DOC modeling and simulation environment. The
LCN and DCO components can be executed in a centralized setting or, alternatively, in a distributed setting using suitable
middleware technologies such as those for
the Common Object Request Broker
Architecture (CORBA). Reusing DEVSDOC model components in a distributed
setting is especially valuable. Simulating
models of distributed-object computing
systems is often computationally intensive.
DEVS-DOC allows system analysts and
designers to execute their elaborate models in a high-performance setting once their
simpler counterparts have been verified for
correctness on standard sequential computing platforms.
The component-based modeling and
simulation architecture emphasizes the
modularity of modeling constructs for
distributed objects and the reuse of primitive DEVS modeling constructs. The
auxiliary module in Figure 2 suggests a
means to add specialized capabilities such
as customized GUI.

he DEVS-DOC environment provides powerful modeling capabilities
to express distributed-object workloads and the hardware processor and

T

network platforms they execute on. More
importantly, the environment enables
designers to explore various alternative
mappings of a software object domain
into potential hardware implementations,
gaining insight into their combined performance and helping to select the more
promising candidates. DEVS-DOC itself
is a distributed system since simulations
can be executed in the DEVS/CORBA
simulation environment, thus supporting
the execution of the complex, heterogeneous realistic models necessary for network-centric computing design. ✸
Hessam S. Sarjoughian is an assistant
research professor in the Electrical &
Computer Engineering Department at
the University of Arizona, Tucson. Contact him at hessam@ece.arizona.edu.
Bernard P. Zeigler is a professor in the
Electrical & Computer Engineering
Department at the University of Arizona,
Tucson. Contact him at zeigler@ece.
arizona.edu.
Daryl R. Hild is lead engineer at Mitre
Corp. Contact him at d.hild@ieee.org.
Editors: Jerzy W. Rozenblit, University of
Arizona, ECE 320E, Tucson, AZ 85721;
jr@ece.arizona.edu; and Sanjaya Kumar,
Honeywell Technology Center, MS MN652200, 3660 Technology Dr., Minneapolis,
MN 55418; skumar@htc.honeywell.com.

March 2000

113

12th IEEE International Workshop on Future Trends of Distributed Computing Systems

Developing Service-based Software Systems with
QoS Monitoring and Adaptation
S. S. Yau, N. Ye, H. Sarjoughian and D. Huang
Arizona State University, Tempe, AZ 85287-8809, USA
{yau,nongye, hessem.sarjoughian, dazhi.huang}@asu.edu
characteristic of SOA, and facilitates adaptation of
SBS in run-time.
Fundamental changes to current software
engineering techniques are needed for designing highquality SBS due to the unique characteristics of SBS.
A major problem to achieve this goal is how to
manage the quality of service (QoS) of SBS, which
may demand satisfaction of multiple QoS
requirements simultaneously, such as timeliness,
throughput,
accuracy,
security,
dependability,
survivability and availability. Because the satisfaction
of requirements in one QoS feature often requires
certain sacrifice in other QoS features, tradeoffs of
requirements among multiple QoS features must be
taken into account in the design of SBS. However,
existing software engineering techniques do not
support the analysis and adaptive control of such
tradeoffs due to lack of comprehensive understanding
of such tradeoffs and their relations to service
operations on computers and networks. Furthermore,
SBS often comprise services owned by various
providers. Each service may support various service
configurations, each of which defines the runtime
properties of the service, such as authentication
mechanism, priority, and maximum bandwidth
reserved for the service. Different service
configurations will result in different QoS in runtime.
The services in SBS often operate in highly dynamic
environments, where the services may become
temporarily unavailable due to various system and
network failures, overloads or other causes. Hence,
high-quality SBS need to have the capability to
monitor the changing system status, analyze and
control tradeoffs among multiple QoS features, and
adapt their service configurations accordingly. Such
SBS are referred as Adaptive SBS (ASBS).
The conceptual view of ASBS is depicted in
Figure 1, in which functional services used to compose
the ASBS and the modules for QoS M/A form a
closed control loop [1]. The QoS monitoring modules
collect the measurements of various QoS features as

Abstract
The rapid adoption of SOA in many large-scale
distributed applications requires the development of
adaptive service-based software systems (ASBS),
which have the capability of monitoring the changing
system status, analyzing and controlling tradeoffs
among multiple QoS features, and adapting its service
configuration to satisfy multiple QoS requirements
simultaneously. In this paper, a performance-modeloriented approach to developing ASBS is presented.
This approach consists of the establishment of
performance models for SBS through controlled
experiments, the development of QoS monitoring and
adaptation (M/A) modules based on the performance
models, and the validation of ASBS design through
simulations. In our approach, four QoS features:
timeliness, throughput, accuracy and security, which
are important for many critical applications, are
considered.
Keyword: Adaptive service-based software systems,
QoS monitoring, QoS adaptation, performance
modeling, SOA-based simulation

1. Introduction
Recent
development
of
service-oriented
computing and grid computing has led to rapid
adoption of Service-Oriented Architecture (SOA) in
distributed computing systems, such as enterprise
computing infrastructures, grid-enabled applications,
and global information systems. One of the most
important advantages of SOA is the capability that
enables the rapid composition of the needed services
provided by various service providers through
networks for distributed applications and integration of
the “system of systems”. Software systems based on
SOA are called service-based software systems (SBS).
Late binding with services is a fundamental

1071-0485/08 $25.00 © 2008 IEEE
DOI 10.1109/FTDCS.2008.44

74

simulation approaches [7-12]. The simulation
environment DEVS/DOC [7], which supports
distributed object computing concepts, may be
used to design distributed software systems,
but it is not suitable for ASBS due to lack of
modeling constructs which are needed for
describing QoS features in SBS. Other
simulation approaches and tools, such as HLA
[8], SimEvents [9] and OPNET [10], are not
suitable for developing ASBS unless they are
extended with new abstractions that can
describe key characteristics of service-oriented
computing. The UML 2.0 has been suggested
to support simulating software systems, but its
concept of time and execution protocol is
limited [11]. The process specification and
modeling language (PSML) [12] is useful in
designing, implementing, and testing services
with simulation support. But it lacks direct
support for simulating time-based services,
which is important for describing dynamics of ASBS.
In our approach, simulation models that are grounded
in service-oriented computing concepts will play a
central role in validating the dynamics of ASBS with
multiple QoS features.

Figure 1. A conceptual view of ASBS
well as system status concerning QoS. Based on the
system status and QoS measurements, decisions on
QoS adaptation are made to adjust the configurations
and service operations of ASBS to satisfy various QoS
requirements simultaneously. In this paper, we will
present a performance-model-oriented approach to
developing ASBS. Our approach includes the
performance models for SBS based on controlled
experiments, development of QoS M/A modules based
on the performance models, and validation of ASBS
design through simulations. In our approach, we
consider four QoS features: timeliness, throughput,
accuracy and security, which are important for many
critical applications.

3. Overview of Our Approach
Our approach to developing ASBS consists of the
following three major steps:
S1) Gather the knowledge of the underlying causeeffect dynamics that drive the performance and
tradeoffs among the four QoS features (timeliness,
throughput, accuracy and security) based on
controlled experiments and data analysis.
S2) Develop QoS M/A capabilities in ASBS based on
the knowledge gathered in S1).
S3) Validate the QoS M/A capabilities developed in
S2) through SOA-based simulations.
Although our approach aims at providing a
general methodology for developing ASBS, it is
obvious that ASBS in various application domains
usually have different QoS requirements, resource
requirements, and workload patterns. Taking these
domain-specific characteristics into consideration,
especially in controlled experiments and data analysis
in S1), will effectively reduce the effort for knowledge
gathering in S1) and improve the quality of ASBS
being developed. Currently, we are developing and
evaluating our approach with a group of applications
utilizing various multimedia services, including video
streaming, voice communication, and motion detection
in real-time video streams. In Sections 4-6, we will
present each step in our approach respectively.

2. Related Work
Currently, software design is usually based on
logic-based
operational
models.
However,
performance models linking service operations to
resource states and QoS performance are needed for
QoS monitoring and adaptation in ASBS. Individual
QoS features related to various activities and resource
states in computing and communication systems have
been investigated both empirically and theoretically
[2-4]. However, how system activities and resource
states affect the tradeoffs among various QoS features
has not been thoroughly investigated [5, 6]. The lack
of such essential knowledge makes it difficult to
design ASBS.
To provide a cost-effective way for validating the
design of an ASBS with QoS M/A capabilities to meet
the QoS requirements for the ASBS, it is necessary to
have proper modeling and simulation support for
ASBS, which cannot be provided by existing

75

4. Controlled Experiments for Modeling
QoS-related Cause-Effect Dynamics
As shown in Figure 1, the QoS M/A modules are
at the core of ASBS because they provide the crucial
capabilities, including measuring QoS performance
and system status, making decisions for adapting the
configurations and service operations in ASBS.
However, these modules cannot be developed without
knowing the QoS-related cause-effect dynamics in
ASBS, especially the following three basic aspects:
i. QoS composition: How the overall QoS of an
ASBS is determined by the QoS of individual
services used to compose the ASBS.
ii. QoS tradeoff: How a QoS feature of an ASBS
affects other QoS features of the ASBS.
iii. Environment and configuration impact: How
the QoS of a service varies when the
configuration of the service or the status of the
execution environment changes.
Hence, it is necessary to have a systematic way to
gather the knowledge on the underlying cause-effect
dynamics driving the performance and tradeoffs
among the four QoS features being considered in our
approach. Figure 2 depicts the cause-effect chain of
user activities, system resource states, system events
and QoS performance. A service request from a user
calls for a system process, which will utilize certain
resources, change the states of the resources and
generate certain events in the system environment. The
changes of the resource state, the generated events and
other extraneous events in the system environment in
turn affect the QoS performance of the process. The
events in the system environment, especially the
extraneous events like network failures and malicious
attacks, reflect the disturbances on the process, and
hence also affect the QoS performance of the process.
The performance models revealing such cause-effect
dynamics in SBS are called the Activity-State-EventQoS (ASEQ) models. Constructing the ASEQ models
is challenging because the differences in the types of
services, ways of service composition, and even
service implementations will have different
characteristics of resource consumptions which in turn
produce different characteristics of various QoS
features and the tradeoffs among them.
We have developed an approach to constructing
the ASEQ models. Our approach consists of the
following three steps:
A1) Design controlled experiments in SBS to collect
data related to user activities, system resource states,
system events, and QoS performance under various
experimental conditions.
A2) Implement and execute the experiments.

Figure 2. The cause-effect chain of
activity-state-event-QoS in SBS
A3) Analyze the collected data to identify the causeeffect chains among activities, states, events, and QoS
performance in SBS.
A2) is straightforward, and will not be discussed
here. We will first discuss ASEQ models for the four
identified QoS features, and then A1) and A3).

4.1 ASEQ Models for Timeliness, Throughput,
Accuracy, and Security
Since ASEQ models should reveal the causeeffect chains of user activities, system resource states,
system events and QoS performance, we need to
define the following components before constructing
ASEQ models for the four identified QoS features:
(1) Abstractions of user activities
(2) Abstractions of system events
(3) Abstractions of system resource states
(4) Evaluation of QoS performance
The first three components are common for the
four QoS features, whereas the fourth component is
distinct for them due to their different nature.
Because user activities in ASBS are carried out
through service invocations, users’ service requests are
used directly in our ASEQ models as the abstractions
of user activities. In particular, parameters for service
invocations will be used as the input parameters for
ASEQ models.
System events are signals indicating certain
system behavior or activities not initiated by legitimate
users, such as service failure, and attacks on networks
and services, and should be captured by monitoring
modules. Due to the message-based nature of SBS,

76

the performance impact on other QoS features when
security-related service configurations, such as
authentication mechanisms and encryption key length,
are changed [14], which can be quantified through
controlled experiments. Such ASEQ models for
security will be used to optimize the performance of
ASBS without violating users’ security requirements.

system events in our ASEQ models are represented as
messages containing the descriptions of the events,
including what, when and where an event has
happened. These descriptions will also be used as the
input parameters for ASEQ models.
A system resource state is represented by a set of
state variables, where state variables are properties of
system resources that can be directly acquired or
derived from runtime monitoring, such as committed
memory, CPU utilization, and the amount of data
transferred to/from physical disks. Specifically, a
system state is a certain combination of particular
ranges of state variables. For example, a simple system
resource state can be “the committed memory is
between 100 and 200 Mbytes, and CPU utilization is
between 10% and 20%”. Note that although the state
variables are pre-selected by system designers, the
definitions of specific system resource states are not
given and will be generated during the construction of
ASEQ models.
Evaluation of QoS performance is the most
important part for constructing ASEQ models. There
are well-defined metrics for timeliness, accuracy, and
throughput based on which analytical models can be
constructed to estimate their performance based on
user activities, current system resource states, and
event history in the system. Table 1 shows the metrics
used for timeliness, accuracy and throughput in our
approach. These metrics are selected based on the
multimedia services and applications we are currently
focusing on. For other services and applications,
different metrics may be used. For instance, the
accuracy of a web search service will be measured by
the precision and recall of the search results, rather
than the loss rate and error rate in Table 1.
However, it is very difficult to evaluate the
security of a service or a system quantitatively [13].
Hence, in the ASEQ models for security, we focus on
QoS
Features
Accuracy

Timeliness

Throughput

Metrics

4.2 Design of Experiments
To achieve correctness, generality and effectiveness of our experimental results, the following two
aspects of our experiment design are most important:
i) Identify an appropriate set of functional services and
construct representative application scenarios using
these services for our experiments. First, the number
of identified services and application scenarios being
constructed should avoid excessive cost for
performing experiments. Second, the services and
application scenarios being constructed should provide
a good coverage of various types of services and
service composition patterns so that the experimental
results will be sufficiently general.
ii) Identify the resource and QoS requirements of the
services in various application scenarios to be used in
the experiments, and the corresponding experiment
conditions and control variables determining these
conditions. Similar to i), the resource and QoS
requirements of the services in various application
scenarios should provide a good coverage of various
characteristics of the QoS features and system
resources for the generality of the experiment results.
We have identified a set of services, including
voice communication, video streaming, and motion
detection in video streams, which have wellunderstood QoS requirements [5]. Table 2 provides an
overview of these identified services. Using these
services, five different application scenarios, including

Table 1. QoS Metrics
Definition

Loss rate

The number of bits lost between two points in telecommunications after
transmission.

Error rate

The bit error rate is the frequency of erroneous bits between two points in
telecommunication after transmission.

Service
delay

The time elapsed between a user submitting a service request and receiving a service
response

Network
transmit
delay
Data rate
Bandwidth

The time elapsed between the emission of the first bit of a data block by the
transmitting end-system, and its reception by the receiving end-system
The rate in which data are encoded
The data transfer rate, measured in bits per second

77

online radio broadcasting, motion analysis, online
video sharing, real-time surveillance camera, and netmeeting, have been designed and identified as an
initial set of our experiments to cover various
composition patterns, QoS and resource requirements.
As shown in Table 3, online radio broadcasting,
motion detection, and online video sharing scenarios
only use one service each. These three scenarios are
mainly designed for constructing performance models
for the three identified services. Real-time surveillance
camera and net-meeting scenarios use different
compositions of services, and are mainly designed to
acquire knowledge on QoS composition.
For each experiment, a set of experiment
conditions are identified based on QoS and resource
requirements, and user activities in each application
scenario. These experimental conditions are
represented by various value assignments for
experiment control variables. There are two types of
experiment control variables: one reflects the initial
system state, such as the current CPU workload and
network bandwidth utilization, and the other reflects
certain characteristics of user activity, such as the
number of concurrent users for a service and the
quality of audio or video streams requested by users.
Usually, these experiment control variables are
parameters of users’ service requests as well as certain
workflow generation tools, such as SWORD and
StreamGen, which can be used to simulate or create
various operational environments for our experiments.

a statistical test, the Mann-Whitney test [15].
B2) Data clustering: The data items are grouped
together based on the similarity in their statistical
behavior using the following method. Apply the
hierarchical clustering method [16] to generate
clusters of data items under each experimental
condition, and examine the generated clusters by
counting the co-occurrences of data items in the same
cluster across different experimental conditions to find
the data items with similar statistical behaviors
B3) Analysis of the impact of experimental
condition changes on data items: Experimental
conditions are determined by a set of experiment
control variables. The statistical significance of
experiment control variables for data items is analyzed
using ANOVA [17].
B4) Establishment of the relationships among
activities, states, events, and QoS performance in
SBS: The data items are first categorized into activity,
state, event, and QoS data. Then, use the Classification
And Regression Tree (CART) analysis [18] to identify
the cause-effect chains among activity, state, event,
and QoS performance data. We have developed a tool
to transform the decision trees generated from CART
analysis automatically into conditional evaluation
statements, which will later be used in the QoS
monitoring modules.
Note that a part of the experimental data will be
reserved for validating simulation models for SBS as
indicated in Section 6.

4.3 Construction of the ASEQ models in ASBS

5. Development of QoS M/A Modules

We have developed a methodology for analyzing
the experimental data generated above to construct the
ASEQ models in ASBS. We summarize our
methodology as follows:
B1) Data reduction: Identify the data items having
significant changes of values when the system changes
from idle condition to an experimental condition using

For large-scale ASBS, QoS M/A tasks often need
to be partitioned and distributed in a hierarchical
manner for better performance and manageability. We
have developed a three-layered intelligent control
architecture for ASBS [1], in which monitoring and
adaptation tasks are distributed to user, workflow, and
service management layers. Here, we will discuss the

Services
Voice communication
Video streaming
Motion detection

Table 2. An overview of the services identified for our experiments
Functional Descriptions
Resource Characteristics
Provide voice-based communication over the network
Network-intensive
Provide the live video feed from a web camera
Network-intensive
Identify the differences in two images
Computation-intensive

Table 3. A summary of service compositions in the five identified scenarios
Scenarios
Services Used
Composition Patterns
Online radio Broadcasting
Voice communication
None
Motion analysis
Motion detection
None
Real-time surveillance camera
Video streaming, motion detection,
Sequential, Exclusive Choice
voice communication
Video sharing
Video streaming
None
Net-meeting
Voice communication, video streaming Parallel

78

development of QoS M/A modules in ASBS based on
the ASEQ models generated in Section 4.
 QoS monitoring modules. As shown in Figure 1,
functional services in ASBS are often used to compose
complex workflows to perform high-level tasks for
users. Such workflows can be viewed as composite
services, and often need to satisfy the overall QoS of
the workflow, rather than the QoS of the individual
services used in the workflows. QoS monitoring
module needs to keep track of the workflow execution,
and detect various execution problems, such as service
failure, network disconnection, and QoS degradation.
In [19], we have developed an adaptive execution
monitoring approach in ASBS, in which distributed
monitoring modules are automatically generated based
on the knowledge of the workflows to be executed, the
systems executing the workflows, and the possible
failure in workflow execution. The generated
monitoring modules will proactively acquire the status
information of computing resources to be used in
future workflow execution for early detection of
potential execution problems, and self-reconfigure or
load new monitoring modules to acquire additional
information if any problem occurs. Currently, we are
improving this approach by incorporating the
knowledge generated in our data analysis (see Section
4.3). Specifically, the hierarchical clustering results
will be used in the indicator selection process to
reduce the number of targets to be monitored. The
decision trees generated from the CART analysis will
be embedded in the monitoring modules for estimating
near-future changes in QoS performance and the
execution environment.
 QoS adaptation module. To develop the QoS
adaptation module in ASBS, we first define QoS
expectation functions to quantify gains and losses for
satisfying and violating users’ QoS requirements,
respectively. Then, the ASEQ models and QoS
expectation functions will be used to derive the
optimization and control synthesis algorithms for
generating adaptive control commands. QoS
adaptation in ASBS is formulated as a multi-objective
optimization problem [20], in which the ASEQ models
and QoS expectation functions are used as the
constraints and objective functions respectively. The
solution of this optimization problem will determine
system and service configurations which result in the
desired QoS tradeoffs. Hence, the optimization
problem includes the representation of the QoS
tradeoffs, and the solution reflects the result from the
tradeoff analysis performed while solving the
optimization problem. The QoS adaptation module in
ASBS will be then developed based on the
mathematical methods to solve this optimization
problem.

6. Validation of ASBS Design through
SOA-based Simulations
Running extensive experiments on a real testbed
to collect a large amount of data can be quite costly,
and it is intrinsically difficult to derive internal
dynamics of services for generality analysis from
experimental data. We have developed simulation
models for SBS, and are using them to validate and
improve our research results as follows:
(1) Develop simulation models for services and
application scenarios in our experiments based on
the service specifications and ASEQ models.
(2) The simulation models developed in (1) will be
used to conduct simulations using the same
experiment conditions that generate the
experimental data. Data collected in simulations
will be compared with the reserved experimental
data to validate the simulation models.
(3) The validated simulation models will be
integrated with the real M/A modules to support
the rapid development for simulation and
evaluation of ASBS with M/A modules.
(4) The validated simulation models will be used to
perform simulations under additional experiment
conditions, which incorporate more service
activities, system events and control structures in
service composition, to obtain more simulated
data for further improvement and generalization
of our results.
We have developed an SOA-compliant simulation
framework based on the DEVS (Discrete Event
System Specification) formalism for modeling SBS
[21]. In this framework, a set of abstract component
models based on DEVS formalism and conforming to
SOA principles, such as autonomy and composability,
is developed to enable modeling and simulating
primitive and composite services. In particular, atomic
DEVS models for publisher, subscriber, and broker in
SOA are developed. The input/output ports with
couplings of DEVS models are extended to provide a
messaging framework supporting various messagebased interactions among publisher, subscriber, and
broker services in SOA. Generic message classes are
developed as abstractions of WSDL and SOAP
specifications used in SOA. Coupled DEVS models
are used to represent composition of services in SOA.
To model network communication effect on the QoS
of SBS, a simple model representing the network with
stochastic delay is developed. The network model
allows the publisher, subscriber, and broker services to
communicate with one another. For each of the
publisher, broker, subscriber, and network models, a

79

transducer model is developed to measure throughput
and timeliness QoS.
Based on this simulation framework, an SOAcompliant DEVS (SOAD) simulator has been designed
and implemented based on DEVSJAVA [22]. Using
this simulator, common variations of SBS can be
systematically
conceptualized,
modeled,
and
efficiently simulated given the inherent support for
component-based modeling and parallel execution.
Two preliminary simulation experiments for a voice
communication service and a travel agent service have
been conducted to validate and demonstrate the
capability and efficiency of SOAD framework and its
support for SOA-based simulations.

[5] Y. Chen, T. Farley and N. Ye, “QoS Requirements of
Network Applications on the Internet”, Information,
Knowledge, Systems Management, vol. 4(1), 2004, pp. 55-76.
[6] J. Zhou, K. Cooper, I. Yen and R. Paul, “Rule-Base
Technique for Component Adaptation to Support QoS-based
Reconfiguration”, Proc. 9th IEEE Int’l Symp. Objectoriented Real-time Distributed Comp., 2005, pp. 426–433.
[7] D. R. Hild, H. S. Sarjoughian and B. P. Zeigler, “DEVSDOC: A Modeling and Simulation Environment Enabling
Distributed Codesign,” IEEE Trans. on Systems, Man and
Cybernetics, Part A, vol. 32(1), 2002, pp. 78 -92.
[8] J. S. Dahmann, “High Level Architecture for
Simulation,” Proc. 1st Int’l Workshop on Distributed
Interactive Simulation and Real-time Applications (DISRT’97), 1997, pp. 9-14.
[9] MathWorks, “MATLAB/Simulink”, 2005. Available at:
http://www.mathworks.com.
[10] OPNET, ”OPNET Modeler”, December 2005.
Available at: http://opnet.com.
[11] D. Huang, and H. Sarjoughian, “Software and
Simulation Modeling for Real-Time Software-Intensive
Systems,” Proc. 8th IEEE Int'l Symp. on Distributed
Simulation and Real-time Applications, 2004, pp. 196-203.
[12] W. T. Tsai, Y. Chen, R. Paul, X. Zhou and C. Fan,
“Simulation Verification and Validation by Dynamic Policy
Specification and Enforcement”, SIMULATION, vol. 82(5),
2006, pp. 295-310.
[13] D. Lie, and M. Satyanarayanan, “Quantifying the
Strength of Security Systems,” Proc. 2nd USENIX Workshop
on Hot Topics in Security, 2007, article no. 3.
[14] S. S. Yau, M. Yan, and D. Huang, “Design of Servicebased Systems with Adaptive Tradeoff between Security and
Service Delay,” Proc. 4th Int’l Conf. on Autonomic and
Trusted Computing, 2007, pp. 394-401.
[15] H. B. Mann, and D. R. Whitney, “On a Test of Whether
One of Two Random Variables is Stochastically Larger than
the Other,” Annals of Mathematical Statistics, vol. 18, 1947,
pp. 50-60.
[16] S. C. Johnson, “Hierarchical Clustering Schemes,”
Psychometrika, vol. 2, 1967, pp. 241-254.
[17] D. C. Montgomery, G. C. Runger and N. F. Hubele,
Engineering Statistic (4th ed.), John Wiley, 2006.
[18] L. Breiman, J. Friedman, R. A. Olshen and C. J. Stone,
Classification and Regression Trees, Wadsworth Int’l Group,
1984.
[19] S. S. Yau, D. Huang and L. Zhu, “An approach to
adaptive distributed execution monitoring for workflows in
service-based systems,” Proc. 31st Annual Int’l Computer
Software and Application Conf., vol. 2, 2007, pp. 211-216.
[20] R. L. Rardin, Optimization in Operations Research,
Prentice Hall, 1998.
[21] H. Sarjoughian, S. Kim, M. Ramaswamy, and S. S. Yau,
“A simulation framework for service-oriented computing,”
Proc. 2008 Winter Simulation Conf., 2008, to appear.
[22]Arizona Center for Integrative Modeling and Simulation,
http://www.acims.arizona.edu/SOFTWARE., 2006

7. Conclusions and Future Research
In this paper, we have presented a performancemodel-oriented approach to developing ASBS. The
current research progress is discussed, including
constructing the ASEQ models through controlled
experiments, developing QoS M/A capabilities in
ASBS based on the ASEQ models, and validating
ASBS design through SOA-based simulations. Future
research includes development of methods for solving
the optimization problem for QoS adaptation,
improvement of our SOA-based simulation
environment to support the integration of real and
simulated SBS, and integration and evaluation of our
research results. We also need to improve our design
of experiments and data analysis methodology for
constructing ASEQ models.

Acknowledgment
This work is supported by National Science
Foundation under grant number CCF-0725340.

References
[1] C.-H. Jiang, H. Hu, K.-Y. Cai, D. Huang, and S. S. Yau,
“An Intelligent Control Architecture for Adaptive Servicebased Software Systems with Workflow Patterns,” Proc. 5th
IEEE Int’l Workshop on Software Cybernetics, in
conjunction with COMPSAC 2008, pp. 824-829.
[2] M. Reisslein, K.W. Ross and S. Rajagopal, “A
Framework for Guaranteeing Statistical QoS”, IEEE/ACM
Trans. on Networking, vol. 10(1), 2002, pp. 27-42.
[3] X. Xiao, T. Telkamp, V. Fineberg, C. Chen and L. M. Ni,
“A Practical Approach for Providing QoS in the Internet
Backbone”, IEEE Communications, vol. 40(12), 2002, pp.
56 – 62.
[4] Z. Yang, N. Ye and Y.-C. Lai, “QoS Model of a Router
with Feedback Control”, Quality and Reliability Engineering
Int’l, vol. 22(4), 2006, pp. 429-444.

80

2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing

Integrating HLA and Service-Oriented Architecture in a Simulation Framework

Monica Dragoicea, Laurentiu Bucur

Wei-Tek Tsai, Hessam Sarjoughian

Faculty of Automatic Control and Computers
POLITEHNICA University
Spl. Independentei nr. 313, Bucharest, Romania
monica.dragoicea@upb.ro, laur.bucur@gmail.com

School of Computing, Informatics and Decision
Systems Engineering
Arizona State University
Tempe, Arizona 85287 USA
{wtsai, sarjoughian}@asu.edu

However, it does not provide consistent modeling
methodologies and tools to model the target’s behavior and
to verify the correctness of the simulation. It is the
responsibility of the user to ensure that the built models meet
the requirements.
This paper addresses these issues by proposing a
approach to integrates HLA and Service-oriented
Architecture (SOA).
The contributions of this paper are as follows: a proposal
on the integration of the HLA simulation standard into a
service-oriented simulation environment (the FCINT
Simulation Framework [13]) together with a brief outline of
the strengths and weaknesses of HLA and SOA. The paper
showcases the FCINT Simulation Engine (the core
component of the FCINT Simulation Framework) and the
composition rules that govern the integration of the simulator
with a service-oriented smart building controller, using HLA.
The proposed approach is one of many possible approaches
that may come in the future, and features proposed can be
used for further development in this area.
Simulation plays an important role in SOA application
development, especially in business modeling and process
management [14]. Due to the dynamic nature of SOA, the
behavior of the SOA, as well as most of the analyses on SOA
applications, may occur in the runtime and may need to be
changed / reconfigured at runtime. Moreover, the workflows
and the architecture of the SOA applications can be
dynamically changed. For this reason, simulations are
necessary to evaluate the runtime behaviors of the SOA
applications. Specifically, simulation can play the following
tasks in SOA software development [2]:
• Dynamic application composition simulation.
• Runtime behavior and performance simulation.
• Dynamic collaboration simulation.
This work will demonstrate an approach to integrating
HLA and SOA using a simulation framework developed for
the FCINT project.
This paper is organized as follows. Section II presents an
integrated approach that combines both HLA and SOA in the
FCINT Smart Building Simulation Framework. It covers the
definition of a simulation model, the software architecture of
the FCINT Simulation Framework, the service composition
rules that govern the HLA and service integration, messaging
protocols and event types exchanged during simulation,
together with a federate-to-service mapping that enables the

Abstract—The High-Level Architecture (HLA) is the de-facto
standard in simulation interoperability. This paper presents a
possible way for HLA to integrate with a service-oriented
architecture (SOA) in the context of a smart building project. The
paper discusses the design of an HLA federate for the inclusion of
a service oriented smart building controller in the simulation loop.
Keywords: Simulation,
HLA, Smart Building

Service–Orientation

Architecture,

I.
INTRODUCTION
Current distributed discrete-event simulation platforms
often use the model-code-run style in which the user creates
a simulation model of the target system, generates the code,
and then executes the code [1][2]. Several centralized
discrete-event simulation frameworks such as Simulink in
Matlab [3], Arena [4], and SimProcess [5], support modeland-run, i.e., after a user develops the model for a target
system, the simulation platform can run the simulation
without coding, and this approach saves significant effort.
Such a process assumes simulation models for target
application domains exist and support assembling them
together. Distributed simulators have been under
development using different modeling approaches,
simulation protocols, and implementation technologies such
as web-services [6][7][8]. These works, however, do not
take advantage of service oriented computing concepts such
as seamless service discovery, composition, and multitenancy.
Two well-known examples of distributed virtual
environments are IEEE DIS (Distributed Interactive
Simulation) [9] and IEEE HLA (High-Level Architecture)
[10]. HLA evolved from DIS, aiming primarily at training
simulations and ALSP (Aggregate-Level Simulation
Protocol) [11]. HLA component-based concepts are strongly
grounded in simulation interoperability with basic support
for modeling [12]. An HLA implementation normally
includes non-runtime components to specify the object
model used by the simulation federation. The components
include a set of object types chosen to represent the real
world, the level of detail at which the objects represent the
world, and the key models, and algorithms used. Currently
DIS and HLA require coding before simulation.
HLA provides powerful rules and templates for users to
specify the simulation-federation and their interoperations.
978-0-7695-4691-9/12 $26.00 © 2012 IEEE
DOI 10.1109/CCGrid.2012.76

861

the composition between the Simulation Engine Service
(SES) and the Smart Building Controller (SBC) service
(Figure 1). The SBC is a Web service that can be discovered
and composed with SES.
A service-oriented simulation is initiated by a user that is
authenticated through the Simulation Console (SC). The SES
serves as a service orchestrator between the services
involved in the simulation.

inclusion of a real Smart Building Controller web service in
the simulation loop. Section III concludes this paper.
In [7] a comparison between HLA and SOA concludes
that:
• HLA has good interoperability, synchronization and
effective and uniform information exchange mechanism
between the communicating components (federates), but
lacks several features of web services, such as: the
integration of heterogenous resources, web-wide
accessibility across firewall boundaries.
• SOA benefits from loose coupling, component reuse and
scalability but lacks a uniform data exchange format and
time synchronization mechanisms.
• The combination of HLA and SOA can extend the
capabilities of the two technologies and thus enable
integrated simulated and real services.

Simulation
console

Dedicated services
(device & building control)

Simulation services

Service repository

S1
Simulation
Service
Repository

S2

Simulation
Engine

S4

Service ontology

Other_purpose
ontology (e.g.
devices)

Simulation service
ontology

The FCINT project [15] uses SOA for intelligent building
management. This paper presents one of its key main
components, the FCINT Simulation Framework [13], which
aims at integrating HLA in a service-oriented architecture.
The simulation framework can be accessed via the Web, and
supports service discovery and composition as well as
execution monitoring. The framework also supports
simulation federation monitoring and report generation.
II.

Simulation
data

Smart Building Controller (SBC)

Environment
configuration

Log data
(events)

Schedules

Policies

Figure 1. The FCINT service oriented simulation framework

The FCINT framework allows the automation for
simulation scenario modeling and testing. A simulation
scenario represents a set of time-ordered events that emulate
a specific situation that may occur, at any given time, during
the operation of a smart building. Three types of simulation
events are currently supported:
Device command events. A device command event is an
interaction sent to one of the devices (federate object
instances) involved in the simulation. When the appropriate
federate receives the interaction, it is translated into either an
internal SES call (to the Simulated Device Manager – figure
2) or to a service call, in the case of the SBC Federate.
Change of Value (COV) events. A COV event
represents an event which modifies the attribute of an object
instance. A COV event is translated into an attribute change
of value event which is sent to the HLA Runtime
Infrastructure (HLA RTI). When the simulated device
federate receives a COV event, it is routed internally to the
Simulated Device Manager. Otherwise, it is mapped to a
corresponding web service call to the Smart Building
Controller that maps attribute / set operations to web service
calls for a particular object instance.
External events. This type of events simulates the
occurrence of unexpected (or scheduled) events external to
the federation. Depending on the existence of a SBC federate
in the federation, these events are routed to a real SBC to
enable live scenario testing in a smart building.

INTEGRATION OF HLA AND SOA

This section illustrates an approach to integrate HLA with
SOA using the FCINT Simulation Framework for Intelligent
Buildings [13].
The Smart Building Simulation Framework (Figure 1) has
a service-oriented design centered around service
composition, discovery, and Web access. The core
component of the framework is the Simulation Engine
Service (SES) (Figure 2). It is a RESTful service that
manages the execution of simulations and can be composed
at runtime based on the user preferences with locally and
remotely available services:
Local services are represented, for example, by the
Report Generation (RG) services running in the same host as
the Simulation Engine (SE); they can be developed and
deployed together with the SES;
Remote services are represented by any Smart Building
Controller (SBC) endpoint accessible from a public IP or any
third party simulation report web services hosted on other
machines that are compatible with the FCINT Simulation
Report WSDL specification. Third party report generation
services create simulation reports in custom file formats (
e.g., PDF, Word, and RTF) based on the simulation data
provided by the SES.
II.1. Simulation Model
A simulation model is defined as the set of federates,
object instances, attributes and interactions involved in a
simulation, together with the HLA-compliant description of
the federation (the FED file). In the context of the FCINT
project, the execution of the simulation model is ensured by

II.2 HLA Service-Oriented Simulation
The SES is exposed as a RESTful service with the
following modules (Figure 2):

862

•

•
•

•
•

These functions return the consumption time series
for all object instances in the federation, and the
simulation console provides native Java graphics
support for displaying them;
o Interface to external services. This allows 3rd-party
services to be used in the framework to generate
custom format reports, e.g., a PDF simulation
report. The discovery of these services can be done
at runtime by the Simulation Console by publishing
in a service repository using ontology information,
or can be added statically in the architecture.

Simulation Manager. This is a service wrapper on
top of the Runtime Infrastructure (HLA RTI) that
will expose access to the RTI's federation
management via a RESTful API. The Simulation
Manager (SM) deals with the creation, initialization,
deletion, starting, stopping, and execution of
simulations;
Configuration. This will assist the user in selecting
the proper services for report generation, workflow
execution and service emulation;
Simulated Device Manager (SDM). This deals with
the management of virtual devices in a federation:
creation, deletion, event type management. The
SDM has an associated federate that responds to
interactions and attribute value requests from the
RTI and sends back notifications to the federation
via the RTI;
Runtime Infrastructure (RTI). The HLA compliant
RTI is the SES that deals with the actual execution
of the simulation.
Report Generation Service (RGS). It accounts for
two aspects:
o Data functions: This allows the Simulation Console
to display the consumption of various devices.

Figure 2.

The FCINT Simulation engine architecture

Figure 3. The FCINT Smart Building Simulator. http://fcint.ro/portal/Simulator/Simulator.html

863

Figure 4 – The FCINT HLA simulation model

The HLA service oriented simulation contains the
following federates:
• Simulated Device Federate;
• Smart Building Controller Federate (optional);
The Simulated Device Federate (SDFed) is hosted
internally by the SES. Its role is to manage virtual devices
and participate in the HLA federation execution. The result
of events and interactions sent to the Simulated Device
Federate are routed to the Simulated Device Manager.
The SBC Federate (SBCFed) is also hosted internally by
the SES, but it mediates attribute updates and interactions by
making calls and receiving updates from an external web
service, i.e. the SBC, a discoverable, composable, executable
and monitorable service that can reside in a building or
environment. This federate is optional in a simulation, but
necessary in testing the behavior of a real environment SBC,
given a simulation scenario, by means of HLA simulation.
After authentication, at the beginning of a simulation, the
user can choose a pure virtual simulation or a real
environment simulation that may include the SBC in the
loop, in which case the SBC Federate is automatically
allocated in the federation model.
Future extensions of the simulation model will enable the
inclusion of other types of federates in the simulation loop,
following the central idea of our architecture: pairing
together a web service via discovery with a federate related
to an external software component, like OpenSim [19].
The FCINT Simulation Engine hosts one or more
simulation models which optionally communicate with real
SBC instances. The Simulation Engine provides the
Simulation Console control over the execution of the
simulation. The time advancing is controlled by the
Simulation Engine which hosts the federates and executes
one or more simulation steps, based on the commands
received from the Simulation Console.

the SES and internal and external services. The SES's
runtime discovery capabilities will allow dynamic
composition with any external services using two
mechanisms:
• External SBC services can be discovered at runtime
by the following protocol: a) the user specifies the
address of the SBC endpoint in the Simulation
Console, and b) the SES initiates the connection to
the service and creates the SBC Federate;
• The endpoints of external reporting services and the
SES will be published in the Service Management
System that is an ontology-based service registry
that will allow dynamic composition and discovery
[20].
The following rules specify the creation of the Smart
Building Controller Federate (SBCFed). (These steps can be
carried out using the FCINT simulator available at
http://www.fcint.ro/portal/simulator/simulator.html.)
1. SBC Federate creation and federate object instantiation
procedure:
a) The SES receives the URL of a SBC service
endpoint;
b) The SES calls a service to enumerate the building
environments the SBC controls;
c) The user chooses an environment from the
simulation console;
d) The SES creates an empty SBCFed object in the
simulation;
e) The SES queries the SBC for the list of real devices
in the selected environment, the list of web services
and the device-to-web service mappings for
property read, property write and device control
mechanisms;
f) The SES creates a device class for each subset of
identical devices sharing a common mapping to a
web service orchestrated by the SBC;

II.3 Service Orchestration – Composition Rules
The service composition policy describes the
composition rules that govern the communication between

864

•

g) The SES populates the class properties and
interactions based on information retrieved from the
SBC;
h) The SES assembles the Federate Object model for
the SBC Federate and joins the HLA federation
together with the existing Simulated Device
Federate;
i) The Simulation Console (SC) displays the list of
virtual devices managed by the Simulated Device
Federate and the list of real devices managed by the
SBC Federate.
2. During federation execution, the following HLA-SBC
web service composition rules apply:
• Interactions received by the SBC Federate are
translated into web service calls that are sent to the SBC
via the SBC web service interface;
• Requests for attribute values changes are handled in a
similar manner;
• Real device instance attribute values are updated from
the SBC after each time advance operation using the
corresponding web service calls.

•

SBC Federate – SBC. The SBC federate translates
all attribute assignments and interactions to
appropriate web service calls to the SBC. The SBC
exposes a discoverable service endpoint URL
SES – Simulation Console.
The entire
communication between the Simulation Console
and the SES uses the SES's REST API [21];

II.5 Mapping SOA Orchestration to HLA Federates
The purpose of using an HLA to service mapping is to
assess the behavior of a real Smart Building Controller web
service under simulated conditions, in which the simulation
scenario is executed in a remote HLA federation.
Specifically, given a SBC and its associated device
control services, the Simulation Framework needs to test the
behavior of the SBC in several simulated scenarios. For
example, the behavior of the SBC under various simulated
fire alarm scenarios can be evaluated. The user will be able
to connect the simulation to an SBC, create a simulation
scenario and schedule a particular event type to be sent to
the SBC at a particular simulation time.
This means the mapping of the FCINT Smart Building
Controller to a HLA federate.
The mapping should meet the following requirements:

II.4 Messaging Protocols
Messages can be handled at two levels: HLA or service.

a) The SBC federate must publish the list of classes in
the real environment controlled by the SBC as classes in
the HLA federation monitored by the simulation
console. This includes adding the device class description to
the Federation Object Model (FOM). Upon creation and
connection to a real SBC, the federate must query the list of
all devices and their properties and update the FOM.
b) The federate must publish the list of interactions
supported by the federate. The SBC Federate interactions
are:

II.4.1 HLA-level communication protocol: This takes
place in internally in the SES, and it is specific to each
simulation. At this level, the communication parties are:
• SES;
• Simulated Device Federate (SDFed);
• SBC federate (SBCFed).
Three simulation event types exist in the SES that the
user can define in a simulation scenario:
• CommandEvent. This defines device command
events of EVENT_COMMAND type, with the body
field
taking
the
form
of
DeviceName.Command(arguments),
which
corresponds to a device command;
• ChangeOfValueEvent. This specifies change of
Value events (COV) of EVENT_COV type, with
the body field of the form : Device.property=value,
which corresponds to a device property assignment
operation;
• CustomEvent. This specifies external events that can
occur at any point in the simulation and may
originate from the Smart Building Controller during
real operation. The user may define Custom events
during simulation to recreate real life conditions in
which asynchronous events occur. The body field of
a custom event can be any valid XML tag. For
example,
<alarmEvent><floor>1</floor><type>Fire</type><
building>1</building></alarmEvent>

-ExecuteSBCHighLevelFunction (deviceName,parameters)
When receiving this interaction, the federate will map it to
the correponding SBC call to a high level device command.
- ExecuteSBCServiceCall(serviceCallCommand)
When receiving this interaction, the federate will map it
to the corresponding call to the web services orchestrated by
the SBC that controls actual devices (figure 4)
- SBCResponse(body)
This interaction is sent by the SBC federate as a response to
the
ExecuteSBCHighLevelFunction
and
ExecuteSBCServiceCall interactions.
- IncomingSBCEvent (body). This interaction corresponds to
an asynchronous event received by the SBC federate from
the SBC. The body of the IncomingSBCEvent encodes the
actual event received from any of the web services
controlled by the real SBC, in XML format.
- OutgoingSBCEvent (body). This interaction is sent by the
simulation engine to the SBC Federate. It corresponds to a
simulated event that needs to be sent to the SBC, as if it
originated from a real device. This is used to trigger the
SBC response to various simulation scenarios.

II.4.2 Service-level communication protocol
The communication between the SES and the related
services is based on the WSDL. The following scenarios
define service-level communication with the SES:

865

c) The federate must join the federation and publish all
devices controlled by the SBC as object instances owned
by the federate. This is accomplished by the simulation
engine after the discovery of a SBC service endpoint.
d) The federate must listen to incoming events from the
SBC and publish them as interactions in the federation.
This implies that the federate must be time regulating and
issue IncomingSBCEvent interactions to the federation. The
body of the IncomingSBCEvent interaction will include the
XML representation of the event received from the SBC.
e) The federate must receive interactions and map them
to their corresponding service calls to the Smart
Building Controller. When the user wishes to send a
device command to a real device, the simulation engine will
send
an
ExecuteSBCHighLevelFunction
or
an
ExecuteSBCServiceCall interaction. The SBC Federate will
subscribe and receive the interaction and translate it into the
corresponding SBC web service call.
f) The federate must send custom simulation events to
the SBC. When the simulation scenario reaches a point
where a custom event should be sent to the SBC (to
simulate a fire alarm for example), the simulation engine
will issue an OutgoingSBCEvent interaction, that is received
by the SBC federate and routed to the real SBC service.
g) The federate must advance time in sync with the
federates involved in the simulation. This implies that the
federate must be time constrained and synchronized with the
Simulated Device Federate.
III.

[3]

[4]

[5]
[6]
[7]

[8]

[9]

[10]

[11]

[12]

[13]

CONCLUSIONS

This paper presents a simulation architecture that uses both
HLA and SOA concepts such as HLA execution model,
service discovery, composition, and orchestration. This is a
useful procedure to integrate HLA and SOA without the
need of code generation and static model definition. The
approach enables to planning, integration and testing of a
Smart Building Controller configuration included in the
simulation loop.

[14]

[15]
[16]

[17]
[18]

ACKNOWLEDGMENTS
This project is supported by the FCINT Project (POSCCE Priority Axis 2, O2.1.2, ID 551, Contract no.
181/18.06.2010) in Romania and the U.S. National Science
Foundation grant #0942453.

[19]

REFERENCES
[1]

[2]

[20]

Wei-Tek Tsai, Wu Li, Hessam Sarjoughian, Qihong Shao. "SimSaaS:
simulation software-as-a-service". In: SpringSim (ANSS) 2011, pp.
77-86, 4-7 April 2011, Boston, Curran Associates Proceedings .
Tsai, W.T., Cao, Z., Wei, X., Ray Paul, Huang, Q., Sun, X., 2007.
"Modeling and Simulation in Service-Oriented Software
Development". In: Special Issue on Modeling and Simulation for and
in Service-Orientated Computing Paradigm, Vol. 83, No. 1,pp. 7-32,

[21]

866

January 2007, Transactions of the Society for Modeling and
Simulation International
Mathworks. MATLAB Simulink – Simulation and Model Based
design.
Online,
available
at:
http://www.tufts.edu/~rwhite07/PRESENTATIONS_REPORTS/simu
link.pdf
Systems Navigator. "Arena simulation software". Online, available at:
http://www.systemsnavigator.com/sn_website/?q=arena_simulation_s
oftware
CACI. "SimProcess Modeling & Simulation Tools". Online, available
at: http://simprocess.com/
Gabriel Wainer and Pieter Mosterman, “Discrete-Event Modeling and
Simulation: Theory and Applications”, Editors, CRC Press, 2010.
Wenguang Wang, Wenguang Yu, Qun Li, Weiping WANG,
“Service-Oriented High Level Architecture”, European Simulation
Interoperability Workshop, 08E-SIW-022. Edinburgh, Scotland,
2008.
Saurabh Mittal, José Risco-Martín, Bernard Zeigler, 2009.
DEVS/SOA: A Cross-Platform Framework for Net-centric Modeling
and Simulation in DEVS Unified Process. Simulation, Vol. 85, issue
(7),pp. 419-450.
IEEE Std 1278.2-1995. 1995. IEEE Standard for Distributed
Interactive Simulation - Communication Services and Profiles, IEEE,
New York, NY, USA.
IEEE Std 1516-2010. 2010. Standard for Modeling and Simulation
(M &S) High Level Architecture (HLA) - Framework and Rules,
IEEE, New York, NY, USA.
Annette Wilson, Richard Weatherly. "The aggregate level simulation
protocol: an evolving system". In: WSC '94 Proceedings of the 26th
conference on Winter simulation, pp. 781-787.
Hessam Sarjoughian, Bernard Zeigler, DEVS and HLA:
Complementary paradigms for modeling and simulation?,
Transactions of the Society for Computer Simulation 2000;17(4):187197.
The FCINT Simulation Framework. Online, available at:
http://www.fcint.ro/portal/simulator/simulator.html
The INNOV8 IBM Business Process Management (BPM) simulation
game.
Online,
available
at:
http://www01.ibm.com/software/solutions/soa/innov8/index.html
The FCINT project. Online, available at: www.fcint.ro
W.T. Tsai, Q. Huang, J. Elston, and Y. Chen, "Service-Oriented User
Interface Modeling and Composition," in International Conference on
e-Busines Enginerring, October 22-24, 2008, Xi'an, China, pp. 21-28.
Freebase. Online, available at: www.freebase.com
Broekstra, J. Kampman A, Van Hermelen, F. " Sesame: A Generic
Architecture for Storing and Querying RDF and RDF Schema". In
Proceedings of the first Int'l Semantic Web Conference (ISWC 2002),
Vol. 2342 9th-12th June, 2002, Sardinia, Italy, pp. 54-68.
Scott Delp, Frank Anderson, Allison Arnold, Peter Loan, Ayman
Habib, Chand John, Eran Guendelman, and Darryl Thelen. 2007,
"OpenSim: Open-Source Software to Create and Analyze Dynamic
Simulations of Movement". In IEEE Transactions on Biomedical
Engineering, Vol. 54, No. 11, pp. 1940-1950.
M. Dragoicea, L. Bucur. "The FCINT Service Management System
specifications
v.2.2".
Online,
available
at:
http://www.devteam.fcint.ro/project-tracking/1-fcintproject/filemanager/0-root/task_download/13servicemanagementsystem-ver-22pdf
L. Bucur, M. Dragoicea. "A refined architecture for the FCINT
Simulation engine. The REST API specifications" Online, available
at:
http://www.devteam.fcint.ro/project-tracking/1-fcintproject/filemanager/0-root/task_download/17-simdesign5docx

A Framework for Executable UML Models
Joe Mooney
General Dynamics
Scottsdale, Arizona
Email: Joe.Mooney@asu.edu

Keywords: Discrete event simulation, DEVS, Executable
UML, State Machine.
Abstract
One approach to support the creation of executable
UML models is to utilize an existing DEVS simulation
environment. The Discrete Event System Specification
(DEVS) formalism excels at modeling complex discrete
event systems. An approach to specifying DEVS-compliant
models is presented via Unified Modeling Language (UML)
state machines. Resultant UML models are executable
within DEVS simulation frameworks such as DEVSJAVA.
Constructing DEVS-compliant UML models enables early
simulation and verification of a design. This paper outlines
how the specifics of simulation can be naturally expressed
in UML models without significant burden to the UML
practitioner. Simulatable models are an excellent precursor
and companion to the current models normally developed
during design and implementation and may result in
significant cost and time savings.
1.

INTRODUCTION

1.1. Motivation
Modeling via a combination of DEVS [1] and UML [2]
provides a structured approach for the creation of a UML
model that can be simulated under an executable DEVS
framework. Although, such models cannot be simply
handed off to developers for implementation, instead these
executable models promote early understanding of a system
and allow for formal verification of important aspects of a
system which would otherwise have been difficult using
UML alone. Additionally, simulatable models have reuse
and extensibility potential throughout the software
development lifecycle. Simulation is a more attractive
proposition when the creation of executable UML models
requires only a basic understanding of simulation for a UML
practitioner and these simulation constructs can be included
into a model with little overhead.

Hessam Sarjoughian
ACIMS
Department of Computer Science and Engineering
Arizona State University
Email: Sarjoughian@asu.edu

1.2. Why Simulate?
El Sheik et al. [3] present thirteen incentives for
employing
simulation.
Simulation
allows
for
experimentation, time compression and expansion,
replaying events to discover why they occurred, animation
and visualization of a system, and training. However,
objections to simulation of software systems have been
raised including the cost of testing twice, once in the
simulated version and then again in the real system, and also
a belief that with a modern iterative development approach
simulation is no longer necessary since the real system can
be evolved incrementally thereby nullifying many of the
incentives for simulation.
We recommend creating a simulation model using
UML wherein most of the aspects of the model particular to
simulation are modeled separately wherever possible. Since
the UML becomes the common modeling language for both
the simulation model and the real software system, the
perception of simulation as a disjoint and unnecessary
exercise can be reduced. Furthermore, the perceived value
of the simulation models will likely be enhanced since these
models are the easiest to create and execute due to their
relative simplicity. In terms of iteratively developing
models, a modeler can begin with a simulatable model and
as requirements are solidified evolve a separate model
driving the specification of the system for the developers. In
so doing we can diminish the resistance to employing
simulation during the conceptualization and development of
a system. It should be noted that it is possible to evolve a
simulatable model into greater and greater levels of
precision through decomposition of the model whilst still
remaining a simulatable model.
Once we progress into the prototype and production
phases, the models become qualitatively different in nature
and this progression should not be seen as a simple one-toone mapping between models and neither should it been
seen as sequential phases, rather the development and
evolution of the simulation models can continue in parallel
through all phases of the software development lifecycle.
Beyond the architectural phases, where the simulatable
models become mature, the models continue to serve as
important tools to verify, experiment, and instruct.

Whenever changes are under consideration, or when various
what-if
scenarios
need
examination,
additional
experimentation using simulatable models may be
employed.
1.3. Why DEVS?
According to Zeigler et al [1] “DEVS is the unique
form of representation that underlies any system with
discrete event behavior”. UML is inherently a discrete
modeling language [4]. It is therefore natural to consider
what forms a DEVS-compliant UML model may take.
Models expressed using the Discrete Event System
Specification (DEVS) represent a class of systems theoretic
models that permit parallel event-based behavior to be
expressed concisely and in a manner that lend themselves to
formal verification [1]. Although many different simulation
formalisms have been advanced over the years, the DEVS
formalism has emerged as the preferred formalism due to
the fact that other formalisms have been proven to have an
equivalent DEVS representation [1]. In particular, a
differential equation system specification (DESS) can be
simulated by a discrete time system specification (DTSS)
through the selection of a sufficiently small constant time
interval. A DTSS model, in turn, can be simulated by a
DEVS model by constraining the time advance to a constant
time. As such, simulations based on DEVS are more general
in nature than other approaches such as continuous
simulation [6]. DEVS is appealing since it operates at a high
level of abstraction yet can yield critical information during
an architectural phase that might otherwise not come to light
until much later.
Further, it has been shown that DEVS models are
particularly suited to the expression of many design patterns
and allow an architect to employ patterns usefully at an
architectural and modeling stage [7]. It is important to
recognize DEVS models solve a general class of problems,
but are by no means suitable for all types of problems.
2.

MODELING APPROACH
Various approaches to modeling DEVS in UML
already exist [5,13,14,11,15]. The focus of this paper is to
remedy issues relating to time and synchronization of
message delivery such that we have an approach that will
enable executable models using existing DEVS simulation
frameworks. The approach outlined in this paper
summarizes existing thesis work [16].
2.1. Model Architecture
In DEVS, any component that contains other
components is called a coupled model; non-container
components are called atomic models. All behavior is
derived from atomic models. Generally in UML, a state
machine is used to model an atomic model. A coupled
model can also be modeled using a state machine. Thus,

from a UML perspective one way to think of a DEVScompliant model is a hierarchy of communicating state
machines. Each non-leaf sub-tree represents a coupled
model. Each leaf node represents an atomic model. In our
approach messages bound for a peer node must travel
through the parent node representing the coupled model and
then down to the peer node. This way the state machine for
each node is only aware of a generic parent state machine.
Data messages always originate from atomic models.
2.2. Ports
In DEVS components (atomic and coupled models)
have input and output ports. Output ports from one
component can be connected to the inputs of another peer
component or to the output ports of a containing component.
Likewise, input ports of a containing component connect to
the input ports of immediate sub-components or directly to
their own output ports.
In UML, a Structured Class is a rough analogue to a
DEVS coupled model though it has the capability to have its
own responsibilities beyond being a simple container and its
ports are bi-directional. We recommend using a UML
Composite Structured Diagram to represent a DEVS
coupled model, with the restrictions that ports must be
named and unidirectional, and there can be no connections
from a part back to itself. Corresponding to each port we
introduce an event signal type – by convention the name of
the port matches the name of the event signal type. In
UML, connectors need not attach to components (more
correctly parts) via ports; this is not an option in DEVS and
hence not an option in DEVS-compliant UML. If ports are
specified to provide or require an interface, there should
only be one such interface specified in DEVS-compliant
UML.
For state machines, we map DEVS input ports to events
and output ports to event signal generation. If desired,
atomic and coupled models could join a system dynamically
at runtime. A registration process maps ports to event signal
types. In the event output ports remain unconnected after
registration they are connected to a null output port for the
coupled model, meaning that their outputs are discarded.
2.3. Time in UML
Time is central to DEVS models but in UML 2.0, the
handling of time, especially as required for simulation, has
significant shortcoming. The UML Profile for
Schedulability, Performance and Time Specification [8]
seeks to address these shortcomings but is unnecessary for
our needs since the profile introduces more complexity than
required to achieve a UML representation of a DEVS
model. Instead, a simple protocol of time-related events is
introduced to resolve these issues. UML does have specific
time-related constructs such as after, but in terms of
simulation the use of UML after is problematic since

synchronized behavior among models cannot be guaranteed
because the time cost of simulation is left unaccounted.
Within our modeling approach, time passes only as
accounted for by the special event evSleep(n). The act of
setting state variables, performing transitions, generating
output etc. all occurs in zero time. This simulation-specific
overhead cannot be reliably accounted for via the UML
after function.
2.4. Simulating Time
When we communicate between models we need to
ensure that, where timing is relevant, the passage of time
witnessed by both models is the same. Although a global
clock is not defined in UML our protocol of event signaling
provides the timing coordination necessary for simulation.
Time is counted via event ticks, which are simulations of
real time. The elapsed real (wall clock) time between each
tick may be of varying duration. The outermost coupled
model, the coordinator, issues a evTick(n,sleepExpired) to
each of its contained models and awaits an
acknowledgement. These evTick events are passed along
recursively to all active sub-models. Depending on the
desired simulation speed, the delay between each tick is
adjusted to run faster or slower than wall clock time. Since
the execution of the simulation itself, such as sending and
receiving messages, takes some time, this time is subtracted
from the amount of time to sleep between clock ticks. Thus,
by specifying a simulation time of zero, thereby indicating
that we should not sleep between ticks, the simulation speed
is dictated by the speed of the computer and its resources.
We can get close to our intended real time in our simulation
if we pause between ticks for the amount of time remaining
after the simulation control logic has completed. Obviously,
if the amount of overhead involved in simulating is longer
that the amount of time we intend each tick to represent then
we need a faster computer for our simulation but rarely do
we need simulated time to match real time. The beauty of
DEVS is that the simulated time unit can be shortened or
lengthened to accommodate whatever level of granularity
we choose to model.
2.5. Sequence of Events
In DEVS, the outputs from (or events generated by) an
atomic model are generated in the output function which is
invoked immediately prior to the internal transition function
and never in direct response to an external event. This is the
primary contractual obligation of a designer creating
UML2.0 state charts compatible with DEVS. Whilst this
may appear counterintuitive at first, it is natural from a
simulation perspective – outputs only occur after some
(perhaps zero) amount of processing time. Maintaining this
restriction keeps the model specification consistent and
reduces complexity for large systems. In our approach the
output event signals are generated as part of a transition

triggered on the internal transition event, evTick, which is
generated in response to an earlier evSleep(n) event.
2.6. Simulating Processing
Our executable models don’t actually perform any real
work instead we simulate the amount of time the real system
would spend on a task by sleeping through the generation of
an evSleep(n) event where n is the number of units of time
after which an evTick signal will be triggered. This is
analogous to the UML after event but after is not suitable
for use in state machines in DEVS-compliant UML since all
events must be globally coordinated due to timing
considerations.
If a signal should be generated after some amount of
time, then instead of using the after keyword we generate an
event signal to the containing coupled model requesting to
be woken after that period of time. In DEVS, it is possible
for the subsequent time expiration event evTick and an input
event to occur simultaneously. We can set a precedence for
which event is to be handled first. In DEVS this is called the
confluent function.
2.7. Modeling Simultaneous Events
There is a thorny issue of handling multiple
simultaneous events. If we perceive the inputs to an atomic
model as events, then we are confronted with the restriction
that multiple simultaneous events cannot be expressed in a
UML state machine unless they occur in orthogonal regions
[2]. This restriction may appear reasonable where events are
processed in close to zero time, but from a DEVS
perspective it represents a fundamental hurdle in the UML
specification for reactive behavior. DEVS supports multiple
events being processed at a given point in time. DEVS also
supports time events (e.g. after 10 seconds) occurring
simultaneously with other events. Practically speaking,
whether two events are truly simultaneous is debatable, but
from a modeling perspective it is nonetheless possible,
reasonable, and practical to say two events happen say
simultaneously precisely 10 seconds from now.
We are left with the challenge of how we react to such
simultaneous events. Since simultaneous events are only
partially supported in UML2.0, we must compensate for this
in our modeling approach. For example, if events e1 and e2
are simultaneous, in DEVS, we can model handling these
events and then ignoring an event e3 that occurs during the
processing of e1 and e2. In UML2.0 this is less
straightforward. In UML events are handled one at a time.
As an aside, one may argue that the likelihood of
accepting multiple simultaneous events and then rejecting
subsequent events does not have many practical applications
and simultaneity is only a function of the accuracy of the
clock: if the clock were at a much finer grain, simultaneous
events may not be simultaneous at all. Simultaneous are
therefore events are those that occur within a given window

of time and from a simulation perspective these event are
unordered – they should be presented together.
In order to simulate the simultaneous arrival of multiple
messages we wrap the atomic model with another model
that is responsible for bagging all the individual messages
destined for the atomic model at the same time. To
complicate matters, DEVS allows a model to react to a
message and send an output message without time passing,
hence multiple bags of input messages may be delivered
separately to model at the same time (the clock is stopped
during the delivery phase). Each such delivery occurs during
a different simulation cycle. The reason we use a message
bag and not a list is to represent the fact that the messages
arriving during one clock period have no order since they
arrive “at the same time” even if, during simulation, one
event appears to precede another (remember the clock is
stopped so any ordering during this time must be invisible to
the observer – any order must be made explicitly in the
model itself).
2.8. Simulation Cycles
Modeled/Real Time

evTick(1,true)
Finite number of
Simulation Cycles

UML “after”

evTick(0,sleepExpired)

Simulation
overhead
processing

Overhead non-model
“Zero Time”

evTick(0,sleepExpired)

Multiple simulation
cycles when we have
evMsg() message
delivery and/or
evSleep(0)

evTick(0,sleepExpired)

sleepExpired=true if tick sent in response to expiring evSleep

Figure 1. Simulation Cycle
A clock cycle may have multiple simulation cycles.
During the first simulation cycle, a model receives an
evTick(n,sleepExpired) event with a parameter indicating
the amount of time that has elapsed since the last evTick
message received. A coupled model will only receive an
evTick message in the event that it has a timeNext of zero.
Since atomic models may generate outputs in response to an
evTick signal those messages must be delivered during this
clock cycle. However, it is preferred that these messages be
delivered as a bag of messages and not delivered
individually. To facilitate delivering bags of messages, a
coupled model marks as active any model to which it sends
an evMsg message. Atomic model simulators do not pass
messages directly to atomic models upon receipt but rather
wait for an evTick(n=0) message to arrive. An evTick(n=0)
will never be the first evTick message in the clock cycle
since there will never be undelivered messages from a
previous clock cycle. Thus, the first evTick message in a

clock cycle will always have a non-zero amount of time
elapsed. An evTick(n=0) may also be triggered by the
generation of an evSleep(0) message by an atomic model.
3.

MODELING BEHAVIOR
As in UML where is no formal action language
specification, there is no formal action language syntax
defined as part of DEVS. Often a DEVS specification
involves informal pseudo-code but for executable UML this
is not an option. Within the DEVJAVA [12] framework
Java is the language of choice. There are reservations to
using a procedural language such as Java, for example
Stephen Mellor [9] objects to using Java or a similar
programming language since modelers are likely to develop
specifications that compromise the intended level of
abstraction with non-domain specific constructs such as
pointers and arrays. Whilst these objections are justified, a
pragmatic approach suggests that Java employed in the
specification of a DEVS model is not necessarily a poor
choice so long as the modeler exercises good choices with
respect to its application. Further since simulation models
are disjoint from the models used as specification for
developers, such code is less likely to leach unchecked into
the production code.
4. MODEL COMPONENTS
To recap, each system is composed of a hierarchy of
models. The leaf nodes are atomic models, each with an
associated atomic model simulator. An atomic model
resides in a coupled model which may in turn reside in
another coupled model. The outermost coupled model is the
coordinator. Also, a special type of atomic model called an
experiment may be specified to drive test execution.
4.1. Atomic Model Simulator
The atomic model simulator acts as the interface to the
atomic model. This separate state machine handles the
arrival and bagging of messages. We also account for the
confluent function in this model. Also, since UML event
signals must be sent to an object, and since we want state
charts for atomic models to be reusable components all
events generated by an atomic model are sent via the atomic
model simulator.
4.2. Simulation State Machines
During simulation the coupled models and atomic
model simulators control message flow and timing. Both
share a similar state machine. The signal evTick comes from
the containing model and is relayed to any active model and
to any model with an expiring timeNext. The evAck signal is
generated by a model when it has received an evAck from
each sub-model to which it sent an evTick signal.

cmUponEvMsg(Event ev)

Coupled Model / Atomic Model Simulator
evTick(n)

evSleep(n)

Registration
evRun()

Simulating

evEnd()

evAck(isActive) evMsg(bag)

Figure 2. Simplified State Machine for Coupled Models
and Atomic Model Simulators
The evAck signal has one parameter, isActive, which
indicates whether any sub-model is active. Note, an active
model is any model for which the corresponding atomic
model simulator has a non-empty message bag. In this way,
the coordinator knows whether there are any active models
in the system, and if so, whether another simulation cycle is
necessary. An atomic model must generate an evAck in
response to an evTick event after it has generated any
external output messages – that is, at the end of its output
function
5.

EVENTS
Events are either application or control events.
Application events are those used for passing the application
messages between models during execution and should be
derived from the evMsg event type. Control events are those
used to control the execution of the simulation itself, such as
controlling time (evTick) and performing registration of
models with their respective containers. Since most of the
classes used in the simulation are state machines, they
receive messages such as the declaration of the input and
output ports of the contained models within their event
loops via event signals from the atomic or coupled model
instances that they contain. The important events are now
presented. There are other control events involved in
registration and test setup that are not presented.
5.1. evMsg
All application data messages should derive from this
event signal type. This event signal as it arrives at its
destination atomic model holds an unordered bag of
messages sent to the model during that clock cycle.
When a coupled model receives an event it is forwarded
to the models that have registered to receive it. If the
coupled model has an output port mapped to the event it is
sent to the containing coupled model.
If the event is simply passed through directly from an
input port to an output port and since these port names must
be different the event is signaled using the new output port
name.

foreach Model m in models {
if ( m.isInputEvent(ev) ) {
m.signal(ev);ackExp++;
activeClients[m]=true;
isActive=true;
}
}
if ( container != null ){
if ( isOutputEvent(ev) )
container.signal(ev);
if ( isPassThruEvent(ev) )
signalPassThru(ev);
}

Figure 3. Transition action for evMsg in Coupled Model
amsUponEvMsg(Event ev)
if ( isOutputEvent(ev) )
container.signal(mapEvent(ev));
else
messageBag.concat(ev.messageBag);

Figure 4. Atomic Model Simulator evMsg transition action
The evMsg signal is generated by an atomic model as
part of its output function. The output function is the logic
performed upon receipt of an evTick signal and before the
evAck signal is generated. The output function is the only
time during which external evMsg messages may be
generated. The evMsg type is itself an abstract message
type. The atomic model must send a concrete sub-class of
this message type. For a coupled model, when an evMsg
message is received, it is relayed to any sub-models that
have the corresponding concrete message type as an input.
The evMsg is also relayed to the containing model if the
coupled model has itself the
corresponding concrete
message type as an output
5.2. evSleep(n)
evSleep(n) signal is a request to be sent a evTick after
expiration of n units of time. This is generally used to
simulate the amount of time take to perform processing.
cmUponEvSleep(Event ev)
Model m=models[ev.sender]
m.timeNext=ev.n; // n is ticks to sleep
m.timeElapsed=0;
int newTimeNext = getMinTimeNext();
if ( this.timeNext != newTimeNext ){
this.timeNext=newTimeNext;
if ( container != null ) // not coordinator
container.signal(new EvSleep(newTimeNext));
}

Figure 5. Transition action for evSleep in Coupled Model

The evSleep(n) signal is initially generated by the
atomic model and relayed through the atomic model
simulator to the coupled model. The coupled model stores
the time contained in the evSleep(n) event as the timeNext
for the model. As part of the event signal, the sender is also
identified. A coupled model also contains a timeNext for
itself representing the earliest timeNext of all its sub-models.
This is recalculated each time an evSleep event arrives since
such an event may change the overall timeNext for the
model. If the new timeNext is different than the coupled
model’s current timeNext, then it communicates this new
timeNext to its own container.
amsUponEvSleep(Event ev)
container.signal(ev);

Figure 6. Atomic Model Simulator evSleep transition
Also note that if the atomic model wishes to indicate a
passive state where evSleep(n) has a value of infinity, it can
pass a value of -1 as the duration of the evSleep(n). In this
way, the atomic model simulator will remain dormant until
it receives another external event
5.3. evTick(n,sleepExpired)
evTick(n,sleepExpired) is an event that represents the
passage of n units of time. Since we must not employ UML
time constructs such as after we must model time explicitly
through our own events. Each evTick(n,sleepExpired) event
has an n parameter that specifies the amount of time that has
elapsed since the last tick. Within the same clock cycle,
there may be a need for the message bag to be relayed to the
atomic model multiple times (each time with a new bag).
The fact of its being in the same clock cycle should be
transparent to the atomic model simulator with a new tick
event having a value of zero for n.
When an external event is received the timeNext == 0
condition will evaluate to false since timeNext will be less
than zero. The confluent function will not be entered. One
thing that has not been addressed thus far is passing the
elapsed time since the last internal or external transition to
the atomic model itself. This is easily accommodated by
adding this time as an argument to the messages sent to the
atomic model from the atomic model simulator.
5.4. Processing Ticks in Coupled Models
The algorithm for accepting evTick events in a coupled
model simulator is as follows:

cmUponEvTick(Event ev)
ackCount=0;ackExp=0;isActive=false;
foreach Model m in models
if ( m.timeNext >= 0 ){
m.timeNext -= ev.n; // n: number of ticks
m.timeElapsed += ev.n;
if ( m.timeNext == 0 ){
m.signal( new EvTick(m.timeElapsed,true) );
ackExp++;
if ( activeModels.contains(m) )
activeModels.delete(m)
m.timeNext= -1;
}
}
}
foreach Model m in activeModels
activeModels.delete(m);
// tick not due
m.signal(new EvTick(0,false) ); // to expired
ackExp++;
// timeNext
}

Figure 7. Transition action for evTick in Coupled Model
For each model contained within the coupled model, a
timeNext is maintained containing the amount of time
remaining until the next scheduled evTick. Also, a
timeElapsed is maintained containing the amount of time
elapsed since the last evTick was sent to that model. If there
is no scheduled evTick, then the time remaining will be a
negative number. Atomic models send an evSleep(-1) to
indicate a passive state where the sleep value should be
considered as infinity. A coupled model is considered active
if any of its sub-models are active. Once a coupled model
sends an evTick to a sub-model, that sub-model is no longer
considered active.
amsUponEvTick(Event ev)
if ( messageBag.isEmpty() ) {
model.signal(ev);
} else { // n will always be zero
model.signal(new EvMsg(messageBag));
if ( e.isSleepExpired ){
container.signal(new EvTick(0));
}
}

Figure 8. Atomic Model Simulator evTick transition
When signaled to an atomic model evTick corresponds
to invocation of the DEVS internal transition function – this
signals the expiration of time intended to simulate the
performance of a task as indicated by the preceding evSleep
signal generated by the atomic model. This signal is issued
by the atomic model simulator and sent to the atomic model
when the atomic model simulator receives an evTick event.

5.5. evAck(isActive)
The evAck signal is generated by the atomic model
when it receives an evTick signal and after it has completed
generating any external evMsg messages.
cmUponEvAck(Event ev)
ackCount++;
if ( ! isActive )
isActive=ev.isActive;
if (ackCount == ackExp){
if ( container == null ) { //coordinator
if ( timeNext < 0 ){ // finished – all passive
self.signal(new EvEnd());
} else {
self.signal(new EvTick(timeNext));
}
} else {
container.signal(new EvAck(isActive));
}
}

Figure 9. Transition action for evAck in Coupled Model
The evAck message has one parameter, called isActive,
which is always set to false in the case of an atomic model.
For coupled models, the evAck signal is generated upon
receipt of the final evAck from each of its sub-models and
the isActive parameter is set to true depending upon whether
there are any active sub-models.
5.6. Confluent Events
By default, any atomic models that have an expiring
evSleep, and thus an imminent internal transition via an
evTick event, receive that expiration notification
simultaneously (during the first simulation cycle of the
clock cycle). Since outputs are delivered as part of the next
simulation cycle, the notion of a confluent function becomes
a non-issue, except where a model issues an evSleep(0)
during a simulation cycle, and that model is also sent
messages during that same simulation cycle by another
model. In such a case, there are confluent internal and
external events. Confluent events are simultaneous events
that occur during the same simulation cycle. Events
occurring at the same time but occurring during different
simulation cycles during the same clock cycle are not
considered confluent events.
The precedence for confluent events is handled in the
atomic model simulator. If external evMsg events are to be
given precedence over internal evTick events then upon a
evTick(n,sleepExpired) event where sleepExpired is true, an
evSleep(0) should be issued and upon the next evTick(0)
received any messages can be passed to the atomic model
and then the evTick can be passed to the atomic model. If
desired you can repeated signal evSleep(0) until there are no
outstanding evMsg messages incoming for the atomic model

and only then the evTick() message can be delivered to the
atomic model.
6.

UML MODELING RULES
Under DEVS-compliant UML, an instance of a class
may participate in a model, if and only if, it is contained
within an instance of simulatable object or is itself an
instance of a simulatable object.
A simulatable object is one that has its behavior defined
via a compliant state machine. This implies that all
communication is essentially asynchronous insofar as
replies to messages require an internal transition before a
response is available. However, since evSleep(n) can be
specified with zero time delay, the distinction is moot.
A compliant state machine has the following characteristics:
 Each event signaled must correspond to an output port
name defined for the model.
 Each event received must correspond to an input port
name defined for the model.
 Input and output port names must be disjoint.
 All time-dependent behavior is expressed via evSleep(n)
signals and evTick events. There shall be no after or other
UML time-related references in the state machine
specification.
 Each atomic model state machine has an associated
atomic model simulator.
 The state machine must only send external signals to
atomic model simulator. A state machine may send
signals to itself.
 The state machine must only receive external signals from
atomic model simulator. A state machine may receive
signals from itself.
 All signals sent to the atomic model simulator must occur
upon receipt of an evTick event and before the evTick
transition completes – this is the output phase. Such
activity is defined in the action part of the specification of
the transition triggered by the evTick event.
 An evAck signal must be generated and sent to the atomic
model simulator upon completion of processing of an
external event. All processing from receipt of the external
event through the generation of the evAck signal must be
atomic – it must run to completion.
 An evSleep(n) event signal is generated and sent to the
atomic model simulator whenever the state machine
wishes to simulate the amount of time required to
complete some hypothetical processing, transmission
time, or other such delay. This event is sent to the
associated atomic simulator object. The atomic simulator
will send an evTick event back to the state machine upon
expiration of this time. The state machine remains
dormant (quiescent) during this period or until it receives
another external event (i.e. other than an evTick event).

 If an evSleep(n) event signal is generated, it must
immediately precede the evAck signal generation.
 Upon receipt of an external event, an atomic model may
issue a new evSleep(n) signal which supersedes any
previously generated evSleep(n) signal event. Otherwise,
any existing evSleep(n) will remain in effect. A duration
of -1 in an evSleep(n) signal indicates infinity or passive
state. As such no evTick event will be issued to the atomic
model and the state machine will be dormant until the
next external event.
 All processing time involved in handling events during
simulation is performed in zero time unless explicitly
accounted for via evSleep(n) signals.
 All messages (event signals) are transmitted and received
in zero time – the simulation clock is stopped. Likewise,
all logic performed in the state machine is performed in
zero time. Any requirement to model this time must be
explicitly accounted for in the model via evSleep(n)
signals.
 At any point there should only be one state in which
simulated processing is ongoing as expressed via an
evSleep(n) signal generation. This signal expresses the
fact that there is processing that takes a certain amount of
time, possibly zero, before it completes. Such states are,
however, be interruptible. Note, in UML it is possible to
have states in orthogonal regions within a state machine
that may be performing actions simultaneously – for
DEVS-compliant UML, the restriction is that only one
state responds to an evTick event at any time.
7.

CONCLUSIONS
Those unfamiliar with DEVS and more comfortable
with the UML now have a convenient and relatively
straightforward modeling approach by which their UML
models can be executed and verified at an early stage of
design. This approach has a wide generality in terms of the
types of systems and problems to which it can be applied.
The models produced are component-based, and given the
closure property of DEVS, any component can be replaced
by a different component with a greater degree of
decomposition with the resulting system having an
equivalent behavior. This modeling approach fits neatly
within the modern iterative approach used to develop
software systems. Modeling in this manner helps broaden
and deepen the appreciation and application of simulation as
a discipline within the field of software architecture.
Reference List or References
[1] Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000.
Theory of modeling and simulation: Integrating discrete
event and continuous complex dynamic systems. 2nd
ed. San Diego: Academic Press.
[2] OMG. 2007. UML Profile for Schedulability,
Performance, and Time. http://www.omg.org.

[3] El Sheik, A., A. Al Ajeeli, and E. Abu-Taieh. 2008.
Simulation and modeling: Current technologies and
applications. New York: IGI Publishing.
[4] Douglass, B. P. 2004. Real-time UML: Developing
efficient objects for embedded systems. 3rd ed. Boston:
Addison-Wesley Longman Publishing Co., Inc.
[5] Hong, S.-Y. and T. G. Kim. 2004. Embedding UML
Subset into Object-oriented DEVS Modeling Process,
Proceedings of the Summer Computer Simulation
Conference, San Jose, CA, July
[6] Kofman, E., M. Lapadula, and E. Pagliero. 2003.
PowerDEVS: A DEVS–based environment for hybrid
system modeling and simulation. TR-LSD0306.
[7] Ferayorni, A., and H. S. Sarjoughian. 2007. Domain
driven modeling for simulation of software
architectures. Proceedings of the Summer Computer
Simulation Conference. San Diego, CA.
[8] OMG. 2005. UML 2.0 Superstructure Specification.
http://www.omg.org/cgi-bin/doc?formal/2005-01-02
[9] Mellor, S. J., and M. J. Balcer. 2002. Executable UML A foundation for model-driven architecture. Boston:
Addison-Wesley Longman Publishing Co., Inc.
[10] OMG. 2003. MDA Guide. http://www.omg.org/cgibin/apps/doc?omg/03-06-01.pdf.
[11] Nikolaidou M., V. Dalakas, G. Kapos, L. Mitsi, and D.
Anagnostopoulos. 2007. A UML2.0 profile for DEVS:
Providing code generation capabilities for simulation.
Proceedings of Software Engineering and Data
Engineering, Las Vegas, NV.
[12] ACIMS. 2007. DEVSJAVA.
http://www.acims.arizona.edu.
[13] Huang, D., and H. S. Sarjoughian. 2004. Software and
simulation modeling for real-time software-intensive
systems. Proceedings of the 8th IEEE International
Symposium on Distributed Simulation and Real-Time
Applications. Washington, DC.
[15]Risco-Martin, J. L., S. Mittal, B. P. Zeigler, and J. de la
Cruz. 2007. From UML state charts to DEVS state
machines using XML. Proceedings of the IEEE/ACM
International Conference on Model-Driven Engineering
Languages and Systems. Nashville, TN.
[14] Zinoviev, D. 2005. Mapping DEVS models onto UML
models. Proceedings of the DEVS Integrative M&S
Symposium, San Diego, CA.
[15] Schulz, S., T. C. Ewing, and J. W. Rozenblit. 2000.
Discrete event system specification (DEVS) and
statemate statecharts equivalence for embedded systems
modeling, Proceedings of the 7th IEEE International
Conference and Workshop on the Engineering of
Computer Based Systems. Edinburgh.
[16] Mooney J. 2008. DEVS/UML - A Framework for
Simulatable UML Models [M.S. thesis], Department of
Computer Science and Engineering, Arizona State
University, Tempe, AZ.

Formatted: English (United States)

Interoperability among Parallel DEVS
Simulators and Models Implemented in
Multiple Programming Languages
Thomas Wutzler
Max-Planck Institute for Biogeochemistry
Hans Knöll Str. 10
07745 Jena, Germany
thomas.wutzler@bgc-jena.mpg.de
Hessam S. Sarjoughian
Arizona Center for Integrative Modeling & Simulation
School of Computing and Informatics
Arizona State University, Tempe, Arizona, USA
Flexible yet efficient execution of heterogeneous simulations benefits from concepts and methods
that can support distributed simulation execution and independent model development. To enable
formal model specification with submodels implemented in multiple programming languages, we
propose a novel approach called the Shared Abstract Model (SAM) approach, which supports simulation interoperability for the class of Parallel Discrete Event System Specification (DEVS) compliant
simulation models. Using this approach, models written in multiple programming languages can be
executed together using alternative implementations of the Parallel DEVS abstract simulator. In this
paper, we describe the SAM concept, detail its specification and exemplify its implementation with
two disparate DEVS-simulation engines. We demonstrate the simplicity of integrating simulation of
component models written in the programming languages Java, C++ and Visual Basic. We describe
a set of illustrative examples that are developed in an integrated DEVSJAVA and Adevs environment. Further, we stage simulation experiments to investigate the execution performance of the proposed approach and compare it with alternatives. We conclude that application domains, in which
independently-developed heterogeneous component models consistent with the Parallel DEVS formalism, benefit from a rigorous foundation and are also interoperable across different simulation
engines.
Keywords: DEVS, interoperability, distributed simulation, middleware, scalability

1. Introduction
Interoperability among simulators continues to be of key
interest within the simulation community [1, 2]. A chief
reason is the existence of legacy simulations which are developed using a variety of software engineering paradigms
that are jointly executed using modern, standardized simulation interoperability infrastructures such as HLA [3] and

SIMULATION, Vol. 83, Issue 6, June 2007 473–490
c 2007 The Society for Modeling and Simulation International
1

DOI: 10.1177/0037549707084490
Figure 8 appears in color online: http://sim.sagepub.com

DEVS-BUS [4]. The latter supports the Discrete Event
System Specification (DEVS) modeling and simulation
approach [5]. Based on general purpose simulation interoperability techniques and high-performance computing
technologies, these approaches offer robust means for a
concerted execution of disparate models. However, use of
such approaches can be prohibitive in terms of time and
redevelopment cost of existing models.
For example, in natural and social sciences application domains, often mathematical and experimental data
are directly represented in (popular) programming languages including C, C++, C#, Fortran, Java and Visual
Basic [e.g. 6, 7] instead of first being cast in appropriate modeling and simulation frameworks. Since computer

Volume 83, Number 6 SIMULATION

473

Wutzler and Sarjoughian

programming languages are intended to be generic and not
specialized for simulation, they do not offer some simulation artifacts—such as causal output to input interactions
and time management—that are essential for separating
simulation correctness versus model validation [8, 9]. The
consequence is often, therefore, custom-built simulations
where separation between models and simulators are weak
or otherwise difficult to understand.
Fortunately, these legacy programming-code models
often have well-defined mathematical formulations, facilitating their conversion to simulation-code models. The
translation from programming-code to simulation-code
models can be valuable since the latter can benefit from
rich modeling concepts and artifacts which in turn enable
rich simulation model formulation, development, execution and reuse. A key advantage of using a well-defined
simulation protocol is that it allows a simulator to execute models independent of their realizations, in particular, programming languages. Achieving model exchange
requires a modular design with well-defined interface
specifications and a mechanism to execute the models
within a concerted simulation environment [10, 11]. Various approaches exist for exchanging model implementations and their concerted execution. Techniques range
from highly-specialized coupling solutions [12], the use of
blackboards for message exchange [13], modeling frameworks [14] and XML-based descriptions of models [15] to
the usage of standardized simulation middleware [16].
In this work, we propose the Shared Abstract Model
(SAM) interoperability approach, which provides a novel
capability for concerted execution of a set of DEVS-based
models written in different programming languages. For
example, a DEVS-compliant adaptation of a model of forest growth [17] implemented in Java may be executed
together with a soil carbon dynamics model [18] implemented in C++, using the DEVS simulation engines
DEVSJAVA [19] and Adevs [20, 21]. Furthermore, the
Abstract Model allows the execution of models written in
a programming language for which no simulator has been
developed. An example of this could be a model written
in Visual Basic, but simulated in DEVSJAVA once it is
wrapped inside a component which implements the Abstract Model.
In the remainder of this paper, we will describe the
SAM concept and its realization for executing DEVS (or
DEVS-compliant) simulation models that are expressed
in one programming language, but that are executed in a
simulation environment implemented in another programming language. We exemplify this approach for discreteevent and optimization models. Finally, we will examine
the scalability of the Abstract Model with respect to the
number of couplings between the models, which shows
potential applicability toward large-scale simulations using high performance computing platforms.

474 SIMULATION

Volume 83, Number 6

2. Background and Related Work
Significant advances have been achieved toward simulation interoperability. The importance of simulation interoperability lies in systematic capabilities to overcome differences between simulations that may have vastly different or partially formal underpinnings, i.e. the types
of dynamics each simulation can express could be very
different. Other key benefits are support for development, execution and management of large-scale simulations and emulations. Interoperability, therefore, is chiefly
used for federating disparate simulations, software applications and physical systems with humans in the loop.
Therefore, simulation interoperability must deal with differences between syntax and semantics at a common level
of abstraction among simulations and software applications that are treated as simulations. A common level of
model expressiveness may exceed or constrain the kinds
of simulations that are federated (for an example, see
[22]). In this context, interoperability serves as a standard
such as High Level Architecture [3, 23]. Nonetheless, interoperability concepts and methods lend themselves to
distributed execution. Examples of these methods are numerous and they are generally used for specific classes of
simulation models.
A key advantage of a well-defined modeling and simulation framework is to support building large, complex
simulation models using system-theoretic concepts. Such
a framework can provide a unified basis for analysis, design and implementation of simulation models for complex systems [24, 25]. Systems-theory and its foundational
concept of hierarchical composition from parts lends itself naturally to object-based modeling and distributed execution. Furthermore, the combination of systems-theory
and object-orientation offers a potent basis for developing scaleable, efficient modeling and simulation environments. A well-known approach to system-theoretic modeling and simulation is the DEVS framework [5].
In the context of the DEVS framework, the concept of
DEVS-Bus [26] was introduced to support distributed execution and later extended using HLA [27, 28] and ACETAO ORB in logical and (near) real-time [4, 29]. A recent development executes DEVS models using Service
Oriented Architecture (SOA) [30]. However, these approaches execute models using a single simulation engine.
Considering a modeling framework such as DEVS, interoperability for a class of DEVS-based simulation models
(e.g. Python DEVS [31] and DEVS-C++ [5]) can address
a different kind of need: handling differences between
alternative realizations of a formal model specifications
and simulation protocols. This research, therefore, is involved in interoperability and enabling standardization
while making use of common interoperability concepts
and technologies.
In this paper we focus on the Parallel DEVS formalism,
which extends classic DEVS [5, 32]. The formalism is
well suited to provide the basic mechanism for interopera-

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

tion of simulation models for the following reasons. First,
models can be combined using input and output ports and
their couplings. These models can have arbitrary complexity (structure and behavior) based on a generic, yet formal specification. Second, it allows concurrency with the
closure under coupling property. Concurrency is important for handling multiple external events arriving simultaneously from one or more models and handling simultaneously scheduled internal and external events. Closure
under coupling ensures correctness of input/output exchanges among components of hierarchical coupled models. Third, DEVS can reproduce the other major DiscreteTime (DTSS) and approximate continuous modeling paradigms (DESS) that are commonly used in describing ecological and other natural systems. Fourth, Object-Oriented
DEVS, which supports model inheritance, provides a basis for model extensibility and distributed execution in
logical and/or real-time.
The DEVS formalism is independent of programming
languages or software design choices. Indeed, there exist a
variety of DEVS simulation engines implemented in several programming languages and distributed using HLA,
CORBA middleware technologies or Web-Services [33,
34]. However, interoperability issues arise among DEVS
simulation engines. For example, implementation and design choice differences between DEVSJAVA and Adevs
prevent sharing and reuse of the DEVS models. In the
context of this paper, we refer to models implemented in
different programming languages and executed using different simulation engines as heterogeneous models, given
the same DEVS formal modeling framework.
Aside from basic research in developing simulation environments such as DEVSJAVA to support combined logical and real-time simulations (Figure 1, bottom left), there
has also been interest in distributed simulation given a single DEVS abstract simulator implementation (Figure 1,
top left) (RTDEVS/CORBA [4] and DEVS/HLA [29]).
Moreover, distributed simulations where models are implemented in different programming languages are also of
interest as noted in the previous section. This is because
a primary objective of reusing model implementation is
to avoid recoding models expressed in different programming languages into a single programming language (Figure 1, bottom right). The interoperation between heterogeneous models can be considered as a general case of
distributed simulation because different implementations
of DEVS will run in different processes that communicate with HLA or general-purpose middleware such as
CORBA (Figure 1, top right).
Given these last two considerations, we can consider
three approaches that enable mixed DEVS-based simulation interoperability: (i) adding translations directly into
the models to account for differences between programming languages and alternative simulation engine designs,
which can be automated to a large extent [35]1 (ii) mapping different DEVS simulations to a middleware that
is less formal than DEVS itself1 and (iii) extending the

Figure 1. DEVS-based simulators realizations, given multiple programming languages and processors

DEVS coupling interface schemes (i.e. the syntax and semantics of the DEVS ports and couplings) to support interoperation among different models and distinct simulators implemented in different programming languages.
In the earth sciences, the most common approach for
executing coupled models in high performance computing is approach (i). All different component models are
combined and compiled in ad hoc fashion by one team
into a single model [36]. Compared to the type of interoperability proposed in this paper, this approach helps customizing performance of simulations, but model development is very laborious and inflexible. Furthermore, this
kind of support for interoperability impedes the independent development of the component models by different
research groups. Combined with weak support for model
verification and validation, there disadvantages are major
obstacles in using approach (i) and thus meeting a growing need for alternative approaches in environmental modeling and simulation [11, 37, 38].
The HLA simulation middleware approach (ii) enables
the joint simulation of different kinds of models. Compared to the type of interoperability proposed in this paper, it emphasizes simulation interoperability and capabilities (e.g. data distribution management) instead of theoretical modeling and abstract simulator concepts and
specifications [8, 22]. HLA standard is considered more
powerful in terms of supporting any kind of simulation
that can be mapped into HLA Object Model Template,
including interoperation with physical software/systems.
In particular, any non-simulated federate may be federated with simulated federates using a common set of HLA
services (e.g. time management). However, it is difficult
with HLA to ensure simulation correctness as described
by Lake et al. [22].
Our primary focus, therefore, is on DEVS simulation
interoperability (iii) which is based on formal syntax and
semantics and where simulation correctness among heterogeneous DEVS-based simulations is ensured. This is
useful given the simplicity and universality of the DEVS
framework for time-stepped logical- and real-time modVolume 83, Number 6 SIMULATION

475

Wutzler and Sarjoughian

els. Also, it is well suited for complex, large-scale models that are common in the natural sciences and can be
described rigorously in the DEVS, DTSS and DEVS formalisms. One technique for implementing this approach is
to design wrappers for the DEVS-models written in different programming languages. These can be used by simulators to allow for example cellular atomic models written in
C++ and C# to exchange messages [39]. In this approach,
simulators use wrappers written for different implementations of atomic models to directly communicate with one
another.
In contrast to the above approaches, we present an Abstract Model of the Parallel DEVS formalism in logical
time to establish an interface for model interoperation between multiple implementations of the DEVS Abstract
simulators. The core of the SAM approach is an Abstract
Model Interface which is based on the DEVS simulation
protocol. This Abstract Model Interface has a fundamental
role in enabling concerted simulation of disparate models
executing using distinct simulators. The Abstract Model
approach requires the DEVS simulation engines to provide adapters that support the Abstract Model Interface for
their native models. In this way, the disparity between different atomic/coupled models and their distinct simulators
is accounted for. We describe a realization of this approach
in terms of the DEVSJAVA and Adevs simulation engines.
We show the benefits of the Abstract Model with examples highlighting (a) interoperation between different
DEVS simulation engines1 (b) implementation of models
in a programming language for which there is no DEVS
simulators1 and (c) integrating non-DEVS models within
a DEVS simulation.
Since DEVS can reproduce time-stepped and approximate continuous systems, it acts as a generic interface
for coupling discrete-event, discrete-time and continuous
models. Each component model needs to specify ports,
initialization, state transitions, time advance and output
functions. Models can be hierarchically combined to form
a coupled model. Simulators and coordinators take care
of the correct simulation of the coupled model. Although
there are a variety of extensions to Parallel DEVS, in this
work we consider logical-time simulations.
Before proceeding further, we provide a brief description of the abstract DEVS simulator [5, 32]. Every atomic
component model M = 2X, S, Y, 1 ext , 1 int , 1 con f , 2, ta3
is simulated by a simulator and every coupled component model N = 2X, Y, D, {M d 4 D }, {I c5 D 67N 8 }, {Z c3d }3 is
simulated by a coordinator. Hence, the DEVS simulation
engine constructs a hierarchy of simulators/coordinators,
which corresponds to the hierarchy of atomic/coupled
models. The coordinator at the root of the hierarchy is
called root coordinator. It manages the global clock and
controls the execution of the simulators/coordinator hierarchy. The simulators and coordinators of one simulation
engine share a common input/output interface (i.e. input
events and ports X and output events and ports Y) which
is used by the parent coordinator.
476 SIMULATION

Volume 83, Number 6

The DEVS simulation proceeds in cycles based on discrete events occurring in continuous time. Coordinator
and simulator advance their time based on the time of
last event (tL) and the time of next event (tN). The tN
for a simulator is determined based on time advance of
the atomic models. The tN for coordinators is determined
by taking the minimum tN of all its simulator and coordinator children. The simulation cycle is controlled by the
root coordinator. The root coordinator starts the cycle by
obtaining the minimum tN of all simulators (global tN).
Second, it advances global time. Next, it asks every atomic
simulator to call its model’s output function 2. The simulator does this if it is imminent, i.e. simulator’s tN is equal
to global time. Then, all output events are sent as input
events to their respective destinations. Subsequently, all
imminent simulators are told to execute one of the model’s
three transition functions. Lastly, tL and tN of the root coordinator are updated. When all atomic models have their
time of next events scheduled for infinity, the root coordinator simulation cycle terminates.
A simulator decides which of the three transition functions (1 ext , 1 int , 1 con f ) of a model is to be invoked by comparing the model’s time advance function (ta()) to global
tN. Time advance function specifies the relative time until a next internal transition function in the model, i.e. a
scheduled next event time. The external transition (1 ext ) is
invoked by the simulator if inputs (i.e. X) for the model
arrive at times prior to the model’s next event time. The
internal transition (1 int ) is invoked once the model’s time
advance is expired, but no external events arrive prior to or
at this time. The confluent transition (1 con f ) is invoked if
the model’s time advance is the same as the time when external events arrive. If there is no input event to the model
and the model’s time advance is not yet expired, no action
is taken. Lastly, simulator tN and tL are updated based on
models ta(), and coordinator tN is updated to the minimum tN of its simulator and coordinator children. The responsibility of a coordinator is to exchange input and output events (X and Y) between itself and its simulators and
coordinator children using external input/output and internal couplings (i.e. {Z c3d }) defined in a coupled model.
All events in DEVS are bags of messages. Each bag
may contain several messages of varying types for each
port at a given time instance. It enables the modeler to deal
with parallel external input or output events. DEVS can
also handle zero time advances, i.e. a complete simulation
cycle taking zero time. To avoid infinite loops there must
be at least one atomic model in each feedback loop that
generates a non-zero time advance after a finite number
of zero time steps.

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

Figure 2. Abstraction of different DEVS implementations

Listing 1. The Abstract Model Interface specification

3. Approach
3.1 An Abstract Model for Alternative DEVS Model
Implementations
Different implementations of the DEVS formalism share
the same semantics due to the DEVS mathematical
specification, but they differ in the underlying software
design. In order to allow an abstraction for different implementations, we have defined an Abstract Model as shown
in Figure 2. The Abstract Model Interface is shared by

all participating simulators and models. Adapters account
for disparities between the implementations. The operations of this Abstract Model can be realized with a middleware. A simulator usually directly invokes operations
on the model. In the presented approach, the method invocations are mediated by the Abstract Model.
We specified the Abstract Model Interface in OMG-idl
(Listing 1) and used CORBA to invoke these operations
expressed in different programming languages. It is important to note that one basic interface is defined for models instead of defining interfaces for simulators. FurtherVolume 83, Number 6 SIMULATION

477

Wutzler and Sarjoughian

Figure 3. Adapting specific DEVS implementations to support the Abstract Model approach

more, we have not defined an interface for coupled models
as the execution of a coordinator of a coupled model can
be specified as an atomic model. This will be explained in
Section 3.2.2.

contents, for which an additional software component is
employed. Error handling has been omitted from Listing 2
for compactness.
3.2.2 Model Adapter for Coupled Models

3.2 Adapting DEVS simulation engines
To support the functionality of the above Abstract Model,
existing DEVS simulation engines are required to provide adapters to the Abstract Model Interface. A simulator does not need to know that some of the submodels are executed in a remote process. This is achieved by
using a Model Proxy as shown in Figure 3. The Model
Proxy translates its method invocations to invocations of
the Abstract Model Interface. While the translation is only
syntactical in nature and preserves Parallel DEVS model
semantics, the handling of message contents can be nontrivial.
The invocations of the Abstract Model Interface can
be mediated to a remote process using a middleware. If
the actual model implementation, as shown in Process
2, cannot directly support the Abstract Model, a Model
Adapter is needed to translate the invocations of the Abstract Model to the invocations of the Model Implementation (Figure 3). In this setting, therefore, the Simulator and
the Model Implementation remain unchanged. The Model
Proxy and the Model Adapter need to be developed only
once for a given simulation engine. The simulator can then
simulate any implementation of the Abstract Model, and
all models specified for the simulation engine can be represented as implementations of the Abstract Models.
3.2.1 Model Proxy and Model Adapter for Atomic
Models
The implementation of the model proxy and the model
adapter for atomic models is straightforward as the abstract model interface actually corresponds to an atomic
model. The model proxy was implemented in both example implementations DEVSJAVA (Listing 2) and Adevs
by extending the atomic model. All invocations are simply
translated to invocations of the Abstract Model Interface.
The only non-trivial issue is the translation of message
478 SIMULATION

Volume 83, Number 6

While the model adapter for atomic models is as straightforward as the model proxy, the model adapter for coupled
models is cleverer. It makes use of the DEVS closure under coupling property, which states that each coupling of
systems defines a basic system [5]. Our basic idea is to
consider the execution of a coordinator as the execution
of an atomic model. Hence, the model adapter for a coupled model employs a coordinator to wrap the execution
of the coupled model according to the specification of the
Abstract Model. Therefore, the simulation cycle of a coordinator is specified as a DEVS-model within the model
adapter. Although this use of the Abstract Model for coupled models is unusual given the simulator/coordinator
separation, this approach has several advantages, which
will be discussed in Section 6.
In the following, the term ‘processor’ is used as a
generic term for both simulator and coordinator. In DEVSenvironments DEVSJAVA and Adevs, the processors exhibited the following methods in their interfaces.
9 ComputeIO(t): with global time t, first invokes
ComputeIO function of associated processors
(which eventually call the output function of the imminent models) and second distributes the outputs
to other processors and the parent coordinator.
9 DeltFunc(t, m): with global time t and message m,
adds given message bag to the inputs of the coordinator, distributes these inputs to the corresponding processors and invokes DeltFunc of the associated processors (which eventually execute transition
functions of the models) for time t and updates the
times of last and next event (tL, tN).
Usually these methods are called by the parentcoordinator. In order to execute a coordinator within an
Abstract Model, these methods have to be called from the
methods of the model adapter in the same correct order as

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

Listing 2. DEVSJAVA implementation of the model proxy

by the parent-coordinator. The crucial point is that each
processor receives all inputs before its delta-Function is
invoked. The parallel DEVS simulation protocol guarantees that the output function is called exactly once before
the internal or confluent transition function. The functions
tL, tN, and getOutputs return the coordinator’s last event
time, next event time and the bag of external outputs respectively. With these conditions, the coordinators’ methods can be mapped to the Abstract Model as shown in
Listing 3. Error handling and message translation have
been omitted from the listing for compactness. In Section 4.1.3, it will be demonstrated that the execution order
is kept correct.
The implementation of the coordinator can be potentially very different from the description in Listing 3 given
other DEVS simulation engines. However, all the implementations are to exhibit the division into calculation and
distribution of processors outputs on the one hand, and execution of the transition on the other hand. Hence, the description in Listing 3 can guide the development of model
adapters for coupled models of other simulation engines.

3.2.3 Integration of Non-DEVS Functionality
In addition to interoperating various DEVS simulation engines, the Abstract Model can also be used to integrate
non-DEVS functionality. In this case, the model adapter
maps the non-DEVS functionality to the Abstract Model.
Three examples of this integration follow.
(1) One use case is the integration of time-stepped
models. The time-advance function has to return the
time until the next time step. The internal transition
executes the transition. The external transition will
only store the inputs for the next transition.
(2) A second use case is the integration of continuous
time models that specify the calculation of derivatives but have no notion of DEVS yet. The model
adapter will employ quantization [40]. The transition functions will invoke the calculation of the new
derivatives within the model implementation. Next,
the transition functions will update a quantized integrator and calculate the time until the next boundary crossing. After an ordinary internal transition,
Volume 83, Number 6 SIMULATION

479

Wutzler and Sarjoughian

Listing 3. DEVSJAVA implementation of the model adapter for coupled models

an internal output transition is scheduled. The timeadvance function will return the calculated time until the next boundary crossing. The output function
of the model adapter will return the output of the
continuous model, but only if it is in the output
phase.
(3) Another use case is the integration of functions that
do not depend on time, i.e. a Mealy-type passive
model, e.g. an optimization procedure. The external transition function of the model adapter will invoke the original f optimization’s function and immediately schedule an internal transition in phase
‘output’. Within the internal transition the model is
again set to a passive state, i.e. with a time-advance
of infinity. The output function will return the result of the function invocation, but only if it is in the
output phase.
4. Example Applications
4.1 Interoperation between Different DEVS
Simulation Engines
The first detailed example demonstrates how a coupled
model, which is developed and implemented in one DEVS
simulation engine, is used as a component model within
a larger coupled model within another DEVS simulation
engine.
480 SIMULATION

Volume 83, Number 6

4.1.1 The ef-p Example Model
The ef-p model is a simple coupled model of three atomic
models (Figure 4). The atomic and coupled models are
shown as blocks and couplings between them are shown
as unidirectional arrows with input and output port names
attached to them. The generator atomic model generates
job-messages at fixed time intervals and sends them via
the Out port. The transducer atomic model accepts jobmessages from the generator at its Arrived port and remembers their arrival time instances. It also accepts jobmessages at the Solved port. When a message arrives at
the Solved port, the transducer matches this job with the
previous job that had arrived on the Arrived port earlier and calculates their time difference. Together, these
two atomic models form an experimental frame coupled
model. The experimental frame sends the generators job
messages on the Out port and forwards the messages received on its In port to the transducers Solved port. The
transducer observes the response (in this case the turnaround time) of messages that are injected into an observed system. The observed system in this case is the
processor atomic model. A processor accepts jobs at its
In port and sends them via the Out port again after some
finite, but non-zero, time period. If the processor is busy
when a new job arrives, the processor discards it.

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

Figure 4. Experimental frame (ef)-processor (p) model

Figure 5. Distributed setup of the ef-p model

4.1.2 Implementation of the ef-p Example Model
using DEVSJAVA, Adevs, and CORBA
This example and the following were developed and run
on several personal computers running the operating system Windows XP. All examples have been tested on a single machine and, in addition, also with running the component models on different machines. We partitioned the
models into two different simulation engines according
to Figure 5. Rounded boxes represent operating system
processes, white angled boxes represent simulators, dark
gray boxes represent models, light grey shapes represent
interoperation components and arrows represent interactions.
First, we implemented the model proxy and the model
adapters for the atomic and coupled models, as described

in Section 3.2, in the two DEVS simulation engines
DEVSJAVA and Adevs using the programming environments Eclipse version 3.1 and Visual Studio version 7.1.
Next, we implemented the ef coupled model in DEVSJAVA and the processor in Adevs in the usual way, using
only one DEVS simulation engine. Finally, we set up and
performed the simulation of the ef-p model. The experimental frame coupled model, a message translator, and the
model adapter were constructed and started in a DEVSJAVA server process (Listing 4a). Further, the CORBAObject of the model adapter was constructed using the
SUN Object Request Broker (ORB) and naming service
which is part of the JDK 1.5 (SUN 2006). The CORBAstub of this adapter was then obtained in the C++/Adevs
client process using the ACE/TAO ORB version 1.5. Together with a message translator the model proxy was conVolume 83, Number 6 SIMULATION

481

Wutzler and Sarjoughian

Listing 4. Constructing and using the remote experimental frame sub-model

structed (Listing 4b). Finally, this model proxy was used
as any other Adevs atomic model within the Adevs simulation (Listing 4c).
4.1.3 Execution of the ef-p Example Model
The execution of the coupled ef model by the model
adapter using a coordinator appears to the model proxy as
the execution of an atomic model. The following section
illustrates that the execution order is kept by an execution
trace. The example is a complex confluent transition of
the model setup according to Figure 5. The generator produces job messages at time intervals of 5 s and the processor has a processing time of 10 s. Before the confluent
transition at time 10 the processor had accepted the first
job at time 0 and neglected the second at time 5. Next
event times are 10 for the processor, 10 for the generator
and infinity for the transducer. Next event time for the ef
coupled model is the minimum of the time advance for the
generator and the transducer models (i.e. ta() = 10).
First, the output function of the Adevs model proxy
is invoked. The model proxy invokes the output function
of the DEVSJAVA model adapter (Figure 6). The signatures of the model adapter correspond to the Abstract
Model (Listing 1). The signatures of the coordinator and
the simulators are according to DEVSJAVA implementation and correspond to the methods described in Section 3.2.2. Most of the complexity is hidden within the
coordinator. There are only two calls between processes,
i.e. between model proxy and model adapter. The model
adapter tells its coordinator to compute and distribute the
outputs. The coordinator does this by first letting the simulator of the only imminent component (generator) invoke
the generators output function via computeIO. Second, the
coordinator calls the simulator’s sendMessages method.
This causes the simulator of the generator (g) to put messages on the transducers (t) input and on the coordinators
output port. Finally, the model adapter is able to return
the outputs of the coordinator as the result of the output
function.
482 SIMULATION

Volume 83, Number 6

The Adevs coordinator of the ef-p model routes the output of the processor (job 1) to the model proxy. Because
the model proxy is imminent, its confluent transition is
invoked. The model proxy invokes the confluent transition of the model adapter. Next, the model adapter places
the external input at the coordinator’s inputs by calling
putMessages. Next, it invokes the coordinator’s transition
function DeltFunc with the next event time. The coordinator first routes the input job to the simulator of the
transducer. Next, it asynchronously invokes the transition
functions of the simulators of the generator and the transducer. The generator’s simulator has no external inputs but
it is imminent, hence it executes the internal transition,
which schedules the next job. The transducer is in passive state, hence its simulator executes the external transition with a bag of two inputs. When both generator’s and
transducer’s transitions have completed, the coordinator
updates its times of last and next event.
After control flow has returned to the model adapter,
the model adapter asks the coordinator for the times of last
and next events and returns the time difference. Finally,
the model proxy holds the atomic model in active state for
this time.
4.2 DEVS-Compliant Models without Simulators
The second application example illustrates the simulation
of a DEVS-compliant model that has been implemented
without a corresponding DEVS simulation engine. We implemented the processor model (see the previous section)
within VBA routines of a workbook of MS-Excel version
2003. The class of the Visual Basic processor model descended directly from the portable object adapter classes
which were generated by VBORB, an object request broker for Visual Basic [41]. This processor directly implemented the Abstract Model. Hence, no model adapter was
needed. The processing time of the processor was obtained from a cell of a workbook. Therefore, the user could
easily change the model behavior before or during the
simulation. Within a start-up routine of the workbook, a

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

Figure 6. Sequence diagram of an output and a subsequent confluent transition

CORBA object was constructed and published. Within a
DEVSJAVA simulation a model proxy was initialized with
the CORBA-stub of this processor and coupled to an experimental frame model. For the DEVSJAVA simulation
it was completely transparent that the processor submodel
was executing a remote process of the Excel-worksheet.
4.3 Non-DEVS Models
The third application example demonstrates integration of
Non-DEVS functionality as described in Section 3.2.3(3).
We wrapped an optimization problem, i.e. a timeless function, by the Abstract Model Interface. The optimization
problem minimized the price for power supply by ordering the power from different providers with different pricing schemes. The proportions of the power that were ordered by the different providers were optimized depending on the amount of required total power. The problem
was modeled within an MS-Excel worksheet and solved
by employing the MS-Excel solver. Further, a simple
DEVSJAVA experiment was devised that consisted of the
proxy of the optimizer model, a generator model and an
observer model. The latter two submodels and the coupled model were specified in DEVSJAVA. The generator

submodel produced events of changes in the amount of
total required power. This output was connected to the
optimizer model. The outcomes of the optimization, i.e.
shares of power and price, were coupled to the observer
model. The observer logged the inputs together with the
time of receiving them. On execution the optimization was
carried out whenever the generator produced changes in
power demand, and the observer logged the result of the
optimization at the same points in logical time when the
changes in required power took place.
5. Simulation Experiments on Model
Performance and Scalability
This section describes some experimental results on the
performance of the Abstract Model approach using a set
of ef-p models, which were presented in Section 4.1.1.
The first aim for these experiments was to show that the
Abstract Model approach scales well with the number
of component models and links. The second aim was to
demonstrate that performance gains are achieved by distributing the simulation to several machines using the Abstract Model. In these performance studies, the overall
coupled model was composed of n = 2..64 ef-p component
Volume 83, Number 6 SIMULATION

483

Wutzler and Sarjoughian

Figure 7. Allocation of model components to different operating system processes

Table 1. Scenarios for measuring the performance impact of the
Abstract Model
InProc
Baseline

n = 2, 4, 8, 16, 32, 64

Local

n = 2, 4, . . . , 64

BetweenProc
n = 2, 4, . . . , 64

Remote

n = 2, 4, . . . , 64

n = 2, 4, . . . , 64

Distributed

n = 2, 4, . . . , 64

n = 2, 4, . . . , 64

models. We set up a suite of experiments including four
scenarios (Table 1). In the Baseline scenario, all models
are simulated using DEVSJAVA in a single operating system process without using the Abstract Model. In the Local scenario, the Abstract Model was used to mediate interoperation between component models. The component
models and their model adapters were executed in two
operating system processes that were different from the
process of the root-coordinator (Figure 7a). All processes
executed on a single machine. In the Remote scenario, the
root coordinator process executed on one machine while
the two model processes executed on another. In the Distributed scenario, all processes executed on different machines (i.e. each process executes on its own dedicated
machine).
Two kinds of overhead are important to consider when
interoperating (heterogeneous or distributed) simulations.
The first is due to time synchronization and the second
to message passing among distinct processors (or logical processes). The second is strongly correlated with the
number of nominal data links [26].
To capture these kinds of overhead, InProc and BetweenProc configurations were devised (Figure 7). Gray
boxes represent models, arrows represent couplings (message passing) and round boxes represent operating system
processes. The atomic models in InProc and BetweenProc
configurations were the same, only the coupling to ef-p
models varied. In the former InProc configuration (Figure
7b), each ef-p model was executed within one operating
system process and message passing between the component models of each ef-p model occurred within only
one operating system process. In this configuration, only
484 SIMULATION

Volume 83, Number 6

synchronization messages were exchanged across process
boundaries as there were no nominal data links between
processes. In the latter BetweenProc configuration (Figure 7c), the component models of each ef-p model resided
in different processes and the couplings (ef-p coupled
model) were specified in the root coordinator’s process.
In this configuration, the component models of each
ef-p model additionally exchanged data messages across
process boundaries. The number of nominal data links increased by two with each additional ef-p model.
The Local, Remote, and Distributed scenarios were
compared to the Baseline scenario and to one another to
quantify the impact of the Abstract Model on total simulation execution times. Scaling effects were studied within
each scenario by varying the number of ef-p models that
were simulated together with one root coordinator.
First, time synchronization overhead was studied by
comparing the DEVSJAVA scenario and the Local InProc
scenario.
overheadSynchronization



timeLocal_InProc  timeBaseline
 100%
timeBaseline

Second, message passing overhead was studied by
comparing the local InProc scenario with the local BetweenProc scenario.
overheadMessagePassing



timeLocal_BetweenProc  timeLocal_InProc
 100%
timeLocal_InProc

Finally, the speedup of distributing the models on different machines was studied by comparing the remote and
the distributed scenario.
speedup 


timeRemote
timeDistributed

Performance studies were performed on an IBM T42
computer with 1 GB main memory and a 1.7 GHz Pentium processor. For the remote and distributed scenarios,

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

Figure 8. (a), (b) Execution times1 (c) overheads1 and (d) speedup of the performance experiments

two identical computers (1 GB main memory, 2.8 GHz
Pentium processor) were connected to the T42 with a
100 Mbps network connection. Execution times were
measured using the System.nanoTime() function. Other
processes (virus protection, etc.) were still running during the performance studies, causing an additional random component in execution times. All atomic and coupled models were implemented and executed using JAVA
version 1.5 and DEVSJAVA version 2.7.2 that was extended with multi-threading. To help measure execution
times, 104 flops were added to the Proc model transition
function.
5.1 Results of the Performance Studies
In all scenarios, the execution time basically increased
linearly with the number of simulated ef-p models (see

Figure 8a and 8b). The execution time of models executing in different processes at the same computer was 2–4
times longer than the Baseline scenario execution time.
For example, in the case of having 64 ef-p models, the
local BetweenProc execution time was 3.5 times that of
the Baseline scenario, corresponding to a 250% overhead.
The overhead was due to synchronization and message
passing (Figure 8c), each roughly accounting for half of
the total overhead. No increasing trend of the overhead
was observed due to the scaling of the number of the simulated models and the associated higher number of data
links. In the case of 16 ef-p models, the execution time for
the local InProc scenario was, by chance, exceptionally
high.
With models residing on one different machine (remote
scenario), the execution times of the InProc configuration
were similar to the case where models were executed on
Volume 83, Number 6 SIMULATION

485

Wutzler and Sarjoughian

Listing 5. Use of abstract interfaces for simulators instead of models

the same machine (local scenario). However, for the BetweenProc configuration, the execution time was higher
than in the local scenario (compare Figure 8a and 8b). Distributing the models on two computers reduced execution
times by two-thirds to nearly half within both scenarios
(see Figure 8b). This corresponds to a speedup of 1.5–2
(see Figure 8d). It is noted that the speedup does not decrease with the number of simulated models.
6. Discussion
High performance computing often deals with complex
models that are built from component models of different scientific fields and often by different teams, organizations or institutions. Two commonly-used approaches
for simulation of the coupled model are (i) translating the
component models into one complex model and (ii) using simulation middleware (e.g. HLA). The first approach
impedes independent component model development and
the second approach may not formally ensure simulation
correctness. The approach of utilizing DEVS to couple
component models (iii) presented in this paper is capable of overcoming both shortcomings at the same time.
First, the component models can be implemented independently with different simulation engines and different
programming languages and run as different processes allowing distributed execution. Second, the DEVS formal486 SIMULATION

Volume 83, Number 6

ism formally ensures simulation correctness. The precondition for the application of the presented approach is that
all component models comply with the (parallel) DEVS
formalism. The execution of the coupled model by several communicating processes naturally leads to distributed simulation platform settings.
In order to overcome differences in various DEVS implementations and programming languages, we proposed
the Abstract Model and specified its interface in a metalanguage. There are also alternative approaches to abstracting these differences. As shown in Listing 5 (Simulator 1), an abstract interface can be defined for simulators instead of models [42]. The simulator is asked at
each simulation cycle to generate outputs if it is imminent.
There is only one transition function and the simulator decides itself based on global time and inputs which kind
of transition is to be performed. A performance optimization may be achieved by integrating the invocation of tN
function into the start function or DeltFunc, which calls
1 ext , 1 int or 1 con f of the model (Listing 5, Simulator 2). In
yet another approach (Listing 5, Simulator 3), the simulators are informed about their coupling information and
send messages directly to the other simulators instead of
the coordinator (e.g. [39]).
However, the SAM approach exhibits a better performance than using an abstract interface for simulators because the SAM utilizes the sparseness of the DEVS formalism, which is grounded in the asynchronous way of

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

model execution. Performance is determined to a great degree by the number of communications between the different operating system processes (possibly executed on different nodes) and only to a smaller degree by in-process
sub-routine invocations. One invocation of a method of
the abstract model interface or abstract simulator interface
respectively corresponds to one inter-process communication. While there are two invocations of the abstract simulator2 interface at each simulation cycle (and even more
invocations with simulator1 and simulator3), the Abstract
Model interface is only invoked in the subset of simulation cycles where there are input messages to the model
or if the model is imminent.
Both time synchronization and message passing overhead of the Abstract Model were about 120%. This
is worse compared to the shared memory version of
the DEVS-BUS (synchronization overhead 20–25%), but
much better than the HLA/RTI implementation of the
DEVS-BUS (5,000–6,000%). The message passing overhead does not increase with the number of messages and
the number of nominal data links (which is true for the
DEVS-BUS). One major advantage of the Abstract Model
compared to the DEVS-BUS is the exploitation of parallelism. The execution of the model on two computers shows almost the same performance as in the case
where no Abstract Model is used (Figure 8). Performance
can improve further when a model can be partitioned appropriately to execute on a larger number of processing
nodes.
The execution of the models can be migrated to a grid.
The configuration of the model servers can be done by
a naming service and the factory pattern. There was no
need to change the configuration of the client (the process
of the root coordinator) to switch between the local, remote and distributed scenario in the performance experiments. The locations of the component models are completely transparent to the simulation. In addition to showing reasonable performance characteristics, the SAM approach for combining heterogeneous DEVS models has
the advantage of making it straightforward for sub-models
to participate in a coupled simulation. Hence, few or
no changes are necessary for the DEVS models that are
written for specific simulation engines, for which model
adapters have been developed. Often relatively simple
adapters will suffice. We demonstrated the simplicity of
specifying component models and the setup of the coupled
simulation (Section 4.1.2). Additionally, we demonstrated
how easy legacy models can be integrated into a coupled simulation by developing specific DEVS-compliant
model adapters (Sections 4.2 and 4.3). Other approaches
(e.g. directly using HLA) place more constraints on the
sub-models. This is because HLA is intended to support
all simulations as well as physical systems with support
for different types of simulation protocols (e.g. combined
conservative and optimistic simulations). A further advantage of the proposed approach is the free availability of
both CORBA implementations and DEVS simulation en-

gines for common programming languages which are used
in industrial-strength settings as well as research and development.
Correctness of model execution, in the presented approach, is grounded in the DEVS formal specification.
The synchronization among distributed models and coordinators is handled by the DEVS protocol. Within DEVS
the models have no notion of ‘global’ time. However,
the transitions of the remote model are invoked in correct order (see background section). In order to support
this functioning, the developer of a model adapter for a
specific DEVS simulation engine has to show that the
model adapter is compliant to the Abstract Model. This
ensures that every model that is legitimate in logical time
in the original simulator is also legitimate in the heterogeneous case. Interoperability between different DEVS simulation engines using the SAM approach allows employing (or implementing) different engines without having a
coupled Abstract Model. Instead, we mapped the execution of a coordinator to an atomic model. Hence, the interoperability takes place at the model level, not at the simulator level. The benefits are that only few constraints need
to be placed on models that were not specifically designed
for a DEVS-simulator and that less messages between
processes are required (see section performance considerations above). The coupled structure of a remote model is
transparent to the simulator. Nonetheless, from the modeling perspective, every hierarchical coupled model can
be systematically mapped onto a flat model without any
side-effect on the approach presented here.
We note that the SAM approach does not rely on
any particular middleware such as CORBA, although the
choice of a middleware has importance including performance and interoperability robustness. Thus, the Abstract
Model may be implemented, for example, with COM,
Web services or MPI.
A recent work that is aimed at combining different
DEVS models is DEVS Modeling Language (DEVSML).
It relies on semi-automatic translation DEVS models to
their XML counterpart [35]. This environment is being
applied to system acquisition test and evaluation [30]. In
terms of execution, the environment supports synthesizing coupled DEVS models. The translation of the models
that are specified using DEVSML to simulation code is far
more complex in comparison to the approach developed
in this paper, and currently only the single programming
language JAVA is supported.
In the context of this work, we have accounted for
logical-time DEVS models. The approach presented in
this paper, however, is in principle also applicable to realtime DEVS models [43]. In this case, the interface of
the Abstract Model can stay the same, but the semantics (execution) of the Abstract Model needs to comply
with real-time DEVS specifications. Activities, which are
defined as abstractions of tasks in a real system, are part
of the real-time DEVS transition functions. Each activity takes non-zero wallclock time to be completed and
Volume 83, Number 6 SIMULATION

487

Wutzler and Sarjoughian

thus cannot return immediately. An activity is to be computed within a finite time period. For example, the external transition of the optimization model (Section 4.3)
would schedule an internal transition after a time greater
than zero that reflects an optimistic estimate of the time
to do the optimization calculation. It would then start
the optimization activity, but return immediately. After
calculating the ‘optimal price’ activity in real-time, the
price is returned with the output function. The implementation of the activity is left to model specification.
RTDEVS/CORBA can simulate any DEVSJAVA model in
real time. Hence, with the DEVSJAVA model proxy, it will
be able to simulate a Parallel DEVS Abstract Model in real
time.
The first use case of the SAM approach is the interoperation of several DEVS models that are implemented
in different DEVS simulation engines or different programming languages. This was already possible by the
DEVS-Bus/HLA implementation [29] or using HLA directly. However, the SAM approach requires far less implementation complexity and in many cases less performance overhead. The second use case is distributing a
simulation of DEVS-models that are implemented in the
same simulation engine. This was also possible before
with several methods (DEVS-Bus, DEVS/CORBA [44],
CD++ [39]). Which of these methods is most efficient
in performance depends upon the number of models, the
number of links between the models and the implementation of the DEVS simulation engine. The SAM approach
also allows the inclusion of non-DEVS component models by designing model adapters. This gives the SAM approach the potential to develop to a simulation middleware
that ensures simulation correctness. The usage of HLA or
MPI to design a distributed model from the beginning will
lead to solutions that are more efficient in performance in
most cases. However, we think that the usage of the Abstract model for setting up an ad hoc integration of existing models will be much easier, flexible, scalable and
easier to maintain in most cases.
The SAM approach places only few constraints on partitioning a complex model into component models and allocating models on different machines. Both structure and
allocation of a component model is completely transparent to the simulator/coordinator that simulates the component model. This gives the possibility of extending the
approach to include variable structure [45] or to perform
dynamic re-allocation of component models during runtime [46]. However, this requires extending the model
proxy described in Section 3.2.1. Furthermore, the SAM
approach presented here can aid the DEVS Standardization effort [47].
6.1 Outlook

7. Conclusions
The Shared Abstract Model (SAM) interoperability approach presented here supports DEVS-based simulations
and their extensions consistent with the DEVS formalism. With this approach, component models can be developed independently in alternative DEVS simulation environments and programming languages. The correctness
of disparate simulations is ensured based on DEVS formalism. The SAM can be implemented using sound distributed middleware concepts and techniques. The Abstract Model supports desirable scalability trait. It utilizes the sparseness inherent in the DEVS formalism and
inter-process communications and exhibits good simulation speedup. The basic interface developed on top of
the DEVS formalism enables flexible and non-tedious
integration of new component models. Furthermore, the
SAM approach may be extended with standardizing message formats and simulation services. We conclude that
the proposed interoperability approach supports DEVSbased simulator interoperability providing several advantages and thus that it is indispensable for the use in application domains where heterogeneous simulation models
are executing on parallel/distributed platforms.
8. Acknowledgements

To further the work presented, it is important to employ
the proposed approach with high performance computing for conducting large-scale simulations. As part of this
488 SIMULATION

effort, simulation services such as message filtering, remote parameterization and automated stop/save/recovery
of simulations may be developed and supported. Given
the interest in simplifying and automating distributed simulation configuration, management and execution, another research direction is devising a testbed with metrics to help evaluate alternative simulation design experiments. This may be done by considering performance features of different simulation engines, complexity of data
transformations, communication bandwidth and serviceoriented realizations. Further research in automated partitioning and configuration of models for distributed simulation is of great interest [46]. In particular, generic model
partitioning offers a basis for assigning execution of
model components to simulation engines given domainindependent computational cost modeling and thus improving the performance and efficiency of complex multiscale biological system modeling. Further, the proposed
approach is planned to be applied to simulating forest carbon dynamics that consists of component models of forest
growth, soil carbon dynamics and carbon in wood products [48]. This research includes developing SAM model
adapters to account for mixed continuous/discrete dynamics described with differential equations and discrete-time
system specifications, as outlined in section 3.2.3.

Volume 83, Number 6

This work was funded by a doctoral scholarship of the
German Academic Exchange Service. The authors thank

INTEROPERABILITY AMONG PARALLEL DEVS SIMULATORS AND MODELS IMPLEMENTED

the anonymous reviewers of an earlier version of this paper. Their critiques and suggestions helped with the organization of the paper and the presentation of the materials
within.
9. References
[1] Fujimoto, R. M. 2000. Parallel and Distributed Simulation Systems.
John Wiley and Sons, Inc.
[2] Righter, R., and J. C. Walrand. 1989. Distributed Simulation of Discrete Event Systems. In Proceedings of the IEEE 77(1): 99–113.
[3] IEEE. 2000. HLA Object Model Template, Version IEEE 1516.22000. IEEE.
[4] Cho, Y. K., X. L. Hu, and B. P. Zeigler. 2003. The RTDEVS/Corba
Environment for Simulation-Based Design of Distributed RealTime Systems. Simulation: Transactions of the Society for Modeling and Simulation International 79(4): 197–210.
[5] Zeigler, B. P., H. Praehofer, and T. G. Kim. 2000. Theory of Modeling
and Simulation 2nd Edition. Academic Press.
[6] Sitch, S., B. Smith, I. C. Prentice, A. Arneth, A. Bondeau, W. Cramer,
J. O. Kaplan, S. Levis, W. Lucht, M. T. Sykes, K. Thonicke, and
S. Venevsky. 2003. Evaluation of Ecosystem Dynamics, Plant
Geography and Terrestrial Carbon Cycling in the LPJ Dynamic
Global Vegetation Model. Global Change Biology 9(2): 161–85.
[7] Thornton, P. E., B. E. Law, H. L. Gholz, K. L. Clark, E. Falge, D.
S. Ellsworth, A. H. Golstein, R. K. Monson, D. Hollinger, M.
Falk, J. Chen, and J. P. Sparks. 2002. Modeling and Measuring
the Effects of Disturbance History and Climate on Carbon and
Water Budgets in Evergreen Needleleaf Forests. Agricultural and
Forest Meteorology 113(1–4): 185–222.
[8] Sarjoughian, H. S., and B. P. Zeigler. 2000. DEVS and HLA: Complementary Paradigms for Modeling and Simulation? Simulation: Transactions of the Society for Modeling and Simulation
International 17(4): 187–97.
[9] Zeigler, B. P., and H. S. Sarjoughian. 2002. Implications of M&S
Foundations for the V&V of Large Scale Complex Simulation Models, Invited Paper. Paper presented at the Verification
& Validation Foundations Workshop, Laurel, Maryland, VA.
Society for Computer Simulation, https://www.dmso.mil/
public/transition/vva/foundations, John Hopkins University,
October 2002.
[10] Reynolds, J. F., and B. Acock. 1997. Modularity and Genericness in
Plant and Ecosystem Models. Ecological Modelling 94(1): 7–16.
[11] Filippi, J. B., and P. Bisgambiglia. 2004. Jdevs: An Implementation
of a DEVS Based Formal Framework for Environmental Modelling. Environmental Modelling & Software 19(3): 261–74.
[12] Valcke, S., E. Guilyardi, and C. Larsson. 2006. Prism and Enes:
A European Approach to Earth System Modelling. Concurrency
And Computation: Practice And Experience 18(2): 231–45.
[13] Liu, J., C. Peng, Q. Dang, M. Apps, and H. Jiang. 2002. A Component Object Model Strategy for Reusing Ecosystem Models.
Computers and Electronics in Agriculture 35: 17–33.
[14] Hillyer, C., J. Bolte, F. van Evert, and A. Lamaker. 2003. The Modcom Modular Simulation System. European Journal of Agronomy 18(3–4): 333–43.
[15] Pullar, D. 2004. Simumap: A Computational System for Spatial
Modelling. Environmental Modelling & Software 19(3): 235–43.
[16] HLA. 2000. HLA Framework and Rules. IEEE 1516-2000, IEEE.
[17] Nagel, J. 2003. TreeGrOSS: Tree Growth Open Source Software – a Tree Growth Model Component. Programmdokumentation, Niedersächsischen Forstlichen Versuchsanstalt, Abteilung
Waldwachstum.
[18] Liski, J., T. Palosuo, M. Peltoniemi, and R. Sievanen. 2005. Carbon and Decomposition Model Yasso for Forest Soils. Ecological
Modelling 189(1–2): 168–82.
[19] ACIMS. 2005. DEVSJAVA Modeling & Simulation Tool.
Arizona Center for Integrative Modelling and Simulation,

4http://www.acims.arizona.edu/SOFTWARE/software.shtml#
DEVSJAVA5. (7 August 2007).
[20] Nutaro, J. J. 2005. Adevs (a Discrete Event System Simulator) C++
Library. 4http://www.ece.arizona.edu/
nutaro/index.php5. (7
August 2007).
[21] Nutaro, J. J. 2003. Parallel Discrete Event Simulation with Application to Continuous Systems. Ph.D. dissertation, Electrical and
Computer Engineering Dept, University of Arizona, 2003.
[22] Lake, T. W., B. P. Zeigler, H. S. Sarjoughian, and J. J. Nutaro.
2000. DEVS Simulation and HLA Lookahead. Paper presented
at the Simulation Interoperability Workshop, Orlando, FL IEEE,
Spring 2000.
[23] IEEE. 2003. HLA Federation Development and Execution Process,
Version IEEE 1516.3. IEEE.
[24] Kim, T. G., S. M. Cho, and W. B. Lee. 2001. DEVS Framework for
Systems Development, Unified Specification for Logical Analysis, Performance Evaluation, and Implementation. In Discrete
Event Modeling and Simulation Technologies: A Tapestry of
Systems and Ai-Based Theories and Methodologies. H. S. Sarjoughian and F. E. Cellier (eds), New York: Springer1 131–166.
[25] Davis, P. K., and R. H. Anderson. 2004. Improving the Composability of Department of Defense Models and Simulations. Santa
Monica, CA: RAND.
[26] Kim, Y. J., J. H. Kim, and T. G. Kim. 2003. Heterogeneous Simulation Framework Using DEVS Bus. Simulation: Transactions of
the Society for Modeling and Simulation International 79: 3–18.
[27] Dahmann, J., M. Salisbury, C. Turrel, P. Barry, and P. Blemberg.
1999. HLA and Beyond: Interoperability Challenges. Paper presented at the Simulation Interoperability Workshop, Orlando, FL
IEEE.
[28] Fujimoto, R. 1998. Time Management in the High-Level Architecture. Simulation: Transactions of the Society for Modeling and
Simulation International 71(6): 388–400.
[29] ACIMS. 2005. DEVS/HLA Software. Arizona Center for Integrative Modeling and Simulation 4http://www.acims.arizona.edu/
SOFTWARE/software.shtml#DEVS/HLA5. (7 August 2007).
[30] Mittal, S., J. L. R. Martín, and B. P. Zeigler. 2007. DEVS-Based
Simulation Web Services for Net-Centric T&E. Paper presented
at the Summer Computer Simulation Conference SCSC’07, San
Diego 2007.
[31] Bolduc, J.-S., and H. Vangheluwe. 2002. A Modeling and Simulation Package for Classic Hierarchical DEVS. Modelling,
Simulation and Design lab, McGill University in Montreal,
Quebec, Canada., 4http://moncs.cs.mcgill.ca/MSDL/research/
projects/DEVS/PythonDEVS/PythonDEVS.pdf5. (7 August
2007).
[32] Chow, A. C. H. 1996. Parallel DEVS: A Parallel, Hierarchical, Modular Modeling Formalism and Its Distributed Simulator. Simulation: Transactions of the Society for Modeling and Simulation
International 13(2): 55–67.
[33] Cheon, S., and B. P. Zeigler. 2006. Web Service Oriented Architecture for DEVS Model Retrieval by System Entity Structure and
Segment Decomposition. Paper presented at the DEVS Integrative M&S Symposium, Huntsville, AL 2006.
[34] Kim, K. H., and W. S. Kang. 2004. A Web Services-Based Distributed Simulation Architecture for Hierarchical DEVS Models. In
Proceedings of 13th International Conference on Artificial Intelligence and Simulation, 370–379.
[35] Mittal, S., and J. L. R. Martín. 2007. DEVSML: Automating DEVS
Execution over SOA Towards Transparent Simulators Special
Session on DEVS Collaborative Execution and Systems Modeling over SOA. Paper presented at the DEVS Integrative M&S
Symposium DEVS’ 07.
[36] Raddatz, T. J., T. J. Schnitzler, E. Roeckner, W. Knorr, C. Reick,
R. Schnur, and P. Wetzel. 2005. Modelling the Carbon Cycle
Response to Anthropogenic CO2 Emissions: Uncertainties and
Constraints. Geophysical Research Abstracts 7, 04642.
[37] Papajorgji, P., H. W. Beck, and J. L. Braga. 2004. An Architecture for Developing Service-Oriented and Component-

Volume 83, Number 6 SIMULATION

489

Wutzler and Sarjoughian

Based Environmental Models. Ecological Modelling 179(1): 61–
76.
[38] Roxburgh, S. H., and I. D. Davies. 2006. Coins: An Integrative
Modelling Shell for Carbon Accounting and General Ecological
Analysis. Environmental Modelling & Software 21(3): 359–74.
[39] Lombardi, S., G. A. Wainer, and B. P. Zeigler. 2006. An Experiment
on Interoperability of DEVS Implementations. Paper presented at
the SIW 2006.
[40] Kofman, E. 2004. Discrete Event Simulation of Hybrid Systems.
Siam Journal on Scientific Computing 25(5): 1771–97.
[41] Both, M. 2006. Vborb – an Visual Basic Object Request Broker.
4http://www.martin-both.de/vborb.html5. (7 August 2007).
[42] Zeigler, B. P., D. Kim, and S. J. Buckley. 1999. Distributed Supply
Chain Simulation in a DEVS/Corba Execution Environment. In
Proceedings of the 1999 Winter Simulation Conference 1999.
[43] Kim, T. G., S. M. Cho, and W. B. Lee. 1997. A Real-Time Discrete
Event System Specification Formalism for Seamless Real-Time
Software Development. Discrete Event Dynamic Systems 7: 355–
75.
[44] Kim, K. H., and W. S. Kang. 2004. Corba-Based, Multi-Threaded
Distributed Simulation of Hierarchical DEVS Models: Transforming Model Structure into a Non-Hierarchical One. Computational Science and Its Applications, ICCSA Pt 4, 167–76.
[45] Hu, X., X. Hu, B. P. Zeigler, and S. Mittal. 2005. Variable Structure
in DEVS Component-Based Modeling and Simulation. Simulation: Transactions of the Society for Modeling and Simulation
International 81(2): 91–102.
[46] Park, S., C. A. Hunt, and B. P. Zeigler. 2006. Cost-Based
Partitioning for Distributed and Parallel Simulation of Decom-

490 SIMULATION

Volume 83, Number 6

posable Multi-Scale Constructive Models. Simulation: Transactions of the Society for Modeling and Simulation International
82(12): 809–26.
[47] Wainer, G., B. Zeigler, H. Sarjoughian, and J. Nutaro. 2004. DEVS
Standardization Study Group Terms of Reference. Simulation Interoperability Standards Organization, 2004.
[48] Wutzler, T. 2004. To Build a Spatial Model of Forest Growth Using Stand-Based Forest Inventory. In Proceedings of International Conference on Modeling Forest Production – Scientific
Tools, Data Needs and Sources, Validation and Application, H.
Hasenauer and A. Mäkelä (eds). Vienna, Austria: Department
of Forest- and Soil Sciences BOKU University of Natural Resources and Applied Life Sciences, pg 503.

Thomas Wutzler is a PhD student at Max-Planck Institute for
Biogeochemistry, Jena, Germany and was a visiting scholar in
2005 at the Arizona Center for Integrative Modeling and Simulation (ACIMS), Tempe, USA.
Hessam Sarjoughian is Assistant Professor of Computer Science and Engineering at Arizona State University, Tempe and
Co-Director of the Arizona Center for Integrative Modeling
and Simulation. His research includes modeling and simulation methodologies, model composability, network co-design
and agent-based simulation.

Standardizing DEVS Models: An Endogenous Standpoint
Hessam S. Sarjoughian

Yu Chen

Arizona Center for Integrative Modeling & Simulation
Computer Science and Engineering Faculty
Arizona State University, Tempe, Arizona, USA
sarjoughian@asu.edu

Indigo Digital Press R&D
Hewlett Packard
Boise, ID 83714, USA
yu.chen8@hp.com

Keywords: DEVS, Domain-Specific Modeling,
MIPS32 Processors, Standardization, XML Schema

Research has focused on DEVS to DEVS
interoperability where simulation models are
implemented in different programming languages using
alternative simulator designs. Methods and tools have
also been developed for DEVS and non-DEVS
simulations to interoperate with one another. They use
XML and middleware to bridge the gap among
different implementations of DEVS and non-DEVS
models and/or simulators.
In this paper we are interested in articulating what
a standard DEVS model can (or should) be given the
formal atomic and coupled parallel DEVS models. It is
important to consider DEVS simulation interoperability
as a separate, complementary standard. The standard is
to support developing DEVS-compliant modeling and
simulation tools. It should lend itself for deciding if a
modeling tool sanctions development of DEVS models
and if a simulator is able to support correct execution of
arbitrary DEVS models. The standard should conform
to the DEVS formalism’s abstract syntax and
semantics, and can be extended for different
application-domains. The generic DEVS standard
model needs to serve as a basis for specialized standard
models, which can support developing and using
domain-specific model libraries.
Starting with Section 2, key M&S concepts
important to standardization of the DEVS formalisms
are reviewed. In Section 3, basic attributes of DEVSbased modeling and simulation tools are examined with
an eye toward their standardization. A taxonomy
identifying the basic constituents for standardizing
DEVS simulation model is devised. In Section 4, an
XML Schema for generic, abstract parallel DEVS
models is developed and offered as a starting point
toward developing a standard DEVS model. Also an
XML Schema is developed for MIPS32 Processors. In
Section 5, related work is briefly discussed. Concluding
remarks on DEVS model standardization is provided in
Section 6.

Abstract
Standardization is important in bringing order and rigor
to model development. Our attempt in this article is to
articulate basics on model standardization from an
endogenous perspective. The basis for a domain-neutral
standard model for parallel DEVS formalism is
described. An XML Schema model representing
structures of parallel DEVS models is developed.
Standardizing domain-specific application models using
the proposed standard DEVS model is exemplified
using MIPS32 processors.

1

Introduction

Growth of system complexity and scale is a well
recognized challenge to the modeling and simulation
(M&S) research and practices. Standards are
recognized as a necessity for rigorous development of
models and simulation experimentations and thus
building robust simulation modeling tools. Since the
middle of the 1980s, standards for modeling and
simulation have been of interest. The latest are High
Level Architecture (HLA) Framework and Rules [12],
the HLA Federate Interface Specification [11], Object
Model Template [13], and the Federation Development
an Execution Process (FEDEP) [10]. Standards also
exist for specialized application domains. Domainspecific modeling is defined to represent structural
and/or behavioral attributes for a domain where there
are accepted artifacts among a community of users and
experts (e.g., VHDL [8]). At times, a tool and its
models are mandated for a user community (e.g.,
OPNET [20]). For other application domains there exist
no mandates or standards. For instance there are no
standards for agent simulation models, although there
exist standards for software agents [19].
The first round of discussions on DEVS [23]
standardization began around 1997, when HLA
standards were under development. A collection of
works in the past decade shows what has been achieved
toward DEVS simulation interoperability [22].

2

Background

A model captures a representation of a system that can
generate some of its observable behaviors. The DEVS

266

formalism provides an abstract model specification
(syntax) and abstract simulator protocol (semantics). A
DEVS simulator can be considered a software
realization of the execution protocol that can correctly
execute the instructions contained in a DEVS model. A
modeling tool, on the other hand, can support
specifying models in accordance to the DEVS model
specification. DEVS-Suite simulator and the
Component-based System Modeler (CoSMo) are
examples of a DEVS simulator and modeling tool,
respectively. They were developed using DEVS theory
and software modeling methods (see Figure 1). Both
the modeler and simulator provide abstract templates
for creating DEVS models. The templates for the
simulator are in JavaTM language, whereas templates for
the modeler are represented as entities and relations.
Knowledge specific to a particular system must be
incorporated to the DEVS models and usually extended
with modeling constructs that are not prescribed in the
DEVS formalism. The key role of the system in
developing concrete models and simulating them is
illustrated in Figure 1. The system provides domain
knowledge which can be standardized as in Electrical
systems.

translators, although different in some parts (e.g., due to
different programming syntax or handling of concurrent
events), should be compliant to the standard DEVS
model.
Figure 2 shows the three phases of the M&S
activities. In the first phase, model abstractions are
conceptualized, formalized, and developed. UML
models (e.g., class diagrams) are sometimes used when
representing complex relationships (e.g., inheritance)
among model components. In the second phase, models
are implemented in specific programming languages
such as C++, Java or macro languages (e.g., SFunctions in Simulink). The simulation model
implementations are executed and their dynamics can
be rendered in textual or other human readable modes.
In the third phase, results from simulation executions
are generally collected as tabular or plots and then
analyzed with the purpose of model validation and
simulation verification. Standardizing DEVS model
representation can help model design and simulation
development.

e2
DEVS-Suite

e1

e3   ext

Descriptive, mathematical, UML, 
and pseudo code are used for 
models.
<<creates>>

Observ atory ControlSy stem

s

AbstractDetectorController_Observ er

1

(from Mount)

1
update(subject_in : AbstractDetectorController_Subject) : v oid
AbstractDetectorControllerNode

-observ ers

(from Detector)

n
<<uses>>

1
1..n
AbstractDetectorController_Subject

<<realize>>

(from Detector)

Model Design

s

observ ers : Array

System

attach(observ er : AbstractDetectorController_Observ er) : v oid
dettach(observ er : AbstractDetectorController_Observ er) : v oid
notif y () : v oid

Simulator

Simulator

AbstractDetectorController
(from Detector)

AbstractMountController
-subject

<<subscribe>>

subjectState : String
1

(from Mount)

observ erState : String

public void deltext(double e, msg x){
Continue(e);
if (phaseIs("passive"))
for (int i=0; i< x.getLength();i++)
if (messageOnPort(x,"in",i)){
job = x.getValOnPort("in",i);
holdIn("busy",processing_time);}
}

Model
Software Specifications

CoSMoS

(realization)

Figure 1: Simulation and modeling tools

Experimentation 

Standards including software modeling languages
(e.g., UML), programming languages (e.g., Java) and
grid computing (e.g., Service-Oriented Architecture)
are used in developing simulation and modeling tools
[6]. These, however, are not a substitute for a standard
DEVS model. With a standard DEVS model
representation, models developed in a modeling tool
should be correctly executed (translatable without loss
of information) in any simulator that complies with the
DEVS formalism. Customized translators are needed to
allow a model developed for a target simulator to be
simulated by another simulator. Models generated by

Simulation Development

getState() : String

Figure 2: Phases in M&S lifecycle

267

3

Table 1: Elements in Defining Standardization

DEVS and Standardization

To exemplify things that influence standardization of
DEVS models, we examine the DEVS-Suite simulator
and the CoSMo modeler. The DEVS-Suite simulator is
built using the DEVS formalism, object-orientation,
software architecture and design, as well as methods for
automated selection of component behavior and their
visual and tabular representations. This simulator is
developed using standardized software modeling
notations (e.g., UML) and software libraries (e.g., Java
SDK). Users must incorporate concrete domain
knowledge for a system that is to be modeled and
simulated. Domain-specific models may be built and
provided as “re-usable” components. For instance,
MIPS32 processor models [4] use abstract atomic and
coupled DEVS model templates. Generic model types
such as Cellular Automata may also be built. Given a
standard for a class of applications such as servicebased software systems, specialized simulators can also
be developed, such as SOAD (SOA-compliant DEVS)
[22]. It should be noted that simulators such as SOAD
have different characteristics as compared, for example,
with MIPS32 simulators. The SOA standard can be
applied to different application domains whereas
MIPS32 processor simulation supports the model
structures and behaviors that have a de facto standard.
In the development of the CoSMo modeler, Entity
Relation (ER), Object-Orientation (OO), System Entity
Structure (SES), Cellular Automata (CA), XML
Schema, and Database (DB) are used (see Figure 3).
CoSMoS is a visual, persistent component-based
modeler with unique features such as guaranteeing
unique name spaces for a family of simulation models
that can be developed visually and stored in relational
databases. Hierarchical component-based visual
notations are defined for DEVS, XML, and CA models.
These notations, however, are not standardized.
As suggested in Figure 3, a standard DEVS model
needs to account not only for individual simulator and
modeler tools, but also their integration (e.g., CoSMoS
which combines DEVS-Suite and CoSMo). It is
important for the standard to aid standardizing different
application domains. Standards for distinct application
domains should be based on the standard DEVS model,
a domain-neutral DEVS standardization. Furthermore, a
standard DEVS model should be assessed given other
kinds of domain-neutral models such as Cellular
Automata.
To determine what a standard DEVS model should
be, we developed the taxonomy shown in Table 1.
Given a single DEVS formalism (i.e., Parallel DEVS), a
general-purpose, textual standard DEVS model is
proposed. The standard should lend itself for both
centralized and distributed execution schemes.

Modeling Formalisms
None
Single

Multiple

Modeling Types
Generalpurpose1

Application
domainspecific

Standardcompliant

No modeling formalism is used.
Models are defined using a single
modeling
formalism. Example:
DEVS.
Models are defined using multiple
modeling formalisms. Example:
cellular
automata,
differential
equations, difference equations, and
linear programs.
Models are void of any specific
domain
knowledge;
general
characteristic of systems are
assumed. Example: discrete-event
processing.
Models
contain
knowledge
specialized to specific application
domains; accepted data, functions,
structures, and abstractions are
assumed.
Example:
computer
networks.
Models
contain
generalized
structures and behavior compliant
with some software or system
standard; Example: Service-Oriented
Computing.

Model Development Languages2
Textual

Visual

Models are specified using textual
notation; programming languages
and mark-up languages may be used.
Example: Java.
Models are specified using some
visual notation; different visual
notations may be used for different
modeling formalisms. Example: CD
Modeler [3].

Simulation Execution Schemes
Centralized

Decentralized

1

Simulation execution is on a single
hardware; it can be sequential or
parallel. Example: adevs [1].
Simulation execution is performed
on a distributed hardware platform; it
is assumed to be distributed.
Example: DEVS/SOA [15].

Models may represent some idea that is purely abstract
in that it accounts for no specialized knowledge. Example: a
memory unit that loses data entered into it after some period
of time. Although it may have some specific meaning to a
person, the model itself is void of any domain knowledge.
2
Simulation models are constructed from simulatable
and non-simulatable parts [5]. The operations of a simulatable
model (e.g., a processor) are carried out according to the
DEVS modeling formalism. A non-simulatable model is
treated as data – for example, the individual operations of a
list are not treated in terms of DEVS external, internal, and
output functions and thus lack direct timing specification.

268

With the above considerations, we recommend
developing a standard for the pure DEVS formalism
(i.e., excluding its variants and inclusion of other
modeling formalisms). Other desired elements of
standardization such as support for visual modeling can
be undertaken once a standard for DEVS modeling
formalism is provided. Thus, we propose the following
definition for the standard DEVS model:

4

To standardize DEVS model representations, both their
structures and behaviors must be accounted for.
Structures refer to input and output data for atomic and
coupled models as well as components of coupled
models. Behaviors refer to the generic semantics for the
internal transition, external transition, confluent, output,
and time advance functions as well as their
relationships to one another. Currently, there does not
exist a suitable language for standardizing concrete
details of arbitrary atomic model functions. Existing
work on standardizing model behaviors are either
implementation language dependent [16] or not
applicable to constructing non-trivial models [15].
Constrained by the capabilities of existing languages,
we propose standardizing DEVS model structures with
a particular language, and leaving the behavior
specification of DEVS models to application domain
standards.
To develop a standard DEVS model from an
endogenous standpoint, we used the following criteria:
1) it should be implementation language and computing
platform independent, 2) it should be able to express
pure DEVS formalism, 3) it should support validation
of pure DEVS models, 4) it should be extensible for
domain-specific DEVS models, 5) it should help with
building modeling and simulator tools, and 6) it should
support model transformation and thus interoperability
(exogenous perspective). On selecting a language (e.g.,
C++, XML, and UML) to represent DEVS models,
using criteria 1, we excluded all programming
languages, such as C#, C++, and Java. Both XML and
UML meet criteria 2. For model validation, XML
schema can be used to define syntax and constraints
(such as the mapping IDREF values to ID values) for
XML document instances and help model validation,
UML has to rely on other languages (such as OCL) to
do similar things. Both XML and UML can be extended
to express domain-specific models, and Eclipse
Modeling Framework project supports both in terms of
tool building and model transformation. We chose
XML over UML as the language to represent DEVS
models due to its validation capability, its wide use as
the data exchange language, and its designation as a
substantially simpler language.

DEVS model standardization is a set of domainneutral and implementation-independent artifacts
that concretize a DEVS formalism syntax and
semantics. The standard’s artifacts are specified in
a standardized computer language that can
automatically check a model’s syntax and
semantics.

Semiconductor MFG

Agents

MIPS32

SOC

…
Computer network protocol

connectivity
patterns

DEVS‐Suite

DEVS     

ER

CA

SES

translator

DB

C
o
S
M
o
S

A Proposed Standard DEVS Model

. . . 

OO

CoSMo
CA: Cellular Automata
SOC: Service Oriented Computing

Figure 3: Standardization of DEVS model in
simulators and CoSMo
It is important to note that the correctness of a
model specification using the standard DEVS model
does not account for domain-specific knowledge.
Separate standards are needed for domain-specific
application models and these standards represent some
de facto or standardized domain-specific nomenclature,
data, and operations.

4.1

MIPS32 Processors Simulation Models

Three MIPS32 processor simulation models – singlecycle, multi-cycle, and pipeline – are developed to help
students learn and instructors teach processor designs at
RT (Register-Transfer) level [4]. These processors were
modeled according to their physical and functional
structures via a bottom-up approach. First, every
Register-Transfer (RT) level component (e.g., Register)
inside a processor is modeled as an atomic model.

269

Additional atomic models are used to model special
situations, such as when a bus is split into several buses
or several buses are combined into one. Then, some
models are grouped into coupled models according to
their functionality to provide higher-level abstractions.
Finally, the three processor models are developed.
Figure 4 shows MCPEF (Multi-Cycle Processor with an
Experimental Frame) model.

for extensibility. Other children of Atomic element are
StateVariables, Ports, Inputs, Outputs, Init, DeltExt,
DeltInt, DeltCon, Lambda, and TA. StateVariables has
one or more StateVariable, which has two attributes,
Name and Type. Ports has zero or more Port, which
has two attributes, Name and Type, where Type is
either “In” or “Out”. Inputs has zero or more Input
elements, and an Input element has a Port attribute
indicating the port name and a DataType attribute
indicating the data type coming into the port. Outputs
has zero or more Output, which has a Port attribute
indicating the port name and a DataType attribute
indicating the data type going out of the Port. Init
element contains model state initialization, which is
more related to implementation so its content is not
defined.

Figure 4: Multi-Cycle processor model

4.2

Standardizing Domain-Neutral
Representations

Model

We propose using a set of XML schemas to specify
general DEVS model structures. The set of schemas
include DevsCore.xsd and DevsTypes.xsd, where
DevsCore.xsd imports DevsTypes.xsd.

Figure 5: DEVS-Models element
DevsCore.xsd defines the top-level element of a
DEVS model specification to be a DEVS-Models
element as shown in Figure 5, which contains zero or
more Include elements and multiple Atomic or Coupled
elements. The Include element allows the inclusion of
model specifications from another model specification
file referred to by an URL, as normally it is impossible
to put all the model specifications into one file. Also
constraints on the relationship between atomic and
coupled models are defined (shown as dotted lines in
Figure 5).
Figure 6 depicts the contents of Atomic element. It
has optional Types children elements, which allow
model specific types to be defined using XML schema

Figure 6: Atomic element
DeltExt, DeltInt, DeltCon, Lambda, and TA
represent external transition, internal transition,
confluent, output, and time advance function,
respectively. All these elements are optional, except TA.
The contents of DeltExt are shown in Figure 7. It has a
Parameters child, which in turn has three children
elements: StateParameter, ElapsedTimeParameter, and
InputsParameter. Each of the three elements has two
attributes, Name and Type, where Name specifies the
parameter name and Type defines data type. It also has
a Body child, which is undefined because it is related to
implementation. DeltCon has the same structure as

270

DeltExt. DeltInt, Lambda, and TA elements all have the
same structure as shown in Figure 7. They all have a
Parameters child, which in turn has a StateParameter
child, and a Body child, which is undefined.
The contents of Coupled element are depicted in
Figure 8. It has six children elements: Types, Ports,
Inputs, Outputs, Components, and Couplings. The
definition of Types, Ports, Inputs, and Outputs are the
same as those of Atomic element. Components element
contains one or more AtomicComponent or
CoupledComponent elements as shown in Figure 9.
AtomicComponent and CoupledComponent have the
same structure. They both have three attributes, Name,
ID, and ModelType. ID is used to uniquely identify a
model within a model specification and is optional.
ModelType connects a model instance with its model
specification. The constraints in Figure 5 define that an
atomic/coupled model instance can only have
ModelType
value
mapped
to
an
existing
Atomic/Coupled model definition.
Element Couplings has one or more Coupling
elements, and each Coupling element has four
attributes: ComponentForm, PortFrom, ComponentTo,
and PortTo.
DevsTypes.xsd defines some simple and complex
types used by DEVS models, including MessageType
as shown in Figure 10. InputParameters’ Type attribute
should have a value referring to MessageType.
DevsTypes.xsd also defines PortType as an
enumeration type with two values, “In” and “Out”.

in Figure 11. The structures of domain specific models
can be defined using the two schemas mentioned above,
as well as domain specific IO schema, such as
MIPSTypes.xsd. We propose standardizing domain
specific atomic model behaviors with some
specifications written in natural languages. For instance
in MIPS32 processor domain, a specification can be
written in English that details the behaviors of common
atomic models, including and gate, ALU, clock, etc.
Then Simulation tools can implement those models
according to the specification, and modelers can use
those atomic models as basic building blocks to build
coupled models. The specification of a coupled model,
ExperimentalFrame, is shown in Figure 12, and the
structure specification of an atomic model, Clock, is
shown in Figure 13.

Figure 8: Coupled element

Figure 7: DeltExt and DeltInt elements
Figure 9: Components element

4.3

Standardizing Domain-Specific
Representations

Model

To illustrate domain-specific modeling we will use
MIPS32 processors described in Section 4.1 as an
example. We propose defining domain specific IO data
types using XML schema to allow interoperability. For
MIPS32 processors, MIPSTypes.xsd was created to
define IO types common to MIPS domain, including
Opcode, Functcode, and BinarySignal_1bit through
BinarySignal_32bits. Most of the MIPS types are string
based simple types. The definition of Opcode is shown

Figure 10: MessageType element

271

models. In this work, domain-neutral standardization in
combination with domain-specific standardization is
proposed while emphasizing the importance of
endogenous perspective.

<xs:simpleType name="Opcode">
<xs:restriction base="xs:string">
<xs:pattern value="([01]){6}"/>
</xs:restriction>
</xs:simpleType>

Figure 11: Opcode SimpleType

<DEVS-Models ... " >
<Atomic Name="Clock">
<Types>
<xs:schema
xmlns:xs="http://www.w3.org/2001/XMLSchema"
targetNamespace="http://www.devs_ms.org/MIPS_Type
s/Clock">
<xs:simpleType name="phaseEnumeration">
<xs:restriction base="xs:string">
...
</xs:restriction>
</xs:simpleType>
</xs:schema>
</Types>
<StateVariables>
<StateVariable Name="phase"
Type="clock:phaseEnumeration"/>
<StateVariable Name="cycleTime"
Type="xs:double"/>
...
</StateVariables>
<Ports>
<Port Type="In" Name="Stop"/>
<Port Type="Out" Name="Out"/>
</Ports>
<Inputs>
<Input Port="Stop"
DataType="mips:ClockStopSignal"/>
</Inputs>
<Outputs>
<Output Port="Out"
DataType="mips:BinarySignal_1Bit"/>
</Outputs>
<Init></Init>
<DeltExt>
<Parameters>
<StateParameter Name="states"
Type="StateValueMap"/>
<ElapsedTimeParameter Name="e"
Type="xs:double"/>
<InputsParameter Name="inputs"
Type="Message"/>
</Parameters>
<Body> ...
</Body>
</DeltExt>
<DeltInt>
<Parameters>
<StateParameter Name="states"
Type="StateValueMap"/>
</Parameters>
<Body>...
</Body>
</DeltInt>
<TA>...</TA>
<Lambda>...</Lambda>
</Atomic>
</DEVS-Models>

<DEVS-Models
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xmlns:xs="http://www.w3.org/2001/XMLSchema"
xmlns:mips="http://www.devs_ms.org/MIPS_Schema_Typ
es"
xmlns:clock="http://www.devs_ms.org/MIPS_Models/Clock
"
xmlns:trans="http://www.devs_ms.org/MIPS_Models/Trans
ducer" xmlns="http://www.devs_ms.org/DEVS_Schema" >
<Coupled Name="ExperimentalFrame">
<Ports>
<Port Name="Instr" Type="In"/>
...
</Ports>
<Inputs>
<Input Port="Instr"
DataType="mips:BinarySignal_32Bits"/>
...
</Inputs>
<Outputs>
<Output Port="C"
DataType="mips:BinarySignal_1Bit"/>
...
</Outputs>
<Components>
<AtomicComponent Name="clock"
ModelType="Clock"/>
<AtomicComponent Name="trans"
ModelType="Transducer"/>
</Components>
<Couplings>
<Coupling ComponentFrom="clock" PortFrom="Out"
ComponentTo="trans" PortTo="C"/>
...
</Couplings>
</Coupled>
</DEVS-Models>

Figure 12: Coupled ExperimentalFrame model instance

5

Related Work

Earlier efforts have been undertaken to standardize
DEVS modeling and simulation [22]. Some of these
works utilize the neutrality of the XML for model
reusability and suggest the models to be standardized.
For example, DEVSML [16] uses XML to define
model structures and JavaML [2] to specify model
behaviors. Although DEVSML can express both model
structures and behaviors, its behavior language JavaML
is tied to JavaTM and specialized syntax. In another
work [15], XLSC (XML-based Language for
Simulation) is proposed. It provides XML schemas for
atomic and coupled models. Structure and behavior
specifications are supported for relatively simple atomic

Figure 13: Atomic Clock model instance

272

6

[3] CD Builder, (2010), sourceforge.net/projects/
cdppbuilder/
[4] Chen, Y., H. Sarjoughian, (2010), “A Componentbased Simulator for MIPS32 Processors”,
Simulation Transactions, 86(5-6), 271–290.
[5] CoSMo, (2009), http://sourceforge.net/projects/
cosmosim/
[6] DEVS, (2011), http://en.wikipedia.org/wiki/DEVS.
[7] DEVS-Suite,
(2009),
http://devssuitesim.sourceforge.net.
[8] IEEE Std. 1076-1987, (1987), IEEE Standard
VHDL Language Reference Manual, IEEE.
[9] IEEE Std. 1278, (1995), IEEE standard for
modeling and simulation. Distributed Interactive
Simulation, IEEE.
[10] IEEE (2003), HLA Federation Development and
Execution Process, Version IEEE 1516.3-2003,
IEEE.
[11] IEEE
(2010),
HLA
Federate
Interface
Specification, Version IEEE 1516.1-2010, IEEE.
[12] IEEE (2010), HLA Framework and Rules, Version
IEEE 1516-2010, IEEE.
[13] IEEE (2010), HLA Object Model Template,
Version IEEE 1516.2-2010, IEEE.
[14] Langenberg, T., (2005), Standardization and
Expectations. Springer-Verlag.
[15] Meseth N., P.Kirchhof, T. Witte (2009). “XMLbased DEVS Modeling and Interpretation,” Spring
Simulation Multiconference, 152–159, San Diego,
CA, USA.
[16] Mittal, S., J. Risco-Martín, B. Zeigler. “DEVSML:
Automating DEVS Execution Over SOA Towards
Transparent Simulators,” Spring Simulation Multiconference, 287–95, Norfolk, VA, USA.
[17] Modelica, (2009), “A Unified Object-Oriented
Language for Physical Systems Modeling
Language
Specification”,
Version
3.1,
http://www.Modelica.org.
[18] Murphy, C., J. Yates, (2008), The International
Organization for Standardization (ISO): Global
Governance Through Voluntary Consensus.
[19] O'Brien, P., R. Nicol, (2010), Towards a Standard
for Software Agents, Springer.
[20] OPNET, (2005), http://www.opnet.com.
[21] Sarjoughian, H, S. Kim, M. Ramaswamy, S. Yau,
(2008), “An SOA-DEVS Modeling Framework for
Service-Oriented Software System Simulation”,
Proceedings of the Winter Simulation Conference,
845–853, December, Miami, FL, USA.
[22] Wainer, G., P. Mosterman, (2011), Discrete-Event
Modeling
and
Simulation:
Theory
and
Applications, Taylor and Francis.
[23] Zeigler, B., H. Praehofer, T. Kim (2000). Theory of
Modeling and Simulation: Integrating Discrete
Event and Continuous Complex Dynamic Systems,
Academic Press.

Discussion

The goal of this work is on standardizing DEVS from
within - an endogenous focus on standardization. In
contrast, exogenous standardization emphasizes
interoperability among DEVS variants as well as nonDEVS models. A standard DEVS model in the spirit
proposed in this paper has its advantages as is the case
for an interoperability standard for simulators. The
combination of these standards can make developing a
complete DEVS standardization practical. The standard
DEVS model idea with its proposed XML Schema is an
attempt to define standardized DEVS model structures,
including IO data, without being restricted to any
specific implementation language. The XML schemas
are more concrete as compared with the DEVS
theoretical specifications, and they can be used as
domain-neutral
standardized
template
models.
Although XML Schema is less expressive as compared
with UML in terms of OO concepts, this is
advantageous from the standardization perspective.
UML and Software Architecture Description
Languages are more appropriate for design and
development of modeling or simulation tools.
The proposed DEVS XML Schemas is offered as a
starting point on helping with further development of
existing DEVS modeling and simulation tools and may
be used to determine which DEVS tool is compliant
with the standard DEVS model. The domain-neutral
standardization is predicated on the importance of
domain-specific model standardization. That is, the
generic DEVS XML Schema can be extended to
express IO data for a specific application domain (as in
MIPS32
Processor).
Standardized
behavioral
specifications are left to specific application domains.
Although domain specific standardization can account
for much of the behaviors of atomic models, modelers
still need to specify behaviors that need special
customization, such as experimental framework. To
meet this need, in the future we plan to investigate the
techniques such as State Chart XML schema to support
standardization of model behaviors. The future work
also includes building tools to assist model
development and transformation. An encompassing
objective is to marshal a community-wide effort to
develop a standard DEVS model. This is no small feat!
Related future efforts include standardization of DEVS
variants and consistent, complementary endogenous
and exogenous DEVS standardizations.

References
[1] adevs, (2006), http://sourceforge.net/projects/
adevs.
[2] Badros, G., (2000), “JavaML: A Markup Language
for Java Source Code,” Computer Networks 33(1),
159–77.

273

Simulation

Simulating adaptive service-oriented
software systems

Simulation: Transactions of the Society of
Modeling and Simulation International
87(11) 915–931
Ó The Author(s) 2010
Reprints and permissions:
sagepub.co.uk/journalsPermissions.nav
DOI: 10.1177/0037549710382431
sim.sagepub.com

Mohammed A Muqsith1, Hessam S Sarjoughian1,3,
Dazhi Huang2 and Stephen S Yau2

Abstract
Simulation of dynamic service-based software systems is important for studying services that may change their composition and thus interactions at run-time. An approach based on Service Oriented Architecture-compliant DEVS (SOAD)
and Dynamic Structure DEVS (DSDEVS) modeling approaches is developed to support structural changes in service
model composition. To achieve this goal, a broker–executive model is devised based on the broker model defined for
SOAD and the executive model defined for DSDEVS. The capability to simulate dynamic services is incorporated to the
DEVS-Suite simulator. To demonstrate modeling of dynamic service-based software systems, a real voice communication
system and a model of this system have been developed. The importance of enabling simulation-based design for
adaptable systems is briefly discussed.

Keywords
adaptive service-based software systems, DEVS-Suite simulator, Dynamic Structure DEVS, SOA-compliant DEVS models

1. Introduction
1,2

Service Oriented Architecture (SOA) is an attractive
approach for developing enterprise scale distributed
software systems. It emphasizes loosely coupled, protocol independent distributed system development with
the ‘software as service’ concept - a self-contained component provided as a publishable contract for use by
independent clients. SOA has evolved to address the
demand to develop and deploy large-scale software systems that are cost-eﬀective to reuse and maintain and
easily adaptable to infrastructure change. A key promise of the SOA is supporting on-demand Qualityof-Service (QoS) for given business logics.
Maintaining QoS, however, is a challenging task as it
depends on the system architecture. Speciﬁc design
decisions (e.g., ﬂat versus hierarchial service composition) may signiﬁcantly impact the run-time QoS, even
though the decisions taken during the design phase are
consistent with SOA. Hence, evaluation of the system
design prior to development is necessary.
Simulation is widely accepted as an eﬀective method
for developing system design. It can be used to determine the run-time behavior of Service-based Software
(SBS) systems (e.g., Bause,3 Sarjoughian et al.,4 and

Tsai et al.5). To simulate SBS systems that are capable
of simultaneously satisfying multiple QoS attributes, it
is desirable to develop models that are not only
grounded in a formal modeling formalism, but also
are based on SOA concepts and principles.
Simulation can be used for studying the time-based
dynamics of systems including QoS tradeoﬀs. In particular, there is a need to model and simulate Adaptive
SBS (ASBS)6 where its dynamics can be observed and
controlled by a monitoring system and an adaptation
system, respectively. The users can select services and
list desired behaviors under the presence of some
1
Arizona Center for Integrative Modeling and Simulation, Department of
Computer Science and Engineering, Arizona State University, USA.
2
Department of Computer Science and Engineering, Arizona State
University, USA.
3
School of Electrical and Computer Engineering, University of Tehran,
Iran.

Corresponding author:
Hessam S. Sarjoughian, Arizona Center for Integrative Modeling and
Simulation, Department of Computer Science and Engineering, Arizona
State University, Tempe, AZ, USA
Email: sarjoughian@asu.edu

916

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

uncontrollable,
but
predictable
environmental
ﬂuctuations.
In the context of this research, we use the Discrete
Event System Speciﬁcation (DEVS) formalism7 and
DEVS-Suite8 as our underlying modeling approach and
simulator, respectively. Speciﬁcally, the SOA-compliant
DEVS (SOAD) simulation framework has been developed to support modeling of SBS systems4. However, it
lacks the basis for modeling structural change that is
required for ASBS designs. There exist alternative methods for specifying component-based models that can
change their structure and behavior at run-time.9–11 The
Dynamic Structure DEVS (DSDEVS)9 is one of the
approaches that can aﬀord structural changes that may
occur in models. The SOAD framework needs to be
extended to support simulation of SBS systems that can
have services added and removed dynamically.
Our contribution in this paper is the development of
the DSOAD (Dynamic Structure SOAD) modeling
approach by introducing dynamic structure modeling
to the hierarchical SOAD modeling framework. A realization has been developed by extending the
DEVS-Suite simulator with the DSDEVS simulation.
To demonstrate the kinds of modeling that are
needed for the design of ASBS systems, an exemplar
real voice communication system is developed and
compared with its simulated counterpart developed in
DEVS-Suite. It is important to note that the focus of
the paper is on DSOAD – a modeling approach to
dynamic aspects of the SOA. It does not account for
SOA as part of the simulation infrastructure and as
such model interactions with real web services are not
accounted for. So, all the DSOAD components (i.e. all
services – publisher, subscriber, broker) are simulated
and are not interchangeable with real web services.

2. Related work
Modeling and simulation research related to serviceoriented computing may be broadly divided into three
categories: (i) developing models that can specify simulatable web services and their interactions; (ii) distributed simulation execution using web services; and
(iii) use of simulation as an aid for software development of actual web services. We provide a view of existing work in light of these categories.
In the ﬁrst category, the aim is to represent the key
aspects of services. In Narayanan and Mcilraith,13 a
Petri-net based approach is presented. The services
are speciﬁed using the DARPA Agent Markup
Language for Services (DAML-S)14 and converted to
Petri-net models and simulated using the KarmaSim
environment.15 While Petri-net provides a strong basis
for veriﬁcation and validation of the models, the mapping DAML-S to Petri-nets is ad hoc. In particular, the

combination of DAML-S and Petri-net lacks a sound
basis for describing time-based dynamics of SBS systems. In Bause,3 the concept of process chain modeling
is used to abstract web services as workﬂows. The
approach supports modeling business processes as
sequences of functional units that together represent
web services. The approach treats services at the high
level of processes rather than explicitly representing
basic traits of services and SOA. It does not directly
address the concept of publisher (provider), subscriber
(client), broker and a service in general with the properties such as discoverability and statelessness, which are
important for developing models that can be said to
comply with SOA. In Sarjoughian et al.,4 an approach
is developed by unifying the DEVS and SOA frameworks. Based on the DEVS and SOA concepts and
principles, a set of primitive and composite service
model abstractions along with their interactions
(i.e. message and service calls) are deﬁned. To simulate
network delay and throughput constraints among distributed services, a basic router model is also developed. The resulting SOAD framework supports
simulations of SBS systems. However, there is no rigorous support for run-time service composition under
dynamic scenarios (i.e. adding and removing services
during simulation execution).
In the second category, the emphasis is on the adoption and extension of service-oriented computing techniques and technologies for stronger support for distributed
and/or large-scale simulations. In Mittal et al.,16 the
DEVS approach is extended to support simulation of
models as web services. The resulting DEVS/SOA environment, unlike DEVS/HLA17 for example, allows
DEVSJAVA18 simulators to be treated as services. Grid
computing as a simulation environment is also explored.
Cosim-Grid19 is a service-oriented simulation grid based
on High Level Architecture (HLA),20 PLM (Product
Lifecycle Management),21 and grid/web services. It
applies OGSA (Open Grid Services Architecture)22 to
modeling and simulation to improve HLA in terms of
dynamic sharing, autonomy, fault tolerance, collaboration, and security mechanisms. The XMSF (Extensible
Modeling and Simulation Framework)23 has also been
developed using a set of technologies including web/
XML, web services, and internet/networking for webenabled simulation.
In the third category, researchers with an interest in
software design of web services advocate the use of
simulation to assist in the analysis, design, and testing
of SBS systems. In Tsai et al.,5 a framework has been
developed to support the design of web services.
Speciﬁcation of real services including workﬂows can
be described using the Process Speciﬁcation and
Modeling Language (PSML). Given pre-built code for
un-timed services with pre-deﬁned composition

Muqsith et al.
policies, workﬂow speciﬁcations can be automatically
generated. The DDSOS environment,24 which employs
HLA/run-time infrastructure,20 is used to test the
dynamics of web services as simulated components.
ISTF (Interface Simulation and Testing Framework)25
is another related eﬀort. It is targeted at simulating endto-end distributed application scenarios and showing
how individual components will interface with each
other in production.

3. Background
The research problem we have addressed in our work
involves modeling and simulation of SBS systems that
undergo structural changes at run-time. In the following, we provide a brief description of the SOA and the
SOAD framework followed by dynamic structure modeling, with an exemplar adaptive SBS system.

A. Service Oriented Architecture
SOA represents architectural design principles toward
building enterprise level distributed computing systems
based on a set of loosely coupled computational components called ‘services’ that interact to provide functional utilities to interested clients.2 All the software
resources in SOA are termed as services. Services are
deﬁned using standard languages (e.g., Web Service
Description Language (WSDL)),26 provide publishable
interfaces, and interact with each other to collectively
execute a common task. Each service is independent of
the state and context of other services. Furthermore,
the interaction and communication is done using protocol independent message schemes (e.g., Simple Object
Access Protocol (SOAP)).27
Similar to the producer–consumer scenario, service
executioner and service requester are logically distinguished as publisher and subscriber, respectively.
Publisher is the service provider whereas subscriber is
the service consumer. The subscriber discovers an available publisher with the help of the third service known
as the broker. The broker contains the publisher information in its registry which represents the published
service interfaces of the publishers. To initiate a service
invocation, the subscriber initiates a communication
with the broker to search for service availability and
if found the broker returns the service information so
that the subscriber can directly interact with the
publisher(s). In essence, a broker is the fundamental
component in establishing the dynamic relations and
interactions between the publisher and the subscriber
and thus helps in maintaining the loosely coupled
property of the SOA. When a publisher provides a
service, it registers the service with the broker using
a service speciﬁcation (i.e. WSDL). The broker

917
adds the new service speciﬁcation to the Universal
Description Discovery and Integration (i.e. UDDI),28
an XML-based29 registry of available services.
Subscribers searching for a speciﬁc service request the
broker to check for service availability and, if found,
notify the subscriber of the service speciﬁcs using the
WSDL speciﬁcation of the publisher. The subscriber
communicates with the broker and the publisher
using messages (e.g., SOAP). Once the publisher gets
the subscriber request, the service is executed and the
resultant data service messages are provided to the
subscriber.
The concept of service re-usability by composition
is important in SOA. Adding new functionality from
existing services is addressed by orchestration that
speciﬁes how the services would interact, including
the execution order so that new functionality can be
composed from existing services including the way
services interact at the message level as well as the
execution order of the interactions.2 The ordering of
the services is deﬁned based on the message ﬂow direction. The orchestration is always controlled from a
single execution authority and can be categorized
into two types – static and dynamic. Static orchestration is based on prior knowledge of the functionalities
of existing services and is pre-conﬁgured. The dynamic
orchestration requires run-time service composition
by matching. Research on workﬂow systems has
demonstrated its promise in dealing with service
orchestration.

B. SOAD Modeling Framework
The basic building blocks for modeling SBS systems are
the publisher, subscriber, broker, and their interactions.
In the SOAD framework,4 the speciﬁcations of the conceptual SOA descriptions are mapped to DEVS atomic
and coupled models. In this approach, SOA compliancy for DEVS modeling is deﬁned by extending
the DEVS syntax and semantics with the SOA concepts
and principles. SOA-compliant interactions among services are supported in accordance with WSDL message
abstractions and SOAP speciﬁcation using DEVS
models that communicate with one another through
input/output (I/O) ports and couplings.
The SOA framework has a higher level of abstraction as compared with the DEVS framework. The basic
SOA model elements are divided into two groups. First,
services, service description, and messages represent the
static part of the SOA. Second, the communication
agreement, messaging framework, and service registry
and discovery represent the dynamic part of the SOA.
As shown in Table 1, a set of DEVS elements have been
developed that represent the static and dynamic aspects
of the SOA. The elements have a one-to-one

918

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

Table 1. DEVS and SOA elements.
SOA Model Elements

SOA–DEVS Model Elements

Services (publisher, subscriber, broker)
Service description entity (service-information)
Messaging framework ports and couplings
Service composition

Atomic models (publisher, subscriber, broker)
Messages entity (service-lookup and service-message)
Service registry and discovery executive model
Coupled models (primitive and composite)

correspondence with the SOA services. A detailed
mapping between DEVS and SOA can be found in
Sarjoughian et al.4
The SOAD publisher, subscriber, and broker service
speciﬁcations represent the basic structural and behavioral aspect of services. For each service, its I/O ports
are deﬁned in relation to those of other services.
Similarly, the behavior of individual services as well
as their ﬂat and hierarchial compositions are deﬁned
such that all messages can be handled according to
the DEVS and SOA speciﬁcations. The level of abstraction of the SOAP and WSDL speciﬁcation accounts
for the basic SOA interactions. Thus, SOA-compliant
DEVS speciﬁcation is consistent with SOA - i.e. service
publication and subscription via broker and direct
bindings between subscribers and publishers. The publisher, subscriber, and broker services are the basic
elements for service-oriented software systems. The
publisher publishes the service information to the
broker. When a subscriber requests service information
(i.e. service endpoint), the broker initiates a lookup
in the service repository and returns the publisher information to the subscriber. If multiple publishers exist
based on the user request, the ﬁrst publisher found in
the lookup is returned. Based on the returned publisher
information, the subscriber initiates a service request to
the publisher and the returned service data is consumed
at the subscriber. Hierarchical composition of services
is also supported. Flat and hierarchical service compositions have been developed and they are supported in
the DEVS-Suite simulator.8,30

C. DSDEVS
Component-based modeling of dynamic structure systems has been well studied.9–12 In Hu et al.,10 the
DSDEVS modeling approach has been developed to
support structural changes of parallel DEVS models.7
In Uhrmacher,11 a variable structure modeling
approach has also been developed using artiﬁcial intelligence concepts to support structural changes of DEVS
models at run-time. The capability to model and simulate dynamic structure models according to DSDEVS
was added to the DEVS-Suite simulator.8 The simulator is developed based on the Dynamic Structure

Discrete
Event
Network
(DSDEN)
system9
speciﬁcation.
The DSDEN is deﬁned as a tuple hx, Mxi, where x is
the name of an executive and Mx is the model of the
executive x. The executive model is deﬁned as a variant
of an atomic DEVS model which has an element representing the network structure and a function that
deﬁnes rules for adding and deleting DEVS model components and their couplings dynamically (i.e. during
simulation execution). The simulator uses a single
executive model for changing the structure of any modular, hierarchical parallel DEVS models. An executive
model which conforms to the DSDEN speciﬁcation is
implemented in DEVSJAVA.10 While the executive
model has the knowledge of a network model structure
at any time instance, it is not coupled to the network
model or any of its components. The dynamic structure
modeling and its implementation in DEVS-Suite is
well suited for enabling dynamic structure modeling
in SOAD.

D. ASBS System
Design and conﬁguration of a SBS system demands
making tradeoﬀs among multiple QoS features.
Satisfying multiple QoS features such as timeliness,
throughput, and accuracy requires the capability not
only to model the logical speciﬁcations of the services,
but also being able to assess their dynamic behaviors.
This is because services often operate in environments
where the services may become temporarily unavailable
due to various system and network failures, overloads
or other causes. Hence, development of SBS systems
demands the capability to monitor the changing
system status, analyze and control tradeoﬀs among
multiple QoS features and adapt their service conﬁgurations accordingly. Such systems are referred to as
ASBS.6 The conceptual view of ASBS design is depicted
in Figure 1, in which functional services used to compose the ASBS and the QoS monitoring and adaptation
systems form a closed loop. The QoS monitoring
system collects the measurements of desirable QoS features. Decisions provided by the QoS adaptation
system are made to adjust the conﬁgurations and service operations of ASBS. Use of simulated services in

Muqsith et al.

919

Figure 1. A conceptual view of ASBS design.

place of real services allows simulation-based design
and provides support for veriﬁcation and validation.

4. Approach
To simulate ASBS systems, it is important to support
run-time addition or deletion of the publishers and subscribers as well as their connections to one another and
the broker. The approach we have taken is to introduce
DSDEVS modeling Barros9 into the SOAD model.4
The dynamic structure capability is appropriate for
allowing SOA-compliant DEVS models to change
their structures during simulation. The resulting
DSOAD has a broker–executive model with a basic
set of rules for supporting subscribers and publishers
to be dynamically added or removed with proper interactions (couplings) supported between publishers and
subscribers. Next, the broker–executive model and its
design in DEVS-Suite are developed.

A. DSOAD
In compliance with the SOA speciﬁcation, the relation
between subscribers and publishers in SOAD is established through the broker. Here, the term ‘relation’
refers to the ‘communication’ between (and among)
services. The SOAD model assumes pre-deﬁned relations among broker and publishers (and subscribers).
Hence, the SOA compliance structure of the system is

deﬁned by the modeler and appropriate abstractions
are provided to relieve the modeler from creating service models at a low level of detail which complicates
model validation. However, dynamic service creation
and structural changes require one to adhere to SOA
basic principles at run-time such that the general
composition of the services can be assured.
With this goal in mind, we have developed DSOAD
as an extension to SOAD with the addition of
DSDEVS, which at its core has an executive model
component with rules for adding (and removing) services and specifying how they are interconnected. The
executive holds template structures along with rules for
SOA-compliant structure changes. In accordance with
DSDEVS, any service model developed in DSDEVS
contains the executive model that enforces SOA
compliancy. The semantics of the SOA include rules
to relate the services. For example, SOA allows a
relation between the broker and subscribers. So if a
subscriber is dynamically instantiated in DSOAD
the executive needs to ensure that it has couplings
to the broker. The structural properties identiﬁed to
make a conﬁguration SOA compliant are deﬁned
below.
1. The subscriber(s) to publishers(s) communication
must be discovered through the broker.
2. The publisher(s) can directly communicate to the
broker.

920

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

3. The subscriber(s) can directly communicate to the
broker.
4. The hierarchical composition of the publishers and
subscribers cannot violate any of the above
properties.
In addition, the direction of message ﬂow among
the broker, publisher and subscriber is also important
for SOA compliancy. Although the concept of direction of message ﬂow is not diﬀerent from the concept
already in SOAD (represented as incoming messages
through input ports and outgoing messages through
output ports), there is a need to account for this in
DSOAD. SOAD component interactions through messages are pre-deﬁned by the modeler and the modeler
takes the direction of message ﬂow into account.
However, couplings are conﬁgured at run-time in
DSOAD. Hence, the executive must contain the
knowledge of the direction of the message ﬂow
(and thus input versus output ports) so that SOAcompliant structural rules are applicable for coupled
models.
The executive model aids in enforcing SOA structural compliancy under dynamical settings. It facilitates
the establishment of relations among publishers, subscribers, and the broker. The DSDEVS executive model
by itself does not account for SOA and in particular
does not account for the concept of the broker. One
possible approach to accommodate the executive model
is to associate it with the SOAD broker model. In this
context, it is important to develop the association
between the DSDEVS executive model and the
SOAD broker. The complementary relation of the
broker and the executive is as follows.
1. The broker mediates the publisher and subscriber
relation. Similarly the executive model can facilitate
dynamic ﬂat and hierarchical structural component
relations.
2. The broker implicitly enforces the direction of
message ﬂow by responding (or not responding) to
incoming messages. The executive model can
connect I/O ports of services to ensure the correct
message ﬂow direction.
However, the following dissimilarities exist between
the broker and executive.
1. The executive can only support composite structures
or can generate structures using predeﬁned rules.
The broker concept is aimed at supporting any publisher–subscriber relation.
2. The executive model, unlike the broker, does not
conceptually distinguish between subscriber and
publisher.

3. The executive model has complete knowledge of the
service components and the structure of their composition. The broker may track publishers and subscribers that have communicated with it.
4. The executive model can support service composition and execution. The broker does not support
composition and execution.
Considering the association between the executive
and the broker models, we account for both the
broker and the executive such that together they form
the broker–executive model. Since the functionality of
the executive is to enforce constraints on the structural
composition of the services, the broker–executive
accounts for the following rules.
A. When a publisher is added to the system:
1. connect the publisher output port publish service
to the broker input port publish service.
B. When a subscriber is added to the system:
1. connect the subscriber output port identify publisher to the broker input port identify
publisher;
2. connect the broker output port found publisher to
the subscriber input port found publisher;
3. connect the subscriber output port request service
to the publisher input port request service;
4. connect the publisher output port publish service
to the subscriber input port publish service.
The rules A.1 and B.1 and B.2 establish the relations
between the broker–executive and the subscriber
and the publisher. Similarly, the rules B.3 and B.4
establish the relation between subscriber and publisher.
In SOA, the subscriber–publisher relation is discoverable with the help of the broker and initiated by the
subscriber. Interestingly, we need to account for the
cases where publisher and subscribers can be dynamically added (and removed). In such cases, we need to
resolve how and when the models and couplings should
be reconﬁgured. Considering SOA’s ‘loosely coupled’
property, delayed coupling is appropriate (i.e. create
coupling prior to communication not at the time of
instantiation). For simplicity, we include rules B.3
and B.4 as part of the broker–executive model. The
rules for removal of the publisher and subscriber can
be easily derived based on the rules that are deﬁned for
addition of services.

B. Broker–Executive Model Design
The broker–executive model in DSOAD is represented
as a SOAD broker and a DSDEVS executive.

Muqsith et al.
The model supports adding, removing, and coupling
publishers, subscribers, and the broker that is subject
to satisfying the SOA structural compliancy rules
described in Section IV.A. As deﬁned, the broker–
executive model is a single logical component of
DSOAD with its realization consisting of the
DSDEVS executive and the SOAD broker.
The DSOAD uses the simple hardware model that
was deﬁned for the SOAD. The hardware model is
deﬁned as a component that orders messages it receives
and routes using ﬁrst in/ﬁrst out (FIFO) discipline.4
The publishers and subscribers exchange messages
through this hardware component. The component
supports dynamically adding (and removing) ports
such that messages are properly routed to the intended
recipient. Since dynamic addition and removal of publishers and subscribers is supported in DSOAD, the
hardware component can accommodate I/O port
addition and removal as needed. The broker–executive
has the rules that couples publishers and subscribers to
the hardware component and also to the SOAD broker
according to the SOA rules. The important functionality of the broker–executive is twofold. First, it
facilitates the change of structure of the system
while maintaining SOA compliancy. Second, it has all
the functionalities of the SOAD broker. The broker–
executive adds couplings considering the direction of
the message ﬂow associated with the type of the
DSOAD component being added. The message ﬂow
is directed from one component output port towards
the input port of another component. So, the broker–
executive takes into account the DSOAD component
I/O port to ensure that couplings maintain compliancy.
For example, if a subscriber is added to the system, the
subscriber output port that sends a message to the
SOAD broker to discover a publisher is coupled to
the SOAD broker input port that handles the messages.
All the couplings needed to connect the subscriber to
the SOAD broker and router are created. Similarly, a
publisher addition is also handled by the broker–
executive. Multiple publishers and subscribers can
also be added in DSOAD. If addition (and removal)
are needed at the same time instance, then services can
be added to (and removed from) the system without
diﬃculty. The broker–executive handles such cases by
adding (or removing) services one at a time with zero
time advance (i.e. dt ¼ 0) using an order deﬁned by the
parallel DEVS conﬂuent function.7 In the broker–
executive model, the executive model has no DEVSbased I/O connection with the broker. However, the
two models are related. The executive uses the broker
model to create (or delete) structural connections with
the model being added (or removed). The executive can
add (and remove) SOA component models maintaining
the SOA-compliant structural relations. However, once

921
the structural compliancy is ensured the logical relations among the publisher and subscriber are maintained by the broker. This way SOA compliancy is
ensured. This is a subtle yet important relation between
the broker and executive – neither the executive nor the
broker alone suﬃces in maintaining the SOA compliancy. This is important to note as their interaction
is not apparent from DSOAD (no port connection or
message interchange). The sequence diagram shows
some of the main interactions among objects. The simulation engine in Figure 3 refers to the DEVS-Suite
simulator that executes models, facilitates DEVS message exchanges among models and schedules events.8
The underlying relation is captured in the UML
model speciﬁcations as given in Figures 2 and 3. The
sequence diagram shows some of the main interactions
among objects. Details including object instantiations
(e.g., creation of the service Publisher) are excluded for
clarity. Whenever a publisher or subscriber is added (or
removed) the executive must use the broker to add (or
remove) couplings. The pseudo-code for the executive
shows the generality of the model. The modeler can
specify time instances to add and remove SOA component models (i.e. tadd, tremove). The addSubscriber and
addPublisher methods provide the generic capability to
ensure SOA compliancy in coupling the components.
As shown in Figure 4, t, ti 2 <þ
½0,‘ denote the simulation
clock and initialization time, respectively. The time
instances for adding or removing services can be arbitrarily scheduled (i.e. tadd, tremove 2 t and the simulation
clock t is updated by dt 2 <þ
½0,‘ ). In addition, the states
as presented in Figures 4(a) and (b) are symbolic representations of the complete states of the model. For
example, the state denoted ‘Waiting’ in Figure 4(a) is
diﬀerent from the notation used to denote the phase of
the model ‘waiting’.

C. Flat and Hierarchical Model Compositions
Flat and hierarchical compositions correspond to the
visibility of the service composition with respect to the
other interested services. By visibility of a service, we
mean whether the published service interface is available (or not) to the interested services. If the interface
is available then it is public otherwise it is private.
In ﬂat composition, the services are visible to each
other. In contrast, the visibility is set at levels of hierarchy in the hierarchical composition. The hierarchical
composition can consist of publishers and subscribers
or combinations of the two. In SOA, both ﬂat and
hierarchical compositions are possible and the hierarchical composition is supported at service levels.
In DSOAD, the ﬂat and hierarchical logical structure
of services in SOAD is supported dynamically.

922

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

Executive
t : Double
dt : Double
addPublisher(pb : Publisher, br : Broker) : void
addSubscriber(sb : Subscriber, br : Broker) : void
removePublisher(pbName : String) : void
removeSubscriber(sbName : String) : void
addHPublisher(hpb : HPublisher, br : Broker) : void
removeHPublisher(hpbName : String) : void
addModel(model : IODevs) : void
removeModel(modelName : String) : void
addCoupling(src : String, srcPort : String, dest : String, destPort : String) : void
removeCoupling(src : String, srcPort : String, dest : String, destPort : String) : void
addPublisherSubscriberCoupling(pbName : String, sbName : String) : void
1
<<use>>
1
ServiceBroker
UDDI : ArrayList
available_time : Double
start : Double
name
executive : Executive
publish(srvInfoMsg : ServiceInfoMessage) : void
subscribe(srvLookupMsg : ServiceLookupMessage) : void

Figure 2. Unified Modeling Language (UML) class diagram for the broker–executive model.

simEngine :
SimulationEngine

Broker :
ServiceBroker

Exec :
Executive

1: addPublisherEvent
2: addPublisher(Publisher, Broker)
publisher :
ServicePublisher

Instantiate publisher.
Also add broker,
publisher couplings
3: PublishMessage
4: PublishMessageEvent
5: publish(ServiceInfoMessage)

6: addSubscriberEvent
subscriber :
ServiceSubscriber

7: addSubscriber(Subscriber, Broker)
Instantiate subscriber.
Also add broker,
subscriber couplings

8: SubscribeMessage

9: SubscribeMessageEvent
10: subscribe(ServiceLookupMessage)

11: addPublisherSubscriberCoupling(String, String)
Add publisher,
subscriber couplings.

Figure 3. Sequence diagram for the broker–executive model.

Add publisher, subscriber
coupling when a service match
is found.

Muqsith et al.

923

(a)
/ holdIn(“waiting”, 0)
[t=tadd]

[t=tremove]
/ holdIn(“removeModel”, 0)

/ holdIn(“addModel”, 0)

AddModel

RemoveModel
Waiting
DO / removeModel()

DO / addModel()
/ holdIn(“waiting”, dtw)

/ holdIn(“waiting”, dtw)

tadd:time at which model is added

[t=tSimulationTime]

tremove:time at which model is removed

/ holdIn(“passive”,INFINITY)

tSimulationTim:time at which simulation ends
dtw:waiting duration before next transition

(b)
/ holdIn(“passive”, startUpTime)
[QueueEmpty] / holdIn(“passive”, sigma - elapsedTime)

NotAvailable

Passive
MessageEvent
/ holdIn(“notAvailable”, 0)
/ holdIn(“active”, availableTime)

[QueueNotEmpty] / holdIn (“notAvailable”, 0)

PublishMessageEvent

SubscribeMessageEvent

/ holdIn(“publishing”, dt)

/ holdIn(“subscribing”, dt)
Subscribing

Publishing
Active

DO / Subscribe( )

DO / Publish( )
/ holdIn(“active”, sigma - elapsedTime)

/ holdIn(“active”, sigma - elapsedTime)

[t= availableTime]
/ holdIn(“passive”, INFINITY)

Figure 4. Statecharts for (a) the executive and (b) the broker models.

Services may be published publicly or privately. The
pseudo-code for adding a publisher and subscriber is
provided (see Listing 1).
The deletion of publishers and subscribers requires
removing their couplings followed by the models.
The implementation also handles the creation and
deletion of router ports.

Based on the above formulation of DSOAD, the
DEVS-Suite8 simulator is extended with the DSDEVS
API that is part of the DEVSJAVA simulator. This was
attractive since the SOAD subscriber, publisher,
broker, and router models as well as the DSDEVS
executive model are implemented in DEVS-Suite.
The extended DEVS-Suite simulator supports the

924

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)
Listing 1
Listing for code template snippets for adding and removing publisher and subscriber

void addPublisher (Publisher pb, Broker br){
// add the publisher
addModel(pb);
// connect publisher to broker
addCoupling(pb.getName(), "publish", br.getName(), "publish");
}
void addSubscriber (Subscriber sb, Broker br){
// add the subscriber
addModel(sb);
// connect subscriber to broker
addCoupling(sb.getName(), "lookup", br.getName(), "subscribe");
addCoupling(br.getName(), sb.getName(), sb.getName(), "found");
// connect subscriber to router
addCoupling(sb.getName(), "request", router.getName(), "in");
}
void addHPublisher(HPublisher hpb){
// add the hierarchical publisher
addModel(hpb);
// add hierarchical publisher to broker
addCoupling(hpb.getName(), "publish", parent.broker.getName(), "publish");
}

basic capabilities for dynamically creating SOAcompliant DEVS simulation models.

5. Voice communication system
example
To exemplify the DSOAD approach, a model of the
voice communication system (VCS) is developed in
DEVS-Suite. This is a simple example of an endto-end system capable of streaming audio data from
voice services to subscribers (i.e. clients).6 The system
can support various load scenarios given user requirements. The voice communication service publishes
various quality audio data speciﬁed by sampling rates
(e.g., 44.1 KHz) to which interested clients can subscribe to. The higher sampling rates produce higher
quality audio data as it encodes more audio information per second. For example, a sampling rate of
220.5 KHz will produce superior quality audio data
with respect to a 44.1 KHz sampling rate. The VCS
under consideration supports two channel (i.e. stereo¼ 2 channel audio data versus mono ¼ 1 channel audio
data) audio data that can be sampled at 44.1, 88.2,
136.4, 176.4, or 220.5 KHz. The subscribers can request
an audio data stream for a speciﬁed amount of time

over the network and expect to receive the desired
voice quality. The VCS can support multiple subscribers simultaneously such that each subscriber may
request diﬀerent qualities of audio data. The throughput provides a measure of the VCS performance. In
general (i.e. under normal operating modes), the VCS
throughput is proportional to the number of audio
streams being delivered. Simulation models of this
VCS system have been developed and then veriﬁed
and validated with respect to its actual .Net implementation. In particular, a model of this system with the
capability to change the number of publishers and subscribers has been developed. The resultant dynamic
behavior in terms of voice publisher throughput has
been compared to the actual implementation of the
VCS. As a general scenario, we consider subscribers
S1, S2, . , Sn to request service from any available publisher. Each publisher P1, P2, . , Pm can provide a service to one or more subscribers at some desired,
speciﬁed sampling rates.

A. Throughput Calculation
The VCS sampling rate and number of audio channels
are related to voice quality and they have an impact on

Muqsith et al.

925

the VCS throughput. For example, let us assume the
following: the given client i 2 {1, . , n}, Si is the sampling rate (Hz), Ci is the number of audio channels, B is
the bits per audio channel (bits/channel). Then the VCS
throughput for the i th subscriber is Ti ¼ Si 3 Ci 3 B
(bits per second). To maintain the voice quality for
any subscriber, the VCS generated data needs to be
transported from the publisher (VCS) to the i th subscriber end (through the network). Hence the
P net VCS
throughput for N subscriber would be T ¼ i ¼ 1. NTi
and for good audio quality the required network bandwidth (NBW) is NBW ˜ T. Considering a scenario with
one publisher (VCS) and one subscriber, let
S ¼ 220.5 KHz, C ¼ 2, B ¼ 16 bits. Then the publisher
(VCS)
throughput,
T ¼ T1 ¼ 220.5 3 2 3 16 3 103
bps ¼ 7.056 Mbps and the required network bandwidth
NBW ˜ 7.056 Mbps.

B. Flat Model Composition Example
To illustrate the core capabilities of the DSOAD framework, we consider systems consisting of ﬂat composition. A router is used to represent the network used by
services at a high level of abstraction. Also transducers
are added that collect the information regarding the
status of the publishers’ throughputs. As the simulation
progresses, additional publishers and subscribers are
dynamically added to the system and later removed.
Each subscriber can request voice data at a 44.1 or
220.5 KHz sampling rate for two-channel stereo data
encoded in 16 bits. The voice publisher throughput is
traced at approximately 0.93 simulation second intervals.
The simulation is run for 60.0 logical seconds. As DSOAD
has a continuous time base, the choice of sampling interval
and simulation run period is chosen based on the actual
VCS experiments. The implementation of the broker–
executive (see Listing 1) ensures consistent model structures (e.g., removal of publishers and subscribers and their
couplings). It is important to note that DSOAD does not
limit the basic SOA system compositions possible in
SOAD.4 However, the approach only addresses the systematic modeling of dynamic SOA system compositions
and as such solutions concerning dynamic service composition (i.e. workﬂow composition) are not addressed in

this paper. Therefore, for illustration, we consider the following dynamic scenarios:
. a single publisher, multiple subscriber system;
. a multiple publisher, multiple subscriber system;
. dynamic publisher discovery.
The above scenarios capture the basic SOA-based
dynamic system compositions and the corresponding
examples 1, 2, and 3 illustrate the modeling and simulation capability in DSOAD for such systems. In each
example, an executive model is conﬁgured with the
time and type of model to be added and deleted.
For simplicity, the broker is always added once
during the simulation start time in each scenario,
although it can be removed and added at a later time.
Similarly, a transducer (collects publisher(s) statistics)
and a network router (routes messages) are also added
at the start of the simulation. All the publishers considered in the examples are instances of the VCS. The
network conﬁguration in each example is a star topology. The simulation event instances of publisher and
subscriber model addition and removal are provided
in Table 2. The simulation duration and throughput
calculation interval are given in Table 3.

B.1. Single publisher, multiple subscriber system
(Example 1). In this scenario, we consider a single
publisher and multiple subscriber system. A publisher
P1 and subscriber S1 is added at the beginning of the
simulation (t ¼ 0.0) by the executive and it initiates the
couplings (code listing 1). Once the service is published,
the service lookup request from S1 (at t ¼ 1.0) to the
broker results in a service being found. The broker then
uses the executive to add the coupling for P1 and S1
communication (code listing 2).
Other subscribers S2, S3, S4 are added to the system
at a later time. Figure 5 shows a snapshot at simulation
time t ¼ 41 s where subscribers S2 and S3 are removed
from the system.

B.2 Multiple publisher, multiple subscriber system
(Example 2). Here, we consider a multiple publisher
and multiple subscriber system where, at t ¼ 20, P2 is

Listing 2
Listing for code snippets for dynamically coupling publisher and subscriber

//Broker searches UDDI repository
//if exists uses executive to initiate couplings
···
iExecutive.addPublisherSubscriberCoupling(publisher,subscriber);
···

926

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

Table 2. Event times of publisher and subscriber addition and
removal.
Example

Simulation
Time (s)

Component

Operation

Sampling
Rate

11.00
11.00
11.00
41.00
41.00
11.00
11.00
11.00
20.00
41.00
41.00
1.00
11.00
30.00
30.00
41.00
41.00

S2
S3
S4
S2
S3
S2
S3
S4
P4
S2
S2
S1
P1
S3
S4
S2
S3

Add
Add
Add
Remove
Remove
Add
Add
Add
Add
Remove
Remove
Add
Add
Add
Add
Remove
Remove

220.5
220.5
220.5
–
–
220.5
220.5
220.5
–
–
–
44.1
–
44.1
44.1
–
–

1

2

3

Table 3. Simulation duration and throughput interval for all
examples in Table (II).
Simulation Duration (s)

Throughput Interval (s)

60.0

0.93 (appx)

added to the system in addition to P1 and S1 at t ¼ 0.
In this case, S1 and S2 request data from P2 (after
being served by P1 at 220.5 KHz) at a 44.1 KHz
sampling rate and simultaneously get a service from
P1 (and P2).

B.3 Dynamic publisher discovery (Example 3). For
dynamic discovery demonstration, we consider another
example where, at t ¼ 1.0, subscriber S1 is added; however, no publisher exists in the system prior to adding P1
at t ¼ 11.0. As a result, the lookup request to the broker
from S1 returns a ‘‘Not found’’ message prior to t ¼ 11.0.
Once P1 is added and it publishes to the broker, the
broker creates the couplings for P1–S1 communication,
returns a service information message on the lookup
request and then S1 initiates the communication with
P1. Listing 3 shows code snippets for publisher discovery.
To show the capability of capturing dynamic characteristics of the service-based system in the DSOAD
framework, we traced the throughput of the real and
simulated VCS with similar scenario and timing characteristics. However, syncing the timing of the real
system with the simulated system is diﬃcult. We can
precisely control the simulation timing events.
However, the real system timing showed drifting for
diﬀerent runs. As we want to demonstrate that the
dynamic characteristics of SBS can be properly captured by DSOAD, precise time syncing is not required
for our purpose. As a rule of thumb, we assume that the
simulation and the real system are time synced if timestamps are the same when rounded to the nearest integer. The real versus simulation throughput of the voice

Listing 3
Listing for code snippets for dynamic discovery of publisher through broker

//Broker
if(!serviceFound) {
//Service(s) are not found
returnMsg = new ServiceInfoMessage("Not Found", null, null, null);
}
else {
//Service(s) are found
iExecutive.addPublisherSubscriberCoupling(publisher,subscriber);
returnMsg = UDDI.get(index);
}
//Publisher
if(phaseIs("publishing")) {
ServiceInfo = new ServiceInfoMessage(ServiceName,Endpoints);
m.add(makeContent("publish", ServiceInfo));
}

Muqsith et al.

927

Figure 5. Component view of a flat structure VCS model with a subscriber added at t ¼ 41 s (Example 1).

Real vs Simulated VCS Throughput

such precision control is diﬃcult in the real VCS
system) the threads get processed sequentially by the
operating system and the resultant data streaming
begins at diﬀerent time instances. The trend in the
real system and the simulation carry the signature of
the dynamic change in the system behavior. Simulating
such a dynamic system and capturing the resultant
behavior is the fundamental capability that DSOAD
provides in addition to modeling and simulating SBS
systems.

C. Hierarchical Model Composition Example
Figure 6. Throughputs for a flat structure VCS and its simulated model (Example 1).

publisher is shown in Figure 6. Each subscriber request
increases the streaming data throughput at the publisher. Once the subscribers are removed from the
system the publisher stops streaming data for that
particular subscriber and a reduction in publisher
throughput is noticed.
An interesting point to note in the graph is the
gradual increase in VCS throughput (t > 10 s) for both
the real system and the simulation. In the simulation,
S2, S3, S4 are instantiated at the same simulation clock
(t ¼ 11 s). The order of the addition of these subscribers
has no signiﬁcance. However, each subscriber’s request
goes through the router (FIFO) incurring delays, which
results in requests being processed in a sequential way
at the publisher. Therefore, the data streaming begins
at diﬀerent time instances resulting in the gradual
increase in VCS throughput. In the real system, the
behavior is attributed to the fact that all threads are
processed sequentially; hence, even if we instantiate
all the threads at the same time (approximately, as

Here, we consider a Hierarchical VCS as depicted in
Figure 7. It consists of one subscriber (i.e. S1),
one broker (i.e. B1), one router (i.e. R1), and three
publishers (i.e. HCVoiceCommService, Sampling, and
SendVoice). To demonstrate hierarchical models in
DSOAD, the Sampling and the Sendvoice are composed together to form the HCVoiceCommService in
a hierarchical structure. Broker B1 serves as the
mediator for the publisher and subscriber. The router
R1 routes messages between the publisher and subscriber. For simplicity, we chose the hierarchy in this
example not as a logical hierarchy from the SOA
perspective. However, it is important to note that
DSOAD does not prohibit or limit modeling such
scenarios. Here, the .Net implementation of VCS is
modiﬁed to show a real world example of hierarchical
composition. The VCS service is composed of two
services – SamplingService and SendVoiceService.
The SamplingService samples voice data and sends it
to the SendVoiceService that sends the data to the
subscriber. In this particular case, the simulation and
the real system hierarchical components have a oneto-one relation. The developed simulation models are
also consistent with the real system hierarchical
structure. Similar to the example in the ﬂat model

928

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

Figure 7. A component view of a hierarchical VCS model at t ¼ 0 s.

composition, we added three subscribers (S2, S3, S4) to
the system at t ¼ 10.0 s and then removed S1, S3 at
t ¼ 40.0 s. The resultant throughput for the real and
the hierarchical VCS is shown in Figure 8.

Real vs Simulated Hierarchic VCS Throughput

6. Discussion
Modeling service behavior in a dynamic scenario in
accordance with SOA has been emphasized in
DSOAD. Based on the idea of ﬁrst principles, the
service models and their interactions are derived from
speciﬁcations in the SOA. The service models support a
detailed interaction semantics and provide timing parameters to account for time-dependent behavior in
service interactions. This level of modeling is important
in simulations to address QoS prediction and performance analysis of SBS systems.6 Simulations in
DSOAD can provide information that are closely
related to the service layer. However, as DSOAD
focuses on modeling abstractions for the software, it
does not account for detailed hardware dynamics.
It accounts for the hardware at a high level of abstraction, i.e. a network router. In real systems, services may
exhibit behaviors that are strongly aﬀected by hardware
constraints. In addition, data collected on real systems
represent an aggregate level view of the system
resources. For example, Windows Performance
Object6 used in developing the above examples is closely related to the system resource and hardware and an

Figure 8. Throughputs for a hierarchical structure VCS and its
simulated model.

Activity-State-QOS (ASQ)6 model was developed to
relate the service level behavior with the traced data.
To bridge the gap between modeling the service behavior and the hardware constraints, a combined service
model and hardware model along with service and
hardware interaction called SOC–DEVS31 is under
development. This kind of modeling is more attractive
for veriﬁcation and validation with real SOA. Thus,
DSOAD can be used for simulation of adaptive SBS

Muqsith et al.

929

systems where the logical dynamical structures at the
service levels are of interest.
4.

7. Conclusion
The main contribution of this paper is the DSOAD
approach and its support in the DEVS-Suite simulator.
The SOAD framework is extended to support simulation of dynamic structure SOA-compliant DEVS
models. Hence, with this approach, dynamic servicebased systems can be modeled using the concept and
principles of DEVS and SOA and simulated in the
DEVS simulator. In particular, we deﬁned the
broker–executive model and implemented it such that
modelers can develop simulations that conform to
dynamically changing SOA structure compositions.
We developed model templates for constructing hierarchical, dynamic SOA-complaint DEVS modeling.
We exempliﬁed the role of DSOAD in simulating
dynamic SBS systems using the DEVS-Suite simulator.
Such simulation models can support the design of
ASBS systems.
There is interesting future research to be undertaken. An ongoing eﬀort is focusing on supporting
mixed real and simulated services. This kind of capability is key for having a testbed that can represent a
large number of interchangeable real and simulated
services. The capability to support a mix of real and
simulated services is important for veriﬁcation and
validation of ASBS designs. Another research direction is to support distributed simulation using web
services. For example, the DEVS/SOA environment16
can be used with the DEVS-Suite simulator.
The results of these research eﬀorts can lead to more
eﬃcient simulations enabling design and development
of complex SBS systems.

Acknowledgments

5.

6.

7.

8.
9.

10.

11.

12.

13.
14.
15.

16.

Part of this research was carried out while the second author
was with the Department of Electrical and Computer
Engineering, University of Tehran, Iran.
17.

Funding
This research was supported by NSF (grant number CCF0725340).

18.

References
1. Chen Y and Tsai WT. Distributed service-oriented software
development. Kendall/Hunt Publishing, 2008.
2. Erl T. Service-oriented architecture concepts. Technology
and design. Prentice Hall, 2006.
3. Bause F, Buchholz P, Kriege J and Vastag S. A framework
for simulation models of service-oriented architectures.
In: Kounev S, Gorton I and Sachs K (eds) SIPEW

19.

20.

(Lecture Notes in Computer Science, Vol. 5119).
Springer, 2008, pp. 208–227.
Sarjoughian HS, Kim S, Ramaswamy M and Yau SS. A
simulation framework for service-oriented computing
systems. In: Proceedings of the Winter Simulation
Conference. Miami, FL, USA, 2008, pp. 845–853.
Tsai WT, Cao Z, Wei X, Paul R, Huang Q and Sun X.
Modeling and simulation in service-oriented software
development. Simul Trans 2007; 83: 7–32.
Yau SS, Ye N, Sarjoughian HS, Huang D, Roontiva A,
Baydogan M and Muqsith MA. Towards development of
adaptive service-based software systems. IEEE Trans
Services Comput 2009; 2: 247–260.
Zeigler BP, Praehofer H and Kim TG. Theory of modeling and simulation: Integrating discrete event and continuous complex dynamic systems, 2nd ed, Academic Press,
2000.
Sarjoughian HS. DEVS-Suite http://devs-suitesim.sourceforge.net (2009, accessed January, 2010).
Barros FJ. Modeling formalisms for dynamic structure
systems. ACM Trans Model Comput Simul 1997; 7:
501–515.
Hu X, Zeigler BP and Mittal S. Variable structure in
DEVS component-based modeling and simulation.
Simul Trans 2005; 81: 91–102.
Uhrmacher AM. Dynamic structures in modeling and
simulation: A reflective approach. ACM Trans Model
Comput Simul 2001; 11: 206–232.
Zeigler BP. Toward a simulation methodology for variable structure modeling. In: Elzas MS, Zeigler BP and
Oren TI (eds) Modeling and simulation of methodology
in the artificial intelligence era. Amsterdam: North
Holland, 1986, pp. 185–210.
Narayanan S and Mcilraith S. Analysis and simulation of
web services. Comput Networks 2003; 42: 675–693.
DARPA Markup Language for Services http://
www.daml.org/services (accessed January 2009).
Narayanan S. Reasoning about actions in narrative
understanding. In: Proceedings of International Joint
Conference on Artificial Intelligence. 1999, pp. 350–358,
Morgan Kaufmann, San Francisco, CA.
Mittal S, Risco-Martı́n JL and Zeigler BP. DEVS/SOA:
A Cross-platform framework for net-centric modeling
and simulation in DEVS unified process. Simul Trans
2009; 85: 419–450.
Zeigler BP, Hall SB and Sarjoughian HS. Exploiting
HLA and DEVS to promote interoperability and reuse
in Lockheed’s corporate environment. Simulation Journal
1999; 73: 288–295.
DEVSJAVA, ‘Arizona Center for Integrative Modeling
and
Simulation’
http://www.acims.arizona.edu/
SOFTWARE (2001, accessed January 2009).
Li BH, Chai X, Di Y, Yu H, Du Z and Peng X. Research
on service oriented simulation grid. In: Proceedings of the
8th
International
Symposium
on
Autonomous
Decentralized Systems 2005; 7–14, Chengdu, China.
IEEE Standard for Modeling and Simulation (M&S)
High Level Architecture (HLA) – Framework and
Rules, Version 1516-2000, IEEE, 2000.

930

Simulation: Transactions of the Society of Modeling and Simulation International 87(11)

21. IBM Product Lifecycle Management http://www03.ibm.com/solutions/plm/index.jsp (accessed December
2008).
22. Open Grid Services Architecture http://www.globus.org/
ogsa (accessed December 2008).
23. Extensible Modeling and Simulation Framework http://
sourceforge.net/projects/xmsf/ (2003, accessed December
2008)
24. Tsai WT, Fan C and Chen Y. DDSOS: A dynamic distributed service-oriented simulation framework. In:
Proceedings of The 39th Annual Simulation Symposium
2006; 160–167, Huntsville, AL.
25. IONA Interface Simulation Testing Framework http://
www.iona.com/solutions/it_solutions/istf.htm (accessed
January 2009).
26. Web Services Description Language http://www.w3.org/
TR/wsdl (accessed January 2010).
27. Simple Object Access Protocol http://www.w3.org/TR/
soap/ (accessed January 2010)
28. Universal Description Discovery and Integration http://
www.uddi.org/pubs/uddi_v3.htm (accessed January
2009).
29. Extensible Markup Language http://www.w3.org/XML/
(accessed January 2010)

30. Kim S, Sarjoughian HS and Elamvazuthi V. DEVS-Suite:
A simulator for visual experimentation and behavior monitoring. In: High Performance Computing & Simulation
Symposium, Proceedings of the Spring Simulation
Conference. San Diego, CA, March 2009, pp. 1–7.
31. Muqsith MA and Sarjoughian HS. A simulator for service-based software system co-design. In: Proceedings of
the 3rd International ICST Conference on Simulation
Tools and Techniques, SIMUTools 2010. Torremolinos,
Malaga, Spain, 2010, pp. 1–9.

APPENDICES
I. Appendix: DSOAD simulation
environment
The examples just described are developed in DEVSSuite, an open source simulator with an API that
supports DSOAD.31 The simulator is eﬃcient30 – the
wall clock time periods required for executing the above
simulations are 0.8 s and 0.9 s for the examples shown
in Figures 9 and 7, respectively. We also ran

Figure 9. DEVS-Suite simulator UI (User Interface) depicting a flat structure VCS model.

Muqsith et al.

931

Simulation Runtime

Figure 10. Simulation run-time efficiency (refer to Figures 5
and 7).

simulations with more than 100 subscribers for both the
ﬂat and hierarchical scenarios to compare the wall
clock time periods for executing the simulations
(see Figure 10). We ran the simulation 10 times for
each data point and took the average. Under ideal
conditions we would expect the hierarchical model to
be less eﬃcient. In general, the simulator’s performance
degrades as the hierarchy of a model increases due to
additional I/O messaging. However, for a model with
hierarchy of depth two, the performance degradation is
negligible as compared with a ﬂat model. The computer
has an Intel Core 2 Duo CPU E8200 at 2.66 GHz and
3.23 GB of memory and an Intel 82566DM Gigabit
network card set at 100 Mbps. The operating system
is Microsoft Windows XP Professional with Service
Pack 3, .NET version 3.5 and JDK version 1.5. For
a comparative view, we also run the same scenario in
the real VCS system with similar timing events and
collected the voice throughput samples. The real
VCS experiment was also executed using the same
machine conﬁguration. It should be noted that
the simulator’s User Interface (UI) supports dynamic
addition and removal of block components (see
Table 2), but not the creation of I/O and state trajectories. Plotting trajectories for components that are
added during simulation execution remains as future
work.

Hessam S. Sarjoughian received a Ph.D. degree in electrical and computer engineering from the University of
Arizona, Tucson, Arizona, USA in 1994. He is an
Associate Professor of Computer Science and
Engineering at Arizona State University, Tempe,
Arizona, USA. He is a co-director of the Arizona
Center for Integrative Modeling & Simulation
(ACIMS). His research focuses on modeling and simulation theory, methodology, model composability, codesign modeling, visual simulation modeling, and
agent-based simulation. He lead the establishment of
the online M&S Masters of Engineering Degree
Program at Arizona State University in 2004.
Mohammed A. Muqsith is currently a Ph.D. candidate in
computer science at Arizona State University, Tempe,
Arizona, USA. He is a member of the Arizona Center
for Integrative Modeling & Simulation at Arizona State
University. His research interest includes simulationbased design, service-oriented software/hardware
co-design, system performance modeling, software engineering and networked embedded systems.
Dazhi Huang is a Ph.D. student in computer science at
Arizona State University, Tempe, Arizona, USA. He
received the BS in computer science from Tsinghua
University in China. His research interests include middleware, mobile and ubiquitous computing, and workﬂow
systems
in
service-oriented
computing
environments.
Stephen S. Yau is a fellow of the IEEE and the American
Association for the Advancement of Science. He received
a Ph.D. degree in electrical engineering from the
University of Illinois, Urbana. He is currently the director
of the Information Assurance Center and a Professor of
Computer Science and Engineering at Arizona State
University, Tempe. He was previously with the
University of Florida, Gainesville, and Northwestern
University, Evanston, Illinois. He served as the president
of the IEEE Computer Society and the editor-in-chief of
Computer. His current research is in distributed and service-oriented computing, cloud computing, software
engineering, ubiquitous computing, trust management,
cyber situation awareness, and data privacy.

Timing Specification and Analysis for Service-Oriented Simulation
W. T. Tsai, Hessam S. Sarjoughian, Wu Li, Xin Sun
School of Computing and Informatics
Arizona State University
Tempe, AZ 85287
{wtsai, sarjoughian, wu.li, xin.sun}@asu.edu
Abstract
This paper is an attempt to complement service-oriented
simulation with the timing concept of the DEVS formalism.
One of the key features of DEVS is its timing specification;
however, currently timing information is not available in
service-oriented models such as WSDL and BPEL yet. This
paper presents timing specification and associated analysis
techniques for PSML, a service-oriented modeling
language. Specifically, timing constraints such as delays,
processing time, deadlines among elements can be specified,
and consistency among timing constraints can be verified.
The timing information can be used for static analysis to
estimate time needed as well as dynamically to verify
runtime behavior of service-oriented application.
Keywords: Service-oriented simulation, DEVS, timing
specifications and analysis.

1. Introduction
This paper deals with time-based service-oriented
simulation. Modeling time is important for service-based
software systems and necessary when services must
complete their tasks within specified timing constraints.
Service-oriented models such as WSDL and BEPL do not
support specifying time yet even though the timing
constraints can be added. Thus, services with timing
information stands to significantly strengthen design,
implementation, and testing of complex, real-time atomic
and composite services.
In this paper, the PSML (Process Specification and
Modeling Language) [13] that supports development of
service-based software system is extended to quantify
timing of services. DEVS (Discrete Event System
Specification) [19] is a component-based modeling and
simulation approach and it supports specification of timing.

2. Related Work
Service-oriented simulation has been an active research
area recently, and some existing projects include XMSF
(Extensible Modeling and Simulation Framework) [1][15],
the simulation grid system Cosim-Grid [6] and GridSim [2],
Interface Simulation and Testing Framework (ISTF),
Dynamic
Distributed
Service-Oriented
Simulation

Copyright held by SCS.

1

(DDSOS)[9], Dynamic Service-Oriented Collaboration
Simulation (DSOCS) [10] , simulation framework with
Microsoft Robotic Studio[11], and SOA DEVS (SOAD)
[16]. XMSF creates a modeling and simulation framework
that utilizes a set of web-enabled technologies to facilitate
modeling and simulation applications. XMSF involves
Web/XML, Web services, and internet/networking to
improve the interoperability. Cosim-Grid is a serviceoriented simulation grid based on HLA, PLM (Product
Lifecycle Management) [4], and grid/web services. It
applies OGSA (Open Grid Services Architecture) [7] to
modeling and simulation to improve HLA in terms of
dynamic sharing, autonomy, fault tolerance, collaboration,
and security mechanisms. GridSim is an open-source
simulation framework that allows users to model and
simulation the characteristics of Grid resources and
network. It provides intensive support for grid simulation,
such as workload trace simulation, jobs allocation, and
network traffic simulation [2]. ISTF [5] is an extensible
SOA simulation tool, and it simulates the end-to-end
distributed application scenarios and demonstrates how
individual components will interface with each other in
production.
DDSOS is an SOA simulation framework that provides
simulation runtime services and supports. DDSOS has
runtime infrastructure (RTI) like the one in HLA. Within
this framework, simulation code can be dynamically
generated and configured whenever demanded by the users.
The DDSOS framework provides the two layers of
modeling support by PSML, an on-demand automated
dynamic code generator (service) can generate executable
code for simulation and for real applications directly from
the model (specification) written in PSML, an on-demand
automated dynamic code deployment service, an simulation
engine engines, and an extended RTI.
As SOA development is mainly model-driven, and it is
different from traditional software development [3], thus
SOA simulation will be model-driven and the modeling
languages used will have a great impact on the lifecycle of
service-oriented application development. Each component
in the simulation framework, including the simulation
engines as well as application components and tasks, are
modeled as services or workflows using various modeling
languages such as BPEL or PSML-S [13], and DEVSML
[17]. The activities that can be simulated include service
publishing, service discovery, service composition, dynamic

architecture, reconfiguration, dynamic collaboration, policy
enforcement. For example, PSML can model serviceoriented applications, simulation code can be automatically
generated, and various kinds of analyses can be performed
on the model specified.
SOA allows services to be discovered and composed at
design or runtime, and for those systems composed at
runtime, its behavior is determined at runtime. The DSOCS
framework [11] addresses this issue by integrating the SOA
dynamic collaboration and simulation concepts, and it
supports modeling and simulating systems with the
distributed, interactive, and discrete–event driven focuses.

2.1. Service-Oriented Simulation Approaches
One approach to service-oriented simulation views
simulation as an integrated part of application development.
Simulation is used to verify a service-oriented model, and
also used at runtime to validate that the application
execution produces the same results as simulation [11][13].
Another important feature is the policy enforcement which
often needs to be simulated [14], as policy software cannot
be executed alone, before policies can be deployed to
enforce constraints. In this way, service-oriented simulation
carefully follows the service-oriented models and lifecycles.
In this approach, the goal is not to simulate the support
function of SOA, but instead use these services as a part of
simulation infrastructure.
Another approach for service-oriented simulating
outgrows from the DEVS framework. The SOA DEVS
(SOAD) approach is developed to support representing and
simulating service-oriented applications [16]. In this
approach, DEVS formalism and the SOA principles together
enable simulating basic aspects of service-oriented
applications such as discovery, composition, publication,
and subscription. Currently, the DEVS-Suite is an objectoriented simulator implemented in Java [19][20]. Unlike
DEVS/SOA which enables execution of models as services
[17], the DEVS-Suite simulator is not service-based.
It is important for SOA DEVS models to be simulated as
services and take advantage of dynamic publication,
discovery, composition, and policy enforcement offered by
SOA. This is similar to an object-oriented simulation, which
was mainly based on object-oriented framework and
designed to take advantage of polymorphism, dynamic
binding offered by object orientation.
This paper provides a way to unify these two approaches
by incorporating the timing information into the serviceoriented modeling language PSML.

3. Specifying Timing in Service-Oriented
Model
This paper proposes several timing specification
techniques to the service-oriented model language PSML
(Process Specification and Modeling Language) based on
ACDATER (Actors, Conditions, Data, Actions, Timing, and
Events, Relations). Specifically, this paper uses Delay, Min

2

(for minimum time needed), Max (for maximum time
needed), Deadline, and Distribution to Actors, Conditions,
and Actions.

3.1. Timing Specifications
The timing information is specified by various guards
including uni-guards, bi-guards, and n-guards. As Data
elements do not have timing information associated with
them, only Actor, Condition, Action elements have timing
specifications. An event is considered as an epoch. Each
model can have zero or more guards, referred as the guard
set, to specify timing and other relationships among
ACDATER elements.
Uni-Guards: These express timing constraints involving a
single element and they can be applied to Actors,
Conditions and Actions. There are four different types of
uni-guards:
• Delay specifies the minimum time that the concerned
elements must wait before computing. Delay is often
used for real-time processing to establish the
environment before execution. An example is a heart
defibrillator where its capacitor needs to be charged
before applying therapy.
• Deadline specifies the time within which the
computation or communication must be completed.
• Min specifies the minimum amount of time that
computation or communication will take.
• Max specifies the maximum amount of time the
computation or communication will take.
• Distribution specifies the time distribution function for
computation or communication. The distribution largely
depends on the characteristics of the input such as size.
A distribution function can be deterministic or
stochastic such as Poisson distribution.
A deadline associated with an element can be from three
sources following the three-party structure of SOA:
providers, clients, or brokers.
• A deadline imposed by a provider can be a QoS
guarantee that the provider is willing to provide for all
of its clients.
• A broker may act as independent agent to verify the
deadline provided by a provider satisfies the stated
specification. With the broker’s assurance, a client has
more confidence of using the timing specification
provided by the provider.
• A deadline imposed by a client is the requirements that
the client desires for a specific application, and the
provider may or may not be able to provide.
In ACDATER specification, timing specifications provided
by providers are associated with elements, but timing
requirements for clients are carried by the current active
process during simulation. As a computation is being
carried out, the timing requirements by clients will be
updated (e.g., at some regular time intervals) to see if the
client requirements are being met. Furthermore, based on
the current status, the active process may make runtime

decision to select different services for execution based on
the timing specification associated with elements supplied
by providers. This will be further discussed in Section 7.
Note that a Deadline should be equal or smaller than the
sum of Max and Delay. Otherwise the Deadline is
meaningless.
Figure 1 shows a simple workflow for an account update
process to illustrate the above timing concepts. It consists
of three services login, update, and logout. The workflow is
modeled as an Actor; login, update, and logout services are
modeled as Actions; IsBlank is a condition. The update
service needs to verify the IsBlank condition is satisfied
before the update is performed. The login service can have
the following timing constraints:
Delay (login) = 0;
Deadline (login) = 0.09;
Min (login) = 0.01;
Max (login) = 0.1;
⎧ 0.01, 0 < x ≤ 10 ⎫
⎪ 0.05,10 < x ≤ 100 ⎪
⎪
⎪
+.
f ( x) = ⎨
⎬, x ∈ N
x
0.08,100
<
≤
1000
⎪
⎪
⎪⎩ 0.1, x > 1000
⎪⎭
This means that the login should act immediately without
delay to minimize customer waiting. The minimum
processing time is 0.01 second, and the maximum
processing time is 0.1 second to reduce customer waiting.
f(x) defines the timing distribution for the login service in
terms of the number of concurrent users x. The timing
requirement will be relaxed with a specified value according
to the distribution function.

Figure 1: A simple account update process
Bi-Guards: They express timing constraints with respect to
two elements in the ACDATER model, and these timing
constraints are also specified using Delay, Deadline, Min,
Max, and Distribution.
• Delay specifies the minimum time that the second
element must wait after completing the first element.
• Deadline specifies the time that the two elements must
be completed.
• Min specifies the minimum amount of time to complete
the two elements.
• Max specifies the maximum amount of time to
complete the two elements.
• Distribution specifies the distribution function.
The relation between the deadline and max is similar to
uni-guards. The following examples show the bi-guards
between the login, update, and logout services. The first biguard shows that after finishing the login service, the update

3

service must wait for 0.1 second to ensure that the database
is properly activated. The second bi-guard shows that these
two operations must be completed within 0.2 second.
Delay (login, update) = 0.1;
Deadline (update, logout) = 0.2;
N-Guards: They express timing constraints with respect to
n elements in the ACDATER model, and these timing
constraints are also specified using Delay, Deadline, Min,
Max, and Distribution where n can be any positive integer
number. Thus one can have tri-guards and quad-guards. For
example, an if-then-else construct requires a tri-guard, and a
sequence of four steps requires a quad-guard.
The following descriptions show more uni-guards and biguards examples.
Actor Administrator: 
Delay (administrator)=0;
Deadline (administrator)= 0.4;
This means that to start the workflow in the Actor, the delay
for the system should be zero, and the entire workflow
should be completed within 0.4 seconds.
Action Login:
Delay (login) = 0;
Min (login) = 0.01;
Max (login) = 0.1;
Deadline (login) = 0.09;
Deadline (login, update) = 0.1;
Action Update:
Delay (update) = 0;
Min (update) = 0.01;
Max (update) = 0.1;
Deadline (update) = 0.09;
Deadline (login, update) = 0.1;
Condition IsBlank:
Delay (IsBlank) = 0;
Deadline (IsBlank) = 0.1;
Action Logout:
Delay (logout) = 0;
Min (logout) = 0.01;
Max (logout) = 0.1;
Deadline (logout) = 0.09;
Deadline (update, logout) = 0.1;
3.2. Consistency of Timing Constraints
Timing constraints specified should be consistent with
each other. For example, a bi-guard must be consistent with
those uni-guards involved in the bi-guard. Figure 2 shows
an if-then-else process. After the services0 verifies the
condition, it can either choose to execute service1 or
services2 depending on the evaluation result. The tri-guard,
as it involves three elements and thus tri-guard, must be
consistent with individual uni-guards and two bi-guards.

Delay(Service1) = Delay(Service2)
Deadline
(Service0,
Service1,
Service2)
=
Deadline(Service0)
+
maximum(Deadline(Service1),
Deadline(Service2)) < 10
Min(Service0, Service1, Service2) = Min(Service0) +
maximum(Min(Service1), Min(Service2)
Max(Service0, Service1, Service2) = Max(Service0) +
maximum(Max(Service1), Max(Service2)
Figure 2: An if-then-else process
Bi-guards should be consistent with the uni-guards:
Delay1 = Delay (Service0, Service1)
= Delay (Service0) + Delay (Service1)
Delay2 = Delay (Service0, Service2)
= Delay (Service0) + Delay (Service2)
Min1 = Min(Service0, Service1)
= Min(Service0) + Min(Service1)
Min2 = Min(Service0, Service2)
= Min(Service0) + Min(Service2)
Max1 = Max (Service0, Service1)
= Max(Service0) + Max(Service1)
Max2 = Max (Service0, Service2)
= Max(Service0) + Max(Service2)
Tri-guards should be consistent with bi-guards:
Delay (Service0, Service1, Service2) = Delay (Service0)
+ or(Delay1, Delay 2);
Min (Service0, Service1, Service2) = Min(Service0) +
minimum (Min 1, Min 2);
Max (Service0, Service1, Service2) = Max(Service0) +
maximum (Max 1, Max 2);
Deadlines are different from Min, Max and Delay. Biguards Deadline (service0, service1) might not be equal to
the Deadline (service0) + Deadline (service1). The same
also applies for the Deadlines for tri-guards.

Figure 3: Two active services in one process

4. Simulation Architecture
Models that are developed according to the extended PSML
approach can be simulated as shown in Figure 4. The
simulation environment consists of Event Generator,
Simulation Engine, and Simulation Analysis Engine. The
Event Generator generates events to drive the simulation.
The Simulation Engine executes simulation code given
simulation configuration files and generated events. The
results of the simulations are stored in log files. The
Simulation Analysis Engine uses the log files at runtime to
generate data depicting how the model dynamics are
changing under timing constrains. These results can also be
visualized and evaluated by modelers.

3.3. States in ACDATER
Actor, Condition and Action elements in ACDATER
have states. Their states can either be active or inactive. An
active state of Actor, Condition and Action means the
element is under execution. Similarly, the inactive state
means the element is idle.
For a workflow, it can either have only one element at
the active states or multiple elements at the active states
simultaneously. When multiple elements are in the active
state simultaneously, the workflow might need
synchronization among the elements.
Guards could be applied to the elements at the active
states to help the synchronization. Figure 3 shows Service1
and Service2 are in the active state at the same time. The
requirement for this flow is that Service1 and Service2 need
to start at the same time and the whole flow needs to finish
in 10 seconds. Deadline of the tri-guards can be applied to
guarantee a sub-flow can be finished within a certain time,
and the delay for Service1 and Service2 need to be
synchronized to make sure the two services start at the same
time.

4

Figure 4: Simulation Environment

5. Simulation Architecture
Once the service-oriented model has the timing information,
it can follow a service-oriented life cycle where application
templates, collaboration templates, workflows, and services
can be published and discovered for reuse to rapidly develop
the simulation model.
Figure 5Error! Reference source not found. shows the
simulation lifecycle as the following phases:
• Service-oriented requirement and design phase: The
model takes the system requirements and then discovers
the needed services and workflows from the existing
repositories. Note that not only do the services and
workflows need to match the functionality requirement;

•

•

•

•

they also need to meet the timing constraints specified
by the timing specifications. If the needed services or
workflows are not available, it will be necessary to
either develop them or modify the existing services or
workflows if their source code or design is available.
The modified or new items will be published in various
repositories for publishing so that they can be reused.
Model evaluation phase: This phase evaluates the
application model developed in the previous phase.
Specifically, all the timing inconsistencies will be
detected and eliminated at this phase.
Code generation phase: This phase generates code
from the application model developed in the previous
phase with identified or developed services and
workflows.
Simulation execution and monitoring phase: This
phase executes the application code including sending
messages to participating services at remote sites.
Furthermore, the execution is monitored and data are
collected for analysis.
Data analysis and evaluation phase: This phase will
analyze the data collected in the previous phase, and the
evaluation results will be used to guide the modification
of the application model in the next cycle. After
completing this step, the process goes back to the
service-oriented requirement and design phase. This
phase will be further explained in Section 6.

information associated with timing and other information.
The information tracked can also be used to evaluate system
security, reliability, and integrity. Note that this is a
simplified version of general SOA data provenance
architecture.

Figure 6: Time Tracking SOA Architecture
When the services used are from third parties and their
source code is not available, the tracking service can only
work in a non-intrusive way by intercepting messages sent
between services and communication bus. For example,
computation time, tcomputation, can be computed from tresponse trequest. When a number of request/response pairs were
recorded, the average, min, max computation time can be
calculated. When an input classifier is added to the tracking
service, the timing distribution function can be obtained.
If the service code is available, the tracking service can
work in an intrusive way by adding code to services
recording the time when interested events happen, therefore
more information, such as delay, can be obtained.
Similarly, as a full specification is available for
workflow tracking, more interesting timing information can
be collected, such N-guards. Figure 7Error! Reference
source not found. shows how to collect the timing
information for a tri-guard. The small circles represent
points of interest in a workflow. In this example, the time
when Service0 receives a request, and the time Servcie1
and Service2 send out response are the points of interest.
With the information collected, Min, Max, Delay can be
easily computed for this tri-guard.

Figure 5: Model-Driven Service-Oriented Simulation
Process
Note that this simulation lifecycle can be an integrated part
of SOA application lifecycle where simulation plays a key
role in verifying and validating system requirements and
performance.

6. Data Collection and Model Tuning
Following the SOSE (Service-Oriented System
Engineering) [8] approach, timing information can be
collected for each Actor, Condition, Action, and Events in a
model. The collected information can be used to tune the
model so that it can better reflect the system.
Figure 6Error! Reference source not found. shows an
SOA communication bus architecture with time tracking
services. As one can see, each service and workflow is
bundled with a tracking service that keeps tracking all the

5

Figure 7: Time Data Collection for Workflow
Data collection and model tuning can happen in three
different stages:
1. Simulated system and simulated environment: At the
initial stage of development, a model is specified for
the system that needs to be produced. Some initial
assumptions, such as the value for Min, Max for
primitive parameter set, can be made for each model
element. The values for more complex input set can

be obtained by running simulation with different
simulated environments.
2. Real system and simulated environment: At this
stage, a system based on the model in stage 1 has
been built, so data can be collected by running the
system with the simulated environment. Some
incorrect assumptions made in stage 1 may be found
and corrected at this stage.
3. Real system and real environment: This stage is
similar to stage 2. The only difference is at this
stage, the data collected is no longer from
simulations, but real execution of the system.
Figure 8 shows the relationship among the three stages.
After the 3-stage tuning, the model will reflect the behaviors
of the real system.
D

at
a

co
lle

ta
Da

ct
io

n

n
io
ct
lle
co

Figure 8: Data Collection and Model Tuning

7. Dynamic Simulation with Aid from Static
Analysis
Essentially, timing analysis for a service-oriented
application can be obtained by aggregating the time needed
for the entire applications from the participating services,
workflows, application templates and collaboration
templates in a bottom-up manner. The bottom-up process
starts from analyzing timing requirement for atomic
services, which usually involves gathering timing
constraints using data collection technique. Once the timing
information about atomic services is known, the timing
information about composite services or workflows that call
only atomic services can now be computed. During
simulation, a process that carries out the computation carries
its own timing requirements. During the simulation
execution, whenever a service is called, the calling process
needs to provide its timing requirements to match with the
timing information of the called services to verify timing
requirements against timing capabilities.
Figure 9 shows the process for the static analysis and
dynamic simulation.

Figure 9: Dynamic simulation with aid from static
analysis
Simulation modeling: Simulation modeling is done in
the PSML modeling tool. The tool can model the services,
workflows and the timing constraints for all services and
workflows.
Code generation: Code generation includes element
generation, path generation and timing constraint derivation.
Element generation will generate the simulation code for all
the atomic elements. Timing constraints can be attached to
the generated code. Path generation will generate the
workflow among the elements.
Timing constraint derivation will derive the timing
constraints based on the generated workflow. The timing
constraints generation algorithm is similar to the N-Guard
timing constraint generation algorithm.
Static simulation analysis: Static simulation analysis
will need at least one set of simulation result and all the
existing timing constraints. The analysis will verify the
correctness and consistency of the current timing model and
evaluate the timing performance of the system.
For example, if a uni-guard specification for service1 is
defined as Deadline (service1) =5 and the simulation result
to run this service1 is 10 second, then the simulation result
is inconsistent with the timing constraints. This means either
the existing timing specifications need to be updated or a
different service or workflow should be used.
Dynamic simulation: Dynamic simulation can either
refer to runtime path selection or runtime timing constraint
configuration. Runtime path selection means given different
timing constraints, the simulation engine would dynamically
choose path to satisfy the timing constraints.
For example, suppose there is a 3-way switch.

Figure 10: Switch flow sample
In Figure 10, the bi-guard between service0 and service1
is defined as Delay (service0, service1) = 5. Now, suppose
the runtime simulation reports that when choosing the path
service0 to service1, the result is 10 seconds. In this way,
runtime path selection will choose the second path service0

6

action

0.15 sec 0.30 sec
2.5 sec

ch
ar
se

se
ar
ch

In this section, a case study based on a popular robotic
game, sumoBot competition, is provided to illustrate the
enhanced timing information for PSML. The main purpose
of sumoBot is to have the robot stay inside of an arena while
trying to push the opponent out. Timing plays an important
role in order to win this competition.
The strategy used is to actively search for the opponent
(Search), and once a target is located, the robot speeds up
and tries to push it off the arena (Chase). However, at any
time during the search or chase process a BorderDetected
event is generated and sent to the system, the robot must
preempt its current action, and start to perform a
Backup&Turn action to avoid going off the arena. A PSML
model for the sumoBot competition has the following
elements:
Actor: sumoBot
Action: Search, Chase, Backup&Turn
Event: BorderDetected
Condition: TargetFound, TargetLost
Figure 11 shows an event-based workflow for the
competition using PSML model. Some important timing
constraints are listed as follows:
• Within 0.1 second after a target is found, a Chase
action must be followed.
• With 0.5 second after a target is lost, a Search
action must be followed.
• Within 0.05 second after a BorderDetected event is
received, a Backup&Turn action must be followed.
This constraint has the top priority.
• A Backup&Turn action should not last more than
2 seconds.

ba
ck
&t
ur
n

8. Case Study

Min (Search) = 1;
Max (Search) = ∞ ;
Deadline (Search) = ∞ ;
Deadline (TargetLost, Search) = 0.5;
Action Chase:
Delay (Chase) = 0;
Min (Chase) = 1;
Max (Chase) = ∞ ;
Deadline (Search) = ∞ ;
Deadline (TargetFound, Chase) = 0.1;
Action Backup&Turn:
Delay (Backup&Turn) = 0;
Min (Backup&Turn) = 0.5;
Max (Backup&Turn) = 2;
Deadline (Backup&Turn) = 2;
Deadline (BorderDetected, Backup&Turn) = 0.05;
Condition TargetFound:
Delay (TargetFound) = 0;
Deadline (TargetFound) = 0.1;
Deadline (TargetFound, Chase) = 0.1;
Condition TargetLost:
Delay (TargetLost) = 0;
Deadline (TargetLost) = 0.1;
Deadline (TargetLost, Search) = 0.5;
According to the data collected during the first
simulation, a time trajectory, as shown in Figure 12Error!
Reference source not found., can be drawn. Static timing
analysis reports that there are 3 violations of time
constraints: the time slot between Search and Chase is 0.15
sec, longer than the specified 0.10 sec; it took 0.30 sec for
the robot to respond to the BorderDetected event, while the
constraint is 0.05 sec; Backup&Turn lasted 2.5 sec,
exceeding the time constraint by 0.5 sec.

ch
as
e

to service2. In this manner, the simulation program
dynamically decide which service to run based on timing
constraints attached to the concerned services.
Runtime timing constraint configuration means the
simulation engine would configure the timing constrains at
runtime. The constraints come from the static analysis step.

event

sumoBot
Border
Detected

Search

Border detected
condition

Back&Turn

TargetFound
No
Yes
Yes

Chase

time
TargetFound==True
TargetLost

No

Figure 11: sumoBot workflow
The timing information and constraints can be imported
into the PSML using the schema explained in Section 3 as
follows:
Action Search:
Delay (Search) = 0;

7

Figure 12 Action, Event and Condition Time Trajectories
After improving the services and workflow, and tuning
the model, the simulation was rerun. Error! Reference
source not found. Figure 13 shows a new timing trajectory.

se
ar
ch

se

ba
ck
&t
ur
n

[3]

h
rc

ch
a

a
se

[4]

[5]
[6]

[7]
Figure 13 Action, Event and Condition Time
Trajectories
If there are multiple Backup&Turn service
implementations, dynamic service selection technique can
be applied. After the workflow is deployed on the robot,
system keeps tracking whether if any service exceeds the
required execution time and tries to switch a different
implementation to eliminate the violations. In Error!
Reference source not found., for example, the
Backup&Turn service exceeded the required execution
deadline, therefore, the system will select another service
that has the shortest execution time guarantee out of the
available implementations.

[8]

[9]

[10]

9. Conclusion
This paper proposes timing specifications into a serviceoriented model and applies timing specification in analysis
and simulation. Timing specification can be analyzed to
ensure their completeness and consistency by static analysis
as well as by simulation. Timing information can also be
obtained by simulation. As service-oriented simulation can
be an integrated part of service-oriented application
development, timing specification and analysis will enable
simulation to obtain more accurate results. A distinct feature
of the timing specification is that timing diagrams can be
automatically generated once the simulation is performed,
and the simulation is performed using the simulated code
that is generated from a service-oriented model.

[11]

[12]

[13]

10. References
[1]

[2]

D. Brutzman, M. Zyda, M. Pullen, and K. L. Morse,
“Extensible Modeling and Simulation Framework
(XMSF): Challenges for Web-Based Modeling and
Simulation:
XMSF
2002
Findings
and
Recommendations Report: Technical Challenges
Workshop and Strategic Opportunities Symposium”,
October 2002.
R Buyya, A. Sulistio, “Service and Utility Oriented
Distributed Computing Systems: Challenges and
Opportunities for Modeling and Simulation
Communities”, The 41st Annual Simulation

8

[14]

[15]

[16]

Symposium (ANSS), Ottawa, April 14-16, 2008, pp.
68-81.
Y. Chen and W. T. Tsai, Distributed ServiceOriented Software Development, Kendall/Hunt, 2008.
IBM. “PLM: Product Lifecycle Management.”
“http://www-03.ibm.com/solutions/plm/index.jsp “
(27 February 2007).
INOA.
ISTF
http://www.iona.com/solutions/
it_solutions/istf.htm.
B. H. Li, X. Chai, X., Y. Di, H. Yu, Z. Du, and X.
Peng, “Research on Service Oriented Simulation
Grid”, In proceedings of the 8th International
Symposium on Autonomous Decentralized Systems
(ISADS 2005), April 4-8, 2005, pp. 7-14.
“Towards Open Grid Services Architecture” <
http://www.globus.org/ogsa/ > (27 February 2007).
W. T. Tsai, “Service-Oriented System Engineering:
A New Paradigm”, Proc. of IEEE International
Workshop on Service-Oriented System Engineering
(SOSE), October 2005, pp. 3 - 8.
W. T. Tsai, C. Fan, and Y. Chen, “DDSOS: A
Dynamic Distributed Service-Oriented Simulation
Framework”, The 39th Annual Simulation
Symposium (ANSS), Huntsville, AL, April 2006, pp.
160-167.
W. T. Tsai, Q. Huang Q, J. Xu, Y. Chen, and R.
Paul,
“Ontology-based
Dynamic
Process
Collaboration in Service-Oriented Architecture”,
Proc. of IEEE International Conference on ServiceOriented Computing and Applications (SOCA),
2007, pp. 39-46.
W. T. Tsai, X. Sun, Q. Huang, and H. Karatza, “An
Ontology-Based
Collaborative
ServiceOriented Simulation Framework with Microsoft
Robotics Studio”, Simulation Modelling Practice and
Theory 16 (2008), pp. 1392-1414
W. T. Tsai, Q. Huang, X. Sun, and Y. Chen,
"Dynamic Collaboration Simulation in ServiceOriented Computing Paradigm", in Proceedings of
40th Annual Simulation Symposium (ANSS), March
2007, Norfolk, VA, USA, pp. 41-48.
W. T. Tsai, Z. Cao, X. Wei, R. Paul, Q. Huang, and
X. Sun, “Modeling and Simulation in ServiceOriented Software Development”, Special Issue on
Modeling and Simulation for and in ServiceOrientated Computing Paradigm, Simulation Journal,
Vol. 83, No. 1, January 2007, pp. 7-32.
W. T. Tsai, X. Zhou, X. Wei, “A Policy Enforcement
Framework for Verification and Control of Service
Collaboration”, Information Systems and E-Business
Management, Springer, Sep, 2007, pp. 83-107.
XMSF, “SAIC Web-Enabled RTI.” 2003.
http://www.movesinstitute.org/xmsf/
projects/WebRTI/XmsfSaicWebEnabledRtiDecembe
r2003.pdf, (27 February 2007).
Sarjoughian, H., Kim, S., Ramaswamy, M., Yau, S.
“A Simulation framework for Service-oriented

[17]

[18]

computing Systems”, Winter Simulation Conference
2008, Miami, FL, USA.
Mittal, S., José L. Risco, Bernard P. Zeigler, “DEVSBased Simulation Web Services for Net-Centric
T&E”, Spring Simulation Conference, 2007.
Zeigler, B. P., Praehofer, H., & Kim, T. G. (2000).
Theory of Modeling and Simulation: Integrating

9

[19]
[20]

Discrete Event and Continuous Complex Dynamic
Systems Second Edition: Academic Press.
DEVS-Suite, http://acims1.eas.asu.edu/WebStarts.
S. Kim, (2008), Simulation of service Based System:
Modeling and Implementation using the DEVS-Suite.
Department of Computer Science and Engineering,
Arizona State University, Tempe, AZ, USA.

